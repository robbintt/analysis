---
ver: rpa2
title: Learning Representations in Video Game Agents with Supervised Contrastive Imitation
  Learning
arxiv_id: '2509.11880'
source_url: https://arxiv.org/abs/2509.11880
tags:
- learning
- supcon
- actions
- action
- games
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Supervised Contrastive Imitation Learning
  (SCIL), a novel approach that applies Supervised Contrastive Learning (SupCon) to
  Imitation Learning (IL) for video game agents. The method aims to learn better state
  representations by structuring the latent space according to action-relevant factors,
  improving decision-making and generalization.
---

# Learning Representations in Video Game Agents with Supervised Contrastive Imitation Learning

## Quick Facts
- **arXiv ID:** 2509.11880
- **Source URL:** https://arxiv.org/abs/2509.11880
- **Reference count:** 17
- **Primary result:** SCIL improves imitation learning performance in video game agents by 3.33% to 33.47% in Atari games and achieves 37.5% completion rate on Returnal boss phase versus 7.5% for baselines

## Executive Summary
This paper introduces Supervised Contrastive Imitation Learning (SCIL), a novel approach that applies Supervised Contrastive Learning (SupCon) to Imitation Learning (IL) for video game agents. The method aims to learn better state representations by structuring the latent space according to action-relevant factors, improving decision-making and generalization. SCIL modifies SupCon to handle continuous action spaces through discretization and positional encoding, while avoiding data augmentations that could distort spatial information critical for game environments. Experiments on 3D games (Astro Bot and Returnal) and 2D Atari games show that SCIL improves learning convergence and policy performance compared to baseline models.

## Method Summary
SCIL integrates Supervised Contrastive Learning into imitation learning by adding a contrastive loss term to the standard prediction loss. The method discretizes continuous action dimensions, applies mixed-radix positional encoding to create single categorical labels, and computes SupCon loss using these labels as pseudo-classes. Unlike standard SupCon, SCIL avoids geometric data augmentations to preserve spatial information crucial for game environments. The approach handles edge cases where mini-batch samples lack positive pairs by implementing safeguards against division-by-zero errors.

## Key Results
- In Atari games, policies trained with SCIL achieved an average score improvement of 3.33% to 33.47% over baselines
- For Astro Bot, SCIL policies showed higher success rates across checkpoints compared to baselines
- For Returnal, SCIL policies achieved 37.5% completion rate of the first boss phase versus 7.5% for baselines
- Validation error decreased faster with SCIL, demonstrating more efficient learning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Structuring the latent space by action labels improves representation quality for imitation learning agents.
- **Mechanism:** The SupCon loss pulls embeddings of observations with identical discretized actions together (positive pairs) while pushing apart embeddings with different actions (negative pairs). This creates a latent geometry where the distance between embeddings reflects decision-relevant similarity rather than mere visual similarity.
- **Core assumption:** Observations leading to the same action share common factors of variation that should be embedded closely; action-relevant features are the primary determinants of good decision-making representations.
- **Evidence anchors:** [abstract] "The goal is to obtain latent representations of the observations that capture better the action-relevant factors, thereby modeling better the cause-effect relationship from the observations that are mapped to the actions performed by the demonstrator."
- **Break condition:** If actions are poorly correlated with observation features (e.g., highly stochastic policies or inconsistent demonstrators), clustering by action may introduce noise rather than signal.

### Mechanism 2
- **Claim:** Discretizing continuous action dimensions and encoding them as single categorical labels enables SupCon to operate in multi-dimensional, mixed action spaces.
- **Mechanism:** Each continuous action dimension is binned into discrete intervals; a mixed-radix positional encoding (Equation 2) converts the multi-dimensional discrete vector into a single integer label. This label defines class membership for contrastive pairing without altering the regression head for action prediction.
- **Core assumption:** Nearby values in continuous action space are functionally similar enough to be treated as the same "class" for representation learning purposes; binning granularity preserves this similarity without excessive fragmentation.
- **Evidence anchors:** [Section II-B1] "This allows us to define similarity in the continuous space by treating nearby values within the same bin as equivalent."
- **Break condition:** If bin granularity is too coarse, dissimilar actions merge into the same class; if too fine, insufficient positive pairs per batch may destabilize training.

### Mechanism 3
- **Claim:** Avoiding geometric data augmentations preserves spatial information critical for decision-making in video game environments.
- **Mechanism:** Standard contrastive learning uses augmentations (crop, flip, rotate) to create positive pairs. SCIL uses action-label identity alone, avoiding transformations that distort spatial cues (e.g., target positions, obstacle distances) on which actions depend.
- **Core assumption:** Spatial configurations in game frames are causally relevant to actions; augmentations that alter these configurations violate state-action consistency.
- **Evidence anchors:** [Section I] "In video game environments, precise spatial configurations are crucial, not just object identity. The arrangement of game entities directly impacts the agent's behavior."
- **Break condition:** In domains where spatial invariance is desirable (e.g., some navigation tasks with rotational symmetry), avoiding augmentations may limit generalization.

## Foundational Learning

- **Concept: Supervised Contrastive Learning (SupCon)**
  - **Why needed here:** SCIL builds directly on SupCon's loss formulation; understanding how positive/negative pairs are formed and how temperature scaling affects embedding geometry is essential.
  - **Quick check question:** Given a batch of embeddings and their class labels, can you manually compute which samples form positive pairs and how the loss gradients pull/push them?

- **Concept: Imitation Learning (Behavioral Cloning)**
  - **Why needed here:** SCIL is applied as a regularizer alongside supervised action prediction; understanding the baseline IL setup (feature extractor + policy head) clarifies where and how SupCon integrates.
  - **Quick check question:** What is the standard loss for behavioral cloning with discrete vs. continuous actions, and where does overfitting typically manifest?

- **Concept: Mixed-Radix Positional Encoding**
  - **Why needed here:** Converting multi-dimensional discretized actions to single labels requires understanding how varying bases per dimension produce unique indices.
  - **Quick check question:** Given action dimensions with bin counts [5, 3, 2], what label does the action vector [2, 1, 1] map to?

## Architecture Onboarding

- **Component map:** Feature extractor (CNN/encoder) → embedding vector (normalized) → policy head (action predictions) and SupCon loss module (takes embeddings + discretized action labels)

- **Critical path:** 1) Forward pass through feature extractor to obtain embeddings, 2) Normalize embeddings (L2) for cosine similarity in SupCon, 3) Discretize continuous actions using configured bin counts; apply mixed-radix encoding to generate labels, 4) Compute SupCon loss using label-based positive/negative masking, 5) Backpropagate combined loss; monitor both prediction loss and SupCon loss separately

- **Design tradeoffs:**
  - **Bin count (B):** Higher B → finer action distinction but fewer positive pairs per batch; lower B → more positive pairs but potential action conflation
  - **Batch size:** Larger batches increase probability of multiple positive pairs per class; small batches risk anchors with no positives (handled by safeguard but suboptimal)
  - **Temperature (τ):** Lower τ sharpens distribution (harder contrasts); higher τ softens (more uniform). Default in code: 0.07

- **Failure signatures:**
  - **Division-by-zero or NaN in loss:** Anchor with no positive pairs in batch; increase batch size or coarsen binning
  - **No improvement over baseline:** SupCon loss may be underweighted or binning misconfigured; verify label distribution per batch
  - **Embedding collapse:** All embeddings near identical; check normalization and temperature settings

- **First 3 experiments:**
  1. **Baseline replication:** Train IL agent with prediction loss only; establish performance baseline on chosen environment (e.g., Atari game)
  2. **Bin sweep:** Add SupCon loss with B ∈ {5, 9, 11} for continuous action dimensions; compare validation error curves and policy performance
  3. **Batch size ablation:** For best B, test batch sizes {64, 128, 256} to assess positive-pair availability impact on convergence speed

## Open Questions the Paper Calls Out

- **Question:** What is the optimal strategy for selecting the number of discretization bins for continuous action dimensions, and how does this choice interact with action space dimensionality?
- **Question:** How does SCIL perform in robotics and physical control domains where spatial observations and continuous actions are prevalent?
- **Question:** What is the minimum batch size required to ensure sufficient positive pair coverage, and how does batch composition affect learning stability?
- **Question:** Can selective data augmentations that preserve action-relevant spatial relationships (e.g., color jittering, harmless crop-and-resize) further improve SCIL's generalization?

## Limitations
- Proprietary 3D game datasets (Astro Bot, Returnal) prevent independent verification of the most compelling results
- No systematic guidance provided for selecting optimal discretization bin counts, a critical hyperparameter
- The approach assumes action-label identity alone is sufficient for positive pair generation, potentially limiting generalization

## Confidence
- **Atari results:** High confidence (public dataset, reproducible setup)
- **3D game results:** Medium confidence (proprietary datasets, potential domain-specific optimizations)
- **Discretization approach:** Medium confidence (theoretically sound but under-validated for complex action spaces)
- **Augmentation avoidance:** High confidence for spatial-critical tasks, but may limit generalization in some domains

## Next Checks
1. **Architecture transparency audit:** Request and verify the complete CNN architectures (layer counts, kernel sizes, hidden dimensions) for both Atari and 3D game experiments to ensure reproducibility.

2. **Hyperparameter sensitivity analysis:** Systematically test the impact of bin counts (B=5, 9, 11), temperature (τ=0.07), and batch sizes (64, 128, 256) on validation error and policy performance across multiple Atari games.

3. **Generalization stress test:** Apply SCIL to a non-Sony 3D environment (e.g., DeepMind Control Suite or OpenAI Procgen) to verify that the action-label-driven embedding structure generalizes beyond the proprietary datasets.