---
ver: rpa2
title: Large Language Models Assisting Ontology Evaluation
arxiv_id: '2507.14552'
source_url: https://arxiv.org/abs/2507.14552
tags:
- ontology
- evaluation
- ontologies
- llms
- suggestions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces OE-Assist, a framework using large language
  models (LLMs) to assist ontology evaluation through competency question (CQ) verification.
  The authors present OntoEval, a dataset of 1,393 CQs paired with ontologies and
  stories, and evaluate both automatic and semi-automatic evaluation methods.
---

# Large Language Models Assisting Ontology Evaluation

## Quick Facts
- arXiv ID: 2507.14552
- Source URL: https://arxiv.org/abs/2507.14552
- Authors: Anna Sofia Lippolis; Mohammad Javad Saeedizade; Robin Keskisärkkä; Aldo Gangemi; Eva Blomqvist; Andrea Giovanni Nuzzolese
- Reference count: 38
- Primary result: LLM-assisted ontology evaluation achieved 0.66-0.72 macro-F1, with correct suggestions improving user accuracy by 13% but incorrect suggestions reducing it by 28%, resulting in no net improvement

## Executive Summary
This paper introduces OE-Assist, a framework that leverages large language models to assist ontology evaluation through competency question (CQ) verification. The authors present OntoEval, a dataset of 1,393 CQs paired with ontologies and stories, and evaluate both automatic and semi-automatic evaluation methods. The framework uses LLMs to verify whether ontologies can answer CQs by generating SPARQL queries and binary labels. While automatic evaluation showed promising results with o1-preview and o3-mini achieving 0.66 and 0.72 macro-F1 respectively, the user study revealed a nuanced trade-off: correct suggestions improved accuracy by 13% but incorrect ones reduced it by 28%, resulting in no net improvement. The study highlights both the potential and limitations of LLM-assisted ontology evaluation.

## Method Summary
The study uses OntoEval, a dataset containing 1,393 CQs paired with ontologies and user stories, to evaluate LLM performance in CQ verification. The method involves prompting LLMs (o1-preview, GPT-4o-0513, o3-mini) with a task description, CQ, story, and ontology to generate binary verification labels and SPARQL queries. Automatic evaluation measures macro-F1 against gold standard labels on the full dataset. Semi-automatic evaluation involves a user study with 19 ontology engineers using a Protégé plugin interface that displays LLM suggestions alongside ontologies. Users can accept, reject, or override suggestions. The study employs statistical tests including paired t-tests, chi-square, and Cohen's d to analyze results.

## Key Results
- Automatic evaluation: o1-preview achieved 0.66 macro-F1, o3-mini achieved 0.72 macro-F1, outperforming GPT-4o-0513
- User study: Correct suggestions improved accuracy by 13%, incorrect suggestions reduced accuracy by 28%
- No net improvement observed in semi-automatic evaluation (no statistically significant difference between assisted and unassisted modes)
- Users found tasks slightly easier with LLM suggestions, but learning effects were diminished
- Starting in unassisted mode led to better learning outcomes compared to starting in assisted mode

## Why This Works (Mechanism)

### Mechanism 1: LLM-Based Semantic Alignment for CQ Verification
LLMs can assess whether an ontology functionally satisfies a competency question by reasoning about semantic correspondence between natural language requirements and formal axioms. The LLM receives a CQ, ontology, and user story, then generates a binary verification label and SPARQL query demonstrating whether the CQ can be answered. Core assumption: LLMs trained on code and structured data can reliably map natural language questions to ontology patterns and assess sufficiency of modeling. Evidence: o1-preview and o3-mini achieved 0.66 and 0.72 macro-F1 respectively, approaching human-level performance. Break condition: LLM accuracy degrades when ontologies lack labels/comments, CQs are under-specified, or domain-specific terminology falls outside training distribution.

### Mechanism 2: Suggestion-Driven Decision Modulation
Human accuracy in CQ verification is modulated by LLM suggestion correctness—improving when suggestions are correct but degrading when incorrect. Users integrate LLM suggestions (label + SPARQL) into their evaluation workflow, using them as starting points. Core assumption: Users exhibit anchoring bias toward LLM output, trusting suggestions without sufficient independent verification. Evidence: Correct suggestions improved accuracy by 13%, but incorrect ones reduced it by 28%. Statistical significance confirmed via chi-squared tests. Break condition: When LLM accuracy drops below ~70%, negative impact of incorrect suggestions may outweigh positive gains from correct ones.

### Mechanism 3: Perceived Difficulty Reduction with Learning Trade-off
LLM assistance reduces perceived task difficulty but may inhibit skill development through reduced "desirable difficulty." Suggestions lower cognitive load by providing starting points, but this shortcut reduces the effortful processing that supports learning and transfer. Core assumption: Effort during evaluation contributes to skill acquisition; reducing effort too early hampers long-term competence. Evidence: Users found tasks slightly easier with LLM suggestions, though learning effects were diminished. Those starting unassisted improved substantially after switching to assistance (+0.17 accuracy), while those starting assisted showed almost no change (-0.04 accuracy). Break condition: Novices trained exclusively with assistance may fail to develop independent evaluation skills.

## Foundational Learning

- Concept: Competency Questions (CQs) as Functional Requirements
  - Why needed here: CQs define what an ontology should answer; verification tests whether the ontology meets functional intent, not just structural validity.
  - Quick check question: Given "What was the disposition of the organ at a specific time?", can you identify the classes and properties required?

- Concept: SPARQL Query Generation for Verification
  - Why needed here: SPARQL provides executable evidence that a CQ can be answered; users can inspect queries to validate LLM reasoning.
  - Quick check question: For CQ "Who built this organ?", what SPARQL triple pattern would confirm this is modeled?

- Concept: Functional vs. Structural Ontology Evaluation
  - Why needed here: The paper addresses functional adequacy (does the ontology answer the CQ?), distinct from structural correctness (is the OWL valid?).
  - Quick check question: If an ontology passes SHACL validation but cannot answer any CQ, what type of evaluation failure is this?

## Architecture Onboarding

- Component map: Input (CQ, Ontology, Story) -> LLM Engine (o1-preview/ o3-mini) -> Output (Binary label + SPARQL query) -> Interface (Protégé + web UI) -> Evaluation (Accuracy/difficulty metrics)

- Critical path:
  1. Load ontology in Protégé
  2. Present CQ + story to LLM via prompt
  3. Generate suggestion (label + SPARQL)
  4. Display suggestion alongside ontology in UI
  5. User reviews, validates, or overrides
  6. Record accuracy and perceived difficulty

- Design tradeoffs:
  - Model selection: o1-preview ($851/run, higher accuracy) vs. o3-mini ($25/run, competitive F1)
  - Suggestion format: Label + SPARQL (full context) vs. SPARQL-only (less anchoring)
  - Mode sequence: Assisted-first (immediate support) vs. Unassisted-first (stronger learning)

- Failure signatures:
  - Low suggestion accuracy → net negative user performance
  - Missing ontology labels → LLM and user confusion
  - Domain-specific CQs → semantic correctness unassessable without expertise

- First 3 experiments:
  1. Baseline calibration: Run o3-mini on 50 OntoEval CQs; compare against gold standard; expect ~0.72 accuracy.
  2. Safeguard test: Pre-validate SPARQL queries before display; measure reduction in accuracy degradation from incorrect suggestions.
  3. Learning curve: Run user cohort with unassisted→assisted sequence; compare accuracy trajectory to assisted→unassisted cohort.

## Open Questions the Paper Calls Out

### Open Question 1
Can interface mechanisms such as confidence scores or automated pre-checking of SPARQL queries mitigate the performance drop observed when LLMs provide incorrect suggestions? The authors suggest this as future work, noting that wrong suggestions caused a 28% accuracy drop.

### Open Question 2
What is the minimum LLM accuracy threshold required to achieve a net positive impact on user performance in semi-automated ontology evaluation? The study used o1-preview at approximately 75% accuracy, which resulted in neutral net performance.

### Open Question 3
How can LLM-assisted interfaces be designed to provide support without hindering the long-term skill acquisition and learning effects of ontology engineers? The study found diminished learning curves in assisted conditions, suggesting the need for dedicated learning phases.

## Limitations
- Single 1.5-hour user session limits generalizability to longer-term usage patterns and skill development
- Effectiveness highly dependent on LLM accuracy thresholds that were not systematically explored
- Does not address ontologies lacking documentation or containing domain-specific terminology outside LLM training distributions
- No comparison with alternative suggestion formats (e.g., SPARQL-only, confidence scores)

## Confidence

- High confidence: LLM models achieving 0.66-0.72 macro-F1 in automatic evaluation, approaching human performance
- Medium confidence: User study results showing differential impact of correct vs. incorrect suggestions on accuracy
- Medium confidence: Perceived difficulty reduction findings and learning trade-off observations
- Low confidence: Long-term learning effects and skill development implications due to single-session study design

## Next Checks

1. **Accuracy Threshold Analysis**: Systematically vary LLM accuracy rates (e.g., 60%, 70%, 80%, 90%) to identify the break-even point where correct suggestion benefits no longer outweigh incorrect suggestion costs.

2. **Extended Longitudinal Study**: Conduct multi-week user studies tracking accuracy, learning curves, and skill retention when ontology engineers use the framework regularly versus sporadically.

3. **Domain Transferability Test**: Evaluate framework performance across diverse ontology domains (medical, legal, engineering) with varying levels of documentation quality to assess generalizability limits.