---
ver: rpa2
title: 'EgoCross: Benchmarking Multimodal Large Language Models for Cross-Domain Egocentric
  Video Question Answering'
arxiv_id: '2508.10729'
source_url: https://arxiv.org/abs/2508.10729
tags:
- video
- question
- segment
- answer
- identification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EgoCross, a new benchmark for evaluating
  cross-domain generalization of multimodal large language models (MLLMs) in egocentric
  video question answering (EgocentricQA). Existing benchmarks focus on common daily
  activities, but real-world applications require models to generalize to unfamiliar
  domains like surgery, industry, extreme sports, and animal perspective.
---

# EgoCross: Benchmarking Multimodal Large Language Models for Cross-Domain Egocentric Video Question Answering

## Quick Facts
- arXiv ID: 2508.10729
- Source URL: https://arxiv.org/abs/2508.10729
- Authors: Yanjun Li; Yuqian Fu; Tianwen Qian; Qi'ao Xu; Silong Dai; Danda Pani Paudel; Luc Van Gool; Xiaoling Wang
- Reference count: 40
- Most existing MLLMs struggle to generalize beyond daily-life domains, with average accuracy below 55% on CloseQA and below 35% on OpenQA.

## Executive Summary
EgoCross introduces a new benchmark for evaluating multimodal large language models' (MLLMs) cross-domain generalization capabilities in egocentric video question answering. While existing benchmarks focus on common daily activities, real-world applications require models to handle unfamiliar domains like surgery, industry, extreme sports, and animal perspectives. The benchmark comprises approximately 1,000 QA pairs across 798 video clips spanning four key tasks: prediction, recognition, localization, and counting, with both OpenQA and CloseQA formats for fine-grained evaluation. Extensive experiments reveal that most MLLMs struggle significantly with domain generalization, showing a 1.6× performance drop compared to standard daily-life benchmarks.

## Method Summary
EgoCross evaluates cross-domain generalization by testing MLLMs on egocentric video question answering across four unfamiliar domains: surgery, industry, extreme sports, and animal perspective. The benchmark includes ~1,000 QA pairs from 798 video clips, covering four task categories (Identification, Localization, Prediction, Counting) with 15 sub-tasks. Evaluation uses both CloseQA (multiple-choice, 25% random baseline) and OpenQA (free-form with LLM-as-a-Judge scoring 0-5). The main experiments use zero-shot, single-round inference with video frames sampled at 0.5-1 fps. Pilot studies employ Qwen2.5-VL-7B with vLLM on 8× H100 GPUs, comparing Supervised Fine-Tuning (SFT: lr=1e-6, 12 epochs) versus Reinforcement Learning (GRPO: lr=1e-6, 16 epochs, beta=0.04).

## Key Results
- MLLMs show substantial performance degradation when tested on domains beyond daily activities, with average accuracy falling below 55% on CloseQA and below 35% on OpenQA
- A 1.6× performance drop is observed when comparing the same question types from EgoSchema to EgoCross, confirming the challenge of domain generalization
- Reinforcement learning shows the most significant average improvement of 22% across domains compared to supervised fine-tuning, achieving 60.12% accuracy versus 43.47% for SFT on CloseQA

## Why This Works (Mechanism)

### Mechanism 1: Domain Shift as a Stress Test
Standard MLLMs trained on web-scale data dominated by third-person or daily egocentric views fail when presented with specialized domains like surgery or extreme sports. The feature extractors cannot map visual inputs to correct semantic tokens due to visual style and semantic content mismatches. The 1.6× performance drop from EgoSchema to EgoCross quantifies this distribution shift, revealing models rely on spurious correlations rather than robust physical reasoning.

### Mechanism 2: Reinforcement Learning for Policy Alignment
RL optimizes a reward signal for correctness rather than forcing the model to mimic specific question-answer pairs like SFT. This allows the model to explore reasoning paths that maximize rewards for temporal localization or prediction in unfamiliar settings, rather than just pattern matching text. The significant 22% average improvement demonstrates RL's ability to adapt reasoning strategies to domain-specific logic.

### Mechanism 3: Fine-Grained Task Taxonomy
Decomposing capabilities into Identification, Localization, Prediction, and Counting isolates specific reasoning failures. Models often fail not just at seeing objects but reasoning about their trajectory or next state in novel contexts. This granular evaluation reveals that high-level reasoning degrades faster under domain shift than low-level perception.

## Foundational Learning

- **Domain Generalization vs. Domain Adaptation**: EgoCross evaluates zero-shot generalization to new domains, while pilot studies explore adaptation through fine-tuning/RL. Distinguishing these explains the 1.6× drop versus 22% gain. *Quick check: Does adding training data from "Industry" improve performance on "Surgery" (generalization) or only "Industry" (specialization)?*

- **Egocentric Video Biases**: Egocentric video features head motion, hand-object interaction, and task-oriented attention, differing significantly from "Animal Perspective" or "Extreme Sports." *Quick check: Why might a model trained on human cooking videos fail to interpret skydiving video motion patterns?*

- **OpenQA Evaluation (LLM-as-a-Judge)**: The paper uses an LLM to grade open-ended answers. Understanding this judge's bias is critical as RL optimization exploits the reward mechanism. *Quick check: If the Judge LLM prefers detailed explanations, might the RL model game the metric by being verbose rather than precise?*

## Architecture Onboarding

- **Component map**: Video clip -> Frame extractor (0.5 fps) -> Vision Encoder (CLIP/ViT) -> Projector -> LLM Backbone -> Generate JSON response -> Evaluator (LLM-as-a-Judge for OpenQA)

- **Critical path**: Data Loading: Load video clip -> Sample frames (0.5 fps) -> Apply domain-specific prompt (if using Pilot Study method) -> Vision Encoder -> Projector -> LLM -> Generate JSON response -> Extract prediction field -> Compare to ground truth (CloseQA) or send to Judge LLM (OpenQA)

- **Design tradeoffs**: Frame Rate: Fixed 0.5 fps vs. original FPS (lower fps reduces compute but risks missing fast actions). SFT vs. RL: SFT is stable but may overfit to small dataset; RL shows higher ceiling (22% gain) but is computationally expensive. Close vs. OpenQA: CloseQA is deterministic and easy to score; OpenQA captures nuance but introduces Judge bias.

- **Failure signatures**: Hallucination in "Surgery" (generic tools vs. specific ones). Temporal Blindness (answers based on last frame only). Instruction Misalignment (GPT-4o referencing frame indices instead of timestamps).

- **First 3 experiments**: 1) Baseline Establishment: Run Qwen2.5-VL-7B on "Surgery" split to verify ~37-46% accuracy range. 2) Ablation on Prompting: Implement "Domain Prompt" strategy to see if context improves Identification scores without training. 3) Judge Consistency Check: Run subset of OpenQA predictions through LLM judge and manually verify semantic equivalence scoring.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do egocentric-specialized models (e.g., EgoGPT, EgoVLPv2) consistently underperform general-purpose MLLMs on cross-domain tasks despite being trained on relevant egocentric data?
- Basis in paper: Table 2 shows EgoGPT (30.66%) and EgoVLPv2 (27.10%) scoring significantly lower than general models like Qwen2.5-VL-7B (44.82%), suggesting current specialized training may lead to overfitting on daily activities.
- Why unresolved: The paper highlights this "surprising" performance gap but doesn't isolate whether the failure is due to architecture, training data distribution, or input frame processing limitations.
- What evidence would resolve it: An ablation study retraining specialized models with diverse domain data, or feature space analysis comparing specialized vs. general visual encoders' robustness to domain shifts.

### Open Question 2
- Question: What specific mechanisms allow Reinforcement Learning to yield significant cross-domain improvements where Supervised Fine-Tuning shows inconsistent results?
- Basis in paper: Section 4.4 notes RL provided significant 22% average increase across all domains while SFT improved only Industry performance.
- Why unresolved: The paper establishes RL's empirical success but doesn't explain why it's more effective at bridging the domain gap than standard supervised approaches.
- What evidence would resolve it: Comparative analysis of gradient updates or attention map shifts during RL vs. SFT training, focusing on how RL handles out-of-distribution visual features.

### Open Question 3
- Question: How can model architectures be adapted to maintain instruction-following fidelity and temporal reasoning in high-speed or novel environments?
- Basis in paper: Section 8.1 notes proprietary models often fail to follow simple instructions (GPT-4.1 returning frame indices instead of timestamps) or struggle with temporal localization when motion blur or speed is introduced.
- Why unresolved: The analysis identifies the bottleneck is often the "brittle" application of high-level skills in new contexts rather than pure perception, but offers no architectural solution.
- What evidence would resolve it: Testing models with specialized temporal encoders or explicit instruction-tuning on specific output formats required by cross-domain benchmarks.

## Limitations
- The benchmark's 1,000 QA pairs across 798 clips provide reasonable coverage but remain limited in scale compared to daily-activity benchmarks like EgoSchema (2,420 QA pairs), potentially constraining domain generalization claims.
- Evaluation relies on LLM-as-a-Judge for OpenQA scoring, introducing potential bias based on the judge's linguistic preferences rather than pure semantic correctness.
- RL experiments show promising gains but use a specific reward model configuration that isn't fully detailed, making it difficult to assess whether improvements generalize beyond EgoCross or could be replicated with alternative reward structures.

## Confidence
- **High Confidence**: The core finding that MLLMs struggle with cross-domain generalization in egocentric video (1.6× performance drop, accuracy below 55% on CloseQA) is well-supported by experimental data and consistent across multiple models and domains.
- **Medium Confidence**: The effectiveness of RL versus SFT is demonstrated but relies on specific hyperparameters and a particular reward model configuration that isn't fully transparent.
- **Medium Confidence**: The claim that domain shift specifically impacts high-level reasoning tasks more than low-level perception is supported by task decomposition but could be confounded by linguistic complexity differences across question types.

## Next Checks
1. **Scale Sensitivity Analysis**: Replicate domain generalization experiments with varying training set sizes (50%, 70%, 100% of available EgoCross data) to determine whether performance gaps are primarily due to limited training data versus fundamental domain shift challenges.

2. **Judge Consistency Verification**: Conduct manual audit of 100 randomly selected OpenQA responses scored by the LLM judge, comparing judge scores to human annotations to quantify potential systematic biases in the evaluation methodology.

3. **Cross-Domain Transfer Testing**: Train models on three of four domains (e.g., Surgery, Industry, Extreme Sports) and test on held-out domain (Animal Perspective) to distinguish between genuine cross-domain generalization versus memorization of domain-specific patterns.