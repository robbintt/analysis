---
ver: rpa2
title: 'RobustSpring: Benchmarking Robustness to Image Corruptions for Optical Flow,
  Scene Flow and Stereo'
arxiv_id: '2505.09368'
source_url: https://arxiv.org/abs/2505.09368
tags:
- robustness
- flow
- corruptions
- optical
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "RobustSpring introduces a comprehensive benchmark for evaluating\
  \ robustness to image corruptions across optical flow, scene flow, and stereo vision\
  \ tasks. The dataset applies 20 diverse corruptions\u2014including noise, blur,\
  \ weather, and compression artifacts\u2014in time-, stereo-, and depth-consistent\
  \ ways to the high-resolution Spring dataset, creating 20,000 challenging images."
---

# RobustSpring: Benchmarking Robustness to Image Corruptions for Optical Flow, Scene Flow and Stereo

## Quick Facts
- arXiv ID: 2505.09368
- Source URL: https://arxiv.org/abs/2505.09368
- Reference count: 40
- Introduces comprehensive benchmark for robustness to image corruptions across optical flow, scene flow, and stereo vision tasks

## Executive Summary
RobustSpring introduces a comprehensive benchmark for evaluating robustness to image corruptions across optical flow, scene flow, and stereo vision tasks. The dataset applies 20 diverse corruptions—including noise, blur, weather, and compression artifacts—in time-, stereo-, and depth-consistent ways to the high-resolution Spring dataset, creating 20,000 challenging images. A novel corruption robustness metric based on Lipschitz continuity disentangles accuracy and robustness, enabling systematic comparison. Initial evaluations of 16 models reveal significant performance drops under corruptions, with weather and noise being most detrimental. Transformer-based architectures generally show better robustness, though no model excels across all corruption types. The benchmark highlights that accurate models are not necessarily robust, underscoring the importance of dedicated robustness evaluation. RobustSpring is available at spring-benchmark.org and serves as a critical tool for advancing robust dense matching models in real-world conditions.

## Method Summary
The authors created RobustSpring by applying 20 different corruption types to the high-resolution Spring dataset, which includes stereo, optical flow, and scene flow annotations. The corruptions are applied consistently across time, stereo pairs, and depth maps to maintain the geometric relationships. A novel corruption robustness metric based on Lipschitz continuity is introduced to disentangle accuracy and robustness in model evaluation. The benchmark includes a diverse set of 16 models spanning different architectures and approaches, allowing systematic comparison of their behavior under various corruption conditions.

## Key Results
- 20 diverse corruptions (noise, blur, weather, compression artifacts) cause significant performance drops across all evaluated models
- Weather and noise corruptions are most detrimental to model performance
- Transformer-based architectures generally show better robustness compared to CNN-based methods
- No single model excels across all corruption types, highlighting the challenge of achieving universal robustness
- Accurate models are not necessarily robust, demonstrating the importance of dedicated robustness evaluation

## Why This Works (Mechanism)
The benchmark works by systematically applying corruptions that preserve the geometric relationships between frames, stereo pairs, and depth maps while challenging the model's ability to extract consistent features across tasks. The Lipschitz-based metric effectively captures how model outputs change in response to input perturbations, distinguishing between models that maintain stability under corruption versus those that exhibit high sensitivity to input changes.

## Foundational Learning
Why needed: Understanding the relationship between corruption types and model performance is crucial for developing robust vision systems.
Quick check: Review how different corruption types affect each task (optical flow, scene flow, stereo) to identify common failure patterns.

Why needed: The Lipschitz-based metric provides a principled way to separate accuracy from robustness in model evaluation.
Quick check: Verify that the metric correctly identifies models that are accurate but not robust, and vice versa.

Why needed: Time-, stereo-, and depth-consistent corruptions maintain the geometric relationships essential for multi-task evaluation.
Quick check: Confirm that corrupted sequences preserve temporal consistency and stereo/depth coherence across frames.

## Architecture Onboarding

Component map: Input images -> Corruption application -> Model inference -> Robustness evaluation

Critical path: The core evaluation pipeline processes corrupted inputs through each model and measures performance using the Lipschitz-based robustness metric, comparing results against clean input baselines.

Design tradeoffs: The benchmark balances between comprehensive corruption coverage and computational feasibility, applying all 20 corruption types across multiple tasks while maintaining manageable dataset size.

Failure signatures: Models typically fail catastrophically under severe weather corruptions, show gradual degradation under blur and noise, and exhibit task-specific vulnerabilities where performance in one modality doesn't predict performance in others.

First experiments:
1. Evaluate model performance on clean Spring dataset to establish baseline accuracy
2. Test each model on individual corruption types to identify specific vulnerabilities
3. Compare transformer-based versus CNN-based architectures across all corruption conditions

## Open Questions the Paper Calls Out
The paper raises several open questions regarding the relationship between synthetic corruption robustness and real-world performance, the potential for transfer learning between different corruption types, and the development of training strategies that can improve robustness across multiple corruption categories simultaneously.

## Limitations
- The benchmark focuses on synthetic corruptions, which may not fully represent real-world degradation patterns encountered in practical applications
- The evaluation is limited to 16 models, potentially missing important architectural variations that could influence robustness
- The study does not explore the relationship between robustness to synthetic corruptions and real-world performance in detail

## Confidence

High Confidence:
- The methodology for creating the corrupted dataset and the novel corruption robustness metric are sound and well-validated

Medium Confidence:
- The initial evaluations showing significant performance drops under corruptions are reliable, but the generalizability to other model architectures and datasets may vary
- The claim that transformer-based architectures generally show better robustness is supported but may depend on specific implementation details and training procedures

## Next Checks

1. Evaluate a broader range of model architectures, including more recent transformer-based methods, to confirm the robustness trends observed

2. Conduct ablation studies to understand which components of the benchmark (e.g., specific corruption types, consistency constraints) most influence model performance

3. Investigate the correlation between synthetic corruption robustness and real-world performance on deployed systems to validate the practical relevance of the benchmark