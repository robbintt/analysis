---
ver: rpa2
title: 'CURE: Confidence-driven Unified Reasoning Ensemble Framework for Medical Question
  Answering'
arxiv_id: '2510.14353'
source_url: https://arxiv.org/abs/2510.14353
tags:
- medical
- reasoning
- knowledge
- arxiv
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces CURE, a confidence-driven multi-model framework\
  \ for medical question answering that avoids fine-tuning by leveraging model diversity.\
  \ The approach uses a two-stage architecture: a confidence detection module evaluates\
  \ the primary model\u2019s certainty, and an adaptive routing mechanism directs\
  \ low-confidence queries to auxiliary models with complementary knowledge."
---

# CURE: Confidence-driven Unified Reasoning Ensemble Framework for Medical Question Answering

## Quick Facts
- arXiv ID: 2510.14353
- Source URL: https://arxiv.org/abs/2510.14353
- Authors: Ziad Elshaer; Essam A. Rashed
- Reference count: 5
- Primary result: CURE achieves 95.0% accuracy on PubMedQA and 78.0% on MedMCQA using a confidence-driven multi-model ensemble without fine-tuning

## Executive Summary
CURE introduces a confidence-driven multi-model framework for medical question answering that avoids fine-tuning by leveraging model diversity. The approach uses a two-stage architecture: a confidence detection module evaluates the primary model's certainty, and an adaptive routing mechanism directs low-confidence queries to auxiliary models with complementary knowledge. Evaluated on MedQA, MedMCQA, and PubMedQA using Qwen3-30B-A3B-Instruct, Phi-4 14B, and Gemma 2 12B, the framework achieves strong results, especially 95.0% on PubMedQA and 78.0% on MedMCQA. Ablation studies show that confidence-aware routing and multi-model collaboration significantly outperform single-model and uniform reasoning strategies. The work demonstrates that strategic model collaboration offers a practical, efficient pathway to improve medical AI systems, with implications for democratizing access in resource-limited healthcare settings.

## Method Summary
The CURE framework implements a confidence-driven multi-model ensemble for medical question answering without fine-tuning. It employs a two-stage architecture: first, a confidence detection module evaluates the primary model's certainty on each query using prompt-based uncertainty estimation; second, an adaptive routing mechanism directs low-confidence queries to auxiliary models selected for their complementary knowledge strengths. The framework was evaluated on three medical QA benchmarks (MedQA, MedMCQA, PubMedQA) using large language models Qwen3-30B-A3B-Instruct, Phi-4 14B, and Gemma 2 12B. Confidence-aware routing and multi-model collaboration were shown to significantly outperform single-model baselines and uniform reasoning strategies through systematic ablation studies.

## Key Results
- Achieved 95.0% accuracy on PubMedQA benchmark
- Achieved 78.0% accuracy on MedMCQA benchmark
- Confidence-aware routing and multi-model collaboration significantly outperformed single-model and uniform reasoning strategies in ablation studies

## Why This Works (Mechanism)
The framework works by leveraging complementary knowledge across multiple models through confidence-driven routing. When the primary model expresses uncertainty about a medical question, the system identifies this through confidence detection and routes the query to auxiliary models that possess complementary expertise. This approach avoids the computational cost and data requirements of fine-tuning while benefiting from the diverse knowledge bases of different models. The routing mechanism ensures that each query is handled by the model most likely to provide accurate responses, optimizing overall system performance through strategic model collaboration rather than attempting to make a single model handle all cases perfectly.

## Foundational Learning
- **Confidence detection**: Understanding how to estimate model uncertainty through prompt-based methods is crucial for determining when to trigger alternative reasoning paths. Quick check: Can the confidence module accurately distinguish between high and low confidence predictions across diverse medical question types?
- **Multi-model ensemble coordination**: Learning how different models can be orchestrated to work together efficiently without redundant computation. Quick check: Does the routing mechanism effectively distribute queries to minimize overlap while maximizing coverage?
- **Medical domain knowledge representation**: Understanding how different models encode and retrieve medical knowledge differently, which enables effective routing decisions. Quick check: Are routing decisions consistent with known strengths and weaknesses of each model's medical knowledge?
- **Prompt engineering for uncertainty**: Developing effective prompts that elicit reliable confidence estimates from models without requiring additional training. Quick check: Do confidence estimates correlate with actual prediction accuracy across the medical QA datasets?
- **Adaptive routing algorithms**: Implementing efficient algorithms that can quickly determine the optimal model for each query based on confidence scores. Quick check: What is the latency overhead introduced by the routing decision process?
- **Benchmark evaluation methodology**: Understanding how to properly evaluate medical QA systems across multiple datasets and model combinations. Quick check: Are the evaluation metrics consistent and comparable across all tested datasets?

## Architecture Onboarding

**Component Map**: Input -> Confidence Detection -> Routing Decision -> Primary/Auxiliary Model -> Output

**Critical Path**: The confidence detection module evaluates the primary model's output confidence score. If confidence exceeds threshold, the primary model's answer is returned. If confidence falls below threshold, the routing mechanism selects an auxiliary model based on complementary knowledge profiles, which then processes the query and returns the answer.

**Design Tradeoffs**: The framework trades increased computational overhead and latency (due to multi-model inference) for improved accuracy and robustness compared to single-model approaches. It avoids fine-tuning costs but requires careful model selection and confidence calibration. The two-stage architecture adds complexity but enables dynamic adaptation to question difficulty and domain specificity.

**Failure Signatures**: Poor performance may result from inadequate confidence detection calibration (leading to incorrect routing decisions), insufficient knowledge diversity among auxiliary models (causing redundant or contradictory outputs), or routing delays that impact real-time applications. The system may also struggle with questions that require synthesis across multiple medical domains not well-covered by any single auxiliary model.

**First Experiments**:
1. Evaluate confidence detection accuracy by comparing predicted confidence scores against actual prediction correctness across all medical QA datasets
2. Test routing efficiency by measuring computational overhead and latency introduced by the two-stage architecture versus single-model inference
3. Analyze knowledge overlap between auxiliary models by quantifying routing patterns and identifying potential redundancy in model selection

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to three specific Chinese-authored benchmarks and three particular model architectures, restricting generalizability
- Confidence detection relies on prompt-based uncertainty estimation without detailed calibration procedures, raising robustness concerns
- No analysis of computational overhead or latency introduced by the two-stage routing architecture
- Does not address potential biases in model selection or impact of model-specific pretraining on routing decisions

## Confidence

**High confidence**: Confidence-aware routing improves performance over single-model baselines, supported by strong empirical results across multiple datasets.

**Medium confidence**: Avoiding fine-tuning is a practical advantage, though computational costs and long-term maintenance requirements were not compared with fine-tuned alternatives.

**Medium confidence**: Generalizability of the framework, given the limited scope of datasets and models evaluated.

## Next Checks

1. **Generalization Test**: Evaluate CURE on additional medical QA benchmarks from diverse sources (e.g., USMLE, clinical guidelines) to assess robustness beyond the current datasets.

2. **Cost-Benefit Analysis**: Measure and compare the computational overhead, inference latency, and energy consumption of the two-stage routing approach against fine-tuned single-model baselines.

3. **Bias and Redundancy Audit**: Quantify knowledge overlap between auxiliary models and analyze routing decisions to identify potential biases or redundant model usage patterns.