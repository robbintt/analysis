---
ver: rpa2
title: Epistemic Reject Option Prediction
arxiv_id: '2511.04855'
source_url: https://arxiv.org/abs/2511.04855
tags:
- uncertainty
- predictor
- epistemic
- reject-option
- bayesian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel framework for epistemic reject-option
  prediction, which enables models to abstain from predictions when training data
  is insufficient to support reliable decisions. The key idea is to minimize expected
  regret - the performance gap between the learned model and the Bayes-optimal predictor
  - instead of traditional expected loss.
---

# Epistemic Reject Option Prediction

## Quick Facts
- arXiv ID: 2511.04855
- Source URL: https://arxiv.org/abs/2511.04855
- Authors: Vojtech Franc; Jakub Paplham
- Reference count: 29
- Primary result: Framework minimizes expected regret vs. Bayes-optimal predictor, enabling models to abstain when training data is insufficient.

## Executive Summary
This paper introduces a novel framework for epistemic reject-option prediction that enables models to abstain from predictions when training data is insufficient to support reliable decisions. The key innovation is minimizing expected regret—the performance gap between the learned model and the Bayes-optimal predictor—instead of traditional expected loss. The model abstains when this regret exceeds a specified rejection cost. Experiments on a synthetic polynomial regression task demonstrate that the proposed epistemic predictor consistently outperforms Bayesian and aleatoric predictors in minimizing regret across various dataset sizes.

## Method Summary
The framework defines a reject-option predictor that abstains when expected regret exceeds a threshold δ. Regret is defined as the difference between the loss of the learned predictor and the loss of the Bayes-optimal predictor with full knowledge of the data-generating process. For squared loss, the conditional regret equals the posterior variance of the conditional mean; for cross-entropy loss, it equals the expected KL divergence between the conditional model and predictive distribution. The method compares three predictors: epistemic (regret-based), Bayesian (total uncertainty), and aleatoric-only (plug-in estimate).

## Key Results
- The epistemic predictor achieves lower regret than Bayesian and aleatoric predictors across various dataset sizes on synthetic polynomial regression
- Area under Regret-Coverage curves (AuReC) shows epistemic predictor consistently outperforms alternatives
- With small datasets, epistemic and Bayesian predictors perform similarly; as data grows, Bayesian converges to aleatoric-only behavior
- Theoretical results show entropy- and variance-based epistemic uncertainty measures are special cases of conditional regret

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Minimizing expected regret enables isolation of epistemic uncertainty for reject decisions
- **Mechanism:** Regret defined as excess loss vs. Bayes-optimal predictor; cancels aleatoric components, leaving only reducible epistemic uncertainty
- **Core assumption:** Bayesian posterior accurately reflects epistemic uncertainty; loss is bounded and well-specified
- **Evidence anchors:** Abstract definition of regret-based reject loss; Theorem 1 on optimal epistemic predictor; limited external validation
- **Break condition:** Misspecified posterior or unbounded/mismatched loss breaks epistemic isolation

### Mechanism 2
- **Claim:** Standard epistemic uncertainty measures are special cases of conditional regret
- **Mechanism:** Under CE loss, E(x,D) equals expected KL divergence (mutual information); under squared loss, equals posterior variance of conditional mean
- **Core assumption:** Conditional model factorizes as p(x,y|θ) = p(x)p(y|x,θ); predictive distribution is posterior mean of conditional models
- **Evidence anchors:** Section 4/Table 1 instantiations; Section 6 equivalence statement; moderate external relevance
- **Break condition:** Non-standard losses break equivalence; alternative epistemic measures needed

### Mechanism 3
- **Claim:** Epistemic rejector outperforms Bayesian and aleatoric predictors on regret-based metrics
- **Mechanism:** Separates epistemic from aleatoric uncertainty; accepts high-noise inputs if well-supported, rejects only when data is insufficient
- **Core assumption:** Test distribution has regions well/poorly supported by training data; δ appropriately encodes utility tradeoffs
- **Evidence anchors:** Abstract performance claims; Section 5/Figure 3 AuReC curves; limited external benchmarking
- **Break condition:** Negligible epistemic uncertainty (large i.i.d. datasets) eliminates performance gains

## Foundational Learning

- **Concept:** Aleatoric vs. Epistemic Uncertainty
  - **Why needed here:** Framework decomposes total uncertainty into irreducible noise (aleatoric) and reducible lack-of-data uncertainty (epistemic); reject decisions target the latter
  - **Quick check question:** Given heteroscedastic regression, identify which part of predictive variance is observation noise vs. posterior uncertainty over mean function

- **Concept:** Bayesian Predictive Distribution and Posterior
  - **Why needed here:** Conditional regret E(x,D) is expectation under joint posterior p(θ,y|x,D); computing/approximating this posterior is essential
  - **Quick check question:** Explain how MC Dropout or deep ensembles approximate predictive distribution in Bayesian neural networks

- **Concept:** Reject-Option Loss and Rejection Cost
  - **Why needed here:** Framework extends standard loss with abstention option incurring fixed cost δ; setting δ encodes error vs. abstention tradeoff
  - **Quick check question:** If δ is very low, what behavior do you expect from epistemic reject-option predictor?

## Architecture Onboarding

- **Component map:** Bayesian core -> Base predictor H_B -> Conditional regret estimator E(x,D) -> Reject decider
- **Critical path:**
  1. Choose loss ℓ and rejection cost δ appropriate to task utility
  2. Train/approximate Bayesian model to obtain posterior samples
  3. At inference, compute E(x,D) using Table 1 formulas or Monte Carlo estimates
  4. Apply threshold δ; return H_B(x,D) or reject
- **Design tradeoffs:**
  - Exact vs. approximate posterior: Analytical for conjugate models; MC Dropout/ensembles for deep networks introduce approximation error
  - Rejection cost δ: Must be tuned to reflect downstream costs; misspecification leads to over/under-rejection
  - Loss choice: Squared loss for regression; cross-entropy for classification; mismatched loss breaks theoretical equivalence
- **Failure signatures:**
  - Rejecting too often in well-supported regions suggests posterior overestimates epistemic uncertainty
  - Accepting in out-of-distribution regions indicates epistemic uncertainty underestimated
  - Instability in E(x,D) across seeds points to poor posterior approximation
- **First 3 experiments:**
  1. Replicate cubic polynomial setup; compare regret-coverage curves for epistemic vs. Bayesian vs. aleatoric predictors across dataset sizes
  2. Replace exact posterior with MC Dropout and deep ensembles; verify estimated E(x,D) correlates with analytical epistemic uncertainty
  3. Vary δ and plot coverage vs. regret; confirm lower δ yields higher rejection rates and lower regret on accepted predictions

## Open Questions the Paper Calls Out
- **Question:** How can the framework be scaled to real-world tasks with complex models requiring computationally intensive approximations?
  - **Basis:** Authors state applying to real-world tasks necessitates intensive computational approximations, left for future research
  - **Why unresolved:** Experiments only validate on synthetic polynomial regression with analytical solutions
  - **What evidence would resolve it:** Empirical results on benchmark datasets using approximate inference methods

## Limitations
- All empirical validation is on a single synthetic polynomial regression task, leaving real-world applicability unproven
- Computing exact conditional regret is intractable for deep models; MC approximations may introduce bias and variance
- Theoretical framework assumes well-specified model family, which rarely holds in practice

## Confidence

- **High:** Theoretical equivalence between conditional regret and standard epistemic uncertainty measures under squared/CE loss
- **Medium:** Claim that epistemic rejectors outperform Bayesian ones in regret-coverage trade-off on synthetic task
- **Low:** General applicability to complex, high-dimensional data and non-conjugate models

## Next Checks

1. Apply epistemic reject-option to UCI regression datasets and standard classification benchmarks; compare AuReC/AuRoC vs. Bayesian and aleatoric-only predictors
2. Implement framework using MC Dropout and deep ensembles; validate estimated E(x,D) correlates with ground-truth epistemic uncertainty
3. Test framework with robust losses (Huber, quantile) and non-decomposable losses (F1-score); check if conditional regret still isolates epistemic uncertainty