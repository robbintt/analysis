---
ver: rpa2
title: Improving Scientific Document Retrieval with Concept Coverage-based Query Set
  Generation
arxiv_id: '2502.11181'
source_url: https://arxiv.org/abs/2502.11181
tags:
- queries
- document
- query
- generation
- concepts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating high-quality synthetic
  queries for scientific document retrieval, where existing methods often produce
  redundant queries with limited concept coverage. The proposed Concept Coverage-based
  Query set Generation (CCQGen) framework adaptively generates queries by identifying
  and leveraging uncovered document concepts as conditions for subsequent query generation.
---

# Improving Scientific Document Retrieval with Concept Coverage-based Query Set Generation

## Quick Facts
- arXiv ID: 2502.11181
- Source URL: https://arxiv.org/abs/2502.11181
- Reference count: 40
- CCQGen improves scientific document retrieval performance with up to 27.92% NDCG@10 improvement over state-of-the-art methods

## Executive Summary
This paper addresses the challenge of generating high-quality synthetic queries for scientific document retrieval, where existing methods often produce redundant queries with limited concept coverage. The proposed Concept Coverage-based Query set Generation (CCQGen) framework adaptively generates queries by identifying and leveraging uncovered document concepts as conditions for subsequent query generation. It uses an academic topic taxonomy and phrase mining to identify core concepts, enriches them with related concepts, and employs a concept extractor to track coverage. CCQGen integrates with existing prompting schemes and introduces Concept Similarity-enhanced Retrieval (CSR) for filtering and retrieval enhancement. Experiments on CSFCube and DORIS-MAE datasets show that CCQGen significantly improves retrieval performance over state-of-the-art methods.

## Method Summary
CCQGen generates high-quality synthetic queries for scientific document retrieval by maximizing concept coverage through adaptive generation. The framework first identifies core concepts from documents using an academic taxonomy and phrase mining, then trains a concept extractor to track which concepts have been covered. During query generation, it conditions subsequent prompts on uncovered concepts to ensure diverse coverage. The method filters generated queries using Concept Similarity-enhanced Retrieval (CSR) to retain only high-quality queries. The approach integrates with existing prompting schemes and can be used to fine-tune PLM-based retrievers like Contriever-MS and SPECTER-v2. The entire process is designed to produce query sets that better represent the full conceptual content of documents.

## Key Results
- CCQGen achieves up to 27.92% improvement in NDCG@10 over state-of-the-art methods when using CSR filtering
- The approach is effective even with limited training data and smaller language models
- Performance improvements are consistent across both CSFCube and DORIS-MAE datasets
- CSR filtering significantly enhances retrieval performance by eliminating low-quality synthetic queries

## Why This Works (Mechanism)
The method works by addressing the fundamental limitation of existing synthetic query generation approaches: redundant queries that fail to cover the full conceptual space of documents. By tracking concept coverage through the concept extractor and conditioning subsequent generation on uncovered concepts, CCQGen ensures comprehensive representation of document content. The CSR filtering mechanism further improves quality by removing queries that don't align well with their source documents in the embedding space, ensuring that only relevant and high-quality synthetic queries are used for retriever fine-tuning.

## Foundational Learning
- **Academic Topic Taxonomy Traversal**: Used to identify relevant concepts from a large ontology; needed to map document content to structured knowledge; quick check: verify taxonomy coverage matches document domains
- **Phrase Mining with AutoPhrase**: Extracts key phrases from documents; needed to identify granular concepts beyond taxonomy nodes; quick check: evaluate phrase quality and relevance scores
- **Multi-gate Mixture of Experts (MMoE)**: The concept extractor architecture; needed to predict concept importance from document embeddings; quick check: validate extractor precision on manually labeled concepts
- **Concept Similarity-enhanced Retrieval (CSR)**: Filtering mechanism using retrieval scores and concept similarity; needed to eliminate low-quality synthetic queries; quick check: monitor retention rate after filtering
- **Adaptive Prompt Conditioning**: Generating queries based on uncovered concepts; needed to maximize concept coverage; quick check: measure concept coverage per query set

## Architecture Onboarding

**Component Map**: Document -> Concept Identification -> Concept Extractor Training -> Adaptive Query Generation -> CSR Filtering -> Retriever Fine-tuning

**Critical Path**: The core pipeline flows from document processing through concept extraction to adaptive generation, with CSR filtering as the quality control gate before fine-tuning.

**Design Tradeoffs**: The method trades computational overhead of concept tracking and filtering for improved query quality and coverage. The choice of M=5 generations per document balances diversity with efficiency.

**Failure Signatures**: 
- Low concept extractor precision leads to redundant queries
- Aggressive CSR filtering may discard too many queries if the base retriever is weak
- Taxonomy coverage gaps result in missed concepts
- Phrase mining may miss domain-specific terminology

**First Experiments**:
1. Train concept extractor on a small manually-labeled set to validate precision
2. Test CSR filtering with varying thresholds (N=3, N=5, N=10) to find optimal retention
3. Compare concept coverage metrics between CCQGen and baseline methods on validation set

## Open Questions the Paper Calls Out
None

## Limitations
- The Microsoft Academic Taxonomy snapshot used is not publicly available, requiring either the original dataset or a suitable alternative ontology
- The concept extractor training relies on GPT-3.5 annotations for selecting relevant topics, but the exact annotation protocol and scale are not fully specified
- The filtering threshold (top-5 retrieval rank) is empirically chosen but not extensively validated across different datasets

## Confidence

**High confidence**: The overall framework design and its core components (concept identification, adaptive generation, filtering) are well-described and reproducible

**Medium confidence**: The experimental methodology and results are clearly presented, though some implementation details require assumptions

**Low confidence**: The exact prompt templates for concept selection and the specific LLM configuration parameters are not fully specified

## Next Checks
1. Implement the concept extractor training pipeline using a proxy taxonomy (e.g., OpenAlex) and validate its precision on a small manually-labeled set
2. Test the CSR filtering mechanism with varying thresholds (N=3, N=10) to assess sensitivity and retention rates
3. Compare query diversity metrics (e.g., unique concept coverage per query set) between CCQGen and baseline methods on a held-out validation set