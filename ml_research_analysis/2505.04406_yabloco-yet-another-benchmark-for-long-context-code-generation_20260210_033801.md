---
ver: rpa2
title: 'YABLoCo: Yet Another Benchmark for Long Context Code Generation'
arxiv_id: '2505.04406'
source_url: https://arxiv.org/abs/2505.04406
tags:
- code
- context
- repository
- functions
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces YABLoCo, a benchmark for evaluating code
  generation in large C/C++ repositories with 215 functions from four large codebases
  spanning 200K-2M lines of code. The benchmark includes function bodies, docstrings,
  signatures, and dependency contexts at multiple levels (none, stdlib, file, package,
  project).
---

# YABLoCo: Yet Another Benchmark for Long Context Code Generation

## Quick Facts
- **arXiv ID:** 2505.04406
- **Source URL:** https://arxiv.org/abs/2505.04406
- **Reference count:** 20
- **Primary result:** Baseline models achieved pass@10 scores of 17.29% (CodeLlama-13B), 30.4% (GPT-4), and 22.42% (DeepSeekCoder-33B) on C/C++ function generation from large repositories

## Executive Summary
This paper introduces YABLoCo, a benchmark for evaluating code generation in large C/C++ repositories. The benchmark consists of 215 functions from four large codebases spanning 200K-2M lines of code, providing function bodies, docstrings, signatures, and dependency contexts at multiple levels. An evaluation pipeline computes pass@k, Exact Match, and Edit Similarity metrics. Baseline models achieved pass@10 scores ranging from 17.29% to 30.4%, with significant improvements when augmented with "oracle" context (e.g., CodeLlama improved from 17.29% to 29.38%).

## Method Summary
YABLoCo was constructed by extracting 215 functions from four large C/C++ repositories (200K-2M lines each). For each function, the dataset includes the function body, docstring, signature, and dependency contexts at five levels: none, stdlib, file, package, and project. The evaluation pipeline computes three metrics: pass@k (whether generated code passes test cases), Exact Match (syntactic similarity), and Edit Similarity (structural similarity). Models are evaluated with and without context augmentation, including an "oracle" context condition where perfect relevant code is provided.

## Key Results
- Baseline models achieved pass@10 scores of 17.29% (CodeLlama-13B), 30.4% (GPT-4), and 22.42% (DeepSeekCoder-33B)
- CodeLlama-13B improved from 17.29% to 29.38% when provided with oracle context
- The benchmark is validated as challenging for current models based on low baseline performance

## Why This Works (Mechanism)
The benchmark addresses the challenge of code generation in large repositories by providing multi-level dependency contexts that mirror real-world development scenarios. By evaluating models across different context availability levels, it reveals how well models can leverage surrounding code to generate accurate functions. The combination of multiple evaluation metrics (pass@k, Exact Match, Edit Similarity) provides a comprehensive assessment of both functional correctness and code quality.

## Foundational Learning
- **Code generation evaluation metrics** - needed to assess functional correctness vs syntactic similarity; quick check: verify metrics align with developer expectations
- **Dependency context in large codebases** - needed to understand how surrounding code influences function generation; quick check: measure performance drop without context
- **Large language model limitations** - needed to interpret baseline performance; quick check: compare model sizes and capabilities
- **C/C++ compilation and testing** - needed for pass@k metric computation; quick check: ensure test harness captures compilation errors
- **Benchmark construction methodology** - needed to assess representativeness; quick check: verify function selection covers diverse scenarios
- **Context retrieval systems** - needed for oracle vs realistic context comparison; quick check: measure retrieval accuracy

## Architecture Onboarding

**Component Map:** Repository -> Function Extraction -> Context Generation -> Evaluation Pipeline -> Metrics Computation

**Critical Path:** Function extraction from repositories → Context generation at multiple levels → Model generation with varying contexts → Automated testing → Metric computation

**Design Tradeoffs:** The benchmark uses static context extraction rather than dynamic retrieval to ensure reproducibility, but this may not reflect real-world retrieval challenges. The limited number of repositories (4) ensures depth but constrains generalizability.

**Failure Signatures:** Low pass@k scores indicate models struggle with either function logic comprehension or dependency resolution. Performance gaps between context levels reveal context sensitivity. Oracle context improvements highlight retrieval limitations.

**3 First Experiments:** 1) Run baseline models without any context to establish lower bound performance. 2) Compare performance across different context levels to measure context utility. 3) Evaluate oracle context performance to identify retrieval system bottlenecks.

## Open Questions the Paper Calls Out
None

## Limitations
- Limited diversity with only four C/C++ repositories constrains generalizability
- Automated metrics may not correlate with actual code quality and runtime behavior
- Oracle context results may overstate real-world applicability since perfect context retrieval is unrealistic
- No statistical significance testing or error analysis across codebases
- Potential annotation biases in docstring or function selection

## Confidence
- Challenging benchmark claim: Medium (supported by low scores but lacks statistical validation)
- Suitable for large-repository context evaluation: High (for intended C/C++ scope)
- Oracle context improvements: Medium (demonstrates potential but unrealistic assumption)
- Generalizability to other languages: Low (only tested on C/C++)

## Next Checks
1. Conduct human evaluation of a random sample of generated code to verify functional correctness and assess whether automated metrics correlate with actual code quality
2. Test model performance across additional C/C++ repositories to evaluate the benchmark's generalizability beyond the four initial codebases
3. Implement a realistic context retrieval system (rather than oracle context) and measure the degradation in performance to establish a more practical baseline for real-world deployment