---
ver: rpa2
title: On the Relation of State Space Models and Hidden Markov Models
arxiv_id: '2601.13357'
source_url: https://arxiv.org/abs/2601.13357
tags:
- state
- ssms
- latent
- space
- kalman
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a unified comparison of Hidden Markov Models
  (HMMs) and State Space Models (SSMs) through a probabilistic graphical modeling
  lens. While HMMs model discrete latent states with Markov chain dynamics, classical
  SSMs employ continuous-valued latent states with Gaussian noise, and modern NLP
  SSMs like S4 and Mamba use deterministic state updates trained via backpropagation.
---

# On the Relation of State Space Models and Hidden Markov Models

## Quick Facts
- arXiv ID: 2601.13357
- Source URL: https://arxiv.org/abs/2601.13357
- Reference count: 6
- Provides unified comparison of HMMs and SSMs through probabilistic graphical modeling lens

## Executive Summary
This paper establishes a comprehensive framework for understanding the relationships between Hidden Markov Models (HMMs), classical State Space Models (SSMs), and modern NLP SSMs (S4, Mamba). Through a unified probabilistic graphical modeling perspective, the authors demonstrate that while these models share identical temporal dependency structures, they differ fundamentally in their probabilistic assumptions, inference procedures, and training paradigms. The work bridges control theory, probabilistic modeling, and deep learning by clarifying that HMMs represent uncertainty over discrete state identities, classical SSMs model continuous state trajectories with Gaussian noise, and modern NLP SSMs operate as deterministic computation graphs without latent uncertainty quantification.

## Method Summary
The authors employ a probabilistic graphical modeling approach to analyze three classes of sequential models: HMMs, classical SSMs, and modern NLP SSMs. They systematically compare these models across five dimensions: (1) latent state space (discrete vs. continuous), (2) temporal dependency structure (Markov property), (3) probabilistic assumptions (stochastic vs. deterministic), (4) inference procedures (forward-backward vs. Kalman filtering vs. deterministic scan), and (5) training paradigms (EM algorithm vs. backpropagation). The analysis reveals that despite sharing identical directed acyclic graph structures, these models represent fundamentally different approaches to sequence modeling, with HMMs focusing on discrete state classification, classical SSMs on continuous state estimation, and modern NLP SSMs on deterministic feature transformation.

## Key Results
- HMMs and classical SSMs share probabilistic foundations but differ in state space (discrete vs. continuous) and inference methods
- Modern NLP SSMs like S4 and Mamba maintain the same temporal dependency structure as HMMs and SSMs but operate deterministically without latent uncertainty
- The probabilistic graphical model framework unifies understanding across classical and modern sequence modeling approaches, revealing that deterministic computation graphs can be embedded within probabilistic frameworks

## Why This Works (Mechanism)
The paper demonstrates that all three model classes share identical temporal dependency structures (Markov chains), but differ in how they handle latent state representation and uncertainty. HMMs use discrete states with transition probabilities and emission distributions, classical SSMs employ continuous states with Gaussian noise models, and modern NLP SSMs use deterministic state updates. This shared structure explains why all three can be effective for sequence modeling while representing fundamentally different assumptions about the data-generating process.

## Foundational Learning
- Markov property: why needed - explains temporal dependencies in all three model classes; quick check - verify that current state depends only on previous state
- Forward-backward algorithm: why needed - inference method for HMMs; quick check - implement on simple discrete sequence
- Kalman filtering: why needed - optimal inference for linear Gaussian SSMs; quick check - apply to 1D tracking problem
- EM algorithm: why needed - parameter learning for latent variable models; quick check - implement on mixture model
- Backpropagation through time: why needed - training method for deterministic NLP SSMs; quick check - implement on simple RNN
- Probabilistic graphical models: why needed - unifying framework for all three approaches; quick check - draw DAG for each model type

## Architecture Onboarding

**Component Map:** HMM -> Continuous SSM -> NLP SSM (all share temporal dependency structure but differ in state space, inference, and training)

**Critical Path:** Latent state representation → temporal dependency → inference procedure → parameter learning

**Design Tradeoffs:** Discrete vs. continuous states (interpretability vs. flexibility), stochastic vs. deterministic updates (uncertainty quantification vs. computational efficiency), EM vs. backpropagation (sample efficiency vs. scalability)

**Failure Signatures:** HMMs fail on continuous-valued data, classical SSMs struggle with nonlinear dynamics, NLP SSMs lack uncertainty quantification

**First Experiments:** 1) Compare HMM and SSM performance on discrete vs. continuous sequential data, 2) Implement probabilistic variant of NLP SSM to test uncertainty quantification, 3) Benchmark EM vs. backpropagation training on equivalent model capacities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can deterministic NLP SSMs (e.g., S4, Mamba) be reformulated as probabilistic generative models to recover latent uncertainty quantification, and would this improve calibration or robustness?
- Basis in paper: The paper notes that "a deterministic computation graph can still be embedded into a probabilistic model" (footnote 1, p. 5), but does not explore implementation or benefits.
- Why unresolved: The analysis establishes that NLP SSMs lack uncertainty representation but does not investigate whether reintroducing probabilistic semantics is feasible or advantageous.
- What evidence would resolve it: Construction of a probabilistic variant of an NLP SSM, with empirical comparison on uncertainty calibration metrics and out-of-distribution detection benchmarks.

### Open Question 2
- Question: Do the continuous latent states learned by NLP SSMs exhibit discrete structure that could be mapped to HMM-like categorical states?
- Basis in paper: Table 1 contrasts discrete vs. continuous latent states, but the paper does not explore whether continuous SSM representations implicitly encode discrete modes.
- Why unresolved: The structural comparison is provided, but the semantic relationship between what discrete and continuous latents capture remains unexplored.
- What evidence would resolve it: Clustering analysis of learned SSM hidden states showing correlation with HMM state assignments on identical sequential data.

### Open Question 3
- Question: How do the optimization landscapes and convergence properties of EM-based learning compare to gradient-based backpropagation when applied to equivalent sequence modeling tasks?
- Basis in paper: Section 4.4 contrasts EM (HMM/LG-SSM) with backpropagation (NLP SSM), but provides no empirical or theoretical comparison of training dynamics.
- Why unresolved: The paper clarifies the algorithmic distinction but does not address practical implications for optimization difficulty or sample efficiency.
- What evidence would resolve it: Controlled experiments tracking loss curves, convergence rates, and final performance for both training paradigms on matched model capacities and datasets.

## Limitations
- Empirical validation is limited to theoretical analysis without experimental demonstrations
- Treatment of modern NLP SSMs as purely deterministic may oversimplify actual implementations
- Does not address hybrid approaches combining elements from multiple model families
- Lacks discussion of practical model selection implications for real-world applications

## Confidence

High: Core theoretical claims about fundamental distinctions between model classes based on probabilistic assumptions and inference procedures
Medium: Claims about practical implications of theoretical distinctions without empirical validation
Low: Statements about specific implementation details of modern NLP SSMs that may vary across architectures

## Next Checks
1. Implement comparative experiments measuring inference accuracy and computational efficiency across HMMs, classical SSMs, and modern NLP SSMs on standardized sequence modeling tasks
2. Analyze the effect of stochastic regularization techniques in modern SSMs on their deterministic characterization and uncertainty representation
3. Conduct ablation studies varying the probabilistic assumptions (discrete vs. continuous states, deterministic vs. stochastic updates) while maintaining identical temporal dependency structures to quantify their impact on model performance