---
ver: rpa2
title: 'Evaluating GenAI for Simplifying Texts for Education: Improving Accuracy and
  Consistency for Enhanced Readability'
arxiv_id: '2501.09158'
source_url: https://arxiv.org/abs/2501.09158
tags:
- text
- grade
- level
- llms
- words
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluates large language models (LLMs) and prompting
  techniques for simplifying educational texts to improve readability for students
  at different grade levels. Researchers used three LLMs (GPT-4 Turbo, Claude 3, and
  Mixtral 8x22B) with four prompting methods (zero-shot, prompt chaining, chain-of-thought,
  directional stimulus prompting, and a novel multi-agent architecture) to simplify
  60 twelve-grade passages down to eighth, sixth, and fourth-grade levels.
---

# Evaluating GenAI for Simplifying Texts for Education: Improving Accuracy and Consistency for Enhanced Readability

## Quick Facts
- **arXiv ID:** 2501.09158
- **Source URL:** https://arxiv.org/abs/2501.09158
- **Reference count:** 0
- **Primary result:** Large language models can effectively simplify educational texts, with directional stimulus prompting showing best results for maintaining keywords and semantic similarity across grade levels.

## Executive Summary
This study evaluates large language models (LLMs) and prompting techniques for simplifying educational texts to improve readability for students at different grade levels. Researchers used three LLMs (GPT-4 Turbo, Claude 3, and Mixtral 8x22B) with four prompting methods to simplify 60 twelve-grade passages down to eighth, sixth, and fourth-grade levels. The study demonstrates that LLMs can achieve better accuracy and consistency for higher grade levels (eighth and sixth) than for fourth grade, with directional stimulus prompting performing best for maintaining keywords and semantic similarity.

## Method Summary
The research team employed three state-of-the-art LLMs (GPT-4 Turbo, Claude 3, and Mixtral 8x22B) to simplify 60 twelve-grade passages down to eighth, sixth, and fourth-grade reading levels. They tested four prompting techniques: zero-shot prompting, prompt chaining, chain-of-thought, and directional stimulus prompting, along with a novel multi-agent architecture. Key metrics assessed included grade level accuracy (measured by Flesch-Kincaid Grade Level), keyword retention, semantic similarity of key phrases (measured by BERTScore), and word count consistency. Human annotators provided keywords and key phrases for each passage to serve as content anchors during simplification.

## Key Results
- LLMs and prompting techniques achieved better accuracy and consistency for eighth and sixth grade levels compared to fourth grade
- Directional stimulus prompting performed best for maintaining keywords and semantic similarity
- Claude 3 best maintained word count consistency across simplifications
- Semantic similarity degraded as a function of the grade-level gap between source and target text

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Directional Stimulus Prompting (DSP) improves keyword and semantic retention by explicitly constraining the LLM's attention to pre-specified content anchors before simplification.
- **Mechanism:** DSP provides "hints" (human-annotated keywords and key phrases) in the prompt, which reduces the model's tendency to paraphrase or compress content during grade-level reduction. By foregrounding what *must* be retained, the model allocates more computational attention to preserving those elements.
- **Core assumption:** The LLM can follow explicit constraint instructions and will not hallucinate around them. Assumption: The keywords/phrases provided are comprehensive enough to represent essential content.
- **Evidence anchors:**
  - [abstract] "Directional stimulus prompting performed best for maintaining keywords and semantic similarity"
  - [section 3.3] "DSP was the sole prompting technique that positively influenced keyword retention accuracy"
  - [corpus] Limited direct validation; EduAdapt dataset (neighbor paper) addresses grade-level adaptability but does not evaluate DSP specifically.
- **Break condition:** If keyword lists are incomplete, noisy, or contradictory, DSP may over-constrain outputs, producing unnatural text. Also degrades if model ignores prompt constraints under high temperature settings.

### Mechanism 2
- **Claim:** Semantic similarity degrades as a function of the grade-level gap between source and target text.
- **Mechanism:** Larger readability gaps require more aggressive syntactic restructuring and vocabulary substitution, which introduces semantic drift. The LLM must make more substitutions, increasing the probability of meaning alteration.
- **Core assumption:** BERTScore accurately captures semantic similarity for this task. Assumption: The relationship is approximately linear or monotonic.
- **Evidence anchors:**
  - [abstract] "LLMs and prompting techniques achieved better accuracy and consistency for higher grade levels (eighth and sixth) than for fourth grade"
  - [section 3.4, Table 7] Linear regression confirmed semantic similarity decreases as grade gap increases; pairwise Tukey comparisons showed significant differences at 95% confidence
  - [corpus] Neighbor paper "Plain language adaptations of biomedical text using LLMs" similarly notes trade-offs between simplification depth and content preservation, though metrics differ.
- **Break condition:** Breaks if BERTScore fails to capture domain-specific semantic nuance (e.g., technical vocabulary substitutions that preserve surface similarity but alter meaning).

### Mechanism 3
- **Claim:** Multi-agent architectures enforce structural constraints (word count, paragraph structure) better than single-prompt methods, but at the cost of semantic fidelity.
- **Mechanism:** Iterative feedback loops between Writer, Calculator (FKGL computation), and Editor agents allow the system to reject outputs that violate constraints and force regeneration. However, this forces trade-offs where keywords may be substituted to meet readability targets.
- **Core assumption:** The Editor agent can reliably evaluate whether constraints are met and provide actionable feedback. Assumption: The iterative loop converges within practical iteration limits.
- **Evidence anchors:**
  - [section 2.3.5] Describes four-agent system: Manager, Selector, Writer, Calculator, Editor
  - [section 4.3] "Multi-agent architecture was less effective for three of the tested metrics, but did excel in maintaining length... makes choices that necessarily compromise semantic and lexical similarity"
  - [corpus] No direct validation in corpus; multi-agent evaluation for long-form factuality (MAD-Fact paper) exists but addresses different domain.
- **Break condition:** High cost and slow inference (~4 min average, $2.43 per run); fails if agents miscommunicate or Editor provides ambiguous feedback. Also limited to OpenAI models due to AutoGen framework constraint.

## Foundational Learning

- **Concept: Flesch-Kincaid Grade Level (FKGL)**
  - **Why needed here:** Primary metric for evaluating whether simplified text hits target grade level. Understanding its formula (words, sentences, syllables) is essential to interpret why LLMs struggle at 4th grade level.
  - **Quick check question:** Given a 300-word passage with 15 sentences and 450 syllables, calculate the approximate FKGL.

- **Concept: BERTScore for Semantic Similarity**
  - **Why needed here:** Measures how well simplified passages retain meaning of key phrases. Understanding its use of cosine similarity between BERT embeddings explains why it captures semantic but not lexical preservation.
  - **Quick check question:** Why might a BERTScore of 0.90 still represent significant loss of specific factual details?

- **Concept: Prompt Chaining vs. Chain-of-Thought vs. Directional Stimulus**
  - **Why needed here:** These are the three core prompting strategies evaluated. Each decomposes the simplification task differently‚Äîchaining splits into sequential subtasks, CoT adds intermediate reasoning, DSP provides explicit content constraints.
  - **Quick check question:** Which prompting method would you select if keyword preservation was the only priority?

## Architecture Onboarding

- **Component map:**
  - Input: 12th-grade passage + human-annotated keywords/key phrases + target grade level
  - LLM Layer: GPT-4 Turbo / Claude 3 / Mixtral 8x22B (API-based)
  - Prompting Layer: Zero-shot ‚Üí Prompt Chaining ‚Üí CoT ‚Üí DSP (progressive complexity)
  - Multi-Agent Layer (optional): Selector ‚Üí Writer ‚Üí Calculator (FKGL/word count) ‚Üí Editor ‚Üí iterative refinement
  - Evaluation Layer: FKGL score, Keyword Accuracy %, BERTScore, Word Count % Change

- **Critical path:**
  1. Human expert annotates keywords/phrases for source passage
  2. Passage + target grade + constraints fed to LLM with selected prompting strategy
  3. LLM outputs simplified passage
  4. Automated evaluation computes FKGL, keyword match rate, BERTScore
  5. (Multi-agent only) If constraints not met, Editor triggers Writer regeneration

- **Design tradeoffs:**
  - **DSP vs. Multi-Agent:** DSP = better semantic retention, lower cost, single-pass. Multi-Agent = better structural control, higher cost ($2.43/run), slower (~4 min), requires OpenAI-only.
  - **Grade gap:** 12th‚Üí8th is reliable; 12th‚Üí4th shows semantic drift across all methods.
  - **LLM choice:** GPT-4 = best semantic similarity; Claude 3 = best word count preservation; Mixtral = open weights but lowest consistency.

- **Failure signatures:**
  - Passage contains only prompt echo (e.g., "Sure, I can help you reduce...") ‚Äî indicates prompt following failure (noted with Mixtral + Prompt Chaining)
  - FKGL output significantly overshoots target (e.g., 6.88 for 4.5 target) ‚Äî model cannot generate sufficiently simple text
  - Keyword accuracy < 60% with BERTScore < 0.85 ‚Äî model is paraphrasing rather than simplifying
  - Multi-agent infinite loop or cost spike ‚Äî Editor feedback unclear or constraints impossible to satisfy simultaneously

- **First 3 experiments:**
  1. **Baseline calibration:** Run zero-shot prompting with GPT-4 on 10 passages at 8th-grade target. Verify FKGL mean is within ¬±0.5 of target and BERTScore > 0.88. If not, adjust prompt wording for clarity.
  2. **A/B test DSP vs. CoT for keyword retention:** Using Claude 3, simplify 20 passages to 6th grade with both methods. Measure keyword accuracy difference. Expect DSP > CoT by 10-15% based on paper results.
  3. **Grade-gap stress test:** Using best LLM/prompting combination from literature (GPT-4 + DSP), simplify 10 passages directly from 12th to 4th grade. Measure semantic similarity drop rate to confirm if trade-off is acceptable for your use case.

## Open Questions the Paper Calls Out
None

## Limitations
- The study's narrow domain focus (exclusively twelve-grade passages) without validation across different subject domains or lower-grade source texts
- Human annotation process for keywords/key phrases introduces potential subjectivity without inter-annotator agreement metrics
- Evaluation framework relies on automated metrics without human readability assessments for pedagogical effectiveness

## Confidence
- **High Confidence (‚òëÔ∏è):** Directional stimulus prompting demonstrably improves keyword retention over baseline methods; semantic similarity consistently decreases with larger grade gaps; GPT-4 achieves highest semantic similarity scores
- **Medium Confidence (üü°):** Claude 3's superior word count preservation; multi-agent architecture's trade-off between structural control and semantic fidelity; prompting techniques perform better at higher grade levels
- **Low Confidence (üî¥):** Generalization to different text domains; long-term pedagogical effectiveness; real-world deployment feasibility without human annotation; Mixtral performance issues being framework-specific vs. model limitation

## Next Checks
1. **Human Readability Validation:** Recruit 30 teachers to evaluate 50 simplified passages (across all grade levels and prompting methods) on clarity, pedagogical appropriateness, and engagement using Likert scales. Compare human ratings against automated metrics to identify gaps.
2. **Cross-Domain Generalization:** Apply the best-performing LLM/prompting combination (GPT-4 + DSP) to simplify 100 passages from three new domains: scientific articles, news reports, and literature. Measure whether grade-level accuracy and keyword retention patterns hold across domains.
3. **Annotation Efficiency Study:** Compare automated keyword extraction (using a pre-trained model like YAKE or RAKE) against human annotation for 100 passages. Measure the trade-off between annotation cost/time savings and degradation in simplification quality metrics.