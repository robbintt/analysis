---
ver: rpa2
title: 'TRIDENT: Benchmarking LLM Safety in Finance, Medicine, and Law'
arxiv_id: '2507.21134'
source_url: https://arxiv.org/abs/2507.21134
tags:
- harmful
- finance
- safety
- ethical
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Trident-Bench, a benchmark for evaluating
  the safety of large language models (LLMs) in high-risk domains such as law, finance,
  and medicine. The benchmark is grounded in professional ethical codes from authoritative
  bodies (CFA Institute, AMA, ABA) and features 2,652 harmful prompts paired with
  safe responses.
---

# TRIDENT: Benchmarking LLM Safety in Finance, Medicine, and Law

## Quick Facts
- **arXiv ID**: 2507.21134
- **Source URL**: https://arxiv.org/abs/2507.21134
- **Reference count**: 40
- **Primary result**: Domain-specialized LLMs often fail subtle ethical safety tests despite high domain proficiency, while safety-aligned models show marked improvement.

## Executive Summary
This paper introduces Trident-Bench, a comprehensive benchmark for evaluating the safety of large language models (LLMs) in high-risk professional domains including law, finance, and medicine. The benchmark is grounded in authoritative professional ethical codes (CFA, AMA, ABA) and features 2,652 harmful prompts paired with safe responses. Through evaluation of 19 diverse models, the study reveals that while strong generalist models like GPT-4o and Gemini 2.5 Flash meet basic safety expectations, domain-specialized models often fail on subtle ethical nuances. The authors demonstrate that safety-aligned models (e.g., LLaMA Guard3-8B) show significant improvement, suggesting that targeted safety fine-tuning is crucial for domain-specific contexts. The study highlights the need for domain-grounded safety evaluation beyond general harm detection.

## Method Summary
The authors construct Trident-Bench by mapping adversarial prompts to specific clauses in professional ethical codes from CFA Institute, AMA, and ABA. They generate harmful prompts using jailbreak techniques (PAIR, GCG, Auto-DAN) and filter them through human expert consensus. The benchmark evaluates 19 models across three domains using a jury-based scoring system where responses are rated by Claude 3.5 and Gemma 2-9B on a 1-5 harmfulness scale. The evaluation pipeline generates three responses per prompt per model, which are then scored by the jury and averaged for final results.

## Key Results
- Domain-specialized models (e.g., DISC-LawLLM, Meditron) scored significantly lower on safety metrics than general-purpose models despite domain proficiency
- Safety-aligned models like LLaMA Guard3-8B showed marked improvement, suggesting targeted fine-tuning is effective
- GPT-4o and Gemini 2.5 Flash met basic safety expectations across all domains
- Models tended to fail on subtle ethical nuances rather than obvious violations

## Why This Works (Mechanism)

### Mechanism 1: Principle-to-Prompt Grounding
The benchmark construction starts with explicit ethical principles from professional codes and uses these as constraints for generating harmful queries. This ensures prompts test domain-specific boundaries (like conflict of interest) rather than general toxicity.

### Mechanism 2: Adversarial Distribution Shifting via Jailbreaks
By using jailbreak methods to generate test cases, the benchmark creates inputs that are structurally similar to benign professional queries but contain ethically toxic intents, revealing models that prioritize "helpfulness" over safety.

### Mechanism 3: Jury-Based Harmfulness Calibration
Averaging harmfulness scores from a diverse two-model jury (Claude 3.5 and Gemma 2-9B) reduces evaluation bias compared to single-model judging, with disagreement between jurors highlighting edge cases.

## Foundational Learning

**Concept: Adversarial Red-Teaming (Jailbreaking)**
- Why needed here: Trident-Bench is synthesized using "attack" models. Understanding prompt-based vs optimization-based attacks is necessary to interpret why certain prompts are included.
- Quick check: Can you explain why a "finetuned jailbreak model" might produce more aggressive prompts than a prompt-engineering approach?

**Concept: Domain-Specific Alignment vs. General Safety**
- Why needed here: The paper's central thesis is that general safety training does not transfer to professional domains.
- Quick check: Why would a legal model, trained to be helpful to lawyers, potentially fail a safety test by suggesting "workarounds" for client confidentiality?

**Concept: LLM-as-a-Judge Evaluation**
- Why needed here: The metric relies entirely on automated model scoring. You must understand the limitations of using LLMs to grade other LLMs.
- Quick check: What are the risks of using a single proprietary model (like G-4) as the sole evaluator for all other models?

## Architecture Onboarding

**Component map:**
Principles DB -> Prompt Generator -> Expert Filter -> Target Model -> Evaluation Jury

**Critical path:**
1. Select Ethical Principle → 2. Generate Adversarial Prompt → 3. Filter with 3-way Human Consensus → 4. Inference on Target Model → 5. Average Jury Score

**Design tradeoffs:**
- Realism vs. Aggression: 75% "natural" prompt-based attacks and only 25% finetuned attacks to balance realism with adversarial difficulty
- Precision vs. Recall: Using unanimous expert agreement creates a smaller but higher-precision dataset, potentially filtering out ambiguous but dangerous edge cases

**Failure signatures:**
- The "Helpful Assistant" Trap: Domain models failing by treating unethical prompts as legitimate client queries
- Evaluation Drift: If jury models are updated, scores may shift, requiring re-benchmarking

**First 3 experiments:**
1. Baseline Calibration: Run GPT-4o and LLaMA Guard3 to establish the upper bound of safety performance on Trident-Bench
2. Ablation on Jury: Compare single-model evaluation vs. the proposed dual-model average on a subset of 100 finance prompts
3. Domain Specificity Test: Compare a general model (LLaMA 3.1) against its domain-tuned variant (e.g., AdaptLLM-Finance) to quantify the "safety degradation" reported in the paper

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: How does LLM safety performance in high-risk domains evolve when evaluated on multi-turn or chained interaction scenarios rather than single-turn prompts?
- Basis in paper: [explicit] The authors explicitly list the focus on single-turn interactions as a limitation, stating that "Extending the benchmark to multi-turn or chained interaction scenarios would allow deeper testing of safety robustness."
- Why unresolved: Unsafe behavior in professional settings often emerges through contextual accumulation over several turns, which the current static benchmark architecture cannot simulate.

**Open Question 2**
- Question: To what extent do model-based juries align with human expert judgment when distinguishing between safe refusals and subtle unsafe compliance?
- Basis in paper: [explicit] The paper acknowledges that relying on model-based juries "may still introduce inductive bias or blind spots compared to human judgment" and suggests future work on "hybrid LLM-human adjudication."
- Why unresolved: While automated juries provide scalability, their validity as proxies for human ethical reasoning in specialized domains is assumed but not verified against a human ground truth.

**Open Question 3**
- Question: Can safety benchmarks grounded in standardized professional codes capture context-dependent ethical violations in legally ambiguous or culturally variable situations?
- Basis in paper: [explicit] Section 6 notes that the definitions of harmfulness are "operationalized for consistency and may not fully capture all context-dependent interpretations of professional ethics—particularly in legally ambiguous or culturally variable situations."
- Why unresolved: Professional codes offer clear standards, but their application often relies on context that rigid prompt-response pairs may fail to adequately test.

## Limitations
- Evaluation relies heavily on automated jury scoring, with uncertain reliability compared to human expert judgment
- The benchmark assumes professional ethical codes comprehensively cover all relevant safety scenarios
- Results may not generalize across different jury model versions or as ethical standards evolve

## Confidence
- **High confidence**: The benchmark construction methodology (ethical code grounding, adversarial prompt generation, unanimous expert filtering) is well-documented and reproducible
- **Medium confidence**: The core finding that domain-specialized models show lower safety scores than general models is supported by the data, though jury evaluation reliability remains uncertain
- **Low confidence**: The generalizability of results across different jury model versions and the long-term stability of the benchmark as ethical standards evolve

## Next Checks
1. **Jury reliability validation**: Run a human expert validation study on 100 randomly selected responses to quantify agreement rates with the automated jury scores and identify systematic biases
2. **Cross-jury consistency test**: Evaluate the same responses using an alternative jury pair (e.g., GPT-4 + LLaMA 3.1) to measure score variance and assess the robustness of the evaluation methodology
3. **Temporal stability assessment**: Re-run the evaluation pipeline with updated versions of the jury models (e.g., Claude 4 instead of Claude 3.5) to quantify how evaluation drift affects benchmark scores over time