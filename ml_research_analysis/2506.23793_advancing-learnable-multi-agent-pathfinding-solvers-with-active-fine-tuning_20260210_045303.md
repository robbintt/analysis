---
ver: rpa2
title: Advancing Learnable Multi-Agent Pathfinding Solvers with Active Fine-Tuning
arxiv_id: '2506.23793'
source_url: https://arxiv.org/abs/2506.23793
tags:
- mapf
- learning
- multi-agent
- agents
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new approach for multi-agent pathfinding
  (MAPF) called Delta Data Generation (DDG), which significantly improves the performance
  of existing imitation learning-based solvers. The method addresses the distributional
  shift problem in imitation learning by selectively fine-tuning the pre-trained MAPF-GPT
  model using newly generated data that focuses on the most challenging states where
  the current policy underperforms.
---

# Advancing Learnable Multi-Agent Pathfinding Solvers with Active Fine-Tuning

## Quick Facts
- arXiv ID: 2506.23793
- Source URL: https://arxiv.org/abs/2506.23793
- Reference count: 40
- Outperforms all existing learning-based MAPF solvers, including 85M-parameter MAPF-GPT, while using only 2M parameters

## Executive Summary
This paper addresses the distributional shift problem in imitation learning for multi-agent pathfinding (MAPF) by introducing Delta Data Generation (DDG). The method fine-tunes a pre-trained MAPF-GPT model using selectively generated data that targets the most challenging states where the current policy underperforms. By using a fast approximate solver to identify critical states and a more accurate solver to generate expert corrections, DDG significantly improves both success rates and solution costs. The approach demonstrates remarkable scalability, handling instances with up to 1 million agents while maintaining constant decision time of 160 microseconds per agent.

## Method Summary
The Delta Data Generation method actively fine-tunes MAPF-GPT by identifying states where the policy performs poorly and generating expert data for those specific scenarios. The process involves running the current policy on MAPF instances, extracting every 16th state, and computing cost deltas using an approximate solver (LaCAM* with 2-second limit). When the maximum delta exceeds a threshold (3), the state is re-solved with an accurate solver (LaCAM* with 10-second limit) and the resulting observation-action pairs are stored. The model is fine-tuned using mixed batches (75% expert data, 25% generated data) for 1000 steps per cycle over 340,000 total steps. This active learning approach focuses computational resources on the most problematic states rather than uniformly collecting data across all encountered states.

## Key Results
- MAPF-GPT-DDG outperforms all existing learning-based MAPF solvers on the POGEMA benchmark
- Achieves superior success rates and solution costs across maze, random, and warehouse map types
- Demonstrates ability to handle instances with up to 1 million agents on a 2048x2048 empty map
- Uses only 2M parameters compared to the original 85M-parameter MAPF-GPT while achieving better performance

## Why This Works (Mechanism)
The effectiveness stems from targeting the distributional shift problem directly. Instead of collecting uniform expert data, DDG identifies states where the policy's decisions lead to significantly higher costs than optimal solutions. By focusing fine-tuning on these "critical states" with cost deltas above threshold, the method efficiently corrects the most harmful errors in the policy. The two-tier solver approach balances computational efficiency (using fast approximate solver for screening) with solution quality (using accurate solver for expert data generation).

## Foundational Learning
- **Distributional shift in imitation learning**: Why needed - Understanding why policies degrade when encountering states not well-represented in training data; Quick check - Compare policy performance on training vs. test distribution
- **Active learning for sequential decision-making**: Why needed - Recognizing that selective data collection can be more efficient than passive collection; Quick check - Measure improvement per unit of expert data collected
- **Solver-based cost estimation**: Why needed - Using optimal or near-optimal solvers as reference for measuring policy performance; Quick check - Verify solver time limits produce acceptable solution quality

## Architecture Onboarding

Component map: MAPF instance -> Local observation extraction -> Policy network (MAPF-GPT 2M) -> Action selection -> Cost delta computation -> Expert data generation -> Fine-tuning

Critical path: Instance generation → Policy execution → Delta computation → Expert re-solving → Data storage → Fine-tuning → Evaluation

Design tradeoffs: Computational efficiency (2s vs 10s solver) vs solution quality; Model size (2M vs 85M parameters) vs performance; Data diversity (expert vs generated) vs overfitting risk

Failure signatures: If DAgger-style collection underperforms, check that k=32 pairs are being collected rather than single-step labels; If catastrophic forgetting occurs, verify that De mixing is at least 75% per batch; If data collection is slow, ensure generation (CPU) and fine-tuning (GPU) are running asynchronously

First experiments: 1) Verify delta computation correctly identifies high-cost states by running on a small test set; 2) Test the filtering mechanism by checking that only states with δ > 3 are passed to the accurate solver; 3) Validate the batch mixing ratio by inspecting training batches for the 75:25 expert-to-generated ratio

## Open Questions the Paper Calls Out
None

## Limitations
- The 1B-pair expert dataset De is not publicly available, creating a significant barrier to reproduction
- The exact instance generator G specifications (distributions, agent placement rules, density) are underspecified
- Key training hyperparameters (batch size, learning rate, optimizer configuration) are omitted from the paper

## Confidence
High confidence in the fundamental approach and reported scalability breakthrough (1M agents)
Medium confidence in quantitative comparisons due to limited public availability of baseline implementations
Medium confidence in the precise implementation details, particularly around data generation and fine-tuning parameters

## Next Checks
1. **Data pipeline replication**: Attempt to regenerate the expert dataset De using LaCAM* on the same map distributions to verify the foundational training data matches the paper's description
2. **DDG loop verification**: Implement the full delta computation and filtering pipeline (2s vs 10s solver thresholds, h=16 spacing, k=32 pair selection) on a small test set and verify that identified "critical states" correspond to high-cost regions
3. **Controlled ablation study**: Run the 2M model with DDG vs without DDG on a fixed set of instances, measuring not just success rate and cost, but also tracking the evolution of the cost delta distribution across fine-tuning cycles