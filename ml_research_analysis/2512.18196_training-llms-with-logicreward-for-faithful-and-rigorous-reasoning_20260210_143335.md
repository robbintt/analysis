---
ver: rpa2
title: Training LLMs with LogicReward for Faithful and Rigorous Reasoning
arxiv_id: '2512.18196'
source_url: https://arxiv.org/abs/2512.18196
tags:
- reasoning
- language
- reward
- logicreward
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of flawed reasoning in large language
  models (LLMs), where correct answers may be produced through logically unsound reasoning.
  The proposed method, LogicReward, introduces a novel reward system that enforces
  step-level logical correctness using a theorem prover.
---

# Training LLMs with LogicReward for Faithful and Rigorous Reasoning

## Quick Facts
- arXiv ID: 2512.18196
- Source URL: https://arxiv.org/abs/2512.18196
- Authors: Jundong Xu; Hao Fei; Huichi Zhou; Xin Quan; Qijun Huang; Shengqiong Wu; William Yang Wang; Mong-Li Lee; Wynne Hsu
- Reference count: 40
- Key outcome: 8B model trained with LogicReward outperforms GPT-4o and o4-mini by 11.6% and 2.0% on NLI and logical reasoning tasks

## Executive Summary
This paper addresses the fundamental problem of flawed reasoning in large language models, where correct answers may be produced through logically unsound reasoning. The proposed LogicReward method introduces a novel reward system that enforces step-level logical correctness using a theorem prover (Isabelle). By combining premise validity (grounding reasoning in correct context) and logic validity (formal logical soundness), the method trains models to produce not just correct answers but faithful reasoning chains. The approach includes Autoformalization with Soft Unification to reduce natural language ambiguity during formalization.

## Method Summary
LogicReward is a training framework that uses theorem prover verification to enforce step-level logical correctness in LLM reasoning. The method generates multiple reasoning responses, converts them to formal logic via autoformalization with soft unification, and verifies them using Isabelle/HOL. A LogicScore combines premise validity (cosine similarity of referenced premises to source context) and logic validity (theorem prover check). The system uses Soft Unification to make implicit assumptions explicit, improving formalization success. Training uses SFT on highest-scoring responses followed by DPO on max-min pairs. The approach achieves 68% faithfulness versus 43% for DeepSeek-R1.

## Key Results
- LogicReward achieves 68% faithfulness compared to 43% for DeepSeek-R1 on reasoning tasks
- Outperforms GPT-4o and o4-mini by 11.6% and 2.0% on NLI and logical reasoning benchmarks
- Soft Unification increases syntactic validity from 13.5% to 39.5% and logic validity from 11.9% to 27.3%
- Ablation studies show Premise Validity and Logic Validity are both critical (removing either reduces accuracy by 4.7% and 8.9% respectively)

## Why This Works (Mechanism)

### Mechanism 1
Formal theorem prover verification provides a more reliable reward signal than probabilistic process rewards. Isabelle evaluates logical validity deterministically, returning binary outcomes for each inference step, replacing soft probabilistic signals with formally grounded feedback. The model optimizes for logically sound reasoning chains rather than plausible-looking ones. Break condition: When autoformalization fails, the system falls back to token probability, undermining the formal guarantee.

### Mechanism 2
Soft Unification enables theorem prover verification of ambiguous natural language by making implicit assumptions explicit. An LLM supplements reasoning steps with unstated commonsense assumptions (e.g., "Dad" = "Father"), reducing the gap between human-interpretable natural language and symbolically verifiable logic. Break condition: Incorrect assumptions injected during soft unification propagate into formalized proofs, producing "valid but wrong" reasoning.

### Mechanism 3
Jointly rewarding premise grounding and logical validity reduces reasoning shortcuts. LogicScore combines Premise Validity (cosine similarity between used premises and given context) and Logic Validity (theorem prover verification). Models learn to ground reasoning in correct evidence AND maintain logical soundness, rather than exploiting spurious correlations. Break condition: Models may learn to mechanically include premise phrases without genuine comprehension.

## Foundational Learning

- **Theorem Provers (specifically Isabelle/HOL)**: Core verification engine that determines logical validity of formalized reasoning steps. Without understanding how Isabelle works, you cannot debug formalization failures. Quick check: Can you explain why "Man x ∧ Speaking e ∧ Agent e x" might fail verification given insufficient axioms?

- **Neo-Davidsonian Event Semantics**: The autoformalization pipeline converts natural language to this semantic representation before Isabelle conversion. Understanding the event-argument structure is essential for debugging intermediate outputs. Quick check: How would you represent "A man gives a speech" using Agent, Patient, and event predicates?

- **Direct Preference Optimization (DPO)**: Training uses SFT followed by DPO with LogicScore-based preference pairs. Understanding DPO's reward-implicit formulation is needed to diagnose why certain pairs improve reasoning. Quick check: What happens to DPO learning if chosen/rejected pairs have very similar LogicScores?

## Architecture Onboarding

- **Component map**: Rollout Module -> Autoformalization Pipeline -> Soft Unification Module -> Isabelle Verifier -> Premise Validator -> LogicScore Aggregator -> Refinement Loop -> Training Data Selector

- **Critical path**: 1) Response generation → Autoformalization with Soft Unification 2) Isabelle verification (bottleneck: 39.5% syntactic success rate) 3) Fallback to token probability on syntax failure 4) LogicScore aggregation 5) Data selection → SFT → DPO

- **Design tradeoffs**: Formal guarantee vs. coverage (only ~27% achieve full logic validity); computation time (slower than outcome-only rewards, limiting on-policy RL scalability); weight balancing (equal weights 0.5/0.5 used, no ablation performed)

- **Failure signatures**: Low syntactic validity rate (>50% syntax errors require formalization prompt refinement); two-humped reward distribution (clustering at 0 and 0.5-0.75); refinement doesn't converge for some reasoning chains

- **First 3 experiments**: 1) Ablate Soft Unification: measure accuracy drop (~26% expected) 2) Stress test Premise Validity: construct adversarial examples with paraphrased premises 3) Alternative theorem provers: replace Isabelle with Lean or Coq to compare success rates

## Open Questions the Paper Calls Out

- Can interactive clarification pipelines improve autoformalization accuracy for ambiguous natural language reasoning? Current Soft Unification uses LLMs to inject assumptions but cannot handle all implicit information or resolve deep ambiguity.

- How can LogicReward be optimized to scale efficiently for on-policy reinforcement learning training? Theorem-prover verification per step adds computational overhead; no efficient approximation or caching strategy has been proposed.

- Does LogicReward scale effectively to larger models (>8B parameters) and does the relative benefit persist or diminish? All experiments use 8B models; it's unclear if gains hold for 70B+ models with stronger base reasoning.

## Limitations

- Theorem prover verification bottleneck limits coverage to ~27% of reasoning steps, requiring fallback to token probability for majority of cases
- Method requires substantial computational resources for theorem proving and may not scale efficiently to on-policy reinforcement learning
- Dependence on Isabelle/HOL may introduce bias toward formal representations that don't capture all aspects of natural language reasoning

## Confidence

**High Confidence**: Formal theorem prover verification provides more reliable reward signals than probabilistic alternatives; Soft Unification significantly improves syntactic and logical validity rates; LogicReward achieves state-of-the-art performance on NLI and logical reasoning benchmarks

**Medium Confidence**: Jointly rewarding premise grounding and logical validity reduces reasoning shortcuts; method generalizes to unseen tasks; Soft Unification accurately captures implicit assumptions

**Low Confidence**: LogicReward provides reliable supervision without ground-truth labels; relative importance of w1 vs w2 weightings; long-term stability of trained models

## Next Checks

1. **Coverage Analysis**: Systematically measure the percentage of reasoning steps that achieve full logical validity across different reasoning types (arithmetic, commonsense, formal logic) to quantify practical coverage

2. **Adversarial Robustness Test**: Construct adversarial examples where premises are paraphrased or semantically equivalent but lexically different to test whether Premise Validity metric can be gamed through embedding space manipulation

3. **Alternative Prover Comparison**: Replace Isabelle/HOL with alternative theorem provers (Lean, Coq) to assess whether 39.5% syntactic validity rate is due to formalization approach or specific prover limitations, and evaluate cross-prover consistency in logical validity judgments