---
ver: rpa2
title: "$\u03C6$-test: Global Feature Selection and Inference for Shapley Additive\
  \ Explanations"
arxiv_id: '2512.07578'
source_url: https://arxiv.org/abs/2512.07578
tags:
- test
- selection
- global
- feature
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "\u03D5-test introduces a statistical framework that links Shapley-based\
  \ global explanations with classical inference. It uses SHAP values for initial\
  \ screening, then fits a linear surrogate to the screened features and applies selective\
  \ inference to compute post-selection p-values and confidence intervals for the\
  \ retained coefficients."
---

# $φ$-test: Global Feature Selection and Inference for Shapley Additive Explanations

## Quick Facts
- arXiv ID: 2512.07578
- Source URL: https://arxiv.org/abs/2512.07578
- Authors: Dongseok Kim; Hyoungsun Choi; Mohamed Jismy Aashik Rasool; Gisung Oh
- Reference count: 19
- Primary result: Introduces a statistical framework linking Shapley-based global explanations with classical inference via selective inference

## Executive Summary
φ-test presents a novel method for global feature selection in black-box models that combines SHAP-based screening with selective inference. The approach first uses SHAP values to identify globally important features, then applies a linear surrogate model with selective inference to compute valid post-selection p-values and confidence intervals. Experiments on four tabular regression datasets demonstrate that φ-test achieves high predictive fidelity while selecting few features and producing stable feature sets across data resamples and backbone models.

## Method Summary
φ-test operates in three stages: (1) SHAP screening - compute local SHAP attributions, aggregate to global scores, and retain top-M features; (2) Selection - fit a linear surrogate to black-box predictions on screened features, apply lasso/forward stepwise selection, and extract polyhedral constraints; (3) Inference - compute truncated-normal selective p-values and confidence intervals for retained coefficients. The method uses split-sample inference with 50/50 training splits for selection and inference phases, evaluating fidelity as R²_selected/R²_full × 100% and stability via pairwise Jaccard overlap across resamples.

## Key Results
- On AIRQUALITY dataset: >99% fidelity with only 5 features, 100% stability, and perfect backbone robustness
- Outperforms or matches baselines that use more features or exhibit lower stability
- Achieves high fidelity (>65-99%) across all four tabular datasets with sparse feature selection
- Maintains feature selection stability across data resamples and backbone model variations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Aggregating local SHAP attributions into global importance scores enables principled feature screening without exhaustive search.
- **Mechanism:** Compute $\phi_j(x_i)$ for each sample $i$ and feature $j$, aggregate via $I_j = \frac{1}{n}\sum_{i=1}^n |\phi_j(x_i)|$, then retain top-$M$ features as candidate set $S_0$. This transforms instance-level explanations into a global ranking, with Lemma 4.1 establishing that $S_0$ is conditionally independent of the surrogate response $y^{(sur)}$ given $(f, X)$.
- **Core assumption:** The underlying SHAP engine provides consistent estimates of Shapley values under the model's prediction function.
- **Evidence anchors:** [Section 3.1] Formal definition of global scores and screening threshold $M$; [Tables 1–4] SHAP-TopK baseline achieves 65–99% fidelity, confirming screening identifies predictive features.
- **Break condition:** High-dimensional settings where $p \gg n$ and SHAP estimation becomes unreliable; models with dense, small-magnitude feature interactions that evade screening.

### Mechanism 2
- **Claim:** A linear surrogate regressing black-box outputs on screened features approximates feature-effect directions with tractable inference.
- **Mechanism:** Define surrogate response $y^{(sur)}_i = f(x_i)$, then fit $y^{(sur)} = X_{S_0}\beta + \varepsilon$ on the screened design. Coefficients $\hat{\beta}_j$ quantify how $f$ responds to feature $j$ along observed covariate support—operationalizing a local linear approximation to an arbitrary black-box.
- **Core assumption:** The working Gaussian model $y^{(sur)} \sim \mathcal{N}(X_{S_0}\beta, \sigma^2 I_n)$ adequately captures surrogate noise; nonlinear effects are absorbed into residual variance.
- **Evidence anchors:** [Section 3.2] Explicit surrogate formulation and interpretation as "linear approximation to the black-box predictions"; [Table 5] Surrogate coefficients for AIRQUALITY with clear standard errors and significant p-values for dominant features (C6H6, NOx).
- **Break condition:** Strong nonlinearities or interactions in $f$ that a linear surrogate cannot approximate; surrogate $R^2$ substantially below 1 indicates potential misspecification.

### Mechanism 3
- **Claim:** Truncated-normal selective inference yields valid post-selection p-values and confidence intervals when the selection event admits a polyhedral representation.
- **Mechanism:** After selection (e.g., lasso), the event $E = \{y^{(sur)} : Ay^{(sur)} \leq b\}$ defines polyhedral constraints. Contrast $T_j = v_j^T y^{(sur)}$ is conditionally independent of $W_j = (I - c_j v_j^T)y^{(sur)}$, and $T_j | (W_j, E)$ follows a $\mathcal{N}(\theta_j, \tau_j^2)$ distribution truncated to $[a_j(W_j), b_j(W_j)]$. P-values and CIs computed from this truncated law satisfy Proposition 4.2's conditional validity.
- **Core assumption:** Selection procedure admits polyhedral form; Gaussian noise model holds conditional on $(f, X, S_0)$.
- **Evidence anchors:** [Section 4.2, Lemma B.1] Formal derivation of truncated-normal law; [Proposition 4.2] Proof that selective p-values control type-I error at level $\alpha$ and CIs achieve $\geq 1-\alpha$ coverage.
- **Break condition:** Non-polyhedral selection rules; non-Gaussian residuals with heavy tails.

## Foundational Learning

- **Shapley Values and SHAP:**
  - Why needed here: The entire $\phi$-test pipeline starts from local Shapley attributions; without understanding what $\phi_j(x_i)$ represents, the global aggregation step is opaque.
  - Quick check question: If $\phi_1(x) = 0.3$ and $\phi_2(x) = -0.2$, what does this imply about features 1 and 2's contributions to $f(x)$ compared to the baseline prediction?

- **Post-Selection Inference (PoSI):**
  - Why needed here: Naive regression p-values after data-driven selection are anti-conservative; understanding why selection induces bias and how conditioning restores validity is essential to interpret $\phi$-test's selective p-values correctly.
  - Quick check question: Why does selecting features based on the same data used for inference invalidate standard t-test p-values?

- **Polyhedral Geometry in Linear Models:**
  - Why needed here: The selective-inference machinery relies on representing selection events as linear inequality constraints $Ay \leq b$; grasping this geometry explains why lasso/forward stepwise admit tractable truncated-normal inference.
  - Quick check question: How does the constraint "feature $j$ enters the lasso path at step $k$ with positive sign" translate into a linear inequality on $y$?

## Architecture Onboarding

- **Component map:** SHAP engine → local $\phi_j(x_i)$ → aggregate $I_j$ → top-$M$ features $S_0$ → surrogate regression on $X_{S_0}$ → polyhedral selector (lasso/stepwise) → selected set $S$, constraints $(A, b)$, contrasts $\{v_j\}_{j \in S}$ → truncated-normal p-values/CIs → global feature-importance table

- **Critical path:** SHAP computation is often the bottleneck for large $n$ or neural backbones (KernelSHAP); selection via LARS-lasso is $O(|S_0|^2 n)$; inference is $O(|S|)$ per feature. All stages require access to trained $f$ and evaluation data $\mathcal{D}$.

- **Design tradeoffs:**
  - Split-sample vs. full-data selective inference: Split-sample (used in experiments) yields classical t-tests that are valid but conservative and require $n/2$ samples per split. Full-data selective inference is more efficient but requires polyhedral constraint extraction and truncated-normal computations.
  - Screening budget $M$ vs. target sparsity $K$: Larger $M$ reduces risk of discarding true signals but increases Stage 2 dimensionality; the paper recommends $M \in [K, 3K]$.
  - Lasso vs. forward stepwise: Both polyhedral; lasso provides path-based selection, stepwise offers interpretable entry order. Ablation (Table 6) shows similar fidelity.

- **Failure signatures:**
  - Surrogate $R^2 \ll 1$: Linear surrogate poorly approximates $f$; p-values remain valid for the surrogate but lose interpretability for the black-box.
  - Empty or full selection: $M$ too small (all features screened out) or selection threshold too permissive (all $S_0$ retained).
  - Unstable $S$ across resamples: Check backbone training stability, SHAP engine randomness, and whether feature effects are genuinely small.

- **First 3 experiments:**
  1. Baseline sanity check: Run $\phi$-test on a simulated linear model $y = X\beta + \varepsilon$ where ground-truth important features are known; verify high fidelity, correct selection, and nominal selective-p-value coverage under repeated trials.
  2. Ablation on $M$: Fix $K=5$, vary $M \in \{5, 7, 10, 15\}$ on CONCRETE dataset; plot fidelity vs. stability to identify robust operating point.
  3. Backbone comparison: Train XGBoost and MLP on the same data split; compute $\phi$-test feature sets and report robustness (Jaccard overlap). Diagnose divergence via SHAP score distributions per feature.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the $\phi$-test framework be extended to validly handle grouped features and low-order interactions?
  - Basis in paper: [explicit] Future work states, "Future directions include extending $\phi$-test to grouped features and low-order interactions..."
  - Why unresolved: The current implementation relies on a linear surrogate for main effects and defines polyhedral constraints for standard variable selection, but lacks a mechanism for modeling hierarchical or interaction terms within the selective inference layer.
  - What evidence would resolve it: A derivation of polyhedral constraints for group or interaction selection events, plus empirical validation on datasets with known interaction structures showing maintained error control.

- **Open Question 2:** How does $\phi$-test behave in high-dimensional regimes where the number of features significantly exceeds the number of observations ($p \gg n$)?
  - Basis in paper: [explicit] Future work suggests "investigating how $\phi$-test behaves in settings with many more features than observations."
  - Why unresolved: Experiments were limited to tabular datasets with small feature counts ($p \le 12$), and it is unclear if the initial SHAP-screening step remains effective or stable when the feature space is vast.
  - What evidence would resolve it: Theoretical analysis or empirical benchmarks on high-dimensional data (e.g., genomics) showing that the screening step effectively reduces dimensionality for the surrogate without sacrificing statistical power.

- **Open Question 3:** Can the method incorporate the uncertainty arising from the training of the black-box predictor $f$ into the selective inference?
  - Basis in paper: [explicit] Future work proposes to "incorporate uncertainty about the training of $f$ into the selective-inference layer."
  - Why unresolved: The current theoretical guarantees condition on a fixed fitted model $f$; they do not account for the variance introduced by the model training process itself, limiting the scope of inference to the specific realization of $f$.
  - What evidence would resolve it: A modified framework that treats $f$ as a random variable and propagates its training variance; simulation results demonstrating that valid coverage is maintained even when accounting for model uncertainty.

## Limitations
- Performance generalizability to high-dimensional or image-based problems remains unclear
- Reliance on linear surrogate approximation may break down with strong nonlinearities
- Limited empirical validation to four tabular datasets and two backbone families

## Confidence
- **High confidence:** SHAP-based screening correctly identifies globally important features when interactions are not overly dense; selective inference framework is mathematically valid under stated assumptions
- **Medium confidence:** Surrogate coefficients and selective p-values are meaningful proxies for the black-box's true feature effects; stability and robustness metrics are robust to resampling and backbone choice
- **Low confidence:** $\phi$-test's performance translates directly to high-dimensional or image-based problems; selection stability persists when SHAP computation is highly noisy

## Next Checks
1. **Surrogate adequacy test:** For each dataset, compute surrogate $R^2$ and residual normality diagnostics; if $R^2 < 0.9$, assess whether selective p-values remain interpretable
2. **Polyhedral constraint extraction:** Verify that the lasso path from LARS on screened features yields explicit $A, b$ matrices; simulate truncated-normal inference to confirm correct implementation
3. **SHAP engine stability:** Run KernelSHAP on AIRQUALITY with background sizes ranging from 50 to 500; measure stability of top-M feature rankings and downstream selection sets