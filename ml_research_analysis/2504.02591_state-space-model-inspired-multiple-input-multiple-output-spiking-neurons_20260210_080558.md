---
ver: rpa2
title: State-Space Model Inspired Multiple-Input Multiple-Output Spiking Neurons
arxiv_id: '2504.02591'
source_url: https://arxiv.org/abs/2504.02591
tags:
- neuron
- state
- output
- spiking
- neurons
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a novel framework for spiking neural networks
  (SNNs) by interpreting neurons as state-space models (SSMs) with linear state evolutions
  and non-linear spiking activation functions. The proposed MIMO spiking neuron model
  allows neurons to have multiple input and output channels, going beyond the traditional
  single-input single-output (SISO) models in SNN literature.
---

# State-Space Model Inspired Multiple-Input Multiple-Output Spiking Neurons

## Quick Facts
- arXiv ID: 2504.02591
- Source URL: https://arxiv.org/abs/2504.02591
- Reference count: 29
- This work proposes a novel MIMO spiking neuron model based on state-space models, demonstrating significant accuracy gains through spatial channel coding.

## Executive Summary
This paper introduces a new framework for spiking neural networks (SNNs) by interpreting neurons as state-space models (SSMs) with linear state evolutions and non-linear spiking activation functions. The proposed MIMO spiking neuron model allows neurons to have multiple input and output channels, going beyond the traditional single-input single-output (SISO) models in SNN literature. The framework is based on the idea that neurons can be seen as SSMs, where the state evolution is parameterized by matrices similar to SSMs, but the output is determined by a spike-based activation function. Results show that significant performance gains can be obtained by increasing the number of output channels of a neuron, especially when the number of neurons is low and the internal state space is large. In particular, a network with spiking neurons with multiple-output channels can achieve the same level of accuracy as the baseline with continuous-valued communications on the same reference network architecture.

## Method Summary
The proposed MIMO spiking neuron model interprets neurons as discrete-time state-space models where the state evolves linearly (v[t+1] = Av[t] + Bi[t]) and the output is generated through a spike-based activation function. The neuron can have multiple input channels (MISO) and multiple output channels (SIMO), with output channels projected through a learnable matrix C and thresholded to generate spikes. The model uses surrogate gradient descent for training, with stability enforced by constraining eigenvalues of the state transition matrix. The network architecture consists of two hidden layers with batch normalization and dropout regularization, trained on the SHD dataset for speech recognition.

## Key Results
- MIMO neurons with multiple output channels can achieve the same accuracy as continuous-valued baseline models on the same architecture
- Significant performance gains are obtained by increasing output channels, particularly when neuron count is low and internal state space is large
- Diagonal state-transition matrices perform comparably to mixing matrices in most configurations, with computational advantages
- MISO configurations generally underperform SISO baselines due to overfitting concerns

## Why This Works (Mechanism)

### Mechanism 1: Spatial Channel Coding to Mitigate Quantization Bottleneck
Increasing output channels ($n_{out}$) in a spiking neuron (SIMO) compensates for information loss caused by binary quantization, effectively encoding amplitude information spatially. In standard SISO neurons, a high-dimensional internal state $v$ is compressed into a single binary spike, creating a severe bottleneck. The MIMO model uses a learnable projection matrix $C$ to map the state $v$ to multiple output channels ($y \in \mathbb{R}^{n_{out}}$). Each channel applies a threshold (bias) to generate a spike. This allows the neuron to transmit a "graded" representation across multiple binary channels rather than being limited to a single bit of information per timestep.

### Mechanism 2: High-Dimensional State Evolution for Temporal Memory
The model defines neuron dynamics as a linear state-space update $v[t+1] = Av[t] + Bi[t]$. Unlike traditional Leaky Integrate-and-Fire (LIF) neurons where the state is reset after spiking, this model allows state variables to evolve independently of the output spike generation (no explicit reset mechanism in Eq 12). This preserves temporal history in the state vector $v$. The state transition matrix $A$ can be parameterized (e.g., diagonal or diagonal+DFT) to retain information over long sequences without instability.

### Mechanism 3: Neuron Count vs. State Dimension Trade-off
In SNNs with spike-based communication, there is a distinct trade-off between the number of neurons ($h$) and the size of their internal state ($n$); large states require sufficient output channels to be useful. A network with few neurons but massive internal states cannot communicate its internal representation efficiently if restricted to single-bit outputs. Increasing the output channels ($n_{out}$) creates a "fat pipe" for information flow, allowing a small population of complex neurons to match the performance of a large population of simple neurons.

## Foundational Learning

- **Concept: State-Space Models (SSMs)**
  - Why needed here: The paper redefines the spiking neuron fundamentally as a discrete-time SSM ($v_{t+1} = A v_t + B x_t$). Without understanding linear system dynamics (state transition, stability), one cannot grasp how these neurons retain memory differently from standard RNNs or LIF neurons.
  - Quick check question: How does the matrix $A$ determine the "memory" length of the neuron?

- **Concept: Surrogate Gradient Descent**
  - Why needed here: The spiking function is non-differentiable (step function). The paper explicitly uses Backpropagation Through Time (BPTT) with surrogate gradients (Section VI-A2). This is the standard technique for training SNNs and is critical for optimizing the proposed MIMO parameters.
  - Quick check question: Why can't we use standard backpropagation directly on the spike generation step $s_{out}[t] = f_\theta(y[t])$?

- **Concept: Quantization and Rate Coding**
  - Why needed here: The move from SISO to MIMO is essentially a move from 1-bit quantization to multi-bit quantization distributed across channels. Understanding the information limits of binary spikes helps explain why adding channels (SIMO) improves accuracy.
  - Quick check question: Does a MIMO neuron with 64 output channels transmit more information per timestep than a SISO neuron?

## Architecture Onboarding

- **Component map:** Input spike trains -> Weight matrix W -> MIMO neuron block (State Update -> Output Projection -> Spike Generation) -> Next layer
- **Critical path:**
  1. Initialize State Matrix $A$ (S4D-Lin initialization) with eigenvalues $< 1$ for stability
  2. Perform State Evolution (Linear)
  3. Project State to Output Space (via $C$)
  4. Generate Spikes (Non-linear)
  5. Backpropagate using Surrogate Gradient
- **Design tradeoffs:**
  - Diagonal vs. Non-Diagonal $A$: Diagonal is computationally cheaper and parallelizable; Non-diagonal (using DFT) mixes state variables but adds complexity ($O(n \log n)$)
  - SISO vs. SIMO: SIMO drastically increases the weight matrix size $W$ (connecting $h \times n_{out}$ channels), increasing compute cost for accuracy gains
  - Neurons ($h$) vs. State Dim ($n$): Fixed budget $h \times n$. Prefer higher $h$ for SISO; prefer higher $n$ + high $n_{out}$ for SIMO
- **Failure signatures:**
  - Instability: State values $v$ explode to infinity. Fix: Check eigenvalue clipping constraints on $A$
  - Dead Neurons: Output is all zeros. Fix: Check bias initialization $c_{bias}$ or learning rate
  - MISO Degradation: Performance drops when increasing input channels. Fix: Use distinct hyperparameter tuning (regularization) to prevent over-using single inputs
- **First 3 experiments:**
  1. Sanity Check (Table II Reproduction): Implement the SISO model with diagonal $A$ on SHD dataset. Verify that accuracy peaks at moderate state dimensions (e.g., $n=16, h=128$) and drops at extreme dimensions ($n=1024, h=2$)
  2. SIMO Ablation: Using a fixed low-neuron configuration ($h=32, n=64$), sweep output channels $n_{out} \in \{1, 8, 64, 128\}$. Confirm the accuracy jump reported in Table III
  3. Signed vs. Unsigned Spikes: Compare binary $\{0, 1\}$ vs. ternary $\{-1, 0, 1\}$ outputs on the SIMO architecture to quantify the value of the sign bit mentioned in Section VI-D

## Open Questions the Paper Calls Out
- Can the performance degradation observed in Multiple-Input Single-Output (MISO) models be resolved through specialized hyper-parameter optimization or regularization techniques?
- Which specific MIMO neuron configurations can be efficiently implemented on physical neuromorphic hardware without negating accuracy gains?
- Do non-diagonal (mixing) state-transition matrices offer distinct advantages over diagonal matrices for spiking networks, or are they redundant given sufficient output channels?

## Limitations
- The core trade-off between neuron count and internal state dimension is primarily validated on the SHD dataset with a fixed architecture (2-layer MLP)
- The claim that MISO is "not recommended" due to overfitting is based on limited experiments and lacks exploration of potential regularization techniques
- The impact of MISO configurations is mentioned but not thoroughly explored, with only one footnote addressing potential overfitting issues

## Confidence
- **High Confidence:** The mathematical formulation of MIMO neurons as state-space models is internally consistent and reproducible
- **Medium Confidence:** The reported performance gains (e.g., SIMO matching continuous-valued baselines) are specific to the tested configurations and datasets
- **Low Confidence:** The claim that MISO is "not recommended" due to overfitting is based on limited experiments and lacks exploration of potential regularization techniques

## Next Checks
1. Test the MIMO framework on a spiking CNN or LSTM for image/speech tasks to verify if the $h$ vs $n$ trade-off holds beyond MLPs
2. Compare S4D-Lin initialization for the state matrix $A$ against random orthogonal or learned initializations to assess sensitivity
3. Implement specific regularization (e.g., group lasso, attention gating) for MISO neurons to explore if their performance can be improved, challenging the "not recommended" conclusion