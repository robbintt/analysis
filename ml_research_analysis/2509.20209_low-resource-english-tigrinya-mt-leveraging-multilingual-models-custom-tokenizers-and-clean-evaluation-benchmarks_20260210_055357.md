---
ver: rpa2
title: 'Low-Resource English-Tigrinya MT: Leveraging Multilingual Models, Custom Tokenizers,
  and Clean Evaluation Benchmarks'
arxiv_id: '2509.20209'
source_url: https://arxiv.org/abs/2509.20209
tags:
- translation
- tigrinya
- languages
- machine
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study addresses the challenge of machine translation for\
  \ low-resource languages, specifically Tigrinya, by proposing a method that leverages\
  \ multilingual pretrained models combined with language-specific tokenization and\
  \ domain-adaptive fine-tuning. The approach uses a custom morpheme-aware tokenizer\
  \ tailored for Tigrinya\u2019s Geez script and integrates it into the MarianMT architecture,\
  \ which is fine-tuned on a curated English-Tigrinya parallel corpus."
---

# Low-Resource English-Tigrinya MT: Leveraging Multilingual Models, Custom Tokenizers, and Clean Evaluation Benchmarks

## Quick Facts
- arXiv ID: 2509.20209
- Source URL: https://arxiv.org/abs/2509.20209
- Reference count: 40
- Key outcome: Custom SentencePiece tokenizer + fine-tuned MarianMT achieves BLEU: 21/19, chrF: 19.50/16.20 for En→Tigrinya/Tigrinya→En, significantly outperforming zero-shot baselines.

## Executive Summary
This study tackles the challenge of machine translation for Tigrinya, a low-resource language with complex morphology and Ge'ez script. The authors propose a method combining a custom morpheme-aware tokenizer with multilingual model fine-tuning to overcome tokenization inefficiencies and negative transfer from related languages. Their approach leverages the NLLB parallel corpus and a manually curated evaluation benchmark, achieving substantial improvements in BLEU and chrF scores over zero-shot baselines. The work also highlights the importance of domain-adaptive training and high-quality evaluation data in low-resource settings.

## Method Summary
The method fine-tunes the MarianMT transformer model using a custom SentencePiece tokenizer trained on Tigrinya data from the NLLB corpus. Training uses AdamW optimizer (lr=1.44e-07, weight decay=0.01), batch size 16, max length 128, and mixed precision, running for 3 epochs. The model is evaluated on a manually curated 4,000-sentence benchmark across four domains, using BLEU and chrF metrics. Code and model checkpoints are publicly available.

## Key Results
- Custom tokenizer + fine-tuned MarianMT: BLEU 21 (En→Ti), 19 (Ti→En); chrF 19.50 (En→Ti), 16.20 (Ti→En)
- Outperforms zero-shot baseline by ~10 BLEU and ~10 chrF points
- Manual benchmark curation and statistical significance testing improve evaluation reliability

## Why This Works (Mechanism)

### Mechanism 1: Morpheme-Aware Subword Segmentation
Replacing a generic multilingual tokenizer with a custom SentencePiece tokenizer trained specifically on Tigrinya script reduces lexical fragmentation and improves character-level translation accuracy. Standard multilingual tokenizers often fragment low-resource languages into individual characters due to limited vocabulary allocation. By training a custom tokenizer on the Tigrinya portion of the NLLB corpus, the model creates subword units that align better with the language's morphological structure, allowing the attention mechanism to process meaningful semantic units rather than scattered characters.

### Mechanism 2: Negative Transfer Mitigation via Domain-Adaptive Fine-Tuning
Fine-tuning a multilingual model (MarianMT) on a curated parallel corpus suppresses cross-lingual interference, specifically confusion with script-similar languages like Amharic. Zero-shot multilingual models handle Tigrinya poorly, sometimes misinterpreting it as Amharic due to shared Ge'ez script. Fine-tuning re-aligns the encoder-decoder attention weights specifically for English-Tigrinya mapping, forcing the model to distinguish Tigrinya morphological patterns from related Semitic languages.

### Mechanism 3: Evaluation Signal Fidelity via Benchmark Curation
Constructing a noise-filtered, domain-diverse evaluation set allows for more reliable measurement of actual model capability compared to raw public corpora. Public datasets like OPUS often contain alignment errors that artificially inflate or deflate BLEU scores. By manually cleaning and aligning a 4,000-sentence benchmark across four domains, the evaluation metrics more accurately reflect the model's semantic preservation rather than its ability to guess amidst noise.

## Foundational Learning

- **Concept**: Subword Tokenization (BPE/Unigram)
  - Why needed: Tigrinya is morphologically rich; word-level tokenization results in huge vocabulary and high OOV rates. Subword tokenization breaks words into meaningful chunks (prefixes, roots, suffixes) allowing the model to compose unseen words.
  - Quick check: If the tokenizer splits the Tigrinya word for "weather" into characters rather than syllables, how does that affect the self-attention mechanism's ability to learn semantic relationships?

- **Concept**: Transfer Learning in NMT
  - Why needed: Tigrinya is low-resource; training a Transformer from scratch requires data volumes that do not exist. Transfer learning allows the system to leverage "knowledge" of language structure learned from high-resource languages (via MarianMT) and adapt it to Tigrinya.
  - Quick check: Why might a multilingual model struggle to translate Tigrinya in a zero-shot setting even if it has seen a related language (Amharic) during pre-training?

- **Concept**: Metric Correlation (BLEU vs. chrF)
  - Why needed: BLEU relies on exact n-gram matches, which can penalize morphologically rich languages where word forms vary significantly. chrF looks at character n-grams, making it more robust for evaluating Tigrinya where a root might appear in many different conjugated forms.
  - Quick check: In Table II, why is the chrF score improvement arguably more significant evidence of success than the BLEU improvement for a morphologically complex language?

## Architecture Onboarding

- **Component map**: Preprocessed NLLB Corpus -> Custom SentencePiece Tokenizer -> MarianMT Model (embedding resize) -> Hugging Face Trainer (AdamW, 3 epochs) -> sacreBLEU/chrF Evaluator
- **Critical path**:
  1. Data Hygiene: Script normalization for Ge'ez characters and sentence alignment verification (filtering OPUS/NLLB data)
  2. Tokenizer Training: Training `sentencepiece` on the filtered Tigrinya corpus to generate `tokenizer.model`
  3. Model Resizing: Resizing MarianMT's embedding layer to match the new tokenizer's vocabulary size
  4. Fine-Tuning: Training on the NLLB English-Tigrinya pair using the new tokenizer
- **Design tradeoffs**:
  - Custom vs. Universal Tokenizer: The custom tokenizer improves performance on Tigrinya but breaks the model's ability to translate other languages (it effectively mono-lingualizes the encoder/decoder for the new vocab)
  - Dataset Size vs. Noise: The authors chose to use NLLB but had to aggressively filter/clean it. Using raw web-scraped data would increase volume but likely introduce noise that degrades the fragile low-resource model
- **Failure signatures**:
  - Script Confusion: Transliterations appearing in Amharic script instead of Tigrinya (or mixed scripts) indicates negative transfer from related languages in the pre-training base
  - Repetition Loops: Generating infinite suffixes (e.g., "the boy went went went") suggests the tokenizer has fragmented morphemes too finely, confusing the decoder
  - Metric Divergence: BLEU rising while chrF stagnates might indicate the model is memorizing rigid phrases rather than learning flexible morphological construction
- **First 3 experiments**:
  1. Baseline Reproduction: Run zero-shot inference on the curated test set using the original MarianMT model and default tokenizer to establish the floor (expected BLEU ~19/17)
  2. Tokenizer Ablation: Fine-tune MarianMT *without* the custom tokenizer (using the default multilingual vocab) to isolate the specific contribution of the Tigrinya-specific vocabulary
  3. Generalization Test: Evaluate the final model on "out-of-domain" sentences (e.g., technical or medical texts not in the training set) to verify if the custom tokenizer aids generalization or causes overfitting

## Open Questions the Paper Calls Out
- What embedding initialization strategies most effectively transfer multilingual pretrained models to low-resource languages with limited pretraining exposure, such as Tigrinya?
- Can shared linguistic structures across Ge'ez-script languages (Amharic, Tigre, Tigrinya) enable more effective cross-lingual transfer than transfer from unrelated high-resource languages?
- How can dialectal variation and syntactic nuances in morphologically rich languages be incorporated into subword tokenization schemes?

## Limitations
- The custom tokenizer improves morphological segmentation but does not yet fully capture dialectal variation or syntactic nuances, which are important for robust translation in real-world settings.
- The role of embedding initialization during transfer from multilingual pretrained models warrants further investigation, particularly for languages with limited direct training exposure.
- The curated evaluation set is high-quality but may not fully represent the full diversity of Tigrinya dialectal variations or out-of-domain contexts.

## Confidence
- **High Confidence**: The custom tokenizer improves BLEU/chrF scores relative to zero-shot baselines.
- **Medium Confidence**: The custom tokenizer reduces lexical fragmentation and improves character-level accuracy.
- **Medium Confidence**: Fine-tuning suppresses negative transfer from script-similar languages (e.g., Amharic).
- **Low Confidence**: The curated evaluation set is both noise-free and representative of the broader Tigrinya translation task.

## Next Checks
1. **Tokenizer Ablation with OOV and Fragmentation Analysis**: Re-run the fine-tuning experiment using the default MarianMT tokenizer and the custom tokenizer, then compare not only BLEU/chrF but also out-of-vocabulary (OOV) rates, average token length, and fragmentation indices (e.g., mean byte-pair merge depth) on held-out Tigrinya data.
2. **Out-of-Domain Generalization Test**: Evaluate the final model on a held-out set of Tigrinya sentences from a domain not present in the NLLB training corpus (e.g., legal or scientific texts, or sentences from a different dialect). Compare performance to a zero-shot baseline and report BLEU/chrF deltas.
3. **Statistical Significance Testing with Confidence Intervals**: Re-run the main experiments with multiple random seeds (e.g., n=5) and report mean ± standard error for BLEU and chrF. Perform paired statistical tests (e.g., bootstrap resampling) between the custom tokenizer + fine-tuned model and both the zero-shot and default tokenizer baselines. Include confidence intervals and p-values to substantiate claims of improvement.