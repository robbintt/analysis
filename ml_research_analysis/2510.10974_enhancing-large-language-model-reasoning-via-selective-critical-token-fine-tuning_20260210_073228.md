---
ver: rpa2
title: Enhancing Large Language Model Reasoning via Selective Critical Token Fine-Tuning
arxiv_id: '2510.10974'
source_url: https://arxiv.org/abs/2510.10974
tags:
- tokens
- critical
- reasoning
- fine-tuning
- qwen2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Critical Token Fine-tuning (CFT), a method
  that improves large language model reasoning by updating only a small subset of
  tokens identified as functionally indispensable for correctness, using counterfactual
  perturbations. Unlike standard fine-tuning that treats all tokens equally, CFT focuses
  gradient updates on critical reasoning steps while preserving the diversity of non-critical
  tokens.
---

# Enhancing Large Language Model Reasoning via Selective Critical Token Fine-Tuning

## Quick Facts
- **arXiv ID:** 2510.10974
- **Source URL:** https://arxiv.org/abs/2510.10974
- **Reference count:** 33
- **Primary result:** CFT improves LLM reasoning accuracy by fine-tuning only critical tokens, achieving higher performance than SFT with <12% of tokens trained.

## Executive Summary
This paper introduces Critical Token Fine-tuning (CFT), a method that improves large language model reasoning by updating only a small subset of tokens identified as functionally indispensable for correctness, using counterfactual perturbations. Unlike standard fine-tuning that treats all tokens equally, CFT focuses gradient updates on critical reasoning steps while preserving the diversity of non-critical tokens. Evaluated across five models and eleven mathematical reasoning benchmarks, CFT achieves higher accuracy than standard fine-tuning and other baselines while training on less than 12% of tokens. It also improves inference-time scaling by enhancing sampling diversity and serves as a stronger initialization for reinforcement learning, sustaining performance gains with better exploration. Results show CFT's effectiveness extends beyond math to domains like medical QA, highlighting its potential as a general fine-tuning framework.

## Method Summary
CFT identifies critical tokens through counterfactual perturbation: for each token in a correct solution, substitute it with top-k alternatives, regenerate continuations greedily, and mark the token as critical if all substitutions yield incorrect answers. The model is then fine-tuned using cross-entropy loss computed only at critical token positions, with non-critical tokens masked. This selective gradient application preserves output diversity at replaceable positions while strengthening decisive reasoning steps. The method is implemented efficiently through parallel batch processing of all position-substitution pairs.

## Key Results
- CFT achieves 81.5% accuracy on GSM8K (vs 77.0% for SFT) while training on only 7.7% of tokens
- CFT improves Pass@N scaling, with Pass@10 improving from 92.7% to 95.3% on GSM8K
- CFT-initialized RL models maintain higher entropy and achieve superior final performance compared to SFT-initialized models
- Performance gains extend to medical QA tasks, demonstrating broader applicability beyond mathematical reasoning

## Why This Works (Mechanism)

### Mechanism 1: Selective Gradient Application Prevents Distribution Collapse
By masking gradients on non-critical tokens, CFT preserves output diversity at replaceable positions while strengthening decisive reasoning steps. Standard SFT applies mode-seeking updates at every position, collapsing token distributions uniformly. CFT applies this pressure only when counterfactual substitution proves alternatives fail, preserving pre-trained entropy at non-critical positions.

### Mechanism 2: Counterfactual Perturbation Identifies Causal Token Dependencies
Token criticality is determined functionally via substitution testing rather than heuristics. For each token, replace it with top-k alternatives, regenerate continuations greedily, and verify final answers. If all substitutions yield incorrect answers, the token is critical, establishing a causal relationship: the token's specific value is necessary for correctness.

### Mechanism 3: Entropy Preservation Enables Better Exploration in RL
CFT-trained models serve as stronger RL initializations because they maintain higher entropy, enabling sustained exploration rather than premature convergence. SFT collapses distributions at all positions, leaving RL with a narrowed solution space. CFT preserves entropy at non-critical positions, so RL starts with a broader search space and continues exploring through later training stages.

## Foundational Learning

- **Mode-Seeking vs. Coverage in Language Models**
  - Why needed: Understanding that cross-entropy loss pushes probability mass toward reference tokens (mode-seeking) helps explain why uniform SFT collapses diversity.
  - Quick check: What happens to the probability of valid alternative tokens when the model is trained to maximize likelihood of a single reference sequence?

- **Counterfactual Reasoning in Verification**
  - Why needed: CFT's token identification relies on comparing "what happens if" scenarios—replacing tokens and observing outcomes.
  - Quick check: If substituting token A with token B still yields a correct answer, what does this imply about token A's functional role?

- **Entropy as Exploration Signal**
  - Why needed: The paper links higher entropy to better RL exploration; understanding this connection is essential for interpreting RL initialization results.
  - Quick check: Why might a model with lower entropy struggle to discover new solution paths during reinforcement learning?

## Architecture Onboarding

- **Component map:** Greedy Correct Subset Construction -> Counterfactual Rollout Engine -> Critical Token Masking -> Selective Loss Computation -> Parallel Batch Optimization
- **Critical path:** Generate greedy solutions → filter correct ones → annotate critical tokens (most expensive step) → train with masked loss → evaluate on benchmarks
- **Design tradeoffs:**
  - k value (alternatives tested): Higher k yields stricter criticality but more annotation compute
  - Strictness definition: "Union" captures more tokens but dilutes signal; "Strict" is more precise
  - Self-annotation vs. transfer: Self-generated responses with self-annotation perform best
- **Failure signatures:**
  - Low critical ratio (<5%): May indicate annotation pipeline bugs or verification function issues
  - No improvement over SFT: Check that critical tokens are actually being used
  - RL instability from CFT init: Higher entropy can cause early volatility
- **First 3 experiments:**
  1. Reproduce GSM8K baseline on Qwen2.5-7B: generate greedy solutions, compute critical ratio, train CFT vs. SFT, compare GSM8K accuracy
  2. Ablate k value: test Strict-2 vs. Strict-3 vs. Union on same model to confirm strictness tradeoff
  3. Measure Pass@N scaling: sample N=1,5,10,20 responses from CFT vs. SFT models, compute Pass@N curves

## Open Questions the Paper Calls Out

- Can CFT be effectively adapted for unstructured tasks involving high creativity or ambiguity, such as storytelling or open-ended writing?
- Is CFT robust to the absence of explicit verifiable ground truths, and can it function using only proxy reward models or weak supervision?
- Does the dominance of numeric tokens in the critical set limit the method's ability to enhance semantic or logical reasoning capabilities?

## Limitations
- CFT's effectiveness is currently validated only on tasks with verifiable answers, limiting its applicability to creative or ambiguous tasks
- Performance degrades when transferring critical token annotations across different model families
- The method's reliance on greedy decoding for counterfactual perturbations may introduce biases in critical token identification

## Confidence

**High Confidence:** CFT outperforms standard fine-tuning on mathematical reasoning benchmarks; CFT preserves diversity better than SFT; CFT serves as superior RL initialization

**Medium Confidence:** Critical token identification via counterfactual perturbation is more reliable than heuristic methods; the ~7.7% critical token ratio is representative; performance gains extend beyond math to medical QA

**Low Confidence:** Exact contribution of each mechanism to overall performance; long-term stability of CFT-initialized RL training; optimal k value and strictness thresholds for different domains

## Next Checks
1. Verification Function Ablation: Systematically vary the answer extraction and verification function F(Y, A) across different regex patterns, numerical tolerance thresholds, and semantic equivalence metrics to establish sensitivity bounds.

2. Cross-Model Family Transfer Study: Conduct controlled experiments transferring critical token annotations from Qwen2.5 models to LLaMA3.1 and OLMo2 models, and vice versa, to quantify performance degradation patterns.

3. Long-Horizon RL Stability Analysis: Extend RL experiments beyond 100 steps to 500+ steps, monitoring entropy trajectories, exploration metrics, and final task performance to establish long-term stability characteristics.