---
ver: rpa2
title: Object-Level Verbalized Confidence Calibration in Vision-Language Models via
  Semantic Perturbation
arxiv_id: '2504.14848'
source_url: https://arxiv.org/abs/2504.14848
tags:
- confidence
- calibration
- probability
- verbalized
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of object-level verbalized confidence
  miscalibration in vision-language models, where models often express high confidence
  in incorrect or hallucinated object detections. The proposed Confidence Calibration
  through Semantic Perturbation (CSP) framework addresses this by constructing a perturbed
  dataset where Gaussian noise is applied to key object regions at varying intensities
  to simulate visual uncertainty, with corresponding confidence labels.
---

# Object-Level Verbalized Confidence Calibration in Vision-Language Models via Semantic Perturbation

## Quick Facts
- **arXiv ID:** 2504.14848
- **Source URL:** https://arxiv.org/abs/2504.14848
- **Reference count:** 40
- **Primary result:** Object-level verbalized confidence calibration framework improves VLM calibration metrics significantly while maintaining task performance

## Executive Summary
This paper addresses the problem of object-level verbalized confidence miscalibration in vision-language models (VLMs), where models often express high confidence in incorrect or hallucinated object detections. The proposed Confidence Calibration through Semantic Perturbation (CSP) framework addresses this by constructing a perturbed dataset where Gaussian noise is applied to key object regions at varying intensities to simulate visual uncertainty, with corresponding confidence labels. The model is then trained through a two-stage process combining supervised fine-tuning and preference optimization. Experiments across multiple benchmarks (POPE and AMBER) and state-of-the-art VLMs show significant improvements in calibration metrics while maintaining or improving overall task performance.

## Method Summary
The CSP framework uses a two-stage training process to improve verbalized confidence calibration in VLMs. First, it constructs a perturbed dataset by applying Gaussian noise to object regions identified by GroundingDINO and segmented by SAM, with noise intensity controlled by a confidence parameter. Second, the model undergoes supervised fine-tuning (SFT) to learn confidence-expression from the perturbed data, followed by preference optimization (SimPO) to refine the ranking between correct overconfidence and appropriate uncertainty. The approach uses third-person query framing to reduce sycophantic overconfidence and focuses perturbations on objects mentioned in queries to simulate visual uncertainty.

## Key Results
- Accuracy improves from 0.25 to 0.67 on average across benchmarks
- F1 score increases from 0.21 to 0.68 on average
- Expected Calibration Error decreases from 0.5699 to 0.4225 on average
- Calibration improvements achieved without sacrificing overall task performance

## Why This Works (Mechanism)

### Mechanism 1: Localized Semantic Perturbation Creates Explicit Uncertainty-to-Confidence Mapping
Applying noise selectively to query-relevant object regions enables VLMs to learn a grounded mapping between visual degradation and appropriate confidence levels. GroundingDINO localizes objects mentioned in queries, SAM generates precise masks, and diffusion-style Gaussian noise is applied proportionally to the inverse of target confidence (c=100% → no noise; c=0% → maximal noise). This creates training pairs where visual ambiguity directly corresponds to expected verbalized confidence. Core assumption: Models can transfer learned uncertainty-to-confidence mappings from synthetic perturbations to naturally occurring visual ambiguity at inference time.

### Mechanism 2: Two-Stage Training Separates Learning from Alignment
SFT establishes baseline confidence-expression capability; preference optimization refines the ranking between correct overconfidence and appropriate uncertainty. Stage 1 (SFT) teaches the model to verbalize confidence scores matching perturbation levels via cross-entropy loss. Stage 2 (SimPO) uses rejected response r_rej = 100% − c paired with winning response r_c to reinforce that lower confidence should accompany higher visual uncertainty. Core assumption: The rejected response formulation creates meaningful preference pairs that teach relative confidence ranking.

### Mechanism 3: Third-Person Perspective Framing Reduces Sycophantic Overconfidence
Framing confidence queries as external assessments of another model's answer reduces inflated self-assessment bias. Queries use format "A Language model was asked: {Query}, Model's answer was {Response}. How certain are you about model's answer?" rather than first-person introspection, which produces inflated confidence. Core assumption: VLMs exhibit less sycophantic behavior when evaluating external agents versus self-evaluating.

## Foundational Learning

- **Expected Calibration Error (ECE)**: Primary metric for evaluating whether verbalized confidence matches empirical correctness. Lower ECE indicates better calibration. Quick check: If a model assigns 80% confidence to 100 predictions and only 60 are correct, what is the calibration error for that bin? (Answer: |0.8 − 0.6| = 0.2)

- **Preference Optimization (SimPO/DPO family)**: Second training stage uses SimPO to refine confidence ranking. Understanding margin-based preference loss is essential for debugging training. Quick check: In preference optimization, what happens if the winning and losing responses are too similar? (Answer: Gradient signal weakens; margin parameter λ helps maintain separation)

- **Visual Grounding and Segmentation Pipelines**: CSP requires extracting precise object masks before perturbation. GroundingDINO provides boxes; SAM refines to pixel-level masks. Quick check: If GroundingDINO fails to localize an object mentioned in the query, what happens to that training sample? (Answer: Cannot generate meaningful mask → sample may be excluded or receive global noise, reducing calibration quality)

## Architecture Onboarding

- **Component map**: Input (Image + Query) → GroundingDINO (bounding boxes) → SAM (segmentation masks) → Gaussian Noise Injection (diffusion-style) → Perturbed Image + Confidence Query → SFT Stage (cross-entropy on confidence labels) → SimPO Stage (preference pairs: c vs 100%-c) → Calibrated VLM

- **Critical path**: Key object extraction quality determines perturbation relevance. If grounding fails, noise is misapplied. Noise schedule (parameter γ, Tmax) controls the confidence-to-degradation mapping. Misconfiguration creates label noise. SFT must converge before SimPO; ablation shows SimPO-alone yields ~0 improvement.

- **Design tradeoffs**: Noise intensity (γ): Higher γ = faster degradation per step, fewer diffusion steps needed, but coarser control over intermediate confidence levels. Third-person vs first-person framing: Third-person reduces sycophancy but may not match deployment context. Dataset source (RLAIF augmentation): Convenient but inherits any biases in base dataset.

- **Failure signatures**: High ECE with good accuracy → model is correct but miscalibrated; check if perturbation levels are too extreme or too similar. SFT improves but SimPO degrades → preference pairs may have reversed ordering or margin λ is too aggressive. Calibration improves on POPE but not AMBER relation → object-level perturbation may not address relational uncertainty.

- **First 3 experiments**: Sanity check perturbation visibility: Manually inspect 10-20 perturbed images across confidence levels (100%, 70%, 40%, 10%). Ensure object degradation is visible but background remains intact. If perturbation is too subtle, model cannot learn the mapping. Ablate noise locality: Compare full CSP against global-noise variant on a held-out subset. Expect significant delta per Figure 4; if delta is small, grounding/segmentation may be failing silently. Probe internal-external alignment: Before/after CSP, compute Spearman correlation between token-level probabilities and verbalized confidence. CSP should improve this correlation indirectly, providing a sanity check that calibration is not purely superficial.

## Open Questions the Paper Calls Out

None

## Limitations

- **Transferability of Perturbation Mappings**: The paper assumes Gaussian noise applied to localized object regions will generalize to natural visual uncertainty, but does not validate transfer to other types of visual degradation like motion blur or occlusion.

- **Third-Person Framing Generalization**: Calibration gains rely on third-person query framing, but many deployment scenarios require first-person confidence expressions, creating a significant deployment gap.

- **Dataset Dependency and Bias**: CSP uses RLAIF-augmented data from the POPE dataset, creating circular dependency where evaluation dataset influences training data without validation on alternative sources.

## Confidence

- Transferability of Perturbation Mappings: Medium
- Third-Person Framing Generalization: Low
- Dataset Dependency and Bias: Medium

## Next Checks

1. **Perturbation Transfer Test**: Apply CSP-trained models to images with naturally occurring uncertainty (low-light, motion blur, occlusion) and measure if calibration gains persist.

2. **First-Person Framing Evaluation**: Re-run POPE and AMBER benchmarks using first-person confidence queries to verify if third-person framing is essential to the calibration gains.

3. **Alternative Data Source Validation**: Train CSP using a different object detection dataset and evaluate on POPE/AMBER to confirm that calibration improvements are not artifacts of dataset-specific patterns.