---
ver: rpa2
title: 'Knowing When Not to Answer: Lightweight KB-Aligned OOD Detection for Safe
  RAG'
arxiv_id: '2508.02296'
source_url: https://arxiv.org/abs/2508.02296
tags:
- covid-19
- question
- queries
- knowledge
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies lightweight, KB-aligned out-of-domain (OOD)
  detection as an always-on gate for RAG systems. The approach applies PCA to KB embeddings
  and scores queries in a compact subspace selected by explained-variance retention
  or separability-driven t-test ranking.
---

# Knowing When Not to Answer: Lightweight KB-Aligned OOD Detection for Safe RAG

## Quick Facts
- arXiv ID: 2508.02296
- Source URL: https://arxiv.org/abs/2508.02296
- Reference count: 40
- Primary result: PCA-based KB-aligned OOD detector uses 15x fewer dimensions than baselines while maintaining accuracy

## Executive Summary
This paper addresses the critical need for lightweight out-of-domain (OOD) detection in retrieval-augmented generation (RAG) systems. The authors propose a KB-aligned approach that applies PCA to knowledge base embeddings and projects queries into a compact subspace selected by explained-variance retention or separability-driven t-test ranking. Evaluated across 16 domains including COVID-19 and Substance Use KBs, the method achieves competitive OOD detection accuracy while using two orders of magnitude fewer dimensions than baselines. The approach is designed as an always-on gate to prevent RAG systems from generating irrelevant or potentially harmful responses to out-of-scope queries.

## Method Summary
The method operates in two phases: offline PCA computation on KB document embeddings (retaining top-k principal components), followed by online query projection into a selected subspace (using either explained-variance retention or p-value ranking from t-tests). Queries are classified using geometric methods (ε-ball, ε-cube, ε-rect) or lightweight ML classifiers (LogReg, SVM, GMM). The detector is applied before retrieval to gate OOD queries, with downstream RAG using top-10 retrieval and top-3 re-ranking before GPT-4o generation.

## Key Results
- OOD queries primarily degrade RAG response relevance (not correctness), showing the need for efficient external OOD detection
- Method achieves competitive OOD detection accuracy using two orders of magnitude fewer dimensions than baselines
- Sub-millisecond latency, near-zero cost, and interpretability compared to prompted LLM-based detectors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Projecting queries into a KB-specific PCA subspace isolates domain-relevant signals while filtering noise.
- Mechanism: PCA identifies dominant variance directions within KB document embeddings. Projecting queries into this space attenuates out-of-domain concepts that are orthogonal to the KB's primary variance, allowing for compact separation.
- Core assumption: In-domain queries share geometric structure with KB documents, whereas OOD queries do not.
- Evidence anchors: PCA-based approach described in abstract and section 3.1; corpus neighbors confirm difficulty of "knowing when not to answer."
- Break condition: If KB is sufficiently broad or multi-modal, geometric boundaries become indistinct.

### Mechanism 2
- Claim: p-value selection improves detection for geometric rules compared to EVR.
- Mechanism: Standard PCA (EVR) prioritizes data spread; p-value method selects dimensions where ID/OOD projections are statistically most distinct, tailoring the subspace to the boundary between safe and unsafe inputs.
- Core assumption: Training OOD distribution is representative enough to identify discriminative axes that generalize.
- Evidence anchors: Abstract mentions p-value ranking; section 5.1 shows p-value ranking emphasizes ID-OOD separability.
- Break condition: Novel attack vectors aligned with ID variance but absent from OOD training set may not be separated.

### Mechanism 3
- Claim: Safety failure of RAG systems for OOD queries is primarily relevance degradation, not correctness.
- Mechanism: Dense retrievers surface weakly related context for OOD queries. LLMs synthesize these weak clues into plausible-sounding but irrelevant answers, creating "confident but irrelevant" safety risks.
- Core assumption: Correctness and relevance are orthogonal dimensions in RAG evaluation.
- Evidence anchors: Abstract states OOD queries degrade relevance; section 5.3 shows significant relevance reduction but no correctness difference.
- Break condition: If retrieval surfaces factually incorrect context, correctness score would also degrade.

## Foundational Learning

### Concept: Principal Component Analysis (PCA)
- Why needed: Entire architecture relies on reducing high-dimensional embeddings (768d) into compact subspace (m < 15)
- Quick check: If a PC captures 5% of variance but perfectly separates ID from OOD, would EVR retain it?

### Concept: Dense Retrieval (Bi-Encoders)
- Why needed: OOD detector operates on same embedding space used for retrieval
- Quick check: Why is OOD detector applied before retrieval step in this architecture?

### Concept: OOD Detection vs. Anomaly Detection
- Why needed: Paper frames this as "KB-alignment" rather than generic anomaly detection
- Quick check: How does labeled ID/OOD training data change choice between geometric rules and supervised classifiers?

## Architecture Onboarding

- Component map: KB Documents → Embedder → PCA (Top-k) → Component Selector (EVR/p-value) → Projector → Detector → [Gate] → Retriever → Generator
- Critical path: Component Selector. Wrong PC selection collapses projection and geometric boundary fails.
- Design tradeoffs:
  - EVR: cheaper, no OOD data, selects for data spread
  - p-value: requires OOD data, tighter boundaries for geometric rules
  - LogReg/SVM: higher accuracy, less interpretable than geometric methods
- Failure signatures:
  - False Positives: ID queries flagged as OOD (radius too small or PCs miss ID variance)
  - Fluent Hallucinations: Gate fails → weak context → LLM generates relevant-sounding nonsense
- First 3 experiments:
  1. Baseline Dimensionality: Fix ε-ball + EVR, sweep m to find accuracy saturation elbow
  2. Ablation on Selection: Compare EVR vs p-value on validation set, measure m reduction
  3. Attack Simulation: Test classifier generalization from clean OOD to LLM-generated attacks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can KB-aligned PCA subspace effectively generalize to multilingual retrieval settings?
- Basis: Authors only experimented on English subset despite COVID-19 dataset having Spanish portion
- Why unresolved: Unclear if single compact PCA subspace can capture in-domain concepts across language boundaries
- Evidence needed: Evaluation on multilingual benchmarks with cross-lingual embeddings

### Open Question 2
- Question: Can p-value criterion detect "Axis B" policy violations or intent risks, not just "Axis A" answerability?
- Basis: Paper explicitly scopes work to Axis A (answerability) vs Axis B (policy/filters)
- Why unresolved: Unknown if intent risks form geometric clusters in PCA subspace same way topic relevance does
- Evidence needed: Training p-value ranker on ID-but-policy-violating queries to test separability

## Limitations
- Performance may degrade for highly heterogeneous or multi-modal knowledge bases where variance is distributed across many dimensions
- Geometric rules are sensitive to radius tuning and may struggle with non-spherical decision boundaries
- p-value selection requires labeled OOD training data that may not be available in all deployment scenarios

## Confidence
- **High Confidence**: OOD queries primarily degrade relevance rather than correctness (well-supported by human/LLM evaluation)
- **Medium Confidence**: Dimensionality reduction efficacy (15x fewer dimensions) demonstrated but may vary with different embedding models/KB domains
- **Medium Confidence**: Latency claims (sub-millisecond) are technically sound but depend on implementation/hardware details

## Next Checks
1. Test method on highly diverse KB (e.g., Wikipedia or multi-domain medical literature) to assess performance degradation as KB becomes less compact
2. Conduct ablation study removing p-value selection criterion to quantify trade-off between requiring OOD training data and achieving geometric rule performance
3. Evaluate method's robustness to embedding model drift by fine-tuning underlying model on domain-specific data and measuring OOD detection accuracy degradation