---
ver: rpa2
title: 'Variance-Adaptive Muon: Accelerating LLM Pretraining with NSR-Modulated and
  Variance-Scaled Momentum'
arxiv_id: '2601.14603'
source_url: https://arxiv.org/abs/2601.14603
tags:
- uni00000013
- uni00000011
- uni00000051
- uni00000052
- uni00000016
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Variance-Adaptive Muon: Accelerating LLM Pretraining with NSR-Modulated and Variance-Scaled Momentum

## Quick Facts
- arXiv ID: 2601.14603
- Source URL: https://arxiv.org/abs/2601.14603
- Authors: Jingru Li; Yibo Fan; Huan Li
- Reference count: 24
- Primary result: 1.36× faster convergence on LLaMA-1.2B pretraining

## Executive Summary
Muon-NSR introduces variance-adaptive normalization before orthogonalization to accelerate LLM pretraining. By computing element-wise noise-to-signal ratios from gradient statistics and normalizing momentum matrices before Newton-Schulz orthogonalization, the method suppresses noisy gradient directions while preserving spectral structure. The approach achieves 1.36× speedup on LLaMA-1.2B and maintains consistent gains across architectures from GPT-2 Small to LLaMA-1.2B.

## Method Summary
Muon-NSR extends the Muon optimizer by adding variance-adaptive normalization through noise-to-signal ratio (NSR) modulation. The algorithm computes momentum M_t and variance surrogate Γ_t using shared EMA decay rates, applies Nesterov extrapolation M̃_t, then normalizes using √(M̃_t² + γΓ_t) before Newton-Schulz orthogonalization. The variance surrogate Γ_t = βΓ_{t-1} + β(1-β)(M_{t-1} - G_t)² estimates gradient variance without requiring separate EMA parameters. Pre-orthogonalization normalization preserves spectral structure better than post-hoc rescaling, and shared decay rates eliminate hyperparameter complexity.

## Key Results
- 1.36× faster convergence on LLaMA-1.2B pretraining compared to baseline Muon
- Consistent performance improvement across model scales (GPT-2 Small to LLaMA-1.2B)
- Optimal γ hyperparameter varies by regime: 10 for GPT-2, 1000 for LLaMA architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Variance-adaptive normalization before orthogonalization accelerates convergence by suppressing noisy gradient directions
- Mechanism: Muon-NSR computes element-wise noise-to-signal ratio (NSR) from gradient statistics, then normalizes momentum matrices before Newton-Schulz orthogonalization. This down-weights coordinates with high uncertainty relative to signal strength, producing cleaner orthogonal updates.
- Core assumption: Gradient variance captures meaningful noise structure that, when suppressed, improves optimization geometry
- Evidence anchors: [abstract] "Muon-NSR applies noise-to-signal ratio (NSR) modulation... experiments demonstrate that our proposed methods accelerate convergence"; [section 3] Equation 9 shows Adam as "sign(mt) * sqrt(1 + σ²_t/m²_t)" where σ/|m| is NSR; [corpus] AdaMuon (arxiv 2507.11005) achieves similar goals through element-wise second momentum on orthogonalized updates

### Mechanism 2
- Claim: Pre-orthogonalization variance scaling preserves spectral structure better than post-hoc rescaling
- Mechanism: Applying variance-based normalization before Newton-Schulz ensures the orthogonalization operates on already-modulated input. Post-hoc rescaling (Muon-NSR-Reshuffled) perturbs orthogonality after it's computed.
- Core assumption: The orthogonal polar factor computation is sensitive to input scaling; normalizing input produces fundamentally different orthogonal matrices than rescaling output
- Evidence anchors: [section 5.3] "Muon-NSR (Pre-Ortho) maintains a superior validation loss trajectory... Muon-NSR-Reshuffled exhibits distinct phase-dependent behavior: it initially underperforms"; [section 4] "applying this normalization prior to the Newton–Schulz orthogonalization ensures that the intended spectral structure of the update remains uncorrupted"

### Mechanism 3
- Claim: Shared EMA decay rate enables variance estimation without separate hyperparameters
- Mechanism: Using β₁=β₂=β allows reformulating Adam's second moment as v_t - m_t², which equals gradient variance estimate σ²_t. This avoids introducing new decay rates for variance tracking.
- Core assumption: Equal decay rates for mean and variance estimation is sufficient for LLM optimization (supported by Orvieto & Gower 2025)
- Evidence anchors: [section 3] "Orvieto and Gower [2025] empirically observed that employing a shared EMA decay rate... maintains competitive performance"; [section 4] "Γ_t captures dispersion around the evolving mean rather than accumulating second moments in Adam"

## Foundational Learning

- Concept: **Matrix sign operation via orthogonal polar factor**
  - Why needed here: Muon's core innovation is treating momentum matrices spectrally; msign(M) = UV^⊤ from SVD M=UΣV^⊤ flattens singular values to 1
  - Quick check question: Given a 2×2 matrix with singular values [3, 0.5], what are the singular values after msign()?

- Concept: **Newton-Schulz iteration for polar decomposition**
  - Why needed here: Exact SVD is O(m²n); NS iterations approximate orthogonal factor efficiently. Muon uses K=5 iterations typically.
  - Quick check question: Why is Newton-Schulz preferred over QR decomposition for this application?

- Concept: **Noise-to-signal ratio in stochastic optimization**
  - Why needed here: NSR = σ/|μ| measures gradient uncertainty; high NSR indicates noise-dominated directions where steps should be smaller
  - Quick check question: In Adam with β₁=β₂, what does v_t - m²_t estimate?

## Architecture Onboarding

- Component map:
  ```
  Gradient G_t
      ↓
  Momentum EMA: M_t = βM_{t-1} + (1-β)G_t
      ↓
  Variance EMA: Γ_t = βΓ_{t-1} + β(1-β)(M_{t-1} - G_t)²
      ↓
  Nesterov lookahead: M̃_t = G_t + β/(1-β) · M_t
      ↓
  Variance normalization: M̃_t / sqrt(M̃²_t + γΓ_t)  [Muon-NSR]
      ↓
  Newton-Schulz (K=5): O_t ≈ polar(M_normalized)
      ↓
  Weight update: W_t = W_{t-1}(1-ηλ) - η·s_scale·O_t
  ```

- Critical path: The variance normalization before NS iteration is the key insertion point; incorrect ordering breaks spectral structure

- Design tradeoffs:
  - Muon-NSR vs Muon-VS: NSR requires tuning γ (sweep 0.1–10000; optimal varies by model); VS is parameter-free but may over-suppress
  - Memory: +1 buffer for Γ_t (same as Adam's v_t)
  - Compute: O(mn) for variance ops, dominated by O(m²n) NS iterations

- Failure signatures:
  - Early-training instability with small γ: insufficient variance damping
  - Convergence stalling with very large γ: over-damping, effectively zero gradients
  - Performance degradation on small batches (Section 5.4): insufficient samples for stable variance estimates

- First 3 experiments:
  1. **Baseline reproduction**: Run standard Muon vs Muon-NSR (γ=10) on GPT-2 Small (124M) for 10K steps; expect ~0.005 validation loss gap
  2. **γ sensitivity sweep**: Test γ∈{1,10,100,1000} on LLaMA-130M; expect unimodal curve with optimal around 100–1000
  3. **Ordering ablation**: Compare pre-orthogonalization (Muon-NSR) vs post-orthogonalization (Reshuffled); expect pre-ortho to lead early and maintain advantage

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the variance-sensitivity coefficient $\gamma$ in Muon-NSR be theoretically derived or adaptively set to eliminate the need for regime-specific tuning?
- **Basis in paper:** [explicit] The ablation study in Section 5.3 notes that the optimal $\gamma$ "appears to be regime-dependent," varying significantly (10 for GPT-2 vs. 1000 for LLaMA-130M).
- **Why unresolved:** The current methodology requires a manual sweep to identify optimal values, which is computationally expensive and lacks a theoretical grounding for why the optimal value shifts by two orders of magnitude between architectures.
- **What evidence would resolve it:** A derivation of $\gamma$ based on model scale or gradient statistics that matches performance of the hand-tuned baselines without search.

### Open Question 2
- **Question:** Can the variance-adaptive mechanism be modified to maintain stability and superiority over baselines in small-batch training regimes?
- **Basis in paper:** [explicit] Section 5.4 ("Regime sensitivity and limitations") states the method performed worse than the baseline on LLaMA-520M due to batch size constraints, noting effectiveness "depends on large batch sizes."
- **Why unresolved:** The current implementation relies on stable gradient statistics which degrade with smaller batches, creating a performance gap that contradicts the method's goals in resource-constrained settings.
- **What evidence would resolve it:** An adaptation of Muon-NSR/VS that consistently outperforms the standard Muon baseline on the LLaMA-520M benchmark using the smaller batch sizes defined in the reference protocol.

### Open Question 3
- **Question:** Does the observed 1.36x acceleration and improvement ratio scale reliably to multi-billion parameter frontier models?
- **Basis in paper:** [inferred] The paper validates results up to 1.2B parameters; however, the introduction emphasizes that pretraining costs span "thousands of GPU-hours," implying the ultimate utility depends on performance at much larger scales (e.g., 7B, 70B).
- **Why unresolved:** While scaling trends are positive within the paper's range, the interaction between orthogonalization and variance scaling may shift as the loss landscape geometry changes drastically at frontier scales.
- **What evidence would resolve it:** Pretraining curves for LLaMA-7B or larger models demonstrating that Muon-NSR/VS maintains the "Improvement Ratio" over standard Muon seen in the 210M–1.2B experiments.

## Limitations

- Performance degrades on small batch sizes due to unstable gradient statistics
- γ hyperparameter requires regime-specific tuning through manual sweeps
- Theoretical justification for NSR modulation's convergence benefits remains incomplete

## Confidence

**High confidence** (empirical evidence with consistent patterns):
- NSR modulation provides convergence benefits across multiple model scales (GPT-2 Small to LLaMA-1.2B)
- Muon-NSR consistently outperforms both AdamW and baseline Muon on validation loss trajectories
- The variance surrogate formula Γ_t = βΓ_{t-1} + β(1-β)(M_{t-1} - G_t)² is correctly implemented when properly applied

**Medium confidence** (empirical but with implementation dependencies):
- Pre-orthogonalization ordering provides measurable advantage (Section 5.3 ablation)
- Shared EMA decay rate (β₁=β₂) is sufficient for variance tracking
- γ hyperparameter sensitivity shows unimodal behavior but optimal values vary by model

**Low confidence** (theoretical claims without rigorous support):
- NSR modulation fundamentally improves optimization geometry by suppressing noisy directions
- The specific interaction between variance normalization and Newton-Schulz iteration has optimal ordering
- Equal decay rates for mean and variance estimation are theoretically justified for LLM pretraining

## Next Checks

1. **Gradient variance distribution analysis**: Profile gradient statistics during training to verify that NSR modulation actually identifies and suppresses high-noise directions. Compare variance-to-signal ratios across layers and training phases.

2. **Orthogonalization sensitivity study**: Systematically test NSR application at different stages (before NS, after NS, both) with fine-grained γ sweeps. Measure orthogonality preservation and convergence impact quantitatively.

3. **Ablation of shared decay rate**: Compare β₁≠β₂ configurations against the shared decay baseline. Measure whether separate variance tracking timescales provide additional benefits in later training phases or with smaller batch sizes.