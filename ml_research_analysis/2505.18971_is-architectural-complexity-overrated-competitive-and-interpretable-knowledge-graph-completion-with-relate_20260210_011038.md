---
ver: rpa2
title: Is Architectural Complexity Overrated? Competitive and Interpretable Knowledge
  Graph Completion with RelatE
arxiv_id: '2505.18971'
source_url: https://arxiv.org/abs/2505.18971
tags:
- relate
- phase
- modulus
- knowledge
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RELATE is a real-valued, interpretable model for knowledge graph
  completion that leverages phase-modulus decomposition to capture relational patterns
  such as symmetry, inversion, and composition. By integrating dual-role entity and
  relation embeddings with sinusoidal phase alignment and lightweight type constraints,
  RELATE achieves competitive performance on standard benchmarks like YAGO3-10 (MRR
  0.521, Hit@10 0.680) while reducing training time by 24%, inference latency by 31%,
  and GPU memory usage by 22% compared to RotatE.
---

# Is Architectural Complexity Overrated? Competitive and Interpretable Knowledge Graph Completion with RelatE

## Quick Facts
- **arXiv ID:** 2505.18971
- **Source URL:** https://arxiv.org/abs/2505.18971
- **Reference count:** 40
- **Primary result:** RELATE achieves MRR 0.521, Hit@10 0.680 on YAGO3-10 while reducing training time by 24%, inference latency by 31%, and GPU memory usage by 22% compared to RotatE

## Executive Summary
RELATE presents a real-valued knowledge graph embedding model that challenges the assumption that complex architectures are necessary for strong performance. By decomposing entities and relations into interpretable phase and modulus components, RELATE achieves competitive link prediction results while being significantly more efficient and interpretable than complex-valued alternatives. The model's key innovation is using sinusoidal phase alignment to capture relational patterns like symmetry and inversion, combined with slope-weighted modulus scoring for hierarchical structures. Despite its architectural simplicity, RELATE demonstrates robustness to structural noise and provides transparent entity-relationship representations.

## Method Summary
RELATE implements knowledge graph completion through a hybrid phase-modulus scoring function. Entities and relations are represented as real-valued vectors split into phase (directional) and modulus (magnitude) components of dimension d/2 each. The phase score uses sinusoidal alignment to capture cyclical patterns, while the modulus score employs slope-weighted L1 distance with learnable type biases. The model is trained using margin-based ranking loss with self-adversarial negative sampling, optimized via Adam with L3 regularization. Training runs up to 200K steps with dataset-specific hyperparameters, and evaluation uses the filtered ranking protocol on standard benchmarks.

## Key Results
- Achieves MRR 0.521, Hit@10 0.680 on YAGO3-10 benchmark
- Reduces training time by 24%, inference latency by 31%, and GPU memory usage by 22% compared to RotatE
- Demonstrates 61% less MRR degradation under structural edits compared to TransE
- Achieves 62% more interpretable relations compared to RotatE through expert evaluation

## Why This Works (Mechanism)

### Mechanism 1: Dual-Space Phase-Modulus Decomposition
RELATE separates entity and relation representations into distinct phase (directional) and modulus (magnitude) vectors, allowing independent learning of relational logic. This disentanglement prevents gradient interference between different relational features, with phase capturing cyclical patterns and modulus handling hierarchical strength.

### Mechanism 2: Sinusoidal Phase Alignment for Logical Patterns
The scoring function uses $|\sin(\frac{\phi_h + \phi_r - \phi_t}{2})|$ to enable strict satisfaction of logical rules like symmetry, inversion, and composition without complex rotation algebra. This angular periodicity naturally handles symmetric relations where $r \approx -r$.

### Mechanism 3: Slope-Weighted Modulus Scoring with Type Bias
A slope-weighted L1 distance in the modulus space, augmented with learnable type biases, creates soft regions or cones in the embedding space rather than strict translations. This improves robustness to structural noise and better handles one-to-many relations.

## Foundational Learning

- **Concept: Knowledge Graph Completion (KGC) / Link Prediction**
  - Why needed here: Core task is ranking plausible triples $(h, r, t)$ against corruptions
  - Quick check question: Can you explain why Mean Reciprocal Rank (MRR) is preferred over accuracy for this task?

- **Concept: Translational vs. Semantic Matching**
  - Why needed here: RELATE is a hybrid using translation logic in modulus/phase space but scoring like semantic matching
  - Quick check question: How does TransE ($h+r \approx t$) fundamentally fail to model symmetric relations, and how does RELATE's phase component fix this?

- **Concept: Self-Adversarial Negative Sampling**
  - Why needed here: Key to performance, sampling difficult negatives based on current model belief
  - Quick check question: Why would sampling random entities as negatives be insufficient for learning fine-grained relational logic?

## Architecture Onboarding

- **Component map:** Head Entity ($e_h$) -> Modulus/Phase Decomposition -> Phase Score (Sinusoidal) -> Modulus Score (Slope-Weighted L1) -> Aggregation -> Final Score

- **Critical path:** 1) Decompose inputs into modulus and phase components 2) Compute angular coherence using $\sin((h_p + r_p - t_p)/2)$ 3) Compute scalar alignment using $|h_m \circ (r_m + b) - t_m \circ (1-b)|$ weighted by $w_r$ 4) Combine scores via learned weights $\lambda$ and subtract from margin $\gamma$

- **Design tradeoffs:** Uses real vectors for speed/interpretability (31% lower latency) but may theoretically lose some rotational nuance compared to complex models like RotatE, specifically noted in WN18RR results

- **Failure signatures:** Low MRR on symmetry-heavy datasets if phase components collapse; over-regularization if type bias is too strong causing entity clustering by type rather than relational structure

- **First 3 experiments:**
  1. Train RELATE on FB15k-237 subset and verify $h_p + r_p \approx t_p$ for symmetric relations (Phase alignment) and $h_m + r_m \approx t_m$ for hierarchical relations (Modulus translation)
  2. Remove phase component (set $\lambda^{(p)}=0$) and measure Hit@10 drop on symmetric vs. asymmetric relation subsets to validate Mechanism 2
  3. Compare inference time against RotatE on fixed batch size to verify claimed 31% latency reduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can RELATE effectively generalize to unseen entities and relations in dynamic, open-world settings using parameterized query embeddings?
- Basis in paper: [explicit] The Conclusion states that "preliminary work explores zero-shot extensions of RELATE... aimed at generalizing to unseen entities and relations in dynamic or open-world settings."
- Why unresolved: Current evaluation focuses exclusively on standard transductive benchmarks with fixed entity sets
- What evidence would resolve it: Successful evaluation on inductive or zero-shot knowledge graph completion benchmarks

### Open Question 2
- Question: How can RELATE be adapted for ontology-scarce or inductive settings where explicit type heuristics are unavailable?
- Basis in paper: [explicit] The Limitations section notes that the model's "reliance on dataset-specific type heuristics limits use in ontology-scarce or inductive settings."
- Why unresolved: Type-informed constraints improve performance but depend on type data availability
- What evidence would resolve it: Demonstrating adaptation that maintains competitive performance on datasets lacking type annotations

### Open Question 3
- Question: To what extent can RELATE's real-valued structure be integrated into Large Language Model (LLM) workflows for retrieval-augmented generation?
- Basis in paper: [explicit] The Broader Impact section identifies "integrating RELATE into large language model (LLM) workflows" as a "promising future direction"
- Why unresolved: Paper focuses on standalone KGC tasks without evaluating utility within generative pipelines
- What evidence would resolve it: Empirical results from hybrid system where RELATE retrieves structured facts to ground LLM outputs

## Limitations
- Reliance on dataset-specific type heuristics limits use in ontology-scarce or inductive settings
- Real-valued phase approximation may underperform complex-valued rotations on certain symmetric relation patterns
- Interpretability claims based on subjective expert evaluation without detailed methodology

## Confidence

**Major Uncertainties:**
Primary uncertainty is the actual impact of phase-modulus decomposition on interpretability versus performance, with claims needing careful validation especially on WN18RR where real-valued approximation may underperform.

**Confidence Assessment:**
- **High Confidence:** Core architectural claims (phase-modulus decomposition, scoring function formulation) are well-specified and reproducible
- **Medium Confidence:** Efficiency claims supported by ablation study but lack absolute baseline comparisons
- **Low Confidence:** Interpretability claims (62% more interpretable relations) rely on subjective expert evaluation

## Next Checks

1. **Symmetry Pattern Validation:** Extract all symmetric relations from FB15k-237 and verify that phase components align perfectly (h_p + r_p â‰ˆ t_p) while modulus components show no correlation, confirming Mechanism 2's effectiveness.

2. **Robustness Benchmark:** Replicate the perturbation study by systematically corrupting entity types and measuring MRR degradation across RELATE, TransE, and RotatE to validate the 61% improvement claim.

3. **Interpretability Audit:** Implement the relation labeling procedure and have independent experts label 50 relations from both RELATE and RotatE embeddings to verify the 62% interpretability advantage.