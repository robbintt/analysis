---
ver: rpa2
title: 'Injection, Attack and Erasure: Revocable Backdoor Attacks via Machine Unlearning'
arxiv_id: '2510.13322'
source_url: https://arxiv.org/abs/2510.13322
tags:
- backdoor
- uni00000013
- unlearning
- attack
- trigger
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first paradigm of revocable backdoor
  attacks, where backdoors can be proactively and thoroughly removed via machine unlearning
  after achieving attack objectives. The key innovation is formulating trigger optimization
  as a bilevel problem that balances high attack success rate with effective backdoor
  revocation.
---

# Injection, Attack and Erasure: Revocable Backdoor Attacks via Machine Unlearning

## Quick Facts
- arXiv ID: 2510.13322
- Source URL: https://arxiv.org/abs/2510.13322
- Reference count: 40
- This paper introduces the first paradigm of revocable backdoor attacks, where backdoors can be proactively and thoroughly removed via machine unlearning after achieving attack objectives.

## Executive Summary
This paper presents a novel approach to backdoor attacks that can be deliberately revoked through machine unlearning. The key innovation is formulating trigger optimization as a bilevel problem that balances high attack success rate with effective backdoor revocation. The authors employ deterministic sample partitioning and Projected Conflicting Gradient (PCGrad) techniques to resolve optimization conflicts between injection and removal objectives. Experiments on CIFAR-10 and ImageNet demonstrate that the proposed method achieves attack success rates comparable to state-of-the-art backdoor attacks while enabling significant backdoor weakening or removal after unlearning.

## Method Summary
The proposed method uses a U-Net-based trigger generator that creates perturbations processed through DCT-based frequency masking and Gaussian blur. The optimization is formulated as a bilevel problem: the inner loop trains a backdoored model and simulates unlearning, while the outer loop updates the trigger generator based on conflicting attack and unlearning objectives. PCGrad is applied to resolve gradient conflicts between these objectives. The method is evaluated on CIFAR-10 and ImageNet-10 with 5% poisoning rate, using First-order and UnrollSGD unlearning methods.

## Key Results
- Achieves attack success rates comparable to state-of-the-art backdoor attacks
- Reduces ASR-U to as low as 0.11% on ImageNet-10 under UnrollSGD unlearning
- Successfully revokes backdoors while maintaining benign accuracy on clean data
- Outperforms traditional attacks in both stealth and revocability

## Why This Works (Mechanism)

### Mechanism 1: Bilevel Optimization for Joint Injection and Revocation
The trigger generator is optimized to maximize attack success rate while minimizing ASR after unlearning, creating a backdoor that is effective during attacks but fragile under unlearning. The inner loop simulates victim environment with training and unlearning, while the outer loop updates the generator based on both attack and unlearning losses.

### Mechanism 2: Gradient Conflict Mitigation via PCGrad
The attack loss and unlearning loss have conflicting gradients. PCGrad resolves this by projecting the unlearning gradient onto the orthogonal plane of the attack gradient when their inner product is negative, stabilizing training and balancing the trade-off.

### Mechanism 3: Frequency-Constrained Trigger Stealth
Triggers are constrained to low-frequency components via DCT, making them imperceptible to human inspection and robust against spatial defenses while retaining sufficient signal for the model. This prevents high-frequency artifacts that typically trigger detection mechanisms.

## Foundational Learning

- **Concept: Bilevel Optimization / Meta-Learning**
  - Why needed here: The trigger generator must "learn to learn" by updating based on the outcome of downstream processes (training and unlearning) rather than just static datasets.
  - Quick check question: Can you explain how the gradient flows from the unlearned model back to the generator in this architecture?

- **Concept: Machine Unlearning (First-order & UnrollSGD)**
  - Why needed here: The attack relies on specific unlearning behaviors (fine-tuning on clean data). Understanding how unlearning shifts model weights is essential to designing a trigger that "breaks" under this shift.
  - Quick check question: How does First-order unlearning differ from simply retraining from scratch, and why does UnrollSGD provide a more robust approximation for this attack?

- **Concept: Gradient Surgery (PCGrad)**
  - Why needed here: Standard gradient descent fails when objectives conflict (negative gradient similarity). Understanding how to project gradients is critical for debugging training stability.
  - Quick check question: If two loss functions have a cosine similarity of -0.9, what happens to the effective update direction if you simply sum the gradients vs. using PCGrad?

## Architecture Onboarding

- **Component map:**
  - Trigger Generator (U-Net) -> DCT Frequency Processor -> Surrogate Environment (Backdoored Model + Unlearned Model) -> Optimizer with PCGrad

- **Critical path:**
  1. Partition dataset into Clean, Poisoned, and Unlearning sets deterministically
  2. Inner Loop: Train backdoored model on poisoned data; derive unlearned model via unlearning
  3. Loss Calculation: Compute attack loss and unlearning loss
  4. Gradient Processing: Calculate grads, check cosine similarity, apply PCGrad projection if conflict exists
  5. Outer Update: Step the Generator

- **Design tradeoffs:**
  - ASR vs. Revocability: High attack success correlates with deep embedding (harder to unlearn). λ_unlearn controls this balance.
  - Trigger Strength (η): Higher η saturates ASR but significantly degrades revocability (ASR-U increases).
  - Sampling: Deterministic partitioning stabilizes conflict but may reduce trigger diversity.

- **Failure signatures:**
  - High ASR / High ASR-U: Trigger too robust; unlearning loss being ignored. Check PCGrad implementation or λ_unlearn.
  - Low ASR / Low ASR-U: Trigger too weak or stealthy. Check trigger strength η or generator capacity.
  - Divergence: Gradient conflicts too severe or learning rate too high. Check PCGrad implementation.

- **First 3 experiments:**
  1. Baseline Conflict: Run optimization without PCGrad to reproduce negative cosine similarity and observe training instability.
  2. Unlearning Sensitivity: Ablate unlearning loss to verify ASR-U remains high, proving loss is necessary for revocation.
  3. Hyperparameter Sweep: Vary poisoning rate and trigger strength to map trade-off frontier between attack effectiveness and erasure capability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do adaptive unlearning strategies impact the robustness of revocable backdoors?
- Basis in paper: [explicit] The conclusion states the authors will "explore... adaptive unlearning strategies" in future work.
- Why unresolved: Current experiments assume standard unlearning interface, ignoring potential defensive adaptations.
- What evidence would resolve it: Experiments evaluating ASR-U against unlearning algorithms modified to detect and retain backdoor correlations.

### Open Question 2
- Question: Can adversarial model auditing effectively detect revocable backdoors before or during the unlearning phase?
- Basis in paper: [explicit] The conclusion lists "adversarial model auditing" as a future research direction.
- Why unresolved: Paper demonstrates static detection is vulnerable to revocation but doesn't evaluate dynamic auditing methods.
- What evidence would resolve it: Development and testing of auditing frameworks that monitor gradient updates or data requests during revocable trigger optimization.

### Open Question 3
- Question: What specific defense mechanisms can prevent revocable backdoors without disabling legitimate unlearning rights?
- Basis in paper: [explicit] The conclusion calls for "new defenses against revocable backdoor attacks" to address challenges posed by this paradigm.
- Why unresolved: Paper shows current static defenses are insufficient but doesn't propose solutions balancing security with unlearning utility.
- What evidence would resolve it: A defense method that successfully identifies poisoned samples or blocks revocable triggers while maintaining model owner's ability to unlearn benign data.

## Limitations

- Effectiveness relies heavily on surrogate unlearning algorithm accurately approximating target system's unlearning behavior
- Comparison limited to CIFAR-10 and ImageNet-10 (10-class subset); generalizability to full-scale ImageNet unclear
- Frequency-constrained trigger stealth claims primarily supported by visualizations rather than systematic adversarial detection experiments

## Confidence

- **High Confidence**: Core methodology of using bilevel optimization with PCGrad is well-grounded and experimental results are reproducible
- **Medium Confidence**: Claim that ASR-U can be reduced to 0.11% on ImageNet-10 is supported by presented experiments but needs verification across different unlearning implementations
- **Low Confidence**: Stealth claims regarding frequency-constrained triggers are primarily supported by visualizations rather than systematic adversarial detection experiments

## Next Checks

1. **Unlearning Algorithm Sensitivity**: Test the proposed attack against multiple unlearning implementations (different forgetting rates, layer selections, and algorithms) to assess robustness of revocability claims
2. **Full Dataset Evaluation**: Extend experiments to full ImageNet and additional datasets (e.g., CIFAR-100) to evaluate scalability and generalizability
3. **Adversarial Detection Analysis**: Conduct systematic experiments using state-of-the-art backdoor detection methods to validate stealth claims of frequency-constrained triggers