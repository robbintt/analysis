---
ver: rpa2
title: 'Munsit at NADI 2025 Shared Task 2: Pushing the Boundaries of Multidialectal
  Arabic ASR with Weakly Supervised Pretraining and Continual Supervised Fine-tuning'
arxiv_id: '2508.08912'
source_url: https://arxiv.org/abs/2508.08912
tags:
- arabic
- speech
- weakly
- data
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of building accurate automatic
  speech recognition (ASR) systems for Arabic, a low-resource language complicated
  by dialectal diversity. The authors propose a scalable training pipeline combining
  weakly supervised pretraining with continual supervised fine-tuning.
---

# Munsit at NADI 2025 Shared Task 2: Pushing the Boundaries of Multidialectal Arabic ASR with Weakly Supervised Pretraining and Continual Supervised Fine-tuning

## Quick Facts
- arXiv ID: 2508.08912
- Source URL: https://arxiv.org/abs/2508.08912
- Reference count: 8
- Key outcome: Achieved state-of-the-art performance on NADI 2025 multidialectal Arabic ASR challenge with 35.69% WER and 12.21% CER

## Executive Summary
This paper addresses the challenge of building accurate automatic speech recognition (ASR) systems for Arabic, a low-resource language complicated by dialectal diversity. The authors propose a scalable training pipeline combining weakly supervised pretraining with continual supervised fine-tuning. First, a Conformer model is pretrained on 15,000 hours of weakly labeled speech covering Modern Standard Arabic (MSA) and various Dialectal Arabic (DA) variants. Next, the model is fine-tuned using a filtered subset of weakly labeled data and an augmented official training set, emphasizing high-quality transcriptions. The approach achieves state-of-the-art performance, ranking first in the multi-dialectal Arabic ASR challenge, with an average Word Error Rate (WER) of 35.69% and Character Error Rate (CER) of 12.21%, demonstrating effectiveness in handling data scarcity and dialectal variation.

## Method Summary
The method employs a two-stage training pipeline. First, a Conformer-Large model is pretrained from scratch on 15,000 hours of weakly labeled speech covering MSA and 8 DA variants using CTC loss. Second, the model undergoes continual supervised fine-tuning on a filtered subset of 3,000 hours of weak data (excluding MSA-heavy news content) combined with an augmented Casablanca training set. The pretraining uses a learning rate of 2×10⁻³ with AdamW optimizer, while fine-tuning reduces the learning rate by a factor of 10 to 2×10⁻⁴. The model uses an 18-layer encoder with 512 hidden dimensions, 8 attention heads, and a 31-size convolutional kernel, trained with bfloat16 precision and a global batch size of 512.

## Key Results
- Ranked first in NADI 2025 multidialectal Arabic ASR challenge
- Achieved average WER of 35.69% across all dialects
- Achieved average CER of 12.21% across all dialects
- Showed significant dialect-specific performance variation (Egyptian: 32.31% WER, Mauritanian: 59.03% WER)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large-scale weakly supervised pretraining provides a robust acoustic foundation that survives label noise when data volume is sufficient.
- Mechanism: The Conformer model learns generalizable acoustic-linguistic representations from 15,000 hours of automatically transcribed speech, where the sheer quantity of diverse MSA and DA examples compensates for individual label errors.
- Core assumption: Weak labels, while noisy, preserve enough statistical signal about the true speech-to-text mapping to enable useful representation learning.
- Evidence anchors:
  - [abstract] "pretrain the model on 15,000 hours of weakly labeled speech covering both Modern Standard Arabic (MSA) and various Dialectal Arabic (DA) variants"
  - [section 3.1] "weakly supervised learning depends on automatically generated or crowdsourced labels byi, which may contain errors or noise"
  - [corpus] Related work (arXiv:2504.12254) by same authors confirms the pretraining approach achieves SOTA without manual transcription
- Break condition: If weak label error rate exceeds ~30-40% or systematically mislabels specific phonemes/dialects, pretraining may encode incorrect mappings that fine-tuning cannot fully correct.

### Mechanism 2
- Claim: Continual supervised fine-tuning on filtered high-quality data selectively overwrites noisy representations while preserving learned acoustic features.
- Mechanism: Initializing from weakly pretrained weights rather than random initialization allows the fine-tuning stage to focus computation on refining transcription accuracy rather than learning basic acoustic patterns.
- Core assumption: The pretrained model has captured useful acoustic representations that transfer to the target dialects in the fine-tuning set.
- Evidence anchors:
  - [section 3.2] "initializing the model with weights obtained from the first stage... enables faster convergence and often better generalization on the target task due to prior knowledge encoded in the pretrained weights"
  - [section 3.4] "For the fine-tuning stage, the learning rate was reduced by a factor of ten"
  - [corpus] GMU IWSLT 2025 systems similarly demonstrate fine-tuning pretrained multilingual models for low-resource speech tasks
- Break condition: If the domain gap between weakly labeled pretraining data and fine-tuning data is too large (e.g., entirely different acoustic conditions, dialects not represented), transfer benefits diminish.

### Mechanism 3
- Claim: Dialect-aware data filtering combined with augmentation improves generalization across Arabic varieties by balancing dialect representation.
- Mechanism: Excluding MSA-dominant news content from the filtered fine-tuning set prevents the model from overfitting to formal Arabic at the expense of dialectal varieties. Augmentation of the official Casablanca training set artificially increases dialect diversity, reducing overfitting to specific speakers or recording conditions.
- Core assumption: MSA-dominant data skews model predictions toward formal Arabic patterns, harming dialectal ASR performance.
- Evidence anchors:
  - [section 3.2] "filtering process was designed to exclude news content—largely composed of Modern Standard Arabic (MSA)—and to retain only segments that passed stringent quality thresholds"
  - [section 3.2] "Casablanca Challenge training dataset, which is further expanded through various data augmentation techniques"
  - [corpus] No direct corpus evidence on MSA filtering effectiveness for DA; related papers focus on fine-tuning approaches without explicit filtering analysis
- Break condition: If filtering is too aggressive, it may inadvertently exclude rare but valid dialectal patterns; if augmentation introduces artifacts, model may learn spurious acoustic features.

## Foundational Learning

- Concept: **Weakly Supervised Learning**
  - Why needed here: The core innovation is using imperfect, automatically-generated labels at scale instead of expensive human annotations. Understanding the trade-off between label quality and quantity is essential.
  - Quick check question: Can you explain why a model trained on noisy labels might still generalize well to clean test data?

- Concept: **Conformer Architecture**
  - Why needed here: The model combines CNNs (local acoustic patterns) with self-attention (long-range dependencies), which is critical for handling Arabic's morphological complexity and variable-length utterances.
  - Quick check question: What advantage does the convolution-augmented transformer have over pure transformer or pure CNN approaches for speech?

- Concept: **Connectionist Temporal Classification (CTC)**
  - Why needed here: The training objective handles the alignment problem between variable-length audio and text without requiring frame-level annotations, enabling end-to-end training.
  - Quick check question: How does CTC handle the case where multiple output timesteps map to the same character?

## Architecture Onboarding

- Component map: Raw Audio → Mel-Spectrogram Extraction (80-dim, 25ms frame, 10ms hop) → Conformer Encoder (18 layers, 512 hidden dim, 8 attention heads, kernel 31) → CTC Loss → SentencePiece Tokenizer (128 tokens)

- Critical path:
  1. **Stage 1 Pretraining**: Train Conformer-Large from scratch on 15K hours weak data, AdamW + Noam scheduler, 10K warmup steps, peak LR 2×10⁻³, dropout 0.1, bfloat16 precision
  2. **Stage 2 Fine-tuning**: Load pretrained weights, train on 3K hours filtered data + augmented Casablanca set, reduce LR by 10x
  3. **Inference**: Mel-spectrogram extraction → encoder forward pass → CTC decoding → detokenization

- Design tradeoffs:
  - **Model size vs. compute**: 121M parameters (Conformer-Large) balances capacity with 8×A100 training feasibility
  - **Vocabulary size**: 128 SentencePiece tokens is relatively small for Arabic; may increase OOV rate but improves sample efficiency per token
  - **Weak data ratio**: 15K hours weak vs. 3K hours filtered—aggressive filtering (80% reduction) suggests quality threshold was stringent
  - Assumption: The 31-size convolutional kernel was chosen to capture ~310ms temporal context at 10ms hop, suitable for phoneme-level patterns

- Failure signatures:
  - **High WER on specific dialects (e.g., MAU: 59.03%)**: Indicates underrepresentation in training data; consider targeted data collection
  - **WER > 100%**: Severe transcription failures, often from vocabulary mismatches or acoustic condition drift (see baseline results)
  - **CER significantly lower than WER**: Model captures characters but struggles with word boundaries—may indicate tokenization issues with Arabic morphology
  - **Evaluation-to-testing regression**: If test WER increases substantially vs. evaluation, suggests overfitting to development set

- First 3 experiments:
  1. **Ablate pretraining scale**: Train on 5K and 10K hours of weak data to quantify the relationship between weak data volume and final WER; expect diminishing returns
  2. **Learning rate sweep for fine-tuning**: Test 1x, 5x, 10x, 20x reductions from pretraining LR to find optimal stability-performance tradeoff
  3. **Dialect-specific evaluation**: Isolate performance per dialect against training set dialect composition to identify which dialects need more data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the marginal performance gain attributable specifically to the 15,000-hour weakly supervised pretraining phase versus the continual supervised fine-tuning?
- Basis: [inferred] The paper presents a full pipeline in Figure 1 but lacks an ablation study isolating the contribution of the initial weak pretraining from the high-quality fine-tuning.
- Why unresolved: Without a baseline trained solely on the fine-tuning data, it is unclear if the "weak" data or the architecture is the primary driver of state-of-the-art performance.
- What evidence would resolve it: A comparison of WER/CER scores between the proposed model and a Conformer trained from scratch on only the filtered/augmented fine-tuning data.

### Open Question 2
- Question: Is the performance gap between Levantine and Maghreb dialects (e.g., Jordanian WER 20.68% vs. Mauritanian WER 59.03%) caused by data scarcity in the pretraining corpus or linguistic model limitations?
- Basis: [inferred] Results in Tables 1 and 2 show significant performance disparities across dialects, yet the paper provides no analysis of the dialectal composition of the 15,000-hour weakly labeled dataset.
- Why unresolved: The authors do not detail the distribution of dialects within the weak data, making it impossible to distinguish between data imbalance and modeling failure.
- What evidence would resolve it: Reporting the per-dialect duration of the training data and analyzing error rates relative to training set size for each dialect.

### Open Question 3
- Question: Does the strategy of filtering out Modern Standard Arabic (MSA) during the fine-tuning stage limit the model's ability to handle formal speech or code-switching?
- Basis: [inferred] Section 3.2 describes filtering to "exclude news content—largely composed of Modern Standard Arabic," but does not evaluate the impact of this removal on general linguistic capability.
- Why unresolved: While optimizing for the specific dialectal challenge, the removal of MSA might introduce domain overfitting or reduce versatility in mixed-language scenarios.
- What evidence would resolve it: Evaluating the fine-tuned model on a mixed MSA-DA test set to measure any degradation in MSA recognition compared to the pre-trained checkpoint.

## Limitations
- Data provenance and quality remain opaque: The 15,000-hour weakly labeled corpus source and automatic labeling methodology are not specified, creating uncertainty about generalizability.
- Dialect representation imbalance: Large WER variation across dialects suggests performance is strongly dependent on dialectal representation in training data, but composition analysis is missing.
- Ablation limitations: The paper presents a two-stage pipeline without ablation studies isolating the contribution of each component (weak pretraining scale, filtering strategy, learning rate reduction, augmentation).

## Confidence
- **High confidence**: The claim that weakly supervised pretraining followed by supervised fine-tuning achieves SOTA performance on the NADI 2025 benchmark.
- **Medium confidence**: The assertion that filtering MSA-heavy content improves dialectal ASR performance.
- **Low confidence**: The generalizability of the approach to other low-resource languages or different weak labeling error distributions.

## Next Checks
1. **Pretraining scale ablation**: Systematically train Conformer models on 5K, 10K, and 15K hours of the same weak corpus to quantify the relationship between weak data volume and final WER.
2. **Dialect representation analysis**: Compare the dialect composition of the 15K-hour pretraining corpus and 3K-hour filtered set against the evaluation and test sets. Calculate dialect-specific data ratios to determine whether performance gaps correlate with training data coverage.
3. **Alternative filtering strategies**: Implement and compare multiple filtering approaches (by dialect, acoustic quality, transcription confidence, MSA density) to identify which filtering criteria most improve multidialectal performance while preserving dialectal diversity.