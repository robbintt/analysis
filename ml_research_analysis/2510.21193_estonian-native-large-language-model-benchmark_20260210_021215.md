---
ver: rpa2
title: Estonian Native Large Language Model Benchmark
arxiv_id: '2510.21193'
source_url: https://arxiv.org/abs/2510.21193
tags:
- benchmark
- estonian
- evaluation
- tasks
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We present a benchmark for evaluating large language models in
  Estonian across seven tasks including grammar, vocabulary, factual knowledge, summarization,
  and information extraction. The benchmark is based on native Estonian data and includes
  6 base and 26 instruction-tuned models.
---

# Estonian Native Large Language Model Benchmark

## Quick Facts
- arXiv ID: 2510.21193
- Source URL: https://arxiv.org/abs/2510.21193
- Reference count: 0
- Primary result: Estonian National Exam task showed highest correlation with human evaluation preferences (r = 0.86)

## Executive Summary
This paper presents the first comprehensive benchmark for evaluating large language models on Estonian, a low-resource Uralic language. The benchmark includes seven tasks: Estonian National Exam (factual knowledge), Trivia (Estonia-specific knowledge), Declension (grammatical case generation), Word Meaning, Grammar Correction, News Summarization, and Speaker Name Extraction. Using both human judgment and an LLM-as-a-judge approach with Claude 3.7 Sonnet, the authors evaluate 6 base and 26 instruction-tuned models. Results show that language-specific fine-tuning significantly improves performance, and that certain tasks (like National Exam) correlate more strongly with perceived quality than others (like summarization).

## Method Summary
The benchmark evaluates models using lm-evaluation-harness with 7 native Estonian datasets from HuggingFace. Base models use 5-shot prompting while instruction-tuned models use 0-shot. Human evaluation involved 47 crowdsourced annotators assessing 10 model pairs across 57 questions using a forced-choice format. LLM-as-a-judge validation used Claude 3.7 Sonnet to evaluate the same 57 question pairs. All tasks are scored 0-1 with specific metrics: accuracy for exams/trivia/declension/word meaning, 1/(1+Levenshtein) for grammar, ROUGE-L for summarization, and F1 for speaker extraction.

## Key Results
- EstLLM-0825 fine-tuned Llama-3.1 8B outperforms all other small models and most medium-sized models
- Claude 3.7 Sonnet showed strong alignment with human ratings (Pearson r = 0.94)
- Estonian National Exam task showed highest correlation with human evaluation preferences (r = 0.86)
- News summarization dataset exhibited weakest correlations with other tasks (r = 0.44)

## Why This Works (Mechanism)

### Mechanism 1
High-performing LLMs can serve as reliable evaluators for low-resource language model outputs. Claude 3.7 Sonnet, selected for strong benchmark performance, evaluates response pairs using the same protocol as human annotators. The judge's strong multilingual capabilities enable consistent evaluation even for Estonian, a Uralic language with ~1M speakers. Core assumption: The judge LLM's training includes sufficient Estonian representation to reliably assess quality, and evaluation patterns transfer across languages. Evidence anchors: [abstract] "Claude 3.7 Sonnet served as the LLM judge and demonstrated strong alignment with human ratings (Pearson r = 0.94)." [Section 5.2] "The agreement between Claude's ratings and human judgments was high, with a Pearson correlation of 0.94 and Spearman correlation of 0.92 (p < 0.001)." Break condition: If judge model has insufficient Estonian pre-training, correlation with human judgment would degrade significantly below reported r=0.94.

### Mechanism 2
Language-specific fine-tuning produces outsized gains relative to model scale alone. Fine-tuning on native Estonian data exposes the model to morphological patterns (14 noun cases), vocabulary distributions, and cultural context absent or underrepresented in general pre-training. The EstLLM-0825 fine-tuned Llama-3.1 8B outperforms all other small models and most medium-sized models (mean score 0.431 vs. base Llama-3.1 8B at 0.295). Core assumption: Base model has sufficient multilingual foundation for fine-tuning to activate rather than inject language capabilities. Evidence anchors: [abstract] "Language-specific fine-tuning significantly improves performance." [Section 4.2] "The 8B EstLLM model outperforms all other small models and all but one medium-sized models." Break condition: If base model lacks foundational multilingual representations, fine-tuning yields diminishing returns and may overfit to fine-tuning set.

### Mechanism 3
Certain benchmark tasks correlate more strongly with perceived real-world usefulness than others. The Estonian National Exam task (testing factual knowledge and reasoning across 7 subjects) showed highest correlation with human preferences (r=0.86), while summarization showed weakest (r=0.44). This suggests factual knowledge tasks better proxy user-perceived quality than generation tasks measured by surface-level metrics like ROUGE-L. Core assumption: Human evaluators prioritize factual accuracy and instruction-following over surface fluency in open-ended tasks. Evidence anchors: [abstract] "The Estonian National Exam task showed the highest correlation with human evaluation preferences (r = 0.86)." [Section 4.3] "News summarization dataset exhibited the weakest correlations with other tasks... ROUGE-based evaluation may not reliably reflect actual model quality." Break condition: If user tasks primarily require generation/summarization rather than factual recall, exam-based benchmarks would show lower correlation with perceived usefulness.

## Foundational Learning

- **Concept: Morphological Complexity in Finno-Ugric Languages**
  - Why needed here: Estonian has 14 noun cases and complex inflection; Declension benchmark specifically tests this (1,400 samples across case-number combinations).
  - Quick check question: Can you explain why a benchmark testing "singular illative" and "plural partitive" forms would challenge models trained primarily on English data?

- **Concept: LLM-as-a-Judge Methodology**
  - Why needed here: The paper validates Claude 3.7 Sonnet as an automated evaluator; understanding this method is essential for scaling evaluation beyond human annotation capacity.
  - Quick check question: What potential biases might an LLM judge introduce when evaluating responses in a low-resource language, and how would you detect them?

- **Concept: Benchmark Localization vs. Translation**
  - Why needed here: The paper explicitly rejects machine-translated benchmarks, creating native datasets to avoid translation noise and ensure cultural relevance.
  - Quick check question: What are two specific ways a trivia question about Estonian geography might fail if translated from English rather than authored natively?

## Architecture Onboarding

- **Component map:**
  ```
  Native Datasets (7) → lm-evaluation-harness tasks → Model Evaluation → Aggregated Scores
                                ↓
  Human Eval (47 MT-Bench + 10 local questions) ←→ LLM Judge (Claude 3.7 Sonnet)
                                ↓
  Correlation Analysis (Pearson/Spearman) → Benchmark Validation
  ```

- **Critical path:**
  1. Create task definitions for base (5-shot) and instruction-tuned (zero-shot) models
  2. Run evaluations across 32 models using standardized harness
  3. Validate benchmark-human correlation using paired response evaluation
  4. Confirm LLM judge reliability before deploying at scale

- **Design tradeoffs:**
  - 5-shot for base models vs. zero-shot for instruction-tuned: Trades evaluation consistency for task-appropriate prompting
  - Levenshtein-based grammar scoring (0-1 scaled) vs. exact match: Tolerates minor variations but may conflate different error types
  - Claude 3.7 Sonnet as judge vs. multi-judge ensemble: Single judge simplifies pipeline but introduces single-point-of-failure risk

- **Failure signatures:**
  - Gemma models failed speaker extraction due to context length limits ("n/a" in results)
  - OpenRouter API providers showed "dramatically" varying quality for large open models (mitigated by provider blocklist)
  - News summarization ROUGE-L scores showed weak correlation with human judgment (r=0.44), indicating metric unsuitability

- **First 3 experiments:**
  1. Replicate Declension benchmark evaluation on a new model; verify morphological generation accuracy across all 14 cases using the Filosoft synthesizer as ground truth
  2. Ablate LLM judge: Compare Claude 3.7 Sonnet judgments against Gemini 2.5 Pro on the same 57 evaluation questions to measure judge-specific variance
  3. Fine-tune a small model (e.g., Llama-3.2 3B) on a subset of Estonian National Exam data; measure whether gains generalize to Trivia and Word Meaning tasks (testing transfer vs. memorization)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What evaluation methodologies could better capture summarization quality for Estonian than ROUGE-L, given the weak correlation between automated metrics and human judgment in this task?
- Basis in paper: [explicit] The authors state that "ROUGE-based evaluation may not reliably reflect actual model quality in text generation tasks" and that "summarization behaves anomalously and may require improved evaluation methodologies."
- Why unresolved: The paper identifies the problem (News Summarization had the lowest correlation with human evaluation at r = 0.44) but does not propose or test alternative metrics.
- What evidence would resolve it: Comparison of alternative evaluation approaches (e.g., embedding-based semantic similarity, LLM-based evaluation, or human-centric metrics) against human judgments for Estonian summarization.

### Open Question 2
- Question: Why do some newer model versions show regression in Estonian-language capabilities, and what factors predict multilingual capability stability across model iterations?
- Basis in paper: [explicit] The authors observe that "Claude Sonnet 4 resulted in noticeable regression in its Estonian abilities, compared to Claude Sonnet 3.7" without explaining the cause.
- Why unresolved: The paper documents the regression but does not investigate whether it stems from training data changes, alignment procedures, or other factors.
- What evidence would resolve it: Systematic comparison of model versions with analysis of training data composition, or controlled experiments varying training corpus language distributions.

### Open Question 3
- Question: Can LLM-as-a-judge approaches using models other than Claude 3.7 Sonnet achieve comparable alignment with human evaluators for Estonian?
- Basis in paper: [inferred] The paper only tests Claude 3.7 Sonnet as the LLM judge, leaving unclear whether this is a unique capability or if other strong models would perform similarly.
- Why unresolved: The methodology demonstrates strong results (r = 0.94) with one model but does not establish generalizability of the LLM-as-a-judge approach for Estonian.
- What evidence would resolve it: Replication of the judging experiment using alternative high-performing models (e.g., GPT-4o, Gemini) and comparison of correlation coefficients with human ratings.

## Limitations
- Benchmark validation limited to one LLM judge model (Claude 3.7 Sonnet) and one language family
- 57 evaluation questions represent a limited sample of possible evaluation scenarios
- Observed correlations range from 0.44 to 0.86, suggesting task-dependent reliability

## Confidence
- **High confidence**: Language-specific fine-tuning significantly improves Estonian model performance
- **Medium confidence**: High-performing LLMs can serve as reliable evaluators for low-resource languages
- **Medium confidence**: Certain benchmark tasks correlate better with perceived real-world usefulness than others

## Next Checks
1. **Cross-linguistic judge validation**: Evaluate the same Estonian models using LLM judges from different language families (e.g., Slavic or Germanic languages) to test whether correlation with human judgment generalizes beyond Uralic languages.

2. **Temporal stability assessment**: Re-run the benchmark evaluation after 6-12 months with newer model versions to determine whether the observed performance hierarchy remains stable or shifts with model evolution.

3. **User study correlation**: Conduct controlled user studies where Estonian speakers complete real-world tasks (information seeking, document summarization) using top-performing models, then correlate task success rates with benchmark scores to validate the benchmark's predictive validity.