---
ver: rpa2
title: A Study on Leveraging Search and Self-Feedback for Agent Reasoning
arxiv_id: '2502.12094'
source_url: https://arxiv.org/abs/2502.12094
tags:
- search
- feedback
- mcts
- tool
- username
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the use of self-feedback during search
  for agent reasoning, examining its effectiveness on mathematical reasoning and tool-calling
  tasks. While search algorithms like MCTS can improve performance, the experiments
  reveal that self-feedback alone is often unreliable for guiding the search process.
---

# A Study on Leveraging Search and Self-Feedback for Agent Reasoning

## Quick Facts
- arXiv ID: 2502.12094
- Source URL: https://arxiv.org/abs/2502.12094
- Reference count: 6
- Self-feedback alone is often unreliable for guiding search algorithms in reasoning tasks.

## Executive Summary
This study investigates whether search algorithms like Monte Carlo Tree Search (MCTS) can effectively use model self-feedback for reasoning, comparing against ground-truth feedback on mathematical reasoning (GSM8K) and tool-calling (ToolTalk) tasks. While search improves performance when reliable feedback signals are available, experiments reveal that self-feedback alone provides weak guidance for answer selection. Majority voting across search nodes consistently outperforms self-feedback-based reward selection. On tool-calling tasks, search with self-feedback can degrade performance due to tool parameter hallucinations. Various augmentation strategies were tested with mixed results, suggesting that search algorithms require carefully designed, task-specific feedback mechanisms rather than relying solely on self-feedback.

## Method Summary
The study employs MCTSr (MCTS with Self-Refine), a framework that integrates Monte Carlo Tree Search with self-feedback mechanisms. Models tested include Llama 3 70B Instruct, Mistral v0.3 7B, Claude 3 Haiku, and Claude 3 Sonnet. The approach uses MCTS with configurable iterations (default: 10), UCT exploration, and node expansion via model generation. Feedback is provided by a separate LLM prompted to critique solutions and assign rewards. Three answer selection strategies are compared: random, majority voting, and maximum reward selection. For tool-calling tasks, hallucination detection modules and ICL examples are added as augmentation strategies. GSM8K accuracy and ToolTalk precision/recall/F1 metrics serve as evaluation measures.

## Key Results
- Majority voting outperforms self-feedback-based reward selection across all models and tasks.
- Ground-truth MCTS improves accuracy by ~10%+ compared to self-feedback on GSM8K.
- Search with self-feedback degrades tool-calling performance due to parameter hallucination amplification.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Search improves reasoning performance when reliable feedback signals are available, but self-feedback provides weak guidance for answer selection.
- Mechanism: MCTS uses feedback to compute Q-values that balance exploration vs. exploitation via UCT criterion. When feedback accurately reflects solution correctness, search can prioritize high-value paths. Self-feedback rewards, however, show poor correlation with actual correctness, making reward-based selection unreliable.
- Core assumption: The quality of Q-value updates directly determines search effectiveness; garbage feedback yields garbage guidance.
- Evidence anchors:
  - [abstract] "For search to work effectively, either access to the ground-truth is needed or feedback mechanisms need to be carefully designed for the specific task."
  - [Section 3.2/Table 1] Ground-truth MCTS improves accuracy by ~10%+ across all models; majority voting outperforms maximum reward selection (e.g., Llama 3: 0.883 vs 0.776).
  - [corpus] Related work (Huang et al., 2024; Stechly et al., 2024) also critiques LLM self-correction reliability—consistent with these findings.
- Break condition: When self-feedback rewards are no better than random for distinguishing correct vs. incorrect solutions, reward-based search degrades to random exploration.

### Mechanism 2
- Claim: Majority voting across search nodes provides a more robust aggregation signal than self-feedback rewards for answer selection.
- Mechanism: Aggregating final answers across multiple reasoning paths exploits the observation that correct solutions tend to converge on the same answer, while errors are more diverse. Self-feedback rewards reflect model confidence, which is poorly calibrated.
- Core assumption: Correct reasoning paths produce convergent answers; incorrect paths produce scattered answers.
- Evidence anchors:
  - [Section 3.1] "Majority voting seems to be the only selection strategy that consistently improves over the no-search baseline."
  - [Table 1] Majority voting consistently outperforms maximum reward selection across all four models tested.
  - [corpus] Corpus neighbors (e.g., "Can Large Reasoning Models Self-Train?") similarly find majority voting effective as a self-feedback mechanism—partial corroboration.
- Break condition: If incorrect answers also converge (e.g., shared systematic errors), majority voting fails.

### Mechanism 3
- Claim: Search with self-feedback can actively degrade performance on complex, open-ended tasks due to hallucination amplification.
- Mechanism: Search explores multiple solution paths, each carrying hallucination risk. In tool-calling, agents exhibit bias toward premature tool calls with fabricated parameters. Self-feedback fails to penalize these hallucinations, so search selects high-reward but hallucinated solutions.
- Core assumption: Hallucination rates are path-independent; more paths explored = more hallucination opportunities.
- Evidence anchors:
  - [Section 4.1/Table 2] "No-search baseline performs better than search" on ToolTalk (Sonnet F1: 0.706 → 0.559 with MCTS).
  - [Section 4.1] "Tool parameter hallucination as a major cause of error... feedback and reward model does not capture these hallucinations."
  - [corpus] Limited direct corpus coverage on tool-calling hallucination in search; this appears to be a domain-specific gap.
- Break condition: When hallucination detection is reliable (e.g., via external module), search should recover utility.

## Foundational Learning

- Concept: Monte Carlo Tree Search (MCTS) and UCT criterion
  - Why needed here: The entire framework builds on MCTS for reasoning; understanding Q-values, exploration bonus, and backpropagation is essential.
  - Quick check question: Can you explain why UCT adds an exploration term to the Q-value?

- Concept: LLM self-evaluation limitations (calibration, reward hacking)
  - Why needed here: The core finding is that self-feedback is unreliable; understanding *why* (overconfidence, inability to detect own errors) contextualizes results.
  - Quick check question: Why might a model rate a hallucinated solution highly?

- Concept: Tool-calling agent architecture and grounding
  - Why needed here: ToolTalk experiments require understanding multi-turn dialogue, parameter grounding, and hallucination modes specific to tool use.
  - Quick check question: What distinguishes parameter hallucination from correct parameter inference?

## Architecture Onboarding

- Component map:
  Search backbone -> Feedback model -> Aggregation strategies -> (Optional) Hallucination detection module

- Critical path:
  1. Initialize search tree with problem prompt.
  2. For each iteration: select node via UCT → expand with model generation → simulate/rollout → compute reward (self-feedback or augmented) → backpropagate.
  3. After max iterations: apply aggregation strategy to select final answer from all visited nodes.

- Design tradeoffs:
  - **Ground-truth vs. self-feedback**: Ground-truth requires labeled data; self-feedback generalizes but is unreliable.
  - **Precision vs. recall**: Hallucination detection module improves precision but drops recall (over-cautious clarification requests).
  - **Search depth vs. hallucination exposure**: More iterations = more paths = more hallucination opportunities in tool-calling.

- Failure signatures:
  - **Reward collapse**: Self-feedback assigns uniformly high rewards regardless of correctness.
  - **Hallucination cascade**: Tool parameters fabricated early propagate through search; feedback model fails to penalize.
  - **Over-confirmation**: Post-hallucination detection, agents request unnecessary user confirmation (precision up, recall down).

- First 3 experiments:
  1. Replicate GSM8K majority voting vs. max reward comparison on a small sample (50 examples) to validate aggregation finding.
  2. Add a calibrated verifier (e.g., external code execution for math) as feedback source; measure gap vs. self-feedback.
  3. On ToolTalk, ablate hallucination detection module alone vs. combined with ICL examples to isolate precision/recall drivers.

## Open Questions the Paper Calls Out

- Can hybrid approaches combining self-feedback with partial external verification outperform pure self-feedback or pure ground-truth methods in search-based reasoning?
- Why does majority voting across search nodes consistently outperform self-feedback-based reward selection for answer aggregation?
- Can the precision-recall tradeoff in hallucination detection modules be mitigated while retaining their benefits for search-based tool-calling?
- Do findings on self-feedback reliability generalize to other search algorithms (e.g., beam search, best-first search) and reasoning domains (e.g., code generation, logical inference)?

## Limitations
- The self-feedback quality appears highly sensitive to prompt design and temperature settings, which were not fully specified in the paper.
- The hallucination detection module for tool-calling tasks may be overfitting to specific parameter types in ToolTalk.
- The study focuses on a single search algorithm (MCTS); results may not generalize to other search paradigms like beam search or breadth-first search.

## Confidence
- **High Confidence**: Majority voting outperforms self-feedback-based reward selection for answer aggregation; this finding is robust across multiple models and datasets.
- **Medium Confidence**: Self-feedback alone is insufficient for effective search guidance; while supported by experimental results, the generalizability across different reasoning tasks needs further validation.
- **Low Confidence**: Search with self-feedback degrades tool-calling performance due to hallucination amplification; this finding is based on a single dataset (ToolTalk) and specific agent architecture.

## Next Checks
1. Test the MCTSr framework on additional reasoning tasks (e.g., logical reasoning, commonsense QA) to validate whether majority voting remains superior to reward-based selection across diverse domains.
2. Implement an external verification mechanism (e.g., symbolic execution for code generation or code execution for math problems) to quantify the gap between self-feedback and ground-truth feedback, and measure how this gap correlates with search effectiveness.
3. Compare MCTS against alternative search strategies (beam search, BFS) under identical self-feedback conditions to determine whether the limitations are specific to MCTS or inherent to search-based reasoning with unreliable feedback.