---
ver: rpa2
title: 'Do Before You Judge: Self-Reference as a Pathway to Better LLM Evaluation'
arxiv_id: '2509.19880'
source_url: https://arxiv.org/abs/2509.19880
tags:
- answer
- judge
- judgment
- evaluation
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study systematically examines the correlation between large\
  \ language models\u2019 (LLMs) answer generation and judgment capabilities within\
  \ the LLM-as-Judge framework. Through dataset- and instance-level analyses across\
  \ 11 models and 21 tasks, the authors find that generation and judgment abilities\
  \ are only weakly correlated under standard Chain-of-Thought prompting, primarily\
  \ due to LLMs\u2019 sensitivity to the responses being judged."
---

# Do Before You Judge: Self-Reference as a Pathway to Better LLM Evaluation

## Quick Facts
- **arXiv ID**: 2509.19880
- **Source URL**: https://arxiv.org/abs/2509.19880
- **Authors**: Wei-Hsiang Lin; Sheng-Lun Wei; Hen-Hsen Huang; Hsin-Hsi Chen
- **Reference count**: 13
- **Primary result**: Self-reference-guided evaluation increases correlation between LLM generation and judgment by 0.35 on average.

## Executive Summary
This study reveals that LLMs' answer generation and judgment capabilities are only weakly correlated under standard Chain-of-Thought prompting, primarily because evaluation outcomes depend more on the correctness of the response being judged than on the judge model's own knowledge. To address this, the authors propose a self-reference-guided evaluation strategy that injects the judge model's own answer as a reference during evaluation. This approach significantly strengthens the correlation between generation and judgment capabilities, with an average increase of 0.35 in partial correlation. The method proves effective across diverse tasks including mathematical reasoning, dialogue evaluation, and humanities, validating its generalizability.

## Method Summary
The study evaluates 11 models across 21 tasks using LLM-as-Judge framework with pointwise binary classification. It first establishes that judgment accuracy is dominated by the correctness label of responses being evaluated, then uses partial correlation analysis to reveal weak intrinsic coupling between generation and judgment. The self-reference-guided method injects the judge model's own generated answer into the evaluation prompt, transforming abstract correctness assessment into concrete comparison against a self-generated standard. The approach was tested on 7 datasets (MMLU Pro, GSM8K, GSM-Symbolic, Chatbot Arena, MT-Bench) with 100 samples per subtask.

## Key Results
- Standard CoT prompting shows only weak correlation between generation and judgment capabilities (r < 0.3 in 25/33 cases after controlling for response correctness)
- Self-reference-guided evaluation increases partial correlation by 0.35 on average, with 22/33 cases exceeding 0.5 correlation
- Effectiveness correlates with judge generation accuracy, showing a ~50% threshold on MMLU Pro
- Performance varies across tasks: strong on mathematical reasoning, mixed on dialogue evaluation

## Why This Works (Mechanism)

### Mechanism 1: Response Correctness Dominates Judgment Behavior
- **Claim:** LLM judgment accuracy is primarily driven by the correctness label of the response being evaluated, not the judge's underlying generation capability.
- **Mechanism:** LLMs exhibit systematic bias toward predicting "correct" and perform substantially better on correctly-labeled responses regardless of their own knowledge state, creating spurious dataset-level correlations.
- **Core assumption:** The model's prediction bias is independent of its actual knowledge about the question domain.
- **Evidence anchors:** Table 1 shows DJ+ outperforming DJ− across all 11 models; Figure 2 heatmap reveals DCorrectA subsets achieve near-perfect scores while DIncorrectA scores remain low.

### Mechanism 2: Partial Correlation Reveals True Independence
- **Claim:** Once controlling for agent response correctness, generation and judgment capabilities show weak residual correlation (r < 0.3 in 25/33 cases).
- **Mechanism:** Partial correlation rG,J|A removes the linear influence of agent answer correctness from both judge generation and judgment variables, stripping away confounding from response quality.
- **Core assumption:** The relationship between generation and judgment is linear, and confounding is adequately captured by the binary correctness variable A.
- **Evidence anchors:** Table 3 shows 25 of 33 partial correlations below 0.3; "In most cases (25 out of 33), the correlations fall below 0.3, indicating only a weak association."

### Mechanism 3: Self-Reference Anchors Judgment to Generation
- **Claim:** Providing the judge model's own generated answer as a reference during evaluation strengthens generation-judgment correlation by 0.35 on average.
- **Mechanism:** The self-reference creates coupling where the judge's evaluation explicitly references its own reasoning pathway, transforming evaluation from abstract correctness assessment to concrete comparison against a self-generated standard.
- **Core assumption:** The judge's self-generated answer, even if incorrect, provides a useful reference frame that makes its evaluation behavior more predictable from its generation behavior.
- **Evidence anchors:** Abstract states "average increase of 0.35 in partial correlation"; Table 4 shows 22/33 cases now exceed 0.5 correlation; Figure 4 demonstrates self-reference outperforms CoT once judge generation accuracy exceeds ~50% threshold.

## Foundational Learning

- **Partial Correlation Analysis:**
  - **Why needed here:** Critical for distinguishing whether generation-judgment correlation is intrinsic or driven by confounding (agent response correctness). Standard correlation masks this distinction.
  - **Quick check question:** If rG,J = 0.5 but rG,J|A = 0.1, what does this imply about the role of variable A?

- **Pointwise vs Pairwise LLM Evaluation:**
  - **Why needed here:** The paper explicitly adopts pointwise judgment rather than pairwise comparison to isolate generation-judgment correlation without selection-bias confounds.
  - **Quick check question:** Why might pairwise comparison artificially inflate apparent judgment capability?

- **Chain-of-Thought Prompting for Evaluation:**
  - **Why needed here:** Both generation and judgment phases use CoT reasoning; this is the baseline against which self-reference-guided evaluation is compared.
  - **Quick check question:** How does CoT change the relationship between generation and judgment compared to zero-shot prompting?

## Architecture Onboarding

- **Component map:** Agent Model (MA) → generates answers â(MA) → Judge Model (MJ) → [Phase 1: Generation] → â(MJ) (self-reference) → [Phase 2: Judgment with Self-Reference] → Binary verdict: Correct/Incorrect

- **Critical path:** The self-reference mechanism requires the judge model to generate its answer before judging, making this a two-stage pipeline where Stage 1 output becomes Stage 2 context.

- **Design tradeoffs:**
  - Error propagation: If judge's self-reference is wrong, it may incorrectly penalize correct agent answers
  - Threshold dependency: Effectiveness correlates with judge generation accuracy; paper observes ~50% accuracy threshold on MMLU Pro
  - Single-turn limitation: Study restricts to single-turn interactions; multi-turn behavior unknown

- **Failure signatures:**
  - Low generation accuracy domain: Self-reference may worsen evaluation if judge is frequently wrong
  - High-saturation tasks: When generation accuracy approaches ceiling, partial correlation analysis becomes unreliable
  - Overconfidence amplification: Models biased toward "correct" predictions may double-down on errors

- **First 3 experiments:**
  1. **Reproduce partial correlation baseline:** Take 2-3 models from different families, compute raw vs partial correlations on a held-out task subset to verify weak coupling finding.
  2. **Self-reference ablation:** Compare three conditions on same evaluation set: (a) standard CoT judgment, (b) self-reference-guided, (c) gold-reference-guided. Measure correlation change and F1 performance.
  3. **Error propagation analysis:** Manually annotate cases where judge's self-reference is wrong; measure false-negative rate to quantify failure mode.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the self-reference-guided evaluation strategy be effectively adapted for pairwise comparison or listwise ranking tasks?
- **Basis in paper:** [explicit] The authors state in the Limitations section that the method was employed for pointwise tasks and that "The applicability of this method to more complex evaluation formats... remains uncertain."
- **Why unresolved:** It is currently unclear how to define a singular "self-reference" when the task involves comparing multiple external candidates rather than judging a single answer against a self-generated one.
- **What evidence would resolve it:** A methodology for generating and utilizing self-references in pairwise settings, along with empirical results testing performance against standard pairwise baselines.

### Open Question 2
- **Question:** Does the correlation between generation and judgment capabilities, and the effectiveness of self-reference guidance, persist in multi-turn dialogue contexts?
- **Basis in paper:** [explicit] The authors explicitly list "Lack of Multi-Turn Interaction Analysis" as a limitation, noting they restricted the scope to single-turn interactions.
- **Why unresolved:** Multi-turn contexts require maintaining long context and adapting to conversation history, which may fundamentally alter the relationship between a model's ability to answer and its ability to judge.
- **What evidence would resolve it:** Replication of the paper's partial correlation analysis and self-reference intervention on multi-turn benchmarks (e.g., MT-Bench multi-turn).

### Open Question 3
- **Question:** What is the specific failure rate of self-reference-guided evaluation relative to a model's generation accuracy due to error propagation?
- **Basis in paper:** [inferred] While Section 5.2 notes a performance threshold of 50% on MMLU Pro, the Limitations section highlights "Potential for Error Propagation" where incorrect self-references penalize correct agent answers.
- **Why unresolved:** The trade-off between the alignment benefit and the risk of the judge enforcing its own incorrect knowledge has not been fully quantified across different accuracy levels.
- **What evidence would resolve it:** Controlled experiments measuring the rate of "false negative" judgments (correct agent answers marked incorrect) relative to the judge model's specific error types.

## Limitations

- The study focuses on single-turn interactions, leaving multi-turn evaluation behavior unexplored
- The ~50% accuracy threshold for self-reference effectiveness appears dataset-specific and may not generalize across all domains
- The binary correctness label used for partial correlation may not fully capture the complexity of confounding factors, particularly for nuanced evaluation tasks

## Confidence

- **High confidence**: The empirical finding that DJ+ consistently outperforms DJ− across all models and tasks is robustly demonstrated (Table 1, Figure 2)
- **Medium confidence**: The claim that partial correlation reveals weak intrinsic coupling between generation and judgment (r < 0.3 in 25/33 cases) is statistically valid but depends on the correctness label adequately capturing confounding
- **Medium confidence**: The average +0.35 partial correlation improvement with self-reference is well-supported, though the dataset-specific threshold effect suggests context dependency

## Next Checks

1. **Cross-dataset threshold verification**: Test the self-reference effectiveness threshold on diverse datasets beyond MMLU Pro to determine if the ~50% accuracy rule generalizes
2. **Multi-turn evaluation extension**: Implement a multi-turn variant of the self-reference method and measure whether generation-judgment correlation patterns persist across conversational contexts
3. **Confounder robustness analysis**: Replace the binary correctness label with alternative confounding variables (e.g., response length, reasoning complexity) to test whether partial correlation results are sensitive to the choice of control variable