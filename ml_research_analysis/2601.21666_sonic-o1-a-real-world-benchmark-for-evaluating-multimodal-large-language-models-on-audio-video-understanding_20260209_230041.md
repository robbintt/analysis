---
ver: rpa2
title: 'SONIC-O1: A Real-World Benchmark for Evaluating Multimodal Large Language
  Models on Audio-Video Understanding'
arxiv_id: '2601.21666'
source_url: https://arxiv.org/abs/2601.21666
tags:
- video
- score
- temporal
- across
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SONIC-O1 is a new benchmark for evaluating multimodal large language
  models on real-world audio-video understanding. It covers 13 conversational domains
  with 4,958 human-verified QA instances and demographic metadata, testing summarization,
  multiple-choice questions, and temporal localization.
---

# SONIC-O1: A Real-World Benchmark for Evaluating Multimodal Large Language Models on Audio-Video Understanding

## Quick Facts
- arXiv ID: 2601.21666
- Source URL: https://arxiv.org/abs/2601.21666
- Reference count: 40
- A benchmark with 4,958 human-verified QA instances across 13 conversational domains for audio-video understanding.

## Executive Summary
SONIC-O1 introduces a new benchmark for evaluating multimodal large language models on real-world audio-video understanding. The benchmark covers 13 conversational domains with human-verified QA instances and demographic metadata. It tests three tasks: summarization, multiple-choice questions, and temporal localization. Experiments reveal significant performance gaps between closed-source and open-source models, with closed-source models outperforming by 22.6% on temporal localization tasks. The benchmark provides an open evaluation suite for reproducible, socially robust multimodal understanding.

## Method Summary
SONIC-O1 is constructed from real-world conversations across 13 domains, with 4,958 human-verified QA instances. Each instance includes audio-video inputs and demographic metadata. The benchmark evaluates models on three tasks: summarization (summarizing conversation content), multiple-choice questions (answering questions about conversation content), and temporal localization (identifying time segments containing specific information). The evaluation protocol is open and reproducible, with code and datasets available for community use.

## Key Results
- Closed-source models outperform open-source models by 22.6% on temporal localization tasks
- Demographic disparities persist across all tasks, with larger effects in temporal localization than summarization or MCQs
- Benchmark covers 13 conversational domains with 4,958 human-verified QA instances

## Why This Works (Mechanism)
SONIC-O1 works by providing real-world audio-video inputs paired with human-verified answers across diverse conversational domains. The benchmark captures the complexity of real conversations including overlapping speech, background noise, and varying speaking styles. By including demographic metadata, it enables assessment of model fairness and robustness across different user groups. The three-task structure tests different aspects of multimodal understanding: comprehension (summarization), inference (MCQs), and precise temporal reasoning (temporal localization).

## Foundational Learning

**Multimodal Learning**: Understanding how models integrate information from multiple modalities (audio and video) to form coherent representations. Why needed: Real-world conversations require processing both spoken content and visual cues simultaneously. Quick check: Can the model identify when audio and visual information contradict each other?

**Temporal Reasoning in Video**: Ability to locate and reason about specific events or segments within a time sequence. Why needed: Many real-world tasks require identifying when specific information occurs in conversations. Quick check: Can the model accurately pinpoint time segments containing specific topics or speakers?

**Demographic Bias in AI**: Understanding how model performance varies across different demographic groups. Why needed: Ensures fair and equitable performance across diverse user populations. Quick check: Does model performance show consistent patterns across different demographic groups?

## Architecture Onboarding

**Component Map**: Raw Audio/Video -> Feature Extraction -> Multimodal Fusion -> Task-Specific Heads -> Output (Summary/Answer/Time Segment)

**Critical Path**: Feature extraction from audio and video streams → multimodal fusion layer → task-specific processing → final prediction

**Design Tradeoffs**: The benchmark balances task difficulty with real-world applicability, requiring models to handle complex conversational scenarios rather than simplified, controlled environments.

**Failure Signatures**: Models may fail on overlapping speech, background noise, or when visual and audio information are misaligned. Performance degradation often occurs on less common demographic groups or conversation types.

**First 3 Experiments**:
1. Test model performance on each individual task (summarization, MCQs, temporal localization) to identify specific weaknesses
2. Evaluate model performance across different demographic groups to assess fairness
3. Compare performance on conversations with varying levels of audio quality or background noise

## Open Questions the Paper Calls Out
None

## Limitations
- Performance gap findings are based on specific model versions and may not generalize
- The 13 conversational domains may not capture full diversity of real-world scenarios
- Demographic metadata analysis is limited by the specific demographics captured

## Confidence

**High**: Benchmark construction methodology, reported performance differences between model types
**Medium**: Generalizability of demographic disparity findings, stability of 22.6% performance gap
**Low**: None identified

## Next Checks

1. Evaluate additional model versions and variants to verify the stability of the reported performance gap between closed- and open-source models.

2. Expand demographic metadata collection to include additional demographic factors and test whether observed disparities persist or change.

3. Conduct cross-dataset validation by testing models on SONIC-O1 and other multimodal benchmarks to assess consistency in performance patterns.