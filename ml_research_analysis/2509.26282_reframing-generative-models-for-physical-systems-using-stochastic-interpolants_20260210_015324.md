---
ver: rpa2
title: Reframing Generative Models for Physical Systems using Stochastic Interpolants
arxiv_id: '2509.26282'
source_url: https://arxiv.org/abs/2509.26282
tags:
- stochastic
- generative
- arxiv
- urlhttps
- interpolants
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks generative models for physical systems like
  PDEs and climate simulations. The key insight is that most generative models transport
  Gaussian noise to model future states, but directly learning a stochastic process
  between current and future states (stochastic interpolants) can be more efficient
  and accurate.
---

# Reframing Generative Models for Physical Systems using Stochastic Interpolants

## Quick Facts
- **arXiv ID:** 2509.26282
- **Source URL:** https://arxiv.org/abs/2509.26282
- **Reference count:** 40
- **Primary result:** Stochastic interpolants consistently achieve lower error with fewer sampling steps by leveraging the proximity of successive physical states in physical system emulation tasks.

## Executive Summary
This paper benchmarks generative models for physical systems like PDEs and climate simulations, demonstrating that stochastic interpolants outperform traditional diffusion models by learning the drift directly between current and future states rather than transporting Gaussian noise. The key insight is that consecutive physical states are statistically closer than a generic Gaussian prior and the target, enabling more efficient sampling. The authors evaluate multiple generative frameworks on Kolmogorov flow, Rayleigh-Bénard convection, and weather forecasting, showing stochastic interpolants achieve superior performance with significantly fewer sampling steps while offering post-training control over the trade-off between pointwise accuracy and spectral consistency.

## Method Summary
The method reframes generative modeling as learning a stochastic process between current and future physical states using stochastic interpolants. An autoencoder compresses high-dimensional physical fields into a lower-dimensional latent space, where a diffusion transformer learns to predict the time-dependent drift between the current latent state and the future state. The model can sample deterministically via probability flow ODE or stochastically via SDE, with the latter injecting noise to recover high-frequency features. The approach leverages the statistical proximity of consecutive physical states to reduce integration steps from 100 (typical diffusion) to 2-5, while a saturation function stabilizes training by limiting latent variance.

## Key Results
- Stochastic interpolants achieve VRMSE of 0.55 on Kolmogorov flow versus 0.74 for FNO with only 2 sampling steps
- SI-EM (SDE sampler) captures turbulent features better than SI-E (ODE sampler) in Rayleigh-Bénard convection, trading pointwise accuracy for spectral consistency
- SI consistently outperforms DDPM and EDM across all tested physical systems while requiring 10-50× fewer sampling steps
- Weather forecasting shows variable-specific performance: smoother fields favor deterministic sampling while high-frequency fields benefit from stochasticity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Transporting samples between physically proximate distributions (current state $u(t)$ to future state $u(t+1)$) requires fewer integration steps than transporting Gaussian noise to a target state.
- **Mechanism:** Standard diffusion models map from a generic Gaussian prior to the target, which is computationally expensive if the prior is far from the target in latent space. Stochastic Interpolants reframe the problem as learning a time-dependent vector field (drift) directly between the source and target distributions. By initializing the chain at the current physical state rather than noise, the trajectory length in probability space is shortened.
- **Core assumption:** The source distribution (current state) is statistically closer to the target distribution (future state) than a standard Gaussian is.
- **Evidence anchors:**
  - [abstract] "By directly learning a stochastic process between current and future states, stochastic interpolants can leverage the proximity of successive physical distributions."
  - [section 4] Figure 2 visualizes distance heuristics (Sliced Wasserstein, MMD), showing $D(u(t), u(t+1))$ is consistently lower than $D(N(0, I), u(t+1))$.
  - [corpus] "Fast Sampling for Flows and Diffusions..." supports the general principle that interpolants allow for schedule control to speed up transport, though specific physical proximity metrics are unique to this paper.
- **Break condition:** Fails if the physical system is so chaotic that $u(t)$ and $u(t+1)$ are decorrelated (effectively indistinguishable from noise relative to each other), removing the proximity advantage.

### Mechanism 2
- **Claim:** The stochastic interpolant framework allows for post-training adjustment of the trade-off between pointwise accuracy (RMSE) and spectral consistency (statistical fidelity) via the sampling scheme.
- **Mechanism:** The model learns a drift $b_\theta(t, x_t)$ and a noise coefficient $\gamma(t)$. During inference, one can solve the probability flow ODE (deterministic, $dW_t=0$) for low pointwise error or the SDE (stochastic, $dW_t \neq 0$) to inject noise. Injecting noise helps recover high-frequency features (turbulence spectra) that deterministic solvers tend to smooth over.
- **Core assumption:** The underlying physical system requires a balance of exact trajectory tracking and statistical realism (unimodality vs. chaotic spread).
- **Evidence anchors:**
  - [section 4] Table 2 and Figure 8 show that SI-E (ODE) minimizes VRMSE (pointwise), while SI-EM (SDE) results in higher pointwise error but better captures turbulent features in Rayleigh-Bénard convection.
  - [abstract] "...generative models need to balance deterministic accuracy, spectral consistency, and probabilistic calibration, and that stochastic interpolants can potentially fulfill these requirements by adjusting their sampling."
  - [corpus] Weak direct evidence in corpus regarding the specific ODE/SDE trade-off, though "Physics-aware generative models..." discusses energy consistency which aligns with spectral goals.
- **Break condition:** Fails if the ODE solver numerical error dominates, or if the SDE noise scale $\gamma(t)$ is set too high, causing the trajectory to diverge from the physical manifold.

### Mechanism 3
- **Claim:** Operating in a compressed latent space stabilizes autoregressive rollouts and prevents the generative model from overfitting to high-frequency noise.
- **Mechanism:** An autoencoder compresses high-dimensional physical fields (e.g., $512 \times 128$) into a lower-dimensional latent vector. The interpolant is trained to predict the latent future state. A "saturation function" (or KL regularization) limits latent variance, preventing the drift network from becoming unstable when differentiating noisy gradients in pixel space.
- **Core assumption:** The autoencoder can lossily compress the state while preserving the essential dynamics (latent space is a valid manifold for the PDE).
- **Evidence anchors:**
  - [section 3.2] "For PDE problems, this can additionally stabilize rollouts... apply a saturation function to latent vectors... to avoid arbitrary variance."
  - [appendix b] Table 5 shows pixel-space models have higher error (NRMSE 0.81/0.96) compared to latent-space models (NRMSE 0.64/0.60) for the same architecture.
  - [corpus] "Scale-Adaptive Generative Flows..." supports the difficulty of multiscale data, implying compression/normalization is beneficial.
- **Break condition:** Fails if the compression ratio is too aggressive (e.g., 256×), destroying essential information, though the paper notes rollout error is robust to moderate compression changes.

## Foundational Learning

- **Concept: Probability Flow ODE vs. SDE**
  - **Why needed here:** The paper explicitly distinguishes between deterministic sampling (SI-E) for accuracy and stochastic sampling (SI-EM) for spectral fidelity. Understanding how a diffusion process can be written as an ODE (deterministic) or SDE (stochastic) is required to interpret the results in Section 4.
  - **Quick check question:** Can you explain why adding noise (Brownian motion) to a sampling path helps recover high-frequency details (spectral consistency) even if it hurts the exact pointwise match to the ground truth?

- **Concept: Fourier Spectrum / Power Spectrum**
  - **Why needed here:** The paper evaluates performance using "Spectral RMSE" (SRMSE) across low/mid/high frequency bands. Turbulence (Kolmogorov Flow) is defined by its energy cascade, which deterministic models often fail to capture (spectral blurring).
  - **Quick check question:** If a model has low RMSE but high Spectral RMSE in the high-frequency band, what does that visually imply about the prediction (e.g., is it too smooth)?

- **Concept: Autoregressive Rollout**
  - **Why needed here:** The models predict $u(t+1)$ given $u(t)$. To forecast to $t=20$, the model feeds its own prediction back as input. The paper analyzes "drift" and error accumulation over time (Figure 5).
  - **Quick check question:** Why might a small error in predicting $u(t+1)$ compound into a large error by $u(t+10)$, and how does the "stochastic" nature of the interpolant potentially mitigate this compared to a deterministic solver?

## Architecture Onboarding

- **Component map:** Autoencoder (DCAE) -> Conditioning (current state + noised future) -> Backbone (DiT with AdaLN) -> Interpolator -> Drift Network $b_\theta$
- **Critical path:**
  1. Train Autoencoder (MSE reconstruction + saturation/KL)
  2. Freeze Autoencoder; generate latent dataset
  3. Train Drift Network (DiT) using the Interpolant Loss (Eq 3) on latent pairs $(z_0, z_1)$
  4. Inference: Start at $z_0$, solve ODE/SDE using $b_\theta$ to get $z_1$, decode to pixel space
- **Design tradeoffs:**
  - **SI-E (ODE) vs. SI-EM (SDE):** Use SI-E for deterministic systems or when pointwise accuracy is paramount. Use SI-EM for turbulent/chaotic systems where statistical "realism" (spectrum) matters more than matching the exact trajectory of a specific realization.
  - **Sampling Steps:** The paper achieves strong results with 2-5 steps for SI, whereas DDPM needs 100. Prioritize SI for efficiency.
- **Failure signatures:**
  - **Smoothing/Blurring:** If the output looks "fuzzy" or lacks fine vortices, the model is likely using an ODE sampler on a chaotic system (over-deterministic) or $\gamma(t)$ is too low.
  - **Divergence/NaNs:** Check the noise schedule $\gamma(t)$ boundaries ($\gamma(0)=\gamma(1)=0$). If using the SDE sampler without antithetic sampling or proper handling of singularities at $t=0,1$, loss variance may explode.
  - **Latent Variance Explosion:** If the autoencoder isn't regularized (saturation/KL), the latent space may have regions of high variance where the drift network fails to converge.
- **First 3 experiments:**
  1. **Sanity Check (Kolmogorov):** Train the SI model on Kolmogorov Flow. Compare VRMSE of SI (2 steps) vs. FNO. Verify that SI achieves $\sim$0.55 VRMSE as per Table 1.
  2. **Sampler Ablation (RBC):** On Rayleigh-Bénard Convection, run inference with both SI-E (ODE) and SI-EM (SDE). Visualize the difference: SI-E should look smoother; SI-EM should show "grainier" turbulent structures. Calculate SRMSE to confirm the trade-off.
  3. **Step Efficiency:** Plot VRMSE vs. NFE (Number of Function Evaluations) for SI vs. DDIM. Verify that SI performance saturates quickly (at ~5 steps) while DDIM requires significantly more steps to converge.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can generative models be trained to simultaneously achieve both high pointwise accuracy and spectral consistency in turbulent systems?
- **Basis in paper:** [explicit] Appendix A.1 states: "Perhaps it is still an open question as to what the desired behavior should be in turbulence modeling or if we can train models to accomplish both point-wise and spectral accuracy."
- **Why unresolved:** The RBC experiments reveal a fundamental trade-off: lower VRMSE typically results in higher SRMSE, as minimizing pointwise error pushes models toward overly smoothed predictions, while stochasticity improves spectra but degrades pointwise accuracy.
- **What evidence would resolve it:** A model architecture or training objective that achieves both low VRMSE (<0.3) and low SRMSE (<0.3) across all frequency bands on the RBC dataset would demonstrate simultaneous optimization is possible.

### Open Question 2
- **Question:** How can the noise coefficient σ in stochastic interpolants be optimally selected for different physical systems without extensive empirical tuning?
- **Basis in paper:** [inferred] The ablation in Table 9 shows σ=1.0 works best for Kolmogorov Flow, but the paper provides no principled method for selecting this hyperparameter across different systems.
- **Why unresolved:** The optimal noise level likely depends on the distance between source and target distributions, the chaoticity of the system, and the desired balance between accuracy and spectral fidelity, but these relationships are not characterized.
- **What evidence would resolve it:** A theoretical or empirical relationship linking σ to measurable properties of the physical system (e.g., Reynolds number, Lyapunov exponent, or distributional distance metrics) that predicts optimal values across multiple PDE systems.

### Open Question 3
- **Question:** What training strategies or architectural modifications can ensure long-term climate consistency while maintaining medium-range forecast accuracy?
- **Basis in paper:** [explicit] "Future work can perhaps mitigate model biases or find better training strategies to ensure long-term consistency."
- **Why unresolved:** The paper shows 10-day forecast accuracy does not correlate with 10-year climatological biases, and no single model excels across all variables—smoother fields favor deterministic samplers while high-frequency fields benefit from stochasticity.
- **What evidence would resolve it:** A training procedure or model that achieves both competitive 10-day lRMSE and uniformly low climatological biases across all variables without requiring variable-specific sampler selection.

## Limitations
- The performance gains rely on statistical proximity between consecutive physical states, which may not hold for highly chaotic or sparsely sampled systems where $u(t)$ and $u(t+1)$ become decorrelated
- While latent-space compression stabilizes training, the 64× compression ratio may not generalize to systems requiring finer spatial resolution
- The fundamental trade-off between pointwise accuracy and spectral consistency remains unresolved, with no demonstrated method to achieve both simultaneously

## Confidence
- **High Confidence:** The mechanism that SI reduces integration steps by learning drift between proximate distributions is well-supported by the sliced Wasserstein/MMD distance heuristics (Figure 2) and the consistent VRMSE improvements across all three physical systems
- **Medium Confidence:** The trade-off between ODE (deterministic) and SDE (stochastic) sampling is empirically demonstrated but lacks theoretical grounding for when each is optimal
- **Medium Confidence:** The latent compression + saturation function approach is effective for the tested systems, but the robustness to extreme compression ratios or different physical quantities is not explored

## Next Checks
1. **Proximity Decay Test:** Systematically measure the statistical distance $D(u(t), u(t+1))$ across different physical regimes (laminar to fully turbulent) to establish when the SI advantage breaks down
2. **Compression Sensitivity Analysis:** Vary the autoencoder compression ratio from 16× to 128× and measure rollout error accumulation to identify the optimal trade-off between stability and information retention
3. **Chaotic System Benchmark:** Test SI on a chaotic PDE (e.g., Kuramoto-Sivashinsky) where $u(t)$ and $u(t+1)$ have high decorrelation to validate whether the proximity assumption still provides benefits or if standard diffusion models become competitive