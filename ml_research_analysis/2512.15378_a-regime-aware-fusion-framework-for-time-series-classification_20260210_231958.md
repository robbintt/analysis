---
ver: rpa2
title: A Regime-Aware Fusion Framework for Time Series Classification
arxiv_id: '2512.15378'
source_url: https://arxiv.org/abs/2512.15378
tags:
- rocket
- fusion
- datasets
- time
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of time series classification
  (TSC) by combining complementary representations through adaptive fusion. The authors
  introduce Fusion-3 (F3), a lightweight framework that adaptively fuses Rocket, Sax,
  and Sfa representations, selected for their complementary coverage of time-domain
  (coarse vs fine) and frequency-domain structure.
---

# A Regime-Aware Fusion Framework for Time Series Classification

## Quick Facts
- arXiv ID: 2512.15378
- Source URL: https://arxiv.org/abs/2512.15378
- Reference count: 26
- This paper introduces Fusion-3 (F3), a lightweight adaptive fusion framework that achieves consistent improvements over Rocket in time series classification by combining SAX, SFA, and Rocket representations.

## Executive Summary
This paper tackles the challenge of time series classification (TSC) by combining complementary representations through adaptive fusion. The authors introduce Fusion-3 (F3), a lightweight framework that adaptively fuses Rocket, Sax, and Sfa representations, selected for their complementary coverage of time-domain (coarse vs fine) and frequency-domain structure. To understand when fusion helps, they cluster 113 UCR datasets into six interpretable regimes using meta-features capturing series length, spectral structure, roughness, and class imbalance. The core method is a gated neural architecture that learns instance-specific weights for each representation, enabling selective prioritisation.

F3 achieves consistent improvements over Rocket: mean accuracy of 91.98% vs 91.47%, with small but reliable gains (HL-median +0.43pp, Wilcoxon p<10⁻⁴). Gains are regime-dependent: strongest in HighImb (imbalance-dominated), SmoothSep (clean separation), and HighFlCx (frequency-complex) regimes. Ablation studies show two-way fusions capture much of the gain at lower cost. SHAP attribution links improvements to spectral complexity and series length. Sample-level case studies reveal fusion primarily rescues specific errors, with adaptive gate weight increases on frequency-domain SFA where corrections occur. The work demonstrates that selective, interpretable fusion provides dependable extension to strong kernel-based methods, correcting weaknesses precisely where data support it.

## Method Summary
The regime-aware fusion framework combines three complementary representations (SAX, SFA, Rocket) through adaptive gating. SAX provides coarse time-domain symbolization, SFA offers frequency-domain symbolization, and Rocket captures fine-grained local shapes through random convolutional kernels. The method extracts ~4000-dimensional features from each representation, projects them to dense embeddings via fully connected layers, and combines them using instance-specific weights learned by a gating network. The framework uses 13 meta-features to cluster 113 UCR datasets into six regimes, enabling regime-aware analysis of fusion performance. Training employs Adam optimizer with grid search over hyperparameters including embedding dimensions and head capacity.

## Key Results
- F3 achieves mean accuracy of 91.98% vs Rocket's 91.47%, with HL-median gain of +0.43pp (Wilcoxon p<10⁻⁴)
- Gains are regime-dependent: strongest in HighImb (+1.48pp), SmoothSep (+1.09pp), and HighFlCx (+1.09pp) regimes
- Two-way ablation F2_SR captures ~73% of F3's gain at 6× lower computational cost
- SHAP attribution links fusion gains to spectral complexity and series length; turning points and permutation entropy negatively associated with gains

## Why This Works (Mechanism)

### Mechanism 1
Adaptive gating selectively upweights frequency-domain representations where spectral structure is discriminative. The gating network learns instance-specific weights (g_SAX, g_SFA, g_ROCKET ∈ [0,1], Σg = 1) based on concatenated embeddings. When a sample exhibits frequency-domain structure that ROCKET alone misses, the gate increases SFA weight precisely on those samples.

Core assumption: The three representations provide genuinely complementary information; their embedding spaces can be meaningfully compared.

Evidence anchors:
- [abstract] "fusion primarily improves performance by rescuing specific errors, with adaptive increases in frequency-domain weighting precisely where corrections occur"
- [Section 7.1, Table 6] Rock dataset shows +0.10 SFA gate shift on rescued samples; RefrigerationDevices shows +0.08 SFA shift with 83 rescued vs 48 hurt
- [corpus] Weak direct evidence on gating mechanisms; related papers focus on ROCKET variants rather than adaptive fusion

Break condition: When local irregularity dominates (high permutation entropy, high turning points), SFA upweighting correlates with negative outcomes—C5 (HighCompOut) shows hurt≈rescued with no systematic gate benefit.

### Mechanism 2
Regime-dependent gains emerge from predictable interactions between meta-features and representation complementarity. Meta-features (spectral entropy, series length, DTW separability, imbalance) cluster datasets into six regimes. SHAP attribution links fusion gains to spectral complexity and length; regimes with rich frequency content (C4 HighFlCx) or structured separability (C3 SmoothSep) show the largest gains.

Core assumption: Dataset-level meta-features are stable proxies for sample-level representation utility.

Evidence anchors:
- [Section 6, Figure 4] SHAP shows ts_length, global_kl_psd, spectral_entropy_var positively associated with gains; turning_points and permutation_entropy negatively associated
- [Section 5.3, Figure 3] Heatmap shows coherent alignment: C4 has highest spectral measures, highest SFA gate weight, largest HL-median gain (1.09pp)
- [corpus] No direct corpus evidence on meta-feature-to-regime mapping; this appears novel to this framework

Break condition: C5 (HighCompOut) violates the pattern—high spectral entropy but negative gains (−0.72pp), suggesting extreme irregularity disrupts all representations.

### Mechanism 3
ROCKET functions as an essential backbone; fusion succeeds by correcting its residual errors rather than replacing it. Two-way ablation F2_SS (SAX+SFA without ROCKET) collapses to 83.46% accuracy (−8.01pp vs ROCKET). Both F2_SR and F2_SFR capture most F3 gains at lower cost, indicating that adding any single complementary representation to ROCKET yields diminishing incremental returns.

Core assumption: ROCKET's random convolutional features provide broad coverage that symbolic methods cannot replicate alone.

Evidence anchors:
- [Section 7, Table 4] F2_SS wins only 12/113 datasets (26.7% win-rate); mean accuracy drops 8.01pp
- [Section 5.3] Gate weights are ROCKET-heavy across all regimes (typically 0.4–0.6), even where fusion helps most
- [corpus] Consistent with corpus—related papers (CUROCKET, SPROCKET) treat ROCKET as foundation and extend rather than replace it

Break condition: If ROCKET accuracy is already >95% (near-ceiling datasets), fusion has little room to improve and may introduce noise—C6 (ShortBase) shows marginal +0.16pp gain.

## Foundational Learning

- Concept: ROCKET (Random Convolutional Kernel Transform)
  - Why needed here: This is the backbone representation; understanding its strengths (fine-grained local shapes, speed) and weaknesses (frequency-blind on some structures) motivates the fusion approach.
  - Quick check question: If you apply 10,000 random kernels of varying lengths/dilations and pool via max/PPV, what types of patterns will you capture vs. miss?

- Concept: SAX vs. SFA (Symbolic Aggregate Approximation vs. Symbolic Fourier Approximation)
  - Why needed here: These are the complementary representations; SAX provides coarse time-domain symbolization, SFA provides frequency-domain symbolization. Their failure modes differ from ROCKET's.
  - Quick check question: Given a series with strong periodic structure but noisy local fluctuations, which representation would likely capture the discriminative signal more robustly?

- Concept: Gated fusion with softmax normalization
  - Why needed here: The adaptive weighting mechanism; understanding how the gate learns instance-specific weights from concatenated embeddings explains both the interpretability and the correction mechanism.
  - Quick check question: If gate weights are constrained to sum to 1, what happens when one representation is uniformly superior across all samples in a dataset?

## Architecture Onboarding

- Component map: Input layer -> Parallel representation extractors (SAX, SFA, ROCKET) -> Embedding projectors (FC + ReLU) -> Gating network (sigmoid + normalize) -> Weighted fusion -> MLP classifier

- Critical path: Representation extraction (embarrassingly parallel) -> embedding projection -> gating -> weighted fusion -> classification. The gate weights are the interpretability surface.

- Design tradeoffs:
  - F3 (three-way) vs. F2 (two-way): F3 captures all complementary effects but requires 6× more hyperparameter configs; F2_SR achieves ~73% of F3's gain at 6× lower search cost.
  - Embedding dimension (64 vs. 128): Paper uses grid over both; pilots showed <1pp median gain difference but 2-3× runtime for larger dims.
  - Head capacity: Fixed across all models to prevent confounding; increasing capacity may mask representation quality differences.

- Failure signatures:
  - C5 (HighCompOut) pattern: High permutation entropy + high kurtosis + low separability -> negative fusion gains. If your dataset has these meta-features, use ROCKET alone.
  - Near-ceiling failure: If baseline accuracy >95%, fusion gains are marginal (C6: +0.16pp) and may not justify complexity.
  - SAX+SFA collapse: If ROCKET is removed, accuracy drops catastrophically (-8pp); always validate with ROCKET backbone.

- First 3 experiments:
  1. Replicate the regime clustering on your target datasets: compute the 13 meta-features, project to 2D (t-SNE/UMAP), identify which regime(s) your data fall into. Use Table 1 characteristics as a sanity check.
  2. Run F2_SR vs. ROCKET baseline on a subset: this two-way fusion (SAX+ROCKET) is computationally cheap (48 configs vs. 288 for F3) and captures ~25% of F3's average gain. If F2_SR shows no improvement, check if you're in C5 or near-ceiling conditions.
  3. Inspect gate weights on a per-sample basis for a representative dataset: identify rescued vs. hurt samples, plot gate weight distributions. If SFA weight increases on rescued samples, the mechanism is working as intended; if gate weights are uniform or hurt samples have high SFA/SAX weight, your data may violate the complementarity assumption.

## Open Questions the Paper Calls Out

### Open Question 1
Can the regime-aware fusion framework be extended effectively to multivariate time series classification, and how should the fusion mechanism account for inter-channel dependencies? Basis: The conclusion states future work includes "extensions to multivariate and irregular time series." Why unresolved: All experiments are restricted to univariate UCR datasets; the gating architecture and meta-features do not capture cross-channel structure. What evidence would resolve it: Evaluation on multivariate benchmarks (e.g., UEA archive) with extended meta-features for channel correlation and modified fusion to handle per-channel or joint representations.

### Open Question 2
Can automatic regime prediction enable zero-shot model selection without requiring a priori clustering of benchmark datasets? Basis: The conclusion lists "automatic regime prediction for zero-shot model selection" as future work. Why unresolved: The current approach clusters datasets retrospectively using meta-features computed from the full dataset; predicting regimes for unseen datasets without re-clustering remains unaddressed. What evidence would resolve it: A supervised classifier trained on regime labels that accurately predicts the appropriate model (F3, F2, or ROCKET) for held-out datasets based solely on their meta-features.

### Open Question 3
What data properties definitively explain why fusion degrades performance in the HighCompOut (high complexity, outlier-rich) regime? Basis: The paper reports negative gains in C5 (HL −0.72 pp) and SHAP links this to high permutation entropy and turning points, but the small sample (n=5) prevents strong conclusions. Why unresolved: The regime has only 5 datasets, and the mechanism—whether failure stems from SFA brittleness, SAX instability, or gate miscalibration—is not isolated. What evidence would resolve it: Targeted experiments on synthetic datasets with controlled outlier rates and irregularity levels, plus ablation of gate constraints in this regime.

## Limitations

- Framework requires ROCKET as backbone; two-way ablation without ROCKET collapses accuracy by ~8pp
- Gains are regime-dependent; C5 (HighCompOut) and near-ceiling datasets show no reliable improvement
- Limited evaluation to univariate data; multivariate extension requires non-trivial modifications to SAX/SFA representations

## Confidence

- High: Adaptive gating mechanism (well-validated through rescued/hurt sample analysis and gate weight shifts)
- Medium: Regime predictions (clustering is interpretable but meta-feature stability across domains is untested)
- Low: Claims about transferability to multivariate time series (framework evaluated only on univariate data)

## Next Checks

1. **Transfer to Multivariate Data**: Evaluate whether regime-aware fusion extends to multivariate series, potentially requiring modified SAX/SFA representations or kernel adaptations.
2. **Meta-feature Stability**: Test whether the 13 meta-features remain discriminative when applied to non-UCR datasets (e.g., medical or industrial time series with different statistical properties).
3. **Gate Interpretability Validation**: Cross-validate gate weight explanations using a held-out probe task (e.g., predict meta-feature clusters from gate weights) to confirm they encode meaningful representation utility.