---
ver: rpa2
title: Training Dynamics of Parametric and In-Context Knowledge Utilization in Language
  Models
arxiv_id: '2510.02370'
source_url: https://arxiv.org/abs/2510.02370
tags:
- knowledge
- training
- parametric
- in-context
- utilization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how training data properties shape the
  ability of language models to utilize both parametric knowledge (stored in model
  parameters) and in-context knowledge (provided at inference). Through controlled
  experiments with synthetic biographies, the authors find that three seemingly detrimental
  data properties - intra-document repetition, factual inconsistency, and skewed knowledge
  frequency distributions - must co-occur to enable robust knowledge utilization and
  conflict resolution.
---

# Training Dynamics of Parametric and In-Context Knowledge Utilization in Language Models

## Quick Facts
- arXiv ID: 2510.02370
- Source URL: https://arxiv.org/abs/2510.02370
- Reference count: 40
- This paper investigates how training data properties shape the ability of language models to utilize both parametric knowledge (stored in model parameters) and in-context knowledge (provided at inference).

## Executive Summary
This paper investigates how training data properties shape the ability of language models to utilize both parametric knowledge (stored in model parameters) and in-context knowledge (provided at inference). Through controlled experiments with synthetic biographies, the authors find that three seemingly detrimental data properties - intra-document repetition, factual inconsistency, and skewed knowledge frequency distributions - must co-occur to enable robust knowledge utilization and conflict resolution. Intra-document repetition enables co-emergence of both capabilities, factual inconsistency prevents over-reliance on context by introducing reliability ceilings, and skewed distributions maintain balanced reliance by ensuring rare facts require in-context knowledge. These findings offer concrete guidance for training models that harmoniously integrate parametric and in-context knowledge, challenging the conventional wisdom that data cleaning and balancing always improve model performance. The results are validated on real-world models including Pythia and OLMo, showing similar training dynamics.

## Method Summary
The paper employs controlled experiments with synthetic biographies to study training dynamics. A 8-layer decoder-only Transformer (512d, 8 heads) is trained on biographies containing 4 attributes (birth_date, birth_city, university, major) for 50k entities over 16k steps. Three corpus variants are tested: SINGLE (one paragraph per entity), REPEATED (two paraphrased paragraphs with 3 entities shuffled per document), and REPEATED with factual inconsistency (1-10% noise) and Zipfian(α=1.0) distribution. Models are evaluated every 100 steps on held-out templates measuring Parametric Knowledge Utilization (PKU), In-Context Knowledge Utilization (ICKU), and conflict resolution preferences. Findings are validated on Pythia and OLMo checkpoints using country-capital probes.

## Key Results
- Three data properties - intra-document repetition, factual inconsistency, and skewed distributions - must co-occur to enable robust knowledge utilization and conflict resolution
- Intra-document repetition enables co-emergence of both parametric and in-context capabilities during next-token prediction
- Factual inconsistency (1%) introduces reliability ceilings that prevent over-reliance on context and enable preference transitions from ICK to PK as training progresses
- Skewed (Zipfian, α=1.0) distributions preserve in-context utilization for rare entities while enabling frequency-dependent arbitration

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Intra-document repetition is necessary for the co-emergence of parametric and in-context knowledge utilization capabilities.
- **Mechanism:** During next-token prediction, the first mention of a fact within a document requires parametric recall from model weights. Later mentions of the same fact can be predicted by attending to earlier context tokens. This dual signaling trains both retrieval pathways simultaneously, with in-context utilization emerging first (copying is structurally simpler) and parametric utilization following (requires binding entity-specific knowledge to recall mechanisms).
- **Core assumption:** The model cannot trivially copy based on attribute patterns alone—multiple entities must be mixed within documents to force disambiguation by subject identity.
- **Evidence anchors:**
  - [abstract] "intra-document repetition of facts fosters the development of both parametric and in-context capabilities"
  - [section 3.1] SINGLE develops only PK; REPEATED develops both; "in-context knowledge utilization emerges earlier than parametric knowledge utilization"
  - [corpus] Related work (Zucchet et al., 2025) confirms parametric recall requires gradual entity-knowledge binding.
- **Break condition:** If documents contain only single mentions per fact (SINGLE condition), in-context utilization circuits never receive training signal and only parametric knowledge develops.

### Mechanism 2
- **Claim:** Moderate within-document factual inconsistency induces a preference transition from in-context to parametric knowledge as training progresses.
- **Mechanism:** Inconsistency noise (even 1%) establishes a reliability ceiling for context-based predictions. Once parametric accuracy exceeds this ceiling for high-frequency entities, the loss gradient increasingly favors parametric retrieval. Over time, in-context circuits receive diminishing training signal and degrade via underuse plus weight decay regularization.
- **Core assumption:** The model learns reliability estimates for different knowledge sources through gradient feedback, not explicit meta-learning.
- **Evidence anchors:**
  - [abstract] "factual inconsistency prevents over-reliance on context by introducing reliability ceilings"
  - [section 3.2, Figure 4] "With 1% inconsistency, preference shifts from in-context knowledge to parametric knowledge as training progresses"
  - [appendix E, Figure 12] Attention analysis shows progressive shift from context tokens to name tokens even on unseen entities, indicating circuit degradation.
  - [corpus] Weak direct evidence for circuit degradation mechanism—primarily inferential from attention patterns.
- **Break condition:** If inconsistency is too high (>5%), in-context utilization degrades severely at convergence; if zero, model over-relies on context regardless of parametric confidence.

### Mechanism 3
- **Claim:** Skewed (Zipfian) knowledge frequency distribution preserves in-context utilization capabilities while enabling frequency-dependent arbitration.
- **Mechanism:** Long-tailed distributions ensure that rare entities remain parametrically unfamiliar throughout training, forcing continued reliance on in-context circuits. High-frequency entities transition to parametric preference; low-frequency entities maintain in-context preference. This preserves balanced capability without requiring the full distribution to be learned parametrically.
- **Core assumption:** Zipf exponent α ≈ 1.0 is approximately optimal—uniform (α=0.5) causes excessive degradation; highly skewed (α=2.0) prevents parametric learning.
- **Evidence anchors:**
  - [abstract] "skewed distributions maintain balanced reliance by ensuring rare facts require in-context knowledge"
  - [section 3.3, Table 1] Zipfian sampling with 1% noise achieves 84.0% ICK accuracy vs. 31.5% for uniform at same noise level
  - [section 3.3, Figure 5] Top 5% entities shift to parametric preference; bottom 5% maintain in-context preference
  - [corpus] No direct corpus validation of the specific α=1.0 optimum—extrapolate from synthetic experiments.
- **Break condition:** Skewed distribution alone (without inconsistency) produces low parametric preference across all confidence levels; all three factors must co-occur.

## Foundational Learning

- **Concept: Parametric vs. In-Context Knowledge**
  - Why needed here: The entire paper concerns how models arbitrate between knowledge stored in weights (seen during training) vs. knowledge provided in the prompt context (unseen entities).
  - Quick check question: Given a trained entity with no context provided, can the model recall its attributes? That's parametric. Given an unseen entity with context, can the model extract attributes? That's in-context.

- **Concept: Next-Token Prediction as Implicit Multi-Task Learning**
  - Why needed here: The paper's central puzzle is how a single objective (predict next token) yields two distinct capabilities and arbitration between them without explicit supervision.
  - Quick check question: When a document repeats a fact, the first prediction must use stored knowledge while the second can attend to context—how does the model learn which strategy to use when?

- **Concept: Confidence-Dependent Arbitration**
  - Why needed here: Real-world models prefer parametric knowledge for high-confidence facts and in-context knowledge for unfamiliar facts—the paper seeks to explain how this calibration emerges.
  - Quick check question: If a model has 0.99 probability on a parametric answer but context contradicts it, which should it choose? The answer depends on training-derived calibration.

## Architecture Onboarding

- **Component map:**
  Training Corpus → [Repetition × Inconsistency × Skew] → Transformer (8L, 512d, 8 heads) → Evaluation Protocol (PKU, ICKU, Conflict)

- **Critical path:**
  1. Construct corpus variant controlling exactly one property at a time
  2. Train from scratch with identical hyperparameters (4e-4 LR, 0.1 weight decay, 16k steps)
  3. Evaluate all three metrics every 100 steps on held-out templates (ensures no memorization shortcut)
  4. Validate synthetic findings on real-world checkpoints (Pythia, OLMo) using country-capital probes

- **Design tradeoffs:**
  - **Synthetic vs. Real:** Synthetic enables causal isolation but may not capture full natural language complexity. Paper validates on Pythia/OLMo to mitigate this.
  - **Entity count:** 50k entities work; 100k+ prevents robust parametric learning (insufficient repetition per entity).
  - **Noise level:** 1% optimal for balance; 5-10% causes excessive ICK degradation.
  - **Assumption:** Biographies are a sufficient proxy for factual knowledge more broadly.

- **Failure signatures:**
  - Model achieves high PKU but near-zero ICKU → likely trained on SINGLE corpus (no intra-document repetition)
  - Model always prefers context under conflict regardless of confidence → likely trained on REPEATED without inconsistency
  - ICKU degrades to near-zero at convergence → either uniform distribution or excessive noise (>5%)
  - PKU never rises above chance → either too many entities (insufficient exposure) or excessive skew (α≥2.0)

- **First 3 experiments:**
  1. **Reproduction check:** Train on REPEATED with 1% noise and Zipfian (α=1.0) for 16k steps. Verify: (a) ICKU emerges first, (b) PKU follows, (c) preference shifts from ICK to PK, (d) ICKU preserved at convergence (~60-80%).
  2. **Ablation sweep:** Run three conditions—SINGLE, REPEATED (no noise), REPEATED (1% noise + uniform)—and confirm each breaks a different capability as predicted.
  3. **Post-training intervention:** Take a converged model with balanced capabilities; fine-tune on 1000 entities with (a) clean data, (b) 5% noise. Measure whether arbitration preferences shift as predicted (clean → more ICK reliance; noisy → more PK reliance for high-confidence facts).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do models trained under these conditions handle partial relevance and multi-document conflicts, as opposed to the binary single-document conflicts studied here?
- Basis in paper: [explicit] Limitations section states: "Real-world retrieval-augmented generation involves diverse scenarios such as retrieval errors, partial relevance, and multi-document conflicts, which we leave for future investigation."
- Why unresolved: The controlled experiments only test conflicts where a single context contradicts parametric knowledge, but RAG systems often retrieve multiple documents with varying relevance and conflicting information.
- What evidence would resolve it: Experiments extending the framework to include multiple retrieved documents, partially relevant contexts, and graded conflict scenarios.

### Open Question 2
- Question: What are the optimal levels of factual inconsistency and distributional skew for different downstream tasks and knowledge domains?
- Basis in paper: [explicit] Appendix G shows varying noise levels (1%, 5%, 10%) and skewness parameters (α = 0.5, 1.0, 2.0) produce different outcomes, but the paper does not systematically characterize optimal settings.
- Why unresolved: The paper demonstrates that moderate levels work but doesn't establish whether optimal values depend on corpus size, knowledge type, or target application.
- What evidence would resolve it: Systematic sweep experiments across different knowledge domains (not just biographical facts) measuring downstream task performance to identify task-specific optimal parameters.

### Open Question 3
- Question: Can the relationship between model scale and the magnitude/timing of preference transitions be precisely characterized?
- Basis in paper: [inferred] Figure 7 shows larger models exhibit stronger parametric preference at convergence, but the mechanistic explanation is speculative ("larger models developing parametric knowledge more rapidly").
- Why unresolved: The paper observes the correlation but doesn't establish whether scaling laws govern the transition dynamics or whether architectural differences matter.
- What evidence would resolve it: Controlled experiments varying model scale while holding data properties constant, measuring training step at which preference transition occurs and final preference magnitude.

## Limitations

- **Synthetic Domain Transfer**: Core mechanisms derived from synthetic biographies may not capture full natural language complexity; optimal parameters for biographies may not generalize to diverse factual domains
- **Mechanism Specificity**: Circuit degradation for in-context utilization is primarily inferred from attention patterns rather than direct intervention studies
- **Training Regime Sensitivity**: Results depend on specific hyperparameters (50k entities, 16k steps, 1% noise, α=1.0 skew) that may not be optimal across different tasks

## Confidence

**High Confidence**: The empirical observations about training dynamics (ICK emerging before PK, preference transitions with noise, frequency-dependent arbitration with skew) are directly measured and reproducible.

**Medium Confidence**: The claim that all three properties must co-occur for robust knowledge utilization is strongly supported by controlled ablations but hasn't been exhaustively tested across all possible combinations.

**Low Confidence**: The assertion that α=1.0 is the optimal skew exponent and that 1% noise is universally optimal represents extrapolation from the synthetic domain.

## Next Checks

1. **Natural Language Ablations**: Create controlled natural language corpora (e.g., Wikipedia snippets about entities with similar attributes) and systematically ablate repetition, inconsistency, and skew to verify that the three-property requirement holds beyond synthetic biographies.

2. **Circuit Intervention Studies**: Design targeted interventions (e.g., freezing specific attention heads, controlled weight decay) during training to directly test whether in-context circuit degradation is caused by disuse versus representational drift or capacity competition.

3. **Cross-Domain Transfer**: Apply the REPEATED+1% noise+α=1.0 training recipe to models trained on different factual domains (e.g., geography facts, scientific relationships) and measure whether the same training dynamics and optimal parameters emerge, or whether domain-specific adjustments are needed.