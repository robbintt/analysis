---
ver: rpa2
title: 'VERBA: Verbalizing Model Differences Using Large Language Models'
arxiv_id: '2507.02241'
source_url: https://arxiv.org/abs/2507.02241
tags:
- dataset
- accmismatch
- accmatch
- differences
- level
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VERBA, a framework that leverages large language
  models to generate natural-language descriptions of differences between machine
  learning models. VERBA addresses the challenge of navigating and selecting among
  a proliferation of models with similar performance but different behaviors.
---

# VERBA: Verbalizing Model Differences Using Large Language Models

## Quick Facts
- arXiv ID: 2507.02241
- Source URL: https://arxiv.org/abs/2507.02241
- Reference count: 25
- Primary result: Framework using LLMs to generate natural language descriptions of differences between machine learning models, achieving up to 80% accuracy

## Executive Summary
VERBA is a framework that leverages large language models to generate natural-language descriptions of differences between machine learning models. It addresses the challenge of navigating and selecting among numerous models with similar performance but different behaviors. The framework samples input-output pairs from two models, serializes them, and uses an LLM to verbalize the differences. VERBA is evaluated on logistic regression, decision trees, and multilayer perceptrons using an LLM-based evaluation protocol, showing promising results for automated model management and comparison.

## Method Summary
VERBA operates by sampling input-output pairs from two machine learning models, serializing these pairs, and feeding them to a large language model to generate natural language descriptions of the differences between the models. The framework employs a systematic pairwise comparison approach to identify behavioral variations even when models have similar performance metrics. An LLM-based evaluation protocol is used to assess the quality and accuracy of the generated verbalizations, measuring both factual correctness and comprehensiveness of the differences described.

## Key Results
- VERBA achieves up to 80% overall accuracy in verbalizing model differences for model pairs with up to 5% performance difference but 20-25% behavioral differences
- Accuracy improves to 90% when structural information about models is included in the verbalization process
- The framework demonstrates effectiveness across logistic regression, decision trees, and multilayer perceptrons

## Why This Works (Mechanism)
VERBA works by leveraging the reasoning and pattern recognition capabilities of large language models to identify and articulate subtle differences between machine learning models. The framework systematically samples diverse input cases to expose behavioral variations, then uses the LLM's ability to process structured data (serialized input-output pairs) and generate coherent natural language explanations. This approach transforms the challenge of comparing complex model behaviors into a task that LLMs are particularly well-suited for, enabling automated and interpretable model comparison at scale.

## Foundational Learning
- **Model behavioral analysis**: Understanding how different models make decisions is crucial for identifying meaningful differences beyond simple performance metrics - quick check: examine decision boundaries for simple models
- **LLM-based evaluation protocols**: Using language models to assess the quality of generated descriptions requires careful design to ensure reliable and consistent evaluation - quick check: validate evaluation metrics with human judgments
- **Pairwise comparison methodology**: Systematic comparison between model pairs provides a structured approach to identifying differences but requires efficient scaling strategies - quick check: analyze computational complexity for n models

## Architecture Onboarding

**Component Map:** Input sampling -> Serialization -> LLM processing -> Difference generation -> LLM evaluation

**Critical Path:** The core workflow involves sampling diverse inputs from both models, serializing the input-output pairs into a structured format, processing through the LLM to generate difference descriptions, and evaluating the quality of these descriptions using another LLM-based assessment.

**Design Tradeoffs:** The framework trades computational efficiency for comprehensive behavioral analysis, using pairwise comparisons that scale quadratically with the number of models. The choice to rely on LLM-based evaluation introduces potential biases but provides scalability and consistency compared to human evaluation.

**Failure Signatures:** The system may struggle with highly complex model architectures where behavioral differences are subtle or difficult to verbalize. Poor sampling strategies could miss critical behavioral differences, and LLM biases could affect the quality of generated descriptions or evaluations.

**First Experiments:** 1) Test VERBA on a small set of logistic regression models with known decision boundary differences 2) Evaluate accuracy degradation when removing structural information from the input 3) Compare LLM-based evaluation scores with human expert assessments on a subset of model pairs

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation focuses on relatively simple model types (logistic regression, decision trees, and multilayer perceptrons), leaving uncertainty about performance with more complex architectures
- The pairwise comparison approach may not scale efficiently to large model collections, as the number of comparisons grows quadratically
- Reliance on LLM-based evaluation introduces potential biases from the specific model used and its training data

## Confidence
- **High**: The core methodology of using LLMs to verbalize model differences is sound and the reported accuracy metrics are likely reliable within the tested scope
- **Medium**: The generalizability to more complex model types and real-world applications
- **Medium**: The scalability of the pairwise comparison approach for larger model collections

## Next Checks
1. Test VERBA on transformer-based models and ensemble methods to evaluate generalizability beyond the current scope
2. Conduct ablation studies with different LLM models to assess robustness to model choice
3. Implement and evaluate a hierarchical comparison strategy to address scalability concerns with larger model collections