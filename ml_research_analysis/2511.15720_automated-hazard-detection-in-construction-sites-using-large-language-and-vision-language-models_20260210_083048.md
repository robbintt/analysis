---
ver: rpa2
title: Automated Hazard Detection in Construction Sites Using Large Language and Vision-Language
  Models
arxiv_id: '2511.15720'
source_url: https://arxiv.org/abs/2511.15720
tags:
- construction
- safety
- page
- hazard
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis proposes a multimodal AI framework combining large
  language models (LLMs) and vision-language models (VLMs) for automated construction
  safety hazard detection. The framework processes unstructured OSHA accident reports
  using GPT-4o-mini for classification and analysis, and analyzes construction site
  images using GPT-4o Vision and open-source VLMs (Molmo 7B and Qwen2 VL 2B) for hazard
  detection.
---

# Automated Hazard Detection in Construction Sites Using Large Language and Vision-Language Models

## Quick Facts
- arXiv ID: 2511.15720
- Source URL: https://arxiv.org/abs/2511.15720
- Reference count: 40
- Open-source VLMs achieved 72.6% F1 (Qwen2 VL 2B) and 67.2% F1 (Molmo 7B) on safety rule detection tasks.

## Executive Summary
This thesis presents a multimodal AI framework that combines large language models (LLMs) and vision-language models (VLMs) for automated construction safety hazard detection. The framework processes unstructured OSHA accident reports using GPT-4o-mini for classification and analysis, while analyzing construction site images using GPT-4o Vision and open-source VLMs (Molmo 7B and Qwen2 VL 2B) for hazard detection. The approach achieved 89% accuracy in classifying 100 OSHA reports into 43 accident categories and successfully identified safety hazards like falls and PPE violations in construction site images. The study validates the feasibility of cost-effective, lightweight models for construction safety monitoring while demonstrating competitive performance compared to larger commercial models.

## Method Summary
The framework employs a dual-pipeline approach: a textual pipeline using GPT-4o-mini to classify and analyze OSHA accident reports, and a visual pipeline using both commercial (GPT-4o Vision) and open-source VLMs (Molmo 7B, Qwen2 VL 2B) for hazard detection. The visual pipeline uses chain-of-thought prompting with four sequential stages: scene description, accident scenario prediction, hazard filtering, and localization. Open-source models employ semantic prompt ensembles with majority voting across 10 semantically equivalent prompts to stabilize outputs. The system processes images locally on GPU hardware, achieving real-time inference capabilities while maintaining competitive accuracy against larger commercial models.

## Key Results
- Textual pipeline achieved 89% accuracy in classifying 100 OSHA reports into 43 accident categories
- Qwen2 VL 2B achieved 72.6% F1 score and Molmo 7B achieved 67.2% F1 score on safety rule detection tasks
- Semantic prompting improved Qwen2-VL-2B recall from 67.3% to 98.0% and Molmo-7B recall from 48.0% to 79.6% using 10-prompt ensembles

## Why This Works (Mechanism)

### Mechanism 1
Chain-of-thought (CoT) prompting improves hazard detection by decomposing visual reasoning into sequential stages rather than direct classification. The visual pipeline uses four chained prompts—scene description → accident scenario prediction → hazard filtering → localization. Each step conditions the next, reducing reasoning errors through intermediate verification. Core assumption: Hazards emerge from spatial and contextual relationships that require multi-step inference. Evidence: The combined reasoning from text and image analysis enabled contextual safety assessments that mirror real-world site evaluations. Break condition: If prompt dependencies introduce compounding errors, performance degrades; single-prompt baselines may outperform in low-complexity scenes.

### Mechanism 2
Semantic prompt ensembles with majority voting stabilize small VLM outputs by mitigating prompt sensitivity. Each image is queried with 10 semantically equivalent prompts. Majority voting aggregates responses, reducing variance from wording-dependent outputs. Core assumption: Small models (2–7B parameters) exhibit higher output variance across phrasings than large models, and semantic equivalence preserves task intent. Evidence: Qwen2-VL-2B recall improved from 67.3% (single prompt) to 98.0% (10 prompts); Molmo-7B recall improved from 48.0% to 79.6%. Break condition: If prompts introduce semantic drift, voting amplifies noise rather than reducing it.

### Mechanism 3
Pre-trained VLMs generalize to construction safety tasks without fine-tuning when prompts align model capacity with task complexity. Larger models respond better to descriptive, context-rich prompts; smaller models perform best with concise, single-rule prompts. No domain-specific training is required. Core assumption: General-purpose pre-training captures sufficient visual-semantic representations for construction hazards when appropriately prompted. Evidence: Despite their smaller size, Molmo 7B and Quen2 VL 2B showed competitive performance, reinforcing the feasibility of low-resource multimodal systems. Break condition: If construction images contain novel hazards underrepresented in pre-training data, zero-shot detection fails.

## Foundational Learning

- **Vision-Language Model (VLM) capabilities:**
  - Why needed here: VLMs jointly process images and text, enabling hazard reasoning beyond object detection.
  - Quick check question: Can you explain how a VLM differs from a standard CNN-based object detector in terms of output flexibility?

- **Prompt engineering (CoT and semantic ensembles):**
  - Why needed here: Performance hinges on prompt design; different strategies suit different model scales.
  - Quick check question: What happens to small-model recall if you use a long, multi-rule prompt instead of a concise single-rule prompt?

- **Construction safety taxonomy (OSHA Fatal Four, PPE rules):**
  - Why needed here: The 43-category taxonomy grounds model outputs in domain-specific hazard classes.
  - Quick check question: Name three hazard categories from OSHA's "Fatal Four" and their visual indicators.

## Architecture Onboarding

- **Component map:**
  - Textual pipeline: Selenium scraper → PDF parser (pdfplumber) → GPT-4o-mini (structured extraction + 43-category classification) → JSON output
  - Visual pipeline (commercial): GPT-4o Vision → CoT prompts (4 stages) → hazard annotations with bounding boxes
  - Visual pipeline (open-source): Molmo-7B / Qwen2-VL-2B (local inference on T4 GPU) → 10-prompt ensemble → majority voting → binary compliance output

- **Critical path:** Textual pipeline accuracy (89%) depends on report formatting consistency; visual pipeline depends on prompt design and model selection aligned with task complexity.

- **Design tradeoffs:**
  - GPT-4o Vision: Higher capability but API cost and rate limits; suitable for detailed scene reasoning.
  - Molmo-7B / Qwen2-VL-2B: No marginal cost after setup, but lower precision and higher prompt sensitivity; best for focused rule-checking.
  - CoT prompting: Improves interpretability but increases latency and token usage.

- **Failure signatures:**
  - Textual: Misclassification on irregularly formatted reports (tables, non-standard layouts).
  - Visual: Hallucinated hazards in cluttered scenes; bounding box misalignment without IoU ground truth.
  - Small models: High recall but low precision with ensemble prompting (over-flagging violations).

- **First 3 experiments:**
  1. Replicate the PPE compliance task using Qwen2-VL-2B with single-prompt vs. 10-prompt ensemble on 50 images; compare precision/recall.
  2. Test the textual pipeline on 20 OSHA reports with non-standard formatting; measure extraction error rates.
  3. Run GPT-4o Vision on 5 construction images using full CoT pipeline vs. single-step direct hazard detection; qualitatively compare annotation coherence.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a unified multimodal architecture that simultaneously processes textual safety regulations and visual site data outperform the current sequential pipeline approach?
- Basis in paper: The conclusion states, "Future work may explore... combining textual and visual input into a unified multi-modal safety monitoring system."
- Why unresolved: The current framework treats text and vision as separate, sequential pipelines rather than a single integrated model processing both streams simultaneously.
- What evidence would resolve it: A comparative study benchmarking the current sequential framework against a fused-architecture model on identical hazard detection tasks to measure differences in accuracy and inference speed.

### Open Question 2
- Question: To what extent do lightweight Vision-Language Models (VLMs) generalize to diverse, real-world construction site conditions (e.g., extreme lighting, occlusions) without domain-specific fine-tuning?
- Basis in paper: The conclusion suggests "expanding evaluation across diverse site conditions," and the limitations section notes that "real-time performance was not evaluated under field conditions."
- Why unresolved: The study relied on specific datasets and a small sample of OSHA reports, which may not capture the full environmental variability of active construction sites.
- What evidence would resolve it: Performance metrics derived from testing the models on a new dataset specifically curated to include adverse environmental conditions and occlusions.

### Open Question 3
- Question: Is it feasible to deploy open-source VLMs like Molmo 7B and Qwen2 VL 2B on edge devices for real-time safety monitoring without significant loss in detection accuracy?
- Basis in paper: The conclusion proposes, "Further studies could assess the feasibility of deployment on edge devices or mobile safety tools."
- Why unresolved: While the study validates the models' performance on a cloud/Colab environment, it does not test the computational efficiency or latency constraints required for real-time applications on low-power hardware.
- What evidence would resolve it: Benchmarks measuring inference latency and memory usage when running these models on edge hardware alongside accuracy metrics.

### Open Question 4
- Question: How does the localization accuracy (Intersection-over-Union) of the prompt-based visual pipeline compare quantitatively to traditional object detection models?
- Basis in paper: The author notes that "The absence of ground-truth bounding boxes prevented quantitative evaluation using metrics such as Intersection-over-Union (IoU)."
- Why unresolved: The visual pipeline's ability to draw bounding boxes was evaluated qualitatively rather than through standard quantitative metrics.
- What evidence would resolve it: A study utilizing a dataset with pixel-level ground truth annotations to calculate IoU and mean Average Precision (mAP) for the VLM against a baseline like YOLO.

## Limitations
- Textual pipeline's 89% accuracy relies on standardized OSHA report formatting, with no validation on irregularly structured accident narratives.
- Visual hazard detection accuracy remains unreported due to missing ground truth bounding boxes and human-annotated datasets.
- Small open-source VLMs show high recall but low precision with ensemble prompting, potentially leading to excessive false alarms in deployment.

## Confidence

**High Confidence:** The CoT prompting mechanism's effectiveness is well-supported by systematic error reduction in scene-to-hazard reasoning chains, with clear performance improvements documented across both commercial and open-source models. The semantic prompt ensemble's ability to stabilize small-model outputs is robustly validated through direct recall improvements.

**Medium Confidence:** Claims about pre-trained VLMs generalizing without fine-tuning rely on indirect evidence from prompt sensitivity studies and neighbor research, lacking direct comparison to fine-tuned baselines on construction-specific datasets.

**Low Confidence:** The framework's real-world deployment readiness is uncertain due to absent field testing, unmeasured false positive/negative rates in operational conditions, and no validation of cross-cultural applicability to non-US construction practices.

## Next Checks
1. Conduct field trials on active construction sites with human safety officers to establish baseline detection accuracy and false alarm rates under real conditions.
2. Test the textual pipeline on 50 non-standard OSHA reports (irregular formatting, non-English content) to quantify robustness to document variability.
3. Compare open-source VLMs against fine-tuned commercial models on a benchmark construction safety dataset to isolate zero-shot vs. domain adaptation benefits.