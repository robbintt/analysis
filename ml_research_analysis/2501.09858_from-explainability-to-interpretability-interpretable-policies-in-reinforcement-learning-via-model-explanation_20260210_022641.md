---
ver: rpa2
title: 'From Explainability to Interpretability: Interpretable Policies in Reinforcement
  Learning Via Model Explanation'
arxiv_id: '2501.09858'
source_url: https://arxiv.org/abs/2501.09858
tags:
- shapley
- interpretable
- policy
- state
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel approach to bridge the gap between
  explainability and interpretability in deep reinforcement learning (RL) by leveraging
  Shapley values. The core method involves analyzing Shapley value vectors to identify
  decision boundaries between action clusters, then formulating interpretable policies
  based on these boundaries.
---

# From Explainability to Interpretability: Interpretable Policies in Reinforcement Learning Via Model Explanation

## Quick Facts
- **arXiv ID:** 2501.09858
- **Source URL:** https://arxiv.org/abs/2501.09858
- **Reference count:** 8
- **Primary result:** Novel method using Shapley values to generate interpretable policies in deep RL while preserving model performance

## Executive Summary
This paper addresses the critical gap between explainability and interpretability in deep reinforcement learning by proposing a model-agnostic framework that transforms complex neural network policies into transparent, interpretable representations. The approach leverages Shapley value analysis to identify decision boundaries between action clusters in learned policies, then formulates interpretable policies based on these boundaries. The method is designed to work with both off-policy (DQN) and on-policy (PPO, A2C) algorithms, aiming to maintain original performance while providing more stable and understandable decision-making processes.

## Method Summary
The proposed method employs Shapley value analysis to transform deep RL policies into interpretable representations. First, the approach analyzes Shapley value vectors from the trained deep RL model to identify decision boundaries between action clusters. These boundaries are then used to formulate interpretable policies that can be expressed as simple, human-understandable rules or decision trees. The method is model-agnostic, meaning it can be applied to any deep RL algorithm regardless of architecture or training procedure. The interpretable policies are designed to preserve the performance characteristics of the original deep RL models while providing enhanced transparency and stability in decision-making.

## Key Results
- In CartPole environment, interpretable policies achieved maximum reward of 500 across all tested algorithms (DQN, PPO, A2C)
- In MountainCar environment, interpretable policies showed consistent performance with smaller standard deviations compared to original models
- PPO and A2C interpretable policies outperformed their original algorithms in MountainCar task

## Why This Works (Mechanism)
The method works by leveraging Shapley values to quantify the contribution of each state feature to the final action selection. By analyzing these contribution patterns across the state space, the approach can identify regions where the model's decision-making is consistent and where boundaries between action preferences exist. These boundaries form the basis for creating interpretable rules that capture the essential decision logic of the original model without requiring access to the internal neural network structure.

## Foundational Learning
- **Shapley Values:** A cooperative game theory concept used to fairly distribute credit among players; in this context, it measures feature importance in decision-making. Needed to quantify each state feature's contribution to action selection; quick check: verify Shapley values sum to total model output.
- **Decision Boundaries:** Regions in state space where the model's action preference changes; essential for creating interpretable rules. Needed to identify where policy decisions shift; quick check: plot boundaries to ensure they separate distinct action regions.
- **Model-Agnostic Analysis:** Approach that works regardless of the underlying neural network architecture. Needed to ensure broad applicability across different RL algorithms; quick check: test with different network architectures.
- **Action Clusters:** Groups of similar states that lead to the same action choice; used to simplify policy representation. Needed to reduce complexity of decision space; quick check: verify cluster coherence through visualization.
- **Policy Interpretability:** The degree to which a policy's decision-making process can be understood by humans. Needed to bridge the gap between complex models and human understanding; quick check: assess whether rules can be easily explained to non-experts.
- **Performance Preservation:** Ensuring that interpretable policies maintain comparable performance to original models. Needed to validate the practical utility of the approach; quick check: compare reward distributions between original and interpretable policies.

## Architecture Onboarding

**Component Map:** Raw State Features -> Shapley Value Computation -> Decision Boundary Detection -> Interpretable Policy Generation -> Policy Evaluation

**Critical Path:** The core workflow involves computing Shapley values for each state-action pair, clustering states based on action preferences, detecting boundaries between clusters using Shapley value patterns, and formulating interpretable rules that approximate the original policy behavior within each region.

**Design Tradeoffs:** The method trades computational overhead of Shapley value analysis for interpretability gains. While the approach provides transparency, it may introduce approximation errors when translating complex neural network decisions into simple rules. The balance between interpretability and performance preservation requires careful tuning of the boundary detection and rule generation parameters.

**Failure Signatures:** Potential failures include: (1) Oversimplification of decision boundaries leading to performance degradation, (2) Inability to capture complex, non-linear relationships in the original policy, (3) Computational intractability for high-dimensional state spaces, and (4) Poor generalization of interpretable rules to unseen states.

**First 3 Experiments:** 
1. Compare performance and stability metrics between original and interpretable policies in CartPole
2. Test scalability by applying method to higher-dimensional control tasks
3. Evaluate interpretability by measuring human understanding and trust in the generated rules

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental validation limited to two classic control environments (CartPole and MountainCar) with only three RL algorithms
- Lack of comparison with existing interpretable RL methods to establish relative effectiveness
- Computational overhead of Shapley value analysis not discussed or quantified
- Claims of generalizability to complex environments require substantial additional validation

## Confidence

**High confidence:** The methodology for using Shapley values to identify decision boundaries is technically sound and well-articulated, with clear mathematical foundations and implementation details.

**Medium confidence:** The performance preservation claims are reasonable given the experimental results, though the scope is limited to simple environments and may not generalize to more complex tasks.

**Low confidence:** The generalizability claims to complex environments and the assertion that this approach "offers a general framework" require substantial additional validation on diverse, high-dimensional tasks.

## Next Checks

1. Test the method on high-dimensional environments (e.g., Atari games or robotic control tasks) to assess scalability and performance preservation across diverse task complexities.
2. Compare against established interpretable RL approaches to establish relative effectiveness in terms of both interpretability metrics and performance retention.
3. Conduct ablation studies to quantify the computational overhead and identify bottlenecks in the Shapley value analysis pipeline, particularly for high-dimensional state spaces.