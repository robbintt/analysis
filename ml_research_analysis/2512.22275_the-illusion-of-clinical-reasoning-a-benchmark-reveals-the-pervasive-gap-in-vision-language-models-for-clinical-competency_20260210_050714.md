---
ver: rpa2
title: 'The Illusion of Clinical Reasoning: A Benchmark Reveals the Pervasive Gap
  in Vision-Language Models for Clinical Competency'
arxiv_id: '2512.22275'
source_url: https://arxiv.org/abs/2512.22275
tags:
- clinical
- text
- reasoning
- medical
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces the Bones and Joints (B&J) Benchmark to evaluate
  clinical reasoning capabilities of AI models in orthopedics and sports medicine.
  The benchmark includes 1,245 questions across seven tasks mirroring real-world clinical
  workflows, from knowledge recall to multimodal diagnosis and treatment planning.
---

# The Illusion of Clinical Reasoning: A Benchmark Reveals the Pervasive Gap in Vision-Language Models for Clinical Competency

## Quick Facts
- arXiv ID: 2512.22275
- Source URL: https://arxiv.org/abs/2512.22275
- Reference count: 40
- Key outcome: VLMs achieved high accuracy (>90%) on structured multiple-choice questions but dropped to below 60% on open-ended multimodal tasks, exhibiting significant text-driven hallucinations and evidence-ignoring behavior.

## Executive Summary
This study introduces the Bones and Joints (B&J) Benchmark to evaluate clinical reasoning capabilities of AI models in orthopedics and sports medicine. The benchmark includes 1,245 questions across seven tasks mirroring real-world clinical workflows, from knowledge recall to multimodal diagnosis and treatment planning. Eleven vision-language models (VLMs) and six large language models (LLMs) were tested against expert-derived ground truth. Results showed VLMs achieved high accuracy (>90%) on structured multiple-choice questions but dropped to below 60% on open-ended multimodal tasks. VLMs exhibited significant text-driven hallucinations, often ignoring contradictory visual evidence, and medical-specific fine-tuning provided no consistent advantage. The study concludes that current AI models are not yet clinically competent for complex multimodal reasoning and should be limited to supportive text-based roles until fundamental advances in multimodal integration are achieved.

## Method Summary
The study developed the B&J benchmark with 1,245 clinically relevant questions across seven tasks that reflect real-world clinical workflows in orthopedics and sports medicine. These tasks range from simple knowledge recall to complex multimodal diagnosis and treatment planning. Eleven VLMs and six LLMs were evaluated using this benchmark, with results compared against ground truth developed by orthopedic surgeons. The evaluation specifically tested both structured multiple-choice questions and open-ended multimodal tasks to assess different aspects of clinical reasoning capability.

## Key Results
- VLMs achieved >90% accuracy on structured multiple-choice questions but <60% on open-ended multimodal tasks
- VLMs consistently exhibited text-driven hallucinations, ignoring contradictory visual evidence
- Medical-specific fine-tuning provided no consistent advantage in clinical reasoning performance

## Why This Works (Mechanism)
The benchmark's strength lies in its systematic progression through clinical reasoning tasks, starting from simple knowledge recall and advancing to complex multimodal diagnosis. This design exposes the specific failure modes of VLMs in handling integrated visual-textual medical information, revealing that while models can perform well on isolated text-based reasoning, they struggle when required to synthesize information across modalities in clinical contexts.

## Foundational Learning
- Clinical reasoning workflow: Why needed - understanding how medical diagnosis and treatment planning should function; Quick check - can you trace the seven task types from knowledge recall to treatment planning
- Multimodal integration: Why needed - recognizing how visual and textual information should be combined in clinical contexts; Quick check - can you explain why ignoring visual evidence represents a fundamental failure
- Evidence weighting: Why needed - understanding how different types of medical evidence should be prioritized; Quick check - can you identify scenarios where visual evidence should override text descriptions

## Architecture Onboarding
Component Map: Text Encoder -> Multimodal Fusion -> Clinical Reasoning Module -> Output Generator
Critical Path: Input processing -> Evidence integration -> Clinical inference -> Response generation
Design Tradeoffs: Text-only vs. multimodal processing, structured vs. open-ended reasoning, general vs. medical-specific training
Failure Signatures: Text-driven hallucinations, evidence-ignoring behavior, hallucination persistence across different fine-tuning approaches
First Experiments: 1) Test VLMs on single-modality questions to isolate multimodal integration issues, 2) Compare performance on visual-only vs. text-only inputs, 3) Evaluate fine-tuned vs. general models on identical clinical scenarios

## Open Questions the Paper Calls Out
The paper does not explicitly enumerate open questions but implicitly raises several critical areas for investigation: 1) Whether architectural improvements in visual grounding could resolve multimodal integration failures, 2) How the performance gap manifests across different medical specialties, and 3) Whether specific fine-tuning methodologies might eventually overcome the observed limitations in clinical reasoning.

## Limitations
- Findings may not generalize beyond orthopedics and sports medicine specialties
- Expert-derived ground truth may contain implicit biases or oversights in question design
- Limited testing of fine-tuned models prevents definitive conclusions about medical fine-tuning effectiveness
- The benchmark focuses on specific clinical tasks that may not represent the full complexity of medical practice

## Confidence
- Core finding of multimodal reasoning gap: High
- Claims about medical fine-tuning ineffectiveness: Medium
- Recommendation against clinical deployment: High

## Next Checks
1. Replicate the B&J benchmark across multiple medical specialties (radiology, dermatology, pathology) to assess generalizability of the multimodal reasoning gap
2. Conduct ablation studies varying the quality and format of visual inputs to determine whether poor performance stems from input preprocessing rather than fundamental multimodal reasoning limitations
3. Test newer VLMs with improved visual grounding architectures (such as those using cross-attention mechanisms or visual token prioritization) to evaluate whether architectural advances have addressed the identified limitations since the benchmark's creation