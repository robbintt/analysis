---
ver: rpa2
title: 'R-WoM: Retrieval-augmented World Model For Computer-use Agents'
arxiv_id: '2510.11892'
source_url: https://arxiv.org/abs/2510.11892
tags:
- world
- r-wom
- arxiv
- claude-3
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large Language Models struggle to reliably simulate future states
  in digital environments due to hallucination and reliance on static knowledge, leading
  to compounding errors in long-horizon planning. To address this, the Retrieval-augmented
  World Model (R-WoM) grounds LLM-based simulations with environment-specific tutorials,
  improving both state prediction and reward estimation.
---

# R-WoM: Retrieval-augmented World Model For Computer-use Agents
## Quick Facts
- arXiv ID: 2510.11892
- Source URL: https://arxiv.org/abs/2510.11892
- Reference count: 36
- Large Language Models struggle to reliably simulate future states in digital environments due to hallucination and reliance on static knowledge, leading to compounding errors in long-horizon planning.

## Executive Summary
Large Language Models struggle to reliably simulate future states in digital environments due to hallucination and reliance on static knowledge, leading to compounding errors in long-horizon planning. To address this, the Retrieval-augmented World Model (R-WoM) grounds LLM-based simulations with environment-specific tutorials, improving both state prediction and reward estimation. R-WoM employs a reasoning-based retrieval pipeline for relevant tutorial chunks, uses long chain-of-thought reasoning for multi-step rollouts, and adopts listwise reward estimation to rank trajectories relatively rather than assign absolute scores. Evaluations on OSWorld and WebArena show R-WoM achieves substantial improvements over baselines, with up to 25.3% and 18.1% gains respectively, particularly in longer-horizon scenarios. This demonstrates the effectiveness of retrieval-augmented grounding for enhancing LLM-based world modeling in dynamic computer-use tasks.

## Method Summary
R-WoM introduces a retrieval-augmented world model that grounds LLM simulations with environment-specific tutorials. The system employs a reasoning-based retrieval pipeline to identify relevant tutorial chunks, uses long chain-of-thought reasoning for multi-step trajectory rollouts, and implements listwise reward estimation to rank trajectories relatively. This approach addresses the hallucination and static knowledge limitations of LLMs in dynamic digital environments, enabling more accurate state prediction and reward estimation for computer-use agents.

## Key Results
- R-WoM achieves up to 25.3% improvement on OSWorld benchmark
- R-WoM shows 18.1% gains on WebArena tasks
- Performance improvements are particularly pronounced in longer-horizon scenarios

## Why This Works (Mechanism)
R-WoM works by grounding LLM simulations with environment-specific tutorials, which reduces hallucination and improves state prediction accuracy. The reasoning-based retrieval pipeline ensures relevant tutorial chunks are incorporated during planning, while long chain-of-thought reasoning enables more coherent multi-step rollouts. The listwise reward estimation approach provides more reliable trajectory ranking compared to absolute scoring methods, leading to better decision-making in dynamic environments.

## Foundational Learning
1. **LLM Hallucination in Simulation** - Why needed: Understanding why LLMs fail to reliably predict future states in dynamic environments without external grounding. Quick check: Compare LLM-only predictions against ground truth in controlled environments.
2. **Retrieval-Augmented Generation** - Why needed: Mechanisms for incorporating external knowledge into LLM workflows to improve accuracy and reduce hallucinations. Quick check: Measure performance improvement when adding retrieval components.
3. **Listwise vs Pointwise Ranking** - Why needed: Understanding different approaches to reward estimation and their impact on trajectory optimization. Quick check: Compare ranking accuracy between listwise and absolute scoring methods.
4. **Chain-of-Thought Reasoning** - Why needed: How extended reasoning processes improve multi-step planning and decision-making in complex environments. Quick check: Measure planning accuracy with varying reasoning depths.
5. **Tutorial-Based Grounding** - Why needed: How environment-specific documentation can serve as effective knowledge bases for simulation grounding. Quick check: Assess performance with different tutorial quality levels.
6. **Computer-Use Agent Simulation** - Why needed: Understanding the unique challenges of simulating user interactions with digital interfaces. Quick check: Evaluate simulation accuracy in realistic computer-use scenarios.

## Architecture Onboarding
**Component Map**: Tutorial Corpus -> Retrieval Pipeline -> LLM Simulator -> State Predictor -> Reward Estimator -> Trajectory Optimizer
**Critical Path**: Tutorial retrieval → State prediction → Reward estimation → Trajectory selection
**Design Tradeoffs**: R-WoM trades computational overhead for improved accuracy through retrieval and extended reasoning. The listwise reward estimation provides better trajectory ranking but requires comparing multiple trajectories simultaneously.
**Failure Signatures**: Performance degradation when tutorial coverage is incomplete, increased hallucination in unfamiliar environments, computational bottlenecks during multi-trajectory comparisons.
**Three First Experiments**: 1) Evaluate retrieval accuracy with varying tutorial chunk sizes, 2) Measure state prediction accuracy with and without tutorial grounding, 3) Compare trajectory ranking performance between listwise and absolute reward estimation methods.

## Open Questions the Paper Calls Out
None

## Limitations
- Dependency on environment-specific tutorials introduces brittleness when coverage is incomplete or environments change rapidly
- Evaluation focuses on synthetic benchmarks rather than real-world production environments
- Listwise reward estimation computational overhead may be prohibitive for real-time applications

## Confidence
**High Confidence**: The retrieval mechanism improves state prediction accuracy compared to non-retrieval baselines; experimental results showing performance gains (25.3% on OSWorld, 18.1% on WebArena) are reproducible within tested environments; the general approach of grounding LLM simulations with external knowledge is sound and technically implemented.

**Medium Confidence**: The scalability of R-WoM to real-world, non-benchmark environments; the robustness of the system when tutorial quality varies; the computational efficiency trade-offs in practical deployments.

**Low Confidence**: Long-term performance stability over extended usage periods; adaptability to completely novel interface paradigms without retraining; security implications of the retrieval and simulation processes in production environments.

## Next Checks
1. Cross-environment generalization test: Evaluate R-WoM on at least three diverse real-world applications (e.g., enterprise software, cloud platforms, and specialized tools) to assess transfer learning and tutorial adaptation capabilities beyond benchmark environments.

2. Tutorial coverage ablation study: Systematically vary tutorial completeness (0%, 50%, 100%) and quality to quantify R-WoM's performance sensitivity to knowledge base coverage, identifying minimum viable tutorial requirements.

3. Runtime and resource efficiency benchmarking: Measure memory usage, inference latency, and computational overhead across different tutorial sizes and trajectory lengths to establish practical deployment constraints and optimization opportunities.