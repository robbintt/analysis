---
ver: rpa2
title: Few-Shot Graph Out-of-Distribution Detection with LLMs
arxiv_id: '2503.22097'
source_url: https://arxiv.org/abs/2503.22097
tags:
- nodes
- detection
- graph
- llms
- labels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LLM-GOOD, a novel framework that combines
  large language models (LLMs) and graph neural networks (GNNs) to address the data-efficiency
  challenge in graph out-of-distribution (OOD) detection. The key innovation is using
  LLMs' zero-shot capabilities to filter out OOD nodes before human annotation, followed
  by training a lightweight GNN filter with pseudo-labels from the LLM to further
  reduce annotation costs.
---

# Few-Shot Graph Out-of-Distribution Detection with LLMs

## Quick Facts
- arXiv ID: 2503.22097
- Source URL: https://arxiv.org/abs/2503.22097
- Authors: Haoyan Xu, Zhengtao Yao, Yushun Dong, Ziyi Wang, Ryan A. Rossi, Mengyuan Li, Yue Zhao
- Reference count: 32
- Primary result: Combines LLMs and GNNs to achieve 17.72% improvement in ID accuracy on Cora with 5×K annotation budget

## Executive Summary
This paper addresses the challenge of few-shot graph out-of-distribution (OOD) detection by introducing LLM-GOOD, a framework that leverages large language models (LLMs) for efficient node filtering before human annotation. The key innovation lies in using LLMs' zero-shot capabilities to identify and remove likely OOD nodes based on semantic text alignment, followed by a lightweight GNN filter to propagate ID/OOD signals across the graph structure. This two-stage filtering approach significantly reduces annotation costs while maintaining high detection performance.

The framework employs an informativeness-aware node selection strategy that maximizes the utility of limited human annotation budgets by ensuring they are spent on diverse, representative ID examples rather than uninformative OOD nodes. Extensive experiments on four real-world text-attributed graph datasets demonstrate that LLM-GOOD outperforms state-of-the-art baselines in both ID classification accuracy and OOD detection performance while substantially reducing annotation costs.

## Method Summary
The LLM-GOOD framework operates in a transductive semi-supervised setting on text-attributed graphs (TAGs). It first uses an LLM (GPT-4o-mini) to perform zero-shot OOD filtering by annotating a subset of 200 random nodes with K ID classes plus a "none" label for OOD. A lightweight 2-layer GCN is then trained on these noisy labels to predict ID/OOD status for all remaining nodes. K-Medoids selection (48 clusters) on GNN embeddings identifies the most informative nodes for human annotation within the budget. The final ID classifier is trained using these accurately labeled nodes. OOD detection is performed using an energy-based approach. The method is evaluated on Cora, Citeseer, Pubmed, and Wiki-CS datasets with ID classification accuracy and OOD detection metrics (AUROC, AUPR, FPR@95).

## Key Results
- Achieves 81.52% ID accuracy on Cora dataset with 5×K annotation budget versus 63.80% for best baseline (17.72% improvement)
- Maintains high OOD detection performance with AUROC/AUPR scores comparable to or exceeding baselines
- Successfully reduces human annotation costs by filtering out uninformative OOD nodes before selection
- Shows consistent performance across four different text-attributed graph datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Large Language Models (LLMs) can reduce the search space for In-Distribution (ID) nodes by performing zero-shot OOD filtering based on semantic text alignment, independent of graph topology.
- **Mechanism:** The framework prompts an LLM with ID class names and a node's raw text. The LLM outputs a class label or "none" (OOD). This acts as a semantic sieve, removing nodes that lack topical relevance to the ID classes before structural analysis begins.
- **Core assumption:** The textual attributes of nodes contain sufficient signal to distinguish semantic relevance (ID vs. OOD) without requiring structural context, and the LLM's internal knowledge aligns with the dataset's specific ID definitions.
- **Evidence anchors:** Abstract states LLMs filter OOD nodes using text information; Section 4.1 describes zero-shot prompting to determine ID class membership.

### Mechanism 2
- **Claim:** A lightweight Graph Neural Network (GNN) trained on noisy LLM pseudo-labels can effectively propagate ID/OOD signals across the graph structure, correcting individual LLM errors and covering unlabeled nodes at low cost.
- **Mechanism:** The GNN Filter is trained on the small set of LLM-annotated nodes ($V_{LLM}$). Through message passing, the GNN learns structural correlations (homophily). It predicts ID status for all remaining nodes, utilizing the graph topology that the LLM missed.
- **Core assumption:** ID nodes and OOD nodes exhibit distinguishable structural patterns (e.g., ID nodes cluster together), allowing the GNN to smooth out noise from the LLM's pseudo-labels.
- **Evidence anchors:** Section 4.2 describes training a lightweight GNN filter using noisy labels from LLMs; Abstract mentions GNN filter reduces annotation costs.

### Mechanism 3
- **Claim:** Performing informativeness-aware node selection after OOD filtering maximizes the utility of the human annotation budget by preventing the waste of labels on uninformative OOD nodes.
- **Mechanism:** Standard active learning often selects high-uncertainty nodes, which are frequently OOD. By applying the GNN filter first to remove predicted OOD nodes, the selection algorithm operates only on a "purified" pool of potential ID nodes, ensuring the budget is spent on diverse, representative ID examples.
- **Core assumption:** The GNN filter has a sufficiently high recall for ID nodes; if valid ID nodes are filtered out early, they are permanently lost to the training set.
- **Evidence anchors:** Section 4.3 explains how OOD filtering enables more effective identification of informative nodes; Table 8 shows significantly higher proportion of annotated ID nodes compared to baselines.

## Foundational Learning

- **Concept: Text-Attributed Graphs (TAGs)**
  - **Why needed here:** The entire framework relies on the duality of the data: LLMs process the "Text" part, and GNNs process the "Graph" part. Understanding that $X$ (embeddings) comes from $T$ (raw text) is crucial.
  - **Quick check question:** Can you explain why a standard GNN fails if the input is just node IDs without features, and why an LLM fails if it ignores edges?

- **Concept: Transductive Learning**
  - **Why needed here:** The paper assumes the full graph structure is visible during training (unlabeled nodes are present). The GNN Filter leverages this by predicting labels for all nodes in $V_{can}$ simultaneously.
  - **Quick check question:** In a transductive setting, does the model have access to the test set's features during training? (Yes/No).

- **Concept: Pseudo-Labeling & Noise**
  - **Why needed here:** The "GNN Filter" is not trained on ground truth. It relies on "noisy labels" from the LLM. Understanding how to train a model on imperfect labels is central to Step 2 of the architecture.
  - **Quick check question:** If the LLM has a 20% error rate, how might a GNN's message passing help or hurt the final prediction? (Hint: Smoothing vs. Noise propagation).

## Architecture Onboarding

- **Component map:** Raw Text + Graph Structure ($T, A$) -> LLM Zero-Shot Annotator -> GNN Filter -> Selector -> Human Annotator -> Target Classifier

- **Critical path:** The accuracy of the **GNN Filter** (Step 3) is the linchpin. If it fails to recall ID nodes (high false negative rate), the final classifier sees insufficient training data. If it fails to filter OOD (high false positive rate), the human annotation budget is wasted on irrelevant nodes.

- **Design tradeoffs:**
  - **LLM Cost vs. GNN Accuracy:** Using more LLM annotations (increasing $m$) improves the GNN filter but raises API costs. The paper settles on 200 nodes as a heuristic balance.
  - **Filter Threshold:** The paper classifies nodes as OOD if the max probability falls on the $(K+1)$-th class. Adjusting confidence thresholds here changes the tradeoff between "Purity of ID set" and "Recall of ID set."

- **Failure signatures:**
  - **Low ID Proportion in Selection:** If Table 8 metrics drop, the GNN Filter is admitting too many OOD nodes.
  - **Stagnant Accuracy:** If ID Accuracy remains low despite budget increases, the GNN Filter may be systematically excluding a specific ID class (Selection Bias).

- **First 3 experiments:**
  1. **Pseudo-Label Quality Check:** Run the LLM prompt on a held-out validation set with known labels. Measure the Zero-Shot OOD detection AUROC to establish an upper bound for the GNN filter.
  2. **Filter Ablation:** Train the GNN Filter using LLM labels, then measure its performance against ground truth on the validation set. Plot accuracy vs. number of LLM labels ($m$).
  3. **End-to-End Budget Comparison:** Compare LLM-GOOD against a "Random Selection + GNN" baseline under a strict label budget (e.g., 5xK) to quantify the value added by the filtering step.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can specialized noise-resistant GNN architectures be designed to optimally leverage both clean human annotations and noisy LLM pseudo-labels?
- **Basis in paper:** [explicit] The Conclusion states, "A potential future research direction is to investigate more effective ways to leverage both clean and noisy labels to train a more noise-resistant ID classifier."
- **Why unresolved:** The current framework uses standard GNN layers (GCN) and simple label aggregation, lacking specific architectural mechanisms to handle the distinct noise distributions inherent in LLM-generated labels.
- **What evidence would resolve it:** Developing a GNN variant with noise-robust loss functions or architecture (e.g., leveraging label confidence scores) that improves ID classification accuracy over the standard implementation in low-budget settings.

### Open Question 2
- **Question:** Can in-context learning (ICL) paradigms enhance node-level OOD detection performance compared to the zero-shot approach used in this study?
- **Basis in paper:** [explicit] The Conclusion asks, "it would be interesting to explore whether in-context learning can improve node-level OOD detection performance compared to zero-shot OOD detection with LLMs."
- **Why unresolved:** The paper focuses exclusively on zero-shot prompting; the potential for providing LLMs with few-shot examples (ICL) to refine their understanding of boundary conditions between ID and OOD nodes remains untested.
- **What evidence would resolve it:** Experimental results comparing the OOD detection precision of LLMs using zero-shot prompts versus those provided with few-shot ID/OOD examples in the prompt context.

### Open Question 3
- **Question:** Can the LLM-GOOD framework be adapted for graphs where node attributes are not text-based (non-TAGs)?
- **Basis in paper:** [inferred] The method is defined exclusively on "Text-Attributed Graphs" (Section 3.1) and relies on LLMs processing "raw text attributes" (Section 4.1) to generate embeddings and pseudo-labels.
- **Why unresolved:** The framework's dependency on textual prompts prevents its direct application to graphs with continuous numerical features or categorical metadata without translation strategies.
- **What evidence would resolve it:** A modified pipeline where numerical features are serialized into text descriptions or processed by multimodal models, showing comparable performance on non-text graph datasets.

## Limitations
- The framework's effectiveness critically depends on textual attributes containing sufficient semantic signal to distinguish ID from OOD nodes; uninformative text breaks the pipeline
- The "few-shot" label is misleading as it still requires 200 LLM annotations upfront, which may be cost-prohibitive in some applications
- Performance relies heavily on strong homophily in graph structure; non-homophilic graphs or adversarially placed OOD nodes could cause the GNN filter to propagate noise

## Confidence
- **High Confidence**: The core mechanism of using LLM zero-shot filtering followed by GNN-based signal propagation is technically sound and well-supported by the ablation studies
- **Medium Confidence**: The reported performance improvements are impressive but depend heavily on specific dataset characteristics and SentenceBERT embedding quality
- **Medium Confidence**: The budget efficiency claims assume human annotation time is the primary constraint but don't account for LLM API costs or computational overhead

## Next Checks
1. **Dataset Transferability Test**: Apply LLM-GOOD to a non-academic text-attributed graph (e.g., social media network with informal text) to test robustness across domains with different linguistic patterns

2. **Cost-Benefit Analysis**: Calculate total cost (LLM API calls + compute time) for 200 annotations versus alternative approaches, and compare against the marginal performance gains to assess practical viability

3. **Robustness to Text Quality**: Systematically degrade text quality (e.g., remove abstracts, truncate titles) in the Cora dataset to determine the minimum textual information required for the LLM filter to maintain acceptable performance