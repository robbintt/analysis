---
ver: rpa2
title: History-Aware Reasoning for GUI Agents
arxiv_id: '2511.09127'
source_url: https://arxiv.org/abs/2511.09127
tags:
- screen
- think
- action
- answer
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of weak short-term memory in GUI
  agents during episodic reasoning, where agents treat chained interactions as discrete
  screen understanding rather than leveraging historical context. The authors propose
  a History-Aware Reasoning (HAR) framework that uses reflective learning with tailored
  correction guidelines and a hybrid RL reward function to transform reasoning from
  history-agnostic to history-aware.
---

# History-Aware Reasoning for GUI Agents

## Quick Facts
- arXiv ID: 2511.09127
- Source URL: https://arxiv.org/abs/2511.09127
- Reference count: 11
- Key outcome: HAR-GUI-3B outperforms current advanced methods with similar parameters and rivals larger models in out-of-distribution scenarios, excelling in challenging Chinese mini-program benchmarks

## Executive Summary
This paper addresses the fundamental problem of weak short-term memory in GUI agents during episodic reasoning, where agents treat chained interactions as discrete screen understanding rather than leveraging historical context. The authors propose a History-Aware Reasoning (HAR) framework that uses reflective learning with tailored correction guidelines and a hybrid RL reward function to transform reasoning from history-agnostic to history-aware. The framework includes constructing a reflective learning scenario, synthesizing tailored correction guidelines, and designing a hybrid RL reward function. HAR-GUI-3B, a native GUI-tailored model developed using this framework, demonstrates superior performance across GUI-related benchmarks, particularly excelling in challenging Chinese mini-program scenarios.

## Method Summary
The proposed three-stage History-Aware Reasoning framework begins with GUI Scenario Warm-up SFT training using 4k GUI understanding instances, 20k grounding instances, 58k System-2 CoT instances, and 100k synthesized Act2Sum entries. Round-1 RL employs GRPO in a reflection scenario using hybrid rewards (r_format × (r_action + γ × r_memory)) with tailored correction guidelines generated by a teacher model. Round-2 RL implements Task Mixing Training Strategy (TMTS) to balance grounding and episodic reasoning capabilities. The framework transforms history-agnostic reasoning into history-aware reasoning through reflective learning and memory-augmented rewards.

## Key Results
- HAR-GUI-3B outperforms current advanced methods with similar parameters across GUI benchmarks
- The model rivals larger models in out-of-distribution scenarios, particularly excelling in Chinese mini-program benchmarks
- Consistent generalization observed with significant performance improvements in challenging episodic reasoning tasks

## Why This Works (Mechanism)
The framework succeeds by explicitly training agents to maintain and utilize historical context during chained interactions rather than treating each interaction as isolated. The reflective learning scenario forces the model to analyze its own reasoning patterns, while tailored correction guidelines provide specific feedback on memory-related errors. The hybrid RL reward function, particularly the Memory-Augmented Reward component, incentivizes the model to maintain coherence across interaction sequences by rewarding memory utilization and penalizing history-agnostic reasoning patterns.

## Foundational Learning
- **Episodic Reasoning**: Understanding chained GUI interactions as connected sequences rather than isolated events; needed to address the core problem of history-agnostic behavior
- **Reflective Learning**: Model analyzes its own reasoning patterns and receives targeted feedback; needed to identify and correct memory-related reasoning errors
- **Task Mixing Training Strategy (TMTS)**: Alternating between grounding and reasoning tasks during training; needed to prevent degradation of grounding capabilities while improving episodic reasoning
- **Memory-Augmented Rewards**: RL rewards that incorporate memory verification; needed to incentivize history-aware reasoning over history-agnostic patterns

## Architecture Onboarding

Component map: Qwen2.5-VL-3B-Instruct -> SFT Warm-up -> Round-1 RL (Reflection) -> Round-2 RL (TMTS) -> HAR-GUI-3B

Critical path: The model must first develop strong grounding capabilities through warm-up SFT, then learn to recognize and correct memory errors through Round-1 RL, before finally balancing grounding and reasoning in Round-2 RL.

Design tradeoffs: The framework prioritizes history-aware reasoning over raw performance, accepting potential short-term degradation in grounding to achieve long-term episodic reasoning improvements.

Failure signatures: 
- Grounding degradation after Round-1 RL indicates insufficient TMTS application
- Persistent history-agnostic CoT patterns suggest MAR mechanism not triggering correctly
- OOD instruction-following collapse indicates post-training alignment issues

First experiments:
1. Test MAR mechanism with simplified binary verification prompt on small GUI interaction sequences
2. Pilot study to determine optimal sampling strategy for collecting historically incorrect samples
3. Ablation study comparing HAR-GUI-3B performance with and without reflection scenario component

## Open Questions the Paper Calls Out
None

## Limitations
- MAR mechanism implementation details underspecified, particularly prompt structure and classification criteria
- Inference procedure for collecting historically incorrect samples lacks clarity on sampling strategy and error thresholds
- Data preprocessing details for benchmark datasets incomplete, especially coordinate normalization and history text summarization

## Confidence

High confidence: Overall three-stage framework architecture and hybrid reward function design are well-specified

Medium confidence: Effectiveness claims for HAR-GUI-3B supported by benchmark results, though reflection scenario implementation needs more detail

Low confidence: Reproducibility of MAR mechanism and hard sample collection methodology due to technical detail gaps

## Next Checks

1. Implement simplified MAR mechanism with binary verification prompt and test on GUI interaction sequences to verify memory classification accuracy
2. Run pilot study to determine optimal sampling strategy for historically incorrect samples, testing different temperature settings and error threshold criteria
3. Conduct ablation studies comparing HAR-GUI-3B performance with and without reflection scenario component to isolate history-aware reasoning impact