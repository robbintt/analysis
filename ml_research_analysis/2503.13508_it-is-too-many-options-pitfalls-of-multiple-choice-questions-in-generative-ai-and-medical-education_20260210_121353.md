---
ver: rpa2
title: 'It is Too Many Options: Pitfalls of Multiple-Choice Questions in Generative
  AI and Medical Education'
arxiv_id: '2503.13508'
source_url: https://arxiv.org/abs/2503.13508
tags:
- multiple-choice
- performance
- free-response
- questions
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study shows that large language models perform significantly
  worse on free-response medical questions compared to multiple-choice questions,
  with an average 39.43% drop in accuracy. This decline was greater than the 22.29%
  drop observed in human medical students.
---

# It is Too Many Options: Pitfalls of Multiple-Choice Questions in Generative AI and Medical Education

## Quick Facts
- arXiv ID: 2503.13508
- Source URL: https://arxiv.org/abs/2503.13508
- Authors: Shrutika Singh; Anton Alyakin; Daniel Alexander Alber; Jaden Stryker; Ai Phuong S Tong; Karl Sangwon; Nicolas Goff; Mathew de la Paz; Miguel Hernandez-Rovira; Ki Yun Park; Eric Claude Leuthardt; Eric Karl Oermann
- Reference count: 40
- Primary result: LLM performance drops 39.43% from MCQs to free-response (worse than human 22.29% drop); masking study shows MCQ accuracy above chance even with full question masking (up to 37.34% for GPT-4o)

## Executive Summary
This study demonstrates that large language models (LLMs) perform significantly worse on free-response medical questions compared to multiple-choice questions, with an average 39.43% drop in accuracy. This decline was greater than the 22.29% drop observed in human medical students. A masking experiment revealed that even with complete question stem masking, LLMs maintained above-chance performance on multiple-choice questions while free-response performance approached zero. These results indicate that MCQ benchmarks overestimate LLM medical capabilities by leveraging pattern recognition rather than true medical knowledge, suggesting the need for free-response or multi-turn dialogue assessments in medical AI evaluation.

## Method Summary
The study used the MultiMedQA dataset (14,965 questions) filtered via GPT-4o to create FreeMedQA (10,278 questions convertible to free-response format). Questions were transformed using RegEx-based conversion (e.g., "which of the following" → "what"). Three models (GPT-4o, GPT-3.5, Llama-3-70B-chat) were evaluated on both formats with temperature=0.0, max context=1024, 5 repetitions. Free-response answers were judged by GPT-4o (blinded to question) for correctness. A masking study progressively masked question stems (0%, 25%, 50%, 75%, 100%) while preserving answer options for MCQ evaluation.

## Key Results
- LLMs showed average 39.43% accuracy drop from MCQ to free-response (p = 1.3 × 10⁻⁵)
- Human medical students showed 22.29% accuracy drop in same comparison
- At 100% masking, LLM MCQ performance averaged 6.70% above random chance (25%), with GPT-4o reaching 37.34% accuracy
- LLM free-response performance approached zero under complete masking

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exploit answer-option patterns to achieve above-chance MCQ accuracy without reading the question
- Mechanism: When answer options remain visible during masking, models attend to statistical regularities in distractor phrasing and correct-answer priors learned during pretraining. This enables "recognition" independent of question comprehension
- Core assumption: The observed above-chance performance under full masking reflects test-taking heuristics encoded in model weights, not reasoning
- Evidence anchors:
  - [abstract] "At 100% masking, the average LLM multiple-choice performance was 6.70% greater than random chance (p = 0.002) with one LLM (GPT-4o) obtaining an accuracy of 37.34%."
  - [Results] "A notable discrepancy is at 100% masking where the multiple-choice performance is on average 6.70% greater than that of a random chance of 25%."
  - [corpus] Related work (Balepur & Rudinger) explicitly models "choices-only" cheating behavior in LLMs, supporting the plausibility of option-based shortcuts
- Break condition: If models were randomly selecting among options under full masking, accuracy would converge to 25% (chance). Significant deviation invalidates random-guessing hypotheses

### Mechanism 2
- Claim: Free-response formats expose a recognition-recall gap analogous to Bloom's Taxonomy in human learners
- Mechanism: MCQs require recognition (selecting among presented candidates); free-response requires recall and synthesis. Models trained on completion-style objectives may overfit to recognition patterns without developing generative knowledge structures
- Core assumption: The performance drop reflects architectural limitations in knowledge retrieval, not just evaluation noise
- Evidence anchors:
  - [abstract] "...average absolute deterioration of 39.43% in performance on free-response questions relative to multiple-choice (p = 1.3 × 10⁻⁵) which was greater than the human performance decline of 22.29%."
  - [Discussion] "In fact, a parallel could be drawn from the different levels of learning witnessed in humans via Bloom's Taxonomy to similar mechanisms in LLM learning."
  - [corpus] No direct corpus corroboration found; this analogy is paper-specific
- Break condition: If the free-response drop were purely due to evaluation methodology (e.g., judge harshness), then masked MCQ performance would not remain above chance while masked free-response collapses to zero

### Mechanism 3
- Claim: MCQ benchmarks conflate pattern-matching ability with medical reasoning capability
- Mechanism: High MCQ scores arise from a mixture of (a) genuine reasoning, (b) option-based heuristics, and (c) dataset artifacts. Standard benchmarks do not disentangle these, inflating reported capabilities
- Core assumption: The gap between MCQ and free-response reflects the contribution of (b) and (c), not measurement error
- Evidence anchors:
  - [abstract] "These results indicate that MCQ benchmarks overestimate LLM medical capabilities by leveraging pattern recognition rather than true medical knowledge."
  - [Introduction] "We hypothesized that existing multiple-choice question (MCQ) benchmarks are poor metrics for assessing the medical knowledge and capabilities of LLMs."
  - [corpus] Griot et al. (2024) report similar concerns with fictional medical MCQ data, suggesting pattern exploitation is not domain-specific
- Break condition: If models performed equally well on free-response with robust evaluation, the "overestimation" claim would weaken

## Foundational Learning

- Concept: **Recognition vs. Recall in Evaluation Design**
  - Why needed here: The paper's central claim hinges on understanding why MCQs (recognition) inflate scores relative to free-response (recall). Without this distinction, the masking results appear anomalous
  - Quick check question: If an LLM scores 85% on MCQs but 40% on matched free-response, does this indicate (a) poor evaluation, (b) superficial pattern knowledge, or (c) both?

- Concept: **Benchmark Artifact Analysis**
  - Why needed here: The masking study is a form of counterfactual analysis—testing what happens when question information is removed. Understanding how benchmarks can contain exploitable shortcuts is essential for interpreting results
  - Quick check question: Name two ways an MCQ benchmark could leak answer information without the model reading the question stem

- Concept: **LLM-as-Judge Evaluation**
  - Why needed here: Free-response answers were evaluated by GPT-4o. Understanding the limitations of model-based grading (stochasticity, leniency/strictness) is critical for assessing whether the observed drops are measurement artifacts
  - Quick check question: What biases might an LLM judge introduce when evaluating free-response medical answers?

## Architecture Onboarding

- Component map:
  - MultiMedQA dataset -> GPT-4o filtering -> FreeMedQA benchmark (10,278 questions)
  - Format transformation (RegEx) -> Paired MCQ/free-response dataset
  - Model inference (GPT-4o, GPT-3.5, Llama-3) -> MCQ (string match) + Free-response (GPT-4o judge)
  - Masking module -> Progressive stem masking (0-100%) -> Re-evaluation

- Critical path:
  1. Question filtering -> determines which MCQs are convertible to free-response without losing answerability
  2. Format transformation -> generates paired benchmark
  3. Model inference -> temperature=0.0, max context=1024, 5 runs per condition
  4. Evaluation -> MCQ via exact match; free-response via LLM judge

- Design tradeoffs:
  - Used GPT-4o for both filtering and evaluation, introducing potential circularity
  - RegEx adaptation may produce awkward free-response phrasing
  - 31.32% of original questions removed due to option-dependency, reducing benchmark coverage

- Failure signatures:
  - Free-response scores near zero under masking indicate evaluation pipeline is working as intended (no false positives from judge)
  - MCQ scores above 25% under full masking indicate option-based shortcut activation
  - Large variance across runs would suggest temperature/settings issues; reported SDs are small

- First 3 experiments:
  1. Reproduce masking curve on a held-out subset (e.g., 500 questions) to validate the above-chance MCQ finding with a different random seed
  2. Swap judge models (e.g., use Claude or Llama instead of GPT-4o for free-response evaluation) to assess judge dependency
  3. Permutation test: Shuffle answer options and re-run MCQ evaluation to confirm that above-chance performance under masking depends on original option ordering/phrasing

## Open Questions the Paper Calls Out
None

## Limitations
- Judge reliability: Free-response evaluation relied entirely on GPT-4o as a blinded judge, introducing potential bias and measurement noise not fully characterized
- Format transformation artifacts: RegEx-based conversion from MCQ to free-response may have introduced phrasing issues that disproportionately penalized LLM performance
- Masking implementation details: The paper does not specify the masking granularity (character, word, or token level) or directionality, which could affect reproducibility

## Confidence

- **High confidence**: The observed 39.43% average accuracy drop from MCQ to free-response for LLMs, and the finding that LLM performance declines more than human performance in the same transition
- **Medium confidence**: The claim that MCQ benchmarks overestimate LLM medical capabilities through pattern recognition, given the above-chance performance under full masking, but without direct evidence ruling out other explanations
- **Low confidence**: The Bloom's Taxonomy analogy connecting recognition-recall gaps in humans to LLM behavior, as this remains speculative without mechanistic validation

## Next Checks

1. **Judge validation**: Re-run the free-response evaluation using a different LLM judge (e.g., Claude-3) or human annotators on a subset of answers to assess whether GPT-4o's evaluation is consistent and unbiased

2. **Masking replication**: Implement the masking study with explicit specification of masking granularity and directionality, then verify whether 100% masked MCQ accuracy remains above chance (~25%) and matches the reported 6.70% average above-chance performance

3. **Option randomization test**: Shuffle answer options for a subset of questions and re-evaluate MCQ performance to determine whether above-chance performance under masking depends on original option ordering or phrasing patterns