---
ver: rpa2
title: 'Verde: Verification via Refereed Delegation for Machine Learning Programs'
arxiv_id: '2502.19405'
source_url: https://arxiv.org/abs/2502.19405
tags:
- training
- referee
- trainers
- repops
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work proposes refereed delegation, a cryptographic approach
  for verifying correctness of machine learning programs executed by untrusted compute
  providers. The authors address two main challenges: designing an efficient dispute
  resolution protocol for neural networks and ensuring bitwise reproducibility across
  hardware.'
---

# Verde: Verification via Refereed Delegation for Machine Learning Programs

## Quick Facts
- arXiv ID: 2502.19405
- Source URL: https://arxiv.org/abs/2502.19405
- Reference count: 11
- Primary result: Practical cryptographic verification of ML computations with 30-60% overhead for matrix multiplication and 67% for Llama-1B training on A100 GPU

## Executive Summary
This paper introduces Verde, a refereed delegation protocol for verifying correctness of machine learning programs executed by untrusted compute providers. The system addresses two key challenges: designing an efficient dispute resolution protocol for neural networks and ensuring bitwise reproducibility across different hardware. Verde narrows disputes to specific training steps and operators, while RepOps provides a library that eliminates hardware non-determinism by controlling floating point operation order. The approach achieves practical overheads dramatically more efficient than cryptographic proofs, providing strong guarantees that clients receive correct results if at least one compute provider is honest.

## Method Summary
The method employs a cryptographic approach where multiple compute providers execute ML programs and a referee adjudicates disputes. The protocol works by having providers commit to intermediate states and challenge each other's results, with disputes resolved through recursive narrowing down to specific operators and training steps. RepOps ensures bitwise reproducibility by controlling floating point operation order across different hardware, eliminating non-determinism that could otherwise prevent effective verification. The system assumes at least one honest provider and uses efficient dispute resolution rather than expensive cryptographic proofs.

## Key Results
- Matrix multiplication incurs 30-60% overhead compared to native execution
- Llama-1B training overhead is approximately 67% on A100 GPU
- The approach is 4 orders of magnitude cheaper than cryptographic proofs for the same verification guarantees

## Why This Works (Mechanism)
The protocol leverages cryptographic commitments and challenge-response mechanisms to create incentives for honest computation. When providers submit different results, the referee can efficiently identify the dishonest party by examining increasingly specific portions of the computation trace. The bitwise reproducibility through RepOps ensures that all honest providers should produce identical results, making any discrepancies easily attributable to malicious behavior rather than acceptable variation.

## Foundational Learning
- Refereed delegation: A cryptographic technique where multiple parties execute computations and a referee resolves disputes - needed because it provides efficient verification without requiring clients to redo expensive computations
- Bitwise reproducibility: Ensuring identical bit-for-bit results across different hardware - needed because verification requires all honest providers to produce exactly the same outputs
- Floating point non-determinism: Variations in floating point operation order across hardware can produce different results - needed to understand why reproducibility is challenging in ML workloads
- Recursive dispute narrowing: The process of progressively focusing disputes on smaller computation segments - needed to achieve efficient verification without examining entire computation traces
- Operator-level verification: Checking correctness at the granularity of individual mathematical operations - needed because ML programs consist of repeated application of specific operators
- Commitment schemes: Cryptographic tools that allow proving knowledge of data without revealing it - needed for the challenge-response mechanism that underpins the verification protocol

## Architecture Onboarding
Component map: Client -> Multiple Providers -> Referee -> Dispute Resolution -> Result Verification
Critical path: Client request → Parallel computation by providers → Result submission → Verification/check for discrepancies → (If dispute) Referee adjudication → Final result delivery
Design tradeoffs: The system trades some computational overhead for verification guarantees, choosing recursive dispute resolution over expensive cryptographic proofs. This makes practical deployment feasible but requires multiple providers and introduces latency for dispute resolution.
Failure signatures: Discrepancies between provider results trigger dispute resolution; complete agreement indicates success. Hardware non-determinism would manifest as unexpected disagreements even with honest providers.
First experiments: 1) Benchmark matrix multiplication overhead across different GPU architectures, 2) Test reproducibility of simple neural network training across different hardware configurations, 3) Simulate dispute scenarios to measure referee resolution time and accuracy.

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Overhead estimates rely heavily on benchmarks using a single GPU architecture (A100), limiting generalizability
- Bitwise reproducibility guarantees assume complete control over floating-point operations, which may not hold for all hardware vendors
- Dispute resolution efficiency may degrade for extremely large or complex models where component interactions create verification challenges

## Confidence
- Core protocol design and mathematical soundness: High
- Practical overhead measurements: Medium
- Long-term reproducibility guarantees: Low to Medium

## Next Checks
1. Benchmark the overhead across multiple GPU architectures (AMD, Intel, different NVIDIA generations) to verify the claimed 30-60% overhead range holds universally
2. Test the protocol with distributed training scenarios involving multiple GPUs/nodes to assess scalability and whether the dispute resolution mechanism maintains efficiency
3. Evaluate the system's behavior under realistic adversarial conditions where compute providers actively attempt to exploit verification gaps rather than simply providing incorrect results