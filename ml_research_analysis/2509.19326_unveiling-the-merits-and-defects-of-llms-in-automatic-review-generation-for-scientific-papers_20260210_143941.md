---
ver: rpa2
title: Unveiling the Merits and Defects of LLMs in Automatic Review Generation for
  Scientific Papers
arxiv_id: '2509.19326'
source_url: https://arxiv.org/abs/2509.19326
tags:
- reviews
- review
- papers
- llms
- iclr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a comprehensive evaluation of large language
  models in automated peer review generation, addressing the growing strain on academic
  reviewing due to increased paper submissions. We construct a high-consensus benchmark
  of 1,683 papers and 6,495 expert reviews from ICLR and NeurIPS, categorizing papers
  by quality and generating reviews using five state-of-the-art LLMs.
---

# Unveiling the Merits and Defects of LLMs in Automatic Review Generation for Scientific Papers

## Quick Facts
- arXiv ID: 2509.19326
- Source URL: https://arxiv.org/abs/2509.19326
- Authors: Ruochi Li; Haoxuan Zhang; Edward Gehringer; Ting Xiao; Junhua Ding; Haihua Chen
- Reference count: 32
- Key outcome: LLMs excel at descriptive/affirmational content but consistently underperform in identifying weaknesses and adjusting feedback based on paper quality, generating 59.42% fewer entities in weaknesses sections compared to human reviewers.

## Executive Summary
This study presents a comprehensive evaluation of large language models in automated peer review generation, addressing the growing strain on academic reviewing due to increased paper submissions. We construct a high-consensus benchmark of 1,683 papers and 6,495 expert reviews from ICLR and NeurIPS, categorizing papers by quality and generating reviews using five state-of-the-art LLMs. Our novel evaluation framework integrates semantic similarity analysis and structured knowledge graph metrics to assess both surface-level comprehension and deeper conceptual analysis. Results show that while LLMs excel in descriptive and affirmational content, producing 15.74% more entities than human reviewers in strengths sections, they consistently underperform in identifying weaknesses and adjusting feedback based on paper quality, generating 59.42% fewer entities in weaknesses sections compared to human reviewers.

## Method Summary
The study constructs a high-consensus benchmark by selecting papers from ICLR (2024, 2025) and NeurIPS (2023, 2024) using KDE-based consistency thresholds, categorizing them into good/borderline/weak tiers. Papers are converted to markdown with Nougat and segmented into IMRaD sections using Qwen2.5-72B. Five LLMs (GPT-4o, Gemini-2.0-Flash, Claude-3.5-Sonnet, Qwen2.5-72B-instruct, LLaMA3.3-70B-instruct) generate reviews using conference-specific rubric prompts. Evaluation combines semantic similarity via BGE-M3 embeddings and knowledge graph metrics (node/edge counts, entropy) using PL-Marker with SciERC schema, comparing human vs. LLM outputs across quality tiers.

## Key Results
- LLMs generate 15.74% more entities than human reviewers in strengths sections but 59.42% fewer entities in weaknesses sections
- GPT-4o increases node count by only 5.7% from good to weak papers, compared to 50% increase in human reviews
- LLM-generated content achieves higher semantic similarity to papers than human-written reviews, reflecting surface-level paraphrasing tendencies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs excel at descriptive/affirmational review content through surface-level text alignment.
- Mechanism: LLMs encode paper sections and review components into dense vectors (via BGE-M3), then reproduce closely paraphrased content with high semantic similarity. This works because summaries and strengths sections primarily require extraction and restatement of explicit claims rather than external reasoning.
- Core assumption: The semantic similarity between review component Ri and paper section Sj (cosine similarity of embeddings) reflects faithful comprehension rather than memorization or superficial pattern matching.
- Evidence anchors:
  - [abstract] "LLMs perform well in descriptive and affirmational content, capturing the main contributions and methodologies... generating 15.74% more entities than human reviewers in the strengths section"
  - [Page 9] "LLM-generated content generally achieves higher semantic similarity than human-written reviews. This reflects the models' tendency to closely paraphrase or replicate paper expressions"
  - [corpus] "Automatic Reviewers Fail to Detect Faulty Reasoning in Research Papers" confirms LLMs struggle with counterfactual reasoning but can reproduce surface claims.
- Break condition: When review sections require synthesizing unstated implications or identifying gaps (Weaknesses, Questions), surface alignment degrades—In-to-Out Ratio increases, indicating LLMs must introduce external knowledge they lack.

### Mechanism 2
- Claim: LLMs fail quality-sensitive feedback modulation, producing uniform evaluations across paper quality tiers.
- Mechanism: Human reviewers exhibit bidirectional gradients—more entities in weaknesses for weak papers (50% increase in node count from good to weak), fewer in strengths. LLMs generate near-constant output regardless of input quality. Assumption: This stems from RLHF alignment toward helpfulness/positivity rather than critical discrimination.
- Core assumption: The lack of structural variation reflects a fundamental training bias rather than prompt design alone (paper tested only fixed rubric-based prompts).
- Evidence anchors:
  - [abstract] "GPT-4o increases node count by only 5.7% from good to weak papers, compared to 50% in human reviews"
  - [Page 5-6] "LLMs tend to assign compressed and inflated scores, particularly overestimating borderline and weak submissions"
  - [corpus] "Pre-review to Peer review" warns that automated reviews risk surface plausibility without genuine critical evaluation; "ReviewRL" specifically notes rating consistency as a key failure mode.
- Break condition: When systematic quality stratification is required (e.g., distinguishing borderline from weak), LLM outputs flatten—score distributions compress, entity counts stabilize.

### Mechanism 3
- Claim: LLMs exhibit reduced conceptual richness in critical analysis, producing sparser knowledge graphs in weaknesses sections.
- Mechanism: Knowledge graphs extracted via PL-Marker (trained on SciERC schema) show human weaknesses contain more nodes, edges, and higher label entropy. LLM weaknesses sections show 59.42% fewer entities (GPT-4o vs. human on ICLR 2025). Assumption: Critical reasoning requires domain expertise and external knowledge not captured in pre-training.
- Core assumption: Entity/relation extraction metrics (node count, label entropy, in-context vs. out-of-context ratio) validly proxy "analytical depth."
- Evidence anchors:
  - [Page 9] "human-written Weaknesses sections tend to integrate a greater number of scientific entities and more diverse conceptual relations"
  - [Table II] GPT-4o shows 71.45% node reduction and 73.91% edge reduction in weaknesses vs. human for ICLR 2025 weak papers
  - [corpus] "Reviewing Scientific Papers for Critical Problems With Reasoning LLMs" proposes reasoning-enhanced models specifically to address this depth gap.
- Break condition: When papers contain subtle methodological flaws requiring inference beyond stated content, LLMs revert to generic or superficial criticisms.

## Foundational Learning

- Concept: Semantic similarity via embedding cosine distance
  - Why needed here: Core metric for measuring review-paper alignment; underpins all Section IV-C analysis.
  - Quick check question: Given two embedded vectors r and s, what does a cosine similarity of 0.92 vs. 0.45 suggest about their semantic relationship?

- Concept: Knowledge graph construction from scientific text
  - Why needed here: Enables structural evaluation beyond surface similarity; entity/relation extraction (SciERC schema, PL-Marker) quantifies conceptual depth.
  - Quick check question: If a review's KG has high node count but low label entropy, what does that indicate about the review's conceptual diversity?

- Concept: Kernel Density Estimation (KDE) for threshold selection
  - Why needed here: Used to identify reviewer consensus threshold non-parametrically; ensures benchmark papers have reliable ground-truth labels.
  - Quick check question: Why choose the second local minimum (Valley 2) rather than zero standard deviation for the consistency threshold?

## Architecture Onboarding

- Component map: PDF -> Nougat (markdown) -> IMRaD segmentation (Qwen2.5-72B) -> Entity/Relation extraction (PL-Marker/SciERC) -> 5 LLMs with rubric prompts -> BGE-M3 embeddings (semantic similarity) + KG metrics (nodes, edges, entropy, in/out-context ratio)

- Critical path:
  1. Paper ingestion and segmentation quality directly impacts all downstream metrics—errors here propagate to both similarity and KG analysis.
  2. Prompt design (rubric alignment) gates LLM review structure fidelity—current study uses fixed prompts per conference.
  3. Entity extraction model (PL-Marker/SciERC) defines what counts as "conceptual depth"—schema choice biases toward CS/AI terminology.

- Design tradeoffs:
  - Fixed prompts enable fair comparison but limit insight into prompt variation effects (explicitly noted as limitation).
  - High-consensus papers ensure reliable evaluation but exclude controversial/disagreed cases where LLMs might differ most from humans.
  - SciERC schema captures CS concepts well but may underrepresent domain-specific entities from interdisciplinary work.

- Failure signatures:
  - Flat score distributions across quality tiers -> LLM not discriminating paper quality
  - High semantic similarity + low out-of-context entities in weaknesses -> superficial criticism, no external knowledge synthesis
  - KG node counts <50% of human baseline in weaknesses -> critical analysis failure

- First 3 experiments:
  1. **Prompt ablation**: Test alternative prompting strategies (chain-of-thought, few-shot examples of quality-differentiated reviews) to isolate prompt vs. model capability effects on quality sensitivity.
  2. **Entity schema validation**: Compare PL-Marker/SciERC vs. domain-specific entity extractors on a held-out subset—measure whether depth gap persists across extraction methods.
  3. **Disagreed-papers extension**: Expand benchmark to papers with high reviewer variance (excluded in current KDE threshold) to test whether LLM-human divergence widens when ground truth is ambiguous.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can fine-tuning on domain-specific scientific corpora improve LLM performance in generating substantive critical feedback, particularly in identifying methodological weaknesses?
- Basis in paper: [inferred] The authors note that LLMs consistently underperform in "identifying weaknesses, raising substantive questions, and adjusting feedback based on paper quality," generating 59.42% fewer entities than humans in weaknesses sections, but do not test whether targeted training could address this.
- Why unresolved: The study evaluates only five LLMs with fixed prompts and no fine-tuning, leaving open whether the critical reasoning deficit is architectural or training-based.
- What evidence would resolve it: Comparative evaluation of domain-specific fine-tuned models versus base models on the same benchmark, measuring improvements in weakness identification and knowledge graph complexity.

### Open Question 2
- Question: How do different prompting strategies (e.g., chain-of-thought, multi-turn critique) affect LLMs' ability to produce quality-sensitive feedback that adjusts based on paper merit?
- Basis in paper: [explicit] The authors state as a limitation that they "adopt a fixed prompt design based on official rubrics" which "limits...insights of how prompt variations influence model performances, especially in evaluative tasks requiring nuanced critique or quality-sensitive reasoning."
- Why unresolved: The study deliberately controls for prompt variability to enable fair comparison, but cannot determine whether more sophisticated prompting might elicit better evaluative behavior.
- What evidence would resolve it: Systematic evaluation of multiple prompting strategies on the same paper set, measuring whether LLM-generated reviews show greater structural variation across quality tiers.

### Open Question 3
- Question: Do the observed behavioral patterns generalize to other scientific disciplines and publication venues with different review criteria and standards?
- Basis in paper: [explicit] The authors explicitly note "the benchmark dataset is limited to papers from ICLR and NeurIPS, which may constrain the generalizability of our findings to other venues with different disciplinary focuses, review practices, or evaluation standards."
- Why unresolved: The study focuses exclusively on ML/AI conferences; findings may not transfer to fields with different review norms, terminology, or evaluation criteria.
- What evidence would resolve it: Replication of the evaluation framework on diverse venues (e.g., biology, physics, humanities journals) with comparable benchmark construction and metrics.

### Open Question 4
- Question: What mechanisms underlie LLMs' evaluative bias toward uniformly positive assessments, and is this behavior inherent to current architectures or a product of training objectives?
- Basis in paper: [inferred] The paper documents "evaluative bias toward leniency" where LLMs "are more likely to assign uniformly positive assessments, particularly overrating borderline and weak submissions," but does not investigate the cause of this systematic bias.
- Why unresolved: Identifying the pattern differs from explaining it—whether the bias stems from RLHF alignment, training data distribution, or architectural limitations remains unexplored.
- What evidence would resolve it: Ablation studies comparing models with different training objectives, plus analysis of training data distributions for critical versus affirmative content.

## Limitations
- The benchmark excludes papers with controversial or disputed evaluations where human reviewers disagree significantly, potentially biasing toward consensus-driven evaluations
- Fixed rubric-based prompts for all LLMs limit insight into whether different prompting strategies could improve quality-sensitive feedback
- KG metrics may not fully capture the nuanced critical reasoning required for effective peer review beyond structural measures

## Confidence
**High Confidence**: The finding that LLMs excel at descriptive and affirmational content but underperform in weaknesses identification is well-supported by multiple evidence sources. The KG metrics showing 59.42% fewer entities in LLM-generated weaknesses sections compared to human reviews provides robust quantitative backing.

**Medium Confidence**: The conclusion about LLM inability to discriminate paper quality through feedback modulation is supported but could be influenced by prompt design limitations. The study notes this as a constraint, suggesting the observed behavior may reflect prompt constraints rather than fundamental model limitations.

**Low Confidence**: The assumption that KG metrics (node count, edge count, label entropy) fully capture "conceptual depth" in critical analysis may oversimplify the complexity of effective peer review. While these metrics show clear differences between human and LLM outputs, they may not capture all aspects of quality review writing.

## Next Checks
1. **Prompt Strategy Exploration**: Conduct systematic prompt ablation studies testing chain-of-thought prompting, few-shot examples demonstrating quality-differentiated reviews, and rubric-specific prompting variations to isolate whether observed quality sensitivity limitations stem from model capabilities or prompt design.

2. **Schema Validation and Expansion**: Compare PL-Marker/SciERC entity extraction against domain-specific schemas and gold-standard human annotations on a held-out subset of papers to validate whether the observed depth gap persists across different conceptual representation methods, particularly for interdisciplinary work.

3. **Controversial Papers Extension**: Expand the benchmark to include papers with high reviewer variance (those excluded by the current KDE threshold) to test whether LLM-human divergence patterns change when ground truth is ambiguous, providing insight into model behavior under uncertainty.