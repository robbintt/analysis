---
ver: rpa2
title: 'TextMineX: Data, Evaluation Framework and Ontology-guided LLM Pipeline for
  Humanitarian Mine Action'
arxiv_id: '2509.15098'
source_url: https://arxiv.org/abs/2509.15098
tags:
- arxiv
- knowledge
- triples
- evaluation
- extraction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TextMineX introduces the first dataset, evaluation framework, and
  ontology-guided LLM pipeline for knowledge extraction from humanitarian mine action
  reports. The approach structures unstructured technical reports into (subject, relation,
  object) triples using ontology-aligned prompts, enabling structured knowledge sharing
  across demining agencies.
---

# TextMineX: Data, Evaluation Framework and Ontology-guided LLM Pipeline for Humanitarian Mine Action

## Quick Facts
- arXiv ID: 2509.15098
- Source URL: https://arxiv.org/abs/2509.15098
- Reference count: 35
- Primary result: Ontology-aligned prompts improve extraction accuracy by up to 44.2% and reduce hallucinations by 22.5% for humanitarian mine action reports

## Executive Summary
TextMineX introduces the first dataset, evaluation framework, and ontology-guided LLM pipeline for knowledge extraction from humanitarian mine action (HMA) reports. The approach structures unstructured technical reports into (subject, relation, object) triples using ontology-aligned prompts, enabling structured knowledge sharing across demining agencies. Experiments demonstrate that ontology-aligned prompts improve extraction accuracy by up to 44.2%, reduce hallucinations by 22.5%, and enhance format adherence by 20.9% compared to baseline models. The work also introduces a bias-aware LLM-as-Judge evaluation framework that mitigates position bias in reference-free scoring, providing a reliable alternative to traditional evaluation methods. The dataset and code are publicly released for research use.

## Method Summary
TextMineX processes PDF reports using Open-Parse for layout-aware paragraph chunking (~127 words), then applies ontology-guided in-context learning with demonstration examples to extract (subject, relation, object) triples. The pipeline uses ontology-aligned prompts that retrieve semantically relevant examples to constrain the label space and improve precision. Evaluation combines reference-based metrics (BLEU, ROUGE, METEOR, BERTScore, hallucination rate) with a bias-aware LLM-as-Judge framework that mitigates positional bias through randomized candidate ordering. The HMA ontology includes 160 entity types and 86 relation types from IMSMA Core plus Empathi, filtered for demining relevance.

## Key Results
- Ontology-aligned prompts improve extraction accuracy by up to 44.2% compared to baseline models
- Hallucination rate reduced by 22.5% through ontology-aligned demonstrations
- LLM-as-Judge framework achieves 90% alignment with reference-based scoring while mitigating positional bias

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ontology-aligned prompt demonstrations can improve triple extraction accuracy and reduce hallucinations for domain-specific information extraction.
- Mechanism: Providing in-context examples that share the same entity and relation types as the target task primes the LLM with ontology-specific reasoning patterns, constraining the output label space and improving faithfulness to the source text.
- Core assumption: The LLM has sufficient foundational knowledge to map textual descriptions to the provided ontology schema, and the demonstration examples are representative of the target extraction patterns.
- Evidence anchors:
  - [abstract] "Our experiments show that ontology-aligned prompts improve extraction accuracy by up to 44.2%, reduce hallucinations by 22.5%, and enhance format adherence by 20.9% compared to baseline models."
  - [section 4.2] "Semantically aligned demonstrations help constrain the label space and improve precision... the contrast between RS/RP and OS/OP confirms the benefit of ontology alignment."
  - [corpus] ODKE+ (arXiv:2509.04696) demonstrates related benefits of ontology-guided extraction for open-domain KG construction, providing contextual support for this mechanism.
- Break condition: Effectiveness degrades if (1) the ontology schema is poorly defined or too complex for the LLM to parse, (2) demonstration examples contain noise or are semantically misaligned, or (3) the target domain language deviates significantly from the demonstration distribution.

### Mechanism 2
- Claim: Positional bias in LLM-as-Judge evaluation can be mitigated through randomized candidate ordering and explicit reasoning criteria.
- Mechanism: By shuffling the order of candidate outputs and requiring structured, criteria-based scoring before ranking, the judge LLM's tendency to favor certain positions is reduced, leading to more reliable rankings.
- Core assumption: The judge LLM is capable of following complex evaluation instructions and that explicit reasoning steps improve judgment consistency.
- Evidence anchors:
  - [abstract] "A bias-aware LLM-as-Judge framework further improves evaluation reliability by mitigating positional bias, achieving 90% alignment with reference-based scoring."
  - [Table 2] Shows Spearman's correlation for GPT-4o improving from 0.4 (Basic) to 1.0 (Randomized), while Llama3.1-70B shows no improvement, indicating model-dependent efficacy.
  - [corpus] Limited direct evidence; corpus papers on ontology learning and KG evaluation do not specifically address positional bias in LLM judging.
- Break condition: Mitigation fails if (1) the judge LLM has strong inherent positional biases that persist despite randomization, or (2) the evaluation criteria are ambiguous or inconsistently applied, leading to unreliable scoring.

### Mechanism 3
- Claim: Layout-aware document chunking preserves contextual information necessary for extracting relations across sentence boundaries.
- Mechanism: Segmenting PDF documents into paragraph-level chunks based on layout analysis maintains semantic coherence and provides sufficient context for the LLM to reason about entities and relations that span multiple sentences.
- Core assumption: Paragraph-level chunks contain enough context for most relevant relations, and chunk boundaries do not arbitrarily split critical relational information.
- Evidence anchors:
  - [section 4.2] "We leverage Open-Parse's document understanding capabilities... to identify layout elements and extract paragraph-level segments. On our reports, this yields chunks averaging 127 words... which aligns well with both small and large LLM context limits."
  - [section 1] Mentions that prior ontology-guided methods are "limited to single-sentence inputs and overlook context-level reasoning," which TextMineX addresses.
  - [corpus] OntoMetric (arXiv:2512.01289) discusses structured document processing for KG generation, indirectly supporting the need for layout-aware approaches.
- Break condition: Chunking fails if (1) critical relations depend on entities appearing in separate chunks, or (2) the chunk size is too small to provide adequate reasoning context or too large for the LLM's effective context window.

## Foundational Learning

- Concept: Knowledge Graph Construction via Triple Extraction
  - Why needed here: TextMineX's core task is extracting (subject, relation, object) triples from unstructured text, which requires understanding how to map free-form language to structured schema.
  - Quick check question: Given the sentence "The mine clearance operation in Village X removed 50 anti-personnel mines," what triples could be extracted if the ontology includes `hasClearanceLocation` and `hasMunitionsCleared` relations?

- Concept: In-Context Learning (ICL) with LLMs
  - Why needed here: The pipeline relies on prompt engineering with demonstration examples to guide extraction without fine-tuning, making ICL fundamental to the approach.
  - Quick check question: How does the performance of zero-shot prompting compare to one-shot prompting with ontology-aligned examples in the TextMineX experiments?

- Concept: LLM Evaluation Metrics and Hallucination Detection
  - Why needed here: Assessing extraction quality requires understanding accuracy metrics (BLEU, ROUGE, BERTScore), format conformance, and methods for detecting ungrounded information.
  - Quick check question: What specific criteria does the paper use to flag a triple as a "hallucination," and why is hallucination rate a critical metric for humanitarian applications?

## Architecture Onboarding

- Component map: PDF reports → Layout-aware chunking → Text chunks → Ontology Layer → Extraction Engine (LLM call) → Raw triples → Evaluation Module → Combined score → Final knowledge graph

- Critical path: High-quality ontology definition → Representative demonstration selection → Consistent prompt construction → Reliable evaluation pipeline

- Design tradeoffs:
  - Model size vs. efficiency: Larger models (GPT-4o, Llama3-70B) show better performance but higher cost/latency.
  - Chunk size vs. context: Larger chunks preserve more context but increase token usage; paragraph-level averaging 127 words was chosen as a balance.
  - Evaluation complexity: Reference-based metrics require annotated data; LLM-as-Judge is more scalable but requires bias mitigation.

- Failure signatures:
  - High hallucination rate: Indicates demonstrations are misaligned or ontology constraints are insufficient.
  - Low format conformance: Suggests prompt template is unclear or model struggles with structured output.
  - Poor LLM-as-Judge alignment: May indicate persistent positional bias or inadequate judge prompt design.
  - Low recall: Could result from overly restrictive ontology or poor chunk segmentation.

- First 3 experiments:
  1. Baseline comparison: Run zero-shot extraction on 10 sample chunks with GPT-4o and a smaller model (e.g., Llama3-8B) to establish performance bounds.
  2. Prompt strategy ablation: Compare zero-shot, random demonstration, and ontology-aligned demonstration prompts on a held-out validation set to quantify ontology alignment benefits.
  3. Judge reliability test: Implement Basic, Fair, and Randomized Fair judge prompts with multiple candidate outputs, measuring correlation with reference-based scores to validate the evaluation framework.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the TextMineX pipeline maintain extraction accuracy and ontology conformance when applied to non-English HMA reports, such as those in Arabic, French, or Ukrainian?
- Basis in paper: [explicit] The authors state in the Limitations section: "In the future, we plan to extend the work to include other languages... Making TextMineX multilingual is, therefore, needed to ensure more inclusiveness."
- Why unresolved: The current dataset, curation, and experiments were restricted exclusively to English-language reports from CMAC and GICHD.
- What evidence would resolve it: Evaluating the pipeline's Combined Score and hallucination rates on a new dataset of reports natively written in the target languages.

### Open Question 2
- Question: Can the ontology-guided extraction pipeline generalize to adjacent humanitarian domains, such as natural disaster management, without significant restructuring?
- Basis in paper: [explicit] The authors note: "the applicability of the pipeline in similar domains (e.g, natural disaster management) could be tested as a potential future work."
- Why unresolved: The framework was validated specifically for the demining domain using specialized ontologies (IMSMA Core, Empathi filtered for HMA).
- What evidence would resolve it: A transfer learning experiment applying the pipeline to natural disaster reports and measuring the extent of manual ontology adjustment required to achieve comparable accuracy.

### Open Question 3
- Question: Can alternative LLMs or ensemble judge strategies achieve higher alignment with reference-based scoring than the current 90% achieved by the Randomized Fair Judge method?
- Basis in paper: [explicit] The authors conclude: "Future research could refine this approach by exploring additional LLMs and judge strategies to approximate a even better judge LLM setting for knowledge triple extraction tasks."
- Why unresolved: While GPT-4o with randomization achieved high alignment, other models (e.g., Llama3.1-70B) showed persistent positional bias, suggesting the optimal evaluation configuration is not fully solved.
- What evidence would resolve it: Systematic testing of diverse LLM judge architectures and ensemble techniques, measuring Spearman's correlation against the human-annotated ground truth.

## Limitations

- The evaluation framework's generalizability to other domains remains unproven, as the LLM-as-Judge approach was tested only on a single dataset of 100 annotated prompts.
- The pipeline's effectiveness is heavily dependent on the quality and completeness of the HMA ontology, with performance potentially degrading if critical relations are missing.
- The claim of being the "first" dataset and framework for HMA knowledge extraction cannot be definitively verified given the limited scope of existing literature in this specialized domain.

## Confidence

- **High Confidence**: The core finding that ontology-aligned demonstrations improve extraction accuracy (44.2%) and reduce hallucinations (22.5%) is well-supported by experimental results across multiple baseline comparisons.
- **Medium Confidence**: The bias-aware LLM-as-Judge framework shows strong performance on the TextMineX dataset, but its generalizability to other domains remains unproven.
- **Low Confidence**: The assertion that TextMineX is the "first" dataset and framework for HMA knowledge extraction cannot be definitively verified.

## Next Checks

1. **Cross-Domain Generalization Test**: Apply the LLM-as-Judge framework to a different knowledge extraction task (e.g., medical or legal document analysis) to validate whether the positional bias mitigation techniques transfer effectively.

2. **Ontology Coverage Analysis**: Systematically measure extraction performance as a function of ontology completeness - remove subsets of relations and measure degradation to quantify how sensitive the approach is to ontology quality.

3. **Real-World Impact Assessment**: Deploy the pipeline on a larger corpus of HMA reports (beyond the 5 CMAC reports) and evaluate whether the structured knowledge extraction actually improves coordination outcomes or decision-making in humanitarian demining operations.