---
ver: rpa2
title: 'ToolSample: Dual Dynamic Sampling Methods with Curriculum Learning for RL-based
  Tool Learning'
arxiv_id: '2509.14718'
source_url: https://arxiv.org/abs/2509.14718
tags:
- tool
- learning
- training
- arxiv
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DSCL, a dual dynamic sampling framework for
  reinforcement learning-based tool learning. The key insight is that standard dynamic
  sampling methods fail in tool learning due to its multi-task structure and multi-valued
  reward mechanisms.
---

# ToolSample: Dual Dynamic Sampling Methods with Curriculum Learning for RL-based Tool Learning

## Quick Facts
- arXiv ID: 2509.14718
- Source URL: https://arxiv.org/abs/2509.14718
- Authors: Zihao Feng; Xiaoxue Wang; Bowen Wu; Hailong Cao; Tiejun Zhao; Qun Yu; Baoxun Wang
- Reference count: 17
- Key outcome: DSCL achieves 3.29% improvement over baselines on BFCLv3 benchmark

## Executive Summary
This paper introduces DSCL, a dual dynamic sampling framework for reinforcement learning-based tool learning. The key insight is that standard dynamic sampling methods fail in tool learning due to its multi-task structure and multi-valued reward mechanisms. DSCL combines Reward-Based Dynamic Sampling, which uses mean and variance statistics to prioritize informative samples, with Task-Based Dynamic Curriculum Learning, which adaptively focuses on sub-tasks based on difficulty and convergence status. Experiments on BFCLv3 and API-Bank benchmarks show DSCL achieves a 3.29% improvement over baselines, with the method successfully filtering out low-value samples while maintaining training stability.

## Method Summary
DSCL implements two complementary sampling strategies: Reward-Based Dynamic Sampling (RDS) that categorizes training samples based on their mean reward and variance across rollouts/epochs, and Task-Based Dynamic Curriculum Learning (TDCL) that progressively upweights harder sub-tasks (format → name/key → value). The method requires a warmup phase where RDS remains inactive until the model achieves stable format prediction (mean reward > 1.0 for 7 consecutive batches). RDS then applies thresholds t_mean=0.5 and t_var=0.1 to assign sampling ratios (0.0, 0.5, or 1.0) to samples, while TDCL implements three-stage curriculum adjusting reward weights from 0.5× to 2.5× across different components.

## Key Results
- DSCL achieves 3.29% improvement on BFCLv3 benchmark over ToolRL baseline
- RDS without warmup (47.34%) performs worse than baseline, demonstrating necessity of curriculum
- TDCL shows steepest improvement on parameter value rewards, confirming focus on hardest sub-tasks
- Method successfully filters low-value samples while maintaining training stability across all sub-tasks

## Why This Works (Mechanism)

### Mechanism 1: Multi-dimensional reward variance as sample quality signal
Using both reward mean AND variance provides complementary signals for identifying valuable training samples in multi-valued reward settings. Unlike binary rewards where variance is mathematically locked to mean, multi-valued rewards in tool learning decouple them, allowing samples with identical means to have vastly different variances. High variance across rollouts indicates exploratory behavior; high variance across epochs indicates unstable learning history—both signal valuable samples.

### Mechanism 2: Warmup-before-sampling curriculum prevents format collapse
Delaying dynamic sampling until the model achieves format stability prevents catastrophic data loss and training instability. Tool learning requires strict structured output formatting. If dynamic sampling activates too early, the model hasn't learned reliable formatting, causing low rewards across most samples. RDS would then discard these "low-value" samples, creating a feedback loop where the model never sees enough data to learn formatting.

### Mechanism 3: Asynchronous sub-task curriculum redirects capacity to harder tasks
Progressively upweighting harder sub-task rewards (format → name/key → value) improves final performance on complex tasks. Tool learning sub-tasks have natural dependencies and different difficulty levels. The three-stage curriculum upweights format rewards 2.5× in Stage 1, then shifts focus to name/key (1.5×) in Stage 2, then value (2.5×) in Stage 3. This prevents early gradient noise from hard tasks while ensuring late-stage focus on bottleneck skills.

## Foundational Learning

- **GRPO (Group Relative Policy Optimization)**
  - Why needed here: DSCL is built on top of GRPO's multi-sample rollout generation. RDS operates on G rollouts per data point (G=8 in this paper). Without understanding GRPO's advantage calculation from grouped samples, the variance-based filtering won't make sense.
  - Quick check question: Can you explain why GRPO generates multiple rollouts per prompt and how advantages are computed relative to the group mean?

- **Curriculum Learning (Easy-to-Hard)**
  - Why needed here: Both RDS (warmup → sampling) and TDCL (format → value) are curriculum strategies. The paper extends classic curriculum learning to operate on fine-grained reward dimensions rather than just task-level difficulty.
  - Quick check question: What happens if you train on randomly shuffled data vs. sorted by difficulty? How does this paper's approach differ?

- **Multi-valued vs Binary Reward Design**
  - Why needed here: The entire mechanism hinges on tool learning having richer reward structure than pass/fail. The paper decomposes rewards into R_format, R_name, R_key, R_value, each with different scales and semantics.
  - Quick check question: In binary reward settings, why is variance fully determined by mean? How does adding intermediate reward values change this relationship?

## Architecture Onboarding

- **Component map:**
  Training Loop -> Sample batch -> Generate G rollouts per sample (GRPO) -> Compute 4 reward components: R_format, R_name, R_key, R_value -> [RDS Module] -> Calculate M_i,j (mean), V_sample (rollout variance), V_epoch (history variance) -> Categorize samples: easy/hard/intermediate × high/low variance -> Output Ratio_i ∈ {0.0, 0.5, 1.0} per sample -> [TDCL Module] -> Check stage via recent batch rewards -> Reweight reward components per Eq 11-13 -> Compute advantage Â = Ratio × TDCL_adjusted_advantage -> Policy update

- **Critical path:**
  1. Implement reward computation (Eqs 1-6) exactly as specified—this is the foundation
  2. Add warmup detection: track running mean of rewards, trigger RDS after 7 consecutive batches > 1.0
  3. Implement RDS categorization with t_mean=0.5, t_var=0.1 as starting hyperparameters
  4. Add TDCL stage transitions based on sub-task reward monitoring

- **Design tradeoffs:**
  - **Threshold sensitivity (t_mean, t_var):** Paper treats these as hyperparameters. Lower t_mean = more aggressive filtering; lower t_var = stricter variance requirement. Both need tuning per dataset.
  - **Warmup length:** 7 batches is operationalized but arbitrary. Longer warmup = safer but slower; shorter = riskier but may work with strong base models.
  - **Stage boundaries:** TDCL doesn't specify exact transition criteria (only "recorded rewards of each sub-task of several latest batches"). You'll need to define explicit thresholds.

- **Failure signatures:**
  - **Format collapse:** Rewards drop sharply after RDS activation → warmup insufficient
  - **Stagnant variance:** All samples converge to same category → thresholds need adjustment
  - **Multi-turn degradation:** Performance on complex tasks doesn't improve → TDCL not transitioning to Stage 3
  - **Data starvation:** Training sees very few samples (Ratio ≈ 0 for most) → t_mean too high or t_var too low

- **First 3 experiments:**
  1. **Ablation warmup:** Run RDS without curriculum warmup on a small subset—replicate Table 3 result to validate your implementation
  2. **Variance threshold sweep:** Test t_var ∈ {0.05, 0.1, 0.2} while holding t_mean fixed; plot samples retained per category over training
  3. **Stage transition timing:** Instrument TDCL to log exactly when stage transitions occur; verify Stage 3 activates and correlates with parameter value reward improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How sensitive is the Reward-Based Dynamic Sampling (RDS) performance to the specific values of the hyperparameters $t_{mean}$ and $t_{var}$?
- Basis in paper: The authors state that "$t_{mean}$ and $t_{var}$ are treated as hyperparameters that are adjusted during training. This allows the method to accommodate the sensitivity of the reinforcement learning process to variations in data and model states."
- Why unresolved: While the paper identifies these thresholds as crucial for categorizing data (easy, hard, intermediate), it does not provide a systematic ablation study on how different threshold values affect the sample retention rate or final model accuracy.
- What evidence would resolve it: A sensitivity analysis plotting model performance and data retention rates against varying values of $t_{mean}$ and $t_{var}$.

### Open Question 2
- Question: Can DSCL be effectively adapted for tasks with binary rewards, or is it strictly dependent on multi-valued reward functions?
- Basis in paper: The introduction contrasts tool learning with standard tasks, noting that binary rewards "mathematically locks the variance to the mean" whereas the proposed method relies on a multi-valued setting which "decouples them."
- Why unresolved: The paper assumes the benefit comes from the decoupling of mean and variance. It is unclear if the specific RDS formulas (which depend on independent mean/variance thresholds) would degrade or fail if applied to tasks where variance is a deterministic function of the mean.
- What evidence would resolve it: An experiment applying the DSCL framework to a standard binary-reward RL task (e.g., mathematical reasoning) and comparing performance against baselines designed for binary rewards.

### Open Question 3
- Question: Does the strategy of discarding "hard" data (low mean, low variance) lead to a regression in performance on the most complex edge cases?
- Basis in paper: The methodology explicitly sets the sampling ratio to 0.0 for "Hard data" (category b.2) where $M_{i,j} < t_{mean}$ and variance is low.
- Why unresolved: Low variance on hard samples indicates consistent failure (the model doesn't know the answer). While excluding them stabilizes training, it may prevent the model from ever learning to solve the most difficult instances in the training distribution, potentially creating a "glass ceiling" for performance.
- What evidence would resolve it: A comparative analysis of error types on the hardest evaluation subset (e.g., BFCL Multi-Turn) between the standard DSCL and an ablation that retains hard samples.

## Limitations
- Threshold values (t_mean=0.5, t_var=0.1) presented as hyperparameters without systematic sensitivity analysis
- TDCL stage transition criteria remain underspecified—only mentions monitoring "recorded rewards of several latest batches"
- Method assumes meaningful reward distribution variance, which may not hold for all tool learning datasets
- Claim that TDCL specifically improves multi-turn dialogue performance lacks detailed breakdown

## Confidence

- **High Confidence:** The core insight about mean-variance decoupling in multi-valued rewards is well-supported by theoretical analysis and empirical validation showing binary rewards collapse to theoretical curves while tool learning rewards scatter widely.
- **Medium Confidence:** The specific threshold values and their impact on sample filtering are reasonable given ablation results, but sensitivity to these hyperparameters isn't thoroughly explored.
- **Low Confidence:** The claim that TDCL specifically improves multi-turn dialogue performance lacks detailed breakdown—overall improvement is shown but contribution of each curriculum stage to different sub-task categories isn't clearly isolated.

## Next Checks

1. **Threshold Sensitivity Analysis:** Systematically vary t_mean and t_var across [0.3, 0.5, 0.7] and [0.05, 0.1, 0.15] respectively, measuring retained sample ratios and final task performance to identify optimal operating ranges.

2. **Stage Transition Definition:** Implement explicit criteria for TDCL stage transitions (e.g., "Stage 1 → 2 when format reward > 0.8 for 3 consecutive batches") and verify that stage progression correlates with expected reward component improvements.

3. **Dataset Generalization Test:** Apply DSCL to a simpler tool learning dataset with more binary-like rewards to validate the claim that variance-based filtering specifically benefits multi-valued reward settings rather than general RL training.