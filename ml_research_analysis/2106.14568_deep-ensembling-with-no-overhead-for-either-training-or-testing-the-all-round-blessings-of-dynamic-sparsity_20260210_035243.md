---
ver: rpa2
title: 'Deep Ensembling with No Overhead for either Training or Testing: The All-Round
  Blessings of Dynamic Sparsity'
arxiv_id: '2106.14568'
source_url: https://arxiv.org/abs/2106.14568
tags:
- ensemble
- training
- sparse
- edst
- dense
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FreeTickets, a novel ensemble learning framework
  that leverages dynamic sparse training to create efficient and accurate ensembles
  of sparse subnetworks. FreeTickets addresses the computational and memory limitations
  of traditional deep ensembles by directly training sparse subnetworks from scratch
  and extracting diverse yet accurate subnetworks during this efficient, sparse-to-sparse
  training.
---

# Deep Ensembling with No Overhead for either Training or Testing: The All-Round Blessings of Dynamic Sparsity

## Quick Facts
- arXiv ID: 2106.14568
- Source URL: https://arxiv.org/abs/2106.14568
- Reference count: 40
- Outperforms dense baselines in accuracy, uncertainty estimation, and robustness while using ~1/5 the training FLOPs of naive deep ensembles on ImageNet

## Executive Summary
This paper introduces FreeTickets, a framework for creating efficient deep ensembles by leveraging dynamic sparse training (DST). The core insight is that DST's continuous exploration of sparse topologies during training naturally yields diverse yet accurate subnetworks. FreeTickets instantiates this idea through two methods: DST Ensemble (independent sparse training runs) and EDST Ensemble (single-run with shared exploration). Both methods significantly reduce computational overhead while improving predictive accuracy, uncertainty estimation, and out-of-distribution robustness compared to dense ensembles and single sparse models.

## Method Summary
FreeTickets trains sparse neural networks using Dynamic Sparse Training (RigL) with periodic pruning and regrowth of connections. DST Ensemble independently trains M sparse networks, while EDST Ensemble performs one exploration phase followed by M sequential refinement phases with global topology perturbations to ensure diversity. Ensemble predictions are formed by averaging the probability distributions from all subnetworks. The framework uses Erdős-Rényi-Kernel initialization and standard RigL hyperparameters, with sparsity levels of 0.8-0.9 applied to CIFAR-10/100 (WideResNet) and ImageNet (ResNet-50) datasets.

## Key Results
- FreeTickets with ResNet50 outperforms naive deep ensemble on ImageNet using only ~1/5 of the training FLOPs
- EDST Ensemble generates diverse subnetworks in a single training run, achieving significant efficiency gains
- Sparse ensembles demonstrate superior uncertainty estimation and out-of-distribution robustness compared to dense ensembles
- Analysis confirms that diversity among subnetworks is a key driver of improved ensemble performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic Sparse Training inherently promotes model diversity by exploring different sparse topologies
- Mechanism: DST starts with a sparse network and periodically prunes and grows connections. This continuous exploration with random or gradient-based regrowth forces the network to converge to different sparse connectivities in separate runs or phases, leading to functional diversity among the subnetworks
- Core assumption: Different sparse connectivities in the parameter space map to functionally diverse predictions in the output space
- Evidence anchors: Abstract states FreeTickets "extract diverse yet accurate subnetworks"; page 3 references plenitude of different topological subnetworks; related work supports link between dynamic sparsity and ensemble effectiveness

### Mechanism 2
- Claim: EDST Ensemble achieves computational efficiency by amortizing the cost of exploration across all subnetworks
- Mechanism: EDST performs one exploration phase (high learning rate) to find a good parameter space region, then performs M sequential refinement phases. The initial exploration cost is shared across all M subnetworks, making individual refinement costs a fraction of a dense model's
- Core assumption: A single exploration phase can find a parameter region containing multiple high-quality, diverse sparse subnetworks
- Evidence anchors: Abstract mentions generating diverse subnetworks in single training run; page 5 states FLOPs required is significantly smaller than individual dense network; corpus evidence for this specific amortization mechanism is weak

### Mechanism 3
- Claim: Ensemble averaging improves predictive performance, uncertainty estimation, and robustness by combining diverse predictions
- Mechanism: The final output averages probability distributions from all M subnetworks, smoothing individual model errors and overconfident predictions. Diversity ensures models make different mistakes, making averaging more effective
- Core assumption: The errors of individual subnetworks are uncorrelated or negatively correlated
- Evidence anchors: Page 4 describes ensemble output as average of probability distributions; page 7 confirms more diversity leads to better accuracy and robustness; related work discusses aggregation methods

## Foundational Learning

- **Dynamic Sparse Training (DST)**: The core technique for generating efficient, diverse subnetworks through prune-and-grow cycles. Why needed: Essential for configuring FreeTickets' parameter exploration mechanism. Quick check: What are the three main steps of a typical DST algorithm like RigL?

- **Ensemble Diversity Metrics**: Measures of how different ensemble members are, such as KL divergence and prediction disagreement. Why needed: Paper explicitly links diversity to performance and uses these metrics for validation. Quick check: Why is high diversity among ensemble members desirable, and what is one metric used to measure it?

- **Uncertainty Estimation (NLL, ECE)**: Metrics for evaluating model confidence and calibration, including Negative Log-Likelihood and Expected Calibration Error. Why needed: Key outcome is improved uncertainty estimation, validated using these metrics. Quick check: What does the Expected Calibration Error (ECE) measure, and is a lower or higher value better?

## Architecture Onboarding

- **Component map**: ERK Initialization -> SGD + Prune/Grow loop (exploration) -> SGD + Prune/Grow loop (refinement, for EDST) -> Ensemble Average
- **Critical path**: Sparse Initialization → [SGD + Prune/Grow] loop (exploration) → [SGD + Prune/Grow] loop (refinement, for EDST) → Ensemble Average
- **Design tradeoffs**:
  - DST Ensemble vs. EDST Ensemble: DST offers potentially higher diversity but requires N training runs; EDST is far more efficient but may yield slightly lower diversity
  - Sparsity (S) vs. Accuracy: Higher sparsity reduces FLOPs and allows more subnetworks but risks degrading individual subnetwork accuracy
  - Exploration Rate (p) vs. Stability: High exploration rate promotes diversity but can destabilize training
- **Failure signatures**:
  - Low Ensemble Accuracy: Check individual subnetwork accuracy and diversity metrics
  - High Training Cost: Verify sparsity enforcement and that EDST isn't re-running exploration phase
  - Unstable Training: Exploration rate may be too high or update interval too small
- **First 3 experiments**:
  1. Reproduce CIFAR-10 Baseline: Implement EDST Ensemble on WideResNet-28-10 with S=0.8, M=3, verify competitive accuracy with dense model
  2. Ablate Exploration: Run EDST Ensemble with parameter exploration disabled during refinement phases, compare accuracy and diversity
  3. Sweep Sparsity: Train EDST Ensembles with sparsity levels S ∈ {0.7, 0.8, 0.9} on CIFAR-10, plot trade-off between FLOPs, ensemble size, and accuracy

## Open Questions the Paper Calls Out

- Can FreeTickets achieve realized wall-clock speedups on hardware platforms that match its theoretical FLOPs reduction? (Section 6 calls for exploring sparse ensembles on hardware for real speedups)
- Why do static sparse ensembles occasionally outperform DST-based ensembles in adversarial robustness settings? (Appendix L notes this observation and calls for further analysis)
- Does the FreeTickets framework translate effectively to Transformer architectures and Natural Language Processing tasks? (Paper limits experiments to CNNs despite acknowledging exploding size of Transformers)

## Limitations

- Diversity analysis relies on KL divergence, which may not fully capture functional diversity relevant to ensemble performance
- Theoretical FLOPs savings may not translate to actual hardware speedups due to lack of optimized sparse tensor cores
- Static sparse ensembles can occasionally outperform dynamic sparse ensembles in adversarial robustness, contradicting general performance trends

## Confidence

- **High Confidence**: Claims about reducing training FLOPs compared to dense ensembles, and basic framework of using DST to generate diverse subnetworks
- **Medium Confidence**: Claims about improved uncertainty estimation and out-of-distribution robustness, relying on specific metric implementations
- **Low Confidence**: Claims about unique advantages of sparse networks beyond efficiency, lacking direct comparative studies isolating sparse network properties

## Next Checks

1. **Ablate Exploration Mechanism**: Run EDST Ensemble with and without the global exploration step (q=0.8) between refinement phases, compare ensemble accuracy and KL divergence
2. **Isolate Sparse Network Properties**: Train a single sparse network with same sparsity and RigL schedule as FreeTickets but without ensemble averaging, compare its NLL and ECE to dense baseline
3. **Vary Exploration Rate**: Systematically sweep exploration rate (p) during refinement phases in EDST Ensemble, identify threshold where further exploration no longer improves performance