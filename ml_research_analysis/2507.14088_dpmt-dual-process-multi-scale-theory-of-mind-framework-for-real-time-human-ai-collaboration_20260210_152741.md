---
ver: rpa2
title: 'DPMT: Dual Process Multi-scale Theory of Mind Framework for Real-time Human-AI
  Collaboration'
arxiv_id: '2507.14088'
source_url: https://arxiv.org/abs/2507.14088
tags:
- human
- partner
- reasoning
- domain
- collaboration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles the challenge of real-time human-AI collaboration\
  \ in dynamic scenarios where AI agents must adapt to diverse and unseen human behaviors.\
  \ The core issue is that existing LLM agents lack the human-like cognitive ability\
  \ known as \u201Ctheory of mind\u201D (ToM), which is essential for understanding\
  \ and predicting others\u2019 mental states and intentions."
---

# DPMT: Dual Process Multi-scale Theory of Mind Framework for Real-time Human-AI Collaboration

## Quick Facts
- arXiv ID: 2507.14088
- Source URL: https://arxiv.org/abs/2507.14088
- Reference count: 4
- Primary result: DPMT achieves higher collaboration scores in Overcooked (121 vs 99 on Ring map) by modeling human partners' mental states through multi-scale Theory of Mind

## Executive Summary
This paper addresses the challenge of real-time human-AI collaboration in dynamic environments where AI agents must adapt to diverse and unseen human behaviors. The core problem is that existing LLM agents lack Theory of Mind (ToM) capabilities—the human-like ability to understand and predict others' mental states and intentions. The authors propose a novel Dual Process Multi-scale Theory of Mind (DPMT) framework inspired by cognitive science's dual process theory, which includes a fast system for quick decision-making and a slow system for deep ToM reasoning. Experiments on the Overcooked collaborative task demonstrate that DPMT significantly outperforms baseline methods, achieving higher collaboration scores and better subjective judgment scores from participants.

## Method Summary
The DPMT framework employs a dual-process architecture where a "Fast System" (using smaller LLM like Llama-13B) handles high-frequency macro-action selection, while a "Slow System" (using larger LLM like GPT-4o) performs complex ToM reasoning through a three-tiered process: inferring domain knowledge, cognitive style, and domain intention. The slow system updates the fast system's context periodically rather than per step. Macro-actions are selected by computing token probabilities for predefined actions and normalizing these to form probability distributions. The framework uses multi-threading to maintain real-time performance while enabling deep social reasoning about human partners.

## Key Results
- DPMT achieves significantly higher collaboration scores than baselines (121 vs 99 on Ring map, 68 vs 43 on Bottleneck map)
- Subjective judgment scores from participants show better game ability, collaboration fluidity, and ToM ability
- Ablation studies confirm the effectiveness of the multi-scale ToM module, especially in complex maps
- The framework successfully handles diverse human behaviors without direct communication

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decoupling deliberation (Slow System) from execution (Fast System) allows LLM agents to meet real-time constraints while maintaining deep social reasoning.
- **Mechanism:** The architecture runs two parallel threads. A "Fast System" (using a smaller LLM, e.g., Llama-13B) handles high-frequency macro-action selection to maintain flow. Concurrently, a "Slow System" (using a larger LLM, e.g., GPT-4o) performs complex Theory of Mind (ToM) reasoning. The Slow System updates the Fast System's context periodically rather than per step.
- **Core assumption:** The latency of large frontier models is too high for step-level decision-making in dynamic environments, but a smaller model can suffice if primed with high-quality context from a slower, smarter observer.
- **Evidence anchors:**
  - [section] Page 4: "our fast system in DPMT leverages a smaller-scale LLM, such as llama-13B, to reduce latency... These two systems are implemented using multi-threading."
  - [abstract] "...drawing inspiration from cognitive science dual process theory."
  - [corpus] "Leveraging Dual Process Theory in Language Agent Framework..." supports the difficulty of real-time interaction due to latency.

### Mechanism 2
- **Claim:** Structuring ToM as a three-stage dependency (Knowledge → Style → Intention) improves generalization to unseen human behaviors compared to direct action prediction.
- **Mechanism:** The "Slow System" enforces a hierarchical reasoning chain. It first infers the partner's **Domain Knowledge** (do they know the rules?), then their **Cognitive Style** (are they methodical or chaotic?), and finally their **Domain Intention** (what is their immediate goal?). This explicit intermediate state prevents the agent from hallucinating intentions without context.
- **Core assumption:** Human behavior is consistent with a latent cognitive style and knowledge state that can be inferred from short-term trajectories.
- **Evidence anchors:**
  - [section] Page 3: "This ToM process follows a three-tiered reasoning process, which progresses from domain knowledge to cognitive style, and ultimately to domain intention."
  - [section] Figure 6: Ablation studies show significant performance drops when "style" or "knowledge" layers are removed, specifically on complex maps.

### Mechanism 3
- **Claim:** Grounding macro-action selection in token probabilities rather than free-form text generation reduces execution errors.
- **Mechanism:** Instead of generating a plan via chat, the Fast System computes the probability of specific token sequences corresponding to a predefined list of macro-actions (e.g., "Chop Tomato"). It normalizes these probabilities to select the most likely valid action.
- **Core assumption:** The LLM's internal log-probabilities for valid action tokens correlate strongly with optimal gameplay moves.
- **Evidence anchors:**
  - [section] Page 4: "The fast system calculates macro-action probabilities by computing token probabilities... These probabilities are then normalized to form the macro-action probability distribution."
  - [corpus] "Thought Communication in Multiagent Collaboration" suggests moving beyond natural language lossiness, which aligns with using probability distributions.

## Foundational Learning

- **Concept:** **Dual Process Theory (System 1 vs. System 2)**
  - **Why needed here:** You cannot understand the architecture's latency management without distinguishing the "fast, automatic" System 1 (Llama-13B) from the "slow, deliberate" System 2 (GPT-4o).
  - **Quick check question:** Which system handles the "cognitive style" reasoning, and why is it separated from the action execution loop?

- **Concept:** **Theory of Mind (ToM)**
  - **Why needed here:** This is the core capability gap the paper addresses. You must understand that ToM implies modeling *mental states* (beliefs, desires), not just predicting *actions* via pattern matching.
  - **Quick check question:** In this framework, is "predicting the partner will move Left" an example of ToM, or is it the result of a deeper ToM process?

- **Concept:** **Hierarchical Action Spaces (Macro-actions)**
  - **Why needed here:** The agent does not output "UP/DOWN" atomic actions directly from the LLM. It outputs "Cook Soup," which is decomposed. This abstraction is necessary to bridge LLM reasoning speeds with game loop frequencies.
  - **Quick check question:** What module converts the LLM's macro-action output into the atomic actions required by the game?

## Architecture Onboarding

- **Component map:** Information Extractor -> Slow System (ToM Module) -> Fast System -> Action Decoder

- **Critical path:** The prompt engineering linking the **Slow System's** output (Partner Style/Intention) to the **Fast System's** input prompt. If this handoff is corrupted or delayed, the Fast System operates blindly.

- **Design tradeoffs:**
  - **Latency vs. Intelligence:** Using GPT-4o for the Fast System would likely improve decision quality but fail real-time requirements; using Llama-13B requires the Slow System to compensate for its lack of world knowledge.
  - **Fixed vs. Adaptive Styles:** The system relies on a predefined "Partner Style Corpus." It cannot invent new styles not listed in its library, limiting adaptability to truly novel human quirks.

- **Failure signatures:**
  - **Stale Context:** The agent repeats an action (e.g., "Chop Onion") because the Slow System hasn't updated the intention context to tell it the partner already finished chopping.
  - **Hallucinated Knowledge:** The agent assumes the partner knows a recipe they clearly don't, resulting in uncoordinated plating.
  - **Probability Collapse:** The Fast System assigns near-zero probability to all valid actions due to a confusing prompt, freezing the agent.

- **First 3 experiments:**
  1. **Ablation of the Slow System:** Run the agent with the ToM module disabled (`w/o MsToM`) to establish a baseline for "mindless" collaboration. Expect to see the score drop to ~44 (per Table 1).
  2. **Latency Profiling:** Measure the inference time of the Slow System (GPT-4o) vs. the Fast System (Llama-13B) to verify that the Fast System can actually cycle multiple times within one Slow System reasoning cycle.
  3. **Style Prediction Accuracy:** Manually label human partners with specific styles (e.g., "Ingredient-preparation-oriented") and check if the `ToM_style` module correctly classifies them from trajectory data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust is the cognitive style reasoning module when partners exhibit novel behaviors not present in the pre-defined "partner style corpus"?
- Basis in paper: [inferred] The "Cognitive style reasoning stage" relies on a corpus where styles are designed based on behaviors "observed in human behavioral experiments."
- Why unresolved: The paper evaluates performance with "diverse" partners but does not explicitly test against "out-of-distribution" styles not included in the hand-crafted corpus.
- What evidence would resolve it: Experiments where partners deliberately adopt hybrid or unclassified behavioral strategies to test if the agent can infer styles outside its library.

### Open Question 2
- Question: Can the DPMT framework maintain real-time performance in scenarios requiring high-frequency strategic adjustments where the slow system cannot run asynchronously?
- Basis in paper: [inferred] The "Fast System" relies on multi-threading where the slow system operates at a "longer time scale" to accommodate LLM latency.
- Why unresolved: The current setup assumes the fast system can tolerate cached ToM reasoning; this may fail if the strategic landscape changes faster than the slow system's inference speed.
- What evidence would resolve it: Evaluation in environments with dynamically shifting goals to measure performance degradation relative to the slow system's update frequency.

### Open Question 3
- Question: Is the three-tiered ToM structure (knowledge → style → intention) the optimal hierarchy for modeling partners with irrational or incomplete domain knowledge?
- Basis in paper: [explicit] The conclusion highlights the method's potential for "tackling more intricate human-AI collaborative tasks" but acknowledges the current fixed hierarchical structure.
- Why unresolved: The paper does not explore if a different order of reasoning or a non-hierarchical approach might be more efficient for partners who possess domain knowledge but lack consistent cognitive styles.
- What evidence would resolve it: Ablation studies comparing the current hierarchical inference against parallel or reversed reasoning orders for partners with varying rationality levels.

## Limitations

- The framework relies on a predefined Partner Style Corpus, limiting its ability to handle truly novel human behaviors outside the taxonomy
- The dual-process architecture trades off real-time responsiveness for deeper reasoning, potentially causing failures if the environment changes faster than the Slow System's update cycle
- Evaluation is limited to the Overcooked game environment, raising questions about generalization to other collaborative domains or more complex real-world scenarios

## Confidence

- **High confidence:** The dual-process architecture effectively manages the latency-accuracy tradeoff in real-time human-AI collaboration. The ablation study results showing significant performance drops without the multi-scale ToM module are well-supported by the experimental data.
- **Medium confidence:** The three-stage hierarchical ToM reasoning (Knowledge → Style → Intention) improves generalization to unseen human behaviors. While ablation results support this claim, the evaluation is limited to Overcooked-specific scenarios and predefined style categories.
- **Low confidence:** The claim that grounding macro-action selection in token probabilities rather than free-form generation reduces execution errors lacks comparative validation. The paper does not benchmark against alternative LLM prompting strategies or demonstrate that probability-based selection is superior to other approaches.

## Next Checks

1. **Corpus Coverage Validation:** Test the DPMT framework with human partners exhibiting behaviors outside the predefined style taxonomy. Specifically, recruit participants to demonstrate novel collaboration strategies not represented in the corpus, then measure whether the ToM module correctly identifies these as "unknown" or misclassifies them, leading to coordination failures.

2. **Latency-Accuracy Tradeoff Analysis:** Instrument the dual-process system to measure inference times at each stage and correlate these with collaboration performance across different map complexities. This will validate whether the claimed benefit of separating deliberation from execution actually holds under varying environmental dynamics and computational constraints.

3. **Cross-Domain Transferability:** Deploy the DPMT framework in a different collaborative environment (e.g., a different cooperative game or simulated physical task) to test whether the hierarchical ToM reasoning and dual-process architecture generalize beyond Overcooked. This will reveal whether the approach is task-specific or represents a more broadly applicable framework for human-AI collaboration.