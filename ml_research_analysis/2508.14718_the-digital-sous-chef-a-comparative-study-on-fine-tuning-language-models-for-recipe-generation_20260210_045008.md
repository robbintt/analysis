---
ver: rpa2
title: The Digital Sous Chef -- A Comparative Study on Fine-Tuning Language Models
  for Recipe Generation
arxiv_id: '2508.14718'
source_url: https://arxiv.org/abs/2508.14718
tags:
- generation
- recipe
- gpt-2
- recipes
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We established a rigorous benchmark for text-based recipe generation,
  a fundamental task in natural language generation. We present a comprehensive comparative
  study contrasting a fine-tuned GPT-2 large (774M) model against the GPT-2 small
  (124M) model and traditional LSTM/RNN baselines on the 5-cuisine corpus from RecipeDB.
---

# The Digital Sous Chef -- A Comparative Study on Fine-Tuning Language Models for Recipe Generation

## Quick Facts
- arXiv ID: 2508.14718
- Source URL: https://arxiv.org/abs/2508.14718
- Reference count: 10
- Fine-tuned GPT-2 large achieves >20% relative improvement in BERTScore and 69.8% perplexity reduction over recurrent baselines for recipe generation

## Executive Summary
This paper establishes a rigorous benchmark for text-based recipe generation by comparing fine-tuned GPT-2 large (774M) against GPT-2 small and traditional LSTM/RNN baselines on a 5-cuisine corpus from RecipeDB. The study introduces a targeted tokenization strategy that augments the vocabulary with 23 common fraction tokens and custom structural markers to address limitations of generic tokenizers. Performance is evaluated using a comprehensive suite of seven automatic metrics spanning fluency, coherence, semantic relevance, and diversity, demonstrating the superiority of transformer-based approaches for this domain-specific task.

## Method Summary
The study fine-tunes GPT-2 large on the 5cuisine Dataset from RecipeDB (~51,000 recipes), split 80/10/10 for train/val/test. A custom tokenizer extends the base vocabulary with 23 common fraction tokens and 11 structural markers to preserve recipe structure and numerical precision. Training uses AdamW optimizer with learning rate 3e-5, batch size 8, and 20 epochs with mixed precision. Evaluation employs BLEU-4, ROUGE-L, METEOR, BERTScore (F1), perplexity, and diversity metrics. Baselines include word-level 2-layer LSTM/RNN models with teacher forcing. Generation uses nucleus sampling (p=0.95), top-k (k=50), and temperature (T=0.7).

## Key Results
- GPT-2 large achieves 0.92 BERTScore (F1) compared to 0.72 for the best recurrent baseline (>20% relative improvement)
- Perplexity reduced by 69.8% compared to LSTM/RNN models
- Transformer-based approach shows consistent superiority across all seven evaluation metrics
- Custom tokenization strategy with fraction tokens and structural markers improves domain-specific performance

## Why This Works (Mechanism)
The success stems from the transformer's ability to capture long-range dependencies in recipe structures, combined with domain-specific tokenization that preserves critical numerical quantities and formatting. The large parameter count (774M) enables learning complex patterns in culinary instructions while the custom tokens prevent fragmentation of essential recipe elements like fractions and structural markers.

## Foundational Learning
- **RecipeDB dataset structure**: Understanding the 5cuisine corpus format and preprocessing requirements is essential for replicating the study
- **GPT-2 tokenizer customization**: Adding fraction tokens and structural markers requires understanding tokenizer internals and embedding resizing
- **Causal language modeling**: The training objective uses cross-entropy loss with padding tokens masked as -100
- **Evaluation metric suite**: Familiarity with BLEU, ROUGE-L, METEOR, BERTScore, perplexity, and diversity calculations is needed
- **Generation strategies**: Nucleus sampling, top-k filtering, and temperature scaling parameters affect output quality
- **Mixed precision training**: FP16 optimization on NVIDIA V100 32GB requires understanding of hardware constraints

## Architecture Onboarding

**Component Map**: Data Preprocessing -> Custom Tokenizer -> GPT-2 Large Model -> Training Loop -> Evaluation Metrics -> Generation

**Critical Path**: Custom Tokenizer -> GPT-2 Large Model -> Training Loop -> Evaluation Metrics

**Design Tradeoffs**: Larger model (774M) provides better performance but requires more compute; custom tokenization improves domain specificity but increases vocabulary size; multiple evaluation metrics provide comprehensive assessment but require careful interpretation

**Failure Signatures**: 
- Fraction tokens tokenized incorrectly (e.g., "1/2" â†’ ["1", "/", "2"])
- Repetitive or incoherent output during generation
- Poor performance on structural elements despite high semantic scores

**First Experiments**:
1. Verify custom tokenizer encodes fractions as single tokens before training
2. Check that special structural tokens appear in generated output
3. Compare perplexity on validation set during training to detect overfitting

## Open Questions the Paper Calls Out
1. **RAG for factual accuracy**: Can Retrieval-Augmented Generation effectively mitigate factual inaccuracies in generated recipes without reducing output diversity? The paper identifies factual inaccuracy as the primary challenge and proposes RAG as a solution, but it's unclear if external knowledge retrieval conflicts with creative generation.

2. **Metric correlation with human judgment**: To what degree do high automatic metric scores correlate with human assessments of culinary feasibility and safety? The paper notes that dangerous instructions can still achieve high semantic similarity scores, indicating a disconnect between metrics and real-world utility.

3. **Constrained decoding effectiveness**: Can constrained decoding techniques enforce logical ingredient usage without causing repetitive loops or degrading fluency? The paper lists this as a future direction to address unrealistic quantities, but it's unknown if constraints will exacerbate repetition issues.

## Limitations
- Fraction tokenization details are incomplete (exact list of 23 tokens not provided)
- Preprocessing thresholds for filtering recipes are unspecified
- No human evaluation to validate automatic metric performance
- Study limited to 5-cuisine corpus, limiting generalizability claims
- Factual accuracy issues persist despite high metric scores

## Confidence
**High confidence**: GPT-2 large architecture, training methodology, and general transformer superiority trends are well-supported
**Medium confidence**: Specific performance improvements depend on implementation details that are partially unspecified
**Low confidence**: Claims about factual accuracy limitations and broader implications are speculative

## Next Checks
1. Implement and test custom tokenizer with 23 common fraction tokens, verifying fractions are encoded as single tokens
2. Experiment with different preprocessing thresholds for recipe length filtering to assess impact on performance
3. Implement LSTM/RNN baselines with multiple hyperparameter configurations to establish robust comparative results