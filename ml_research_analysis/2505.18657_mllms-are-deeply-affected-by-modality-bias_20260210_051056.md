---
ver: rpa2
title: MLLMs are Deeply Affected by Modality Bias
arxiv_id: '2505.18657'
source_url: https://arxiv.org/abs/2505.18657
tags:
- modality
- bias
- mllms
- multimodal
- modalities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This position paper identifies modality bias as a critical issue
  in Multimodal Large Language Models (MLLMs), where models disproportionately rely
  on language data while underutilizing visual inputs. The authors define modality
  bias mathematically as the imbalance between contributions of different modalities,
  where certain modalities dominate the learning process.
---

# MLLMs are Deeply Affected by Modality Bias

## Quick Facts
- **arXiv ID**: 2505.18657
- **Source URL**: https://arxiv.org/abs/2505.18657
- **Reference count**: 40
- **Key outcome**: MLLMs show 56.53% consistency between complete and text-only inputs vs 27.17% for complete and image-only, confirming over-reliance on language

## Executive Summary
This position paper identifies modality bias as a critical issue in Multimodal Large Language Models (MLLMs), where models disproportionately rely on language data while underutilizing visual inputs. The authors define modality bias mathematically as the imbalance between contributions of different modalities, where certain modalities dominate the learning process. Through experiments using MMMU-Pro dataset and Qwen2.5VL models, they demonstrate that MLLMs show significantly higher consistency between complete and text-only inputs compared to complete and image-only inputs, confirming the over-reliance on language. The paper identifies three key factors contributing to this bias: imbalanced dataset distributions, asymmetric backbone capabilities, and training objectives that prioritize text-image alignment.

## Method Summary
The paper evaluates modality bias through prediction consistency experiments using the MMMU-Pro dataset with Qwen2.5-VL models. They compare model outputs across three input conditions: complete image-text pairs, text-only with white/black/noise images, and image-only with minimal prompts. Consistency rates are computed between these conditions using both Direct and Chain-of-Thought inference under "Standard 4" and "Standard 10" protocols. The methodology involves systematic ablation of each modality to measure prediction changes, revealing the extent to which models rely on language versus visual information.

## Key Results
- MLLMs show 56.53% consistency between complete and text-only inputs versus 27.17% for complete and image-only inputs
- Three primary factors cause modality bias: dataset imbalance (language data is more compact), backbone capability gap (LLMs outpace visual encoders), and training objectives that prioritize text-image alignment
- Current solutions include enhancing visual contribution in datasets, redirecting model focus toward visual information, and applying preference optimization strategies

## Why This Works (Mechanism)

### Mechanism 1: Information Density Asymmetry
- Claim: Language data's compact, abstract nature dominates learning compared to visual data's inherent redundancy, causing models to prioritize text as the lower-cost signal path.
- Mechanism: Textual data is semantically dense and structurally explicit; visual data is high-dimensional with spatial redundancy. During gradient-based optimization, the modality with higher signal-to-noise ratio and easier label alignment receives disproportionately larger effective learning signal.
- Core assumption: Information density and label alignment difficulty directly influence which modality captures gradient dominance during joint training.
- Evidence anchors:
  - [abstract]: "Data Characteristics: Language data is compact and abstract, while visual data is redundant and complex, creating an inherent imbalance in learning dynamics."
  - [Section 3.2]: "Textual data is often more semantically dense or informative than visual data in certain tasks, due to its structured and explicit nature. As a result, models tend to prioritize textual inputs during learning, treating accompanying modalities such as images merely as auxiliary cues."
  - [corpus]: "Rethinking the Text-Vision Reasoning Imbalance in MLLMs through the Lens of Training Recipes" confirms current MLLMs over-rely on textual cues while under-attending to visual cues.
- Break condition: If visual data were pre-processed to achieve comparable semantic density (e.g., via dense captioning or scene graph extraction), the density-driven bias would diminish.

### Mechanism 2: Backbone Capability Gap
- Claim: Pre-trained language backbones substantially outpace visual encoders in representational maturity, causing automatic bias toward text during multimodal fusion.
- Mechanism: Language models benefit from mature transformer architectures with extensive pretraining corpora and sustained research investment. Visual encoders require specialized, less-optimized backbones with smaller-scale pretraining. The stronger initialized text representations dominate the shared representation space after fusion.
- Core assumption: Relative backbone strength at initialization persists through multimodal training, with stronger modality capturing more representational capacity.
- Evidence anchors:
  - [abstract]: "Imbalanced Backbone Capabilities: The dominance of pretrained language models in MLLMs leads to overreliance on language and neglect of visual information."
  - [Section 3.2]: "Language models often benefit from mature and highly optimized transformer-based architectures... The rapid advancement of language models, fueled by large-scale datasets and sustained community focus, has further widened the performance gap across modalities."
  - [corpus]: Limited direct corpus evidence for architectural asymmetry specifically; related papers focus on reasoning and consistency rather than backbone comparison.
- Break condition: If visual backbone pretraining scale matched language backbone scale (e.g., comparable parameter count, data volume, compute budget), the capability gap would narrow.

### Mechanism 3: Training Objective Shortcut Capture
- Claim: Training objectives that prioritize text-image alignment implicitly designate text as the semantic anchor, encouraging language-biased shortcut learning.
- Mechanism: CLIP-style contrastive learning, image-text matching, masked language modeling, and caption generation objectives optimize for alignment quality. Since text provides explicit semantic structure while visual semantics are implicit, gradients favor text-derived shortcuts. Modalities harder to align (audio, video) receive weaker optimization signals.
- Core assumption: Training loss landscapes contain modality-specific shortcut paths; objectives that don't explicitly enforce balanced cross-modal contributions allow dominant modality shortcuts to emerge.
- Evidence anchors:
  - [abstract]: "Training Objectives: Current objectives often fail to promote balanced cross-modal alignment, resulting in shortcut learning biased toward language."
  - [Section 3.2]: "These objectives implicitly encourage the model to rely heavily on language as the semantic anchor... most objectives do not explicitly encourage consistent cross-modal alignment or robust fusion across diverse modalities."
  - [corpus]: "When Modalities Conflict: How Unimodal Reasoning Uncertainty Governs Preference Dynamics in MLLMs" examines how models resolve conflicts but approaches from uncertainty rather than objective design.
- Break condition: If training objectives included explicit modality balance constraints (e.g., minimum visual contribution thresholds, cross-modal consistency penalties), shortcut capture would be inhibited.

## Foundational Learning

- **Concept: Modality Contribution Function C(Mᵢ)**
  - Why needed here: The paper formalizes modality bias as an imbalance in contributions, where C(M_dominant) >> C(M_underutilized). Understanding this framing is essential for diagnosis.
  - Quick check question: For your multimodal model, if you ablate each modality independently, what fraction of correct predictions does each modality preserve?

- **Concept: Convergence Rate Differential**
  - Why needed here: Different modalities converge at different rates during training; faster-converging modalities can "lock in" representations before slower modalities contribute meaningfully.
  - Quick check question: Plot validation accuracy over training for text-only vs image-only branches—at what epoch does each reach 90% of final performance?

- **Concept: Cross-Modal Consistency Testing**
  - Why needed here: The paper's case study methodology directly compares (image+text) vs (text-only) vs (image-only) predictions to quantify bias; this is the core diagnostic technique.
  - Quick check question: On held-out samples, what percentage of predictions are consistent between (image+text) and (text-only) vs (image+text) and (image-only)?

## Architecture Onboarding

- **Component map**:
  - Visual encoder (e.g., ViT, CLIP vision tower) -> projection layer (MLP/Q-Former) -> fusion mechanism (concatenation with text tokens) -> language backbone (pre-trained LLM) -> output head

- **Critical path**:
  - Text tokens → LLM embedding layer → transformer layers → output (dominant pathway)
  - Image → visual encoder → projection → prepended to text sequence → LLM (auxiliary pathway, often underweighted)

- **Design tradeoffs**:
  - Stronger LLM backbone → better reasoning but amplified text bias; the paper found 56.53% consistency between complete and text-only inputs
  - Larger visual encoder → improved visual features but doesn't address fusion imbalance
  - Visually-dependent datasets (MMStar, MMMU-Pro) → force visual attention during evaluation but don't change training dynamics
  - Preference optimization (BPO, NaPO) → can reduce bias but requires careful negative sample construction

- **Failure signatures**:
  - Model produces confident, fluent answers with blank/noisy images (text priors dominate)
  - Predictions remain unchanged when image contradicts text prompt
  - Large accuracy gap between (image+text) and (image-only) conditions on tasks where image should suffice
  - Paper reports: 27.17% consistency between complete and image-only inputs vs 56.53% for text-only

- **First 3 experiments**:
  1. **Consistency probe**: Run inference on MMMU-Pro (or equivalent) with three conditions—(image+text), (text-only with white image), (image-only with minimal prompt). Compute consistency matrices as in Table 2.
  2. **Modality ablation sensitivity**: Systematically mask or noise each modality and measure performance degradation. A modality with <15% impact on final accuracy indicates underutilization.
  3. **Convergence timing analysis**: Train separate unimodal models (text-only, image-only) on the same task; compare epochs to convergence. If text converges 2-3x faster, implement modality-balanced sampling or gradient modulation during multimodal training.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can an objective, systematic metric be developed to quantify modality bias in MLLMs, similar to how IoU advanced semantic segmentation?
- **Basis in paper:** [explicit] The authors state in Section 5.2 that "the exploration of an objective and systematic metric to measure modality bias is crucial" and that this area "still remains almost blank."
- **Why unresolved:** Current evaluation relies on observing output inconsistencies or task-specific performance drops rather than a standardized, mathematical measurement tool.
- **What evidence would resolve it:** A quantitative metric that reliably measures the contribution of each modality ($C(M_i)$) across diverse models and tasks.

### Open Question 2
- **Question:** Can debiasing strategies designed for vision-language models be effectively generalized to complex, real-world modality combinations like audio or tactile inputs?
- **Basis in paper:** [explicit] The paper notes in Section 5.2 that research focus is currently limited to LVLMs and explicitly calls for "more generalized debiasing strategies" for modalities like tactile and audio.
- **Why unresolved:** Existing methods focus on text-image shortcuts and may not handle the distinct noise profiles and integration challenges of other modalities.
- **What evidence would resolve it:** A single debiasing method that improves performance on benchmarks containing three or more modalities (e.g., audio-visual-tactile).

### Open Question 3
- **Question:** What are the internal theoretical mechanisms causing modality bias, and how can Explainable AI (XAI) visualize these interaction failures?
- **Basis in paper:** [explicit] The authors argue in Section 5.2 that "the internal mechanism of modality bias still lacks exploration" and explicitly call for applying XAI to visualize interactions.
- **Why unresolved:** Current understanding is based on "phenomenon level" output analysis rather than internal architectural diagnosis of how modalities are suppressed.
- **What evidence would resolve it:** Visualizations identifying specific layers or attention heads responsible for suppressing non-textual modalities during inference.

## Limitations

- The analysis relies heavily on prediction consistency metrics that may not fully capture underlying representation dynamics
- White/black/noise image substitution assumes complete modality ablation, but MLLMs may still extract spurious visual features from placeholder patterns
- The three-factor causal explanation (dataset characteristics, backbone capabilities, training objectives) is identified qualitatively rather than through systematic ablation studies
- Practical quantification methods lack robustness against prompt sensitivity and model-specific architectural quirks

## Confidence

- **High confidence**: The empirical observation that MLLMs show substantially higher consistency between complete and text-only inputs versus complete and image-only inputs. This finding is directly measurable and reproducible across multiple models and datasets.
- **Medium confidence**: The three-factor causal explanation (dataset imbalance, backbone capability gap, training objectives). While each factor is supported by existing literature and logical reasoning, the relative contribution of each factor to overall bias magnitude remains unquantified.
- **Low confidence**: The proposed systematic research roadmap and current solutions. These recommendations are largely aspirational and lack empirical validation of their effectiveness in reducing modality bias.

## Next Checks

1. **Attention-based modality contribution analysis**: Extract attention weights from the visual encoder and language backbone during inference on modality-ablated inputs. Quantify the actual information flow from each modality rather than relying solely on prediction consistency.

2. **Cross-dataset bias generalization**: Evaluate the same MLLM across datasets with varying modality distributions (text-heavy, image-heavy, balanced). Determine whether the observed bias patterns persist across domains or are specific to MMMU-Pro's particular construction.

3. **Architectural intervention testing**: Implement targeted architectural modifications (e.g., modality-specific gating mechanisms, symmetric backbone scaling, explicit balance objectives) and measure their impact on both consistency metrics and task performance. This would validate whether the proposed factors are truly causal rather than correlational.