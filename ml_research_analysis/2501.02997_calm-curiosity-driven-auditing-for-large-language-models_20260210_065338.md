---
ver: rpa2
title: 'CALM: Curiosity-Driven Auditing for Large Language Models'
arxiv_id: '2501.02997'
source_url: https://arxiv.org/abs/2501.02997
tags:
- auditing
- audit
- target
- intrinsic
- calm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of auditing black-box large language
  models (LLMs) without access to their internal parameters, focusing on uncovering
  harmful or biased input-output pairs. To tackle this, the authors propose CALM (Curiosity-Driven
  Auditing for Large Language Models), which uses intrinsically motivated reinforcement
  learning to fine-tune an LLM as an auditor agent.
---

# CALM: Curiosity-Driven Auditing for Large Language Models

## Quick Facts
- **arXiv ID**: 2501.02997
- **Source URL**: https://arxiv.org/abs/2501.02997
- **Reference count**: 8
- **Primary result**: CALM achieves over 80% auditing objective accuracy on Llama-3-8B with approximately 15,000 queries, outperforming baseline methods through curiosity-driven exploration.

## Executive Summary
This paper addresses the challenge of auditing black-box large language models without access to internal parameters, focusing on uncovering harmful or biased input-output pairs. The authors propose CALM (Curiosity-Driven Auditing for Large Language Models), which uses intrinsically motivated reinforcement learning to fine-tune an LLM as an auditor agent. The key innovation is a token-level intrinsic bonus based on policy cover theory, encouraging exploration of novel regions in the token embedding space. Experiments on inverse suffix generation and toxic completion tasks demonstrate CALM's effectiveness in identifying problematic behaviors, such as generating derogatory content or eliciting specific names, with consistent improvements over baseline methods.

## Method Summary
CALM uses intrinsically motivated reinforcement learning to fine-tune an LLM as an auditor agent for black-box auditing. The method employs a regularized objective combining extrinsic auditing rewards with token-level intrinsic bonuses based on policy cover theory. At each generation step, CALM computes a novelty score using Random Network Distillation (RND) to measure prediction error against fixed random networks. This drives exploration toward diverse prompts in token embedding space. The auditor (GPT-2) generates candidate prompts, submits them to the black-box target LLM, and receives outputs evaluated by a toxicity classifier or suffix-matching criterion. Policy updates use PPO with the combined reward signal.

## Key Results
- CALM achieved over 80% auditing objective accuracy on Llama-3-8B with approximately 15,000 queries
- Token-level intrinsic bonus outperformed sentence-level diversity (CRT baseline) in auditing tasks
- GPT-2 as auditor successfully discovered vulnerabilities in larger target LLMs including Llama-3-8B and Llama-2-7B

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Token-level intrinsic bonuses based on policy cover theory enable systematic exploration of the prompt space
- Mechanism: At each generation step, CALM computes a novelty score for each token using Random Network Distillation (RND). The intrinsic bonus R̂I(s) = ||ψ₁(h) - g₁(h)|| / ||ψ₂(h) - g₂(h)|| measures prediction error against fixed random networks, where higher error indicates less-visited regions of token embedding space. This drives the auditor toward diverse prompts rather than repetitive, high-probability sequences.
- Core assumption: Prediction error correlates with state novelty in token embedding space, and novel prompts are more likely to uncover rare harmful behaviors.
- Evidence anchors:
  - [abstract] "token-level intrinsic bonus based on policy cover theory, encouraging exploration of novel regions in the token embedding space"
  - [section 4.2] Derivation showing RI(s) = 1/√(P^π_s(h)/ρ_l(h)) approximated via RND
  - [corpus] Limited direct corpus support; "Text-Diffusion Red-Teaming" (arXiv:2501.08246) addresses similar exploration challenges but uses diffusion-based methods, not policy cover theory
- Break condition: If token embedding space is dense and uniformly covered, prediction errors converge and intrinsic bonuses provide diminishing exploration signal.

### Mechanism 2
- Claim: Regularized objective balancing extrinsic auditing rewards with intrinsic exploration prevents premature convergence to low-diversity attack strategies
- Mechanism: The optimization objective max_π [J_A(s) + λ_I J_I(s) - λ_KL J_KL(s)] combines: (1) extrinsic reward r(s,o) for triggering harmful outputs, (2) intrinsic reward for exploration, and (3) KL divergence penalty to prevent deviation from reference policy. The hyperparameters λ_I and λ_KL control trade-offs.
- Core assumption: Harmful input-output pairs are sparse but discoverable through systematic exploration rather than random sampling.
- Evidence anchors:
  - [abstract] "intrinsically motivated reinforcement learning to fine-tune an LLM as an auditor agent"
  - [section 4.1] Equation 1 defining the regularized auditing objective
  - [corpus] "Audit Me If You Can" (arXiv:2601.03087) conceptualizes auditing as uncertainty estimation, supporting the framing of auditing as optimization, though without intrinsic motivation
- Break condition: If λ_I is too high, the auditor explores indefinitely without exploiting; if too low, it converges to local optima in prompt space.

### Mechanism 3
- Claim: Lightweight auditor models (GPT-2) can discover vulnerabilities in larger target LLMs when equipped with curiosity-driven exploration
- Mechanism: The auditor generates candidate prompts s_T sequentially via π(s_t|s_{t-1}), submits them to the black-box target, and receives outputs o. The policy is updated via PPO using combined rewards. Surprisingly, small models succeed because exploration strategy matters more than model capacity for discovering harmful regions.
- Core assumption: The structure of harmful prompt space is learnable and transferable across model scales; vulnerability patterns are not unique to specific architectures.
- Evidence anchors:
  - [section 1] "even finetuning a relatively small transformer-based model like GPT-2 can discover the undesired behaviors of larger LLMs like Llama-3-8B"
  - [section 5.1] "GPT-2 is lightweight and has the essential text generation ability" as justification for auditor backbone
  - [corpus] "White-Box Sensitivity Auditing" (arXiv:2601.16398) notes black-box methods "limited to tests constructed" from observable behavior, consistent with this mechanism's constraints
- Break condition: If target LLM has fundamentally different tokenization or prompt-response dynamics not captured by auditor's vocabulary/embedding space, transfer fails.

## Foundational Learning

- Concept: Proximal Policy Optimization (PPO)
  - Why needed here: CALM uses PPO as its RL backbone for stable policy updates during fine-tuning; understanding clipping, advantage estimation, and value function updates is essential for debugging training instability.
  - Quick check question: Can you explain why PPO uses a clipped surrogate objective rather than direct policy gradient updates?

- Concept: Intrinsic Motivation / Curiosity-Driven Exploration
  - Why needed here: The core innovation is the intrinsic bonus; understanding prediction-error-based exploration (RND, ICM) clarifies why the method works and when it might fail.
  - Quick check question: What happens to intrinsic motivation signals if the environment is deterministic and fully explored?

- Concept: Policy Cover Theory
  - Why needed here: The theoretical foundation for the intrinsic bonus design; understanding how coverage-based exploration guarantees differ from entropy-based methods informs hyperparameter choices.
  - Quick check question: How does maximizing deviation from a policy cover differ from maximizing action entropy?

## Architecture Onboarding

- Component map:
  Audit LLM (GPT-2) -> Target LLM (Black-box) -> Toxicity Classifier / Suffix Matcher -> Reward Composer -> PPO Optimizer

- Critical path:
  1. Initial prompt z → Audit LLM generates s_T token-by-token
  2. At each token step, compute intrinsic bonus via ψ, g networks
  3. Submit complete prompt s_T to Target LLM → receive o
  4. Compute extrinsic reward r(s,o) via toxicity/suffix matching
  5. Compute advantage A(s_{t-1}, s_t) via GAE
  6. Update π_θ via PPO loss; update V via regression; update ψ₁ via prediction loss

- Design tradeoffs:
  - **GPT-2 vs larger auditor**: Paper acknowledges larger auditor backbone likely improves performance; current choice prioritizes computational efficiency
  - **Word-based vs neural toxicity classifier**: Paper deliberately avoids neural classifiers due to adversarial vulnerability; trades nuance for robustness
  - **Token-level vs sentence-level intrinsic bonus**: Paper shows token-level outperforms sentence-level diversity (CRT baseline) for auditing tasks

- Failure signatures:
  - High entropy, low auditing objective: Exploration not converging to harmful regions; increase λ_I decay or adjust intrinsic bonus scale
  - Low entropy, low auditing objective: Premature exploitation; increase λ_I or check KL penalty strength
  - Intrinsic bonus plateau: RND predictors overfitting; verify ψ₂ reinitialization is occurring

- First 3 experiments:
  1. Reproduce inverse suffix generation on GPT-2 target with λ_I = 10; verify convergence curve matches Figure 1 baseline before attempting larger targets
  2. Ablate intrinsic bonus (set λ_I = 0) on Llama-2-7B target; expect significant performance drop confirming exploration contribution
  3. Test alternative auditor backbone (e.g., GPT-2 Medium vs GPT-2 Small) on same target; assess whether capacity gains outweigh computational costs

## Open Questions the Paper Calls Out
- How does CALM's auditing performance scale with more powerful auditor backbones beyond GPT-2?
- How does CALM compare to non-RL black-box optimization methods such as evolutionary algorithms?
- Does CALM generalize effectively to auditing objectives beyond inverse suffix generation and toxic completion?

## Limitations
- Effectiveness demonstrated only on synthetic tasks; real-world harmful behaviors not tested
- Token embedding space exploration relies on prediction error correlation with semantic novelty, which is not validated
- Computational cost analysis lacks comprehensive evaluation of practical auditing campaign expenses

## Confidence
- **High confidence**: Mathematical formulation of regularized objective and PPO implementation are sound; token-level intrinsic bonus mechanism via RND is theoretically grounded
- **Medium confidence**: Empirical results on synthetic tasks are convincing, but generalization to real-world auditing scenarios remains unproven; lightweight auditor choice is reasonable but not thoroughly justified
- **Low confidence**: Claim that token embedding space exploration effectively discovers semantically meaningful harmful prompts lacks validation beyond controlled synthetic tasks; relationship between intrinsic bonus signals and actual harmful content discovery is not rigorously established

## Next Checks
1. **Semantic validity test**: Apply CALM to discover harmful prompts on a real-world LLM with known vulnerabilities (e.g., jailbreak prompts for GPT-4), then manually verify whether discovered prompts are semantically meaningful rather than token-distribution artifacts.

2. **Exploration efficiency measurement**: Compare CALM's prompt discovery rate against random sampling and entropy-based exploration baselines on the same tasks, measuring both query efficiency and the diversity of discovered harmful behaviors.

3. **Intrinsic bonus ablation with semantic analysis**: Remove the intrinsic bonus entirely and compare discovered prompts' semantic properties using sentence embeddings or human evaluation, confirming whether exploration genuinely improves the quality of discovered vulnerabilities.