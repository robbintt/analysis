---
ver: rpa2
title: 'StableMTL: Repurposing Latent Diffusion Models for Multi-Task Learning from
  Partially Annotated Synthetic Datasets'
arxiv_id: '2506.08013'
source_url: https://arxiv.org/abs/2506.08013
tags:
- tasks
- task
- flow
- multi-task
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: StableMTL addresses the challenge of multi-task learning from partially
  annotated synthetic datasets by repurposing pre-trained latent diffusion models.
  The core method involves fine-tuning a UNet on synthetic datasets using task tokens
  and a unified latent loss, followed by a multi-stream architecture with task-attention
  mechanism to promote cross-task information exchange.
---

# StableMTL: Repurposing Latent Diffusion Models for Multi-Task Learning from Partially Annotated Synthetic Datasets

## Quick Facts
- **arXiv ID:** 2506.08013
- **Source URL:** https://arxiv.org/abs/2506.08013
- **Reference count:** 40
- **Primary result:** Outperforms partially labeled MTL methods by +83.54 Δm across 7 tasks on 8 benchmarks

## Executive Summary
StableMTL addresses multi-task learning from partially annotated synthetic datasets by repurposing pre-trained latent diffusion models. The method fine-tunes a UNet on synthetic data using task tokens and a unified latent loss, then employs a multi-stream architecture with task-attention mechanism to promote cross-task information exchange. By using task-gradient isolation and efficient 1-to-N attention, the approach scales effectively to multiple tasks without requiring task-specific losses. The method demonstrates strong generalization to real-world data in a zero-shot fashion, addressing twice as many tasks as prior baselines.

## Method Summary
StableMTL repurposes pre-trained latent diffusion models for multi-task dense prediction through a two-stage training process. First, a single-stream UNet is fine-tuned on synthetic datasets using task tokens and task-gradient isolation, with a unified MSE loss in latent space replacing task-specific losses. Second, a multi-stream architecture is trained where a frozen auxiliary UNet provides cross-task features to a main UNet via a novel task-attention mechanism. This converts N-to-N task interactions into efficient 1-to-N attention, allowing the main task stream to selectively incorporate features from auxiliary task streams. The unified approach eliminates the need for task-specific loss balancing while promoting effective cross-task sharing.

## Key Results
- Outperforms existing partially labeled MTL methods by +83.54 Δm across 7 tasks
- Achieves strong zero-shot generalization to real-world benchmarks (Cityscapes, KITTI, DIODE, MIDIntrinsics)
- Addresses twice as many tasks as prior baselines while maintaining competitive performance
- Multi-stream architecture with task-attention provides +4.78 Δm improvement over single-stream baseline

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Repurposing a pre-trained Latent Diffusion Model (LDM) for multi-task learning is achieved via task-token conditioning and a unified latent regression loss, which eliminates the need for task-specific loss functions.
- **Mechanism:** A pre-trained LDM's UNet is fine-tuned to predict the latent representation of a task's annotation in a single, deterministic step. This prediction is conditioned on a task-specific token (e.g., a CLIP embedding of "normal"). The model is trained end-to-end using a single unified Mean Squared Error (MSE) loss in the latent space, bypassing complex, task-specific loss balancing.
- **Core assumption:** The pre-trained LDM's latent space contains rich, transferable visual priors that can be adapted for diverse dense prediction tasks through this simple conditioning and regression objective.
- **Evidence anchors:**
  - [abstract] "...repurposes image generators for latent regression. Adapting a denoising framework with task encoding... Instead of per-task losses requiring careful balancing, a unified latent loss is adopted..."
  - [section 3.1] "We extend such models for multi-task learning by adopting task tokens... For each task available, we minimize the Mean Squared Error (MSE) loss in the latent space..."
  - [corpus] Corpus evidence for this specific "unified latent regression with task tokens" mechanism is weak or missing in the provided neighbors.
- **Break condition:** This approach would likely fail if the tasks are fundamentally incompatible with the visual features encoded in the LDM's latent space, or if the single MSE loss cannot adequately capture the objectives of highly divergent tasks.

### Mechanism 2
- **Claim:** A task-gradient isolation training scheme prevents tasks with larger gradient magnitudes from overwhelming those with smaller gradients, leading to improved overall multi-task performance.
- **Mechanism:** During training, each mini-batch is composed of data for a single task. Gradient accumulation is also isolated by task: gradients are accumulated only from batches of the same task, followed by an optimizer step and gradient reset before moving to the next task. This decouples the gradient updates for different tasks.
- **Core assumption:** A primary source of suboptimal performance in MTL is conflicting or dominating gradients during joint updates. Isolating updates allows each task to be learned more effectively while still benefiting from a shared set of weights.
- **Evidence anchors:**
  - [abstract] "...by employing task-gradient isolation..."
  - [section 3.1] "...we adopt a task-isolation training scheme, where at each training step, each mini-batch contains only a single task... we accumulate gradients only from mini-batches of the same task..."
  - [section 4.2, Figure 5] "...when removing gradient isolation, tasks with smaller gradient magnitudes are overwhelmed by those with larger ones, leading to a significant performance drop."
  - [corpus] Corpus evidence for this specific "task-gradient isolation" scheme is weak or missing in the provided neighbors.
- **Break condition:** The mechanism could hinder performance if tasks are highly interdependent and require simultaneous co-adaptation within a single update step to learn effectively. It also increases training time.

### Mechanism 3
- **Claim:** A multi-stream architecture with a scalable 1-to-N task-attention mechanism facilitates cross-task information sharing, improving performance over a simple multi-task conditioned single-stream model.
- **Mechanism:** The architecture combines a frozen, pre-trained "auxiliary" UNet with a trainable "main" UNet. The main UNet's transformer blocks feature a novel "task-attention" layer that attends to the feature maps from the auxiliary UNet, which processes all other tasks. This allows the main task stream to selectively incorporate features from auxiliary task streams, converting a computationally expensive N-to-N interaction into an efficient 1-to-N attention.
- **Core assumption:** Features from a model trained on individual tasks (the frozen auxiliary) provide a stable and beneficial source of cross-task information that can be integrated via attention to improve the main multi-task model.
- **Evidence anchors:**
  - [abstract] "...introduce a multi-stream model with a task-attention mechanism that converts N-to-N task interactions into efficient 1-to-N attention, promoting effective cross-task sharing."
  - [section 3.2] "...our multi-stream architecture enables a more efficient N-to-1 task inter-task attention mechanism... This new layer enriches the main stream features by attending to relevant task-specific features from the auxiliary streams..."
  - [section 4.2, Table 4] "...using only single-stream... degrading performance on all tasks (drop of 4.98 in Δm)..."
  - [corpus] Corpus evidence for this specific "multi-stream 1-to-N attention" architecture is weak or missing in the provided neighbors.
- **Break condition:** This mechanism may fail if the features from the auxiliary streams are not useful for the main task or if the attention mechanism cannot learn to filter out detrimental information. The authors also note potential "attention saturation" which they mitigate with a masking strategy.

## Foundational Learning

**Concept:** Latent Diffusion Models (LDMs)
- **Why needed here:** The entire StableMTL method is built on top of a pre-trained LDM. You must understand what a latent space is, what a UNet is, and how diffusion models are trained to grasp why this repurposing is possible and effective.
- **Quick check question:** Can you explain the role of the VAE encoder/decoder and the UNet in a standard Latent Diffusion Model?

**Concept:** Multi-Task Learning (MTL)
- **Why needed here:** StableMTL is an MTL method. Foundational knowledge of why MTL is challenging (e.g., negative transfer, gradient conflicts) and common architectures (e.g., encoder sharing) is essential to understand the problem being solved.
- **Quick check question:** What are two common problems that arise when training a single neural network on multiple tasks simultaneously?

**Concept:** Attention Mechanisms
- **Why needed here:** The core architectural contribution is a novel "task-attention" layer. You need to understand the basics of queries, keys, and values to understand how the main stream attends to auxiliary task features.
- **Quick check question:** In a standard cross-attention block, what do the Query and Key inputs typically represent?

## Architecture Onboarding

**Component map:** Stage 1: UNet (Uθ) fine-tuned on synthetic datasets with task tokens and task-gradient isolation → becomes auxiliary UNet. Stage 2: Main UNet (Uϕ) with task-attention layers attends to frozen auxiliary UNet features.

**Critical path:** During a forward pass for task T, the most important computation is the Task Attention, where Uϕ,T's features are enriched by attending to the pre-computed features from the auxiliary Uθ for all τ ∈ T*. This is where cross-task synergy is enforced.

**Design tradeoffs:**
- **Single-Stream vs. Multi-Stream:** The single-stream model is simpler and faster but underperforms (Tab. 4: -1.57 Δm). The multi-stream model adds complexity and inference cost (requires a forward pass through the auxiliary UNet) but gains +4.78 Δm.
- **N-to-N vs. 1-to-N Attention:** Standard N-to-N attention scales poorly with the number of tasks. The proposed 1-to-N attention is more scalable but assumes the auxiliary features are a sufficient source of cross-task information.

**Failure signatures:**
- **Negative Transfer / Task Interference:** Performance on some individual tasks may degrade compared to single-task baselines. This is visible in the paper's results (Fig. 6) and mitigated, but not solved, by the multi-stream architecture.
- **Attention Saturation:** The Task Attention mechanism might "spike" on one or two dominant auxiliary tasks, ignoring others. This is mitigated by the attention-guided task masking strategy, but could still be a failure mode if the masking probability ρ is set incorrectly.

**First 3 experiments:**
1. **Reproduce Single-Stream Baseline:** Implement the Stage 1 training pipeline. Train the model on the synthetic datasets with task-gradient isolation. Evaluate on all 7 tasks to establish the baseline performance, which should be close to the reported StableMTL-S results.
2. **Ablate Task Attention:** Implement the Stage 2 multi-stream architecture. Run a comparative experiment between (a) using standard cross-attention for task fusion vs. (b) using the proposed Task Attention (Eq. 2). Measure the difference in Δm to validate the architectural contribution.
3. **Test Gradient Isolation:** Train the Stage 1 model *with* and *without* the task-gradient isolation scheme. Compare the final performance, especially on tasks known to have weaker gradients (e.g., semantic, albedo), to confirm the mechanism's benefit reported in Figure 5.

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: Can improved multi-token conditioning enable simultaneous multi-output prediction to remove the sequential inference bottleneck?
- Basis in paper: [explicit] The authors state: "In terms of inference, multi-task prediction is currently sequential. We believe this is a constraint that future work could address with models enabling simultaneous multi-output prediction via improved multi-token conditioning."
- Why unresolved: The current architecture requires separate forward passes for each task, which is computationally inefficient compared to single-pass MTL models.
- What evidence would resolve it: A variation of StableMTL that predicts all 7 tasks in a single forward pass while maintaining comparable Δm performance.

**Open Question 2**
- Question: How would adaptive task sampling strategies improve upon the uniform sampling strategy used during training?
- Basis in paper: [explicit] The paper lists as a limitation that "the uniform task sampling strategy could be refined through adaptive approaches."
- Why unresolved: Uniform sampling may over-represent easier tasks or under-represent tasks with smaller datasets (e.g., Hypersim vs. FlyingThings3D), potentially slowing convergence or hurting final accuracy.
- What evidence would resolve it: Ablation studies comparing uniform sampling against curriculum learning or gradient-based sampling strategies, showing improved convergence rates or higher final benchmark scores.

**Open Question 3**
- Question: How can cross-task interactions be optimized to prevent negative transfer when adding new tasks?
- Basis in paper: [explicit] The authors note "cross-task interaction is far from optimal yet" and observe that "adding some new tasks (e.g., shading) affects the individual performances on other tasks."
- Why unresolved: Despite the multi-stream architecture, specific task combinations still exhibit interference, degrading performance relative to single-task baselines.
- What evidence would resolve it: Architectural modifications to the task-attention mechanism that result in strictly positive or neutral transfer for all task pairs (e.g., depth performance remaining stable when shading is added).

## Limitations
- The multi-stream architecture introduces inference-time overhead through the auxiliary forward pass
- The method's reliance on synthetic data with specific annotation patterns may limit generalizability to real-world partially labeled scenarios
- Cross-task interactions are far from optimal, with some new tasks (e.g., shading) affecting individual performances on other tasks

## Confidence
**High Confidence:** The core claim that repurposing pre-trained latent diffusion models for dense prediction tasks is viable through task-token conditioning and unified latent regression. This is well-supported by the training methodology and baseline comparisons, and aligns with established transfer learning principles.

**Medium Confidence:** The effectiveness of task-gradient isolation for preventing gradient domination. While the mechanism is clearly described and Figure 5 shows positive results, the ablation study could benefit from comparison against alternative gradient balancing techniques like gradient normalization or uncertainty weighting.

**Medium Confidence:** The benefit of the multi-stream 1-to-N task-attention architecture over single-stream alternatives. Table 4 demonstrates clear improvements, but the comparison is only against the single-stream version of their own method rather than established cross-task fusion mechanisms.

**Low Confidence:** The absolute zero-shot generalization capability to real-world data. While Δm improvements are reported, the paper doesn't provide extensive qualitative analysis or failure case studies that would help understand the method's limitations in practical scenarios.

## Next Checks
1. **Ablation Against Alternative Gradient Balancing:** Implement and compare task-gradient isolation against two alternative approaches: (a) gradient normalization by task gradient norms, and (b) learnable uncertainty weighting. This would validate whether isolation is the optimal solution for gradient conflicts.

2. **Cross-Dataset Generalization Study:** Test the pre-trained StableMTL model on a new partially annotated dataset that differs significantly from the training distribution (e.g., outdoor scenes if trained on mostly indoor data). Measure task-wise performance degradation to understand the limits of zero-shot transfer.

3. **Attention Mechanism Dissection:** Visualize and analyze the attention distributions across all transformer blocks and tasks. Quantify how often attention saturation occurs despite masking, and measure the correlation between attention focus patterns and task performance improvements to validate the cross-task information sharing hypothesis.