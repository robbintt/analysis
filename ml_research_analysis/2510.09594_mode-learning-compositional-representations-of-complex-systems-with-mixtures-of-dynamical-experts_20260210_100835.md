---
ver: rpa2
title: 'MODE: Learning compositional representations of complex systems with Mixtures
  Of Dynamical Experts'
arxiv_id: '2510.09594'
source_url: https://arxiv.org/abs/2510.09594
tags:
- mode
- dynamical
- dynamics
- data
- cycle
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MODE tackles the problem of modeling complex, overlapping dynamical
  regimes in biological systems where traditional single-flow approaches fail due
  to ambiguous velocity signals in noisy transition zones. The core idea is a neurally-gated
  mixture of dynamical experts that jointly learns multiple governing equations and
  their spatial transitions, enabling both unsupervised discovery of behavioral regimes
  and accurate long-term forecasting across regime switches.
---

# MODE: Learning compositional representations of complex systems with Mixtures Of Dynamical Experts

## Quick Facts
- arXiv ID: 2510.09594
- Source URL: https://arxiv.org/abs/2510.09594
- Authors: Nathan Quiblier; Roy Friedman; Matthew Ricci
- Reference count: 40
- Primary result: MODE achieves NMI scores above 0.96 on synthetic benchmarks and outperforms NODE, MLP, and SINDy baselines on biological switch modeling

## Executive Summary
MODE addresses the fundamental challenge of modeling complex biological systems with overlapping dynamical regimes by introducing a neurally-gated mixture of dynamical experts. Traditional single-flow approaches fail when velocity signals become ambiguous in noisy transition zones, leading to poor regime detection and inaccurate long-term forecasting. The framework jointly learns multiple governing equations and their spatial transitions, enabling both unsupervised discovery of behavioral regimes and accurate prediction across regime switches.

Applied to both synthetic benchmarks and single-cell RNA sequencing data, MODE demonstrates superior performance in identifying regime boundaries and predicting system behavior. The framework achieves high accuracy in distinguishing cycling from differentiation dynamics with 0.98 AUC and predicts differentiation timing with approximately 25% cycle lead time. Beyond prediction accuracy, MODE provides interpretable expert dynamics that offer mechanistic insight into the underlying biological processes.

## Method Summary
MODE employs a neurally-gated mixture of dynamical experts to model complex systems with overlapping regimes. The framework learns multiple governing equations simultaneously with spatial transition functions, allowing it to handle ambiguous velocity signals in noisy transition zones where traditional approaches fail. Each expert represents a distinct dynamical regime, while a neural gating mechanism determines the contribution of each expert at any given point in state space. The model is trained end-to-end using gradient descent on trajectory data, with the mixture coefficients optimized to accurately capture regime transitions and maintain smooth transitions between experts.

## Key Results
- Achieves NMI scores above 0.96 on synthetic benchmarks with spatially overlapping attractors and chaotic dynamics
- Outperforms NODE, MLP, and SINDy baselines on synthetic biological switches with lower Wasserstein errors
- Correctly identifies regime boundaries and commits cells to branches in cell cycle exit and lineage branching scenarios
- Distinguishes cycling from differentiation dynamics with 0.98 AUC and predicts differentiation timing with ~25% cycle lead time

## Why This Works (Mechanism)
MODE works by decomposing complex dynamical systems into multiple expert models, each responsible for a specific regime, with neural gating mechanisms that smoothly transition between experts. This composition allows the model to handle ambiguous velocity signals in transition zones where single-flow approaches fail. The joint learning of expert dynamics and transition functions enables accurate long-term forecasting across regime switches while maintaining interpretability through the learned expert equations.

## Foundational Learning
- Mixture of Experts: Multiple specialized models combined with gating weights to handle different regimes; needed because single models cannot capture overlapping dynamical behaviors
- Neural Ordinary Differential Equations: Continuous-time dynamics learned through neural networks; needed for modeling smooth transitions between states
- Gating Mechanisms: Neural networks that compute mixture coefficients based on state space; needed to determine which experts are active at each point
- Wasserstein Distance: Metric for comparing probability distributions; needed for evaluating forecast accuracy in trajectory space
- Normalizing Flows: Techniques for learning invertible transformations; needed for handling complex probability distributions in state space

## Architecture Onboarding
- Component map: State Space -> Gating Network -> Mixture Coefficients -> Expert Networks -> Combined Dynamics -> Trajectory Prediction
- Critical path: Input state → gating network → mixture weights → weighted sum of expert dynamics → integrated trajectory
- Design tradeoffs: Number of experts vs. computational complexity, gating smoothness vs. sharp transitions, model interpretability vs. prediction accuracy
- Failure signatures: Poor regime detection when transition zones are too narrow, inaccurate long-term forecasting with too few experts, overfitting with excessive model complexity
- First experiments: 1) Test on synthetic dataset with two clearly separated regimes, 2) Evaluate performance on dataset with overlapping attractors, 3) Assess robustness by adding Gaussian noise to benchmark data

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- Performance degradation with severe noise contamination and narrow transition zones between many overlapping regimes
- Computational complexity scaling challenges with multiple expert networks on high-dimensional biological datasets
- Limited validation on diverse biological systems beyond synthetic benchmarks and single-cell RNA-seq data

## Confidence
- Biological interpretability claims: Medium confidence (heavily reliant on synthetic datasets with known ground truth)
- Forecasting accuracy (25% cycle lead time): High confidence (well-documented quantitative metrics)
- Performance advantages over baselines: High confidence (demonstrated across multiple synthetic and biological benchmarks)

## Next Checks
1. Test MODE on biological datasets with experimentally validated transition points to assess real-world accuracy of regime boundary detection
2. Evaluate computational scaling by training MODE on high-dimensional single-cell datasets (10,000+ cells, 20,000+ genes) and measuring training time and memory requirements
3. Assess robustness to varying noise levels by systematically adding Gaussian noise to benchmark datasets and measuring performance degradation across different signal-to-noise ratios