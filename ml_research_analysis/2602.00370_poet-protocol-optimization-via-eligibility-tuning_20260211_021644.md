---
ver: rpa2
title: 'POET: Protocol Optimization via Eligibility Tuning'
arxiv_id: '2602.00370'
source_url: https://arxiv.org/abs/2602.00370
tags:
- criteria
- generation
- similarity
- trial
- guided
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: POET addresses the challenge of automating clinical trial eligibility
  criteria generation, which is often time-consuming and cognitively demanding for
  clinicians. The method introduces guided generation using interpretable semantic
  axes (e.g., Demographics, Laboratory Parameters) to steer LLM-based criteria generation,
  offering a practical middle ground between overly specific and generic approaches.
---

# POET: Protocol Optimization via Eligibility Tuning

## Quick Facts
- **arXiv ID**: 2602.00370
- **Source URL**: https://arxiv.org/abs/2602.00370
- **Reference count**: 17
- **Primary result**: Guided LLM generation using semantic axes improves clinical trial eligibility criteria quality by 18–300% over unguided approaches

## Executive Summary
POET introduces a method for automating clinical trial eligibility criteria generation using large language models (LLMs). The approach employs interpretable semantic axes (e.g., Demographics, Laboratory Parameters) to guide generation, providing a practical middle ground between overly specific entity-based approaches and generic generation. A reusable rubric-based evaluation framework assesses generated criteria along clinically meaningful dimensions. Results show consistent improvements across rare, medium, and common criteria, with clinician evaluations confirming strong agreement with LLM assessments (77–88% agreement rate).

## Method Summary
POET generates clinical trial eligibility criteria using zero-shot LLM prompting with semantic axis constraints. The method extracts trial metadata and existing criteria from the AACT database, then masks one criterion per trial for generation. Guided generation uses axis-conditioned prompts while unguided generation omits this constraint. Rarity classification employs a consensus approach combining LLM-based semantic labeling with data-driven BioBERT embedding similarity. Evaluation uses a decomposed rubric scoring criteria similarity, axis similarity, and rarity similarity, with LLM-as-Judge comparing against clinician assessments.

## Key Results
- Guided generation consistently outperforms unguided generation across all rarity levels
- 18–300% improvement in total score metrics when using semantic axis guidance
- 77–88% agreement rate between LLM judgments and clinician assessments
- Best-of-N sampling shows quality improves with more candidate generations

## Why This Works (Mechanism)

### Mechanism 1: Semantic Axis Guidance Narrows Output Space
By constraining generation to specific semantic categories (e.g., "Laboratory Parameters"), the model's output space is narrowed toward clinically meaningful regions, reducing irrelevant or generic outputs while preserving flexibility. This operates as a soft constraint—unlike predefined entities—allowing the model to hallucinate plausible criteria within a bounded semantic zone.

### Mechanism 2: Consensus-based Rarity Labeling Reduces Benchmark Noise
Combining LLM-based and data-driven similarity approaches for rarity classification produces more robust evaluation subsets than either method alone. LLM provides semantic judgment of criterion rarity; data-driven approach uses BioBERT embeddings + cosine similarity to compute empirical frequency within disease-specific corpora.

### Mechanism 3: Structured Rubrics Enable LLM-as-Judge Alignment
The rubric breaks evaluation into three concrete dimensions—Criteria Similarity (0–3), Axis Similarity (binary), Rarity Similarity (binary)—constraining LLM assessment to clinically grounded criteria rather than generic text similarity. This reduces evaluator drift and improves inter-rater reliability.

## Foundational Learning

- **Zero-shot LLM prompting**: Why needed here: POET operates without task-specific fine-tuning; understanding prompt engineering and temperature settings is essential for reproducibility.
- **Embedding-based semantic similarity**: Why needed here: The data-driven rarity classification uses BioBERT embeddings and cosine similarity; understanding vector representations is prerequisite to the masking strategy.
- **Rubric-based vs. metric-based evaluation**: Why needed here: POET's contribution includes a reusable rubric; distinguishing this from BERTScore/n-gram metrics clarifies what "better evaluation" means.

## Architecture Onboarding

- **Component map**: Trial metadata → (optional: rarity classification for benchmarking) → prompt construction with axis constraint → LLM generation → rubric scoring → optional clinician validation
- **Critical path**: Trial metadata → (optional: rarity classification for benchmarking) → prompt construction with axis constraint → LLM generation → rubric scoring → optional clinician validation
- **Design tradeoffs**: Zero-shot vs. fine-tuning (flexibility vs. performance), guided vs. unguided (input burden vs. metric improvements), Best-of-N sampling (quality vs. latency/cost)
- **Failure signatures**: Low Axis Similarity scores (prompt not constraining output), high variance in rarity labels (sparse disease corpus), clinically implausible criteria (model lacks domain knowledge)
- **First 3 experiments**:
  1. Replicate guided vs. unguided gap on rare criteria: Sample 20 rare criteria; compare first-output quality using the rubric
  2. Ablate semantic axes: Remove axis constraint entirely and measure degradation; test with only high-level axes vs. full 12-axis taxonomy
  3. Validate LLM-clinician agreement locally: Have 1–2 domain experts score 30 generated criteria; compute agreement rate against LLM-as-Judge

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does guided generation generalize to rare diseases and trials outside the 10 common disease categories studied?
- Basis in paper: The evaluation used only 10 standardized MeSH disease categories from 344 U.S.-based interventional drug trials.

### Open Question 2
- Question: Can POET be extended to exclusion criteria generation with comparable performance?
- Basis in paper: The paper exclusively extracted and evaluated inclusion criteria; exclusion criteria were not addressed despite being equally critical to trial design.

### Open Question 3
- Question: How does the choice and granularity of semantic axes affect generation quality, and can axes be dynamically adapted per therapeutic area?
- Basis in paper: The 13 axes were LLM-derived and expert-validated, but their optimality was not systematically tested.

### Open Question 4
- Question: How does POET compare comprehensively to retrieval-augmented and fine-tuned baselines on larger, non-overlapping test sets?
- Basis in paper: The EC-RAFT comparison used only 75 criteria (5 rare) to avoid train/test overlap, limiting statistical conclusions about relative performance.

## Limitations
- Rarity labeling depends heavily on disease-specific corpus quality; sparse data may invalidate data-driven thresholds
- LLM-as-Judge evaluation was conducted by a small pool of clinicians (n=5) with potentially variable domain expertise
- The semantic axes, while interpretable, were defined by the authors without external validation of their clinical completeness

## Confidence

- **High**: Guided generation improves relevance over unguided generation (strong quantitative and qualitative support)
- **Medium**: Semantic axis guidance provides practical middle ground between entity-level and generic generation (mechanism plausible but axis definition not externally validated)
- **Medium**: LLM-as-Judge rubric reliably approximates clinician assessment (agreement rates good but small sample size)

## Next Checks

1. Test robustness across disease domains: Apply POET to 3–5 diseases outside the original 10 to verify axis effectiveness and rarity classification generalizes
2. Validate axis taxonomy completeness: Have independent clinicians map 50 criteria to the 13 axes to identify coverage gaps or misclassifications
3. Compare against entity-based generation: Implement a baseline that uses specific criterion entities rather than axes to quantify the practical advantage of the proposed approach