---
ver: rpa2
title: 'Mxplainer: Explain and Learn Insights by Imitating Mahjong Agents'
arxiv_id: '2506.14246'
source_url: https://arxiv.org/abs/2506.14246
tags:
- agents
- tile
- tiles
- mahjong
- game
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Mxplainer is a framework for explaining black-box Mahjong agents
  by imitating their decision-making process through a parameterized search algorithm.
  It constructs a classical agent framework that can be converted into an equivalent
  neural network, allowing gradient-based optimization to learn the parameters of
  any target agent.
---

# Mxplainer: Explain and Learn Insights by Imitating Mahjong Agents

## Quick Facts
- **arXiv ID**: 2506.14246
- **Source URL**: https://arxiv.org/abs/2506.14246
- **Reference count**: 36
- **Primary result**: Mxplainer achieves over 90% top-three action prediction accuracy on both AI and human Mahjong agents while providing interpretable explanations of agent decision-making.

## Executive Summary
Mxplainer is a framework for explaining black-box Mahjong agents by imitating their decision-making process through a parameterized search algorithm. It constructs a classical agent framework that can be converted into an equivalent neural network, allowing gradient-based optimization to learn the parameters of any target agent. The learned parameters are then mapped back into the interpretable classical agent, enabling both high-accuracy policy approximation and transparent, step-by-step explanations of actions. Experiments demonstrate that Mxplainer achieves over 90% top-three action prediction accuracy on both AI and human agents, significantly outperforming decision-tree methods, while providing faithful, interpretable approximations that reveal agent characteristics such as fan and tile preferences.

## Method Summary
Mxplainer implements imitation learning by converting a hand-crafted Mahjong search algorithm into a differentiable neural network structure. The system takes game state features (hand tiles, unshown tile counts, and goal-related tensors) as input and outputs action predictions. A small neural network with 126 parameters is trained using supervised learning with cross-entropy loss and L2 regularization to prevent negative fan preferences. The framework decomposes agent decision-making into goal-value pairs, where the Search Component enumerates valid game goals and the Calculation Component estimates their values based on learned parameters. After training, the parameters are mapped back to the interpretable classical agent framework for explanation purposes.

## Key Results
- Achieves over 90% top-three action prediction accuracy on both AI and human Mahjong agents
- Significantly outperforms decision-tree methods in policy approximation
- Provides faithful, interpretable approximations that reveal agent characteristics including fan preferences and tile selection strategies

## Why This Works (Mechanism)

### Mechanism 1: Classical-to-Neural Equivalence Mapping
The system enables gradient-based optimization of an interpretable search algorithm by mapping its discrete logic to an equivalent differentiable neural network structure. The hand-crafted search framework is transformed into a neural network using Boolean-to-Arithmetic Masking and Identity Padding, allowing parameters to serve as neurons. This bidirectional conversion between symbolic search and tensor operations is domain-specific to Mahjong's decision structure.

### Mechanism 2: Goal-Value Decomposition
The framework explains decisions by decomposing the agent's policy into weighted evaluation of discrete winning patterns ("fans") and estimated tile acquisition probabilities. The Search Component enumerates up to 64 reachable game goals, while the Calculation Component estimates each goal's value by multiplying tile acquisition probability with learned fan preference weights. This multiplicative combination of "closeness" and "preference" captures the agent's strategic priorities.

### Mechanism 3: Supervised Imitation with Semantic Regularization
High-fidelity policy approximation is achieved by training the network to mimic action labels while constraining parameters to remain semantically meaningful. Cross-Entropy loss is combined with L2-regularization that penalizes negative fan preferences, forcing the model to learn a ranking of fans rather than arbitrary weights that might fit data noise but break interpretability.

## Foundational Learning

- **Concept: Differentiable Programming / Masking**
  - **Why needed here**: You must understand how to convert `if/else` statements and `for` loops into tensor operations to build the Network $N$.
  - **Quick check question**: In the PyTorch implementation, how do you ensure that "padding" values in a batch do not affect the product operation used to calculate goal probability?

- **Concept: Imitation Learning (Behavior Cloning)**
  - **Why needed here**: This is the training paradigm. You are not training an agent to *win* (RL), but to *copy* (Supervised).