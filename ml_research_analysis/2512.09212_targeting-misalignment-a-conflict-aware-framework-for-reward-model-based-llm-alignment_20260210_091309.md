---
ver: rpa2
title: 'Targeting Misalignment: A Conflict-Aware Framework for Reward-Model-based
  LLM Alignment'
arxiv_id: '2512.09212'
source_url: https://arxiv.org/abs/2512.09212
tags:
- reward
- base
- proxy
- alignment
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses misalignment in reward-model-based LLM fine-tuning,\
  \ where proxy rewards can diverge from true human preferences due to noise or bias.\
  \ The authors propose treating fine-tuning as knowledge integration and focus on\
  \ detecting \"proxy-policy conflicts\"\u2014cases where the base model and proxy\
  \ reward strongly disagree."
---

# Targeting Misalignment: A Conflict-Aware Framework for Reward-Model-based LLM Alignment

## Quick Facts
- **arXiv ID**: 2512.09212
- **Source URL**: https://arxiv.org/abs/2512.09212
- **Reference count**: 12
- **Key outcome**: Conflict-aware sampling improves alignment performance and reduces human feedback needs compared to RSO and random sampling baselines.

## Executive Summary
This paper addresses a critical challenge in reward-model-based LLM fine-tuning: the misalignment between proxy rewards and true human preferences due to noise or bias. The authors propose treating fine-tuning as knowledge integration and focus on detecting "proxy-policy conflicts"—instances where the base model and proxy reward strongly disagree. By introducing two conflict metrics (PACS and Kendall-Tau Distance) and an algorithm (SHF-CAS) that targets high-conflict examples for additional human feedback, the framework achieves improved alignment performance while reducing the overall need for human feedback compared to standard approaches.

## Method Summary
The framework introduces a novel approach to reward-model-based alignment by focusing on proxy-policy conflicts rather than simply maximizing proxy rewards. The authors define the Proxy-Policy Alignment Conflict Score (PACS) to measure local disagreements between the base model and proxy reward, and use Kendall-Tau Distance for global conflict assessment. Their SHF-CAS algorithm iteratively samples high-conflict examples for human feedback, refining both the reward model and policy. The approach treats fine-tuning as knowledge integration, strategically selecting examples where the proxy reward is most likely to be wrong, thereby improving alignment efficiency.

## Key Results
- Conflict-aware sampling (SHF-CAS) outperforms both RSO and random sampling baselines on safety and helpfulness alignment tasks
- The framework reduces the need for extensive human feedback while maintaining or improving alignment performance
- PACS and Kendall-Tau Distance effectively identify problematic examples where proxy rewards diverge from true human preferences

## Why This Works (Mechanism)
The framework works by recognizing that proxy rewards, while useful, are imperfect representations of true human preferences. By explicitly detecting and targeting examples where the proxy reward and base model disagree (proxy-policy conflicts), the method focuses human feedback resources where they are most needed—on cases where the current reward model is likely wrong. This conflict-aware approach is more efficient than random sampling or reward optimization alone because it prioritizes examples that will most improve the alignment between the final policy and true human preferences.

## Foundational Learning

**Proxy-Policy Conflict Score (PACS)**: A metric measuring disagreement between base model and proxy reward on specific examples. Why needed: Identifies where the reward model is most likely incorrect. Quick check: High PACS scores should correlate with examples where human annotators disagree with the proxy reward.

**Kendall-Tau Distance**: A statistical measure of rank correlation used to assess global alignment between different reward signals. Why needed: Provides a holistic view of reward model quality across the dataset. Quick check: Kendall-Tau values should decrease as the reward model improves.

**Reward Model Fine-tuning as Knowledge Integration**: The conceptual framework treating alignment as integrating true human preferences into the existing model. Why needed: Shifts focus from simple reward maximization to resolving systematic disagreements. Quick check: Should result in more robust alignment than pure reward optimization.

**Conflict-Aware Sampling**: The strategy of prioritizing examples with high proxy-policy conflict for human feedback. Why needed: Makes efficient use of limited human feedback resources. Quick check: Should converge faster than random sampling with fewer total human annotations.

## Architecture Onboarding

**Component map**: Base model -> Proxy reward model -> Conflict metrics (PACS, Kendall-Tau) -> SHF-CAS sampling algorithm -> Human feedback loop -> Refined reward model -> Aligned policy

**Critical path**: Base model outputs → Proxy reward evaluation → Conflict score computation → High-conflict example selection → Human feedback acquisition → Reward model refinement → Policy fine-tuning

**Design tradeoffs**: The framework trades computational complexity (multiple sampling and retraining rounds) for improved alignment quality and reduced human feedback requirements. This represents a shift from resource-intensive continuous human feedback to more strategic, conflict-focused annotation.

**Failure signatures**: If proxy-policy conflicts don't correlate with actual human preference disagreements, the method will waste resources on irrelevant examples. Similarly, if the conflict metrics are poorly calibrated, the sampling may either miss critical misalignments or overemphasize minor disagreements.

**3 first experiments**:
1. Validate that PACS scores correlate with actual human preference disagreements on a held-out test set
2. Compare convergence rates of SHF-CAS versus random sampling with equal total human feedback
3. Test whether high-conflict examples identified by the framework show greater improvement after human feedback compared to low-conflict examples

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- The framework's assumption that proxy-policy conflicts reliably indicate true misalignment may not hold across all domains, as disagreements might reflect complementary strengths rather than actual conflicts
- Experimental validation is limited to safety and helpfulness alignment tasks, leaving generalizability to other objectives uncertain
- SHF-CAS requires multiple sampling and retraining rounds, raising questions about computational overhead and scalability for very large models

## Confidence

**High confidence**:
- Mathematical formulation of PACS and Kendall-Tau Distance is internally consistent and well-justified
- Experimental results showing performance improvements over baselines are methodologically sound

**Medium confidence**:
- Claim that conflict-aware sampling significantly reduces human feedback needs requires testing across more diverse tasks and model scales
- Assumption that proxy-policy conflicts reliably indicate true misalignment needs further validation

## Next Checks

1. Test framework effectiveness on alignment objectives beyond safety and helpfulness (e.g., factual consistency, task-specific accuracy) to assess generalizability
2. Evaluate computational and resource overhead of SHF-CAS across different model sizes and training regimes to determine practical scalability
3. Conduct ablation studies to verify that high PACS scores consistently correspond to genuine human preference divergence rather than complementary model strengths