---
ver: rpa2
title: The AI Language Proficiency Monitor -- Tracking the Progress of LLMs on Multilingual
  Benchmarks
arxiv_id: '2507.08538'
source_url: https://arxiv.org/abs/2507.08538
tags:
- language
- languages
- multilingual
- evaluation
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The AI Language Proficiency Monitor is a comprehensive benchmark
  for evaluating large language models across up to 200 languages, with a focus on
  low-resource languages. It aggregates diverse tasks including translation, question
  answering, math, and reasoning, using datasets such as FLORES+, MMLU, GSM8K, TruthfulQA,
  and ARC.
---

# The AI Language Proficiency Monitor -- Tracking the Progress of LLMs on Multilingual Benchmarks

## Quick Facts
- arXiv ID: 2507.08538
- Source URL: https://arxiv.org/abs/2507.08538
- Authors: David Pomerenke; Jonas Nothnagel; Simon Ostermann
- Reference count: 22
- Primary result: A comprehensive benchmark evaluating LLMs across up to 200 languages, revealing that English and large European languages achieve highest proficiency scores while larger low-resource languages like Swahili show lower scores despite having many speakers.

## Executive Summary
The AI Language Proficiency Monitor is a comprehensive benchmark system designed to evaluate large language models across up to 200 languages, with particular emphasis on low-resource languages. It aggregates diverse tasks including translation, question answering, math, and reasoning using established datasets like FLORES+, MMLU, GSM8K, TruthfulQA, and ARC. The system provides an open-source, auto-updating leaderboard and dashboard that enables comparative evaluation of multilingual capabilities across different models and languages. By using machine translation to extend coverage to languages lacking human-translated evaluation data, the monitor offers unprecedented breadth in multilingual assessment while highlighting performance gaps between high-resource and low-resource language communities.

## Method Summary
The monitor constructs an aggregate "Language Proficiency Score" by evaluating models across multiple task-specific benchmarks using few-shot language-agnostic prompts. For languages without human-translated evaluation data, Google Neural Machine Translation extends coverage. Models are evaluated through OpenRouter (API models) and HuggingFace endpoints (open-weights), with accuracy used for classification and question answering tasks, SpBLEU for translation tasks, and final number accuracy for math problems. The system employs min-max normalization to rescale task-specific metrics to a [0,1] range, then computes the mean of normalized scores to produce the composite Language Proficiency Score. Daily auto-updates via GitHub Actions maintain relevance as the LLM landscape evolves, with current sampling at 10 instances per model-task-language combination.

## Key Results
- English and large European languages consistently achieve the highest Language Proficiency Scores
- Larger low-resource languages like Swahili score lower despite having many speakers
- Machine translation enables coverage of 200 languages but may introduce systematic bias
- The auto-updating dashboard reveals evolving model performance patterns over time

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aggregating multiple task-specific benchmarks into a single "Language Proficiency Score" enables cross-lingual and cross-model comparison.
- Mechanism: Min-max normalization rescales task-specific metrics to a [0,1] range; the mean of normalized scores produces a composite score that partially compensates for task difficulty variance. Accuracy is used for classification/QA tasks; SpBLEU (with SentencePiece tokenization trained on all FLORES+ languages) is used for translation to reduce tokenization bias against low-resource languages.
- Core assumption: Task scores are approximately commensurable after normalization, and averaging preserves meaningful ranking.
- Evidence anchors:
  - [abstract] "The primary result is an aggregate 'Language Proficiency Score' metric that enables comparative evaluation of multilingual capabilities across languages and models."
  - [section 3.4] "We define an aggregate metric, the Language Proficiency Score, as the mean of the individual task metrics."
  - [corpus] Neighbor papers (e.g., "BenchMAX") similarly aggregate multilingual tasks; no direct replication of this specific scoring method in corpus.
- Break condition: If task distributions are highly skewed or non-overlapping across languages, normalization may distort comparisons. Assumption: SpBLEU eliminates (not just reduces) tokenizer bias—unverified at scale.

### Mechanism 2
- Claim: Auto-updating daily evaluations maintain relevance as the LLM landscape evolves.
- Mechanism: A GitHub Actions pipeline retrieves current popular models via OpenRouter metadata, runs inference through OpenRouter (API-only models) and HuggingFace endpoints (open-weights), and updates the dashboard. OpenRouter's "no-training-on-user-data" setting reduces contamination risk.
- Core assumption: Model popularity rankings from OpenRouter are a reasonable proxy for community relevance; daily sampling (10 instances per model-task-language) is sufficient to detect meaningful changes.
- Evidence anchors:
  - [section 3.3] "Our pipeline updates daily via Github Actions to retrieve the latest models and run evaluations on them automatically."
  - [section 5.1] "Our automated runs currently sample 10 instances per combination of model, task, and language."
  - [corpus] No comparable auto-updating multilingual leaderboard in neighbor papers; most are static benchmarks.
- Break condition: If a model's API behavior changes without versioning, longitudinal comparisons become invalid. Assumption: 10 samples provide stable estimates—likely insufficient for high-variance tasks.

### Mechanism 3
- Claim: Machine translation extends benchmark coverage to low-resource languages that lack human-translated evaluation data.
- Mechanism: Google Neural Machine Translate extends benchmarks (MMLU, ARC, GSM8K, TruthfulQA) to uncovered languages from the CLDR top-100 list. Human-translated versions are preferred when available.
- Core assumption: Translation quality is sufficient to preserve task semantics; translation errors do not systematically advantage/disadvantage specific models.
- Evidence anchors:
  - [section 3.1] "We select Google Neural Machine Translation... which performs better than any general-purpose LLM in our FLORES+ translation evaluation."
  - [section 5.1] "Machine-translating the prompt into different languages would have been an alternative, but potential translation errors would have an overproportionate impact on performance."
  - [corpus] "Translation as a Scalable Proxy for Multilingual Evaluation" (FMR=0.615) explores this approach but flags risks; corpus suggests this remains an open problem.
- Break condition: If translation introduces systematic artifacts (e.g., making reasoning problems easier/harder), cross-lingual comparisons are compromised.

## Foundational Learning

- Concept: Few-shot prompting for evaluation
  - Why needed here: The system uses language-agnostic few-shot prompts (not translated instructions) to demonstrate tasks to models, avoiding translation error propagation in prompts.
  - Quick check question: Can you explain why few-shot demonstration without explicit instructions might reduce prompt translation artifacts but increase task ambiguity?

- Concept: SpBLEU metric and tokenizer bias
  - Why needed here: Translation evaluation uses SpBLEU with a SentencePiece tokenizer trained across all FLORES+ languages to reduce BLEU's bias against languages with coarser tokenization.
  - Quick check question: How does tokenizer vocabulary size per language affect BLEU scores, and why might SpBLEU mitigate this?

- Concept: Min-max normalization across heterogeneous tasks
  - Why needed here: Combining accuracy (classification/QA) and SpBLEU (translation) into one score requires normalization; min-max assumes bounded score distributions.
  - Quick check question: If a task has near-ceiling or near-floor performance across all models, what happens to its contribution to the aggregate score after min-max normalization?

## Architecture Onboarding

- Component map:
  Dataset layer (FLORES+, SIB-200, MMLU variants, ARC variants, GSM8K variants, TruthfulQA variants) -> Translation layer (Human-translated preferred, Google NMT fallback, Unicode CLDR script selection) -> Evaluation layer (Few-shot prompting, OpenRouter/HuggingFace inference, task-specific scoring) -> Aggregation layer (Min-max normalization, Language Proficiency Score computation) -> Presentation layer (HuggingFace Spaces dashboard with leaderboard, scoreboard, infoboard, visuals board)

- Critical path:
  1. Identify language-task combinations missing human translations
  2. Apply Google NMT to extend coverage
  3. Run few-shot inference per model-task-language (currently 10 samples)
  4. Compute task scores, normalize, aggregate
  5. Update dashboard via GitHub Actions daily

- Design tradeoffs:
  - Human vs. machine translation: Human is more reliable but costly; machine enables 200-language coverage but introduces noise
  - Sample size vs. compute: 10 samples/model-task-language limits compute but increases variance; full runs would require "substantial compute" (Section 5.1)
  - Language-agnostic prompts vs. translated instructions: Reduces translation artifacts but may under-specify tasks

- Failure signatures:
  - Scores for a language suddenly spike/drop across all models → likely dataset or API change, not model improvement
  - Translation task scores show unexpected variance → check SpBLEU tokenizer coverage for that language
  - Model ranking reverses between daily runs → inspect sample-level responses; 10 samples may be insufficient

- First 3 experiments:
  1. Reproduce one language's scores end-to-end (e.g., Swahili on MMLU) to validate the pipeline, comparing 10 vs. 100 samples for stability
  2. Add a new open-weights model via HuggingFace endpoint to test the submission workflow and verify scoring consistency
  3. Evaluate a low-resource language with both human-translated and machine-translated versions of the same benchmark (where available) to quantify translation-induced variance

## Open Questions the Paper Calls Out

- Question: Does performance on academic multilingual benchmarks correlate with downstream application performance in low-resource languages?
  - Basis in paper: [explicit] Stakeholders criticized "that still only academic benchmarks were used, the performance on which doesn't necessarily correlate with downstream application performance in a language."
  - Why unresolved: The monitor aggregates existing benchmarks that offer "only a very specific snapshot of capabilities and may not reflect real-world utility, particularly across diverse, low-resource contexts."
  - What evidence would resolve it: Systematic comparison of benchmark scores against performance on real-world tasks (e.g., healthcare Q&A, customer service) across low-resource languages.

- Question: Is sampling 10 instances per model-task-language combination sufficient to produce reliable Language Proficiency Scores?
  - Basis in paper: [explicit] "Our automated runs currently sample 10 instances per combination of model, task, and language."
  - Why unresolved: Score variance at this sample size is unknown, and statistical reliability for low-resource languages with higher model uncertainty has not been established.
  - What evidence would resolve it: Analysis of score stability across varying sample sizes (10, 50, 100, 500) to determine minimum required for consistent rankings.

- Question: How does machine-translating benchmarks for missing languages affect the validity of cross-lingual performance comparisons?
  - Basis in paper: [inferred] The paper uses Google Translate for 61-94 languages across tasks, acknowledging it is "inferior to human translation for the purpose of benchmarking."
  - Why unresolved: Translation errors may introduce systematic bias, potentially conflating model capability with translation quality.
  - What evidence would resolve it: Comparison of model scores on human-translated vs. machine-translated portions of the same benchmarks for languages with both versions available.

## Limitations
- Machine translation quality for low-resource languages may introduce systematic bias that confounds model capability assessment
- 10-sample evaluation per model-task-language combination likely insufficient for reliable score estimation, particularly for high-variance reasoning tasks
- Language-agnostic few-shot prompts may under-specify tasks for structurally different languages, potentially disadvantaging certain language groups

## Confidence
- High Confidence: Benchmark construction methodology (dataset selection, translation approach, scoring metrics) is clearly specified and reproducible. The observation that English and large European languages achieve highest scores is robust given the data sources.
- Medium Confidence: The aggregate "Language Proficiency Score" meaningfully captures cross-lingual capabilities. While the methodology is sound, the assumption that min-max normalized task scores are truly comparable across such diverse tasks (translation vs. reasoning vs. QA) requires further validation.
- Low Confidence: Translation quality is sufficient to preserve task semantics for low-resource languages. This is the most critical assumption for the benchmark's validity but is asserted rather than demonstrated through direct evaluation of NMT output quality for reasoning tasks.

## Next Checks
1. **Translation Quality Validation**: For a subset of low-resource languages with available human-translated benchmarks (where possible), evaluate the same models on both human-translated and NMT-translated versions to quantify the variance introduced by machine translation.

2. **Sample Size Sensitivity Analysis**: Re-run evaluations for 3-5 languages with varying sample sizes (10, 50, 100) to determine the point at which score estimates stabilize. This will identify whether the current 10-sample approach provides reliable rankings.

3. **Cross-Lingual Task Difficulty Calibration**: For tasks evaluated across multiple languages, analyze whether certain languages consistently show higher or lower difficulty independent of model capabilities. This could reveal systematic translation effects or linguistic biases in the few-shot prompts.