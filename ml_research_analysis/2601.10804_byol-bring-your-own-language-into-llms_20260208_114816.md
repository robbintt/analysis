---
ver: rpa2
title: 'BYOL: Bring Your Own Language Into LLMs'
arxiv_id: '2601.10804'
source_url: https://arxiv.org/abs/2601.10804
tags:
- language
- languages
- english
- chichewa
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Bring Your Own Language (BYOL), a framework
  that addresses the severe imbalance in global language resources by tailoring large
  language model development to each language's digital footprint. BYOL classifies
  languages into four tiers (Extreme-Low, Low, Mid, High) based on curated web-scale
  corpora, and applies a data refinement and expansion pipeline for low-resource languages.
---

# BYOL: Bring Your Own Language Into LLMs

## Quick Facts
- arXiv ID: 2601.10804
- Source URL: https://arxiv.org/abs/2601.10804
- Reference count: 40
- Primary result: Achieves ~12% average improvement over multilingual baselines on 12 benchmarks for low-resource languages

## Executive Summary
BYOL (Bring Your Own Language) addresses the severe imbalance in global language resources by providing a framework that tailors large language model development to each language's digital footprint. The approach classifies languages into four tiers based on curated web-scale corpora and applies appropriate adaptation pathways. Applied to Chichewa and Maori, the pipeline achieves significant performance improvements while preserving English and multilingual capabilities through weight-space model merging. For extreme-low-resource languages like Inuktitut, a translation-mediated inclusion pathway enables high-accuracy LLM access.

## Method Summary
BYOL uses a multi-stage pipeline: (1) Classify languages into four tiers based on web-scale corpus counts, (2) For low-resource languages, refine native text and English data using LLMs, (3) Translate refined English to target language and create 1:1:1 CPT data mixture, (4) Perform continual pretraining with full-parameter tuning, (5) Apply SFT on bilingual instruction data, (6) Merge with generalist model via weight-space interpolation to preserve multilingual safety. For extreme-low-resource languages, train custom NMT systems and use LLM-as-judge translation quality evaluation.

## Key Results
- ~12% average improvement over multilingual baselines across 12 benchmarks for Chichewa and Maori
- Full-parameter CPT outperforms LoRA adaptation (51.82 vs 49.60 average score)
- Translation-mediated pathway achieves 4 BLEU improvement over commercial baseline for Inuktitut
- Merged models preserve English and multilingual capabilities while improving target language performance

## Why This Works (Mechanism)

### Mechanism 1
Language resource classification routes languages to appropriate adaptation pathways, enabling resource-efficient LLM development. The framework quantifies digital footprint via word count from curated web-scale corpora (FineWeb2), creating four tiers (Extreme-Low, Low, Mid, High). This classification determines whether to use direct finetuning, continual pretraining, or translation-mediated access, matching intervention intensity to available data.

### Mechanism 2
Bilingual data mixture during continual pretraining improves low-resource language performance while preserving English capability. The CPT stage mixes refined native-language text, synthetic text translated from English, and refined English text at 1:1:1 ratios. This strengthens internal representation of the target language while maintaining cross-lingual alignment and mitigating catastrophic forgetting of English.

### Mechanism 3
Weight-space model merging combines language-specific expertise with generalist multilingual behavior without additional training. M(α, β) = G_PT + α(G_IT - G_PT) + β(E_ℓ - G_PT) interpolates between a generalist instruction-tuned model and a language-specific expert. The first difference vector transfers instruction-following; the second injects language knowledge.

## Foundational Learning

- Concept: Catastrophic forgetting
  - Why needed here: CPT must add new language knowledge without degrading existing English/multilingual performance. Understanding this trade-off is critical for designing data mixtures and learning rates.
  - Quick check question: Can you explain why training only on a new language would hurt existing capabilities?

- Concept: Translate-Test paradigm
  - Why needed here: For extreme-low-resource languages, this is the fallback when direct modeling is infeasible. Understanding its dependencies on MT quality sets realistic expectations.
  - Quick check question: What component quality most limits Translate-Test accuracy?

- Concept: Tokenization efficiency
  - Why needed here: Low-resource languages often have inefficient tokenization, increasing inference cost and latency. Understanding this helps evaluate native support vs. translation trade-offs.
  - Quick check question: Why might a language with fewer tokens in vocabulary have higher inference costs?

## Architecture Onboarding

- Component map: Language Resource Classifier (FineWeb2 word count → tier assignment) -> Tool Evaluator (RTTBench-Mono → RTT scores → best LLM/MT selection) -> Data Pipeline (refinement, translation, mixing for CPT/SFT) -> Training Stages (CPT → SFT → Model Merging) -> Translation Pathway (NMT training + back-translation + post-editing)

- Critical path: For low-resource pathway: (1) Identify best base LLM via RTT evaluation, (2) Build CPT data mixture (refine → translate → mix at 1:1:1), (3) Run CPT with full-parameter tuning, (4) Build SFT mixture, (5) Run SFT, (6) Merge with generalist using λ≈0.6.

- Design tradeoffs:
  - LoRA vs. full-parameter: LoRA is faster/cheaper but underperforms at all tested ranks (49.60 vs. 51.82 for full-param)
  - Data mixture ratio: More target-language data improves target performance but risks English degradation; λ in merging trades target vs. multilingual capability
  - Synthetic data: Translation-based augmentation helps when native data is scarce but propagates MT errors

- Failure signatures:
  - English performance drops significantly after CPT → data mixture may be unbalanced or learning rate too high
  - Merged model shows high toxicity/bias → λ may be too high (over-reliance on unaligned expert)
  - Target language still poor after full pipeline → base LLM may have had insufficient initial representation; consider better base or more CPT data

- First 3 experiments:
  1. Baseline identification: Run RTT evaluation on RTTBench-Mono to select the best starting LLM and MT system for your target language
  2. Data mixture ablation: Test C1-C4 configurations to quantify contribution of refinement, translation, and English data to both target-language and English performance
  3. Merge weight sweep: Vary λ from 0.0 to 1.0 to find the optimal trade-off between target-language accuracy and multilingual/English retention

## Open Questions the Paper Calls Out

### Open Question 1
Can the BYOL framework be extended to simultaneous multilingual specialization by merging multiple low-resource language experts without causing interference or degrading the generalist model's capabilities? The current study only validates single-language injection, leaving scalability for multiple languages untested.

### Open Question 2
How effective are specialized guard models compared to weight-space merging for preventing cross-lingual safety leaks and translation-based jailbreaks in low-resource languages? The paper suggests language-specific guards as complementary to merging but doesn't evaluate against adversarial attacks.

### Open Question 3
Can an end-to-end pipeline combining Omnilingual ASR with BYOL text-adapted LLMs effectively handle real-world speech conditions like noise, code-switching, and multi-speaker audio? The current framework operates strictly on text, with ASR error impact unquantified.

## Limitations

- Tier boundary generalizability across different web corpora remains untested
- Synthetic data quality heavily depends on MT system performance, which varies significantly across language pairs
- Proprietary component dependencies (Azure OpenAI GPT-5) create reproduction challenges
- Generalization beyond demonstrated languages (Chichewa, Māori, Inuktitut) is uncertain

## Confidence

**High Confidence**: ~12% improvement on benchmarks, full-parameter CPT outperforming LoRA, weight-space merging preserving capabilities, translation-mediated access effectiveness

**Medium Confidence**: Tier classification effectiveness, 1:1:1 data mixture optimality, safety preservation through merging

**Low Confidence**: Tier boundary generalizability, cross-linguistic performance generalization, synthetic data quality sufficiency for all languages

## Next Checks

**Check 1**: Validate tier boundary classification using multiple web-scale corpora (CommonCrawl, OSCAR, mC4) to measure consistency and predictive accuracy for intervention pathways.

**Check 2**: Perform CPT data mixture ablation varying translated vs. native text ratios (0:2:0, 0:1:1, 1:1:1, 2:1:1, 2:0:0) to identify optimal mixture ratio and quality thresholds.

**Check 3**: Apply full BYOL pipeline to 5-10 additional low-resource languages spanning different families and writing systems to test tier classification accuracy and performance generalization.