---
ver: rpa2
title: Do We Really Need Permutations? Impact of Width Expansion on Linear Mode Connectivity
arxiv_id: '2510.08023'
source_url: https://arxiv.org/abs/2510.08023
tags:
- layer
- input
- relu
- test
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work demonstrates that simply widening neural networks enables
  merged models to achieve accuracy comparable to the originals, without requiring
  parameter permutations. This finding challenges the widely held belief that permutations
  are essential for linear mode connectivity (LMC).
---

# Do We Really Need Permutations? Impact of Width Expansion on Linear Mode Connectivity

## Quick Facts
- arXiv ID: 2510.08023
- Source URL: https://arxiv.org/abs/2510.08023
- Reference count: 40
- This work demonstrates that simply widening neural networks enables merged models to achieve accuracy comparable to the originals, without requiring parameter permutations.

## Executive Summary
This work challenges the widely held belief that permutations are essential for linear mode connectivity (LMC) by demonstrating that simply widening neural networks enables merged models to achieve accuracy comparable to the originals. The authors introduce layerwise exponentially weighted connectivity (LEWC), showing that in wide models, the intermediate-layer outputs of merged models can be expressed as exponentially weighted averages of the corresponding outputs from the original models. This finding suggests that width expansion is a powerful alternative to permutation-based approaches for model merging and LMC.

## Method Summary
The authors propose a model merging approach that avoids permutations by leveraging width expansion. Two models are trained independently from different random seeds, then merged through simple weight interpolation without permutation alignment. The key insight is that sufficient width expansion induces low-rank weight structure, which enables a phenomenon called Layerwise Exponentially Weighted Connectivity (LEWC). This property ensures that the merged model behaves like a weighted ensemble of the originals. The method involves training with strong weight decay (3×10^-3) to encourage low-rank weights, recalibrating BatchNorm statistics after merging, and applying temperature calibration to minimize the loss barrier.

## Key Results
- Simply widening neural networks enables merged models to achieve accuracy comparable to the originals, without requiring parameter permutations.
- Layerwise exponentially weighted connectivity (LEWC) implies that merged models behave like weighted ensembles of the originals, facilitating LMC.
- Weakening weight decay disrupts both LEWC and LMC, underscoring the role of low-rank weight structure in enabling permutation-free merging.

## Why This Works (Mechanism)

### Mechanism 1: Layerwise Exponentially Weighted Connectivity (LEWC)
- Claim: Merged models behave like weighted ensembles of original models when width is sufficiently large.
- Mechanism: At each layer ℓ, the merged model's output equals λ^ℓ f_ℓ(x;θ_a) + (1-λ)^ℓ f_ℓ(x;θ_b). After normalization, final logits become a weighted average of both models' predictions.
- Core assumption: Both weak additivity of ReLU activations and reciprocal orthogonality hold simultaneously.
- Break condition: LEWC fails when weight matrices become high-rank (e.g., from weak weight decay), causing cosine similarity between merged and ensemble outputs to drop significantly.

### Mechanism 2: Weak Additivity of ReLU Activations
- Claim: ReLU behaves approximately linearly along interpolation paths in wide models.
- Mechanism: Two factors: (1) curse of dimensionality causes high cosine similarity (~0.93) between σ(u+v) and σ(u)+σ(v) for Gaussian vectors; (2) low-rank weights reduce overlap among active neurons across models, making σ(z_a + z_b) ≈ σ(z_a) + σ(z_b).
- Core assumption: Pre-activations have approximately Gaussian structure.
- Break condition: Weak weight decay (10^-4 vs standard 3×10^-3) reduces linearity, dropping cosine similarity below 0.9 in deeper layers.

### Mechanism 3: Reciprocal Orthogonality
- Claim: Activations from one model fall into the null space of the other model's weight matrices.
- Mechanism: When W^(a)_ℓ has low effective rank, its null space is large. If z^(b)_{ℓ-1} lies primarily in this null space, then W^(a)_ℓ z^(b)_{ℓ-1} ≈ 0, and symmetrically for the reverse.
- Core assumption: Low-rank weight structure persists through training.
- Break condition: Figure 7 shows E[‖W_a z_b‖]/E[‖W_a z_a‖] increases (violating orthogonality) for narrow widths and weak weight decay.

## Foundational Learning

- Concept: **Linear Mode Connectivity (LMC)**
  - Why needed here: This paper redefines when LMC emerges—without permutations—if you understand the loss barrier definition.
  - Quick check question: Can you explain why a "loss barrier" of nearly zero implies two models lie in the same loss basin?

- Concept: **Permutation Symmetry in Neural Networks**
  - Why needed here: Prior work assumed permutations were necessary for LMC; understanding why helps contrast this paper's contribution.
  - Quick check question: Why does permuting hidden units preserve input-output behavior but change parameter coordinates?

- Concept: **Weight Decay and Implicit Low-Rank Bias**
  - Why needed here: Low-rank weights are essential for LEWC; weight decay strength directly controls this property.
  - Quick check question: How would you predict the effect of reducing weight decay from 3×10^-3 to 10^-4 on merged model accuracy?

## Architecture Onboarding

- Component map:
  - Base architectures: MLP (3 hidden layers, 512 units base), VGG-11, ResNet-20
  - Width expansion: Linear scaling of channel/hidden dimensions (multipliers: 0.125× to 32×)
  - Merging: θ_c = λθ_a + (1-λ)θ_b without permutations
  - Temperature calibration: Inverse temperature τ applied to softmax (estimated on 20% held-out test data)

- Critical path:
  1. Train two models independently from different random seeds
  2. Verify weight decay is strong enough (3×10^-3 recommended) to induce low-rank structure
  3. Merge at λ = 0.5 (or sweep 0-1 for barrier analysis)
  4. Recalibrate BatchNorm statistics on training data
  5. Apply temperature calibration if evaluating loss barrier

- Design tradeoffs:
  - Width vs. compute: 16-32× width multipliers are often needed for near-zero barriers—this is more expensive than permutation search
  - Weight decay strength: Higher decay improves LMC but may hurt individual model accuracy
  - Dataset complexity: Simple datasets (MNIST, CIFAR-10) show effects clearly; CIFAR-100 requires even wider models

- Failure signatures:
  - High loss barrier at λ=0.5 with acceptable accuracy → missing temperature calibration
  - Low merged accuracy even at high width → check weight decay (may be too weak)
  - Accuracy drops mid-interpolation but recovers at endpoints → insufficient width (try 2× increase)

- First 3 experiments:
  1. Baseline width sweep: Train two MLPs on MNIST with width multipliers [1, 2, 4, 8, 16], merge at λ=0.5, plot accuracy vs. width to verify monotonic improvement.
  2. Weight decay ablation: Fix width at 16×, train with weight decay [10^-4, 10^-3, 3×10^-3], measure LEWC cosine similarity at final layer.
  3. Reciprocal orthogonality check: For a 16× MLP, compute E[‖W^(a)_ℓ z^(b)_{ℓ-1}‖]/E[‖W^(a)_ℓ z^(a)_{ℓ-1}‖] per layer—values should decrease with width and correlate with merged accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do the phenomena of Layerwise Exponentially Weighted Connectivity (LEWC) and permutation-free Linear Mode Connectivity (LMC) persist in large-scale settings (e.g., ImageNet, LLMs) and non-image modalities?
- **Basis in paper:** [explicit] The authors state in the conclusion: "An important direction for future work is to test whether these phenomena persist in large-scale settings and other modalities."
- **Why unresolved:** The experiments were computationally limited to relatively simple datasets (MNIST, CIFAR-10/100) because achieving LMC requires significant width expansion (e.g., 32× multiplier), making large-scale verification resource-intensive.
- **What evidence would resolve it:** Experiments demonstrating that merging independently trained large language models or large vision transformers without permutations maintains test accuracy comparable to the originals.

### Open Question 2
- **Question:** Can the sufficient conditions for LEWC (weak additivity and reciprocal orthogonality) be formally derived as a theoretical guarantee of width expansion, rather than just empirically observed?
- **Basis in paper:** [inferred] While Section 5 introduces sufficient conditions (Definitions 5.1 and 5.2) and Theorem 5.3 proves they imply LEWC, the text relies on empirical verification (Section 5.2 and 5.3) and heuristic Gaussian arguments to explain why widening induces these conditions.
- **Why unresolved:** The paper empirically links widening to low-rank weights which satisfy these conditions, but lacks a rigorous theoretical proof that increasing width necessitates reciprocal orthogonality or weak additivity in deep non-linear networks.
- **What evidence would resolve it:** A formal proof showing that for a specific class of wide neural networks, SGD solutions almost surely satisfy reciprocal orthogonality.

### Open Question 3
- **Question:** Does the LEWC mechanism apply to architectures with non-ReLU activations, such as Transformers utilizing Softmax attention?
- **Basis in paper:** [inferred] The theoretical framework relies heavily on the "weak additivity of ReLU activations" (Definition 5.1). The analysis focuses on MLPs, VGG, and ResNets, all utilizing ReLU or similar activations, leaving the behavior of attention-based mechanisms unstated.
- **Why unresolved:** It is unclear if the linearity assumptions and reciprocal orthogonality hold for the multi-head attention mechanism and Softmax normalization, which have different mathematical properties than ReLU layers.
- **What evidence would resolve it:** Empirical evaluation of merged Transformers without permutations to see if LEWC holds, or a derivation of equivalent sufficient conditions for attention layers.

## Limitations

- The theoretical framework relies heavily on assumptions about Gaussian pre-activations and low-rank weight structure that, while intuitive, lack extensive empirical validation beyond the reported experiments.
- The computational cost of extreme width scaling (16-32×) may limit practical utility as an alternative to permutation-based approaches.
- The temperature calibration method, while shown effective, is presented without rigorous analysis of its optimality or sensitivity to the held-out validation subset size.

## Confidence

- **High confidence**: Core experimental findings showing monotonic accuracy improvement with width (Figures 2, 9) and the monotonic relationship between width and loss barrier reduction (Figure 10).
- **Medium confidence**: The theoretical connection between low-rank weights, LEWC, and LMC (Theorems 5.3, 5.4) - proofs are sound but assumptions may not hold broadly.
- **Medium confidence**: The claim that width expansion is a practical alternative to permutation-based approaches - while mathematically valid, the computational cost of extreme width scaling may limit practical utility.

## Next Checks

1. **Cross-dataset generalization test**: Apply the width-expansion approach to CIFAR-100 and ImageNet-10 to verify if the monotonic accuracy improvement persists on more complex datasets.

2. **Architecture universality check**: Test whether the same width-scaling benefits hold for architectures with skip connections (e.g., DenseNet, EfficientNet) where low-rank assumptions may be violated.

3. **Weight decay sensitivity analysis**: Systematically vary weight decay from 10^-5 to 10^-2 on a fixed-width model to map the precise boundary where LEWC breaks down, beyond the three-point comparison in Figure 15.