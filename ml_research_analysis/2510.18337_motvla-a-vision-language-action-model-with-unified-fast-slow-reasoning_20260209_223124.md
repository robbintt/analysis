---
ver: rpa2
title: 'MoTVLA: A Vision-Language-Action Model with Unified Fast-Slow Reasoning'
arxiv_id: '2510.18337'
source_url: https://arxiv.org/abs/2510.18337
tags:
- reasoning
- motvla
- expert
- action
- fast
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MoTVLA, a vision-language-action model that
  integrates fast and slow reasoning into a single architecture for robotic manipulation.
  It combines a pre-trained generalist transformer with a domain expert that shares
  global attention to generate task-specific motion decompositions efficiently.
---

# MoTVLA: A Vision-Language-Action Model with Unified Fast-Slow Reasoning

## Quick Facts
- arXiv ID: 2510.18337
- Source URL: https://arxiv.org/abs/2510.18337
- Authors: Wenhui Huang; Changhe Chen; Han Qi; Chen Lv; Yilun Du; Heng Yang
- Reference count: 21
- Primary result: Achieves 81% success rate in simulation and superior reasoning accuracy by integrating fast/slow reasoning in unified VLA architecture

## Executive Summary
MoTVLA introduces a vision-language-action model that unifies fast and slow reasoning within a single architecture for robotic manipulation. It combines a pre-trained generalist transformer with a domain expert that shares global attention to generate task-specific motion decompositions efficiently. The model conditions a diffusion-based action policy on these decomposed motions, enabling faster execution while preserving language steerability. MoTVLA achieves superior performance in both reasoning benchmarks (e.g., high BLEU and token accuracy) and robotic manipulation tasks (e.g., up to 81% success rate in simulation and strong zero-shot generalization with distractions). It outperforms strong baselines like π0.5 and GR-MG by leveraging shared global knowledge and explicit motion reasoning.

## Method Summary
MoTVLA employs a Mixture-of-Transformers architecture with two parallel Qwen2.5-7B backbones: a frozen generalist for slow reasoning (dialogue/semantic planning) and a fine-tuned domain expert for fast reasoning (motion decomposition). These share a unified global self-attention mechanism allowing knowledge transfer without catastrophic forgetting. The domain expert generates motion decompositions in a single bidirectional pass (token-wise prediction), which conditions a Diffusion Transformer action expert. The action expert takes visual observations, robot configuration, and the domain expert's hidden states to generate language-steered action trajectories. Training occurs in two stages: first fine-tuning the domain expert on motion VQA data, then training the diffusion policy on trajectory data.

## Key Results
- Achieves 81% success rate in robotic manipulation simulation tasks
- Demonstrates strong zero-shot generalization with distractions
- Outperforms baselines (π0.5, GR-MG) on reasoning metrics (BLEU, token accuracy) and task success rates
- Achieves 9 Hz inference speed for fast reasoning vs <1 Hz for slow reasoning baselines

## Why This Works (Mechanism)

### Mechanism 1: Knowledge Sharing via Global Attention
MoTVLA preserves pre-trained general intelligence while efficiently acquiring domain-specific reasoning through shared global attention between a generalist and a domain expert transformer. Two parallel transformers (generalist and domain expert) share a unified global self-attention mechanism, allowing modality-specific QKVs to attend across both experts. This enables knowledge transfer from the pre-trained generalist (Qwen2.5 7B) to the domain expert without modifying generalist parameters, avoiding catastrophic forgetting.

### Mechanism 2: Fast Reasoning via Token-wise Prediction
Fast reasoning generates motion decompositions in a single forward pass using token-wise prediction, significantly reducing inference latency compared to autoregressive next-token prediction. The domain expert uses bidirectional attention and generates all tokens in parallel (token-wise prediction) rather than sequentially. The hidden states from this single forward pass directly encode the decomposed motion instruction, which conditions the action expert. This avoids multiple sequential forward passes inherent in slow reasoning.

### Mechanism 3: Conditioning Diffusion Policy on Decomposed Motion
Conditioning the diffusion policy on the generated motion decomposition representation improves language steerability and task success rates by explicitly grounding action generation in an interpretable, language-aligned plan. The action expert (a Diffusion Transformer/DiT) is conditioned on four inputs: visual observations, robot configuration, and crucially, the motion decomposition hidden state from the domain expert. This conditions the denoising process on a semantic plan (e.g., "Pick up the red cube"), aligning actions with the language instruction.

## Foundational Learning

- **Mixture-of-Experts / Mixture-of-Transformers**
  - Why needed here: MoTVLA's core architecture is a Mixture-of-Transformers. Understanding how multiple transformer experts can be combined, specifically how parameters are shared or separated, is critical.
  - Quick check question: How does MoT differ from simply training two separate models?

- **Diffusion Models for Policy Learning (Diffusion Policy)**
  - Why needed here: The action expert is a DiT trained via diffusion. Understanding the noise-denoise process and conditioning is essential.
  - Quick check question: What is the loss function used to train the denoising network? (See Eq. 5)

- **Autoregressive vs. Non-Autoregressive (Token-wise) Generation**
  - Why needed here: The core distinction between slow (autoregressive) and fast (token-wise) reasoning in MoTVLA.
  - Quick check question: Why is token-wise prediction faster than next-token prediction?

## Architecture Onboarding

- **Component map:** Input (Image + Text + Learnable Queries) → Tokenization → [Generalist & Domain Expert] → Shared Global Attention → Output Logits (Slow Reasoning) / Hidden States (Fast Reasoning) → DiT (Action Expert) → Action Trajectory

- **Critical path:** Input (Image + Text + Learnable Queries) → Tokenization → [Generalist & Domain Expert] → Shared Global Attention → Output Logits (Slow Reasoning) / Hidden States (Fast Reasoning) → DiT (Action Expert) → Action Trajectory

- **Design tradeoffs:**
  - **Performance vs. Efficiency:** Two 7B models increase parameter count and VRAM usage (MoTVLA-14B) compared to a single model. The paper notes a 1B variant for speed but requires pre-training.
  - **Reasoning Accuracy vs. Speed:** Token-wise fast reasoning is faster but may be less accurate for complex tasks than slow reasoning. The paper assumes motion decomposition is simple enough.

- **Failure signatures:**
  - **Reasoning Hallucination:** Domain expert generates incorrect or impossible motion steps. The diffusion policy will execute the wrong action.
  - **Catastrophic Forgetting (Ablation):** If the generalist is fine-tuned, it loses general intelligence. The MoT architecture prevents this.
  - **Precision Failure:** The Peg-in-Hole task has a relatively low success rate (40%), suggesting the method may struggle with high-precision contact tasks where the motion decomposition abstraction is insufficient.

- **First 3 experiments:**
  1. **Inference Latency Test:** Measure tokens/sec for fast reasoning vs. slow reasoning. Compare against baseline VLMs.
  2. **Reasoning Ablation:** Train a variant where the domain expert is randomly initialized (no knowledge sharing). Verify failure on motion decomposition tasks, as shown in Table 3.
  3. **Language Steerability Test:** Provide ambiguous instructions (e.g., "Sort rubbish") and compare task success against a baseline that treats all objects as targets.

## Open Questions the Paper Calls Out

### Open Question 1
Can the MoTVLA architecture be effectively miniaturized to smaller parameter sizes (e.g., 0.5B) without losing general intelligence, thereby avoiding the prohibitive multi-stage pre-training currently required for small models? The conclusion notes that while inference speed improves at 0.5B scale, "pre-training a 0.5B model to acquire general intelligence remains highly challenging."

### Open Question 2
Does jointly pre-training the action expert with the reasoning backbone on large-scale open-sourced robotics datasets resolve the "strong reasoning but insufficient execution" failures observed in long-horizon tasks? The authors identify that "relatively limited amount of data available for training the action expert sometimes leads to... failures on long-horizon tasks," and suggest this joint pre-training as a future direction.

### Open Question 3
Is the architectural constraint requiring the generalist and domain expert to share the same model size (doubling parameters) fundamental to the shared global attention mechanism? Section 3.1 notes, "the current design requires the generalist and domain expert to share the same model size," which results in the reasoning backbone containing twice the parameters of the generalist alone.

### Open Question 4
Can the reliance on manually annotated motion decompositions be replaced by automated extraction from existing datasets without degrading policy accuracy? The authors note the "workload of collecting and annotating such data [motion decompositions] in-house is prohibitively large," limiting the action expert training data to only 1,050 trajectories.

## Limitations
- High-precision tasks (e.g., Peg-in-Hole) show notably lower success rates (40%) compared to other tasks
- Dual-7B architecture significantly increases computational requirements and memory footprint
- Generalization robustness to substantially different environments and out-of-distribution objects not fully quantified

## Confidence
- **Mechanism 1 (Knowledge Sharing): High** - Architecture and training procedure are clearly specified
- **Mechanism 2 (Fast Reasoning): High** - Token-wise prediction vs. autoregressive generation is well-established
- **Mechanism 3 (Diffusion Conditioning): Medium** - Conditioning mechanism is clear but depends on domain expert reasoning accuracy

## Next Checks
1. **Hallucination Robustness Test:** Systematically inject synthetic hallucinations into domain expert outputs and measure the diffusion policy's ability to recover or fail gracefully
2. **Catastrophic Forgetting Verification:** Train an ablation variant where the generalist is fine-tuned and benchmark its performance on general VQA datasets before and after robotic training
3. **Scaling Analysis:** Evaluate MoTVLA-1B (pre-trained variant) on the same tasks to determine whether the performance-efficiency tradeoff remains favorable at smaller scales