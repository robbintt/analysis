---
ver: rpa2
title: Exploiting Vulnerabilities in Speech Translation Systems through Targeted Adversarial
  Attacks
arxiv_id: '2503.00957'
source_url: https://arxiv.org/abs/2503.00957
tags:
- adversarial
- speech
- target
- attack
- music
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first adversarial attack study on speech
  translation systems, demonstrating two novel approaches to compromise these models
  through imperceptible audio manipulations. The first method injects adversarial
  perturbations into source audio, enhanced through Multi-language Enhancement and
  Target Cycle Optimization to improve semantic attack effectiveness across languages.
---

# Exploiting Vulnerabilities in Speech Translation Systems through Targeted Adversarial Attacks

## Quick Facts
- arXiv ID: 2503.00957
- Source URL: https://arxiv.org/abs/2503.00957
- Reference count: 40
- This paper presents the first adversarial attack study on speech translation systems, demonstrating two novel approaches to compromise these models through imperceptible audio manipulations.

## Executive Summary
This paper introduces the first systematic study of adversarial attacks on speech translation (ST) systems. The authors demonstrate two novel attack methods: injecting adversarial perturbations into source audio and generating adversarial music using diffusion models. Both approaches successfully force targeted mistranslations across multiple languages and ST models while remaining imperceptible to human listeners. The research reveals systemic vulnerabilities in current ST architectures and emphasizes the need for more robust defense mechanisms.

## Method Summary
The paper implements two attack strategies against speech translation systems. The first method optimizes adversarial perturbations added to source audio, enhanced through Multi-language Enhancement (optimizing for multiple target languages simultaneously) and Target Cycle Optimization (finding semantically equivalent target phrases). The second method generates adversarial music by optimizing the initial latent noise of a diffusion model, which produces audio that sounds like music to humans but translates to malicious content for machines. Both attacks use gradient-based optimization with teacher forcing to align model outputs with target translations, evaluated across multiple ST models (Seamless, Canary) and language pairs.

## Key Results
- Perturbation attacks achieve over 90% success rate in controlled conditions
- Music attacks maintain approximately 50% success rate in over-the-air physical testing
- Multi-language Enhancement improves cross-lingual transferability of attacks
- Signal processing defenses fail to fully restore semantic integrity of original speech

## Why This Works (Mechanism)

### Mechanism 1: Multi-language Semantic Alignment
The paper claims that optimizing adversarial perturbations for multiple "seen" languages simultaneously improves transferability to "unseen" languages compared to single-language optimization. The mechanism forces perturbations to align with a more abstract "global semantic center" in the model's shared multilingual latent space. This works under the assumption that ST models like Seamless map different languages into a shared, language-agnostic semantic space where a "central" representation exists. The approach fails if the target model uses entirely disjoint decoders or embedding spaces for different languages.

### Mechanism 2: Diffusion Latent Optimization for Covert Attacks
The music attack conceals adversarial content within generated music by optimizing the initial latent noise of a diffusion model rather than applying post-hoc perturbations. By backpropagating the ST model's translation loss through the diffusion model's reverse process, the initial noise vector and rhythm embeddings are adjusted to ensure the resulting waveform is acoustically recognized as "music" by humans but decodes to target semantics by the machine. This approach exploits the natural imperceptibility of music and assumes the ST model will attempt to transcribe/translate non-speech audio rather than rejecting it.

### Mechanism 3: Target Cycle Optimization (TCO)
TCO pre-processes the target phrase to find a model-specific "preferred" equivalent text that increases attack success rates. The method translates the intended target text into multiple languages and back (cycle translation), selecting the variation that appears most frequently as the new optimization target. The assumption is that this frequent variation sits closer to the model's stable semantic manifold, making it easier for gradient descent to locate. The mechanism relies on the model having internal semantic preferences due to training data biases.

## Foundational Learning

- **Autoregressive Teacher Forcing**: Required to understand how the attack calculates gradients at every step of text generation. The model predicts tokens one by one, and teacher forcing keeps the model on track for the target sentence during optimization. Quick check: If the model predicts the wrong token at step $m$, how does teacher forcing ensure the optimization for step $m+1$ remains aligned with the target?

- **Latent Diffusion Models (LDM)**: Essential for understanding the music attack, which manipulates the compressed latent space of a diffusion model rather than editing audio waveforms directly. Understanding the forward (adding noise) and reverse (denoising) process is required to know where to inject the adversarial gradient (specifically at the initial noise $\omega_T$). Quick check: Why is optimizing the initial latent noise $\omega_T$ generally more effective for generating coherent adversarial music than adding a perturbation to a final waveform?

- **Cross-Lingual Transferability**: Needed to understand why attacking the English output affects the French output. Modern ST models share encoders and sometimes decoders across languages, creating shared vulnerabilities. Quick check: Does the Multi-language Enhancement mechanism imply that the model has separate encoders for every language, or a shared encoder?

## Architecture Onboarding

- **Component map**: Source Audio (Perturbed) OR Diffusion Latent Noise (Music) -> Speech Encoder -> Text Decoder (Autoregressive) -> CrossEntropy Loss -> Optimizer Updates

- **Critical path**: The gradient flow from the Text Decoder back to the Input (either raw audio or latent noise). If the connection is severed (e.g., non-differentiable pre-processing or black-box constraints), the white-box attack fails.

- **Design tradeoffs**:
  - Perturbation Attack: High success rate (90%) but requires a specific carrier audio file
  - Music Attack: Lower success rate (50%), but can be played "over-the-air" and disguises the attack as background noise
  - TCO: Adds computational overhead to pre-processing but significantly improves ASR for unseen languages

- **Failure signatures**:
  - Low ESIM/NSCORE: The model outputs gibberish or the original translation
  - "Music" Label: The model correctly identifies the adversarial audio as music and outputs a description (e.g., "<music playing>") rather than the target text

- **First 3 experiments**:
  1. Single-Lang Baseline: Implement the perturbation attack on a single language pair to verify gradient propagation and basic effectiveness
  2. Multi-Lang Ablation: Add one "seen" language at a time to the loss function and measure the drop in ASR for "unseen" languages to validate the semantic center hypothesis
  3. OTA Simulation: Generate adversarial music and play it through a speaker/microphone setup to test robustness against real-world channel noise and reverberation

## Open Questions the Paper Calls Out

### Open Question 1
How effective are the proposed adversarial attacks against Speech Translation (ST) systems in strictly black-box scenarios where attackers lack access to model parameters and gradients? The paper assumes white-box access with gradient availability, leaving the feasibility of query-based or transfer-based attacks in real-world API-only environments unexplored. Success rates of attacks generated using query-based methods or surrogate models against proprietary ST systems would resolve this question.

### Open Question 2
Can architectural modifications or adversarial training regimes be developed to mitigate these attacks without significantly degrading translation performance or semantic integrity? The paper concludes with the need for advanced defense mechanisms, noting that tested signal processing defenses do not fully restore semantic integrity. Evaluation of translation accuracy and attack resilience on ST models fine-tuned with adversarial examples or incorporating specific robustness constraints would provide evidence.

### Open Question 3
Can over-the-air attack success rates be improved beyond the observed ~50% threshold while maintaining acoustic imperceptibility in diverse physical environments? While digital attacks succeed up to 90%, physical over-the-air music attacks achieve approximately 50%, indicating a significant performance gap caused by channel distortions. Demonstration of enhanced physical-aware optimization techniques achieving higher success rates in unconstrained real-world settings would resolve this question.

## Limitations
- The Multi-language Enhancement mechanism's effectiveness relies on the semantic center hypothesis, which may not hold for models with separate decoders or different training distributions
- Physical robustness of attacks shows significant degradation from lab conditions, with success rates dropping from ~90% to ~50%
- The choice of semantic similarity metrics and their thresholds could influence reported results, though not fully validated against human judgment in over-the-air tests

## Confidence
- **High Confidence**: The core methodology for gradient-based optimization and basic perturbation attack mechanism are well-established in adversarial ML literature and directly reproducible
- **Medium Confidence**: The Multi-language Enhancement's effectiveness relies on the semantic center hypothesis, which is theoretically plausible but not definitively proven for all multilingual ST models
- **Medium Confidence**: The music attack's success depends on the ST model's willingness to translate non-speech audio, which varies by implementation
- **Low Confidence**: The specific numerical results (exact ASR percentages) may vary significantly with different hyperparameter choices, model versions, or environmental conditions during over-the-air testing

## Next Checks
1. **Architecture Dependency Test**: Implement the perturbation attack against a different ST model (e.g., one with separate language decoders) to verify if the Multi-language Enhancement mechanism fails as predicted when the shared semantic center assumption is violated

2. **OTA Environmental Validation**: Conduct a controlled over-the-air experiment varying only the distance and background noise level to quantify the relationship between physical conditions and attack success rates, confirming the ~50% success rate is not an artifact of a single test scenario

3. **TCO Ablation Study**: Run the perturbation attack with and without Target Cycle Optimization on both "seen" and "unseen" languages to empirically measure the claimed improvement in ASR and determine if the computational overhead is justified for different target phrases