---
ver: rpa2
title: 'SHAP Distance: An Explainability-Aware Metric for Evaluating the Semantic
  Fidelity of Synthetic Tabular Data'
arxiv_id: '2511.17590'
source_url: https://arxiv.org/abs/2511.17590
tags:
- data
- synthetic
- shap
- real
- tabular
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces the SHAP Distance, a novel explainability-aware
  metric for evaluating the semantic fidelity of synthetic tabular data. Traditional
  evaluation methods like KL divergence and TSTR accuracy focus on distributional
  similarity and predictive performance but fail to assess whether models trained
  on synthetic data follow the same reasoning patterns as those trained on real data.
---

# SHAP Distance: An Explainability-Aware Metric for Evaluating the Semantic Fidelity of Synthetic Tabular Data

## Quick Facts
- arXiv ID: 2511.17590
- Source URL: https://arxiv.org/abs/2511.17590
- Authors: Ke Yu; Shigeru Ishikura; Yukari Usukura; Yuki Shigoku; Teruaki Hayashi
- Reference count: 37
- Introduces SHAP Distance, a novel metric measuring semantic fidelity between real and synthetic tabular data by comparing feature attribution patterns

## Executive Summary
This study addresses a critical gap in synthetic data evaluation by introducing SHAP Distance, an explainability-aware metric that measures semantic fidelity between real and synthetic tabular data. Traditional metrics like KL divergence and TSTR accuracy focus on distributional similarity and predictive performance but fail to assess whether models trained on synthetic data follow the same reasoning patterns as those trained on real data. The SHAP Distance computes the cosine distance between global SHAP attribution vectors derived from classifiers trained on real and synthetic datasets, capturing feature importance shifts and underrepresented tail effects that standard metrics miss.

Experiments on three diverse datasets—UCI Heart Disease, Enterprise Invoice Usage, and Telco Churn—demonstrate that synthetic data refined using SHAP Distance achieves the highest downstream utility, indicating that semantic alignment is more critical than perfect statistical resemblance for practical applications. The metric provides a practical tool for auditing semantic fidelity and offers guidelines for integrating attribution-based evaluation into synthetic data benchmarking pipelines.

## Method Summary
The SHAP Distance metric involves training separate classifiers on real and synthetic datasets, computing global SHAP attribution vectors for each classifier, and calculating the cosine distance between these vectors. The method includes data preprocessing (missing value imputation, categorical encoding, normalization, undersampling), dual classifier training, SHAP computation using TreeExplainer or KernelExplainer, and distance calculation. An optional iterative refinement procedure adjusts generation based on attribution misalignment gaps. The metric is compared against KL divergence, PCA variance ratio, and TSTR accuracy baselines.

## Key Results
- SHAP Distance effectively captures semantic discrepancies that standard metrics overlook, even when synthetic data exhibits statistical deviations from real data
- Synthetic data refined using SHAP Distance feedback achieves highest downstream utility across all three tested datasets
- The metric successfully detects feature importance shifts and underrepresented tail effects, such as Telco Churn synthetic models overemphasizing Contract while diminishing MonthlyCharges
- Results demonstrate that semantic alignment is more critical than perfect statistical resemblance for practical applications

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Cosine distance between SHAP attribution vectors captures semantic fidelity gaps that distributional metrics miss
- **Mechanism:** Train classifiers C_real ← Train(D_real) and C_syn ← Train(D_syn). Extract global SHAP attribution vectors φ_real = [φ_real_1, ..., φ_real_d] and φ_syn = [φ_syn_1, ..., φ_syn_d]. Compute D_SHAP = 1 - (φ_real · φ_syn) / (||φ_real|| ||φ_syn||). Values near 0 indicate preserved reasoning patterns; values near 1 indicate divergent decision logic
- **Core assumption:** Feature attribution profiles reflect the "reasoning" of a classifier in a comparable way across datasets
- **Evidence anchors:**
  - [abstract] "cosine distance between the global SHAP attribution vectors derived from classifiers trained on real versus synthetic datasets"
  - [Section III.C] Formal definition of D_SHAP as angular dissimilarity
  - [corpus] Related work "What's Wrong with Your Synthetic Tabular Data?" (arXiv:2504.20687) uses XAI to diagnose synthetic data flaws, supporting attribution-based evaluation
- **Break condition:** If classifiers overfit or SHAP values are unstable, attribution vectors may not reflect true decision logic

### Mechanism 2
- **Claim:** SHAP Distance detects feature importance shifts and tail effects that KL divergence and TSTR accuracy miss
- **Mechanism:** Marginal distribution metrics (KL) measure univariate alignment. Predictive metrics (TSTR) measure outcome accuracy. Neither captures whether models rely on the same features in the same way. SHAP compares the "why" behind predictions
- **Core assumption:** Synthetic data can achieve statistical similarity or high TSTR accuracy while using altered feature importance patterns
- **Evidence anchors:**
  - [abstract] "SHAP Distance captures feature importance shifts and underrepresented tail effects that the Kullback–Leibler divergence and TSTR accuracy fail to detect"
  - [Section V.C] Telco Churn synthetic model overemphasizes Contract (40%) while diminishing MonthlyCharges
  - [corpus] FEST framework (arXiv:2508.16254) notes lack of comprehensive assessment frameworks, validating the gap SHAP Distance addresses
- **Break condition:** If synthetic data perfectly matches real data distributionally, SHAP Distance becomes redundant

### Mechanism 3
- **Claim:** Iterative refinement using SHAP Distance feedback improves downstream utility
- **Mechanism:** Compute D_SHAP^(t). If > ε, refine generation by emphasizing divergent features in prompts or model configuration. Repeat until convergence. This aligns feature attribution patterns between real and synthetic data
- **Core assumption:** Attribution misalignment can be reduced through targeted adjustments to the generation process
- **Evidence anchors:**
  - [Section III.D] Formal iterative refinement procedure with convergence threshold ε
  - [Section V.E] "synthetic data refined using the SHAP Distance achieved the highest downstream utility"
  - [corpus] Evidence limited; TabStruct (arXiv:2509.11950) addresses structural fidelity but not iterative feedback specifically
- **Break condition:** If generation process lacks controllable parameters for feature-specific adjustment, feedback cannot be applied

## Foundational Learning

- **Concept: SHAP (SHapley Additive exPlanations)**
  - Why needed here: Core to the metric—understanding how Shapley values attribute prediction contributions per feature is essential
  - Quick check question: Given a classifier, can you explain why SHAP provides consistent feature attributions across different model types?

- **Concept: Cosine Distance vs. Euclidean Distance**
  - Why needed here: The paper uses cosine distance specifically to measure angular dissimilarity, not magnitude differences
  - Quick check question: Why would cosine distance be preferred over Euclidean distance when comparing attribution vectors of different magnitudes?

- **Concept: Train-on-Synthetic-Test-on-Real (TSTR)**
  - Why needed here: The baseline metric SHAP Distance is compared against; understanding TSTR limitations motivates the approach
  - Quick check question: If a synthetic dataset achieves 95% TSTR accuracy, what can you conclude about its semantic fidelity?

## Architecture Onboarding

- **Component map:** Data preprocessing -> Dual classifier training -> SHAP computation -> Distance calculation -> Optional feedback loop
- **Critical path:** Classifier quality → SHAP stability → Attribution vector reliability → D_SHAP interpretability. Assumption: Classifiers must achieve sufficient accuracy for SHAP values to be meaningful
- **Design tradeoffs:**
  - Model choice for classifiers: XGBoost/Random Forest (faster SHAP via TreeExplainer) vs. neural nets (slower, requires DeepExplainer or GradientExplainer)
  - Global vs. local SHAP: Paper uses global (averaged) attributions; local SHAP could reveal instance-level semantic gaps but increases complexity
  - Threshold ε for iterative refinement: Too low → excessive iterations; too high → insufficient refinement
- **Failure signatures:**
  - D_SHAP ≈ 0 but TSTR accuracy low: Synthetic data mimics feature importance but lacks predictive signal
  - D_SHAP ≈ 1 but TSTR accuracy high: Synthetic data achieves outcomes via spurious correlations
  - SHAP values highly variable across runs: Classifier instability or insufficient training data
- **First 3 experiments:**
  1. Replicate D_SHAP computation on UCI Heart Disease dataset using XGBoost classifier; compare φ_real and φ_syn rankings
  2. Generate synthetic data with deliberately altered feature correlations; verify D_SHAP increases while KL divergence remains low
  3. Implement single-iteration feedback: Identify top-3 divergent features, adjust generation prompt, measure D_SHAP reduction

## Open Questions the Paper Calls Out
None

## Limitations
- Metric dependence on classifier stability: If models trained on either real or synthetic data exhibit poor generalization (accuracy < 80% on holdout sets), attribution vectors may reflect spurious patterns rather than genuine decision logic
- Assumes global SHAP values adequately represent feature importance across all instances, potentially masking instance-specific reasoning variations
- Limited generalizability across domains: Results demonstrated on three tabular datasets with binary classification tasks; extension to multi-class problems, time-series data, or text-based tabular features remains unverified

## Confidence
- **Core claim:** Medium-High - Mechanism is well-founded in XAI literature, empirical results show consistent patterns across three diverse datasets, but lacks ablation studies on SHAP explainer configuration and classifier architecture
- **Iterative refinement mechanism:** Medium - Reports improved downstream utility but specific implementation details are not fully specified and reference to prior work [37] limits reproducibility
- **Generalizability across domains:** Medium-Low - Demonstrated on three tabular datasets with binary classification tasks; unverified for multi-class problems, time-series data, or text-based tabular features

## Next Checks
1. Reproduce SHAP Distance calculations on UCI Heart Disease using multiple classifier types (XGBoost, Random Forest, Neural Network) to assess attribution stability across model families
2. Generate synthetic data with controlled feature importance distortions (e.g., swap importance rankings) and verify SHAP Distance sensitivity to these semantic shifts while KL divergence remains unchanged
3. Implement ablation study comparing SHAP Distance with and without iterative refinement on a held-out dataset to quantify the marginal benefit of the feedback loop