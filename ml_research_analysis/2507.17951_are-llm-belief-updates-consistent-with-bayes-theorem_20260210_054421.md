---
ver: rpa2
title: Are LLM Belief Updates Consistent with Bayes' Theorem?
arxiv_id: '2507.17951'
source_url: https://arxiv.org/abs/2507.17951
tags:
- evidence
- class
- more
- arxiv
- updates
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel metric, the Bayesian Coherence Coefficient
  (BCC), to measure how consistently large language models update their beliefs in
  accordance with Bayes' theorem when presented with evidence. The authors generate
  a dataset with multiple categories, each containing classes, evidences, and conversation
  histories, and compute the BCC as the correlation between expected and observed
  credence updates.
---

# Are LLM Belief Updates Consistent with Bayes' Theorem?

## Quick Facts
- **arXiv ID:** 2507.17951
- **Source URL:** https://arxiv.org/abs/2507.17951
- **Reference count:** 34
- **Primary result:** Larger LLMs update beliefs more consistently with Bayes' theorem, with Bayesian Coherence Coefficient (BCC) increasing log-linearly with parameter count (r=0.906, p<10^-6).

## Executive Summary
This paper introduces the Bayesian Coherence Coefficient (BCC) to measure how consistently large language models update beliefs in accordance with Bayes' theorem. The authors generate synthetic datasets with classes, evidences, and conversation histories, then compute BCC as the correlation between expected and observed credence updates. They evaluate multiple pre-trained language models across five model families, finding that larger and more capable models exhibit higher BCC values. These results suggest that model scale induces internal representations that better approximate Bayesian belief updates, with implications for understanding, governing, and aligning future AI systems.

## Method Summary
The authors generate synthetic datasets containing class pairs, evidences, and conversation histories. They extract belief states using cumulative token probabilities and compute expected log-odds updates based on Bayes' theorem. The Bayesian Coherence Coefficient (BCC) is defined as the correlation between expected and observed log-odds updates. The methodology is tested across multiple pre-trained language models from five different model families, with temperature fixed at 1.0 to ensure robustness of the correlation metric.

## Key Results
- BCC increases log-linearly with model parameter count (r=0.906, p<10^-6)
- Larger models exhibit more coherent belief updates
- BCC shows significant positive correlations with BIG-Bench Hard, GPQA, MMLU-PRO, and Math Lvl 5 benchmark scores
- Models display systematic conservatism, under-updating for low-likelihood evidence

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Increased model scale induces internal representations that better approximate Bayesian belief updates.
- **Mechanism:** As parameter count increases, LLMs develop more structured internal world models, allowing better separation of priors from likelihoods during in-context processing.
- **Core assumption:** Cumulative token probability serves as a valid proxy for the model's internal credence.
- **Evidence anchors:** Strong correlation (r=0.906, p<10^-6) between BCC and log parameter count; larger models exhibit more coherent updates.
- **Break condition:** Future models using architectures that decouple sequence prediction from internal belief states might degrade this relationship.

### Mechanism 2
- **Claim:** Correlation-based metrics (BCC) are more robust than error-based metrics (BCE) because they penalize high-entropy "safe" strategies.
- **Mechanism:** Error-based metrics can be minimized to zero by outputting uniform distributions, while correlation metrics require consistent directional updates regardless of absolute error.
- **Core assumption:** A coherent agent must change beliefs in the correct direction relative to evidence strength.
- **Evidence anchors:** BCC definition as correlation; BCE exhibits bias toward high-entropy distributions; appendix A.2 shows this limitation.
- **Break condition:** Systematic bias in update magnitude (always 10x too small) would maintain high BCC despite poor calibration.

### Mechanism 3
- **Claim:** LLMs display systematic conservatism or under-updating when presented with low-likelihood evidence.
- **Mechanism:** Observed update gradient (slope < 1) suggests models dampen impact of surprising evidence compared to strict Bayesian ideal.
- **Core assumption:** Evidence in synthetic dataset is often lower probability than class context.
- **Evidence anchors:** Gradient < 1 for all models; inverse correlation between update gradient and negative evidence log likelihood.
- **Break condition:** If evidence is semantically strong but high-probability, gradient may approach ideal 1.0.

## Foundational Learning

- **Concept:** Log-Odds and Bayes' Rule
  - **Why needed here:** Paper defines updating exclusively in log-space; understanding how multiplication becomes addition is required to interpret BCC metric.
  - **Quick check question:** If a prior is 0.5 and evidence doubles the likelihood of hypothesis A vs B, what is the new log-odds?

- **Concept:** Cumulative Token Probability
  - **Why needed here:** Belief states are extracted by multiplying probabilities of sequential tokens (e.g., P("Virginia") * P("Woolf" | "Virginia")).
  - **Quick check question:** How does a high-entropy token at the start of a class name affect the cumulative probability of the full string?

- **Concept:** Temperature Scaling in LLMs
  - **Why needed here:** Fixed temperature=1.0; temperature distorts output entropy, which breaks error-based metrics but preserves correlation-based metrics.
  - **Quick check question:** If temperature is set to 0.01, would you expect BCC to change significantly compared to BCE?

## Architecture Onboarding

- **Component map:** Dataset Generator -> Elicitation Engine -> Metric Calculator
- **Critical path:** Extraction of P(Evidence | Class, History) is most sensitive; prompt must strictly condition on class and history before generating evidence tokens.
- **Design tradeoffs:**
  - BCC vs. BCE: BCC measures alignment with Bayes rule, not closeness to it; doesn't detect magnitude bias
  - Cumulative Probability: Easy via API logprobs but assumes optimal tokenization
- **Failure signatures:**
  - BCC ≈ 0: Model not updating beliefs based on evidence
  - High BCE / Low BCC: Model outputting uniform distributions
  - Gradient ≈ 0: Model ignoring evidence entirely
- **First 3 experiments:**
  1. Verify Entropy Robustness: Run evaluation at temperatures [0.1, 0.5, 1.0, 1.5]; plot BCC vs. BCE
  2. Sanity Check with Random Weights: Evaluate untrained model; BCC should be ~0
  3. Instruction Tuning Ablation: Compare base vs instruction-tuned models to test hypothesis about alignment effects

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Cumulative probability extraction assumption may be noisy if model tokenization obscures semantic meaning
- Synthetic dataset scope limits generalizability to real-world, multi-step reasoning tasks
- Conservative update pattern observed but not fully explained; could be feature or bug

## Confidence
- BCC increases with model scale (r=0.906): High confidence
- BCC correlates with benchmark performance: Medium confidence
- Correlation metrics are more robust than error metrics: High confidence
- Systematic conservatism in low-likelihood evidence: Low confidence

## Next Checks
1. **Real-World Dataset Validation:** Construct dataset of real-world belief update scenarios (medical diagnosis, scientific hypothesis testing) and re-evaluate BCC to test generalizability.

2. **Magnitude Calibration Test:** Design experiment to measure magnitude of updates, compute ratio of observed to expected log-odds, and test correlation with separate calibration benchmark.

3. **Architecture Ablation Study:** Evaluate BCC of different architectures (pure transformer, transformer + retrieval, state-space models) at same parameter count to isolate whether scaling trend is due to scale or architectural biases.