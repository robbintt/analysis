---
ver: rpa2
title: 'Eval Factsheets: A Structured Framework for Documenting AI Evaluations'
arxiv_id: '2512.04062'
source_url: https://arxiv.org/abs/2512.04062
tags:
- evaluation
- data
- evaluations
- what
- documentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Eval Factsheets introduces a systematic framework for documenting
  AI evaluation methodologies across five dimensions: Context, Scope, Structure, Method,
  and Alignment. Through a questionnaire-based approach, it addresses the documentation
  gap in evaluation transparency that exists despite established frameworks for datasets
  (Datasheets) and models (Model Cards).'
---

# Eval Factsheets: A Structured Framework for Documenting AI Evaluations
## Quick Facts
- arXiv ID: 2512.04062
- Source URL: https://arxiv.org/abs/2512.04062
- Reference count: 5
- Primary result: Systematic framework for documenting AI evaluation methodologies across five dimensions: Context, Scope, Structure, Method, and Alignment

## Executive Summary
Eval Factsheets introduces a comprehensive framework for documenting AI evaluation methodologies through a questionnaire-based approach spanning five key dimensions: Context, Scope, Structure, Method, and Alignment. The framework addresses a critical documentation gap in AI evaluation transparency, complementing existing standards like Datasheets for datasets and Model Cards for models. By providing structured templates and integration guidelines, it enables consistent documentation across diverse evaluation paradigms, from traditional benchmarks to LLM-as-judge methodologies.

The framework demonstrates practical applicability through case studies on ImageNet, HumanEval, and MT-Bench, showing how different evaluation types can be documented while maintaining cross-paradigm comparability. The systematic approach aims to improve reproducibility, enable informed decision-making, and advance standardized reporting practices in AI evaluation, ultimately creating a more transparent and accountable evaluation ecosystem.

## Method Summary
The framework employs a structured questionnaire-based approach to document evaluation methodologies across five dimensions. The Context dimension captures the motivation and intended use cases for the evaluation. The Scope dimension defines what is being evaluated and under what conditions. The Structure dimension documents the evaluation design and organization. The Method dimension details the specific procedures and techniques used. The Alignment dimension addresses how the evaluation relates to broader goals and values. The framework provides templates and integration guidelines for both manual documentation and potential automated extraction from evaluation codebases.

## Key Results
- Framework successfully captures evaluation details across diverse paradigms including traditional benchmarks, LLM-as-judge, and human evaluations
- Case studies on ImageNet, HumanEval, and MT-Bench demonstrate practical applicability and cross-paradigm comparability
- Questionnaire-based approach enables systematic documentation while maintaining flexibility for domain-specific extensions
- Integration with existing standards (Datasheets, Model Cards) creates a comprehensive documentation ecosystem

## Why This Works (Mechanism)
The framework's effectiveness stems from its systematic decomposition of evaluation documentation into five orthogonal dimensions that capture all essential aspects of an evaluation. The questionnaire-based approach provides structure while remaining flexible enough to accommodate diverse evaluation paradigms. By aligning with established documentation standards and providing clear templates, it reduces the cognitive burden of creating comprehensive documentation. The framework addresses the fundamental problem that evaluations are often under-documented, making it difficult to understand what was actually measured, under what conditions, and how results should be interpreted.

## Foundational Learning
- Evaluation transparency gap: Evaluations often lack sufficient documentation to understand methodology, conditions, and limitations. This is problematic because users cannot properly interpret results or compare across evaluations.
- Five-dimensional taxonomy: The Context, Scope, Structure, Method, and Alignment dimensions provide a comprehensive framework for capturing evaluation details. This is needed because evaluations have diverse characteristics that traditional documentation often misses.
- Questionnaire-based documentation: Structured questions ensure comprehensive coverage while remaining accessible. This is important because manual documentation can be inconsistent and incomplete.
- Cross-paradigm comparability: Standardized documentation enables comparison across different evaluation types. This is valuable because the AI field uses diverse evaluation approaches that are currently difficult to compare.
- Integration with existing standards: Alignment with Datasheets and Model Cards creates a unified documentation ecosystem. This is beneficial because it builds on established practices rather than creating entirely new ones.

## Architecture Onboarding
Component map: Questionnaire -> Template generation -> Documentation -> Integration guidelines
Critical path: Questionnaire completion → Template population → Documentation review → Community adoption
Design tradeoffs: Comprehensive coverage vs. documentation burden, flexibility vs. standardization, manual vs. automated generation
Failure signatures: Incomplete documentation, inconsistent application across evaluations, resistance to adoption due to overhead
First experiments:
1. Document an existing evaluation using the questionnaire to test completeness
2. Compare documentation quality between manual and semi-automated approaches
3. Conduct user studies to assess documentation burden and adoption barriers

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How can the Eval Factsheets taxonomy be adapted for specialized domains (e.g., medical AI, robotics) without fragmenting the standard or losing cross-domain comparability?
- Basis in paper: [explicit] The conclusion explicitly invites the community to "contribute domain-specific extensions," while Section 2.4 notes that current domain guidelines lack a unified framework.
- Why unresolved: The paper provides a general-purpose taxonomy but leaves the specific design and validation of domain-specific extensions as future community-driven work.
- What evidence would resolve it: A set of validated, domain-specific addenda (e.g., for clinical NLP or autonomous driving) that integrate with the core 27 questions without creating conflicting documentation standards.

### Open Question 2
- Question: Does the implementation of Eval Factsheets measurably improve reproducibility and informed model selection compared to current ad-hoc reporting?
- Basis in paper: [inferred] The paper states the framework "aims to improve reproducibility [and] enable informed decision-making," but validates the framework only via structural case studies (ImageNet, HumanEval, MT-Bench) rather than user studies or reproducibility trials.
- Why unresolved: The paper demonstrates that the framework *can* capture evaluation details, but does not provide empirical evidence that the documentation format actually changes researcher behavior or success rates.
- What evidence would resolve it: A controlled study measuring the speed and accuracy with which researchers can reproduce results or select models when provided with Eval Factsheets versus standard paper documentation.

### Open Question 3
- Question: To what extent can the generation of Eval Factsheets be automated or semi-automated from evaluation codebases to minimize the documentation burden?
- Basis in paper: [inferred] The framework relies on a "questionnaire-based approach" and manual templates (Section 4.2), which creates a trade-off between comprehensiveness and the "completion burden" mentioned in Section 4.1.
- Why unresolved: The paper focuses on defining the taxonomy and structure but does not explore technical methods to extract this metadata directly from benchmarking code or papers.
- What evidence would resolve it: The development of tools that can parse evaluation scripts (e.g., from HELM or BigBench) to automatically populate fields like "Input Source," "Judge Type," or "Metric," reducing manual entry.

## Limitations
- The framework's effectiveness relies on widespread adoption across diverse evaluation communities, which remains unproven
- Case studies demonstrate conceptual applicability but don't show practical improvements in reproducibility or transparency
- The questionnaire-based approach may create documentation fatigue, particularly for rapid prototyping scenarios
- No empirical validation of whether the framework actually improves researcher behavior or evaluation practices

## Confidence
- High confidence: The framework's logical structure and alignment with existing documentation standards (Datasheets, Model Cards)
- Medium confidence: The framework's ability to improve reproducibility and comparability across evaluation paradigms
- Medium confidence: The questionnaire design effectively captures relevant evaluation dimensions

## Next Checks
1. Conduct user studies with evaluation practitioners to assess documentation burden and actual adoption barriers
2. Implement longitudinal tracking of whether documented evaluations using the framework show improved reproducibility rates over time
3. Perform cross-paradigm validation by having independent teams document identical evaluations to test consistency and completeness