---
ver: rpa2
title: Efficient Reinforcement Learning by Reducing Forgetting with Elephant Activation
  Functions
arxiv_id: '2509.19159'
source_url: https://arxiv.org/abs/2509.19159
tags:
- learning
- frame
- elephant
- activation
- functions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates catastrophic forgetting in reinforcement
  learning through the lens of activation functions. The authors theoretically analyze
  the role of activation functions in training dynamics, showing that sparse gradients,
  alongside sparse representations, are crucial for mitigating forgetting.
---

# Efficient Reinforcement Learning by Reducing Forgetting with Elephant Activation Functions

## Quick Facts
- **arXiv ID**: 2509.19159
- **Source URL**: https://arxiv.org/abs/2509.19159
- **Reference count**: 40
- **Primary result**: Elephant activation functions improve sample efficiency and memory efficiency in value-based RL by reducing catastrophic forgetting through dual sparsity (activation and gradient).

## Executive Summary
This paper addresses catastrophic forgetting in reinforcement learning by analyzing the role of activation functions. The authors introduce "elephant activation functions" that produce both sparse activations and sparse gradients, theoretically satisfying a "mild forgetting" property via the Neural Tangent Kernel (NTK). Experiments show these activations significantly improve performance, particularly in memory-constrained settings, by maintaining high performance even with small replay buffers.

## Method Summary
The method replaces standard activation functions with elephant activation functions defined as $Elephant(x) = \frac{h}{1 + |x/a|^d}$, where $a$ controls the width of activation, $d$ controls slope/gradient sparsity, and $h$ is the height. The activation is applied after Layer Normalization to ensure inputs fall within the active range. The function is tested in value-based RL algorithms (DQN, Rainbow) on classic control and Atari benchmarks, with particular focus on memory efficiency through buffer ablation studies.

## Key Results
- Elephant activation functions maintain high performance with very small replay buffers (size 32) while ReLU performance collapses
- The method significantly improves sample efficiency and memory efficiency in value-based RL algorithms
- Performance gains are most pronounced under memory constraints, with consistent improvements across classic control and Atari games

## Why This Works (Mechanism)

### Mechanism 1: Dual Sparsity (Activation & Gradient)
Simultaneous sparsity in both forward activations and backward gradients significantly reduces interference during gradient updates compared to standard sparse activations. The Elephant function vanishes for large inputs (sparse activation) and has vanishing derivatives (sparse gradient), theoretically satisfying the "mild forgetting" property where the NTK â‰ˆ 0 for dissimilar inputs.

### Mechanism 2: Induced Local Elasticity
The architecture enforces "local elasticity," enabling "point-wise" function editing without global distortion. By minimizing the dot product of gradient vectors for dissimilar inputs, the NTK effectively becomes diagonal, confining weight changes to the local neighborhood of the current training sample.

### Mechanism 3: Alleviation of Buffer Reliance
Reducing architectural forgetting allows RL agents to maintain performance with significantly smaller replay buffers. Since the network inherently resists overwriting past knowledge due to gradient sparsity, it requires less rehearsal from a replay buffer to stabilize learning.

## Foundational Learning

- **Concept**: Neural Tangent Kernel (NTK)
  - **Why needed here**: The paper's core theoretical argument (Lemma 1, Theorem 1) is built on analyzing the NTK to prove that Elephant activations reduce interference. You cannot understand *why* it works without this.
  - **Quick check question**: How does the inner product of gradients for two different inputs relate to "forgetting"?

- **Concept**: Catastrophic Forgetting in RL
  - **Why needed here**: This is the specific problem being solved. You need to distinguish between forgetting in supervised learning vs. RL (where non-stationarity comes from bootstrapping).
  - **Quick check question**: Why does the TD target changing value for a specific state constitute a "continual learning" problem?

- **Concept**: Sparse Representations
  - **Why needed here**: The paper positions itself as an improvement over methods that only use sparse activations (like ReLU or SR-NN).
  - **Quick check question**: Why is ReLU considered only "half-sparse" in the context of this paper's Definition 1?

## Architecture Onboarding

- **Component map**: Layer Normalization -> Elephant Activation -> Neural Network Layers
- **Critical path**: **Layer Normalization**. The Elephant function has a narrow active range (-a to a). The paper explicitly notes in Section C that LayerNorm is applied to inputs to ensure they land in the active region. Without this, the network may fail to train.
- **Design tradeoffs**:
  - **Parameter $a$**: Controls the stability-plasticity trade-off. Small $a$ (e.g., 0.1) is more stable but may miss details; large $a$ is more plastic but risks interference. Must be tuned per task.
  - **Parameter $d$**: Generally robust (values like 4 or 8 work well). Higher $d$ increases sparsity.
  - **Bias Initialization**: Unlike standard ReLU (zeros), Elephant prefers uniform spacing for biases to diversify initial features (Section C).
- **Failure signatures**:
  - **Dead Network**: Gradients vanish entirely. Check if input magnitudes are vastly larger than $a$.
  - **Instability**: If $a$ is too large, the benefits of local elasticity vanish, and it behaves like a dense network with forgetting issues.
- **First 3 experiments**:
  1. **Sanity Check (Regression)**: Replicate the sine wave approximation (Section 6.1). Verify that the NTK heatmap is diagonal for Elephant but diffused for ReLU.
  2. **Buffer Ablation**: Run DQN on a classic control task (e.g., CartPole/MountainCar) with buffer sizes [32, 100, 1000, 10000]. Compare Elephant vs. ReLU to verify memory efficiency.
  3. **Input Scaling Check**: Run a single task with and without LayerNorm on the inputs to the Elephant layer. Confirm that performance drops without normalization due to input mismatch with the [-a, a] window.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Why do elephant activation functions fail to provide significant performance advantages in policy gradient methods compared to their success in value-based methods?
- **Basis in paper**: [explicit] Appendix D.2.3 states, "Elephant does not show significant advantage over other activation functions for policy gradient methods... We leave a deeper investigation of this phenomenon for future work."
- **Why unresolved**: While the paper validates the method for DQN and Rainbow, the theoretical mechanism (sparse gradients/outputs) appears to interact differently with the optimization dynamics of PPO and SAC, producing mixed or neutral results.
- **What evidence would resolve it**: A comparative analysis of gradient interference and curvature in on-policy (PPO) versus off-policy (SAC) learning to identify why local elasticity is less beneficial in these contexts.

### Open Question 2
- **Question**: Can elephant activation functions effectively mitigate catastrophic forgetting in model-based reinforcement learning environments?
- **Basis in paper**: [explicit] The conclusion notes, "While our results demonstrate the effectiveness of elephant activation functions in model-free RL, we have not yet explored its applicability in model-based RL."
- **Why unresolved**: Model-based RL involves learning environment dynamics which may have different stability requirements and non-stationarity profiles than the value functions tested in the paper.
- **What evidence would resolve it**: Benchmarks of elephant activation functions in popular model-based algorithms (e.g., Dreamer or MuZero) measuring sample efficiency and retention of dynamics models.

### Open Question 3
- **Question**: Is there a principled, data-driven method for automatically selecting the hyperparameters of elephant activation functions?
- **Basis in paper**: [explicit] The conclusion lists as a limitation the "lack of a principled method for selecting the hyper-parameters... The optimal values... depend on both the input data and architectural factors."
- **Why unresolved**: Currently, optimal parameters like width ($a$) and slope ($d$) require manual tuning for each task, which hinders the method's general applicability.
- **What evidence would resolve it**: A derivation of a self-tuning rule or adaptive mechanism that sets the width $a$ based on input statistics or loss dynamics, eliminating the need for grid search.

## Limitations
- The theoretical analysis relies on NTK regime assumptions (infinite width, linear training dynamics) that may not fully capture practical deep RL scenarios
- The empirical evaluation lacks ablation studies on the individual contributions of activation sparsity vs. gradient sparsity
- Performance gains are most pronounced under memory-constrained settings, but scalability to more complex tasks and longer training horizons remains unexplored

## Confidence

- **High**: The dual sparsity mechanism (activation + gradient) is theoretically sound and the regression experiments support the NTK analysis
- **Medium**: The memory efficiency claims are well-supported by buffer ablation experiments, though the absolute performance gains vary by task
- **Medium**: The sample efficiency improvements are demonstrated but could be more thoroughly analyzed across different learning rates and initial conditions

## Next Checks

1. **Ablation Study**: Isolate the contributions of activation sparsity vs. gradient sparsity by testing variants of Elephant with only one sparsity property
2. **NTK Analysis Extension**: Verify the orthogonality properties in finite-width networks through empirical NTK computation on trained models
3. **Scalability Test**: Evaluate Elephant on more complex control tasks (e.g., Humanoid) and longer training horizons to assess performance stability over time