---
ver: rpa2
title: Learning Power Control Protocol for In-Factory 6G Subnetworks
arxiv_id: '2505.05967'
source_url: https://arxiv.org/abs/2505.05967
tags:
- power
- signaling
- subnetwork
- subnetworks
- allocation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses power control in dense 6G In-Factory subnetworks,
  where interference from high AP density degrades performance. Existing approaches
  either assume full CSI availability or rely on predefined signaling protocols, leading
  to high signaling overhead.
---

# Learning Power Control Protocol for In-Factory 6G Subnetworks

## Quick Facts
- **arXiv ID:** 2505.05967
- **Source URL:** https://arxiv.org/abs/2505.05967
- **Reference count:** 16
- **Primary result:** Learning-based power control framework achieves 5% gap to optimal performance while reducing signaling overhead by 8× compared to CSI-based baselines

## Executive Summary
This paper addresses the challenge of power control in dense 6G In-Factory subnetworks where interference from high AP density degrades performance. Traditional approaches either assume full channel state information (CSI) availability or rely on predefined signaling protocols, leading to high signaling overhead. The authors propose a multi-agent reinforcement learning framework where access points (APs) autonomously learn both signaling and power control protocols. By modeling the problem as a partially observable Markov decision process (POMDP) and solving it using multi-agent proximal policy optimization (MAPPO), the framework reduces signaling overhead by a factor of 8 compared to an ideal Genie approach while achieving a success rate within 5% of optimal performance.

## Method Summary
The method employs MAPPO where each of the 10 AP agents learns to control power allocation and signaling decisions based only on local buffer observations. The framework formulates power control as a POMDP where APs observe only their queued packet count while a Central Controller (CC) holds full CSI. Agents use PPO with clipped objectives to learn cooperative policies, sharing a network-wide reward structure. The CC runs a GNN-based power allocation algorithm when requested. Training occurs over 1000 episodes of 300 steps each, with agents learning to request CSI/power allocation only when transmission failures occur, thereby implicitly reducing signaling overhead.

## Key Results
- MAPPO-based framework achieves 85% median success rate with 8× reduction in signaling overhead compared to CSI-based Genie baseline
- The approach maintains performance within 5% of optimal while eliminating the need for continuous CSI feedback
- Scalability testing shows the framework adapts to increased signaling demands without explicit overhead penalties in the reward function

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** POMDP formulation enables APs to learn effective policies using only local buffer observations
- **Mechanism:** APs observe only queued packet count while CC holds CSI, learning to act based on partial observations plus received messages
- **Core assumption:** Buffer state provides sufficient indirect signal about interference conditions
- **Break condition:** If buffer state cannot discriminate between high/low interference scenarios

### Mechanism 2
- **Claim:** Shared reward structure plus PPO's clipped objective enables stable cooperative learning
- **Mechanism:** Each AP receives averaged reward (1/M Σ rewards) with PPO clipping (ε = 0.2) preventing large policy destabilization
- **Core assumption:** Action spaces are sufficiently constrained for tractable non-stationarity
- **Break condition:** If subnetwork count increases substantially (>50), non-stationarity may overwhelm cooperative signal

### Mechanism 3
- **Claim:** Implicit signaling cost reduction emerges from agents learning to request CSI only when needed
- **Mechanism:** Agents explore communication actions and learn that unnecessary signaling wastes transmission opportunities
- **Core assumption:** Environment dynamics create sufficient pressure to avoid wasteful signaling
- **Break condition:** Different latency budgets or packet arrival rates may shift implicit cost balance

## Foundational Learning

- **Concept: POMDPs (Partially Observable Markov Decision Processes)**
  - Why needed here: Enables power control where APs cannot observe global channel state
  - Quick check question: Can you explain why a POMDP is preferred over a standard MDP when agents only observe local buffer state?

- **Concept: Proximal Policy Optimization (PPO)**
  - Why needed here: Provides training stability through clipped surrogate objective
  - Quick check question: What problem does the clipping parameter (ε = 0.2) solve in policy gradient methods?

- **Concept: Multi-Agent Credit Assignment**
  - Why needed here: Shared rewards vs. individual rewards affect cooperation in interference-limited networks
  - Quick check question: Why might shared averaged rewards promote cooperation better than individual rewards?

## Architecture Onboarding

- **Component map:** AP Agent → Actor Network → Critic Network → Action Space → Environment; Central Controller → GNN Power Allocation → APs

- **Critical path:** Initialize APs → Each timestep: AP observes buffer, selects action → CC receives requests, runs GNN allocation → APs transmit → Environment computes SINR/success → Reward computed → Experience stored → Policy updated after each episode

- **Design tradeoffs:**
  - Buffer capacity (100 packets) vs. episode length (300 steps)
  - Shared reward vs. individual rewards: Shared promotes cooperation but complicates credit assignment
  - CC as expert vs. learning CC: Current design reduces state space but limits adaptability
  - Assumption: Error-free control channels for signaling

- **Failure signatures:**
  - Success rate plateauing below 0.7 → insufficient exploration or reward scaling issues
  - Signaling overhead not decreasing over training → communication action rewards may need explicit penalty
  - High variance across episodes → increase PPO epochs or reduce learning rate
  - Agents converging to "never transmit" → check positive reward value is sufficient incentive

- **First 3 experiments:**
  1. Baseline replication: Implement MAPPO architecture to match reported ~0.85 median success rate and ~8× signaling reduction
  2. Ablation on reward structure: Test individual vs. shared rewards to confirm cooperation necessity
  3. Scalability probe: Increase subnetwork count from 10 to 20 while keeping area constant to test scaling assumptions

## Open Questions the Paper Calls Out
- How does the framework scale to deployments with significantly more subnetworks? (Simulations only consider M=10)
- How do multiple devices per subnetwork affect the learned protocols? (Current model assumes single device per subnetwork)
- Can the 5% performance gap to the Genie baseline be reduced or eliminated? (Source of gap not investigated)
- Is online learning feasible on embedded AP hardware? (Training utilized remote computing clusters)

## Limitations
- Simulation setup lacks validation against real factory channel measurements
- Critical GNN power allocation component from reference [3] is not specified
- Scalability test assumes CC remains non-learning with no analysis of performance degradation
- Implicit signaling cost reduction mechanism is not directly incentivized by the reward

## Confidence
- **High confidence** in Mechanism 1 (POMDP formulation works as described)
- **Medium confidence** in Mechanism 2 (shared rewards enable cooperation)
- **Medium confidence** in Mechanism 3 (emergent signaling reduction)

## Next Checks
1. **Scale test:** Increase M from 10 to 20 APs while keeping area constant; monitor for training instability or success rate collapse
2. **Reward ablation:** Implement explicit signaling penalty and compare learned policies and overhead
3. **Channel sensitivity:** Replace path loss model with measured factory channel data to assess robustness of learned protocols