---
ver: rpa2
title: 'Learning with Expected Signatures: Theory and Applications'
arxiv_id: '2505.20465'
source_url: https://arxiv.org/abs/2505.20465
tags:
- signature
- expected
- process
- estimator
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes asymptotic theory for estimating the expected
  signature of continuous-time stochastic processes observed at discrete points. The
  authors prove consistency and asymptotic normality results for expected signature
  estimators under general conditions, bridging the gap between discrete-time empirical
  estimators and their continuous-time theoretical values.
---

# Learning with Expected Signatures: Theory and Applications

## Quick Facts
- **arXiv ID**: 2505.20465
- **Source URL**: https://arxiv.org/abs/2505.20465
- **Reference count**: 40
- **Primary result**: Establishes asymptotic theory for expected signature estimation with martingale correction for variance reduction

## Executive Summary
This paper develops theoretical foundations and practical improvements for estimating expected signatures of stochastic processes observed at discrete points. The authors prove consistency and asymptotic normality results for expected signature estimators under general conditions, bridging the gap between discrete-time empirical estimators and their continuous-time theoretical values. A key practical contribution is introducing a variance reduction technique using martingale correction, which consistently improves predictive performance across multiple machine learning applications including time series classification, derivative pricing, and systematic trading.

## Method Summary
The method involves computing truncated signatures of discretely-observed stochastic processes and applying a control variate correction for variance reduction. The correction replaces the outer Stratonovich integral with an Itô integral in the signature estimator, treating the underlying process as a martingale regardless of its true nature. The optimal correction coefficient is estimated via regression on the training data. Implementation requires the esig package and involves signature computation with truncation level K, followed by computation of the correction term and optimal coefficient estimation.

## Key Results
- Proved consistency and asymptotic normality for expected signature estimators under general conditions
- Developed martingale correction technique achieving 20-30 percentage point improvements in classification accuracy for some synthetic data experiments
- Validated theoretical results across five ML applications: time series classification, derivative pricing, distributional regression, and systematic trading
- Showed variance reduction technique works even for non-martingale processes when treated as data transformation

## Why This Works (Mechanism)
The expected signature characterizes the law of a continuous-time process through its iterated integrals. For bounded variation processes, the expected signature can be characterized by its expectation. The martingale correction works by replacing the Stratonovich integral (which has higher variance) with an Itô integral (lower variance) while maintaining unbiasedness. This creates a control variate that reduces overall estimator variance without introducing bias, even when the underlying process isn't truly a martingale.

## Foundational Learning

**Rough Paths Theory**: Mathematical framework for handling highly oscillatory stochastic processes beyond semimartingales.
*Why needed*: Provides the theoretical foundation for analyzing processes with unbounded variation like Brownian motion.
*Quick check*: Verify understanding of how rough paths extend classical stochastic calculus to handle rough signals.

**Stratonovich vs Itô Integrals**: Two different ways to define stochastic integration with different variance properties.
*Why needed*: The correction technique relies on replacing Stratonovich integrals with Itô-style sums to reduce variance.
*Quick check*: Confirm understanding of when Stratonovich vs Itô integrals are unbiased for martingale expectations.

**Control Variates**: Variance reduction technique using correlated random variables.
*Why needed*: The martingale correction is essentially a sophisticated control variate application.
*Quick check*: Verify that the correction coefficient is computed to minimize estimator variance.

## Architecture Onboarding

**Component Map**: Discrete observations -> Signature computation (K-level truncation) -> Control variate calculation -> Optimal coefficient estimation -> Final estimator

**Critical Path**: The most computationally intensive step is signature computation, which grows exponentially with truncation level K. The martingale correction adds minimal overhead since it requires only one additional signature computation per path.

**Design Tradeoffs**: Higher truncation levels K capture more process information but suffer from exponential computational complexity and overfitting risk. The martingale correction improves finite-sample performance but may introduce bias for processes with large drift components.

**Failure Signatures**: Exploding gradients during GPES training indicate tensor normalization issues. Poor performance of martingale correction for highly non-martingale processes suggests the drift component is too large relative to the martingale part.

**First Experiments**:
1. Implement signature computation with truncation levels K=3,4,5 on Brownian motion paths to verify exponential growth in computation time
2. Compare MSE of basic vs corrected estimators on Ornstein-Uhlenbeck processes with varying mean-reversion speeds
3. Test classification performance on synthetic data with and without martingale correction across different truncation levels

## Open Questions the Paper Calls Out
None

## Limitations
- Extension to rough paths relies on technical conditions that may be difficult to verify in practice
- Martingale correction effectiveness for non-martingales with significant drift remains theoretically justified only as a heuristic
- Computational complexity grows exponentially with truncation level K, limiting practical applicability
- Optimal correction coefficient depends on entire observation grid, challenging for real-time applications

## Confidence

**High Confidence**: Consistency and asymptotic normality for bounded variation processes; variance reduction for martingales

**Medium Confidence**: Extension to rough paths under technical assumptions; empirical performance for non-martingales using martingale correction

**Low Confidence**: Practical guidelines for selecting truncation levels K and tensor normalization parameters C

## Next Checks

1. **Asymptotic Behavior Verification**: Implement experiment varying observation frequency to empirically verify convergence rates for both bounded variation and rough path cases

2. **Bias-Variance Tradeoff Analysis**: Systematically compare MSE, bias, and variance of signature estimators with and without martingale correction across different process types

3. **Hyperparameter Sensitivity Study**: Conduct comprehensive sensitivity analysis of GPES and SES performance to truncation level K, tensor normalization C, and cross-validation procedures