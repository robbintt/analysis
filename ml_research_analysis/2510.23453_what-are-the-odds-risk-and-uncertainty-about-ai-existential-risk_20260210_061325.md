---
ver: rpa2
title: What are the odds? Risk and uncertainty about AI existential risk
arxiv_id: '2510.23453'
source_url: https://arxiv.org/abs/2510.23453
tags:
- risk
- uncertainty
- will
- probability
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper critiques the Swiss Cheese risk model used to estimate
  AI existential risk by highlighting its limitations in capturing complex interdependencies
  and uncertainties. It argues that the model underestimates the probability of AI
  doom (P(D)) by not accounting for structural relationships between protective layers,
  such as how Alignment and Oversight methods are interdependent.
---

# What are the odds? Risk and uncertainty about AI existential risk

## Quick Facts
- **arXiv ID**: 2510.23453
- **Source URL**: https://arxiv.org/abs/2510.23453
- **Reference count**: 4
- **Primary result**: Swiss-Cheese models of AI existential risk systematically underestimate P(D) by ignoring layer dependencies and uncertainties

## Executive Summary
This paper critiques Swiss-Cheese risk models for estimating AI existential risk by highlighting how they systematically underestimate the probability of doom (P(D)) through two key oversights: ignoring structural dependencies between protective layers and failing to account for irreducible uncertainties. By analyzing how layers like Alignment and Oversight are not truly independent—since alignment methods can serve as oversight—the paper demonstrates that conditional failure probabilities increase P(D) from 6.25% to 9.375% under epistemic indifference. The paper further identifies two types of uncertainty (Option Uncertainty and State-Space Uncertainty) that cannot be captured in standard probability models, arguing that any probability estimate for P(D) must be conditionalized on catch-all hypotheses representing unknown survival strategies.

## Method Summary
The paper employs analytical probability calculations using conditional probability chain rules to critique linear Swiss-Cheese models of AI existential risk. Starting with the standard formula P(D) = P(¬T) × P(¬C|¬T) × P(¬A|¬T&¬C) × P(¬O|¬T&¬C&¬A), the author demonstrates how layer dependencies systematically increase P(D). The analysis refines probability decompositions for Oversight by splitting it into AI-dependent (O₁), non-AI (O₂), and Cultural Plateau-dependent (O₃) components. The paper also introduces conceptual analysis of Knightian uncertainty versus quantifiable risk, distinguishing between Option Uncertainty (where safety attempts can backfire) and State-Space Uncertainty (unknown survival stories). These refinements show that standard models produce false precision by ignoring both structural relationships and irreducible uncertainties.

## Key Results
- Linear Swiss-Cheese models underestimate P(D) from 6.25% to 9.375% by ignoring dependencies between Alignment and Oversight layers
- State-Space Uncertainty from unknown survival stories means any P(D) estimate must be conditionalized on catch-all hypotheses
- Reflexivity effects can cause safety interventions to paradoxically increase overall P(D) by undermining future protective capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structural dependencies between protective layers cause linear Swiss-Cheese models to systematically underestimate P(D).
- Mechanism: When layers share dependencies (e.g., Alignment methods enable AI-based Oversight), conditional failure probabilities increase. If O₁ (AI-dependent oversight) requires Alignment A, then P(O₁|¬A) = 0. Under epistemic indifference with refined probability fields, this raises P(D) from 6.25% to 9.375%—a 33% increase.
- Core assumption: Layer dependencies are correctly identified and can be formalized as conditional probabilities.
- Evidence anchors:
  - [abstract]: "analyzing how layers like Alignment and Oversight are not truly independent—since alignment methods can serve as oversight"
  - [section 1.2]: "P(O/¬A) is now only 25%, since P(O₁/¬A) = 0. Thus P(¬O/¬A) = 75%... roughly a 33% increase in the chances of Doom"
  - [corpus]: Weak corpus support; neighboring papers discuss existential risk frameworks but not structural dependency modeling specifically.
- Break condition: If layers are genuinely independent (no shared methods/resources), the correction collapses to the linear model.

### Mechanism 2
- Claim: Reflexivity effects cause safety interventions to influence the risk landscape itself, potentially increasing overall P(D).
- Mechanism: Attempting Cultural Plateau (banning AI research) reduces institutional knowledge and oversight capabilities. If a powerful AI emerges anyway via recursive self-improvement, outdated oversight methods lower P(Oversight), while the AI may infer human hostility and dis-align.
- Core assumption: Powerful AI systems will form goals based on historical data about human intentions toward them.
- Evidence anchors:
  - [abstract]: "reflexivity effects where safety attempts can increase risks"
  - [section 2.2]: "us trying to reach Cultural Plateau impairs our ability to oversee AI later, in case Cultural Plateau fails"
  - [corpus]: "Why do Experts Disagree on Existential Risk" (arXiv:2502.14870) notes expert disagreement on P(doom), consistent with reflexivity complicating forecasts.
- Break condition: If safety interventions preserve research capacity (e.g., targeted bans with safety-focused exceptions), reflexivity harms are minimized.

### Mechanism 3
- Claim: State-Space Uncertainty (SU) makes any P(D) estimate conditional on an uncalibratable catch-all hypothesis.
- Mechanism: Unknown survival stories (CH) cannot be meaningfully quantified. Assigning CH = 50% halves P(D), but if CH contains four equally probable sub-stories, P(D) drops to 0.39%. The variance is so large that no sensible probability range exists—only conditionalized estimates.
- Core assumption: The Principle of Indifference applies meaningfully to catch-all hypotheses.
- Evidence anchors:
  - [abstract]: "any probability estimate for P(D) must be conditionalized on catch-all hypotheses"
  - [section 2.1]: "the variation in the possible values of CH is so large that there is no sensible basis for giving it neither a specific number nor range"
  - [corpus]: "AI Survival Stories" (arXiv:2601.09765) proposes the original taxonomy this paper critiques.
- Break condition: If the taxonomy of survival stories is genuinely exhaustive (no unknown unknowns), CH = 0 and this correction vanishes.

## Foundational Learning

- Concept: Conditional probability chains (P(A|B))
  - Why needed here: The core formula P(D) = P(¬T) × P(¬C|¬T) × P(¬A|¬T&¬C) × P(¬O|¬T&¬C&¬A) requires understanding how to combine conditional probabilities.
  - Quick check question: If P(A) = 0.5 and P(B|¬A) = 0.75, what is P(¬A & ¬B)?

- Concept: Knightian uncertainty vs. quantifiable risk
  - Why needed here: The paper distinguishes between odds we can estimate (risk) and odds that are undefined (uncertainty), which determines whether probability assignments are even meaningful.
  - Quick check question: Can you assign a meaningful probability to "unknown unknown survival stories"?

- Concept: Swiss-Cheese model limitations
  - Why needed here: The paper critiques linear Swiss-Cheese models for assuming independent layers; understanding the original model reveals why dependencies matter.
  - Quick check question: In Reason's Swiss-Cheese model, what must happen for an accident to occur?

## Architecture Onboarding

- Component map:
  - Layer T (Technical Plateau): Scientific barriers to AI power
  - Layer C (Cultural Plateau): Human-imposed research bans
  - Layer A (Alignment): AI goals compatible with human survival
  - Layer O (Oversight): Detection/disablement of hostile AI (subdivides into O₁: AI-dependent, O₂: non-AI, O₃: neither)
  - CH (Catch-all Hypothesis): Unknown survival stories

- Critical path: ¬T → ¬C → ¬A → ¬O → Doom. Each transition requires estimating conditional failure probabilities, which depend on structural relationships between layers.

- Design tradeoffs:
  - Simplicity vs. accuracy: Linear models are tractable but systematically underestimate P(D)
  - Definitional compatibility vs. realism: Authors make layers mutually exclusive for simplicity; this paper argues real dependencies exist
  - Quantification vs. honesty: Assigning numbers to CH provides false precision

- Failure signatures:
  - Model reports P(D) ≈ 6.25% under epistemic indifference → likely ignored layer dependencies
  - No catch-all hypothesis → likely ignored state-space uncertainty
  - No reflexivity analysis → likely ignored option uncertainty

- First 3 experiments:
  1. Map dependencies: For your risk model, explicitly list which protective layers share methods, resources, or prerequisites. Recalculate P(D) with conditional adjustments.
  2. Stress-test reflexivity: For each safety intervention, ask: "If this fails, does attempting it make subsequent layers weaker?"
  3. Sensitivity analysis on CH: Vary the catch-all hypothesis from 0% to 50% and observe how P(D) changes. Report the range, not a point estimate.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the catch-all hypothesis (CH) for unknown survival stories be calibrated given that it consists of "unknown unknowns"?
- Basis in paper: [explicit] The author identifies the "No-calibration problem": "Since by definition, CH is made of unknown unknowns, we have no idea how many other survival stories are out there, so there is no way of calibrating CH."
- Why unresolved: Historical examples show technological forecasting is often impossible (steam engine, laser), making it unclear how to assign probabilities to unknown possibilities.
- What evidence would resolve it: Development of principled methods for bounding or characterizing unknown survival scenarios, or empirical studies on how well experts historically identified all relevant possibilities in emerging technology risks.

### Open Question 2
- Question: How do reflexivity effects—where risk assessments influence the risk itself—quantitatively impact P(D) in Swiss Cheese models?
- Basis in paper: [explicit] The author argues that "us attempting Cultural Plateau will affect our chances of Oversight and our chances of Alignment" through feedback loops, citing scenarios where safety attempts backfire (nuclear energy example, AI warning shots leading to reduced oversight capacity).
- Why unresolved: Current models treat each layer's probability as fixed, but reflexivity introduces dynamic dependencies not captured in linear sequential models.
- What evidence would resolve it: Historical case studies of safety interventions that paradoxically increased systemic risk, or formal models incorporating reflexive dynamics.

### Open Question 3
- Question: What is the optimal safety strategy when structural dependencies between layers imply that some "independent" protective layers may not exist?
- Basis in paper: [explicit] The conclusion states: "This will be especially relevant for deciding which is the optimal strategy for survival." The paper shows layer interdependencies mean "there might only be two" layers when four are assumed.
- Why unresolved: If attempting one protective strategy undermines others, standard decision-theoretic approaches to choosing among safety investments may be inadequate.
- What evidence would resolve it: Formal decision analysis comparing portfolio strategies under dependency structures, or expert elicitation on perceived dependencies between AI safety approaches.

## Limitations

- Probability adjustments (6.25% → 9.375% → 10.416%) rely on illustrative examples rather than empirically validated structural relationships between AI safety mechanisms
- The Principle of Indifference assumption (50% failure per layer) is acknowledged as controversial but not justified beyond epistemic humility
- State-Space Uncertainty quantification remains fundamentally problematic—any probability assignment to unknown survival stories involves arbitrary assumptions about their number and distribution

## Confidence

- **High Confidence**: The structural argument that Swiss-Cheese models underestimate P(D) when layers share dependencies is logically sound and mathematically demonstrable.
- **Medium Confidence**: The specific numerical adjustments (33% increase from 6.25% to 9.375%) are valid given the stated assumptions, but the assumptions themselves are debatable.
- **Low Confidence**: Any specific probability estimates for P(D) remain highly uncertain due to unquantifiable uncertainties in both Option Uncertainty (reflexivity effects) and State-Space Uncertainty (unknown survival stories).

## Next Checks

1. **Dependency Mapping**: Identify actual shared methods, resources, or prerequisites between AI safety layers in current technical literature and calculate conditional failure probabilities based on empirical evidence rather than illustrative examples.

2. **Reflexivity Analysis**: For each proposed AI safety intervention, conduct a formal analysis of how attempting that intervention might weaken subsequent protective layers if it fails, using game-theoretic or decision-theoretic frameworks.

3. **Uncertainty Bounds**: Instead of point estimates, calculate probability ranges for P(D) by systematically varying the catch-all hypothesis (CH) from 0% to 50% and reporting the full range of outcomes, making uncertainty explicit rather than hidden in point estimates.