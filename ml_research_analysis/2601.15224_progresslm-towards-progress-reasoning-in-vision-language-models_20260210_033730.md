---
ver: rpa2
title: 'PROGRESSLM: Towards Progress Reasoning in Vision-Language Models'
arxiv_id: '2601.15224'
source_url: https://arxiv.org/abs/2601.15224
tags:
- progress
- reasoning
- task
- score
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PROGRESSLM addresses the challenge of estimating task progress
  from single observations in vision-language models (VLMs). The paper introduces
  PROGRESS-BENCH, a benchmark with over 3,000 instances evaluating progress reasoning
  across controlled variations in demonstration modality, viewpoint, and answerability.
---

# PROGRESSLM: Towards Progress Reasoning in Vision-Language Models

## Quick Facts
- **arXiv ID**: 2601.15224
- **Source URL**: https://arxiv.org/abs/2601.15224
- **Reference count**: 40
- **Primary result**: Most VLMs struggle with progress estimation; PROGRESSLM achieves consistent improvements through two-stage retrieval-simulation approach

## Executive Summary
PROGRESSLM addresses the challenge of estimating task progress from single observations in vision-language models (VLMs). The paper introduces PROGRESS-BENCH, a comprehensive benchmark with over 3,000 instances evaluating progress reasoning across controlled variations in demonstration modality, viewpoint, and answerability. Experiments reveal that most VLMs struggle significantly with progress estimation, showing high sensitivity to demonstration modality and viewpoint changes, poor handling of unanswerable cases, and often producing collapsed or heuristic predictions. To address these limitations, the authors propose a human-inspired two-stage approach combining episodic retrieval and mental simulation, implemented through both training-free prompting and a training-based method using PROGRESSLM-45K dataset. The resulting PROGRESSLM-3B model achieves consistent improvements over base models, even at small scale, and demonstrates robust performance across all benchmark dimensions.

## Method Summary
The paper proposes a two-stage reasoning framework for progress estimation: episodic retrieval to locate the current observation within task trajectories, followed by mental simulation to estimate fine-grained progress. The method is implemented in two variants: a training-free prompting approach using carefully designed few-shot examples, and a training-based approach that fine-tunes Qwen2.5-VL-3B on PROGRESSLM-45K dataset (25K for SFT, 20K for RL). The training procedure uses LLaMA-Factory with LoRA rank=8, followed by GRPO-based reinforcement learning with multi-component rewards. The benchmark PROGRESS-BENCH evaluates models across vision-based (same-view/cross-view) and text-based demonstrations, including unanswerable cases, using metrics like Normalized Score Error (NSE↓), Progress Rank Correlation (PRC↑), Answerable False Rejection Rate (AFRR↓), and Unanswerable Detection Accuracy (UDA↑).

## Key Results
- Most VLMs struggle with progress estimation, showing high sensitivity to demonstration modality and viewpoint changes
- PROGRESSLM-3B consistently outperforms base models across all benchmark dimensions, even at small scale
- The two-stage retrieval-simulation approach demonstrates superior performance compared to direct regression methods
- PROGRESSLM shows robust generalization across different robot embodiments (Franka, UR5e, AgileX Dual-Arm, TienKung Humanoid)

## Why This Works (Mechanism)
The paper demonstrates that effective progress reasoning requires explicit coupling between retrieval and estimation rather than treating it as direct regression. The two-stage approach mimics human reasoning by first locating the observation within task context (retrieval), then simulating the remaining steps to estimate progress (mental simulation). This decomposition allows the model to handle unanswerable cases by failing retrieval rather than forcing incorrect predictions, and provides interpretable reasoning traces that improve robustness to viewpoint changes and demonstration modalities.

## Foundational Learning
**Episodic Retrieval**: Locating current observation within task trajectory
- Why needed: Provides contextual grounding for progress estimation
- Quick check: Verify reference step prediction accuracy on PROGRESS-BENCH

**Mental Simulation**: Estimating remaining steps from retrieved context
- Why needed: Enables fine-grained progress estimation beyond discrete steps
- Quick check: Compare progress estimates with and without simulation component

**Progress Reasoning**: Estimating task completion percentage from single observation
- Why needed: Core task requiring temporal understanding and state tracking
- Quick check: Evaluate NSE across different task types and difficulty levels

**Unanswerable Detection**: Recognizing impossible progress estimation scenarios
- Why needed: Critical for selective uncertainty recognition
- Quick check: Measure UDA vs AFRR tradeoff across different confidence thresholds

**Cross-Modal Reasoning**: Handling both vision-based and text-based demonstrations
- Why needed: Evaluates model's ability to reason across different input modalities
- Quick check: Compare performance gap between vision-based and text-based inputs

## Architecture Onboarding

**Component Map**: Qwen2.5-VL-3B (base) -> SFT on PROGRESSLM-45K -> GRPO RL fine-tuning -> PROGRESSLM-3B

**Critical Path**: Retrieval (find reference step) → Simulation (estimate progress from reference) → Output (formatted score)

**Design Tradeoffs**: 
- Small model (3B) vs performance: PROGRESSLM-3B outperforms larger models through specialized training
- Training-free vs training-based: Training-based shows consistent gains but requires dataset construction
- Text-based vs vision-based: Text-based requires implicit state accumulation, harder but more generalizable

**Failure Signatures**: 
- Score collapse to 0%, 50%, 100% indicates heuristic rather than genuine estimation
- High AFRR suggests over-conservative uncertainty recognition
- Poor cross-view performance indicates reliance on pixel similarity over semantic reasoning

**First Experiments**:
1. Baseline progress estimation on PROGRESS-BENCH without retrieval stage
2. Ablation study: retrieval-only vs simulation-only vs combined approach
3. Cross-view generalization test with systematic viewpoint rotations

## Open Questions the Paper Calls Out
**Open Question 1**: Can VLMs effectively bridge the performance gap between vision-based and text-based progress reasoning by mastering implicit state accumulation? While PROGRESSLM improves text-based performance, the paper notes that the modality gap remains large because models fail to maintain and update an implicit world state over time from action semantics alone.

**Open Question 2**: To what extent can progress reasoning capabilities learned from robotic manipulation data generalize to unconstrained, "in-the-wild" human activities? The paper establishes that domain shift exists (higher NSE on human activities) but does not investigate methods to bridge this gap, such as cross-embodiment pre-training or domain adaptation techniques.

**Open Question 3**: How can models be trained to distinguish truly unanswerable cases from difficult answerable ones without increasing the Answerable False Rejection Rate (AFRR)? The paper observes that models like InternVL-3.5-38B achieve high unanswerable detection but at the cost of rejecting many valid answerable samples, leaving the calibration problem open.

## Limitations
- Dataset construction process for unanswerable cases lacks full specification, particularly regarding image editing parameters and human filtering criteria
- Reliance on ground-truth reasoning traces generated by Qwen2.5-VL-72B may introduce bias toward that model's reasoning patterns
- Limited exploration of cross-embodiment generalization, with evaluation focusing primarily on Franka and UR5e robots

## Confidence
**High Confidence**: Core empirical finding that VLMs struggle with progress estimation is well-supported by comprehensive benchmark results across 14 models
**Medium Confidence**: Two-stage retrieval-simulation approach shows improvements, but implementation details are only partially specified
**Low Confidence**: Claim that explicit coupling between retrieval and estimation is necessary lacks rigorous quantitative comparison against direct regression baselines

## Next Checks
1. Generate histograms of predicted progress scores across all benchmark categories to verify absence of collapsed predictions at 0%, 50%, 100%, or other discrete values
2. Systematically vary viewpoint differences (e.g., 30°, 60°, 90° rotations) in controlled experiments to quantify exact relationship between viewpoint change magnitude and performance degradation
3. Implement exact image editing procedure and human filtering criteria to generate new unanswerable samples, then compare PROGRESSLM's performance on independently generated cases versus original benchmark