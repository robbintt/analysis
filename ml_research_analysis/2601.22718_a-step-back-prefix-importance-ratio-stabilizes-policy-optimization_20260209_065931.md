---
ver: rpa2
title: 'A Step Back: Prefix Importance Ratio Stabilizes Policy Optimization'
arxiv_id: '2601.22718'
source_url: https://arxiv.org/abs/2601.22718
tags:
- ratio
- off-policy
- training
- policy
- prefix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the instability of reinforcement learning
  post-training for large language models under large off-policy drift. It shows that
  the commonly used token-level importance sampling ratio is an inaccurate approximation
  of the theoretically correct prefix importance ratio, leading to training instability.
---

# A Step Back: Prefix Importance Ratio Stabilizes Policy Optimization

## Quick Facts
- **arXiv ID**: 2601.22718
- **Source URL**: https://arxiv.org/abs/2601.22718
- **Reference count**: 40
- **Primary result**: MinPRO substantially improves training stability and peak performance of RL post-training for LLMs under large off-policy drift compared to GRPO, GSPO, CISPO, and M2PO.

## Executive Summary
This paper addresses the instability of reinforcement learning post-training for large language models under large off-policy drift. It shows that the commonly used token-level importance sampling ratio is an inaccurate approximation of the theoretically correct prefix importance ratio, leading to training instability. The authors propose MinPRO, a simple yet effective objective that replaces the unstable cumulative prefix ratio with a non-cumulative surrogate based on the minimum token-level ratio observed in the preceding prefix. Extensive experiments on both dense and mixture-of-experts LLMs across multiple mathematical reasoning benchmarks demonstrate that MinPRO substantially improves training stability and peak performance compared to strong baselines like GRPO, GSPO, CISPO, and M2PO, especially under large off-policy regimes.

## Method Summary
MinPRO addresses RL instability in LLMs by correcting the importance sampling ratio from token-level to prefix-aware. It computes the running minimum of token-level importance ratios within each prefix and uses this as a non-cumulative surrogate for the unstable cumulative prefix ratio. The method applies soft clipping to the product of the current token ratio and the prefix minimum, effectively suppressing gradients from unlikely trajectories. MinPRO is implemented within the VeRL framework, uses group reward normalization instead of a critic network, and supports both dense and mixture-of-experts models.

## Key Results
- MinPRO significantly improves stability and performance over GRPO, GSPO, CISPO, and M2PO on mathematical reasoning benchmarks
- Training collapse observed in CISPO is prevented by MinPRO's prefix-aware gradient suppression
- Performance gains are particularly pronounced under large off-policy drift conditions
- MinPRO achieves state-of-the-art results on AMC23, AIME24, AIME25, MATH500, Olympiad, Minerva, and GSM8K benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Prefix-Aware Importance Correction
The theoretically correct correction term for the policy gradient is the prefix importance ratio, not the single-step token ratio. In long autoregressive sequences, ignoring the prefix probability mismatch allows gradients from highly unlikely trajectories (under the current policy) to update the model, leading to oscillation. MinPRO addresses this by incorporating prefix-level correction through the minimum token ratio.

### Mechanism 2: Minimum Ratio as a Variance-Stabilizing Surrogate
Directly using the cumulative prefix ratio is numerically unstable due to multiplicative explosion/underflow and length bias. MinPRO stabilizes this by using the minimum token-level ratio observed in the preceding prefix as a "bottleneck" filter. This preserves the signal that the trajectory is unlikely without the high variance of a cumulative product.

### Mechanism 3: Suppression of Destructive Soft-Clipping Gradients
In large off-policy regimes, extreme token ratios appear frequently. MinPRO multiplies the token ratio by the prefix minimum, naturally yielding smaller values that more aggressively down-weight tokens with large token ratios. This prevents the collapse observed in soft-clipping baselines when off-policy drift is high.

## Foundational Learning

- **Concept: Importance Sampling (IS) in RL**
  - **Why needed here**: The entire paper revolves around correcting the "off-policy" distribution shift using importance sampling ratios
  - **Quick check**: If ρ_t = 0.5 for a token, does that mean the current policy is more likely or less likely to generate that token than the old policy? (Answer: Less likely)

- **Concept: The "Prefix" in Autoregressive Generation**
  - **Why needed here**: In LLMs, token probability depends on all previous tokens, making prefix-level correction necessary
  - **Quick check**: Why does the cumulative product grow exponentially unstable as sequence length T increases? (Answer: Due to multiplicative accumulation of small/large values)

- **Concept: Hard vs. Soft Clipping (PPO vs. CISPO)**
  - **Why needed here**: MinPRO is presented as a fix for failure modes of both hard and soft clipping methods
  - **Quick check**: In hard clipping, what happens to the gradient for a token where ρ > 1+ε? (Answer: It is zeroed/masked)

## Architecture Onboarding

- **Component map**: Rollout Buffer -> Ratio Calculator -> Prefix Tracker (MinPRO specific) -> MinPRO Objective
- **Critical path**: The calculation of the running minimum ρ̄_t during the forward pass of the training step
- **Design tradeoffs**: Numerical stability vs. theoretical exactness (rejects perfect cumulative product for minimum heuristic), simplicity (no critic network required)
- **Failure signatures**: CISPO-like collapse (entropy spike, reward crash), GRPO-like stagnation (clipped token fraction approaching 1.0)
- **First 3 experiments**:
  1. Staleness ablation: Run MinPRO vs. GRPO with buffer delays n ∈ {0, 1, 2, 4} and plot reward curves
  2. Metric tracking: Log "Clipped Token Fraction" and ρ̄_t values to confirm correlation with stabilized gradients
  3. Ablation on "Min": Replace min operator with mean or prod on a small model to validate the minimum operator's specific stabilizing effect

## Open Questions the Paper Calls Out

### Open Question 1
Does MinPRO generalize to non-mathematical reasoning domains like code generation or open-ended conversation where optimal policy distributions might differ significantly? The paper limits validation to mathematical reasoning benchmarks without verifying performance on broader language tasks.

### Open Question 2
Is the "minimum prefix ratio" the optimal statistic for approximating the prefix importance ratio, or would other functions (mean, median, geometric mean) provide better stability-accuracy trade-offs? The paper does not prove the minimum is optimal or ablate other aggregate statistics.

### Open Question 3
Can MinPRO's prefix-correction mechanism be effectively integrated with hard-clipping methods like GRPO/PPO, or is its success fundamentally dependent on the soft-clipping strategy from CISPO? The paper does not test MinPRO with hard-clipping objectives.

## Limitations
- Theoretical foundation gap: The claim that token-level approximation is theoretically invalid lacks rigorous proof in the autoregressive setting
- Empirical scope: All experiments use mathematical reasoning tasks with single-turn generation, not validating on multi-turn conversational or code generation domains
- Implementation complexity: The running minimum computation for very long sequences (20k tokens) may introduce computational overhead not fully addressed

## Confidence
- **High Confidence**: Empirical demonstration of stability improvements over baselines in mathematical reasoning benchmarks
- **Medium Confidence**: Mechanism that MinPRO stabilizes training by suppressing gradients from unlikely prefixes
- **Low Confidence**: Generalization to domains beyond mathematical reasoning without additional validation

## Next Checks
1. Apply MinPRO to a multi-turn conversational dataset to verify stability and performance in settings with different prefix dynamics
2. Conduct ablation study replacing min operation with mean, max, and median aggregations to validate the minimum operator's specific importance
3. Derive and prove bounds on the variance of MinPRO estimator compared to theoretical prefix importance ratio estimator and token-level approximations