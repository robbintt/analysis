---
ver: rpa2
title: 'MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid Vision
  Tokenizer'
arxiv_id: '2509.16197'
source_url: https://arxiv.org/abs/2509.16197
tags:
- image
- arxiv
- understanding
- generation
- unified
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Manzano introduces a unified multimodal framework that couples\
  \ a hybrid image tokenizer with a unified autoregressive large language model to\
  \ reduce the performance trade-off between understanding and generation. It employs\
  \ a shared vision encoder feeding two lightweight adapters\u2014one producing continuous\
  \ embeddings for understanding and one producing discrete tokens for generation\u2014\
  within a common semantic space, followed by a diffusion decoder to render images\
  \ from generated tokens."
---

# MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer

## Quick Facts
- **arXiv ID:** 2509.16197
- **Source URL:** https://arxiv.org/abs/2509.16197
- **Reference count:** 40
- **Primary result:** Unified multimodal model achieving state-of-the-art performance among unified models, outperforming specialist models on text-rich benchmarks while maintaining competitive generation capabilities

## Executive Summary
MANZANO introduces a unified multimodal framework that couples a hybrid image tokenizer with a unified autoregressive large language model to reduce the performance trade-off between understanding and generation. It employs a shared vision encoder feeding two lightweight adapters—one producing continuous embeddings for understanding and one producing discrete tokens for generation—within a common semantic space, followed by a diffusion decoder to render images from generated tokens. The model is trained on a mixture of text, image understanding, and image generation data across multiple stages, using a unified autoregressive objective without additional task-specific heads. Evaluations show that Manzano achieves state-of-the-art performance among unified models, outperforming specialist models on text-rich benchmarks and delivering competitive results on generation tasks. Scaling studies confirm consistent improvements in both understanding and generation when increasing the LLM decoder and image decoder sizes, with minimal task interference observed across benchmarks.

## Method Summary
MANZANO consists of three key components: (1) A hybrid tokenizer using a shared ViT backbone with two adapters—continuous (for understanding) and discrete (for generation)—producing embeddings in a common semantic space, (2) A unified autoregressive LLM that accepts text and continuous embeddings while predicting text and discrete image tokens, and (3) A DiT-Air diffusion decoder that renders high-fidelity images from discrete tokens. The training follows a three-stage pipeline: tokenizer pre-alignment with random adapter sampling, unified LLM training on mixed data (40% understanding, 40% generation, 20% text), and progressive resolution training of the image decoder. The model uses FSQ quantization for discrete tokens and employs a 1:0.5 text:image loss weighting ratio.

## Key Results
- Achieves state-of-the-art performance among unified models, outperforming specialist models on text-rich benchmarks like ChartQA and DocVQA
- Demonstrates consistent scaling improvements in both understanding and generation when increasing LLM decoder and image decoder sizes
- Shows minimal task interference across benchmarks, maintaining strong performance across diverse modalities

## Why This Works (Mechanism)

### Mechanism 1: Homogenization of Visual Representations
- **Claim:** Utilizing a single shared vision encoder to produce both continuous and discrete tokens reduces task conflict compared to dual-encoder architectures.
- **Mechanism:** By forcing the understanding (continuous) and generation (discrete) pathways to originate from the same ViT backbone, the model projects both modalities into a "common semantic space." This prevents the LLM from receiving heterogeneous inputs (e.g., semantic features vs. pixel features) that require conflicting optimization directions.
- **Core assumption:** The shared encoder has sufficient capacity to retain both high-level semantics (required for understanding) and spatial details (required for generation) without degradation.
- **Evidence anchors:**
  - [Abstract]: "...common semantic space... enables scalable joint learning of both capabilities."
  - [Section 3.1]: "Shared unified semantic space. Both branches originate from the same encoder backbone; thus, continuous and discrete tokens inhabit a common semantic space, which reduces potential task conflict."
  - [Corpus]: Paper "Vision as a Dialect" supports the need for unified representations via text-aligned tokenizers.
- **Break condition:** If the shared encoder over-commits to one modality during pre-training, the other adapter may fail to extract useful features (e.g., generation details are lost if the encoder becomes too abstract).

### Mechanism 2: Decoupled Semantic Prediction and Pixel Rendering
- **Claim:** Separating high-level semantic prediction (autoregressive LLM) from pixel-level rendering (diffusion decoder) allows for more efficient scaling and specialization.
- **Mechanism:** The LLM is relieved of the burden of modeling low-level pixel noise, focusing instead on predicting semantic "image tokens." The diffusion decoder then acts as a specialized renderer conditioned on these tokens. This decoupling allows the system to scale the reasoning component (LLM) independently of the texture generation component.
- **Core assumption:** The discrete image tokens serve as a sufficient bottleneck to transfer semantic layout and content to the diffusion decoder without losing necessary structural details.
- **Evidence anchors:**
  - [Abstract]: "...unified autoregressive LLM predicts high-level semantics... with an auxiliary diffusion decoder subsequently translating..."
  - [Section 3.1]: "The LLM decoder focuses on regressing high-level semantics... while the diffusion decoder is responsible for rendering high-fidelity details..."
- **Break condition:** If the discrete token vocabulary (64K codebook) is too small or the compression ratio is too aggressive, the diffusion decoder receives insufficient conditioning, resulting in hallucinated or blurred structural details.

### Mechanism 3: Random Adapter Sampling for Alignment
- **Claim:** Randomly selecting between the continuous and discrete adapters during tokenizer pre-training creates a unified feature space compatible with the LLM.
- **Mechanism:** During the tokenizer training phase, the model randomly samples either the continuous or discrete adapter for a given image. This stochastic process forces the shared vision encoder to learn weights that are robust and transferable between the two distinct output formats (regression vs. quantization), ensuring the LLM can process both seamlessly later.
- **Core assumption:** The small 300M LLM decoder used during tokenizer training is sufficiently representative of the larger target LLMs (up to 30B) to establish this alignment.
- **Evidence anchors:**
  - [Section 4.2.1]: "For each training sample, we randomly select one adapter and feed the corresponding embeddings to the LLM decoder... This process enhances the tokenizer's understanding capability..."
- **Break condition:** If the sampling ratio is biased heavily toward one adapter, the encoder will specialize, causing the "minority" task (e.g., generation) to fail or degrade significantly in the unified model.

## Foundational Learning

- **Concept: Finite Scalar Quantization (FSQ)**
  - **Why needed here:** Manzano uses FSQ instead of standard VQ-VAE lookup tables to map continuous features to discrete tokens. Understanding FSQ is critical to grasping how the model maintains a 64K codebook without the training instability of traditional vector quantization.
  - **Quick check question:** How does rounding vector elements to integers (FSQ) differ fundamentally from looking up vectors in a codebook (VQ-VAE), and what does this imply for gradient flow?

- **Concept: Spatial-to-Channel (STC) Layer**
  - **Why needed here:** The tokenizer uses a 3×3 STC operation to reduce spatial tokens (compression) before the adapter. This is the mechanism controlling the sequence length fed to the LLM.
  - **Quick check question:** If you apply a 3×3 STC to a 42×42 feature map, what are the resulting spatial dimensions, and why is reducing token count critical for unified LLMs?

- **Concept: Task Conflict in Multimodal Learning**
  - **Why needed here:** The paper frames its entire contribution around "mitigating task conflict." You must understand why standard joint training often degrades performance (e.g., generation loss dominating understanding loss).
  - **Quick check question:** Why would a standard VAE tokenizer (good for generation) typically conflict with a CLIP-style encoder (good for understanding) inside a single LLM?

## Architecture Onboarding

- **Component map:**
  Vision Encoder (ViT) -> Shared Encoder Backbone -> Continuous Adapter (STC + MLP) + Discrete Adapter (STC + FSQ + MLP) -> Unified LLM (64K extended tokens) -> Diffusion Decoder (DiT-Air)

- **Critical path:**
  The **Hybrid Image Tokenizer Training** (Sec 4.2.1) is the most critical initialization step. You must pre-train the tokenizer + small LLM (300M) with random adapter sampling *before* attempting to train the full Unified LLM. Skipping this alignment results in an LLM that cannot interpret the visual tokens effectively.

- **Design tradeoffs:**
  - *FSQ vs. VQ-VAE:* FSQ offers simplicity and scalability to large codebooks (64K) but may lack the latent space compactness of learned VQ dictionaries.
  - *Shared Encoder vs. Dual Encoder:* Shared encoder reduces conflict and parameters but introduces a risk of feature dilution where neither task gets optimal features.

- **Failure signatures:**
  - **Semantic Drift:** If text-to-image prompts are ignored (e.g., "blue car" generates a red car), the LLM is likely failing to predict the correct discrete semantic tokens, or the diffusion decoder is ignoring conditioning.
  - **Understanding Regression:** If VQA scores drop significantly compared to an understanding-only baseline, check the loss weighting; the image generation loss (1.0) may be overpowering the text loss (1.0).

- **First 3 experiments:**
  1. **Tokenizer Ablation:** Replace the Hybrid Tokenizer with a Pure-Discrete or Dual-Encoder setup (as in Table 1) to verify the performance drop specifically on text-rich understanding benchmarks (ChartQA/DocVQA).
  2. **Unified vs. Single-Task Baseline:** Train two 300M models—one unified, one understanding-only. Compare the delta on benchmarks like TextVQA to quantify "task conflict" (target: < 1.0 delta as per Fig 5).
  3. **LLM Scaling Validation:** Train 300M and 1B variants on a small subset of data. Verify if the "scaling curve" in Fig 6a holds (i.e., do both understanding and generation scores rise monotonically with parameter count?).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Why does scaling the image decoder lead to a decrease in aesthetic quality despite improvements in structural integrity, and how can this trade-off be mitigated?
- **Basis in paper:** [explicit] The authors state in Section 5.3 regarding image decoder scaling: "A drop in aesthetic quality is observed, which we leave for more in-depth study in future work."
- **Why unresolved:** The paper identifies the negative correlation between decoder size and aesthetic score as an empirical finding but does not investigate the underlying mechanism or propose a solution.
- **What evidence would resolve it:** An ablation study analyzing the feature distributions of larger decoders or a modified training objective that decouples structural learning from aesthetic style, demonstrating restored aesthetic scores at 3B+ decoder scale.

### Open Question 2
- **Question:** How can evaluation benchmarks be redesigned to capture emergent capabilities in unified models when standard metrics like GenEval and DPG saturate at larger scales?
- **Basis in paper:** [explicit] Section 5.3 notes that "performance on the GenEval and DPG benchmarks becomes saturated when the model becomes larger. This saturation motivates a re-examination of how emergent capabilities of unified models could be assessed..."
- **Why unresolved:** Current benchmarks fail to differentiate the performance gains (e.g., world knowledge reasoning) seen when scaling from 3B to 30B parameters, suggesting the metrics are insufficient for modern unified models.
- **What evidence would resolve it:** The proposal and validation of a new benchmark suite containing complex, reasoning-heavy generation tasks that show non-saturating performance curves for large-scale unified models.

### Open Question 3
- **Question:** Can the hybrid tokenizer and unified AR approach effectively extend to dynamic modalities like video or audio without introducing new task conflicts?
- **Basis in paper:** [explicit] The conclusion states the authors are "eager to explore... unification with more capabilities and more modalities in our future works."
- **Why unresolved:** The current architecture is optimized for static image understanding and generation; temporal dependencies in video or audio waveforms may require different tokenizer compression strategies (e.g., temporal vs. spatial tokens) that could conflict with the existing semantic alignment.
- **What evidence would resolve it:** A successful implementation of Manzano for video/audio that retains the hybrid tokenizer's benefits, showing minimal performance regression compared to single-modality specialist models.

## Limitations
- The architecture's success critically depends on the shared vision encoder's ability to simultaneously preserve high-level semantics for understanding and low-level spatial details for generation, which may not scale optimally across all tasks
- Evaluation scope presents significant coverage gaps, particularly in real-world visual understanding tasks and comprehensive human evaluation of generation quality
- Scaling analysis shows monotonic improvements but does not establish optimal model sizes for different use cases or investigate diminishing returns and computational efficiency trade-offs

## Confidence
**High Confidence** (4 claims):
- The hybrid tokenizer architecture with shared encoder + dual adapters is novel and technically sound
- The three-stage training pipeline (tokenizer pre-align → unified LLM → image decoder) is correctly implemented
- The 64K discrete codebook size provides sufficient capacity for semantic image representation
- The unified autoregressive objective without task-specific heads is implemented as described

**Medium Confidence** (3 claims):
- The performance improvements on text-rich benchmarks are attributable to the hybrid architecture rather than data quality or scale advantages
- The decoupling of semantic prediction and pixel rendering provides genuine efficiency benefits
- The random adapter sampling effectively mitigates task conflict without introducing new optimization challenges

**Low Confidence** (2 claims):
- The model achieves "state-of-the-art performance among unified models" across all evaluated dimensions
- The minimal task interference claim