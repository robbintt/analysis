---
ver: rpa2
title: 'DAPPER: Discriminability-Aware Policy-to-Policy Preference-Based Reinforcement
  Learning for Query-Efficient Robot Skill Acquisition'
arxiv_id: '2505.06357'
source_url: https://arxiv.org/abs/2505.06357
tags:
- policy
- learning
- query
- queries
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of low query efficiency in preference-based
  reinforcement learning (PbRL) for legged robot skill acquisition. The core issue
  stems from policy bias in traditional PbRL, where a single policy limits trajectory
  diversity, reducing the number of discriminable queries available for learning human
  preferences.
---

# DAPPER: Discriminability-Aware Policy-to-Policy Preference-Based Reinforcement Learning for Query-Efficient Robot Skill Acquisition

## Quick Facts
- arXiv ID: 2505.06357
- Source URL: https://arxiv.org/abs/2505.06357
- Reference count: 20
- One-line result: DAPPER achieves superior query efficiency for learning legged robot skills by generating diverse policy trajectories and prioritizing discriminable preference queries.

## Executive Summary
This paper addresses low query efficiency in preference-based reinforcement learning (PbRL) for legged robot skill acquisition by introducing DAPPER, a framework that combines policy-to-policy query generation with discriminability-aware sampling. Traditional PbRL suffers from policy bias where a single policy limits trajectory diversity, reducing the number of meaningful preference queries. DAPPER overcomes this by training multiple policies from scratch, comparing their trajectories, and using a learned discriminator to prioritize queries humans can distinguish. The approach jointly maximizes preference reward and discriminability score during training, resulting in policies that are both aligned with human preferences and easily distinguishable from past behaviors.

## Method Summary
DAPPER generates preference queries by comparing trajectories from multiple policies trained from scratch rather than sampling from a single policy's incremental updates. A policy dataset stores all previously trained policies, and query pairs are sampled using a discriminability-weighted probability that prioritizes policy pairs humans are likely to distinguish. The framework employs a discriminator that learns to estimate preference discriminability from historical queries labeled as indistinguishable. During policy training, the total reward combines preference reward (learned from human labels) and discriminability reward (computed from discriminator outputs over past policies). The method uses Lagrangian PPO with constraint terms to prevent unrealistic behaviors and resets the discriminator each iteration to prevent overfitting.

## Key Results
- DAPPER outperforms baseline methods in query efficiency across multiple legged robot tasks in both simulation and real-world environments.
- The approach successfully learns policies with fewer queries, achieving better discrimination rates and convergence to target features.
- Query efficiency improvements are particularly pronounced under challenging preference discriminability conditions where human discriminability is low.

## Why This Works (Mechanism)

### Mechanism 1: Policy-to-Policy Query Generation Eliminates Policy Bias
- Training multiple policies from scratch produces more diverse trajectories than sampling from a single policy's incremental updates, breaking behavioral correlation chains.

### Mechanism 2: Discriminability-Guided Query Sampling
- A learned discriminator predicts which policy pairs humans will find distinguishable, enabling prioritized sampling of more informative queries using temperature-controlled probability sampling.

### Mechanism 3: Dual-Objective Reward Structure
- Jointly maximizing preference reward and discriminability score produces policies that are both aligned with human preferences and easily distinguishable from past policies.

## Foundational Learning

- **Preference-based Reinforcement Learning (PbRL)**: PbRL learns reward functions from human preference comparisons rather than hand-designed rewards. Why needed: Core paradigm DAPPER builds upon.
  - Quick check: Can you explain how a preference label over two trajectories updates a reward model using cross-entropy loss?

- **Proximal Policy Optimization (PPO) with Lagrangian constraints**: DAPPER uses LPPO for policy optimization with constraint terms to prevent unrealistic behaviors. Why needed: Enables constrained optimization while maintaining policy stability.
  - Quick check: How does a Lagrangian constraint prevent an agent from exceeding a cost threshold during policy updates?

- **Logistic Regression for Preference Modeling**: Preference probability P(π_i ≻ π_j) uses softmax over cumulative rewards. Why needed: Connects feature differences to label probabilities in the preference model.
  - Quick check: Given two trajectories with rewards r(τ_1)=2 and r(τ_2)=1, what is P(τ_1 ≻ τ_2)?

## Architecture Onboarding

- **Component map**: Policy π_i(s; θ_i) -> Trajectory Generator G(π) -> Preference Reward Model R_H(s,a; ϕ) -> Feature Extractor f(s,a) -> Discriminator D(G(π_i), G(π_j); ψ) -> Policy Dataset Π

- **Critical path**: 
  1. Initialize new policy π_i and train with r = (1-β)R_H + βR_D until convergence
  2. Store π_i in policy dataset Π
  3. Sample policy π_x from Π using discriminability-weighted sampling (Eq. 6)
  4. Generate trajectories τ_x, τ_i and collect human labels y ∈ {0, 0.5, 1}
  5. Update R_H from separable labels (y ∈ {0,1}) using Eq. 7
  6. Update D from indistinguishable labels (y=0.5) using Eq. 8
  7. Repeat for next iteration

- **Design tradeoffs**: 
  - Computational cost vs. policy diversity: Retraining from scratch costs ~12-13 min/iteration but avoids policy bias
  - α (sampling temperature): Higher values favor discriminable queries but reduce query diversity
  - β (exploration coefficient): Higher values promote exploration but risk convergence failure
  - Query budget N per iteration: More queries improve reward model but increase human burden

- **Failure signatures**:
  - Discriminator overfitting: Reset discriminator parameters each iteration; use Monte Carlo dropout
  - Insufficient discriminable queries: Lower discriminability threshold or increase exploration coefficient β
  - Policy convergence to similar features: Check if R_D signal is stable; verify feature extractor captures meaningful differences
  - Label noise accumulation: Filter inconsistent responses; consider robust loss functions

- **First 3 experiments**:
  1. Replicate human discriminability threshold experiment (Section V-C): Train policies with varying body height/incline angle, collect human labels on pairwise comparisons, verify discriminability threshold d_disc ≈ 0.3-0.4 matches paper findings
  2. Compare query efficiency on 2-feature task: Run DAPPER vs. Baseline on "Posture" task (body height + incline angle), measure queries to reach d_pref < 0.02 under "Medium" discriminability setting
  3. Ablate discriminability reward: Compare full DAPPER vs. Ours w/o R_D to isolate contribution of discriminability-aware training on convergence speed

## Open Questions the Paper Calls Out

- Can the DAPPER framework maintain query efficiency as the dimensionality of feature parameters increases significantly beyond the six dimensions tested? [explicit] The authors state investigating potential limitations as feature dimensionality increases further "remains an important direction for future work."

- Can warm-starting or meta-learning techniques be effectively integrated to reduce the computational overhead of retraining policies from scratch without re-introducing policy bias? [explicit] Section VI identifies the "Trade-off Between Computational Cost and Policy Diversity" and lists "warm-starting policy initialization... or leveraging meta-learning techniques" as potential directions to mitigate this limitation.

- Can automated feature extraction methods replace hand-crafted features to reduce sensitivity to manual design choices while maintaining discriminability? [explicit] Section VI notes that "A promising future direction is to reduce this dependence on hand-crafted features by integrating automated feature extraction or richer representations."

## Limitations
- High computational cost of training policies from scratch each iteration (12-13 minutes per policy) limits real-world applicability
- Underspecified discriminator architecture and training procedure details make exact replication challenging
- Human subject experiments lack systematic analysis of human labeling consistency across iterations

## Confidence
- **High confidence**: Query efficiency improvements over baselines are well-supported by quantitative results (Fig. 5, Table I) across multiple tasks and discriminability settings
- **Medium confidence**: The claim that policy-to-policy training eliminates policy bias is mechanistically sound but relies on assumptions about feature space coverage that aren't fully validated
- **Medium confidence**: Discriminability-guided sampling consistently improves query quality, though the exact contribution varies by task and discriminability setting

## Next Checks
1. **Ablation study on policy initialization**: Compare DAPPER against a variant that fine-tunes previous policies (rather than training from scratch) to quantify the policy bias elimination effect
2. **Discriminator generalization test**: Evaluate how discriminator performance on predicting human discriminability changes as more policies are added to the dataset across multiple iterations
3. **Query efficiency under varying human consistency**: Systematically vary the proportion of inconsistent human labels (e.g., 0%, 10%, 20%) to test DAPPER's robustness to noisy preference data