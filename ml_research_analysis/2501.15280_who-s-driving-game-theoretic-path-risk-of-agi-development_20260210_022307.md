---
ver: rpa2
title: Who's Driving? Game Theoretic Path Risk of AGI Development
arxiv_id: '2501.15280'
source_url: https://arxiv.org/abs/2501.15280
tags:
- content
- corpus
- documents
- data
- harmful
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes iterative constitutional corpus curation as
  a method for improving model safety through pretraining data filtering. The core
  idea is that models trained on filtered data can be used to filter the corpus more
  effectively, creating a virtuous cycle that converges to a self-consistent corpus
  where the trained model approves of its own training data.
---

# Who's Driving? Game Theoretic Path Risk of AGI Development

## Quick Facts
- arXiv ID: 2501.15280
- Source URL: https://arxiv.org/abs/2501.15280
- Authors: Robin Young
- Reference count: 4
- Primary result: Iterative constitutional corpus curation converges to self-consistent corpus where trained model approves of its own training data, enabling exponential decay in harmful content

## Executive Summary
This paper proposes iterative constitutional corpus curation as a method for improving model safety through pretraining data filtering. The core idea is that models trained on filtered data can be used to filter the corpus more effectively, creating a virtuous cycle that converges to a self-consistent corpus where the trained model approves of its own training data. The authors provide theoretical analysis showing guaranteed convergence to a fixed point, exponential decay in harmful content even with constant filter quality, and bounds on capability-safety tradeoffs. They argue this approach offers a novel form of scalable oversight since the resulting corpus is human-auditable, unlike opaque model internals.

## Method Summary
The method iteratively filters a pretraining corpus using models trained on progressively cleaner data. Starting with an initial corpus D and constitution φ defining acceptable content, the algorithm trains model M_n on corpus C_n, scores all documents in C_n using M_n against φ, filters out documents above threshold τ, and repeats until convergence. The process guarantees monotone convergence to a self-consistent corpus where the final model approves of its own training data. The approach is framed as a game-theoretic solution where shared investments in safety become more valuable as participation grows, enabling cooperation to dominate defection in AGI development dynamics.

## Key Results
- Iterative corpus curation converges to a self-consistent corpus where the trained model approves of its own training data
- Harmful content decays exponentially even with constant filter quality across iterations
- Capability loss is bounded by safety gain times the entanglement ratio of dual-use to useful documents

## Why This Works (Mechanism)

### Mechanism 1: Monotone Convergence to Fixed Point
Each filtering step produces C_{n+1} ⊆ C_n (monotone decreasing). A finite set with monotone removal must converge in ≤ |D| iterations. Core assumption: filtering removes documents but never adds them.

### Mechanism 2: Exponential Decay Under Constant Filter Quality
If each iteration removes fraction p of remaining harmful content, after n iterations the remaining fraction is (1-p)^n. Core assumption: filter quality is at least constant (doesn't degrade).

### Mechanism 3: Bounded Capability Loss via Entanglement Ratio
Worst case, all removed documents are maximally useful. When harmful and useful content are mostly disjoint, high safety is achievable with minimal capability loss. Core assumption: harmful capabilities reside primarily in identifiable documents rather than emerging from benign combinations.

## Foundational Learning

- **Fixed Point Theory**: Understanding what "convergence" guarantees (existence, not quality) and why self-consistency ≠ alignment. Quick check: Does convergence to a fixed point guarantee the resulting corpus is safe, or merely that it's stable under iteration?

- **Constitutional AI (Bai et al. 2022)**: The constitution ϕ defines acceptability; its specification quality determines fixed point quality. Quick check: If a constitution is underspecified, what happens to constitutional interpretation across iterations?

- **Dual-Use Content and Entanglement**: The capability-safety tradeoff is governed by how much harmful and useful content overlap. Quick check: Why does low entanglement ratio |B|/|U| imply high safety is achievable with minimal capability loss?

## Architecture Onboarding

- Component map: D -> C_0 -> M_0 -> SCORE() -> C_1 -> M_1 -> ... -> C_N -> M_N
- Critical path: 1) Initialize C_0 ← D, 2) Train M_n ← TRAIN(C_n), 3) Score all d ∈ C_n using M_n and constitution ϕ, 4) Filter: C_{n+1} ← {d ∈ C_n : SCORE(M_n, d, ϕ) < τ}, 5) Repeat until |C_{n+1}| = |C_n|, 6) Return (C_N, M_N)
- Design tradeoffs: Binary filtering vs. preference-based reweighting (sharp removal vs. gentle downweighting); threshold τ strictness (faster convergence vs. capability preservation); constitution specificity (misaligned fixed point vs. brittle edge-case behavior)
- Failure signatures: Stalling (filter quality degrades, iterations remove little), Drift (constitutional interpretation shifts, fixed point doesn't match intended semantics), Compositional blindspot (benign documents combine to enable harm)
- First 3 experiments: 1) Baseline single-pass filtering with safety/capability evaluation, 2) Multi-iteration decay tracking to validate exponential decay claim, 3) Disagreement analysis to reveal constitutional drift across iterations

## Open Questions the Paper Calls Out
- Is the fixed point unique, or do multiple fixed points exist depending on initialization?
- What determines convergence rate, and when do diminishing returns make further iteration not worthwhile?
- How does filter quality evolve across iterations? Does it improve, degrade, or remain constant?
- Does iterative filtering select for compositional risks—combinations of individually benign documents that together enable harmful capabilities?

## Limitations
- Entanglement complexity: The central claim about bounded capability loss assumes harmful and useful content are largely disjoint, which may fail for compositional risks where benign documents combine to enable harm
- Constitution specification: The quality of the fixed point depends entirely on the constitution's specification, with no guidance on writing constitutions that capture intended safety properties
- Filter quality degradation: The exponential decay mechanism assumes filter quality doesn't degrade across iterations, but models might need exposure to harmful content to recognize it

## Confidence
- High confidence: Convergence to fixed point is mathematically guaranteed by monotone convergence theory
- Medium confidence: Exponential decay in harmful content under constant filter quality follows from geometric series mathematics
- Low confidence: Capability-safety tradeoff bounds depend on the entanglement ratio assumption, which is theoretically derived but empirically untested

## Next Checks
1. **Multi-iteration decay tracking experiment**: Run 5-10 iterations on a web-scale corpus with concrete constitution, track harmful content fraction and capability metrics at each iteration to validate exponential decay and bounded capability loss

2. **Disagreement analysis for constitutional drift**: Sample documents where consecutive models disagree on scores, analyze what changed in constitutional interpretation to reveal how training data affects judgment

3. **Compositional risk stress test**: After reaching fixed point, sample document triplets from final corpus and check for emergent harmful combinations to test the entanglement assumption