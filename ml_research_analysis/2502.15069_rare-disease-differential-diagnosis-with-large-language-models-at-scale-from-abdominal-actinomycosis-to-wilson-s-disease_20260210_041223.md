---
ver: rpa2
title: 'Rare Disease Differential Diagnosis with Large Language Models at Scale: From
  Abdominal Actinomycosis to Wilson''s Disease'
arxiv_id: '2502.15069'
source_url: https://arxiv.org/abs/2502.15069
tags:
- disease
- rare
- patient
- diseases
- diagnosis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RareScale improves LLM-based rare disease diagnosis by combining
  a smaller rare disease candidate generation model with a larger black-box LLM. The
  approach generates synthetic history-taking conversations from an expert system,
  trains a model to propose relevant rare diseases, and uses these candidates to enhance
  LLM differential diagnoses.
---

# Rare Disease Differential Diagnosis with Large Language Models at Scale: From Abdominal Actinomycosis to Wilson's Disease

## Quick Facts
- arXiv ID: 2502.15069
- Source URL: https://arxiv.org/abs/2502.15069
- Authors: Elliot Schumacher; Dhruv Naik; Anitha Kannan
- Reference count: 40
- One-line result: RareScale improves LLM-based rare disease diagnosis by combining a smaller rare disease candidate generation model with a larger black-box LLM, boosting Top-5 accuracy from 56.8% to 74.1%.

## Executive Summary
RareScale addresses the challenge of incorporating rare diseases into LLM-based differential diagnosis without overwhelming providers with uncommon conditions. The approach uses an expert system to generate synthetic history-taking conversations, trains a specialized candidate generation model to propose relevant rare diseases, and integrates these candidates into a larger LLM's diagnostic reasoning. This method significantly improves diagnostic accuracy for rare diseases while maintaining clinical practicality. The framework achieves up to 88.8% accuracy on rare disease candidate generation and improves exact match rates by up to 37% compared to baseline LLM approaches.

## Method Summary
RareScale employs a two-stage cascade architecture where an expert system (derived from Internist-1/QMR) generates structured cases with disease-finding relationships, which are then converted to conversational format by an LLM. A fine-tuned Llama 3.1 7B model serves as a rare disease candidate generator, trained on synthetic conversations to maximize recall of relevant rare diseases. The final differential diagnosis is produced by a larger black-box LLM (GPT-4o, Claude 3.5 Sonnet, or Llama 3.3 70B) that integrates these candidates with its own common disease knowledge using a multi-stage prompt that first generates independent diagnoses, then evaluates rare candidates, and finally selects the most appropriate top-5 diagnoses.

## Key Results
- Top-5 accuracy improves from 56.8% (baseline GPT-4o) to 74.1% with RareScale candidates
- Rare disease candidate generation model achieves up to 88.8% accuracy on GPT-4o-generated chats
- Exact match rates improve by up to 37% (33% for GPT-4o, 37% for Claude)
- Expert-evaluated synthetic chats show 90.48% agreement on positive cases, validating the simulation pipeline
- Reduces unrelated diagnoses by 25% while increasing relevant rare disease inclusion

## Why This Works (Mechanism)

### Mechanism 1: Expert System-Anchored Synthetic Data Generation
Using an expert system rather than LLMs to generate ground truth diagnoses avoids circular evaluation and produces training labels the LLM cannot simply recall. The expert system generates structured cases with disease-finding relationships scored by medical experts, while an LLM converts these to conversational format. This separates knowledge extraction (expert system) from linguistic realization (LLM). Core assumption: The expert system's diagnostic rules accurately capture rare disease presentations; synthetic conversations preserve diagnostic signal.

### Mechanism 2: Two-Model Cascade with Recall-Oriented Candidate Generation
A smaller specialized model for rare disease candidate generation outperforms using the same large LLM for both candidate generation and final diagnosis. The candidate model (Llama 3.1 7B) is fine-tuned on synthetic conversations to maximize recall of relevant rare diseases. A larger black-box LLM then integrates these candidates with its own common disease knowledge, using clinical judgment to accept or reject. Core assumption: Fine-tuning on specialized data produces better rare disease recall than prompting alone; the larger LLM can appropriately weigh candidates against common alternatives.

### Mechanism 3: Multi-Stage Prompting for Balanced Diagnosis
Instructing the LLM to first generate diagnoses independently, then consider rare candidates, then select final top-5 prevents over-reliance on the candidate list while ensuring rare diseases are considered. The prompt separates reasoning into "First pass" (LLM's own judgment), "Candidate pass" (evaluate rare suggestions), and final selection. The LLM can discard all rare candidates if inappropriate. Core assumption: The LLM has sufficient common disease knowledge and clinical reasoning to appropriately balance rare vs. common diagnoses when explicitly prompted.

## Foundational Learning

- **Expert Systems vs. LLMs for Knowledge Encoding**: Understanding why the hybrid approach outperforms either alone requires distinguishing symbolic rule-based reasoning from statistical pattern matching. *Quick check*: Can you explain why using GPT-4o to generate both training labels and final diagnoses would create circular evaluation?

- **Recall vs. Precision in Candidate Generation**: The paper optimizes the candidate model for recall (include possibilities), while the final LLM handles precision (select appropriateness). This asymmetry is intentional. *Quick check*: Why is a recall-oriented candidate model paired with a precision-oriented final selection preferable to a single high-precision model?

- **Closed-World Assumption in Simulation**: The expert system operates under a closed-world assumption (only 575 diseases, only symptoms in knowledge base). Understanding this boundary clarifies where the system can and cannot work. *Quick check*: What types of real-world presentations would fail to be captured by this simulation pipeline?

## Architecture Onboarding

- **Component map**: Expert System -> Chat Simulator -> Candidate Model -> Final DDx Generator
- **Critical path**: 
  1. Expert system generates (findings, DDx) pairs for each seed disease
  2. Chat simulator creates conversation from findings
  3. Candidate model trained on (conversation, DDx) pairs
  4. At inference: candidate model generates rare suggestions → final LLM produces balanced DDx
- **Design tradeoffs**: 
  - Coverage vs. Quality: Only 575 of 10,000+ rare diseases covered; paper notes ~100 diseases account for 80% of diagnoses
  - Synthetic vs. Real Data: Synthetic avoids privacy issues but may not capture real patient language variation
  - Model Size: 7B candidate model is efficient but may miss patterns larger models would catch
- **Failure signatures**: 
  - Low candidate accuracy on real conversations: May indicate synthetic-reality gap; check domain adaptation
  - Final LLM ignores candidates: Prompt may need adjustment; verify candidate format matches prompt expectations
  - Over-production of rare diagnoses: Final LLM over-weighting candidates; adjust prompt to emphasize common disease priority
- **First 3 experiments**: 
  1. Validate synthetic-real gap: Compare candidate model performance on synthetic vs. real (expert-annotated) conversations
  2. Ablate candidate source: Compare RareScale candidates vs. GPT-4o-generated candidates vs. no candidates
  3. Test on out-of-distribution diseases: Select 10 diseases not in expert system; evaluate whether cascade helps or harms

## Open Questions the Paper Calls Out

### Open Question 1
The optimal clinical deployment strategy for RareScale—should it be triggered only after common conditions are ruled out, or used proactively to guide earlier information gathering? The authors state this is an open problem requiring physician training and clinical testing, as they haven't evaluated clinical workflow integration or provider acceptance.

### Open Question 2
Can the RareScale approach be extended to rare diseases beyond the 575 covered by the expert system without requiring manual curation of expert knowledge bases? The conclusion notes that expanding an expert system to all rare diseases with human experts alone is likely impossible, suggesting future work should focus on using medical literature with techniques beyond retrieval augmented generation.

### Open Question 3
Does RareScale's performance on synthetic history-taking conversations transfer to real patient-provider interactions in clinical settings? The entire training and evaluation pipeline uses LLM-generated synthetic chats, and while expert validation showed 90.48% agreement on positive cases, this was limited to a small sample and focused on whether cases were possible, not whether they reflected realistic clinical presentations.

## Limitations
- The proprietary expert system used for case generation is unavailable, limiting independent verification of the synthetic data pipeline
- Method's generalizability is constrained by coverage of only 575 rare diseases, representing roughly 5-6% of known rare diseases
- All performance metrics rely on LLM judges rather than clinician validation at scale

## Confidence
- High confidence in the mechanism of using expert system-anchored synthetic data to avoid circular evaluation
- Medium confidence in the two-model cascade architecture improving rare disease recall
- Medium confidence in the multi-stage prompting approach balancing common vs. rare disease consideration
- Low confidence in the approach's performance on rare diseases outside the 575-disease coverage

## Next Checks
1. Conduct clinician validation on a held-out set of real patient conversations to verify the gap between synthetic training data and actual clinical presentations
2. Test the candidate generation model on diseases outside the expert system's coverage to quantify performance degradation and identify failure patterns
3. Compare RareScale's differential diagnosis quality against human expert physicians on identical cases to establish clinical utility beyond numerical metrics