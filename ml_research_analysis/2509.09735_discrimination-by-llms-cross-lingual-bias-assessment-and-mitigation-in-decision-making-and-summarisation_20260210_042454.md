---
ver: rpa2
title: 'Discrimination by LLMs: Cross-lingual Bias Assessment and Mitigation in Decision-Making
  and Summarisation'
arxiv_id: '2509.09735'
source_url: https://arxiv.org/abs/2509.09735
tags:
- bias
- language
- mitigation
- biases
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated bias in large language models (LLMs) across
  decision-making and summarization tasks, focusing on background, gender, and age
  discrimination. Using an expanded dataset adapted from Tamkin et al.
---

# Discrimination by LLMs: Cross-lingual Bias Assessment and Mitigation in Decision-Making and Summarisation

## Quick Facts
- arXiv ID: 2509.09735
- Source URL: https://arxiv.org/abs/2509.09735
- Authors: Willem Huijzer; Jieying Chen
- Reference count: 40
- Key outcome: Significant bias in decision-making tasks favoring female gender, younger ages, and African-American backgrounds, with mitigation strategies reducing bias by up to 27%

## Executive Summary
This study systematically investigates bias in large language models (LLMs) across decision-making and summarization tasks, focusing on background, gender, and age discrimination. Using an expanded dataset with 151,200 decision-making prompts and 176,400 summarization prompts, the research tests GPT-3.5 and GPT-4o in English and Dutch. Results reveal that bias manifests strongly in decision-making tasks but minimally in summarization, with explicit demographic mentions amplifying bias effects. Novel prompt-instructed mitigation strategies achieved up to 27% bias reduction, with combined approaches proving most effective. Cross-lingual analysis indicates consistent bias patterns across languages, though Dutch responses showed larger disparities.

## Method Summary
The study adapted Tamkin et al.'s (2023) dataset to create 70 templates across decision-making and summarization tasks, generating prompts with demographic variations (background, gender, age) in explicit and implicit mention conditions. GPT-3.5 and GPT-4o were tested in English and Dutch, with decision tasks using temperature=0 for deterministic outputs and summarization using temperature=0.7. Bias was quantified through log probability extraction and beta regression with mixed effects, analyzing demographic impacts on decision probabilities and summary characteristics (ROUGE scores, sentiment, decision alignment). Six mitigation strategies ranging from simple instructions to complex multi-component prompts were evaluated for effectiveness.

## Key Results
- Decision-making tasks showed significant bias favoring female gender, younger ages, and African-American backgrounds
- Summarization tasks exhibited minimal bias except for age-related differences in GPT-3.5 English outputs
- Cross-lingual analysis revealed consistent bias patterns across English and Dutch, though Dutch showed larger disparities
- Combined mitigation strategies (rules + role + step-wise guidance) reduced bias by up to 27% on average
- GPT-4o demonstrated improved mitigation effectiveness compared to GPT-3.5

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task type determines whether demographic bias manifests in LLM outputs
- Mechanism: Decision-making tasks requiring binary choices expose model biases because they force selection without nuance; summarization tasks allow multiple valid outputs, distributing bias across information selection and articulation
- Core assumption: Bias reveals itself primarily when models must commit to single decisions rather than generate diverse outputs
- Evidence anchors:
  - [abstract] "Our analysis revealed that both models were significantly biased during decision-making, favouring female gender, younger ages, and certain backgrounds such as the African-American background. In contrast, the summarisation task showed minimal evidence of bias"
  - [section 5.2] "demographic variables (background, gender, and age) generally showed no significant impact compared to the reference group... The sole exception was a significant positive coefficient for 25-year-olds when implicitly mentioned in English GPT-3.5 outputs"
  - [corpus] Weak direct evidence; corpus neighbors focus on decision-making bias but not task-type comparisons
- Break condition: If summarization tasks show systematic demographic-based content selection differences (not just sentiment/style), task-dependency claim weakens

### Mechanism 2
- Claim: Explicit demographic mentions amplify bias compared to implicit name-based cues
- Mechanism: Direct demographic labels activate stronger associations in model representations than names carrying implicit demographic signals
- Core assumption: Models encode demographic associations more strongly through explicit labels than through name-based inference
- Evidence anchors:
  - [section 5.1] "Particularly under explicit demographic mention, both GPT-3.5 and GPT-4o showed a stronger preference for younger individuals, females, and the African-American background"
  - [section 5.3] "The impact of mitigation strategies was more pronounced for explicit demographic mentions, with an average percentage change of -14.57% compared to -8.92% for implicit mentions"
  - [corpus] Corpus neighbors confirm explicit bias amplification in decision-making tasks
- Break condition: If implicit cues (names) produce equal or larger bias than explicit mentions in other model families or languages

### Mechanism 3
- Claim: Combined prompt strategies (rules + role + step-wise guidance) reduce bias more effectively than single-strategy approaches
- Mechanism: Multi-component prompts provide redundant bias-suppression signals through equality rules, role adoption, and structured reasoning chains
- Core assumption: Models can follow complex multi-part instructions to override default biased patterns
- Evidence anchors:
  - [abstract] "The most effective instruction achieved a 27% mean reduction in the gap between the most and least favorable demographics"
  - [section 5.3] "The 'Rules + Role + Step-wise Guidance' strategy emerged as the most consistent in decreasing bias across models and languages. This strategy decreased the maximum discrepancy by 27.70% on average"
  - [corpus] Corpus neighbors confirm prompt-based mitigation effectiveness but lack comparative strategy analysis
- Break condition: If single-strategy approaches match or exceed combined strategy performance in newer model versions

## Foundational Learning

- Concept: Beta regression with mixed effects
  - Why needed here: Models probability data bounded between 0-1 (decision probabilities) while accounting for request-level random variation
  - Quick check question: Why can't standard linear regression model binary decision probabilities correctly?

- Concept: Log probability extraction from LLM outputs
  - Why needed here: Captures model's true decision tendencies before thresholding, revealing subtle biases that binary outputs might mask
  - Quick check question: What does a log probability of -0.693 translate to in regular probability?

- Concept: Cross-lingual bias transfer
  - Why needed here: Understanding whether biases are language-agnostic or culturally/linguistically specific informs deployment decisions across markets
  - Quick check question: If bias patterns are identical across languages, what does this suggest about their origin?

## Architecture Onboarding

- Component map:
  - Prompt template system (70 templates × demographic variations × languages × instruction types)
  - Decision task evaluator (log probability extraction → beta regression analysis)
  - Summarization evaluator (masking → ROUGE/sentiment/decision probability analysis)
  - Mitigation prompt library (6 strategies with varying complexity)
  - Cross-lingual comparison framework (Dutch/English parallel analysis)

- Critical path:
  1. Template population with demographic variables (explicit/implicit conditions)
  2. Model inference (temperature=0 for decision, 0.7 for summarization)
  3. Probability extraction and aggregation
  4. Beta regression with demographic fixed effects
  5. Mitigation effectiveness comparison (max range reduction)

- Design tradeoffs:
  - Binary decisions vs. Likert scales: Binary enables probability analysis but reduces nuance
  - Temperature 0 vs. higher values: Deterministic outputs reveal true tendencies but don't reflect real-world variation
  - Dutch/English only: Limited language diversity constrains cross-lingual generalization
  - GPT-3.5/GPT-4o only: Excludes open-source models and other proprietary systems

- Failure signatures:
  - Near-certain responses (>95% or <5% probability) across all demographics indicate ceiling/floor effects
  - Mitigation strategies that reduce bias but also reduce correlation with original responses may sacrifice task performance
  - Large discrepancies between explicit and implicit conditions suggest prompt engineering sensitivity

- First 3 experiments:
  1. Replicate with your target model using 10 randomly sampled templates across all demographic conditions to establish baseline bias levels
  2. Test mitigation strategies (default vs. rules+role+stepwise) on highest-bias templates identified in step 1
  3. Extend to your deployment language if not Dutch/English, measuring whether cross-lingual patterns hold

## Open Questions the Paper Calls Out

None

## Limitations

- Study focuses exclusively on two proprietary models (GPT-3.5 and GPT-4o) in English and Dutch, limiting generalizability
- Decision-making tasks use binary choices which may not reflect real-world complexity where nuanced, multi-factor decisions are typical
- Cross-lingual analysis limited to two languages constrains claims about bias universality

## Confidence

- High confidence: Task-type dependency of bias manifestation, with decision-making showing significantly more bias than summarization
- Medium confidence: Explicit demographic mentions amplifying bias compared to implicit name-based cues
- Medium confidence: Combined mitigation strategies showing superior effectiveness over single-strategy approaches
- Low confidence: Cross-lingual consistency claims due to limited language sample (only Dutch/English)

## Next Checks

1. Replicate the study using open-source models (e.g., Llama, Mistral) to test whether bias patterns and mitigation effectiveness generalize beyond proprietary systems
2. Extend the language analysis to include at least 3-4 additional languages from different language families to validate cross-lingual bias transfer claims
3. Test mitigation strategies on real-world deployment scenarios with temperature >0 to assess practical effectiveness under stochastic conditions