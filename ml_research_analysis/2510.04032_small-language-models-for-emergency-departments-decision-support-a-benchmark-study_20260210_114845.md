---
ver: rpa2
title: 'Small Language Models for Emergency Departments Decision Support: A Benchmark
  Study'
arxiv_id: '2510.04032'
source_url: https://arxiv.org/abs/2510.04032
tags:
- medical
- tasks
- slms
- clinical
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper benchmarks small language models (SLMs) for use in\
  \ emergency departments (EDs), addressing the need for timely, accurate clinical\
  \ decision support under hardware and privacy constraints. The authors evaluate\
  \ 17 transformer-based SLMs (6B\u20138B parameters) across four ED-relevant tasks:\
  \ MedMCQA, MedQA-4Options, PubMedQA, and Medical Abstracts summarization."
---

# Small Language Models for Emergency Departments Decision Support: A Benchmark Study

## Quick Facts
- arXiv ID: 2510.04032
- Source URL: https://arxiv.org/abs/2510.04032
- Reference count: 40
- Key outcome: General-domain SLMs outperformed medical-specialized models on ED-relevant QA and summarization tasks, recommending general-purpose models for real-time ED decision support.

## Executive Summary
This paper benchmarks 17 small language models (SLMs) with 6B-8B parameters for emergency department (ED) decision support, addressing the need for timely, accurate clinical assistance under hardware and privacy constraints. The study evaluates models across four ED-relevant tasks: MedMCQA, MedQA-4Options, PubMedQA, and Medical Abstracts summarization. Using standardized frameworks and quantization for practical deployment, the results reveal that general-domain SLMs consistently outperformed their medical-specialized counterparts across all tasks. This suggests that broad reasoning and knowledge, rather than domain-specific training, may be more critical for ED decision support. The study recommends using general-purpose SLMs, such as Microsoft Phi3-small-8k, for real-time ED applications, highlighting their potential to meet clinical needs without requiring high-end infrastructure.

## Method Summary
The study evaluates 17 transformer-based SLMs (6B-8B parameters) on four ED-relevant tasks: MedMCQA, MedQA-4Options, PubMedQA (multiple-choice QA), and Medical Abstracts summarization. Models were assessed using the lm-evaluation-harness framework on Google Colab Pro with an NVIDIA A100 (40GB VRAM), and quantized to 4-bit/8-bit using Hugging Face transformers and bitsandbytes. Batch sizes were set to 8 for models under 8B parameters and 4 for ~8B models. The evaluation included 9 general and 8 medical-specialized models, with results measured in accuracy for QA tasks and Unitxt Accuracy, Macro-F1, and Micro-F1 for summarization. Task-incompatible models (e.g., encoder-only or non-generative) were excluded from certain evaluations.

## Key Results
- General-domain SLMs outperformed medical-specialized models across all evaluated tasks.
- Microsoft Phi3-small-8k emerged as a top-performing general-purpose model for ED applications.
- Quantized SLMs demonstrated feasibility for real-time ED decision support under hardware constraints.

## Why This Works (Mechanism)
The study leverages standardized evaluation frameworks and quantization techniques to assess SLMs' performance on ED-relevant tasks. By comparing general-domain and medical-specialized models, it identifies the critical role of broad reasoning capabilities over domain-specific training for clinical decision support. The use of publicly available datasets and reproducible evaluation methods ensures the findings are applicable to real-world ED settings.

## Foundational Learning
- **SLM Quantization**: Reducing model size and memory usage for deployment on resource-constrained hardware. *Why needed*: Enables real-time inference on GPUs with limited VRAM. *Quick check*: Verify model size reduction without significant accuracy loss.
- **lm-evaluation-harness Framework**: A standardized tool for benchmarking language models across diverse tasks. *Why needed*: Ensures consistent and reproducible evaluation. *Quick check*: Confirm task configurations and metrics align with study requirements.
- **Hugging Face Transformers**: A library for loading and fine-tuning pre-trained models. *Why needed*: Facilitates model loading and quantization. *Quick check*: Ensure compatibility with the target models and datasets.
- **Bitsandbytes**: A library for efficient model quantization. *Why needed*: Reduces memory usage for large models. *Quick check*: Validate quantization settings and their impact on performance.

## Architecture Onboarding
- **Component Map**: Hugging Face Transformers -> Bitsandbytes -> lm-evaluation-harness -> Evaluation Results
- **Critical Path**: Model loading and quantization -> Task-specific evaluation -> Performance metrics calculation
- **Design Tradeoffs**: General-domain models offer broader reasoning but may lack domain-specific terminology; medical-specialized models provide domain knowledge but underperform in reasoning tasks.
- **Failure Signatures**: Out-of-memory errors during evaluation; missing outputs for incompatible model-task pairs.
- **First Experiments**: 1) Test model loading and quantization on a single model. 2) Evaluate a subset of models on one task to validate setup. 3) Compare results across general and medical-specialized models on a single dataset.

## Open Questions the Paper Calls Out
1. Does fine-tuning general-purpose SLMs on specific ED datasets (e.g., triage notes, shift handovers) yield superior performance for generative tasks compared to off-the-shelf leaders? The study suggests this could improve sensitivity to ED-specific nuances.
2. Can a modular, multi-agent architecture of specialized SLMs outperform a single general-purpose model in complex ED workflows? The authors propose this as a potential approach for handling subtasks like summarization and guideline retrieval.
3. Do high accuracy scores on standardized medical benchmarks reliably predict a model's capability to support diagnostic reasoning in live ED settings? The study acknowledges the gap between benchmark performance and real-world clinical utility.

## Limitations
- The study does not specify identical evaluation conditions (e.g., few-shot examples, prompt templates) across all models, which could influence comparative performance.
- The evaluation was conducted on a single GPU setup, and the impact of hardware constraints on results is not fully explored.
- The exclusion criteria for task-incompatible models are not transparently detailed.

## Confidence
- **High Confidence**: The methodology and use of standardized frameworks support reproducibility; the recommendation for general-purpose SLMs is well-supported by empirical results.
- **Medium Confidence**: The claim that general-domain SLMs outperform medical-specialized models is robust but may be influenced by unaccounted factors like prompt design or quantization settings.
- **Low Confidence**: The lack of detail on random seeds, run counts, and variance estimation limits the assessment of statistical significance.

## Next Checks
1. Replicate the evaluation using identical few-shot prompts, random seeds, and quantization settings to verify the robustness of performance differences.
2. Test the models on alternative hardware configurations to assess generalizability under varying resource constraints.
3. Evaluate the models on additional clinical datasets or real-world ED case studies to confirm generalizability across diverse clinical scenarios.