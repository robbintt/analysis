---
ver: rpa2
title: 'FAIRE: Assessing Racial and Gender Bias in AI-Driven Resume Evaluations'
arxiv_id: '2504.01420'
source_url: https://arxiv.org/abs/2504.01420
tags:
- bias
- resumes
- claude
- gender
- resume
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces FAIRE, a benchmark to assess racial and\
  \ gender bias in large language models used for resume screening. The authors use\
  \ two methods\u2014direct scoring and ranking\u2014to measure how model performance\
  \ changes when resumes are slightly altered to reflect different racial or gender\
  \ identities."
---

# FAIRE: Assessing Racial and Gender Bias in AI-Driven Resume Evaluations

## Quick Facts
- **arXiv ID:** 2504.01420
- **Source URL:** https://arxiv.org/abs/2504.01420
- **Reference count:** 27
- **Primary result:** LLMs exhibit measurable racial and gender bias in resume screening, with bias magnitude and direction varying significantly by model and evaluation method

## Executive Summary
FAIRE introduces a benchmark to assess racial and gender bias in large language models used for resume screening. The authors employ two complementary methods—direct scoring and ranking—to measure how model performance changes when resumes are slightly altered to reflect different racial or gender identities. Results demonstrate that while every model exhibits some degree of bias, the magnitude and direction vary considerably. GPT-4o notably favored Asian resumes, scoring them 0.29 higher than the original, while Claude 3.5 Haiku was the most unbiased model with average differences not exceeding 0.04. In ranking evaluations, GPT-4o-mini showed the most extreme bias, ranking Black resumes much lower than Native American resumes. The study underscores the need for careful fairness checks and bias reduction methods to ensure AI hiring tools treat all candidates equitably.

## Method Summary
FAIRE evaluates LLM bias using a dataset of 10 job categories from Kaggle, applying demographic perturbations to resumes through name substitutions (from NYC census data) and experience modifications. The benchmark employs two evaluation methods: direct scoring across five dimensions (Relevance, Significance/Impact, Skill Relevance, Achievements/Impact, Cultural Fit) on a 1-5 scale, and ranking evaluations where models rank 5 resumes (one per racial demographic) against aggregated job descriptions. Bias is measured as the score difference from the original resume and average ranking position per demographic group, with statistical significance assessed through paired t-tests.

## Key Results
- GPT-4o showed the strongest racial bias, scoring Asian resumes 0.29 points higher than original resumes
- Claude 3.5 Haiku was the most unbiased model, with maximum bias gap not exceeding 0.04 across all racial groups
- Ranking evaluations revealed larger bias magnitudes than direct scoring, with GPT-4o-mini ranking Black resumes at 1.7 versus Native American at 4.3
- Subjective evaluation dimensions (Cultural Fit, Personal Traits, Significance/Impact) exhibited stronger bias than objective dimensions (Skill Relevance, Achievements)

## Why This Works (Mechanism)

### Mechanism 1: Demographic Signal Sensitivity via Name and Experience Perturbations
LLMs encode and respond to demographic signals embedded in resume text, producing systematically different evaluation scores for identical qualifications when names or racialized experience markers are changed. The benchmark introduces controlled perturbations—replacing neutral names with racially-associated names and modifying activity descriptions to reference race-specific experiences. When LLMs process these perturbed resumes, their scoring distributions shift, revealing statistical associations between demographic markers and evaluation outcomes that persist across models and industries.

### Mechanism 2: Multi-Dimensional Scoring Surfaces Subjectivity-Linked Bias Amplification
Bias manifests more strongly in subjective evaluation dimensions (Cultural Fit, Personal Traits, Significance/Impact) than in objective dimensions (Skill Relevance, Achievements). The direct scoring method evaluates resumes across five dimensions with explicit 1-5 rubrics. Subjective dimensions lack clear external grounding criteria, allowing implicit demographic associations in the model to influence judgments. The paper reports GPT-4o showing +0.80 bias toward Asian resumes in Cultural Fit—the largest dimension-specific bias observed—compared to smaller biases in more structured dimensions.

### Mechanism 3: Comparative Ranking Amplifies Relative Bias Through Position Competition
Ranking evaluations (comparing multiple resumes simultaneously) produce larger bias magnitudes and different bias directions than isolated direct scoring. In ranking mode, models must assign ordinal positions to a set of 5 demographically-distinct resumes. This forces relative comparisons where demographic associations can directly influence rank ordering. The paper shows GPT-4o-mini ranking Black resumes at 1.7 (favored) versus Native American at 4.3 (disfavored)—a 2.6-point spread—while its direct scoring bias gap was only 0.03. The competitive frame amplifies bias expression.

## Foundational Learning

- **Concept: Counterfactual Fairness Testing**
  - **Why needed here:** FAIRE's core methodology relies on comparing outcomes between original and perturbed resumes where only demographic markers differ. Understanding counterfactual fairness principles is necessary to interpret whether observed differences represent bias versus legitimate qualification signals.
  - **Quick check question:** If a resume is modified only by changing "supporting students" to "supporting Black students" and scores change, what confounds must be ruled out before attributing this to racial bias?

- **Concept: Statistical Significance in Bias Detection**
  - **Why needed here:** The paper reports paired t-tests (Tables 10-13) with p-values to distinguish systematic bias from random variation. Interpreting these results requires understanding when observed score differences reach statistical significance versus noise.
  - **Quick check question:** Table 10 shows GPT-4o has t=8.314, p=0.000 for Asian resumes. What does this tell you about the likelihood that the +0.29 score difference occurred by chance?

- **Concept: Multi-Dimensional Evaluation Decomposition**
  - **Why needed here:** FAIRE's five-dimension scoring framework (Relevance, Significance, Skill Relevance, Achievements, Cultural Fit) allows isolating where bias concentrates. Engineers need to understand how dimension-level analysis reveals bias patterns invisible in aggregate scores.
  - **Quick check question:** Why might Cultural Fit show +0.80 bias while Skill Relevance shows only +0.40 for the same model-demographic pair? What does this suggest about bias sources?

## Architecture Onboarding

- **Component map:** Resume Dataset (Kaggle, 10 categories) -> Perturbation Engine (name substitution, experience racialization) -> Evaluation Layer (Direct Scoring or Ranking) -> Analysis Pipeline (score aggregation, paired t-tests, bias gap calculation)

- **Critical path:** 1) Select baseline resume from dataset, 2) Apply demographic perturbation (name swap OR experience modification), 3) Submit to LLM with standardized prompt (Direct Scoring OR Ranking), 4) Extract scores/ranks, compute delta from baseline, 5) Aggregate across N=10 job categories, run statistical tests

- **Design tradeoffs:** Coverage vs. depth (only 10 job categories tested), perturbation types limited to race/gender, model scope restricted to 5 models, prompt dependency creating results that are prompt-specific

- **Failure signatures:** Claude 3.5 Sonnet/Llama 3.3 70B showing negative bias for ALL racial groups vs. original, GPT-4o-mini showing extreme ranking inconsistency with minimal direct-scoring bias, Llama 3.3 70B showing largest negative biases with highest baseline scores

- **First 3 experiments:** 1) Reproduce baseline bias measurements on GPT-4o and Claude 3.5 Haiku across all 10 job categories, 2) Test prompt sensitivity by modifying Cultural Fit rubric to be more concrete, 3) Cross-validate ranking vs. scoring consistency for models showing large divergence

## Open Questions the Paper Calls Out

- **Open Question 1:** How does bias manifest in LLM-based resume evaluations when resumes signal protected characteristics such as age, educational background, or disability status? The authors call for future research to investigate other forms of bias beyond race and gender.

- **Open Question 2:** How does the magnitude of bias in LLM-based screening compare to that of traditional, non-LLM automated screening tools or human reviewers? The study isolated LLMs without providing baseline comparisons against keyword-matching algorithms or human biases.

- **Open Question 3:** To what extent is the "negative bias" observed in Claude and Llama models a reaction to the synthetic style of the GPT-4o-generated perturbations rather than the demographic signals themselves? The methodology's use of GPT-4o to generate racial experience descriptions may introduce a confound.

- **Open Question 4:** Do the bias patterns identified in generalist roles (e.g., HR, IT) persist in highly specialized or emerging job categories? The benchmark's 10-category scope may not reflect specialized or emerging positions in today's job market.

## Limitations
- Perturbation approach may not fully isolate demographic bias from confounding factors introduced by text changes
- Generalizability across different job categories and models remains uncertain given limited coverage (10 categories, 5 models tested)
- Lack of standardized baselines for "unbiased" human hiring decisions makes it difficult to calibrate what bias magnitudes are practically significant

## Confidence
- **High confidence:** That LLMs exhibit measurable bias in resume evaluation (supported by consistent p-values across methods)
- **Medium confidence:** That bias magnitude and direction vary by model and evaluation method (though sample sizes and perturbation consistency need verification)
- **Low confidence:** That the specific perturbation approach cleanly isolates demographic bias from other text-level effects

## Next Checks
1. Replicate the core findings using an independent perturbation method (e.g., minimal name-only changes without experience modifications) to verify whether observed biases persist when controlling for confounding text changes.

2. Conduct inter-rater reliability testing by having human evaluators score the same perturbed resumes to establish baseline human bias levels and determine whether LLM biases exceed or align with human tendencies.

3. Test the benchmark's sensitivity by systematically varying perturbation intensity (minimal vs. extensive changes) to identify the threshold at which demographic signals begin influencing scores, helping distinguish between genuine bias and text-processing artifacts.