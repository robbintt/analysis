---
ver: rpa2
title: Explanation-Driven Counterfactual Testing for Faithfulness in Vision-Language
  Model Explanations
arxiv_id: '2510.00047'
source_url: https://arxiv.org/abs/2510.00047
tags:
- image
- counterfactual
- explanation
- answer
- original
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Explanation-Driven Counterfactual Testing
  (EDCT), an automated framework to evaluate the faithfulness of vision-language models'
  explanations. EDCT treats a model's own explanation as a falsifiable hypothesis
  by generating targeted counterfactual edits to visual concepts cited in the explanation,
  then measuring whether the model's answers and explanations change consistently.
---

# Explanation-Driven Counterfactual Testing for Faithfulness in Vision-Language Model Explanations

## Quick Facts
- **arXiv ID**: 2510.00047
- **Source URL**: https://arxiv.org/abs/2510.00047
- **Reference count**: 29
- **Primary result**: Introduces EDCT framework achieving 0.674 CCS on Gemini 2.5 Flash for faithfulness evaluation

## Executive Summary
This paper introduces Explanation-Driven Counterfactual Testing (EDCT), an automated framework to evaluate the faithfulness of vision-language models' explanations. EDCT treats a model's own explanation as a falsifiable hypothesis by generating targeted counterfactual edits to visual concepts cited in the explanation, then measuring whether the model's answers and explanations change consistently. The method extracts visual concepts from explanations, creates minimal image edits using generative inpainting, and computes a Counterfactual Consistency Score (CCS) via LLM-assisted analysis of prediction and explanation changes. Evaluated on 120 OK-VQA examples across multiple VLMs, EDCT reveals substantial faithfulness gaps: Gemini 2.5 Flash achieves the highest CCS (0.674), while open-source models score lower. The approach provides regulator-aligned audit artifacts for verifying causal claims in model explanations.

## Method Summary
EDCT operates through a systematic pipeline: first, it extracts visual concepts from a VLM's explanation using vision-language segmentation models (Grounded SAM, BLIP-2); second, it generates minimal counterfactual images by inpainting the identified concepts using models like Stable Diffusion; third, it queries the VLM with both original and counterfactual images to obtain new predictions and explanations; finally, it evaluates faithfulness by checking whether the removal of cited concepts causes consistent changes in both answers and explanations. The framework computes three scores: faithfulness via CCS (measuring counterfactual consistency), plausibility via Plausibility Consistency Score (PCS) for explanation quality, and correctness via Normalized Correctness Change (NCC) for answer accuracy. An LLM acts as judge to analyze whether counterfactual changes align with the original explanations.

## Key Results
- Gemini 2.5 Flash achieves the highest Counterfactual Consistency Score (CCS) of 0.674 across tested models
- Open-source models show lower faithfulness scores compared to proprietary models
- EDCT successfully identifies inconsistencies between explanations and actual visual reasoning
- The framework provides actionable audit artifacts for model verification

## Why This Works (Mechanism)
EDCT works by treating model explanations as falsifiable hypotheses that can be empirically tested through controlled visual manipulation. By systematically removing or altering concepts explicitly mentioned in explanations and observing whether the model's reasoning changes accordingly, the framework creates a causal test of faithfulness. The LLM judge serves as a consistency checker that can detect whether explanation changes align with the visual modifications, providing a scalable automated evaluation method where human annotation would be prohibitively expensive.

## Foundational Learning
- **Counterfactual consistency**: The principle that explanations should remain valid when their underlying premises are removed; needed to establish causal relationships between explanations and reasoning.
- **Vision-language segmentation**: The ability to identify specific visual concepts within both images and textual descriptions; needed to bridge the gap between what the model sees and what it claims to see.
- **Generative inpainting**: The technique of modifying images by removing or replacing specific visual elements; needed to create controlled counterfactuals without introducing confounding changes.
- **LLM-assisted evaluation**: Using language models as automated judges for complex reasoning tasks; needed to scale consistency checking beyond human annotation capabilities.
- **Explanation faithfulness**: The property that model explanations accurately reflect the actual reasoning process; needed as the core evaluation target.
- **Visual concept grounding**: The ability to link abstract concepts to concrete visual elements; needed to create meaningful counterfactual tests.

## Architecture Onboarding

**Component Map**
EDCT -> VLM (query) -> Segmentation Model -> Generative Inpainting -> LLM Judge -> Consistency Scores

**Critical Path**
Original image + query → VLM prediction + explanation → Concept extraction → Counterfactual generation → VLM re-query → LLM consistency analysis → CCS/PCS/NCC scores

**Design Tradeoffs**
- Automation vs. accuracy: LLM judges provide scalability but may lack human-level reasoning precision
- Generality vs. specificity: Broad concept extraction captures more cases but may introduce noise
- Minimal vs. complete edits: Small changes preserve image coherence but may not fully remove concept influence

**Failure Signatures**
- Low CCS with high PCS suggests plausible but unfaithful explanations
- High CCS with low NCC indicates faithful reasoning but incorrect answers
- Segmentation failures lead to missed concepts and incomplete counterfactual tests
- Generative artifacts may cause answer changes unrelated to targeted concept removal

**First 3 Experiments**
1. Run EDCT on a single OK-VQA example with a clear visual concept in the explanation
2. Compare CCS scores across different LLM judges for the same example
3. Test counterfactual generation with and without segmentation masks on the same concept

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can EDCT be adapted to verify faithfulness in video or dialog-based VLMs where temporal consistency is required?
- Basis in paper: [explicit] The authors state: "Our scope is VQA-style NLEs; extensions to dialog/video require temporal edits and persistence checks."
- Why unresolved: The current framework relies on static image edits (inpainting), which do not account for concept persistence or state changes across sequential frames or conversation turns.
- What evidence would resolve it: An extension of the framework incorporating temporal segmentation and video generation models to test if explanations remain faithful to dynamic visual changes.

### Open Question 2
- Question: How sensitive is the Counterfactual Consistency Score (CCS) to the specific choice and bias of the LLM utilized as the judge?
- Basis in paper: [explicit] The authors note: "Because PCS and NCC are LLM-assisted, scores can vary by judge and prompting."
- Why unresolved: While the authors suggest ensemble-judge variants as a mitigation, the degree to which the "faithfulness" score is an artifact of the judge's own reasoning bias versus the target VLM's behavior remains quantified only by ablation.
- What evidence would resolve it: A systematic study comparing CCS calculated by diverse judge architectures against human-annotated ground truth for consistency.

### Open Question 3
- Question: Does the introduction of segmentation masks to constrain edits significantly improve the validity of the counterfactual test?
- Basis in paper: [explicit] The authors suggest: "We can improve this by using segmentation masks to guide edits... and using metrics like LPIPS."
- Why unresolved: Current generative edits might alter unintended pixels, introducing confounders that cause the VLM's answer to change for reasons other than the targeted concept.
- What evidence would resolve it: Ablation results showing the correlation between edit isolation (locality) scores and the stability of the resulting CCS.

## Limitations
- Relies on LLM-assisted annotation which introduces potential subjectivity despite majority voting
- Focus on OK-VQA may not represent full diversity of vision-language reasoning tasks
- Generative inpainting may struggle with abstract concepts or unusual visual elements

## Confidence

**High confidence**: Core methodology follows established counterfactual testing principles with multiple annotators for reliability

**Medium confidence**: Generalizability across different VLM architectures and reasoning tasks (focused on single benchmark)

**Medium confidence**: Scoring methodology due to potential LLM subjectivity in counterfactual detection

**Low confidence**: Framework's ability to handle complex multimodal reasoning scenarios beyond OK-VQA domain

## Next Checks

1. Evaluate the framework across multiple diverse vision-language benchmarks (e.g., A-OKVQA, VQA, GQA) to assess generalizability of faithfulness measurements across reasoning types.

2. Implement cross-validation with human annotators for a subset of examples to quantify the reliability of LLM-assisted counterfactual consistency scoring versus expert judgment.

3. Test the framework with different generative models for counterfactual creation to measure sensitivity of faithfulness scores to the quality and type of visual edits.