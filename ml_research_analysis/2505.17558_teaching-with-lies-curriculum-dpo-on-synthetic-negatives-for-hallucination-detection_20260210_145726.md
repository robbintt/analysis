---
ver: rpa2
title: 'Teaching with Lies: Curriculum DPO on Synthetic Negatives for Hallucination
  Detection'
arxiv_id: '2505.17558'
source_url: https://arxiv.org/abs/2505.17558
tags:
- halucheck
- samples
- curriculum
- medhallu
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HaluCheck, a curriculum-guided DPO framework
  for training hallucination detection models using high-quality hallucinated negative
  samples instead of standard failed generations. The approach leverages a curriculum
  learning strategy that progressively exposes the model to increasingly difficult
  hallucinated samples, identified via fact-checking model confidence scores.
---

# Teaching with Lies: Curriculum DPO on Synthetic Negatives for Hallucination Detection

## Quick Facts
- arXiv ID: 2505.17558
- Source URL: https://arxiv.org/abs/2505.17558
- Reference count: 40
- Primary result: HaluCheck 3B achieves up to 24% relative F1 gains over baseline models on MedHallu and HaluEval hallucination detection benchmarks

## Executive Summary
HaluCheck introduces a curriculum-guided DPO framework for training hallucination detection models using high-quality hallucinated negative samples instead of standard failed generations. The approach leverages a curriculum learning strategy that progressively exposes the model to increasingly difficult hallucinated samples, identified via fact-checking model confidence scores. Experiments show HaluCheck 3B achieves up to 24% relative F1 gains over baseline models on MedHallu and HaluEval benchmarks, with strong zero-shot performance on DROP, CovidQA, and PubMedQA, outperforming larger models like Llama-3.2 3B.

## Method Summary
HaluCheck trains hallucination detection models using Direct Preference Optimization (DPO) with a curriculum learning approach. The method constructs preference pairs from factual and hallucinated answers in MedHallu and HaluEval datasets, then scores each hallucinated sample using MiniCheck's grounding probability. Samples are filtered (p_l < 0.25) and sorted by ascending difficulty (low to high p_l), then trained in stages to gradually expose the model to harder examples. The framework uses Llama-3.2 with LoRA adapters and optimizes the preference pairs using standard DPO with β=0.1.

## Key Results
- HaluCheck 3B achieves 24% relative F1 gains over baseline models on difficult benchmarks like MedHallu and HaluEval
- Zero-shot performance on DROP, CovidQA, and PubMedQA exceeds Llama-3.2 3B baseline (59.16% avg vs 54.60% avg)
- Curriculum ordering (easy-to-hard) provides 6-7 F1 point improvements over random ordering
- Curated hallucinated negatives show higher mean/median MiniCheck scores than standard negatives across all difficulty tiers

## Why This Works (Mechanism)

### Mechanism 1
Curated hallucinated negatives provide higher-quality training signal than standard failed generations for DPO alignment. Adversarial hallucinations exhibit higher "deceptive quality" (grounded factuality scores) that force the model to learn finer decision boundaries. Standard negatives are often trivially wrong, yielding uninformative gradients. By ranking hallucinations via MiniCheck grounding probabilities and selecting those with scores in the 0.25–1.0 range, the method retains plausible-but-false examples that sharpen the preference boundary.

### Mechanism 2
Curriculum-based progression (easy-to-hard) stabilizes DPO optimization and improves final detection performance. Samples are sorted by ascending MiniCheck grounding probability p_l (lower p_l = easier to identify as hallucinated). Training proceeds in staged batches over sorted data, exposing the model first to clear-cut negatives and gradually to more deceptive cases. This structured escalation ensures stable, incremental learning.

### Mechanism 3
Formulating hallucination detection as DPO preference optimization (factual as chosen, hallucinated as rejected) transfers detection capability to unseen datasets. Each training example forms a preference pair (x, y_w=factual, y_l=hallucinated). The DPO loss optimizes the policy π_θ to increase likelihood of factual completions and decrease likelihood of hallucinated ones relative to a frozen reference. This binary alignment objective generalizes when hallucinated negatives span diverse patterns.

## Foundational Learning

- **Concept**: Direct Preference Optimization (DPO)
  - Why needed here: HaluCheck replaces RLHF-style reward modeling with a direct preference loss over (chosen, rejected) pairs. Understanding the DPO objective (log-sigmoid of log-probability differences) is essential to interpret how the model learns to prefer factual over hallucinated completions.
  - Quick check question: Can you write the DPO loss for a single preference pair (x, y_w, y_l) and explain what β controls?

- **Concept**: Curriculum Learning
  - Why needed here: The method orders training samples by MiniCheck-scored difficulty and presents them in stages. Understanding why easy-to-hard ordering can stabilize training (vs random sampling) clarifies the reported F1 improvements.
  - Quick check question: What are two common ways to define "difficulty" in curriculum learning, and which does HaluCheck use?

- **Concept**: Fact-Checking / Grounding Verification
  - Why needed here: MiniCheck provides the grounding probability used to score and filter hallucinated negatives. Interpreting these scores determines how negatives are ranked and which samples are discarded (p_l < 0.25).
  - Quick check question: If a hallucination receives a high grounding probability from MiniCheck, is it likely to be classified as easy or hard for HaluCheck's curriculum?

## Architecture Onboarding

- **Component map**: Context + Question -> Llama-3.2 Backbone -> LoRA Adapters -> DPO Trainer <- Preference Pairs (factual/hallucinated) <- MiniCheck Scoring <- Curriculum Builder
- **Critical path**: 1) Build DPO preference pairs from MedHallu + HaluEval (17,000 pairs total) 2) Score each hallucinated negative with MiniCheck; filter and sort by p_l 3) Train Llama-3.2 backbone with LoRA using staged DPO over sorted batches (25 epochs, lr=1e-5) 4) Evaluate in-domain (MedHallu/HaluEval test splits) and zero-shot (DROP/CovidQA/PubMedQA)
- **Design tradeoffs**: Curriculum ordering stabilizes training but relies on MiniCheck alignment with model difficulty (Table 5 shows +6–7 F1 gains over random; ablation required to confirm per-domain); filtering threshold 0.25 retains plausible negatives but sensitivity varies by model scale; binary formulation is simple but may miss partial/span-level hallucinations
- **Failure signatures**: Random or reversed curriculum shows degraded F1 (Table 5), suggesting ordering matters; training on single dataset overfits (in-domain F1 high but cross-dataset drops sharply); very low grounding-probability negatives may be too easy, yielding less informative gradients
- **First 3 experiments**: 1) Reproduce curriculum vs random comparison (Table 5) on a single benchmark to validate training pipeline and confirm reported F1 gap 2) Ablate MiniCheck filtering threshold (0.25 vs 0.0–0.75 vs 0.25–0.75) as in Table 6 to find optimal grounding range 3) Test zero-shot generalization on external hallucination benchmark (e.g., Hades) to probe transfer and identify failure modes

## Open Questions the Paper Calls Out

- **Question**: Can the curriculum DPO framework effectively transfer to hallucination detection in non-QA domains such as abstractive summarization or multi-turn dialogue?
- **Question**: Is it possible to extend the binary classification head used in HaluCheck to support span-level or partial hallucination detection?
- **Question**: How sensitive is the curriculum learning strategy to errors or biases inherent in the external fact-verification model (MiniCheck)?

## Limitations
- Binary detection formulation restricts ability to identify partial or span-level hallucinations
- Evaluations focus on QA contexts, leaving unexplored effectiveness in other NLP tasks like dialogue generation, summarization, or multi-lingual settings
- Method heavily relies on quality and accuracy of external fact-verification model, potentially propagating inherent biases

## Confidence
- **High Confidence**: Reported F1 improvements from curriculum ordering and curated negatives are supported by direct comparisons and internal ablation results
- **Medium Confidence**: Generalizability to unseen domains demonstrated but relies on limited zero-shot benchmarks; robustness of 0.25 filtering threshold across domains not fully explored
- **Low Confidence**: Exact prompt templates and curriculum staging granularity unspecified, which may affect reproducibility and performance

## Next Checks
1. Test zero-shot performance on external hallucination benchmark (e.g., Hades) not used in training to validate domain transfer and identify structural generalization limits
2. Experiment with alternative detection prompt templates to assess sensitivity to instruction phrasing and ensure binary formulation captures intended task
3. Conduct systematic ablation of MiniCheck filtering threshold (e.g., 0.0, 0.1, 0.25, 0.5, 0.75, 1.0) across multiple datasets to confirm optimal range and its stability