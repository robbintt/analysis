---
ver: rpa2
title: 'Predicting Startup Success Using Large Language Models: A Novel In-Context
  Learning Approach'
arxiv_id: '2601.16568'
source_url: https://arxiv.org/abs/2601.16568
tags:
- startup
- success
- startups
- learning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel few-shot learning framework called
  kNN-ICL for predicting startup success in data-scarce environments typical of venture
  capital settings. The approach leverages large language models (LLMs) through in-context
  learning, where predictions are made by providing the model with relevant examples
  rather than training it on large datasets.
---

# Predicting Startup Success Using Large Language Models: A Novel In-Context Learning Approach

## Quick Facts
- arXiv ID: 2601.16568
- Source URL: https://arxiv.org/abs/2601.16568
- Reference count: 13
- Primary result: kNN-ICL achieves 71.3% balanced accuracy for startup success prediction, outperforming supervised baselines (63.1%) and vanilla ICL (69.6%)

## Executive Summary
This paper introduces kNN-ICL, a few-shot learning framework that predicts startup success using large language models in data-scarce environments typical of venture capital settings. The approach leverages in-context learning by providing the model with relevant historical examples rather than training it on large datasets. By combining structured attributes and unstructured textual descriptions through a k-nearest-neighbor retrieval mechanism, kNN-ICL creates a contextualized "micro-training set" at inference time. When evaluated on real-world startup data from Crunchbase, the method achieves state-of-the-art performance while maintaining interpretability through case-based reasoning.

## Method Summary
The kNN-ICL framework retrieves k-nearest neighbors from a historical startup corpus using fused embeddings of structured features (scaled) and text descriptions. It employs stratified sampling to balance SUCCESS and FAILURE examples in the prompt, addressing the inherent class imbalance in startup data. The LLM (Gemini 2.0 Flash) receives these examples through Algorithm 1 templates with company names redacted to prevent memorization. Predictions are made at inference time without any model training or fine-tuning, making it particularly suitable for data-scarce environments where labeled examples are limited.

## Key Results
- kNN-ICL achieves 71.3% balanced accuracy, outperforming supervised ML baselines (63.1%) and vanilla ICL (69.6%)
- Performance remains robust across different numbers of examples, with strong results even using as few as 50 examples
- Combining structured and textual data yields the strongest performance, consistently outperforming either modality alone
- The approach maintains interpretability through case-based reasoning by displaying retrieved examples

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Retrieving similar historical examples (kNN) creates a localized "micro-training set" that focuses the LLM's reasoning on relevant patterns, outperforming random example selection.
- **Mechanism:** The framework concatenates structured attributes (scaled) and textual embeddings into a fused vector. By querying this space with cosine similarity, the system retrieves k-nearest neighbors. This ensures the prompt contains startups analogous to the target, reducing noise and aiding the LLM's pattern matching.
- **Core assumption:** The LLM possesses sufficient latent reasoning capabilities to generalize from a small set of analogous examples (in-context learning) without weight updates.
- **Evidence anchors:**
  - [abstract] "The kNN-ICL method uses a k-nearest-neighbor retrieval mechanism to select the most similar historical startups as examples."
  - [section 3.2.1] "By providing the LLM with a targeted set of comparable startups, kNN-ICL creates a contextual 'micro-training set' at inference time."
  - [corpus] Related work "Policy Induction" suggests memory-augmented ICL is an active area for startup prediction, supporting the efficacy of retrieval-based context.
- **Break condition:** If the embedding space fails to capture semantic business similarity (e.g., recommends a biotech startup for a fintech target based on superficial keyword overlap), the "micro-training set" becomes irrelevant, likely causing performance to drop to vanilla ICL levels.

### Mechanism 2
- **Claim:** Stratified sampling of retrieved examples (forcing class balance) mitigates the base-rate fallacy typical in startup data (where failures dominate).
- **Mechanism:** The retrieval mechanism selects the k/2 most similar "SUCCESS" cases and k/2 most similar "FAILURE" cases. This manual balancing prevents the prompt from being dominated by the majority class (failures), which might otherwise bias the LLM toward predicting "FAILURE" regardless of the target's attributes.
- **Core assumption:** The LLM's probability of predicting a class is sensitive to the frequency of that class in the in-context examples.
- **Evidence anchors:**
  - [section 3.1] "Randomly choosing a set... may contain predominantly negative cases, causing the model to reflect the overall failure rate rather than learn meaningful distinctions."
  - [section 3.2.1] "...we adopt a stratified approach, where an equal number of SUCCESS and FAILURE examples are drawn."
  - [corpus] The corpus does not explicitly validate stratified sampling for ICL, but general ML literature on imbalanced datasets supports the intuition.
- **Break condition:** In regimes where positive examples are extremely scarce (e.g., < k/2 available), the mechanism may force the inclusion of dissimilar positive examples, diluting the relevance of the support set.

### Mechanism 3
- **Claim:** Combining structured data and textual descriptions in the prompt yields higher accuracy than either modality alone.
- **Mechanism:** Structured data (e.g., "founding year") provides hard constraints, while text captures semantic nuance (e.g., "AI interface"). The paper uses a scaling parameter (α) to balance the contribution of structured vs. textual vectors during retrieval, ensuring the LLM receives both "signals" and "narrative."
- **Core assumption:** LLMs can effectively synthesize heterogeneous data types (tabular and natural language) when presented in a unified prompt format.
- **Evidence anchors:**
  - [abstract] "...balancing both structured attributes... and unstructured textual descriptions."
  - [section 5.3] "Combining structured and textual data yields the strongest performance... consistently outperforms both the structured-only and text-only variants."
  - [corpus] Weak corpus signal; related papers often focus on one modality or the other, highlighting this fusion as a specific design choice in this architecture.
- **Break condition:** If the structured data is high-dimensional and sparse while the text is dense, simple concatenation (Eq. 3) might fail to align the feature spaces properly without careful scaling (α).

## Foundational Learning

- **Concept:** **In-Context Learning (ICL)**
  - **Why needed here:** This is the core paradigm. Unlike standard ML, ICL performs "learning" at inference time by conditioning on examples in the prompt. You must understand that no gradients are updated.
  - **Quick check question:** If you feed the model 50 examples and get a wrong prediction, can you "fine-tune" the model on those 50 examples to fix it next time? (Answer: No, that would violate the ICL definition used in this paper.)

- **Concept:** **k-Nearest Neighbors (kNN) with Vector Embeddings**
  - **Why needed here:** The paper relies on retrieving "similar" startups. This requires converting text and numbers into vectors and calculating distances (cosine similarity).
  - **Quick check question:** How does the system determine that "Uber" (text) is similar to "Lyft" (text) if the words don't match exactly? (Answer: Semantic embeddings map them to nearby points in vector space.)

- **Concept:** **Class Imbalance & Stratified Sampling**
  - **Why needed here:** Startups have a ~90% failure rate. A model predicting "always fail" is 90% accurate but useless. You need to understand why the paper forces a 50/50 split in the prompt.
  - **Quick check question:** Why is "Balanced Accuracy" used instead of standard "Accuracy"? (Answer: Standard accuracy is misleading in imbalanced datasets; balanced accuracy averages the recall of both classes.)

## Architecture Onboarding

- **Component map:** Crunchbase data -> Structured features + Text descriptions -> StandardScaler + embedding-001 -> Fused vectors (α-scaled) -> kNN retrieval (stratified) -> Algorithm 1 prompt construction -> Gemini 2.0 Flash -> SUCCESS/FAILURE prediction

- **Critical path:** The **Retrieval Step**. If the retrieved examples are not truly similar to the target startup (poor embedding or bad α), the LLM receives irrelevant context ("noise"), causing the prediction to fail.

- **Design tradeoffs:**
  - **Choice of k (Shots):** Higher k (e.g., 50) provides more context but saturates performance and costs more tokens. Lower k (e.g., 10) is cheaper but less accurate.
  - **Modality Weighting (α):** Setting α (Eq. 4) determines if "industry sector" (structured) matters more than "business model description" (text). The paper settles on α=0.5 but notes performance is stable across 0.3-0.7.

- **Failure signatures:**
  - **Majority Class Bias:** Model predicts "FAILURE" 100% of the time. *Likely cause:* Stratified sampling in kNN is broken or disabled.
  - **Random Performance:** Accuracy hovers near 50%. *Likely cause:* Embeddings are not loading correctly, or company names were not redacted (Section 4.7), leading the model to memorize names rather than patterns.
  - **Hallucination:** Model generates paragraphs instead of "SUCCESS/FAILURE". *Likely cause:* Prompt constraint (Algorithm 1) not enforced or temperature too high.

- **First 3 experiments:**
  1. **Sanity Check (Zero-Shot vs. Random ICL):** Run the pipeline with 0 examples vs. 10 random examples. Confirm ICL > Zero-Shot to verify the model is actually using the context.
  2. **Ablation (Structured vs. Text):** Run retrieval using *only* text embeddings and *only* structured features. Compare against the fused approach to replicate Table 8.
  3. **Sensitivity Analysis (α):** Vary the fusion scaling parameter α (e.g., 0.0 to 1.0) to find the optimal balance between text and structured signals for your specific dataset.

## Open Questions the Paper Calls Out

- **Open Question 1:** How do alternative, multi-class outcome definitions (e.g., sustainable growth, gradual revenue increase, moderate returns without formal exit) affect kNN-ICL prediction performance compared to the binary acquisition/IPO success criterion?
  - **Basis in paper:** [explicit] The authors state: "it does not capture the broader range of potential startup trajectories, such as sustainable long-term operation, gradual growth, or moderate returns without a formal exit. Future work could explore alternative outcome definitions."
  - **Why unresolved:** The binary classification conflates diverse outcomes (acquisition vs. IPO vs. follow-on funding) and ignores viable startups that operate profitably without exit events.
  - **What evidence would resolve it:** Evaluation of kNN-ICL on multi-class outcome labels (e.g., 4-5 trajectory categories) using the same Crunchbase cohort with extended longitudinal data.

- **Open Question 2:** What mechanisms can provide deeper interpretability for kNN-ICL predictions beyond the case-based transparency of displaying retrieved examples?
  - **Basis in paper:** [explicit] The authors state: "deeper interpretability remains an open area for future research."
  - **Why unresolved:** While retrieved examples offer "case-based interpretability," the LLM's internal reasoning for weighting structured versus textual features and combining evidence across shots remains opaque.
  - **What evidence would resolve it:** Development and validation of interpretability methods (e.g., attention analysis, feature attribution across examples) that explain which aspects of retrieved startups drive predictions.

- **Open Question 3:** How well does the kNN-ICL framework transfer to other data-scarce business prediction domains such as credit risk evaluation or business failure forecasting?
  - **Basis in paper:** [explicit] The authors propose "promising applications in, for example, business failure forecasting (e.g., Borchert et al. 2023), credit risk evaluation (e.g., Gunnarsson et al. 2021, Kriebel and Stitz 2022), and other domains in which only limited structured data is available."
  - **Why unresolved:** Startup success prediction involves specific feature types and outcome dynamics; generalizability to domains with different data structures (e.g., financial statements for credit risk) is untested.
  - **What evidence would resolve it:** Application of kNN-ICL to established credit risk or business failure datasets with controlled comparisons to domain-specific baselines.

## Limitations
- The approach depends heavily on the quality of the embedding space and retrieval mechanism—if semantic similarity is not well-captured, the "micro-training set" becomes irrelevant noise
- The stratification mechanism assumes sufficient positive examples exist for balanced retrieval, which may not hold in extreme scarcity scenarios
- The method's performance with different LLMs and embedding models remains unexplored, raising questions about generalization across model families

## Confidence
- **High Confidence**: The core mechanism of combining structured and textual data through fusion and retrieval is well-specified and theoretically sound. The empirical improvement over baselines (71.3% vs 63.1% balanced accuracy) is substantial and statistically meaningful.
- **Medium Confidence**: The specific scaling parameter α=0.5 and the choice of k=50 examples are reported as optimal but may be dataset-dependent. The stability claims across different k values need independent verification.
- **Low Confidence**: The long-term generalization of kNN-ICL to different startup ecosystems (non-US, different time periods) and its performance with more complex multi-class outcomes are not established.

## Next Checks
1. **Ablation on Modality Weighting**: Systematically vary the fusion parameter α from 0.0 to 1.0 in 0.1 increments to map the full sensitivity landscape and verify that α=0.5 is truly optimal for this dataset, not just locally optimal.

2. **Cross-Model Generalization**: Replicate the kNN-ICL pipeline using a different LLM (e.g., GPT-4 or Claude) and a different text embedding model to assess whether performance gains are specific to the Gemini 2.0 Flash + embedding-001 combination or represent a more general ICL principle.

3. **Scalability Stress Test**: Evaluate kNN-ICL performance as the corpus size varies from 50 to 5,000 examples to determine the retrieval mechanism's break points and identify the optimal corpus size where retrieval quality plateaus while computational costs remain manageable.