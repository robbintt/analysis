---
ver: rpa2
title: How to Improve the Robustness of Closed-Source Models on NLI
arxiv_id: '2505.20209'
source_url: https://arxiv.org/abs/2505.20209
tags:
- training
- data
- examples
- computational
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Closed-source LLMs suffer large OOD performance drops when fine-tuned.
  We propose selecting training data to improve robustness without changing model
  internals or increasing data size.
---

# How to Improve the Robustness of Closed-Source Models on NLI

## Quick Facts
- **arXiv ID:** 2505.20209
- **Source URL:** https://arxiv.org/abs/2505.20209
- **Reference count:** 40
- **One-line primary result:** Data selection strategies (uncertainty sampling and complexity-focused synthetic data) improve OOD robustness of closed-source models within realistic budgets

## Executive Summary
This paper addresses the critical problem that fine-tuning closed-source LLMs for Natural Language Inference (NLI) improves in-distribution (ID) performance but significantly degrades out-of-distribution (OOD) robustness. The authors propose data-centric strategies that work within API fine-tuning constraints, avoiding costly augmentation or model modifications. Their approach involves selecting challenging examples based on predictive uncertainty and generating complexity-controlled synthetic data. The key finding is that autoregressive LLMs (like GPT-4o-mini) are substantially more robust than encoder models even with minimal training data, making them better baselines for robustness research.

## Method Summary
The authors propose data-centric methods to improve OOD robustness within closed-source API constraints. The approach involves: (1) Fine-tuning a base model on an initial subset D_init (10k instances), (2) Using this model to score a candidate pool D_potential (10k instances) via Uncertainty Sampling (high entropy examples) or Difficulty Sampling (LLM-scored complexity), (3) Replacing a portion of D_init with top-scoring examples to maintain fixed training budget, and (4) Fine-tuning the final model on the modified dataset. For synthetic data, they generate examples using LLM prompting with varying complexity levels and replace portions of the training set. The methods are evaluated across multiple OOD benchmarks categorized as challenging (ANLI, COPA-NLI, INLI-I, WANLI) and standard (MNLI, FEVER, Scitail, INLI-NLI).

## Key Results
- Fine-tuning improves SNLI accuracy (ID) by ~3% but reduces ANLI performance (Challenge-OOD) by ~10 percentage points
- Uncertainty sampling improves challenging OOD performance by up to 1.5% by upsampling high-entropy examples
- Synthetic data improves standard OOD performance by up to 3.7%, with complexity control being crucial
- Autoregressive LLMs outperform encoder models by more than 10 percentage points on Challenge-OOD even with <2% of training data
- Replacing 5% of training data with synthetic examples yields optimal results; 15% replacement degrades performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Selecting training examples with high predictive entropy improves generalization on challenging OOD data.
- **Mechanism:** High entropy signals "hard" or "ambiguous" examples that force the model to learn robust reasoning patterns rather than spurious correlations. This helps the model generalize to other challenging OOD instances.
- **Core assumption:** High prediction entropy correlates with example complexity and the model has access to output probabilities.
- **Evidence anchors:** Uncertainty sampling improves challenging OOD performance by up to 1.5%; high entropy instances represent challenging under-represented cases.
- **Break condition:** If the API doesn't provide log-probabilities (e.g., Gemini, Command R).

### Mechanism 2
- **Claim:** LLM-generated synthetic data improves performance on easier OOD data but requires increased complexity for challenging OOD data.
- **Mechanism:** Synthetic data introduces domain diversity and paraphrase variations. Simple synthetic data broadens distributional coverage for standard benchmarks, but fails on challenge datasets because generated entailment relationships are too trivial.
- **Core assumption:** The LLM can produce logically valid NLI pairs, and filtering strategies can mitigate label noise.
- **Evidence anchors:** Synthetic data improves easier OOD performance by up to 3.7%; Short & Simple Generation produces less difficult examples that don't help Challenge-OOD.
- **Break condition:** If generation creates examples with high label noise (>12-24%) or domain is too narrow.

### Mechanism 3
- **Claim:** Autoregressive LLMs are substantially more robust than encoder models even with minimal fine-tuning data.
- **Mechanism:** Pre-training scale and autoregressive nature provide a "robustness prior" that encoder models lack. Fine-tuning with only ~2% of data yields better OOD performance than fully trained encoder models.
- **Core assumption:** Observed robustness is intrinsic to architecture/scale, not pre-training data contamination.
- **Evidence anchors:** Autoregressive LLMs outperform encoder models by more than 10 percentage points on Challenge-OOD with less than 2% of training data.
- **Break condition:** If OOD test data is present in LLM's pre-training corpus.

## Foundational Learning

**Distributional Shift (ID vs. OOD):** The core tension is that fine-tuning maximizes ID performance (learning dataset artifacts) at the expense of OOD robustness (general reasoning). Quick check: When fine-tuning a closed-source model, does an increase in accuracy on the validation set guarantee better performance on unseen domains?

**Spurious Correlations (Shortcut Learning):** The paper assumes OOD failure is caused by models learning "dataset-specific heuristics" (e.g., ignoring the premise). Methods like Uncertainty Sampling aim to bypass these shortcuts. Quick check: If a model predicts "Entailment" solely because the hypothesis sentence is highly plausible in the real world, what type of bias is it exhibiting?

**Data-Centric AI (Fixed Budget Optimization):** Since closed-source APIs prevent modifying training loops, the only lever for improvement is changing the input data composition (swapping examples). Quick check: Why is "upsampling" preferred over "data augmentation" (increasing dataset size) in the context of closed-source API fine-tuning?

## Architecture Onboarding

**Component map:** M (Base Model) -> M_base (Fine-tuned on D_init) -> Scores D_potential -> D_up/D_down selection -> Final training set

**Critical path:**
1. Fine-tune M on D_init (10k) to create M_base
2. Use M_base to score D_potential using Uncertainty (entropy) or Difficulty (LLM-prompted score)
3. Select top-K high-score examples for D_up and randomly select K from D_init for D_down
4. Fine-tune original M on (D_init âˆª D_up) \ D_down

**Design tradeoffs:**
- Uncertainty vs. Difficulty Sampling: Uncertainty requires log-prob access (API constraint); Difficulty requires expensive inference calls
- Synthetic vs. Human Data: Synthetic data improves Standard-OOD but risks annotation errors; Human data is reliable but limited in domain coverage
- Replacement Ratio (K): 5% optimal; 15% risks overfitting to "hard" examples and degrading ID performance

**Failure signatures:**
- API Incompatibility: Attempting Uncertainty Sampling on models without log-probabilities
- Over-complexity: Using "Long & Complex" synthetic data introducing >12% label noise
- In-distribution Collapse: Aggressive replacement causing drop in SNLI accuracy

**First 3 experiments:**
1. Verify Robustness Gap: Fine-tune M on D_init and compare SNLI (ID) vs. ANLI (Challenge-OOD) performance
2. Uncertainty Ablation: Implement Uncertainty Sampling (5% replacement) and measure delta on Challenge-OOD
3. Generation Complexity Test: Generate "Short & Simple" vs. "Long & Complex" synthetic data, replace 5% of D_init, evaluate on Standard-OOD vs. Challenge-OOD

## Open Questions the Paper Calls Out

**Generalization to Other Tasks:** Do the proposed strategies effectively generalize to tasks outside of NLI? The authors state findings may generalize to other tasks but only validated on NLI benchmarks. Resolution would require applying protocols to diverse NLP tasks and observing OOD performance trends.

**Domain-Specific Synthetic Data Paradox:** Why does domain-specific synthetic data fail to improve in-domain test sets compared to general synthetic data? The paper notes this was "Surprising" but doesn't explain why diverse general synthetic data outperforms targeted domain augmentation. Resolution requires analyzing feature distributions and bias patterns of domain-specific vs. general synthetic data.

**Predictive Strategy Selection:** Can the optimal robustness strategy be predicted from in-distribution data characteristics without OOD test sets? The conclusion states strategy depends on OOD complexity, but no proxy metric from training data correlates with unseen OOD complexity. Resolution requires developing metrics from training data that predict whether sampling or generation is preferable.

## Limitations
- **Data Contamination Risk:** Paper doesn't address potential pre-training data overlap between LLMs and OOD test sets, which could inflate robustness gains
- **Cost Assumptions:** Claims approach is "compatible with API fine-tuning constraints" but provides no cost estimates for methods requiring log-probabilities or LLM inference calls
- **Label Quality Assumptions:** Synthetic data generation assumes LLM-labeled examples are reliable enough, but only briefly mentions "if in doubt, discard" filtering without validation of label noise levels

## Confidence

**High Confidence:** Encoder models are weaker baselines than autoregressive LLMs for robustness; fine-tuning improves ID while degrading OOD performance

**Medium Confidence:** Uncertainty Sampling effectively improves challenging OOD datasets; synthetic data improves easier OOD datasets (1.5% and 3.7% improvements respectively)

**Low Confidence:** Specific mechanisms about why Uncertainty Sampling works; assertion that autoregressive LLMs provide intrinsic "robustness prior" without systematic ablation studies

## Next Checks
1. **Data Contamination Audit:** Systematically check whether ANLI, COPA-NLI, and other OOD test sets appear in pre-training corpora of evaluated LLMs using approximate nearest neighbor search
2. **Cost-Benefit Analysis:** Calculate total API costs for each method (Uncertainty vs. Difficulty Sampling vs. synthetic data generation) and compare against performance improvements
3. **Label Quality Validation:** Measure actual label noise in generated synthetic data by having human annotators validate samples, particularly comparing "Short & Simple" vs. "Long & Complex" generation strategies