---
ver: rpa2
title: 'Around the World in 24 Hours: Probing LLM Knowledge of Time and Place'
arxiv_id: '2506.03984'
source_url: https://arxiv.org/abs/2506.03984
tags:
- time
- hours
- knowledge
- very
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the ability of large language models to jointly
  reason over time and space by introducing GeoTemp, a dataset of 320k prompts covering
  289 cities across 37 time zones. The authors test eight open chat models across
  three families on tasks requiring varying combinations of temporal and geographical
  knowledge.
---

# Around the World in 24 Hours: Probing LLM Knowledge of Time and Place

## Quick Facts
- arXiv ID: 2506.03984
- Source URL: https://arxiv.org/abs/2506.03984
- Reference count: 40
- Primary result: Models achieve only 25.4% accuracy on complex geotemporal reasoning tasks despite having relevant knowledge

## Executive Summary
This paper evaluates large language models' ability to jointly reason over time and space through a novel dataset of 320k prompts covering 289 cities across 37 time zones. The authors test eight open chat models on four task types ranging from simple temporal verification to complex time-place-time reasoning. They find that while models can accurately identify time zones when explicitly asked, performance drops significantly on tasks requiring combined temporal and geographical reasoning. The study reveals that performance is strongly influenced by location name perplexity in training data rather than geographic region, and that explicit time zone injection can substantially improve results.

## Method Summary
The authors construct GeoTemp, a dataset of 332,928 prompts using the Olson Time Zone Database and Opendatasoft API to select 289 cities covering all 37 UTC zones. They generate four task types (VERIFICATION, TIME TIME, TIME PLACE, TIME TIME PLACE) with three instruction variants each (neutral, chain-of-thought, short). Eight open chat models (Llama2, Llama3, Qwen2) are evaluated with temperature 0 and max length 256 using a regex-based parser validated against human annotations. Accuracy is measured through majority voting across instruction types.

## Key Results
- Models achieve 25.4% accuracy on the most complex TIME TIME PLACE task versus 57.9% on simple VERIFICATION
- Performance strongly correlates with location name perplexity, with high-perplexity cities showing significantly lower accuracy
- Explicit time zone injection improves Llama3-70B performance from 33.4% to 76.3% on complex tasks
- Chain-of-thought prompting surprisingly decreases performance on simpler tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Models fail to solve geotemporal tasks not due to a lack of stored knowledge, but due to a failure to retrieve and compose that knowledge under multi-step reasoning loads.
- **Mechanism:** When probed directly (e.g., "City: UTC ±X"), models demonstrate high time-zone accuracy (up to 90%). However, when the same fact retrieval is nested inside a reasoning task (e.g., "What time is it in X hours in Location Y?"), accuracy collapses. This suggests the mechanism of failure is **retrieval interference** or **compositional bandwidth limits**, where the model prioritizes the reasoning procedure (time arithmetic) over the factual retrieval (zone lookup), leading to hallucination.
- **Core assumption:** Assumption: The probing setup (direct asking) accurately reflects the presence of retrievable knowledge in the weights.
- **Evidence anchors:**
  - [abstract] "We find a significant performance increase for location names with low model perplexity..."
  - [section 6.4] "All models can predict the correct UTC timezone for at least 65% of the locations... when explicitly asked... Consequently, the poor results... cannot be attributed solely to a lack of knowledge."
  - [corpus] Corpus neighbor "Do We Know What LLMs Don't Know?" supports the fragility of probing methods, suggesting retrieval consistency is a known systemic issue.
- **Break condition:** If models failed the direct probing task (Table 2) at rates similar to the reasoning task, this mechanism would be invalid (it would be a storage issue, not a retrieval issue).

### Mechanism 2
- **Claim:** Performance is causally linked to the frequency of location tokens in the training data, mediated by perplexity.
- **Mechanism:** The paper finds that low perplexity (a proxy for high training frequency) correlates with higher accuracy. Locations like "London" (low perplexity) are solved more reliably than obscure locations. The mechanism is **associative strength**: frequently co-occurring tokens (City + Time Zone) have stronger synaptic-like pathways, making them easier to retrieve during inference than rare associations, regardless of the country's economic status or region.
- **Core assumption:** Assumption: Perplexity calculated on a static template ("I live in {city}, {country}") serves as a reliable proxy for training data frequency.
- **Evidence anchors:**
  - [abstract] "...suggesting their repeated occurrence during model training."
  - [section 6.2] "We observe a gradual decrease in accuracy from low to very high perplexity locations... suggesting that model performance is biased in favor of locations that likely appear more frequently..."
  - [corpus] Corpus neighbor "When Facts Change" discusses temporal knowledge conflicts in training, aligning with the view that training distribution dictates retrieval success.
- **Break condition:** If performance were uniform across perplexity deciles but varied by geographic region (e.g., "Western vs. Non-Western"), this mechanism would be rejected.

### Mechanism 3
- **Claim:** Explicit knowledge injection bypasses the retrieval failure, serving as an external "working memory" that restores reasoning capability.
- **Mechanism:** Injecting time zone data directly into the prompt (e.g., adding "Time zone: UTC+1") significantly boosts performance (Llama3-70B from 33.4% to 76.3%). This works by **offloading retrieval**: the model no longer needs to access internal weights for the geographic fact, freeing up capacity for the temporal arithmetic. Notably, this helps most for cities the model *already knows*, suggesting it acts as an "activation key" rather than new information.
- **Core assumption:** Assumption: The performance gain is strictly due to information utility and not merely prompt lengthening or format changes.
- **Evidence anchors:**
  - [abstract] "...a direct injection of geographical knowledge leads to performance gains..."
  - [section 7] "Llama3-70B achieves a performance of 76.3% by adding the time zone information... improvements are mostly for locations for which... we found the models to know the time zone of already."
  - [corpus] Corpus neighbor "R4: Retrieval-Augmented Reasoning" supports the general efficacy of external context injection for complex reasoning.
- **Break condition:** If injecting *incorrect* time zone information also improved performance (placebo effect), the mechanism would be knowledge-agnostic.

## Foundational Learning

- **Concept: Perplexity**
  - **Why needed here:** The paper uses perplexity as the primary explanatory variable for why models fail on specific cities. Understanding that lower perplexity = higher probability/surprise avoidance is essential to grasp the "training frequency" hypothesis.
  - **Quick check question:** If a model assigns a high perplexity score to a specific city name, does the paper predict higher or lower accuracy for that city? (Answer: Lower accuracy).

- **Concept: Compositional Generalization**
  - **Why needed here:** The core failure is a lack of compositionality—models know Time and Place separately but fail to compose them. This concept explains why scaling alone doesn't solve the issue; the architecture struggles to bind distinct knowledge types.
  - **Quick check question:** Why does the Time-Time task (arithmetic only) score higher than Time-Place (arithmetic + fact retrieval)? (Answer: Compositional load; the model cannot effectively compose the retrieval operation with the arithmetic operation).

- **Concept: Direct Injection / Scaffolding**
  - **Why needed here:** Section 7 shows that models are "retrieval-limited" rather than "reasoning-limited." Understanding how to structure prompts to bypass weak internal retrieval is a key practical takeaway.
  - **Quick check question:** According to the paper, why does adding "UTC+2" to a prompt about Rome help more than just asking about Rome? (Answer: It offloads the retrieval of the time zone fact from the internal weights to the context window).

## Architecture Onboarding

- **Component map:**
  - GeoTemp Generator -> Inference Layer -> Regex Evaluator

- **Critical path:**
  1.  **Data Construction:** Select l1 (start) and l2 (target) from 289 locations.
  2.  **Prompting:** Inject current time/date context.
  3.  **Evaluation:** Use Regex to extract times/dates (98%+ accuracy validated against human annotators).

- **Design tradeoffs:**
  - **Regex vs. LLM-as-Judge:** Authors chose Regex for efficiency and determinism over LLM-judges, accepting minor noise in exchange for processing 320k prompts.
  - **Synthetic vs. Natural:** Tasks are synthetic ("What time is it in X hours") to minimize data contamination, trading "real-world phrasing" for "causal isolation" of specific reasoning steps.

- **Failure signatures:**
  - **CoT Over-reasoning:** On simple tasks (Verification), CoT causes models to "get stuck" and hallucinate non-existent conversions (e.g., converting local time to UTC unnecessarily).
  - **Low-Perplexity Hallucination:** Models generate confident but wrong time zones for high-perplexity (rare) cities, indicating they are guessing rather than retrieving.

- **First 3 experiments:**
  1.  **Perplexity Correlation Check:** Calculate perplexity of your specific domain entities (e.g., product names, locations) to predict where the model will hallucinate geography or time.
  2.  **Scaffolding Ablation:** Replicate Section 7. Test if extracting the "hard fact" (e.g., time zone or formula) and injecting it into the prompt improves output quality vs. standard CoT.
  3.  **Instruction Robustness:** Test "Neutral" vs. "Short" vs. "CoT" on your specific task. Do not assume CoT is better; verify if your task is simple enough that CoT causes distraction (as seen in Section 5.3).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can instruction strategies be optimized to prevent performance degradation, such as that caused by chain-of-thought prompting, on simpler geotemporal tasks?
- **Basis in paper:** [explicit] The authors note in Section 5.3 that chain-of-thought prompting surprisingly decreases performance on simpler tasks and state: "This highlights the need for a more comprehensive analysis of the effect of different instruction types on questions of varying difficulty, which we leave for future work."
- **Why unresolved:** The paper identifies the counter-intuitive failure mode (CoT hurting performance) but does not determine the specific cognitive load or distraction mechanisms causing it.
- **What evidence would resolve it:** A study varying instruction complexity across different task difficulties to identify a strategy that improves complex reasoning without impairing basic retrieval or simple calculation tasks.

### Open Question 2
- **Question:** What specific training interventions or architectural changes are required to enable models to systematically retrieve and combine latent time-zone knowledge during reasoning?
- **Basis in paper:** [explicit] In the Discussion, the authors conclude that models "possess the necessary knowledge but fail to retrieve and combine it effectively" and suggest "improvements towards more systematic and stepwise knowledge retrieval or the use of tools may be necessary."
- **Why unresolved:** The study demonstrates that knowledge injection fixes the problem, but it remains unclear how to train the model to perform this retrieval internally without external aid.
- **What evidence would resolve it:** Experiments fine-tuning models on compositional geotemporal tasks or modifying attention mechanisms to prioritize geographical entity linking, resulting in performance gains comparable to explicit knowledge injection.

### Open Question 3
- **Question:** Does increasing model scale eventually overcome the inability to perform joint geotemporal reasoning, or is this a fundamental limitation of current transformer architectures?
- **Basis in paper:** [explicit] Section 5.2 observes that while performance improves with scale, the gains for joint tasks are marginal compared to temporal-only tasks. The authors state: "This raises doubts as to whether the models will understand the underlying mechanisms for solving these tasks simply by upscaling."
- **Why unresolved:** The tested models (up to 72B/70B parameters) showed poor performance on complex tasks, leaving open the question of whether further scaling or structural changes are needed.
- **What evidence would resolve it:** Evaluation of frontier-scale models (e.g., >1T parameters) or retrieval-augmented architectures on the GeoTemp dataset to see if the "joint reasoning" accuracy gap closes naturally with scale.

### Open Question 4
- **Question:** To what extent is geotemporal performance determined by the frequency of specific city names in the pre-training corpus versus the model's inherent reasoning capabilities?
- **Basis in paper:** [explicit] In the Limitations section, the authors note they used perplexity as a proxy because they lack access to pre-training data, stating: "A future evaluation of fully open models and their training data could provide additional insights."
- **Why unresolved:** The correlation between low perplexity and high performance is strong, but causation cannot be proven without analyzing the actual training text.
- **What evidence would resolve it:** A controlled study training models on corpora with manipulated frequencies of specific geographic locations to observe the direct causal impact on geotemporal reasoning accuracy.

## Limitations

- The study relies on perplexity as a proxy for training data frequency, which may not perfectly capture the relationship between token occurrence and retrieval success.
- The regex-based evaluation methodology, while validated, may miss nuanced reasoning failures that don't manifest in the specific time/date format being matched.
- The study focuses exclusively on UTC-based reasoning, which may not generalize to other temporal reference frames or cultural contexts.

## Confidence

- **High Confidence:** The observation that models perform significantly worse on tasks requiring composition of temporal and geographical knowledge versus either alone.
- **Medium Confidence:** The claim that performance degradation is primarily driven by training data frequency rather than geographic region.
- **Medium Confidence:** The effectiveness of explicit knowledge injection for improving performance.

## Next Checks

1. **Perplexity Mechanism Validation:** Conduct controlled experiments varying the training frequency of specific location tokens in synthetic datasets to establish causal relationships between token frequency and retrieval accuracy.

2. **Alternative Temporal Reference Frames:** Replicate the study using relative time references (e.g., "tomorrow," "next week") and local time zones to assess whether the observed patterns hold across different temporal reasoning paradigms.

3. **Knowledge Injection Ablation:** Systematically test whether the performance gains from explicit time zone injection persist when injecting incorrect information, or when injecting the information in different formats (e.g., as part of the question versus separate context).