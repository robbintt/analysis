---
ver: rpa2
title: User-centric Subjective Leaderboard by Customizable Reward Modeling
arxiv_id: '2508.09463'
source_url: https://arxiv.org/abs/2508.09463
tags:
- criteria
- preference
- reward
- subjective
- preferences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a user-centric subjective leaderboard (USL)
  for large language models (LLMs) that enables dynamic, personalized rankings based
  on individual preferences and needs. The key innovation is a customizable reward
  model (CRM) that explicitly conditions on user-specified preference criteria, achieving
  97.27% accuracy on preference recognition tasks compared to GPT-4.1's 91.96%.
---

# User-centric Subjective Leaderboard by Customizable Reward Modeling

## Quick Facts
- arXiv ID: 2508.09463
- Source URL: https://arxiv.org/abs/2508.09463
- Reference count: 5
- Key outcome: Customizable reward models achieve 97.27% accuracy on preference recognition vs. GPT-4.1's 91.96%, enabling user-centric LLM leaderboards

## Executive Summary
This paper introduces a user-centric subjective leaderboard (USL) that enables dynamic, personalized rankings of large language models based on individual preferences. The key innovation is a customizable reward model (CRM) that explicitly conditions on user-specified preference criteria, achieving 97.27% accuracy on preference recognition tasks. Unlike traditional leaderboards that reflect aggregated crowd preferences, the USL captures diverse user needs by allowing explicit criteria specification, showing strong negative correlations when models are evaluated against contradictory preferences like "prefer detailed" versus "prefer concise" responses.

## Method Summary
The method trains CRMs by fine-tuning Qwen3 backbones (0.6B-8B parameters) on 9,714 human preference samples from LMArena, converting each (query, response_A, response_B) tuple into criteria-conditioned format using GPT-4o for criteria extraction. The CRM uses pairwise classification loss (L_cls) with cross-entropy, processing both candidate responses jointly to capture comparative features. Noise injection during training (removal, addition, replacement of criteria) improves robustness to imperfect user specifications. The USL ranks models by win rate against a baseline (Gemini-2.0-Flash-001) when evaluated using the CRM on DailyBench queries filtered by user-selected topics.

## Key Results
- CRMs with 4B parameters achieve 97.27% accuracy on preference recognition tasks, outperforming GPT-4.1 (91.96%) and Gemini-2.5-pro
- The CRM demonstrates superior generalization capabilities, maintaining high accuracy on new topics and criteria compared to baseline models
- Strong negative correlations (Kendall's τ = -0.83) are observed when models are evaluated against contradictory criteria like "prefer detailed" versus "prefer concise" responses
- 4B parameter CRMs achieve 95.6% accuracy on topic generalization tests versus 71.1% with ranking loss and 51.8% with 0.6B parameters

## Why This Works (Mechanism)

### Mechanism 1
Explicit criteria conditioning enables bidirectional preference modeling, breaking the 70-80% accuracy ceiling that plagues aggregated preference approaches. By conditioning the reward model on user-specified criteria `c`, the model learns criteria-specific preferences rather than aggregated crowd preferences. This allows the same response pair to be correctly evaluated under contradictory criteria (e.g., "prefer detailed" vs. "prefer concise"), which is impossible for models trained on aggregated preferences that internalize majority bias. The core assumption is that user-provided criteria accurately reflect their true preferences, and criteria can be reliably extracted or specified.

### Mechanism 2
Pairwise classification loss (joint response evaluation) outperforms traditional Bradley-Terry ranking loss for subjective criteria by enabling token-level comparison. The classification loss `L_cls` processes both candidate responses in a single forward pass, allowing the model to attend to comparative features between responses, whereas ranking loss evaluates each response independently (pointwise), missing subtle distinctions critical for subjective judgments like "more vivid imagery" or "better narrative flow." The core assumption is that comparative features exist and are learnable, and that subjective preferences are inherently relative rather than absolute.

### Mechanism 3
Noise injection during training (removal, addition, replacement of criteria) improves robustness to imperfect user specifications. Training with perturbed criteria simulates real-world scenarios where user-specified criteria may be incomplete, include irrelevant preferences, or conflict with available response features. The model learns to focus on applicable criteria while ignoring noise, preventing over-reliance on exact criterion matching. The core assumption is that real user criteria will be noisy and that the noise distribution in training approximates deployment conditions.

## Foundational Learning

- **Concept: Bradley-Terry Model & Reward Modeling**
  - Why needed here: The paper positions CRMs against traditional reward models trained with Bradley-Terry loss. Understanding `L_ranking = -log(σ(r_θ(chosen) - r_θ(rejected)))` is essential to grasp why pointwise scoring fails for subjective tasks.
  - Quick check question: Can you explain why a reward model trained on aggregated preferences might score Response A higher than Response B, even when a user explicitly prefers B for a specific criterion?

- **Concept: Criteria Conditioning in Transformers**
  - Why needed here: CRMs condition on criteria `c` alongside query `q`. Understanding how to concatenate or inject criteria into the input (likely prepending or special tokens) is necessary for implementation.
  - Quick check question: How would you modify a standard LLM forward pass to condition on a textual criterion like "prefer concise responses"?

- **Concept: Pairwise vs. Pointwise Evaluation**
  - Why needed here: The paper's key architectural choice is pairwise classification `r_θ(c, q, o_A, o_B)` vs. pointwise scoring. Understanding the computational and representational differences is critical.
  - Quick check question: What is the memory complexity difference between evaluating two responses jointly versus independently?

## Architecture Onboarding

- **Component map:**
  User Input: [Topics selection] + [Criteria text] -> DailyBench (522 queries) -> Filter by topic -> Pre-collected LLM responses for each query -> CRM-4B (Qwen3 backbone): Input = (criterion, query, response_A, response_B) -> Binary classification -> Preference prediction -> Win rate calculation vs. baseline (Gemini-2.0-Flash-001) -> Dynamic leaderboard ranking

- **Critical path:**
  1. Data preparation: Extract criteria from preference data using GPT-4o; apply noise augmentation
  2. CRM training: Fine-tune Qwen3 backbone with L_cls on 9,714 samples; early stopping on validation loss
  3. Inference: For each query-response pair, compute preference against baseline; aggregate win rates

- **Design tradeoffs:**
  - Model size: 4B chosen as sweet spot (97%+ accuracy); 0.6B fails (51.8% criterion generalization)
  - Training objective: L_cls chosen over L_ranking despite higher compute (joint encoding) for +24% accuracy gain
  - Noise injection: Trains slower but essential for deployment robustness (Table 3 shows degradation without it)

- **Failure signatures:**
  - Accuracy plateaus at ~70%: Model learning aggregated preferences, not criteria-conditioned preferences → check that criteria are being used as input
  - Large gap between T+/T- or C+/C-: Model has internal bias → increase noise augmentation or check training data balance
  - Rankings don't change with different criteria: Criteria not being incorporated → verify input formatting

- **First 3 experiments:**
  1. Reproduce baseline comparison: Train CRM-0.6B and CRM-4B on provided data; verify 4B achieves ~95%+ on test sets (Table 2 benchmarks)
  2. Ablate noise strategies: Train without noise augmentation; confirm performance drop on D-_replace subset (expect ~4% degradation per Table 3)
  3. Validate negative correlation: Apply CRM-4B to DailyBench with contradictory criteria (e.g., c1="prefer detailed" vs. c2="prefer concise"); verify Kendall's τ ≈ -0.8 between rankings

## Open Questions the Paper Calls Out

- **Open Question 1:** Does incorporating negative preference criteria (user dislikes) improve the robustness and accuracy of CRMs compared to positive-only descriptions? The current CRM architecture is trained solely to identify positive attributes, and it is unknown if modeling negative constraints adds predictive signal or introduces optimization conflicts during training.

- **Open Question 2:** Can CRMs function effectively as the reward signal within a Reinforcement Learning from Human Feedback (RLHF) pipeline to align LLMs with specific user preferences? While CRMs excel at evaluating static responses, it is unclear if their gradient signals are stable or precise enough to guide the optimization of a generative model without causing reward hacking.

- **Open Question 3:** Does a similarity-based activation mechanism for topic selection provide significantly more precise user-centric rankings than the current cluster-based selection? The current USL relies on users selecting broad topic clusters, which may include irrelevant queries, potentially diluting the signal for personalized ranking.

## Limitations

- The 97.27% accuracy claim relies heavily on the quality of GPT-4o-generated criteria extraction, which isn't independently verified and may introduce systematic bias
- Performance on genuinely novel criteria (not seen during training) remains untested - the "criterion generalization" tests may not capture true out-of-distribution generalization
- The negative correlation findings (-0.83 Kendall's τ) are impressive but based on a single dataset (DailyBench) with limited query diversity (522 queries)

## Confidence

- **High Confidence:** CRM architecture design and pairwise classification loss implementation are technically sound and well-documented
- **Medium Confidence:** The 97.27% accuracy vs 91.96% comparison is valid within the controlled test environment, but real-world deployment may show performance degradation
- **Low Confidence:** Claims about solving the "user-centricity" problem are overstated - the approach still requires users to articulate their preferences as criteria, which may be cognitively demanding

## Next Checks

1. **Independent criteria extraction:** Have humans rate the quality of GPT-4o-extracted criteria on a subset of samples to verify they accurately capture user preferences
2. **Zero-shot criterion testing:** Evaluate CRM performance on entirely new criteria never seen during training to test true generalization capability
3. **User study:** Conduct a small-scale user study where participants specify their own criteria and compare USL rankings against their subjective preferences