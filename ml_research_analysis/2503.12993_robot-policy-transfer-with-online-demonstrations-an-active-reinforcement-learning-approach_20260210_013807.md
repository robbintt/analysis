---
ver: rpa2
title: 'Robot Policy Transfer with Online Demonstrations: An Active Reinforcement
  Learning Approach'
arxiv_id: '2503.12993'
source_url: https://arxiv.org/abs/2503.12993
tags:
- policy
- transfer
- demonstrations
- learning
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Policy Transfer with Online Demonstrations,
  an active learning from demonstrations (LfD) algorithm for robot policy transfer
  that leverages online episodic expert demonstrations during training to address
  covariance shift issues inherent in offline demonstrations. The method extends the
  EARLY framework to policy transfer, using uncertainty-based queries to optimize
  both the timing and content of demonstration requests under a limited budget.
---

# Robot Policy Transfer with Online Demonstrations: An Active Reinforcement Learning Approach

## Quick Facts
- arXiv ID: 2503.12993
- Source URL: https://arxiv.org/abs/2503.12993
- Reference count: 39
- Primary result: Active learning from demonstrations (LfD) algorithm using online episodic expert demonstrations achieves up to 100% success rates and improved sample efficiency compared to offline baselines

## Executive Summary
This paper presents Policy Transfer with Online Demonstrations (POTOD), an active learning from demonstrations algorithm for robot policy transfer that addresses covariance shift issues inherent in offline demonstrations. The method extends the EARLY framework to policy transfer by leveraging online episodic expert demonstrations during training, using uncertainty-based queries to optimize both timing and content of demonstration requests under a limited budget. Evaluated across eight robotic scenarios with diverse environment characteristics, task objectives, and robot embodiments, the approach achieves significantly higher success rates (up to 100% in six scenarios) and improved sample efficiency compared to baselines including AWAC and EARLY with offline demonstrations. Preliminary sim-to-real experiments on a Franka Emika robot manipulator validate the effectiveness of transferred policies in real-world settings.

## Method Summary
The method initializes the target policy and Q-function with source task weights, then executes rollouts in the target environment while computing trajectory uncertainty using temporal difference (TD) error. When uncertainty exceeds an adaptive threshold, the system queries expert demonstrations starting from the initial state of the highest-uncertainty recent rollout. The algorithm uses Advantage Weighted Actor-Critic (AWAC) updates with balanced sampling from both demonstration and rollout buffers. The approach assumes shared state-action spaces between source and target tasks, and requires an environment reset function to enable online demonstrations.

## Key Results
- Achieved up to 100% success rates in six out of eight robotic transfer scenarios
- Demonstrated improved sample efficiency compared to offline demonstration baselines
- Validated sim-to-real transfer effectiveness on Franka Emika robot manipulator
- Outperformed AWAC and EARLY with offline demonstrations across diverse task objectives and robot embodiments

## Why This Works (Mechanism)

### Mechanism 1: Online Demonstrations Resolve Covariance Shift
- **Claim:** Requesting demonstrations during the learning process resolves state distribution mismatch that occurs with static, pre-collected data
- **Mechanism:** Expert demonstrations are collected from states visited by the current target policy rather than states drawn from a static source distribution, aligning training data with policy trajectory distribution
- **Core assumption:** Covariance shift - where state distribution of offline demonstrations differs from robot's live policy rollouts - is a primary bottleneck in policy transfer
- **Evidence anchors:** Abstract states offline demonstrations "may suffer from the intrinsic issue of covariance shift" while online demonstrations "can effectively alleviate covariance shift"; paper notes information about learning process is only available iteratively
- **Break condition:** Fails if expert demonstrator cannot reset to specific queried state or sufficiently close state

### Mechanism 2: TD Error-Based Uncertainty Queries
- **Claim:** Episodic uncertainty measured via Temporal Difference error serves as reliable signal for determining when and what to query
- **Mechanism:** Trajectory-based uncertainty score calculated using absolute difference between predicted Q-values and realized returns (TD error); adaptive threshold created from shifting history; queries triggered when uncertainty exceeds threshold for initial state of highest-uncertainty recent rollout
- **Core assumption:** High TD error correlates with policy confusion or states poorly covered by current value function estimate, benefiting most from expert intervention
- **Evidence anchors:** Algorithm 1 details adaptive thresholding and query selection; uncertainty defined as absolute difference between Q-values and returns
- **Break condition:** Degrades if reward signal is too sparse or noisy to provide meaningful TD error gradient

### Mechanism 3: AWAC-Based Weight Transfer
- **Claim:** Transferring actor-critic weights and applying Advantage Weighted Actor-Critic allows efficient integration of new demonstrations without forgetting source dynamics
- **Mechanism:** Target policy and Q-function initialized with source task weights; updates use AWAC which biases policy toward actions with high estimated advantage, blending prior source knowledge with new target constraints
- **Core assumption:** Source and target tasks share sufficient structural similarity that initializing with source weights provides functional "prior" rather than local minimum trap
- **Evidence anchors:** AWAC updates described as biasing policy to favor actions with larger estimated action-values; assumes shared neural network structure between source and target policies
- **Break condition:** Fails if source and target require fundamentally different action representations despite sharing dimensions

## Foundational Learning

- **Concept: Markov Decision Processes (MDPs) & Transfer Learning**
  - **Why needed here:** Method frames policy transfer as moving policy from Source MDP to Target MDP with different dynamics or rewards
  - **Quick check question:** If state space dimensions changed (e.g., adding sensor), would this algorithm still apply directly? (Answer: No, assumes shared state/action spaces)

- **Concept: Covariance Shift (in Imitation Learning)**
  - **Why needed here:** Core failure mode paper claims to solve - small policy errors compound into different state distributions than seen in training data
  - **Quick check question:** Why is demonstration collected at t=0 potentially useless for training policy at t=100?

- **Concept: Off-Policy RL & Replay Buffers**
  - **Why needed here:** System mixes two data distributions - self-generated rollouts and expert demonstrations - understanding off-policy sampling is critical
  - **Quick check question:** How does "balanced-sampling strategy" differ from standard uniform sampling from replay buffer?

## Architecture Onboarding

- **Component map:** Source Checkpoint -> Target Actor-Critic -> Buffers (Demo_Buffer, Rollout_Buffer) -> Query Module -> Environment
- **Critical path:** 1) Execute target policy in target env → get trajectory 2) Compute uncertainty using Q-network 3) If uncertainty > threshold AND budget remains → Request Demo for initial state 4) Store new data; perform AWAC update
- **Design tradeoffs:** Query Budget (Nd=10/20) - too low fails to recover, too high wastes effort; Shifting History (Nh=20) - longer smooths threshold but slower to react; Distance Metric - Euclidean selection may be poor in high-dimensional spaces
- **Failure signatures:** Threshold Saturation (consistently high uncertainty drains budget), Stagnation (consistently low uncertainty but task unsolved), Reset Failure (cannot reset to queried state)
- **First 3 experiments:** 1) Source Initialization - run source policy on target without learning to quantify gap 2) Threshold Ablation - compare Adaptive vs Fixed Threshold 3) Budget Scaling - run transfer with Nd={0,5,10,20} to map efficiency curve

## Open Questions the Paper Calls Out

- **Open Question 1:** How can performance be maintained when accounting for dynamic human factors like user fatigue or variable demonstration quality during online queries?
  - **Basis:** Authors explicitly state this as future work focus
  - **Why unresolved:** Current setup uses pre-collected successful demonstrations, excluding variability and errors from live human demonstrators
  - **What evidence would resolve:** User studies with live demonstrations analyzing correlation between user cognitive load/fatigue metrics and policy convergence success rates

- **Open Question 2:** Can the method generalize to scenarios where source and target tasks have different state-action spaces or policy network structures?
  - **Basis:** Paper notes limitation to scenarios sharing same state-action space and network structure
  - **Why unresolved:** Algorithm relies on direct weight initialization from source to target, failing if architectures don't align
  - **What evidence would resolve:** Demonstrations transferring policies between heterogeneous agents using latent space alignment or policy distillation

- **Open Question 3:** Is it possible to remove dependency on perfect environment reset function for real-world applications without increasing human burden?
  - **Basis:** Authors acknowledge assuming access to environment reset function constrains real-world applicability
  - **Why unresolved:** Query strategy requests demonstrations from specific initial states, requiring human intervention or specialized controllers in real world
  - **What evidence would resolve:** Successful transfers in unstructured environments without programmatic reset capability

## Limitations
- Requires perfect environment reset function to enable online demonstrations, constraining real-world applicability
- Assumes source and target tasks share same state-action space and policy network structure
- Performance may be sensitive to task selection and demonstration quality given exceptional 100% success claims

## Confidence
- **High Confidence:** Online demonstration framework architecture and AWAC-based learning mechanism are well-specified and technically sound
- **Medium Confidence:** Claims of 100% success rates require careful scrutiny as performance appears exceptional and may be sensitive to task selection
- **Low Confidence:** Assertion of generalization across "diverse environment characteristics" is supported but not rigorously tested across truly diverse domains

## Next Checks
1. **Reset Feasibility Test:** Implement algorithm in simulated environment with artificially constrained expert reset capability to determine minimum reset precision required for method to work
2. **Covariance Shift Quantification:** Design experiment explicitly measuring state distribution divergence between offline demonstrations and online rollouts at different training stages to verify claimed reduction in covariance shift
3. **Query Efficiency Analysis:** Conduct ablation studies varying demonstration budget (Nd) across multiple orders of magnitude to establish relationship between query efficiency and final performance, identifying diminishing returns