---
ver: rpa2
title: 'Overcoming Spurious Solutions in Semi-Dual Neural Optimal Transport: A Smoothing
  Approach for Learning the Optimal Transport Plan'
arxiv_id: '2502.04583'
source_url: https://arxiv.org/abs/2502.04583
tags:
- transport
- optimal
- neural
- plan
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the convergence problem in learning the Optimal
  Transport (OT) map using Semi-dual Neural OT approaches. These methods often generate
  spurious solutions that fail to accurately transfer one distribution to another.
---

# Overcoming Spurious Solutions in Semi-Dual Neural Optimal Transport: A Smoothing Approach for Learning the Optimal Transport Plan

## Quick Facts
- arXiv ID: 2502.04583
- Source URL: https://arxiv.org/abs/2502.04583
- Reference count: 39
- This paper proposes a novel method (OTP) that uses source distribution smoothing to eliminate spurious solutions in Semi-dual Neural OT approaches, achieving better performance in image-to-image translation tasks.

## Executive Summary
This paper addresses a fundamental problem in learning optimal transport maps using semi-dual neural approaches: the existence of spurious solutions that fail to accurately transfer one distribution to another. The authors identify a sufficient condition on the source distribution (not placing positive mass on sets of Hausdorff dimension ≤ d−1) and propose a novel method called OTP that learns both the OT map and plan by smoothing the source distribution. Under sharp assumptions, OTP is proven to eliminate spurious solutions and correctly solve the OT problem. Experiments show OTP recovers the optimal transport map where existing methods fail and outperforms current OT-based models in image-to-image translation tasks, including learning stochastic transport maps for one-to-many tasks like colorization.

## Method Summary
The OTP method works by smoothing the source distribution μ with Gaussian noise to create μ_ε, then gradually decreasing the noise level during training while learning the optimal transport plan. The algorithm alternates between updating the potential network V_φ (ICNN) and transport network T_θ using the semi-dual formulation. The noise scheduler controls the smoothing level ε_k, which is decreased throughout training according to a linear or variance-preserving schedule. The key insight is that smoothing eliminates spurious solutions by ensuring the source distribution satisfies a measure concentration condition, while gradual annealing preserves convergence to the correct OT plan via weak continuity of optimal transport.

## Key Results
- OTP successfully eliminates spurious solutions in 2D synthetic experiments where other methods fail, recovering the correct transport map
- Achieves state-of-the-art performance in image-to-image translation tasks (MNIST→CMNIST, CelebA Male→Female, AFHQ Wild→Cat) with lower FID scores
- Can learn stochastic transport maps for one-to-many tasks like colorization, where deterministic maps don't exist
- The method is robust to noise schedule parameters and maintains performance even with minimal residual noise

## Why This Works (Mechanism)

### Mechanism 1: Source Distribution Smoothing
- Claim: Smoothing the source distribution eliminates spurious solutions by enforcing a sufficient condition on measure concentration.
- Mechanism: Theorem 3.1 proves that when source distribution μ doesn't place positive mass on measurable sets of Hausdorff dimension ≤ d−1, the T-parametrization becomes uniquely determined μ-almost surely. Gaussian convolution makes μ_ε absolutely continuous w.r.t. Lebesgue measure, automatically satisfying this condition.
- Core assumption: Cost function satisfies regularity conditions; weak convergence of smoothed measures to original.
- Evidence anchors: Abstract statement about Hausdorff dimension condition; Theorem 3.1 proof in section 3.1.
- Break condition: If noise scheduling decreases too rapidly or stops at ε_min too large, convergence may fail or produce biased estimates.

### Mechanism 2: Gradual Noise Annealing
- Claim: Gradual noise annealing preserves convergence to the correct OT plan via weak continuity of optimal transport.
- Mechanism: Theorem 4.1 establishes that if μ_ε_k weakly converges to μ as k→∞, then the OT plans π*_k converge (subsequentially) to π*. The algorithm decreases ε_k progressively, allowing the learned transport map to track the changing optimal solution continuously.
- Core assumption: Optimal transport cost remains finite across the sequence; smoothing sequence satisfies absolute continuity and weak convergence.
- Evidence anchors: Abstract description of smoothing and gradual adjustment; section 4 proof of convergence theorem.
- Break condition: If inner-loop optimization (K_T iterations) is insufficient for each noise level, the map may not track the optimal trajectory.

### Mechanism 3: Stochastic Transport Maps
- Claim: Stochastic transport maps emerge naturally from the OT plan formulation when deterministic maps don't exist.
- Mechanism: When only the OT plan π* exists (not a deterministic Monge map T*), the conditional distribution π(y|x) captures one-to-many mappings. OTP's noise injection at test time samples from this conditional, enabling stochastic outputs for tasks like colorization.
- Core assumption: Target distribution's support structure admits multiple valid transport targets per source point; noise variable provides sufficient entropy to cover modes.
- Evidence anchors: Abstract mention of stochastic transport for colorization; section 3.2.2 Example 3 demonstrating failure of deterministic SNOT.
- Break condition: If noise dimension or magnitude is insufficient, mode collapse occurs.

## Foundational Learning

- Concept: **Semi-dual formulation of optimal transport**
  - Why needed here: The entire SNOT framework is built on Eq. 5's max-min formulation. Understanding how V (potential) and T (transport) interact via the c-transform is essential.
  - Quick check question: Can you explain why Eq. 7 (T-parametrization) is the computational realization of the c-transform in Eq. 6?

- Concept: **Hausdorff dimension and measure concentration**
  - Why needed here: The sufficient condition (Thm 3.1) hinges on whether μ charges lower-dimensional sets. Intuitively, distributions concentrated on lines/planes in ℝ^d violate this.
  - Quick check question: Does a uniform distribution on a line segment in ℝ² satisfy the condition? What about a Gaussian in ℝ²?

- Concept: **Pushforward and transport plan notation**
  - Why needed here: Distinguishing T#μ (deterministic pushforward) from π(y|x) (stochastic coupling) is critical for understanding when Monge vs. Kantorovich formulations apply.
  - Quick check question: If T(x) = (x₁, 1) for x = (x₁, 0) ∈ X, what is T#μ when μ is uniform on [-1,1]×{0}?

## Architecture Onboarding

- Component map:
  - V_φ (Potential network) -> T_θ (Transport network) -> Loss computation
  - Noise scheduler -> Sample generation -> Training loop
  - Outer loop (ε_k decay) -> Inner loop (K_T T_θ updates)

- Critical path:
  1. Sample x ~ μ, y ~ ν, z ~ N(0,I)
  2. Apply noise: x̃ ← x + √ε_k z
  3. Update V_φ to maximize L_φ = -V_φ(T_θ(x̃)) + V_φ(y)
  4. Inner loop: Update T_θ to minimize L_θ = c(x̃, T_θ(x̃)) - V_φ(T_θ(x̃)) (repeat K_T times)
  5. Decrease ε_k per schedule

- Design tradeoffs:
  - ε_min selection: Lower values reduce bias but risk returning to spurious solutions; paper uses 0.05-0.5 depending on task
  - K_T (inner iterations): More iterations improve T optimization but slow training; paper uses 10-20
  - Variance-preserving vs. additive noise: VP preserves sample scale but may under-smooth; additive is simpler but changes distribution scale

- Failure signatures:
  - Mode collapse: Generated samples cover only subset of target distribution (see Fig. 6a)
  - Non-convergence: D_target plateaus at high values; check if ε_k decays too fast
  - Gradient instability: ICNN constraints violated; verify weight non-negativity in potential network

- First 3 experiments:
  1. **2D Perpendicular test**: Uniform source on [-1,1]×{0}, target on {0}×[-1,1}. Verify OTP covers target while OTM collapses. Measure D_target.
  2. **One-to-Many synthetic**: Source on line, target on two parallel lines. Confirm stochastic mapping via visual inspection and FID.
  3. **MNIST→CMNIST colorization**: Grayscale to colored digits. Check that all three color modes are covered (not just blue collapse as in OTM-s).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the necessary and sufficient conditions for the max-min solution of Semi-dual Neural OT to recover the true OT Map?
- Basis in paper: The conclusion states, "Our analysis provides a sufficient condition, rather than a necessary and sufficient one (Thm. 3.1), leaving room for further refinement in understanding the exact conditions under which spurious solutions occur."
- Why unresolved: Theorem 3.1 establishes that the source distribution not placing mass on sets of Hausdorff dimension $\le d-1$ is sufficient to prevent spurious solutions, but the authors do not prove if this condition is strictly required.
- What evidence would resolve it: A theoretical proof identifying a stricter or different set of conditions that are both necessary and sufficient, or a counter-example showing the current condition is not necessary.

### Open Question 2
- Question: Can the theoretical convergence guarantee of the OTP model be strengthened from subsequence convergence to full sequence convergence?
- Basis in paper: The conclusion notes, "One limitation of our work is that our convergence theorem holds up to a subsequence (Thm. 4.1)."
- Why unresolved: Theorem 4.1 relies on weak convergence properties that guarantee an optimal plan exists along a subsequence, but it does not guarantee the entire training trajectory converges without oscillating or diverging at specific steps.
- What evidence would resolve it: A modified proof of Theorem 4.1 showing that $\pi_k^\star$ converges weakly to $\pi^\star$ for the full sequence $k \to \infty$, potentially requiring additional assumptions on the noise schedule or objective function smoothness.

### Open Question 3
- Question: How does the choice of smoothing kernel in the OTP algorithm interact with non-quadratic cost functions?
- Basis in paper: The main text and Theorem 3.1 focus on the quadratic cost $c(x,y)=\alpha\|x-y\|^2$. The OTP method uses Gaussian convolution for smoothing (Eq. 21), which pairs naturally with Euclidean geometry, but Appendix A.1 introduces generalized cost conditions without specifying the smoothing mechanism for them.
- Why unresolved: It is unclear if Gaussian convolution is the optimal or even correct smoothing approach for costs that do not respect Euclidean geometry (e.g., strictly convex non-quadratic costs), potentially limiting the generalization of the OTP method.
- What evidence would resolve it: Experiments or theory demonstrating that the specific smoothing kernel must be adapted to the cost function's geometry to maintain convergence guarantees for non-quadratic costs.

## Limitations
- Theoretical guarantees rely heavily on specific distributional assumptions (Hausdorff dimension conditions) that may not hold for real-world data
- Empirical evaluation focuses primarily on image-to-image translation, with limited validation on other domains where OT maps are critical
- Several implementation details for the image experiments (ICNN structures, exact noise schedules) remain underspecified

## Confidence
- **High confidence**: The core mechanism of noise-induced smoothing preventing spurious solutions in controlled 2D synthetic settings, supported by both theory (Thm 3.1) and experiments
- **Medium confidence**: Generalization to complex image domains, as improvements are demonstrated but could be influenced by other factors like network architectures or training procedures
- **Low confidence**: The sufficiency of gradual annealing for tracking optimal solutions across all problem regimes, as this mechanism is proposed but not rigorously validated across diverse scenarios

## Next Checks
1. Test OTP on synthetic distributions with varying Hausdorff dimensions to empirically validate the theoretical threshold for spurious solution prevention
2. Compare OTP against established methods (e.g., W2GN, Scored-based approaches) on non-image domains like tabular data or point clouds where OT maps are commonly used
3. Conduct ablation studies varying ε_min and annealing schedules to quantify robustness to noise schedule parameters and identify failure modes