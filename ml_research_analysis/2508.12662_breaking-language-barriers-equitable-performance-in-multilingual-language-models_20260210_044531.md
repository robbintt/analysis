---
ver: rpa2
title: 'Breaking Language Barriers: Equitable Performance in Multilingual Language
  Models'
arxiv_id: '2508.12662'
source_url: https://arxiv.org/abs/2508.12662
tags:
- language
- arxiv
- performance
- code-switched
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the performance gap between high-resource
  languages (HRLs) like English and low-resource languages (LRLs) like Hindi in multilingual
  language models on commonsense reasoning tasks. To address this, the authors propose
  fine-tuning LLMs on synthetic code-switched datasets, generated using controlled
  language-mixing methods.
---

# Breaking Language Barriers: Equitable Performance in Multilingual Language Models

## Quick Facts
- arXiv ID: 2508.12662
- Source URL: https://arxiv.org/abs/2508.12662
- Reference count: 9
- Primary result: Code-switched fine-tuning significantly improves low-resource language performance while maintaining high-resource language capabilities

## Executive Summary
This paper addresses the performance gap between high-resource languages (HRLs) like English and low-resource languages (LRLs) like Hindi in multilingual language models on commonsense reasoning tasks. The authors propose fine-tuning large language models on synthetic code-switched datasets with controlled language-mixing ratios. Empirical results show substantial improvements in LRL performance (up to 85.6% accuracy for Hindi) while preserving or enhancing HRL performance (up to 90.4% for English), with the medium Code-Mixing Index (CMI) configuration yielding optimal results. This approach offers a promising path to more equitable multilingual language model performance.

## Method Summary
The authors fine-tune LLaMA-3-8B-Instruct using QLoRA on synthetic code-switched datasets derived from CommonSenseQA. They generate three variants with controlled CMI ratios (low: 0–16.7%, medium: 16.7–30%, high: 30–50%) using CoCoa for controlled mixing and GPT-3.5 for comparison. Answers remain in English while questions are code-switched. The models are evaluated on both English and Hindi test sets using 5-fold cross-validation with majority voting over 5 inference runs per question.

## Key Results
- Medium CMI configuration (16.7–30%) achieved best performance: 90.40% English accuracy, 85.60% Hindi accuracy
- Code-switched fine-tuning substantially improves LRL performance while preserving HRL capabilities
- GPT-3.5-generated datasets showed inconsistent CMI ratios and lower performance compared to CoCoa-generated datasets
- Cross-validation showed moderate variance (up to 16.16% std dev), indicating sensitivity to data partitioning

## Why This Works (Mechanism)

### Mechanism 1: Cross-Linguistic Transfer via Shared Representations
Code-switching forces models to process both languages within the same context window, encouraging shared semantic representations rather than language-specific ones. This aligns with how bilingual humans process languages through similar representations.

### Mechanism 2: Optimal Language-Mixing Ratio (CMI Sweet Spot)
The medium CMI configuration (16.7–30%) outperformed both low and high mixing ratios by providing an optimal balance that enables effective cross-linguistic transfer and representation learning.

### Mechanism 3: Mitigation of Catastrophic Forgetting
Code-switched fine-tuning maintains HRL tokens in training data, avoiding the performance degradation in HRLs that typically occurs when fine-tuning exclusively on LRL data.

## Foundational Learning

**Code-Mixing Index (CMI)**
- Why needed: Quantifies degree of language mixing to enable controlled experiments on mixing ratios
- Quick check: Given 10 words with 4 Hindi, 4 English, and 2 language-independent tokens, what is CMI? (Answer: 50%)

**Catastrophic Forgetting in Multilingual Models**
- Why needed: Explains why LRL-only fine-tuning degrades HRL performance
- Quick check: Why might fine-tuning only on Hindi cause English performance to drop? (Answer: Finite model capacity; Hindi representations may overwrite English representations)

**Synthetic Data Generation for Low-Resource Languages**
- Why needed: Natural code-switched datasets are scarce for LRLs
- Quick check: What are two risks of using synthetically generated code-switched data? (Answer: (1) Biases in generation models propagate to fine-tuned models; (2) Synthetic text may not capture natural code-switching nuances)

## Architecture Onboarding

**Component map:** CommonSenseQA → Code-switched generation (CoCoa/GPT-3.5) → Fine-tuning (LLaMA-3-8B-Instruct + QLoRA) → Evaluation

**Critical path:** Transform English questions → code-switched Hindi-English (answers in English) → generate three CMI variants → fine-tune separate models → evaluate on both English and Hindi test sets

**Design tradeoffs:**
- Answer language: English answers leverage strong semantics but may not transfer to fully code-switched scenarios
- CMI control method: CoCoa offers precise control but may not scale to long sentences; GPT-3.5 generates more naturally but with inconsistent ratios
- Model size: 8B model chosen for research accessibility; scaling effects unknown

**Failure signatures:**
- High variance across folds (std dev up to 16.16%) suggests sensitivity to data partitioning
- GPT-3.5 failed to maintain specified language ratios, producing skewed outputs
- CMI >50% flips dominant/matrix language roles, duplicating lower-CMI scenarios

**First 3 experiments:**
1. Baseline measurement: Evaluate base LLaMA-3-8B-Instruct on both English and Hindi CommonSenseQA translations
2. CMI sweep: Fine-tune separate models on low, medium, and high CMI datasets; compare HRL and LRL accuracy
3. Ablation on answer language: Fine-tune with fully code-switched questions AND answers vs. questions only

## Open Questions the Paper Calls Out

**Generalization to other language pairs:** Does the optimal medium CMI configuration generalize to low-resource languages outside the Hindi-English pair?

**Full code-switching impact:** Does fully code-switching both questions and answer choices improve cross-lingual transfer compared to code-switching only questions?

**Benchmark against translation:** Is fine-tuning on synthetic code-switched data more effective than fine-tuning on fully translated monolingual datasets?

**Architecture and benchmark generalization:** Do these findings generalize to other model architectures and commonsense reasoning benchmarks?

## Limitations

- Results are based on synthetic code-switched data from a single source (CommonSenseQA), raising questions about generalization to natural code-switching patterns
- Findings are specific to Hindi-English pair; optimal CMI ratios and effectiveness may vary for other language pairs with different typological characteristics
- Proposed mechanisms explaining improvements remain largely hypothetical without direct empirical validation through representation analysis or controlled ablation studies

## Confidence

**High Confidence:** Empirical finding that code-switched fine-tuning improves LRL performance while maintaining HRL performance is well-supported by experimental results.

**Medium Confidence:** Claim about optimal CMI ratio (16.7–30%) is supported by data but requires further validation across datasets and language pairs.

**Low Confidence:** Proposed mechanisms (cross-linguistic representation alignment, optimal mixing ratios, forgetting mitigation) remain largely hypothetical without direct evidence.

## Next Checks

1. Apply code-switched fine-tuning to at least two additional commonsense reasoning datasets to validate generalization beyond CommonSenseQA.

2. Repeat experiments with Hindi-English and two additional language pairs (one similar typologically, one substantially different) to test if findings are language-pair specific.

3. Design controlled experiments isolating specific mechanisms: test necessity of English answers, robustness of medium CMI finding, and compare against continued English exposure baseline.