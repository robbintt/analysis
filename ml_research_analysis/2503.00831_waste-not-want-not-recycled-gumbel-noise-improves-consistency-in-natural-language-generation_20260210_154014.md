---
ver: rpa2
title: Waste Not, Want Not; Recycled Gumbel Noise Improves Consistency in Natural
  Language Generation
arxiv_id: '2503.00831'
source_url: https://arxiv.org/abs/2503.00831
tags:
- responses
- gumbel
- sampling
- language
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Gumbel Consistency Sampling (GCS), a novel
  decoding algorithm that improves response consistency across different prompts for
  language models without degrading response quality. The method achieves this by
  incorporating a latent variable based on the Gumbel reparametrization trick into
  the next-token sampling process, creating statistical dependencies between responses
  to similar prompts.
---

# Waste Not, Want Not; Recycled Gumbel Noise Improves Consistency in Natural Language Generation

## Quick Facts
- arXiv ID: 2503.00831
- Source URL: https://arxiv.org/abs/2503.00831
- Reference count: 40
- Primary result: GCSwR improves response consistency across semantically similar prompts without degrading quality, achieving >10% improvement in semantic similarity metrics

## Executive Summary
This paper introduces Gumbel Consistency Sampling (GCS), a novel decoding algorithm that improves response consistency across different prompts for language models without degrading response quality. The method achieves this by incorporating a latent variable based on the Gumbel reparametrization trick into the next-token sampling process, creating statistical dependencies between responses to similar prompts. The approach can be applied to any model without additional training and adds negligible computational overhead.

The primary results show that GCS with recycling (GCSwR) outperforms standard sampling across multiple benchmarks covering semantic and stylistic consistency. On semantic similarity tasks using the E5 mistral-7b model, GCSwR improves consistency by more than 10% compared to baseline sampling across all tested models (Llama2, Mistral, Llama3). The method also significantly enhances stylistic consistency across dimensions like programming language choice, use of code snippets, bullet points, and terseness.

## Method Summary
The method introduces Gumbel Consistency Sampling (GCS) that shares a latent Gumbel noise vector across different prompts, creating statistical dependencies between responses to similar prompts. The key innovation is Gumbel Consistency Sampling with Recycling (GCSwR), which preserves the model's original marginal probability distribution while recycling noise across sequence positions through quantile transformation. This is achieved by adding pre-sampled Gumbel noise to log-probabilities before taking the argmax, then updating the noise vector after each token selection using a specific quantile function. The method requires no additional training and can be applied as an inference-time modification to any language model.

## Key Results
- GCSwR improves semantic consistency by >10% across all tested models (Llama2, Mistral, Llama3) using E5 mistral-7b embeddings
- Stylistic consistency significantly enhanced across programming language choice, code snippet usage, bullet point formatting, and terseness
- Consistency improvements observed across multiple semantic similarity metrics including Jaccard similarity and all-mpnet-base-v2 embeddings
- Method adds negligible computational overhead while achieving substantial consistency gains

## Why This Works (Mechanism)

### Mechanism 1: Latent Variable Coupling via Shared Noise
- **Claim:** Sharing a latent Gumbel noise vector across different prompts creates statistical dependencies between responses, increasing the probability of selecting identical tokens from overlapping probability distributions.
- **Mechanism:** The method replaces independent sampling with a shared latent variable $g$. For two prompts yielding distributions $p$ and $q$, the same noise vector $g$ is added to log-probabilities before taking the argmax. Theorem 4.1 proves this maximizes the probability $P(Y=k, V=k)$ compared to independent sampling.
- **Core assumption:** The prompts are semantically similar enough that their next-token probability distributions ($p$ and $q$) share high-probability tokens; otherwise, shared noise cannot force alignment.
- **Evidence anchors:**
  - [abstract] "incorporating a latent variable... creating statistical dependencies between responses."
  - [Section 4] "Theorem 4.1... increases the probability of selecting the same category... compared to sampling... independently."
  - [corpus] Corpus evidence for this specific decoding mechanism is weak; neighbor papers focus on reliability via voting ("Self-Consistency") rather than latent variable coupling.
- **Break condition:** If distributions $p$ and $q$ have disjoint supports (no common high-probability tokens), consistency gains will not materialize.

### Mechanism 2: Noise Recycling for Marginal Preservation
- **Claim:** Gumbel Consistency Sampling with Recycling (GCSwR) allows the reuse of noise across sequence positions while strictly preserving the model's original marginal probability distribution $p(Y_t|X)$.
- **Mechanism:** After sampling a token $k$, the noise vector is updated via a quantile transformation (Theorem 4.2). The winning token gets fresh noise, while losing tokens have their noise rescaled based on the probability ratio. This ensures the marginal distribution matches the standard categorical distribution, $Cat(Y_{t+1}; h_{t+1})$, satisfying the auto-regressive factorization requirements.
- **Core assumption:** The numerical precision of the hardware (e.g., bfloat16) is sufficient to maintain the strict equality required by the quantile rescaling without introducing drift.
- **Evidence anchors:**
  - [Section 4] "Theorem 4.2... functionally equivalent to independently sampling... indistinguishable from those obtained by independent sampling."
  - [Section 6] "GCSwR improves the semantic consistency... across all temperatures, except temperature 0."
  - [corpus] Weak relevance; neighbor papers do not discuss Gumbel reparameterization for sequence generation.
- **Break condition:** Numerical underflow/overflow in the quantile function $Q(\cdot)$ during the rescaling step for tokens with extremely low probabilities.

### Mechanism 3: Distributional Ensembling (Geometric Mean)
- **Claim:** Sampling from an ensembled distribution of paraphrased prompts reduces the impact of semantically irrelevant prompt variations on the next-token distribution.
- **Mechanism:** The system generates $n$ paraphrases of the input, runs forward passes to get probability distributions $\{P_i\}$, and samples from a normalized geometric mean $Q = \frac{1}{Z} \prod P_i^{1/n}$. This minimizes the average forward-KL divergence to the set of possible distributions.
- **Core assumption:** The paraphrasing model accurately preserves semantic intent while varying syntax/style.
- **Evidence anchors:**
  - [Section 5] "sampling tokens from an ensembled probability distribution... minimiz[es] impact of semantically irrelevant prompt variations."
  - [Section 6] "GCSwR with ensembling significantly increases response similarity... by more than 10%."
  - [corpus] "Reliability-Aware Adaptive Self-Consistency" addresses reliability via sampling budgets, offering tangential support for the goal but not the method.
- **Break condition:** Paraphraser introduces semantic drift, causing the ensembled distribution $Q$ to represent an average of different meanings rather than a robust representation of the original meaning.

## Foundational Learning

- **Concept: Gumbel-Max Trick**
  - **Why needed here:** This is the fundamental primitive. The entire method relies on the identity that sampling from a categorical distribution $p$ is equivalent to $\text{argmax}_i (\log p_i + g_i)$ where $g_i \sim \text{Gumbel}(0,1)$.
  - **Quick check question:** If you add a constant $C$ to all Gumbel noise values $g_i$, does the sampled token change?

- **Concept: Joint vs. Marginal Distributions**
  - **Why needed here:** The paper's "free lunch" claim rests on modifying the *joint* distribution $p(Y, V)$ (coupling responses) while keeping *marginals* $p(Y)$ (quality) identical. Understanding this distinction is critical to see why quality isn't degraded.
  - **Quick check question:** Can two random variables $Y$ and $V$ be dependent but still have the exact same individual probability distributions as if they were independent?

- **Concept: Forward vs. Reverse KL Divergence**
  - **Why needed here:** The ensembling method uses the geometric mean, which minimizes *forward* KL. The paper notes that direct averaging (reverse KL) generates lower quality responses. Understanding mode-seeking vs. mass-covering behavior explains this design choice.
  - **Quick check question:** Why does minimizing Reverse KL tend to "mode-collapse" (selecting one high-probability region) while Forward KL tends to cover the support?

## Architecture Onboarding

- **Component map:** Noise Store -> Recycling Engine -> Aggregator (for ensembling) -> Language Model
- **Critical path:** The `recycle_gumbel` function (Algorithm 1, lines 7-9). This update must happen atomically after every token generation step. If the rescaling math is incorrect or suffers numerical instability, the marginal distribution guarantee voids, potentially degrading response quality.
- **Design tradeoffs:**
  - **Consistency vs. Diversity:** Increasing consistency necessarily reduces diversity. This method is unsuitable for creative generation where varied outputs are desired.
  - **Latency vs. Consistency:** The ensembling approach adds significant inference overhead ($N$ forward passes per step), whereas pure GCSwR adds negligible overhead.
  - **Temperature:** The method behaves identically to standard sampling at Temperature 0 (greedy).
- **Failure signatures:**
  - **Quality Degradation:** If responses become repetitive or incoherent, the Recycling Engine (Theorem 4.2) may be implemented incorrectly, failing to preserve marginals.
  - **No Consistency Gain:** If semantic similarity scores don't improve, verify that the *same* Gumbel seed/noise store is being reused for the batch of paraphrased prompts.
  - **Numerical Drift:** Watch for `NaN` or `Inf` in the noise store, particularly with very small probabilities or low precision (bfloat16).
- **First 3 experiments:**
  1. **Sanity Check (Marginal Preservation):** Generate 10k samples from a fixed distribution using both Standard Sampling and GCSwR. Verify the frequency counts of tokens are statistically indistinguishable (chi-squared test).
  2. **Coupling Verification:** Run GCSwR on two identical prompts. Confirm outputs are 100% identical. Run on two prompts with slightly different distributions. Measure token overlap vs. independent sampling.
  3. **Stylistic Consistency Benchmark:** Implement the "Aleatoric-List" test (Section 6.3). Generate responses to list-based questions using a shared seed and verify if formatting (bullets vs. numbers) is consistent across questions.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can treating the Gumbel noise as a learnable, task-specific parameter enhance model safeguards (such as safety or alignment) while preserving general utility?
- **Basis in paper:** [explicit] Conclusion section.
- **Why unresolved:** The current approach uses fixed random noise to enforce consistency; the authors suggest optimizing this noise for specific constraints is a promising but untested direction.
- **What evidence would resolve it:** Experiments demonstrating that optimized noise vectors reduce harmful outputs or policy violations more effectively than fixed noise, without degrading benchmark performance.

### Open Question 2
- **Question:** Can the Gumbel consistency framework be extended to impose local rather than global correlations to allow for finer-grained control and increased response diversity?
- **Basis in paper:** [explicit] Conclusion section.
- **Why unresolved:** Currently, all responses are globally coupled via a single shared latent variable, which restricts the ability to make localized adjustments and inherently reduces diversity.
- **What evidence would resolve it:** A modified algorithm that varies the latent variable based on prompt specifics, showing higher diversity scores (e.g., distinct n-grams) while maintaining consistency metrics comparable to GCSwR.

### Open Question 3
- **Question:** Does Gumbel Consistency Sampling improve factual consistency or correctness in reasoning tasks, rather than just semantic and stylistic similarity?
- **Basis in paper:** [inferred] The paper evaluates semantic and stylistic consistency, but explicitly notes in Related Works that prior literature focused on factual consistency, implying this dimension remains untested for GCS.
- **Why unresolved:** High semantic similarity scores between responses do not guarantee that the responses are factually consistent or logically equivalent in reasoning chains.
- **What evidence would resolve it:** Evaluation on benchmarks like TruthfulQA or GSM8K to measure if GCSwR increases the probability that paraphrased prompts yield the same *correct* answer, rather than just similar text.

### Open Question 4
- **Question:** How does Gumbel Consistency Sampling interact with other advanced decoding methods like Mirostat or Contrastive Search?
- **Basis in paper:** [inferred] The authors describe their method as complementary to approaches that modify probability distributions (like nucleus sampling), but only test standard sampling.
- **Why unresolved:** It is unclear if the "recycling" of Gumbel noise interferes with the dynamic thresholding or penalty mechanisms used in other sophisticated decoding strategies.
- **What evidence would resolve it:** Experiments combining GCSwR with Mirostat or Contrastive Search to determine if consistency gains persist without destabilizing the perplexity or quality controls of those methods.

## Limitations
- **Theoretical Generalization Gap:** The extension to sequence generation assumes token independence across time steps, which may not hold in practice for morphologically rich languages
- **Paraphrasing Reliability:** The method depends entirely on the paraphraser's ability to preserve semantic intent, with no evaluation of paraphrase quality or potential semantic drift
- **Numerical Precision Constraints:** The quantile transformation requires strict preservation of marginal distributions, and numerical stability for tokens with extremely low probabilities remains unverified

## Confidence

**High Confidence:** The core mechanism of shared Gumbel noise for increasing token-level consistency is theoretically sound and experimentally validated. The marginal preservation guarantee through quantile recycling is mathematically rigorous.

**Medium Confidence:** The semantic consistency improvements across benchmarks are well-documented, but the causal relationship between shared noise and observed improvements could be confounded by other factors like prompt phrasing or model architecture.

**Low Confidence:** The generalizability of GCS to non-chat models (like code generation) and the long-term stability of the recycling mechanism across diverse domains require further validation.

## Next Checks

1. **Marginal Preservation Stress Test:** Generate 100k samples from a fixed distribution using GCSwR with varying temperature settings. Perform chi-squared tests to verify statistical indistinguishability from standard sampling across all token frequencies, including rare tokens.

2. **Cross-Lingual Consistency Evaluation:** Apply GCSwR to multilingual prompts in morphologically rich languages (e.g., Turkish, Finnish) where token dependencies are stronger. Measure if consistency gains persist or degrade compared to English.

3. **Numerical Drift Analysis:** Implement floating-point error tracking during quantile recycling. Monitor the evolution of the Gumbel noise vector over 1000+ tokens to detect any systematic drift that could violate the marginal preservation guarantee.