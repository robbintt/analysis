---
ver: rpa2
title: 'ProteoKnight: Convolution-based phage virion protein classification and uncertainty
  analysis'
arxiv_id: '2508.07345'
source_url: https://arxiv.org/abs/2508.07345
tags:
- classification
- learning
- sequences
- uncertainty
- protein
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of classifying phage virion proteins
  (PVPs) from high-throughput sequencing data, where traditional alignment-based methods
  struggle due to low sequence conservation. To overcome this, the authors propose
  ProteoKnight, a novel image-based encoding method that adapts the DNA-walk algorithm
  for protein sequences, incorporating pixel colors and adjusting walk distances to
  capture intricate protein features.
---

# ProteoKnight: Convolution-based phage virion protein classification and uncertainty analysis

## Quick Facts
- arXiv ID: 2508.07345
- Source URL: https://arxiv.org/abs/2508.07345
- Authors: Samiha Afaf Neha; Abir Ahammed Bhuiyan; Md. Ishrak Khan
- Reference count: 40
- Primary result: 90.8% accuracy in binary classification of phage virion proteins

## Executive Summary
ProteoKnight addresses the challenge of classifying phage virion proteins (PVPs) from high-throughput sequencing data, where traditional alignment-based methods struggle due to low sequence conservation. The authors propose a novel image-based encoding method called "Knight Encoding" that adapts the DNA-walk algorithm for protein sequences, incorporating pixel colors and adjusting walk distances to capture intricate protein features. Encoded sequences are classified using pre-trained convolutional neural networks (CNNs), with prediction uncertainty assessed via Monte Carlo Dropout (MCD). The approach achieves 90.8% accuracy in binary classification, outperforming existing methods, though multi-class accuracy remains suboptimal.

## Method Summary
The method encodes amino acid sequences into 512x512 RGB images using a Knight Encoding approach that maps 20 amino acids to specific angles (18° intervals) and uses polar coordinates with fixed radius to create 2D trajectories. These images are resized to 224x224 and classified using pre-trained GoogleNet CNN. For uncertainty analysis, Monte Carlo Dropout is applied during inference with 100 stochastic forward passes per sample to calculate prediction variance and entropy. The approach is evaluated on a benchmark dataset containing 35,213 PVP and 46,883 non-PVP sequences.

## Key Results
- Binary classification achieves 90.8% accuracy and 88.6% F1-score
- Multi-class classification accuracy remains suboptimal compared to binary task
- Uncertainty analysis reveals variability in prediction confidence influenced by protein class and sequence length
- GoogleNet achieves optimal balance between computational efficiency and classification quality

## Why This Works (Mechanism)

### Mechanism 1
The "Knight Encoding" method preserves local spatial sequence information better than frequency-based encoding (FCGR) by mapping amino acids to specific angles and using fixed radius to create 2D trajectories where adjacent residues are drawn as connected steps, maintaining neighborhood topology.

### Mechanism 2
Pre-trained CNNs can classify biological sequences by treating them as texture-rich images, as the encoding generates colored dot-plots that function as "fingerprints" which CNNs pre-trained on ImageNet can recognize using low-level feature detectors.

### Mechanism 3
Monte Carlo Dropout acts as a proxy for model confidence by measuring prediction variance through multiple stochastic forward passes during inference, where high variance in the prediction distribution indicates epistemic uncertainty.

## Foundational Learning

- **Concept: Random Walk / DNA-Walk**
  - **Why needed here:** The core "Knight Encoding" is an adaptation of a random walk that maps 1D strings to 2D coordinates based on amino acid values
  - **Quick check question:** If amino acid 'C' has angle 18° and radius 15, and the current position is (100, 100), what is the new coordinate?

- **Concept: Transfer Learning in Vision**
  - **Why needed here:** The paper relies on GoogleNet/EfficientNet pre-trained on ImageNet, requiring understanding of why features learned on natural images transfer to biological dot-plots
  - **Quick check question:** Why does the paper resize input images to 224x224 before feeding them into the CNN?

- **Concept: Bayesian Approximation via Dropout**
  - **Why needed here:** To interpret the "Uncertainty Analysis" section, you must grasp how dropping neurons randomly during testing acts as an ensemble method
  - **Quick check question:** In MCD, does a "narrow" distribution of predictions (low variance) imply high or low confidence?

## Architecture Onboarding

- **Component map:** Input: Raw Amino Acid Sequence (String) -> Encoder (Knight): Sequence → Angle/Color Lookup → Polar to Cartesian → Canvas -> Preprocessing: Resize image to 224x224 -> Backbone: GoogleNet with modified final layer -> Uncertainty Head: Softmax output looped 100 times with Dropout active
- **Critical path:** The Encoding Radius (r=15) and Canvas Size determine if the "walk" fits on the screen without folding over itself too much
- **Design tradeoffs:** GoogleNet vs. EfficientNet chosen for computational economy (5.5M vs ~20M params) over raw accuracy
- **Failure signatures:** Point Overlap from long sequences causing indistinguishable "blobs" where trajectory is lost; High Variance on PVPs indicating model bias
- **First 3 experiments:**
  1. Generate images for "Short PVP" (<350 aa) and "Long PVP" (>350 aa) to visually inspect for trajectory overlap
  2. Train GoogleNet encoder on 32 samples; if loss doesn't drop near zero, resize operation (512→224) or coordinate calculation is destroying data
  3. Run MCD on 100 known test samples; plot "Variance" vs "Correctness" to verify high variance correlates with errors

## Open Questions the Paper Calls Out
None

## Limitations
- Encoding Generalization: Knight Encoding may struggle with very long sequences due to point convergence, limiting applicability to larger proteins
- Multi-class Performance Gap: Binary classification excels but multi-class accuracy remains suboptimal due to feature overlap in the encoding
- Uncertainty Calibration: MCD-based uncertainty estimates may not be fully calibrated, with consistently lower confidence on PVPs potentially indicating model bias

## Confidence
- **High Confidence:** Binary classification accuracy (90.8%) and F1-score (88.6%) for GoogleNet
- **Medium Confidence:** General mechanism of image-based encoding for protein classification
- **Low Confidence:** Specific claim that Knight Encoding uniquely preserves spatial information better than FCGR

## Next Checks
1. Implement stratified k-fold validation to ensure reported performance isn't dependent on a particular train/test split
2. Systematically vary the radius parameter (r=10, 15, 20) and canvas size to quantify impact on accuracy for sequences near length thresholds
3. Calculate expected calibration error (ECE) on test set by comparing predicted confidence levels with actual accuracy across confidence bins