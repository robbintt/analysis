---
ver: rpa2
title: Do LLMs Need to Think in One Language? Correlation between Latent Language
  and Task Performance
arxiv_id: '2505.21458'
source_url: https://arxiv.org/abs/2505.21458
tags:
- language
- answer
- latent
- adversarial
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether maintaining consistency in the
  latent language used by large language models (LLMs) enhances downstream task performance.
  The authors hypothesize that disrupting the internal reasoning language through
  adversarial prompts will degrade robustness, and they systematically analyze this
  by varying input prompt languages across translation and geo-culture tasks.
---

# Do LLMs Need to Think in One Language? Correlation between Latent Language and Task Performance

## Quick Facts
- **arXiv ID:** 2505.21458
- **Source URL:** https://arxiv.org/abs/2505.21458
- **Reference count:** 40
- **Primary result:** Maintaining latent language consistency is not always necessary for optimal LLM task performance due to final-layer adaptation capabilities

## Executive Summary
This study investigates whether consistency in the latent language used by large language models enhances downstream task performance. The authors introduce the Latent Language Consistency Score (LLC Score) to quantify internal language consistency and measure its correlation with task robustness under adversarial prompts. Experimental results across multiple LLMs show that while consistency is often disrupted by adversarial prompts, maintaining such consistency is not always necessary for optimal performance. This is attributed to models' ability to adapt internal representations in final layers to match the target language, reducing the impact of consistency on overall accuracy.

## Method Summary
The study analyzes latent language consistency across multiple transformer-based LLMs using LogitLens to extract hidden states at each layer. The LLC Score quantifies consistency by measuring KL divergence between layer activations and the probability of dominant latent language. Researchers generated 2,000 cloze-style questions per task using GPT-4o, ensuring single-token answers in English, Japanese, and Chinese. Adversarial prompts containing cultural context in different languages were injected at varying ratios (20%-100% of context length) to disrupt internal consistency. Performance was measured as task accuracy, and correlations between LLC Score and robustness were analyzed across models including LLM-jp-3, Qwen2.5, and Gemma3.

## Key Results
- LLC Score shows negative correlation with robustness for LLM-jp-3 (r=-0.82) but weak or positive correlations for other models
- Maintaining latent language consistency is not always necessary for optimal task performance
- Models can adapt internal representations in final layers to compensate for inconsistent intermediate representations
- Adversarial prompts disrupt internal stability and reduce model confidence, even when accuracy remains stable

## Why This Works (Mechanism)

### Mechanism 1: Latent Language Consistency (LLC) as a Conditional Robustness Indicator
- **Claim:** If a model maintains a single "latent language" across intermediate layers (low LLC Score), it may exhibit higher task robustness, but this correlation is conditional and model-dependent.
- **Mechanism:** The model processes input by mapping it to an internal "proficient" language (e.g., English for Llama). Consistency is measured by the KL divergence between layer activations and the probability of the dominant latent language. High consistency implies stable internal reasoning, which correlates with accuracy in specific models (e.g., LLM-jp-3 on geo-culture tasks).
- **Core assumption:** Stable internal representations (low KL divergence between layers) imply coherent reasoning.
- **Evidence anchors:** [abstract] "Experimental results... indicate that maintaining consistency in latent language is not always necessary... This is because these models adapt..." [section] Table 2 shows negative correlations (r < 0) between LLC Score and Robustness for LLM-jp-3 (e.g., r=-0.82), supporting the hypothesis for this specific model.
- **Break condition:** For models like Qwen2.5 and Gemma3, or specific tasks, the correlation breaks (r approaches 0 or becomes positive), meaning consistency is not a prerequisite for performance.

### Mechanism 2: Late-Stage Representation Adaptation
- **Claim:** LLMs can decouple internal reasoning consistency from final output accuracy by adapting representations in the final layers.
- **Mechanism:** Even if intermediate layers show high inconsistency (language switching), the "unembedding" layers can align the final hidden state with the target language, compensating for "noisy" internal thoughts.
- **Core assumption:** The unembedding matrix and final transformer blocks have sufficient capacity to "translate" inconsistent internal states into correct target tokens.
- **Evidence anchors:** [abstract] "...models adapt their internal representations near the final layers to match the target language, reducing the impact of consistency on overall accuracy." [section] Section 7.1 notes that models "flexibly adjust to different latent languages" and that consistency breakdown does not always occur even with mismatched adversarial prompts.
- **Break condition:** If the adversarial noise is excessively high or the model lacks capacity (e.g., smaller models), the final layers may fail to adapt, causing performance degradation.

### Mechanism 3: Adversarial Linguistic Noise Injection
- **Claim:** Injecting adversarial prompts in a language distinct from the model's latent language disrupts internal stability and reduces confidence, even if accuracy doesn't always drop.
- **Mechanism:** Noise increases the "linguistic entropy" in the attention heads, forcing the model to allocate capacity to process irrelevant cultural/linguistic context, thereby lowering output probability confidence.
- **Core assumption:** The model cannot fully ignore semantically coherent but task-irrelevant adversarial context.
- **Evidence anchors:** [section] Section 7.4 & Figure 6: "inserting adversarial prompts reduces model confidence and robustness, as indicated by the overall downward shift in output probabilities." [section] Section 5.3: Adversarial prompts described as containing cultural context (e.g., "act as if you are in Chinese") to force latent shifts.
- **Break condition:** If the adversarial prompt language matches the model's proficient latent language (e.g., English adversarial prompts for an English-centric model), the disruption is minimized.

## Foundational Learning

- **Concept:** **LogitLens Interpretability**
  - **Why needed here:** This is the core tool used to "read" the model's thoughts at every layer. You cannot compute the LLC Score without projecting hidden states back to the vocabulary.
  - **Quick check question:** Can you explain how to project a hidden state vector $h_l$ at layer $l$ to a probability distribution over the vocabulary using the unembedding matrix $W_U$?
- **Concept:** **Residual Stream Analysis**
  - **Why needed here:** The paper analyzes how information flows and accumulates. Understanding how tokens influence the residual stream is key to grasping how "noise" disrupts the signal.
  - **Quick check question:** How does the log-odds (logit) of a token typically evolve as it moves from early layers to the final layer in a Transformer?
- **Concept:** **Multilingual Latent Space**
  - **Why needed here:** The paper debates whether LLMs have a "pivot" language. Understanding that multilingual models often share a semantic subspace across languages helps explain why "adaptation" in final layers is plausible.
  - **Quick check question:** Why might a model trained on English and Japanese data prefer one language for internal arithmetic reasoning even if the prompt is in the other language?

## Architecture Onboarding

- **Component map:** Input Processor -> Probe (LogitLens) -> LangID Module -> Metric Calculator
- **Critical path:** The experiment flow relies on generating the dataset → injecting noise → running inference → extracting layer-wise logits → calculating LLC Score
- **Design tradeoffs:**
  - Model Size vs. Context: The study uses smaller models (1B–3B) because maximum noise injection (filling the full context window) requires significant memory for intermediate state storage
  - LLC Score vs. Simple Accuracy: The LLC Score is a proxy for "internal health." Relying only on accuracy would miss the internal consistency breakdown that the paper highlights
- **Failure signatures:**
  - Constant LLC Score: If the score doesn't change with adversarial prompts, the LogitLens projection or LangID mapping is likely failing to capture the dominant token
  - OOM (Out of Memory): Occurs if trying to extract hidden states for all layers on large models with full context (32k tokens) without batching or offloading
- **First 3 experiments:**
  1. Sanity Check (LogitLens): Replicate the latent language detection on a clean prompt (no noise) for Llama2 or LLM-jp to verify they default to their known "proficient" language (English or Japanese)
  2. Noise Ablation: Implement the adversarial prompt injection at a 0.5 ratio on a translation task and manually inspect the top tokens at layers 10, 20, and 30 to see the language switching
  3. Metric Validation: Calculate the LLC Score for a small batch (n=50) and plot it against accuracy to see if the negative correlation holds for your specific model architecture

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the correlations between latent language consistency and robustness observed in smaller models persist in significantly larger state-of-the-art LLMs under extensive adversarial noise?
- Basis in paper: [explicit] Appendix B.10 states, "Future work aims to extend our work to larger models with more extensive noise injection using multiple GPUs."
- Why unresolved: Computational resource constraints limited the current study to models with 1B–7B parameters and required reduced noise ratios for the 7.2B model, making the behavior of larger architectures unknown.
- What evidence would resolve it: Experiments replicating the methodology on models exceeding 70B parameters using distributed computing to handle full-length adversarial noise.

### Open Question 2
- Question: Is the proposed Latent Language Consistency (LLC) Score the optimal metric for quantifying internal language consistency, or do alternative formulation methods exist?
- Basis in paper: [explicit] Appendix A notes that "other, potentially better, approaches may exist" for determining the dominant language, acknowledging the current approach is just one option.
- Why unresolved: The authors selected the LLC Score based on correlation with potential methods like averaging KL scores, but they did not exhaustively compare against all possible quantitative measures of consistency.
- What evidence would resolve it: A comparative analysis evaluating the predictive power of various consistency metrics (e.g., averaging KL divergence) on the same downstream task performance.

### Open Question 3
- Question: To what degree are the observed effects of latent language disruption dependent on the specific semantic content of the adversarial prompts rather than the linguistic disruption itself?
- Basis in paper: [explicit] Appendix A raises the concern that "the observed effects may depend on the specific adversarial prompts used" to investigate the correlation.
- Why unresolved: The study relied on specific cultural and historical prompts to disrupt consistency; it remains unclear if different types of noise would alter the relationship between consistency and robustness.
- What evidence would resolve it: Ablation studies using diverse categories of adversarial content (e.g., nonsensical text, technical jargon) to determine if the correlation holds regardless of prompt semantics.

## Limitations
- The relationship between latent language consistency and robustness is highly model and task dependent, with mixed empirical support across different model-task combinations
- The mechanism of final-layer adaptation remains somewhat speculative, with unclear universal applicability across models
- Findings may not generalize beyond the specific models tested (LLM-jp-3, Qwen2.5, Gemma3) and tasks (translation, geo-culture)

## Confidence

- **High Confidence:** The LLC Score methodology for quantifying latent language consistency across layers is well-defined and reproducible. The observation that different models exhibit varying degrees of consistency-robustness correlation is empirically supported by the correlation tables.
- **Medium Confidence:** The claim that final-layer adaptation can compensate for inconsistent intermediate representations is plausible given the architecture of transformer models, but the specific mechanism and its universal applicability across models remains to be fully validated.
- **Low Confidence:** The generalizability of findings beyond the specific models tested and tasks is uncertain, particularly given the conditional nature of the consistency-robustness relationship.

## Next Checks
1. **Model-Specific Correlation Analysis:** Replicate the correlation analysis between LLC Score and accuracy for additional models (e.g., larger Llama variants or other multilingual models) to determine if the negative correlation pattern holds or if it's specific to smaller, task-specific models like LLM-jp-3.

2. **Ablation of Final Layer Adaptation:** Design an experiment that isolates the final layers' adaptation capability by comparing performance when intermediate layers are frozen versus when they can freely adapt, to directly test whether the claimed compensation mechanism exists.

3. **Cross-Task Robustness Testing:** Apply the adversarial prompt methodology to non-linguistic tasks (e.g., mathematical reasoning or code generation) to determine whether latent language consistency plays a similar conditional role across different task domains.