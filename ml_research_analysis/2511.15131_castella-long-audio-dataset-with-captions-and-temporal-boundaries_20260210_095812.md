---
ver: rpa2
title: 'CASTELLA: Long Audio Dataset with Captions and Temporal Boundaries'
arxiv_id: '2511.15131'
source_url: https://arxiv.org/abs/2511.15131
tags:
- audio
- castella
- captions
- dataset
- moments
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CASTELLA, a large-scale human-annotated dataset
  for audio moment retrieval (AMR), addressing the lack of reliable real-world benchmarks
  in the field. The dataset consists of 1,862 audio recordings (1-5 minutes long)
  with 3,881 captions and temporal boundaries, making it 24 times larger than the
  previous AMR dataset.
---

# CASTELLA: Long Audio Dataset with Captions and Temporal Boundaries

## Quick Facts
- **arXiv ID**: 2511.15131
- **Source URL**: https://arxiv.org/abs/2511.15131
- **Reference count**: 0
- **Primary result**: CASTELLA dataset introduces 1,862 long audio recordings (1-5 minutes) with 3,881 captions and temporal boundaries, 24x larger than previous AMR benchmarks

## Executive Summary
This paper introduces CASTELLA, a large-scale human-annotated dataset for audio moment retrieval (AMR) that addresses the lack of reliable real-world benchmarks in the field. The dataset consists of 1,862 audio recordings (1-5 minutes long) with 3,881 captions and temporal boundaries, making it 24 times larger than the previous AMR dataset. The authors establish a baseline AMR model using DETR-based architectures and demonstrate that fine-tuning on CASTELLA after pre-training on synthetic data significantly improves performance (10.4 points in Recall1@0.7) compared to training solely on synthetic data. The experiments also show that model architectures effective in video moment retrieval perform well in AMR, though short moments remain challenging.

## Method Summary
The CASTELLA dataset contains 1,862 audio recordings from YouTube (AudioCaps subset), downsampled to 32 kHz, with 3,881 captions and 11,308 timestamps. The authors use AM-DETR architecture combining MS-CLAP feature extraction with DETR-based heads. Training involves pre-training on synthetic Clotho-Moment data followed by fine-tuning on CASTELLA using AdamW optimizer (lr=1e-4, batch=32) with early stopping. The evaluation uses IoU-based metrics including Recall1@0.5, Recall1@0.7, and mAP@avg. 242 audio files without local captions were removed before feature extraction.

## Key Results
- CASTELLA dataset is 24x larger than previous AMR benchmarks (1,862 recordings vs. 78 in AudioCaps)
- Model fine-tuned on CASTELLA after pre-training on synthetic data outperformed synthetic-only training by 10.4 points in Recall1@0.7
- UVCOM architecture achieved best performance among DETR-based models, consistent with video moment retrieval trends
- Short audio moments (<10 seconds) show significantly lower retrieval performance compared to longer moments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-training on synthetic data followed by fine-tuning on human-annotated data substantially improves AMR performance compared to either strategy alone.
- Mechanism: Synthetic data (Clotho-Moment) provides large-scale coarse alignment between audio and text; CASTELLA fine-tuning then refines temporal boundary prediction on realistic audio distributions with human-verified captions.
- Core assumption: The synthetic pre-training learns transferable audio-text representations that generalize to real-world acoustic complexity when refined.
- Evidence anchors:
  - [abstract]: "model fine-tuned on CASTELLA after pre-training on the synthetic data outperformed a model trained solely on the synthetic data by 10.4 points in Recall1@0.7"
  - [section 5.2, Table 3]: Model 3 (Clotho-Moment + CASTELLA) achieved 16.2 R1@0.7 vs. Model 1 (synthetic only) at 5.8 and Model 2 (CASTELLA only) at 9.7
  - [corpus]: TACOS paper addresses temporally-aligned audio captions for pretraining, suggesting temporal alignment quality matters—corpus supports importance of annotation quality but does not validate the specific pre-train/fine-tune delta.
- Break condition: If synthetic data distribution diverges too far from target domain (e.g., different acoustic environments, caption styles), pre-training may not transfer, potentially causing negative transfer.

### Mechanism 2
- Claim: DETR-based architectures designed for video moment retrieval transfer effectively to audio moment retrieval with similar relative performance rankings.
- Mechanism: DETR's object detection framework treats audio moments as temporal "objects" to detect; the transformer attention mechanism captures inter-frame relationships and the bipartite matching loss handles variable-length moment predictions without hand-crafted anchors.
- Core assumption: Temporal structure in audio has similar properties to temporal structure in video for retrieval purposes.
- Evidence anchors:
  - [section 5.2]: "UVCOM (Model 5) performed best... while Moment-DETR (Model 4) was the worst. This trend is also observed in the VMR task"
  - [section 5.1]: "AM-DETR consists of audio-text model, such as CLAP, with Detection Transformer (DETR) inspired by video moment retrieval models"
  - [corpus]: No direct corpus evidence for audio-video transfer; this is a paper-internal claim requiring external validation.
- Break condition: If audio moments have fundamentally different temporal characteristics than video (e.g., overlapping sounds, absence of visual frame boundaries), architecture assumptions may misalign.

### Mechanism 3
- Claim: Short audio moments (<10 seconds) are systematically harder to retrieve than longer moments, following a pattern observed in video moment retrieval.
- Mechanism: Shorter moments provide less temporal context for feature extraction; sliding window features may have insufficient resolution; IoU-based metrics penalize small timing errors more severely for short intervals.
- Core assumption: The difficulty stems from both representation limitations and metric sensitivity rather than annotation noise.
- Evidence anchors:
  - [section 5.2, Fig. 3]: "performance is low for short moments of less than 10 seconds"
  - [section 3.2, Fig. 2]: "distribution has a clear long-tail and indicates that short timestamps are dominant"
  - [corpus]: No corpus papers directly address short-moment retrieval difficulty in audio.
- Break condition: If annotation granularity (1-second resolution) is the bottleneck rather than model capacity, increasing model size won't help; need finer annotations or different evaluation protocols.

## Foundational Learning

- Concept: **Detection Transformer (DETR)**
  - Why needed here: Core architecture for moment prediction; uses bipartite matching loss and learned object queries to predict variable numbers of temporal segments.
  - Quick check question: Can you explain why DETR doesn't require hand-designed anchor boxes, and what the Hungarian matching loss optimizes?

- Concept: **Contrastive Language-Audio Pretraining (CLAP)**
  - Why needed here: Provides the audio-text feature alignment backbone; MS-CLAP extracts features via sliding window for temporal resolution.
  - Quick check question: How does CLAP differ from CLIP, and why would a sliding window approach be necessary for long audio?

- Concept: **Moment Retrieval Evaluation Metrics (Recall@k, mAP@IoU)**
  - Why needed here: Understand what R1@0.7 and mAP@avg measure; IoU threshold determines prediction quality requirements.
  - Quick check question: Why would R1@0.7 be a stricter metric than R1@0.5, and what does this reveal about model precision?

## Architecture Onboarding

- Component map: Long audio (60-300s) -> MS-CLAP feature extractor (1s window/hop) -> sequence of audio embeddings -> DETR-based network (QD-DETR/Moment-DETR/UVCOM) processes audio embeddings with text conditioning -> Moment boundaries (start, end) + confidence scores per predicted moment

- Critical path:
  1. Audio preprocessing (32kHz, sliding window extraction) — errors here propagate through all stages
  2. CLAP feature quality — if audio-text alignment is poor, downstream retrieval fails
  3. DETR moment prediction — primary learned component
  4. IoU threshold selection at evaluation — determines reported metrics

- Design tradeoffs:
  - Window size: 1s provides temporal resolution but may miss fine-grained acoustic patterns
  - Pre-training strategy: Synthetic data scales but may introduce domain shift; real data is limited but higher quality
  - Architecture choice: UVCOM best but more complex; Moment-DETR simpler but lower performance
  - Annotation resolution: 1-second granularity balances annotation cost vs. temporal precision

- Failure signatures:
  - Very low R1@0.5 but reasonable mAP: Model predicts moments but ranking is wrong
  - Good performance on long moments, near-zero on short moments: Feature resolution insufficient
  - Large gap between R1@0.5 and R1@0.7: Boundary prediction imprecise
  - Training loss decreases but metrics plateau early: Overfitting to synthetic pre-training distribution

- First 3 experiments:
  1. **Baseline replication**: Train QD-DETR from scratch on CASTELLA train split, evaluate on test split using R1@0.5, R1@0.7, mAP@avg. Compare to Table 3 Model 2 results to validate setup.
  2. **Pre-training ablation**: Compare three conditions—(a) random init → CASTELLA, (b) Clotho-Moment pre-train → CASTELLA, (c) Clotho-Moment only. Quantify the 10.4 point delta claimed.
  3. **Moment length stratification**: Bin test set by ground-truth moment duration (<10s, 10-30s, 30-60s, >60s). Report R1@0.7 per bin to confirm short-moment difficulty and identify where the model fails.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can model architectures be adapted to improve the retrieval accuracy of short-duration audio moments (less than 10 seconds)?
- **Basis in paper:** [explicit] The authors state in Section 5.2 that "accurately retrieving short moments remains a significant challenge" and Figure 3 shows a distinct performance drop for moments with a maximum length of 0-10 seconds compared to longer segments.
- **Why unresolved:** Current DETR-based baselines struggle with the high frequency of short timestamps in the dataset (Fig. 2), resulting in significantly lower Recall1 scores for these events.
- **What evidence would resolve it:** The development of a specific architectural variant or loss function that achieves comparable Recall1@0.7 scores for moments under 10 seconds as for longer moments.

### Open Question 2
- **Question:** Can CASTELLA effectively support the joint task of simultaneous audio moment localization and captioning?
- **Basis in paper:** [explicit] Section 6 explicitly lists "developing technologies for the simultaneous localization and captioning of salient events in a long audio recording" as a future application of the dataset.
- **Why unresolved:** The current study establishes baselines only for the retrieval task (moment localization given a query), not for generation tasks.
- **What evidence would resolve it:** A trained model that successfully utilizes CASTELLA's annotations to output both temporal boundaries and descriptive text for salient events simultaneously.

### Open Question 3
- **Question:** What methodologies are required to scale AMR to retrieve moments from a corpus of multiple long audio recordings rather than a single file?
- **Basis in paper:** [explicit] The conclusion in Section 6 identifies the need for "methods for retrieving specific audio moments relevant to a query from a collection of multiple long audio recordings."
- **Why unresolved:** The current experimental setup and benchmark focus exclusively on searching within a single input audio recording.
- **What evidence would resolve it:** A retrieval system that efficiently indexes the entire CASTELLA dataset to identify specific moments across different audio files in response to a text query.

## Limitations
- Synthetic pre-training data may introduce domain shift from natural human annotations
- 1-second temporal annotation resolution may be insufficient for capturing fine-grained audio events
- Evaluation uses fixed IoU thresholds that may not reflect practical deployment scenarios

## Confidence
- **High Confidence**: Dataset size claim (24x larger than existing AMR datasets) and basic pre-training/fine-tuning performance improvement (10.4 point R1@0.7 gain)
- **Medium Confidence**: DETR-based architectures transfer effectively from video to audio moment retrieval
- **Low Confidence**: Short moments (<10s) are inherently harder due to representation limitations rather than annotation granularity constraints

## Next Checks
1. **Temporal Resolution Analysis**: Re-analyze the short moment performance by comparing models trained with 0.5s vs 1s temporal resolution annotations to determine if annotation granularity is the limiting factor.

2. **Cross-Dataset Generalization**: Evaluate the best-performing model (UVCOM) on the TACOS dataset to test whether the architecture and pre-training approach generalize beyond CASTELLA's domain.

3. **Annotation Quality Validation**: Conduct a small-scale human evaluation where annotators rate caption relevance and temporal boundary accuracy for a random sample of CASTELLA instances to quantify annotation noise.