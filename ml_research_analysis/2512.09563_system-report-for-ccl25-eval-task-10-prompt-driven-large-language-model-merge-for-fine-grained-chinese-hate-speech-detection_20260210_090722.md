---
ver: rpa2
title: 'System Report for CCL25-Eval Task 10: Prompt-Driven Large Language Model Merge
  for Fine-Grained Chinese Hate Speech Detection'
arxiv_id: '2512.09563'
source_url: https://arxiv.org/abs/2512.09563
tags:
- hate
- speech
- prompt
- detection
- merging
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a three-stage LLM-based framework for fine-grained
  Chinese hate speech detection. The method combines domain-specific prompt engineering
  with supervised fine-tuning and LLM merging to improve detection of implicit hate
  patterns and context-dependent rhetoric.
---

# System Report for CCL25-Eval Task 10: Prompt-Driven Large Language Model Merge for Fine-Grained Chinese Hate Speech Detection

## Quick Facts
- arXiv ID: 2512.09563
- Source URL: https://arxiv.org/abs/2512.09563
- Reference count: 4
- This paper presents a three-stage LLM-based framework for fine-grained Chinese hate speech detection. The method combines domain-specific prompt engineering with supervised fine-tuning and LLM merging to improve detection of implicit hate patterns and context-dependent rhetoric. Evaluated on the STATE-ToxiCN benchmark, the merged model achieved F1-scores of 0.3553 (hard) and 0.4604 (soft) on the preliminary test set, demonstrating superior performance over baseline approaches with over 15% accuracy improvement in hate detection and enhanced generalization to complex scenarios.

## Executive Summary
This paper addresses fine-grained Chinese hate speech detection by developing a three-stage framework that leverages large language models (LLMs). The approach combines domain-specific prompt engineering, supervised fine-tuning, and LLM merging to improve detection of implicit hate patterns and context-dependent rhetoric. Evaluated on the STATE-ToxiCN benchmark, the merged model demonstrates superior performance over baseline approaches with significant improvements in both hard and soft matching metrics.

## Method Summary
The framework employs a three-stage approach: (1) Domain-specific Prompt Engineering using structured prompts with four-tuple outputs, non-hate contrast examples, and category definitions; (2) Supervised Fine-Tuning on Qwen2.5-7B-Instruct with L2 regularization to prevent catastrophic forgetting; (3) LLM Merging via task vector extraction, pruning, direct alignment, and parameter averaging. The method processes raw Chinese social media text through prompt templates, fine-tunes multiple models with different prompt strategies, and merges them to synthesize complementary detection capabilities.

## Key Results
- Merged model achieved F1-scores of 0.3553 (hard) and 0.4604 (soft) on preliminary test set
- Demonstrated over 15% accuracy improvement in hate detection compared to baseline approaches
- Showed enhanced generalization to complex scenarios and out-of-distribution cases

## Why This Works (Mechanism)

### Mechanism 1: Domain-Specific Prompt Engineering
Structured prompts with bidirectional examples and explicit category definitions improve fine-grained hate speech extraction by enforcing a four-tuple output format (Target—Argument—Targeted Group—Hateful), embedding contrastive non-hate boundaries to reduce false positives, and providing predefined group category mappings. This shifts model attention from general semantics to targeted feature extraction. The core assumption is that the model can internalize detection boundaries through explicit linguistic rules embedded in prompts.

### Mechanism 2: Task-Oriented Supervised Fine-Tuning
Full-parameter SFT with L2 regularization transfers domain-specific hate patterns while preventing catastrophic forgetting. The loss function balances classification accuracy against parameter deviation from pre-trained weights using AdamW optimizer with decaying gradient moments. The core assumption is that high-quality annotated quadruples in STATE-ToxiCN capture representative implicit hate patterns.

### Mechanism 3: Dynamic LLM Merging via Task Vectors
Merging multiple fine-tuned models synthesizes complementary detection capabilities through sparse task vector aggregation. The algorithm prunes extreme outliers using thresholds, resolves sign conflicts via directional alignment, and averages only consensus-aligned parameters. The core assumption is that different prompt strategies induce models to learn non-redundant, complementary features.

## Foundational Learning

- **Concept: Task Vectors**
  - Why needed here: The merge algorithm represents learned capabilities as additive modifications (τ = θ_finetuned − θ_base) that can be combined without retraining.
  - Quick check question: If two task vectors have opposite signs for the same parameter, how does the Direct step resolve this?

- **Concept: Hard vs Soft Matching Metrics**
  - Why needed here: Evaluation requires understanding that hard matching demands exact four-tuple equality, while soft matching accepts 50% string overlap for Target/Argument but exact match for Targeted Group and Hateful.
  - Quick check question: Why might a model achieve high soft F1 but low hard F1 on implicit hate speech?

- **Concept: Pruning Thresholds (α, β)**
  - Why needed here: The Prune step removes both extreme outliers (right-tail α) and minor perturbations (left-tail β) from task vectors before merging.
  - Quick check question: What failure mode would result from setting β too high (aggressive lower-bound pruning)?

## Architecture Onboarding

- **Component map:** Input: Raw Chinese social media text → Prompt Module: Four-tuple templates with ICL examples + Non-Hate contrast + Category Explanations + Judge Criteria → SFT Module: Qwen2.5-7B-Instruct, lr=1e-5, batch=64, 8 epochs, DeepSpeed Zero-3, Flash Attention 2.0 → Merge Module: Task vector extraction → Prune (α,β masking) → Direct (sign consensus γ_m) → Merge (parameter average) → Output: Structured quadruple (Target, Argument, Targeted_Group, Hateful)

- **Critical path:** Base model → Apply prompt strategy A → SFT → Save checkpoint → Repeat for strategies B, C, D → Extract task vectors → Three-step merge → Final θ_m for inference

- **Design tradeoffs:** More prompt components → better scores but higher token costs; More merge iterations → diminishing returns after ~3 models; Full-parameter SFT → better adaptation vs. LoRA efficiency

- **Failure signatures:** RFT scored 0.2021: rule-based rewards fail on implicit patterns; Single-model ceiling ~0.3436: individual prompts don't capture all hate variants; Merge saturation: <1% gain between Merge2 and Merge3

- **First 3 experiments:**
  1. Ablate prompt components: Test ICL alone, then incrementally add Non-Hate, Category, Judge Criteria to quantify each contribution
  2. Merge depth test: Compare 2-model merge vs 3-model merge vs 4-model merge to identify saturation point
  3. Generalization check: Evaluate merged model on Test2 (unseen distribution) to validate out-of-distribution robustness claim

## Open Questions the Paper Calls Out

### Open Question 1
Can extended Continual Pre-training (CPT) with larger domain-specific corpora significantly outperform the current supervised fine-tuning (SFT) baselines on instruction-tuned models? The authors state, "We hypothesize that extended CPT training with additional domain-specific corpora could further enhance this performance gap," noting that CPT+SFT on a base model was already competitive with SFT on an Instruct model. This remains unresolved as the current study compared CPT+SFT on a base model against SFT on an instruct model but did not validate the hypothesis of scaling CPT data further.

### Open Question 2
How can the observed saturation in performance gains be overcome when iteratively merging large language models for hate speech detection? The paper notes "diminishing returns between Merge2... and Merge3... suggest a potential limit to current merging strategies' effectiveness, possibly requiring novel fusion techniques." The current merging algorithm relies on linear sparsification and averaging of task vectors, which appears to cap the benefits of adding more diverse prompt-tuned models. This could be resolved by applying non-linear or attention-based merging architectures that demonstrate statistically significant improvements over the Merge3 baseline.

### Open Question 3
What specific mechanisms are required to improve the precision of exact entity extraction (Hard Score) to match the model's semantic understanding capabilities (Soft Score)? Despite high soft matching scores (0.4602), the hard matching score remains low (0.2504), indicating a methodological limitation where the model captures the semantic gist but fails to extract the precise "four-tuple" structure required for strict evaluation. This could be resolved by ablation studies showing that specific architectural changes or constrained decoding strategies significantly narrow the gap between Hard and Soft F1 scores.

## Limitations
- Prompt template opacity: Exact prompt text for four-tuple extraction, non-hate contrast examples, category definitions, and judge criteria is only partially shown, blocking exact replication
- Merge hyperparameters: Pruning thresholds (α, β), scaling factor (λ), and per-layer normalization are unspecified, critically influencing merge quality
- Dataset splits and preprocessing: Train/validation/test splits are not explicitly defined, and multi-target handling is unclear, limiting fair comparison
- Single-task focus: Framework is tuned for narrow four-tuple extraction task, potentially limiting generalization to other hate speech detection formats

## Confidence

- **High**: The three-stage framework (Prompt Engineering → SFT → LLM Merging) is clearly described and methodologically sound. The reported improvements over baselines (up to 15% in hard score, 5.1% in soft score) are internally consistent and supported by ablation results in the paper.
- **Medium**: The use of task vectors and three-step merging is well-grounded in literature, but the specific hyperparameters (α, β, λ) and prompt details are not fully specified, limiting exact reproducibility.
- **Low**: The claim of enhanced generalization to out-of-distribution cases is supported only by a single unseen test set (Test2), with no further robustness analysis or cross-domain validation.

## Next Checks

1. **Prompt component ablation**: Systematically test the contribution of each prompt component (ICL, Non-Hate contrast, Category Explanations, Judge Criteria) by incrementally removing them and measuring impact on both hard and soft F1 scores.

2. **Merge saturation analysis**: Evaluate merged models with 2, 3, and 4 fine-tuned base models to confirm the presence and extent of diminishing returns, as suggested by the small gain between Merge2 and Merge3.

3. **Cross-domain robustness**: Test the merged model on an external hate speech dataset (e.g., English or another Chinese corpus) to verify the claim of improved out-of-distribution generalization beyond the single unseen Test2 set.