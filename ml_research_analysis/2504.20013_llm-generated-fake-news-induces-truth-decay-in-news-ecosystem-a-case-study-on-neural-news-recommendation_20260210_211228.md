---
ver: rpa2
title: 'LLM-Generated Fake News Induces Truth Decay in News Ecosystem: A Case Study
  on Neural News Recommendation'
arxiv_id: '2504.20013'
source_url: https://arxiv.org/abs/2504.20013
tags:
- news
- fake
- llm-generated
- recommendation
- real
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how LLM-generated fake news affects neural
  news recommendation systems. The authors constructed a dataset of 56k human and
  LLM-generated news items with veracity labels and user interaction sequences.
---

# LLM-Generated Fake News Induces Truth Decay in News Ecosystem: A Case Study on Neural News Recommendation

## Quick Facts
- **arXiv ID**: 2504.20013
- **Source URL**: https://arxiv.org/abs/2504.20013
- **Reference count**: 12
- **Key outcome**: LLM-generated fake news causes real news to lose ranking advantage in neural news recommendation systems through "Truth Decay."

## Executive Summary
This study investigates how LLM-generated fake news undermines the integrity of neural news recommendation systems. Using a dataset of 56k human and LLM-generated news items with veracity labels, the authors simulate four scenarios of fake news involvement in recommendation pipelines. They demonstrate a "truth decay" phenomenon where real news gradually loses its ranking advantage over fake news as LLM-generated content is introduced. The decay occurs because recommendation models, particularly their PLM backbones, become more familiar with the lower-perplexity LLM-generated text. The findings highlight a significant threat to news ecosystem credibility and call for collaborative countermeasures among LLM providers, recommender systems, and news services.

## Method Summary
The authors constructed user interaction sequences from GossipCop (5,000 sequences, history length 4-10, candidate list K=20) and generated approximately 56,000 news items using GPT-4o-mini and Llama-3.1-8B-Instruct in three modes: L1 (paraphrase), L2 (rewrite), and L3 (conditional). They trained two neural news recommendation models (LSTUR and NRMS) with BERT-base-uncased backbones, evaluating performance using Relative Real Advantage (RRA) = (Metric_real - Metric_fake) / Metric_fake across metrics like MRR, nDCG@K, and Ratio@K. The study simulated four phases of LLM-news intrusion: Baseline (nowhere), Candidates (in candidate lists), User History (in user history), and Training Data (in training data).

## Key Results
- LLM-generated fake news causes real news to gradually lose its ranking advantage in neural news recommendation systems.
- Truth decay is driven by recommendation models' greater familiarity with lower-perplexity LLM-generated text.
- Different recommendation architectures (LSTUR vs NRMS) show varying susceptibility to truth decay.

## Why This Works (Mechanism)
Truth decay occurs because LLM-generated fake news typically has lower perplexity than human-written fake news. Neural news recommendation systems, particularly their PLM backbones, become more familiar with this lower-perplexity content during training or inference. This familiarity bias causes the models to rank LLM-generated fake news more favorably over time, even when real news is available. The phenomenon demonstrates how content generation characteristics (perplexity) can systematically bias recommendation outcomes.

## Foundational Learning
- **Relative Real Advantage (RRA)**: A metric comparing real vs fake news performance ((Metric_real - Metric_fake) / Metric_fake). Why needed: Quantifies the degradation of real news ranking as fake news is introduced. Quick check: RRA > 0 indicates real news still outperforms fake.
- **Perplexity-based familiarity**: The concept that models become biased toward content with lower perplexity due to training exposure. Why needed: Explains the mechanism behind truth decay. Quick check: Compare perplexity distributions of LLM vs human-generated content.
- **Neural news recommendation architecture**: Two-tower models (LSTUR, NRMS) using PLM backbones for content understanding. Why needed: Understanding model structure reveals where decay occurs. Quick check: Verify BERT is frozen during training.
- **User interaction sequence simulation**: Creating synthetic user histories with temporal dynamics. Why needed: Enables controlled experiments with different fake news exposure scenarios. Quick check: Ensure temporal split (80/20) is maintained.
- **LLM news generation modes**: L1 (paraphrase), L2 (rewrite), L3 (conditional) transformations. Why needed: Different generation strategies affect how closely fake news mimics real content. Quick check: Verify generation prompts match those in Table 2.

## Architecture Onboarding
- **Component map**: GossipCop data -> User interaction sequences -> LLM news generation (L1-L3) -> Neural models (LSTUR/NRMS) -> BERT backbone -> Evaluation metrics (RRA)
- **Critical path**: Data generation → Model training → Evaluation with RRA metric
- **Design tradeoffs**: Synthetic data generation allows controlled experiments but limits generalizability; single dataset (GossipCop) simplifies analysis but may not represent broader news ecosystems.
- **Failure signatures**: If LLM-fake perplexity doesn't exceed human-fake perplexity, truth decay won't occur; high RRA variance suggests convergence issues.
- **First experiments**:
  1. Train baseline LSTUR on human-only data, verify real news outperforms fake (positive RRA).
  2. Introduce LLM fake news into candidate lists, measure RRA drop.
  3. Replace training data with LLM news, retrain, and verify RRA approaches zero or negative.

## Open Questions the Paper Calls Out
None

## Limitations
- The study relies on synthetic perturbations of a single dataset (GossipCop), limiting generalizability to other news domains or real-world dynamics.
- The perplexity-based mechanism driving decay is plausible but not empirically validated against alternative hypotheses.
- The model-specific nature of decay raises questions about whether results reflect general recommendation vulnerabilities or architecture-specific artifacts.

## Confidence
- **High Confidence**: The core empirical finding that LLM-generated content can reduce real news ranking advantage in controlled simulations.
- **Medium Confidence**: The attribution of decay primarily to perplexity differences and PLM familiarity.
- **Low Confidence**: The scalability and severity of Truth Decay in actual news ecosystems.

## Next Checks
1. **Ablation on Perplexity**: Generate LLM-fake news with constrained perplexity (matching human-fake levels) and test whether decay persists.
2. **Cross-Architecture Verification**: Replicate the decay phenomenon using NRMS with a different backbone (e.g., RoBERTa or DeBERTa).
3. **Real-World Simulation**: Integrate the pipeline with a continuously updating news stream and simulated user interaction dynamics.