---
ver: rpa2
title: 'Poivre: Self-Refining Visual Pointing with Reinforcement Learning'
arxiv_id: '2509.23746'
source_url: https://arxiv.org/abs/2509.23746
tags:
- reward
- pointing
- arxiv
- visual
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses visual pointing, where a vision-language model
  must predict the coordinates of a target object in an image. Current models typically
  produce only a single estimate, limiting performance.
---

# Poivre: Self-Refining Visual Pointing with Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2509.23746
- **Source URL**: https://arxiv.org/abs/2509.23746
- **Authors**: Wenjie Yang; Zengfeng Huang
- **Reference count**: 23
- **Primary result**: Poivre-7B achieves state-of-the-art results on Point-Bench, surpassing both proprietary and large open-source models by over 3%.

## Executive Summary
This paper addresses visual pointing, where a vision-language model must predict the coordinates of a target object in an image. Current models typically produce only a single estimate, limiting performance. The authors propose Point, Visualize, then Refine (Poivre), a self-refining procedure that enables iterative refinement by marking and observing its own predictions. They employ reinforcement learning with a process reward inspired by potential-based reward shaping to encourage improvement across refinement steps. Their Poivre-7B model achieves state-of-the-art results on Point-Bench, surpassing both proprietary and large open-source models by over 3%. The method also generalizes to robotics tasks and demonstrates extrapolation capability when inference steps exceed training steps. The code, dataset, and model are publicly released.

## Method Summary
Poivre is a self-refining visual pointing framework that enables iterative refinement of object coordinates through visual feedback. The method employs a vision-language model (Qwen2.5-VL-7B) that generates initial coordinates, then observes its own prediction marked on the image and refines it through multiple steps. The key innovation is a process reward function based on potential-based reward shaping that encourages improvement at each refinement step rather than only rewarding the final outcome. Training uses GRPO (Group Relative Policy Optimization) with a fixed sequence length of T=2, but the model demonstrates the ability to extrapolate to longer inference chains (T=3+). The approach achieves state-of-the-art performance on Point-Bench and generalizes to robotics tasks.

## Key Results
- Poivre-7B achieves state-of-the-art results on Point-Bench, surpassing both proprietary and large open-source models by over 3%.
- The method demonstrates successful generalization to robotics tasks beyond visual pointing.
- Poivre-7B shows interesting extrapolation ability, with performance improving when scaling to T=3 inference steps beyond the T=2 training limit.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: If a Vision-Language Model (VLM) is provided with a visual trace of its own prediction, it can reduce localization error through iterative correction.
- **Mechanism**: The model generates an initial coordinate $P_1$. An external environment visualizes this point onto the image $I_0$, creating $I_1$. The model receives $I_1$ as input for the next turn, allowing it to "see" the error relative to the target and generate a refined coordinate $P_2$.
- **Core assumption**: The VLM possesses sufficient visual acuity to detect the discrepancy between the visualized marker and the target object in the feedback image.
- **Evidence anchors**:
  - [abstract] "This procedure enables a VLM to first mark its estimated point, then iteratively refine the coordinates if necessary."
  - [section 4] "The image with marked points $I_1$ is then fed back to the model... $P_{i+1} = F_\theta(\{I_j\}_{j=0}^i, q)$."
  - [corpus] *Learning GUI Grounding with Spatial Reasoning from Visual Feedback* supports the efficacy of visual feedback loops for coordinate prediction tasks.
- **Break condition**: If the visual marker (e.g., a colored dot) obstructs the target object or is imperceptible due to resolution constraints, the feedback loop degrades.

### Mechanism 2
- **Claim**: Rewarding the *rate of improvement* between steps (process reward) encourages stable refinement better than rewarding only the final accuracy (outcome reward).
- **Mechanism**: The reward function incorporates Potential-Based Reward Shaping (PBRS), calculating the reward as $\gamma^j(R_O(d_{j+1}) - R_O(d_j))$. This explicitly assigns positive value to the action of "getting closer" during intermediate steps, rather than assigning zero reward to all steps preceding the final one.
- **Core assumption**: The optimal policy involves monotonic improvement, and the discount factor $\gamma$ correctly balances immediate vs. future refinement incentives.
- **Evidence anchors**:
  - [abstract] "...design a neat process reward that is not only empirically effective but also grounded in appealing properties."
  - [section 4] Equation 4 defines the PBRS-inspired reward; Proposition 4.1 proves it weights the initial and final steps heavily.
  - [corpus] Corpus evidence for this specific PBRS application in VLMs is weak; neighbors like *ViCrit* focus on verifiable proxy tasks rather than process shaping.
- **Break condition**: If the discount factor $\gamma$ is too low, the model may ignore intermediate improvements, effectively collapsing the mechanism back to a sparse outcome reward.

### Mechanism 3
- **Claim**: Training on a fixed sequence length ($T=2$) generalizes to longer inference chains ($T=3+$) via recursive application of the refinement operator.
- **Mechanism**: The model learns a conditional policy: "Given Image + Marker, output Better Marker." This policy is compositional; applying it recursively allows the model to perform test-time scaling without explicit training on longer trajectories.
- **Core assumption**: The state distribution shifts minimally between step $T$ and $T+1$ so that the learned policy remains valid during extrapolation.
- **Evidence anchors**:
  - [section 5] "Poivre-7B demonstrates interesting extrapolation ability: when scaling to $T=3$, beyond what the model has encountered in training, performance further improves."
  - [corpus] *VTool-R1* and related works discuss test-time scaling, but specific extrapolation in visual pointing is unique to this paper's evaluation.
- **Break condition**: If error accumulation or "drift" occurs during recursive steps, performance may plateau or degrade as $T$ increases significantly.

## Foundational Learning

- **Concept**: **Potential-Based Reward Shaping (PBRS)**
  - **Why needed here**: The paper's core contribution is a specific reward formula derived from PBRS theory. Understanding that $F(s, a, s') = \gamma \Phi(s') - \Phi(s)$ guarantees policy invariance while guiding the agent is critical.
  - **Quick check question**: Why does subtracting the potential of the current state from the next state prevent the "reward hacking" seen in naive intermediate rewards?

- **Concept**: **Group Relative Policy Optimization (GRPO)**
  - **Why needed here**: The authors select GRPO (a variant of PPO) for training. You must understand how it computes advantages relative to a group baseline rather than a value function.
  - **Quick check question**: How does GRPO estimate the baseline advantage $\hat{A}$ in Equation 1, and why does this remove the need for a critic network?

- **Concept**: **Visual Grounding / Pointing**
  - **Why needed here**: This is the task domain. You need to distinguish "pointing" (coordinate regression) from "grounding" (bounding box) or classification.
  - **Quick check question**: How does the evaluation metric (Euclidean distance to ground truth) differ from standard IoU metrics used in object detection?

## Architecture Onboarding

- **Component map**: Qwen2.5-VL-7B (Visual Encoder + LLM) -> Rollout Worker (Image Renderer) -> GRPO Trainer
- **Critical path**:
  1. **Input Parsing**: Extracting coordinates from the VLM's text output.
  2. **Marker Rendering**: modifying the image tensor without destroying features.
  3. **Reward Calculation**: Computing the difference in distance potentials across turns.
- **Design tradeoffs**:
  - **Rendering vs. Embedding**: The authors choose to render a visual marker (brown dot) on the image pixels. Assumption: This is more robust than simply appending coordinate text to the prompt, but it permanently alters the image data.
  - **Training Steps ($T=2$)**: Training with only 2 steps saves compute but relies on the model's ability to extrapolate to $T>2$ at inference.
- **Failure signatures**:
  - **Oscillation**: The model points to location A, then B, then A, failing to converge.
  - **Ignoring Marker**: The VLM fails to recognize the visual marker as a distinct entity and treats the marked image as noise.
  - **Reward Hacking**: With Outcome Reward, the model might output random coordinates for $T-1$ steps; the Process Reward is designed specifically to fix this.
- **First 3 experiments**:
  1. **Sanity Check**: Run inference with $T=1$ vs. $T=2$ on a held-out set to verify the refinement mechanism actually lowers error distance.
  2. **Reward Ablation**: Train two small models (or steps)—one with Outcome Reward, one with PBRS Process Reward—to replicate the 1.3% gap cited in Table 2.
  3. **Extrapolation Test**: Run inference on Point-Bench with $T=3$ and $T=4$ to verify if performance continues to improve or if it plateaus (replicating Table 3).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can inference-time scaling be optimally controlled for visual pointing—specifically, when should the model stop refining, and what determines the upper limits of extrapolation beyond training rounds?
- Basis in paper: [explicit] "While continuing to scale inference time is a possible direction, optimizing this scaling remains an open question for future investigation."
- Why unresolved: The paper demonstrates extrapolation (T=3 outperforms T=2 despite training at T=2), but does not investigate stopping criteria, failure modes at higher T, or principled methods to determine optimal refinement depth.
- What evidence would resolve it: Systematic experiments varying T from 1 to 10+ rounds with analysis of when performance plateaus or degrades; development of confidence-based or error-based stopping criteria.

### Open Question 2
- Question: Would incorporating more recent RL algorithms (DAPO, GSPO, GMPO) yield significant improvements over GRPO for visual pointing?
- Basis in paper: [explicit] "Incorporating these methods in place of GRPO could potentially yield further improvements, but we leave this exploration to future work due to resource constraints."
- Why unresolved: Only GRPO was tested; the authors acknowledge newer algorithms exist but did not compare them.
- What evidence would resolve it: Controlled experiments training Poivre with DAPO, GSPO, and GMPO under identical conditions, reporting success rates on Point-Bench.

### Open Question 3
- Question: How does Poivre perform on real-world robotics tasks such as physical grasping, beyond the where2place benchmark?
- Basis in paper: [explicit] "A promising direction for future work is to extend our method to robotics training data, and it would also be interesting to evaluate it on real-world tasks, such as grasping."
- Why unresolved: Evaluation was limited to the where2place dataset; no physical robot experiments were conducted.
- What evidence would resolve it: Deployment of Poivre on a physical robot arm for grasping tasks, with success rate comparisons against baseline models.

### Open Question 4
- Question: How does training data scale affect Poivre's performance, given the current study used only 8,192 samples?
- Basis in paper: [explicit] "We are aware that both the scale and the scope of training data can be further expanded, which we will leave for future work."
- Why unresolved: Resource constraints limited training to 8K samples; scaling effects remain unexplored.
- What evidence would resolve it: Ablation experiments training Poivre with 8K, 50K, 200K, and full Pixmo-Points samples, reporting performance curves on Point-Bench.

## Limitations
- **Extrapolation Stability**: While the paper claims performance improves with test-time scaling beyond training steps (T=2 to T=3), the evidence is limited to a single dataset (Point-Bench). The mechanism may not generalize to more complex visual environments or significantly longer chains (T>3).
- **Marker Obfuscation Risk**: The current implementation uses a fixed brown dot (size=6) to mark predictions. For small targets or crowded scenes, this marker could partially obscure the target object, degrading the feedback signal.
- **Compute Overhead**: Each refinement step requires a full forward pass through the VLM and image rendering. For applications requiring real-time performance, the latency may be prohibitive.

## Confidence
- **High Confidence**: The core refinement mechanism (Mechanism 1) is well-supported by empirical results showing consistent error reduction across evaluation sets. The implementation details are clear and reproducible.
- **Medium Confidence**: The extrapolation capability (Mechanism 3) is demonstrated but only on Point-Bench. The compositional nature of the learned policy is theoretically sound but needs validation across diverse domains.
- **Medium Confidence**: The process reward advantage (Mechanism 2) is empirically validated, but the theoretical grounding relies on idealized assumptions about monotonic improvement that may not hold in practice.

## Next Checks
1. **Cross-Dataset Generalization Test**: Evaluate Poivre-7B on non-Point-Bench visual grounding datasets (e.g., RefCOCO, Flickr30k Entities) to verify if the refinement mechanism transfers beyond the training distribution.
2. **Long-Chain Stability Analysis**: Systematically test inference with T=4, T=5, and T=10 on Point-Bench to identify if error accumulation or oscillation emerges in extended refinement chains.
3. **Marker Size Sensitivity Study**: Conduct ablation experiments varying marker size (2px to 12px) to quantify the tradeoff between visibility and target obstruction across different object scales and image resolutions.