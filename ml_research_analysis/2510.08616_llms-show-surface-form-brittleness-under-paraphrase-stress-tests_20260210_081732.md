---
ver: rpa2
title: LLMs Show Surface-Form Brittleness Under Paraphrase Stress Tests
arxiv_id: '2510.08616'
source_url: https://arxiv.org/abs/2510.08616
tags:
- qwen2
- arxiv
- b-instruct
- accuracy
- paraphrase
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines the vulnerability of large language models
  to surface-form brittleness under paraphrase stress tests. The authors introduce
  a protocol to measure generalization by re-evaluating models on paraphrased versions
  of benchmark questions, using two 7B models (Mistral-7B-Instruct and Qwen2.5-7B-Instruct)
  on ARC-Easy and ARC-Challenge.
---

# LLMs Show Surface-Form Brittleness Under Paraphrase Stress Tests

## Quick Facts
- arXiv ID: 2510.08616
- Source URL: https://arxiv.org/abs/2510.08616
- Reference count: 3
- Large language models drop 6-10 points in accuracy on paraphrased benchmark questions

## Executive Summary
This study reveals that large language models exhibit significant surface-form brittleness when evaluated on paraphrased versions of benchmark questions. Using a systematic stress test protocol, the research demonstrates that models achieve 6-10 percentage points lower accuracy on paraphrased items compared to original questions. The findings suggest that high benchmark scores may reflect memorization of surface patterns rather than genuine reasoning capabilities, challenging current evaluation practices in the field.

## Method Summary
The study introduces a controlled protocol for measuring LLM generalization through paraphrase stress testing. The pipeline involves generating paraphrases of benchmark questions using targeted prompting, applying cleaning steps to preserve semantic fidelity, and evaluating two 7B models (Mistral-7B-Instruct and Qwen2.5-7B-Instruct) on ARC-Easy and ARC-Challenge datasets. The evaluation uses controlled decoding and multiple-choice output formatting to ensure consistent assessment conditions. The paraphrase-cleaning process includes automated checks to remove semantically altered or invalid variations while maintaining the original question meaning.

## Key Results
- Both evaluated models showed 6-10 point accuracy drops on paraphrased items versus original questions
- The surface-form brittleness was consistent across both ARC-Easy and ARC-Challenge datasets
- Results indicate benchmark scores may overestimate true reasoning capabilities due to reliance on brittle surface patterns

## Why This Works (Mechanism)
The surface-form brittleness occurs because LLMs trained on large corpora often learn to recognize specific patterns and phrasings rather than developing robust understanding of underlying concepts. When question phrasing changes through paraphrasing, models struggle to map the new surface form to their learned representations, revealing a dependency on exact or similar formulations seen during training. This mechanism suggests models are leveraging statistical associations with specific word patterns rather than performing genuine logical reasoning.

## Foundational Learning
1. **Surface-form brittleness**: The tendency of models to fail when input surface forms change while meaning remains constant. Needed to understand evaluation limitations; quick check: test model on paraphrased versions of known questions.

2. **Benchmark contamination**: When training data includes test questions or similar content. Needed to contextualize evaluation results; quick check: verify dataset overlap between training and test sets.

3. **Paraphrase generation**: Creating semantically equivalent text with different surface forms. Needed for stress testing evaluation; quick check: ensure generated paraphrases maintain original meaning.

## Architecture Onboarding
**Component Map**: Benchmark questions -> Paraphrase generation -> Cleaning pipeline -> Model evaluation -> Accuracy comparison
**Critical Path**: Original question → Controlled paraphrase generation → Automated cleaning → Model inference → Performance measurement
**Design Tradeoffs**: The study prioritizes semantic fidelity over paraphrase diversity, choosing controlled generation over natural variation to ensure valid comparisons. This tradeoff limits the stress test's ecological validity but increases experimental control.
**Failure Signatures**: Consistent accuracy drops across models and datasets when evaluated on paraphrased items; successful performance only on exact or near-exact matches to training data patterns.
**First 3 Experiments**: 1) Generate controlled paraphrases of benchmark questions, 2) Apply cleaning pipeline to validate semantic preservation, 3) Evaluate models on both original and paraphrased versions using identical inference settings.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to two 7B parameter models on ARC datasets, restricting generalizability
- Paraphrase generation process may not capture full diversity of natural paraphrasing
- Focus on multiple-choice questions may not reflect performance on open-ended reasoning tasks

## Confidence
- High: Surface-form brittleness finding based on systematic methodology and consistent quantitative results
- Medium: Broader implications for benchmark reliability, suggesting potential overestimation of capabilities
- Low: Proposed solutions, as the paper identifies the problem but doesn't extensively explore mitigation strategies

## Next Checks
1. Replicate study with larger models (70B+ parameters) and additional benchmark datasets across different domains
2. Conduct ablation studies to isolate impact of different paraphrase generation methods and cleaning steps
3. Implement controlled experiments comparing model performance on paraphrased vs. original items with verified training set contamination status