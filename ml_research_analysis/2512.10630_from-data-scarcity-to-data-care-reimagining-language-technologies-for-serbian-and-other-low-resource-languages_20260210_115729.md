---
ver: rpa2
title: 'From Data Scarcity to Data Care: Reimagining Language Technologies for Serbian
  and other Low-Resource Languages'
arxiv_id: '2512.10630'
source_url: https://arxiv.org/abs/2512.10630
tags:
- language
- data
- serbian
- languages
- linguistic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes Data Care, a framework grounded in CARE principles
  (Collective Benefit, Authority to Control, Responsibility, and Ethics), to address
  data bias in low-resource language technologies. Through interviews with ten scholars
  and practitioners, it identifies challenges including historical destruction of
  Serbian textual heritage, limited corpora, restrictive copyright, and engineering-first
  approaches that prioritize functionality over linguistic nuance.
---

# From Data Scarcity to Data Care: Reimagining Language Technologies for Serbian and other Low-Resource Languages

## Quick Facts
- arXiv ID: 2512.10630
- Source URL: https://arxiv.org/abs/2512.10630
- Reference count: 6
- This study proposes Data Care, a framework grounded in CARE principles (Collective Benefit, Authority to Control, Responsibility, and Ethics), to address data bias in low-resource language technologies.

## Executive Summary
This study introduces Data Care, a framework that embeds CARE principles into the design, annotation, and governance of language technology corpora to address systemic bias in low-resource languages. Through interviews with ten scholars and practitioners, the research identifies critical challenges facing Serbian language technologies: historical destruction of textual heritage, copyright restrictions, and engineering-first approaches that prioritize functionality over linguistic nuance. The framework reframes bias mitigation as an integral part of corpus development rather than a post-hoc technical fix. By incorporating community and interdisciplinary perspectives, Data Care offers a replicable model for building inclusive, sustainable, and culturally grounded language technologies for underrepresented languages.

## Method Summary
The research employed qualitative methods, conducting ten semi-structured interviews with scholars and practitioners in Serbian language technology between March and July 2025. The study synthesized insights from corpus linguistics, computer science, and digital humanities to develop the Data Care framework. While no specific algorithmic implementation is provided, the framework establishes procedural recommendations for building authentic native corpora, implementing opt-in copyright frameworks, and ensuring rigorous annotation that captures dialectal and phonological variants.

## Key Results
- Native language models trained on authentic data systematically outperform multilingual alternatives for low-resource languages by capturing morphosyntactic complexity and cultural-linguistic patterns
- Embedding CARE principles at corpus design stage addresses representational gaps at their source, reducing bias more effectively than post-hoc mitigation
- Addressing data scarcity requires coordinated institutional intervention across digitization, copyright reform, and corpus construction as foundational infrastructure

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Native language models trained on authentic data outperform multilingual alternatives for low-resource languages.
- Mechanism: Training on language-specific corpora captures morphosyntactic complexity, dialectal variation, and cultural-linguistic patterns that cross-lingual transfer cannot replicate.
- Core assumption: Authentic, high-quality training data in the target language exists or can be created in sufficient quantity.
- Evidence anchors:
  - [abstract] "dataset curation lacking cultural specificity" and reliance on "English-trained models" produces biased outputs
  - [section] Respondent Marko states: "what isn't in the data won't be in the model"; research cited shows native models "systematically outperform" multilingual alternatives (Virtanen et al., 2019)
  - [corpus] Neighbor paper "Cross-Lingual Transfer of Cultural Knowledge" confirms cultural knowledge transfer is asymmetric and incomplete
- Break condition: If authentic corpus falls below critical size threshold, native model may underperform even flawed multilingual transfer.

### Mechanism 2
- Claim: Embedding CARE principles at corpus design stage reduces data bias more effectively than post-hoc mitigation.
- Mechanism: By integrating Collective benefit, Authority to control, Responsibility, and Ethics into data selection, annotation, and governance decisions, the framework addresses representational gaps at their source.
- Core assumption: Stakeholders from linguistics, humanities, and speaker communities can meaningfully participate in technical decisions with appropriate translation of concerns.
- Evidence anchors:
  - [abstract] Framework "reframes bias mitigation from a post hoc technical fix to an integral component of corpus design, annotation, and governance"
  - [section] Vera notes: "We're implicitly blaming some abstract entity—an algorithm or dataset—instead of focusing on how datasets are created"; Ana argues "We're all responsible—from those who collect and label data to those who design the systems"
  - [corpus] Weak direct evidence; neighbor papers focus on data creation but not governance frameworks specifically
- Break condition: If interdisciplinary collaboration breaks down or community input is tokenized without implementation authority, CARE becomes performative rather than functional.

### Mechanism 3
- Claim: Addressing data scarcity requires coordinated institutional intervention across digitization, copyright reform, and corpus construction.
- Mechanism: Historical destruction of textual heritage + contemporary barriers (limited digitization, restrictive copyright, uncoordinated efforts) create compounding scarcity. Systematic intervention builds the foundational infrastructure that enables all downstream language technology.
- Core assumption: Political will and sustained investment exist or can be mobilized through reframing language technology as "core infrastructure."
- Evidence anchors:
  - [abstract] Challenges include "historical destruction of Serbian textual heritage, limited corpora, restrictive copyright"
  - [section] National Library destroyed 500,000+ volumes in 1941; Kišobran corpus shows Serbian at 7-9 billion words vs. English at 3-14 trillion; copyright described as why "larger high-quality corpus for Serbian does not exist"
  - [corpus] "ELR-1000" neighbor paper demonstrates community-generated dataset approach for endangered languages
- Break condition: If copyright holders refuse opt-in, institutions remain uncoordinated, or funding treats language technology as optional rather than infrastructural.

## Foundational Learning

- Concept: **Low-resource language (LRL)**
  - Why needed here: The entire paper centers on languages lacking sufficient digital corpora, annotated datasets, and computational resources for standard ML/NLP pipelines.
  - Quick check question: Can you name three structural factors that make a language "low-resource" beyond just speaker population size?

- Concept: **Cross-lingual transfer**
  - Why needed here: The paper critiques reliance on transferring knowledge from high-resource languages (typically English) to LRLs as producing "superficial" representations.
  - Quick check question: What types of linguistic or cultural knowledge might fail to transfer from English to Serbian through cross-lingual methods?

- Concept: **Corpus representativeness and bias**
  - Why needed here: The paper argues that bias originates in corpus design—what's included, excluded, and how it's annotated—rather than emerging solely from algorithms.
  - Quick check question: If a Serbian corpus were built entirely from web-scraped content, what populations and registers would likely be over- and under-represented?

## Architecture Onboarding

- Component map:
  - Corpus construction layer: Digitization pipeline (OCR for both Cyrillic/Latin), copyright clearance mechanism, source selection (literature, news, spoken, domain-specific), quality filtering
  - Annotation layer: Interdisciplinary annotation protocols, linguist/computational expert collaboration, bias documentation at annotation time
  - Model training layer: Native model training vs. multilingual fine-tuning decision point, evaluation benchmarks including cultural/linguistic quality metrics
  - Governance layer: CARE principle checkpoints, community input mechanisms, transparent documentation of data selection criteria

- Critical path: Corpus availability → Annotation quality → Model architecture choice → Evaluation with cultural metrics. Without authentic corpus, all downstream steps degrade.

- Design tradeoffs:
  - **Speed vs. authenticity**: Multilingual models provide immediate functionality but sacrifice linguistic/cultural accuracy; native models require sustained investment
  - **Breadth vs. depth**: Including related languages (Croatian, Bosnian) increases data volume but may dilute Serbian-specific features
  - **Open access vs. copyright compliance**: Restrictive copyright limits high-quality literary content; opt-in frameworks require sustained outreach

- Failure signatures:
  - Hallucinated transliterations (e.g., "аффекција" from English "affection") indicate training on machine-translated data rather than authentic text
  - Offensive or stereotyped outputs for identity-related prompts signal training data bias
  - Inability to handle both Cyrillic and Latin scripts or dialectal variation indicates narrow corpus

- First 3 experiments:
  1. **Corpus gap audit**: Map existing Serbian corpora against a typology of registers (literature, news, spoken, legal, medical, etc.) to quantify representational gaps and prioritize digitization/clearance efforts.
  2. **Annotation protocol pilot**: Test interdisciplinary annotation with a linguist + engineer pair on a sample corpus slice; document where disagreement reveals hidden assumptions about "quality" or "representativeness."
  3. **Bias probing baseline**: Before building native model, establish baseline bias measurements using the multilingual model currently available—test for stereotyped outputs, script handling failures, and cultural knowledge gaps to define what improvement looks like.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does implementing the Data Care framework yield measurable improvements in model performance, bias reduction, and cultural authenticity compared to standard corpus development practices?
- Basis in paper: [explicit] The paper proposes Data Care as a "replicable model" but does not empirically test it; outcomes are theorized rather than demonstrated.
- Why unresolved: The framework is presented as a conceptual contribution grounded in CARE principles, without applied validation in Serbian or other low-resource language contexts.
- What evidence would resolve it: Comparative studies training models on Data Care-guided corpora versus conventional web-scraped datasets, evaluated on linguistic accuracy, bias metrics, and community-assessed cultural authenticity.

### Open Question 2
- Question: What institutional mechanisms and incentive structures enable sustained state investment in native language models for low-resource languages?
- Basis in paper: [explicit] Respondents describe limited institutional engagement, weak strategic support, and a national mindset that fails to recognize language technologies as infrastructure; the paper calls for state intervention but does not specify how to achieve it.
- Why unresolved: The paper diagnoses the problem (lack of political will and funding) but does not identify which policy instruments, governance models, or advocacy strategies have proven effective.
- What evidence would resolve it: Comparative case studies of countries that successfully established sovereign language models (e.g., Bulgarian BgGPT, Dutch GPT-NL), analyzing funding mechanisms, legislative frameworks, and stakeholder coalitions.

### Open Question 3
- Question: How can opt-in copyright frameworks balance author rights with the data needs of low-resource language technologies?
- Basis in paper: [explicit] The paper identifies restrictive copyright as a critical barrier and cites Iceland and Wales as examples of opt-in models, but does not evaluate their effectiveness or applicability to Serbia.
- Why unresolved: The proposal to adopt similar frameworks is suggested without analysis of implementation challenges, participation rates, or legal harmonization requirements.
- What evidence would resolve it: Empirical data on participation rates, corpus quality, and model performance in jurisdictions with opt-in copyright arrangements, alongside legal analysis of compatibility with EU and Serbian copyright law.

## Limitations
- Framework is primarily qualitative, built on interviews rather than empirical validation of proposed technical interventions
- Copyright reform pathway (opt-in frameworks) is theoretically sound but untested at scale for literary corpora
- Claims about native model superiority lack empirical backing in the paper itself

## Confidence
- **High confidence**: Problem identification (historical destruction, copyright barriers, representational bias) is well-supported by documented facts and expert testimony
- **Medium confidence**: Mechanism that integrating humanities expertise reduces bias is plausible but relies on untested assumptions about interdisciplinary collaboration effectiveness
- **Low confidence**: Specific technical claims about native model superiority lack empirical backing in the paper itself

## Next Checks
1. **Corpus gap audit**: Map existing Serbian corpora against a comprehensive register typology to quantify representational gaps and prioritize digitization/clearance efforts.
2. **Annotation protocol pilot**: Test interdisciplinary annotation with linguist + engineer pairs on a sample corpus slice; document where disagreement reveals hidden assumptions about "quality" or "representativeness."
3. **Baseline bias measurement**: Establish quantitative bias baselines using the current multilingual Serbian model (testing for stereotyped outputs, script handling failures, cultural knowledge gaps) to define what "improvement" looks like before building native alternatives.