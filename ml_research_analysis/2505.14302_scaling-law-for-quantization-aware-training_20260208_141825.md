---
ver: rpa2
title: Scaling Law for Quantization-Aware Training
arxiv_id: '2505.14302'
source_url: https://arxiv.org/abs/2505.14302
tags:
- quantization
- scaling
- error
- training
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a unified scaling law for quantization-aware
  training (QAT) of large language models (LLMs), addressing the gap in existing scaling
  laws that ignore the effects of training data size and quantization granularity.
  The authors model quantization error as a function of model size, training data
  volume, and quantization group size, validated through 268 QAT experiments.
---

# Scaling Law for Quantization-Aware Training

## Quick Facts
- arXiv ID: 2505.14302
- Source URL: https://arxiv.org/abs/2505.14302
- Reference count: 40
- One-line primary result: Unified scaling law for QAT quantization error as function of model size, training tokens, and quantization granularity

## Executive Summary
This paper proposes a unified scaling law for quantization-aware training (QAT) of large language models (LLMs), addressing the gap in existing scaling laws that ignore the effects of training data size and quantization granularity. The authors model quantization error as a function of model size, training data volume, and quantization group size, validated through 268 QAT experiments. They find that quantization error decreases with larger model size but increases with more training tokens and coarser quantization granularity. By decomposing quantization error into weight and activation components, they identify activation quantization—especially in the FC2 layer—as the primary bottleneck, caused by outliers. Applying mixed-precision quantization to the FC2 layer reduces quantization error and sensitivity to granularity. The study also shows that with more training data, weight quantization error eventually exceeds activation error, highlighting the importance of optimizing both error sources in future QAT algorithm design.

## Method Summary
The authors propose a unified scaling law for W4A4 QAT quantization error, modeled as δ_p(N, D, G) = k · D^γ_D · (log₂(G))^γ_G / N^γ_N, where N is model size, D is training tokens, and G is quantization group size. They validate this through 268 experiments training Llama3-style models (74M–595M parameters) on OLMo2-Mix-1124 dataset with varying N, D, and G values. The models use AbsMax quantizer for weights and AbsMax/LAC for activations, trained with AdamW optimizer. The law is fitted by minimizing Huber loss between predicted and observed log error, with validation against a separate 973M parameter model.

## Key Results
- Unified scaling law accurately predicts W4A4 QAT quantization error across model sizes, training tokens, and quantization granularities
- Weight quantization error grows more rapidly with training tokens than activation error (γ_D = 0.1610 vs 0.0331)
- FC2 layer activation quantization is primary bottleneck due to outliers, with kurtosis averaging 89
- Mixed-precision quantization (8-bit FC2 input) reduces total error and sensitivity to granularity

## Why This Works (Mechanism)

### Mechanism 1
A unified scaling law accurately predicts W4A4 QAT quantization error as a function of model size, training tokens, and quantization group size. The proposed formula explicitly models the interactions: larger models distribute error across more parameters (denominator N^γ_N), more training tokens amplify error (numerator D^γ_D), and coarser granularity increases error (numerator (log₂(G))^γ_G). The scaling law may not hold for extremely low bits (e.g., ternary) or MoE architectures.

### Mechanism 2
Weight and activation quantization errors exhibit different sensitivities to training data volume, with weight error growing faster. As models see more data, weights adapt to capture more information, making them harder to quantize without loss. The weight quantization error sensitivity to training tokens (γ_D = 0.1610) is much larger than activation's 0.0331. This relationship is derived for 4-bit quantization and may not generalize directly to other precisions.

### Mechanism 3
Activation quantization error in the FC2 layer is the primary bottleneck for W4A4 QAT, caused by persistent outliers. The input to the FC2 layer comes from the SwiGLU module, which creates complex activation distributions with amplified outliers (high kurtosis). These outliers exceed the representational capacity of 4-bit quantization. Mitigating this via mixed-precision (8-bit for FC2 input) reduces total error and sensitivity to granularity. If outlier suppression techniques are perfected, this bottleneck could be removed, shifting focus to weight error.

## Foundational Learning

**Chinchilla Scaling Law**
- Why needed here: The proposed QAT scaling law is built upon and compared to the Chinchilla law. It adds a quantization error term to the standard loss prediction.
- Quick check question: How does the Chinchilla law relate model size and training data to loss?

**Quantization Granularity**
- Why needed here: The paper shows quantization error is highly sensitive to group size. A core finding is that finer groups (smaller G) reduce error.
- Quick check question: What does "per-token/channel" quantization mean, and how does it compare to "per-tensor" in terms of granularity and expected accuracy?

**Kurtosis**
- Why needed here: The paper uses kurtosis to identify layers with outlier-heavy activation distributions (specifically FC2), which are problematic for 4-bit quantization.
- Quick check question: Does a higher kurtosis value indicate more or fewer outliers in a distribution?

## Architecture Onboarding

**Component map**: Llama3-style architecture with SwiGLU FFN and GQA → Quantizer (AbsMax for weights/activations, LAC for coarse activations) → Scaling Law Model (Eq. 5) → Target Architecture (Llama-3 style with SwiGLU FFN and GQA). The FC2 layer within the FFN is identified as the critical bottleneck.

**Critical path**: 1) Fit or use the provided scaling law parameters (Table 1) for your target model size/data/granularity. 2) Prioritize fine-grained quantization groups (e.g., G=32). 3) Implement mixed-precision by keeping the FC2 input at 8-bit. 4) Adjust QAT algorithms to jointly address both weight and activation error, especially if training on very large datasets.

**Design tradeoffs**:
- **Granularity vs. Inference Overhead**: Finer quantization groups (lower G) improve accuracy but require storing more scaling factors, increasing memory and potentially inference complexity.
- **Mixed-Precision vs. Complexity**: Using 8-bit for FC2 input improves error but requires a kernel that supports different precisions for different layers.
- **Data Scaling vs. Weight Error**: Training on more data improves model capability but disproportionately increases weight quantization error, potentially requiring higher precision or more sophisticated weight quantization techniques.

**Failure signatures**:
- Unexpectedly high error on large models: Using per-tensor or very coarse (e.g., G=256) quantization, especially without mixed-precision on FC2.
- Divergence of scaling law predictions: The law may not extrapolate to MoE models or precisions other than W4A4 (e.g., ternary).
- Focus on activation outliers only: In high data-to-parameter ratio regimes, weight error may become dominant, and methods focusing solely on activations will see diminishing returns.

**First 3 experiments**:
1. Replicate scaling law fit on a smaller scale: Train a series of small models (e.g., 74M-297M) with different G values and plot the quantization error δ_W4A4 against N, D, and G to verify the trends in Figure 4.
2. Validate FC2 mixed-precision benefit: Take a pre-trained QAT model and switch the FC2 input quantization from 4-bit to 8-bit. Measure the drop in quantization error as shown in Figure 9b.
3. Test weight vs. activation sensitivity: Train two models with high data-to-parameter ratios. Measure the decomposition of error (δ_W4A16 vs δ_W16A4) to see if weight error is indeed becoming the dominant factor, as suggested by Figure 8.

## Open Questions the Paper Calls Out

### Open Question 1
How does the unified QAT scaling law apply to Mixture-of-Experts (MoE) architectures regarding the ratio of weight to activation quantization error? The authors state they did not conduct experiments on MoE architectures and hypothesize that the different ratio of weight parameters to activation sizes may result in a different error ratio compared to dense models. This remains unresolved because the current law is fitted exclusively on dense Llama-style models, whereas MoE models have sparse activation patterns and significantly higher parameter counts, potentially altering the relationship between model size (N) and quantization error (δ). Empirical validation on MoE models (e.g., Mixtral) would resolve this.

### Open Question 2
Can the proposed unified scaling law be accurately extended to extremely low-bit quantization methods, such as ternary (1.58-bit) quantization? Appendix A notes that while this study focuses on W4A4, "investigating unified scaling laws for these [ternary] settings is also valuable." This is unresolved because the functional form of the error δ_p is derived from 4-bit experiments; it is unverified if the logarithmic dependency on group size (G) or the power-law dependency on data/model size applies when precision drops to binary or ternary levels. Fitting the proposed equation to ternary QAT experiments would resolve this.

### Open Question 3
Does the QAT scaling law maintain its predictive accuracy when extrapolating to model sizes significantly larger than 1 billion parameters? Appendix A notes that the largest compute consumed was for a 595M model (with validation up to 973M) and suggests accuracy "would be further improved by increasing both the model size and the number of training tokens." This remains unresolved because scaling laws often exhibit deviations at extreme scales (e.g., Chinchilla vs. Kaplan), and the current fit relies on experiments with models ≤ 973M parameters. Conducting QAT experiments on models in the 7B to 70B range and measuring deviation from predicted quantization error contour would resolve this.

## Limitations
- Law validity is limited to tested parameter space (models up to 595M, 50B tokens, 4-bit quantization)
- Findings specific to W4A4 quantization may not directly translate to other bit-widths
- Extrapolation to trillion-parameter models or extremely large datasets is not guaranteed

## Confidence

**High Confidence**: The experimental results strongly support the existence of a power-law relationship between quantization error and the three factors (model size, training tokens, granularity). The fit quality (R²) on the 268 experiments is reported as good, and the individual mechanisms (e.g., FC2 as bottleneck, weight error growing faster than activation error) are well-demonstrated within the tested range.

**Medium Confidence**: The decomposition of total W4A4 error into W4A16 and W4A4 components to analyze weight vs. activation contributions is sound, but it relies on the assumption that these errors are additive. While reasonable, the exact nature of their interaction could be more complex. The identification of FC2 layer outliers as the primary bottleneck is well-supported, but the generalizability of kurtosis as a universal proxy for quantization difficulty requires more investigation.

**Low Confidence**: The scaling law's predictions for extremely large models or datasets are speculative. The paper does not provide strong evidence for how the law breaks down or what the new limiting factors would be in such scenarios. The sensitivity analysis (fitted parameters γ_N, γ_D, γ_G) is specific to the Llama3 architecture and the OLMo2-Mix-1124 dataset.

## Next Checks
1. **Cross-Architecture Validation**: Apply the scaling law to a different LLM architecture (e.g., GPT-NeoX) trained on a different dataset (e.g., SlimPajama) to test robustness to architectural and data distribution changes.
2. **Extreme Data Regime Test**: Train a 595M or 1.1B parameter model on a dataset significantly larger than 50B tokens (e.g., 100B-200B). Measure quantization error and its decomposition to verify if weight error surpassing activation error holds and identify new dominant error source.
3. **Low-Bit Precision Extrapolation**: Conduct small-scale study (e.g., 74M-148M models) using 3-bit or ternary quantization. Compare observed quantization error trends with W4A4 scaling law predictions to assess validity and identify required modifications for ultra-low precision.