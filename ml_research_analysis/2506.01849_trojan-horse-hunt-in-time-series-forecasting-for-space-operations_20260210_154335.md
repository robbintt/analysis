---
ver: rpa2
title: Trojan Horse Hunt in Time Series Forecasting for Space Operations
arxiv_id: '2506.01849'
source_url: https://arxiv.org/abs/2506.01849
tags:
- space
- competition
- trigger
- triggers
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This competition addressed the problem of detecting and reconstructing
  trojan triggers in poisoned satellite telemetry forecasting models. The task was
  to identify 45 hidden triggers injected into N-HiTS models for multivariate time
  series forecasting.
---

# Trojan Horse Hunt in Time Series Forecasting for Space Operations

## Quick Facts
- **arXiv ID:** 2506.01849
- **Source URL:** https://arxiv.org/abs/2506.01849
- **Reference count:** 11
- **Key outcome:** Competition to detect and reconstruct 45 hidden trojan triggers in poisoned N-HiTS satellite telemetry forecasting models using a modified Neural Cleanse optimization method

## Executive Summary
This paper presents a competition task focused on detecting and reconstructing trojan triggers in poisoned satellite telemetry forecasting models. The core challenge involves identifying 45 hidden triggers injected into N-HiTS models for multivariate time series forecasting, where participants must reconstruct triggers they cannot directly observe. The task leverages a modified Neural Cleanse approach that optimizes for behavioral divergence between clean and poisoned forecasts while tracking trigger coherence. The competition ran for three months with the goal of advancing security methods for safety-critical space AI applications.

## Method Summary
The competition task involves reconstructing 45 hidden trojan triggers (75-sample, 3-channel time series segments) injected into poisoned N-HiTS forecasting models trained on ESA-ADB satellite telemetry data. Participants are provided with clean data, a reference model trained on clean data, and the poisoned models, but not the triggers themselves. The baseline method uses a modified Neural Cleanse approach that optimizes a candidate input perturbation to maximize behavioral divergence between poisoned model forecasts and clean baseline forecasts while tracking trigger coherence. The optimization employs a composite loss function that balances forecast divergence, trigger shape tracking, and high-magnitude trigger preference.

## Key Results
- Competition successfully framed trojan trigger reconstruction as a benchmark task for space AI security
- Modified Neural Cleanse baseline provides semi-automatic reconstruction approach
- NMAErange metric enables fair comparison across diverse trigger patterns
- 45 triggers successfully reconstructed as competition deliverables

## Why This Works (Mechanism)

### Mechanism 1: Optimization-Based Reverse Engineering of Additive Triggers
The method iteratively optimizes a candidate input perturbation to maximize behavioral divergence between a poisoned model's forecast and a clean baseline. The loss function formulation simultaneously maximizes forecast difference when the trigger is applied, encourages the forecast to mirror the poisoned input's shape, and favors high-magnitude, expressive triggers. This works under the assumption that the trojan is a consistent, additive pattern causing a distinct deviation in the model's output that can be isolated via optimization against normal behavior.

### Mechanism 2: Behavioral Isolation via Reference Model Comparison
A known clean model serves as a reference to isolate the trojan's specific behavioral signature from the model's normal forecasting function. By probing both poisoned and clean models with inputs including candidate triggers, any consistent trigger-specific divergence in the poisoned model's output that is absent in the clean model's output can be attributed to the backdoor. This comparison provides ground truth for normal behavior, assuming the clean reference model is a faithful proxy for the poisoned model's pre-compromised state.

### Mechanism 3: Robust Evaluation via Scale-Invariant, Bounded Error Metric
The NMAErange metric normalizes absolute error by the ground truth trigger's value range, making scores interpretable as a fraction of the trigger's scale and bounding maximum penalty to 1. This enables fair and robust comparison of reconstruction quality across diverse triggers while preventing outliers from dominating the average score across all 45 triggers.

## Foundational Learning

- **Backdoor Attacks / Trojan Triggers in Machine Learning:** Understanding that a trojan is a hidden pattern injected during training that causes a specific malfunction at inference is prerequisite to any detection or reconstruction task. Quick check: How does a backdoor attack differ from adversarial evasion or standard data poisoning?
- **Time Series Forecasting with N-HiTS (Neural Hierarchical Interpolation):** The target architecture uses hierarchical interpolation to produce forecasts at different time scales. Understanding its structure is essential for designing triggers that exploit its structure and for interpreting its forecasts. Quick check: How does N-HiTS use hierarchical interpolation to produce forecasts at different time scales, and what are its key learnable components?
- **Inverse Problems & Gradient-Based Input Optimization:** The core reconstruction method involves solving an inverse problem via optimization. Familiarity with techniques for optimizing inputs to achieve a target output is required. Quick check: What are the primary challenges when optimizing an input (a trigger) for a fixed neural network, as opposed to optimizing network weights?

## Architecture Onboarding

- **Component map:** ESA-ADB dataset (channels 44-46 from Mission1) -> Clean reference N-HiTS model + 45 poisoned N-HiTS models -> Modified Neural Cleanse optimization algorithm -> NMAErange evaluation metric -> CSV submission file
- **Critical path:** Understand trigger injection mechanism (additive, 75-sample, 3-channel pattern causing output copy behavior) -> Analyze baseline optimization loss (div, track, L2 terms) -> Establish performance baseline by running provided code -> Iterate on algorithm by modifying loss terms and hyperparameters
- **Design tradeoffs:** Baseline vs. novelty (Neural Cleanse is semi-automatic and not designed for time series), known vs. unknown trigger length (competition provides length to reduce complexity), automation vs. tuning (baseline requires manual parameter tuning)
- **Failure signatures:** Optimization divergence (loss fails to converge, δ becomes noise), incorrect trigger (reconstructed pattern very different from ground truth), no behavioral change (applying reconstructed δ causes no forecast change), overfitting to public leaderboard
- **First 3 experiments:** 1) Baseline execution and profiling (run provided code, record NMAErange scores and runtime), 2) Loss function ablation (systematically disable/re-weight div, track, L2 terms, measure impact on quality), 3) Trigger effect validation (apply reconstructed triggers to both poisoned and clean models, confirm forecast deviation is specific to poisoned model)

## Open Questions the Paper Calls Out

### Open Question 1
Can trigger reconstruction methods be effective when the trigger length and position are unknown, unlike the controlled competition setup? The paper notes that while the trigger length is fixed (75 samples) in the competition to manage complexity, "In practical scenarios, the trigger length is typically unknown" (Section 1.5). This remains unresolved because the provided baseline and competition data constrain the search space to a fixed window size, avoiding the computational and algorithmic challenges of searching for variable-length triggers in continuous streams. Evidence would be a method capable of reconstructing triggers with high fidelity on data where duration and injection point are variable and unlabeled.

### Open Question 2
How can algorithms distinguish between learned adversarial triggers and legitimate signal patterns resulting from non-adversarial data drifts? Section 1.2 states that "poisoning" signatures can appear similar to "novel sequences of telecommands" or data drifts, emphasizing the need for experts to "assess whether they are indeed adversarial or not." This remains unresolved because the current task focuses on mathematical reconstruction of anomalous patterns, but determining the semantic intent (malicious vs. operational) of the reconstructed segment remains a manual, expert-driven process. Evidence would be an automated classification framework that flags reconstructed triggers as adversarial or benign with a quantifiable false positive rate.

### Open Question 3
Can the semi-automated Neural Cleanse baseline be replaced by a fully automated approach suitable for operational safety-critical systems? The authors admit the adapted Neural Cleanse baseline is "semi-automatic and not flexible enough to be used in practice" (Section 1.7). This remains unresolved because the baseline relies on manual parameterization of loss weights to balance divergence and tracking, which is brittle for deployment in dynamic space environments. Evidence would be a competition submission or post-hoc study demonstrating robust trigger reconstruction without manual hyperparameter tuning for individual models.

## Limitations
- Exact hyperparameter values and mathematical formulations for loss function components are unspecified
- NMAErange metric may behave unpredictably for triggers with very small value ranges
- Reference model assumption may be violated if training dynamics differ from original clean state
- Optimization approach assumes additive, consistent trigger pattern that may not generalize to sophisticated trojan implementations

## Confidence

- **High confidence:** Overall task structure, use of reference model for behavioral isolation, and NMAErange evaluation metric are clearly specified and well-founded
- **Medium confidence:** Mechanism of how modified Neural Cleanse loss function works depends on unspecified hyperparameter values and exact mathematical formulations
- **Low confidence:** Performance ceiling of baseline method and difficulty of 45-trigger reconstruction task cannot be assessed without experimental results

## Next Checks

1. **Hyperparameter Sensitivity Analysis:** Systematically vary α, β, and λ in the loss function and measure their impact on reconstruction quality to determine optimal weighting and establish robustness
2. **Reference Model Fidelity Test:** Train two separate clean N-HiTS models on identical clean data with different random seeds. Compare their forecasting behavior to assess variability and validate reference model assumption
3. **Trigger Robustness Evaluation:** Apply reconstruction algorithm to triggers of varying amplitude, frequency content, and duration (within 75-sample constraint) to identify failure modes and establish method's limits of applicability