---
ver: rpa2
title: The impact of fine tuning in LLaMA on hallucinations for named entity extraction
  in legal documentation
arxiv_id: '2506.08827'
source_url: https://arxiv.org/abs/2506.08827
tags:
- extraction
- entities
- text
- entity
- disability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of extracting information about
  traffic accidents from legal documents, focusing on identifying disability percentages
  and compensation amounts. A two-step method is proposed: first, segmenting documents
  using either regular expressions or vector-based semantic search; second, applying
  large language models (LLaMA-2, LLaMA-3, and GPT-4 Turbo) to extract entities from
  relevant segments.'
---

# The impact of fine tuning in LLaMA on hallucinations for named entity extraction in legal documentation

## Quick Facts
- arXiv ID: 2506.08827
- Source URL: https://arxiv.org/abs/2506.08827
- Reference count: 20
- One-line primary result: Fine-tuning LLaMA-2 7B reduces hallucinations by 47.78% for legal named entity extraction

## Executive Summary
This paper addresses the challenge of extracting disability percentages and compensation amounts from Argentine traffic accident legal documents. The authors propose a two-stage pipeline combining semantic document segmentation with large language model extraction. They demonstrate that vector-based segmentation significantly outperforms traditional regex methods, and that fine-tuning substantially reduces hallucinations in smaller LLaMA models while maintaining high accuracy. The approach enables automated generation of actionable statistics for insurance purposes while preserving data sovereignty through open-source models.

## Method Summary
The methodology employs a two-step process: first, documents are segmented into 120-token blocks using either regex or vector-based semantic search with MiniLM-L12-v2 embeddings. Relevant segments are retrieved via FAISS similarity search and expanded to 360-token contexts. Second, entities are extracted using LLaMA-2, LLaMA-3, or GPT-4 Turbo models. Fine-tuning is performed using QLoRA with 8-bit quantization on attention layers only. The dataset was created from 650 Argentine legal rulings, filtered to 278 documents, with entities labeled by GPT-4 Turbo and manually verified. Two training datasets were created: one with all entities and one cleaned to only include samples where entities were present in retrieved segments.

## Key Results
- Semantic segmentation achieved 83.33% accuracy versus 39.5% for regex-based methods
- LLaMA-2 70B fine-tuned achieved 79.4% extraction accuracy, while LLaMA-3 8B base achieved 76.6%
- Fine-tuning LLaMA-2 7B reduced hallucinations by 47.78% compared to base model
- GPT-4 Turbo achieved the highest accuracy at 86.1%

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning with domain-specific data substantially reduces hallucinations in smaller LLMs during named entity extraction. LoRA-based fine-tuning on attention layers appears to align the model's generation patterns with the specific extraction task, reducing the tendency to generate plausible-but-incorrect entities when the information is absent from the context. Core assumption: Hallucinations stem from task misalignment rather than fundamental model architecture limitations. Evidence: LLaMA-2 7b fine-tuned showed 47.78% improvement over base version, and related work on reducing hallucinations via fine-tuning supports this pattern.

### Mechanism 2
Vector-based semantic segmentation retrieves more relevant document contexts than keyword-based regex for long legal documents. Embedding models create dense representations of text blocks; cosine similarity search against task-specific queries retrieves segments based on semantic relevance rather than exact keyword matches, capturing context that regex misses. Core assumption: The embedding model captures task-relevant semantics across languages (Spanish legal text). Evidence: Semantic search achieved 83.33% segmentation accuracy versus 39.5% for regex, with RAG-based NER approaches showing similar improvements.

### Mechanism 3
Dataset quality (noise reduction) has greater impact on fine-tuning effectiveness than dataset size alone. Removing samples where entities are not present in retrieved segments reduces contradictory training signals, leading to better generalization despite fewer samples. Core assumption: Label noise from imperfect segment-entity alignment actively harms learning. Evidence: Dataset 2 (861 samples) significantly outperformed Dataset 1 (1120 samples) with a 9.4% accuracy gain, though this claim requires further validation.

## Foundational Learning

- Concept: **LoRA/QLoRA fine-tuning**
  - Why needed here: Full fine-tuning of 70B models requires >96GB VRAM; QLoRA with 8-bit quantization enables training within hardware constraints.
  - Quick check question: Can you explain why LoRA reduces memory requirements while preserving model quality?

- Concept: **RAG (Retrieval-Augmented Generation)**
  - Why needed here: Legal documents exceed model context windows; segmentation + retrieval provides relevant context for extraction.
  - Quick check question: How does chunk size affect retrieval quality vs. context completeness?

- Concept: **Hallucination detection via token probabilities**
  - Why needed here: Paper tested minimum probability threshold approach but found it insufficient for entity extraction tasks.
  - Quick check question: Why might high-confidence tokens still correspond to incorrect extractions?

## Architecture Onboarding

- Component map:
  Document Preprocessing -> PDF extraction -> text cleaning -> header filtering
  Segmentation: Token blocking (120 tokens) -> embedding (MiniLM-L12-v2 or ada-002) -> FAISS indexing -> query-based retrieval -> context expansion (360 tokens)
  Extraction LLM: LLaMA-2/3 (base or fine-tuned) or GPT-4 Turbo with task-specific prompts
  Training Pipeline: QLoRA on attention layers, LR=5e-5, 3 epochs, LoRA rank=8, 8-bit quantization

- Critical path:
  1. Segmentation quality determines extraction ceiling (83% segmentation accuracy limits overall pipeline)
  2. Fine-tuning dataset quality > quantity (Dataset 2 outperformed Dataset 1 with 23% fewer samples)
  3. Model scale vs. generation trade-off (LLaMA-3 8B base ≈ LLaMA-2 70B fine-tuned)

- Design tradeoffs:
  - Open-source vs. proprietary: LLaMA-2 70B fine-tuned (79.4%) vs. GPT-4 Turbo (86.1%)—7% accuracy gap for data sovereignty
  - Segment length: 120 tokens for embedding vs. 360 after expansion—longer context improves disambiguation but increases compute
  - Base vs. fine-tuned: For LLaMA-3, base model may be sufficient; for LLaMA-2 7B, fine-tuning is essential

- Failure signatures:
  - Low recall with high accuracy: Model being conservative, refusing to extract when uncertain
  - Hallucinations spike on out-of-distribution segments: Base LLaMA-2 7B hallucinates 47% more than fine-tuned version
  - Segmentation misses relevant sections: Regex approach misses 60% of entities vs. semantic search

- First 3 experiments:
  1. Baseline segmentation comparison: Implement both regex and semantic search on 30 documents; measure segment retrieval accuracy before any LLM extraction.
  2. Hallucination quantification: Run base LLaMA-2 7B on segments known to lack target entities; count false extractions at temperature=0.
  3. Dataset cleaning ablation: Fine-tune on Dataset 1 vs. Dataset 2; isolate the impact of sample quality vs. quantity on validation accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
How can hallucinations be reliably detected in legal named entity extraction tasks when token probability thresholds prove insufficient? The authors evaluated minimum probability threshold approach but found it insufficient because models can be confident but wrong. An alternative detection metric or decoding strategy demonstrating statistical correlation with factual errors would resolve this.

### Open Question 2
Can the proposed extraction methodology effectively scale to detect jurisdictional anomalies or individual judge-level biases in ruling patterns? The study focused on aggregate accuracy and macro-level statistics rather than granular anomaly detection across different legal jurisdictions. A follow-up study applying the model to multi-jurisdictional dataset would validate detected outliers against known judicial biases.

### Open Question 3
How does the performance of fine-tuned generative LLMs compare to classical discriminative models like BERT for legal named entity recognition? The study focused entirely on generative models versus regular expressions, leaving the comparative efficiency and accuracy of encoder-based architectures unexplored. A benchmark comparison on the same legal corpus would resolve this.

## Limitations
- Dataset size relatively small (1120 training samples, 120 test samples) may not capture full variability of legal document structures
- Spanish language focus limits generalizability to other jurisdictions
- Manual verification process for entity presence introduces potential subjectivity

## Confidence

- **High Confidence**: Semantic segmentation effectiveness over regex methods is well-supported with clear quantitative differences (39.5% vs. 83.33% segmentation accuracy). Hallucination reduction through fine-tuning is strongly evidenced (47.78% improvement for LLaMA-2 7B).
- **Medium Confidence**: Claim that dataset quality matters more than quantity is supported by observed improvements but lacks controlled experiments varying only quality vs. quantity.
- **Low Confidence**: Assertion that base LLaMA-3 8B performs comparably to fine-tuned LLaMA-2 70B may be model-specific and task-dependent.

## Next Checks

1. Create two additional datasets—one with Dataset 2's quality but Dataset 1's quantity, and vice versa—to isolate whether sample quality or quantity drives observed improvements.

2. Apply the fine-tuned LLaMA-2 70B model to a different legal domain (e.g., contract analysis or criminal rulings) to assess whether hallucination reduction generalizes beyond traffic accident documentation.

3. Conduct a blind human evaluation where annotators classify extractions as correct, hallucinated, or ambiguous, comparing base vs. fine-tuned models on segments known to lack entities to quantify actual hallucination reduction.