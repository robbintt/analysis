---
ver: rpa2
title: Psychometric-Based Evaluation for Theorem Proving with Large Language Models
arxiv_id: '2502.00855'
source_url: https://arxiv.org/abs/2502.00855
tags:
- evaluation
- llms
- theorem
- difficulty
- theorems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a psychometric-based evaluation framework
  for theorem proving with large language models (LLMs). The key challenge addressed
  is that current evaluation methods, which rely on proof pass rates across all theorems,
  fail to account for varying theorem importance and result in high computational
  costs.
---

# Psychometric-Based Evaluation for Theorem Proving with Large Language Models

## Quick Facts
- **arXiv ID**: 2502.00855
- **Source URL**: https://arxiv.org/abs/2502.00855
- **Reference count**: 40
- **Primary result**: Introduces adaptive evaluation reducing testing costs by 76.13% while providing more fine-grained LLM performance discrimination through ability scores.

## Executive Summary
This paper addresses the challenge of evaluating large language models for theorem proving by proposing a psychometric-based adaptive evaluation framework. Traditional evaluation methods relying on proof pass rates across all theorems are computationally expensive and fail to account for varying theorem importance. The authors introduce a method that annotates theorems with difficulty and discrimination metrics based on LLM performance, then dynamically selects the most informative theorems for testing. Experiments show the method reduces evaluation costs significantly while providing more nuanced ability scores that better distinguish between model capabilities, particularly for models with similar pass rates but different underlying success rates.

## Method Summary
The method involves two main phases: dataset annotation and adaptive evaluation. First, 488 theorems from the miniF2F dataset are annotated by four annotation LLMs (codegeex4-9B, llemma-7B, TheoremLlama, DeepSeek-Prover-V1.5-RL) using 128 proof attempts per theorem. Difficulty and discrimination metrics are computed using Item Response Theory formulas. The resulting miniF2F-Graded dataset contains annotated theorems with these psychometric properties. For adaptive evaluation, the algorithm initializes an ability score θ=0.5 and iteratively selects the top 5 theorems by Fisher information (based on current θ and annotated metrics), attempts proofs, updates θ using success rates, and converges when θ stabilizes for 10 consecutive rounds. The process uses a 2048 token limit and Lean 4.13 for verification.

## Key Results
- The adaptive method reduces evaluation costs by 76.13% (using only 23% of theorems) while maintaining discriminative power.
- miniF2F-Graded shows monotonic decrease in pass rates from Level 1 to 4 for all LLMs, unlike the original miniF2F dataset.
- Ability scores better capture performance gaps between models with similar Pass@128 rates (e.g., DeepSeek-Prover-V1.5-RL vs SFT: 0.41% Pass@128 difference vs 0.0239 ability score difference).

## Why This Works (Mechanism)

### Mechanism 1: LLM-Calibrated Difficulty Annotation
The method computes theorem difficulty based on actual LLM performance rather than human judgment, using the formula `Difficulty(x) = -P'(x)/(1-P'(x))` where `P'(x)` is the attempt success rate across annotation LLMs. This reciprocal transformation amplifies differences near success-rate extremes, separating high/low difficulty theorems more clearly than linear scaling. The approach assumes LLMs perceive difficulty differently than humans, and a grading system aligned with model perception will discriminate better. Evidence shows pass rates decrease monotonically from Level 1→4 in miniF2F-Graded, unlike the original dataset where Level 2 avg > Level 1.

### Mechanism 2: Information-Theoretic Theorem Selection
The algorithm selects theorems that maximize Fisher information at the model's current ability score using `I(x,θ) = a(x)^f · P(x,θ) · (1-P(x,θ))`. This approach assumes the modified Fisher information function correctly identifies theorems that maximally reduce uncertainty about ability at score `θ`. The method reduces evaluation costs by using only 23% of theorems while maintaining discriminative power. The exclusion of recently used theorems (last 50) prevents overfitting but may reduce diversity in later stages.

### Mechanism 3: Iterative Ability Score Convergence with Success-Rate Weighting
Ability scores are updated based on per-theorem success rates using `θ ← θ + η · a(x) · (r_i - P(x,θ))`, where `r_i` is the success rate on theorem `i`. For sparse successes, a log transform `r_i ← log(1+r_i)` dampens noise. The method assumes this update rule approximates gradient descent on a latent ability parameter and that success-rate weighting captures more signal than binary pass/fail. Convergence requires `|θ - θ_prev| < 0.01` for 10 consecutive rounds, with the stability check mitigating but not eliminating oscillation risks for models with inconsistent performance.

## Foundational Learning

- **Item Response Theory (IRT) basics**: The method adapts IRT's difficulty/discrimination/ability framework to LLM evaluation. Understanding the 2-parameter logistic model clarifies why `P(x,θ) = 1/(1+e^{-a(x)(θ-b(x))})` is used. Quick check: Given discrimination `a=2`, difficulty `b=0.5`, and ability `θ=0.7`, compute `P(x,θ)`.

- **Fisher Information**: Theorem selection relies on maximizing information, requiring understanding why `I(x,θ) ∝ P·(1-P)` peaks when `P≈0.5` (maximum uncertainty reduction). Quick check: Why does a theorem with `P(x,θ)=0.95` provide less information about `θ` than one with `P(x,θ)=0.5`?

- **Pass@N vs Attempt Success Rate**: The paper argues Pass@N conflates models with different underlying success rates. Understanding this distinction is key to interpreting results. Quick check: Model A succeeds 1/128 times, Model B succeeds 64/128 times. Both have Pass@128=1. Which metric distinguishes them?

## Architecture Onboarding

- **Component map**: Dataset Annotation Pipeline -> 4 annotation LLMs → attempt success rates → difficulty/discrimination formulas → miniF2F-Graded (488 theorems with metrics) → Theorem Selection Module: Current `θ` + annotated metrics → Fisher information computation → top-5 selection with exclusion filter → Adaptive Test Loop: Selected theorems → proof attempts (128 per theorem) → success rates → ability update → convergence check → Proof Verification: Generated proofs → Lean 4.13 interaction → correctness check

- **Critical path**: 1. Annotate dataset once (expensive upfront; 4 LLMs × 488 theorems × 128 attempts) 2. For each evaluation run: initialize `θ=0.5`, iteratively select→test→update until convergence 3. Output final ability score and theorems used

- **Design tradeoffs**: Annotation LLM selection: Broader ability spread improves discrimination estimation but requires more models; current 4-model set may not generalize to future architectures. Hyperparameters `[f, η] = [0.49, 0.004]`: Tuned on specific LLMs; sensitivity not fully analyzed. Exclusion window (last 50 theorems): Prevents overfitting but may exclude informative theorems in later stages.

- **Failure signatures**: Non-convergence: Ability score oscillates without stabilizing → may indicate inconsistent model performance or poorly calibrated difficulty metrics. All theorems at difficulty extremes: If annotation LLMs cannot solve any Level 4 theorems (127 with difficulty=1, discrimination≈0), these provide no information and are effectively dead weight. Ranking inversion with Pass@128: If ability ranking diverges significantly from Pass@N on held-out data, the metrics may be misaligned with actual proof capability.

- **First 3 experiments**: 1. Validate annotation transfer: Annotate miniF2F using a different 4-LLM set. Compare difficulty rankings and check if evaluation results correlate with original annotation. 2. Convergence sensitivity: Run adaptive evaluation on a single LLM with varying `[f, η]` values. Measure convergence speed, final ability score variance, and number of theorems used. 3. Held-out generalization: For each evaluated LLM, hold out 20% of miniF2F-Graded. Run adaptive evaluation on 80%, then compute Pass@128 on held-out 20%. Test correlation between ability scores and held-out pass rates.

## Open Questions the Paper Calls Out

### Open Question 1
The paper states the authors plan to "expand the method to additional datasets" and hypothesize it will offer "even greater efficiency improvements when applied to larger-scale data collections." This remains unresolved as the study validates the method solely on the miniF2F dataset using Lean 4. Evidence would require experimental results showing convergence time and cost reduction percentages on datasets significantly larger than miniF2F or formalized in languages other than Lean.

### Open Question 2
The authors explicitly exclude tree search methods because they "face challenges in establishing uniform evaluation standards" compared to whole-proof generation. The current method assumes a "whole-proof generation" approach, but it's unclear if the framework can be effectively adapted for tree-search-based theorem provers. Evidence would require a modified adaptive algorithm that successfully differentiates performance disparities between tree-search models using a compatible success metric.

### Open Question 3
The method relies on a fixed set of four Annotation LLMs to calculate metrics, but the authors note they plan to update annotations using "progressively enhanced SOTA models." It's unclear if the "miniF2F-Graded" dataset remains valid or requires total re-annotation as state-of-the-art models used for annotation improve or change. Evidence would require a correlation analysis of metric rankings when generated by different combinations of annotation models versus a ground-truth theoretical distribution.

## Limitations
- The method's generalization depends heavily on the annotation LLM pool representing the ability distribution of future evaluation targets, with only 4 annotation models potentially limiting applicability to new architectures.
- The choice of hyperparameters `[f, η]` is not sensitivity-analyzed, and alternative values could alter rankings or convergence behavior.
- The claim that manual grading is inferior is based on indirect comparison rather than direct empirical testing of manual grading's discriminative power on LLMs.

## Confidence

- **High Confidence**: The psychometric framework (IRT-based annotation, ability score computation, Fisher information selection) is internally consistent and mathematically sound. The monotonic relationship between miniF2F-Graded levels and LLM pass rates directly supports the claim that LLM-calibrated difficulty better discriminates.
- **Medium Confidence**: The cost reduction claim (76.13% using 23% of theorems) is supported by internal experiments but lacks external validation on a held-out LLM population. The assertion that ability scores better reflect capability gaps than Pass@N is demonstrated within the paper's LLM set but not confirmed on independent data.
- **Low Confidence**: The claim that manual grading is inferior is based on indirect comparison rather than direct empirical testing of manual grading's discriminative power on LLMs.

## Next Checks

1. **Annotation Transfer Test**: Re-annotate miniF2F using a disjoint 4-LLM set (e.g., swap in Code-Llama-7B and MetaMath-Llemma for two current annotators). Compare difficulty rankings and validate if evaluation results (ability scores) correlate with the original annotation.

2. **Hyperparameter Sensitivity Analysis**: Run adaptive evaluation on a single LLM (e.g., DeepSeek-Prover-V1.5-RL) with varying `[f, η]` (e.g., `f ∈ [0.3, 0.7]`, `η ∈ [0.001, 0.01]`). Measure convergence speed, final ability score variance, and number of theorems used to identify stable regions.

3. **Held-Out Generalization Test**: For each evaluated LLM, hold out 20% of miniF2F-Graded. Run adaptive evaluation on the remaining 80%. Compute Pass@128 on the held-out set and test correlation with the ability score from the adaptive run.