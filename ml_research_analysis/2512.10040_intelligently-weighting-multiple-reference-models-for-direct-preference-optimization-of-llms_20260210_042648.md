---
ver: rpa2
title: Intelligently Weighting Multiple Reference Models for Direct Preference Optimization
  of LLMs
arxiv_id: '2512.10040'
source_url: https://arxiv.org/abs/2512.10040
tags:
- reference
- mrpo
- preference
- methods
- ultrafeedback
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work evaluates methods for setting reference weights in multiple-reference\
  \ preference optimization (MRPO). The authors propose four new strategies\u2014\
  two offline (validation-based discrimination and accuracy weighting) and two online\
  \ (sliding-window cumulative weighting and Thompson sampling)."
---

# Intelligently Weighting Multiple Reference Models for Direct Preference Optimization of LLMs

## Quick Facts
- **arXiv ID**: 2512.10040
- **Source URL**: https://arxiv.org/abs/2512.10040
- **Reference count**: 6
- **Key outcome**: All four proposed multiple-reference weighting methods (VDW, VAW, SWCW, TSW) outperform existing MRPO approaches on test accuracy, but single-reference DPO with 6/7 reference models consistently outperforms all multiple-reference methods.

## Executive Summary
This paper evaluates methods for setting reference weights in multiple-reference preference optimization (MRPO). The authors propose four new strategies—two offline (validation-based discrimination and accuracy weighting) and two online (sliding-window cumulative weighting and Thompson sampling). Experiments on UltraFeedback and SafeRLHF datasets using Qwen2.5-0.5B as policy model with seven reference models show all four proposed methods outperform existing MRPO approaches in test accuracy. More surprisingly, single-reference DPO with any of six out of seven reference models consistently outperforms all tested multiple-reference approaches, suggesting that multiple-reference methods may not provide practical advantages. The reference model's primary role appears to be as a grammatical coherence check rather than a source of task-specific distillation.

## Method Summary
The paper introduces four new reference weighting strategies for multiple-reference preference optimization: validation-based discrimination weighting (VDW), validation accuracy weighting (VAW), sliding-window cumulative weighting (SWCW), and Thompson sampling weighting (TSW). These methods address the problem of setting per-example reference model weights αₖ in MRPO/MDPO. VDW and VAW are offline methods that compute weights on held-out validation data using discriminative confidence or accuracy respectively. SWCW and TSW are online methods that adaptively update weights during training using sliding-window discriminative confidence or Thompson sampling for multi-armed bandit selection. The methods are evaluated against baselines including original MRPO/MDPO and single-reference DPO on UltraFeedback and SafeRLHF datasets using Qwen2.5-0.5B-Instruct as the policy model with seven reference models.

## Key Results
- All four proposed weighting methods (VDW, VAW, SWCW, TSW) outperform existing MRPO approaches on test accuracy
- Single-reference DPO with any of six out of seven reference models consistently outperforms all tested multiple-reference approaches
- Reference model discriminative confidence does not correlate with accuracy, leading VDW and SWCW to overweight Phi-3-Medium-128K despite its poor accuracy
- Thompson sampling shows potential but faces challenges from noisy validation rewards (30-40% of mini-batches show zero accuracy change)
- MRPO and MDPO with β=0.1 experience numerical instability on long sequences, causing NaN gradients on UltraFeedback

## Why This Works (Mechanism)

### Mechanism 1: Validation-based weighting prevents overfitting from label-dependent weighting
The original MRPO per-example weighting (Eq. 3) computes αₖ using the same labeled pair (x, y⁺, y⁻) that the loss is computed on, causing "hyperparameter leakage" and biased empirical loss. By computing αₖ on a held-out validation set (VDW, VAW), the weighting signal becomes independent of training gradients, yielding a faithful estimator of generalization performance. This breaks when validation and test distributions diverge significantly or when the validation set is too small to reliably estimate reference quality.

### Mechanism 2: Thompson sampling treats reference selection as a K-armed bandit
Each reference model is an arm; after each gradient step, a binary reward records whether stochastic validation accuracy improved. Beta posteriors are updated per arm (αₖ for success, βₖ for failure), and arm selection follows sampled posterior values. This balances exploration and exploitation but breaks when validation accuracy changes are too noisy (as observed, 30-40% of mini-batches show zero change), causing slow or random convergence.

### Mechanism 3: Single-reference DPO outperforms multi-reference approaches
Multiple KL-regularization terms toward different references create competing gradient directions that "obfuscate the fine-tuning signal." The preference dataset provides the dominant signal; the reference merely prevents degradation of grammatical coherence. This breaks if reference models possess strongly complementary task-specific capabilities and the policy is sufficiently capacity-limited to benefit from explicit distillation.

## Foundational Learning

- **Concept: Direct Preference Optimization (DPO) and Length-Normalized DPO (LN-DPO)**
  - **Why needed here:** The entire paper builds on DPO as the base algorithm; LN-DPO is used throughout to avoid length bias. Understanding the loss function (Eq. 1) and why length normalization helps is prerequisite to understanding MRPO/MDPO modifications.
  - **Quick check question:** Given a preference pair (x, y⁺, y⁻), what does the DPO loss optimize? Why would longer responses receive artificially higher implicit rewards without length normalization?

- **Concept: KL Divergence Regularization in RLHF**
  - **Why needed here:** MRPO/MDPO extend DPO by adding KL regularization toward multiple reference models. Understanding why KL regularization prevents policy degradation is essential to grasp why weighting schemes matter—and why the paper concludes reference models are "coherence checks."
  - **Quick check question:** In RLHF, why is the policy regularized toward a reference model via β·D_KL(π_θ || π_ref)? What happens if β is too small or too large?

- **Concept: Thompson Sampling for Multi-Armed Bandits**
  - **Why needed here:** One of the proposed methods (TSW) formulates reference selection as a K-armed bandit using Thompson sampling. Understanding Beta-Bernoulli conjugacy and the exploration-exploitation tradeoff is necessary to implement and debug this method.
  - **Quick check question:** In Thompson sampling with K arms and Beta(αₖ, βₖ) priors, how does the posterior update after observing reward r ∈ {0, 1}? Why does this balance exploration and exploitation?

## Architecture Onboarding

- **Component map:**
  Policy model (Qwen2.5-0.5B-Instruct) -> Reference models (K=7) -> Weight computation module (VDW, VAW, SWCW, or TSW) -> Loss module (MRPO or MDPO) -> Validation monitor -> Policy parameters

- **Critical path:**
  1. Precompute reference log-probabilities πᵏ_ref(y⁺|x) and πᵏ_ref(y⁻|x) for all K references on train/val/test splits
  2. For offline methods (VDW, VAW), compute fixed α before training using validation set statistics
  3. For online methods (SWCW, TSW), initialize α and update per mini-batch based on sliding-window discriminative confidence or Thompson sampling posterior
  4. Forward pass: compute MRPO/MDPO loss using current α; backward pass: update policy parameters
  5. Evaluate test accuracy periodically; monitor for NaN gradients

- **Design tradeoffs:**
  - MRPO vs. MDPO: MRPO uses harmonic mean of reference distributions; MDPO uses linear combination of DPO losses. Paper finds MDPO "effectively strictly preferable" but both suffer numerical instability on long sequences
  - Offline vs. online weighting: Offline methods (VDW, VAW) are simpler and stable but cannot adapt to training dynamics. Online methods (SWCW, TSW) adapt but introduce noise and instability
  - Soft vs. one-hot α: Soft weights blend references but may dilute signal; one-hot provide clear single-reference updates but cause variable gradient directions

- **Failure signatures:**
  - NaN gradients on long sequences: Occurs with Original MRPO, SWCW, and some MDPO variants on UltraFeedback. Caused by numerical instability in MRPO correction term L⁻_ref − L⁺_ref
  - Weight collapse to overconfident reference: VDW and SWCW assign dominant weight to Phi-3-Medium-128K despite its poor accuracy, because discriminative confidence ≠ correctness
  - Noisy Thompson sampling rewards: 30-40% of mini-batches show zero validation accuracy change, providing little learning signal

- **First 3 experiments:**
  1. Reproduce single-reference DPO baseline: Fine-tune Qwen2.5-0.5B on UltraFeedback/SafeRLHF using each of the 7 reference models individually. Verify that 6/7 outperform multi-reference baselines.
  2. Compare VDW vs. VAW on validation set size: Vary validation split size and measure whether accuracy-based weighting (VAW) is more robust to small validation sets than discriminative-confidence weighting (VDW).
  3. Debug Thompson sampling reward signal: Instrument TSW to log per-mini-batch validation accuracy changes and arm selection frequencies. Quantify noise-to-signal ratio; test alternative reward definitions.

## Open Questions the Paper Calls Out

- **Question:** Do multiple-reference approaches provide any benefit over single-reference DPO when reference models have substantially heterogeneous performance and complementary strengths?
  - **Basis:** Authors note "our reference models generally had comparable performance on UltraFeedback and SafeRLHF: our findings could potentially change if there were larger differences between our reference models."

- **Question:** Can the numerical instability of MRPO/MDPO on long-sequence datasets be resolved while preserving theoretical benefits?
  - **Basis:** "Both the Original and VAW methods when paired with either the MDPO or MRPO loss functions fail to train on the UltraFeedback dataset, consistently resulting in NaNs."

- **Question:** Does the finding that single-reference DPO suffices generalize to larger policy models beyond 0.5B parameters?
  - **Basis:** "Our base model Qwen2.5-0.5B-Instruct is also on the smaller side and training dynamics could change if we had a more powerful base model."

## Limitations
- The finding that single-reference DPO consistently outperforms multi-reference approaches is likely constrained to settings where reference models have comparable performance levels
- The Thompson sampling method's effectiveness is significantly limited by the noisy validation reward signal, suggesting practical limitations for real-world deployment
- The paper does not explore heterogeneous reference quality scenarios or examine whether multi-reference approaches might excel when combining models with complementary strengths

## Confidence

- **High confidence:** The experimental finding that 6/7 single-reference DPO models outperform all tested multi-reference approaches on both UltraFeedback and SafeRLHF datasets
- **Medium confidence:** The theoretical explanation that multiple KL-regularization terms create competing gradient directions that "obfuscate the fine-tuning signal"
- **Low confidence:** The conclusion that Thompson sampling shows "potential" given the reported 30-40% noise rate in validation accuracy changes

## Next Checks

1. Test heterogeneous reference quality scenarios: Repeat the single-vs-multi reference comparison using a carefully curated set of reference models with deliberately varied performance levels (e.g., mix high-quality general models with specialized safety models).

2. Ablation study on regularization strength: Systematically vary the KL regularization coefficient β across a wider range for both single and multi-reference DPO to quantify how reference model weighting interacts with regularization strength.

3. Reward signal quality analysis for Thompson sampling: Conduct a controlled experiment measuring the correlation between validation accuracy changes and actual test performance improvements across mini-batches. Additionally, test alternative reward definitions to determine if the fundamental limitation is the validation metric itself.