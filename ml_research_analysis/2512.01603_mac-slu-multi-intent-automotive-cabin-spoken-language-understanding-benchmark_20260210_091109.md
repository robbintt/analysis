---
ver: rpa2
title: 'MAC-SLU: Multi-Intent Automotive Cabin Spoken Language Understanding Benchmark'
arxiv_id: '2512.01603'
source_url: https://arxiv.org/abs/2512.01603
tags:
- arxiv
- language
- lalms
- llms
- mac-slu
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the need for more complex and diverse datasets
  for Spoken Language Understanding (SLU) tasks, particularly in the automotive domain,
  and the absence of a unified benchmark for evaluating Large Language Models (LLMs)
  and Large Audio Language Models (LALMs) on these tasks. The core method idea involves
  introducing MAC-SLU, a novel Multi-Intent Automotive Cabin Spoken Language Understanding
  Dataset, which features authentic and complex multi-intent data derived from real-world
  automotive text commands and TTS-synthesized speech.
---

# MAC-SLU: Multi-Intent Automotive Cabin Spoken Language Understanding Benchmark

## Quick Facts
- arXiv ID: 2512.01603
- Source URL: https://arxiv.org/abs/2512.01603
- Authors: Yuezhang Peng; Chonghao Cai; Ziang Liu; Shuai Fan; Sheng Jiang; Hua Xu; Yuxin Liu; Qiguang Chen; Kele Xu; Yao Li; Sheng Wang; Libo Qin; Xie Chen
- Reference count: 0
- One-line primary result: MAC-SLU introduces a novel multi-intent automotive SLU benchmark, showing E2E LALMs avoid ASR error propagation and SFT significantly outperforms in-context learning.

## Executive Summary
This paper addresses the critical need for complex, diverse datasets for Spoken Language Understanding (SLU) tasks in the automotive domain by introducing MAC-SLU, a novel Multi-Intent Automotive Cabin Spoken Language Understanding Dataset. The dataset features authentic and complex multi-intent data derived from real-world automotive text commands and TTS-synthesized speech. The authors conduct a comprehensive benchmark of leading open-source LLMs and LALMs on this dataset, covering various methods such as in-context learning, supervised fine-tuning (SFT), and end-to-end (E2E) and pipeline paradigms.

The primary findings demonstrate that while LLMs and LALMs have the potential to complete SLU tasks through in-context learning, their performance lags significantly behind SFT. Additionally, E2E LALMs demonstrate performance comparable to pipeline approaches and effectively avoid error propagation from speech recognition. The paper provides a valuable benchmark for the community and releases the code and datasets publicly.

## Method Summary
The MAC-SLU dataset contains 8 domains, 81 intents, and 192 slots, featuring real-world automotive text commands and TTS-synthesized speech. The benchmark evaluates both text-based LLMs (Qwen3 family) and audio LALMs (Qwen2.5-Omni) using in-context learning (via vLLM) and supervised fine-tuning (via Llama-Factory with LoRA rank=16, alpha=32). The evaluation covers intent classification (IC) accuracy, slot filling (SF) F1 score, and overall accuracy (joint correctness). Pipeline systems use Whisper or Paraformer for ASR before LLM processing, while E2E systems process audio directly.

## Key Results
- LLMs and LALMs can complete SLU tasks through in-context learning but lag significantly behind SFT (29-47% overall accuracy gap).
- E2E LALMs achieve comparable performance to pipeline approaches and effectively avoid error propagation from speech recognition (pipeline drops 13-25% due to ASR errors).
- MAC-SLU features greater intent and slot diversity than existing datasets, with 81 intents and 192 slots compared to ATIS (16 intents) and SNIPS (7 intents).

## Why This Works (Mechanism)

### Mechanism 1: End-to-End LALMs Bypass ASR Error Propagation
E2E LALMs achieve comparable performance to pipeline systems by avoiding transcription error accumulation. Direct audio-to-semantics mapping eliminates the intermediate ASR step where character errors (CER 3.64%-10.40% in experiments) compound into semantic errors. Pipeline systems lose 13-25% overall accuracy from ASR noise alone.

### Mechanism 2: Supervised Fine-Tuning Activates Domain-Specific Semantic Parsing
SFT on task-specific data aligns model outputs to exact schema formats (domain-intent-slot structures), whereas in-context learning relies on pattern matching from few examples without weight updates. SFT boosted Qwen2.5-Omni-7B's performance over in-context learning, with increases of 29% in IC accuracy, 39% in SF F1 score, and 47% in overall accuracy.

### Mechanism 3: Multi-Intent Complexity Exposes Model Limitations
Multi-intent queries (up to 5 intents) require models to segment utterances into multiple semantic frames, a harder structural task than single-label classification. 81 intents and 192 slots create a finer-grained output space. Existing SLU datasets lack sufficient diversity and complexity, with ATIS and SNIPS containing only 16 and 7 intents respectively.

## Foundational Learning

- **SLU Task Structure (Intent Classification + Slot Filling)**: Why needed: MAC-SLU evaluates both IC (what user wants) and SF (parameters for the action). Understanding joint evaluation is essential. Quick check: Can you explain why a model could achieve 91% IC accuracy but only 55% overall accuracy?

- **Pipeline vs End-to-End Paradigms**: Why needed: The paper benchmarks both approaches. Pipeline = ASR → text → NLU model. E2E = audio → semantics directly. Error propagation in pipeline is a central finding. Quick check: Why does a 3.64% character error rate in ASR cause >13% drop in SLU overall accuracy?

- **In-Context Learning vs Supervised Fine-Tuning**: Why needed: The paper evaluates both paradigms. ICL uses few-shot prompts without weight updates; SFT modifies model parameters. The 29-47% gaps are central results. Quick check: What are the trade-offs between deploying an ICL system vs an SFT system in production?

## Architecture Onboarding

- **Component map**: Audio waveforms → Speech Encoder → LALM Backbone → Output Parser → Structured Output
- **Critical path**: Audio → Encoder → LALM → Structured Output → Exact Match Evaluation
- **Design tradeoffs**:
  - E2E vs Pipeline: E2E avoids error propagation but requires speech-text aligned training data; Pipeline allows independent optimization of ASR and NLU but compounds errors
  - ICL vs SFT: ICL enables zero/few-shot deployment but underperforms significantly; SFT requires labeled data but achieves 55-60% overall accuracy
  - Model Size: Qwen3-32B outperforms Qwen3-8B in ICL (14.42% vs 10.69% OA), suggesting scaling matters, but SFT narrows gaps
- **Failure signatures**:
  - Low Overall Accuracy with high IC Accuracy: Slot filling failing; model captures intent but misses or misaligns slot values
  - Pipeline >> E2E performance: Likely insufficient audio-text alignment training or domain mismatch in speech encoder
  - High semantic correctness but low metrics: Exact string matching penalizes valid semantic variants
- **First 3 experiments**:
  1. Run Qwen3-8B + Paraformer pipeline with provided code. Verify CER and overall accuracy match reported ~47%
  2. Fine-tune Qwen2.5-Omni-7B on MAC-SLU training set using Llama-Factory with LoRA (rank=16, alpha=32). Compare to pipeline baseline
  3. On test set failures, categorize errors into: (a) ASR errors, (b) intent misclassification, (c) slot boundary errors, (d) valid semantic variants. Quantify each to guide next iteration

## Open Questions the Paper Calls Out

### Open Question 1
Can the performance gap between in-context learning and supervised fine-tuning (SFT) be closed for complex, multi-intent SLU tasks without relying on domain-specific training data? The conclusion states that "future work should focus on enhancing the in-context learning capabilities of models." Developing an in-context learning method that achieves comparable Overall Accuracy to SFT baselines on the MAC-SLU dataset would resolve this.

### Open Question 2
What evaluation metrics can accurately reflect the semantic correctness of slot filling predictions that differ lexically from the ground truth? The authors note that "standard SLU metrics relying on exact string matching likely underestimate the true intent comprehension capabilities," and conclude by calling for "exploring more semantically-aligned evaluation metrics." A proposed metric that aligns more closely with human judgment of semantic correctness than exact-match F1 scores would resolve this.

### Open Question 3
Does scaling Large Audio Language Models (LALMs) to larger parameter sizes yield proportional improvements in multi-intent SLU performance compared to text-based LLMs? The paper notes that the 7B LALM lagged behind the larger 32B text model, "indicating substantial potential for further performance improvements by scaling LALMs to larger sizes in the future." Benchmarking results of LALMs with parameter counts comparable to the largest text models (e.g., 30B+) on the MAC-SLU dataset would resolve this.

## Limitations

- The MAC-SLU dataset, while publicly released, remains unvalidated for whether its multi-intent utterances genuinely reflect natural automotive cabin speech patterns or if they're artificially constructed.
- The reported superiority of SFT over ICL (29-47% accuracy gains) depends entirely on having sufficient high-quality labeled training data, which isn't quantified in terms of data efficiency curves.
- The E2E vs pipeline comparison assumes the speech encoder's pretraining distribution adequately covers the automotive domain's acoustic characteristics - this remains untested across diverse noise conditions and speaker variations.

## Confidence

- **High Confidence**: The comparative evaluation methodology is sound, with clear metrics (IC Accuracy, SF F1, Overall Accuracy) and reproducible experimental setup using Llama-Factory and vLLM. The error propagation phenomenon from ASR to NLU in pipeline systems is well-documented and the MAC-SLU results align with established findings in the literature.

- **Medium Confidence**: The claim that MAC-SLU is the "first unified benchmark" for LLMs/LALMs on multi-intent automotive SLU tasks is credible given the cited dataset limitations, but requires verification that no concurrent work emerged during the review period. The superiority of SFT over ICL is robust but depends on the assumed sufficiency of training data.

- **Low Confidence**: The assertion that E2E LALMs "effectively avoid error propagation" assumes the speech encoder's performance generalizes beyond the test conditions. Without extensive cross-corpus validation (e.g., testing on non-TTS data, noisy environments, accented speech), this advantage remains provisional.

## Next Checks

1. **Cross-Corpus Generalization**: Test the best-performing Qwen2.5-Omni-7B model (post-SFT) on an independent automotive SLU dataset like Fluent Speech Commands or SNIPS to verify domain robustness beyond MAC-SLU's TTS-synthesized speech.

2. **Data Efficiency Analysis**: Re-run the SFT experiments with 10%, 30%, and 50% of the training data to establish performance scaling curves and determine the minimum labeled data threshold for SFT to outperform ICL.

3. **Error Source Attribution**: On the test set, manually categorize model failures into: (a) ASR errors, (b) intent misclassification, (c) slot boundary errors, (d) valid semantic variants incorrectly penalized by exact string matching. This will reveal whether the 13-25% pipeline degradation stems primarily from transcription errors or NLU limitations.