---
ver: rpa2
title: 'PERCS: Persona-Guided Controllable Biomedical Summarization Dataset'
arxiv_id: '2512.03340'
source_url: https://arxiv.org/abs/2512.03340
tags:
- summaries
- summary
- persona
- biomedical
- abstract
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces PERCS, a dataset of 500 biomedical abstracts
  with 2,000 expert-annotated persona-specific summaries tailored for four audiences:
  laypersons, pre-medical students, non-medical researchers, and medical experts.
  The dataset was created by generating initial summaries with GPT-4, then having
  medical experts review and correct them for factual accuracy and persona alignment
  using a detailed error taxonomy.'
---

# PERCS: Persona-Guided Controllable Biomedical Summarization Dataset

## Quick Facts
- arXiv ID: 2512.03340
- Source URL: https://arxiv.org/abs/2512.03340
- Authors: Rohan Charudatt Salvi; Chirag Chawla; Dhruv Jain; Swapnil Panigrahi; Md Shad Akhtar; Shweta Yadav
- Reference count: 40
- Primary result: Expert-annotated dataset of 2,000 persona-specific biomedical summaries showing measurable readability and vocabulary differences across four audience types.

## Executive Summary
This paper introduces PERCS, a dataset of 500 biomedical abstracts with 2,000 expert-annotated persona-specific summaries tailored for four audiences: laypersons, pre-medical students, non-medical researchers, and medical experts. The dataset was created by generating initial summaries with GPT-4, then having medical experts review and correct them for factual accuracy and persona alignment using a detailed error taxonomy. Evaluation showed high inter-rater agreement (Krippendorff's alpha 0.79–1.0) and consistently high quality across personas. Technical analysis revealed systematic differences in readability, vocabulary, and content depth by persona. Benchmarking four LLMs showed that few-shot prompting outperformed zero-shot and self-refine approaches, with GPT-4o and LLaMA-3.1 providing the most stable performance across personas.

## Method Summary
PERCS contains 500 biomedical abstracts (350 train/150 test) paired with four persona-specific summaries each, totaling 2,000 summaries. GPT-4 generated initial summaries using persona-specific prompts with three exemplars, then medical experts reviewed and corrected them using an 11-category error taxonomy. Quality was evaluated across four dimensions (comprehensiveness, layness, faithfulness, usefulness) using both automatic metrics (ROUGE, SARI, readability scores, SummaC) and human rubric scoring. Four LLMs were benchmarked using zero-shot, few-shot, and self-refine prompting strategies, with few-shot emerging as the most reliable approach.

## Key Results
- Expert review achieved high inter-rater agreement (Krippendorff's alpha 0.79–1.0) across all personas
- Persona-specific summaries showed measurable differences in readability, vocabulary, and semantic overlap with source abstracts
- Few-shot prompting consistently outperformed zero-shot and self-refine approaches across all evaluation metrics
- GPT-4o and LLaMA-3.1 demonstrated the most stable performance across different personas

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Few-shot in-context examples improve persona-aligned biomedical summarization more reliably than self-refinement.
- Mechanism: Providing 3 persona-matched exemplars in the prompt grounds the LLM's lexical choices, syntactic complexity, and information density to the target audience profile, reducing drift toward generic or misaligned outputs.
- Core assumption: In-context examples encode persona-specific patterns (vocabulary level, detail depth) that generalize within the biomedical domain.
- Evidence anchors:
  - [abstract]: "Few-shot prompting achieved the best balance of comprehensiveness, readability, and faithfulness."
  - [section]: Table 5 shows few-shot models achieving highest normalized composite scores across personas; self-refine often reduced lexical overlap (e.g., R-1 dropped from 0.602 to 0.529 for GPT-4o lay summaries).
  - [corpus]: Limited direct corroboration—neighbor papers focus on summarization methods but not persona-specific few-shot comparisons.
- Break condition: If source abstracts diverge significantly from exemplar topics or structure, exemplar guidance may mislead rather than help.

### Mechanism 2
- Claim: Expert-in-the-loop correction reduces hallucinations and misalignment in LLM-generated medical summaries.
- Mechanism: Physicians identify and correct errors using a 11-category taxonomy (incorrect definitions, entity errors, contradictions, hallucinations), then revise summaries for factual accuracy and persona relevance before release.
- Core assumption: Expert medical knowledge catches subtle factual errors that automatic metrics miss.
- Evidence anchors:
  - [abstract]: "Each summary in PERCS was reviewed by physicians for factual accuracy and persona alignment using a detailed error taxonomy."
  - [section]: Table 1 defines error types; inter-annotator agreement (Krippendorff's α = 0.992 overall) indicates reliable expert correction.
  - [corpus]: PlainQAFact paper addresses factual consistency evaluation but does not validate expert-correction mechanisms directly.
- Break condition: If expert reviewers lack domain-specific expertise for niche topics, corrections may be incomplete or introduce new errors.

### Mechanism 3
- Claim: Persona-specific summaries exhibit measurable differences in readability, lexical diversity, and semantic overlap with source abstracts.
- Mechanism: As target persona expertise increases, summaries become shorter, more lexically diverse (higher TTR, MTLD), more complex (higher DCRS, CLI), and closer to abstract semantics (higher BERTScore).
- Core assumption: Readability formulas and lexical diversity metrics meaningfully capture persona-appropriate language variation.
- Evidence anchors:
  - [abstract]: "Technical validation shows clear differences in readability, vocabulary, and content depth across personas."
  - [section]: Table 4 shows lay summaries (DCRS=8.17, BERT_S=0.811) vs. expert summaries (DCRS=11.76, BERT_S=0.890).
  - [corpus]: Neighbor papers on plain language adaptation confirm readability differences but do not provide multi-persona benchmarks.
- Break condition: Readability metrics may not capture conceptual accessibility (e.g., simplified terms without adequate explanation).

## Foundational Learning

- Concept: **In-context learning (few-shot prompting)**
  - Why needed here: Understanding how exemplars shape LLM output style and content without weight updates is critical for designing effective prompts for each persona.
  - Quick check question: Given a lay-persona prompt, would adding a researcher-style exemplar improve or degrade output alignment?

- Concept: **Readability metrics (FKGL, DCRS, CLI)**
  - Why needed here: These metrics quantify whether generated summaries match the intended persona's reading level.
  - Quick check question: Why might lower readability scores not guarantee better comprehension for lay audiences?

- Concept: **Factual consistency evaluation (SummaC, hallucination detection)**
  - Why needed here: Biomedical summarization requires faithfulness to source material; understanding NLI-based consistency metrics helps interpret benchmark results.
  - Quick check question: What types of factual errors might SummaC miss in medical summarization?

## Architecture Onboarding

- Component map:
  Data layer: 500 abstracts × 4 personas = 2,000 summaries (JSON format with id, abstract, summaries{layman, premed, researcher, expert})
  Generation layer: GPT-4 initial generation → Expert correction (11-error taxonomy) → Quality evaluation (4 rubric dimensions)
  Evaluation layer: Automatic metrics (ROUGE, SARI, FKGL, DCRS, CLI, LENS, SummaC) + human rubric (comprehensiveness, layness, faithfulness, usefulness)

- Critical path:
  1. Abstract selection (PLOS + PLABA sources, topic diversity)
  2. Persona-specific prompt design (Table 8)
  3. LLM generation with few-shot exemplars
  4. Expert review and error correction
  5. Human quality evaluation
  6. Benchmarking with normalized metric comparison

- Design tradeoffs:
  - Dataset size (500 abstracts) vs. annotation cost (expert MD review)
  - Prompt specificity vs. generalization across biomedical subdomains
  - Automatic metrics vs. human evaluation for faithfulness assessment

- Failure signatures:
  - Self-refine degrading performance (R-1 drops in Table 5): indicates self-critique may introduce drift
  - Gemini producing overly long summaries with unnecessary technical terms for lay audiences (Table 6)
  - Mistral extracting irrelevant statistical details (p-values) inappropriate for persona

- First 3 experiments:
  1. Reproduce few-shot baseline for lay-persona summaries using provided prompts (Table 8) and 3 exemplars; compare ROUGE/SARI to Table 5 benchmarks.
  2. Ablate persona prompt components (remove jargon-avoidance instruction) to measure impact on DCRS and layness scores.
  3. Test zero-shot vs. few-shot on held-out test split (150 abstracts) for expert persona to validate whether experts require fewer exemplars.

## Open Questions the Paper Calls Out
- Can hybrid methods combining in-context examples with targeted feedback mechanisms improve faithfulness and information control over standard few-shot prompting?
- How can self-refinement mechanisms be modified to yield consistent improvements over few-shot prompting in persona-based summarization?
- Can fine-tuning open-source models on the PERCS training split correct specific weaknesses (e.g., missing definitions) observed in prompting baselines?

## Limitations
- Limited dataset size (500 abstracts, 2,000 summaries) constrains generalizability across biomedical subdomains
- Unknown exemplars used in few-shot prompting are not specified in the paper or appendix
- API access and model versions are not documented with specific snapshots

## Confidence
- **High confidence**: The mechanism that expert-in-the-loop correction improves factual accuracy and persona alignment, supported by Krippendorff's alpha of 0.992 and detailed error taxonomy
- **Medium confidence**: The superiority of few-shot prompting over self-refine and zero-shot approaches, based on composite normalized scores (Table 5), though exact exemplars are unknown
- **Low confidence**: The assertion that self-refine consistently degrades performance, as this may depend on specific prompt design and iteration criteria not fully specified in the paper

## Next Checks
1. Replication of persona alignment differences: Run the provided prompts on a small subset (e.g., 10 abstracts) with GPT-4 and measure DCRS, CLI, and BERTScore differences across personas. Verify that lay summaries have significantly lower DCRS and CLI values than expert summaries, confirming the claimed readability gradient.

2. Few-shot exemplar sensitivity test: Create three different sets of persona-matched exemplars and measure their impact on ROUGE and layness scores for lay-persona summaries. Determine whether exemplar quality significantly affects persona alignment, testing the core assumption that exemplars encode generalizable persona patterns.

3. Self-refine ablation study: Apply self-refine to GPT-4o outputs for 20 test abstracts and compare ROUGE-1 scores before and after refinement. Quantify the frequency and magnitude of performance degradation to validate whether self-refine systematically introduces drift.