---
ver: rpa2
title: 'From Basic Affordances to Symbolic Thought: A Computational Phylogenesis of
  Biological Intelligence'
arxiv_id: '2508.15082'
source_url: https://arxiv.org/abs/2508.15082
tags:
- relations
- mapping
- object
- memory
- multi-place
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the fundamental question of what distinguishes\
  \ human symbolic thought from the cognitive abilities of other animals. The authors\
  \ propose that two kinds of hierarchical integration\u2014multi-place predicates\
  \ and structure mapping\u2014are minimal requirements beyond dynamic binding to\
  \ achieve symbolic cognition."
---

# From Basic Affordances to Symbolic Thought: A Computational Phylogenesis of Biological Intelligence

## Quick Facts
- **arXiv ID**: 2508.15082
- **Source URL**: https://arxiv.org/abs/2508.15082
- **Reference count**: 0
- **Key outcome**: Architectures with both multi-place predicates and structure mapping successfully solved symbolic tasks, while those lacking either capability failed, supporting the hypothesis that both capacities are necessary for basic symbolic thought.

## Executive Summary
This paper addresses the fundamental question of what distinguishes human symbolic thought from the cognitive abilities of other animals. The authors propose that two kinds of hierarchical integration—multi-place predicates and structure mapping—are minimal requirements beyond dynamic binding to achieve symbolic cognition. Through 17 simulations of cognitive architectures with varying capabilities, they systematically compare performance on tasks requiring dynamic binding, multi-place predicates, structure mapping, or combinations thereof. The results show that architectures with both multi-place predicates and structure mapping successfully solve symbolic tasks, while those lacking either capability fail, supporting the hypothesis that both capacities are necessary for basic symbolic thought.

## Method Summary
The authors tested their hypothesis using 17 simulations of cognitive architectures with varying capabilities, systematically comparing their performance on tasks requiring dynamic binding, multi-place predicates, structure mapping, or combinations thereof. They created four architecture variants by toggling mapping learning rate (μ ∈ {0, 0.9}) and single-place vs multi-place predicate representations. The simulations used hand-coded LISAese representations with Perception and Memory analogs, where Perception serves as driver and Memory as recipient. Propositions were fired once per simulation (twice for MO task). Success was measured by whether the Affordance semantic unit fires in synchrony with the Critical object semantic unit, visualized as time traces of unit activations.

## Key Results
- Architectures with both multi-place predicates and structure mapping successfully solved all symbolic tasks
- Removing either multi-place predicates or structure mapping caused failure on relational reasoning tasks
- Basic affordance inference succeeded without structure mapping but failed on allocentric relations
- The findings suggest that the evolution of symbolic cognition may have been constrained by the need for multiple independent mutations to occur in the same organism

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Dynamic binding via temporal synchrony is necessary but insufficient for symbolic thought; it enables generalization from few examples by separating invariants (features) from their arguments.
- **Mechanism**: Neurons representing relational roles fire in synchrony (40–80 Hz gamma) with neurons representing their fillers (arguments) and out of synchrony with other bindings. Proposition units integrate these bindings over a slower timescale (10 Hz theta/beta). This separation prevents "entanglement" (conjunctive coding), allowing "red" to be treated as an invariant property independent of the specific object.
- **Core assumption**: Biological neural systems can exploit precise timing (synchrony/asynchrony) to represent structure.
- **Evidence anchors**: Page 5 description of temporal synchrony for "larger-than" relation; Page 37 discussion of invariants and binding necessity; arXiv:2512.14709 on attention as binding.

### Mechanism 2
- **Claim**: Integrating multiple role bindings into multi-place predicates (relations) is the minimal requirement for reasoning about allocentric (world-to-world) relations, which cannot be reduced to single-place affordances.
- **Mechanism**: Separate role-binding units are connected to a shared localist Proposition unit. This hierarchical structure forces the system to treat the relation R(x, y) as a whole, constraining the mapping process. This allows the system to override misleading semantic similarities between individual roles.
- **Core assumption**: Cognitive architectures must represent relations explicitly as structures of bindings, not just as correlated features.
- **Evidence anchors**: Page 7 on hierarchical composition of role-bindings; Page 25 on RO and R&M architectures overcoming semantic bias; arXiv:2505.13180 on symbolic predicates for visual planning.

### Mechanism 3
- **Claim**: Structure mapping is an independent capacity from forming multi-place predicates; it is required to track correspondences across contexts and use one system to reason about another.
- **Mechanism**: The architecture divides structures into a "driver" (focus of attention) and "recipient" (long-term memory). As the driver activates propositions one-by-one, the system learns mapping connections (μ) between corresponding elements. It enforces 1-to-1 mapping constraints, allowing early mappings to constrain later ones.
- **Core assumption**: Mapping is an active, incremental learning process constrained by structural consistency, not just passive similarity matching.
- **Evidence anchors**: Page 14 on LISA's sequential mapping process; Page 28 on MO task failure without learned mappings; arXiv:2503.04135 on LLM reasoning limitations.

## Foundational Learning

- **Concept**: Dynamic Binding via Synchrony
  - **Why needed here**: To understand how the paper proposes neural hardware solves the "variable binding" problem without infinite conjunctive neurons.
  - **Quick check question**: If neurons A and B fire in synchrony, and C fires out of synchrony with them, which two concepts are bound together?

- **Concept**: Allocentric vs. Egocentric Relations
  - **Why needed here**: The paper distinguishes "affordances" (often egocentric, like "graspable" where the hand is implicit) from true relations (allocentric, like "fits-inside") to define the boundary of symbolic thought.
  - **Quick check question**: Can the predicate "can-grasp(x)" be reduced to a single-place predicate according to the authors? (Yes, if the "hand" argument is fixed).

- **Concept**: Systematicity and Compositionality
  - **Why needed here**: The paper posits that symbolic thought allows for unbounded expression from finite elements (recursion).
  - **Quick check question**: Why does the "power set" of expressions grow faster with recursion over multi-place predicates than with single-place predicates alone?

## Architecture Onboarding

- **Component map**: Semantic Units -> Token Units (Objects, Predicates, Role-Bindings) -> Proposition Units -> Mapping Connections

- **Critical path**:
  1. Input: Perception structure (Driver) activates Propositions
  2. Unpacking: Propositions activate Role-Bindings, which activate Predicates and Objects
  3. Binding: Semantic units fire in phase-locked synchrony based on Role-Binding constraints
  4. Mapping: Semantic overlap activates candidate matches in Recipient (Memory); Mapping Connections (μ) refine these choices based on 1-to-1 constraints
  5. Inference: Successful mapping triggers the Affordance semantic unit in Memory to fire in synchrony with the Object in Perception

- **Design tradeoffs**:
  - Sequential vs. Parallel: LISA maps sequentially (one proposition at a time) rather than in parallel (like ACME/SME). This mimics human working memory limits but creates path-dependency (early mappings bias later ones)
  - Hand-coded vs. Learned Semantics: The paper notes semantic features were hand-coded to control similarity (Page 11), whereas modern approaches might learn these, risking entanglement

- **Failure signatures**:
  - "Cat" Architecture (DBO only): Can infer basic affordances (walkability) but fails on allocentric relations (fits-inside) because it lacks the Proposition unit to bind two roles
  - "Balint's" Architecture: Has relations in memory but cannot perceive them in the driver; fails relational tasks, demonstrating that knowing a relation is insufficient without perceiving it
  - Transformer/LLM Failure: Tends to hybridize objects or relations in statistically unlikely scenarios (e.g., "plane on a seagull") due to entangled representations and lack of explicit dynamic binding (Page 40)

- **First 3 experiments**:
  1. DBO Task (Basic Affordance): Set mapping rate μ=0 and use single-place predicates. Verify that the system can still bind an affordance to a critical object based on semantic similarity
  2. RO Task (Relational Reasoning): Enable multi-place predicates but keep μ=0. Test if the system can infer "fits-inside" by utilizing the shared Proposition unit to overcome semantic distractors
  3. MO Task (Mapping Only): Disable multi-place predicates (use single-place) but enable mapping (μ > 0). Test if the system can maintain object correspondence across time steps to infer an affordance despite feature ambiguity

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Do patients with Balint's syndrome fail to infer novel allocentric spatial relations between objects (e.g., whether one unfamiliar object can be stacked atop another)?
- **Basis in paper**: [explicit] The authors state: "The failure of the Balint's version of the simulation also constitutes a novel prediction: Patients with Balint's syndrome should be unable to decide whether it is possible to, say, stack one novel object on top of another."
- **Why unresolved**: This prediction emerged from simulation results but has not been empirically tested with actual patients.
- **What evidence would resolve it**: Behavioral testing of Balint's syndrome patients on novel object stacking/fitting tasks compared to matched controls.

### Open Question 2
- **Question**: Can transformer-based AI models solve analogical reasoning problems through mechanisms functionally equivalent to structure mapping, or do they rely solely on statistical regularities in training data?
- **Basis in paper**: [explicit] The authors note: "It is worth pointing out that it is not well understood exactly how these models solve analogies" and discuss whether attention mechanisms might substitute for dynamic binding.
- **Why unresolved**: The internal mechanisms by which LLMs succeed on analogy tasks remain opaque; it is unclear whether they implement genuine structure mapping or approximate it statistically.
- **What evidence would resolve it**: Systematic testing of LLMs on novel analogies with carefully controlled featural confounds, combined with interpretability analyses of their internal representations.

### Open Question 3
- **Question**: Did the same neurocomputational capacities (multi-place predicates and structure mapping) that enabled symbolic thought also play a central role in the evolution of human language?
- **Basis in paper**: [explicit] The authors speculate: "It is therefore at least possible that the same mutations that gave rise to our capacity for multi-place relations and structure mapping also played a central role in the evolution of language."
- **Why unresolved**: The paper focuses on visual inference tasks; the connection to language evolution remains theoretical and untested.
- **What evidence would resolve it**: Cross-linguistic studies linking capacities for multi-place predicates and structure mapping to language acquisition and processing, or neuroimaging studies identifying shared neural substrates.

## Limitations
- The biological plausibility of the precise temporal synchrony mechanism for dynamic binding remains uncertain and represents a computational-level rather than mechanistic neural explanation
- The simulations use hand-coded semantic features and controlled architectures that may not capture the complexity of real neural systems or modern machine learning approaches
- The claim that Transformers cannot perform dynamic binding due to entanglement remains contested in the literature

## Confidence
- **High Confidence**: The necessity of both multi-place predicates and structure mapping for successful relational reasoning (supported by systematic simulation results across 17 conditions)
- **Medium Confidence**: The claim that biological neural systems can exploit precise timing for binding (mechanistic plausibility supported but not proven)
- **Medium Confidence**: The assertion that LLMs fail on symbolic tasks due to lack of explicit structure mapping (based on controlled comparisons but not comprehensive LLM evaluation)

## Next Checks
1. **Empirical Validation**: Test modern neural network architectures (including Transformers with specialized attention mechanisms) on the same four task types to empirically verify the claim about their limitations on symbolic reasoning tasks.

2. **Parameter Sensitivity Analysis**: Conduct systematic variation of LISA's numerous parameters beyond the mapping rate to determine whether the observed failures are robust to parameter changes or represent narrow conditions.

3. **Neuroscientific Corroboration**: Design experiments to test whether human subjects show similar performance patterns when structure mapping is disrupted (e.g., through working memory load) or when relational reasoning requires explicit predicate formation.