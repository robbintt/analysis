---
ver: rpa2
title: 'GemiRec: Interest Quantization and Generation for Multi-Interest Recommendation'
arxiv_id: '2510.14626'
source_url: https://arxiv.org/abs/2510.14626
tags:
- interest
- user
- multi-interest
- recommendation
- interests
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GemiRec tackles interest collapse and insufficient evolution modeling
  in multi-interest recommendation by combining interest quantization and generation.
  It maintains a shared quantized interest dictionary for structural separation and
  employs a decoupled generative model to explicitly capture evolving user interests.
---

# GemiRec: Interest Quantization and Generation for Multi-Interest Recommendation

## Quick Facts
- arXiv ID: 2510.14626
- Source URL: https://arxiv.org/abs/2510.14626
- Authors: Zhibo Wu; Yunfan Wu; Quan Liu; Lin Jiang; Ping Yang; Yao Hu
- Reference count: 40
- Key outcome: GemiRec achieves superior performance over state-of-the-art methods in multi-interest recommendation, with significant gains in metrics like Recall@120 (+28.78%) and interest separation (AMR@120 +0.4004), and has been deployed in production since March 2025.

## Executive Summary
GemiRec addresses two critical challenges in multi-interest recommendation: interest collapse and insufficient modeling of interest evolution. The method introduces a novel approach combining interest quantization with a decoupled generative model. By maintaining a shared quantized interest dictionary, GemiRec achieves structural separation between different user interests while the generative component explicitly captures how these interests evolve over time. The system has demonstrated substantial improvements over existing methods across multiple evaluation metrics and has already seen successful deployment in real-world production environments.

## Method Summary
GemiRec operates through a two-pronged approach to multi-interest recommendation. First, it implements interest quantization by maintaining a shared dictionary of quantized interest representations, which creates structural separation between different interest clusters. This quantization process induces a Voronoi partition in the interest space with theoretical guarantees on separation bounds. Second, a decoupled generative model captures the evolution of user interests over time by explicitly modeling how quantized interests transform and develop. The method combines these components to address both the collapse of diverse interests into a single representation and the failure of existing methods to adequately model interest dynamics. The theoretical foundation includes analysis showing that the quantization approach provides non-trivial separation guarantees that traditional regularization methods cannot achieve.

## Key Results
- Achieves 28.78% improvement in Recall@120 compared to state-of-the-art baselines
- Demonstrates 0.4004 improvement in interest separation (AMR@120)
- Successfully deployed in production environment since March 2025 with verified online A/B test improvements

## Why This Works (Mechanism)
GemiRec succeeds by addressing the fundamental tension between maintaining distinct interest representations while allowing for natural evolution over time. The interest quantization creates a shared dictionary that acts as anchor points in the interest space, preventing different interests from collapsing into each other. Meanwhile, the decoupled generative model provides the flexibility needed to capture how interests transform, shift, and develop without being constrained by the quantization boundaries. This separation of concerns allows the system to maintain clear interest distinctions while still modeling complex evolutionary patterns. The Voronoi partition induced by the quantization provides theoretical guarantees on interest separation that traditional regularization approaches cannot match, ensuring that the structural benefits of quantization are maintained even as interests evolve.

## Foundational Learning
- **Interest quantization**: The process of discretizing continuous interest representations into a finite set of prototypical interest vectors. Why needed: Prevents interest collapse by maintaining clear boundaries between different interest clusters. Quick check: Verify that quantized interests maintain semantic distinctiveness through clustering analysis.
- **Voronoi partition**: A geometric partitioning of space where each region contains points closest to a particular prototype vector. Why needed: Provides theoretical foundation for interest separation guarantees in the quantized space. Quick check: Confirm that interest representations fall within appropriate Voronoi cells using distance calculations.
- **Decoupled generative modeling**: A modeling approach that separates the generation of interest evolution from the quantization structure. Why needed: Allows flexible modeling of interest dynamics without compromising the structural separation provided by quantization. Quick check: Validate that generated interest trajectories maintain coherence with quantized anchor points.
- **Multi-interest recommendation**: The task of capturing and recommending items across multiple distinct user interest dimensions rather than a single unified preference representation. Why needed: Reflects the reality that users have diverse, sometimes unrelated interests that should be treated separately. Quick check: Measure interest diversity metrics to ensure multiple distinct interest clusters are being maintained.

## Architecture Onboarding

**Component Map**: User Interaction History -> Interest Quantization Module -> Shared Quantized Dictionary -> Decoupled Generative Model -> Interest Evolution Predictions -> Recommendation Output

**Critical Path**: The core inference pipeline processes user interaction history through the quantization module to identify relevant interest clusters, then passes these through the generative model to predict current interests, which are finally used to generate recommendations. The quantization step is critical as it provides the structural foundation that prevents interest collapse, while the generative model adds the temporal dimension necessary for capturing evolving preferences.

**Design Tradeoffs**: The shared quantized dictionary approach trades some flexibility in representing highly individualized interest patterns for the benefit of structural separation and computational efficiency. This design choice means the system may not capture extremely niche interests as precisely as methods with fully personalized representations, but gains robustness in maintaining distinct interest clusters. The decoupled architecture separates structural concerns (quantization) from dynamic modeling (generation), which simplifies the learning problem but requires careful coordination between modules.

**Failure Signatures**: Potential failure modes include: (1) Over-quantization leading to loss of fine-grained interest distinctions, observable as reduced recommendation diversity; (2) Generative model failure to capture complex interest evolution patterns, visible as stale or irrelevant recommendations despite recent user interactions; (3) Shared dictionary scalability issues with rapidly expanding interest spaces, manifesting as performance degradation over time; (4) Misalignment between quantization boundaries and actual user interest distributions, resulting in boundary-crossing artifacts in recommendations.

**First Experiments**:
1. **Interest separation validation**: Measure the distribution of interest representations across Voronoi cells before and after quantization to verify structural separation improvements.
2. **Generative model trajectory analysis**: Visualize and quantify the evolution paths generated for representative users to assess whether the model captures meaningful interest dynamics.
3. **Ablation study**: Compare performance with and without the quantization module to isolate its contribution to overall system effectiveness.

## Open Questions the Paper Calls Out
None specified in the provided material.

## Limitations
- Theoretical analysis relies on Euclidean distance metric without exploring alternatives that might better capture semantic interest similarity
- Decoupled generative model's ability to capture complex interest evolution over extended time horizons lacks thorough validation
- Shared quantized dictionary approach may face scalability challenges with rapidly evolving interest spaces or high-dimensional item representations
- Production deployment details lack disclosure of specific business context and potential confounding factors in A/B testing

## Confidence
- **High Confidence**: Experimental methodology including controlled offline comparisons and online A/B testing results is rigorous and well-documented, with clear quantitative improvements in standard recommendation metrics across multiple datasets
- **Medium Confidence**: Theoretical guarantees for interest quantization are sound but rely on assumptions that may not hold in all real-world scenarios; claim that decoupled generative model explicitly captures interest evolution is supported but could benefit from more granular pattern analysis
- **Low Confidence**: Long-term stability and adaptation capabilities of shared quantized dictionary in dynamic environments, and potential performance degradation when scaling to much larger interest spaces, remain uncertain without additional longitudinal studies

## Next Checks
1. Conduct ablation studies specifically isolating the contribution of the quantization module versus the generative module to quantify their individual impacts on reported improvements
2. Test the model's performance and adaptation speed when deployed in a simulated dynamic environment where user interest distributions shift significantly over time
3. Evaluate the model's scalability and performance degradation when the item vocabulary and user base are expanded by an order of magnitude beyond current experimental settings