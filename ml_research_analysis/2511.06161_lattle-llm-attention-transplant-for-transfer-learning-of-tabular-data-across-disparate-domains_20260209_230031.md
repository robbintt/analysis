---
ver: rpa2
title: 'LATTLE: LLM Attention Transplant for Transfer Learning of Tabular Data Across
  Disparate Domains'
arxiv_id: '2511.06161'
source_url: https://arxiv.org/abs/2511.06161
tags:
- data
- learning
- tabular
- transfer
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of transfer learning across tabular
  datasets with disparate feature spaces. It proposes a novel method, LATTLE, that
  uses cross-attention between a lightweight LLM (DistilGPT2) and a gated feature
  tokenizer transformer (gFTT) by transplanting attention-specific weights from the
  LLM.
---

# LATTLE: LLM Attention Transplant for Transfer Learning of Tabular Data Across Disparate Domains

## Quick Facts
- arXiv ID: 2511.06161
- Source URL: https://arxiv.org/abs/2511.06161
- Reference count: 40
- Achieves best average rank (2.0) in AUC and accuracy across 10 source-target pairs

## Executive Summary
LATTLE addresses cross-domain transfer learning for tabular classification where source and target datasets have completely disjoint feature spaces. The method uses cross-attention between a lightweight LLM (DistilGPT2) and a gated feature tokenizer transformer (gFTT), transplanting attention-specific weights from the LLM to enable domain-agnostic transfer without shared features or prompt engineering. Experiments on 12 baselines show LATTLE achieves the best average rank (2.0) in AUC and accuracy, outperforming traditional ML, deep learning methods, and other transfer learning approaches.

## Method Summary
LATTLE employs a two-phase approach: first jointly fine-tuning DistilGPT2 and gFTT on source data, then transplanting attention weights (Key and Value matrices) from the LLM's top layer to initialize the target gFTT's bottom layer. The method serializes tabular data into text for the LLM and uses BERT embeddings of column names multiplied by values for the gFTT. During transfer, the transplanted weights are frozen while the rest of the target model is trained on the new domain. This enables effective transfer learning without requiring shared features between source and target datasets.

## Key Results
- LATTLE achieves average rank 2.0 across 10 source-target pairs in both AUC and accuracy
- Outperforms 12 baseline methods including traditional ML, deep learning, and other transfer learning approaches
- Demonstrates effectiveness in low-resource settings without requiring shared features or prompt engineering
- Shows robust performance across diverse tabular datasets from OpenML

## Why This Works (Mechanism)
The method works by leveraging the contextual knowledge captured in LLM attention weights and transplanting it to initialize a target model for a new domain. By freezing the transplanted Key and Value matrices while training the Query matrix and upper layers, the model preserves the source domain's learned relationships while adapting to the target domain's specific patterns. The cross-attention mechanism allows the LLM to provide contextual understanding that guides the gFTT's processing of tabular features, effectively bridging the gap between domains with disjoint feature spaces.

## Foundational Learning
- **Cross-attention mechanisms**: Why needed - enables information flow between LLM context and tabular features; Quick check - verify attention weights properly propagate through both modalities
- **Weight transplant methodology**: Why needed - transfers learned knowledge without requiring shared features; Quick check - ensure dimension alignment between source and target attention layers
- **Gated feature tokenization**: Why needed - handles both numerical and categorical features in tabular data; Quick check - verify feature embeddings preserve semantic relationships
- **Joint fine-tuning stability**: Why needed - prevents catastrophic forgetting during source training; Quick check - monitor training loss for spikes or divergence

## Architecture Onboarding
**Component Map**: DistilGPT2 (text encoder) -> gFTT (feature transformer) -> Classifier head
**Critical Path**: Text serialization -> LLM processing -> Cross-attention fusion -> Feature transformation -> Classification
**Design Tradeoffs**: Lightweight LLM vs performance tradeoff; fixed attention transplant vs adaptive selection; joint vs separate training phases
**Failure Signatures**: Dimension mismatch crashes during weight transplant; overfitting on small target datasets; training instability in joint phase
**3 First Experiments**: 1) Joint train on Credit-g dataset and verify convergence; 2) Transplant weights to Diabetes dataset and confirm no dimension errors; 3) Compare performance against random initialization baseline

## Open Questions the Paper Calls Out
- **Scalability to larger LLMs**: The scaling of the proposed model remains an open problem because computational constraints prevented testing larger models (e.g., Llama, GPT-3), leaving the relationship between parameter count and transplant efficacy unknown
- **Data characteristic sensitivity**: The algorithm underperforms in several datasets due to unexplained data-specific characteristics that the authors observed but did not identify
- **Optimal transplant strategy**: The hierarchical "top-to-bottom" weight transplant strategy may not be optimal for all types of domain shifts, as the ablation study compared rigid layer mappings without determining if different source-target distances require different configurations

## Limitations
- Implementation complexity due to precise architectural alignment requirements between DistilGPT2 and gFTT
- Limited evaluation to relatively small OpenML datasets, leaving uncertainty about performance on industrial-scale data
- Reliance on source domain with sufficient labeled data for pretraining, which may not be available in truly low-resource settings

## Confidence
**High Confidence**: Core experimental results showing LATTLE achieving average rank 2.0 across 10 source-target pairs are well-supported
**Medium Confidence**: Claims about best average rank validity within tested setup, but may not generalize beyond OpenML corpus
**Low Confidence**: Scalability claims for millions of rows are not empirically validated; universal superiority assertions lack comprehensive ablation studies

## Next Checks
1. **Architectural Compatibility Test**: Evaluate LATTLE's performance when transplanting weights between different model sizes to quantify sensitivity to architectural alignment
2. **Large-Scale Dataset Validation**: Test LATTLE on tabular datasets exceeding 1 million rows to validate scalability claims and measure performance degradation
3. **Zero-Shot Transfer Evaluation**: Implement rigorous zero-shot transfer protocol where source model is trained on one domain and evaluated directly on completely unseen target domains without target-specific fine-tuning