---
ver: rpa2
title: Self-supervised learning of speech representations with Dutch archival data
arxiv_id: '2507.04554'
source_url: https://arxiv.org/abs/2507.04554
tags:
- data
- speech
- pre-training
- dataset
- dutch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of Dutch archival television data
  for self-supervised learning (SSL) of speech representations using wav2vec 2.0.
  The authors first analyze how data quality factors like music, noise, and speaker
  overlap affect SSL performance.
---

# Self-supervised learning of speech representations with Dutch archival data

## Quick Facts
- arXiv ID: 2507.04554
- Source URL: https://arxiv.org/abs/2507.04554
- Authors: Nik Vaessen; Roeland Ordelman; David A. van Leeuwen
- Reference count: 0
- This paper achieves state-of-the-art Dutch ASR results with 7.1% WER on N-Best test set using 55k hours of Dutch archival data

## Executive Summary
This paper investigates self-supervised learning of speech representations using Dutch archival television data, addressing the challenge of data quality in SSL pre-training. The authors systematically analyze how factors like music, noise, and speaker overlap affect wav2vec 2.0 training, finding that vocal music severely degrades performance while instrumental music is more tolerable. They develop an effective preprocessing pipeline using WhisperX to segment raw archival audio into clean, single-speaker utterances suitable for SSL. Their mono-lingual Dutch pre-trained model significantly outperforms both English LS pre-training and multi-lingual XLSR models on Dutch ASR tasks, achieving state-of-the-art results with 9.5% WER on N-Best when continuing from XLS-R checkpoint.

## Method Summary
The authors preprocess 81k hours of raw Dutch archival TV broadcast data into 55.7k hours of clean speech segments using a pipeline combining Whisper large-v2 for transcription, WhisperX for forced alignment and speaker diarization, and Pyannote for segmentation. They filter out segments containing music keywords (e.g., "muziek"), merge segments from the same speaker, and discard those shorter than 3 seconds. For SSL pre-training, they use wav2vec 2.0 BASE and LARGE architectures with contrastive loss, training on 400k-500k steps with batch sizes ranging from 5 to 40 minutes. Fine-tuning employs CTC loss with frozen CNN layers and a tri-stage learning rate schedule. The best model is obtained by continuing pre-training from XLS-R 300M checkpoint on the Dutch data for 200k steps with 40-minute batches.

## Key Results
- Vocal music contamination causes wav2vec 2.0 training divergence, while instrumental music is more tolerable (31.8-35.6 WER vs DIV)
- WhisperX preprocessing with 3-second speaker segments produces clean training data achieving 17.1-17.5 WER on MLS-test
- Mono-lingual Dutch pre-training with 55k hours outperforms XLSR multi-lingual model on Dutch ASR tasks (11.1 vs 14.3 WER on N-Best)
- Final LARGE model achieves state-of-the-art 7.1% WER on N-Best test set when continuing from XLS-R checkpoint

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Vocal music in pre-training data causes training instability or divergence in wav2vec 2.0 contrastive learning, while instrumental music is more tolerable.
- Mechanism: The contrastive objective in wav2vec 2.0 masks latent speech representations and identifies true targets from distractors. Vocal music—containing pitch contours, harmonics, and phoneme-like sounds—may create false positives in the contrastive task, corrupting the learned quantization codebook.
- Core assumption: The degradation stems from interference with the quantization module's ability to distinguish speech units from melodic content (not directly tested in paper).
- Evidence anchors:
  - [Table 1]: `mix-music` and `mix-vocal` conditions show "DIV" (divergence) across all evaluation metrics; `mix-instr` achieves 31.8-35.6 WER on test-other.
  - [Section 4.1]: "pre-training is not stable. We only managed to find a converging LR when mixing instrumental music."
  - [corpus]: No direct corpus corroboration on music-specific degradation; related work on data bias [12] addresses speech rate but not music.
- Break condition: If the contrastive objective were replaced with a prediction-based objective (e.g., HuBERT), vocal music sensitivity might differ—though this remains untested.

### Mechanism 2
- Claim: Using a strong ASR model (Whisper/WhisperX) as a pre-filter effectively segments noisy archival audio into speech-only utterances suitable for SSL.
- Mechanism: Whisper successfully transcribes audio only when speech is intelligible and sufficiently clean. Audio segments that Whisper cannot transcribe (or hallucinates over) are implicitly filtered out. WhisperX adds forced alignment and speaker diarization, producing single-speaker utterances similar to LibriSpeech structure.
- Core assumption: Whisper's transcription success correlates with SSL training utility—segments useful for ASR are also useful for contrastive speech representation learning.
- Evidence anchors:
  - [Table 2]: `w-raw` and `w-pp-1s` diverged during pre-training; `wx-diar-3s` achieved 17.1-17.5 WER on MLS-test with 40-minute batch size.
  - [Section 4.2]: "We found the resulting segmentations [from Pyannote SAD] to contain a lot of speaker overlap, music and noise. Moreover, initial pre-training experiments were unsuccessful."
  - [corpus]: No corpus papers validate ASR-based filtering specifically for SSL preprocessing.
- Break condition: If Whisper's training data distribution is severely mismatched from archival audio (e.g., different recording eras, dialects), filtering may exclude valid speech or retain poor-quality segments.

### Mechanism 3
- Claim: Mono-lingual pre-training with equivalent data volume yields more robust representations for target-language ASR than multi-lingual pre-training, particularly for out-of-domain evaluation.
- Mechanism: With fixed model capacity and training budget, mono-lingual exposure allocates all representational capacity to phonetic, prosodic, and acoustic patterns of one language. Multi-lingual models must partition capacity across languages, potentially under-representing language-specific phenomena like local dialects.
- Core assumption: Dutch dialectal variation and acoustic diversity in the 55k-hour archival dataset exceeds what multi-lingual models capture through their Dutch subset (assumption stated but not isolated).
- Evidence anchors:
  - [Table 3]: Mono-lingual LARGE model achieves 11.1 WER on N-Best (out-of-domain) vs. 14.3 for XLS-R 128; however, multi-lingual slightly outperforms on in-domain MLS-test (12.7 vs. 13.0).
  - [Section 1]: "when equally sized, a mono-lingual dataset leads to higher-quality speech representations in that language, compared to a multi-lingual dataset, as more variation of that specific language can be observed, such as local dialects."
  - [corpus]: Related paper [10] shows Japanese mono-lingual pre-training (500h) outperforms XLSR LARGE on Japanese ASR—consistent pattern but different language.
- Break condition: If mono-lingual data lacks sufficient acoustic diversity (e.g., single speaker, narrow domain), multi-lingual pre-transfer may still dominate.

## Foundational Learning

- Concept: **Contrastive Self-Supervised Learning (wav2vec 2.0)**
  - Why needed here: The entire methodology builds on wav2vec 2.0's masked contrastive objective; understanding how latent representations are quantized and contrasted is essential for interpreting why music causes divergence.
  - Quick check question: Can you explain why the contrastive loss requires both a quantization module and a set of negative samples?

- Concept: **Transfer Learning via Checkpoint Continuation**
  - Why needed here: The best model (9.5% WER on N-Best) was obtained by continuing XLS-R pre-training with Dutch data, not training from scratch—this leverages multi-lingual representations while specialising on Dutch.
  - Quick check question: How does continuing pre-training differ from fine-tuning, and why might it preserve more transferable representations?

- Concept: **Batch Size Scaling Laws in SSL**
  - Why needed here: Table 2 and Table 3 show dramatic improvements when scaling from 5-minute to 40-minute batches (WER reductions >50%); this is not just compute efficiency but representation quality.
  - Quick check question: Why does larger batch size in contrastive SSL improve downstream performance, and what is the square root scaling rule for learning rate adjustment?

## Architecture Onboarding

- Component map:
  - Raw archival audio -> WhisperX preprocessing -> Single-speaker 3-30s utterances
  - wav2vec 2.0 CNN encoder (7 layers) -> Transformer backbone (12/24 layers) -> Quantization module (pre-training)
  - Contrastive loss -> Fine-tuning head (CTC) -> ASR evaluation

- Critical path:
  1. Data preprocessing: Raw archival audio → WhisperX segmentation → single-speaker 3-30s utterances
  2. Pre-training (400k-500k steps): Masked contrastive learning on 55k hours Dutch audio
  3. Fine-tuning (80k steps): Freeze CNN, train transformer with CTC on labeled Dutch data (MLS/CV/CGN)
  4. Evaluation: Greedy letter decoding on MLS-test, CV-test, N-Best

- Design tradeoffs:
  - `wx-diar-3s` vs `w-pp-3s`: Former is 20% faster to compute; latter may yield slightly better WER (not fully tested)
  - BASE vs LARGE: BASE converges faster; LARGE achieves 2-4 point WER improvement but requires 2x compute
  - Mono-lingual from scratch vs continuation from XLS-R: Continuation gives best results (9.5 vs 11.1 WER on N-Best) but requires pre-trained checkpoint access

- Failure signatures:
  - Training divergence with "DIV" in validation loss: Check for vocal music contamination (filter with Whisper keywords like "muziek")
  - High WER on out-of-domain test despite good in-domain performance: Likely multi-lingual model without mono-lingual continuation
  - Pre-training plateaus early: Batch size may be too small; scale to 40+ minutes with learning rate adjustment

- First 3 experiments:
  1. **Data quality validation**: Pre-train BASE wav2vec 2.0 on a 500-hour subset with `wx-diar-3s` preprocessing; fine-tune on 10 minutes of labeled Dutch data; verify convergence and baseline WER (~27-30 on MLS-test expected from Table 2).
  2. **Contamination stress test**: Add controlled `mix-vocal` augmentation at 10% ratio to a clean pre-training run; confirm divergence occurs within first 50k steps (replicates Table 1 finding).
  3. **Mono-lingual vs multi-lingual ablation**: Fine-tune XLS-R 128 checkpoint vs your mono-lingual Dutch checkpoint on CGN training data; compare on N-Best out-of-domain evaluation to verify mono-lingual robustness claim (expect ~3-5 WER gap per Table 3).

## Open Questions the Paper Calls Out
None

## Limitations
- The exact configuration of WhisperX preprocessing pipeline is not fully specified, particularly the complete keyword filtering list for music removal
- The computational requirements for 40-minute batch sizes with large-scale models are substantial but not fully characterized
- The comparison with multi-lingual models may be confounded by dataset size and domain differences, as XLS-R models were pre-trained on different data distributions

## Confidence
- **High confidence**: Empirical results showing vocal music contamination causes training divergence, effectiveness of WhisperX preprocessing, and superiority of mono-lingual vs multi-lingual pre-training on Dutch ASR tasks
- **Medium confidence**: Claim that instrumental music is more tolerable than vocal music (based on single successful run), and that batch size scaling improvements are due to representation quality rather than just compute efficiency
- **Low confidence**: Precise mechanism by which vocal music causes divergence in the contrastive objective, and whether mono-lingual benefits would generalize to other languages

## Next Checks
1. **Music contamination mechanism validation**: Design an experiment that systematically varies vocal music presence (0%, 5%, 10%, 20%) in pre-training data while keeping other factors constant. Monitor not just WER but also contrastive loss stability and quantization codebook behavior to isolate the specific failure mode.

2. **Cross-lingual generalization test**: Pre-train mono-lingual models for two additional languages (e.g., German and French) using similar archival data volumes and preprocessing. Compare against XLSR pre-training to determine if the mono-lingual advantage is language-specific or a general SSL phenomenon.

3. **Pipeline component ablation**: Create controlled experiments that isolate the contribution of each preprocessing component (Whisper transcription, WhisperX alignment/diarization, keyword filtering, segment length filtering). Measure the marginal benefit of each step to quantify their relative importance in achieving stable pre-training.