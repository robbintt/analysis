---
ver: rpa2
title: 'Unveiling Cultural Blind Spots: Analyzing the Limitations of mLLMs in Procedural
  Text Comprehension'
arxiv_id: '2502.14315'
source_url: https://arxiv.org/abs/2502.14315
tags:
- language
- cultural
- procedural
- zhang
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CAPTex is a new benchmark for evaluating multilingual large language
  models (mLLMs) on culturally diverse procedural texts. The dataset includes 1,400
  human-written procedural texts across seven languages (Chinese, Japanese, Persian,
  Hindi, Indonesian, Urdu, Hausa) and ten cultural domains, with English translations.
---

# Unveiling Cultural Blind Spots: Analyzing the Limitations of mLLMs in Procedural Text Comprehension

## Quick Facts
- **arXiv ID:** 2502.14315
- **Source URL:** https://arxiv.org/abs/2502.14315
- **Reference count:** 40
- **Key outcome:** CAPTex is a new benchmark for evaluating multilingual large language models (mLLMs) on culturally diverse procedural texts.

## Executive Summary
CAPTex introduces a novel benchmark to evaluate mLLMs on culturally contextualized procedural text comprehension across seven languages and ten cultural domains. The dataset includes 1,400 human-written procedural texts with English translations, tested through four distinct tasks: step reordering, direct questioning, conversation-based questioning, and open generation. The results reveal significant performance gaps for mLLMs, particularly in low-resource languages and domains requiring implicit cultural knowledge. Even the best-performing models like GPT-4o show notable limitations in cross-cultural procedural understanding, highlighting the need for more culturally aware evaluation frameworks.

## Method Summary
The study evaluates 31 multilingual models on 1,400 human-written procedural texts across seven languages (Chinese, Japanese, Persian, Hindi, Indonesian, Urdu, Hausa) and ten cultural domains. Each text contains 5-10 steps, with English translations provided. Four evaluation tasks assess different aspects of procedural comprehension: Step Reordering (sequence reconstruction), Procedure-Based MCQ (direct questions), Conversation-Based MCQ (dialogue-embedded questions), and Conversation-Based QA (open generation). Models are evaluated using zero-shot prompts in English only, with performance measured through Spearman/Kendall correlations, Levenshtein distance, accuracy, and generation metrics like ROUGE-L and BERTScore.

## Key Results
- mLLMs exhibit significant performance declines on culturally contextualized procedural texts, especially in low-resource languages like Hausa and Urdu
- GPT-4o achieves the highest overall performance but still shows notable limitations in cross-cultural procedural text understanding
- Performance varies substantially across cultural domains, with craftsmanship being easier than environmental practices
- Conversation-based MCQ tasks outperform direct questioning, suggesting dialogue context provides beneficial scaffolding

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Conversational framing improves procedural reasoning compared to direct questioning.
- **Mechanism:** Dialogue context provides implicit scaffolding—preceding utterances establish topic, partial procedure exposure, and discourse expectations. This grounds the model's retrieval in a narrower hypothesis space than cold-start MCQs.
- **Core assumption:** The benefit stems from contextual narrowing rather than the content of Person B's explanation itself (not isolated).
- **Evidence anchors:** [abstract] "language models exhibit better performance on multiple-choice tasks within conversational frameworks compared to direct questioning"; [section 3.2] Describes CB-MCQ structure where Person A/B dialogue establishes context before the query; [corpus] Weak direct evidence—related work on cultural grounding (arXiv:2508.07414) discusses knowledge grounding but not dialogue effects specifically.
- **Break condition:** If dialogue utterances contained no procedural content yet still boosted accuracy, the mechanism would shift toward pure discourse priming rather than content-grounded reasoning.

### Mechanism 2
- **Claim:** Performance degradation in low-resource languages correlates with training data distribution skew, not solely tokenization inefficiency.
- **Mechanism:** mLLMs encode cultural-linguistic patterns proportionally to training corpus representation. Low-resource languages (Hausa Class 2, Urdu Class 3 per Joshi et al. 2020) have fewer cultural procedure exemplars, weakening both language fluency and culturally-bound procedural schema acquisition.
- **Core assumption:** The gap reflects representational scarcity rather than architectural inability to transfer procedural reasoning cross-lingually.
- **Evidence anchors:** [abstract] "mLLMs face difficulties with culturally contextualized procedural texts, showing notable performance declines in low-resource languages"; [section 4.2, Figure 3] Qwen2.5-14B-Instruct shows sharper English-vs-native gap for Hausa/Urdu than Chinese; GPT-4o shows smaller gaps, suggesting training data quality matters; [corpus] arXiv:2512.10630 examines structural/historical factors in low-resource language representation, supporting data-scarcity narratives.
- **Break condition:** If parameter scaling alone closed the low-resource gap without targeted data augmentation, architectural capacity—not data scarcity—would be the primary bottleneck.

### Mechanism 3
- **Claim:** Domain-specific procedural difficulty varies with implicit cultural knowledge density, not procedural complexity.
- **Mechanism:** Domains like "environmental practices" require tacit, non-explicit cultural reasoning (e.g., seasonal significance, symbolic meanings) while "craftsmanship" follows more explicit, tool-driven sequences. Models trained on surface text lack the latent cultural priors humans acquire through embodied experience.
- **Core assumption:** The difficulty differential stems from implicit knowledge requirements, not from step count or lexical complexity.
- **Evidence anchors:** [abstract] "performance fluctuates across cultural domains, with some areas presenting greater difficulties"; [section 1/Table 1] Contrasts Iranian vs Indonesian funeral procedures showing culturally-embedded steps; [section 4.2/Figure 4] Qwen2.5-14B-Instruct performs better on craftsmanship than environmental practices; [corpus] arXiv:2505.11275 (TCC-Bench) notes MLLM limitations on non-Western cultural contexts, consistent with tacit knowledge gaps.
- **Break condition:** If controlled studies showed step-count alone predicted difficulty independent of cultural domain, the mechanism would shift toward procedural length/complexity rather than cultural knowledge depth.

## Foundational Learning

- **Concept: Procedural Text Comprehension**
  - **Why needed here:** CAPTex evaluates sequential step understanding, not just fact retrieval. Tasks require inferring ordering constraints, antecedent/subsequent relationships, and step dependencies.
  - **Quick check question:** Given shuffled steps "Boil water → Add pasta → Drain → Serve" and "Serve → Drain → Add pasta → Boil water," can you identify which ordering violations make a sequence invalid vs merely suboptimal?

- **Concept: Cross-Lingual Transfer in LLMs**
  - **Why needed here:** Performance gaps between English and native-language versions of the same procedure reveal how well procedural reasoning transfers across languages vs being language-bound.
  - **Quick check question:** If a model scores 70% on English procedural MCQs but 45% on the same procedures in Hausa, what are three possible causes (hint: tokenization, training data, cultural concept alignment)?

- **Concept: Cultural Knowledge as Latent Context**
  - **Why needed here:** Procedural texts omit shared cultural knowledge (e.g., "facing Mecca" implies directional significance). Models must infer this unstated context to reason correctly.
  - **Quick check question:** In "The body is buried facing Mecca," what unstated cultural knowledge determines whether a model can predict the next step correctly?

## Architecture Onboarding

- **Component map:** Input Layer (procedural texts in 7 native languages + English) → Task Layer (Step Reordering, PB-MCQ, CB-MCQ, CB-QA) → Metric Layer (Spearman/Kendall, Levenshtein, accuracy, ROUGE-L/BERTScore/Semantic Similarity)

- **Critical path:** Start with PB-MCQ (simplest evaluation) → CB-MCQ (conversational variant) → Step Reordering (hardest sequence task) → CB-QA (open-ended, lowest correlation with other tasks per section 4.2)

- **Design tradeoffs:**
  - English prompts vs native-language prompts: Paper uses English-only prompts for consistency, but this may artificially inflate English-procedure performance
  - 4-utterance dialogues: Simplified from real conversations; practical for benchmarking but limits ecological validity
  - GPT-4o-generated conversations: Creates dependency on model being evaluated (circularity risk if evaluating GPT-4o)

- **Failure signatures:**
  - Low Spearman but high Kendall: Minor swap errors, not catastrophic misordering
  - High PB-MCQ but low CB-MCQ: Model lacks discourse grounding, not procedural knowledge
  - Large English-native gap on same procedure: Language-specific representation failure, not cultural knowledge gap
  - Mamba models near-zero MCQ accuracy (Table 4): State-space architecture may lack procedural sequence handling

- **First 3 experiments:**
  1. **Baseline establishment:** Run Qwen2.5-7B-Instruct on all 4 tasks for one language (e.g., Indonesian) to establish per-task performance distribution and identify weakest task type.
  2. **Language-gap analysis:** Compare PB-MCQ accuracy on identical procedures in native language vs English translation for Hausa (low-resource) and Chinese (high-resource) to quantify cross-lingual transfer deficit.
  3. **Domain difficulty profiling:** Aggregate performance by cultural category (Table 6) for a single model to identify which domains require additional training focus; prioritize "Environmental" if lowest per section 4.2 findings.

## Open Questions the Paper Calls Out
- **Open Question 1:** Does incorporating visual inputs (images) alongside procedural text enhance mLLM comprehension of culturally nuanced steps compared to text-only analysis? [explicit] The authors explicitly state in the Limitations section that the research is "limited to textual data and does not include multimodal inputs," noting that this is "reserved for future work." Why unresolved: Current benchmarks focus on text, leaving the potential for visual context to clarify ambiguous cultural procedures untested. What evidence would resolve it: A multimodal extension of the CAPTex benchmark showing improved model performance when images accompany the procedural texts.

- **Open Question 2:** Can mLLMs maintain procedural reasoning and coherence in conversational contexts that extend beyond the four-utterance limit used in this study? [explicit] The authors acknowledge that while "real-world dialogues are typically longer and more dynamic," the conversation dataset is limited to four utterances. Why unresolved: The current evaluation only validates short-context reasoning, failing to test if models can sustain cultural understanding in extended interactions. What evidence would resolve it: Evaluation results from a dataset featuring longer, multi-turn dialogues showing sustained or degraded performance.

- **Open Question 3:** To what extent do the observed cultural blind spots in these seven countries generalize to other diverse linguistic and cultural contexts? [explicit] The paper notes the dataset "focuses on seven countries" due to budget constraints, arguing findings are generalizable given sample representativeness, though this remains an untested boundary. Why unresolved: The specific cultural domains and languages chosen may not capture the full variance of global cultural procedures. What evidence would resolve it: Expanding the benchmark to additional countries and languages to verify if the same performance patterns (e.g., specific domain difficulties) persist.

## Limitations
- The study's reliance on English-only prompts may conflate language-specific representation issues with cultural comprehension deficits
- GPT-4o-generated conversations introduce circularity concerns when evaluating GPT-4o itself
- The paper doesn't adequately address potential data contamination from pretraining for widely-used models
- Simplified 4-utterance dialogue structure may not capture the full complexity of cultural procedural reasoning

## Confidence
- **High Confidence:** The observation that mLLMs struggle with culturally contextualized procedural texts (supported by systematic across-language performance gaps and domain-specific variations)
- **Medium Confidence:** The claim that conversational framing improves procedural reasoning (supported by task comparisons but lacking direct causal evidence)
- **Medium Confidence:** The correlation between low-resource language performance and training data distribution (consistent with model behavior but not definitively proven)
- **Low Confidence:** The assertion that domain difficulty stems from implicit cultural knowledge density (plausible but not directly tested against procedural complexity controls)

## Next Checks
1. **Cross-linguistic Prompt Validation:** Repeat PB-MCQ evaluation using native-language prompts for each procedural text to isolate language representation effects from cultural comprehension gaps.

2. **Data Contamination Audit:** Analyze whether any CAPTex procedural texts or similar cultural scenarios appear in public training corpora for evaluated models, particularly for GPT-4o and Qwen2.5.

3. **Cultural Knowledge Transfer Test:** Design controlled experiments where identical procedural structures are tested across cultures (e.g., same cooking technique with culturally-specific ingredients) to determine if difficulty stems from procedural complexity versus cultural context.