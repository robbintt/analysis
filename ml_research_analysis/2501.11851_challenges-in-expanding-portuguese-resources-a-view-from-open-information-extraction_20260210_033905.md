---
ver: rpa2
title: 'Challenges in Expanding Portuguese Resources: A View from Open Information
  Extraction'
arxiv_id: '2501.11851'
source_url: https://arxiv.org/abs/2501.11851
tags:
- open
- annotation
- corpus
- extraction
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a high-quality manually annotated corpus for
  Open Information Extraction in Portuguese, addressing the lack of such resources
  for languages other than English. The corpus, OIEC-PT, is based on a rigorous methodology
  grounded in established semantic theories and structural and contextual annotation
  rules.
---

# Challenges in Expanding Portuguese Resources: A View from Open Information Extraction

## Quick Facts
- arXiv ID: 2501.11851
- Source URL: https://arxiv.org/abs/2501.11851
- Reference count: 8
- This paper presents a high-quality manually annotated corpus for Open Information Extraction in Portuguese, OIEC-PT, based on rigorous semantic theories and structural annotation rules.

## Executive Summary
This paper addresses the scarcity of Open Information Extraction (Open IE) resources for Portuguese by presenting a manually annotated corpus, OIEC-PT. The corpus was created through an iterative annotation process involving five linguists who applied structural and contextual rules grounded in semantic theory. The resulting gold set of 100 sentences contains 473 high-quality extractions, validated through state-of-the-art Open IE systems. The work demonstrates high inter-annotator agreement (0.94 kappa) and establishes a foundation for developing and evaluating Open IE methods for Portuguese and potentially other languages.

## Method Summary
The method involved creating an Open IE corpus for Portuguese using the first 300 sentences from the PUD Portuguese treebank, filtered for morphosyntactic errors. Five linguists conducted iterative annotation over three steps, applying 7 structural rules (S1-S5 with sub-rules) and 2 contextual rules to identify valid binary relational extractions. The process yielded a silver set of 200 sentences and a gold set of 100 sentences with high inter-annotator agreement. The corpus was validated using TabOIEC (CatBoost) and CrossOIE (CNN) classifiers, achieving an AUC of 0.84 on the gold set.

## Key Results
- Achieved high inter-annotator agreement (0.94 kappa) through iterative annotation and guideline refinement
- Created OIEC-PT corpus with 100 gold sentences containing 473 extractions and 200 silver sentences
- Classifier validation showed gold set outperformed silver set (AUC 0.84 vs 0.78) demonstrating corpus quality
- Established rigorous methodology grounded in semantic theories for Portuguese Open IE annotation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Grounding annotation guidelines in "structured proposition" theory allows for the principled derivation of syntactic extraction rules.
- Mechanism: The theory posits that the structure of a proposition is a function of sentence structure. Therefore, valid extractions can be defined by specific dependency configurations (e.g., NP-VP-NP) and sense-preserving transformations, reducing the vagueness typical of Open IE tasks.
- Core assumption: The syntax of a sentence reliably maps to the semantic structure of the expressed proposition.
- Evidence anchors:
  - [abstract] Mentions a "rigorous methodology grounded in established semantic theories."
  - [section] Section 2 explicitly links "structured propositions" to the validity of "syntactic-based rules," and Section 4.2.1 details these rules (e.g., S1, S4).
  - [corpus] Corpus evidence is internal to the study; external citations in the provided neighbor list do not address this specific theoretical mechanism.
- Break condition: If sentences contain significant idiomatic usage or non-compositional semantics where syntax diverges from semantic structure.

### Mechanism 2
- Claim: An iterative annotation process with disagreement resolution increases inter-annotator agreement (IAA) and corpus consistency.
- Mechanism: Annotators work independently, identify disagreements (e.g., handling n-ary relations or pronouns), discuss these edge cases to update guidelines (e.g., adding Rule S1.1), and re-annotate. This loop aligns annotator mental models.
- Core assumption: Disagreements stem primarily from ambiguous guidelines rather than subjective interpretation of text.
- Evidence anchors:
  - [abstract] Notes the corpus was created through an "iterative annotation process."
  - [section] Table 1 in Section 4.4 shows Randolph's Kappa improving from 0.63 (Step 1) to 0.94 (Step 3).
  - [corpus] Corpus evidence for this mechanism is derived from the reported internal statistics (Table 1) rather than external validation.
- Break condition: If the annotation task is inherently subjective (e.g., sentiment) rather than structural.

### Mechanism 3
- Claim: High-consistency "Gold" annotations improve the discriminative performance of classifiers compared to "Silver" annotations created with evolving guidelines.
- Mechanism: Consistent labeling reduces label noise. When classifiers (like TabOIEC) are trained or evaluated on data with high consensus (Gold set), they achieve higher precision/recall separation between valid and invalid extractions compared to noisy data (Silver set).
- Core assumption: The classifier architecture is sensitive enough to detect the structural patterns defined in the Gold guidelines.
- Evidence anchors:
  - [section] Section 5.2 reports that the Gold set (PUD100) achieved an AUC of 0.84 with TabOIEC, outperforming the Silver set (PUD200) at 0.78.
  - [section] Section 5.1 describes the use of "invalid" extractions judged by humans as negative examples.
  - [corpus] External corpus data is not applicable; evidence is strictly internal experimental results.
- Break condition: If the classifier is underfit or if the "Silver" set contains a distribution of relation types vastly different from the Gold set.

## Foundational Learning

- Concept: **Universal Dependencies (UD)**
  - Why needed here: The source corpus (PUD) and the structural rules (S1-S5) rely entirely on dependency parsing concepts (e.g., nsubj, obl, root).
  - Quick check question: In a UD tree, which node represents the "relation descriptor" in Rule S1?

- Concept: **Open Information Extraction (Open IE)**
  - Why needed here: This is the core task. Unlike traditional IE, it extracts schema-free relations (e.g., "moved to") rather than fixed attributes (e.g., "birth_place").
  - Quick check question: Does the system require a pre-defined ontology of relations before processing the text?

- Concept: **Randolph's Kappa**
  - Why needed here: This is the metric used to validate the quality of the dataset. It measures inter-rater reliability for multiple raters.
  - Quick check question: Why is Kappa preferred over raw percentage agreement when validating the "Gold" set?

## Architecture Onboarding

- Component map:
  - **Source:** Parallel Universal Dependencies (PUD) Portuguese treebank
  - **Filter:** Removal of sentences with morphosyntactic errors (Section 4.3)
  - **Annotation Layer:** Human annotators applying 7 Structural Rules (S1-S5) and 2 Contextual Rules (R4)
  - **Validation Layer:** TabOIEC (CatBoost) and CrossOIE (CNN) classifiers to probe dataset consistency

- Critical path: The definition of **Rule S1 (Structural Validity)** and **Rule R4 (Informativeness)**. These determine what enters the Gold set. If these are misapplied, the classifier validation (Mechanism 3) will fail to show improvements.

- Design tradeoffs:
  - **Binary vs. N-ary relations:** The architecture forces n-ary relations into binary extractions or ignores them to simplify the task (Section 6). This loses context but standardizes the output format.
  - **Silver vs. Gold:** Trading speed (annotating 200 sentences with draft rules) for quality (annotating 100 sentences with final rules).

- Failure signatures:
  - **Low Kappa (e.g., < 0.6):** Indicates guidelines are too vague or "semantic relation" is not sufficiently grounded in syntax.
  - **Low Classifier AUC on Gold:** Indicates the "Gold" rules are contradictory or the classifier is not learning the structural features (e.g., failing to detect preposition attachment in S4).

- First 3 experiments:
  1. **Reproduce Inter-Annotator Agreement:** Take a 10-sentence sample from the Silver set and apply the final rules (S1-S5, R4) to see if you can achieve >0.9 agreement with a peer.
  2. **Classifier Consistency Check:** Train the TabOIEC classifier on the Silver set (PUD200) and evaluate on the Gold set (PUD100). Confirm if the AUC is significantly lower than training on Gold, as per Section 5.2.
  3. **Error Analysis on Rule S1.1:** Identify extractions invalidated by pronoun references (Rule S1.1) and manually verify if the referent is truly retrievable from the sentence context.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the annotation methodology and guidelines developed for Portuguese be effectively adapted to create high-quality Open IE corpora for other languages (e.g., English, Spanish) within the Parallel Universal Dependencies (PUD) framework?
- Basis in paper: [explicit] The authors state, "As future work, we intend to expand the corpus by annotating the remaining sentences in the PUD treebank for Portuguese, as well as extending the annotation to other languages, such as English and Spanish."
- Why unresolved: The paper only reports on the creation of the Portuguese corpus. The transferability of the methodology, especially the structural and contextual rules designed for Portuguese syntax and semantics, to typologically different languages remains untested.
- What evidence would resolve it: The publication of Open IE corpora for additional languages, created using the same methodology, demonstrating high inter-annotator agreement and utility for system evaluation.

### Open Question 2
- Question: What is the impact of reannotating the "silver set" (the initial 200 sentences) with the finalized guidelines on classifier performance and overall corpus consistency?
- Basis in paper: [explicit] The authors acknowledge, "Due to time constraints, we were unable to perform this reannotation as part of the current work. We recognize this as a limitation and suggest it as an important avenue for future work."
- Why unresolved: The initial silver set was annotated with evolving guidelines, potentially introducing noise. The current validation results (e.g., AUC differences between PUD200 and PUD100) cannot definitively separate the effects of guideline refinement from inherent text difficulty.
- What evidence would resolve it: Results from an experiment where the silver set is reannotated using the final gold-standard guidelines, followed by a comparative evaluation of classifier performance trained on the original versus the reannotated silver set.

### Open Question 3
- Question: How can n-ary relations, which are common in Portuguese but were simplified to binary relations in this work, be formally represented and reliably annotated in an Open IE corpus?
- Basis in paper: [inferred] From the Discussion section: "In these cases, the sentences commonly present n-ary relations among entities, which cannot always be reified into binary relations... We acknowledge that this is a limitation and that handling n-ary relations is an important area for future work."
- Why unresolved: The authors explicitly excluded n-ary relations to simplify the annotation process. A principled extension of the current formalism and annotation guidelines to handle n-ary relations without introducing ambiguity or inconsistency is an open challenge.
- What evidence would resolve it: A proposed extension to the annotation schema and rules that defines n-ary relation extraction, validated through an annotation round on a subset of complex sentences, showing acceptable inter-annotator agreement.

## Limitations
- The methodology's generalizability to Portuguese text genres beyond PUD (news + Wikipedia) remains untested.
- N-ary relations were excluded and simplified to binary relations, losing contextual information.
- The silver set was annotated with evolving guidelines and not reannotated with final guidelines due to time constraints.

## Confidence

- **High Confidence (0.85)**: The reported inter-annotator agreement (Îº = 0.94) is internally consistent with the methodology described. The improvement from 0.63 to 0.94 across annotation steps is well-documented in Table 1.
- **Medium Confidence (0.70)**: The classifier validation results (AUC = 0.84 for Gold set) are reliable for the specific experimental setup, but external validation on different datasets is lacking.
- **Low Confidence (0.55)**: The claim that grounding rules in "structured proposition" theory reduces Open IE ambiguity is theoretically plausible but not empirically validated against alternative annotation frameworks.

## Next Checks

1. **Cross-Domain Evaluation**: Apply the annotated corpus rules to a Portuguese news dataset from a different source (e.g., Brazilian newspapers vs. PUD news) and measure IAA drop-off.
2. **Baseline Comparison**: Compare IAA and classifier performance against a corpus annotated using surface-level syntactic rules without semantic grounding.
3. **Error Propagation Analysis**: Trace how morphosyntactic errors in the source PUD sentences (removed in preprocessing) might have affected the remaining sentence selection and annotation patterns.