---
ver: rpa2
title: 'AutoForge: Automated Environment Synthesis for Agentic Reinforcement Learning'
arxiv_id: '2512.22857'
source_url: https://arxiv.org/abs/2512.22857
tags:
- arxiv
- user
- environment
- agent
- tool
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AutoForge introduces a unified pipeline for automated synthesis
  of simulated environments and high-difficulty tasks, enabling efficient and scalable
  reinforcement learning for language-based agents. The key innovation lies in (1)
  a method for automatically generating complex tool-use tasks and environments from
  tool descriptions, and (2) an Environment-level Relative Policy Optimization (ERPO)
  algorithm that mitigates simulated user instability and improves training stability
  via environment-level advantage estimation.
---

# AutoForge: Automated Environment Synthesis for Agentic Reinforcement Learning

## Quick Facts
- arXiv ID: 2512.22857
- Source URL: https://arxiv.org/abs/2512.22857
- Reference count: 17
- Primary result: Synthesizes complex environments from tool descriptions and trains agents via ERPO, significantly outperforming open-source baselines and narrowing the gap with closed-source models.

## Executive Summary
AutoForge introduces a unified pipeline for automated synthesis of simulated environments and high-difficulty tasks, enabling efficient and scalable reinforcement learning for language-based agents. The key innovation lies in (1) a method for automatically generating complex tool-use tasks and environments from tool descriptions, and (2) an Environment-level Relative Policy Optimization (ERPO) algorithm that mitigates simulated user instability and improves training stability via environment-level advantage estimation. Comprehensive experiments on three benchmarks demonstrate that AutoForge significantly outperforms open-source baselines and narrows the gap with closed-source models.

## Method Summary
AutoForge's synthesis pipeline takes tool description documents and automatically generates Python-based environments with SQLite databases. It constructs a tool dependency graph, samples and merges tool sequences, and inserts reasoning nodes to create complex Directed Acyclic Graphs (DAGs) representing tasks. The system then instantiates these tasks with initial and golden states for verification. Training employs ERPO, which uses environment-level advantage estimation and masks trajectories where the simulated user makes errors (MEU), improving stability over standard GRPO.

## Key Results
- AutoForge outperforms open-source baselines (SuperAgent, CrewAI, E2B) on τ-bench, τ2-Bench, and VitaBench by significant margins.
- ERPO with environment-level advantage estimation and MEU masking achieves higher and more stable reward curves compared to standard GRPO.
- Strong out-of-domain generalization demonstrated on ACEBench-zh, validating the effectiveness of simulated environment training.

## Why This Works (Mechanism)

### Mechanism 1
The system constructs tool dependency graphs, samples sequences, merges them to handle multiple requirements, and inserts "reasoning nodes" (intermediate inference steps). This structure forms a Directed Acyclic Graph (DAG) which serves as a blueprint for complex tasks, forcing the agent to perform multi-step logic rather than single-turn API calls.

### Mechanism 2
Masking Erroneous User (MEU) behaviors prevents the agent from learning false negative penalties when the simulated user hallucinates or errs. During RL rollouts, if the simulated user provides wrong information, these specific trajectories are excluded from the advantage estimation and loss calculation.

### Mechanism 3
Environment-level advantage estimation stabilizes training across heterogeneous environments better than group-level estimation. Standard GRPO normalizes rewards within a single question's group of trajectories, while AutoForge normalizes across all questions within a specific environment, mitigating the impact of outlier samples.

## Foundational Learning

- **Concept: Directed Acyclic Graphs (DAGs)**
  - Why needed here: AutoForge represents complex tasks not as linear chains but as DAGs to model parallel tool use and reasoning dependencies.
  - Quick check question: Can you explain why a DAG is superior to a linear chain for representing a task that requires retrieving two independent pieces of data to compute a final result?

- **Concept: Advantage Estimation (Actor-Critic methods)**
  - Why needed here: The ERPO algorithm relies on calculating the "advantage" (how much better an action is than the average) to update the policy.
  - Quick check question: If you normalize advantages using a mean that is skewed by a few extremely high rewards, how does that affect the gradient updates for the majority of average samples?

- **Concept: Simulated Environments / Sandboxes**
  - Why needed here: The core premise is training agents in a mock environment (Python code + DB) to avoid the latency and cost of real-world API calls.
  - Quick check question: What is the primary risk of training an agent in a simulated environment regarding "sim-to-real" transfer gaps?

## Architecture Onboarding

- **Component map:**
  1. **Synthesis Module:** Tool Docs -> Tool Graph -> Random Walks -> Merge/Reasoning DAG -> Task Intent & Python Environment
  2. **Task Instantiator:** Initializes DB state -> Executes DAG -> Saves Golden State -> Refines User Intent
  3. **RL Loop (ERPO):** Agent + Simulated User interact -> MEU Filter checks validity -> Advantage Calculator computes env-level advantages -> Optimizer updates policy

- **Critical path:** The dependency lives in the **Synthesis Module**. If the generated Python code for the environment does not match the state structure or the Tool Graph logic, the "Golden State" ($S^*$) used for rewards will be incorrect, rendering the RL signal noise.

- **Design tradeoffs:**
  - *Automation vs. Verification:* The system relies on LLMs to generate code and tasks. While scalable (low human effort), it requires robust execution handling (try/except) to prevent the synthesis pipeline from crashing on bad code.
  - *Masking vs. Bias:* Masking user errors cleans the signal but requires running an additional LLM (Judge) for every trajectory, increasing compute cost and potentially introducing judge bias.

- **Failure signatures:**
  - **Reward Collapse without Noise:** If the MEU filter is too aggressive, the agent might overfit to a small subset of "perfect" user interactions and fail on realistic, noisy inputs.
  - **Synthesis Drift:** Generated tasks become repetitive or trivial if the "Random Walks" on the tool graph do not sufficiently cover the tool space.

- **First 3 experiments:**
  1. **Unit Test Synthesis:** Run the synthesis pipeline on 3 diverse tool sets (e.g., calendar, SQL, shopping). Verify that the generated Python environment code executes without error and that the Golden State ($S^*$) is reachable.
  2. **MEU Ablation:** Train two agents, one with MEU and one without, on a fixed environment. Plot reward curves side-by-side to verify that the "dip" caused by user error penalties is resolved.
  3. **Generalization Check:** Train on the synthesized "Retail" environment and immediately test on the "Airline" environment from τ-bench (zero-shot) to confirm the agent learns general agentic capabilities, not just tool memorization.

## Open Questions the Paper Calls Out

- **Open Question 1:** How does increasing the number of synthetic environments impact RL training stability and out-of-domain generalization?
  - Basis: The paper states investigating scaling up synthetic environments would be beneficial, noting current experiments used only a few environments.
  - Why unresolved: The current study validates the pipeline on a fixed set of 10 environments, leaving the scaling laws for this specific synthesis method undefined.
  - What evidence would resolve it: A comparative analysis of model performance and training curves when varying the count of synthesized environments (e.g., 10 vs. 100 vs. 1000) against fixed out-of-domain benchmarks.

- **Open Question 2:** Can the synthesis pipeline be adapted to generate environments from task topics or general text rather than structured tool descriptions?
  - Basis: The authors identify that the pipeline "relies on tool description documents" and aim to "relax these input requirements, enabling the automatic construction... from task topics or general text."
  - Why unresolved: The current method requires structured documentation (state structure/functions) to initialize the environment, which restricts the breadth of scenarios that can be automated.
  - What evidence would resolve it: A demonstration of the pipeline successfully constructing valid executable environments and tasks using only unstructured topic descriptions or raw text as input.

- **Open Question 3:** Does incorporating turn-level value supervision improve decision-making accuracy compared to the current outcome-based reward system?
  - Basis: The paper notes that ERPO is "limited to outcome-based reward supervision" and proposes exploring "turn-level value supervision to improve the agent's step-by-step decision-making."
  - Why unresolved: Outcome-based rewards (0 or 1) may lead to sparse credit assignment issues in long-horizon tasks, but the benefit of denser turn-level signals in this specific architecture is untested.
  - What evidence would resolve it: Experiments comparing the sample efficiency and final success rates of agents trained with turn-level value functions versus the baseline binary reward.

## Limitations

- The effectiveness of automated task synthesis hinges on the quality and reliability of LLM-generated environments and reasoning nodes. If the synthesized code contains bugs or the reasoning nodes are flawed, the golden states become unreliable, corrupting the RL signal.
- The ERPO algorithm's performance boost relies on environment-level advantage estimation and MEU masking. While ablation studies show improvements, the relative contribution of each component is not isolated.
- Generalization claims on ACEBench-zh are promising but based on a single downstream benchmark. More diverse out-of-domain tests are needed to confirm robustness.

## Confidence

- **High Confidence:** The core methodology (synthesis pipeline → ERPO training → benchmark evaluation) is clearly described and internally consistent. The technical innovations (reasoning DAGs, environment-level advantage) are logically sound.
- **Medium Confidence:** The reported performance gains over baselines are significant, but the lack of public access to synthesized environments and the specific LLM prompts limits independent verification. The comparison to closed-source models is against reported numbers, not head-to-head tests.
- **Low Confidence:** The long-term stability of agents trained on simulated environments is not addressed. Sim-to-real transfer gaps for tool-use tasks could emerge in production settings.

## Next Checks

1. **Synthesis Reliability Audit:** Run the synthesis pipeline on 10 diverse tool sets and measure: (a) percentage of environments that execute without runtime errors, (b) percentage of generated tasks that are solvable, (c) average number of reasoning nodes per task vs. task complexity.

2. **ERPO Component Isolation:** Train three agents on the same environment: (a) standard GRPO, (b) ERPO without MEU, (c) ERPO without environment-level advantage. Compare reward curves and final success rates to quantify each component's contribution.

3. **Sim-to-Real Transfer Gap:** Take an agent trained on synthesized environments and test it on real API calls (e.g., Google Calendar API, Wikipedia API). Measure the drop in success rate compared to the simulated environment to quantify the sim-to-real gap.