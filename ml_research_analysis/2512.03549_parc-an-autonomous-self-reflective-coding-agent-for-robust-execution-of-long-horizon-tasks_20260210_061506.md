---
ver: rpa2
title: 'PARC: An Autonomous Self-Reflective Coding Agent for Robust Execution of Long-Horizon
  Tasks'
arxiv_id: '2512.03549'
source_url: https://arxiv.org/abs/2512.03549
tags:
- tasks
- task
- agent
- structure
- simulation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PARC is an autonomous coding agent designed to tackle long-horizon
  computational tasks using a hierarchical multi-agent architecture with self-assessment
  and self-feedback. It decomposes tasks into manageable units, each executed by a
  worker agent, while a planner coordinates the overall workflow.
---

# PARC: An Autonomous Self-Reflective Coding Agent for Robust Execution of Long-Horizon Tasks

## Quick Facts
- arXiv ID: 2512.03549
- Source URL: https://arxiv.org/abs/2512.03549
- Reference count: 40
- Primary result: Hierarchical multi-agent architecture with self-reflection successfully executes long-horizon computational tasks across materials science and data science domains

## Executive Summary
PARC is an autonomous coding agent designed to tackle long-horizon computational tasks using a hierarchical multi-agent architecture with self-assessment and self-feedback. It decomposes tasks into manageable units, each executed by a worker agent, while a planner coordinates the overall workflow. The self-reflection mechanism enables detection and correction of both local errors and high-level strategic issues, enhancing reliability in complex workflows.

Evaluated across computational science and data science domains, PARC autonomously reproduced key results from studies on lithium-ion conduction and alloy segregation. In materials science, it coordinated dozens of parallel simulations (each ~43 hours) for lithium diffusion in solid electrolytes, achieving an activation energy of 0.23 eV, close to the reference value of ~0.18 eV. For alloy segregation, it simulated Cr-Ni systems with light interstitials (B, N), reproducing structural changes consistent with published findings despite minor implementation oversights.

## Method Summary
PARC employs a hierarchical multi-agent architecture where a planner decomposes long-horizon tasks into sequences of 10-20 subtasks (approximately 100 total steps), each executed by specialized worker agents in isolated contexts. Workers access a shared workspace directory and can pass task summaries and aggregated context between sequential tasks. The self-reflection mechanism operates through independent self-assessment and self-feedback loops that detect both local errors (code bugs) and approach-level issues, triggering re-execution or plan revision as needed. The system uses Claude Sonnet 4.5 as the base LLM and focuses on autonomous error detection and correction without continuous human oversight.

## Key Results
- Achieved activation energy of 0.23 eV for Li-ion diffusion in solid electrolytes, close to reference value of ~0.18 eV
- Successfully reproduced alloy segregation structural changes in Cr-Ni systems with light interstitials (B, N)
- Achieved average R² score of 0.781 in polymer property prediction Kaggle competition, exceeding human baselines
- Improved puzzle solution scores by ~1% through autonomous algorithm selection and optimization

## Why This Works (Mechanism)
The hierarchical decomposition allows complex long-horizon tasks to be broken into manageable units that can be verified independently. Self-assessment provides an independent check on outputs, catching errors that might compound over long sequences. The feedback mechanism enables both local corrections and strategic adjustments to the overall approach.

## Foundational Learning

**Hierarchical task decomposition**: Breaking long-horizon tasks into manageable subtasks enables verification and error isolation at each step. Needed because monolithic approaches fail on complex multi-stage workflows. Quick check: Can the planner successfully decompose a 100-step task into coherent 10-20 task sequences?

**Self-assessment loops**: Independent evaluation of task outputs catches errors that might compound over long sequences. Needed because local errors can cascade and derail entire workflows. Quick check: Does the self-assessment mechanism detect both code bugs and approach-level errors?

**Context isolation**: Worker agents execute in isolated contexts while maintaining access to shared workspace. Needed to prevent context contamination while preserving necessary information flow. Quick check: Do later tasks reference only relevant prior artifacts?

## Architecture Onboarding

**Component map**: Planner -> Worker agents (isolated contexts) -> Self-assessment -> Self-feedback -> Task re-execution or plan revision

**Critical path**: Task decomposition → Worker execution → Self-assessment → Error correction → Result aggregation

**Design tradeoffs**: Granularity of task decomposition vs. verification tractability; isolation of worker contexts vs. information sharing; autonomous error detection vs. computational overhead

**Failure signatures**: 
- Context contamination: Later task outputs reference unrelated earlier artifacts
- Missed approach errors: Strategic mistakes go undetected, leading to wasted computation  
- Over-correction loops: Endless revisions without progress

**3 first experiments**:
1. Test self-assessment on a simple multi-step task with known planted errors
2. Compare performance with and without self-reflection components
3. Evaluate context isolation by checking cross-task contamination in long sequences

## Open Questions the Paper Calls Out

**Open Question 1**: How can the self-assessment mechanism be enhanced to reliably detect complex implementation errors (e.g., missing cell shape relaxation, incomplete trial move logic) that currently escape detection in long-horizon workflows?

**Open Question 2**: How can coding agents autonomously discover and integrate domain-appropriate external tools and libraries without explicit human guidance?

**Open Question 3**: What is the optimal granularity for task decomposition, and can it be dynamically adjusted based on task complexity and verification difficulty?

## Limitations
- Self-assessment misses complex implementation errors, particularly subtle algorithmic details
- Performance benefits from human hints about appropriate external tools
- Fixed task decomposition granularity may not be optimal for all problem types

## Confidence

**High confidence**: The architectural framework (hierarchical multi-agent with self-reflection) is well-specified and logically sound for addressing long-horizon tasks

**Medium confidence**: The reported performance improvements and task completion success rates, as these are supported by specific numerical results

**Low confidence**: The robustness of self-assessment across diverse error types and the generalizability of results beyond the demonstrated examples

## Next Checks

1. Implement a minimal PARC-like system with clear specifications for agent communication and self-assessment triggers, then evaluate on a simple multi-step computational task with known ground truth
2. Conduct ablation studies comparing performance with and without self-reflection components to quantify the specific contribution of autonomous error detection and correction
3. Test the system's ability to detect and correct approach-level errors (not just local bugs) using controlled experiments where the initial plan contains fundamental methodological flaws