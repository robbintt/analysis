---
ver: rpa2
title: Selecting Auxiliary Data via Neural Tangent Kernels for Low-Resource Domains
arxiv_id: '2511.07380'
source_url: https://arxiv.org/abs/2511.07380
tags:
- data
- answer
- which
- information
- auxiliary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Selecting Auxiliary Data via Neural Tangent Kernels for Low-Resource Domains

## Quick Facts
- arXiv ID: 2511.07380
- Source URL: https://arxiv.org/abs/2511.07380
- Reference count: 40
- Key outcome: None

## Executive Summary
This paper addresses the challenge of domain adaptation for LLMs in low-resource settings by proposing a novel method for selecting auxiliary training data from large general corpora. The key insight is that the Neural Tangent Kernel (NTK) remains stable during LoRA fine-tuning, enabling the use of initial-kernel similarity as a proxy for data utility. The method combines a coarse embedding-based pre-selection with a fine-grained NTK scoring stage, demonstrating consistent performance improvements across multiple domains.

## Method Summary
The method employs a two-stage NTK-Selector for auxiliary data selection. First, it performs coarse-grained pre-selection using embedding similarity after warm-up LoRA tuning on the small domain dataset. This creates a manageable candidate set from the massive general corpus. Second, it applies a computationally efficient Jacobian-free approximation of the NTK to rank candidates based on their average similarity to the domain data. The top-ranked samples form the final auxiliary dataset, which is then used alongside the original domain data for LoRA fine-tuning.

## Key Results
- NTK-Selector consistently improves model performance across medical, financial, legal, and psychological domains
- Shows +8.7 (Llama3) and +5.1 (Qwen3) improvement versus domain-only fine-tuning
- Outperforms random selection and embedding-only baselines in all tested scenarios
- Demonstrates effectiveness with as few as 1K domain examples

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs under LoRA fine-tuning exhibit stable "NTK-like" behavior, where the kernel's directional structure remains highly consistent with its initial state throughout training.
- **Mechanism:** The Neural Tangent Kernel (NTK) captures training dynamics. A candidate sample with high NTK similarity to target domain data implies its gradient updates will align with those from the domain, making it valuable auxiliary data. This stability allows the initial NTK to serve as a reliable proxy for data utility.
- **Core assumption:** The Frobenius cosine similarity between the time-evolved NTK and initial NTK remains very high (e.g., > 0.99) during the fine-tuning window.
- **Evidence anchors:**
  - [abstract] "...by empirically demonstrating a stable NTK-like behavior in LLMs during LoRA fine-tuning..."
  - [section 3.1] "This phenomenon is characterized by the time-evolved kernel being nearly collinear with the initial kernel, as quantified by a high Frobenius cosine similarity..."
  - [corpus] Corpus analysis (e.g., "Issues with Neural Tangent Kernel Approach to Neural Networks") highlights theoretical challenges with NTK assumptions, but this work provides specific empirical validation for the LoRA fine-tuning regime.
- **Break condition:** Excessive fine-tuning epochs or extremely high learning rates may cause the NTK to drift significantly, violating the NTK-like assumption and reducing the effectiveness of selection based on the initial NTK.

### Mechanism 2
- **Claim:** A Jacobian-free approximation, computed from the gradient of the sum of logits, can efficiently estimate the NTK while preserving the relative ranking of sample utility.
- **Mechanism:** The exact NTK is computationally infeasible for LLMs. The proposed approximation calculates gradients on the summed output, which linearizes the problem. Empirically, cross-output gradient interactions are small (~6%), making this a viable proxy for the full Jacobian-based NTK.
- **Core assumption:** Gradient directions for different output dimensions in the LLM are nearly orthogonal, so ignoring cross-terms does not significantly distort the relative similarity scores.
- **Evidence anchors:**
  - [abstract] "...proposing a Jacobian-free approximation method."
  - [section 3.2] "...gradient directions for different output dimensions tend to be nearly orthogonal, making the cross terms very small (approximately 6% of the exact NTK)."
  - [corpus] No direct corpus support for this specific Jacobian-free technique; evidence is derived primarily from the paper's empirical validation.
- **Break condition:** For tasks where model outputs are not orthogonal (e.g., highly interdependent predictions), approximation error may increase, potentially degrading selection quality.

### Mechanism 3
- **Claim:** A two-stage process combining embedding-based pre-selection with fine-grained NTK scoring efficiently isolates high-value auxiliary data from massive corpora.
- **Mechanism:** A coarse-grained embedding filter (using KNN on a domain-tuned encoder) reduces millions of candidates to a tractable set. The computationally intensive NTK approximation is then applied only to this smaller set, ranking samples by their average NTK similarity to the domain data.
- **Core assumption:** High-value auxiliary data exists within the semantic neighborhood of the domain data (captured by embeddings), and its training utility can be further refined by analyzing gradient alignment (captured by NTK).
- **Evidence anchors:**
  - [section 4.1] "It first performs a coarse-grained pre-selection of candidates using embedding similarity. This is followed by a fine-grained NTK selection stage..."
  - [section 5.2] "...NTK-Selector consistently enhances model performance... selects auxiliary data whose optimization trajectories align with those of the target domain"
  - [corpus] Corpus evidence for this specific two-stage NTK-based selection architecture is weak; validation is internal to the paper.
- **Break condition:** If the embedding space fails to capture relevant semantic features for the target domain, valuable candidates may be filtered out before NTK scoring.

## Foundational Learning

- **Concept: Neural Tangent Kernel (NTK)**
  - **Why needed here:** NTK is the core theoretical tool, converting non-linear LLM training dynamics into a linear kernel framework. It enables the prediction of auxiliary data utility by measuring similarity in gradient space.
  - **Quick check question:** In the infinite-width limit, how does the NTK of a network change during training? (Answer: It remains constant.)

- **Concept: LoRA (Low-Rank Adaptation) Fine-Tuning**
  - **Why needed here:** The method is explicitly designed for LoRA. LoRA constrains gradient updates to a low-rank subspace, making NTK computation feasible and contributing to the empirically observed NTK-like stability.
  - **Quick check question:** In LoRA, which parameters are updated during fine-tuning? (Answer: Only the low-rank adapter matrices; main model weights are frozen.)

- **Concept: Data Selection for Domain Adaptation**
  - **Why needed here:** The central problem is augmenting a low-resource domain dataset with auxiliary data from a general corpus to prevent overfitting and improve generalization. Understanding this goal is critical for evaluating the method's design.
  - **Quick check question:** What is the primary risk of fine-tuning an LLM on a very small, domain-specific dataset? (Answer: Severe overfitting and poor generalization.)

## Architecture Onboarding

- **Component Map:** Input domain dataset and general corpus → Warm-up LoRA tuning → Embedding pre-selection (KNN) → Jacobian-free NTK approximation → Rank by average NTK similarity → Output top N auxiliary samples

- **Critical Path:** The Jacobian-free NTK approximation (Component 3) is the most critical and novel step. Its accuracy dictates the quality of data selection. The pre-selector (Component 2) is vital for scalability.

- **Design Tradeoffs:**
  - **Pre-selection size (M) vs. Runtime:** Larger M improves candidate pool quality but increases compute.
  - **Projection Dimension (p) vs. Accuracy:** Higher p preserves more gradient information, improving accuracy but consuming more memory and compute.
  - **Approximation vs. Exact NTK:** The approximation makes the method feasible but introduces error. The paper empirically shows this error is acceptable for ranking purposes.

- **Failure Signatures:**
  - **Degraded Performance:** Selected data hurts performance vs. domain-only tuning. Indicates NTK similarity is a poor proxy, possibly due to broken assumptions or poor approximation.
  - **No Gain over Random:** Method fails to outperform random selection, suggesting a failure in the ranking logic or gradient/projection steps.
  - **Computational Bottleneck:** Timeouts or OOM errors, likely from an M or p that is too large for available resources.

- **First 3 Experiments:**
  1. **Ablation on Pre-selection Size (M):** Test M = 2N, 4N, 16N. Plot accuracy vs. runtime to find the optimal trade-off.
  2. **Ablation on Projection Dimension (p):** Test p = 1024, 2048, 4096, 8192. Measure impact on accuracy and memory to select a suitable dimension.
  3. **Cross-Domain Validation:** Run the full pipeline on different low-resource domains (medical, financial, legal) using the same candidate corpus (e.g., CoT Collection) to verify robustness and identify domain-specific sensitivities.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does a "saturation threshold" exist for auxiliary data utility, beyond which adding general-domain samples yields negligible returns regardless of selection quality?
- **Basis in paper:** [explicit] In Section 6 (Analysis), the authors note "diminishing returns" when scaling data from ×5 to ×20 and "hypothesize that the supply of highly relevant cross-domain samples is inherently limited."
- **Why unresolved:** The paper observes the phenomenon empirically on the TFNS task but does not quantify the theoretical limit or determine if this threshold varies significantly across different domains.
- **What evidence would resolve it:** A theoretical analysis of domain similarity bounds or large-scale empirical curves mapping performance saturation points across diverse domain pairs (e.g., Medical vs. General vs. Code).

### Open Question 2
- **Question:** Are higher-capability base models disproportionately more susceptible to performance degradation from low-quality auxiliary data compared to weaker models?
- **Basis in paper:** [explicit] In Section 5.2 (Main Results), the authors "hypothesize that [QWEN3-8B's] superior initial performance... makes it more sensitive to the quality of auxiliary data," but verify this only on two model architectures.
- **Why unresolved:** It remains unclear if this sensitivity is a consistent scaling law of model capability or a specific artifact of the QWEN3 training data/instruction tuning.
- **What evidence would resolve it:** A controlled ablation study varying model sizes (e.g., 1B to 70B parameters) and measuring the "sharpness" of the performance drop when fixed amounts of random/low-quality auxiliary data are introduced.

### Open Question 3
- **Question:** Is the observed "NTK-like" stability and the success of NTK-Selector dependent on the low-rank constraint of LoRA, or does it generalize to full-parameter fine-tuning?
- **Basis in paper:** [inferred] Section 3.1 demonstrates NTK stability and Section 4.3 computes gradients restricted to LoRA parameters ($\nabla_{\theta_{LoRA}}$). Theoretical NTK assumptions (infinite width) clash with practical LoRA constraints, leaving the role of the low-rank bottleneck unclear.
- **Why unresolved:** The paper does not investigate if the gradient dynamics in full-parameter fine-tuning remain stable enough (cosine similarity > 0.99) for the initial-kernel approximation to hold.
- **What evidence would resolve it:** Comparing the Frobenius cosine similarity of NTK evolution during full fine-tuning vs. LoRA fine-tuning, and testing the selector's performance on full fine-tuning runs.

### Open Question 4
- **Question:** Does the Jacobian-free approximation effectively capture gradient interactions in generative tasks with long output sequences?
- **Basis in paper:** [inferred] Section 3.2 introduces a Jacobian-free approximation that ignores cross-output interaction terms ($k \neq m$), justified by observing these terms are small (~6%) in the tested tasks. This assumption is unverified for generative tasks where output tokens have strong sequential dependencies.
- **Why unresolved:** In classification (the paper's focus), output dimensions are often independent. In generation, the gradient of the 100th token depends on the 1st, potentially making the ignored cross-terms significant.
- **What evidence would resolve it:** Calculating the correlation between exact NTK and the Jacobian-free approximation on sequence-to-sequence tasks (e.g., summarization or translation) with varying output lengths.

## Limitations

- Theoretical grounding for NTK stability in LoRA fine-tuning remains incomplete and needs deeper mathematical analysis
- Jacobian-free NTK approximation error bounds and task-dependent variations are not rigorously validated
- Scalability challenges with computing LoRA gradients for millions of candidates from massive corpora

## Confidence

**High Confidence:** The empirical results showing NTK-Selector outperforming random selection across multiple domains (medical, financial, legal, psychological) are well-supported. The consistent 5-9 point improvements with Llama3 and Qwen3 models are convincing given the controlled experimental setup.

**Medium Confidence:** The two-stage architecture (embedding pre-selection + NTK fine-selection) is logically sound and demonstrates practical effectiveness. However, the assumption that embedding similarity correlates with NTK utility isn't thoroughly validated, and the optimal parameters for pre-selection (M, K) appear somewhat arbitrary.

**Low Confidence:** The theoretical claims about NTK stability during LoRA training are primarily empirical observations rather than proven theorems. The Jacobian-free approximation's error bounds and their impact on different types of tasks need more rigorous investigation.

## Next Checks

1. **NTK stability stress test:** Systematically vary LoRA rank (r=4, 8, 16, 32) and learning rate (1e-4 to 1e-3) during the warm-up phase, then measure Frobenius cosine similarity between initial and time-evolved NTKs. Plot stability curves to identify the operational boundaries where NTK-like behavior breaks down.

2. **Approximation error quantification:** For a subset of domains, compute both the exact NTK (using full Jacobian) and the Jacobian-free approximation on the same candidate samples. Calculate ranking correlation (Kendall tau, Spearman rho) between the two methods and analyze how approximation error varies with output dimensionality and task complexity.

3. **Cross-corpus generalization:** Test the NTK-Selector methodology using a different general corpus (e.g., The Pile, C4) instead of Cot Collection. Measure whether the same parameter settings (M=4N, p=8192, K=M/4) maintain performance, or if corpus-specific tuning is required. This validates whether the approach generalizes beyond the specific candidate pool used in the paper.