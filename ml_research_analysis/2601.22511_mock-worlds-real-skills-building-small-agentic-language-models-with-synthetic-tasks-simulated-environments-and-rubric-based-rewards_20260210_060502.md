---
ver: rpa2
title: 'Mock Worlds, Real Skills: Building Small Agentic Language Models with Synthetic
  Tasks, Simulated Environments, and Rubric-Based Rewards'
arxiv_id: '2601.22511'
source_url: https://arxiv.org/abs/2601.22511
tags:
- tool
- agentic
- data
- tasks
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of enabling small language models
  to achieve strong agentic capabilities, which are typically limited to much larger
  models due to the high cost and deployment overhead. The key problem is the lack
  of diverse, challenging training data and stable, scalable environments for reinforcement
  learning in tool-use and reasoning tasks.
---

# Mock Worlds, Real Skills: Building Small Agentic Language Models with Synthetic Tasks, Simulated Environments, and Rubric-Based Rewards

## Quick Facts
- arXiv ID: 2601.22511
- Source URL: https://arxiv.org/abs/2601.22511
- Reference count: 26
- Small models (8B-14B) outperform larger baselines (32B) on 14+ agentic benchmarks

## Executive Summary
This paper addresses the challenge of enabling small language models to achieve strong agentic capabilities, which are typically limited to much larger models due to the high cost and deployment overhead. The key problem is the lack of diverse, challenging training data and stable, scalable environments for reinforcement learning in tool-use and reasoning tasks. The authors propose SYNTHAGENT, a framework that jointly synthesizes diverse tool-use tasks and simulates complete environments using a strong teacher model to generate novel tasks and tools, rewrites instructions to be intentionally underspecified, and provides stable mock tools and users. A novel rubric-based reward system derived from observable behavior is used to train the model. Across 14 challenging datasets in math, search, and tool use, models trained on synthetic data achieve substantial gains, with small models (8B-14B) outperforming larger baselines (32B), demonstrating that diverse synthetic tasks and stable simulated environments enable small models to rival much larger ones.

## Method Summary
The SYNTHAGENT framework generates synthetic tool-use tasks by first creating personas and corresponding virtual toolsets using a strong teacher model. Instructions are rewritten to be intentionally underspecified by splitting them into visible parts and hidden user contexts, forcing agents to query for missing information. A mock environment simulates tool execution and user responses with task-level finite mapping for consistency. Rubric-based rewards are derived from observable behavior rather than subjective LLM judging, using subgoals and constraint violations extracted from teacher demonstrations. The system trains small models (8B-14B) using GRPO with combined synthetic tool-use and reasoning data, achieving performance that rivals or exceeds much larger models.

## Key Results
- Small models (8B-14B) outperform 32B baselines on TAU-2, BFCL-V4 Multi-turn, and math benchmarks
- GRPO training with rubric-based rewards achieves stable learning on synthetic tasks
- Task-level finite mapping provides sufficient consistency for stable RL without global databases
- Underspecified instructions force interactive planning and prevent gradient collapse

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Underspecified instructions (information gaps) prevent gradient degeneration and force interactive planning.
- **Mechanism:** The framework splits the initial state $s_0$ into an agent-visible instruction $I$ and a hidden user context $C$. Because $I$ lacks critical details, the agent cannot immediately determine the optimal action $a^*$ (high conditional entropy $H(a^*|I)$). This forces the policy to explore by querying the user to reduce uncertainty before acting, preventing the advantage variance from collapsing to zero.
- **Core assumption:** The teacher model can generate solvable tasks where the missing information is strictly necessary for the workflow, rather than just confusing.
- **Evidence anchors:**
  - [abstract] "rewrites them into intentionally underspecified instructions. This compels agents to actively query users for missing details."
  - [section] Page 4, Section 3.1 "Fuzzy Task Generation": discusses Eq. (1) where $Var[A(s_t, a_t)] \approx 0$ in overspecified tasks, and Eq. (3) defining the split $(I, C)$.
  - [corpus] Related work (ToolBrain, CASCADE) emphasizes tool usage, but this paper specifically targets the *planning bottleneck* caused by over-determined datasets.
- **Break condition:** If the "fuzzy task" is too vague or the hidden context is irrelevant, the agent learns to randomly guess or hallucinate rather than query.

### Mechanism 2
- **Claim:** Task-level finite mapping stabilizes the advantage estimation $\hat{A}_t$ during RL rollouts.
- **Mechanism:** LLM-based simulators are stochastic; identical tool calls can yield different responses, causing the same state-action pair $(s_t, a_t)$ to produce inconsistent future returns. The paper introduces a lightweight dictionary $M=\{(u_i, y_i)\}$ per task. If a tool call $u$ matches a previous entry, the simulator returns the cached $y$, ensuring the environment is deterministic for a given task session.
- **Core assumption:** Consistency within a single task session is sufficient for stable learning, and strict global consistency across all tasks is not required.
- **Evidence anchors:**
  - [abstract] "provides stable mock tool and user simulations with lightweight task-level consistency."
  - [section] Page 5, Section 3.2 "Stable Environments": "When the model issues a valid tool call $u$, the simulator checks similar entries in $M$... Equivalence checking and response generation can be handled in a single forward pass."
  - [corpus] Corpus papers (e.g., *Enhancing Vision-Language Model Training...*) validate synthetic worlds but do not explicitly address the *consistency/stability* mechanism via finite mapping.
- **Break condition:** If the tool arguments contain semantic equivalence (e.g., "getNYTime()" vs "getTime('New York')") that the LLM simulator fails to match, the mapping misses the cache, introducing noise.

### Mechanism 3
- **Claim:** Execution-grounded rubrics provide a denser, verifiable reward signal than generic LLM judging.
- **Mechanism:** Instead of asking an LLM to subjectively score a trajectory, the system extracts concrete subgoals from the teacher's workflow (e.g., "Verify DB State"). The reward function $R(\tau)$ is a deterministic calculation based on the fraction of subgoals completed and constraint violations. This aligns the reward directly with the task logic rather than generic conversational quality.
- **Core assumption:** The teacher-generated workflow accurately represents the necessary steps to solve the task.
- **Evidence anchors:**
  - [abstract] "Rubric-based rewards are derived from high-level workflows and actual execution trajectories."
  - [section] Page 5, Section 3.3: "we avoid subjective LLM-written rubrics and derive rewards from observable behavior... $R(\tau) = I(\tau) \cdot (N_{subgoals}(\tau) + I_{user\_query}(\tau))$."
  - [corpus] *CORE: Full-Path Evaluation...* supports evaluating the full path over final states, validating the rubric-based approach.
- **Break condition:** If the "forbidden behaviors" or "subgoals" are defined too loosely, the agent may "reward hack" by satisfying the rubric without solving the user's actual intent.

## Foundational Learning

- **Concept: Policy Gradient Variance**
  - **Why needed here:** The paper explicitly frames the "information gap" as a solution to vanishing advantage variance ($Var \approx 0$). You must understand why low variance in the advantage function kills learning in RL to grasp why "harder/ambiguous" tasks are mathematically necessary.
  - **Quick check question:** If an agent always takes the same optimal path in a dataset, why does the gradient update become zero or negligible?

- **Concept: Semantic Equivalence / Consistency**
  - **Why needed here:** The "Task-level Finite Mapping" relies on the simulator detecting that `restart_pod(123)` is equivalent to `restart_pod(pod_id=123)`. Understanding how LLMs handle semantic matching vs. string matching is critical for debugging the mock environment.
  - **Quick check question:** Why does the paper include the mapping $M$ directly in the simulator's prompt rather than using a vector database?

- **Concept: Rubrics vs. Outcome Rewards**
  - **Why needed here:** The paper moves away from "Did you get the right answer?" to "Did you follow the safe process?". This is a shift from Outcome-based RL (ORL) to Process-based RL (PRL).
  - **Quick check question:** According to Equation 5, what happens to the reward $R(\tau)$ if the agent violates a constraint (e.g., deletes a database without a backup)?

## Architecture Onboarding

- **Component map:**
  Synthesizer (Teacher) -> Mock Environment (Mock User + Mock Tool) -> Evaluator (LLM Judge) -> Agent (Student)

- **Critical path:** The **Fuzzy Task Generation** logic. If the split between $I$ and $C$ is flawed (e.g., $I$ contains the answer, or $C$ is irrelevant), the RL signal fails. The interaction between the *Generator* and the *Rubric Creator* must be perfectly aligned.

- **Design tradeoffs:**
  - **Task-Scoped vs. Global Memory:** The paper chooses a finite mapping per task (lightweight, fast) over a global vector database (slower, complex). This works because synthetic tasks are isolated ecosystems.
  - **Teacher Model Size:** The ablation (Fig 3) suggests the simulator/evaluator can be small (30B), but the *Data Synthesizer* likely needs to be strong (235B) to create coherent logic puzzles.

- **Failure signatures:**
  - **"The Silent Agent":** Agent never queries the user, tries to solve immediately, and fails repeatedly. *Cause:* Information gap too wide or instruction unclear.
  - **"The Simulator Drift":** Reward variance increases over training steps. *Cause:* The "Mock Tool" is hallucinating new responses for cached calls instead of using the mapping $M$.
  - **"The Box Checker":** Agent hits all rubric subgoals but output is nonsense. *Cause:* Rubric is too permissive (misses semantic correctness checks).

- **First 3 experiments:**
  1. **Verify Gradient Signal:** Train a small model on *over-specified* data (no information gap) vs. *fuzzy* data. Confirm that the loss/gradient norm behaves as predicted in Eq. 1-2.
  2. **Stress Test the Mapping:** Log the "Mock Tool" hit/miss rate for the finite mapping $M$. If the miss rate is high, the simulator is failing to recognize semantic equivalence.
  3. **Rubric Alignment Check:** Manually inspect 20 trajectories. Check if $R(\tau)$ (high score) correlates with human judgment of "successful task completion" to detect reward hacking.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific, high-leverage factors in synthetic data pipelines (e.g., persona diversity vs. tool complexity) that most directly drive agentic capability improvements?
- Basis in paper: [explicit] The authors state in the Limitations section that "future work should... identify the key factors that are most critical for building effective agents" because leading models do not release their synthesis procedures.
- Why unresolved: The paper demonstrates that SYNTHAGENT works but does not isolate which specific components of the synthesis (persona selection, tool generation logic, or constraint complexity) contribute most to the gains.
- What evidence would resolve it: A comprehensive ablation study varying individual synthesis parameters while holding others constant to measure their impact on final benchmark performance.

### Open Question 2
- Question: Does training on LLM-simulated tool responses create a "sim-to-real" gap where agents fail to handle the strict error handling or latency of real-world APIs?
- Basis in paper: [inferred] The paper relies on "LLM-based tool responses" (Section 3.2) for stability. While effective for training, simulated logic may be more forgiving or semantically consistent than real API errors.
- Why unresolved: The evaluation uses real benchmarks (TAU-2, BFCL), but the paper does not analyze if failure modes differ between agents trained on mock tools versus those trained on real API calls.
- What evidence would resolve it: A comparative error analysis of agent behavior when facing malformed API responses or network errors in a real-world deployment scenario.

### Open Question 3
- Question: Does the "Fuzzy Task" design, which intentionally creates information gaps, bias agents to over-query users for clarification even when instructions are fully specified?
- Basis in paper: [inferred] Section 3.1 explicitly generates tasks where $H(a^*|I) \gg \epsilon$ to force information retrieval. This trains the agent to ask questions, potentially creating a prior that instructions are always incomplete.
- Why unresolved: The experiments measure success on complex tasks, but do not report false positive rates (unnecessary queries) on tasks that are already complete and unambiguous.
- What evidence would resolve it: Evaluating the trained agent on a control set of fully specified tasks to measure the frequency of redundant user interactions.

## Limitations
- Dependency on high-quality teacher model (235B) for generating coherent synthetic tasks
- Potential sim-to-real gap when transferring from LLM-simulated tools to real API calls
- No analysis of which synthesis parameters (persona diversity, tool complexity) drive the most improvement

## Confidence

- **High Confidence:** Empirical results showing small models (8B-14B) outperforming larger baselines (32B) on multiple benchmarks
- **Medium Confidence:** Theoretical mechanism linking information gaps to gradient stability
- **Medium Confidence:** Finite mapping consistency mechanism for stable RL

## Next Checks

1. **Gradient Signal Verification:** Train two identical small models on synthetic data - one with properly underspecified tasks and one with overspecified tasks. Track policy entropy and advantage variance over the first 1,000 training steps to confirm the theoretical prediction that underspecification prevents gradient collapse.

2. **Semantic Equivalence Stress Test:** For a subset of 100 synthetic tasks, deliberately introduce semantically equivalent tool calls with different parameter formats (e.g., `restart_pod(123)` vs `restart_pod(pod_id=123)`). Measure the mock tool's cache hit rate and manually inspect failures to quantify the consistency mechanism's reliability.

3. **Rubric Alignment Study:** Have three independent human raters evaluate 50 trajectories using the same rubric criteria. Calculate inter-rater reliability (Cohen's kappa) and compare against the LLM judge's scores to quantify reward signal noise and potential for reward hacking.