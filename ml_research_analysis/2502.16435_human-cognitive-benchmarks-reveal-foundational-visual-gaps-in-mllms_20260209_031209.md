---
ver: rpa2
title: Human Cognitive Benchmarks Reveal Foundational Visual Gaps in MLLMs
arxiv_id: '2502.16435'
source_url: https://arxiv.org/abs/2502.16435
tags:
- test
- image
- visual
- cognitive
- first
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VISFACTOR, a benchmark that digitizes 20
  vision-centric subtests from the Factor-Referenced Cognitive Test (FRCT) battery,
  providing the first systematic evaluation of foundational visual cognition in Multimodal
  Large Language Models (MLLMs). Unlike existing benchmarks focused on high-level
  tasks, VISFACTOR isolates core visual abilities such as mental rotation, figure-ground
  discrimination, and spatial reasoning, directly mapping to human psychometric factors.
---

# Human Cognitive Benchmarks Reveal Foundational Visual Gaps in MLLMs

## Quick Facts
- arXiv ID: 2502.16435
- Source URL: https://arxiv.org/abs/2502.16435
- Reference count: 40
- Primary result: VISFACTOR benchmark reveals MLLMs consistently fail on core visual cognition tasks, with best score 30.17% and chance level at 2.89%

## Executive Summary
This paper introduces VISFACTOR, the first systematic benchmark for evaluating foundational visual cognition in Multimodal Large Language Models (MLLMs) using 20 vision-centric subtests adapted from the Factor-Referenced Cognitive Test battery. Unlike existing benchmarks focused on high-level tasks, VISFACTOR isolates core visual abilities like mental rotation, figure-ground discrimination, and spatial reasoning. Evaluation of 23 frontier MLLMs reveals consistently low performance—best score only 30.17%—with failures across spatial and perceptual tasks regardless of model size or prompting strategy. The findings suggest current MLLMs lack genuine human-like visual cognition, highlighting a critical gap for advancing embodied AI and safety-critical applications.

## Method Summary
The VISFACTOR benchmark digitizes 20 vision-centric subtests from the FRCT battery, covering 10 cognitive factors across 4 domains. To reduce guessing bias, the benchmark employs rule-based variants including decomposed multiple-choice questions, grouped-consistency items, and symmetry variants, lowering chance-level accuracy to 2.89%. The evaluation protocol uses standardized prompts and exact-match scoring rules for 23+ frontier MLLMs, with performance measured across individual subtests and overall accuracy. A parametric generator enables unlimited, difficulty-controllable test cases for scalable future evaluation.

## Key Results
- VISFACTOR evaluation of 23 MLLMs reveals consistently low performance across all models (best: 30.17%, average: ~22%)
- All models fail systematically on spatial and perceptual tasks regardless of size, prompting strategy, or architecture
- The "Middle Score Anomaly" shows models achieving intermediate accuracy (30-50%) on trivial tasks, suggesting flawed rather than random reasoning
- Performance collapses when replacing semantic objects with abstract patterns, indicating reliance on concept-level rather than low-level visual processing

## Why This Works (Mechanism)
VISFACTOR works by isolating core visual cognition abilities that are fundamental to human perception but absent in current MLLMs. By adapting psychometric subtests from FRCT and reducing guessing bias to 2.89%, the benchmark creates a rigorous evaluation framework that reveals genuine cognitive gaps rather than statistical artifacts.

## Foundational Learning
- **Factor-Referenced Cognitive Test (FRCT)**: A battery of psychometric tests that isolate specific cognitive factors like spatial reasoning and visual memory. Why needed: Provides validated, factor-analyzed tasks that map directly to human cognitive abilities. Quick check: Verify FRCT subtests have established reliability and validity in human studies.
- **Chance-level reduction mechanisms**: Techniques like grouped-consistency items and decomposed multiple-choice that lower random guessing probability. Why needed: Prevents inflated scores from lucky guesses in large test suites. Quick check: Confirm calculated 2.89% chance level through Monte Carlo simulation.
- **Parametric test generation**: Creating synthetic variants of existing tasks with controlled difficulty parameters. Why needed: Enables scalable evaluation and curriculum-style training approaches. Quick check: Verify generated items maintain task validity while varying difficulty.
- **Exact-match scoring**: Strict evaluation rules requiring precise answers rather than fuzzy matching. Why needed: Ensures objective measurement of model capabilities without leniency for partial understanding. Quick check: Test scoring consistency across different answer formats.
- **Cognitive factor mapping**: Organizing tasks by psychometric dimensions like Visualization, Memory, and Spatial Scanning. Why needed: Enables identification of specific cognitive gaps rather than generic performance metrics. Quick check: Confirm factor loadings align with original FRCT validation studies.
- **Zero-shot evaluation**: Testing models without any task-specific training or fine-tuning. Why needed: Measures baseline visual cognition capabilities rather than memorization of training data. Quick check: Verify no overlap between training data and VISFACTOR items.

## Architecture Onboarding
- **Component map**: Benchmark repository -> Item pool (808 questions/3,046 queries) -> Evaluation protocol (prompt templates + scoring rules) -> Model inference interface (23+ MLLMs) -> Parametric generator -> Scoring engine
- **Critical path**: Prompt model with task instruction and visual stimulus -> Model generates textual response -> Parse response and match against ground-truth using exact-match rules -> Track success/failure per variant -> Aggregate to compute subtest and overall accuracy
- **Design tradeoffs**: Psychometric validity vs. digital feasibility (simplifying paper-based FRCT for API interface may alter difficulty); comprehensive evaluation vs. computational intensity (variant generation increases test suite size); strict scoring vs. model interpretability (exact-match may miss partial understanding)
- **Failure signatures**: "Middle Score Anomaly" (intermediate accuracy on trivial tasks suggesting flawed mechanisms); angular bias (defaulting to 45-degree approximations); verbal overshadowing (longer CoT outputs negatively correlate with accuracy)
- **First 3 experiments**: 1) Baseline evaluation of target models to establish cognitive factor gaps; 2) Ablation study replacing semantic images with abstract patterns to quantify concept-level reliance; 3) Difficulty scaling using parametric generator to evaluate performance degradation curves

## Open Questions the Paper Calls Out
### Open Question 1
- **Question:** Can curriculum-style pre-training or factor-aligned loss functions effectively bridge the foundational visual cognition gap in MLLMs?
- **Basis in paper:** [explicit] The Conclusion states that bridging the gap "will likely require curriculum-style pre-training that interleaves psychometric micro-tasks with natural images, embodied or 3-D data that grounds spatial relations, and factor-aligned loss functions."
- **Why unresolved:** The paper introduces VISFACTOR to *diagnose* the gap but does not implement or evaluate the specific architectural or training interventions proposed to fix it.
- **What evidence would resolve it:** An evaluation showing that models trained with the proposed "psychometric micro-tasks" curriculum achieve significantly higher scores on VISFACTOR and generalize better to downstream embodied tasks compared to standard training regimes.

### Open Question 2
- **Question:** What are the underlying mechanisms causing the "Middle Score Anomaly" in MLLMs?
- **Basis in paper:** [explicit] The authors identify a "Middle Score Anomaly" (Section 3.2) where models achieve intermediate accuracy (30–50%) on tasks humans typically solve perfectly or fail completely. They ask: "Why do models... achieve, say, 70% accuracy on this task... suggesting partial understanding but inexplicable failures?"
- **Why unresolved:** The paper observes the anomaly and attributes it to a lack of "genuine reasoning" or "all-or-nothing" mastery, but does not isolate whether this results from attention dropout, tokenization artifacts, or probabilistic guessing strategies inherent to the LLM architecture.
- **What evidence would resolve it:** A mechanistic interpretability study mapping specific attention heads or features to visual reasoning steps, or a demonstration that targeted data augmentation can shift model performance from intermediate scores to bimodal (all-or-nothing) distributions.

### Open Question 3
- **Question:** Can MLLMs be trained to process low-level visual features (e.g., abstract patterns) without relying on high-level concept mapping?
- **Basis in paper:** [inferred] Section 4.1 ("Visual Comparison Or Concept Recognition?") demonstrates that models rely on "interpretable, concept-level representations rather than low-level visual patterns," as performance collapses when using abstract figures (CF2) instead of semantic objects.
- **Why unresolved:** While the paper identifies this reliance as a failure mode, it does not propose a method for forcing models to process raw visual primitives (edges, orientations) without hallucinating or mapping them to known semantic concepts.
- **What evidence would resolve it:** A training regimen using strictly abstract, non-semantic synthetic data (like the generated CF2 images) that results in models successfully generalizing to novel abstract patterns without any text-based semantic mapping.

### Open Question 4
- **Question:** How does the digitization of psychometric tests alter their difficulty and factor structure compared to the original human protocols?
- **Basis in paper:** [explicit] Appendix I (Limitations) notes the "Digitization Gap," stating that "These changes inevitably alter item difficulty, so direct numerical comparisons with legacy human norms are inappropriate."
- **Why unresolved:** The paper evaluates MLLMs using a digital interface but lacks a contemporary human baseline on that specific digital protocol, making it difficult to distinguish between model failure and test difficulty inflation caused by the removal of tactile or timed cues.
- **What evidence would resolve it:** A user study with human participants taking the VISFACTOR benchmark under the same conditions (zero-shot, digital interface) as the models to establish a valid comparative baseline.

## Limitations
- Digitization gap: Simplification of paper-based FRCT for digital evaluation may significantly alter item difficulty and cognitive demands
- No human baseline: Lack of contemporary human performance data on the digitized protocol prevents valid comparison
- Interpretation uncertainty: Low scores may reflect either genuine cognitive gaps or evaluation artifacts from translation to digital format

## Confidence
- High confidence: Methodology for reducing guessing bias (2.89% chance level) and systematic evaluation of 23 models are well-documented and reproducible
- Medium confidence: Interpretation that low scores indicate fundamental visual cognition gaps is reasonable but not definitively proven
- Low confidence: Claim of being "first systematic evaluation" is difficult to verify given evolving landscape of AI cognitive benchmarks

## Next Checks
1. **Prompt Template Verification**: Obtain and validate exact prompt templates for all 20 subtests against the benchmark repository to ensure faithful reproduction
2. **Cross-Benchmark Correlation**: Compare VISFACTOR results with other established vision benchmarks (e.g., MMMU, MathVista) to assess performance pattern consistency
3. **Human-MLLM Difficulty Alignment**: Conduct small-scale human study using digitized VISFACTOR items to determine if tasks maintain intended difficulty and cognitive discrimination