---
ver: rpa2
title: 'RefTool: Enhancing Model Reasoning with Reference-Guided Tool Creation'
arxiv_id: '2505.21413'
source_url: https://arxiv.org/abs/2505.21413
tags:
- tool
- tools
- reftool
- reasoning
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RefTool addresses the challenge of limited tool availability in
  complex problem-solving by introducing a reference-guided framework for automatic
  tool creation. The core method leverages structured external materials like textbooks
  to generate executable tools, validate them with illustrative examples, and organize
  them hierarchically into a toolbox.
---

# RefTool: Enhancing Model Reasoning with Reference-Guided Tool Creation

## Quick Facts
- arXiv ID: 2505.21413
- Source URL: https://arxiv.org/abs/2505.21413
- Reference count: 40
- Primary result: 11.3% average accuracy improvement on causality, physics, and chemistry benchmarks

## Executive Summary
RefTool addresses the challenge of limited tool availability in complex problem-solving by introducing a reference-guided framework for automatic tool creation. The core method leverages structured external materials like textbooks to generate executable tools, validate them with illustrative examples, and organize them hierarchically into a toolbox. During inference, a hierarchical selection process guides the model to choose and apply appropriate tools for solving problems. Experiments on causality, physics, and chemistry benchmarks demonstrate that RefTool outperforms existing tool-creation and domain-specific reasoning methods while maintaining cost efficiency.

## Method Summary
RefTool creates a hierarchical toolbox from structured reference materials (textbooks) by extracting their chapter/section structure, generating executable Python tools from each section with descriptions and examples, validating tools through execution testing, and refining failed tools. The hierarchical toolbox mirrors the reference structure. During inference, a two-stage selection process first identifies relevant chapters, then selects specific tools within those chapters. The selected tools are integrated into reasoning through Program-of-Thoughts (PoT) or ReAct generation methods. The approach combines the knowledge grounding of reference materials with the precision of executable code and the efficiency of hierarchical organization.

## Key Results
- 11.3% average accuracy improvement over existing tool-creation and domain-specific reasoning methods
- Hierarchical tool selection outperforms similarity-based retrieval by 2.6% on average
- Tool validation and refinement successfully recover 14% of initially failed tools
- Cost efficiency maintained with a single tool selection step (nc=1, nt=1 for causality/physics; nt=2 for chemistry)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: External references create domain knowledge-grounded tools more reliably than internal LLM knowledge.
- Mechanism: RefTool extracts structured knowledge from authoritative sources (e.g., textbooks), converts sections into executable Python functions (tools), validates these tools via example execution, and organizes them hierarchically. This bypasses hallucination risks inherent in purely internal tool generation.
- Core assumption: The reference material is accurate, relevant, and structured (e.g., LaTeX with chapters/sections).
- Evidence anchors:
  - [abstract] "RefTool... leverages structured external materials such as textbooks... generates executable tools from reference content, validate[s] them using illustrative examples."
  - [section] "LLMs generate executable tools from reference content... Failed tools trigger a refinement step... valid tools are organized hierarchically into a toolbox."
  - [corpus] "ToolLibGen: Scalable Automatic Tool Creation..." addresses scarcity of domain-specific tools, supporting the need for external tool creation frameworks, though it doesn't specifically validate RefTool's reference-based grounding.
- Break condition: If reference material is outdated, incorrect, or lacks the necessary depth/concepts for the target domain.

### Mechanism 2
- Claim: Hierarchical tool organization and selection improves retrieval accuracy and relevance compared to flat or purely similarity-based methods.
- Mechanism: RefTool mirrors the source document's hierarchy (chapters -> sections -> tools). During inference, it performs a two-stage selection: first selecting relevant chapters, then selecting specific tools within those chapters. This narrows the search space and leverages the inherent semantic grouping of the reference material.
- Core assumption: The hierarchy of the reference material reflects a meaningful and useful organization of domain concepts for problem-solving.
- Evidence anchors:
  - [abstract] "organize[s tools] hierarchically into a toolbox... hierarchical selection process guides the model to choose and apply appropriate tools."
  - [section] "hierarchical selection outperforms similarity-based retrieval by 2.6% on average... higher agreement in chapter selection... supporting our hierarchical selection step which narrows down the tool search space."
  - [corpus] "Self-Reflective Planning with Knowledge Graphs: Enhancing LLM Reasoning Reliability for Question Answering" uses structured KG for reliability, providing weak parallel support for structured guidance aiding reasoning.
- Break condition: If the problem domain is not well-structured hierarchically or if the model consistently fails at the chapter selection step.

### Mechanism 3
- Claim: Code-form tools with validation examples enhance knowledge application and error detection compared to raw textual retrieval.
- Mechanism: By converting knowledge into executable Python code (tools) with example usage, RefTool forces the model to concretize abstract concepts. The validation step (running the tool on the example) provides objective feedback on tool correctness, filtering out syntactically or semantically flawed tools before deployment.
- Core assumption: The LLM can accurately translate textual knowledge into correct, executable code and that the provided examples are sufficient for validation.
- Evidence anchors:
  - [abstract] "grounding tool creation in references produces accurate and faithful tools."
  - [section] "average 1.9% accuracy gain of tools over textual knowledge... Each tool is validated through execution testing and output verification using the model-generated demonstration example."
  - [corpus] Weak/missing. Corpus focuses on tool use/creation but not specifically on code vs. text advantages in this specific context.
- Break condition: If the LLM is unable to translate domain concepts into executable code due to lack of programming proficiency or domain complexity.

## Foundational Learning

- Concept: Program-of-Thoughts (PoT)
  - Why needed here: RefTool's tool utilization module integrates tools into reasoning using PoT (single-turn code generation). Understanding PoT is essential to grasp how RefTool orchestrates solution generation with tools.
  - Quick check question: Can you explain the difference between Chain-of-Thought (CoT) reasoning and Program-of-Thoughts (PoT) reasoning?

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: RefTool is compared against RAG baselines. RAG retrieves relevant text chunks, while RefTool retrieves/generates structured tools. Understanding RAG highlights the trade-off between direct text access and tool-mediated knowledge application.
  - Quick check question: In a standard RAG pipeline, what are the two main phases, and how does RefTool's "Hierarchical Tool Selection" modify this?

- Concept: Tool-Augmented LLMs
  - Why needed here: The entire framework is built around the concept of LLMs using external tools. This includes understanding how tools are defined (description, function, examples) and invoked by the model.
  - Quick check question: What are the three key components of a tool generated by RefTool, and how are they used?

## Architecture Onboarding

- Component map:
  1. **Tool Creation Module**:
     - Input: Structured reference material (e.g., LaTeX textbook).
     - Sub-components: Structure Extractor -> Initial Tool Generator -> Tool Validator/Refiner.
     - Output: A hierarchical `Toolbox` (JSON structure mapping chapters/sections to validated tools).
  2. **Tool Utilization Module**:
     - Input: A user `question` and the pre-built `Toolbox`.
     - Sub-components: Hierarchical Tool Selector (Chapter -> Tool) -> Solution Generator (PoT or ReAct).
     - Output: The final `answer`.

- Critical path:
  1. **Tool Creation (Offline/Pre-processing):** Converting raw textbook content into a validated, hierarchical toolbox. This is the most critical step for knowledge quality.
  2. **Hierarchical Selection (Inference):** Correctly navigating the toolbox to find the *relevant* tool. Errors here propagate to solution generation.
  3. **Solution Generation (Inference):** Writing correct code that effectively *uses* the selected tool to solve the problem.

- Design tradeoffs:
  - **Tool Granularity:** More tools per section (`m`) might increase coverage but also increase selection complexity and validation cost.
  - **Selection Depth (`nc`, `nt`):** Selecting more chapters/tools (`nc > 1`, `nt > 2`) might help with multi-faceted problems but increases noise and token usage.
  - **Code vs. Text:** Code-form tools enable validation but require the model to have coding capabilities. Text-based RAG is simpler but lacks this validation loop.

- Failure signatures:
  - **Tool Creation:** Low validation pass rate (<60%), tools with incorrect logic in comments/description, or tools too specific to the example.
  - **Tool Selection:** Consistently selecting irrelevant chapters or tools (low human-model consistency), or defaulting to no-tool reasoning for domain-specific questions.
  - **Solution Generation:** Code that imports non-existent libraries, fails to call the selected tool, or misinterprets the tool's input/output schema.

- First 3 experiments:
  1.  **End-to-End Validation:** Run the full pipeline on a small subset (e.g., 20 questions) from one domain (e.g., causality). Verify that tools are created, selected, and lead to correct answers. Check logs for refinement steps and selection choices.
  2.  **Ablation: Tool Creation Model:** Swap the LLM used for tool creation (e.g., from GPT-4o to Gemini-1.5-Pro or a smaller model) and observe the impact on toolbox quality (validation pass rate) and final accuracy.
  3.  **Ablation: Hierarchical Selection:** Compare the hierarchical selection strategy against a simple similarity-based tool retrieval baseline. Measure the accuracy difference and analyze selection consistency with human experts on a sample of questions.

## Open Questions the Paper Calls Out

- Question: Can RefTool be effectively adapted for domains where the LLM lacks the foundational knowledge required to interpret the reference materials and generate accurate tools?
  - Basis in paper: [explicit] Section E (Limitations) states that RefTool requires LLMs to possess basic domain understanding to interpret references. It suggests future work explore "integrating lightweight domain adaptation techniques" to handle unfamiliar domains.
  - Why unresolved: The current framework assumes the model can understand the textbook content enough to code it. If the model is completely "out-of-distribution" regarding the domain concepts, the tool generation fails.
  - What evidence would resolve it: Experiments applying RefTool to highly specialized or novel domains using smaller, non-expert models, supplemented by domain-adaptive pre-training or curriculum learning strategies.

- Question: How does RefTool perform when applied to unstructured knowledge sources that lack the explicit chapter/section hierarchy found in textbooks?
  - Basis in paper: [inferred] The "Structure Extraction" phase (Section 2.1) relies on converting LaTeX documents with clear hierarchical structures (`\chapter`, `\section`) to organize the toolbox.
  - Why unresolved: The hierarchical tool selection depends on this tree structure. It is unclear how the framework would extract structure or organize tools from flat documents (e.g., scientific papers, raw web text) without manual segmentation.
  - What evidence would resolve it: Evaluation of RefTool on benchmarks using unstructured reference corpora (e.g., arXiv dumps or loosely organized documentation) where structure must be inferred rather than extracted.

- Question: Does the hierarchical selection mechanism scale efficiently to massive reference libraries containing thousands of tools without succumbing to retrieval bottlenecks?
  - Basis in paper: [inferred] Section 3.1 notes the toolbox sizes were relatively small (e.g., 515 tools for Physics). The selection process involves multiple LLM calls (Chapter Selection + Tool Selection).
  - Why unresolved: The two-step LLM-based selection might introduce latency or context-window issues if the table of contents or chapter tool lists grow significantly larger than the textbook examples used.
  - What evidence would resolve it: Stress-testing the framework with a "Super-Textbook" (combining multiple books) to measure the accuracy degradation and latency increase as the tool count rises into the thousands.

## Limitations

- The approach requires accurate, structured reference materials and cannot function with unstructured or poorly organized knowledge sources
- The framework depends on the LLM having sufficient foundational domain knowledge to interpret references and generate accurate tools
- Substantial offline tool creation effort is required, with 27% of initially generated tools failing validation and requiring refinement

## Confidence

- **High confidence**: The core mechanism of using structured references to generate validated tools is well-supported by the experimental results showing 11.3% average accuracy improvement over baselines. The hierarchical selection approach is also well-validated with specific performance gains documented.
- **Medium confidence**: The generalizability of the approach across different domains, particularly chemistry where example correctness was 11% lower. The paper demonstrates effectiveness on three specific domains but doesn't establish broader applicability.
- **Low confidence**: The long-term scalability of the approach given the substantial offline tool creation effort and the 27% initial tool failure rate requiring refinement.

## Next Checks

1. **Cross-domain transferability test**: Apply the RefTool pipeline to a new domain (e.g., biology or mathematics) using a different reference structure to validate whether hierarchical organization consistently improves tool selection across varied knowledge domains.

2. **Cost-benefit analysis**: Measure the total computational cost of the full pipeline (tool creation, validation, refinement, selection) against the accuracy gains to establish when the approach is cost-effective versus simpler baselines like RAG.

3. **Robustness to reference quality**: Systematically degrade reference material quality (e.g., remove sections, introduce errors, flatten hierarchy) to determine the minimum viable reference quality needed for RefTool to outperform simpler approaches.