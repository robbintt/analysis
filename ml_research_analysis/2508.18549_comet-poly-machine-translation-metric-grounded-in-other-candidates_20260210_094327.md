---
ver: rpa2
title: 'COMET-poly: Machine Translation Metric Grounded in Other Candidates'
arxiv_id: '2508.18549'
source_url: https://arxiv.org/abs/2508.18549
tags:
- translation
- additional
- comet
- translations
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "COMET-poly addresses the gap between human and automated machine\
  \ translation evaluation by incorporating additional context\u2014either alternative\
  \ translations of the same source or retrieved examples with human quality scores\u2014\
  into the COMET quality estimation framework. It introduces two models: COMET-poly-cand,\
  \ which leverages other translations of the same source, and COMET-poly-ic, which\
  \ uses in-context examples via retrieval."
---

# COMET-poly: Machine Translation Metric Grounded in Other Candidates
## Quick Facts
- arXiv ID: 2508.18549
- Source URL: https://arxiv.org/abs/2508.18549
- Reference count: 35
- Primary result: Incorporates alternative translations and retrieved examples into COMET metric, improving segment-level correlation from 0.079 to 0.118 tau-b

## Executive Summary
COMET-poly addresses the gap between human and automated machine translation evaluation by incorporating additional context—either alternative translations of the same source or retrieved examples with human quality scores—into the COMET quality estimation framework. It introduces two models: COMET-poly-cand, which leverages other translations of the same source, and COMET-poly-ic, which uses in-context examples via retrieval. Results show that adding a single alternative translation improves segment-level metric performance from 0.079 to 0.118 Kendall's tau-b correlation, with further gains using more translations. Incorporating retrieved examples yields similar improvements (0.079 to 0.116 tau-b). COMET-poly-cand outperforms larger LLM-based baselines like GEMBA in both performance and efficiency, demonstrating that targeted, smaller models can match or exceed general-purpose systems while being more computationally efficient.

## Method Summary
COMET-poly extends the COMET quality estimation framework by integrating additional translation context. The system operates in two modes: candidate-based (COMET-poly-cand) and retrieval-based (COMET-poly-ic). In candidate-based mode, the model takes a source sentence, its reference translation, and additional alternative translations of the same source as input. In retrieval-based mode, it retrieves examples from a corpus of source-reference pairs with human quality scores and uses them as in-context examples. Both variants maintain the core COMET architecture while adding mechanisms to incorporate this additional information, with candidate-based using parallel translations and retrieval-based using indexed examples from human-annotated corpora.

## Key Results
- Segment-level correlation improves from 0.079 to 0.118 tau-b when adding a single alternative translation
- Retrieval-based approach achieves similar gains, improving from 0.079 to 0.116 tau-b
- COMET-poly-cand outperforms larger LLM-based baseline GEMBA in both performance and efficiency
- Performance scales with number of alternative translations, with diminishing returns beyond 5-10 candidates

## Why This Works (Mechanism)
The approach works by providing the metric with richer context about what constitutes good translation for a given source. Alternative translations capture the inherent ambiguity and multiple valid ways to express the same meaning, while retrieved examples provide real-world reference points of human-annotated quality. By grounding evaluation in this broader context rather than just a single reference, the metric better aligns with human judgment which naturally considers multiple valid translations. The retrieval mechanism is particularly effective because it brings in examples that have been explicitly judged by humans, providing stronger signals about quality than synthetic alternatives alone.

## Foundational Learning
- **COMET quality estimation framework**: Core architecture that scores translation quality based on source-reference pairs; needed to understand baseline performance and how COMET-poly extends it; quick check: verify understanding of how COMET differs from traditional n-gram overlap metrics
- **Kendall's tau-b correlation**: Statistical measure used to evaluate metric performance against human judgments; needed to interpret the reported improvements; quick check: confirm that tau-b handles ties appropriately for this evaluation task
- **In-context learning**: Technique where models use retrieved examples as prompts rather than fine-tuning; needed to understand COMET-poly-ic's approach; quick check: verify that retrieved examples are formatted appropriately for the model to use as examples
- **Reference-free evaluation**: Metrics that don't require reference translations; needed to understand the broader context of COMET's design; quick check: distinguish between reference-based and reference-free evaluation scenarios
- **Translationese effects**: Artifacts introduced by translation processes that can bias evaluation; needed to understand why multiple alternatives help; quick check: identify how alternative translations might capture different translation styles
- **Kendall's tau-b calculation**: Method for computing rank correlation that accounts for tied values; needed for understanding evaluation methodology; quick check: verify formula handles the specific structure of WMT human judgments

## Architecture Onboarding
**Component map**: COMET-poly-cand: Source -> COMET encoder -> Alternative translations encoder -> Fusion layer -> Quality score; COMET-poly-ic: Source -> COMET encoder -> Retriever -> Retrieved examples -> Fusion layer -> Quality score

**Critical path**: Input processing -> Context integration -> Feature fusion -> Scoring -> Correlation evaluation

**Design tradeoffs**: Uses smaller, task-specific models instead of large LLMs for efficiency; balances between adding context and maintaining computational feasibility; chooses retrieval over fine-tuning for flexibility

**Failure signatures**: Poor retrieval relevance leading to noisy examples; alternative translations that are too similar or too divergent from the reference; computational overhead from processing multiple candidates

**3 first experiments**:
1. Evaluate with varying numbers of alternative translations (1, 3, 5, 10) to identify optimal context quantity
2. Compare retrieval-based examples from different domains to assess domain sensitivity
3. Test performance on language pairs beyond English-centric directions to validate multilingual generalization

## Open Questions the Paper Calls Out
- How well do artificially generated alternative translations capture the diversity of human translation choices?
- What is the impact of retrieval quality on the effectiveness of the in-context approach?
- How does the performance of COMET-poly generalize across different domains and language pairs?

## Limitations
- Evaluation limited to English-centric translation directions, potentially limiting multilingual generalization
- Reliance on artificially generated alternative translations may not reflect realistic human translation diversity
- Performance improvements measured primarily through Kendall's tau-b correlation, which may not fully capture practical utility

## Confidence
- High confidence: Core claim that incorporating alternative translations improves metric performance is well-supported by consistent improvements across multiple experimental conditions
- Medium confidence: Efficiency claims comparing COMET-poly-cand to GEMBA lack detailed computational cost measurements
- Low confidence: Generalization of retrieval-based improvements across different domains and language pairs remains uncertain due to limited evaluation scope

## Next Checks
1. Evaluate COMET-poly across diverse language pairs beyond English-centric directions to assess multilingual robustness
2. Test the retrieval-based approach with domain-specific corpora to measure performance variation across technical, literary, and conversational translation contexts
3. Conduct ablation studies removing the COMET-QE base model to isolate the contribution of candidate-based improvements from the underlying quality estimation architecture