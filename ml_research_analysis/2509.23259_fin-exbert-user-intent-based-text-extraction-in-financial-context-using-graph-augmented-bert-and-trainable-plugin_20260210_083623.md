---
ver: rpa2
title: 'Fin-ExBERT: User Intent based Text Extraction in Financial Context using Graph-Augmented
  BERT and trainable Plugin'
arxiv_id: '2509.23259'
source_url: https://arxiv.org/abs/2509.23259
tags:
- arxiv
- financial
- extraction
- fin-exbert
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Fin-ExBERT addresses sentence-level information extraction in financial
  dialogue transcripts by integrating a GNN-augmented BERT backbone with LoRA-based
  domain adaptation and a trainable plugin head for span extraction. It uses a two-stage
  training strategy with progressive unfreezing and dynamic thresholding based on
  probability curvature to improve precision under uncertainty.
---

# Fin-ExBERT: User Intent based Text Extraction in Financial Context using Graph-Augmented BERT and trainable Plugin

## Quick Facts
- arXiv ID: 2509.23259
- Source URL: https://arxiv.org/abs/2509.23259
- Reference count: 13
- Primary result: Achieves F1=0.84 on financial call extraction task using GNN-augmented BERT with LoRA domain adaptation

## Executive Summary
Fin-ExBERT is a sentence-level information extraction model designed for financial dialogue transcripts, combining a Graph-Augmented BERT backbone with a trainable plugin head for precise span extraction. It employs a two-stage training strategy with progressive unfreezing and differential learning rates to balance adaptation and stability. Evaluated on a newly curated synthetic CreditCall12H dataset and two general QA benchmarks, it achieves strong performance with high precision and LLM judge scores. The framework is lightweight, interpretable, and tailored for real-world deployment in financial dialogue mining.

## Method Summary
Fin-ExBERT uses a GNN-augmented BERT encoder to process premise-hypothesis pairs from financial dialogues, capturing syntactic dependencies via dependency parsing and message-passing layers. LoRA adapters (rank=8, alpha=32) are added for domain adaptation to financial terminology. A two-stage training schedule first freezes the encoder while training a classifier head, then unfreezes with differential learning rates (encoder: 1e-5, head: 1e-3). The plugin head predicts start/end logits for span extraction, with dynamic thresholding based on transcript-level median deviation to improve precision. The model is trained on CreditCall12H and evaluated via LLM judges on SQuAD and FinQA-10K.

## Key Results
- F1-score of 0.84 on the CreditCall12H financial call extraction task
- LLM judge scores of 4.93/5 on SQuAD and 4.84/5 on FinQA-10K
- Ablation study confirms GNN module contributes to performance
- Dynamic thresholding improves precision in class-imbalanced scenarios

## Why This Works (Mechanism)

### Mechanism 1: Syntactic Graph Augmentation for Relational Reasoning
- Claim: GNN-based dependency graph processing captures syntactic relationships that BERT's sequential attention may miss, improving entailment detection in informal financial dialogue.
- Mechanism: spaCy generates dependency trees from premise-hypothesis pairs; two rounds of message-passing GNN layers propagate relational signals across syntactic edges. The GNN outputs for premise and hypothesis are concatenated with BERT's [CLS] token to form fused representation FR = [CLS, GNN_premise, GNN_hypothesis] (Eq. 1).
- Core assumption: Dependency parse quality on informal transcripts remains sufficient for GNN reasoning—this is not explicitly validated in the paper.
- Evidence anchors:
  - [abstract]: "Graph-Augmented BERT" highlighted as core contribution
  - [section 3.1]: "To capture syntactic dependencies missed by BERT, we augment it with Graph Neural Networks (GNNs) that operate on dependency graphs"
  - [section 4, Figure 3]: Ablation study shows performance drop when graph module removed
  - [corpus]: Weak direct evidence; related work on graph-augmented reasoning (e.g., Graph-RAG frameworks) shows similar gains in domain-specific tasks but not on financial dialogue specifically
- Break condition: If dependency parsing fails on highly informal utterances (e.g., fragmented speech, code-switching), GNN inputs degrade and may add noise rather than signal.

### Mechanism 2: Two-Stage Progressive Unfreezing with Differential Learning Rates
- Claim: Freezing the backbone during early training stabilizes the classifier head; subsequent unfreezing with lower encoder learning rates preserves pretrained knowledge while adapting to domain specifics.
- Mechanism: Stage 1 trains only the classifier head (encoder frozen) with BCEWithLogitsLoss. At epoch 4, the encoder unfreezes with lr=10⁻⁵ (encoder) vs lr=10⁻³ (head)—a 100× differential. Linear warmup over 10% of steps prevents early destabilization.
- Core assumption: The classifier head reaches reasonable local optimum before unfreezing—if head is severely misaligned, gradient signal post-unfreeze may be unreliable.
- Evidence anchors:
  - [abstract]: "initially training a classifier head while freezing the backbone, followed by gradual fine-tuning of the entire model with differential learning rates"
  - [section 3.6]: "The encoder is initially frozen and later unfrozen with differential learning rates"
  - [section 4.3, Figure 5]: Sharp improvement in all metrics after epoch 4 unfreezing validates the strategy
  - [corpus]: Progressive unfreezing is a known technique in transfer learning (e.g., ULMFiT), but specific evidence for BERT+LoRA in financial domains is sparse
- Break condition: If initial frozen-phase training is too short or learning rate too high, head may overfit to noise; subsequent encoder fine-tuning cannot recover.

### Mechanism 3: Dynamic Thresholding via Local Median Deviation
- Claim: Sentence selection based on relative deviation from transcript-level median scores outperforms fixed probability thresholds in class-imbalanced, variable-density intent scenarios.
- Mechanism: For sigmoid scores S = {s₁, ..., sₙ}, compute local median μ_S. Select sentence i if sᵢ ≥ μ_S + δ (default δ=0.15). This adapts to each transcript's score distribution rather than applying a universal cutoff.
- Core assumption: Intent-relevant sentences cluster as outliers within each transcript's score distribution—valid only if irrelevant sentences dominate numerically.
- Evidence anchors:
  - [abstract]: "dynamic thresholding strategy based on probability curvature (elbow detection), avoiding fixed cutoff heuristics"
  - [section 4.4]: "This thresholding method increases precision by prioritizing confident deviations... especially useful in class-imbalanced extractive tasks"
  - [corpus]: No direct external validation; similar adaptive thresholding appears in anomaly detection literature but not specifically for sentence extraction
- Break condition: If a transcript has many relevant sentences (high intent density), median shifts upward and true positives may fall below threshold, hurting recall.

## Foundational Learning

- **Concept: Natural Language Inference (NLI) as Pretraining Objective**
  - Why needed here: The base model is trained on SNLI to learn premise-hypothesis entailment. This transfers to financial extraction where customer utterances (hypothesis) must be matched to latent intent categories (premise).
  - Quick check question: Can you explain why "The customer is asking about credit card interest rates" is treated as a premise that must entail from "I can't find the interest charges on my last bill"?

- **Concept: LoRA (Low-Rank Adaptation) Adapters**
  - Why needed here: LoRA inserts trainable low-rank matrices (r=8, alpha=32) into attention layers, enabling domain adaptation (401k, 529 terminology) without updating full BERT parameters—critical for limited labeled financial data.
  - Quick check question: Given LoRA rank r=8 and BERT hidden dimension D=768, how many trainable parameters does a single LoRA adapter add per attention weight matrix?

- **Concept: Span Extraction via Start/End Logits**
  - Why needed here: The plugin head predicts token-level start_logits and end_logits; the optimal span maximizes P_start(i)×P_end(j) under i≤j. This enables precise localization beyond sentence-level classification.
  - Quick check question: If P_start(token 5)=0.7 and P_end(token 8)=0.6, what is the joint probability for span [5,8]? What happens if the no_span probability exceeds threshold?

## Architecture Onboarding

- **Component map:**
  Input: Premise-hypothesis pairs → BERT-base-encoder → spaCy dependency parse → 2-layer GNN → [CLS] ⊕ GNN_premise ⊕ GNN_hypothesis → LoRA-adapted attention → plugin head (MLP) → start/end logits + no_span_logit → dynamic thresholding → extracted span

- **Critical path:**
  1. BERT encoding → dependency parsing → GNN message passing → fusion
  2. Fused representation through LoRA-adapted attention during fine-tuning
  3. Plugin head inference with dynamic thresholding on sigmoid outputs

- **Design tradeoffs:**
  - GNN adds ~197K parameters (Table 2) for syntactic reasoning vs. pure BERT baseline—increased compute for improved relational capture
  - Two-stage training doubles epochs but reduces catastrophic forgetting risk
  - Dynamic thresholding improves precision at potential recall cost in high-intent-density transcripts

- **Failure signatures:**
  - If validation loss diverges post-unfreeze: encoder learning rate too high relative to head
  - If precision high but recall consistently low: δ threshold too aggressive or class imbalance undersampled
  - If spans frequently truncated or over-extended: character-length normalization penalty may need tuning

- **First 3 experiments:**
  1. **Ablate GNN**: Train identical model without graph module on CreditCall12H; compare F1 to validate Figure 3 ablation on your data
  2. **Threshold sensitivity**: Sweep δ ∈ [0.05, 0.30] on validation set; plot precision-recall curve to find optimal operating point
  3. **Unfreeze timing**: Test unfreezing at epochs 2, 4, 6; monitor for instability (loss spikes) and final F1 to confirm epoch 4 as optimal

## Open Questions the Paper Calls Out
- Can incorporating multi-hop reasoning mechanisms into the architecture improve the recall of Fin-ExBERT on indirect conversational dependencies without degrading its high precision?
- What is the correlation between the LLM-based judge evaluation scores and human expert evaluations for sentence-level extraction in financial contexts?
- Does utilizing clause-level or partial-span supervision during training yield better generalization for user intent extraction than the current sentence-level annotations?
- To what extent does the synthetic nature of the CreditCall12H dataset (generated via GPT-4o) affect the model's robustness when deployed on authentic, noisy financial dialogue?

## Limitations
- GNN architecture details (hidden sizes, aggregation function, edge handling) are underspecified
- Dynamic thresholding lacks external validation beyond the presented dataset
- Performance claims rely on proprietary or limited-access data (CreditCall12H, LLM judges)
- Two-stage training schedule assumes specific convergence behavior that may not generalize

## Confidence
- **High confidence**: Two-stage progressive unfreezing strategy; LoRA adaptation mechanism
- **Medium confidence**: Dynamic thresholding efficacy; GNN contribution
- **Low confidence**: Exact performance claims on proprietary datasets without open access; robustness to highly informal or code-switched financial dialogue

## Next Checks
1. **Ablate GNN on external financial dialogue corpus**: Remove the graph module and retrain on a publicly available financial dialogue dataset (e.g., Financial Dialogue Dataset) to verify if syntactic augmentation provides consistent gains beyond CreditCall12H.
2. **Threshold sensitivity sweep on imbalanced intent scenarios**: Systematically vary δ across [0.05, 0.30] on validation data with known intent density variations; plot precision-recall curves to identify operating points and test the claim that median-based thresholding outperforms fixed cutoffs.
3. **Unfreeze timing robustness test**: Repeat the two-stage training with unfreezing at epochs 2, 4, and 6; monitor for loss divergence and final F1 to confirm that epoch 4 is optimal and that the strategy is robust to schedule perturbations.