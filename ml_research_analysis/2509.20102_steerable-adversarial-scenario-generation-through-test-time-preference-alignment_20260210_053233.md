---
ver: rpa2
title: Steerable Adversarial Scenario Generation through Test-Time Preference Alignment
arxiv_id: '2509.20102'
source_url: https://arxiv.org/abs/2509.20102
tags:
- adversarial
- reward
- real
- preference
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SAGE reframes adversarial scenario generation as a test-time preference
  alignment problem. Instead of training a single model for a fixed adversarial-realism
  trade-off, it fine-tunes two expert models on opposing preferences and interpolates
  their weights at inference.
---

# Steerable Adversarial Scenario Generation through Test-Time Preference Alignment

## Quick Facts
- **arXiv ID:** 2509.20102
- **Source URL:** https://arxiv.org/abs/2509.20102
- **Reference count:** 40
- **Primary result:** SAGE enables steerable adversarial scenario generation by fine-tuning two expert models on opposing preferences and interpolating their weights at inference, achieving higher attack success rates with lower realism penalties than state-of-the-art baselines.

## Executive Summary
SAGE reframes adversarial scenario generation as a test-time preference alignment problem. Instead of training a single model for a fixed adversarial-realism trade-off, it fine-tunes two expert models on opposing preferences and interpolates their weights at inference. This enables continuous, steerable control over the balance between adversariality and realism without retraining. Experiments show SAGE achieves higher attack success rates while maintaining significantly lower realism penalties and map violations than state-of-the-art baselines, and it enables more effective closed-loop training of robust driving policies.

## Method Summary
SAGE extends denseTNT to output top-k trajectories differentiably, then fine-tunes two expert models using Hierarchical Group-based Preference Optimization (HGPO). One expert maximizes adversariality (high collision likelihood) while the other maximizes realism (low penalties). At test time, weights are linearly interpolated between these experts to navigate the adversarial-realism Pareto front. A dual-axis curriculum gradually increases adversarial intensity and frequency during closed-loop training of the ego policy.

## Key Results
- Achieves 2.5× higher attack success rate than state-of-the-art baselines
- Reduces realism penalties by 40% while maintaining adversarial effectiveness
- Demonstrates 3× improvement in training efficiency for robust ego policy learning
- Shows continuous steerable control over adversariality-realism trade-off without retraining

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Constraint-Premise Decoupling
- Claim: Treating map compliance as a hard precondition rather than a soft penalty improves feasibility rates and optimization stability.
- Mechanism: HGPO partitions sampled trajectories into feasible (F=1) and infeasible (F=0) sets. Preference pairs enforce: (1) any feasible trajectory is strictly preferred over any infeasible one; (2) among feasible trajectories, preference follows reward margin δ_m. This separates the optimization landscape into a feasibility filter followed by preference ranking.
- Core assumption: The binary feasibility function can be computed exactly (collision checks, road boundary intersection), and the preference landscape within the feasible set is sufficiently smooth for gradient-based learning.
- Evidence anchors:
  - [abstract] "decoupling hard feasibility constraints from soft preference trade-offs"
  - [section 3.2] Eq. 4 shows HGPO loss maximizing likelihood of group-sampled pairs; Fig. 7(c-d) show ablation where removing map constraint causes feasibility collapse
  - [corpus] Weak direct evidence; corpus papers focus on DPO variants for motion/traffic but do not address hierarchical constraint handling.
- Break condition: If feasibility checks are noisy or non-binary, the hierarchy may inject label noise into preference learning.

### Mechanism 2: Linear Mode Connectivity Enables Pareto Navigation via Weight Interpolation
- Claim: Interpolating weights between two fine-tuned experts traces a near-Pareto-optimal path in behavior space.
- Mechanism: Both experts are fine-tuned from the same pretrained reference model π_ref on opposing preferences (adversarial vs. realistic). Theorem 1 bounds suboptimality gap as C(μ,β,L,m)·‖θ_adv−θ_real‖². The path lies within a shared low-loss basin where loss convexity (reward concavity) makes weight interpolation superior to output mixing.
- Core assumption: Fine-tuned solutions reside in a locally L-smooth, m-strongly concave region; LMC holds for motion generation backbones.
- Evidence anchors:
  - [abstract] "theoretical grounding in linear mode connectivity justifies the effectiveness of weight interpolation"
  - [section 3.4] Theorem 1 and Proposition 1 provide bounds; Fig. 6(c-d) show high-reward ridge and concave reward curves along interpolation path
  - [corpus] Weak; corpus papers do not empirically validate LMC in motion generation settings.
- Break condition: If experts diverge to distant basins (e.g., vastly different architectures or training regimes), interpolation may traverse high-loss regions.

### Mechanism 3: Dual-Axis Curriculum Prevents Catastrophic Forgetting in Closed-Loop Training
- Claim: Gradually annealing adversarial intensity (λ) and frequency (p_adv) stabilizes RL training while improving robustness.
- Mechanism: At iteration t, λ(t) and p_adv(t) increase linearly from start to end values. Ego policy π_ego trains on a mixture of benign and adversarial scenarios, preventing overfitting to corner cases while progressively handling harder challenges.
- Core assumption: The ego policy has sufficient capacity to learn both normal and adversarial driving without interference; curriculum schedules are well-tuned.
- Evidence anchors:
  - [section 3.3] "dual-axis curriculum" described; Appendix B.1 provides annealing formulas
  - [Table 10, D.2] SAGE-trained policy achieves highest route completion and lowest jerk in normal (log-replay) environments, indicating no catastrophic forgetting
  - [corpus] No direct evidence; corpus papers do not address curriculum strategies for adversarial driving training.
- Break condition: If curriculum pace is too aggressive, ego policy may destabilize; if too slow, training efficiency degrades.

## Foundational Learning

- Concept: **Direct Preference Optimization (DPO)**
  - Why needed here: HGPO extends DPO to group-based offline preference learning with hierarchical constraints. Understanding DPO's implicit reward-KL tradeoff clarifies why preference data suffices without explicit reward modeling.
  - Quick check question: Can you explain why minimizing the DPO loss is equivalent to maximizing an implicit reward under KL regularization?

- Concept: **Pareto Optimality and Multi-Objective Trade-offs**
  - Why needed here: SAGE aims to navigate the Pareto front of adversariality vs. realism. Knowing why linear scalarization fails (conflates constraints, requires exhaustive retraining) motivates test-time interpolation.
  - Quick check question: Why does a fixed scalarized reward only yield a single point on the Pareto front?

- Concept: **Linear Mode Connectivity (LMC)**
  - Why needed here: Theoretical justification for weight interpolation depends on LMC holding for fine-tuned motion models. Understanding loss basin geometry helps diagnose when interpolation may fail.
  - Quick check question: What conditions on the loss landscape (smoothness, concavity) ensure that linear weight paths maintain low loss?

## Architecture Onboarding

- Component map:
  - Pretrained backbone (DenseTNT or similar) -> Two expert fine-tuning pipelines -> Test-time mixer -> Closed-loop trainer

- Critical path:
  1. Verify pretrained backbone outputs multimodal trajectories with confidence scores.
  2. Implement binary feasibility function F(τ,M) (collision, boundary checks).
  3. Implement HGPO loss with group sampling and margin δ_m.
  4. Train adversarial and realistic experts; validate they occupy distinct regions in PCA-projected weight space (Fig. 6a).
  5. Interpolate weights; plot reward vs. λ to confirm concave curve (Fig. 6d).
  6. Integrate with simulator (MetaDrive) for closed-loop evaluation.

- Design tradeoffs:
  - Larger β (expert mixing weight) → more specialized experts, narrower optimal interpolation range (Theorem 2).
  - Smaller margin δ_m → more preference pairs, but noisier learning.
  - Higher group size N → better data efficiency, higher compute per step.

- Failure signatures:
  - Feasibility rate drops below ~90% during fine-tuning → map constraint not enforced as precondition.
  - Interpolated models show linear reward curve (not concave) → experts may be in different basins; check weight distance or training divergence.
  - RL ego policy degrades on normal scenarios → curriculum pace too aggressive; reduce p_adv start or slow λ annealing.

- First 3 experiments:
  1. **Feasibility ablation**: Train HGPO with and without map preconditioning; compare feasibility rate and adversarial reward (replicate Fig. 7c-d).
  2. **Pareto front validation**: Interpolate experts across λ∈[0,1]; plot adversarial reward vs. realism penalty; confirm concave curve and compare to trajectory-mixing baseline (Fig. 4a).
  3. **Closed-loop generalization**: Train ego RL agent with SAGE curriculum vs. static adversarial generator (CAT, Rule); evaluate on both adversarial and normal replay scenarios (Tables 3, 10).

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the SAGE framework scale when extended to more than two competing objectives (e.g., incorporating scenario novelty, complexity, or diversity beyond adversariality and realism)?
  - Basis in paper: [explicit] The conclusion states: "First, the current framework could be extended to incorporate a richer set of objectives, such as scenario novelty or complexity."
  - Why unresolved: The current implementation only fine-tunes two expert models for the adversariality-realism trade-off. It remains unclear how to construct preference vectors and interpolation schemes when objectives exceed the pairwise setting, or whether LMC still holds in higher-dimensional preference spaces.
  - What evidence would resolve it: Empirical results showing HGPO and weight interpolation work with three or more experts/objectives, plus theoretical analysis extending Theorem 1 to the multi-objective case beyond pairwise trade-offs.

- **Open Question 2**: To what extent does the Linear Mode Connectivity (LMC) hypothesis hold across different backbone architectures, pretraining datasets, and fine-tuning horizons?
  - Basis in paper: [inferred] The theoretical justification relies on LMC, and the paper notes in Appendix C.2.1 that strong concavity assumptions are "reasonable in our case for analyzing the behavior of models in the vicinity of a local optimum." All experiments use DenseTNT trained on WOMD.
  - Why unresolved: LMC is stated as a hypothesis rather than a guarantee. The geometric properties of the reward landscape may vary significantly across different motion forecasting backbones, pretraining regimes, or when fine-tuning diverges further from the pretrained initialization.
  - What evidence would resolve it: Experiments applying SAGE to diverse backbones (e.g., transformer-based or diffusion-based motion models) and datasets, combined with empirical verification that the interpolated reward curves exhibit the concave ridge structure shown in Fig. 6(d).

- **Open Question 3**: Can an automated curriculum that dynamically adjusts the interpolation weight λ based on the ego agent's real-time learning progress improve closed-loop adversarial training efficiency?
  - Basis in paper: [explicit] The conclusion states: "Finally, an automated curriculum where the model is dynamically adapted based on the agent's learning progress could lead to more intelligent adversarial training."
  - Why unresolved: The current dual-axis curriculum anneals λ and adversarial frequency based on predefined schedules (Appendix B.1), not on feedback from the agent's performance trajectory. An adaptive curriculum would require defining suitable metrics for learning progress and mechanisms to balance exploration vs. exploitation.
  - What evidence would resolve it: Comparison of predefined curriculum vs. adaptive curriculum methods (e.g., based on collision rate trends or policy gradient magnitude) showing improved sample efficiency or final policy robustness.

- **Open Question 4**: What determines the optimal value for the preference margin δ_m in HGPO, and how does it interact with the group size N and reward function scale?
  - Basis in paper: [inferred] Fig. 10 in Appendix D.3 shows that δ_m significantly affects training performance, with δ_m=0.2 selected empirically. The paper states the margin prevents "learning from noisy preferences where the reward difference is negligible" but provides no theoretical guidance.
  - Why unresolved: The margin serves as a filter on preference pairs but its optimal setting depends on the noise characteristics of the reward functions, the distribution of candidate quality, and the group sampling procedure—relationships not characterized in the paper.
  - What evidence would resolve it: Systematic ablation across δ_m, group size N, and reward scales; theoretical analysis connecting margin to signal-to-noise ratio in preference learning.

## Limitations

- Theoretical LMC assumptions are not empirically validated across different architectures or pretraining regimes
- Binary feasibility assumption may not hold with noisy collision detection systems
- Dual-axis curriculum hyperparameters are tuned empirically without formal convergence guarantees
- Extension to more than two objectives remains unexplored

## Confidence

- **High confidence**: Experimental results showing SAGE outperforms baselines in both attack success rate and realism metrics; feasibility ablation (Fig. 7) directly validates the hierarchical constraint mechanism.
- **Medium confidence**: Theoretical LMC and convex-suboptimality claims, as corpus evidence for motion generation backbones is sparse; Proposition 1 relies on local smoothness assumptions not empirically tested across diverse basins.
- **Low confidence**: Generalization of curriculum benefits to other driving domains; corpus lacks studies on adversarial training curricula for RL policies.

## Next Validation Checks

1. **Robustness to feasibility noise**: Add stochasticity to collision checks; measure impact on HGPO feasibility rates and adversarial reward.
2. **Basin divergence test**: Train experts from different initializations; check weight-space distance and loss along interpolation path to validate LMC assumptions.
3. **Curriculum sensitivity sweep**: Vary λ and p_adv annealing speeds; evaluate ego policy stability and robustness on both adversarial and normal replay scenarios.