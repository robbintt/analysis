---
ver: rpa2
title: 'RePO: Replay-Enhanced Policy Optimization'
arxiv_id: '2506.09340'
source_url: https://arxiv.org/abs/2506.09340
tags:
- repo
- grpo
- policy
- samples
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Replay-Enhanced Policy Optimization (RePO),
  an extension of Group Relative Policy Optimization (GRPO) that integrates off-policy
  samples from a replay buffer into the policy optimization process. RePO addresses
  the limitations of GRPO, which relies solely on on-policy samples and can suffer
  from high computational costs and reduced data efficiency, particularly when all
  samples receive identical rewards.
---

# RePO: Replay-Enhanced Policy Optimization

## Quick Facts
- arXiv ID: 2506.09340
- Source URL: https://arxiv.org/abs/2506.09340
- Reference count: 13
- Primary result: Achieves 18.4 point absolute performance gain on Qwen2.5-Math-1.5B vs GRPO

## Executive Summary
RePO (Replay-Enhanced Policy Optimization) extends Group Relative Policy Optimization (GRPO) by integrating off-policy samples from a replay buffer into the policy optimization process. The method addresses GRPO's limitations with high computational costs and reduced data efficiency, particularly when all samples receive identical rewards. By incorporating historical samples retrieved through diverse replay strategies (recency-based, reward-oriented, and variance-driven), RePO enables policy optimization based on a broader and more diverse set of outputs per prompt.

## Method Summary
RePO combines on-policy GRPO-style updates with off-policy replay buffer updates. The method stores tuples of (prompt, output, generation probability) in a replay buffer and retrieves historical samples using strategies like recency-based or reward-oriented approaches. During training, RePO computes advantages separately for on-policy and off-policy groups (Split strategy) and performs gradient updates on a combined loss. The approach adds approximately 15% computational cost while increasing effective optimization steps by 48%.

## Key Results
- Achieves 18.4 and 4.1 absolute performance gains for Qwen2.5-Math-1.5B and Qwen3-1.7B respectively vs GRPO
- Increases computational cost by 15% while raising effective optimization steps by 48%
- Demonstrates optimal performance with 8 replay samples per prompt using Split advantage estimation

## Why This Works (Mechanism)

### Mechanism 1: Restoration of Valid Gradients via Historical Variance
When on-policy samples receive identical rewards, GRPO's normalized advantages collapse to zero, causing optimization to stall. RePO retrieves off-policy samples from the replay buffer that likely have different reward profiles, restoring gradient flow. The Split advantage strategy computes advantages separately for old and new data, ensuring valid optimization even when on-policy samples are homogeneous.

### Mechanism 2: Importance Sampling as Automatic Weighting
The off-policy loss includes the ratio π_θ / π_θ_off, which naturally down-weights stale or irrelevant historical data. If the current policy assigns low probability to an old sample, its gradient contribution is automatically reduced, preventing the model from undoing its learning to fit outdated data.

### Mechanism 3: Reduction of Overfitting via Sample Diversity
Pure on-policy methods optimize solely based on the current model's distribution, which can reinforce errors or narrow the solution space. RePO optimizes over a broader and more diverse set of samples, acting as a regularizer that prevents overfitting to immediate outputs of the current policy iteration.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**: GRPO estimates advantages by normalizing rewards across a group of outputs for the same prompt, eliminating the need for a value model. Quick check: How does GRPO calculate the advantage for a specific output token sequence?

- **On-Policy vs. Off-Policy RL**: On-policy uses data generated by the current policy, while off-policy uses historical data. Quick check: Why is it generally unsafe to train a policy on old data without corrections like importance sampling?

- **Importance Sampling Ratios**: The off-policy loss is scaled by the probability ratio π_θ / π_θ_off. Quick check: What does a low importance sampling ratio indicate about the relationship between the current policy and the behavior policy?

## Architecture Onboarding

- **Component map**: Policy Model → Replay Buffer → Advantage Estimator → Joint Objective
- **Critical path**: 
  1. Generate G_on outputs for prompt q using current π_θ
  2. Store outputs and probabilities in replay buffer B
  3. Retrieve G_off historical outputs using strategy (Recency-based/Reward-oriented)
  4. Compute A_on and A_off separately (Split strategy)
  5. Gradient step on combined loss
- **Design tradeoffs**: 
  - Split vs. Mixed Advantage: Split is strictly better, preventing interference
  - Replay Strategy: Recency-based for base models, Reward-oriented for instruct models
  - Compute vs. Efficiency: +15% wall-clock time, +48% effective steps
- **Failure signatures**: 
  - Collapse to Zero Gradient: All samples (old/new) have identical rewards
  - Buffer Drift: Older, irrelevant samples degrade policy with Full-scope replay
- **First 3 experiments**: 
  1. Baseline GRPO vs. RePO on Qwen2.5-Math-1.5B GSM8K, verify performance and effective steps
  2. Ablation Split vs. Mixed advantage estimation to confirm interference reduction
  3. Test Recency-based vs. Reward-oriented on base vs. instruct models

## Open Questions the Paper Calls Out
- Does RePO maintain performance improvements when scaling to LLMs with significantly larger parameter counts (70B+)?
- What are the optimal hyperparameter configurations for maximizing RePO's effectiveness?
- Can a "Mixed" advantage estimation strategy be optimized to outperform the "Split" strategy?

## Limitations
- Experiments limited to LLMs with up to 7B parameters, leaving scaling behavior unknown
- Hyperparameters like off-policy update weight were not thoroughly investigated due to computational constraints
- Theoretical analysis lacks formal bounds for importance sampling ratios in the LLM setting

## Confidence

| Evidence Claim | Confidence Level |
|---|---|
| Core mechanism of using replay buffers to restore gradient flow | High |
| Performance improvements across multiple benchmarks | Medium |
| Generalization of replay strategy recommendations | Low |

## Next Checks

1. **Ablation on Replay Buffer Capacity**: Systematically vary buffer size from 100 to 10,000 samples per prompt to identify optimal capacity balancing diversity against noise.

2. **Statistical Significance Testing**: Run RePO training with 5 different random seeds for Qwen2.5-Math-1.5B on GSM8K and MATH-500, reporting mean performance with 95% confidence intervals.

3. **Baseline Comparison with PPO**: Implement PPO-style off-policy update without replay (using separate value network) to determine whether gains stem from replay mechanism specifically or from having more diverse data.