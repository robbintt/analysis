---
ver: rpa2
title: How Far Do SSL Speech Models Listen for Tone? Temporal Focus of Tone Representation
  under Low-resource Transfer
arxiv_id: '2511.12285'
source_url: https://arxiv.org/abs/2511.12285
tags:
- tone
- speech
- languages
- temporal
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how self-supervised learning (SSL) speech
  models encode lexical tone in low-resource languages, focusing on Burmese, Thai,
  Lao, and Vietnamese. The authors first establish that optimal temporal spans for
  tone classification are ~100 ms for Burmese and Thai, and ~180 ms for Lao and Vietnamese,
  using logistic regression on acoustic features.
---

# How Far Do SSL Speech Models Listen for Tone? Temporal Focus of Tone Representation under Low-resource Transfer

## Quick Facts
- arXiv ID: 2511.12285
- Source URL: https://arxiv.org/abs/2511.12285
- Reference count: 0
- Key outcome: SSL models show optimal tone representation spans when fine-tuned on ASR in target language; prosody/voice tasks degrade tone encoding via overly broad temporal spans.

## Executive Summary
This study investigates how self-supervised learning (SSL) speech models encode lexical tone in low-resource languages (Burmese, Thai, Lao, Vietnamese). The authors establish that optimal temporal spans for tone classification are ~100 ms for Burmese/Thai and ~180 ms for Lao/Vietnamese using acoustic baselines. Through layer-wise probing and gradient-based sensitivity analysis, they show that ASR fine-tuning in the target language yields the best tone representation with spans closely matching language-specific baselines. In contrast, fine-tuning for prosody- or voice-related tasks produces overly broad spans and weaker tone encoding. The findings demonstrate that tone transfer in SSL models is task-dependent, with ASR fine-tuning being optimal for preserving language-specific temporal focus in tone modeling.

## Method Summary
The study combines acoustic baseline analysis with SSL model probing. First, optimal temporal spans for tone classification are established using logistic regression on log-Mel features across 20-300 ms windows for each language. Then, four SSL models (wav2vec 2.0, XLS-R, MMS, mHuBERT) are fine-tuned on various tasks (target-language ASR, cross-lingual ASR, emotion, gender, speaker verification) and analyzed via linear probes per frozen encoder layer. Input gradients are computed with respect to tone class logits to measure temporal focus, quantified as center-of-mass radius (r_com) of gradient energy distribution. The effective span is defined as 2 × r_com, and model performance is evaluated using macro-F1 for tone classification.

## Key Results
- Optimal temporal spans: ~100 ms for Burmese/Thai, ~180 ms for Lao/Vietnamese (acoustic baseline)
- ASR fine-tuning aligns model spans with language-specific baselines; other tasks produce overly broad spans
- Tone information is primarily encoded in mid-to-high encoder layers (12-24 in XLS-R)
- Prosody/voice fine-tuning tasks degrade tone transfer by biasing toward long temporal spans
- Cross-lingual ASR fine-tuning (Mandarin) provides good transfer without requiring target-language data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ASR fine-tuning aligns SSL model temporal receptive fields with language-specific tone cue spans.
- Mechanism: Gradient-based sensitivity analysis reveals that ASR-trained models develop sharper temporal focus around tone centers for languages with short-span cues (Burmese/Thai ≈100ms) and broader focus for long-span cues (Lao/Vietnamese ≈180ms). The fine-tuning objective forces the model to attend to phonemically-relevant temporal windows.
- Core assumption: The acoustic baseline (log-Mel + logistic regression) correctly estimates the true temporal span needed for tone recognition in each language.
- Evidence anchors:
  - [abstract] "automatic speech recognition fine-tuning aligns spans with language-specific tone cues"
  - [section 4.2] "Burmese and Thai show sharp concentration around tone centers... whereas Lao and Vietnamese display a bit broader spreads"
  - [corpus] Related work (arXiv:2506.03606) confirms layer-wise tone probing is effective for low-resource tonal languages
- Break condition: If acoustic baseline estimates are inaccurate, or if tone cues are speaker-dependent rather than language-specific, span alignment may not generalize.

### Mechanism 2
- Claim: Tone information is localized in mid-to-high encoder layers (12–24 in XLS-R), not lower layers.
- Mechanism: Lower layers (0–11) capture general acoustic features with weak temporal localization; higher layers abstract toward phonological categories including tone. Probe performance peaks at mid-high layers, and gradient center-of-mass radii stabilize there.
- Core assumption: Linear probes trained on frozen representations accurately reflect where tone information is encoded, not just where it's easily separable.
- Evidence anchors:
  - [section 4.3] "probe performance usually peaks in mid-to-high layers, showing that lexical tone information is mainly captured at those layers"
  - [section 4.3] "in higher layers, spans match the language-specific baselines... whereas early layers lack such concentration"
  - [corpus] arXiv:2506.03606 reports consistent layer-wise findings for Northeast Indian tonal languages
- Break condition: If downstream tasks require different abstraction levels, optimal layer may shift. Probes may detect separability rather than functional encoding.

### Mechanism 3
- Claim: Prosody- and voice-related fine-tuning tasks bias models toward overly long temporal spans, degrading tone transfer.
- Mechanism: Tasks like emotion recognition, gender classification, and speaker verification require integrating information over longer windows (sentence-level prosody, voice quality). This shifts gradient sensitivity toward broad temporal contexts, misaligning with short-span tone cues.
- Core assumption: The gradient energy distribution reflects task-induced attention patterns rather than pre-existing model biases.
- Evidence anchors:
  - [abstract] "prosody- and voice-related tasks bias the model toward overly long spans and lower performance"
  - [section 4.3] "fine-tuning for prosody- or voice-related tasks produces little to no improvement... nearly indistinguishable from the vanilla model"
  - [corpus] No direct corpus corroboration; this task-specific temporal bias finding appears novel
- Break condition: If prosody tasks are reformulated to operate on shorter windows, span alignment might improve. Cross-language transfer from tonal prosody tasks remains unexplored.

## Foundational Learning

- Concept: **Gradient-based sensitivity analysis**
  - Why needed here: The paper uses input gradients (∂z/∂x) to measure which time frames influence tone predictions, quantified as gradient energy E(t). Understanding this explains how "temporal focus" is measured.
  - Quick check question: If gradient energy is concentrated within ±50ms of a tone center, what does that imply about the model's temporal receptive field?

- Concept: **Lexical tone vs. prosody**
  - Why needed here: The paper distinguishes lexical tone (phonemic, short-span, word-level meaning distinction) from prosody (sentence-level, longer-span). Confusing these leads to misinterpreting why prosody fine-tuning harms tone transfer.
  - Quick check question: Why would emotion recognition fine-tuning be a poor choice for improving lexical tone classification?

- Concept: **Center-of-mass radius (r_com)**
  - Why needed here: This metric summarizes gradient energy distribution as a single temporal span value. Smaller r_com = sharper focus; larger r_com = broader attention.
  - Quick check question: If a model has r_com = 90ms for Burmese and r_com = 150ms for Vietnamese, which aligns better with its respective baseline?

## Architecture Onboarding

- Component map: FLEURS corpus -> G2P conversion -> CTC forced alignment -> Tone segment extraction -> SSL encoder (XLS-R 300M) -> Frozen hidden states -> Linear probes per layer -> Input gradient computation -> Gradient energy binning -> Center-of-mass radius calculation

- Critical path:
  1. Fine-tune SSL model on downstream task (ASR, emotion, etc.)
  2. Extract hidden states at tone segment centers
  3. Train linear probes per layer; compute input gradients
  4. Bin gradient energies by temporal offset; compute r_com per layer
  5. Compare r_com distributions to acoustic baseline spans

- Design tradeoffs:
  - **Target-language ASR**: Best span alignment, requires labeled data in target language
  - **Cross-lingual ASR (Mandarin)**: Second-best, transfers tonal awareness without target-language data
  - **Cross-lingual ASR (English)**: Modest improvement over vanilla, no tonal transfer
  - **Prosody/voice tasks**: No improvement; long-span bias degrades tone localization

- Failure signatures:
  - Probe macro-F1 near random → model not encoding tone (check: vanilla model, wrong layers)
  - Gradient energy flat across time → no temporal localization (check: prosody-finetuned models)
  - r_com >> baseline span → overly broad attention, likely task mismatch
  - High layer-to-layer variance in r_com → unstable tone representation

- First 3 experiments:
  1. Reproduce acoustic baseline: Train logistic regression on log-Mel with 20–300ms windows for target language; verify optimal span matches paper (~100ms or ~180ms).
  2. Layer-wise probe sweep: For XLS-R-vanilla and XLS-R-target-ASR, train probes on layers 0–24; confirm performance peaks at layers 12–24.
  3. Task comparison: Fine-tune XLS-R on target-language ASR vs. emotion recognition; compare gradient span distributions. Expect ASR to match baseline, emotion to overshoot.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the alignment between SSL model temporal spans and tone recognition generalize to non-Southeast Asian tonal languages, such as those in Africa or the Americas, which have distinct phonological profiles?
- Basis in paper: [explicit] The authors state that "existing analyses have largely focused on Mandarin" and aim to fill the gap for Southeast Asian languages, implicitly leaving other diverse tone systems unexplored.
- Why unresolved: The study is restricted to Burmese, Thai, Lao, and Vietnamese, which may share regional typological features or data constraints not present in other language families.
- What evidence would resolve it: Applying the same gradient-based probing methodology to a diverse set of under-studied tonal languages from different continents.

### Open Question 2
- Question: Can enforcing the identified optimal temporal spans (e.g., 100 ms for Burmese) as an explicit constraint during model training improve downstream performance in low-resource ASR?
- Basis in paper: [inferred] The conclusion states that the findings "provid[e] a guide for seed model selection and performance tuning," suggesting that this knowledge should be actionable for improving system performance.
- Why unresolved: The paper establishes a correlation between span alignment and probe accuracy but does not test if manually enforcing this alignment causally improves the final ASR or synthesis quality.
- What evidence would resolve it: Intervention experiments using regularization techniques to penalize attention outside the optimal acoustic spans and measuring the resulting Word Error Rate (WER) changes.

### Open Question 3
- Question: What is the underlying mechanism that causes prosody- and voice-related fine-tuning tasks (like emotion recognition) to bias SSL models toward "overly long" temporal spans?
- Basis in paper: [explicit] The authors observe that these tasks "bias the model toward overly long spans" and attribute this to "broad acoustic focus," but do not provide a detailed causal explanation.
- Why unresolved: The paper identifies the divergence in behavior between ASR and prosody tasks but does not fully explain how the optimization objectives of prosody tasks restructure the temporal receptive fields.
- What evidence would resolve it: A comparative analysis of attention head distributions or gradient flow dynamics specifically between ASR-fine-tuned and prosody-fine-tuned models.

## Limitations

- The acoustic baseline spans are derived from logistic regression on log-Mel features, which may not perfectly capture the true minimal sufficient temporal windows for tone recognition.
- Linear probes may capture feature separability rather than functional tone encoding, making it unclear whether probe performance directly reflects model behavior during downstream tasks.
- The mechanism explaining why prosody/voice fine-tuning degrades tone transfer through overly broad spans is intuitive but lacks deeper investigation into the interaction between task objectives and temporal attention patterns.

## Confidence

**Claim 1: ASR fine-tuning optimally aligns temporal spans with language-specific tone cues** - **High confidence**
- Multiple layers of evidence: consistent span alignment across all four languages, strong performance correlation, and gradient analysis supporting the mechanism
- The result is robust to the specific SSL architecture used (XLS-R, wav2vec 2.0, MMS, mHuBERT)

**Claim 2: Tone information is primarily encoded in mid-to-high encoder layers** - **High confidence**
- Clear probe performance patterns across all languages and models
- Consistent with established understanding of SSL model layer specialization
- Gradient analysis provides complementary evidence

**Claim 3: Prosody/voice fine-tuning degrades tone transfer through overly broad spans** - **Medium confidence**
- The empirical observation is clear, but the mechanism explanation is less developed
- Task-specific effects on temporal attention are demonstrated but not deeply characterized
- Cross-task transfer effects remain underexplored

## Next Checks

1. **Baseline span robustness test:** Re-run the acoustic baseline analysis using alternative feature representations (MFCC, filter banks) and windowing strategies to verify that the ~100ms and ~180ms optimal spans are stable across different acoustic frontends.

2. **Probe vs. fine-tuned model comparison:** For XLS-R models fine-tuned on ASR vs. emotion recognition, directly compare tone classification performance when using (a) linear probes on frozen representations versus (b) fine-tuned model predictions. This would validate whether probe performance accurately reflects functional tone encoding.

3. **Task formulation sensitivity analysis:** Investigate whether prosody tasks formulated with shorter temporal windows (e.g., emotion detection within 200ms windows rather than sentence-level) preserve better tone span alignment, helping to isolate whether the issue is task type or temporal scope.