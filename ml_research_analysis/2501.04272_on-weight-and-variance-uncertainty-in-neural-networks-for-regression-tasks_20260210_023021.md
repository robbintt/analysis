---
ver: rpa2
title: On weight and variance uncertainty in neural networks for regression tasks
arxiv_id: '2501.04272'
source_url: https://arxiv.org/abs/2501.04272
tags:
- variance
- uncertainty
- posterior
- regression
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates variance uncertainty in Bayesian neural
  networks for regression tasks. The authors extend Blundell et al.'s (2015) Bayes
  by Backprop framework by introducing a variational posterior distribution over the
  variance parameter, rather than treating it as fixed.
---

# On weight and variance uncertainty in neural networks for regression tasks

## Quick Facts
- arXiv ID: 2501.04272
- Source URL: https://arxiv.org/abs/2501.04272
- Reference count: 20
- Primary result: VBNET-SVAR improves prediction accuracy and uncertainty quantification by modeling variance uncertainty alongside weight uncertainty in Bayesian neural networks

## Executive Summary
This paper introduces a Bayesian neural network framework that simultaneously learns uncertainty in both the weights and the observation noise variance for regression tasks. The authors extend the Bayes by Backprop framework by placing a variational posterior distribution over the variance parameter, rather than treating it as fixed. The proposed method, VBNET-SVAR, is evaluated on both synthetic and real-world datasets, demonstrating improved mean squared prediction error and better coverage probabilities compared to fixed-variance Bayesian neural networks and frequentist approaches.

## Method Summary
The paper extends the Bayes by Backprop framework by introducing a variational posterior distribution over the variance parameter in Bayesian neural networks for regression. Instead of treating the observation noise as fixed, the authors model it as a random variable S with a log-normal distribution. This allows the network to learn both weight uncertainty and variance uncertainty simultaneously through stochastic optimization using the reparametrization trick. The approach is implemented as VBNET-SVAR and tested with both Gaussian and spike-and-slab priors for the weights.

## Key Results
- VBNET-SVAR outperforms fixed-variance Bayes by Backprop in mean squared prediction error and coverage probabilities
- The method demonstrates improved generalization and uncertainty quantification on a nonlinear function approximation problem
- Results on a high-dimensional riboflavin genetic dataset show superior performance compared to frequentist neural networks

## Why This Works (Mechanism)
The paper introduces variance uncertainty modeling to complement weight uncertainty in Bayesian neural networks. By treating the observation noise as a random variable with its own posterior distribution rather than a fixed parameter, the network can adapt its uncertainty estimates based on the data. This dual uncertainty modeling allows for more accurate uncertainty quantification and potentially better generalization, especially in scenarios where the noise characteristics are not known a priori.

## Foundational Learning

### Bayesian Neural Networks
- Why needed: Provides the probabilistic framework for uncertainty quantification in neural networks
- Quick check: Understand how priors and posteriors are defined over network weights

### Variational Inference
- Why needed: Enables tractable approximation of intractable posterior distributions in BNNs
- Quick check: Know how the evidence lower bound (ELBO) is derived and optimized

### Reparametrization Trick
- Why needed: Allows gradient-based optimization through stochastic nodes
- Quick check: Understand how sampling from a distribution can be expressed as a deterministic function of parameters and noise

## Architecture Onboarding

### Component Map
Data -> Neural Network (weights W) -> Mean Prediction ŷ -> Variance Parameter S -> Output Distribution y ~ N(ŷ, log(1+exp(S)))

### Critical Path
1. Input features are processed through the neural network to produce mean predictions
2. Variance parameter S is sampled from its variational posterior
3. Output distribution is constructed using both ŷ and S
4. ELBO is computed and optimized

### Design Tradeoffs
- Diagonal Gaussian assumption for variational posteriors: Improves computational efficiency but may underestimate correlations
- Global variance parameter vs. input-dependent variance: Simpler but cannot capture heteroscedasticity
- Spike-and-slab vs. Gaussian priors: Tradeoff between sparsity and smoothness

### Failure Signatures
- Poor coverage probabilities indicate miscalibration of uncertainty estimates
- High variance in S parameter may indicate instability in variance learning
- Performance degradation on heteroscedastic data reveals limitations of global variance assumption

### First Experiments
1. Replicate synthetic function approximation results to verify basic implementation
2. Compare coverage probabilities on a simple dataset with known noise characteristics
3. Test sensitivity to prior choices (Gaussian vs. spike-and-slab) on a small dataset

## Open Questions the Paper Calls Out

### Open Question 1
Can the variance uncertainty framework be extended to model heteroscedastic noise, where the variance is a function of the input data rather than a single global parameter?

### Open Question 2
Does the accuracy of the learned variance posterior degrade when using diagonal Gaussian approximations compared to full-covariance variational distributions?

### Open Question 3
Does the inclusion of variance uncertainty improve calibration and predictive performance in deep architectures such as Convolutional Neural Networks (CNNs)?

## Limitations
- The paper uses diagonal Gaussian approximations for variational posteriors, which may underestimate posterior correlations
- Experiments are limited to fully connected networks and dropout NNs on tabular data
- The method assumes global variance rather than input-dependent heteroscedastic noise

## Confidence
The major claims about improved prediction accuracy and uncertainty quantification are supported by experiments on both synthetic and real-world datasets, but the confidence is Medium due to limited implementation details and experimental setup information.

## Next Checks
1. Reproduce the results on the riboflavin dataset with publicly available code to verify the reported improvements in mean squared prediction error and coverage probabilities
2. Conduct ablation studies to isolate the impact of variance uncertainty modeling from other aspects of the experimental setup
3. Test the method on additional regression datasets with different characteristics (e.g., varying levels of noise, different dimensionalities) to assess the generalizability of the findings