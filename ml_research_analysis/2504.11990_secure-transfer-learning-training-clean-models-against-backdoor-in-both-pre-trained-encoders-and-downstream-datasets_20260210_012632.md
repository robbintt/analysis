---
ver: rpa2
title: 'Secure Transfer Learning: Training Clean Models Against Backdoor in (Both)
  Pre-trained Encoders and Downstream Datasets'
arxiv_id: '2504.11990'
source_url: https://arxiv.org/abs/2504.11990
tags:
- clean
- backdoor
- dataset
- samples
- encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses backdoor attacks in transfer learning where
  pre-trained encoders and/or downstream datasets may be poisoned. The key challenge
  is defending against unknown backdoor threats with limited computational resources,
  as existing defenses fail due to their reactive nature and assumptions that don't
  scale across different training paradigms.
---

# Secure Transfer Learning: Training Clean Models Against Backdoor in (Both) Pre-trained Encoders and Downstream Datasets

## Quick Facts
- arXiv ID: 2504.11990
- Source URL: https://arxiv.org/abs/2504.11990
- Reference count: 40
- Primary result: T-Core achieves attack success rates below 10% while maintaining high accuracy against unknown backdoor threats in transfer learning

## Executive Summary
This paper addresses the critical challenge of defending transfer learning against backdoor attacks where either the pre-trained encoder, the downstream dataset, or both may be poisoned. The key innovation is a proactive Trusted Core (T-Core) Bootstrapping framework that identifies clean data and channels without requiring knowledge of the specific attack vector. Unlike reactive defenses that search for known triggers, T-Core builds a trusted subset through topological invariance analysis and loss-guided expansion, then filters encoder channels to isolate backdoor functionality. The framework is specifically designed for resource-constrained scenarios where fine-tuning the entire encoder is infeasible.

## Method Summary
T-Core Bootstrapping operates in three stages: First, Topological Invariance Sifting (TIS) analyzes intermediate layer activations to identify high-credibility seed data by selecting samples with consistent nearest neighbors and largest cluster membership across layers. Second, Selective Channel Recovery unlearns normalization parameters on untrusted data while recovering performance on the clean subset using soft channel masks to isolate trusted channels. Third, the Bootstrapper trains the head and untrusted channels using the clean subset, progressively expanding the trusted data pool through meta-guidance. The method assumes frozen encoders and only trains the head, making it efficient for practical deployment.

## Key Results
- T-Core achieves attack success rates below 10% across 5 encoder poisoning attacks, 7 dataset poisoning attacks, and 14 baseline defenses
- The framework maintains high clean accuracy while defending against unknown backdoor threats
- T-Core demonstrates effectiveness against adaptive attacks, transformer architectures, and resource-constrained settings
- Outperforms state-of-the-art defenses like NLPR, CLIP, and Unsupervised Activation Norm across all benchmark datasets

## Why This Works (Mechanism)

### Mechanism 1: Topological Invariance Sifting (TIS)
If a sample maintains consistent nearest neighbors and resides within the largest cluster (majority group) across multiple network layers, it is highly likely to be clean, bypassing the need for latent separability in the final layer. TIS applies Majority Rule using density-based clustering to filter out small clusters and Consistency Rule to select samples with stable neighbor sets. Evidence shows 100% precision in 212/225 scenarios. Fails if poison ratio is so high that poisoned samples form the majority cluster.

### Mechanism 2: Selective Channel Recovery
By unlearning normalization parameters on untrusted data and recovering performance using only the clean subset, one can isolate channels critical for the clean task versus those facilitating the backdoor. The method unlearns on full dataset then recovers on clean subset using soft masks. Channels with high mask values are trusted; others are re-initialized. Evidence shows this outperforms SSL-Distillation on noisy datasets like SVHN.

### Mechanism 3: Loss-Guided Expansion with Confusion
Standard loss metrics cannot distinguish poison from clean data in transfer learning, but Confusion Training forces clean samples into the high-loss region, enabling safe expansion. The method uses small clean seed sets to "confuse" the model, then selects samples with highest loss for expansion. Visual proof shows near-zero poison samples in the largest loss region even at high expansion ratios.

## Foundational Learning

- **Frozen Encoder Transfer Learning**: Defense assumes user lacks compute to fine-tune entire network, limiting optimization space. Quick check: If you freeze encoder and only train head, does euclidean distance in feature space change for poisoned samples? (Answer: No, only linear boundary moves).

- **Proactive vs. Reactive Defense**: Proactive builds trusted core without knowing attack vector, unlike reactive defenses searching for known triggers. Quick check: Does proactive defense need to know trigger pattern T(x)? (Answer: No, only needs clean statistics).

- **Lipschitz Continuity & Channel Pruning**: Understanding Lipschitz constants measures sensitivity to input perturbations explains why CLP fails (backdoor channels in TL don't necessarily have higher Lipschitz bounds). Quick check: Why might high Lipschitz constant channel still be safe? (Answer: It might react to complex clean features, not necessarily a trigger).

## Architecture Onboarding

- **Component map**: TIS (Sifter) -> DBSCAN on intermediate activations -> Seed Data (S); Seed Expansion -> Confusion Training on S vs D -> Clean Subset (D_sub); Channel Filter -> Unlearn/Recover loop using D_sub -> Trusted Channels (χ) and Untrusted Channels (ψ); Bootstrapper -> Trains head + ψ on D_sub, progressively expanding D_sub

- **Critical path**: Seed Selection (S). If initial seed contains poisoned samples, Confusion Training will misidentify poison as clean, corrupting Channel Filter and final model.

- **Design tradeoffs**: Sifting Ratio (α): Lower is safer but requires more expansion steps (paper uses 1%). Unlearning Threshold (ACC_min): Must drop accuracy sufficiently to force channel reliance during recovery but not destroy all structure.

- **Failure signatures**: High ASR with Low ACC indicates channel filter failed or expansion absorbed poisoned samples. SVHN Performance Drop expected due to inherent noise affecting topology.

- **First 3 experiments**: 1) Visualize t-SNE of penultimate layer vs intermediate layers on CIFAR-10 to verify poisoned samples cluster separately in intermediate layers. 2) Plot Trigger-Activated Change (TAC) vs Recovered Mask Value to verify low-mask channels correlate with backdoor behavior. 3) Run Seed Expansion on known poisoned set (20% poison) and measure cumulative poison count at 10%, 20%, 50% expansion ratios.

## Open Questions the Paper Calls Out

- **Open Question 1**: How can the trade-off between security and utility be optimized in low-data regimes where initial sifting of clean seed data is difficult? The framework shows accuracy drops on smaller datasets like STL-10 (5,000 samples), indicating bootstrap mechanism struggles when clean element pool is restricted.

- **Open Question 2**: Can T-Core be effectively extended to language-domain backdoors or multimodal zero-shot settings (e.g., CLIP-like models)? Current methodology relies on visual topological invariance and channel filtering specific to vision architectures; effectiveness on text embeddings or aligned multimodal latent spaces is unclear.

- **Open Question 3**: How robust is the framework when "Majority Rule" assumption is violated, such as in extreme poisoning where poisoned samples constitute majority of a class? The "Topological Invariance Sifting" module selects largest cluster as clean seed; if attacker injects majority of poisoned samples, this heuristic would inadvertently select poisoned data as trusted core.

## Limitations
- Method assumes poison samples form minority clusters with inconsistent topological behavior, which may not hold for advanced adaptive attacks that mimic clean topology
- Critical dependency on initial seed selection (1% of data) creates single point of failure—if seeds contain poison, entire pipeline can be compromised
- Effectiveness on extremely noisy datasets like SVHN is limited due to inherent data instability affecting topological consistency

## Confidence

- **High Confidence**: Topological Invariance Sifting achieving high precision in clean data selection (0 false positives in 212/225 scenarios)
- **Medium Confidence**: Framework's effectiveness against adaptive attacks and transformer architectures
- **Low Confidence**: Claim that T-Core requires no prior knowledge while maintaining sub-10% ASR across all threat scenarios

## Next Checks

1. **Adaptive Attack Resilience**: Test T-Core against advanced adaptive attacks that specifically target topological invariance assumption by creating poison samples with consistent clustering behavior across network layers. Measure whether ASR exceeds 10% under these conditions.

2. **Seed Contamination Sensitivity Analysis**: Systematically vary initial seed purity (100% clean, then 95%, 90%, 85% clean) and measure how ASR degrades across pipeline to quantify critical dependency on clean seed selection.

3. **Cross-Domain Transferability**: Evaluate T-Core when transferring from one domain to another (e.g., pre-trained on CIFAR-10, fine-tuning on medical imaging) to assess whether topological invariance holds across domain shifts and whether channel filtering remains effective.