---
ver: rpa2
title: Grounding Chest X-Ray Visual Question Answering with Generated Radiology Reports
arxiv_id: '2505.16624'
source_url: https://arxiv.org/abs/2505.16624
tags:
- report
- radiology
- questions
- question
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a novel approach to Chest X-ray Visual Question
  Answering (VQA), addressing both single-image and image-difference questions. The
  method integrates predicted radiology reports as additional input to improve the
  accuracy of the VQA model.
---

# Grounding Chest X-Ray Visual Question Answering with Generated Radiology Reports

## Quick Facts
- arXiv ID: 2505.16624
- Source URL: https://arxiv.org/abs/2505.16624
- Reference count: 40
- Primary result: State-of-the-art CXR VQA performance on Medical-Diff-VQA with report grounding

## Executive Summary
This work presents a novel approach to Chest X-ray Visual Question Answering (VQA) that integrates predicted radiology reports as additional input to improve accuracy. The method handles both single-image and image-difference questions through a unified two-stage pipeline: a Report Generator creates findings and impression sections from the CXR, and an Answer Generator uses this report as context to produce answers. The approach achieves state-of-the-art results on the Medical-Diff-VQA dataset, with the most significant improvements observed on image-difference questions and single-image open-ended questions.

## Method Summary
The method uses a two-stage pipeline where a Report Generator (RG) first produces a radiology report (findings + impression sections) from the CXR using anatomical tokens extracted by Faster R-CNN, then an Answer Generator (AG) takes this report as additional context to answer questions. The RG and AG share a 68M parameter VLM architecture (3-layer Transformer encoder-decoder) and are trained separately but with AG initialized from RG weights. For image-difference questions, a Longitudinal Projection Module concatenates current and prior scan anatomical tokens before report generation. The model generates answers autoregressively and is evaluated on Medical-Diff-VQA with NLG metrics (BLEU, METEOR, ROUGE-L, CIDEr) for difference questions and exact-match accuracy for open-ended/closed-ended questions.

## Key Results
- State-of-the-art BLEU-4 score of 0.551 on Medical-Diff-VQA image-difference questions
- Most significant improvements on image-difference questions (+2.6 BLEU-4 over previous best) and single-image open-ended questions
- Ablation shows combining findings and impression sections yields best performance
- Ground-truth reports improve accuracy from 0.693 to 0.751, demonstrating report grounding benefits despite error propagation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Providing a predicted radiology report as additional context improves VQA answer accuracy, particularly for open-ended and image-difference questions.
- Mechanism: The Report Generator produces a structured description of the CXR (findings + impression sections), which is then concatenated with the question and fed to the Answer Generator. The report acts as an intermediate reasoning step—similar to chain-of-thought—surfacing relevant clinical details before the final answer is generated.
- Core assumption: The predicted report contains information useful for answering the question, and errors in the report do not systematically mislead the answer generator.
- Evidence anchors:
  - [abstract] "Taking inspiration from 'Chain-of-Thought reasoning', we demonstrate that performance on the CXR VQA task can be improved by grounding the answer generator module with a radiology report predicted for the same CXR."
  - [Section V-B, Table IV] Ablation shows F+I (findings + impression) yields best results; removing visual input drops BLEU-4 from 0.551 to 0.479.
  - [corpus] Related work on multi-agent reasoning (arXiv:2508.02841) supports staged reasoning for radiology VQA, though direct comparison is limited.
- Break condition: If report quality degrades (e.g., high hallucination rate), error propagation may negate benefits. Paper notes: "predicted reports provided to the AG model might contain errors which can lead to the generation of wrong answers" (Section VI).

### Mechanism 2
- Claim: Jointly representing current and prior CXR scans enables better temporal comparison for image-difference questions.
- Mechanism: The Longitudinal Projection Module (LPM) concatenates anatomical token pairs (current, prior) for each of N=36 regions, passes them through an MLP with residual connection, producing a joint representation V_joint. This allows the model to reason about changes rather than just absolute findings.
- Core assumption: Anatomical regions can be consistently detected across timepoints, and the residual MLP can learn meaningful delta representations.
- Evidence anchors:
  - [Section III-B-1] Equation 2: "v_joint,n = MLP([v_c,n, v_p,n]) + [v_c,n, v_p,n]"
  - [Section V, Table II] Diff-VQA results show RG-AG achieves 0.551 BLEU-4 vs 0.525 for AG without report grounding.
  - [corpus] "Saliency Guided Longitudinal Medical VQA" (arXiv:2509.25374) similarly emphasizes temporal consistency, though methods differ.
- Break condition: If prior scan is unavailable or anatomical detection fails for a region, V_p is set to zero vectors, reducing temporal reasoning capacity.

### Mechanism 3
- Claim: Pre-training the Answer Generator on report generation before VQA fine-tuning improves performance.
- Mechanism: AG weights are initialized from the trained RG model. This transfers visual-language alignment learned during report generation to the VQA task.
- Core assumption: The report generation task teaches transferable representations for answering questions about the same images.
- Evidence anchors:
  - [Section III-D] "Following [4], we initialise the AG model using the RG weights, i.e. the RG task effectively acts as pre-training."
  - [Section V] AG (w/o report) still outperforms PLURAL baseline, suggesting initialization itself provides benefit.
  - [corpus] Limited corpus evidence on this specific transfer mechanism; related work PLURAL [4] uses similar pre-training but without report-as-input.
- Break condition: If the RG task diverges significantly from VQA requirements (e.g., different vocabulary, longer outputs), transfer may be suboptimal.

## Foundational Learning

- Concept: **Chain-of-Thought Reasoning**
  - Why needed here: The RG-AG pipeline explicitly generates intermediate reasoning (the report) before the final answer, mirroring CoT approaches in LLMs.
  - Quick check question: Can you explain why generating a radiology report before answering might improve accuracy compared to direct answer generation?

- Concept: **Vision-Language Token Alignment**
  - Why needed here: The model must map anatomical visual tokens (from Faster R-CNN) to language model embeddings for coherent text generation.
  - Quick check question: How does the Longitudinal Projection Module bridge visual anatomical tokens and the text encoder?

- Concept: **Autoregressive Text Generation**
  - Why needed here: Both RG and AG use a Transformer encoder-decoder to generate text (reports or answers) token-by-token.
  - Quick check question: What loss function is used to train the autoregressive model, and how is the best model selected?

## Architecture Onboarding

- Component map:
  1. **Visual Anatomical Token Extractor** (Faster R-CNN, pre-trained separately) → N=36 anatomical tokens per image (d=1024)
  2. **Longitudinal Projection Module** → MLP with residual, outputs V_joint
  3. **Language Model** → 3-layer encoder-decoder Transformer (8 heads, 512 hidden)
  4. **Report Generator** → Takes V_joint + indication + instruction → generates findings OR impression
  5. **Answer Generator** → Takes V_joint + question + predicted report → generates answer

- Critical path: CXR image → Token Extractor → LPM → [RG generates report] → [AG takes report + question] → Answer
  - For single-image: V_p = zero vectors
  - For diff-VQA: V_p = prior scan tokens

- Design tradeoffs:
  - Two-stage RG-AG vs end-to-end: Modular but risks error propagation
  - Separate findings/impression generation: Allows task-specific prompts but may cause inconsistencies (noted in limitations)
  - Small model (68M params): Efficient but may limit expressiveness; paper suggests larger models might unify stages

- Failure signatures:
  - Error propagation: Incorrect report leads to wrong answer (Figure 5, row 3)
  - Inconsistency: Findings and impression sections contradict, confusing AG (Figure 5, rows 1-2, 4)
  - Uninformative report: If RG fails to capture relevant details, AG has no useful grounding (Figure 5, row 5)

- First 3 experiments:
  1. **Reproduce baseline comparison**: Train AG without report grounding, evaluate on Medical-Diff-VQA test set using BLEU/meteor/accuracy metrics. Compare to reported AG scores (BLEU-4: 0.525 for diff questions).
  2. **Ablate report components**: Test AG with findings-only, impression-only, and both sections. Verify findings contribute more than impression (Table IV pattern).
  3. **Error propagation analysis**: Manually inspect cases where RG produces incorrect reports; measure correlation between report error rate and answer accuracy drop. This validates the break condition for Mechanism 1.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can larger, more capable Vision-Language Models (VLMs) effectively perform report grounding and answer generation in a single unified step, removing the need for a separate two-stage pipeline?
- Basis in paper: [explicit] The conclusion states the authors hypothesise that the two-stage approach "may only be necessary for smaller models," leaving the capability of larger models to perform all tasks in one step as an open direction.
- Why unresolved: The current study utilized a specific 68M parameter architecture requiring distinct Report Generator and Answer Generator modules.
- What evidence would resolve it: Experiments demonstrating if large VLMs can achieve comparable or superior VQA accuracy using single-stage multimodal Chain-of-Thought prompting without explicit intermediate report generation.

### Open Question 2
- Question: To what extent does integrating other types of clinical patient information (beyond radiology reports) improve the answer generation capability of the VQA model?
- Basis in paper: [explicit] The conclusion explicitly identifies the use of "other types of clinical information related to a patient" provided as evidence as a direction for future work.
- Why unresolved: The current study focused exclusively on grounding the model with predicted radiology reports (Findings and Impression sections).
- What evidence would resolve it: Ablation studies incorporating electronic health record data or lab results alongside images to measure performance improvements on specific question types.

### Open Question 3
- Question: Does the RG-AC pipeline maintain its performance when applied to more nuanced, free-form clinical questions that fall outside the 18 unique templates used in the Medical-Diff-VQA dataset?
- Basis in paper: [explicit] The limitations section notes that the dataset questions are limited in scope to templates, suggesting "additional evaluation on more varied questions" is necessary.
- Why unresolved: The model was validated on semi-automatically derived QA pairs which may not capture the complexity of real clinical queries (e.g., assessing "rate of change").
- What evidence would resolve it: Evaluation on a dataset containing open-ended, clinician-generated questions requiring complex reasoning rather than template-based information extraction.

## Limitations
- Error propagation from report generation to answer generation creates performance gap between ground-truth (0.751 accuracy) and predicted reports (0.693 accuracy)
- Findings and impression sections are generated independently, potentially leading to internal inconsistencies that confuse the answer generator
- The 68M parameter architecture may limit expressiveness compared to larger models that could potentially unify the two-stage pipeline

## Confidence
- **High Confidence**: The mechanism of report grounding improving VQA performance is well-supported by ablation studies and quantitative metrics across multiple evaluation measures.
- **Medium Confidence**: The claim about pre-training benefits through weight initialization from RG to AG is supported by reported results but lacks direct comparison to alternative initialization strategies.
- **Medium Confidence**: The architectural improvements for image-difference questions through the Longitudinal Projection Module are demonstrated, though the paper doesn't fully explore alternative temporal reasoning approaches.

## Next Checks
1. **Error Propagation Analysis**: Systematically measure the correlation between report generation errors and subsequent answer generation errors across different question types. This would validate whether the error propagation concern significantly impacts real-world performance.

2. **End-to-End Training Comparison**: Implement and evaluate an end-to-end trained variant that jointly optimizes both report generation and answer generation, comparing against the current two-stage approach to assess whether error propagation can be mitigated.

3. **Consistency Evaluation**: Develop automated metrics to quantify inconsistencies between findings and impression sections in generated reports, and measure their correlation with answer quality degradation.