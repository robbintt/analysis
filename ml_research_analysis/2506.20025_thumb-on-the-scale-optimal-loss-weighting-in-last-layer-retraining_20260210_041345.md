---
ver: rpa2
title: 'Thumb on the Scale: Optimal Loss Weighting in Last Layer Retraining'
arxiv_id: '2506.20025'
source_url: https://arxiv.org/abs/2506.20025
tags:
- weighting
- class
- loss
- error
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper analyzes optimal loss weighting in last layer retraining\
  \ for imbalanced binary classification. The authors develop a theoretical framework\
  \ showing that loss weighting effectiveness depends on the overparameterization\
  \ ratio \u03B4=d/n, where d is latent dimension and n is sample size."
---

# Thumb on the Scale: Optimal Loss Weighting in Last Layer Retraining

## Quick Facts
- **arXiv ID:** 2506.20025
- **Source URL:** https://arxiv.org/abs/2506.20025
- **Reference count:** 40
- **Primary result:** Optimal class weighting for last-layer retraining requires overparameterization-dependent offset beyond prior ratio

## Executive Summary
This paper analyzes optimal loss weighting in last layer retraining for imbalanced binary classification. The authors develop a theoretical framework showing that loss weighting effectiveness depends on the overparameterization ratio δ=d/n, where d is latent dimension and n is sample size. For square loss in the underparameterized regime (δ<1), they derive a closed-form optimal weighting scheme ρ̃ that balances worst-class error, which outperforms the classical ratio of priors except when class separation is large. Experiments on CelebA and CIFAR10 datasets confirm that using the effective latent dimension (instead of full dimension) to compute ρ̃ predicts the empirically optimal weighting, achieving better balanced accuracy than using the ratio of priors.

## Method Summary
The authors analyze weighted empirical risk minimization (wERM) for last-layer retraining (LLR) with square loss in the underparameterized regime. They assume class-conditional Gaussian latent features and derive a closed-form optimal weighting ρ̃ that minimizes worst-class error. The method computes effective latent dimension d_eff via PCA (99% variance threshold) and uses this to calculate δ_eff = d_eff/n. The optimal weight ρ̃ = π₋/π₊ + (π₋/π₊ - 1)·δ/(2π₊ - δ) is then applied during retraining of the final linear layer. Experiments compare this approach against classical prior-ratio weighting and downsampling baselines.

## Key Results
- Optimal weight ρ̃ = π₋/π₊ + (π₋/π₊ - 1)·δ/(2π₊ - δ) outperforms classical prior ratio except for large signal strength
- Effective latent dimension (d_eff) computed via PCA provides better δ estimate than raw model dimension
- wERM with ρ̃ achieves lower worst-class error than downsampling across multiple datasets
- Theoretical framework breaks down when δ ≥ 2π₊ as per-class errors never intersect

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimal class weighting for worst-class error (WCE) in underparameterized LLR requires an overparameterization-dependent offset beyond the classical prior ratio.
- Mechanism: The optimal weight ρ̃ = π₋/π₊ + (π₋/π₊ - 1)·δ/(2π₊ - δ) balances per-class risks. As δ → 0, this converges to the prior ratio; as δ increases toward 2π₊, the offset grows, requiring stronger minority upweighting to compensate for finite-sample bias.
- Core assumption: Class-conditional Gaussian latent features; square loss; δ < 2π₊ (otherwise per-class errors never intersect).
- Evidence anchors:
  - [abstract]: "optimal weight is not simply the prior ratio but includes an overparameterization-dependent offset"
  - [Theorem 2, Equation 10]: Closed-form ρ̃ formula with δ-dependent correction term
  - [corpus]: "Closed-Form Last Layer Optimization" discusses leveraging closed-form last-layer solutions under squared loss

### Mechanism 2
- Claim: Effective latent dimension—not raw model dimension—should determine δ for computing optimal weights.
- Mechanism: Deep model latent spaces are sparse; most dimensions capture negligible variance. Using PCA to identify dimensions capturing ~99% variance yields an "effective" d_eff ≪ d_architecture. Computing δ_eff = d_eff/n produces weights that better match empirical optima.
- Core assumption: Irrelevant dimensions don't contribute to classification signal; variance-based cutoff approximates task-relevant dimensionality.
- Evidence anchors:
  - [Section 4]: "we perform PCA to quantify the number of 'effective' dimensions... This is 3 dimensions for CelebA and 2 for CIFAR10"
  - [Figure 9]: PCA spectra showing most variance captured by few features
  - [corpus]: No direct corpus support for effective dimension; related work on LLR assumes fixed latent dimension

### Mechanism 3
- Claim: Optimal wERM with ρ̃ outperforms downsampling because downsampling artificially increases δ, worsening generalization.
- Mechanism: Downsampling majority class to minority size changes effective δ̃ = δ/(2π₊), pushing it closer to or beyond the separability threshold. Higher δ increases per-class error for both classes even when balanced.
- Core assumption: Per-class risks are monotonic in ρ (R⁺ decreasing, R⁻ increasing).
- Evidence anchors:
  - [Corollary 3]: "The solution to the downsampled problem is given by taking δ̃ ≜ δ/(2π₊)"
  - [Figure 5]: wERM achieves lower balanced error than downsampling across δ values
  - [Theorem 3]: Proves ρ̃ minimizes WCE under monotonicity assumption
  - [corpus]: Weak—no direct comparison of weighting vs. downsampling in related papers

## Foundational Learning

- Concept: **Convex Gaussian Minimax Theorem (CGMT)**
  - Why needed here: Core theoretical tool reducing high-dimensional optimization to scalar equations; enables closed-form analysis of LLR solution properties.
  - Quick check question: Can you explain how CGMT relates a primary optimization with Gaussian matrix to an auxiliary scalar problem?

- Concept: **Moreau Envelope & Proximal Operators**
  - Why needed here: Theorem 1 expresses LLR solution via Moreau envelope expectations; square loss admits closed-form envelope enabling analytical solutions.
  - Quick check question: For a convex loss ℓ, what is the relationship between M_ℓ(x; λ) and prox_ℓ(x; λ)?

- Concept: **Overparameterization Ratio δ = d/n**
  - Why needed here: Central parameter governing weighting effectiveness; defines regimes (population δ→0, underparameterized δ∈(0,1), overparameterized δ>1) with qualitatively different optimal strategies.
  - Quick check question: In which regime does classical importance weighting fail to affect the learned model, and why?

## Architecture Onboarding

- Component map: Pretrained backbone -> Latent feature extraction -> PCA for d_eff -> Last layer retraining with ρ̃
- Critical path:
  1. Extract latent features from pretrained backbone on retraining data
  2. Compute d_eff via PCA; calculate δ_eff = d_eff/n
  3. Compute ρ̃ using Equation 10 with δ_eff and class priors
  4. Retrain last layer with square loss and weight ρ̃
  5. Evaluate per-class error on test set
- Design tradeoffs:
  - **Square vs. cross-entropy loss**: Theory requires square loss for closed-form ρ̃; paper argues square loss is competitive in low-data settings [27,28]
  - **Raw vs. effective dimension**: Raw d underestimates δ → underweights minority; d_eff via PCA is heuristic but empirically effective
  - **Weighting vs. downsampling**: Weighting preserves data but requires correct ρ; downsampling is robust but wastes majority samples
- Failure signatures:
  - **δ_eff ≥ 2π₊**: Per-class errors never intersect; minority class dominates WCE regardless of ρ
  - **High signal strength (large s)**: Overcorrection possible; unweighted may outperform ρ̃ (Figure 8)
  - **Non-Gaussian latent features**: ρ̃ prediction may shift; still directionally useful
- First 3 experiments:
  1. **Synthetic validation**: Generate class-conditional Gaussian data with known π₊, s=||μ||, δ. Solve Eq. (8) numerically for (α,γ,λ,b) and verify per-class risks match Eq. (1).
  2. **Effective dimension sensitivity**: On CelebA/CIFAR10, sweep variance threshold (90%, 95%, 99%, 99.9%) for d_eff; plot correlation between computed ρ̃ and empirical optimal ρ
  3. **Baseline comparison**: Compare wERM(ρ̃) vs. wERM(π₋/π₊) vs. downsampling vs. unweighted across n∈{20,50,100,500,1000}; measure WCE and per-class gaps

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the effective latent dimension be estimated in a principled manner rather than using PCA-based heuristics?
- Basis in paper: [explicit] Section 5 states "A weighting methodology which learns this effective dimension in a more principled manner could increase the practical application of our findings and is a major focus of our future work."
- Why unresolved: The current PCA approach (capturing 99% variance) is acknowledged as heuristic; no theoretical justification connects this specific threshold to optimal weighting.
- What evidence would resolve it: A provably consistent estimator of effective dimension that yields optimal weights without manual threshold selection, validated on diverse architectures and tasks.

### Open Question 2
- Question: How do optimal weights change when the class-conditional distributions have non-isotropic covariance?
- Basis in paper: [explicit] Section 5 notes "Our analysis is also limited to isotropic noise, but we hope to extend this in future works."
- Why unresolved: The CGMT analysis relies on the isotropic Gaussian assumption to derive closed-form scalar equations; extending to structured covariance requires new theoretical tools.
- What evidence would resolve it: Theoretical derivation of optimal weights under structured covariance (e.g., class-conditional Σ₊ ≠ Σ₋), with empirical validation showing improved WCE over isotropic-derived weights.

### Open Question 3
- Question: Does the derived optimal weighting scheme extend to cross-entropy loss and other convex losses?
- Basis in paper: [inferred] The paper restricts analysis to square loss (Corollary 1), noting practical justification but acknowledging it as a limitation. Theorem 1 provides equations for general convex losses but closed forms only exist for square loss.
- Why unresolved: Cross-entropy lacks a closed-form Moreau envelope, preventing direct analytical solution of the scalar equations in Theorem 1.
- What evidence would resolve it: Numerical solutions to the general system (6) for cross-entropy, or approximation bounds relating cross-entropy optimal weights to square-loss optimal weights.

### Open Question 4
- Question: Can the optimal weighting formula account for spurious correlations in addition to class imbalance?
- Basis in paper: [explicit] Section 5 states "a more sophisticated mixture could be useful in explaining class-imbalanced learning under spurious correlation or related settings."
- Why unresolved: The binary Gaussian mixture model assumes class labels are the only source of distributional difference; spurious features create additional subgroups not captured by the current theory.
- What evidence would resolve it: Extension of the CGMT analysis to group-conditional Gaussian mixtures, yielding group-aware optimal weights that improve worst-group accuracy on benchmarks like Waterbirds or CelebA with spurious attributes.

## Limitations
- The closed-form optimal weighting ρ̃ depends critically on the Gaussian latent feature assumption, which may not hold for complex real-world datasets.
- The PCA-based effective dimension estimation is heuristic and may mischaracterize task-relevant dimensions when variance and classification signal are decorrelated.
- The monotonic per-class risk assumption for downsampling analysis is unproven and may fail in finite samples.
- For δ ≥ 2π₊, the theoretical framework breaks down as per-class errors never intersect.

## Confidence

**High confidence:** The closed-form derivation of ρ̃ under Gaussian assumptions (Theorem 2), the PCA-based effective dimension estimation methodology, and the experimental validation on synthetic and real datasets.

**Medium confidence:** The effectiveness of ρ̃ versus prior ratio in underparameterized regimes, the theoretical analysis of downsampling via effective δ̃, and the general applicability to non-Gaussian latent features.

**Low confidence:** The assumption of monotonic per-class risks for downsampling analysis, the universal optimality of ρ̃ across all signal strengths, and the PCA-based dimension estimation for highly non-linear feature spaces.

## Next Checks

1. **Synthetic Gaussian validation:** Generate class-conditional Gaussian data with controlled π₊, δ, and s; verify that the numerically computed optimal ρ̃ exactly matches the theoretical prediction (Equation 10) and outperforms the prior ratio π₋/π₊ in worst-class error across the underparameterized regime.

2. **Effective dimension sensitivity analysis:** For CelebA and CIFAR10, systematically vary the PCA variance threshold (90%, 95%, 99%, 99.9%) to compute different d_eff values; plot the correlation between the resulting ρ̃ predictions and the empirically optimal weighting across multiple random seeds and dataset sizes.

3. **Downsampling monotonicity verification:** On the same real datasets, empirically measure per-class risks for various downsampling ratios; test whether the effective δ̃ = δ/(2π₊) accurately predicts the worst-class error behavior, and verify whether the monotonicity assumption holds in practice or if there exist non-monotonic regimes.