---
ver: rpa2
title: 'Crown, Frame, Reverse: Layer-Wise Scaling Variants for LLM Pre-Training'
arxiv_id: '2509.06518'
source_url: https://arxiv.org/abs/2509.06518
tags:
- layers
- scaling
- training
- layer-wise
- variants
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates Layer-Wise Scaling (LWS) and three novel
  variants (Framed, Reverse, and Crown) for pre-training transformer language models.
  The authors conduct controlled experiments on a fixed 180M parameter model trained
  for 5B tokens, systematically ablating LWS effects.
---

# Crown, Frame, Reverse: Layer-Wise Scaling Variants for LLM Pre-Training

## Quick Facts
- arXiv ID: 2509.06518
- Source URL: https://arxiv.org/abs/2509.06518
- Authors: Andrei Baroian; Kasper Notebomer
- Reference count: 13
- Primary result: Layer-wise scaling variants improve validation perplexity over isotropic baselines by up to 6% at 18 layers, but the specific variant profile matters little.

## Executive Summary
This paper systematically investigates Layer-Wise Scaling (LWS) and three novel variants (Framed, Reverse, and Crown) for pre-training transformer language models. Through controlled experiments on a fixed 180M parameter model trained for 5B tokens, the authors demonstrate that all LWS variants outperform an isotropic baseline in validation perplexity, with up to 6% improvement observed for 18-layer models. However, the specific variant profile (vanilla, framed, reverse, or crown) matters little, suggesting that any heterogeneous parameter allocation is preferable to uniform allocation at equal compute. The study reveals that LWS alone does not replicate the claimed data efficiency gains from previous work, indicating that performance benefits arise from synergies with other architectural components.

## Method Summary
The authors conduct controlled experiments on 180M parameter decoder-only transformer models (OLMo2-based) with 12 or 18 layers, using Grouped Query Attention with 4 KV heads. They implement four layer-wise scaling variants (Vanilla, Framed, Reverse, Crown) through linear or three-point interpolation of FFN width and attention head count across layers, maintaining total parameter budget. Models are trained on 5B tokens from Dolmino Mix 1124 with batch size 384 and learning rate 6e-4, using validation perplexity as the primary metric. Baseline models are trained five times with different seeds, while LWS variants are run once due to resource constraints.

## Key Results
- All LWS variants show improved validation perplexity compared to isotropic baseline, with up to 6% improvement observed for 18-layer models
- The specific variant profile (vanilla, framed, reverse, or crown) matters little, suggesting heterogeneity itself drives gains
- Benefits are more pronounced with deeper networks (18 layers) than shallow networks (12 layers)
- LWS alone does not replicate data efficiency gains claimed in previous work, suggesting synergies with other architectural components

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Heterogeneous parameter allocation across layers improves perplexity compared to isotropic baselines at equivalent compute.
- Mechanism: Different transformer layers serve different functional roles—early layers capture local syntactic patterns while deeper layers handle abstraction and reasoning. Non-uniform capacity allocation better matches these varying computational demands.
- Core assumption: Layer functional specialization implies differentiated capacity requirements that can be exploited during pre-training.
- Evidence anchors:
  - [abstract] "All LWS variants show improved validation perplexity compared to an isotropic baseline, with up to 6% improvement observed for 18-layer models."
  - [section 4] "Results suggest that the presence of heterogeneity, not its exact shape, matters."
  - [corpus] Limited direct corpus support; neighboring papers focus on scaling laws and curriculum approaches rather than layer-wise capacity allocation.

### Mechanism 2
- Claim: The specific profile of layer-wise scaling (vanilla, framed, reverse, crown) has minimal impact on final performance—all variants converge similarly.
- Mechanism: LWS reallocates existing representational capacity rather than creating new capacity. The total parameter budget constraint means different allocation profiles are approximately equivalent in expressivity, yielding incremental rather than transformative gains.
- Core assumption: Parameter count is the primary constraint on model capacity under fixed compute.
- Evidence anchors:
  - [section 5] "LWS reallocates rather than creates representational capacity, yielding incremental gains rather than transformative improvements."
  - [section 4] "All the variants performed almost the same, all significant better than the baseline."
  - [corpus] No direct corpus evidence on allocation profile equivalence.

### Mechanism 3
- Claim: LWS benefits emerge more clearly with deeper networks (18 layers) than shallow networks (12 layers).
- Mechanism: With fewer layers, interpolation produces coarse jumps between layer sizes rather than smooth progressions. More layers allow finer-grained capacity gradients that better match functional layer differentiation.
- Core assumption: Smooth capacity transitions correlate with improved optimization dynamics.
- Evidence anchors:
  - [section 4] "The difference is insignificant at the 12 layers, but it is meaningful at the 18 layer models, vanilla LWS having a 6% better perplexity."
  - [section 4] "With so few layers, the interpolation occurring in LWS would produce noticeable jumps rather than a smooth progression."
  - [corpus] No corpus papers directly address layer-count interactions with scaling strategies.

## Foundational Learning

- Concept: **Validation Perplexity as Pre-training Metric**
  - Why needed here: The paper uses perplexity as the primary evaluation metric, noting that downstream benchmarks show no learning signal at 5B token scale. Understanding why perplexity is appropriate here (geometric mean of inverse token probabilities) is essential.
  - Quick check question: Can you explain why perplexity might be more reliable than benchmark accuracy when training on <10B tokens?

- Concept: **Grouped Query Attention (GQA)**
  - Why needed here: All experiments use GQA with 4 KV heads. GQA reduces KV cache memory by sharing key/value projections across query head groups, which constrains how LWS can scale attention (query heads scale, but KV heads remain fixed).
  - Quick check question: Why does fixing G=4 KV heads limit the granularity of attention head scaling in LWS?

- Concept: **Parameter Budget Constraints**
  - Why needed here: All variants maintain ~180M parameters. Understanding that comparisons are compute-equivalent (same total params, different allocation) is critical for interpreting why gains are "incremental" rather than transformative.
  - Quick check question: If you doubled total parameters, would you expect LWS gains to scale proportionally or diminish?

## Architecture Onboarding

- Component map:
  - Baseline: Isotropic layers with uniform FFN width (β=constant) and attention heads (α=constant)
  - Vanilla LWS: Linear interpolation from (α_start, β_start) to (α_end, β_end), typically increasing toward deeper layers
  - Framed LWS: Same as vanilla but first and last layers fixed at max(α_end, α_start) and max(β_end, β_start)
  - Reverse LWS: Decreasing allocation (α_start ≥ α_end, β_start ≥ β_end), framed
  - Crown LWS: Three-point interpolation [α_start, α_middle, α_end] peaking at middle layers, framed
  - GQA integration: Fixed KV heads (G=4), only query heads scale via α

- Critical path:
  1. Define layer count N and target total parameters (~180M)
  2. Choose scaling profile and set interpolation endpoints (α_start, α_end, β_start, β_end)
  3. Compute per-layer dimensions: d^i_ffn = β_i × d_model, n^i_h = α_i × d_model / d_h
  4. Enforce constraints: H_q mod G = 0 for GQA compatibility
  5. Verify total parameter count matches budget
  6. Train and compare validation perplexity at equivalent token counts

- Design tradeoffs:
  - **Depth vs. width**: Paper traded width for depth (12L→18L at same params), which may affect baseline performance but reveals LWS benefits more clearly
  - **Smoothness vs. framing**: Framing preserves first/last layers at max capacity (motivated by pruning literature) but reduces effective scaling range
  - **Training speed vs. flexibility**: LWS 12L showed ~10% throughput drop from kernel switching overhead; 18L models showed no slowdown (possibly due to baseline configuration choices)

- Failure signatures:
  - **No improvement over baseline at 12 layers**: Indicates insufficient depth for smooth interpolation
  - **Perplexity bumps during training**: Observed in Framed, Crown, and Vanilla variants at different points; typically self-correcting but may indicate instability
  - **Unusually low perplexity values (~5)**: May indicate data distribution issues (training/validation from same source) rather than genuine model quality

- First 3 experiments:
  1. **Reproduce baseline vs. vanilla LWS at 18 layers**: Train both for 5B tokens on identical data split, measure validation perplexity difference. Expected: ~5-6% improvement for LWS.
  2. **Ablate framing effect**: Compare vanilla LWS vs. framed LWS with identical interpolation endpoints. This isolates whether preserving first/last layer capacity matters (paper suggests minimal difference).
  3. **Scale depth test**: Run baseline and vanilla LWS at 24 layers with same 180M budget. If smoothness hypothesis holds, LWS advantage should increase further.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Which specific architectural components or training recipes synergize with Layer-Wise Scaling (LWS) to produce the data efficiency gains (e.g., 2x token reduction) claimed in prior work?
- Basis in paper: [explicit] The authors conclude that LWS alone does not replicate OpenELM's data efficiency, suggesting performance benefits arise from "synergies with other architectural components."
- Why unresolved: This study isolated LWS in a controlled environment, effectively ruling it out as the sole cause of OpenELM's efficiency, but did not test interactions with other variables to identify the actual cause.
- What evidence would resolve it: Ablation studies combining LWS with other architectural features (e.g., specific normalization techniques, activation functions) to identify which combination restores the data efficiency.

### Open Question 2
- Question: Do the relative performance differences between LWS profiles (Vanilla, Framed, Reverse, Crown) change significantly when scaling model size to 7B parameters and training data to 100B+ tokens?
- Basis in paper: [explicit] The authors explicitly state in the Limitations and Future Work sections that experiments must scale to "orders of magnitude more tokens and parameters to fully assess their potential."
- Why unresolved: The current study is limited to 180M parameters and 5B tokens, which may obscure scaling behaviors or "smoothness" benefits that only emerge in deeper, larger networks.
- What evidence would resolve it: Training the proposed LWS variants at the 7B parameter scale and comparing the validation perplexity and convergence rates against the isotropic baseline.

### Open Question 3
- Question: Do the observed perplexity improvements from LWS variants translate to measurable gains on downstream natural language understanding and reasoning benchmarks?
- Basis in paper: [explicit] Section 5.1 notes the evaluation scope is limited because the model is too small to generate coherent text or show learning signals on standard benchmarks, forcing reliance solely on perplexity.
- Why unresolved: It remains unverified whether the lower validation perplexity correlates with better performance on specific tasks (e.g., SciQ, reasoning) or if the gains are superficial.
- What evidence would resolve it: Evaluating scaled-up versions of these models on standard benchmarks (like MMLU or HellaSwag) to correlate perplexity drops with accuracy improvements.

## Limitations
- Limited depth range tested (12-18 layers) constrains conclusions about optimal depth for LWS, with only one data point (18L vs 12L) supporting the smoothness hypothesis
- Single-run reporting for LWS variants versus five runs for the baseline introduces statistical uncertainty that cannot be quantified
- Evaluation scope is limited to validation perplexity at 5B tokens, as models are too small to generate coherent text or show learning signals on standard benchmarks

## Confidence
- Layer-wise scaling improves perplexity over isotropic baselines at equal compute: **High** confidence
- The specific variant profile matters little compared to heterogeneity itself: **Medium** confidence
- Benefits are more pronounced with deeper networks: **Low** confidence
- LWS alone does not replicate data efficiency claims from previous work: **Medium** confidence

## Next Checks
1. **Statistical validation of variant equivalence**: Run each LWS variant (vanilla, framed, reverse, crown) at 18 layers for 5B tokens with 5 random seeds each. Compute confidence intervals on validation perplexity and test whether observed differences between variants are statistically significant.
2. **Intermediate depth analysis**: Evaluate baseline and vanilla LWS at 15 layers (midway between 12 and 18). If smoothness hypothesis holds, improvement should emerge gradually rather than appearing abruptly at 18 layers.
3. **Larger model scaling test**: Scale to 360M parameters (double capacity) and compare baseline vs vanilla LWS at both 18 and 24 layers. This tests whether LWS benefits scale proportionally with model size or diminish as capacity increases.