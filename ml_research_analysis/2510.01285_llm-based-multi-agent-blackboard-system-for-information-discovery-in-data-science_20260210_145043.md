---
ver: rpa2
title: LLM-Based Multi-Agent Blackboard System for Information Discovery in Data Science
arxiv_id: '2510.01285'
source_url: https://arxiv.org/abs/2510.01285
tags:
- data
- agent
- files
- blackboard
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of data discovery in large,
  heterogeneous data lakes, a bottleneck for applying LLMs in real-world data science.
  The authors propose a novel multi-agent system inspired by the blackboard architecture,
  where a central agent posts requests to a shared blackboard and autonomous subordinate
  agents (responsible for data partitions or web retrieval) volunteer to respond based
  on their capabilities.
---

# LLM-Based Multi-Agent Blackboard System for Information Discovery in Data Science

## Quick Facts
- arXiv ID: 2510.01285
- Source URL: https://arxiv.org/abs/2510.01285
- Reference count: 40
- Primary result: 13%-57% relative improvements in end-to-end success and up to 9% relative gain in data discovery F1 over best baseline

## Executive Summary
This paper addresses the challenge of data discovery in large, heterogeneous data lakes - a bottleneck for applying LLMs in real-world data science. The authors propose a novel multi-agent system inspired by the blackboard architecture, where a central agent posts requests to a shared blackboard and autonomous subordinate agents volunteer to respond based on their capabilities. This design removes the need for a central coordinator to know each agent's expertise, improving scalability and flexibility. Evaluated on three benchmarks - KramaBench, DSBench, and DA-Code - the approach achieves significant performance gains over master-slave baselines.

## Method Summary
The system uses a blackboard architecture where a main agent orchestrates data discovery through autonomous sub-agents. First, the data lake is clustered into partitions (using filename-based or embedding-based methods), with each file agent responsible for one cluster. File agents perform offline analysis to learn file structure and processing requirements. During online execution, the main agent (using ReAct framework) posts requests to a shared blackboard, and file agents self-select whether to respond based on their capability assessment. A search agent provides web-based information retrieval when needed. The main agent then generates Python code that identifies, loads, and processes relevant files to answer the query. The system was evaluated on three benchmarks with 104-253 tasks each, comparing against master-slave and RAG baselines.

## Key Results
- 13%-57% relative improvement in end-to-end success rate compared to best baselines
- Up to 9% relative gain in data discovery F1 score over master-slave baseline
- Runtime comparable to sequential methods (132.0-145.2 seconds) while achieving higher accuracy
- Blackboard system is ~2.3× more expensive than RAG but provides 54.1% relative improvement in accuracy

## Why This Works (Mechanism)

### Mechanism 1: Agent Self-Selection Based on Autonomous Capability Assessment
Subordinate agents independently evaluate whether their data holdings match blackboard requests rather than being assigned by a central controller. This enables more precise matching between requests and relevant data sources, improving data discovery accuracy.

### Mechanism 2: Data Lake Partitioning into Specialized Clusters
Partitioning the data lake into specialized clusters assigned to individual file agents improves scalability by reducing per-agent context complexity. Each agent handles only a subset of files, avoiding context window overflow and reducing reasoning complexity.

### Mechanism 3: Broadcast-Based Request Posting with Parallel Sub-Agent Processing
The main agent posts requests to a shared blackboard, and all file agents process requests independently and in parallel, writing responses to a response board. This avoids sequential coordinator-to-agent handoffs and achieves latency comparable to sequential methods.

## Foundational Learning

- **ReAct Framework (Reasoning + Acting)**: The main agent operates via iterative action selection (plan/reason/run_code/request_help/answer) interleaved with observations - this is the core execution loop. *Quick check: Can you explain how a ReAct agent decides between "planning" and "executing code" as next actions?*

- **Blackboard Architecture (Classical AI)**: This paper adapts the 1980s blackboard model (Hearsay-II) where knowledge sources monitor shared state and contribute opportunistically. *Quick check: What are the three components of a classical blackboard system, and which component is replaced by LLM-based agents here?*

- **Data Lake Organization & Retrieval**: The system addresses heterogeneous file discovery; understanding why standard RAG struggles with tabular/domain-specific data motivates the multi-agent approach. *Quick check: Why might embedding-based retrieval fail on domain-specific tabular data compared to agent-based file exploration?*

## Architecture Onboarding

- **Component map**: Main Agent -> Blackboard (request broadcast) -> File Agents + Search Agent (autonomous response) -> Response Board -> Main Agent -> Code Generation
- **Critical path**: Offline Phase: Cluster data lake → instantiate file agents → each agent analyzes assigned files. Online Phase: Main agent receives query → ReAct loop begins → posts request to blackboard → file agents evaluate relevance → responding agents write to response board → main agent integrates responses → generates/executes Python code → final answer
- **Design tradeoffs**: Cluster granularity (fewer clusters = more files per agent; more clusters = more agents), clustering method (filename-based vs. content-embedding), action budget (higher max actions improve exploration but increase latency), cost vs. accuracy (Blackboard is ~2.3× more expensive than RAG)
- **Failure signatures**: Silent blackboard (no agents respond - main agent reformulates or falls back to code-based search), incorrect file selection (wrong file loaded), domain knowledge gaps (main agent requests external knowledge via search agent)
- **First 3 experiments**: 1) Cluster count sweep on KramaBench with K ∈ {2, 4, 8, 16} clusters, measuring accuracy and F1. 2) Ablate search agent on KramaBench to quantify external knowledge contribution. 3) Scalability stress test by synthetically expanding data lake size 10×, 100× and comparing Blackboard vs. Master-Slave performance degradation.

## Open Questions the Paper Calls Out

- **Adaptive data partitioning**: How can the system implement adaptive strategies for data partitioning to handle dynamic and evolving data environments? The current methodology relies on an offline clustering phase with static partitioning.
- **Generalization beyond data science**: To what degree does the blackboard architecture generalize to multi-agent domains outside of data science? The paper evaluates exclusively on data science benchmarks.
- **Cost reduction without autonomy loss**: Can the system's monetary cost be reduced without compromising the decentralized autonomy that drives its performance? The current design broadcasts requests to all agents, incurring high inference costs.

## Limitations

- The exact blackboard coordination protocol (how multiple responses are aggregated and presented to the main agent) is described at a high level without code-level details
- The clustering method shows significant impact on performance, but the optimal cluster count appears dataset-dependent without clear guidelines for new data lakes
- The search agent provides measurable gains but its contribution varies substantially across domains, suggesting sensitivity to external API reliability

## Confidence

- **High confidence**: The 13%-57% relative improvement in end-to-end success over baselines is well-supported by results across three independent benchmarks
- **Medium confidence**: The 9% relative gain in data discovery F1 is credible but depends heavily on the clustering strategy chosen and the quality of file agents' self-assessment capabilities
- **Medium confidence**: The claim of "no substantial difference in latency" between Blackboard and sequential methods holds for the tested scale but may not generalize to much larger agent counts

## Next Checks

1. **Scale sensitivity test**: Reproduce the experiment from Figure 20 by synthetically expanding the data lake size 10× and 100×, then measure how relative Blackboard vs. Master-Slave performance changes with scale
2. **Clustering granularity sweep**: For each benchmark, systematically vary cluster count K across a wider range (2-32) and measure the tradeoff between accuracy and coordination overhead to identify optimal cluster counts per dataset type
3. **Ablation on self-selection**: Implement a variant where the main agent assigns requests to specific agents (like the Master-Slave baseline) rather than allowing self-selection, then compare data discovery F1 to isolate the contribution of autonomous agent capability assessment