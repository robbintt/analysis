---
ver: rpa2
title: 'Dion2: A Simple Method to Shrink Matrix in Muon'
arxiv_id: '2512.16928'
source_url: https://arxiv.org/abs/2512.16928
tags:
- dion2
- muon
- matrix
- selection
- columns
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Dion2, a simple method to reduce the computational
  overhead of the Muon optimizer by selecting and orthonormalizing only a fraction
  of rows or columns of the momentum matrix at each iteration. Unlike prior approaches
  that rely on complex low-rank approximations, Dion2 directly samples a subset of
  the matrix, making updates sparse and reducing both computation and communication
  costs.
---

# Dion2: A Simple Method to Shrink Matrix in Muon

## Quick Facts
- arXiv ID: 2512.16928
- Source URL: https://arxiv.org/abs/2512.16928
- Authors: Kwangjun Ahn; Noah Amsel; John Langford
- Reference count: 4
- Primary result: Dion2 achieves near-identical validation loss to full Muon while significantly reducing computational overhead through selective orthonormalization

## Executive Summary
This paper introduces Dion2, a simple method to reduce the computational overhead of the Muon optimizer by selecting and orthonormalizing only a fraction of rows or columns of the momentum matrix at each iteration. Unlike prior approaches that rely on complex low-rank approximations, Dion2 directly samples a subset of the matrix, making updates sparse and reducing both computation and communication costs. Experiments on 300M and 1B parameter models trained on the FineWeb dataset show that Dion2 achieves nearly identical final validation losses to full Muon, even when only 25% of the matrix is orthonormalized. A micro-benchmark further demonstrates that Dion2 significantly reduces optimizer step time.

## Method Summary
Dion2 implements selective momentum decay and orthonormalization by first accumulating gradients into the full momentum matrix (M ← M + G), then selecting a subset K of rows/columns based on ℓ1-norm or random criteria, applying Newton-Schulz orthonormalization only to M[K,:], and critically, decaying only the selected subset (M[K,:] ← μ·M[K,:]) while leaving unselected portions untouched. This error-feedback approach ensures all gradient directions eventually contribute while maintaining the spectral norm constraint (∥O∥₂ = 1) for the selected portion. The method reduces computation by processing only α fraction of the matrix at each iteration, with α=0.25 showing near-identical optimization quality to full Muon while providing substantial speedups.

## Key Results
- 0.25-Dion2 achieves final validation loss of 2.635 vs Muon 2.623 on 1B parameter model
- Selective decay is essential - decaying all rows instead of selective decay significantly degrades convergence
- ℓ1-norm and random selection achieve nearly identical convergence, suggesting selection strategy agnosticism
- Micro-benchmark shows Dion2 significantly reduces optimizer step time compared to full Muon

## Why This Works (Mechanism)

### Mechanism 1: Selective Momentum Decay (Error Feedback)
Decaying only the selected subset of momentum preserves gradient information from unselected portions, enabling sparse updates without optimization quality loss. The algorithm accumulates gradients into momentum across all rows/columns, but only decays the selected subset, creating an error feedback loop where unselected portions retain their accumulated signal until eventually selected.

### Mechanism 2: Submatrix Orthonormalization Preserves Update Quality
Orthonormalizing only a fraction α of rows/columns via Newton-Schulz maintains near-identical optimization quality to full Muon when α ≥ 0.25. The Newton-Schulz iteration projects the momentum submatrix onto the set of orthonormal matrices, satisfying Muon's spectral norm constraint for the selected portion.

### Mechanism 3: Selection Strategy Agnosticism
Both ℓ1-norm-based selection and uniform random selection achieve nearly identical convergence, suggesting the specific selection criterion is less important than the error feedback mechanism. Both work because error feedback ensures all directions eventually contribute.

## Foundational Learning

- **Newton-Schulz Iteration**: GPU-efficient orthonormalization primitive preferred over SVD because it uses only matrix multiplications and additions
- **Error Feedback in Optimization**: Dion2's selective decay is an error feedback mechanism; full decay would cause significantly worse convergence
- **Spectral vs. RMS Norm Constraints**: Muon uses spectral norm (∥O∥₂ = 1) rather than Frobenius norm because it provides the tightest bound on RMS-to-RMS operator norm for activation changes

## Architecture Onboarding

- **Component map**: Gradient G → Momentum Accumulation (M ← M + G) → Selection (K ← Select_α(M)) → Newton-Schulz on M[K,:] → Selective Decay (M[K,:] ← μ·M[K,:]) → Sparse Weight Update (W[K,:] ← W[K,:] - η·√(fan-out/fan-in)·O)
- **Critical path**: Selection → Newton-Schulz → selective decay sequence must complete before weight update; in distributed settings, submatrix synchronization is on the critical path
- **Design tradeoffs**: Lower α → faster but potentially slower convergence; random selection enables simpler DP-sync; paper selects along shorter dimension
- **Failure signatures**: Convergence lagging baseline (check selective decay implementation); validation loss diverging (verify momentum accumulation); no speedup observed (confirm submatrix orthonormalization)
- **First 3 experiments**: 1) Sanity check: compare selective decay vs full decay on small model; 2) Selection fraction sweep: test α ∈ {0.5, 0.25, 0.125} on target model; 3) Selection method comparison: test ℓ1-norm vs random with identical α

## Open Questions the Paper Calls Out
- Do sparse updates provide implicit regularization or generalization benefits beyond computational efficiency?
- Does Dion2 preserve efficiency advantages at training scales significantly larger than 1B parameters?
- Can alternative selection strategies outperform currently tested ℓ1-norm and uniform random methods?

## Limitations
- No theoretical convergence guarantees established for selective orthonormalization approach
- Performance benefits at very large scale (>1B parameters) remain unexplored
- Does not analyze potential side-effects of sparsity on generalization or regularization

## Confidence
- **High Confidence**: Selective decay mechanism and its necessity
- **Medium Confidence**: α=0.25 provides near-optimal quality-speed tradeoff
- **Medium Confidence**: Selection strategy agnosticism
- **Low Confidence**: Theoretical justification for preserving spectral norm properties across full matrix

## Next Checks
1. Run longer training (150B+ tokens) on 1B model to determine if narrowing gap between α-Dion2 and full Muon persists or widens
2. Measure spectral norm of full momentum matrix throughout training to confirm it remains bounded despite selective orthonormalization
3. Track selection frequency to quantify "eventual selection" property and identify potential stale gradient accumulation