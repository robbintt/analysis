---
ver: rpa2
title: 'LIMR: Less is More for RL Scaling'
arxiv_id: '2502.11886'
source_url: https://arxiv.org/abs/2502.11886
tags:
- training
- data
- learning
- limr
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether scaling up reinforcement learning
  (RL) training data improves reasoning capabilities in language models. The authors
  propose Learning Impact Measurement (LIM), a method to evaluate and prioritize training
  samples based on their alignment with model learning trajectories.
---

# LIMR: Less is More for RL Scaling

## Quick Facts
- arXiv ID: 2502.11886
- Source URL: https://arxiv.org/abs/2502.11886
- Authors: Xuefeng Li; Haoyang Zou; Pengfei Liu
- Reference count: 16
- Key result: Strategic sample selection via LIM achieves better performance than scaling up training data

## Executive Summary
This paper challenges the conventional wisdom that scaling up reinforcement learning training data improves reasoning capabilities in language models. Instead, the authors propose Learning Impact Measurement (LIM), a method that evaluates and prioritizes training samples based on their alignment with model learning trajectories. Using LIM, they demonstrate that a carefully selected subset of 1,389 samples outperforms the full 8,523-sample dataset, achieving 16.7% higher accuracy on AIME24 and outperforming other data-efficient approaches on MATH500. The results suggest that sample selection quality, rather than data scale, is key to enhancing reasoning capabilities in LLMs.

## Method Summary
The authors introduce Learning Impact Measurement (LIM), a metric that evaluates training samples based on their alignment with a model's learning trajectory. This approach prioritizes samples that are most impactful for the model's development rather than simply using all available data. The LIMR method uses this metric to select a strategic subset of training samples (1,389 out of 8,523) for RL training. The paper demonstrates that this selective approach not only reduces computational requirements but also improves performance on reasoning benchmarks compared to using the full dataset or other data-efficient methods.

## Key Results
- LIMR achieves 16.7% higher accuracy on AIME24 compared to using the full dataset
- LIMR outperforms LIMO by 13.0% and s1 by 22.2% on MATH500
- A strategically selected subset of 1,389 samples outperforms the full 8,523-sample dataset

## Why This Works (Mechanism)
The LIMR method works by identifying and prioritizing training samples that have the highest impact on the model's learning trajectory. By focusing on these high-impact samples rather than scaling up the dataset, the method ensures more efficient use of computational resources while maintaining or improving performance. The approach suggests that not all training data contributes equally to model improvement, and that strategic selection based on learning impact can yield better results than simply increasing data volume.

## Foundational Learning
- Reinforcement Learning (RL): Why needed - to optimize model behavior through reward signals; Quick check - verify understanding of policy gradients and value functions
- Learning Trajectory Analysis: Why needed - to track how models learn over time; Quick check - confirm ability to interpret learning curves and convergence patterns
- Sample Selection Strategies: Why needed - to understand different approaches to data curation; Quick check - compare random sampling vs. impact-based selection methods
- Reasoning Benchmark Evaluation: Why needed - to measure reasoning capabilities effectively; Quick check - review AIME and MATH benchmark structures
- Data Efficiency in ML: Why needed - to understand trade-offs between data volume and model performance; Quick check - examine computational costs of different training approaches

## Architecture Onboarding

**Component Map:**
Model -> LIM Metric -> Sample Selector -> RL Trainer -> Performance Evaluator

**Critical Path:**
LIM Metric evaluation → Sample selection → RL training → Performance assessment

**Design Tradeoffs:**
- Sample quantity vs. quality: Fewer high-impact samples vs. more diverse but potentially less impactful data
- Computational overhead: LIM metric calculation vs. training efficiency gains
- Generalization risk: Potential overfitting to selected samples vs. broader dataset coverage

**Failure Signatures:**
- Overfitting to selected samples if LIM metric is too narrow
- Missing important reasoning patterns if selection is too aggressive
- Computational inefficiency if LIM metric calculation is too expensive

**First Experiments:**
1. Compare LIMR performance using different sample selection thresholds
2. Test LIM metric effectiveness on a held-out reasoning dataset
3. Measure computational overhead of LIM metric during training

## Open Questions the Paper Calls Out
None

## Limitations
- Unclear comparison methodology with baseline methods - specific training configurations and compute budgets are not fully specified
- LIM metric generalization beyond AIME24 dataset needs validation for other reasoning domains
- Computational overhead of LIM metric during training is not discussed
- Lack of analysis on how LIM-selected samples differ from randomly selected ones in terms of difficulty and diversity

## Confidence

**Confidence Labels:**
- High confidence in the basic premise that sample selection can improve training efficiency
- Medium confidence in the specific LIM methodology and its claimed advantages
- Medium confidence in the quantitative results due to unclear baseline comparisons

## Next Checks
1. Replicate the LIMR results using the same training compute budget as baseline methods to isolate the effect of sample selection
2. Test LIM metric transferability by applying it to a different reasoning dataset (e.g., GSM8K or MATH500 with different splits)
3. Conduct ablation studies removing the LIM metric while keeping all other training parameters constant to quantify its specific contribution