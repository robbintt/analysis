---
ver: rpa2
title: Riemannian Optimization for LoRA on the Stiefel Manifold
arxiv_id: '2508.17901'
source_url: https://arxiv.org/abs/2508.17901
tags:
- lora
- stiefel
- matrix
- optimization
- adamw
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Stiefel-LoRA, a novel parameter-efficient
  fine-tuning approach that enforces orthogonality constraints on the B matrix of
  LoRA by optimizing it on the Stiefel manifold. The method addresses the problem
  of basis redundancy and underutilization of rank in conventional LoRA fine-tuning
  with AdamW.
---

# Riemannian Optimization for LoRA on the Stiefel Manifold

## Quick Facts
- arXiv ID: 2508.17901
- Source URL: https://arxiv.org/abs/2508.17901
- Reference count: 40
- One-line primary result: Stiefel-LoRA achieves 9-12% accuracy improvements over AdamW on commonsense reasoning tasks by enforcing orthogonality on LoRA's B matrix

## Executive Summary
This paper introduces Stiefel-LoRA, a parameter-efficient fine-tuning approach that addresses basis redundancy in LoRA's B matrix by optimizing it on the Stiefel manifold. By constraining B to have orthonormal columns, the method ensures full effective rank utilization and eliminates correlation between basis vectors. Experiments across seven commonsense reasoning datasets show consistent performance improvements of 9-12% over AdamW, with effective rank analysis confirming that Stiefel optimization achieves 16/16 dimensional utilization versus 12/16 for AdamW.

## Method Summary
Stiefel-LoRA optimizes LoRA's B matrix on the Stiefel manifold (B^T B = I_r) using Riemannian optimization with QR-based retraction. The method combines standard AdamW for matrix A with Riemannian Adam for B, where gradients are projected to the tangent space before retraction. QR decomposition projects Euclidean updates back to the manifold after each step. The approach addresses the fundamental limitation of AdamW in LoRA fine-tuning where basis vectors become correlated, leading to underutilization of the allocated rank capacity.

## Key Results
- Stiefel-LoRA achieves 9-12% average accuracy gains over AdamW on LLaMA-3.2-1B across seven commonsense reasoning datasets
- Effective rank analysis shows Stiefel optimization utilizes all 16 dimensions fully versus ~12 dimensions for AdamW
- The method is particularly effective when both LoRA matrices are trained jointly, with fixed matrix A showing limited benefits
- Similar improvements observed on larger models (LLaMA-3.2-3B and LLaMA-3-8B) across multiple benchmark types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Enforcing orthogonality on the B matrix via Stiefel manifold optimization eliminates basis redundancy and increases effective rank utilization.
- Mechanism: Standard LoRA with AdamW updates B in Euclidean space without constraints, allowing column vectors to become correlated. By constraining B to St(d,r) where B^T B = I_r, columns remain orthonormal, ensuring each basis direction contributes independently to the r-dimensional subspace. The paper reports full effective rank (16/16) vs. 12.1/16 for AdamW at r=16.
- Core assumption: The performance gap between LoRA and full fine-tuning stems partly from underutilization of the allocated rank capacity, not solely from insufficient parameter count.
- Evidence anchors:
  - [abstract] "basis redundancy in LoRA's B matrix when using AdamW, which fundamentally limits performance"
  - [section 4.2] "Stiefel optimization consistently utilizes all 16 dimensions fully, while AdamW effectively uses only 12 dimensions on average"
  - [corpus] StelLA (arXiv:2510.01938) independently validates that "insufficient exploitation of the geometric structure underlying low-rank manifolds" limits LoRA performance
- Break condition: If the downstream task requires learning highly correlated features that benefit from redundant representations, forced orthogonality could constrain expressiveness.

### Mechanism 2
- Claim: QR-based retraction provides a computationally tractable way to maintain Stiefel manifold constraints during gradient updates.
- Mechanism: After computing Euclidean gradient ∇_B f and projecting it to tangent space as ξ = M'_B - B·sym(B^T M'_B), the algorithm takes a step Y' = B - α_ξ, then projects back to the manifold via QR decomposition: B_{k+1} = Q where Y' = QR. This ensures B^T B = I_r after every update without expensive exponential maps.
- Core assumption: QR decomposition's computational overhead (O(dr²)) is acceptable given the typically small rank r (16-64) compared to hidden dimension d.
- Evidence anchors:
  - [section 3.1] "This QR-based retraction robustly ensures that B_{k+1} satisfies the orthonormality constraint"
  - [appendix C.2, Algorithm 1] Complete update procedure with QR retraction at line 36
  - [corpus] Geometric design paper (arXiv:2507.15638) discusses landing algorithms as alternative retraction-free approaches, suggesting QR is one valid choice among several
- Break condition: If r approaches d/2 or larger, QR cost becomes non-trivial; numerical instability may arise with ill-conditioned Y'.

### Mechanism 3
- Claim: Adam momentum adapted to Riemannian setting preserves second-order optimization benefits while respecting manifold geometry.
- Mechanism: First and second moments (m_B, v_B) are computed in ambient Euclidean space using ∇_B f, yielding preconditioned direction M'_B. This is then projected to the tangent space before retraction—preserving Adam's adaptive learning rate benefits without violating manifold constraints.
- Core assumption: Euclidean momentum provides useful curvature information even when the final update must follow manifold geometry.
- Evidence anchors:
  - [section 3.1] "Momentum-related computations are first performed in the ambient Euclidean space...This direction M'_{k+1} is then projected onto the tangent space"
  - [appendix C.2, lines 25-33] Explicit Adam preconditioning followed by tangent projection
  - [corpus] No direct corpus validation; Muon optimizer paper (arXiv:2507.12142) proposes alternative Riemannian frameworks but doesn't compare momentum strategies
- Break condition: If the loss landscape has highly curved geometry, Euclidean momentum may point in directions that, once projected, lose their intended adaptive behavior.

## Foundational Learning

- Concept: **Stiefel Manifold St(n,p)**
  - Why needed here: The constraint space for B—understanding that B^T B = I defines a curved surface in R^(d×r), not a simple box constraint, is essential for implementing correct updates.
  - Quick check question: Can you explain why gradient descent in Euclidean space cannot guarantee B stays orthonormal, even with regularization?

- Concept: **Tangent Space Projection**
  - Why needed here: The Riemannian gradient must lie in the tangent plane at the current point; direct Euclidean gradients point "off" the manifold.
  - Quick check question: Given B with orthonormal columns and Euclidean gradient G, what is the projection of G onto the tangent space at B?

- Concept: **Retraction vs. Exponential Map**
  - Why needed here: The paper uses QR-based retraction as a practical approximation to the (computationally expensive) exponential map for moving along the manifold.
  - Quick check question: Why does the simple update B + α∇_B f not preserve orthonormality, and what property must a valid retraction satisfy?

## Architecture Onboarding

- Component map:
```
Pre-trained weights W_0 (frozen)
         ↓
    LoRA branch: B (d×r) × A (r×k)
         ↓              ↓
   Stiefel manifold    Euclidean
   (QR retraction)     (Adam)
         ↓              ↓
      Combined: W = W_0 + BA
```

- Critical path:
  1. Initialize B_0 with orthonormal columns (e.g., random matrix → QR)
  2. Forward pass: compute W_0 + BA
  3. Backward pass: compute ∇_A f (Euclidean), ∇_B f (Euclidean → project → retract)
  4. A update: standard Adam step
  5. B update: Adam momentum → project to tangent space → QR retraction

- Design tradeoffs:
  - **Learning rates differ**: Stiefel uses 0.2-0.3 vs. AdamW's 1e-4 (Table 6-8). The manifold's geometry permits larger steps.
  - **B-only vs. A+B training**: Table 4 shows Stiefel underperforms AdamW when A is fixed and random—orthogonality cannot extract signal from uninformative projections.
  - **Warmup**: AdamW requires 100 warmup steps; Stiefel experiments show none listed, suggesting different scheduling needs.

- Failure signatures:
  - Effective rank stuck below nominal r → orthonormality lost (check B^T B ≈ I)
  - Loss diverging with Stiefel → learning rate too high for manifold curvature
  - Performance worse than AdamW on simple tasks → potential over-constraint; check if task actually benefits from orthogonality

- First 3 experiments:
  1. **Validation replica**: Fine-tune LLaMA-3.2-1B on BoolQ with r=16, compare AdamW (lr=1e-4) vs. Stiefel (lr=0.3). Verify ~12-point gap reproduces.
  2. **Effective rank monitoring**: Log effective rank of BA every 100 steps. Confirm Stiefel maintains 16/16 while AdamW degrades to ~12/16.
  3. **Ablation on fixed A**: Replicate Table 4 scenario—freeze A at random initialization, train only B. Expect AdamW to match or exceed Stiefel, confirming co-adaptation requirement.

## Open Questions the Paper Calls Out
- How does Stiefel-LoRA perform on instruction-tuned (Instruct) models compared to the base LLaMA models tested in this study?
- How does the orthogonality constraint in Stiefel-LoRA affect the qualitative properties of generated text (coherence, diversity, factual accuracy)?
- What is the optimal initialization and co-adaptation strategy for matrices A and B in Stiefel-LoRA when both are trainable?
- Can Stiefel-LoRA's full rank utilization benefits be combined with adaptive rank allocation methods like AdaLoRA?

## Limitations
- The paper exclusively uses LLaMA series base models, omitting experiments on instruction-tuned models prevalent in practical LLM applications
- The study lacks qualitative analysis of generated text, focusing only on accuracy metrics without evaluating coherence, diversity, or factual consistency
- Implementation details such as exact B initialization strategy and Adam hyperparameters for the Stiefel component remain underspecified

## Confidence
- **High confidence**: The orthogonality constraint on B improves effective rank utilization (16/16 vs 12/16) - this follows directly from the mathematical structure of Stiefel manifold optimization and is supported by explicit measurements
- **Medium confidence**: Average 9-12% accuracy improvements over AdamW - while experimental results are presented across multiple datasets, implementation details (initialization, hyperparameters) could affect reproducibility
- **Low confidence**: Claims about orthogonality being "essential" when both matrices are trained - the paper shows benefits when both are trained but doesn't rigorously test scenarios where A alone might compensate for non-orthogonal B

## Next Checks
1. **Effective rank monitoring during training**: Log the effective rank of BA every 100 steps during training to verify that Stiefel optimization maintains full rank utilization (16/16) while AdamW degrades, confirming the core mechanism.

2. **Initialization sensitivity analysis**: Test multiple B initialization strategies (random orthogonal vs. projected random vs. warm-start) to determine whether initialization method affects convergence speed and final performance.

3. **Task-dependent orthogonality requirement**: Systematically evaluate Stiefel-LoRA vs AdamW on tasks known to require correlated features (e.g., certain image classification subtasks) to identify when orthogonality constraints may be detrimental.