---
ver: rpa2
title: 'Process vs. Outcome Reward: Which is Better for Agentic RAG Reinforcement
  Learning'
arxiv_id: '2505.14069'
source_url: https://arxiv.org/abs/2505.14069
tags:
- reasoning
- query
- evidence
- answer
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ReasonRAG, a process-supervised reinforcement
  learning method for improving agentic retrieval-augmented generation (RAG). The
  approach addresses challenges in existing outcome-supervised RL methods, including
  low exploration efficiency, gradient conflicts, and sparse rewards.
---

# Process vs. Outcome Reward: Which is Better for Agentic RAG Reinforcement Learning

## Quick Facts
- arXiv ID: 2505.14069
- Source URL: https://arxiv.org/abs/2505.14069
- Authors: Wenlin Zhang, Xiangyang Li, Kuicai Dong, Yichao Wang, Pengyue Jia, Xiaopeng Li, Yingyi Zhang, Derong Xu, Zhaocheng Du, Huifeng Guo, Ruiming Tang, Xiangyu Zhao
- Reference count: 40
- Primary result: ReasonRAG achieves SOTA on multi-hop reasoning with only 5k training instances vs. 90k for outcome-supervised methods

## Executive Summary
This paper introduces ReasonRAG, a process-supervised reinforcement learning method for agentic retrieval-augmented generation (RAG). The approach addresses challenges in outcome-supervised RL methods including low exploration efficiency, gradient conflicts, and sparse rewards. ReasonRAG uses Monte Carlo Tree Search to explore reasoning paths and Shortest Path Reward Estimation to provide fine-grained process-level rewards. Experiments on five benchmark datasets show ReasonRAG achieves superior performance using only 5k training instances compared to Search-R1 which requires 90k instances, with state-of-the-art results on multi-hop reasoning tasks.

## Method Summary
ReasonRAG introduces a three-stage agentic RAG framework (Reasoning, Grounding, Terminal) with Monte Carlo Tree Search exploration and Shortest Path Reward Estimation (SPRE) for process-level rewards. The method constructs RAG-ProGuide dataset by comparing sibling nodes in MCTS trees and optimizing the policy via Direct Preference Optimization (DPO) on preference pairs. The approach uses a Qwen2.5-7B-Instruct backbone and requires only 5k training instances versus 90k for outcome-supervised baselines. The framework automatically generates high-quality preference pairs by evaluating reasoning paths through Monte Carlo rollouts with step-based penalties.

## Key Results
- Achieves state-of-the-art performance on multi-hop reasoning tasks with 5k training instances
- Requires 18x less training data compared to outcome-supervised Search-R1 (90k instances)
- Outperforms existing agentic RAG methods on five benchmark datasets including HotpotQA and 2WikiMultiHopQA

## Why This Works (Mechanism)

### Mechanism 1: Shortest Path Reward Estimation (SPRE) for Dense Feedback
SPRE calculates step-level rewards by simulating rollouts to terminal states, averaging final correctness scores while applying decay penalties for longer reasoning chains. This provides fine-grained feedback that mitigates sparse rewards and gradient conflicts inherent in outcome-only supervision. The core assumption is that partial reasoning path validity can be approximated by average success of potential completions.

### Mechanism 2: Stage-Constrained MCTS for Agentic Exploration
MCTS efficiently navigates agentic RAG decision space by constraining actions to specific workflow stages (Reasoning, Grounding, Terminal). Upper Confidence Bound selection balances exploring new queries versus exploiting known evidence paths, structuring exploration and preventing incoherent sequences. The rigid state decomposition reduces search space while potentially limiting mixed operations.

### Mechanism 3: Preference Optimization via MCTS-Guided Data Construction
The system converts MCTS search paths into preference dataset (RAG-ProGuide) by comparing sibling nodes with higher SPRE rewards. DPO training maximizes likelihood of high-reward paths relative to lower-reward ones, distilling search process into policy weights. The assumption is that relative SPRE reward differences correlate with human preferences for reasoning quality.

## Foundational Learning

- **Concept: Direct Preference Optimization (DPO)**
  - Why needed: ReasonRAG uses DPO instead of standard PPO/RL to remove need for explicit reward model during training
  - Quick check: How does DPO handle the "rejected" response ($y_l$) in the loss function compared to standard Supervised Fine-Tuning (SFT)?

- **Concept: Monte Carlo Tree Search (MCTS)**
  - Why needed: ReasonRAG's performance depends entirely on MCTS's ability to explore high-reward paths
  - Quick check: In this paper, what acts as the "simulation" or "rollout" to determine the value of a leaf node during expansion?

- **Concept: Agentic RAG Loops**
  - Why needed: ReasonRAG imposes specific state machine (Reasoning → Search → Grounding) requiring understanding of stage separation
  - Quick check: According to Algorithm 1, what triggers the transition from the "Grounding" stage back to the "Reasoning" stage?

## Architecture Onboarding

- **Component map:** Data Generator (LLM + Search Engine + MCTS + SPRE) → Dataset (RAG-ProGuide) → Trainer (DPO) → Inference Engine (Agentic loop with Reasoning/Grounding stages)
- **Critical path:** SPRE calculation is the bottleneck, requiring multiple rollouts per node that can be computationally expensive
- **Design tradeoffs:** Data Efficiency vs. Compute Cost (5k vs 90k instances but expensive MCTS), Flexibility vs. Structure (rigid stages reduce search space but may limit mixed operations)
- **Failure signatures:** Stuck in Reasoning (infinite queries), Empty Evidence (repeated `<evidence>None</evidence>` outputs), Unreliable SPRE rewards from weak judge models
- **First 3 experiments:** 1) Verify SPRE reward correlation by checking 100 MCTS nodes manually, 2) Remove Grounding stage distinction and compare performance, 3) Train on subsets (1k, 2k, 3k pairs) to verify data efficiency claims

## Open Questions the Paper Calls Out
The paper identifies computational time cost of process-level data generation as a key limitation, noting that acquiring sufficient process-level annotations incurs higher time cost compared to outcome supervision during data rollout. The authors focus on training efficiency rather than optimizing MCTS exploration cost, leaving unresolved how to reduce high computational time while maintaining reward quality.

## Limitations
- High computational expense of SPRE requiring multiple LLM rollouts per node for dataset generation
- Reliance on judge model competence for accurate partial reasoning evaluation
- Rigid stage-based state definition may not generalize to tasks requiring interleaved reasoning and evidence extraction

## Confidence
- **High confidence**: Primary claim that process supervision requires less data than outcome supervision is well-supported by 5k vs 90k comparison
- **Medium confidence**: SPRE rewards specifically improve gradient flow is theoretically sound but depends on judge model competence
- **Medium confidence**: Stage decomposition is reasonable simplification but may not generalize to all agentic RAG tasks

## Next Checks
1. Extract 100 nodes from MCTS, compute their SPRE scores, and manually verify if higher scores correspond to shorter, correct reasoning paths
2. Remove explicit Grounding stage constraint and allow continuous query/answer generation in single loop; measure performance impact
3. Train ReasonRAG on subsets (1k, 2k, 3k pairs) and plot performance curves to confirm data efficiency advantage across different dataset sizes