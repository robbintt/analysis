---
ver: rpa2
title: Enhancing Speech Emotion Recognition with Graph-Based Multimodal Fusion and
  Prosodic Features for the Speech Emotion Recognition in Naturalistic Conditions
  Challenge at Interspeech 2025
arxiv_id: '2506.02088'
source_url: https://arxiv.org/abs/2506.02088
tags:
- speech
- emotion
- recognition
- fusion
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a multimodal speech emotion recognition system
  for the INTERSPEECH 2025 Speech Emotion Recognition in Naturalistic Conditions Challenge.
  The authors address the challenge of recognizing emotions in spontaneous, naturalistic
  speech where emotions are expressed subtly.
---

# Enhancing Speech Emotion Recognition with Graph-Based Multimodal Fusion and Prosodic Features for the Speech Emotion Recognition in Naturalistic Conditions Challenge at Interspeech 2025

## Quick Facts
- arXiv ID: 2506.02088
- Source URL: https://arxiv.org/abs/2506.02088
- Reference count: 0
- Macro F1-score of 39.79% on official test set, 42.20% on validation

## Executive Summary
This paper presents a multimodal speech emotion recognition system for the INTERSPEECH 2025 SER Challenge on naturalistic speech. The authors address the challenge of recognizing subtle emotions in spontaneous speech by combining state-of-the-art self-supervised speech models with text representations, enriched with prosodic and spectral features. Their approach uses Whisper Large V3 as the primary speech encoder, RoBERTa for text, quantized F0 embeddings for prosody, and Consistent Ensemble Distillation for spectral features. The system achieves a Macro F1-score of 39.79% on the official test set through a Graph Attention Network-based multimodal fusion strategy.

## Method Summary
The system employs pre-extracted features from five self-supervised speech models (Whisper Large V3, WavLM, HuBERT, Wav2Vec2, and XEUS) combined with text representations from RoBERTa. Prosodic features are extracted using RMVPE, mel-scaled, and quantized into 256 learnable embedding bins. Spectral features are processed through Consistent Ensemble Distillation (CED-Small). Multiple fusion strategies are explored including Graph Attention Networks (GATs), Hierarchical Cross Attention Models (HCAM), and simple concatenation. The best configuration uses MDAT with 8-head attention and GAT layers for multimodal fusion, followed by a SwiGLU-based MLP classifier. Training uses weighted cross-entropy loss, cosine learning rate scheduling, and ensemble majority voting across 4+ model variants.

## Key Results
- Macro F1-score of 39.79% on official test set (42.20% on validation)
- MDAT fusion with GAT layers achieved Macro F1 of 40.1% vs Simple fusion at 38.8%
- Whisper Large V3 outperformed Wav2Vec2 (17.8% F1) and HuBERT (27.4% F1) on this naturalistic dataset
- Ensemble of 4+ models improved performance by 1.1% F1 over best single model

## Why This Works (Mechanism)

### Mechanism 1
Graph Attention Network (GAT) fusion improves multimodal emotion recognition by dynamically weighting cross-modal dependencies. GAT layers assign learned attention weights to speech and text embedding nodes, enabling the model to adaptively emphasize modality-specific features while capturing cross-modal interactions. Unlike static concatenation, this allows the network to learn which features matter for each emotional context.

### Mechanism 2
Quantized F0 embeddings capture prosodic emotion cues more effectively than continuous F0 processing. Raw F0 contours are mel-scaled and discretized into 256 learnable embedding bins. This quantization maps continuous pitch variations to a finite vocabulary of prosodic patterns, allowing the model to learn prototypical emotional intonations as discrete tokens rather than raw signal statistics.

### Mechanism 3
Whisper's pre-training on diverse, spontaneous speech transfers better to naturalistic SER than SSL models trained primarily on read speech. Whisper's weak supervision on 680K hours of diverse audio (including noisy, conversational speech) produces representations robust to subtle, unpredictable emotional expressions in naturalistic conditions.

## Foundational Learning

- **Self-Supervised Speech Representations**: Why needed: The system relies on pre-extracted embeddings from SSL models; understanding what these encode is essential for debugging fusion failures. Quick check: Can you explain why Whisper's weak supervision differs from Wav2Vec2's contrastive learning, and what each captures?

- **Graph Attention Networks**: Why needed: MDAT's core fusion mechanism uses GAT layers to weight cross-modal nodes; understanding attention propagation helps diagnose which modalities dominate predictions. Quick check: Given speech and text node embeddings, how would a GAT layer compute updated node representations?

- **F0 (Fundamental Frequency) and Prosody**: Why needed: Quantized F0 is a key feature; understanding pitch extraction, voice/unvoiced detection, and mel-scaling is necessary to debug prosodic feature pipelines. Quick check: Why might F0 extraction fail on creaky voice or whispered speech, and how would this affect the quantized embedding?

## Architecture Onboarding

- **Component map**: Audio → Whisper embeddings → MDAT fusion (with text/F0/spectral) → SwiGLU classifier → emotion logits
- **Critical path**: The fusion module is the integration bottleneck, combining pre-extracted speech, text, prosodic, and spectral features
- **Design tradeoffs**: MDAT fusion (40.1% F1) vs Simple concatenation (38.8% F1): +1.3% F1 but significantly more parameters and complexity
- **Failure signatures**: Validation F1 < 0.30: Check SSL model selection (Wav2Vec2/HuBERT likely cause); Fusion F1 < Simple baseline: Overfitting in complex fusion
- **First 3 experiments**: 1) Unimodal baseline with Whisper embeddings only; 2) Fusion comparison: Simple concatenation vs MDAT with RoBERTa text; 3) Prosodic ablation: Add quantized F0 to best fusion config

## Open Questions the Paper Calls Out

### Open Question 1
What fusion strategies beyond GAT, HCAM, and simple concatenation could better capture nuanced emotional expressions in naturalistic speech? The authors plan to explore additional fusion strategies to further enhance cross-modal interactions and capture nuanced emotional expressions. Only three fusion approaches were tested; MDAT (GAT-based) performed best but simple concatenation surprisingly outperformed Transformer-based fusion and HCAM.

### Open Question 2
Why does simple concatenation outperform more complex fusion architectures like HCAM and Transformer-based fusion in this naturalistic SER setting? The authors note that simple fusion strategy outperformed more complex techniques, possibly due to increased susceptibility to overfitting with the more complex models given the limited dataset size. This hypothesis was not tested empirically.

### Open Question 3
Can end-to-end training frameworks match or exceed the performance of pre-extracted feature approaches while reducing computational overhead? The authors aim to investigate end-to-end training frameworks that may reduce computational overhead while improving model interpretability and generalization. All experiments used pre-extracted features from frozen SSL models for efficiency.

## Limitations
- Lack of detailed MDAT architecture specifications, including exact number of GAT layers and hidden dimensions
- No controlled experiments comparing Whisper's weak supervision advantage over Wav2Vec2/HuBERT on the same dataset
- Unspecified mel-scale range and bin distribution parameters for quantized F0 approach
- Ensemble strategy introduces significant computational overhead and potential overfitting to validation set

## Confidence

**High Confidence**: Whisper Large V3 outperforms Wav2Vec2 and HuBERT on this dataset (directly validated in Table 1); MDAT fusion improves over simple concatenation (validated in Table 3 with +1.3% F1); Quantized F0 provides modest but consistent improvement (validated in Table 3)

**Medium Confidence**: Prosodic features contribute meaningfully to emotion recognition (supported by quantized F0 results but not ablated separately); The computational overhead of GAT fusion is justified by performance gains (reasonable but depends on deployment constraints); Ensemble methods improve robustness (supported by reported results but potential overfitting concerns)

**Low Confidence**: Whisper's weak supervision specifically enables better naturalistic SER (plausible but not directly tested); 256 quantization bins are optimal for prosodic emotion representation (reasonable but not validated across datasets); The specific architectural choices in MDAT are optimal (no ablation study provided)

## Next Checks

1. **Controlled SSL Model Comparison**: Train Wav2Vec2 and HuBERT with the same fine-tuning procedure as Whisper on the MSP-Podcast dataset to isolate whether the performance gap is due to pre-training domain mismatch or fine-tuning differences.

2. **F0 Quantization Sensitivity Analysis**: Vary the number of quantization bins (64, 128, 256, 512) and mel-scale parameters to determine the optimal configuration for prosodic emotion representation.

3. **MDAT Architecture Ablation**: Implement and compare different GAT layer counts (1, 2, 3), attention head configurations, and cross-attention mechanisms against the simple fusion baseline to quantify the marginal benefit of each architectural complexity.