---
ver: rpa2
title: 'The Collective Turing Test: Large Language Models Can Generate Realistic Multi-User
  Discussions'
arxiv_id: '2511.08592'
source_url: https://arxiv.org/abs/2511.08592
tags:
- conversations
- social
- reddit
- comments
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study tests whether Large Language Models (LLMs) can convincingly
  simulate human-like conversations on social media using authentic Reddit data. Participants
  were shown side-by-side conversations, one human-generated and one LLM-generated,
  and asked to identify which was human.
---

# The Collective Turing Test: Large Language Models Can Generate Realistic Multi-User Discussions

## Quick Facts
- arXiv ID: 2511.08592
- Source URL: https://arxiv.org/abs/2511.08592
- Authors: Azza Bouleimen; Giordano De Marzo; Taehee Kim; Nicol`o Pagan; Hannah Metzler; Silvia Giordano; David Garcia
- Reference count: 40
- Primary result: LLM-generated conversations mistaken for human-created content 39% of the time

## Executive Summary
This study tests whether Large Language Models can convincingly simulate human-like conversations on social media using authentic Reddit data. Participants were shown side-by-side conversations, one human-generated and one LLM-generated, and asked to identify which was human. LLM-generated conversations were mistaken for human-created content 39% of the time. Llama 3 70B outperformed GPT-4o, with participants correctly identifying Llama 3 conversations as AI-generated only 56% of the time—barely better than random chance. These findings demonstrate that LLMs can generate realistic social media conversations, highlighting their potential for generative agent-based modeling while also raising concerns about misuse for generating inauthentic content.

## Method Summary
The study sampled 16 Reddit posts from December 2023 across 8 topics, extracting conversation tree structures and word counts from the first 20 comments per post. AI conversations were generated using Llama 3 70B and GPT-4o with three temperature settings (0.2, 0.7, 1.2), preserving original tree structure and word counts. A new LLM instance was created for each comment with cumulative conversation context. 203 participants completed 6 side-by-side comparisons each via the Potato annotation platform, with quality control including pre-screening and mid-task attention checks. Analysis employed multilevel logistic regression to account for the nested data structure, with success defined as correctly identifying the human conversation.

## Key Results
- LLM-generated conversations mistaken for human-created content 39% of the time
- Llama 3 70B outperformed GPT-4o, with only 56% detection accuracy (barely better than random)
- Conversation length showed inverse-U relationship, with detection accuracy peaking at 8 comments

## Why This Works (Mechanism)

### Mechanism 1: Training Corpus Alignment with Target Domain
Llama 3 70B generates more convincing social media discussions than GPT-4o due to training data that includes informal online dialogue. Models exposed to domain-specific corpora during pre-training produce stylistically aligned outputs without requiring explicit conditioning. The performance gap stems from training data composition rather than architectural differences. Evidence shows Llama 3's training data likely includes social media conversations, making outputs appear more human-like in informal online discussions.

### Mechanism 2: Safety Alignment Suppresses Naturalistic Discourse Markers
Commercial assistant-oriented safety training produces outputs that are detectably over-polite, formal, and conflict-averse. RLHF and safety fine-tuning systematically reduce profanity, emotional expression, and controversial stances that characterize authentic social media. The observed stylistic differences originate from alignment procedures rather than inherent model limitations in generating informal text. Qualitative analysis revealed participants relied on cues like overly polite tone, lack of slang or profanity, and generic content to distinguish AI conversations.

### Mechanism 3: Context Accumulation Creates Detection Opportunities (Inverted-U Pattern)
Detection accuracy peaks at moderate conversation lengths (~8 comments) because longer threads provide more signal but also induce reader fatigue. Short conversations lack sufficient AI signature samples; moderate lengths expose inconsistencies; very long threads may cause cognitive overload or conversational drift masking. The non-linear relationship reflects human cognitive processing limits rather than model degradation at longer contexts. Length generally has no significant effect on success rates, except for length = 8, which is associated with a significantly higher success probability.

## Foundational Learning

- **Concept: Agent-Based Modeling (ABM) Limitations**
  - Why needed here: The paper positions LLM-based generative agents as a solution to ABM's data scarcity and behavioral oversimplification problems
  - Quick check question: What two core limitations of traditional ABM do LLM-based agents potentially address?

- **Concept: Turing Test Threshold Interpretation**
  - Why needed here: The study references Turing's 30% deception threshold; understanding this benchmark is essential for interpreting the 39% misclassification rate
  - Quick check question: What does the 30% threshold represent in Turing's original imitation game formulation?

- **Concept: Multilevel Logistic Regression for Nested Data**
  - Why needed here: The analysis accounts for participants contributing multiple annotations; ignoring this clustering would inflate Type I error
  - Quick check question: Why is varying intercepts for participants necessary when each person completes 6 annotations?

## Architecture Onboarding

- **Component map**: Pushshift Reddit API -> Dec 2023 posts (post-cutoff) -> conversation tree extraction -> anonymization -> Llama 3 70B/GPT-4o -> identical prompts (no personas) -> structure-preserving word-count matching -> 3 temperature settings -> Potato annotation framework -> custom sampling (no repeat posts per participant) -> side-by-side HTML rendering -> binary judgment + confidence + qualitative explanation per trial -> R/lme4 multilevel logistic regression -> qualitative coding with Cohen's Kappa inter-rater agreement

- **Critical path**: 1. Sample 16 Reddit posts from Dec 2023 across 8 topics 2. Extract conversation tree structure and word counts per comment 3. Generate AI replacements using new LLM instance per comment with conversation history as context 4. Render paired conversations with identical usernames, avatars, structure 5. Collect binary judgment + confidence + qualitative explanation per trial 6. Fit multilevel model: success ~ model + length + (1|participant)

- **Design tradeoffs**: Minimal prompting vs. persona-based (chose generic "Reddit user" baseline); Structure preservation vs. natural generation (fixed tree structure and word counts); Fresh instance per comment vs. persistent agent (new instance each turn prevents agent drift); Temperature variation (0.2/0.7/1.2) included but not analyzed as separate factor

- **Failure signatures**: 46/251 participants (18%) failed attention checks and were excluded; Cohen's Kappa ranged 0.00-1.00 across subcategories; Length effect only significant at one level (8 comments, p=0.0018); Qualitative analysis restricted to high-performers (success ≥0.8, n=58)

- **First 3 experiments**: 1. Persona-driven prompting: Assign distinct user profiles to each LLM agent; test if individual identity coherence improves authenticity scores 2. Cross-model validation: Add Claude and Mistral to benchmark whether Llama 3's advantage generalizes 3. Contentious topic stress test: Sample from subreddits with high conflict/toxicity to evaluate whether politeness ceiling limits simulation validity

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** Does integrating distinct, persona-driven prompting for each agent improve the authenticity of simulated multi-user discussions, specifically regarding personal storytelling and controversial discourse?
**Basis in paper:** The Conclusion suggests using multiple persona-driven LLM agents to assess whether this setup improves authenticity, fosters personal storytelling, or generates more emotionally intense or controversial discourse.
**Why unresolved:** This study utilized a "minimal prompting strategy" where agents were simply asked to behave like Reddit users without assigned personas, resulting in a noted lack of authenticity and excessive agreement.
**What evidence would resolve it:** A follow-up experiment comparing the detection rates of persona-driven simulations against the generic prompting approach, specifically analyzing qualitative metrics for personal anecdotes and disagreement.

### Open Question 2
**Question:** Is the decline in detection accuracy for very long conversation threads (e.g., 16 comments) caused by participant fatigue or by increased natural variance in the AI text?
**Basis in paper:** The Discussion notes an inverse-U relationship in detection success and states, "longer conversations... may lead to participant fatigue. Both hypotheses warrant further investigation."
**Why unresolved:** While the study found detection success peaked at length 8 and dropped at length 16, the statistical analysis could not determine the mechanism behind this non-linear pattern.
**What evidence would resolve it:** Data on time-spent-per-comment or eye-tracking metrics to measure participant fatigue, alongside computational analysis of text variance in longer AI threads.

### Open Question 3
**Question:** Can LLMs be effectively calibrated to simulate toxic or emotionally charged behaviors necessary for studying online conflict and misinformation?
**Basis in paper:** The Discussion highlights a limitation where "current LLMs struggle to replicate emotionally charged discourse... factors that are central to many social media phenomena," noting that AI conversations were overly polite.
**Why unresolved:** Safety alignment in current models suppresses the profanity, slang, and hostility inherent in authentic social media conflict, limiting the validity of simulations for polarization research.
**What evidence would resolve it:** Testing "uncensored" or specifically fine-tuned models within the Collective Turing Test framework to measure if they can mimic toxic realism without triggering safety refusals.

## Limitations

- The mechanism attributing Llama 3's superior performance to social media training data alignment remains speculative without direct training data analysis
- The safety alignment hypothesis lacks experimental validation through ablation studies comparing base versus safety-tuned models
- The inverted-U relationship for conversation length shows statistical significance only at one level (8 comments), with high uncertainty in categorical analysis

## Confidence

- **High**: The core finding that LLMs generate conversations mistaken for human content 39% of the time, based on robust experimental design with attention checks and multilevel modeling
- **Medium**: The comparative performance of Llama 3 versus GPT-4o, supported by statistical significance but lacking mechanistic explanation
- **Low**: The attribution of performance differences to training data composition and safety alignment effects, which are speculative without direct evidence

## Next Checks

1. Conduct ablation studies comparing base versus safety-aligned versions of the same model family to isolate the impact of alignment on detection rates
2. Perform training data analysis to verify whether Llama 3's corpus includes social media content relative to GPT-4o, testing the domain alignment hypothesis directly
3. Replicate the length effect analysis with expanded sample sizes across all conversation lengths to confirm whether the inverted-U pattern generalizes beyond the single significant data point