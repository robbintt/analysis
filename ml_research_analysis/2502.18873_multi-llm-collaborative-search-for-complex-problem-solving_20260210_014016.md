---
ver: rpa2
title: Multi-LLM Collaborative Search for Complex Problem Solving
arxiv_id: '2502.18873'
source_url: https://arxiv.org/abs/2502.18873
tags:
- search
- reasoning
- mosa
- llms
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Mixture-of-Search-Agents (MoSA) paradigm,
  which uses multiple LLMs to enhance search-based reasoning by combining independent
  exploration with iterative refinement. The approach integrates diverse reasoning
  pathways and employs Monte Carlo Tree Search as a backbone, with multiple agents
  proposing and aggregating reasoning steps.
---

# Multi-LLM Collaborative Search for Complex Problem Solving

## Quick Facts
- arXiv ID: 2502.18873
- Source URL: https://arxiv.org/abs/2502.18873
- Reference count: 40
- Key outcome: Introduces MoSA paradigm using multiple LLMs for search-based reasoning, achieving 1.71% average accuracy improvement over single-agent and other multi-agent baselines across four reasoning benchmarks

## Executive Summary
This paper addresses the challenge of complex reasoning in LLMs by introducing the Mixture-of-Search-Agents (MoSA) paradigm. MoSA enhances search-based reasoning by combining independent exploration from multiple LLMs with iterative refinement through neural aggregation. The approach integrates diverse reasoning pathways and employs Monte Carlo Tree Search as a backbone, with multiple agents proposing and aggregating reasoning steps. Comprehensive evaluations across four reasoning benchmarks demonstrate that MoSA consistently outperforms single-agent and other multi-agent baselines, particularly excelling in complex mathematical and commonsense reasoning tasks.

## Method Summary
MoSA implements a multi-agent collaborative search framework built on Monte Carlo Tree Search. The method employs multiple LLMs as both proposers (generating sub-questions and candidate sub-answers) and aggregators (synthesizing candidate answers via neural refinement). During search, the system uniformly samples from the LLM pool for each generation task, collects multiple candidate sub-answers per sub-question, and uses neural aggregators to refine answers before majority voting. The approach aims to overcome the diversity-quality tradeoff in single-LLM sampling by leveraging inherent diversity from different model distributions rather than temperature tuning.

## Key Results
- MoSA achieves 79.97% average accuracy across four reasoning benchmarks, outperforming single-agent baselines
- Proposers contribute -1.23% improvement when ablated, while aggregators contribute -0.47% improvement
- The synergy between search and collaboration shows larger gains than either approach independently (1.44% vs 0.62% improvement gap)
- Performance degrades when increasing LLM count from 3 to 4 on MATH-500 benchmark

## Why This Works (Mechanism)

### Mechanism 1: Intrinsic Diversity from Heterogeneous Model Distributions
Different LLMs trained on distinct corpora/architectures produce inherently different reasoning distributions. This replaces manual temperature tuning with structural diversity—each model naturally explores different regions of the reasoning space without quality degradation from excessive randomness. The diversity-quality tradeoff that plagues single-model sampling is mitigated when diversity comes from model heterogeneity rather than sampling stochasticity.

### Mechanism 2: Neural Aggregation as Error Correction Layer
LLM-based aggregation improves upon majority voting by enabling cross-pollination of correct partial solutions. When candidate sub-answers include both correct and incorrect responses, aggregator LLMs read all candidates and synthesize. Even a "bad" aggregator receiving at least one good candidate can produce an answer better than its original output—effectively amplifying correct signals before final voting.

### Mechanism 3: Synergistic Amplification of Search and Collaboration
Combining multi-agent collaboration with search-based reasoning yields larger gains than either approach independently. Search provides structured exploration of reasoning trajectories; multi-agent collaboration provides diverse trajectory proposals. The interaction creates compounding benefits—better proposals lead to better search paths, better search finds higher-value regions for collaboration to exploit.

## Foundational Learning

- **Monte Carlo Tree Search (MCTS)**
  - Why needed: MCTS is the backbone search algorithm. You must understand Selection (UCT), Expansion, Simulation, and Backpropagation to debug trajectory quality and reward propagation.
  - Quick check: Given a node with Q(s,a)=3, N(s,a)=5, parent visits=20, and c=1.0, calculate its UCT score.

- **Sampling Diversity Tradeoffs (temperature, top-k, top-p)**
  - Why needed: The paper positions MoSA as solving the diversity-quality tradeoff. You need to understand what temperature manipulation does to understand what MoSA replaces.
  - Quick check: What happens to token distribution variance as temperature increases from 0.5 to 1.5?

- **Self-Consistency and Majority Voting**
  - Why needed: The baseline aggregation method; understanding its failure modes (incorrect consensus) motivates the neural aggregator design.
  - Quick check: If 3 LLMs propose answers [A, B, B] where A is correct, what does majority voting return?

## Architecture Onboarding

- **Component map**: Selection via UCT -> GenerateActions (sub-question + candidate sub-answers) -> Expand tree -> Simulate to terminal -> Backpropagate reward -> Neural aggregation -> Majority voting

- **Critical path**: 
  1. Selection via UCT → choose node s_i
  2. GenerateActions (Algorithm 1): SelectLLM for sub-question → GenerateSubQuestion → SelectLLM × n_a for candidate sub-answers → FinalizeSubAnswer (aggregate + vote)
  3. Expand tree with new actions
  4. Simulate to terminal, backpropagate reward

- **Design tradeoffs**: 
  - More LLMs = higher diversity but increased latency/cost (paper shows 3→4 can degrade on some benchmarks)
  - Aggregators add computation but contribute ~0.5% improvement vs ~1.2% from proposers
  - Extended action set (A1-A5) helps math tasks but hurt StrategyQA

- **Failure signatures**:
  - Low accuracy on simpler benchmarks despite high complexity → over-engineering, reduce search depth
  - Aggregator outputs worse than majority voting alone → aggregator LLM lacks task competence
  - Performance plateaus at 2-3 LLMs → diminishing returns from additional model heterogeneity

- **First 3 experiments**:
  1. Reproduce single-LLM diversity curve: Run RAP + Single Aggregator across temperatures {0.25, 0.5, 0.75, 1.0, 1.25} on MATH-500 to confirm the diversity-accuracy inverted-U pattern (Figure 4).
  2. Ablate proposer count: Test MoSA with 1, 2, 3, 4 distinct LLMs (holding total forward calls constant) to verify Figure 5 scaling pattern on your target benchmark.
  3. Isolate aggregator contribution: Compare MoSA as Proposers-only vs Proposers+Aggregators to quantify aggregation value for your specific LLM pool.

## Open Questions the Paper Calls Out

- **Open Question 1**: Why does increasing the number of distinct LLMs from 3 to 4 cause a slight decrease in performance on the MATH-500 benchmark? The paper observes this trend but does not isolate whether the drop is model-specific or a systemic scaling issue.

- **Open Question 2**: Why does enriching the action set degrade performance on the StrategyQA benchmark compared to the standard action set? The authors confirm the observation but don't explain whether additional actions introduce unnecessary complexity in commonsense reasoning.

- **Open Question 3**: To what extent is the performance improvement dependent on the specific selection of the most capable models in the initial agent pool? It's unclear if the synergy relies heavily on the baseline competence of all participants or if it's robust to low-quality agents.

- **Open Question 4**: How does the computational overhead of the neural aggregation step scale compared to heuristic majority voting in large-scale search spaces? The efficiency trade-off is critical for practical deployment but remains unquantified.

## Limitations

- Performance improvements are relatively modest (1.71% average), with aggregation mechanism contributing less than claimed
- Limited number of datasets (4 benchmarks) and specific LLM pool (4 models) constrain generalizability
- Paper doesn't address computational cost trade-offs or latency implications of multi-LLM deployment

## Confidence

- **High confidence**: MoSA's basic architecture and performance on MATH-500
- **Medium confidence**: The diversity-quality tradeoff mechanism
- **Medium confidence**: Aggregation layer contribution
- **Medium confidence**: Synergy claims between search and collaboration

## Next Checks

1. Test MoSA with heterogeneous LLM pools (varying sizes, architectures, training data) to verify diversity gains aren't dataset-specific or dependent on particular model combinations.
2. Conduct ablation studies isolating search depth vs. LLM count to quantify whether performance gains come from better exploration or simply more computation.
3. Evaluate on additional reasoning benchmarks including commonsense QA, code generation, and multi-hop reasoning tasks to assess generalizability beyond mathematical problem solving.