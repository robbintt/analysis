---
ver: rpa2
title: Data- and Variance-dependent Regret Bounds for Online Tabular MDPs
arxiv_id: '2602.01903'
source_url: https://arxiv.org/abs/2602.01903
tags:
- theorem
- regret
- bounds
- stochastic
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of designing algorithms for online
  tabular Markov decision processes (MDPs) that achieve refined data-dependent regret
  bounds in both adversarial and stochastic regimes. The authors develop best-of-both-worlds
  algorithms based on global optimization and policy optimization using optimistic
  follow-the-regularized-leader with log-barrier regularization.
---

# Data- and Variance-dependent Regret Bounds for Online Tabular MDPs

## Quick Facts
- arXiv ID: 2602.01903
- Source URL: https://arxiv.org/abs/2602.01903
- Reference count: 40
- Primary result: Algorithms achieving data-dependent regret bounds (first-order, second-order, path-length) in adversarial regimes and variance-aware gap-independent/dependent bounds in stochastic regimes for online tabular MDPs

## Executive Summary
This paper develops best-of-both-worlds algorithms for online tabular Markov decision processes that achieve refined data-dependent regret bounds across both adversarial and stochastic regimes. The authors build on optimistic follow-the-regularized-leader (OFTRL) with log-barrier regularization, implementing both global optimization over occupancy measures and per-state policy optimization. Their key innovation is introducing data-dependent complexity measures (second-order quantity and path-length) for adversarial settings and variance-based measures for stochastic settings. The algorithms automatically adapt their regret bounds to the actual difficulty of the environment without prior knowledge of the regime.

## Method Summary
The authors develop algorithms based on optimistic follow-the-regularized-leader with log-barrier regularization for online tabular MDPs with known transitions. They implement two approaches: global optimization operating directly on occupancy measures, and policy optimization with per-state updates. Both use adaptive learning rates updated via data-dependent terms ζ_t(s,a), and two predictor schemes (gradient descent and empirical mean) to enable different types of adaptivity. The global optimization achieves tighter bounds but requires solving convex optimization, while policy optimization is more scalable but has additional H factors.

## Key Results
- First-order and second-order regret bounds in adversarial regimes: O(√(SAQ_∞)) and O(√(SAmin{L^*, Q_∞}))
- Path-length regret bound in adversarial regimes: O(√(SAH²V₁))
- Variance-aware gap-independent bound in stochastic regimes: O(√(SAVT)) when T > H
- Variance-aware gap-dependent bound in stochastic regimes: O(√(SAT log(SA/ε))) when T < H

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: OFTRL with log-barrier regularization enables simultaneous data-dependent adaptivity in adversarial settings and variance-aware bounds in stochastic settings.
- **Mechanism**: The log-barrier regularizer provides negative curvature that stabilizes updates when predictions are accurate while maintaining robustness when they fail. The regularizer form ψ_t(q) = Σ_{s,a} (1/η_t(s,a)) log(1/q(s,a)) creates state-action-wise learning rates that adapt to local complexity.
- **Core assumption**: Known transition dynamics P; losses bounded in [0,1].
- **Evidence anchors**:
  - [Abstract]: "develop algorithms based on global optimization and policy optimization, both built on optimistic follow-the-regularized-leader with log-barrier regularization"
  - [Section 4.1]: Describes OFTRL over occupancy measures with time-varying state-action-wise log-barrier regularizer and adaptive learning rates
  - [Corpus]: Related work on best-of-both-worlds algorithms (e.g., "Efficient Best-of-Both-Worlds Algorithms for Contextual Combinatorial Semi-Bandits") uses FTRL with entropy regularizers for similar adaptivity goals, but log-barrier is distinct here.
- **Break condition**: If transitions are unknown, the analysis no longer holds directly; additional estimation error terms would dominate.

### Mechanism 2
- **Claim**: Adaptive learning rates updated via data-dependent terms (ζ_t(s,a)) balance penalty and stability terms in OFTRL analysis.
- **Mechanism**: The update 1/η_{t+1}(s,a) = 1/η_t(s,a) + η_t(s,a)ζ_t(s,a)/(log(T)) ensures that when accumulated ζ_t(s,a) is small (low local variance/fluctuation), η_t stays large, allowing faster adaptation. Conversely, high ζ_t shrinks η_t, providing stability.
- **Core assumption**: The data-dependent term ζ_t(s,a) = q^{π_t}(s,a)^2 min{(ℓ̂_t - m_t)^2, (ℓ̂_t + g_t - m_t)^2} captures local complexity; this requires occupancy measures to be well-defined and estimators to be bounded.
- **Evidence anchors**:
  - [Section 4.1]: Learning rates updated via data-dependent ζ_t(s,a)
  - [Appendix D.3, Lemma D.7]: Shows η_t(s,a) ≤ √(log(T)) / √(2H²log(T) + Σ_{τ≤t} ζ_τ(s,a)), linking learning rate to accumulated complexity
  - [Corpus]: Weak direct evidence; adaptive learning rates in bandit/MDP literature often use similar principles but not this specific construction.
- **Break condition**: If ζ_t(s,a) is misspecified (e.g., due to poor loss predictors), learning rates may not adapt correctly, degrading bounds to worst-case.

### Mechanism 3
- **Claim**: Loss-shifting with predictors m_t enables path-length bounds in adversarial regimes and variance-aware gap-dependent bounds in stochastic regimes.
- **Mechanism**: Two predictor schemes: (1) gradient descent-based update m_{t+1} = (1-ξ)m_t + ξℓ_t for path-length bounds, and (2) empirical mean predictor m_t = Σ_{τ<t} I_τ(s,a)ℓ_τ(s,a) / max{1, N_{t-1}(s,a)} for variance-aware bounds. The shifting function g_t(s,a) = Q^{π_t}(s,a; ℓ̃_t) - V^{π_t}(s; ℓ̃_t) - ℓ̃_t(s,a) makes regret self-bounding in stochastic regimes.
- **Core assumption**: In stochastic regime with corruption, uncorrupted losses are i.i.d. with known mean structure; path-length bound assumes losses change slowly enough that gradient descent predictor tracks them.
- **Evidence anchors**:
  - [Section 5.1]: Describes two predictor schemes and their roles in different bounds
  - [Lemma D.1 and D.2]: Loss-shifting invariance property enabling self-bounding analysis
  - [Corpus]: Similar predictor-based approaches appear in "Adapting to Stochastic and Adversarial Losses in Episodic MDPs with Aggregate Bandit Feedback" for adaptivity, but specific constructions differ.
- **Break condition**: If corruption C is too large (Ω(T)), stochastic bounds degrade to adversarial; if losses are adversarial, path-length bound may not hold (depends on V_1 being small).

## Foundational Learning

- **Concept**: **Optimistic Follow-the-Regularized-Leader (OFTRL)**
  - Why needed here: Core optimization framework for both global and policy optimization approaches.
  - Quick check question: Can you explain how OFTRL differs from standard FTRL and why optimism (loss predictions m_t) improves regret when predictions are accurate?

- **Concept**: **Occupancy Measures and MDP Duality**
  - Why needed here: Global optimization operates directly on occupancy measures q^π ∈ Ω(P); regret is expressed as Σ_t ⟨q^{π_t} - q^*, ℓ_t⟩.
  - Quick check question: Given a policy π and known transitions P, can you compute the occupancy measure q^π(s,a) for all state-action pairs?

- **Concept**: **Bregman Divergence and Stability in Online Learning**
  - Why needed here: OFTRL regret bound contains terms involving D_{ψ_t}(p_{t+1}, p_t); log-barrier regularizer ensures these are controlled.
  - Quick check question: For log-barrier regularizer ψ(x) = log(1/x), what is the Bregman divergence D_ψ(y,x)?

## Architecture Onboarding

- **Component map**:
  - OFTRL Core -> Regularizer Module -> Predictor Module -> Loss Estimator -> Learning Rate Adapter -> Shifting Function (stochastic) -> Dilated Bonus (policy optimization)

- **Critical path**:
  1. At episode t, compute predictor m_t for all (s,a) using chosen scheme
  2. Run OFTRL to get q^{π_t} (global) or π_t(·|s) per state (policy optimization)
  3. Execute policy, observe trajectory, compute loss estimator ℓ̂_t
  4. Update learning rates η_{t+1}(s,a) and predictor m_{t+1}(s,a)

- **Design tradeoffs**:
  - **Global vs. Policy Optimization**: Global achieves tighter bounds (no H factor) but requires solving convex optimization over Ω(P) each episode; policy optimization is more scalable but bounds are H-times looser.
  - **Predictor Choice**: Gradient descent predictor enables V_1 path-length bounds but not gap-dependent stochastic bounds; empirical mean predictor enables variance-aware gap-dependent bounds but not V_1 bounds.
  - **Virtual Episodes (Policy Optimization)**: Introduced to enforce η_t(s,a)π_t(a|s)B_t(s,a) ≲ 1/H condition; adds O(HSA log²T) virtual episodes, absorbed into lower-order terms.

- **Failure signatures**:
  - If regret degrades to worst-case O(√(HSAT)) despite benign environment: likely predictor m_t is not tracking well (check m_t vs. ℓ_t) or learning rate η_t is not adapting (check if ζ_t(s,a) accumulation is correct).
  - If stochastic regime bounds fail to achieve O(log T) or O(√(VT)): check if loss-shifting function g_t is correctly computed; ensure corruption budget C is properly accounted for.
  - If policy optimization performs significantly worse than global: check if dilated bonus B_t(s,a) is computed correctly via recursive Eq. (14); verify virtual episode handling.

- **First 3 experiments**:
  1. **Sanity check on simple MDP**: Implement global optimization on a 2-state, 2-action MDP with known transitions. Use gradient descent predictor. Verify that in adversarial regime with slowly varying losses, regret scales with √(SAV_1) rather than √(HSAT).
  2. **Variance-aware validation**: Use empirical mean predictor on a stochastic MDP with varying per-state-action variances. Confirm that gap-dependent bound scales with V^c(s) rather than worst-case H², and that increasing variance for some (s,a) increases regret proportionally.
  3. **Best-of-both-worlds test**: Run the same algorithm (with empirical mean predictor) on both adversarial and stochastic instances of the same MDP. Verify that regret automatically adapts: O(√(SAmin{L^*, Q_∞})) in adversarial case and O(√(SAVT)) or polylog(T) in stochastic case, without prior knowledge of regime.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can data- and variance-dependent regret bounds be extended to the online MDP setting with unknown transitions?
- **Basis in paper**: [explicit] Page 2 states that for unknown transitions, "extending these techniques to refined data-dependent guarantees or to the best-of-both-worlds setting remains open."
- **Why unresolved**: The current analysis relies on known transitions; controlling transition-estimation error in a data-dependent manner (necessary for unknown transitions) presents a significant challenge not addressed by the current methods.
- **What evidence would resolve it**: An algorithm for unknown transitions that achieves first-order, second-order, or path-length regret bounds with a corresponding proof.

### Open Question 2
- **Question**: Can the multiplicative factor of the episode horizon $H$ in policy optimization regret bounds be removed?
- **Basis in paper**: [explicit] Page 11 notes that policy optimization bounds are worse by a factor of $H$ compared to global optimization and states, "Closing this $H$-gap in minimax regret remains an important open problem."
- **Why unresolved**: Policy optimization typically performs local updates at each state, which introduces an inherent dependence on the horizon $H$ that current techniques have not eliminated.
- **What evidence would resolve it**: A policy optimization algorithm achieving a regret bound matching the global optimization rate (without the extra $H$ factor).

### Open Question 3
- **Question**: Is the path-length regret bound optimal, and can the $\sqrt{SA/H}$ gap between the derived upper and lower bounds be closed?
- **Basis in paper**: [explicit] Page 11 states that for path-length bounds, "our upper bound leaves an $\sqrt{SA/H}$-dependent gap" compared to the lower bounds.
- **Why unresolved**: The current lower bound construction does not preclude the $\sqrt{SA/H}$ dependence, and the upper bound analysis leaves this specific gap.
- **What evidence would resolve it**: A refined lower bound matching the $\sqrt{SA/H}$ dependence, or an improved algorithm/analysis that removes this term from the upper bound.

## Limitations
- The algorithms require known transition dynamics P, which is a significant limitation compared to model-free settings.
- The virtual episode mechanism in policy optimization adds algorithmic complexity and implementation challenges.
- Lower bound proofs assume uniform transitions, potentially limiting their generality to arbitrary MDP structures.

## Confidence

**High Confidence**: Regret bounds for global optimization approach (both adversarial and stochastic regimes), as these follow established OFTRL analysis with explicit tracking of data-dependent terms.

**Medium Confidence**: Policy optimization regret bounds, due to the additional H factor and complexity of virtual episode mechanism which may introduce implementation challenges.

**Medium Confidence**: Variance-aware gap-dependent bounds, as these depend on the empirical mean predictor's ability to accurately estimate loss means in the stochastic regime with potential corruption.

## Next Checks

1. Implement the global optimization algorithm on a synthetic 2-state MDP and verify that regret scales with √(SAV₁) in the adversarial regime with slowly varying losses, as opposed to the worst-case √(HSAT).

2. Test the empirical mean predictor on a stochastic MDP with heterogeneous per-state-action variances; confirm that regret scales with V^c(s) rather than worst-case H², and validate the relationship between variance magnitude and regret.

3. Run both global and policy optimization versions on identical MDPs and compare empirical regret; verify that the H-factor degradation in policy optimization is consistent with theoretical predictions, and check if virtual episode count remains within O(HSA log²T) as claimed.