---
ver: rpa2
title: Evaluating and Mitigating Bias in AI-Based Medical Text Generation
arxiv_id: '2504.17279'
source_url: https://arxiv.org/abs/2504.17279
tags:
- generation
- text
- groups
- performance
- fairness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses fairness issues in AI-based medical text generation,
  focusing on performance disparities across demographic groups in radiology report
  generation, report summarization, and paper summarization tasks. The authors identify
  significant biases affecting different races, sexes, and age groups, with intersectional
  subgroups experiencing compounded biases.
---

# Evaluating and Mitigating Bias in AI-Based Medical Text Generation

## Quick Facts
- **arXiv ID**: 2504.17279
- **Source URL**: https://arxiv.org/abs/2504.17279
- **Reference count**: 40
- **Primary result**: Selective optimization framework reduces demographic disparities by >30% in medical text generation while maintaining performance

## Executive Summary
This paper addresses fairness issues in AI-based medical text generation across three tasks: radiology report generation, report summarization, and paper summarization. The authors identify significant performance disparities affecting different races, sexes, and age groups, with intersectional subgroups experiencing compounded biases. To mitigate these issues, they propose a selective optimization framework that prioritizes cases with higher cross-entropy loss and integrates ranking loss to ensure pathology accuracy. Their approach significantly reduces unfairness across various metrics and model scales without compromising overall performance, achieving more than 30% reduction in disparities while maintaining relative accuracy changes within 2%.

## Method Summary
The paper proposes a selective optimization framework for mitigating bias in medical text generation. The method works by identifying and prioritizing training examples that exhibit higher cross-entropy loss during training, focusing the model's attention on challenging cases that contribute to fairness disparities. Additionally, a ranking loss component ensures pathology accuracy by training the model to correctly rank pre-generated candidate summaries by quality. The approach is evaluated across three tasks using MIMIC-CXR for radiology data and PubMed for paper summarization, with performance measured using ROUGE metrics for text generation quality and CheXpert for pathology detection accuracy. Fairness is quantified using Metric-aware Fairness Difference (MFD), which measures the mean absolute metric difference between demographic subgroups.

## Key Results
- Achieved >30% reduction in demographic performance disparities across race, sex, and age groups
- Maintained overall performance with relative accuracy changes within 2% of baseline
- Demonstrated effectiveness across multiple backbones (R2Gen, BART, LLaMA2) and datasets (MIMIC-CXR, PubMed)
- Showed consistent fairness improvements across different model scales and modalities

## Why This Works (Mechanism)
The selective optimization framework works by strategically redistributing training focus toward underrepresented or high-loss cases that contribute to fairness disparities. By computing cross-entropy loss for each training example and selecting the top-γ cases per batch, the model receives additional training on examples where it performs poorly, particularly for marginalized demographic groups. The ranking loss component provides additional grounding by ensuring the model learns to distinguish between high and low-quality candidate generations based on established metrics like ROUGE and CheXpert scores. This dual approach addresses both the optimization imbalance that leads to bias and the quality consistency needed for clinical reliability.

## Foundational Learning
- **Metric-aware Fairness Difference (MFD)**: A metric measuring mean absolute difference in performance between demographic subgroups; needed to quantify fairness disparities and track mitigation effectiveness
- **CheXpert labeler**: A radiology-specific evaluation tool that extracts and evaluates medical findings from generated reports; needed to ensure pathology accuracy isn't sacrificed for fairness
- **Selective optimization**: Training strategy that prioritizes high-loss examples; needed to focus model updates on cases contributing to bias
- **Ranking loss in text generation**: Loss function that trains models to correctly order candidate summaries by quality; needed to maintain clinical accuracy while improving fairness
- **Intersectional subgroup analysis**: Evaluation approach considering combined demographic attributes (e.g., elderly Black females); needed to identify compounded bias effects
- **Cross-entropy loss prioritization**: Method of selecting training examples based on prediction uncertainty; needed to identify cases where model struggles most

## Architecture Onboarding

**Component Map**
Input Data -> Preprocessor -> Selective Optimization Module -> Model Backbone -> Candidate Generator -> Ranking Loss Calculator -> Output Generator

**Critical Path**
The critical path flows from input data through preprocessing, where demographic metadata is aligned, to the selective optimization module that computes losses and selects cases, then through the model backbone for training, with the candidate generator and ranking loss calculator providing the quality signals for the ranking component.

**Design Tradeoffs**
The framework balances fairness improvement against performance maintenance by using selective optimization rather than uniform retraining, which could degrade overall accuracy. The ranking loss introduces additional computational overhead but provides crucial quality grounding. The selection ratio γ represents a key hyperparameter tradeoff between fairness gains and training efficiency.

**Failure Signatures**
Failure mode 1: No fairness improvement occurs when γ is too high (near 1), making selection nearly uniform, or when candidate summaries lack diversity and representativeness. Failure mode 2: CheXpert scores become unstable or undefined when the labeler fails to process generated text correctly or when radiology examples lack valid reference labels.

**3 First Experiments**
1. Baseline run: Train model without selective optimization to establish performance and fairness baselines across all subgroups
2. Sensitivity analysis: Test selective optimization with varying γ values (0.1, 0.3, 0.5, 0.7) to identify optimal selection ratio
3. Candidate diversity test: Generate candidate summaries using different base models and quantities to assess impact on ranking loss effectiveness

## Open Questions the Paper Calls Out
**Open Question 1**: To what extent are the observed performance disparities caused by biological differences in disease prevalence versus systemic inequities in healthcare quality documented in the training data? The paper correlates performance with disease prevalence but explicitly states this needs further exploration, particularly regarding healthcare quality disparities.

**Open Question 2**: Does a reduction in Metric-aware Fairness Difference (MFD) directly correlate with a reduction in clinically significant diagnostic errors for under-served populations? The paper relies on proxy metrics rather than measuring actual clinical outcomes, leaving a gap between statistical fairness and clinical utility.

**Open Question 3**: How sensitive is the ranking loss component to the quality and diversity of the pre-generated candidate summaries used during training? The paper doesn't analyze how biased or non-diverse candidates might limit the fairness improvements achievable through the ranking loss mechanism.

## Limitations
- Unknown selection ratio γ creates uncertainty about optimal hyperparameter settings
- Missing details on candidate generation protocol (number, source model, ranking margins)
- Training duration and convergence criteria unspecified
- Relies on proxy metrics rather than clinical outcome validation
- Requires careful alignment of demographic metadata with medical records

## Confidence
**High Confidence**: Core methodology of selective optimization based on cross-entropy and ranking loss is clearly specified and theoretically sound. Fairness evaluation framework using Metric-aware Fairness Difference is well-defined and reproducible.

**Medium Confidence**: Experimental setup with three tasks and multiple backbones is clearly described, but lack of precise training details introduces uncertainty. Claims of effectiveness across "multiple backbones, datasets, and modalities" extend beyond exhaustive demonstration.

**Low Confidence**: Critical unknowns include specific selection ratio γ, candidate generation parameters, and training duration. Claim of maintaining performance "without sacrificing overall performance" cannot be fully verified without convergence details.

## Next Checks
1. **Selection Ratio Sensitivity**: Run ablation studies with γ ∈ {0.1, 0.3, 0.5, 0.7} to determine optimal selection rate that maximizes fairness improvement while maintaining performance, documenting the trade-off curve.

2. **Candidate Generation Protocol**: Implement and test multiple candidate generation strategies (different base models, candidate counts K ∈ {5, 10, 20}) to verify that ranking loss component is robust to quality and diversity of pre-generated candidates.

3. **Intersectional Group Stability**: Specifically evaluate method's performance on intersectional subgroups (e.g., young Black females, elderly Asian males) across multiple random seeds to assess consistency and identify failure modes in high-bias-risk populations.