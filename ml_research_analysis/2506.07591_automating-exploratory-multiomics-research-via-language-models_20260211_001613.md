---
ver: rpa2
title: Automating Exploratory Multiomics Research via Language Models
arxiv_id: '2506.07591'
source_url: https://arxiv.org/abs/2506.07591
tags:
- data
- research
- proteus
- hypothesis
- hypotheses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "PROTEUS is a fully automated system for generating data-driven\
  \ hypotheses from clinical multiomics datasets without human intervention. It simulates\
  \ the full scientific research process through five LLM-driven modules\u2014Explorer,\
  \ Hypothesizer, Decomposer, Validator, and Integrator\u2014using unified graph structures\
  \ to manage complex analyses."
---

# Automating Exploratory Multiomics Research via Language Models

## Quick Facts
- arXiv ID: 2506.07591
- Source URL: https://arxiv.org/abs/2506.07591
- Authors: Shang Qu; Ning Ding; Linhai Xie; Yifei Li; Zaoqu Liu; Kaiyan Zhang; Yibai Xiong; Yuxin Zuo; Zhangren Chen; Ermo Hua; Xingtai Lv; Youbang Sun; Yang Li; Dong Li; Fuchu He; Bowen Zhou
- Reference count: 40
- PROTEUS generates 360 hypotheses from 10 multiomics datasets with 70%+ support rates on external validation

## Executive Summary
PROTEUS is a fully automated system for generating data-driven hypotheses from clinical multiomics datasets without human intervention. It simulates the full scientific research process through five LLM-driven modules—Explorer, Hypothesizer, Decomposer, Validator, and Integrator—using unified graph structures to manage complex analyses. Applied to 10 multiomics datasets, PROTEUS produced 360 hypotheses that were evaluated using external CPTAC cohorts and LLM-based scoring, demonstrating the system's ability to generate reliable, novel, and biologically meaningful hypotheses from high-dimensional data.

## Method Summary
PROTEUS operates as a five-module pipeline that automates the entire scientific discovery process. The Explorer module selects general research directions by identifying entity type pairs from a predefined relationship graph. The Hypothesizer narrows these directions to specific entities of interest. The Decomposer breaks down research objectives into sequences of ≤5 relationship edges to validate. The Validator selects appropriate bioinformatics tools and automatically assigns parameters, including a retry loop for failed executions. Finally, the Integrator synthesizes validation results into ranked hypotheses. The system maintains both a static relationship graph defining all supported entity types and relationships, and a dynamic conclusion graph recording specific entities and relationships discovered during analysis.

## Key Results
- PROTEUS generated 360 hypotheses across 10 multiomics datasets, averaging 36 hypotheses per dataset
- External validation using CPTAC cohorts showed 70%+ support rates across most datasets
- PROTEUS outperformed baseline approaches in hypothesis generation capability
- LLM-based scoring yielded high novelty, significance, and general quality ratings (average 4.0-4.5/5)

## Why This Works (Mechanism)

### Mechanism 1: Relationship-Graph Structuring of Research Process
The system maintains a static relationship graph (entity types and their supported relationships) and a dynamic conclusion graph (specific entities and relationships discovered through analysis). All research directions, bioinformatics tools, and analysis results are mapped to these graph structures, allowing the LLM to reason about biological relationships rather than raw data. This provides a biologically meaningful scaffold for planning and memory.

### Mechanism 2: Five-Module Pipeline with Role Specialization
Each module performs a distinct cognitive task: Explorer selects general research directions; Hypothesizer narrows to specific entities; Decomposer breaks objectives into ≤5 relationship edges; Validator selects tools with parameter tuning and retry loops; Integrator synthesizes results into ranked hypotheses. Modules receive context from previous iterations and the conclusion graph, enabling cumulative reasoning.

### Mechanism 3: Context-Dependent Iterative Exploration
After each iteration, Integrator outputs are provided to the next Explorer/Hypothesizer call as short-term context, while conclusion graph edges provide long-term analysis history. This allows the system to deepen promising lines of inquiry rather than exhaustively covering all directions shallowly.

## Foundational Learning

- **Concept: Multiomics Data Integration**
  - Why needed here: The system operates on datasets combining proteomics, transcriptomics, phosphoproteomics, genomics, and clinical features; understanding what each modality measures and their biological relationships is essential for interpreting tool selection and hypothesis formation.
  - Quick check question: Given a gene with a somatic mutation, which omics layers could show downstream effects, and what statistical relationships would you test?

- **Concept: LLM Tool-Calling and Parameter Binding**
  - Why needed here: The Validator module must select appropriate bioinformatics tools and assign parameters based on natural language descriptions of tool capabilities; understanding how LLMs map structured tool schemas to executable calls is critical.
  - Quick check question: If a differential expression tool requires `group1`, `group2`, and `max_p_value` parameters, how would you describe these constraints in a tool documentation block?

- **Concept: Graph-Based Knowledge Representation**
  - Why needed here: The relationship and conclusion graphs structure all system reasoning; understanding how entity–relationship schemas are defined and queried is necessary for extending the system.
  - Quick check question: Design a minimal relationship graph for a dataset with proteins, pathways, and survival outcomes. What edge types would you include?

## Architecture Onboarding

- **Component map**: Explorer -> Hypothesizer -> Decomposer -> Validator -> Integrator -> (back to Explorer with context)
- **Critical path**:
  1. Load dataset → parse data description → initialize relationship/conclusion graphs
  2. Explorer selects direction → Hypothesizer specifies entities → Decomposer generates edge list
  3. For each edge: Validator selects tool → executes → retries if needed → updates conclusion graph
  4. Integrator synthesizes results → outputs hypotheses
  5. Loop back to step 2 with updated context until termination criteria met
- **Design tradeoffs**:
  - Predefined tools vs. code generation: Current design restricts analysis to 41 tools; the paper notes this limits flexibility but ensures reliability. Code generation would increase flexibility but introduce execution risks.
  - Fixed vs. dynamic relationship graph: Static graph provides stability but cannot adapt to unexpected entity types in novel datasets.
  - Module specialization vs. unified prompting: Specialization improves coherence but increases prompt engineering overhead and inter-module dependency.
- **Failure signatures**:
  - Tool retry exhaustion: Validator fails to find successful parameters after 3 attempts; may indicate missing data, incorrect entity names, or incompatible tool-objective pairing.
  - Hypothesis repetition: Hypothesizer outputs identical or near-identical entities across iterations; suggests conclusion graph context not being properly provided or parsed.
  - Low support rates on CPTAC validation: May indicate hypotheses are dataset-specific artifacts rather than generalizable findings; check for overfitting to noise.
- **First 3 experiments**:
  1. Single-module ablation: Run the system with Hypothesizer bypassed (use Explorer outputs directly) on one dataset; compare hypothesis complexity and support rates to full pipeline to quantify Hypothesizer contribution.
  2. Conclusion graph relevance analysis: Log which conclusion graph edges are retrieved as context for each module call; analyze correlation between edge relevance scores and downstream hypothesis quality.
  3. Tool coverage audit: On a held-out dataset, manually identify statistical relationships of interest; check whether the 41-tool library can express all necessary analyses; document gaps for future tool additions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can integrating automatic code generation with predefined tools mitigate the flexibility limitations of PROTEUS in multiomics analysis?
- Basis in paper: [explicit] The authors state that "predefined tools are currently the only avenue for the LLM to interact with omics data" and suggest combining them with code generation to mitigate this.
- Why unresolved: The current architecture restricts the system to 41 specific bioinformatics tools, limiting analysis to pre-defined statistical methods.
- What evidence would resolve it: A demonstration that a hybrid system successfully executes novel, non-predefined analyses without increasing error rates or hallucinations.

### Open Question 2
- Question: Does incorporating external knowledge sources (literature, knowledge graphs) into the primary hypothesis proposal loop improve the biological significance of generated outputs?
- Basis in paper: [explicit] The authors note that "we currently use external information solely for result evaluation, not for the main hypothesis proposal process."
- Why unresolved: It is unclear if external context improves the initial generation phase or if it introduces bias/noise that hinders open-ended discovery.
- What evidence would resolve it: Comparative experiments measuring "Biological Significance" and "Scientific Novelty" scores for hypotheses generated with and without external knowledge integration.

### Open Question 3
- Question: Does the addition of complex entity types such as metabolites or acetylation data increase the management complexity for the LLM beyond current graph capabilities?
- Basis in paper: [explicit] The authors identify "incorporating additional biological entity types... such as acetylation... and metabolites" as a specific direction for extending the system.
- Why unresolved: Current modules are optimized for specific omics layers; the impact of higher data dimensionality and new relationship types on the LLM's reasoning is untested.
- What evidence would resolve it: Performance benchmarks on datasets containing metabolites, specifically tracking tool selection accuracy and conclusion graph coherence.

### Open Question 4
- Question: Can a hybrid architecture using reasoning models (e.g., DeepSeek-R1) for generation and instruction-following models (e.g., GPT-4o) for validation optimize the trade-off between novelty and reliability?
- Basis in paper: [inferred] The authors found DeepSeek-R1 produced higher novelty but lower reliability, whereas GPT-4o balanced reliability with lower novelty.
- Why unresolved: The current system relies on a single model type, potentially missing the optimal balance between creative hypothesis formation and rigorous logical checking.
- What evidence would resolve it: An ablation study where different model combinations are used for different modules (e.g., Hypothesizer vs. Validator) to maximize the composite evaluation score.

## Limitations

- The system's performance depends heavily on the completeness of its predefined tool library and relationship graph, with only 41 tools covering 22 relationship types
- The static relationship graph structure cannot adapt to unexpected entity types, potentially limiting exploration in datasets with unusual feature combinations
- While LLM-based validation shows high novelty scores, the biological plausibility of hypotheses remains uncertain without expert review of all generated claims

## Confidence

- **Hypothesis Generation Pipeline (High)**: Strong evidence from 360 hypotheses across 10 datasets, with consistent support rates >70% on external validation.
- **LLM-Based Evaluation (Medium)**: High average ratings (4.0-4.5/5) for novelty and significance, but evaluation relies entirely on LLM scoring without human expert validation.
- **System Architecture (High)**: Clear modular design with well-defined interfaces between components, demonstrated through successful execution on multiple datasets.

## Next Checks

1. **Tool Library Coverage Analysis**: Manually audit a held-out dataset to identify all statistically relevant relationships that cannot be expressed using the current 41-tool library, documenting specific gaps for future tool additions.

2. **Expert Biological Validation**: Have domain experts review a stratified sample of 20 hypotheses from different datasets to assess actual biological plausibility beyond LLM-based novelty scores.

3. **Context Window Saturation Test**: Systematically increase the size of the conclusion graph during extended exploration runs to measure the point at which context relevance filtering breaks down and hypothesis quality degrades.