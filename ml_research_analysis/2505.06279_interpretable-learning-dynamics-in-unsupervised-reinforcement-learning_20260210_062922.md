---
ver: rpa2
title: Interpretable Learning Dynamics in Unsupervised Reinforcement Learning
arxiv_id: '2505.06279'
source_url: https://arxiv.org/abs/2505.06279
tags:
- agents
- attention
- learning
- agent
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces an interpretability framework to analyze\
  \ how unsupervised reinforcement learning agents perceive and represent their environments.\
  \ The study compares five agents\u2014DQN, ICM, RND, PPO, and Transformer-RND\u2014\
  using gradient-based attention visualization (Grad-CAM, LRP), exploration metrics,\
  \ and VAE-based latent space analysis."
---

# Interpretable Learning Dynamics in Unsupervised Reinforcement Learning

## Quick Facts
- arXiv ID: 2505.06279
- Source URL: https://arxiv.org/abs/2505.06279
- Reference count: 19
- Key outcome: A framework analyzes how five unsupervised RL agents perceive and represent environments, showing curiosity-driven agents have broader attention, richer exploration, and better latent representations than value-based DQN.

## Executive Summary
This work introduces an interpretability framework to analyze how unsupervised reinforcement learning agents perceive and represent their environments. The study compares five agents—DQN, ICM, RND, PPO, and Transformer-RND—using gradient-based attention visualization (Grad-CAM, LRP), exploration metrics, and VAE-based latent space analysis. Novel metrics including attention diversity and attention change rate quantify spatial breadth and temporal dynamics of visual focus. Results show that curiosity-driven agents (ICM, RND) exhibit broader, more structured attention and richer exploration compared to value-based DQN. Transformer-RND achieves the best balance of wide attention, high exploration coverage, and compact, clustered latent representations. DQN displays diffuse attention and poorly structured embeddings, correlating with lower performance. VAE reconstructions confirm that curiosity-driven agents produce higher-fidelity internal representations. The proposed framework offers diagnostic tools for understanding agent perception, highlighting the role of architectural and training signal choices in shaping interpretable behavior.

## Method Summary
The framework trains five agents on CoinRun for 1M steps, extracting Grad-CAM and LRP visualizations at checkpoints, training a VAE on collected frames, and analyzing latent space clustering. Key agents include DQN, PPO, ICM, RND, and Transformer-RND with a 4-layer Transformer encoder. Metrics include attention coverage, trajectory entropy, clustering scores (Silhouette, Davies-Bouldin, Calinski-Harabasz), and VAE reconstruction fidelity.

## Key Results
- Transformer-RND achieves the best balance of wide attention coverage (35.2%), high exploration coverage, and compact, clustered latent representations.
- Curiosity-driven agents (ICM, RND) exhibit broader, more structured attention and richer exploration than value-based DQN.
- VAE reconstructions reveal ICM and Transformer-RND preserve structural detail, while DQN produces blurry reconstructions indicating poor representation quality.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Intrinsic motivation signals (curiosity) expand perceptual breadth and improve representation quality compared to pure extrinsic reward signals.
- Mechanism: Curiosity-driven agents generate intrinsic rewards via prediction error—either forward dynamics prediction error (ICM) or distillation error from a random target network (RND). These error signals create gradients that push the encoder to attend to novel, poorly-predicted regions, expanding the set of features the agent learns to encode.
- Core assumption: Prediction error correlates with meaningful novelty (not just noise or stochasticity), and minimizing prediction error requires building better internal representations.
- Evidence anchors:
  - [abstract] "Curiosity-driven agents display broader, more dynamic attention and exploratory behavior than their extrinsically motivated counterparts."
  - [Section 4.1] "Agents equipped with intrinsic motivation (ICM, RND, Transformer-RND) consistently exhibit broader spatial attention, higher exploration coverage, and richer latent representations than extrinsically motivated agents like DQN."
  - [corpus] "Wonder Wins Ways" notes curiosity "often confuses environmental stochasticity with meaningful novelty"—a known failure mode, though not observed in this CoinRun study.
- Break condition: Environments with high stochasticity or visual noise without semantic structure; prediction error ceases to correlate with meaningful novelty.

### Mechanism 2
- Claim: Transformer self-attention inductive biases support more structured abstraction while maintaining wide perceptual coverage.
- Mechanism: The Transformer-RND agent uses multi-head self-attention over spatial CNN tokens. This allows the policy to learn which spatial relationships matter (via attention weights), producing compact latent clusters (highest Calinski-Harabasz score) while maintaining high coverage (35.2%).
- Core assumption: Self-attention's ability to model long-range dependencies translates to better policy abstraction in RL, not just improved sequence modeling in supervised learning.
- Evidence anchors:
  - [abstract] "Transformer-RND combined wide attention with the highest Calinski-Harabasz clustering score and lowest attention spread."
  - [Section 4.2] "Transformer-RND stands out for its ability to balance wide attention coverage with compact, structured latent representations."
  - [corpus] No direct corpus validation for Transformer architectures in RL interpretability; this remains an open research direction.
- Break condition: Very short-horizon tasks where spatial relationships don't matter; environments where CNN local features are sufficient.

### Mechanism 3
- Claim: VAE reconstruction fidelity serves as a proxy for internal representation quality and exploration diversity.
- Mechanism: A VAE trained on agent-collected frames encodes the state distribution each agent visits. High-fidelity reconstructions (ICM, Transformer-RND) indicate agents visited diverse, structurally coherent states. Blurry reconstructions (DQN) suggest narrow, repetitive state distributions with poor semantic encoding.
- Core assumption: The VAE's 32-dimensional bottleneck is sufficient to capture relevant environment structure; reconstruction quality reflects agent experience diversity, not just VAE capacity.
- Evidence anchors:
  - [abstract] "VAE reconstructions revealed ICM and Transformer-RND preserved structural detail, while DQN produced blurry reconstructions."
  - [Section 3.3] "Agents like ICM and Transformer-RND produce high-fidelity reconstructions that preserve scene layout, object boundaries, and agent positions—indicating strong internal state representations."
  - [corpus] No corpus papers validate VAE reconstruction as a general proxy for RL representation quality; this is a methodological contribution of this work.
- Break condition: Environments with high visual complexity exceeding VAE capacity; agents with good exploration but poor encoder architectures.

## Foundational Learning

- Concept: **Intrinsic motivation in RL (ICM, RND)**
  - Why needed here: The paper's central comparison is between extrinsically motivated agents (DQN, PPO) and intrinsically motivated ones. Without understanding prediction-error-based curiosity, the results are uninterpretable.
  - Quick check question: Can you explain why prediction error can serve as a reward signal, and under what conditions it would fail?

- Concept: **Attention visualization via Grad-CAM and LRP**
  - Why needed here: The interpretability framework relies on these methods to diagnose agent perception. Understanding their limitations is critical for evaluating claims.
  - Quick check question: What does Grad-CAM's "coarse localization" mean, and why might LRP give different results for the same network?

- Concept: **Latent space clustering metrics (Silhouette, Davies-Bouldin, Calinski-Harabasz)**
  - Why needed here: Quantitative claims about representation quality depend on these metrics. Understanding what they measure—and what they don't—is essential.
  - Quick check question: Why might high clustering scores not correlate with generalization or semantic richness?

## Architecture Onboarding

- Component map:
  - **Agent backbone**: CNN encoder (64×64 RGB input) → policy/value heads (DQN, PPO, ICM, RND) or Transformer encoder (Transformer-RND)
  - **Intrinsic modules**: ICM adds forward/inverse dynamics models; RND adds random target network + predictor
  - **Interpretability pipeline**: Grad-CAM (gradient-based), LRP (relevance propagation), VAE (separate training on collected frames)
  - **Analysis outputs**: Attention maps, UMAP/t-SNE projections, clustering scores

- Critical path:
  1. Train agent for 1M steps with replay buffer (200k transitions)
  2. At checkpoints (0k, 100k, 500k, 800k, 995k), extract Grad-CAM and LRP visualizations
  3. Collect observation dataset; train VAE separately
  4. Extract latent vectors; compute clustering metrics; visualize with UMAP/t-SNE

- Design tradeoffs:
  - Grad-CAM provides spatial interpretability but is coarse; LRP is fine-grained but computationally heavier
  - On-policy methods (PPO) give more dynamic attention shifts but require more samples; off-policy (DQN) is sample-efficient but shows weaker representation quality
  - VAE bottleneck size (32-dim) trades reconstruction fidelity against clustering interpretability

- Failure signatures:
  - Diffuse, non-evolving Grad-CAM maps across training (indicates poor attention learning—seen in DQN)
  - Blurry VAE reconstructions with loss of object boundaries (indicates narrow state distribution)
  - High clustering scores but poor task performance (may indicate overfitting to a narrow state subset—caution noted in Section 4.4)
  - ICM/RND intrinsic rewards that plateau early without corresponding exploration gains

- First 3 experiments:
  1. **Replicate the attention evolution comparison**: Train ICM and DQN on CoinRun for 100k steps each; generate Grad-CAM at 0k, 50k, 100k. Verify that ICM shows progressive focusing while DQN remains diffuse.
  2. **Validate VAE reconstruction proxy**: Train VAE on frames from a well-explored agent vs. a poorly-exploring one; quantify reconstruction SSIM to confirm the paper's qualitative claims.
  3. **Test the stochasticity break condition**: Introduce visual noise or stochastic transitions to CoinRun; check if ICM/RND attention becomes less structured or if intrinsic rewards track noise rather than meaningful novelty.

## Open Questions the Paper Calls Out

- **Question**: Do the observed correlations between attention breadth, exploration, and representation quality persist across diverse tasks and hyperparameter settings?
  - Basis in paper: [explicit] The "Limitations" section states the analysis focuses on a single environment (CoinRun) and fixed hyperparameters, necessitating tests of generalizability.
  - Why unresolved: The results may be specific to the procedural generation or visual complexity of CoinRun, limiting claims about the universal role of curiosity in representation learning.
  - What evidence would resolve it: Replicating the experimental framework across multiple Procgen environments or with varied hyperparameter configurations to verify robustness.

- **Question**: How do interpretability dynamics differ when applied to model-based or hierarchical reinforcement learning agents?
  - Basis in paper: [explicit] The "Future Work" section explicitly proposes extending the interpretability framework to these architectures to gain richer reasoning insights.
  - Why unresolved: The current study is restricted to model-free, flat policy architectures (DQN, PPO, etc.), leaving the internal dynamics of planning or hierarchical agents unexplored.
  - What evidence would resolve it: Applying Grad-CAM, LRP, and VAE analysis to agents like Dreamer (model-based) or FeUdal Networks (hierarchical) and comparing latent structures.

- **Question**: How does temporal attention evolve in Transformer-based agents, and can it explain long-term dependency management?
  - Basis in paper: [explicit] The "Future Work" section suggests incorporating temporal attention visualization in Transformer-based agents as a specific next step.
  - Why unresolved: The current methodology relies on spatial visualizations (Grad-CAM) and static latent analysis, which cannot capture time-dependent attention shifts critical to the Transformer architecture.
  - What evidence would resolve it: Extending the visualization pipeline to analyze attention weights across temporal sequences rather than single frames.

## Limitations
- Grad-CAM's coarse localization may mask important attention differences; LRP's computational cost limited its application to a subset of agents.
- VAE reconstruction proxy assumes visual fidelity directly reflects representational quality, which may not hold for abstract or temporally extended features.
- Clustering metrics provide geometric measures but don't directly validate semantic meaningfulness or generalization capability.

## Confidence
- **High confidence**: DQN shows diffuse attention and poor representation quality compared to curiosity-driven agents; Transformer-RND achieves the best balance of attention coverage and latent structure; ICM and RND demonstrate broader exploration and higher-fidelity reconstructions than extrinsic agents.
- **Medium confidence**: Attention diversity and change rate metrics reliably quantify perceptual breadth and temporal dynamics; VAE reconstruction quality serves as a valid proxy for representation quality across diverse agent types.
- **Low confidence**: Transformer self-attention architectures will consistently improve interpretability across RL domains; curiosity-driven exploration will maintain advantages in stochastic or noisy environments.

## Next Checks
1. **Stochasticity Break Condition Test**: Introduce visual noise or stochastic transitions to CoinRun and verify whether ICM/RND attention becomes less structured or whether intrinsic rewards track noise rather than meaningful novelty.
2. **VAE Proxy Validation**: Train VAEs on frames from well-explored versus poorly-exploring agents and quantify reconstruction SSIM to confirm that reconstruction quality correlates with exploration diversity as claimed.
3. **Attention Method Comparison**: Apply both Grad-CAM and LRP to the same agent checkpoints to measure agreement rates and identify systematic differences in what each method reveals about attention patterns.