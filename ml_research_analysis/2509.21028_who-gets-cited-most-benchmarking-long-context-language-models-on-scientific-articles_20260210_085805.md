---
ver: rpa2
title: Who Gets Cited Most? Benchmarking Long-Context Language Models on Scientific
  Articles
arxiv_id: '2509.21028'
source_url: https://arxiv.org/abs/2509.21028
tags:
- article
- articles
- author
- reasoning
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SciTrek is a long-context reasoning benchmark using full-text scientific
  articles, generating questions and answers via SQL queries over article metadata.
  It requires models to synthesize information across multiple articles using operations
  like counting, sorting, and filtering.
---

# Who Gets Cited Most? Benchmarking Long-Context Language Models on Scientific Articles

## Quick Facts
- **arXiv ID**: 2509.21028
- **Source URL**: https://arxiv.org/abs/2509.21028
- **Reference count**: 32
- **Primary result**: SciTrek benchmark reveals LLMs struggle with multi-article reasoning, especially as context length increases, with limited gains from fine-tuning.

## Executive Summary
SciTrek is a long-context reasoning benchmark using full-text scientific articles that requires models to synthesize information across multiple articles using SQL-based queries over article metadata. The benchmark automatically generates questions and answers via SQL queries, enabling fine-grained error analysis when models fail. Experiments demonstrate that current LLMs, both open-weight and proprietary, struggle significantly with this task, particularly for operations involving negation, sorting, and compound conditions. Performance degrades substantially as context length increases from 64K to 1M tokens, and post-training fine-tuning provides limited improvement. The benchmark provides valuable insights into the limitations of long-context models for scientific reasoning tasks.

## Method Summary
The benchmark constructs scientific article collections from Semantic Scholar API, retrieving PDFs and converting them to markdown using Marker. Three database tables (articles, article-author, citing-cited) are created per collection. SQL templates targeting specific reasoning skills (aggregation, sorting, filtering, relational filtering) are instantiated with collection-specific values, executed to generate ground-truth answers, and converted to natural language questions via Qwen2.5-Coder-32B-Instruct with roundtrip validation. Models are evaluated on both full-text articles and database tables across context lengths from 128K to 10M tokens, with exact match and F1 scores as primary metrics. Post-training experiments include supervised fine-tuning (500 steps, batch 32, lr=2×10⁻⁶) and GRPO-based reinforcement learning with EM+F1 rewards.

## Key Results
- Proprietary models significantly outperform open-weight models on both full-text and database table contexts
- Performance drops substantially as input length increases, with o4-mini achieving 46.5% exact match at 128K tokens vs 87.8% on database tables
- Models show systematic weaknesses in numerical operations, negation handling, and producing correctly formatted outputs
- Post-training fine-tuning (SFT and GRPO) provides limited improvement, with GRPO improving abstract reasoning but not fine-grained counting accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SQL-to-natural-language conversion creates verifiable reasoning chains for error attribution.
- Mechanism: Article metadata is structured into database tables. SQL templates targeting specific skills are instantiated with collection-specific values, then converted to natural language questions via LLM. The SQL query provides explicit reasoning steps that enable fine-grained error analysis.
- Core assumption: SQL operations approximate the cognitive steps required for information synthesis across documents.
- Evidence anchors: SQL queries provide explicit, verifiable reasoning processes; weak corpus support from CURIE evaluation.

### Mechanism 2
- Claim: Performance degradation with context length stems from failures in information localization, not reasoning ability.
- Mechanism: Models process full-text articles (up to 1M tokens) but must locate sparse facts distributed across documents. When the same questions are presented with database tables (~2K tokens) instead of full-text, performance improves substantially.
- Core assumption: The reasoning operations required are equivalent whether reading full-text or database tables.
- Evidence anchors: Performance dropping as input length increases; proprietary models outperform open-weight ones; PERK corroborates localization challenge.

### Mechanism 3
- Claim: Reinforcement learning with GRPO improves abstract reasoning but not fine-grained counting operations.
- Mechanism: GRPO training uses mixed EM/F1 rewards to encourage reasoning trace generation. Analysis of 200 reasoning traces shows overall reasoning remains logically sound but most reasoning steps that require counting are not accurate.
- Core assumption: Verifiable rewards (EM/F1) provide sufficient training signal for complex multi-step operations.
- Evidence anchors: SFT and GRPO slightly improve performance across dimensions; GRPO improves abstract reasoning without enhancing accuracy in fine-grained operations; Chain-of-Thought Matters supports reasoning-structure hypothesis.

## Foundational Learning

- Concept: **SQL aggregation operations (COUNT, SUM, MAX, MIN, AVG, DISTINCT)**
  - Why needed here: The benchmark maps reasoning skills to SQL commands; understanding what each operation computes is prerequisite to interpreting error patterns.
  - Quick check question: Given a table of articles with author_count column, write a query returning the maximum author count.

- Concept: **Rotary Position Embeddings (RoPE) and context window extrapolation**
  - Why needed here: The paper evaluates models with context windows from 128K to 10M tokens; RoPE is the dominant positional encoding enabling such lengths.
  - Quick check question: Why does RoPE generalize better to unseen sequence lengths than absolute positional embeddings?

- Concept: **Group Relative Policy Optimization (GRPO) with verifiable rewards**
  - Why needed here: Post-training experiments use GRPO with EM/F1 rewards; understanding how verifiable rewards differ from preference-based RL clarifies training dynamics.
  - Quick check question: How do verifiable rewards (exact match) differ from reward models in standard RLHF?

## Architecture Onboarding

- Component map: Semantic Scholar API -> PDF retrieval -> Marker (PDF-to-markdown) -> Article clusters -> Database construction -> SQL templates -> Question conversion -> Evaluation
- Critical path: SQL template design -> Template instantiation -> Question validation (SQL↔NL consistency). Invalid conversions discard queries for that collection.
- Design tradeoffs:
  - Natural vs. structured context: Full-text articles are ecologically valid but harder to debug; database tables enable isolation of reasoning vs. localization failures.
  - Question complexity vs. error attribution: Simple SQL operations enable fine-grained analysis but may not capture deep scientific reasoning.
  - Scalability vs. domain depth: Automatic generation scales to 1M+ tokens but covers only metadata elements (titles, authors, references), not figures or domain-specific content.
- Failure signatures:
  - "NULL" responses: Weaker models default to fallback answers rather than attempting retrieval (70-90% NULL rates for some skills).
  - Format violations: Models return author lists when counts are requested; sorting questions produce incorrectly formatted answers.
  - Negation failures: All models perform poorly on questions with "not"/"never" operators (near-zero performance on negation-based filtering).
  - Incomplete aggregation: Models return partial lists when full aggregation is requested.
- First 3 experiments:
  1. Baseline establishment: Run zero-shot evaluation on both full-text and database table contexts across all context lengths to measure the reasoning-localization gap.
  2. Skill-level error profiling: Disaggregate performance by skill category to identify systematic weaknesses; focus on negation handling and counting accuracy.
  3. Post-training ablation: Apply SFT and GRPO separately to quantify gains from reasoning-structure training vs. fine-grained operation improvement.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can extending SciTrek to include figures, tables, and domain-specific scientific content reveal different model failure modes than those observed with metadata-only queries?
  - Basis in paper: Future work will explore additional long-context model capabilities for understanding scientific articles, such as interpreting figures and performing domain-specific content reasoning.
  - Why unresolved: Current benchmark only covers titles, authors, and references; no evaluation exists for visual or deep content reasoning in scientific contexts.
  - What evidence would resolve it: Comparative evaluation of models on figure interpretation and domain-specific reasoning tasks using the same SQL-grounded methodology.

- **Open Question 2**: What architectural or training modifications can address the systematic weaknesses models exhibit in numerical operations and negation handling within long contexts?
  - Basis in paper: Models exhibit systematic weaknesses in numerical operations, handling negation, and producing correctly formatted outputs, particularly for reference-related questions.
  - Why unresolved: Neither supervised fine-tuning nor reinforcement learning resolved these specific failures; the root cause remains unclear.
  - What evidence would resolve it: Targeted ablation studies testing specialized attention mechanisms or training objectives designed for counting and logical negation.

- **Open Question 3**: Why do reasoning improvements from GRPO-based reinforcement learning not transfer to improved accuracy in fine-grained operations such as counting references?
  - Basis in paper: GRPO-based reinforcement learning improves abstract reasoning without enhancing the model's accuracy in fine-grained operations, such as counting references.
  - Why unresolved: The disconnect between improved reasoning traces and unimproved counting accuracy suggests a fundamental gap in how models process numerical information in long contexts.
  - What evidence would resolve it: Mechanistic interpretability studies examining attention patterns during counting operations before and after GRPO training.

## Limitations

- The complete set of 387 SQL templates is only partially documented, making exact replication impossible
- Performance evaluation uses only exact match and F1 scores without considering reasoning trace quality
- The study focuses on English-language articles from 2020 onward, limiting generalizability to other domains or languages
- Current benchmark only covers metadata-based reasoning, excluding domain-specific content like figures and equations

## Confidence

- **High confidence**: SQL-based error attribution mechanism works as described; performance degradation pattern across context lengths is reliably demonstrated; database vs. full-text performance gap is a robust finding
- **Medium confidence**: GRPO improves abstract reasoning without enhancing fine-grained counting accuracy; effectiveness of verifiable rewards for long-context reasoning
- **Low confidence**: SQL operations adequately capture cognitive steps for information synthesis; metadata focus may miss crucial reasoning aspects

## Next Checks

1. **SQL Template Completeness Verification**: Request and validate the complete list of 387 SQL templates from the authors. Test whether these templates can capture increasingly complex scientific reasoning scenarios beyond metadata operations.

2. **Localization vs. Reasoning Isolation Test**: Systematically evaluate the same question set across varying article distributions (sparse vs. dense information) while holding context length constant. This would definitively separate localization failures from reasoning limitations.

3. **Cross-Domain Generalizability Assessment**: Apply the benchmark to non-scientific domains (legal documents, technical manuals) with similar metadata structures. Compare performance patterns to determine whether observed limitations are domain-specific or fundamental to long-context processing.