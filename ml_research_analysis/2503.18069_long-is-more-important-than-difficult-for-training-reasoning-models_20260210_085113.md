---
ver: rpa2
title: Long Is More Important Than Difficult for Training Reasoning Models
arxiv_id: '2503.18069'
source_url: https://arxiv.org/abs/2503.18069
tags:
- reasoning
- problems
- length
- problem
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether problem difficulty or reasoning
  length is more important for training reasoning models. It hypothesizes that reasoning
  length, rather than problem difficulty, primarily drives model performance.
---

# Long Is More Important Than Difficult for Training Reasoning Models
## Quick Facts
- arXiv ID: 2503.18069
- Source URL: https://arxiv.org/abs/2503.18069
- Reference count: 40
- Primary result: Models trained on easier long reasoning problems perform comparably to those trained on difficult single problems.

## Executive Summary
This paper investigates whether problem difficulty or reasoning length is more important for training reasoning models. The authors hypothesize that reasoning length, rather than problem difficulty, primarily drives model performance. To test this, they construct two datasets: one with long but easy composite problems and another with difficult single problems, both having similar total reasoning lengths. Experiments show that models trained on easier long problems perform comparably to those trained on difficult problems, leading to the identification of a scaling law where model performance increases nearly linearly as reasoning data length grows exponentially.

## Method Summary
The authors propose a simple method to generate arbitrarily long reasoning sequences by concatenating problems. They create the Long1K dataset (1,000 samples: 800 concatenated problem pairs from OpenThoughts-114k at ~32k tokens, 200 single problems from s1.1) and fine-tune Qwen2.5-32B-Instruct with LoRA (rank=16, alpha=32), 3 epochs, batch size 2 per GPU, learning rate 2e-4, cosine LR scheduler, max output length 32k tokens, on 8× A800-80GB GPUs. Reasoning traces are generated using DeepSeek-R1 or Gemini Flash Thinking API, with problems wrapped in `<|begin of thought|>...<|end of thought|>` and solutions in `<|begin of solution|>...<|end of solution|>`.

## Key Results
- Models trained on easier long problems performed comparably to those trained on difficult problems
- A scaling law was identified showing model performance increases nearly linearly as reasoning data length grows exponentially
- Long1K-32B achieved 95.6% accuracy on MATH500 and 71.1% on GPQA Diamond, outperforming DeepSeek-R1-Distill-Qwen-32B with only 1,000 training samples

## Why This Works (Mechanism)
The paper demonstrates that extending reasoning length is more effective than increasing problem difficulty for improving reasoning model performance. By exposing models to longer reasoning traces, they develop better problem-solving capabilities and reasoning skills that generalize across difficulty levels.

## Foundational Learning
- **Reasoning length scaling**: Understanding how model performance scales with reasoning sequence length is crucial for optimizing training data composition. Quick check: Plot performance vs. reasoning length to verify linear scaling.
- **Problem concatenation synthesis**: The method of creating long reasoning data by combining simpler problems with connectives is key to the approach. Quick check: Verify that concatenated problems maintain logical coherence.
- **LoRA fine-tuning**: Low-Rank Adaptation enables efficient fine-tuning of large models with minimal parameter updates. Quick check: Monitor training loss and validation metrics to ensure stable convergence.
- **Long context handling**: Models must effectively process and generate reasoning traces up to 32k tokens. Quick check: Validate that models can generate coherent reasoning without degradation in quality.

## Architecture Onboarding
- **Component map**: Problem datasets (OpenThoughts-114k, s1.1) -> Data synthesis (concatenation + reasoning generation) -> Long1K dataset -> Qwen2.5-32B-Instruct (LoRA fine-tuning) -> Long1K-32B model -> Evaluation on MATH500, GPQA Diamond
- **Critical path**: Data synthesis and reasoning generation are the most critical steps, as they determine the quality and length of training data that drives performance improvements.
- **Design tradeoffs**: The approach trades problem difficulty for reasoning length, which may limit performance on extremely difficult problems but enables strong general reasoning capabilities.
- **Failure signatures**: If models underperform, check for overfitting to concatenated formats, OOM errors during training, or excessive transition word usage in failures.
- **First experiments**: 1) Generate a small sample of Long1K data and verify reasoning trace quality. 2) Test model inference on concatenated problems to ensure coherence. 3) Fine-tune a smaller model on a subset of Long1K to validate the training pipeline.

## Open Questions the Paper Calls Out
- Does the observed log-linear scaling law between reasoning length and performance hold across non-mathematical domains such as code generation or open-ended logical deduction?
- How does the semantic coherence of synthesized long reasoning chains affect training efficiency compared to simple problem concatenation?
- How can the relationship between inference length and task difficulty be optimized specifically for smaller model architectures (<7B parameters)?

## Limitations
- Exact sampling criteria for selecting problems from source datasets are unspecified, affecting reproducibility
- Limited detail on connective templates and rewriting strategies makes faithful reproduction difficult
- Prompting strategy for generating reasoning traces is not provided, introducing variability in synthetic data quality
- Acceptable variance around the 32k token target is not quantified, potentially affecting model performance

## Confidence
- **High confidence**: The scaling law showing performance improvement with increased reasoning length is empirically demonstrated and aligns with results from Long1K-32B
- **Medium confidence**: The core claim that reasoning length trumps difficulty is supported, but reproducibility is limited by missing implementation details
- **Low confidence**: The claim that 1,000 training samples suffice for strong performance is based on a single model and dataset, requiring further validation

## Next Checks
1. **Dataset reproducibility check**: Recreate the Long1K dataset using the specified source datasets (OpenThoughts-114k, s1.1) with random sampling, and verify that the generated samples meet the 32k token target length with acceptable variance.
2. **Controlled ablation study**: Train two models—one on the Long1K dataset and one on a dataset of 1,000 single difficult problems of similar total length—to confirm that reasoning length, not difficulty, drives performance gains.
3. **Scaling law validation**: Extend the reasoning length beyond 32k tokens (e.g., 64k) and measure performance on MATH500 and GPQA Diamond to test whether the observed linear performance increase with exponential length growth holds.