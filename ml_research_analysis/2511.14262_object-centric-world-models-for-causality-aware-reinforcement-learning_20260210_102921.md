---
ver: rpa2
title: Object-Centric World Models for Causality-Aware Reinforcement Learning
arxiv_id: '2511.14262'
source_url: https://arxiv.org/abs/2511.14262
tags:
- stica
- world
- learning
- object-centric
- objects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces STICA, an RL agent that combines object-centric
  world models with causality-aware policy and value networks. The core innovation
  is representing observations as object-centric tokens using a slot-based autoencoder,
  enabling the model to predict token-level dynamics and interactions.
---

# Object-Centric World Models for Causality-Aware Reinforcement Learning

## Quick Facts
- arXiv ID: 2511.14262
- Source URL: https://arxiv.org/abs/2511.14262
- Reference count: 13
- Primary result: STICA achieves up to 5.5x normalized mean return on Safety Gym tasks compared to state-of-the-art RL agents.

## Executive Summary
This paper introduces STICA, a model-based RL agent that combines object-centric world models with causality-aware policy and value networks. The core innovation is representing observations as object-centric tokens using a slot-based autoencoder, enabling the model to predict token-level dynamics and interactions. The policy and value networks estimate causal relationships among tokens and use them to guide attention, leading to more structured decision-making. Experiments on Safety Gym and OCVRL benchmarks show STICA outperforms state-of-the-art agents in both sample efficiency and final performance, achieving up to 5.5x normalized mean return on Safety Gym tasks. Ablation studies confirm that both the object-centric representations and causal attention contribute significantly to the gains.

## Method Summary
STICA uses a slot-based autoencoder with Slot Attention to decompose observations into object tokens plus a dedicated background token, explicitly separating static scene elements from dynamic objects. The world model is a Transformer-XL that processes object tokens, actions, and rewards as a sequence to predict future latent states and rewards. The policy and value networks use causal attention mechanisms that reweight attention based on estimated causality scores for each object token, focusing decision-making on task-relevant objects. The model is trained end-to-end using A2C with GAE on imagined trajectories generated by the world model, with separate learning rates for world model (1e-4) and policy/value networks (1e-4 and 1e-5 respectively).

## Key Results
- STICA achieves 5.5x normalized mean return on Safety Gym tasks compared to DreamerV3 and other baselines
- Achieves state-of-the-art success rates on OCVRL benchmark across multiple tasks
- Ablation studies show both object-centric representations and causal attention are critical, with performance dropping significantly when either component is removed
- Visualizations reveal STICA selectively attends to task-relevant objects while ignoring distractors and background elements

## Why This Works (Mechanism)

### Mechanism 1: Object-Centric Disentanglement via Slot Attention
Decomposing observations into discrete object tokens while separating static background information allows the world model to learn precise dynamics for individual entities rather than entangled holistic representations. A Slot-based Autoencoder uses competitive attention to bind pixels to specific latent slots, with a dedicated learnable latent absorbing static scene elements to prevent object slots from wasting capacity on background noise.

### Mechanism 2: Causal Attention Reweighting
Modulating attention weights in the policy/value networks based on estimated "causality scores" focuses decision-making capacity on task-relevant objects, accelerating learning. The network estimates a causality score for each object token, constructing a weight matrix that scales the standard Transformer attention map, effectively suppressing gradients and attention from irrelevant objects.

### Mechanism 3: Factored Dynamics with Transformers
Processing object tokens as a sequence in a Transformer-XL allows the world model to capture pairwise object interactions and long-term dependencies more effectively than RNN-based holistic models. The dynamics model takes object tokens, actions, and rewards as a combined sequence, with self-attention layers modeling interactions directly while Transformer-XL handles longer temporal contexts.

## Foundational Learning

- **Slot Attention**: The perceptual front-end of STICA that binds pixels to slots. Why needed: Without understanding how iterative attention creates object tokens, later stages are opaque. Quick check: Can you explain how Slot Attention uses competition to ensure one slot captures one object?

- **Model-Based Reinforcement Learning (MBRL)**: STICA learns a world model to generate "imagined" rollouts. Why needed: Understanding the difference between model-based planning and model-free policy gradient is essential. Quick check: Why does learning a world model often lead to better sample efficiency than direct policy optimization?

- **Transformer-XL**: Used for the dynamics model to handle long sequences. Why needed: STICA specifically uses this architecture for its dynamics model. Quick check: What mechanism in Transformer-XL allows it to handle dependencies longer than the fixed input length?

## Architecture Onboarding

- **Component map**: Observation -> Slot AE -> Object Latents + Background -> Transformer-XL Dynamics -> Predicted Latents, Rewards -> Causal Transformers (Policy/Value) -> Action, Value

- **Critical path**: The Slot-based Autoencoder. If the "Background Removal" fails or slots collapse, the Causal Attention mechanism will receive garbage inputs, rendering the policy blind to actual task structure.

- **Design tradeoffs**: Fixed slot count (n=5) - too few slots cause object merging, too many waste computation. Causal Graph hardcoding vs learning - STICA estimates scores but uses a fixed graph structure, less flexible than full causal discovery but more stable for RL.

- **Failure signatures**: Ghosting - reconstruction shows faint residuals of moving objects in the background slot, indicating poor separation. Attention Smear - without Causal Attention, the policy attends uniformly to everything.

- **First 3 experiments**: 
  1. Visualize Slot Decomposition - pass observations through encoder and visualize reconstruction of individual slots vs background to verify disentanglement
  2. Ablate Background Token - run comparison on simple environment with and without z_BG token to confirm it improves feature extraction
  3. Attention Mask Inspection - visualize the attention weight matrix during a task with distractors to confirm the model learns to ignore them

## Open Questions the Paper Calls Out

### Open Question 1
Can STICA maintain its sample efficiency and performance advantages when applied to high-dimensional control tasks involving complex agent morphologies, such as the "Doggo" agent, or dense physical interactions like the "Push" tasks? The authors explicitly excluded these tasks because they are "substantially more difficult and require 10 to 100 times more environmental steps," leaving their method's efficacy on these challenges unproven.

### Open Question 2
How does the reliance on a fixed number of object slots (n=5) affect the agent's robustness in environments where the number of task-relevant objects fluctuates or exceeds the slot capacity? The paper sets n=5 and shows unused slots are ignored, but doesn't analyze scenarios where distinct objects exceed the allocated slot limit.

### Open Question 3
Can the causality-aware attention mechanism be extended to handle safety constraints (costs) explicitly, enabling the agent to distinguish between reward-causal and safety-critical objects? While STICA uses Safety Gym benchmark, it modifies it to ignore safety costs, focusing solely on maximizing rewards.

## Limitations

- The object-centric approach assumes discrete, independent objects with static backgrounds, which may fail with amorphous textures or dynamic backgrounds
- The causal attention mechanism assumes only a small subset of visible objects matters for optimal decisions, potentially discarding critical information in tasks requiring global statistics
- Computational scaling is a concern with O(NÂ²) attention becoming prohibitive as slot count increases

## Confidence

- **Object-Centric Disentanglement Works**: Medium-High - ablation removing z_BG and slot attention both hurt performance significantly, and visualizations show clean separation
- **Causal Attention Improves Performance**: Medium-High - CA ablation shows uniform attention without it, and STICA outperforms baselines on tasks with distractors
- **Overall Approach Beats Baselines**: High - achieves 5.5x normalized mean return on Safety Gym with comprehensive comparison, though exact training details are underspecified

## Next Checks

1. **Test Slot Attention Failure Modes**: Apply STICA to a task with highly deformable objects or amorphous textures. Measure slot binding quality and check if performance degrades as predicted by the "discrete object" assumption.

2. **Stress-Test Causal Attention**: Design a task where the reward depends on global statistics of all objects. Run STICA with and without causal attention, and measure if suppressing "irrelevant" objects hurts performance.

3. **Scale Slot Count and Measure Attention Cost**: Increase slot count from 5 to 10 or 20 on a medium-complexity task. Measure performance change and wall-clock time per step to confirm if computational limits emerge.