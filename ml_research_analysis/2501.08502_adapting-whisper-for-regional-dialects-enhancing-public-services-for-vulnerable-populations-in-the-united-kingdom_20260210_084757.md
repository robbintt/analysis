---
ver: rpa2
title: 'Adapting Whisper for Regional Dialects: Enhancing Public Services for Vulnerable
  Populations in the United Kingdom'
arxiv_id: '2501.08502'
source_url: https://arxiv.org/abs/2501.08502
tags:
- data
- whisper
- fine-tuned
- nesac
- sesha
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the performance of Whisper large-v3 on regional
  Scottish accents in the UK, specifically for North East Scotland and South East
  Scotland. The study reveals that Whisper exhibits higher word error rates (WER)
  on these regional accents compared to a baseline dataset.
---

# Adapting Whisper for Regional Dialects: Enhancing Public Services for Vulnerable Populations in the United Kingdom

## Quick Facts
- **arXiv ID:** 2501.08502
- **Source URL:** https://arxiv.org/abs/2501.08502
- **Reference count:** 6
- **Key result:** Whisper large-v3 shows higher WER on regional Scottish accents compared to British Isles baseline; fine-tuning improves performance on matching dialects.

## Executive Summary
This paper evaluates Whisper large-v3's performance on regional Scottish accents (North East Scotland and South East Scotland) in the UK. The study finds that Whisper exhibits higher word error rates on these regional accents compared to a baseline dataset. Fine-tuning Whisper on region-specific data improves performance on the corresponding test sets, with indications of cross-region transferability. Manual analysis reveals limitations of WER as an evaluation metric, highlighting transcription style differences and contextual bias issues in fine-tuned models.

## Method Summary
The study fine-tunes Whisper large-v3 on approximately 47 hours of region-specific accented audio data from NESAC (North East Scotland) and SESHA (South East Scotland) datasets. Two separate models are trained with learning rate 5×10⁻⁶ and batch size 64. The models are evaluated on matching test sets, cross-region test sets, and a British Isles accents baseline. Manual error analysis is conducted to identify transcription style differences and contextual bias issues.

## Key Results
- Fine-tuning reduces WER on matching test sets (NESAC: 0.240 vs 0.336 baseline, ~28.6% improvement)
- Cross-region transferability observed (SESHA model achieves 0.208 WER on NESAC test set)
- Manual analysis reveals WER limitations due to transcription style differences (spacing, spelling, date formats)
- Fine-tuned models exhibit contextual bias errors (e.g., "neutrally assured destruction" instead of "mutually assured destruction")

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fine-tuning Whisper on region-specific accented data reduces word error rate for matching dialects.
- **Mechanism:** Gradient updates from supervised training on accented speech adjust the encoder-decoder attention patterns to better align phonetic representations with underrepresented dialectal variations.
- **Core assumption:** The pre-trained Whisper model contains latent representations capable of distinguishing these accents but lacks sufficient exposure during pre-training to activate them reliably.
- **Evidence anchors:**
  - [abstract] "fine-tuning on a given data improves performance on the test dataset with the same domain and accent"
  - [section 6.1, Table 5] NESAC fine-tuned model achieved 0.240 WER on NESAC test data versus 0.336 for Whisper large-v3 baseline—a 28.6% relative improvement
  - [corpus] Limited direct evidence; corpus neighbors focus on dialect resources rather than fine-tuning mechanisms
- **Break condition:** Catastrophic forgetting may occur if fine-tuning data is too narrow or learning rate too high, degrading performance on general speech.

### Mechanism 2
- **Claim:** Fine-tuning on one Scottish dialect transfers to another Scottish dialect, suggesting shared phonetic features are learned.
- **Mechanism:** Regional accents within a geographic area share phonological patterns; fine-tuning amplifies sensitivity to these common features, creating cross-dialect generalization.
- **Core assumption:** North East and South East Scottish accents share sufficient acoustic-phonetic characteristics that improvements on one partially transfer to the other.
- **Evidence anchors:**
  - [abstract] "The fine-tuned models also appear to show improved performance when applied to the test data outside of the region it was trained on"
  - [section 6.1] SESHA fine-tuned model achieved 0.208 WER on NESAC test data, outperforming Whisper large baseline (0.336)
  - [corpus] No comparable transferability studies found in corpus neighbors
- **Break condition:** Transfer likely fails across more linguistically distant accents (e.g., Scottish to Southern English); geographic and phonological proximity is required.

### Mechanism 3
- **Claim:** WER metric inflation occurs due to transcription style differences rather than genuine recognition errors.
- **Mechanism:** WER penalizes semantically equivalent variations (e.g., "alright" vs "all right," British vs American spellings, date formats) that do not reflect comprehension failures.
- **Core assumption:** Human transcription conventions are the ground truth, but multiple valid transcription styles exist for the same spoken content.
- **Evidence anchors:**
  - [abstract] "Manual analysis highlights the limitations of WER as an evaluation metric, revealing transcription style differences"
  - [section 7.1, Figure 3] Post-processing normalization (spacing, spelling, date formats) reduced fine-tuned models' WER gap relative to baseline by addressing stylistic differences
  - [corpus] Weak corpus evidence; neighbors do not address WER limitations directly
- **Break condition:** Normalization rules are ad-hoc and may mask genuine errors; over-normalization could artificially deflate WER without improving actual comprehension.

## Foundational Learning

- **Concept: Word Error Rate (WER)**
  - Why needed here: Primary evaluation metric in the study; understanding its computation and limitations is essential for interpreting results.
  - Quick check question: If a model outputs "April 7th at 10:30" and the reference is "April the 7th at half past 10," what type of error does WER count this as?

- **Concept: Fine-tuning vs. continued pre-training**
  - Why needed here: The adaptation method used; distinguishing supervised fine-tuning from self-supervised continued pre-training clarifies data requirements.
  - Quick check question: Why does fine-tuning with ~47 hours of labeled data improve performance, while Whisper's original training required hundreds of thousands of hours?

- **Concept: Contextual bias in ASR**
  - Why needed here: Fine-tuned models exhibited errors like "neutrally assured destruction" instead of "mutually assured destruction," indicating overfitting to domain vocabulary at the expense of general language understanding.
  - Quick check question: If a housing-domain fine-tuned model transcribes "courtney" as "court name," what does this suggest about what the model learned?

## Architecture Onboarding

- **Component map:**
  - Whisper large-v3: Encoder-decoder Transformer trained on 680K hours of weakly supervised multilingual audio
  - Fine-tuning layer: Full model parameter updates (not adapter-based) with learning rate 5×10⁻⁶
  - Evaluation pipeline: WER computation + manual error categorization (spacing, homophones, reparandum, date formats, contextual bias)

- **Critical path:**
  1. Collect and transcribe ~47 hours of domain-specific accented audio (NESAC/SESHA)
  2. Reserve ~5 hours for test set (held-out before training)
  3. Fine-tune with low learning rate, batch size 64
  4. Evaluate on matching test set, cross-region test set, and baseline dataset
  5. Conduct manual error analysis on sample of discrepancies

- **Design tradeoffs:**
  - Domain specificity vs. generalization: Fine-tuning on narrow public-service data improves target accent recognition but may reduce contextual understanding of general speech (evidenced by "neutrally assured destruction" error)
  - WER transparency vs. accuracy: Raw WER is reproducible but conflates stylistic differences with errors; normalized WER better reflects comprehension but requires subjective rule selection

- **Failure signatures:**
  - Cross-region test WER exceeds baseline: Model overfit to training accent; reduce training epochs or increase learning rate
  - Baseline dataset WER increases significantly after fine-tuning: Catastrophic forgetting; apply regularization or use lower learning rate
  - Manual analysis shows phonetically plausible but contextually wrong transcriptions: Contextual bias introduced; consider mixing general speech data during fine-tuning

- **First 3 experiments:**
  1. Establish baseline WER of Whisper large-v3 on NESAC and SESHA test sets to quantify the accent gap relative to the open-source British Isles corpus
  2. Fine-tune separate models on NESAC and SESHA training data; evaluate on both matching and cross-region test sets to measure direct improvement and transferability
  3. Apply cumulative post-processing normalization (spacing → spelling → date formats) to human transcripts; re-compute WER to isolate transcription style effects from genuine recognition errors

## Open Questions the Paper Calls Out
- **Open Question 1:** To what extent do models fine-tuned on specific Scottish dialects transfer to the wider range of accents found across the rest of the United Kingdom? (The authors state they "hope to investigate the transferability of fine-tuned Whisper models further... by collecting more data that represents a wider range of accents from within the UK.")

- **Open Question 2:** Can fine-tuning strategies be modified to prevent the loss of general contextual understanding while maintaining improved accent recognition? (The paper identifies this trade-off as a drawback but does not propose or test mitigation strategies to preserve general language comprehension during domain adaptation.)

- **Open Question 3:** What alternative training approaches can achieve high performance on regional dialects without requiring access to confidential, sensitive data from public service calls? (The authors state they "aim to incorporate approaches that avoid the use of confidential and sensitive data" in future work.)

## Limitations
- NESAC and SESHA datasets are private and cannot be reproduced or independently validated
- WER is an imperfect metric that conflates transcription style differences with genuine recognition errors
- Transferability findings are limited to two Scottish dialect pairs and may not generalize to more distant accents

## Confidence
**High Confidence Claims:**
- Whisper large-v3 exhibits higher WER on regional Scottish accents compared to a British Isles baseline
- Fine-tuning on region-specific accented data reduces WER on matching test sets
- Manual analysis confirms WER limitations due to transcription style differences

**Medium Confidence Claims:**
- Cross-region transferability of fine-tuned models (based on two dialect pairs only)
- ~30% relative WER improvement after fine-tuning (dependent on private data)
- Contextual bias introduction during fine-tuning (evidenced by specific error examples)

**Low Confidence Claims:**
- The exact magnitude of WER improvement cannot be independently verified
- Normalization rules' impact on WER interpretation is speculative without systematic validation
- Generalizability to other regional accents or public service domains is unproven

## Next Checks
1. **Reproduce fine-tuning pipeline on accessible Scottish accent data:** Use publicly available Scottish English datasets (e.g., the British Isles corpus extended with Scottish samples) to fine-tune Whisper large-v3 and measure WER improvements.

2. **Conduct systematic manual error analysis:** Select 100 random utterances from fine-tuned model outputs and categorize errors into phonetic recognition failures vs. transcription style differences. Apply consistent normalization rules and measure WER changes.

3. **Test cross-dialect transferability beyond Scottish accents:** Fine-tune on North East Scottish data, then evaluate on both South East Scottish and a linguistically distinct accent (e.g., Northern English or Welsh).