---
ver: rpa2
title: 'LR-DWM: Efficient Watermarking for Diffusion Language Models'
arxiv_id: '2601.12376'
source_url: https://arxiv.org/abs/2601.12376
tags:
- watermarking
- diffusion
- token
- language
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces LR-DWM, a watermarking method for diffusion
  language models that avoids the computational and memory overhead of existing approaches.
  Instead of inverting the hashing process, it independently biases token logits using
  both left and right neighbors when available, allowing efficient embedding of a
  statistical watermark signal.
---

# LR-DWM: Efficient Watermarking for Diffusion Language Models

## Quick Facts
- arXiv ID: 2601.12376
- Source URL: https://arxiv.org/abs/2601.12376
- Authors: Ofek Raban; Ethan Fetaya; Gal Chechik
- Reference count: 27
- Key outcome: Achieves >99% TPR at 1% FPR with negligible runtime overhead compared to non-watermarked diffusion language models

## Executive Summary
LR-DWM introduces an efficient watermarking scheme for diffusion language models that avoids the computational overhead of existing methods. By leveraging both left and right neighboring tokens as independent sources of watermark signal through additive logit biasing, it enables detection without requiring sequential generation order. The method achieves high detectability (>99% TPR at 1% FPR) on LLaDA-8B and DREAM-7B models while maintaining negligible runtime and memory overhead. LR-DWM matches or exceeds the quality-detectability trade-off of prior diffusion watermarking approaches while remaining robust to standard non-adaptive attacks.

## Method Summary
LR-DWM embeds watermarks by independently biasing token logits based on both left and right neighboring tokens when available. Two hash functions with distinct keys map each neighbor to a green token set. The method uses additive logit modification where each constraint contributes δ bias, naturally favoring tokens satisfying both neighbors. Detection employs ternary scoring with empirical calibration to achieve controlled false positive rates. This order-agnostic design matches diffusion language models' non-monotonic decoding schedule while avoiding the computational costs of inverting the hashing process required by prior approaches.

## Key Results
- Achieves >99% TPR at 1% FPR on LLaDA-8B and DREAM-7B models
- Maintains negligible runtime and memory overhead compared to non-watermarked baseline
- Matches or exceeds quality-detectability trade-off of DMARK and WM-DLM methods
- Robust to standard non-adaptive attacks including word deletion and substitution

## Why This Works (Mechanism)

### Mechanism 1
Bidirectional neighbor-based hashing enables watermark signal embedding without requiring sequential generation order. Each token's watermark signal is anchored to whichever neighboring tokens (left, right, or both) are already determined at the current diffusion step. Two independent hash functions with distinct keys map each neighbor to a green token set. This order-agnostic design matches DLMs' non-monotonic decoding schedule. The core assumption is that sufficient neighbors become "locked in" during diffusion to propagate watermark constraints through the sequence.

### Mechanism 2
Additive logit biases from each direction naturally favor tokens satisfying both constraints, concentrating watermark signal. Logit modification sums independent contributions: l'_v = l_v + δ·I(v∈G_L) + δ·I(v∈G_R). Tokens matching both neighbors receive +2δ bias; single-sided matches receive +δ; non-matches receive 0. This creates gradient pressure toward higher detection scores without requiring explicit optimization or process inversion. The core assumption is that the model's base logits permit selection of green-list tokens without catastrophic quality degradation.

### Mechanism 3
Ternary scoring with empirical calibration enables reliable statistical detection under the null hypothesis. Each token contributes s_i ∈ {+1, 0, -1} based on dual-green-list membership. Under H₀ (human text), membership is random with E[s_i] = 0. The standardized sum Z = (1/σ√T)Σs_i yields approximately normal distribution, enabling threshold-based detection at controlled FPR. The core assumption is that adjacent token scores have only weak local dependence, which empirical calibration captures.

## Foundational Learning

- **Autoregressive watermarking (green/red list method)**
  - Why needed here: LR-DWM adapts the K-G watermarking paradigm to non-sequential generation. Understanding the original formulation clarifies what constraints must change.
  - Quick check question: In AR watermarking, what token(s) determine the green list for the next token, and why does this fail in DLMs?

- **Diffusion Language Model decoding dynamics**
  - Why needed here: DLMs update token positions non-monotonically across diffusion steps. The watermarking strategy must accommodate this unknown schedule.
  - Quick check question: At diffusion step t, can you guarantee that y_{i-1} is finalized before y_i? Why or why not?

- **Statistical hypothesis testing (Z-scores, FPR/TPR)**
  - Why needed here: Detection relies on calibrated Z-score thresholds. Understanding null/alternative hypotheses is essential for interpreting results.
  - Quick check question: If you set a detection threshold corresponding to 1% FPR, what fraction of human-written texts should (incorrectly) trigger detection?

## Architecture Onboarding

- Component map:
  - Hash functions H(·, k_L) and H(·, k_R): Map neighbor tokens → green sets
  - Logit modifier: Adds δ bias per satisfied constraint
  - Decoder: Samples from modified distribution at each diffusion step
  - Detector: Computes ternary scores, aggregates to Z-statistic, thresholds

- Critical path:
  1. At diffusion step s, identify positions I_s for update
  2. For each position i ∈ I_s: check if y_{i-1}, y_{i+1} are revealed
  3. Compute green sets G_L, G_R from available neighbors
  4. Apply additive logit bias
  5. Sample token; proceed to next diffusion step
  6. At detection: compute s_i per token, aggregate to Z, compare to threshold

- Design tradeoffs:
  - **Bias strength δ**: Higher → better detection, worse perplexity (Figure 3 shows sharp tradeoff)
  - **Detection threshold**: Lower → higher TPR but higher FPR
  - **Boundary handling**: Tokens at sequence edges fall back to single-sided constraints, slightly reducing signal density

- Failure signatures:
  - **Paraphrasing attacks**: Detection drops to ~16% (Table 2) due to disruption of local lexical structure
  - **Short sequences**: Insufficient tokens for statistical significance
  - **Very high detection requirements**: Quality degradation becomes non-negligible at 99.5%+ TPR

- First 3 experiments:
  1. Reproduce efficiency benchmark: Measure wall-clock time and peak GPU memory for vanilla DLM vs. LR-DWM on identical prompts. Verify overhead is negligible (Figure 1).
  2. Calibrate FPR on human text: Sample 1,000+ human-written documents; compute Z-scores; confirm empirical FPR matches target (e.g., 1%).
  3. Sweep δ and plot quality-detectability curve: Generate text at multiple bias strengths; plot TPR@1%FPR vs. perplexity. Compare to DMARK and WM-DLM baselines if available.

## Open Questions the Paper Calls Out

### Open Question 1
How does LR-DWM perform under adaptive adversarial attacks compared to the standard non-adaptive perturbations evaluated in the paper? The authors evaluate robustness against random or rule-based noise (deletion, substitution) but do not test against attackers who might know the watermarking scheme and optimize perturbations specifically to disrupt the green list statistics while preserving semantics.

### Open Question 2
Can the reliance on local bidirectional context be modified to withstand paraphrasing attacks without incurring the computational overhead the paper aims to avoid? While LR-DWM is efficient, its dependency on immediate left and right neighbors makes it fragile against semantic rewriting that alters token sequences. The paper does not propose a mechanism to bridge this fragility without reverting to heavy computation.

### Open Question 3
What is the precise trade-off curve between text quality and detection power when targeting extremely high detection thresholds (e.g., >99.9% TPR)? The experiments focus on standard operating points (up to 99.5% detection), but the authors acknowledge that pushing detectability further might degrade text naturalness significantly, a boundary not fully explored in the results.

### Open Question 4
Is there a theoretical or empirical minimum sequence length required for LR-DWM's ternary scoring system to maintain statistical significance? The evaluation uses fixed-length sequences (300 tokens), but the specific behavior of the standardized sum statistic Z on much shorter texts (e.g., < 100 tokens) remains uncharacterized.

## Limitations
- Performance degradation under paraphrasing attacks (detection drops to ~16%)
- Potential quality degradation at extremely high detection thresholds (>99.9% TPR)
- Requires minimum text length for statistical significance
- No evaluation against adaptive adversarial attacks

## Confidence
- **High**: Detection methodology using ternary scoring and Z-statistics (explicitly described in section 4.3)
- **High**: Efficiency claims regarding negligible runtime overhead (Figure 1 shows clear comparison)
- **Medium**: Quality-detectability trade-off curves (Figure 3 shows empirical results but with limited parameter sweeps)
- **Medium**: Robustness to standard attacks (Table 2 shows results but only for non-adaptive perturbations)

## Next Checks
1. Reproduce efficiency benchmark: Measure wall-clock time and peak GPU memory for vanilla DLM vs. LR-DWM on identical prompts. Verify overhead is negligible.
2. Calibrate FPR on human text: Sample 1,000+ human-written documents; compute Z-scores; confirm empirical FPR matches target (e.g., 1%).
3. Sweep δ and plot quality-detectability curve: Generate text at multiple bias strengths; plot TPR@1%FPR vs. perplexity. Compare to DMARK and WM-DLM baselines if available.