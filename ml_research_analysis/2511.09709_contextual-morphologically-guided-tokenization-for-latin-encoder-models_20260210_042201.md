---
ver: rpa2
title: Contextual morphologically-guided tokenization for Latin encoder models
arxiv_id: '2511.09709'
source_url: https://arxiv.org/abs/2511.09709
tags:
- morphological
- latin
- language
- tokenization
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates morphologically-aware tokenization for Latin
  language models, comparing standard BPE/WordPiece methods with approaches that incorporate
  morphological analysis. The key innovation is using a contextual morphological segmenter
  based on part-of-speech tagging to guide tokenization.
---

# Contextual morphologically-guided tokenization for Latin encoder models

## Quick Facts
- arXiv ID: 2511.09709
- Source URL: https://arxiv.org/abs/2511.09709
- Reference count: 27
- Key outcome: Morphologically-aware tokenization improves Latin language model performance, especially on out-of-domain texts

## Executive Summary
This paper evaluates morphologically-aware tokenization approaches for Latin language models, comparing standard BPE/WordPiece methods with morphological analysis-guided approaches. The key innovation is a contextual morphological segmenter that uses part-of-speech tagging to guide tokenization decisions. Results demonstrate that incorporating morphological awareness improves performance on downstream tasks, particularly for out-of-domain texts where standard tokenization approaches struggle. The study shows that linguistic resources can help compensate for limited pretraining data in morphologically rich languages.

## Method Summary
The authors develop a contextual morphological segmenter that integrates morphological analysis with tokenization by using POS tagging to guide word segmentation decisions. This approach contrasts with standard byte-pair encoding and WordPiece methods that treat morphology as an opaque sequence of characters. The morphological segmenter breaks words into their constituent morphemes based on their grammatical functions, enabling the model to better capture morphological relationships. The approach is evaluated on Latin, a highly inflected language where morphological information is crucial for meaning and grammatical function.

## Key Results
- Morphological feature tagging accuracy improves by 1.5-1.3 percentage points
- Named Entity Recognition F1 improves by 7.6-6.6 points
- Performance gains are most pronounced for out-of-domain texts, demonstrating better generalization

## Why This Works (Mechanism)
Morphologically-aware tokenization works by exposing the internal structure of words to the model, allowing it to learn morphological patterns directly rather than having to infer them from character sequences. In highly inflected languages like Latin, words can have dozens of forms depending on case, number, gender, tense, voice, and mood. By breaking words into their constituent morphemes, the model can recognize shared patterns across different word forms and generalize better to unseen combinations. The contextual component ensures that segmentation decisions account for the surrounding linguistic context, avoiding incorrect segmentations that might occur with purely rule-based approaches.

## Foundational Learning
- **Morphological segmentation**: Breaking words into constituent morphemes (roots, prefixes, suffixes) - needed to understand word structure in highly inflected languages; quick check: identify morphemes in "amabant" (ama-bant)
- **Byte-Pair Encoding (BPE)**: Frequency-based subword tokenization algorithm - needed as baseline comparison; quick check: trace BPE merges in "amabant" â†’ "ama" + "bant"
- **Part-of-Speech tagging**: Assigning grammatical categories to words - needed to provide contextual information for morphological segmentation; quick check: tag "puella" as noun, feminine, singular
- **Named Entity Recognition**: Identifying proper nouns and named entities - needed as downstream evaluation task; quick check: identify "Caesar" as a person entity
- **Out-of-domain generalization**: Model performance on texts different from training data - needed to evaluate real-world applicability; quick check: test on medical vs. literary Latin texts

## Architecture Onboarding

**Component map:**
Raw text -> POS tagger -> Morphological segmenter -> Tokenized input -> Language model

**Critical path:**
The critical path runs through the POS tagger to the morphological segmenter, as segmentation decisions depend on grammatical context. Errors in POS tagging propagate to incorrect morphological segmentation, which then affects model performance.

**Design tradeoffs:**
The approach trades computational complexity for linguistic accuracy. The POS tagger adds processing overhead but provides crucial context for segmentation decisions. Alternative approaches might use simpler segmentation rules but would lose contextual sensitivity.

**Failure signatures:**
- POS tagging errors leading to incorrect morpheme boundaries
- Over-segmentation creating too many rare tokens
- Under-segmentation missing important morphological distinctions
- Domain mismatch between POS tagger training data and target text

**First experiments to run:**
1. Compare segmentation accuracy on morphologically complex vs. simple words
2. Measure POS tagging accuracy degradation on out-of-domain texts
3. Analyze token frequency distributions for standard vs. morphological tokenization

## Open Questions the Paper Calls Out
None

## Limitations
- The study focuses exclusively on Latin, limiting generalizability to other morphologically rich languages
- Evaluation relies on specific downstream tasks (morphological feature tagging and NER) that may not capture all aspects of language understanding
- The morphological segmenter's dependency on POS tagging introduces potential cascading errors that could affect tokenization quality

## Confidence

**High confidence:**
- Performance improvements on downstream tasks are well-documented with specific metrics across multiple datasets

**Medium confidence:**
- Generalizability claims to other morphologically rich languages, as the study is limited to Latin
- The mechanism by which morphological awareness improves out-of-domain performance, though plausible, lacks detailed ablation studies

## Next Checks
1. Test the morphological tokenization approach on additional morphologically rich languages (e.g., Ancient Greek, Sanskrit, or Finnish) to assess generalizability
2. Conduct ablation studies to isolate the contribution of morphological awareness versus other factors (e.g., tokenization vocabulary size, pretraining data)
3. Evaluate performance on additional downstream tasks (e.g., syntactic parsing, semantic role labeling) to assess broader applicability of the approach