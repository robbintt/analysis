---
ver: rpa2
title: Uncovering Gaps in How Humans and LLMs Interpret Subjective Language
arxiv_id: '2503.04113'
source_url: https://arxiv.org/abs/2503.04113
tags:
- thesaurus
- more
- phrase
- language
- phrases
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "TED uncovers surprising misalignments between how LLMs and humans\
  \ interpret subjective language. By comparing the LLM\u2019s operational thesaurus\
  \ (capturing how prompts with different subjective phrases affect outputs) to a\
  \ human-constructed semantic thesaurus, TED finds pairs of phrases that have similar\
  \ or dissimilar effects under the LLM but not as expected by humans."
---

# Uncovering Gaps in How Humans and LLMs Interpret Subjective Language

## Quick Facts
- arXiv ID: 2503.04113
- Source URL: https://arxiv.org/abs/2503.04113
- Reference count: 40
- Key outcome: TED reveals surprising misalignments between human and LLM interpretations of subjective language, with implications for downstream task failures

## Executive Summary
This paper introduces TED (Term Evaluation Difference), a novel methodology for uncovering unexpected behavior in large language models when interpreting subjective language. By comparing an LLM's operational thesaurus—which captures how different subjective phrases affect model outputs—with a human-constructed semantic thesaurus, TED identifies pairs of phrases that produce similar or dissimilar effects under the LLM compared to human expectations. These "clashes" can predict failures in downstream tasks like output editing and inference steering, where prompting with one phrase unexpectedly induces behavior associated with another.

The methodology reveals striking examples: Llama 3 8B's "enthusiastic" articles are "dishonest" 97% of the time, while Mistral 7B's "witty" edits are "harassing" 78% of the time. By scrutinizing abstract concept relationships, TED exposes unexpected LLM behavior without directly supervising outputs, demonstrating that these uncovered failures have much higher success rates than baseline approaches.

## Method Summary
TED constructs two parallel thesauri: a human thesaurus where people judge semantic similarity between subjective phrases, and an LLM thesaurus where the model's responses to different prompts are compared to identify operational similarity. The methodology then identifies "clashes" where human and LLM interpretations diverge—either phrases that humans consider dissimilar but the LLM treats as similar, or vice versa. For downstream validation, TED tests whether these clashes predict failures in practical tasks like content editing (steering model outputs toward desired styles) and inference steering (controlling model behavior through subjective prompts). The approach requires no labeled output supervision, instead relying on comparing internal operational patterns against human semantic expectations.

## Key Results
- Llama 3 8B interprets "enthusiastic" as "dishonest" 97% of the time, revealing a significant semantic clash
- Mistral 7B's "witty" edits are "harassing" 78% of the time, demonstrating unexpected behavioral associations
- TED-discovered failures show much higher success rates than baseline approaches for detecting LLM behavioral misalignments

## Why This Works (Mechanism)
TED works by exposing the gap between human semantic understanding and LLM operational behavior. While humans interpret subjective phrases based on cultural and contextual meaning, LLMs develop operational associations through training data patterns. When these operational associations diverge from human semantic expectations, the LLM may exhibit unexpected behavior when prompted with certain phrases. By systematically comparing these two interpretation frameworks, TED identifies precisely where these divergences occur, revealing blind spots in LLM behavior that would be difficult to discover through direct output analysis alone.

## Foundational Learning
- **Semantic thesaurus construction**: Humans judge phrase similarity through pairwise comparisons (why needed: establishes ground truth for human interpretation; quick check: inter-rater reliability analysis)
- **Operational thesaurus mapping**: LLM response patterns are compared to identify functional phrase similarity (why needed: captures how LLMs actually use subjective language; quick check: clustering quality metrics)
- **Clash identification**: Systematic comparison of human and LLM thesauri to find misalignments (why needed: pinpoints specific failure modes; quick check: statistical significance of mismatches)
- **Downstream validation framework**: Testing whether clashes predict practical task failures (why needed: demonstrates real-world impact; quick check: baseline comparison performance)
- **Prompt engineering methodology**: Systematic generation of prompts for testing (why needed: ensures consistent experimental conditions; quick check: prompt diversity analysis)
- **Behavioral signature extraction**: Quantifying how prompts affect output characteristics (why needed: enables operational comparison; quick check: feature importance rankings)

## Architecture Onboarding
**Component Map:** Subjective phrases → Human Thesaurus (pairwise similarity judgments) -> LLM Thesaurus (response pattern comparison) -> Clash Detection (semantic vs operational mismatch) -> Downstream Validation (task failure prediction)

**Critical Path:** Phrase selection → Human evaluation → LLM response collection → Operational similarity computation → Clash identification → Downstream task testing

**Design Tradeoffs:** Manual human evaluation provides semantic ground truth but limits scale; automated LLM response collection enables systematic testing but requires careful prompt engineering; clash detection balances precision (missing some misalignments) against recall (false positives)

**Failure Signatures:** Unexpected output characteristics when clash phrases are used; systematic patterns of task failure correlating with specific clash types; consistent behavioral shifts across multiple downstream applications

**Three First Experiments:** 1) Validate clash predictions on held-out subjective phrases not used in thesaurus construction; 2) Test clash impact on additional downstream tasks like content moderation or creative writing; 3) Compare clash patterns across different model sizes and architectures

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Subjective phrase selection from existing thesauri may not capture real-world language usage breadth
- Clash patterns are model-specific and may not generalize across different LLMs
- Human evaluation relies on crowdsourced judgments that may introduce noise and interpretation variations

## Confidence
**High Confidence:**
- TED methodology is technically sound and novel
- Empirical results showing specific clash examples are reproducible
- Connection between clashes and downstream task failures is demonstrated

**Medium Confidence:**
- General importance of these misalignments for practical applications
- Semantic similarity directly causes downstream failures
- Represents broader pattern across subjective language interpretation

**Low Confidence:**
- Extent of generalization to untested subjective phrases
- Systematic discovery of all significant behavioral gaps
- Practical utility for improving LLM fine-tuning or prompting

## Next Checks
1. **Cross-Model Validation**: Apply TED to diverse LLMs to determine if clash patterns are universal or model-specific

2. **Phrase Space Expansion**: Test TED with dynamically generated subjective phrases rather than curated thesaurus entries

3. **Downstream Task Generalization**: Validate failure predictions across additional subjective language applications like content moderation and personalized recommendations