---
ver: rpa2
title: 'RCP-Merging: Merging Long Chain-of-Thought Models with Domain-Specific Models
  by Considering Reasoning Capability as Prior'
arxiv_id: '2508.03140'
source_url: https://arxiv.org/abs/2508.03140
tags:
- reasoning
- merging
- task
- rcp-merging
- eosinoplus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RCP-Merging addresses the challenge of merging domain-specific
  models with long chain-of-thought reasoning models without degrading reasoning capability.
  The method introduces a reasoning preservation indicator that treats reasoning model
  weights as a prior, using Fisher Information Matrix gradients to constrain updates
  and preserve long CoT capability.
---

# RCP-Merging: Merging Long Chain-of-Thought Models with Domain-Specific Models by Considering Reasoning Capability as Prior

## Quick Facts
- **arXiv ID**: 2508.03140
- **Source URL**: https://arxiv.org/abs/2508.03140
- **Reference count**: 22
- **Primary result**: Improves domain task performance by 9.5% in BioMedicine and 9.2% in Finance over state-of-the-art methods while maintaining long CoT reasoning capability with only 14.3% gibberish rate

## Executive Summary
RCP-Merging addresses the challenge of merging domain-specific models with long chain-of-thought reasoning models without degrading reasoning capability. The method introduces a reasoning preservation indicator that treats reasoning model weights as a prior, using Fisher Information Matrix gradients to constrain updates and preserve long CoT capability. It combines this with domain knowledge sensitivity to identify crucial domain-specific parameters. Extensive experiments on Qwen2.5-7B, Llama3.1-8B, and Qwen2.5-1.5B models show RCP-Merging significantly improves domain task performance while maintaining reasoning capability and producing stable outputs.

## Method Summary
RCP-Merging computes task vectors (δ = θ_finetuned - θ_pre) for both domain-specific and reasoning models, then selectively merges them using a binary mask. The method calculates Domain Knowledge Sensitivity via gradient × weight products on domain calibration data, and Reasoning Preservation Indicator using diagonal Fisher Information Matrix from reasoning calibration data. A majority-vote constraint metric combines these indicators to generate a binary mask that gates which domain task vector components merge with the reasoning vector. The final merged weights are θ_merged = θ_pre + δ_reasoning + λ·(mask ⊙ δ_domain), with λ_r = 0.3 for 7B models and 0.7 for 1.5B models.

## Key Results
- Improves domain task performance by 9.5% in BioMedicine and 9.2% in Finance over state-of-the-art methods
- Maintains long CoT reasoning capability as evidenced by GSM8K and AIME score preservation
- Produces stable outputs with only 14.3% gibberish rate compared to 82.3-99.7% for baseline methods

## Why This Works (Mechanism)

### Mechanism 1: Reasoning Preservation via Bayesian Priors
Treating reasoning model weights as a prior distribution prevents catastrophic forgetting of long CoT capabilities during model merging. RCP-Merging uses Fisher Information Matrix (FIM) gradients to identify weights critical for reasoning, then applies a preservation penalty (derived from Laplace approximation of the posterior) constraining how much merged weights can deviate from reasoning-optimal values.

### Mechanism 2: Domain Knowledge Sensitivity-Based Selective Merging
First-order Taylor approximation of loss sensitivity identifies which domain-specific parameters are crucial for domain performance. RCP computes `S_i,k ≈ ||g_i,dk · θ_i^t||`—the product of gradient and weight magnitude on calibration data. Lower sensitivity indicates positive contribution to domain performance, creating a domain importance mask that selectively retains beneficial weights.

### Mechanism 3: Majority-Vote Conflict Resolution for Parameter Selection
A constraint metric combining sensitivity and preservation indicators, filtered through majority voting, resolves conflicts between reasoning preservation and domain integration. RCP computes `C_i,k = S_i,k + λ_r · p_i,r,t` for each parameter on each calibration sample. Parameters update only if a majority yield negative constraint scores.

## Foundational Learning

- **Task Vectors in Model Merging**: Understanding that fine-tuned knowledge = `δ = θ_ft - θ_pre` is essential to grasp how RCP filters domain task vectors while preserving reasoning vectors. *Quick check*: Given θ_pre=0.5 and θ_ft=0.8, what is the task vector? What happens if you scale by λ=0.3?

- **Fisher Information Matrix for Parameter Importance**: FIM quantifies how much information each parameter carries about data distribution, enabling identification of reasoning-critical weights through expected squared gradients. *Quick check*: If gradient `g_i,d = 0.1` for a parameter across all calibration samples, what does this imply about Fisher information F_i?

- **Bayesian Prior in Neural Network Regularization**: RCP treats reasoning weights as a prior in MAP estimation. The preservation penalty is mathematically equivalent to FIM-weighted L2 regularization. *Quick check*: If the reasoning preservation penalty dominates optimization, where will merged weights cluster? What does λ_r control?

## Architecture Onboarding

- **Component map**: Calibration Data Loader -> Gradient Computer -> FIM Estimator -> Sensitivity Calculator -> Preservation Matrix Builder -> Constraint Evaluator -> Task Vector Merger
- **Critical path**: Calibration selection → Gradient computation for FIM → Majority-vote mask generation → Final weight assembly
- **Design tradeoffs**: FIM diagonal vs. full (diagonal is efficient but ignores parameter correlations); λ_r hyperparameter varies by model size (0.3 for 7B, 0.7 for 1.5B); majority vote vs. threshold (binary mask is simple but continuous weighting could be more nuanced)
- **Failure signatures**: Gibberish output (>50%) indicates insufficient reasoning preservation; domain performance drops below original model suggests over-aggressive masking; repetitive outputs indicate unresolved parameter conflicts; truncation indicates task misunderstanding
- **First 3 experiments**: 1) Baseline sanity check: Merge with Task Arithmetic λ=0.3 (expect >80% gibberish); 2) Ablation study: Run full RCP vs. w/o domain sensitivity vs. w/o reasoning preservation (expect drops from 68.3→48.7→41.4); 3) Hyperparameter sweep: Vary λ_r 0.1-1.0 on 7B and 1.5B models

## Open Questions the Paper Calls Out

- **Open Question 1**: How does RCP-Merging performance scale with the quantity and quality of calibration samples, and what is the minimum calibration set size required before performance degrades significantly?
- **Open Question 2**: How much performance improvement would be gained by using the full Fisher Information Matrix instead of the diagonal approximation, and is the computational overhead justified?
- **Open Question 3**: Can RCP-Merging be extended to merge multiple domain-specific models simultaneously with a reasoning model, and how should the constraint mechanism handle competing domain sensitivities?
- **Open Question 4**: Is there a principled method to automatically determine the optimal reasoning-preserving coefficient (λ_r) without manual tuning for different model sizes?

## Limitations
- The FIM diagonal approximation may fail for complex reasoning capabilities encoded in parameter correlations
- Calibration data quality and representativeness heavily impact sensitivity estimates and reasoning pattern capture
- Per-parameter conflict resolution may not accommodate coordinated parameter changes required for reasoning capabilities

## Confidence
- **High Confidence**: Domain performance improvements (9.5% BioMedicine, 9.2% Finance over SOTA) and gibberish rate reduction (14.3% vs 82-99% baselines) are well-supported
- **Medium Confidence**: Reasoning preservation effectiveness is supported by GSM8K/AIME score maintenance, but FIM diagonal approximation's adequacy remains uncertain
- **Low Confidence**: "Produces stable outputs" claim is primarily supported by gibberish rate comparison rather than comprehensive stability analysis

## Next Checks
1. **FIM Approximation Validation**: Systematically compare diagonal FIM against full FIM for a subset of parameters to quantify information loss and test reasoning preservation degradation
2. **Calibration Data Sensitivity Analysis**: Conduct ablation studies varying calibration dataset sizes (16, 64, 256, 512 samples) to determine minimum viable calibration requirements
3. **Long-term Reasoning Retention**: Implement longitudinal evaluation tracking reasoning performance (GSM8K, AIME) over extended inference periods (100+ queries) to detect potential reasoning capability drift