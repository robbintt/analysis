---
ver: rpa2
title: 'FiCoTS: Fine-to-Coarse LLM-Enhanced Hierarchical Cross-Modality Interaction
  for Time Series Forecasting'
arxiv_id: '2512.00293'
source_url: https://arxiv.org/abs/2512.00293
tags:
- time
- series
- forecasting
- text
- modality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'FiCoTS is a fine-to-coarse LLM-enhanced hierarchical framework
  for multimodal time series forecasting that uses LLM only to encode text as an enhancer
  rather than as the predictor, overcoming semantic misalignment between time series
  and text modalities. The framework progressively aligns and fuses modalities through
  three semantic levels: token-level dynamic heterogeneous graph alignment, feature-level
  global cross-attention interaction, and decision-level adaptive gated fusion.'
---

# FiCoTS: Fine-to-Coarse LLM-Enhanced Hierarchical Cross-Modality Interaction for Time Series Forecasting

## Quick Facts
- **arXiv ID**: 2512.00293
- **Source URL**: https://arxiv.org/abs/2512.00293
- **Reference count**: 40
- **Primary result**: FiCoTS achieves SOTA performance on 7 time series forecasting benchmarks, with 2.0% MSE improvement over TimeVLM, using LLM only as text encoder (not predictor).

## Executive Summary
FiCoTS introduces a fine-to-coarse hierarchical framework for multimodal time series forecasting that leverages LLM only to encode text as an enhancer rather than as the predictor. The framework progressively aligns and fuses modalities through three semantic levels: token-level dynamic heterogeneous graph alignment, feature-level global cross-attention interaction, and decision-level adaptive gated fusion. Evaluated on seven real-world benchmarks, FiCoTS demonstrates state-of-the-art performance with significant improvements over leading LLM-based methods, while also showing strong few-shot learning capability.

## Method Summary
FiCoTS implements a three-stage hierarchical interaction scheme for multimodal time series forecasting. First, a dual stream encoder processes time series patches and text prompts separately (using frozen GPT-2 for text). Second, token-level alignment constructs a dynamic heterogeneous graph where time series patches connect to relevant text tokens based on cosine similarity with adaptive thresholding. Third, feature-level cross-attention allows temporal features to attend to textual contexts, followed by decision-level gated fusion that combines enhanced and original temporal predictions. The framework uses a fine-to-coarse approach (token→feature→decision) specifically designed for time series forecasting where local patterns are critical.

## Key Results
- Achieves state-of-the-art performance with MSE improvements over leading LLM-based methods (e.g., 2.0% vs. TimeVLM on average)
- Demonstrates strong few-shot learning capability, maintaining consistent gains even with only 5-10% training data
- Ablation studies confirm necessity of each hierarchical component and advantage of fine-to-coarse scheme over coarse-to-fine approaches

## Why This Works (Mechanism)

### Mechanism 1: Fine-to-Coarse Hierarchical Interaction Scheme
- Claim: Fine-to-coarse cross-modality interaction (token→feature→decision) is better suited for time series forecasting than coarse-to-fine approaches.
- Mechanism: Time series data has 1D continuous, causal structure where local fluctuations critically influence predictions. Starting with fine-grained token-level alignment allows time series patches to flexibly connect with relevant text tokens before coarser global interactions, preventing over-smoothing of temporal patterns.
- Core assumption: Text serves as auxiliary (not primary) information in time series forecasting, with no strict semantic correspondence between text and time series.
- Evidence anchors: [abstract], [section 4.1] with text on local fluctuations being critical; weak direct evidence from corpus.
- Break condition: If text data has explicit, fine-grained correspondence to specific time series variables, coarse-to-fine might perform equivalently or better.

### Mechanism 2: LLM-as-Enhancer (Not Predictor) Paradigm
- Claim: Using LLM only to encode text modality mitigates semantic misalignment between time series and text while reducing computational cost.
- Mechanism: LLM processes text prompts to generate contextual embeddings via frozen GPT-2, complementing time series features through cross-attention rather than having LLM directly process numerical time series.
- Core assumption: LLM's text understanding capability transfers useful semantic context to time series even without LLM directly ingesting numerical patterns.
- Evidence anchors: [abstract], [section 2.2] with studies showing LLMs offer minimal advantages for time series analysis; BALM-TSF and MAP4TS support this paradigm.
- Break condition: If text prompts lack domain-relevant information, LLM enhancement will be minimal.

### Mechanism 3: Dynamic Heterogeneous Graph for Token-Level Alignment
- Claim: Dynamic heterogeneous graph filters noise and aligns time series patches with semantically relevant text tokens more effectively than homogeneous graphs or simple concatenation.
- Mechanism: Cosine similarity between time series patches and text tokens forms initial similarity matrix; time-determined dynamic threshold (μi + α·σi) filters weak connections; GraphSAGE propagates information bidirectionally between modalities.
- Core assumption: Sparse, dynamically-filtered connections reduce noise better than fixed thresholds or full connectivity.
- Evidence anchors: [abstract], [section 4.3, Table 4] showing ablation results; no direct comparison in corpus.
- Break condition: If α filtering sensitivity is set too low, connections become too sparse; if too high, noise increases.

## Foundational Learning

- **Concept: Patch-based Time Series Representation**
  - Why needed here: Time series is segmented into patches before embedding to enable token-level alignment with text tokens.
  - Quick check question: Can you explain why patching (vs. point-wise processing) enables better cross-modality alignment?

- **Concept: Cross-Attention for Multimodal Fusion**
  - Why needed here: Feature-level interaction uses multi-head cross-attention where time series serves as Query and text as Key/Value, allowing temporal features to attend to relevant textual contexts.
  - Quick check question: In cross-attention, what happens if you swap Query and Key modalities?

- **Concept: Gated Fusion Mechanisms**
  - Why needed here: Decision-level fusion uses learned gates to adaptively weight two prediction branches, preserving original patterns while incorporating multimodal information.
  - Quick check question: Why use a gate instead of simple averaging for fusing predictions?

## Architecture Onboarding

- **Component map:** Linear patch embedding (time series) -> GPT-2 frozen encoder (text) -> Dynamic heterogeneous graph (GraphSAGE) -> Global cross-attention -> Gated fusion

- **Critical path:** Text prompt quality → LLM embedding quality → similarity matrix construction → dynamic filtering threshold (α) → graph edge sparsity → cross-attention effectiveness → gate learning stability

- **Design tradeoffs:**
  - Fine-to-coarse preserves local patterns but requires three separate modules vs. one unified fusion
  - Heterogeneous graph preserves modality distinctiveness but prevents within-modality message passing
  - Gated fusion with residual branch preserves original temporal patterns but adds parameter overhead
  - Frozen LLM reduces computational cost but limits domain adaptability

- **Failure signatures:**
  - Performance drops close to baselines → check text prompt relevance and LLM embedding quality
  - No improvement from token-level module → α filtering too aggressive (sparse edges) or too permissive (noise)
  - Degradation on certain datasets → model dimension mismatch (smaller datasets need smaller dimensions)
  - Few-shot worse than full-data → input length too short (512 optimal)

- **First 3 experiments:**
  1. **Baseline sanity check**: Run FiCoTS with text modality ablated (w/o LLM). Confirm ~1.1-1.7% MSE degradation per Table 4 to verify implementation.
  2. **Filtering sensitivity sweep**: Vary α ∈ {0.3, 0.5, 0.7} on validation split. Confirm 0.5 is optimal; document performance curve.
  3. **Paradigm comparison**: Compare FiCoTS vs. LLM-as-Predictor baseline (e.g., Time-LLM or GPT4TS) on same benchmark. Confirm FiCoTS shows ~3-5% MSE improvement.

## Open Questions the Paper Calls Out
None

## Limitations
- Hyperparameter specifics like exact patching parameters and precise prompt templates require inference from related work
- Performance on long-horizon or high-frequency forecasting tasks remains untested
- Three-stage hierarchical interaction adds architectural complexity that may impact training efficiency on very large-scale datasets

## Confidence
- **High Confidence**: Core FiCoTS framework design and state-of-the-art performance on seven benchmark datasets
- **Medium Confidence**: Specific mechanism by which dynamic heterogeneous graph filtering improves performance
- **Medium Confidence**: Claimed robustness in few-shot settings (5-10% data)

## Next Checks
1. **Parameter sensitivity validation**: Systematically vary α (filtering sensitivity) and model dimensions across all seven datasets to confirm optimal settings and map full performance landscape.

2. **Fine-to-coarse vs coarse-to-fine ablation**: Implement and compare FiCoTS against coarse-to-fine variant on same benchmarks to empirically validate claimed advantage.

3. **Prompt quality dependence**: Create controlled experiments varying prompt informativeness to quantify LLM enhancement's dependence on prompt quality.