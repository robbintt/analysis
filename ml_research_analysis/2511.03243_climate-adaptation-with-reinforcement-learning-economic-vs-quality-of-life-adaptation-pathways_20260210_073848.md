---
ver: rpa2
title: 'Climate Adaptation with Reinforcement Learning: Economic vs. Quality of Life
  Adaptation Pathways'
arxiv_id: '2511.03243'
source_url: https://arxiv.org/abs/2511.03243
tags:
- adaptation
- impacts
- available
- travel
- economic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of designing climate adaptation
  policies under uncertainty, specifically for urban flood risks. It proposes a reinforcement
  learning-based framework integrated with an assessment model linking rainfall projections,
  flood simulations, transport routing, and quality of life indices.
---

# Climate Adaptation with Reinforcement Learning: Economic vs. Quality of Life Adaptation Pathways

## Quick Facts
- arXiv ID: 2511.03243
- Source URL: https://arxiv.org/abs/2511.03243
- Reference count: 33
- Primary result: RL-based climate adaptation framework shows economic-focused (EC) and quality-of-life (QoL) policies yield dramatically different spending patterns and geographic distributions in Copenhagen flood risk planning.

## Executive Summary
This paper presents a reinforcement learning framework for designing climate adaptation policies under uncertainty, specifically for urban flood risks. The framework integrates rainfall projections, flood simulations, transport routing, and quality of life indices through an Integrated Assessment Model (IAM). Two adaptation strategies are compared: one focused solely on economic impacts (EC) and another prioritizing quality of life (QoL). The study finds that incorporating QoL into adaptation planning leads to significantly different policy pathways and resource allocations, demonstrating the importance of explicitly modeling societal trade-offs in climate adaptation.

## Method Summary
The framework uses reinforcement learning (PPO algorithm via Stable-Baselines3) to learn optimal adaptation policies over a 75-year horizon (2023-2100) in Copenhagen's 29 Traffic Analysis Zones. It chains together domain-specific models: rainfall sampling from Klimaatlas probability distributions, SCALGO terrain-based flood depth modeling, transport routing with depth-disruption functions, and impact aggregation (infrastructure damage, delays, QoL). The agent selects from 8 discrete adaptation actions per TAZ to maximize a reward function combining infrastructure damage, travel delays, trip cancellations, QoL impacts, and adaptation costs. Two policy variants are trained with different reward weight configurations: EC weights (βI=βD=βC=βA=βM=1, βQ=0) and QoL weights (βQ=0.5, βA=βM=1, βI=βD=βC=0).

## Key Results
- EC model resulted in lower spending (DKK 2.1 billion over 75 years, ~280 million USD) and fewer actions (0.103 per year), while QoL model spent more (DKK 20 billion, ~2.7 billion USD) with more actions (0.559 per year)
- QoL-focused policy distributed spending more evenly across the study area and used a wider range of adaptation actions
- The framework successfully links climate projections through physical impacts to policy decisions using RL

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scalar reward weight configurations produce divergent spatial adaptation pathways.
- Mechanism: The reward function R = Σᵢ(βI·Iᵢ + βD·Dᵢ + βC·Cᵢ + βQ·Qᵢ + βA·Aᵢ + βM·Aᵢ) translates normative priorities into learnable signals. Setting βQ = 0.5 while zeroing economic β weights forces the agent to maximize QoL retention regardless of spending, whereas βQ = 0 with positive economic weights triggers cost-sensitive investment.
- Core assumption: Normative trade-offs can be adequately captured through linear scalarization without inducing reward gaming.
- Evidence anchors:
  - [abstract]: "models prioritising QoL over economic impacts results in more adaptation spending as well as a more even distribution of spending over the study area"
  - [section 3]: "the QoL-focused policy distributed its spending more evenly over the study area and used a wider range of actions when compared to the EC-focused policy"
  - [corpus]: arxiv 2504.10031 and 2601.18586 similarly use reward-weighted QoL/economic trade-offs in climate adaptation RL, supporting generalizability of this mechanism.

### Mechanism 2
- Claim: Cascading domain-specific models enables end-to-end policy learning from climate projections to infrastructure decisions.
- Mechanism: The IAM chains: (1) rainfall sampling from Klimaatlas probability distributions → (2) SCALGO terrain-based flood depth modeling → (3) transport routing with depth-disruption functions → (4) impact aggregation (infrastructure damage, delays, QoL) → (5) reward signal. Each module passes structured outputs to the next, creating a differentiable pathway from action to consequence.
- Core assumption: Model coupling preserves sufficient fidelity for policy decisions; errors do not compound destructively.
- Evidence anchors:
  - [abstract]: "We use an Integrated Assessment Model (IAM) to link together a rainfall and flood model, and compute the impacts of flooding in terms of quality of life (QoL), transportation, and infrastructure damage"
  - [section 2.1-2.3]: Detailed module descriptions with depth-disruption functions [15,16] and depth-damage functions [19]
  - [corpus]: arxiv 2601.18586 uses similar IAM coupling for transport adaptation; arxiv 2510.26017 employs cascading neural networks for coastal flood prediction, suggesting this pattern is emerging but not yet validated across domains.

### Mechanism 3
- Claim: PPO learns temporal allocation strategies balancing immediate costs against long-term impact reduction over a 75-year horizon.
- Mechanism: The agent observes state (flood depths, current infrastructure, TAZ-level impacts), selects from 8 adaptation actions (drainage/storage measures), and receives delayed reward. PPO's clipped objective prevents destructive policy updates during long-horizon credit assignment.
- Core assumption: The single-event-per-year simplification adequately represents climate uncertainty for policy learning.
- Evidence anchors:
  - [section 2.4]: "Over time, the agent learns to balance trade-offs between competing actions and input uncertainty and the best sequence of policies that maximise the cumulative reward"
  - [section 3]: "the agent takes about 0.103 adaptation measures per year until 2100" (EC) vs. "0.559 per year" (QoL)
  - [corpus]: Weak direct evidence on algorithm suitability; no comparative RL algorithm analysis in this paper or neighbors. Assumption: PPO is appropriate for this sparse-action, long-horizon problem.

## Foundational Learning

- Concept: Proximal Policy Optimization (PPO)
  - Why needed here: Core algorithm for learning sequential adaptation policies with stable updates over long horizons.
  - Quick check question: How does PPO's clipping objective prevent large policy degradation during training on sparse-reward, long-horizon problems?

- Concept: Integrated Assessment Models (IAMs)
  - Why needed here: Architectural pattern for coupling climate, impact, and economic models; understanding error propagation is critical.
  - Quick check question: If SCALGO underestimates flood depth by 10cm on average, how does this bias propagate through depth-damage and depth-disruption functions?

- Concept: Depth-disruption and depth-damage functions
  - Why needed here: These translate physical flood depths into economic losses and mobility impacts—the core of the reward signal.
  - Quick check question: What happens to trip cancellation estimates if the depth threshold for "impassable" is misspecified by 5cm?

## Architecture Onboarding

- Component map:
  1. **Rainfall Projection** (Klimaatlas, RCP4.5): Samples one rainfall event/year from probability distributions
  2. **Flood Model** (SCALGO Live): Computes water depth distribution from precipitation + terrain
  3. **Transport Model**: Routing (OSM/osmnx, 29 TAZs) + Accessibility (h3 hex grid, POI counts within travel thresholds)
  4. **Impact Aggregation**: Infrastructure damage (depth-damage), Travel delays (VoT), Trip cancellations (80% VoT), QoL (per-capita POI accessibility)
  5. **RL Environment**: Gymnasium interface, 8 discrete adaptation actions per TAZ
  6. **RL Agent**: PPO via Stable-Baselines3, 2023–2100 horizon

- Critical path:
  Rainfall sample → SCALGO depth map → OSM routing with depth-disruption → TAZ-level impact aggregation → Reward computation → PPO policy update → Action deployment modifies future flood response

- Design tradeoffs:
  1. **β-weight selection**: EC vs. QoL prioritization determines ~10x spending difference (DKK 2.1bn vs. 20bn)
  2. **Single event/year**: Reduces compute but may miss compound/seasonal flooding patterns
  3. **29 TAZ aggregation**: Balances spatial resolution against RL state-space dimensionality
  4. **QoL index construction**: Weights from logistic regression on life satisfaction—sensitive to survey instrument

- Failure signatures:
  1. **Reward hacking**: Agent exploits QoL metric artifacts (e.g., investing in high-POI areas regardless of flood exposure)
  2. **Spatial concentration**: EC policy concentrates ~100% of spending in few TAZs (observed in Figure 2)
  3. **Action collapse**: Agent repeatedly selects single action type if others have unfavorable cost/benefit ratios under current β weights
  4. **Distribution shift**: Policy trained on RCP4.5 fails under RCP8.5 rainfall intensities

- First 3 experiments:
  1. **Baseline replication**: Run EC (βQ=0) vs. QoL (βQ=0.5) configurations; verify DKK 2.1bn vs. 20bn spending and spatial distribution patterns match Figure 2.
  2. **βQ sensitivity sweep**: Run βQ ∈ {0.0, 0.1, 0.25, 0.5, 0.75, 1.0} with economic weights fixed; plot total adaptation spending and Gini coefficient of spatial distribution.
  3. **Climate scenario stress test**: Train on RCP4.5, evaluate policy performance under RCP8.5 rainfall samples; measure reward degradation and identify failure modes.

## Open Questions the Paper Calls Out

- **Open Question 1**: How do different βQ weight values (beyond 0.5) affect optimal adaptation pathways, total spending, and spatial distribution of investments?
  - Basis: Authors state in footnote: "Future research will explore the implications of setting βQ to different values."
  - Why unresolved: Only βQ = 0.5 was tested; the sensitivity of outcomes to this parameter remains unknown.
  - What evidence would resolve it: Running experiments with varying βQ values (e.g., 0.1–0.9) and comparing resulting spending patterns and spatial distributions.

- **Open Question 2**: What are the distributional and socio-demographic effects of QoL-focused versus EC-focused adaptation policies across different population groups?
  - Basis: Authors state they expect to "explore distributional (socio-demographic) effects of policies" and use transportation justice indices in future work.
  - Why unresolved: Current analysis aggregates outcomes at TAZ level without examining how benefits/burdens fall across demographic groups.
  - What evidence would resolve it: Integrating socio-demographic data and equity metrics (e.g., Foster-Greer-Thorbecke indices) into the reward function and analyzing differential impacts.

- **Open Question 3**: How robust are the adaptation pathways to the simplifying assumption of one rainfall event per year and using accumulated daily rainfall as intensity?
  - Basis: Authors note these were "for simplicity and as proof-of-concept"; real flood risk involves multiple events and varied temporal patterns.
  - Why unresolved: More frequent or intense rainfall events could change optimal policy timing and location.
  - What evidence would resolve it: Sensitivity analysis with multiple annual rainfall events and sub-daily precipitation data.

- **Open Question 4**: Would Copenhagen-specific QoL weights (rather than European-city-derived weights) produce different adaptation pathways?
  - Basis: QoL weights came from a general European survey [22], not Copenhagen-specific data; local preferences may differ.
  - Why unresolved: Weighted sum of POI categories may not reflect Copenhagen residents' actual wellbeing priorities.
  - What evidence would resolve it: Surveying Copenhagen residents on amenity satisfaction and re-deriving weights; comparing resulting policies.

## Limitations
- Linear scalarization of normative trade-offs through reward weights may not capture truly incommensurable objectives.
- Single-event-per-year simplification may miss compound or seasonal flooding patterns critical for policy decisions.
- Model coupling between rainfall, flood, transport, and impact modules could compound errors, though IAM architecture mitigates this.

## Confidence
- High: Quantitative results on spending differences (DKK 2.1bn vs. 20bn) and spatial distribution under EC vs. QoL objectives.
- Medium: Generalization of findings to other urban contexts; transferability of QoL weighting methodology.
- Low: Absence of comparative RL algorithm analysis and sensitivity to PPO hyperparameters.

## Next Checks
1. **Climate scenario stress test**: Train on RCP4.5, evaluate policy performance under RCP8.5 rainfall samples; measure reward degradation and identify failure modes.
2. **Sensitivity analysis**: Run βQ ∈ {0.0, 0.1, 0.25, 0.5, 0.75, 1.0} with economic weights fixed; plot total adaptation spending and Gini coefficient of spatial distribution.
3. **Objective alignment audit**: Validate that the QoL index weights (from logistic regression on life satisfaction) correctly capture wellbeing impacts by testing against independent satisfaction surveys.