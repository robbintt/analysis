---
ver: rpa2
title: 'MC-GRPO: Median-Centered Group Relative Policy Optimization for Small-Rollout
  Reinforcement Learning'
arxiv_id: '2601.22582'
source_url: https://arxiv.org/abs/2601.22582
tags:
- rollout
- mc-grpo
- grpo
- arxiv
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies baseline-induced advantage sign flips as a
  key failure mode in small-rollout GRPO-style RL, where noisy group means cause incorrect
  update directions and degrade accuracy. It proposes Median-Centered GRPO (MC-GRPO),
  which replaces the shared mean baseline with a median baseline computed from G+1
  rollouts and excludes the zero-advantage median sample from backprop, preserving
  the effective update size at G.
---

# MC-GRPO: Median-Centered Group Relative Policy Optimization for Small-Rollout Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2601.22582
- **Source URL:** https://arxiv.org/abs/2601.22582
- **Reference count:** 30
- **Primary result:** Median-centered baselines stabilize small-rollout RL, improving accuracy by up to 7.89% at G=2.

## Executive Summary
This paper addresses a fundamental failure mode in small-rollout Group Relative Policy Optimization (GRPO): baseline-induced sign flips in advantage estimates. When group means are estimated from very few rollouts, noise can invert the direction of policy updates, causing performance degradation. MC-GRPO solves this by replacing the shared group mean baseline with a median baseline computed from G+1 rollouts, excluding the median sample from backprop to preserve update size. The approach improves accuracy across GRPO variants and rollout budgets, with gains persisting under composite rewards and OOD contest math problems.

## Method Summary
MC-GRPO modifies GRPO's baseline computation by using the median of G+1 group rollouts instead of the mean of G. The median sample is excluded from backpropagation to maintain the effective rollout count at G. This change stabilizes within-prompt advantages by reducing sensitivity to outlier rewards and noisy group statistics. The method is applied across GRPO, DAPO, and DR-GRPO variants, with experiments using G∈{2,4,8} and scalar or composite rewards. The key innovation is statistical: median-based baselines are less sensitive to extreme values than means, preventing spurious advantage sign inversions that can corrupt gradient updates.

## Key Results
- MC training improves accuracy by up to 7.89% at G=2 across GRPO variants and five model–dataset settings.
- The G=2 to G=8 accuracy gap is reduced to within 1% with MC baselines.
- Benefits persist with composite rewards (up to 5.27% improvement) and generalize to OOD contest math problems.

## Why This Works (Mechanism)
Small rollout counts produce noisy group means that can flip advantage signs, causing incorrect policy updates. The median is more robust to outliers than the mean, preventing spurious sign inversions. Excluding the median sample maintains the effective rollout budget while providing a more stable baseline. This stabilization is most critical at G=2, where statistical noise is highest relative to the group size.

## Foundational Learning
- **Group Relative Policy Optimization (GRPO):** A sample-efficient RL algorithm that computes advantages within groups of rollouts rather than against a global baseline. Needed to understand the baseline's role in update direction and the impact of noisy group statistics.
- **Advantage Estimation:** The difference between action value and baseline, determining update direction. Quick check: verify that advantage signs correlate with policy improvement direction.
- **Median vs Mean Statistics:** Median is robust to outliers; mean is sensitive to extreme values. Quick check: compare baseline variance between median and mean under simulated noise.
- **Rollout Efficiency:** Small rollout budgets (G∈{2,4}) trade statistical reliability for computational efficiency. Quick check: measure accuracy degradation as G decreases.
- **Composite Rewards:** Weighted combinations of multiple reward components. Quick check: isolate individual reward contributions to identify which drive MC benefits.
- **Policy Gradient Backpropagation:** The process of computing gradients through sampled trajectories. Quick check: ensure median exclusion doesn't bias gradient estimates.

## Architecture Onboarding
- **Component map:** Environment -> Prompt Generator -> Policy Network -> Rollout Sampler (G+1 samples) -> Reward Evaluator -> Median Baseline Computation -> Advantage Calculation -> Policy Update
- **Critical path:** Rollout generation → median baseline → advantage computation → gradient update
- **Design tradeoffs:** MC-GRPO adds one rollout overhead but gains stability; excludes median to preserve update size but may introduce bias; simple to implement but limited to deterministic rewards.
- **Failure signatures:** Sign flips in advantages → incorrect update directions → performance degradation; high variance in small groups → unreliable baselines.
- **First experiments:** 1) Compare median vs mean baseline stability under synthetic noise; 2) Measure accuracy at G=2 with and without median exclusion; 3) Test MC-GRPO with composite rewards to isolate component effects.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does MC-GRPO maintain its stability benefits when applied to learned reward models that exhibit non-stationarity and stochastic noise, rather than deterministic verifier-based rewards?
- Basis: [explicit] The Conclusion states the empirical study is limited to "verifier-based" rewards and it remains unclear if gains transfer to "learned reward models or human-preference signals, which can be stochastic, non-stationary, and sensitive to prompt phrasing."
- Why unresolved: The paper only evaluates deterministic, rule-based math rewards; it does not test scenarios where the reward signal itself is noisy or changes over time, which is common in RLHF.
- Evidence: An evaluation of MC-GRPO on a standard RLHF task (e.g., summarization or chat) using a learned Bradley-Terry reward model, measuring training variance and final preference metrics.

### Open Question 2
- Question: How does median-centering affect the optimization dynamics in multi-objective settings where the baseline must coordinate across competing objectives?
- Basis: [explicit] The Conclusion notes that in "multi-objective rewards... a robust within-prompt baseline may need to be task-adaptive or incorporate coordination across objectives."
- Why unresolved: The current experiments use a scalar reward (or simple composite). It is untested whether a single group median can effectively balance trade-offs (e.g., accuracy vs. safety) without favoring one objective.
- Evidence: An experiment applying MC-GRPO to a constrained generation task (e.g., code generation with correctness and security rewards) comparing scalar median baselines against per-objective baselines.

### Open Question 3
- Question: Is MC-GRPO compatible with dynamic rollout pruning methods (e.g., CPPO), or does the requirement to generate G+1 rollouts negate the efficiency gains from pruning?
- Basis: [inferred] The Related Work section claims MC-GRPO is "orthogonal to pruning" and "naturally compatible," but the paper does not empirically test this combination.
- Why unresolved: MC-GRPO adds a fixed overhead of one extra rollout to define the median. Pruning methods aim to reduce rollout generation. It is unclear if these two efficiency strategies interfere or compound effectively.
- Evidence: A latency and accuracy benchmark combining MC-GRPO with a pruning method like CPPO on a small-rollout budget.

## Limitations
- Benefits not tested on learned reward models with non-stationarity or stochastic noise.
- Composite reward ablation missing to identify key driving components.
- OOD generalization sample size and diversity unspecified.
- No comparison to state-of-the-art small-sample RL methods like adaptive rollout sampling.

## Confidence
- **High confidence:** Median baseline reduces sign-flip failures in small groups; accuracy gains at G=2 are consistent and measurable.
- **Medium confidence:** Benefits generalize to composite rewards and OOD problems; stability gains persist across GRPO variants.
- **Low confidence:** Universal superiority across all composite-reward formulations; compatibility with learned reward models and pruning methods.

## Next Checks
1. Evaluate MC-GRPO at G≥8 and under high-variance reward conditions to test robustness beyond the current regime.
2. Perform an ablation study isolating each component of composite rewards to identify which terms are most influential.
3. Compare MC-GRPO against state-of-the-art small-sample RL methods such as adaptive rollout and response reuse in identical experimental setups.