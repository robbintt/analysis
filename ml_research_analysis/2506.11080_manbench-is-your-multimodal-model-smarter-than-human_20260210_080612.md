---
ver: rpa2
title: 'MANBench: Is Your Multimodal Model Smarter than Human?'
arxiv_id: '2506.11080'
source_url: https://arxiv.org/abs/2506.11080
tags:
- image
- dataset
- manbench
- human
- mllms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MANBench is a bilingual benchmark (English and Chinese) for evaluating
  multimodal reasoning capabilities of humans and Multimodal Large Language Models
  (MLLMs). It contains 1,314 questions across nine tasks with 2,231 images, designed
  to emphasize intuitive reasoning, cross-modal integration, and real-world complexity.
---

# MANBench: Is Your Multimodal Model Smarter than Human?

## Quick Facts
- **arXiv ID:** 2506.11080
- **Source URL:** https://arxiv.org/abs/2506.11080
- **Reference count:** 31
- **Primary result:** State-of-the-art MLLMs achieve less than 60% accuracy, falling short of average human performance on multimodal reasoning tasks.

## Executive Summary
MANBench is a bilingual benchmark (English and Chinese) for evaluating multimodal reasoning capabilities of humans and Multimodal Large Language Models (MLLMs). It contains 1,314 questions across nine tasks with 2,231 images, designed to emphasize intuitive reasoning, cross-modal integration, and real-world complexity. The benchmark distinguishes between knowledge-based and non-knowledge-based questions, requiring reasoning rather than simple retrieval and mandating integration of textual and visual information. Through extensive human experiments with 575 participants and evaluation of 12 state-of-the-art MLLMs, the study found that while MLLMs excel in knowledge-based and text-image understanding tasks, they struggle with deeper cross-modal reasoning tasks like Transmorphic Understanding, Image Consistency, and Multi-image Understanding. Both humans and MLLMs face challenges in highly complex tasks like Puzzles and Spatial Imagination. The results reveal that even advanced MLLMs achieve less than 60% accuracy, falling short of average human performance across many domains.

## Method Summary
The benchmark uses VLMEvalKit to evaluate 12 state-of-the-art MLLMs (6 closed-source, 8 open-source) on 1,314 bilingual questions with 2,231 images. Each question requires integration of text and image, categorized as either knowledge-based or non-knowledge-based. Human evaluation involved 575 participants across diverse demographics, establishing average (62.26%) and best (90.87%) performance baselines. The evaluation configuration used temperature=0 and retry count=10, with an ablation study removing images to test visual dependency.

## Key Results
- MLLMs achieve less than 60% accuracy overall, below average human performance (62.26%)
- GPT-o1 achieved the highest model score (59.97%), still below average human performance
- MLLMs excel at knowledge-based and text-image understanding tasks but struggle with cross-modal reasoning tasks
- Both humans and MLLMs perform near-random on highly complex tasks like Puzzles and Spatial Imagination
- Ablation study shows GPT-4o performance drops to near-random when images are removed, confirming visual necessity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task categorization into knowledge-based and non-knowledge-based domains isolates reasoning capability from retrieval ability in MLLMs.
- Mechanism: The benchmark enforces a causal link between task design and performance attribution. Questions requiring prior knowledge evaluate information retrieval and cross-modal knowledge transfer, while non-knowledge-based questions evaluate fundamental cognitive processing. This separation allows failures on tasks like Puzzles or Spatial Imagination to be attributed to reasoning deficits rather than lack of encyclopedic knowledge.
- Core assumption: The design of "non-knowledge-based" questions successfully eliminates the need for any implicit, culturally-specific, or common-sense knowledge that might not be universally held.
- Evidence anchors:
  - [abstract] "The benchmark distinguishes between knowledge-based and non-knowledge-based questions, requiring reasoning rather than simple retrieval..."
  - [section 3.1] "Knowledge-based questions require participants to possess prior knowledge... In contrast, non-knowledge-based questions do not demand advanced domain-specific knowledge."
  - [corpus] Related work "MME-Reasoning" also employs explicit categorization to evaluate logical reasoning, supporting the method's utility.
- Break condition: This mechanism weakens if non-knowledge tasks inadvertently rely on unstated common-sense priors, or if knowledge-based tasks can be solved via visual cues alone, blurring the intended causal separation.

### Mechanism 2
- Claim: Mandating visual-textual integration prevents models from exploiting single-modality shortcuts, forcing genuine cross-modal reasoning.
- Mechanism: Each question is constructed to require joint interpretation of an image and its accompanying text. An ablation study showed GPT-4o's performance dropping to near-random levels on most tasks when images were removed, confirming that the model could not rely on text priors. This design choice creates a causal dependency: success requires processing both modalities, making accuracy a more direct proxy for multimodal reasoning ability.
- Core assumption: The ablation study on GPT-4o is representative of other MLLMs, and the prompt used to force an answer without images did not itself introduce a systematic bias.
- Evidence anchors:
  - [abstract] "...mandating integration of textual and visual information."
  - [section 4.4 Analysis] "Is the visual contents necessary?... As shown in Figure 7, GPT-4o demonstrated performance divergence across 9 task categories... For other task types, the model's performance showed no statistically significant deviation from random chance."
  - [corpus] Evidence from related benchmarks is not provided in the corpus for this specific integration mechanism.
- Break condition: The mechanism would be invalidated if a model could achieve high accuracy by using the image as a minor corrective signal on top of a primarily text-based reasoning process, or if the text prompts contained subtle cues that leaked the answer.

### Mechanism 3
- Claim: A large-scale, diverse human baseline provides a statistically grounded reference point, revealing that MLLMs still lag behind average human multimodal intelligence.
- Mechanism: By testing 575 humans across varied demographics, the study establishes a performance distribution (average: 62.26%, best: 90.87%). Comparing MLLMs to this baseline shows that even top models (e.g., GPT-o1: 59.97%) fall short of the *average* human score. The causal insight is that while MLLMs approach or exceed human performance on knowledge retrieval, they have not yet mastered the general-purpose, cross-modal reasoning that characterizes typical human cognition.
- Core assumption: The paper assumes the participant pool is sufficiently representative and that the online testing conditions provided a fair assessment of human capability.
- Evidence anchors:
  - [abstract] "...even advanced MLLMs achieve less than 60% accuracy, falling short of average human performance across many domains."
  - [Table 1] "Human (Average) 62.26... GPT-o1 59.97," showing a clear performance gap on the overall score.
  - [corpus] Related papers like "Pixels, Patterns, but No Poetry" also discuss the gap between human and model perception, providing converging evidence for the general conclusion.
- Break condition: This finding would be less meaningful if the human participant pool was biased (e.g., predominantly from a single highly-educated demographic) or if the testing interface disadvantaged human performance relative to the model's API access.

## Foundational Learning

- Concept: **Cross-Modal Integration**
  - Why needed here: This is the foundational skill the benchmark is designed to test. Understanding it is crucial for distinguishing between a model that is simply a good knowledge retriever and one that can genuinely reason from combined sensory inputs.
  - Quick check question: If a model correctly answers a question by relying on patterns in the text prompt alone, has it demonstrated the cross-modal integration the benchmark is designed to measure?

- Concept: **Human Baseline Distribution**
  - Why needed here: The paper's core conclusion is framed against human performance. Interpreting this requires understanding the difference between comparing to an "average" human versus a "best" human, and the statistical value of a large, diverse sample.
  - Quick check question: Why is comparing a model's score to a single expert's score a less reliable measure of general AI progress than comparing it to the average score of 500+ diverse humans?

- Concept: **Task-Based Decomposition of Reasoning**
  - Why needed here: The benchmark breaks down "multimodal reasoning" into nine specific tasks (e.g., Spatial Imagination, Puzzles). This decomposition is the primary analytical tool for diagnosing *where* a model's reasoning fails (e.g., abstract logic vs. concrete knowledge).
  - Quick check question: If a model excels at Knowledge questions but fails at Spatial Imagination, what does this diagnostic pattern suggest about its underlying architecture or training data?

## Architecture Onboarding

- Component map:
  - Benchmark Dataset: 1,314 bilingual (EN/CN) questions, 2,231 images, organized into 9 task categories. Each sample includes a question, image(s), multiple-choice options, and metadata (task type, knowledge requirement).
  - Evaluation Harness: Built on VLMEvalKit. Runs inference on MLLMs (temperature=0, 10 retries) and computes accuracy per task and overall. Includes an ablation mode to test without images.
  - Human Experiment Platform: A web-based interface for collecting human responses, demographic data, and response times. Data is aggregated to produce average and best human performance baselines.
  - Analysis Module: Calculates per-task and overall accuracy, compares model vs. human performance, and visualizes results (e.g., performance vs. response time).

- Critical path:
  1.  **Dataset Curation:** Carefully construct questions to ensure they are knowledge/non-knowledge categorized, require text-image integration, and cover the nine tasks.
  2.  **Human Evaluation:** Recruit a large, diverse pool (N=575) to establish a robust performance baseline and validate question quality.
  3.  **Model Inference:** Run all target MLLMs on the full dataset under standardized conditions (temp=0) to generate raw predictions.
  4.  **Comparative Analysis:** Compare model accuracy against the human baseline across the task taxonomy to identify specific strengths and weaknesses.

- Design tradeoffs:
  - **Comprehensiveness vs. Participant Burden:** The full 1,314-question set is too long for a single human participant. The tradeoff is splitting the dataset into subsets, which could introduce variance, versus a smaller, less comprehensive benchmark.
  - **Reasoning vs. Knowledge:** The benchmark is designed to minimize reliance on prior knowledge. This makes it a purer test of reasoning but potentially less representative of real-world tasks that often require domain expertise.
  - **MLLM API vs. Weights:** Evaluating closed-source models via API is necessary for state-of-the-art comparison but reduces reproducibility and control compared to open-weight models run on local infrastructure.

- Failure signatures:
  - **Near-Random Performance on All Tasks:** Indicates a catastrophic failure in multimodal grounding or instruction following.
  - **High Knowledge / Low Reasoning Score:** A classic signature of current MLLMsâ€”strong on retrieval but weak on deeper cognitive tasks like Puzzles or Spatial Imagination.
  - **Performance Collapse in Ablation:** A desirable failure mode on the benchmark, confirming that the model relies on the image. A lack of performance drop when images are removed indicates a flawed task design.

- First 3 experiments:
  1.  **Establish Human Baseline:** Run the benchmark with a pilot group (N=5-10) to validate question quality, then deploy to the full participant pool to generate the average and best performance baselines.
  2.  **Zero-Shot MLLM Evaluation:** Run all selected MLLMs on the full dataset without any task-specific fine-tuning to get a pure measure of their inherent multimodal reasoning capabilities.
  3.  **Visual Ablation Study:** Re-run the top-performing MLLM (e.g., GPT-4o) on the dataset with all images removed and a prompt forcing a choice. This validates that the benchmark's questions genuinely require visual information.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the specific linguistic and cultural context of the participant pool (predominantly Chinese) affect the generalizability of the multimodal capability benchmarks established in MANBench?
- **Basis in paper:** [explicit] Section 6 (Limitations) states that the subjects were drawn from a specific group within the Chinese linguistic and cultural context, noting that "Cultural differences, variations in knowledge bases, and distinct cognitive patterns could significantly impact the generalizability of the results."
- **Why unresolved:** The study acknowledges that its human baseline is localized, but it does not provide comparative data from other cultural or linguistic groups to measure the variance in human performance.
- **What evidence would resolve it:** A cross-cultural replication of the human experiments with diverse participant pools from different linguistic and cultural backgrounds to establish a more universal "human norm."

### Open Question 2
- **Question:** What specific mechanisms or training interventions are necessary to improve MLLM performance in highly complex tasks like "Puzzles" and "Spatial Imagination," where models currently achieve near-random accuracy?
- **Basis in paper:** [explicit] Section 4.3.1 notes that "On the Puzzles task, the accuracy of MLLMs is approximately 30%, which is comparable to random guessing," and highlights that this suggests "substantial room for improvement in this type of image reasoning task."
- **Why unresolved:** The paper identifies the failure mode (low accuracy) but does not propose or test architectural changes to address the lack of inductive or spatial reasoning capabilities in current models.
- **What evidence would resolve it:** Future studies demonstrating improved accuracy on the specific "Puzzles" and "Spatial Imagination" subsets of MANBench through new training paradigms (e.g., chain-of-thought spatial reasoning).

### Open Question 3
- **Question:** How does the over-representation of highly educated participants (e.g., 40.5% with bachelor's degrees) influence the "human average" baseline, and does this demographic skew risk amplifying educational bias in multimodal benchmarking?
- **Basis in paper:** [inferred] Section 6 mentions the educational backgrounds "do not fully represent the broader human population," and Section 7 (Ethical Considerations) warns that "The over-representation of highly educated participants... may inadvertently reinforce existing societal biases" if misused.
- **Why unresolved:** The paper reports an "average human" score but does not analyze the performance variance across education levels, leaving the impact of this bias on the final benchmark scores unknown.
- **What evidence would resolve it:** A statistical analysis of participant performance stratified by education level to determine if the current "average" is skewed by the high proportion of university-educated participants.

### Open Question 4
- **Question:** To what extent do MLLMs rely on linguistic priors versus visual processing in tasks like "Transmorphic Understanding," given that GPT-4o showed a bias toward positive emotions even without images?
- **Basis in paper:** [inferred] Section 4.4 (Analysis) describes an ablation study where GPT-4o's accuracy in Transmorphic Understanding exceeded random chance without images, attributing this to "the model's preference for positive emotions, such as 'joy', which appear more frequently as correct answers."
- **Why unresolved:** The benchmark aims to test cross-modal integration, but the results suggest models may achieve non-random scores through text-based statistical correlations rather than genuine visual understanding.
- **What evidence would resolve it:** An analysis of the label distribution within the dataset (to check for frequency biases) and experiments that control for or balance the emotional labels to isolate visual reasoning from linguistic priors.

## Limitations
- Human baseline representativeness: The participant pool may not be sufficiently diverse or representative of the broader population
- Ablation study scope: Visual ablation study conducted only on GPT-4o, limiting generalizability across MLLM architectures
- Reproducibility constraints: Exclusive use of closed-source models reduces architectural transparency and ability to diagnose failure modes

## Confidence

- **High confidence**: Core empirical finding that MLLMs perform below average human accuracy on multimodal reasoning tasks, supported by large-scale human and model evaluation
- **Medium confidence**: Mechanism separating knowledge-based from non-knowledge-based tasks, as the claim relies on the assumption that "non-knowledge" questions truly do not require implicit or culturally specific priors
- **Medium confidence**: Ablation study's inference that images are necessary for performance, since it is based on a single model and may not generalize across architectures

## Next Checks

1. Replicate the visual ablation study across multiple MLLM architectures (e.g., GPT-4o, Gemini-Pro, InternVL2.5) to confirm that performance collapse is a consistent pattern and not model-specific
2. Conduct a demographic analysis of the human participant pool to quantify potential biases and assess the generalizability of the human baseline
3. Perform a robustness analysis by having a subset of participants re-answer a random sample of questions after a time delay, to measure intra-human consistency and detect potential ambiguities in question design