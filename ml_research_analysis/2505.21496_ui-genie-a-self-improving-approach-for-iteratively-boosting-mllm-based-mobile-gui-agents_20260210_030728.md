---
ver: rpa2
title: 'UI-Genie: A Self-Improving Approach for Iteratively Boosting MLLM-based Mobile
  GUI Agents'
arxiv_id: '2505.21496'
source_url: https://arxiv.org/abs/2505.21496
tags:
- reward
- agent
- task
- action
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: UI-Genie is a self-improving framework that addresses the challenges
  of verifying GUI agent trajectories and scaling high-quality training data. It introduces
  UI-Genie-RM, a specialized reward model with image-text interleaved architecture
  for accurate action-level and task-level reward evaluation.
---

# UI-Genie: A Self-Improving Approach for Iteratively Boosting MLLM-based Mobile GUI Agents

## Quick Facts
- arXiv ID: 2505.21496
- Source URL: https://arxiv.org/abs/2505.21496
- Reference count: 40
- Primary result: Introduces a self-improving framework with UI-Genie-RM (reward model) and synthetic data generation, achieving state-of-the-art GUI agent performance

## Executive Summary
UI-Genie addresses the challenge of scaling high-quality training data for mobile GUI agents by introducing a self-improving framework that iteratively generates and verifies synthetic trajectories. The core innovation is UI-Genie-RM, a specialized reward model with image-text interleaved architecture that provides both action-level and task-level reward evaluation. Through reward-guided exploration and process supervision, the framework produces UI-Genie-RM-517k (the first large-scale reward dataset) and UI-Genie-Agent-16k (synthetic trajectory dataset) without manual annotation. Experiments demonstrate significant performance improvements, achieving 77.0% success rate on AndroidLab and 20.4% on Android Arena.

## Method Summary
UI-Genie employs a self-improving framework that generates synthetic trajectories through reward-guided exploration. The system uses UI-Genie-RM, a reward model with image-text interleaved architecture, to evaluate trajectories at both action and task levels. The framework iteratively improves both agent and reward models via process supervision and outcome verification. Synthetic trajectories are generated, evaluated by the reward model, and filtered based on reward scores before being used to train improved versions of both models. This creates a closed-loop system that progressively enhances performance without requiring manual data annotation.

## Key Results
- Achieves 77.0% success rate on AndroidLab benchmark
- Achieves 20.4% success rate on Android Arena benchmark
- Produces UI-Genie-RM-517k (largest reward dataset) and UI-Genie-Agent-16k (synthetic trajectory dataset) without manual annotation

## Why This Works (Mechanism)
The framework's effectiveness stems from its ability to generate high-quality synthetic training data through a self-reinforcing loop. The image-text interleaved reward model architecture enables accurate evaluation of both visual and textual aspects of GUI interactions. By combining process supervision (step-by-step evaluation) with outcome verification (task completion assessment), the system ensures that generated trajectories are both valid and goal-oriented. The reward-guided exploration strategy focuses sampling on promising regions of the action space, making data generation more efficient than random exploration.

## Foundational Learning

1. **Reward-guided exploration** - why needed: Enables efficient sampling of promising trajectories; quick check: compare success rates of guided vs random exploration
2. **Process supervision** - why needed: Provides step-by-step feedback for trajectory refinement; quick check: measure improvement when adding intermediate reward signals
3. **Image-text interleaved architecture** - why needed: Captures both visual and textual GUI elements; quick check: ablate image vs text components to measure impact
4. **Self-improving loop** - why needed: Creates virtuous cycle of model improvement; quick check: track performance gains across iterations
5. **Outcome verification** - why needed: Ensures task completion is validated; quick check: measure false positive rate in trajectory acceptance
6. **Synthetic trajectory generation** - why needed: Enables scalable data creation without manual annotation; quick check: compare agent performance trained on synthetic vs real data

## Architecture Onboarding

**Component map**: Mobile GUI App -> Screenshot Pipeline -> UI-Genie-RM -> Reward Scores -> Trajectory Filter -> Agent Training -> Improved Agent

**Critical path**: GUI interaction → screenshot capture → reward evaluation → trajectory filtering → model training → deployment

**Design tradeoffs**: 
- Prioritizes automation over manual data quality
- Sacrifices exploration diversity for reward-guided efficiency
- Balances computational cost against iterative improvement gains

**Failure signatures**: 
- Suboptimal reward signals leading to poor trajectory generation
- Premature convergence to local optima in action space
- Reward model bias toward common UI patterns

**3 first experiments**:
1. Test reward model accuracy on held-out validation trajectories
2. Measure exploration efficiency vs random baseline
3. Evaluate iterative improvement gains across 3 rounds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would increasing the context window beyond five historical screenshots yield further performance gains on tasks requiring more than 10 steps, or does performance plateau?
- Basis in paper: [inferred] The ablation study (Table 7) shows progressive improvement from 1→3→5 images, but does not test beyond 5 despite "hard tasks" defined as >10 steps.
- Why unresolved: The paper demonstrates scaling benefits up to 5 images but does not establish the optimal depth or whether longer contexts introduce noise/diminished returns.
- What evidence would resolve it: Systematic evaluation of context windows of 7, 10, and 15 screenshots on the hard task subset, measuring both accuracy and computational cost.

### Open Question 2
- Question: Does the self-improvement framework exhibit diminishing returns after three iterations, or could continued iterations further close the gap on the remaining ~23% of failed high-level AndroidControl tasks?
- Basis in paper: [explicit] The authors state "our framework cannot guarantee the production of fully correct trajectories across all GUI tasks" and only report three generations.
- Why unresolved: Figure 4 shows continued improvement through round 3, but the trajectory appears sublinear and it is unclear whether an asymptote is approaching.
- What evidence would resolve it: Extended experiments with 5–10 self-improvement rounds, tracking marginal gains per iteration and identifying convergence criteria.

### Open Question 3
- Question: How robust is the framework to reward model errors during early exploration phases, and does error propagation create a performance ceiling?
- Basis in paper: [explicit] The authors acknowledge "our reward model may occasionally generate suboptimal rewards signals, resulting in failed trajectories during training data expansion."
- Why unresolved: No analysis quantifies the rate or impact of erroneous reward signals on downstream agent training quality.
- What evidence would resolve it: Controlled injection of varying noise rates into reward model outputs during exploration, measuring final agent performance degradation.

## Limitations

- Success rates remain relatively low on challenging tasks (20.4% on Android Arena)
- Performance may not generalize to apps with dynamic or rapidly changing interfaces
- Self-improving loop may exhibit diminishing returns after several iterations

## Confidence

- **High**: Technical contributions (UI-Genie-RM architecture, reward-guided exploration, iterative framework) and benchmark results
- **Medium**: Claims about generalization to broader, more complex GUI environments
- **Low**: Long-term scalability claims beyond reported dataset sizes

## Next Checks

1. Evaluate UI-Genie on diverse real-world Android applications with varying UI complexity and dynamic content
2. Conduct ablation studies to quantify impact of each self-improvement component on performance and robustness
3. Test framework performance and stability over multiple iterations as dataset sizes scale up