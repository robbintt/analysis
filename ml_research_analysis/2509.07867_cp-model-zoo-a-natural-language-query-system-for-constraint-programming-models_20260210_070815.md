---
ver: rpa2
title: 'CP-Model-Zoo: A Natural Language Query System for Constraint Programming Models'
arxiv_id: '2509.07867'
source_url: https://arxiv.org/abs/2509.07867
tags:
- problem
- language
- arxiv
- constraint
- description
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CP-Model-Zoo is a system that retrieves expert-validated constraint
  programming models from a database using natural language queries. It uses embeddings
  to match problem descriptions to existing models, avoiding the need for manual data
  labeling.
---

# CP-Model-Zoo: A Natural Language Query System for Constraint Programming Models

## Quick Facts
- arXiv ID: 2509.07867
- Source URL: https://arxiv.org/abs/2509.07867
- Reference count: 40
- Expert-validated models retrieved using natural language queries with MRR scores above 0.95

## Executive Summary
CP-Model-Zoo is a natural language query system that retrieves expert-validated constraint programming models from a database without requiring manual data labeling. The system uses embeddings to match problem descriptions to existing models, achieving high accuracy with Mean Reciprocal Rank (MRR) scores above 0.95 for expert and novice queries. It's designed to simplify model discovery by allowing users to describe problems in natural language and automatically finding relevant constraint programming models from the database.

## Method Summary
The system employs a novel approach using embeddings to match natural language problem descriptions with existing constraint programming models in a database. Rather than requiring manual labeling of training data, it leverages embedding techniques to compute similarity scores between user queries and model descriptions. This enables automatic retrieval of relevant models based on semantic similarity, making the system scalable and reducing the need for expert curation. The architecture supports both expert and novice users, with evaluation showing high performance across different query types.

## Key Results
- Achieved MRR scores above 0.95 for expert and novice natural language queries
- Demonstrated MRR of 0.90 for CSPLib formal problem descriptions
- Open-source web application available for community use

## Why This Works (Mechanism)
The system works by using embedding techniques to capture semantic meaning in both problem descriptions and model representations. By avoiding manual data labeling, it creates a scalable approach where new models can be added simply by generating a description and computing its embedding. The embedding-based matching allows the system to understand semantic relationships between different problem formulations, enabling accurate retrieval even when users describe problems using different terminology than the original model descriptions.

## Foundational Learning
- **Embedding techniques**: Why needed - to capture semantic meaning and enable similarity matching; Quick check - verify that embeddings preserve semantic relationships between similar problems
- **Mean Reciprocal Rank (MRR)**: Why needed - to measure retrieval effectiveness; Quick check - confirm MRR calculation correctly handles ranking of relevant models
- **Constraint Programming models**: Why needed - core domain of the system; Quick check - validate that model descriptions accurately represent the underlying CP problems
- **Natural language processing**: Why needed - to handle user queries in everyday language; Quick check - test system's ability to handle synonyms and varied phrasing
- **Database retrieval systems**: Why needed - to efficiently search and return relevant models; Quick check - measure query response times and scalability

## Architecture Onboarding

Component map: User Query -> Embedding Generator -> Similarity Matcher -> Model Database -> Retrieved Models

Critical path: User submits natural language query → Query embedding computed → Similarity scores calculated with all model embeddings → Top-ranked models returned to user

Design tradeoffs: The system prioritizes semantic matching over exact keyword matching, which improves retrieval of conceptually similar problems but may miss cases where exact terminology matching is desired. The embedding approach avoids manual labeling but requires careful selection of embedding models to ensure good semantic capture.

Failure signatures: Poor performance on highly specialized terminology, difficulty with ambiguous problem descriptions, potential degradation when dealing with significantly larger model repositories, and challenges with cross-domain problem descriptions that share surface-level similarities but have different underlying structures.

First experiments:
1. Test basic query functionality with simple, well-defined problems from the current model corpus
2. Evaluate system performance with varied phrasing of the same problem description
3. Measure retrieval accuracy when adding new models to the database

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation based on limited query sets (20 per group) may not capture full complexity of real-world usage
- Small model corpus (56 models) may not represent system's scalability to larger repositories
- Exclusion of exact string matching models from testing could introduce selection bias
- Performance degradation on CSPLib descriptions (MRR 0.90) indicates challenges with formal problem descriptions

## Confidence

High confidence: The system architecture and methodology (using embeddings without manual labeling) are clearly described and technically sound.

Medium confidence: The retrieval performance metrics are well-documented but may not generalize to larger, more diverse model collections or query types.

Low confidence: The long-term scalability and robustness of the system when dealing with significantly larger model repositories or more complex problem domains.

## Next Checks

1. Evaluate the system with a substantially larger model corpus (500+ models) and more diverse query sets to assess scalability and generalization

2. Test the system's performance on real-world problem descriptions from multiple domains beyond the current examples

3. Conduct a user study with constraint programming practitioners to validate the practical utility and usability of the system in actual problem-solving scenarios