---
ver: rpa2
title: Experience Replay Addresses Loss of Plasticity in Continual Learning
arxiv_id: '2503.20018'
source_url: https://arxiv.org/abs/2503.20018
tags:
- learning
- continual
- replay
- loss
- plasticity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes that experience replay addresses loss of plasticity
  in continual learning. The authors show that when using experience replay with Transformers,
  the loss of plasticity disappears across multiple tasks: regression, classification,
  and policy evaluation.'
---

# Experience Replay Addresses Loss of Plasticity in Continual Learning

## Quick Facts
- arXiv ID: 2503.20018
- Source URL: https://arxiv.org/abs/2503.20018
- Authors: Jiuqi Wang; Rohan Chandra; Shangtong Zhang
- Reference count: 20
- Key outcome: Transformers with experience replay maintain plasticity across continual learning tasks while MLPs degrade; conjectured mechanism is in-context learning

## Executive Summary
This paper demonstrates that experience replay can address loss of plasticity in continual learning, but only when combined with Transformer architectures. The authors show that Transformers maintain or improve performance across multiple tasks—regression, classification, and policy evaluation—while MLPs suffer from the expected plasticity loss. The key insight is that Transformers may leverage in-context learning capabilities during the forward pass to adapt to new tasks without parameter updates, though this mechanism remains unproven. The experiments use a simple, stripped-down Transformer architecture with only attention layers and a fixed-size replay buffer.

## Method Summary
The method combines experience replay with Transformer architectures to combat loss of plasticity in continual learning. A fixed-size replay buffer (100 examples, FILO) stores recent task data, which is structured as an embedding Z matrix containing both features and targets. The Transformer processes this embedding through L attention-only layers without FFN, positional encoding, or layer normalization. Output is extracted from the last element of the final column. The approach is tested on three tasks: slowly-changing regression (1000 tasks, 10000 samples each), permuted MNIST classification (7000 tasks), and Boyan's Chain policy evaluation (5000 tasks). AdamW optimizer is used with different learning rates for Transformers (0.0001) versus MLPs (0.01).

## Key Results
- Transformers with experience replay maintain or improve performance across all three continual learning tasks
- MLPs with experience replay show classic plasticity loss, with rising loss on later tasks
- The stripped-down Transformer (10 layers, ~1% of MLP parameters) outperforms the MLP on permuted MNIST
- Buffer content remains static during testing yet performance improves, suggesting effective context utilization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformers with experience replay maintain plasticity through in-context learning capabilities
- Mechanism: The attention-based architecture processes the replay buffer as a contextual sequence, potentially implementing learning algorithms during the forward pass rather than requiring parameter updates for each new task. This allows adaptation without the parameter degradation that causes plasticity loss in MLPs.
- Core assumption: Transformers can implement learning algorithms in their forward pass (unproven in this paper)
- Evidence anchors:
  - [abstract] "We conjecture that experience replay and Transformers can address the loss of plasticity because of the in-context learning phenomenon."
  - [section 7] "If the Transformers are indeed learning in context, they do not have to update their parameters for a new task in principle and thus remain plastic."
  - [corpus] Limited support; "Spectral Collapse Drives Loss of Plasticity" (arxiv:2509.22335) provides complementary evidence that plasticity loss relates to Hessian spectral collapse, but doesn't directly validate the in-context learning hypothesis.
- Break condition: If in-context learning is not the actual mechanism, this explanation fails. The paper provides no decisive evidence.

### Mechanism 2
- Claim: Experience replay provides task-relevant context that enables forward-pass inference
- Mechanism: The replay buffer (size 100) stores recent examples, which are structured as embeddings Z = [features; targets] for supervised tasks or [features; next-features; rewards] for RL. This creates a consistent format where the model can reference patterns from past data when processing new inputs.
- Core assumption: The buffer contains sufficient task-relevant information to support generalization
- Evidence anchors:
  - [section 3] The embedding Z "straightforwardly merges the memory and the input feature in one elongated matrix and is directly compatible with the Transformer"
  - [section 4] "The buffer did not change throughout the testing phase for each task" yet performance improved, suggesting the model leverages buffer content effectively
  - [corpus] "Pareto Continual Learning" (arxiv:2503.23390) discusses stability-plasticity trade-offs with replay methods, supporting replay's role in balancing adaptation.
- Break condition: If buffer size is insufficient or data distribution shifts too rapidly, the context becomes unreliable

### Mechanism 3
- Claim: MLPs and RNNs fail because they lack the architectural capacity for effective in-context learning from replay data
- Mechanism: MLPs flatten the embedding into a long vector, losing sequential structure. RNNs process sequentially but suffer from gradient flow issues and cannot efficiently attend across the buffer. Both architectures require parameter updates to adapt, making them vulnerable to plasticity loss.
- Core assumption: Sequential/attention-based processing is necessary for in-context learning
- Evidence anchors:
  - [section 3] "The RNN and ERMLP showed no clear signs of learning" in regression tasks
  - [section 7] "works investigating in-context learning with MLP or RNN are much more scarce"
  - [corpus] Weak direct evidence; no corpus papers specifically address why RNNs/MLPs fail with replay in this context.
- Break condition: If the failure is due to hyperparameter choices rather than architecture, this mechanism is incorrect

## Foundational Learning

- Concept: Loss of Plasticity
  - Why needed here: Understanding that neural networks trained via backpropagation gradually lose their ability to adapt to new tasks is essential for appreciating why this result matters. The paper positions itself as addressing this fundamental problem without modifying standard deep learning components.
  - Quick check question: Can you explain why a network that performs well on task N might perform poorly on task N+1, even with the same architecture?

- Concept: In-Context Learning
  - Why needed here: The paper's conjectured explanation relies on this concept—models learning from input data during forward propagation without parameter updates. Without understanding this, the proposed mechanism is opaque.
  - Quick check question: How does in-context learning differ from standard gradient-based learning in terms of when "learning" occurs?

- Concept: Experience Replay Buffers
  - Why needed here: The method uses a fixed-size buffer (100 examples) in a first-in-last-out manner. Understanding how this buffer is constructed and used as model input is necessary for implementation.
  - Quick check question: How should the buffer be updated during training, and what happens when the buffer is not yet full?

## Architecture Onboarding

- Component map:
  Replay Buffer -> Embedding Constructor -> Transformer Core -> Output Extraction

- Critical path:
  1. Receive new training example
  2. Construct embedding Z using current buffer content (zero-pad if buffer incomplete)
  3. Forward pass through Transformer
  4. Extract output from last column
  5. Compute loss and gradient step
  6. Push new example to buffer (evict oldest if full)

- Design tradeoffs:
  - Buffer size vs. computational cost: Attention is O(n²), so larger buffers slow training significantly (authors note this limitation)
  - Transformer depth vs. parameter efficiency: The 10-layer Transformer for MNIST had <1% of the MLP's parameters yet outperformed it
  - Architectural simplicity vs. potential performance: The stripped-down Transformer (no FFN, layer norm, positional encoding) works but may not be optimal

- Failure signatures:
  - RNN/ERMLP showing no learning: Likely indicates inability to process the buffer format effectively
  - Rising loss on later tasks (MLP): Classic plasticity loss pattern
  - Slower training with replay buffer: Expected due to O(n²) attention complexity

- First 3 experiments:
  1. Reproduce plasticity loss baseline: Train MLP on Slowly-Changing Regression (1000 tasks, 10,000 examples each) without replay to confirm loss of plasticity occurs
  2. Ablate buffer size: Test Transformer with replay buffer sizes [10, 50, 100, 200] to understand sensitivity to memory capacity
  3. Cross-architecture comparison: Implement the exact embedding construction for Transformer, RNN, and ERMLP with matched parameter counts to verify the architecture-specific effect

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does in-context learning definitively explain why Transformers with experience replay maintain plasticity, or are other architectural factors responsible?
- Basis in paper: [explicit] The authors state, "We conjecture without proof that in-context learning emerges in the Transformers... though it is not clear why learning an algorithm can resolve this issue."
- Why unresolved: The paper establishes a correlation between Transformer performance and plasticity but lacks mechanistic proof or ablation studies confirming that in-context learning is the causal driver.
- What evidence would resolve it: Interpretability studies identifying the implementation of specific learning algorithms in the Transformer's forward pass, or ablations that disable in-context learning capabilities while observing the return of plasticity loss.

### Open Question 2
- Question: Can this approach scale to high-dimensional, complex domains (e.g., ImageNet or modern RL environments) given the computational overhead?
- Basis in paper: [explicit] The authors acknowledge, "Our problems are simple in scale, leaving the scalability of this approach unexamined," and note that "processing long sequences with attention is expensive."
- Why unresolved: The experiments utilize low-dimensional benchmarks (permuted MNIST, Boyan's chain), leaving the efficacy and resource requirements for real-world, high-dimensional data unknown.
- What evidence would resolve it: Successful application of the proposed method on large-scale continual learning benchmarks (e.g., continual ImageNet) without prohibitive memory or compute costs.

### Open Question 3
- Question: Can efficient sequence models, such as structured state-space models (SSMs), replicate the plasticity benefits of Transformers without the quadratic computational cost?
- Basis in paper: [explicit] The authors identify the complexity of attention as a limitation and suggest, "One promising future extension is to use the structured state-space models... to speed up processing."
- Why unresolved: It is undetermined if the plasticity benefits are unique to the specific attention mechanism of Transformers or if they generalize to other sequence-processing architectures.
- What evidence would resolve it: Experiments substituting the Transformer with linear-complexity SSMs (e.g., Mamba) to see if they can similarly leverage experience replay to prevent plasticity loss.

## Limitations
- The in-context learning mechanism remains unproven and is only conjectured as the explanation for plasticity maintenance
- The stripped-down Transformer architecture may be crucial but this architectural choice is not systematically investigated
- Replay buffer size (100) appears arbitrary and may significantly impact results without systematic study
- The claim that MLPs and RNNs "fail" could reflect suboptimal hyperparameter choices rather than fundamental architectural limitations

## Confidence

- **High confidence**: Transformers with experience replay outperform MLPs in continual learning settings (regression, classification, RL). The empirical results are consistent and statistically significant across multiple tasks.
- **Medium confidence**: Experience replay alone can prevent loss of plasticity in Transformers, while MLPs degrade despite using the same replay mechanism. The results are robust but the explanation remains speculative.
- **Low confidence**: The in-context learning hypothesis is the actual mechanism driving the plasticity advantage. This remains an educated conjecture without direct evidence or ablation studies.

## Next Checks

1. **Ablation study**: Remove components from the Transformer (FFN, layer norm, positional encoding) systematically to determine which architectural choices are essential for the plasticity advantage.

2. **Mechanistic probing**: Conduct controlled experiments to test whether the model is actually implementing learning algorithms in its forward pass, rather than simply storing and retrieving patterns from the replay buffer.

3. **Parameter matching**: Re-run experiments with Transformers and MLPs matched for parameter count and training compute, and systematically sweep hyperparameters for both architectures to ensure fair comparison.