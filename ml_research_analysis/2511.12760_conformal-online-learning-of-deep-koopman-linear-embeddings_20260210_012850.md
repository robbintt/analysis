---
ver: rpa2
title: Conformal Online Learning of Deep Koopman Linear Embeddings
arxiv_id: '2511.12760'
source_url: https://arxiv.org/abs/2511.12760
tags:
- learning
- online
- koopman
- prediction
- coloke
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Conformal Online Learning of Koopman embeddings
  (COLoKe), a framework for adaptively updating Koopman-invariant representations
  from streaming data. COLoKe combines deep feature learning with multistep prediction
  consistency in the lifted space, where dynamics evolve linearly.
---

# Conformal Online Learning of Deep Koopman Linear Embeddings

## Quick Facts
- arXiv ID: 2511.12760
- Source URL: https://arxiv.org/abs/2511.12760
- Reference count: 40
- Primary result: COLoKe improves Koopman embedding accuracy by nearly two orders of magnitude over baselines on the chaotic Lorenz system while reducing unnecessary updates.

## Executive Summary
COLoKe introduces a framework for adaptive Koopman embedding learning from streaming data that combines deep feature learning with multistep prediction consistency. The key innovation is a conformal-style mechanism that triggers updates only when prediction error exceeds a dynamically calibrated threshold, preventing overfitting while maintaining accuracy. The method learns a nonlinear feature map that concatenates the identity with learned features, eliminating the need for a separate decoder network. Evaluated on both synthetic and real-world datasets, COLoKe consistently outperforms state-of-the-art online Koopman learning methods.

## Method Summary
COLoKe learns Koopman-invariant representations online from streaming data by maintaining a deep feature map Φ_θ(x) and linear Koopman operator K. The feature map includes the identity mapping [x, Φ̃_θ(x)]^T to enable built-in reconstruction. Updates are triggered only when a conformal prediction conformity score exceeds a dynamically calibrated threshold, computed using multistep prediction consistency within a rolling buffer. The training objective accumulates prediction errors across all valid multi-step pairs within the buffer, promoting spectral consistency and learning of Koopman eigenfunctions. The method uses AdamW optimization with learning rate 10^-3 and conformal PI control for threshold adaptation.

## Key Results
- COLoKe achieves lower prediction error while significantly reducing unnecessary updates compared to existing online Koopman learning methods
- On the chaotic Lorenz system, COLoKe improves accuracy by nearly two orders of magnitude over baselines
- The method demonstrates superior computational efficiency, achieving lower test error in less time than offline training approaches
- COLoKe successfully recovers known Koopman eigenvalues on benchmark systems with relative error within 1%

## Why This Works (Mechanism)

### Mechanism 1: Conformal-Based Adaptive Update Triggering
COLoKe computes a prediction conformity score that accumulates multi-step prediction errors. Updates are triggered only when this score exceeds a dynamically calibrated threshold, shifting from fixed-step updates to data-driven decisions. This mechanism reduces unnecessary updates while maintaining accuracy by focusing on significant prediction deviations. The method assumes slowly evolving or piecewise-stationary dynamics where prediction errors cluster. Failure occurs in highly non-stationary systems where the threshold grows linearly, violating theoretical assumptions.

### Mechanism 2: Multi-Step Prediction Loss for Spectral Consistency
The training loss accumulates prediction errors across all valid multi-step pairs within the buffer, forcing the learned Koopman operator to be consistent across multiple prediction horizons. This promotes identification of persistent spectral modes and approximate Koopman eigenfunctions, enabling better long-term dynamics capture than single-step losses. The underlying assumption is that the dynamics admit an approximately finite-dimensional Koopman-invariant subspace. The method may struggle with chaotic systems where multi-step losses explode.

### Mechanism 3: Built-In Reconstruction via Identity Embedding
COLoKe's feature map concatenates the raw state with learned nonlinear features, eliminating the need for a separate decoder network. This couples reconstruction directly into the consistency loss, where prediction errors in lifted space reflect discrepancies in original coordinates. The approach assumes state dimension is small enough that concatenation doesn't dominate the embedding. For very high-dimensional states like images, this would become impractical.

## Foundational Learning

- **Concept: Koopman Operator Theory**
  - Why needed: The entire framework assumes nonlinear dynamics can be represented as linear evolution in a lifted observable space
  - Quick check: Can you explain why the Koopman operator is linear even when the underlying dynamics T is nonlinear?

- **Concept: Conformal Prediction**
  - Why needed: The update triggering mechanism adapts conformal PI control from uncertainty quantification to model adaptation decisions
  - Quick check: In standard conformal prediction, what does the prediction set C_t represent, and what assumption is required for coverage guarantees?

- **Concept: Online Convex Optimization (Dynamic Regret)**
  - Why needed: Theoretical analysis uses dynamic regret bounds with path variation terms to characterize performance in non-stationary settings
  - Quick check: What is the difference between static regret and dynamic regret in online learning?

## Architecture Onboarding

- **Component map:** Buffer manager -> Feature encoder Φ̃_θ -> Koopman operator K -> Conformal PI controller -> Loss computer
- **Critical path:** New observation arrives → update buffer → compute score s_t → update threshold q_{t+1} → if s_t > q_t: gradient descent → wait for next observation
- **Design tradeoffs:** Window size w balances temporal dependencies vs. computation (O(w^2) loss scaling); embedding dimension m trades expressivity vs. overfitting; α parameter controls update frequency vs. accuracy; learning rates require manual tuning
- **Failure signatures:** Score never decreases (increase m or network depth); threshold grows linearly (model lacks expressivity or data has abrupt shifts); eigenvalues diverge (reduce w or add gradient clipping); update frequency ≈ 100% (α too high or initial q_{t_0} too low)
- **First 3 experiments:** 1) Sanity check on 2D single attractor system with known eigenvalues, verify recovery within 1% relative error; 2) Ablation on conformal vs. fixed-step updates, reproduce Pareto front confirming adaptive triggering outperforms fixed budgets; 3) Stress test on Lorenz system, target online error < 4×10^-3 while monitoring cumulative thresholds grow sublinearly

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the sublinear growth of cumulative conformity thresholds (Assumption A3), necessary for the dynamic regret bound, be rigorously derived from first principles for autonomous systems?
- Basis: The Conclusion states that deriving assumption (A3) remains an important direction for future work
- Why unresolved: The proof of Theorem 3.3 requires cumulative sum of thresholds to be sublinear, but the paper lacks theoretical proof that conformal PI control guarantees this for all stable autonomous systems
- What evidence would resolve it: A formal proof linking Koopman operator spectral properties or system stability margin to convergence rate of threshold updates

### Open Question 2
- Question: How can the COLoKe framework be extended to non-autonomous systems where the governing dynamics change over time?
- Basis: The Conclusion identifies extension to non-autonomous systems as a methodological direction
- Why unresolved: Current formulation and regret analysis assume time-invariant map T or slowly evolving dynamics, not accounting for abrupt changes in underlying system physics
- What evidence would resolve it: A modified algorithm that detects or adapts to parameter shifts in K without violating consistency requirements of conformal update rule

### Open Question 3
- Question: Can the conformal update mechanism and dynamic regret bounds be maintained when applied to stochastic dynamical systems with process noise?
- Basis: The Introduction reviews stochastic Koopman operators, but the methodological formulation and theoretical analysis assume deterministic dynamics
- Why unresolved: Prediction conformity score measures discrepancy against deterministic linear evolution; inherent stochastic noise could trigger continuous updates or require redefinition of consistency set
- What evidence would resolve it: Deriving dynamic regret bound accounting for noise variance or demonstrating empirically that threshold converges despite stochastic fluctuations

## Limitations

- Theoretical regret bounds rely on assumptions (slowly evolving dynamics, sublinear threshold growth) that may not hold for highly non-stationary or chaotic systems
- Critical hyperparameters (window size w, α for update frequency) require manual tuning without automatic calibration methods
- Paper lacks ablation studies isolating individual contributions of multi-step loss vs. conformal triggering vs. built-in reconstruction

## Confidence

- **High confidence**: Prediction accuracy improvements over baselines (validated on 6 benchmark systems), computational efficiency gains, eigenvalue recovery on known systems
- **Medium confidence**: Theoretical regret bounds (limited to synthetic experiments), effectiveness of conformal triggering (no ablation vs. fixed-step methods)
- **Low confidence**: Claims about generalizability to high-dimensional real-world systems (only 3 real datasets tested, all relatively low-dimensional)

## Next Checks

1. Test on a system with known abrupt regime changes to validate whether the conformal threshold grows sublinearly under Assumption A3
2. Conduct ablation study isolating: (a) multi-step loss vs. single-step loss, (b) conformal vs. fixed-step update schedules, (c) identity embedding vs. separate decoder
3. Evaluate on a high-dimensional chaotic system (e.g., Mackey-Glass or high-dimensional Lorenz) to stress-test eigenvalue estimation and update efficiency claims