---
ver: rpa2
title: 'Overestimation in LLM Evaluation: A Controlled Large-Scale Study on Data Contamination''s
  Impact on Machine Translation'
arxiv_id: '2501.18771'
source_url: https://arxiv.org/abs/2501.18771
tags:
- contamination
- data
- performance
- training
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates data contamination in large language models
  (LLMs) during pre-training, focusing on machine translation tasks. The authors conduct
  a controlled study by decontaminating their train-test splits, then systematically
  introducing contamination at various stages, scales, and formats to isolate its
  effects on performance.
---

# Overestimation in LLM Evaluation: A Controlled Large-Scale Study on Data Contamination's Impact on Machine Translation

## Quick Facts
- arXiv ID: 2501.18771
- Source URL: https://arxiv.org/abs/2501.18771
- Reference count: 40
- Primary result: Source-target contamination inflates BLEU scores up to 30 points, 2.5× larger for 8B vs 1B models

## Executive Summary
This paper investigates data contamination in large language models (LLMs) during pre-training, focusing on machine translation tasks. The authors conduct a controlled study by decontaminating their train-test splits, then systematically introducing contamination at various stages, scales, and formats to isolate its effects on performance. Using models of 1B and 8B parameters, they find that contaminating both source and target sides of translation pairs significantly inflates BLEU scores, with the 8B model showing up to 30 BLEU points overestimation—2.5 times larger than the 1B model. Source-only or target-only contamination yields smaller, inconsistent effects. The timing and frequency of contamination also matter: early or uniform contamination causes the largest overestimations. Notably, contamination only impacts languages with sufficient representation in the pre-training data.

## Method Summary
The authors perform a controlled study on data contamination's impact on machine translation performance. They first decontaminate their train-test splits, then systematically introduce contamination at various stages, scales, and formats. The experiments use 1B and 8B parameter models, measuring performance through BLEU scores across different contamination scenarios including source-only, target-only, and source-target contamination, with variations in timing and frequency.

## Key Results
- Source-target contamination inflates BLEU scores up to 30 points
- 8B models show 2.5× larger overestimation than 1B models
- Contamination timing matters: early or uniform contamination causes largest effects
- Contamination only impacts languages with sufficient pre-training data representation

## Why This Works (Mechanism)
Data contamination occurs when pre-training data overlaps with test data, allowing models to memorize rather than generalize. When both source and target sides of translation pairs are present in pre-training data, models can directly retrieve memorized translations rather than learning genuine translation capabilities. The 8B model's larger capacity enables more effective memorization of contaminated pairs, explaining its 2.5× higher overestimation compared to the 1B model. Early contamination is most problematic because models have more training iterations to reinforce memorized patterns.

## Foundational Learning
- **BLEU Score**: Measures translation quality by comparing n-gram overlap between candidate and reference translations; needed to quantify contamination effects on translation performance
- **Data Contamination**: Overlap between pre-training and test data; quick check: verify no shared examples between training and evaluation sets
- **Parameter Scaling**: Model size impacts memorization capacity; quick check: compare performance across different model sizes
- **Source-Target Contamination**: Contamination affecting both input and output sides of translation pairs; quick check: verify contamination on both source and target sides

## Architecture Onboarding
- **Component Map**: Pre-training corpus -> Model training -> Evaluation -> BLEU scoring
- **Critical Path**: Clean data preparation → Controlled contamination injection → Model training → Performance evaluation
- **Design Tradeoffs**: Model size vs. contamination susceptibility, timing of contamination vs. memorization strength
- **Failure Signatures**: Overestimated BLEU scores, inconsistent performance across contamination scenarios
- **First Experiments**: 1) Test contamination at different model sizes 2) Verify contamination effects across multiple languages 3) Measure performance degradation when contaminated data is removed

## Open Questions the Paper Calls Out
None

## Limitations
- Findings may not generalize to other model sizes beyond 1B and 8B parameters
- BLEU metric may not capture all qualitative aspects of contamination effects
- Limited language coverage may affect generalizability of language-specific findings

## Confidence
- High: Core finding that source-target contamination inflates BLEU scores
- Medium: Timing and frequency effects of contamination
- Low: Language-specific findings due to limited language coverage

## Next Checks
1. Test contamination effects across diverse model sizes (100M to 70B parameters) to determine scaling relationships and identify threshold effects
2. Evaluate performance using complementary metrics beyond BLEU (e.g., COMET, human evaluation) to verify that contamination effects persist across quality assessment methods
3. Replicate experiments with non-translation tasks (summarization, question answering) to determine whether contamination's magnitude varies by task type and whether the source-target contamination pattern generalizes