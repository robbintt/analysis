---
ver: rpa2
title: 'The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models'
arxiv_id: '2509.26543'
source_url: https://arxiv.org/abs/2509.26543
tags:
- explanations
- gender
- language
- contrastive
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first method for contrastive explanations
  in speech-to-text (S2T) models, addressing the challenge of explaining why models
  generate one word over another. The core method builds on perturbation-based feature
  attribution by aggregating subword-level probabilities into word-level probabilities
  and introducing a relative scoring function that quantifies probability changes
  between target and foil words.
---

# The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models

## Quick Facts
- arXiv ID: 2509.26543
- Source URL: https://arxiv.org/abs/2509.26543
- Reference count: 40
- Primary result: Introduces first method for contrastive explanations in S2T models using relative scoring function

## Executive Summary
This paper presents the first method for generating contrastive explanations in speech-to-text (S2T) models, addressing the critical question of why models generate one word over another. The core innovation is a relative scoring function that aggregates subword-level probabilities into word-level probabilities and quantifies probability changes between target and foil words. Through a case study on gender assignment in speech translation, the method successfully isolates gender-relevant features, demonstrating its effectiveness in focusing on contrastive aspects of word generation.

## Method Summary
The method builds on perturbation-based feature attribution by first aggregating subword-level probabilities into word-level probabilities. It introduces a relative scoring function that quantifies probability changes between target and foil words, overcoming limitations of previous difference-based scoring functions that fail to produce truly contrastive explanations. The approach involves perturbing audio features, measuring how probability distributions shift between competing word candidates, and ranking features based on their contrastive impact.

## Key Results
- Coverage rates above 30% achieved for isolating gender-relevant features
- Flip rates over 70% for feminine predictions when progressively deleting top features
- Relative scoring function produces distinctly different saliency maps compared to non-contrastive approaches

## Why This Works (Mechanism)
The relative scoring function captures the differential impact of feature perturbations on competing word candidates, naturally focusing attention on features that distinguish between alternatives rather than features that generally increase probability. By quantifying changes in relative probabilities between target and foil words, the method aligns with human reasoning patterns for contrastive explanations.

## Foundational Learning
- **Contrastive explanations**: Why explanations focus on differences between alternatives rather than absolute properties
  - *Why needed*: Human explanations typically involve comparing alternatives
  - *Quick check*: Compare absolute vs relative feature importance scores

- **Perturbation-based attribution**: Method for determining feature importance by measuring impact of feature removal
  - *Why needed*: Provides model-agnostic approach to explanation
  - *Quick check*: Verify monotonic decrease in target probability when removing features

- **Subword aggregation**: Process of combining subword probabilities into word-level probabilities
  - *Why needed*: S2T models operate at subword level but explanations need word-level interpretation
  - *Quick check*: Ensure aggregated probabilities sum to 1 across all word candidates

- **Relative scoring function**: Mathematical formulation that quantifies probability changes between competing words
  - *Why needed*: Enables truly contrastive explanations rather than absolute importance
  - *Quick check*: Verify higher scores for features that differentially affect competing words

## Architecture Onboarding

**Component Map**: Audio input -> S2T model -> Subword probabilities -> Word aggregation -> Relative scoring -> Feature ranking

**Critical Path**: The perturbation and probability aggregation pipeline is critical - errors in feature masking or probability normalization will propagate through the entire explanation generation process.

**Design Tradeoffs**: The method trades computational efficiency for explanatory quality, as perturbation-based approaches require multiple model evaluations. The choice of perturbation magnitude and feature grouping strategy significantly impacts explanation quality.

**Failure Signatures**: Explanations that highlight irrelevant features, fail to distinguish between target and foil words, or show high variance across repeated perturbations indicate problems with the relative scoring function or aggregation process.

**3 First Experiments**:
1. Verify subword-to-word probability aggregation preserves ranking of top candidates
2. Test relative scoring on synthetic data where ground truth contrastive features are known
3. Compare saliency maps from relative vs difference-based scoring on simple translation pairs

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Empirical validation limited to single case study with one translation pair and model architecture
- Computational expense may limit scalability for production deployment
- Manual curation of foil candidates introduces potential evaluation bias
- Definition of "top features" lacks rigorous statistical justification

## Confidence
- High: Technical innovation of relative scoring function and its distinction from existing methods
- Medium: Case study results showing successful feature isolation in gender assignment task
- Low: Broader claims about method's applicability across diverse S2T scenarios without additional validation

## Next Checks
1. Test the method across multiple S2T tasks (ASR, translation, multimodal) and model architectures to assess generalizability
2. Conduct ablation studies on the relative scoring function to quantify its contribution versus baseline perturbation methods
3. Evaluate computational efficiency and scalability on production-scale models with extended audio sequences