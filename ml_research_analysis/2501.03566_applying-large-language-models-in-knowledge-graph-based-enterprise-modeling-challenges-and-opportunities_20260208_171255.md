---
ver: rpa2
title: 'Applying Large Language Models in Knowledge Graph-based Enterprise Modeling:
  Challenges and Opportunities'
arxiv_id: '2501.03566'
source_url: https://arxiv.org/abs/2501.03566
tags:
- modeling
- knowledge
- enterprise
- language
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of large language models (LLMs)
  in knowledge graph-based enterprise modeling, focusing on the task of mapping ArchiMate
  modeling elements to domain concepts. The authors conduct an expert survey and ChatGPT-4o-based
  experiments across five use cases, evaluating the consistency and accuracy of LLM-generated
  model mappings compared to human experts.
---

# Applying Large Language Models in Knowledge Graph-based Enterprise Modeling: Challenges and Opportunities

## Quick Facts
- arXiv ID: 2501.03566
- Source URL: https://arxiv.org/abs/2501.03566
- Reference count: 40
- Primary result: ChatGPT shows high consistency (75-100%) in selecting relevant ArchiMate elements but struggles with identifying irrelevant elements (25% false positives)

## Executive Summary
This paper investigates the application of large language models to knowledge graph-based enterprise modeling, specifically examining how ChatGPT-4o can map domain concepts to ArchiMate modeling elements. Through expert surveys and experiments across five justice domain use cases, the authors evaluate the consistency and accuracy of LLM-generated model mappings. The study reveals that while LLMs demonstrate high consistency in selecting relevant elements, they exhibit significant challenges in identifying irrelevant elements and maintaining precise semantic relationships, highlighting the need for human oversight in enterprise modeling workflows.

## Method Summary
The study employs a zero-shot prompting approach using ChatGPT-4o to map domain concepts from the U.S. National Information Exchange Model (NIEM) to ArchiMate modeling elements. Five use cases from the justice domain were selected, each run 20 times to measure consistency. The prompt structure includes context description, domain concept specification, ArchiMate viewpoint definition, and task instructions. Evaluation metrics focus on element priority ranking, instantiation probability assignments, and relationship type determinations, with results compared against human expert rankings.

## Key Results
- ChatGPT achieved 75-100% consensus in selecting the most relevant ArchiMate element across repeated trials
- The model incorrectly marked 25% of irrelevant elements as highly probable to instantiate
- LLM-based selections aligned closely with expert rankings for top elements but showed less variability overall
- Both experts and ChatGPT struggled with defining precise relationship types, with LLMs preferring broader relations like "Matches" and "Related"

## Why This Works (Mechanism)

### Mechanism 1: Probabilistic Semantic Mapping via Natural Language Prompts
The LLM processes natural language definitions of domain concepts and ArchiMate elements to predict statistically probable associations based on semantic similarity learned during pre-training. This approach assumes semantic meaning is sufficiently captured in textual descriptions rather than requiring formal logic execution. The method works well for finding relevant connections but may fail when domain concepts are ambiguous or modeling element definitions exceed context windows.

### Mechanism 2: Consistency vs. Discrimination (The False Positive Trap)
LLMs exhibit lower variability than human experts in element selection but fail to effectively apply negative constraints. When presented with potential elements, the model tends to find some degree of connection for most options, leading to high probability scores for elements that should be marked irrelevant. This over-generalization results in the 25% false positive rate observed in the study.

### Mechanism 3: Semantic Drift in Relation Typing
The LLM interprets relation definitions based on colloquial usage in its training data, whereas experts interpret them as formal ontological constraints. This leads to a drift where the model suggests valid associations that are technically imprecise, preferring broader relations like "Matches" and "Related" over narrower types like "Identical" and "Similar."

## Foundational Learning

- **ArchiMate Enterprise Modeling (specifically Capability Maps)**: Understanding ArchiMate elements (Capability, Resource, Outcome) is essential for evaluating mapping correctness. Quick check: Can you distinguish between an "Outcome" (a result) and a "Capability" (an ability) in a business context?

- **Knowledge Graphs (KG) as Semantic Input**: The paper proposes shifting from manual text input to KG-based input to ground the LLM. Quick check: Do you understand how a natural language description of "Electronic Court Filing" converts into a graph structure (Subject-Predicate-Object)?

- **Zero-Shot Prompting**: This mechanism queries the LLM without prior examples, testing "out-of-the-box" reasoning. Quick check: Do you know how to structure a prompt that provides context and instructions without giving the model a solved example to mimic?

## Architecture Onboarding

- **Component map**: Input Layer (Domain Concepts + ArchiMate Standard) -> Processing Engine (LLM with Zero-Shot Prompting) -> Control Mechanism (Human Expert) -> Output (Instantiated Model Elements)

- **Critical path**: 1. Context Loading: Inject ArchiMate viewpoint definition and domain description into prompt 2. Ranking Task: Prompt LLM to rank elements and assign probability 3. Human Validation: Expert reviews "High Probability" selections to filter false positives

- **Design tradeoffs**: The system gains high consistency (low variability) using LLM but trades off semantic precision (loose relations). Automated generation is faster but requires significant human post-processing to catch irrelevant elements marked as "High" probability.

- **Failure signatures**: The "Helpful Assistant" Bias (model rates almost everything as "High" or "Medium" probability) and Relation Dilution (model defaults to "Related" or "Matches" for almost all connections).

- **First 3 experiments**: 1. Consistency Check: Run the exact same prompt 20 times on a single domain concept to measure variance 2. Negative Testing: Introduce a clearly irrelevant domain concept to see if LLM still assigns "High" probability 3. Relation Strictness: Ask LLM to map relations using strict definitions vs. no definitions to test constraint effects

## Open Questions the Paper Calls Out

- How does LLM performance differ when applied to flow-oriented viewpoints compared to structure-oriented viewpoints tested? The current study restricted scope to static structure elements, leaving flow-based ArchiMate elements untested.

- What technical or prompt-engineering interventions can effectively reduce the LLM's tendency to assign high instantiation probabilities to irrelevant elements? The paper identifies this over-generation error but does not propose calibration methods for discrimination thresholds.

- How can a modeling process be designed to effectively integrate LLM generation with human semantic validation? The research focused on isolated mapping tasks rather than proposing a holistic workflow for interactive modeling.

## Limitations

- The evaluation is constrained to a single modeling language (ArchiMate) and domain (justice), limiting generalizability
- Zero-shot prompting may underutilize the model's potential compared to fine-tuned or few-shot alternatives
- The 25% false positive rate for irrelevant elements suggests significant gaps between statistical consistency and semantic accuracy

## Confidence

- **High Confidence**: Consistency findings (75-100% consensus on top elements) are robust given repeated trials and clear measurement approach
- **Medium Confidence**: Comparison between expert and LLM relation types is well-documented but may reflect prompting style rather than inherent limitations
- **Low Confidence**: Generalizability of the "irrelevant element" failure mode across different modeling languages or domains remains uncertain

## Next Checks

1. **Domain Generalization Test**: Apply the same methodology to a non-justice domain (e.g., healthcare or finance) to assess whether consistency-accuracy tradeoff persists across contexts

2. **Language Extension Study**: Evaluate the approach with alternative enterprise modeling languages (BPMN, UML) to determine if semantic mapping challenges are language-specific or universal

3. **Fine-tuning Impact Analysis**: Compare zero-shot results against a few-shot variant where the model is provided with 2-3 example mappings to assess whether training examples reduce false positive rates for irrelevant elements