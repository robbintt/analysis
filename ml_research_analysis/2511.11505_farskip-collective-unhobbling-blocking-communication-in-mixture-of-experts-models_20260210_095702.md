---
ver: rpa2
title: 'FarSkip-Collective: Unhobbling Blocking Communication in Mixture of Experts
  Models'
arxiv_id: '2511.11505'
source_url: https://arxiv.org/abs/2511.11505
tags:
- communication
- farskip-collective
- training
- computation
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces FarSkip-Collective, a method to convert model
  architecture and execution to enable overlapping of computation and communication
  in Mixture-of-Experts (MoE) models. By modifying residual connectivity and employing
  a self-distillation approach, the authors demonstrate that state-of-the-art MoE
  models (16B to 109B parameters) can be converted to avoid blocking communication
  while maintaining model accuracy.
---

# FarSkip-Collective: Unhobbling Blocking Communication in Mixture of Experts Models

## Quick Facts
- **arXiv ID:** 2511.11505
- **Source URL:** https://arxiv.org/abs/2511.11505
- **Reference count:** 40
- **Primary result:** Converts MoE models (16B-109B) to avoid blocking communication while maintaining <2.5% accuracy loss; achieves up to 88.4% overlap in training and 18.5% speedup in inference.

## Executive Summary
This work introduces FarSkip-Collective, a method to convert Mixture-of-Experts (MoE) model architecture and execution to enable overlapping of computation and communication. By modifying residual connectivity patterns and employing a self-distillation approach, the authors demonstrate that state-of-the-art MoE models can be converted to avoid blocking communication while maintaining model accuracy. The method is validated on three MoE models (16B to 109B parameters), showing average accuracy within 1% of original instruction-tuned releases across diverse evaluations.

## Method Summary
The method involves two key components: (1) Architecture modification using FarSkip connectivity, which breaks blocking residual dependencies by substituting outdated or partial activations as inputs to subsequent layers, enabling communication to proceed in parallel with computation; (2) Far-Collective Self-Distillation (FCSD), a knowledge distillation approach using KL divergence to recover capabilities degraded by the architecture modification. The converted models are then implemented with optimized async communication strategies for both training (Megatron-LM) and inference (vLLM & SGLang), achieving significant communication overlap percentages.

## Key Results
- Llama-4 Scout (109B) converted via FCSD achieves average accuracy within 1% of original across 11 benchmarks
- DeepSeek-V2 Lite (16B) achieves 87.6% forward and 89.0% backward communication overlap in training
- Llama-4 Scout shows up to 18.5% speedup in Time-To-First-Token during inference
- Communication overlap reaches 88.4% during training for converted models

## Why This Works (Mechanism)

### Mechanism 1: Residual Dependency Breaking via FarSkip Connectivity
Modifying residual connections to use available activations (outdated or partial) instead of blocking on communicated results enables computation-communication overlap without fundamentally degrading model capacity. Standard MoE layers compute ok = o0 + f1(o0) + ... + fk(ok-1), where fk involves blocking communication. FarSkip substitutes ok-1 as input to fk+1 (outdated) or ok-1 + partial fk output (partial), allowing communication to proceed in parallel with fk+1 computation. The communicated result is "far-skipped" to the residual of a future layer.

### Mechanism 2: Self-Distillation Recovery (FCSD)
KL-based knowledge distillation using the original model as teacher recovers capabilities degraded by architecture modification, achieving near-parity with <10B tokens. The FarSkip-modified model (student) is trained to match the original model's (teacher) predictive distribution via KL divergence. This provides granular signal for realigning internal representations that were disrupted by changed connectivity patterns.

### Mechanism 3: Explicit Async Execution with Autograd Manipulation
Overlapping requires deliberate scheduling via async operations, custom autograd functions, and backward-hook synchronization; frameworks do not automatically achieve this. Forward pass uses torch.distributed async_op=True to launch non-blocking collectives. Backward pass uses a stateful dictionary to store communication handles and backward hooks to synchronize at correct points. Sequence Number manipulation reprioritizes autograd's execution order to maximize overlap window.

## Foundational Learning

- **Expert Parallelism (EP) and All-to-All Collectives**: Understanding EP and the blocking nature of all-to-all collectives is prerequisite to grasping what communication is being overlapped. Quick check: In EP, why does the Dispatch collective require per-token routing information before communication can begin?

- **Residual Connection Semantics in Transformers**: Understanding that ok = ok-1 + fk(ok-1) is the standard formulation clarifies what "outdated" vs "partial" means mathematically. Quick check: If you use ok-1 as input to fk+1 instead of ok, which block's output is missing from the residual stream at layer k+1?

- **KL Divergence for Knowledge Distillation**: Understanding KL divergence differs from SFT (which uses ground-truth labels) explains why self-distillation is more effective for architecture conversion. Quick check: Why would matching the teacher's probability distribution over vocabulary provide more signal than matching one-hot ground-truth labels?

## Architecture Onboarding

- **Component map**: Original checkpoint -> FarSkip-modified architecture (outdated/partial activations) -> FCSD trainer (KL distillation) -> async communication handler -> validation on benchmarks

- **Critical path**: Load original checkpoint into FarSkip-modified architecture definition -> Run FCSD distillation (500M–10B tokens depending on model scale) with batch-size and LR sweeps -> Validate on MBPP+ every 1000 steps; early stop on 2% degradation with patience 20 -> Export converted checkpoint; integrate with vLLM/SGLang using async all-reduce and CUDA-graph-compatible communication API

- **Design tradeoffs**: Outdated vs Partial activations (Partial preserves more information but reduces overlap window; outdated maximizes overlap but increases degradation), Conversion scope (converting fewer layers is easier but leaves blocking communication in unconverted layers), Training vs inference optimization (different communication patterns require different async strategies)

- **Failure signatures**: Random baseline accuracy on MMLU / 0% on HumanEval+ after full-layer conversion without distillation, Mode collapse late in distillation training (sudden performance crash on code generation benchmarks), Communication not overlapped despite modified architecture (frameworks do not auto-schedule async execution), First/last layer communication bubbles persist (inherent dependency barriers)

- **First 3 experiments**:
  1. Baseline degradation measurement: Load Qwen-3-30B MoE checkpoint, apply FarSkip to N∈{1,25,50,100}% of layers from start vs end, evaluate on MMLU/HumanEval+/GSM-8K without training
  2. Single-layer async overlap microbenchmark: In Megatron-LM, instrument one MoE layer with async all-to-all and CUDA events. Measure achieved overlap percentage under varying sequence lengths and EP sizes
  3. FCSD ablation on small model: Train DeepSeek-V2-Lite (16B) with FCSD vs SFT vs KL+intermediate-L2 for 500M tokens. Compare recovery on 11 benchmarks

## Open Questions the Paper Calls Out

### Open Question 1
Can "multi-block" FarSkip variants effectively overlap communication when communication duration significantly exceeds sub-block computation time (e.g., in extremely sparse MoEs or high-latency networks)? The current study limits "far-skipping" to a single block of depth. Skipping multiple blocks increases the staleness of activations, potentially degrading model accuracy or convergence stability beyond the 1% drop observed in single-block skipping.

### Open Question 2
How does FarSkip-Collective interact with highly fused kernels (e.g., fused MLA attention) that reduce the computational granularity required for overlap windows? FarSkip relies on finding "overlappable computation" to hide latency. If fused kernels drastically shorten computation duration, the inequality for overlap may fail, preventing overlap without suffering accuracy loss via deeper skips.

### Open Question 3
Does the removal of blocking communication dependencies fundamentally shift the optimal trade-off point for Expert Parallelism (EP) versus Tensor Parallelism (TP) degrees? Standard configuration choices are often constrained by the cost of blocking all-to-all communication. While the paper demonstrates speedups at fixed EP sizes, it does not map the new Pareto frontier for model configuration now that this constraint is relaxed.

## Limitations
- Self-distillation effectiveness for MoE models is demonstrated but not deeply validated against alternative recovery methods
- Backward hook synchronization for communication overlap is described but not fully specified across PyTorch versions
- Conversion scalability beyond 109B parameters is untested; larger models may have different sensitivity to residual modification patterns

## Confidence
- **High confidence**: Architecture modification (FarSkip) correctly identifies and modifies blocking communication patterns; core mechanism is mathematically sound
- **Medium confidence**: FCSD recovery recipe achieves claimed accuracy (within 2.5% across 11 benchmarks) based on three model demonstrations, but lacks broader validation
- **Medium confidence**: Implementation details for async overlap are specific and measurable, but exact PyTorch autograd manipulation may not be portable

## Next Checks
1. **FCSD Ablation Study**: Train DeepSeek-V2-Lite with SFT-only, SFT+intermediate-L2, and FCSD (KL) for 500M tokens each; compare recovery on HumanEval+/MBPP+ to validate KL distillation superiority
2. **Portability Test**: Attempt to reproduce backward-hook synchronization on PyTorch 2.4 vs 2.3; document any autograd API changes that break the implementation
3. **Scale Boundary Test**: Apply FarSkip-Collective to a 200B+ MoE model (e.g., Qwen-3-235B MoE) and evaluate whether the same FCSD recipe maintains <5% degradation, or if larger models require modified recovery strategies