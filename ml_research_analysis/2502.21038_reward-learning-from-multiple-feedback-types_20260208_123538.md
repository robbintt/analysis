---
ver: rpa2
title: Reward Learning from Multiple Feedback Types
arxiv_id: '2502.21038'
source_url: https://arxiv.org/abs/2502.21038
tags:
- feedback
- reward
- types
- learning
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a framework for learning reward functions\
  \ from multiple types of human feedback, moving beyond traditional binary preferences.\
  \ The authors implement synthetic generation of six feedback types\u2014rating,\
  \ comparison, demonstration, correction, description, and descriptive preference\u2014\
  and corresponding reward models."
---

# Reward Learning from Multiple Feedback Types

## Quick Facts
- **arXiv ID:** 2502.21038
- **Source URL:** https://arxiv.org/abs/2502.21038
- **Reference count:** 40
- **Primary result:** Framework learns reward functions from 6 feedback types (rating, comparison, demonstration, correction, description, descriptive preference) showing descriptive feedback most robust to noise

## Executive Summary
This paper introduces the first comprehensive framework for learning reward functions from multiple types of human feedback in reinforcement learning. The authors implement synthetic generation of six distinct feedback types and corresponding reward models, then evaluate their effectiveness across ten RL environments. Their experiments demonstrate that diverse feedback types can effectively train reward models and downstream agents, with descriptive feedback showing particular robustness to noise. The study also presents preliminary work on combining multiple feedback types through ensemble methods, showing that joint modeling can match single-type performance in some environments. This represents the first empirical investigation of multi-type feedback for RLHF, providing insights into which feedback types work best under different conditions.

## Method Summary
The framework trains reward models using six types of synthetic human feedback generated from expert RL policies. Expert models provide value functions for evaluative feedback, policy rollouts for demonstrative/corrective feedback, and state-action clustering for descriptive feedback. Each feedback type is mapped to either MSE loss for absolute judgments (ratings, descriptions) or cross-entropy loss with Bradley-Terry model for relative judgments (comparisons, corrections, demonstrations, descriptive preferences). Reward models are 6-layer MLPs trained with Masksembles for uncertainty estimation, then used to train downstream RL agents. The authors evaluate both single-type and ensemble approaches across ten environments, systematically varying noise levels and feedback quantities.

## Key Results
- Diverse feedback types (rating, comparison, demonstration, correction, description, descriptive preference) can effectively train reward models and downstream RL agents
- Descriptive feedback shows particular robustness to noise compared to other feedback types
- Ensemble methods combining multiple feedback types can match or exceed single-type performance in some environments
- The framework enables more flexible and robust reward learning compared to traditional binary preference approaches

## Why This Works (Mechanism)

### Mechanism 1: Unified Reward Learning from Structurally Diverse Feedback
Different feedback types can be transformed into a common reward signal through type-specific loss functions. The framework maps each feedback type to either scalar rewards via MSE loss for absolute judgments (ratings, descriptions), or preference rankings via cross-entropy loss with Bradley-Terry model for relative judgments (comparisons, corrections, demonstrations, descriptive preferences). Core assumption: human feedback, regardless of type, reflects an underlying ground-truth reward function that can be approximately recovered through supervised learning.

### Mechanism 2: Synthetic Feedback Generation via Expert Policy Grounding
High-quality simulated human feedback can be generated from pre-trained expert RL models by mapping ground-truth rewards to feedback-type-specific formats. Expert models provide value functions for evaluative feedback, policy rollouts for demonstrative/corrective feedback, and state-action clustering for descriptive feedback. Noise injection via truncated Gaussian perturbation simulates human irrationality. Core assumption: expert policy performance correlates with ground-truth reward, and human feedback noise can be modeled as additive perturbations to underlying rewards.

### Mechanism 3: Ensemble Combination for Robust Multi-Type Learning
Averaging or uncertainty-weighting predictions from multiple feedback-type-specific reward models can match or exceed single-type performance in some environments. Pre-trained reward models for each feedback type are queried jointly; predictions are normalized then combined via simple averaging or weighted by inverse uncertainty. Core assumption: different feedback types provide complementary information; their errors are partially uncorrelated so averaging reduces variance.

## Foundational Learning

- **Concept: Reward Rational Choice Framework (Jeon et al., 2020)**
  - Why needed here: Provides theoretical grounding that diverse feedback types can be interpreted as implicit choices over trajectories, unified under a common reward optimization objective
  - Quick check question: Can you explain how a "demonstration" and a "preference" both represent implicit choices over trajectory distributions?

- **Concept: Bradley-Terry Preference Model**
  - Why needed here: Core mathematical model for converting pairwise preferences into reward predictions via softmax over cumulative rewards
  - Quick check question: Given two trajectories with rewards [1, 2, 3] and [3, 2, 1], compute P[ξ1 ≻ ξ2] assuming β=1.

- **Concept: Masksembles for Uncertainty Estimation**
  - Why needed here: Enables efficient ensemble predictions without training multiple independent models; provides uncertainty estimates for weighted ensemble combination
  - Quick check question: How do Masksembles differ from Monte-Carlo Dropout for uncertainty estimation?

## Architecture Onboarding

- **Component map:**
  Expert Models (PPO/SAC) -> Rollout Buffer -> Feedback Generator (6 types) -> Feedback Dataset -> Reward Model Trainer (MSE/CE loss) -> Reward Models (6) -> Ensemble Combiner (average/uncertainty-weighted) -> Reward Wrapper -> Downstream RL Agent (PPO/SAC with learned reward)

- **Critical path:**
  1. Train expert RL agents to convergence (or load pre-trained)
  2. Generate 10,000 segments from checkpoints across training
  3. Synthesize feedback for all 6 types (ensure comparable sample sizes)
  4. Train 6 reward models with early stopping (100 epochs, patience 5)
  5. Evaluate via downstream RL training (not just validation loss)
  6. Optionally: ensemble via averaging or uncertainty weighting

- **Design tradeoffs:**
  - Feedback dataset size vs. quality: Paper uses 10,000 segments; reducing this degrades some types (evaluative, descriptive) more than others (comparative is relatively efficient)
  - Noise level β: Higher β simulates more human error; descriptive feedback is most robust, evaluative/comparative most sensitive
  - Ensemble strategy: Simple averaging is robust; uncertainty-weighting can help if well-calibrated but fails if uncertainties are miscalibrated

- **Failure signatures:**
  - Reward model validation loss decreases but downstream RL performance plateaus early → learned reward may not align with ground-truth optimization landscape
  - Ensemble underperforms single types → feedback types may be correlated or one type dominates with poor signal
  - Corrective feedback consistently underperforms → expert corrections may not improve over learner trajectories

- **First 3 experiments:**
  1. Reproduce single feedback-type results on HalfCheetah-v5 with optimal (β=0) and noisy (β=0.5) feedback; validate that descriptive feedback is most noise-robust
  2. Generate feedback from only early vs. late expert checkpoints; test sensitivity to demonstration quality
  3. Implement 2-type ensemble (descriptive + comparative) and compare against 6-type ensemble; identify minimal effective combinations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the multi-type feedback framework scale to complex domains like large language model alignment?
- Basis in paper: The authors state in Section 6 that extending findings to "more complex domains remains crucial, particularly LLM alignment and fine-tuning."
- Why unresolved: The current experiments are restricted to RL locomotion and control tasks (Mujoco, Highway, MetaWorld) and do not test high-dimensional language generation tasks.
- What evidence would resolve it: Successful implementation of the multi-type reward modeling pipeline in a large language model fine-tuning benchmark, demonstrating improvements over single-type baselines.

### Open Question 2
- Question: Can dynamic reward models that actively query specific feedback types during training outperform the fixed pre-trained ensembles used in this study?
- Basis in paper: Section 6 identifies "transitioning from fixed pre-trained to continuously updating models" that can leverage "different feedback types at various training stages" as a key area for future work.
- Why unresolved: The current methodology utilizes a single pre-trained reward model phase followed by RL training, without exploring active or adaptive querying strategies.
- What evidence would resolve it: Experiments showing that an online, active learning agent can achieve equivalent performance with fewer total feedback queries by strategically selecting the most informative feedback type at each step.

### Open Question 3
- Question: How does the assumption of additive Gaussian noise in synthetic feedback generation differ from the biases and noise structures inherent in real human feedback?
- Basis in paper: The authors note in Section 6 that they base their simulation on "simple assumptions, such as modeling noise via an additive Gaussian distribution" and encourage human-subject studies to determine appropriate values.
- Why unresolved: The paper relies on synthetic data to enable large-scale evaluation, leaving the specific noise characteristics of real human multi-type feedback uncharacterized.
- What evidence would resolve it: A comparative study fitting noise parameters to actual human annotator data, followed by an analysis of how these realistic noise profiles impact reward model convergence and agent performance.

## Limitations
- Reliance on synthetic expert-generated feedback rather than real human feedback introduces uncertainty about real-world applicability
- The ensemble approach is highly sensitive to noise calibration and feedback type correlation, with Walker2d-v5 serving as a clear failure case
- Claims about descriptive feedback being "most robust to noise" and ensemble benefits require real human studies to validate

## Confidence
- **High confidence:** Basic reward model architecture and training pipeline (MLP with MSE/CE losses, Masksembles for uncertainty)
- **Medium confidence:** Synthetic feedback generation methodology - translation from expert returns to human-style feedback involves assumptions
- **Low confidence:** Claims about descriptive feedback being "most robust to noise" and ensemble benefits - require real human studies to validate

## Next Checks
1. **Noise sensitivity validation:** Generate feedback with varying β values (0.0, 0.3, 0.6, 0.9) and verify the claimed noise robustness hierarchy across all environments
2. **Expert quality dependency:** Train reward models using checkpoints from different training stages (early vs. late) and measure performance degradation to establish minimum expert competence requirements
3. **Minimal effective ensemble:** Systematically test 2-type and 3-type ensembles to identify which combinations consistently outperform single types