---
ver: rpa2
title: Investigating Large Language Models in Diagnosing Students' Cognitive Skills
  in Math Problem-solving
arxiv_id: '2504.00843'
source_url: https://arxiv.org/abs/2504.00843
tags:
- cognitive
- student
- evidence
- llms
- diagnostic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models can diagnose
  students' cognitive skills in math problem-solving beyond just grading answers.
  The authors created MathCog, a benchmark dataset with 639 student responses to 110
  math problems, each annotated by teachers using TIMSS-grounded cognitive skill checklists.
---

# Investigating Large Language Models in Diagnosing Students' Cognitive Skills in Math Problem-solving

## Quick Facts
- **arXiv ID:** 2504.00843
- **Source URL:** https://arxiv.org/abs/2504.00843
- **Reference count:** 40
- **Primary result:** All 18 evaluated LLMs achieved F1 scores below 0.5 on diagnosing cognitive skills from student math work, with performance heavily dependent on evidential strength.

## Executive Summary
This paper investigates whether large language models can diagnose students' cognitive skills in math problem-solving beyond just grading answers. The authors created MathCog, a benchmark dataset with 639 student responses to 110 math problems, each annotated by teachers using TIMSS-grounded cognitive skill checklists. They evaluated 18 LLMs and found that all models struggled with the task, achieving F1 scores below 0.5. Performance was strongly dependent on evidential strength, with models over-attributing evidence when it was vague and frequently producing unsupported explanations. The authors discuss implications for designing evidence-aware, teacher-in-the-loop systems for cognitive diagnosis in educational settings.

## Method Summary
The study created MathCog, a benchmark dataset containing 639 student responses to 110 math problems, annotated with teacher checklists based on TIMSS cognitive framework. Each response was evaluated for 15 cognitive skills across Knowing, Applying, and Reasoning domains. The authors evaluated 18 LLMs using Chain-of-Thought prompting with temperature=0, testing both zero-shot and 2-shot configurations. Models were required to output structured JSON with evidence quotes and verdict classifications (Evident Yes, Vague Yes, Evident No, Vague No). Performance was measured using macro F1, accuracy, and two error attribution metrics: OverAttr (over-attributing evidence) and FalseAttr (false attribution of evidence for incorrect diagnoses).

## Key Results
- All 18 evaluated LLMs achieved F1 scores below 0.5 on the MathCog benchmark
- Model size positively correlates with diagnosis performance (r_s=.771)
- Performance drops dramatically when evidence is vague: ∆F1=.51 and ∆Acc=.41 between Evident vs Vague conditions
- Reasoning models show highest over-inference rates (46.61% of errors)
- Models frequently over-attribute evidence when it's vague (mean OverAttr=.580) and produce unsupported explanations

## Why This Works (Mechanism)

### Mechanism 1: Model Scale Benefits
- **Claim:** Model scale positively correlates with cognitive diagnosis performance
- **Mechanism:** Larger parameter capacity enables more robust interpretation of implicit student reasoning patterns, supporting better evidence identification and skill inference.
- **Core assumption:** Scale improvements transfer meaningfully to pedagogical inference tasks, not just problem-solving.
- **Evidence anchors:**
  - [abstract] "model size positively correlates with the diagnosis performance (r_s=.771)"
  - [section] Table 2 shows Large Models (e.g., GPT-4o, Gemini-1.5-Pro) outperforming Small Models (e.g., GPT-4o-mini, Llama-3.1-8B) with F1 gaps of ~0.08-0.10
  - [corpus] Related work on LLM cognitive diagnosis (Knowledge is Power: Harnessing LLMs for Enhanced Cognitive Diagnosis) corroborates that LLMs can enhance diagnosis but struggles with sparse data
- **Break condition:** Scale benefits plateau when evidential ambiguity is high (vague evidence cases); smaller models hallucinate more (48.78% hallucination rate vs. larger models)

### Mechanism 2: Evidential Strength Dependency
- **Claim:** Diagnosis performance depends on evidential strength in student responses
- **Mechanism:** Explicit, structured evidence (formulas, clear steps) allows models to pattern-match against diagnostic rubrics; vague or omitted reasoning forces inference beyond observed data.
- **Core assumption:** Models lack robust abductive reasoning to fill evidence gaps reliably.
- **Evidence anchors:**
  - [abstract] "performance was strongly dependent on evidential strength"
  - [section] Figure 4 shows ∆F1 = .51 and ∆Acc = .41 between Evident vs Vague conditions (t=64.60, p<.001)
  - [corpus] Limited corpus evidence on evidential strength specifically; related benchmarks focus on problem-solving, not diagnosis
- **Break condition:** When evidence is vague, models over-attribute (OverAttr M=.580) and over-infer (33.5% of errors), leading to confident but incorrect diagnoses

### Mechanism 3: Reasoning Architecture Risks
- **Claim:** Reasoning-oriented architectures increase over-inference risk
- **Mechanism:** Extended chain-of-thought encourages models to generate plausible reasoning chains that exceed what student work actually shows.
- **Core assumption:** More reasoning steps amplify extrapolation beyond evidence.
- **Evidence anchors:**
  - [abstract] "models over-attributing evidence when it was vague and frequently producing unsupported explanations"
  - [section] Figure 5 shows Reasoning Models commit 46.61% over-inference errors vs. 33.45% overall average
  - [corpus] Related work on reasoning curricula (Reasoning Curriculum: Bootstrapping Broad LLM Reasoning from Math) focuses on eliciting reasoning but does not address diagnostic restraint
- **Break condition:** Reasoning models still underperform (F1 < 0.5) and show highest OverAttr (.767 for DeepSeek-R1) and FalseAttr (.862)

## Foundational Learning

- **Concept:** TIMSS Cognitive Framework (Knowing, Applying, Reasoning domains)
  - **Why needed here:** Diagnosis requires mapping student work to cognitive skill categories; the paper grounds checklists in TIMSS (15 skills across 3 domains)
  - **Quick check question:** Can you distinguish "Recall" (Knowing) from "Determine" (Applying) when reading a student solution?

- **Concept:** Evidential Strength (Evident vs Vague)
  - **Why needed here:** Core finding that diagnosis fails under vague evidence; systems must distinguish clear proof from implicit hints
  - **Quick check question:** Given a partial student solution with erased steps, would you label the evidence for "implemented strategy correctly" as Evident or Vague?

- **Concept:** Error Attribution Metrics (OverAttr, FalseAttr)
  - **Why needed here:** OverAttr captures overstating evidence; FalseAttr captures asserting strong evidence for wrong diagnoses—both critical for safety
  - **Quick check question:** If a model outputs "Evident Yes" but ground truth is "Vague No," which error type does this represent?

## Architecture Onboarding

- **Component map:** OCR transcription -> LaTeX formulas -> Optional student work images -> Diagnosis Engine (LLM with CoT) -> Evidence Attribution Module -> Error Detection Layer -> Human-in-the-Loop Interface

- **Critical path:**
  1. Pre-filter responses by evidential richness (warn if >50% of check items would rely on vague evidence)
  2. Run diagnosis with explicit evidence extraction (force quote from student work)
  3. Check OverAttr/FalseAttr thresholds; defer to teacher if exceeded
  4. Return diagnosis with confidence calibration

- **Design tradeoffs:**
  - Multimodal input: +accuracy (~0.04), inconsistent F1 improvement; adds latency and OCR noise
  - Few-shot prompting: -over-attribution risk, but no F1 gain; better for safety than performance
  - Reasoning models: Higher F1 on some tasks, but highest over-inference; use with output constraints
  - Model size: Better performance, higher deployment cost; small models unsuitable for vague evidence cases

- **Failure signatures:**
  - Vague evidence input + "Evident" output → high OverAttr risk
  - Incorrect diagnosis + "Evident" attribution → FalseAttr (correlated r_s=.837 with OverAttr)
  - Over-inference: Model invents reasoning steps student never wrote (33.5% of errors)
  - Hallucination: Model references non-existent formulas or steps (19.8% overall; 48.78% in small models)

- **First 3 experiments:**
  1. Baseline calibration: Run 3 model sizes (small/medium/large) on held-out MathCog subset; measure F1, OverAttr, FalseAttr; confirm scale-evidence interaction
  2. Vague evidence detection: Build classifier to predict evidential strength before diagnosis; route vague cases to teacher review or structured elicitation prompts
  3. Output constraint ablation: Compare free-form CoT vs. forced-evidence-quoting prompts on OverAttr reduction; target <30% over-attribution on vague cases

## Open Questions the Paper Calls Out

- **Open Question 1:** Can interactive problem-solving interfaces that explicitly prompt students for intermediate steps and justifications improve LLM diagnostic accuracy by reducing vague evidence?
  - **Basis in paper:** [explicit] The Discussion section suggests "designing problem-solving tasks and interfaces that better elicit students' reasoning processes" to address the models' heavy reliance on explicit evidence.
  - **Why unresolved:** The current study evaluated static, post-hoc handwritten work; it did not test dynamic interfaces designed to elicit richer cognitive evidence.
  - **What evidence would resolve it:** An experiment comparing LLM diagnostic performance on student data collected from standard interfaces versus data from "evidence-eliciting" interfaces.

- **Open Question 2:** Does integrating teacher verification at specific intermediate stages (e.g., evidence identification vs. rubric interpretation) effectively mitigate cascading diagnostic errors?
  - **Basis in paper:** [explicit] The Discussion highlights the need for "teacher-in-the-loop approaches" to scrutinize specific failure modes like evidence misidentification and over-inference.
  - **Why unresolved:** The paper identifies distinct error types but does not implement or evaluate intervention mechanisms where teachers validate model reasoning before the final verdict.
  - **What evidence would resolve it:** A user study measuring the reduction in hallucination and over-attribution errors when teachers verify the "evidence extraction" phase of the LLM pipeline.

- **Open Question 3:** How do LLMs perform in diagnosing "Reasoning" cognitive skills (e.g., justifying, generalizing) compared to the "Knowing" and "Applying" skills assessed in MathCog?
  - **Basis in paper:** [explicit] The Future Work section notes that "Reasoning-oriented problems... are currently underrepresented in MathCog" because the dataset problems primarily focus on calculation and application.
  - **Why unresolved:** The current benchmark was deliberately scoped to exclude "Reasoning" skills due to the nature of the selected math problems, leaving this domain untested.
  - **What evidence would resolve it:** An extension of the MathCog dataset including problems requiring justification, evaluated on the same suite of 18 LLMs.

## Limitations

- **Dataset Construction and Translation Artifacts:** The MathCog benchmark relies on Korean educational data translated to English via Google Translate and "manually verified," potentially introducing semantic drift for nuanced cognitive skill descriptors.
- **Ground Truth Reliability:** Teacher annotations form the sole ground truth, but inter-rater reliability statistics are not reported, which could introduce systematic bias affecting model evaluation.
- **Evidential Strength Subjectivity:** The binary distinction between Evident and Vague evidence lacks formal definition or inter-annotator agreement metrics, making this categorization highly subjective.

## Confidence

- **High Confidence:** Model scale positively correlates with diagnosis performance (r_s=.771). This relationship is consistently observed across multiple model families and evaluation metrics.
- **Medium Confidence:** Reasoning models show higher over-inference rates. While the directional finding is clear, the absolute magnitude depends heavily on prompt design and output constraints.
- **Low Confidence:** Claims about multimodal input benefits and few-shot prompting effects. These findings are based on limited comparisons and lack systematic ablation studies.

## Next Checks

1. **Translation Validation Study:** Conduct blind comparison of original Korean teacher annotations against translated English versions with back-translation verification. Measure semantic drift in cognitive skill descriptors and evidence strength judgments.

2. **Evidential Strength Inter-Rater Reliability:** Recruit 10-15 teachers to independently annotate a 100-response subset of MathCog. Calculate Cohen's kappa for Evident/Vague judgments and test whether model performance correlates more strongly with ground truth or rater consensus.

3. **Cross-Context Generalization Test:** Apply the best-performing MathCog models to a comparable math diagnosis dataset from a different educational system (e.g., US Common Core assessments). Measure performance drop and error pattern shifts to assess whether findings generalize beyond the Korean TIMSS context.