---
ver: rpa2
title: A Floating Normalization Scheme for Deep Learning-Based Custom-Range Parameter
  Extraction in BSIM-CMG Compact Models
arxiv_id: '2501.15190'
source_url: https://arxiv.org/abs/2501.15190
tags:
- parameter
- extraction
- parameters
- values
- range
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a floating normalization scheme for deep learning-based
  extraction of BSIM-CMG compact model parameters from Cgg-Vg and Id-Vg measurements.
  Unlike traditional fixed-range normalization, this approach uses dynamically generated
  local min-max values for each parameter instance, enabling user-defined extraction
  ranges and greater flexibility.
---

# A Floating Normalization Scheme for Deep Learning-Based Custom-Range Parameter Extraction in BSIM-CMG Compact Models

## Quick Facts
- arXiv ID: 2501.15190
- Source URL: https://arxiv.org/abs/2501.15190
- Reference count: 0
- Introduces floating normalization scheme for BSIM-CMG parameter extraction from Cgg-Vg and Id-Vg measurements

## Executive Summary
This paper presents a novel floating normalization scheme for deep learning-based extraction of BSIM-CMG compact model parameters. Unlike traditional fixed-range normalization methods, the proposed approach uses dynamically generated local min-max values for each parameter instance, enabling user-defined extraction ranges and greater flexibility. The methodology employs a cascaded forward-inverse ANN architecture where both networks accept local min-max pairs as additional inputs, allowing fine-tuned control over parameter ranges and even fixing specific parameters to exact values. Experimental validation on a TCAD-calibrated 14 nm FinFET process demonstrates high accuracy with RMSE values of ~2.38% for Cgg-Vg and ~3.47% for Id-Vg parameter extraction.

## Method Summary
The methodology introduces a floating normalization scheme that dynamically generates local min-max values for each parameter instance during extraction, replacing traditional fixed-range normalization. The approach uses a cascaded forward-inverse ANN architecture where both networks receive the local min-max pairs as additional inputs. This design enables user-defined extraction ranges and provides the capability to fix specific parameters to exact values. The forward network predicts parameters from measurements, while the inverse network can be used for validation and refinement. The dynamic normalization approach allows for greater flexibility in parameter extraction across different operating conditions and device characteristics.

## Key Results
- Achieved RMSE of ~2.38% for Cgg-Vg parameter extraction on 14nm FinFET process
- Achieved RMSE of ~3.47% for Id-Vg parameter extraction on 14nm FinFET process
- Demonstrated comparable accuracy to fixed-range models while providing enhanced user control for custom parameter ranges

## Why This Works (Mechanism)
The floating normalization scheme works by replacing static normalization boundaries with dynamically calculated local min-max values for each parameter instance. This approach addresses the limitation of traditional fixed-range normalization, which can constrain parameter extraction accuracy when device characteristics fall outside predetermined ranges. By incorporating local min-max pairs as inputs to both forward and inverse ANN networks, the system can adapt to varying device behaviors and measurement conditions in real-time. The cascaded architecture allows for iterative refinement, where the inverse network can validate and correct predictions from the forward network, leading to improved overall accuracy. The dynamic nature of the normalization enables the model to handle parameter variations across different operating conditions without requiring extensive retraining.

## Foundational Learning
- **BSIM-CMG Compact Models**: Why needed - Standard industry models for FinFET devices; Quick check - Verify model equations match device physics
- **Forward-Inverse ANN Architecture**: Why needed - Enables bidirectional prediction and validation; Quick check - Confirm both networks converge during training
- **Dynamic Min-Max Normalization**: Why needed - Adapts to varying parameter ranges across devices; Quick check - Test normalization stability across different operating points
- **RMSE Evaluation Metrics**: Why needed - Quantifies extraction accuracy for model validation; Quick check - Compare against industry standard benchmarks
- **TCAD-Calibrated Process Models**: Why needed - Provides ground truth for validation; Quick check - Verify TCAD calibration against physical measurements
- **Cascaded Network Training**: Why needed - Improves prediction accuracy through iterative refinement; Quick check - Monitor training loss convergence for both networks

## Architecture Onboarding
- **Component Map**: Measurements -> Forward ANN (with local min-max) -> Parameter Predictions -> Inverse ANN (with local min-max) -> Refined Predictions
- **Critical Path**: Measurement input → Forward network prediction → Local min-max normalization → Parameter extraction → Inverse network validation
- **Design Tradeoffs**: Flexibility vs computational complexity, dynamic adaptation vs training stability, user control vs automation
- **Failure Signatures**: Training divergence in cascaded networks, normalization instability with extreme parameter values, reduced accuracy for parameters with narrow operating ranges
- **First 3 Experiments**:
  1. Test dynamic normalization stability across different measurement noise levels
  2. Validate parameter fixing capability by constraining specific parameters to known values
  3. Compare extraction accuracy between floating and fixed normalization on benchmark datasets

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Validation limited to single TCAD-calibrated 14 nm FinFET process, raising generalization concerns
- RMSE accuracy claims lack comparison with established parameter extraction methodologies
- Cascaded ANN complexity may affect practical implementation stability and training convergence

## Confidence
- Accuracy improvements: Medium
- Generalization to other technologies: Low
- Practical implementation stability: