---
ver: rpa2
title: Reinforcement Learning Driven Generalizable Feature Representation for Cross-User
  Activity Recognition
arxiv_id: '2509.01031'
source_url: https://arxiv.org/abs/2509.01031
tags:
- temporal
- tprl-dg
- activity
- feature
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes TPRL-DG, a reinforcement learning framework
  for cross-user human activity recognition that learns user-invariant and temporally
  coherent features without target user labels. The core idea is to reformulate feature
  extraction as a sequential decision-making process driven by a Transformer-based
  autoregressive generator, optimized via a multi-objective reward function balancing
  class discrimination and user invariance.
---

# Reinforcement Learning Driven Generalizable Feature Representation for Cross-User Activity Recognition

## Quick Facts
- arXiv ID: 2509.01031
- Source URL: https://arxiv.org/abs/2509.01031
- Reference count: 30
- Outperforms state-of-the-art by 2.79% (DSADS) and 2.20% (PAMAP2)

## Executive Summary
This paper proposes TPRL-DG, a reinforcement learning framework for cross-user human activity recognition that learns user-invariant and temporally coherent features without target user labels. The core idea is to reformulate feature extraction as a sequential decision-making process driven by a Transformer-based autoregressive generator, optimized via a multi-objective reward function balancing class discrimination and user invariance. Experiments on DSADS and PAMAP2 datasets show TPRL-DG outperforms state-of-the-art methods, achieving 88.29% average accuracy on DSADS (2.79% gain over best baseline) and 74.15% on PAMAP2 (2.20% gain).

## Method Summary
TPRL-DG reformulates feature extraction as a sequential decision-making process using reinforcement learning. A Transformer-based autoregressive generator produces feature tokens conditioned on previous tokens, preserving temporal coherence. The RL policy is trained with PPO using a dual reward function that balances class discrimination (inter-class centroid separation) and user invariance (intra-user variance minimization and inter-user centroid alignment). The framework operates in a label-free domain generalization setting, requiring no target user data during training.

## Key Results
- Achieves 88.29% average accuracy on DSADS dataset (2.79% improvement over best baseline)
- Achieves 74.15% average accuracy on PAMAP2 dataset (2.20% improvement over best baseline)
- Demonstrates superior cross-user generalization through user-invariant feature learning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Reformulating feature extraction as sequential decision-making enables discovery of user-invariant representations that static loss functions miss.
- **Mechanism:** RL policy πθ(zi,j | xi, zi,1:j−1) actively explores feature configurations through stochastic sampling, optimized via PPO with multi-objective rewards (Rcls for class separation, Rinv for user alignment). Unlike cross-entropy which passively minimizes local classification error, RL's reward-driven exploration can discover representations encoding shared temporal dynamics while explicitly penalizing user-specific artifacts.
- **Core assumption:** User-invariant features exist in an exploratory region of feature space that supervised gradients rarely reach when optimizing source-domain accuracy alone.
- **Evidence anchors:** Abstract mentions RL-driven feature extraction with multi-objective rewards; Section 3.2 discusses RL's dynamic reward-based optimization; neighbor work uses adversarial alignment but relies on gradient-based optimization.

### Mechanism 2
- **Claim:** Autoregressive token generation with causal masking preserves temporal coherence that parallel encoding disrupts.
- **Mechanism:** Each token zi,j ∼ N(µi,j, e^log σi,j) is conditioned on prior tokens zi,1:j−1 via masked self-attention, ensuring features evolve incrementally to reflect activity progression. The Transformer's global attention captures long-range dependencies while autoregression enforces sequential dependency, preventing temporal fragmentation common in non-causal architectures.
- **Core assumption:** Temporal coherence is necessary for cross-user generalization because shared activity dynamics manifest as sequential patterns rather than isolated features.
- **Evidence anchors:** Abstract mentions autoregressive tokenization for temporal coherence; Section 3.3 describes causal structure enabling progressive representation evolution; corpus lacks direct support for autoregressive temporal modeling.

### Mechanism 3
- **Claim:** The dual-reward formulation (Rcls + Rinv) explicitly trades off discriminability against invariance without requiring target-domain labels.
- **Mechanism:** Rcls maximizes inter-class centroid distances while Rinv minimizes intra-user variance and aligns user centroids for each class. The RL objective J(θ) = E[wcls·Rcls + winv·Rinv] allows dynamic balancing through weight tuning, enabling generalization without adversarial discriminators that often suffer training instability.
- **Core assumption:** User-invariant features for the same activity class exist and can be aligned without destroying class-discriminative information.
- **Evidence anchors:** Section 3.4 discusses dual objective forcing focus on shared temporal patterns; Section 4.4 shows hyperparameter analysis with accuracy increases; neighbor work uses adversarial methods but lacks direct comparison to reward-based invariance.

## Foundational Learning

- **Concept: Proximal Policy Optimization (PPO)**
  - **Why needed here:** The paper uses PPO as the RL optimizer for training the autoregressive generator. Understanding clipping, advantage estimation, and policy stability is essential for debugging training dynamics.
  - **Quick check question:** Can you explain why PPO's clipping mechanism (1-ε, 1+ε) prevents destructive policy updates compared to vanilla policy gradient?

- **Concept: Transformer self-attention with causal masking**
  - **Why needed here:** The feature generator uses masked multi-head attention to enforce autoregressive dependency. Understanding query-key-value computation and how masking prevents attending to future positions is critical for implementing the decoder correctly.
  - **Quick check question:** Given a sequence of length L, what is the shape of the causal mask, and what values does it contain to prevent information leakage?

- **Concept: Domain generalization vs. domain adaptation**
  - **Why needed here:** TPRL-DG targets domain generalization (no target data during training), distinguishing it from domain adaptation methods that require target-domain access. This framing justifies the label-free reward design.
  - **Quick check question:** Why would a method requiring target-domain labels (even unlabeled) be impractical for continuous-user-stream scenarios in real-world HAR deployments?

## Architecture Onboarding

- **Component map:** Sensor sequences X → Transformer encoder → hidden representations h → Autoregressive Transformer decoder → token parameters [µ, log σ] → feature tokens z → Rewards (Rcls, Rinv) → PPO optimizer → flattened features → Logistic regression classifier

- **Critical path:**
  1. Implement encoder-decoder with correct causal masking (bug-prone: verify no future information leaks)
  2. Implement PPO training loop with reward computation from feature statistics
  3. Tune wcls/winv balance on validation split before final testing

- **Design tradeoffs:**
  - Fewer tokens (s=5-10) → better generalization, lower compute; more tokens → risk of noise overfitting
  - Higher winv → better cross-user transfer, risk of class boundary blurring
  - Autoregressive generation → temporal coherence, slower inference than parallel

- **Failure signatures:**
  - High reward but low validation accuracy → reward hacking, add regularization
  - Class-specific collapse (µc ≈ µc') → Rinv too dominant, reduce winv
  - High variance across leave-one-group-out folds → policy instability, reduce learning rate or increase PPO clipping

- **First 3 experiments:**
  1. **Baseline sanity check:** Train ERM (standard cross-entropy) on DSADS leave-one-group-out to reproduce paper's 77.56% baseline; confirms data pipeline correctness.
  2. **Ablation on reward weights:** Fix s=5, sweep wcls ∈ {3.0, 5.0, 7.0} and winv ∈ {0.0, 0.5, 1.0} on PAMAP2 to replicate Figure 9-12 sensitivity curves; validates reward contribution.
  3. **Token length sensitivity:** Fix wcls=5.0, winv=0.5, sweep s ∈ {5, 10, 15, 20} on DSADS; should show peak at s=5 with degradation at higher values per Figure 8.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can adaptive or self-tuning reward weighting strategies reduce the performance variance observed in TPRL-DG across different user transfer scenarios?
- **Basis in paper:** The conclusion states "investigating adaptive reward weighting strategies could improve stability in highly variable scenarios, reducing the observed variance in performance." The paper also notes higher standard deviations compared to baselines (2.33 vs 1.47 on DSADS, 4.20 vs 3.56 on PAMAP2).
- **Why unresolved:** The current framework uses fixed weights (wcls, winv) tuned on validation sets, which may not generalize optimally across diverse transfer tasks with varying user heterogeneity.
- **What evidence would resolve it:** Experiments comparing fixed versus adaptive reward weighting schemes, measuring variance reduction across leave-one-group-out folds while maintaining or improving average accuracy.

### Open Question 2
- **Question:** How can TPRL-DG be extended to handle multi-modal sensor data (e.g., combining IMU with physiological signals, GPS, or environmental sensors)?
- **Basis in paper:** The conclusion explicitly states future work will include "extending the framework to multi-modal sensor data to further enhance robustness."
- **Why unresolved:** The current framework only validates on IMU-based datasets (accelerometer and gyroscope), and the autoregressive token generation mechanism assumes homogeneous sensor modalities.
- **What evidence would resolve it:** Evaluation on multi-modal HAR datasets demonstrating that user-invariant temporal features can be learned across heterogeneous sensor streams with different sampling rates and characteristics.

### Open Question 3
- **Question:** What lightweight Transformer architectures can maintain TPRL-DG's generalization performance while enabling deployment on resource-constrained wearable devices?
- **Basis in paper:** The conclusion identifies "optimizing computational efficiency for resource-constrained wearable devices" and "exploring lightweight Transformer architectures" as future directions.
- **Why unresolved:** The Transformer-based autoregressive generator requires substantial computation for self-attention and sequential token generation, without analysis of latency, memory footprint, or energy consumption in the current work.
- **What evidence would resolve it:** Benchmarks of compressed or efficient Transformer variants (e.g., pruning, distillation, linear attention) on wearable hardware, reporting accuracy-efficiency trade-offs.

## Limitations

- Modest performance improvements (2.79% and 2.20%) suggest incremental rather than transformative gains over baselines
- High variance across transfer tasks (±4.20% on PAMAP2) indicates training instability or sensitivity to user heterogeneity
- Lack of ablation studies comparing RL-based feature generation to supervised alternatives with explicit invariance constraints

## Confidence

- **High Confidence:** Experimental methodology (leave-one-group-out validation, per-user normalization, windowing parameters) is clearly specified and reproducible. Core architectural components (Transformer encoder-decoder with causal masking) are well-defined and implementable.
- **Medium Confidence:** PPO training procedure and reward computation are described in sufficient detail for reproduction, though exact hyperparameter values remain unspecified. Performance improvements over baselines are reported with appropriate statistical variation measures.
- **Low Confidence:** Theoretical justification for why RL-based feature generation outperforms supervised alternatives lacks rigorous empirical validation. Claim that RL's exploration uniquely discovers user-invariant features remains correlative rather than causative.

## Next Checks

1. **Reward Ablation:** Systematically disable Rcls or Rinv individually to quantify each component's contribution to cross-user generalization, verifying the dual-reward formulation is necessary rather than redundant.
2. **Feature Space Analysis:** Visualize feature distributions using t-SNE or UMAP for source and target users to empirically demonstrate user-invariant clustering without target labels, validating the invariance mechanism.
3. **Generalization to Other Sensors:** Test TPRL-DG on non-IMU datasets (e.g., ECG, PPG) to assess whether the method's benefits extend beyond accelerometer-based HAR to different physiological signal domains.