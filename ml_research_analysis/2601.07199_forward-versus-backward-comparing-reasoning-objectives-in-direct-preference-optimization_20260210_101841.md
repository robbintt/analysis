---
ver: rpa2
title: 'Forward versus Backward: Comparing Reasoning Objectives in Direct Preference
  Optimization'
arxiv_id: '2601.07199'
source_url: https://arxiv.org/abs/2601.07199
tags:
- training
- reasoning
- verification
- preference
- forward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper investigates how different training objectives affect
  reasoning reliability in large language models. Two training approaches are compared:
  forward chain-of-thought generation (training models to produce correct reasoning
  traces) and backward verification (training models to verify and acknowledge errors
  in candidate solutions).'
---

# Forward versus Backward: Comparing Reasoning Objectives in Direct Preference Optimization

## Quick Facts
- arXiv ID: 2601.07199
- Source URL: https://arxiv.org/abs/2601.07199
- Authors: Murtaza Nikzad; Raghuram Ramanujan
- Reference count: 18
- Primary result: Forward-only DPO training achieves +3.5 percentage points accuracy improvement on GSM8K, while backward-only training substantially reduces false positive rate from 13.4% to 4.3%

## Executive Summary
This paper investigates how different training objectives affect reasoning reliability in large language models by comparing forward chain-of-thought generation and backward verification training approaches. Experiments on GSM8K show that forward-only Direct Preference Optimization training achieves the highest accuracy improvement (+3.5 percentage points), while backward-only training substantially reduces false positive rate with minimal accuracy gains. Both training variants reduce acknowledgement rate compared to the baseline, indicating that preference optimization increases model confidence in outputs. The findings suggest forward and backward reasoning objectives provide distinct learning signals: forward training improves problem-solving capability while backward training improves verification calibration.

## Method Summary
The authors compare two training approaches using Direct Preference Optimization (DPO) on LLaMA 3.1 8B-Instruct: forward chain-of-thought generation (training models to produce correct reasoning traces) and backward verification (training models to verify and acknowledge errors in candidate solutions). Forward training uses preference pairs of correct versus incorrect reasoning traces, while backward training uses preference pairs of correct versus incorrect verification verdicts. The complete training and evaluation pipeline is released for reproducibility, using rejection sampling to generate realistic negative examples and weighted preference pairs to emphasize informative training signals.

## Key Results
- Forward-only DPO training achieves highest accuracy improvement: 83.1% to 86.6% (+3.5 percentage points)
- Backward-only training reduces false positive rate from 13.4% to 4.3% with minimal accuracy gains (+0.5 pp)
- Both training variants reduce acknowledgement rate compared to baseline (67.8% → 44.7% and 46.3%)
- Forward-only training provides superior problem-solving capability while backward-only training improves verification calibration

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Forward-only DPO improves problem-solving accuracy by shifting the policy toward correct reasoning traces.
- Mechanism: The model learns to prefer complete reasoning chains that lead to correct answers over those that lead to incorrect answers. Rejection sampling provides realistic negative examples (actual failed traces rather than synthetic errors), and the preference weight boost (α=1.2) emphasizes these informative training signals.
- Core assumption: The preference signal from correct/incorrect trace pairs generalizes to unseen problems within the same domain.
- Evidence anchors:
  - [abstract]: "Forward-only DPO training achieves the highest accuracy improvement, increasing from 83.1% to 86.6% (+3.5 percentage points)"
  - [section 5.1]: "Forward-only DPO training achieves the highest accuracy at 86.6%, representing a 3.5 percentage point improvement over the baseline."
  - [corpus]: Related work MDPO confirms multi-granularity preference optimization improves mathematical reasoning.

### Mechanism 2
- Claim: Backward-only DPO improves verification calibration (reducing false positive rate) without transferring to generation capability.
- Mechanism: Training on verification pairs (problem + candidate answer → PASS/FAIL verdict) teaches the model to evaluate solution correctness independently. The lower inference temperature (τb=0.3) for verification encourages conservative, consistent judgments.
- Core assumption: Verification and generation are separable skills that do not automatically transfer.
- Evidence anchors:
  - [abstract]: "backward-only training yields minimal accuracy gains but substantially reduces the false positive rate from 13.4% to 4.3%"
  - [section 5.1]: "Training exclusively on backward verification produces minimal impact on forward reasoning performance, confirming that backward training signal does not transfer directly to problem-solving capability."
  - [corpus]: Weak corpus signal; related work on verification models (Cobbe et al., Lightman et al.) focuses on separate verifier models rather than unified policy training.

### Mechanism 3
- Claim: Preference optimization increases model confidence, paradoxically reducing error acknowledgement rate.
- Mechanism: DPO explicitly trains the model to prefer chosen outputs over rejected alternatives. This signal appears to generalize beyond the training distribution, reducing the model's tendency to express uncertainty—even when incorrect. The paper notes this effect occurs in both forward-only and backward-only training.
- Core assumption: The confidence increase is a property of preference optimization itself, not an artifact of the specific hyperparameter configuration.
- Evidence anchors:
  - [abstract]: "Notably, both training variants reduce acknowledgement rate compared to the baseline, suggesting that preference optimization increases model confidence in its outputs."
  - [section 6.1]: "The baseline model, without any preference optimization, correctly flags 67.8% of its errors... After training, both forward-only and backward-only models flag fewer errors (44.7% and 46.3% respectively)."
  - [corpus]: No direct corpus evidence on this phenomenon; warrants further investigation.

## Foundational Learning

- Concept: **Direct Preference Optimization (DPO)**
  - Why needed here: The entire training framework uses DPO to optimize the policy on preference pairs without a separate reward model.
  - Quick check question: Can you explain why DPO avoids learning an explicit reward function compared to RLHF?

- Concept: **Chain-of-Thought Reasoning**
  - Why needed here: Forward training optimizes preference for correct reasoning traces; understanding CoT structure is essential for data generation and evaluation.
  - Quick check question: What makes a chain-of-thought trace "correct" vs "incorrect" in this framework?

- Concept: **Calibration and False Positive Rate**
  - Why needed here: The paper introduces acknowledgement rate and FPR as key metrics; understanding these is critical for interpreting the trade-offs between forward and backward training.
  - Quick check question: If a verifier always outputs PASS, what would its FPR be? What would its acknowledgement rate be?

## Architecture Onboarding

- Component map:
  Base model: LLaMA 3.1 8B-Instruct -> Adapter: LoRA modules on attention projections (Wq, Wk, Wv, Wo), rank=16, α=32 -> Training: DPO on forward or backward preference pairs -> Inference: Two-stage (forward generation at τ=0.7, backward verification at τ=0.3)

- Critical path:
  1. Generate forward traces with rejection sampling until both correct and incorrect solutions obtained per problem
  2. Construct preference pairs: forward pairs (correct vs incorrect traces) or backward pairs (correct vs incorrect verdicts)
  3. Train with weighted DPO (boost real negatives by α=1.2)
  4. Evaluate on accuracy, acknowledgement rate, FPR, and calibration F1

- Design tradeoffs:
  - Forward-only: Maximizes accuracy (+3.5 pp) but higher FPR (10.2%) and lower acknowledgement (44.7%)
  - Backward-only: Minimizes FPR (4.3%) but minimal accuracy gain (+0.5 pp) and lower acknowledgement (46.3%)
  - Both reduce acknowledgement vs baseline (67.8%), indicating a confidence-accuracy trade-off

- Failure signatures:
  - Low acknowledgement rate after training: Model fails to recognize its own errors
  - High FPR: Verifier incorrectly rejects correct solutions
  - No accuracy improvement from backward training: Verification skill doesn't transfer to generation

- First 3 experiments:
  1. Reproduce baseline vs forward-only vs backward-only on GSM8K subset to validate pipeline
  2. Ablate preference weight boost (α) to test sensitivity to real negative emphasis
  3. Combine forward and backward training (hybrid objective) to explore complementary benefits

## Open Questions the Paper Calls Out
None

## Limitations
- Narrow evaluation scope limited to GSM8K, preventing generalization to other reasoning domains
- Training data generation relies heavily on rejection sampling, potentially introducing selection bias
- Confidence-accuracy trade-off lacks mechanistic explanation and may be hyperparameter-dependent
- Backward-only training's minimal accuracy gains suggest fundamental separation between verification and generation capabilities

## Confidence
- Forward-only DPO accuracy improvement (+3.5 pp): **High confidence**
- Backward-only verification calibration (FPR reduction 13.4%→4.3%): **High confidence**
- Preference optimization increases model confidence (reduced acknowledgement): **Medium confidence**
- Forward/backward training signals are complementary: **Medium confidence**
- Forward/backward objectives do not transfer: **Low confidence**

## Next Checks
1. **Cross-domain generalization test**: Evaluate the same forward/backward training pipeline on MATH and other reasoning datasets to determine whether the observed trade-offs hold across mathematical complexity levels and problem types.

2. **Combined training ablation**: Implement and evaluate hybrid training that mixes forward and backward preference pairs to determine whether the distinct benefits can be achieved simultaneously or whether they represent fundamental trade-offs.

3. **Confidence-accuracy mechanistic study**: Design controlled experiments to isolate whether the reduced acknowledgement rate is caused by DPO's preference signal itself or by specific training hyperparameters (temperature, weight boosting, rejection sampling strategy), using synthetic preference distributions to test causal relationships.