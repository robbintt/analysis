---
ver: rpa2
title: 'GECOBench: A Gender-Controlled Text Dataset and Benchmark for Quantifying
  Biases in Explanations'
arxiv_id: '2406.11547'
source_url: https://arxiv.org/abs/2406.11547
tags:
- feature
- methods
- gender
- bias
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GECOBench, a framework for quantifying biases
  in explanations generated by post-hoc feature attribution methods for NLP models.
  The authors created GECO, a gender-controlled dataset with sentences in male, female,
  and non-binary variants, enabling objective evaluation of explanation correctness.
---

# GECOBench: A Gender-Controlled Text Dataset and Benchmark for Quantifying Biases in Explanations

## Quick Facts
- **arXiv ID:** 2406.11547
- **Source URL:** https://arxiv.org/abs/2406.11547
- **Reference count:** 35
- **Primary result:** Introduced GECOBench framework showing fine-tuning embedding layers mitigates gender bias in NLP explanations, with IG and Gradient SHAP performing best at identifying gender-relevant features.

## Executive Summary
This paper introduces GECOBench, a framework for quantifying biases in explanations generated by post-hoc feature attribution methods for NLP models. The authors created GECO, a gender-controlled dataset with sentences in male, female, and non-binary variants, enabling objective evaluation of explanation correctness. Using BERT fine-tuned to different degrees, they found that Integrated Gradients and Gradient SHAP perform best in identifying gender-relevant features. Crucially, they showed that gender bias in pre-trained models leads to residual asymmetries in feature attributions, and fine-tuning or retraining embedding layers significantly mitigates this bias, improving explanation correctness. The framework provides a principled approach to assess both the correctness and bias of feature attributions in language models.

## Method Summary
The study constructs GECO, a gender-controlled text dataset created by manually altering Wikipedia sentences to produce male, female, and non-binary variants. The dataset provides ground truth feature attributions by systematically manipulating gender-related words while keeping other content constant. BERT models are trained using five different paradigms that vary which layers are fine-tuned or frozen. Post-hoc XAI methods (Integrated Gradients, Gradient SHAP, LIME, Kernel SHAP, Saliency) are applied to these models, and their outputs are evaluated against the ground truth using Mass Accuracy (MA) metric, which measures the proportion of importance assigned to ground-truth tokens. The framework enables objective assessment of explanation correctness and bias quantification.

## Key Results
- Fine-tuning or retraining embedding layers significantly improves explanation correctness compared to frozen embeddings
- Integrated Gradients and Gradient SHAP perform best at identifying gender-relevant features with highest Mass Accuracy scores
- Gender bias in pre-trained models leads to residual asymmetries in feature attributions even after fine-tuning on balanced datasets
- Models with fine-tuned embeddings (BERT-CEf) achieve higher Mass Accuracy than those with frozen embeddings (BERT-C)
- The GECO dataset successfully controls for gender bias while maintaining task performance above 80% accuracy

## Why This Works (Mechanism)

### Mechanism 1: Ground Truth Feature Attribution via Controlled Text Manipulation
Creating controlled sentence variants with minimal, systematic gender-based alterations provides objective, word-level ground truth for evaluating feature attribution correctness. The GECO dataset is constructed from Wikipedia sentences which are manually altered to create three variants: male, female, and non-binary. This process ensures that only the altered words (e.g., pronouns like "he"/"she") have a statistical association with the classification target. XAI methods are evaluated on their ability to assign high importance to these ground-truth tokens and low importance to all others, quantified by the Mass Accuracy (MA) metric.

### Mechanism 2: Residual Bias Propagation from Pre-training Embeddings
Biases encoded in the embeddings of pre-trained language models persist after fine-tuning and manifest as systematic errors in feature attribution methods. Pre-trained models like BERT encode gender stereotypes from their training corpora in their weights. When fine-tuned on the balanced GECO dataset, the model achieves high accuracy. However, post-hoc XAI methods may still attribute importance to non-ground-truth words that had strong associations in the pre-training data (e.g., "kitchen" for female), creating an "explanation bias" quantifiable as a deviation from the ground truth.

### Mechanism 3: Mitigation of Explanation Bias via Targeted Layer Re-training
Explanation correctness is strongly dependent on the choice of layers that are fine-tuned or re-trained, with updates to the embedding layer being particularly effective for bias mitigation. This works by directly modifying the weights responsible for token representation. When the embedding layer is fine-tuned (BERT-CEf) or re-trained (BERT-CE) on the unbiased GECO dataset, stereotypical associations are overwritten with the task-specific, unbiased associations. This forces the model to rely on ground-truth tokens, which in turn causes feature attribution methods to assign them higher importance, improving Mass Accuracy.

## Foundational Learning

**Post-hoc Feature Attribution in NLP** - Why needed: This is the object of study. You must understand methods like Integrated Gradients and SHAP, which take a trained model and an input to produce a list of importance scores for each token. Quick check: Can a feature attribution method be correct even if it highlights a token the model did not use for its decision?

**Transfer Learning / Fine-tuning in Transformers** - Why needed: The paper's intervention is based on how different parts of a pre-trained model are adapted. Understanding the difference between freezing layers and fine-tuning is essential. Quick check: In a standard BERT fine-tuning pipeline, which layers are typically updated?

**The Model-Centric vs. Data-Centric Tension** - Why needed: The paper adopts a data-centric view of correctness (SAP). Understanding that an explanation can be faithful to the model (showing what it used) but unfaithful to the data (showing what is statistically relevant) is the core conceptual conflict. Quick check: If a model uses a spurious correlation to make a correct prediction, should a faithful explanation highlight that correlation?

## Architecture Onboarding

**Component map:** Data Generation -> Model Adaptation -> Explanation & Evaluation -> Metric Computation
- **Data Generation:** Source sentences are processed and manually altered to create male/female/non-binary variants with ground truth masks
- **Model Adaptation:** A pre-trained model is adapted using one of the defined paradigms (e.g., BERT-C, BERT-CEf). The critical choice is which layers to update
- **Explanation & Evaluation:** An XAI method is applied to the adapted model on test sentences. The output is word-level importance scores, which are compared to the ground truth masks using the Mass Accuracy metric

**Critical path:**
1. Data Generation: Source sentences are processed and manually altered to create male/female/non-binary variants with ground truth masks
2. Model Adaptation: A pre-trained model is adapted using one of the defined paradigms (e.g., BERT-C, BERT-CEf). The critical choice is which layers to update
3. Explanation & Evaluation: An XAI method is applied to the adapted model on test sentences. The output is word-level importance scores, which are compared to the ground truth masks using the Mass Accuracy metric

**Design tradeoffs:**
- Dataset Simplicity vs. Realism: GECO is a controlled, synthetic benchmark. It allows for objective evaluation but may not reflect the complexity of real-world NLP tasks like sentiment analysis or toxicity detection
- Evaluation Metric (MA): Mass Accuracy penalizes false positives and is forgiving of false negatives. This aligns with a specific notion of correctness but may not suit all explanation goals
- Scope of Bias: The study focuses on grammatical gender bias as a proxy. It does not claim to capture all forms of social bias

**Failure signatures:**
- Low Mass Accuracy (near random baseline) suggests the XAI method fails to detect statistical associations or the model is not learning the intended task
- High classification accuracy but low Mass Accuracy indicates explanation bias; the model is likely using spurious features from pre-training
- High variance across training seeds suggests instability in either the model's internal representations or the XAI method

**First 3 experiments:**
1. Baseline Reproduction: Train BERT-C (frozen embeddings, fine-tuned classifier) on GECO and apply LIME. Measure its Mass Accuracy. The expected failure signature is high accuracy but low MA
2. Embedding Ablation: Train BERT-CEf (fine-tuned embeddings) and BERT-CEfAf (fine-tuned all) on the same data. Compare their Mass Accuracy to the BERT-C baseline. The goal is to confirm the primary finding: embedding updates improve explanation correctness
3. XAI Method Comparison: On a single, fine-tuned model (e.g., BERT-CEf), run Integrated Gradients, Gradient SHAP, and Saliency. Compare their MA scores to rank their data-centric correctness and compare against the "Pattern Variant" covariance baseline

## Open Questions the Paper Calls Out

**Generalization to Other Architectures:** Do the bias mitigation effects observed in BERT via embedding fine-tuning generalize to autoregressive or decoder-only architectures (e.g., GPT models)? The authors note the framework can be "extended to include more models" like GPT or XLNet but restrict experiments to BERT.

**Capturing Non-linear Feature Interactions:** How can explanation benchmarks evolve to capture non-linear feature interactions rather than relying solely on univariate statistical associations? The authors state a limitation: the "criterion of univariate statistical association... does not include nonlinear feature interactions that are present in many real-world applications."

**Extension to Discourse-level Tasks:** Can the GECO framework be adapted to evaluate explanation correctness in discourse-level tasks like co-reference resolution? The authors propose that "In Future research, co-reference resolution could be an immediate extension of GECO."

## Limitations

- The synthetic GECO dataset may not capture the complexity of real-world NLP tasks where gender bias manifests through subtle semantic cues rather than explicit pronouns
- The analysis focuses on grammatical gender bias as a proxy for broader social biases, which may not generalize to other protected attributes or intersectional bias patterns
- The controlled experimental design, while enabling objective evaluation, may not reflect the noisy and complex nature of real-world text data

## Confidence

**High Confidence:** The finding that fine-tuning or retraining embedding layers significantly improves explanation correctness has strong empirical support (Section 4, RQ2). The controlled experimental design with multiple training paradigms and consistent performance improvements across XAI methods provides robust evidence for this claim.

**Medium Confidence:** The assertion that pre-trained model biases manifest as systematic errors in feature attributions is supported but requires careful interpretation. While the study shows residual asymmetries, the causal link between pre-training biases and explanation errors is inferred rather than directly measured.

**Medium Confidence:** The ranking of XAI methods by data-centric correctness (IG and Gradient SHAP performing best) is based on controlled experiments but may not generalize. The evaluation framework measures statistical association detection, which favors methods designed for this purpose.

## Next Checks

1. **Cross-task generalization:** Apply GECOBench evaluation to BERT models fine-tuned on real-world tasks like sentiment analysis or coreference resolution. Measure whether explanation correctness on GECO correlates with explanation quality on these tasks using human evaluation or downstream task performance.

2. **Bias type extension:** Construct GECO variants that target different bias types (e.g., racial, occupational) using similar manipulation schemes. Test whether the observed relationship between embedding updates and explanation correctness holds across different social bias categories.

3. **Alternative explanation goals:** Design experiments that compare data-centric correctness (SAP) against model-centric faithfulness metrics. Train models with known spurious correlations and evaluate whether explanation methods that perform well on GECO also faithfully reveal model reasoning rather than just statistical associations.