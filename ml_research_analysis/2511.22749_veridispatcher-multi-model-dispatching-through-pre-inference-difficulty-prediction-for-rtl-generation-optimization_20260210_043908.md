---
ver: rpa2
title: 'VeriDispatcher: Multi-Model Dispatching through Pre-Inference Difficulty Prediction
  for RTL Generation Optimization'
arxiv_id: '2511.22749'
source_url: https://arxiv.org/abs/2511.22749
tags:
- uni00000014
- uni00000015
- uni00000013
- difficulty
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VeriDispatcher is a framework that improves multi-LLM RTL generation
  by routing each task to the most suitable model using pre-inference difficulty prediction.
  It trains lightweight classifiers per model on semantic embeddings of task descriptions,
  predicting whether each LLM can handle the task.
---

# VeriDispatcher: Multi-Model Dispatching through Pre-Inference Difficulty Prediction for RTL Generation Optimization

## Quick Facts
- arXiv ID: 2511.22749
- Source URL: https://arxiv.org/abs/2511.22749
- Reference count: 37
- Key outcome: Multi-LLM RTL generation routing improves accuracy up to 18.18% on RTLLM and reduces commercial API calls by up to 40%

## Executive Summary
VeriDispatcher addresses the challenge of selecting the most suitable LLM for RTL generation tasks without running all models. It trains lightweight per-model classifiers to predict task difficulty using semantic embeddings of task descriptions. By dispatching each task to the top-k models predicted to handle it successfully, VeriDispatcher achieves significant accuracy improvements over single-model baselines while reducing costly commercial API calls. The framework demonstrates that intelligent routing based on pre-inference difficulty prediction outperforms both single-model approaches and random selection.

## Method Summary
VeriDispatcher implements a per-model dispatching framework for RTL generation optimization. For each of 10 LLMs (5 open-source, 5 commercial), it trains a lightweight classifier to predict task difficulty using semantic embeddings. Open-source models use internal hidden states while commercial models use external embeddings as proxies. The framework generates 10 prompt variants per task through reverse engineering, produces 10 generations per variant per model, and computes difficulty scores from verification outcomes. Classifiers are trained on filtered and binarized difficulty labels, with temperature scaling applied for probability calibration. At inference, tasks are routed to top-k models with highest predicted success probability.

## Key Results
- Achieves 18.18% accuracy improvement on RTLLM and 5.77% on VerilogEval over single-model baselines
- Reduces commercial API calls by up to 40% on RTLLM and 75% on VerilogEval
- Top-3 routing recovers 85-94% of oracle performance using only 3 LLM calls per task
- External embeddings (OpenAI) achieve near-commercial model performance (0.69 Pass@1 vs 0.70 for GPT-5)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Task difficulty is model-dependent rather than task-intrinsic, enabling beneficial routing.
- **Mechanism:** Different LLMs have distinct capability boundaries due to training data and architecture differences. The same task can cluster as "easy" for one model and "hard" for another in embedding space. VeriDispatcher trains a separate classifier per model rather than a global predictor, capturing these idiosyncratic boundaries.
- **Core assumption:** Semantic embeddings encode enough information about task complexity to enable pre-inference difficulty estimation before code generation.
- **Evidence anchors:**
  - [Section 1] "t-SNE plots visualize semantic embeddings of the same task set for different LLMs, where easy and hard instances cluster differently per model, revealing that task difficulty is inherently model specific."
  - [Section 3.2] "Applying a threshold of 0.5, we create binary labels: tasks scoring above 0.5 are labeled hard, while those below 0.5 are easy. This model-specific labeling captures that difficulty varies across LLMs."
  - [Corpus] RTL++ and ScaleRTL papers similarly observe model-dependent performance variations but do not formalize routing; corpus lacks direct evidence for model-specific difficulty predictors.
- **Break condition:** If all models had correlated failure patterns on the same task types, per-model classifiers would provide no routing advantage over a single global predictor.

### Mechanism 2
- **Claim:** External semantic embeddings (e.g., text-embedding-3-large) can proxy internal model states for difficulty prediction, including for commercial APIs lacking hidden-state access.
- **Mechanism:** Commercial models expose no internal representations, but external embedding models provide general-purpose semantic features that correlate with task difficulty. VeriDispatcher uses these as input to classifiers trained on empirical difficulty labels derived from pass/fail outcomes.
- **Core assumption:** Task difficulty correlates with semantic properties that external embeddings capture, independent of any specific model's internal state.
- **Evidence anchors:**
  - [Section 3.1] "For commercial models (e.g., GPT-5 and Claude-4.5) where internal hidden states are inaccessible, we use external embedding models, text-embedding-3-large and Qwen3-Embedding-8B, as semantic proxies."
  - [Section 5.3] "On VerilogEval, our best strategy using a combined averaging scheme with OpenAI embeddings achieves a pass@1 score of 0.69, nearly matching GPT-5's 0.70."
  - [Corpus] No corpus papers directly validate external embeddings as difficulty proxies; this remains specific to VeriDispatcher's methodology.
- **Break condition:** If task difficulty were primarily determined by model-internal factors (e.g., attention head configurations, training corpus quirks) invisible to external embeddings, proxy-based prediction would fail for closed models.

### Mechanism 3
- **Claim:** Top-k dispatching with calibrated probability estimates approximates oracle performance (best-of-all-models) using a fraction of model calls.
- **Mechanism:** Rather than calling all models, VeriDispatcher ranks models by predicted success probability and dispatches to top-k. With k=3, it recovers 85-94% of oracle performance on VerilogEval while reducing API calls. Temperature scaling calibrates MLP outputs for reliable probability comparisons.
- **Core assumption:** Classifier confidence correlates with actual generation success; higher predicted probability implies genuinely better routing decisions.
- **Evidence anchors:**
  - [Section 5.4] "With OpenAI embeddings, Top-1 solves 102 cases (123 w/ commercial), Top-2 reaches 107 (131), and Top-3 hits 111 (133), recovering 85-94% of optimal (130/142) using just three LLM calls."
  - [Section 4.3] "As MLP achieves the best performance, we apply temperature scaling on the validation set to improve probability calibration for reliable easy/hard predictions."
  - [Corpus] DeepCircuitX and related work focus on generation quality rather than routing efficiency; corpus does not provide comparative routing baselines.
- **Break condition:** If classifiers were systematically miscalibrated (e.g., overconfident on hard tasks), higher k would not improve recovery rate, and Top-1 would approach random selection performance.

## Foundational Learning

- **Concept: Pass@k metric**
  - **Why needed here:** VeriDispatcher evaluates success using Pass@k (probability that at least one of k samples passes), the standard metric for code generation benchmarks. Understanding this is essential for interpreting Table 1 results.
  - **Quick check question:** If a model generates 10 samples for a task and 3 pass verification, what is its Pass@1 estimate (assuming unbiased sampling)?

- **Concept: Semantic embeddings vs. hidden states**
  - **Why needed here:** The paper distinguishes between internal model embeddings (hidden states from transformer layers) and external embeddings (from dedicated embedding models). This distinction determines what features are available for commercial vs. open-source models.
  - **Quick check question:** Why can VeriDispatcher extract "average pooling" embeddings from OriGen but must use "text-embedding-3-large" for GPT-5?

- **Concept: Calibration (temperature scaling)**
  - **Why needed here:** Classifier raw outputs may not reflect true probabilities. Temperature scaling adjusts the confidence distribution, which matters when ranking models for Top-k selection.
  - **Quick check question:** If a classifier outputs 0.9 confidence but only 60% of such predictions are correct, is the classifier over-confident or under-confident?

## Architecture Onboarding

- **Component map:**
  1. Data Augmentation Module: Generates N prompt variants per task using LLM-based reverse engineering from reference code
  2. Embedding Extractor: Produces semantic vectors (internal: last-token/average/decay; external: OpenAI/Qwen)
  3. Difficulty Labeler: Computes S_diff from 10 generations per variant → filters → binarizes at 0.5 threshold
  4. Per-Model Classifiers: MLP/XGB/LGBM trained on (embedding, difficulty_label) pairs for each of 10 LLMs
  5. Dispatcher: At inference, queries all classifiers, ranks by P(easy), selects top-k with optional commercial exclusion

- **Critical path:** Embedding quality → classifier F1-macro → dispatch accuracy → Pass@k improvement. The paper shows F1-macro correlates with semantic richness (RTLLM has 363.90 avg tokens vs. VerilogEval's 183.28), which cascades to larger dispatching gains on RTLLM.

- **Design tradeoffs:**
  - Internal vs. external embeddings: Internal captures model-specific biases; external provides cross-model consistency but may miss idiosyncratic signals
  - k selection: Higher k increases success rate but costs more API calls; paper shows diminishing returns after k=2 for RTLLM
  - Commercial inclusion: Including GPT-5/Claude improves accuracy but increases cost and may violate data confidentiality constraints

- **Failure signatures:**
  - Classifier F1 < 0.5: Embeddings lack discriminative signal; check task description richness or try alternative encoders
  - Top-k ≈ Random selection: Calibration failure; apply temperature scaling or switch to MLP from tree-based methods
  - Large gap between Top-1 and Top-3: Predictions are noisy; increase training data or apply data augmentation

- **First 3 experiments:**
  1. Baseline calibration: Train MLP classifiers for a single open-source model (e.g., OriGen) using average-pooled internal embeddings on RTLLM; report F1-macro on held-out tasks
  2. Embedding ablation: Compare last-token, average, and decay pooling on same model/dataset; identify which pooling strategy yields highest F1-macro
  3. End-to-end dispatch: Implement Top-3 routing across 3 models with and without commercial APIs; measure Pass@1/5/10 and commercial API call reduction vs. random selection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a hierarchical dispatch strategy, routing tasks from cheap to premium models based on successive failure, further optimize the cost-quality trade-off compared to the current flat Top-k selection?
- Basis in paper: [explicit] Section 6 states future work includes "hierarchical dispatch from cheap to premium models."
- Why unresolved: The current framework dispatches to a selected subset based on pre-inference prediction but does not implement a cascading retry mechanism upon generation failure.
- What evidence would resolve it: An evaluation comparing Top-k flat dispatch against a cascading strategy where tasks failing on open-source models are progressively escalated to commercial ones.

### Open Question 2
- Question: Does the difficulty-aware dispatching mechanism transfer effectively to other EDA tasks such as testbench generation and assertion synthesis?
- Basis in paper: [explicit] Section 6 lists "extending to other EDA tasks, e.g., testbench generation, assertion synthesis" as future work.
- Why unresolved: The current evaluation is restricted to RTL code generation using VerilogEval and RTLLM benchmarks.
- What evidence would resolve it: Application of VeriDispatcher to testbench or assertion datasets, measuring the correlation between predicted difficulty and functional coverage or assertion satisfaction rates.

### Open Question 3
- Question: Can end-to-end training of "difficulty-aware embeddings" improve classification accuracy over the current method of using fixed pre-trained embeddings?
- Basis in paper: [explicit] Section 6 identifies "developing difficulty-aware embeddings" as a future direction.
- Why unresolved: The framework currently uses static embeddings (e.g., OpenAI or average pooling) as input to classifiers without fine-tuning the representation layer for the specific difficulty prediction task.
- What evidence would resolve it: A comparison of F1-macro scores between the current approach and a setup where the embedding encoder is jointly optimized with the difficulty classifier.

## Limitations

- Effectiveness depends on semantic embeddings capturing task difficulty, which may not generalize to all hardware design domains
- Resource-intensive labeling requires 10 generations per variant per model, limiting scalability
- Results are benchmark-specific to RTLLM (50 tasks) and VerilogEval (156 tasks), with uncertain generalization

## Confidence

- **High confidence**: Multi-LLM routing approach outperforms single-model baselines (18.18% accuracy gain on RTLLM, 5.77% on VerilogEval)
- **Medium confidence**: External embeddings can effectively proxy internal model states for difficulty prediction
- **Low confidence**: 40% commercial API call reduction claim may not generalize beyond tested benchmarks

## Next Checks

1. **Cross-domain embedding validation**: Test whether external embeddings can predict difficulty for non-RTL tasks (e.g., Python code generation) to verify proxy mechanism's generality

2. **Label sensitivity analysis**: Systematically relax or tighten S_diff filtering thresholds and observe classifier F1-macro and final Pass@k impacts to quantify robustness

3. **Scaling experiment**: Evaluate VeriDispatcher on a larger, more diverse RTL benchmark (500+ tasks) to test whether commercial API reduction claim holds under increased volume and variety