---
ver: rpa2
title: Epistemic Uncertainty Quantification for Pre-trained VLMs via Riemannian Flow
  Matching
arxiv_id: '2601.21662'
source_url: https://arxiv.org/abs/2601.21662
tags:
- uncertainty
- epistemic
- repvlm
- flow
- riemannian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces REPVLM, a method for quantifying epistemic
  uncertainty in pre-trained Vision-Language Models (VLMs) by estimating the probability
  density of their embeddings on the hyperspherical manifold using Riemannian Flow
  Matching. The core idea is that low-density regions of the embedding space indicate
  model ignorance, so negative log-likelihood serves as a principled uncertainty score.
---

# Epistemic Uncertainty Quantification for Pre-trained VLMs via Riemannian Flow Matching

## Quick Facts
- arXiv ID: 2601.21662
- Source URL: https://arxiv.org/abs/2601.21662
- Reference count: 40
- Primary result: Near-perfect correlation (Spearman's S ≈ 1.0) between uncertainty and prediction error across multiple datasets

## Executive Summary
This paper introduces REPVLM, a method for quantifying epistemic uncertainty in pre-trained Vision-Language Models (VLMs) by estimating the probability density of their embeddings on the hyperspherical manifold using Riemannian Flow Matching. The core idea is that low-density regions of the embedding space indicate model ignorance, so negative log-likelihood serves as a principled uncertainty score. REPVLM learns a unified conditional vector field to model both visual and textual modality distributions, respecting the intrinsic geometry of the embedding space. Empirical results show that REPVLM achieves near-perfect correlation (Spearman's S ≈ 1.0) between uncertainty and prediction error across multiple datasets, significantly outperforming existing baselines like ProbVLM and Monte Carlo Dropout.

## Method Summary
REPVLM estimates epistemic uncertainty for frozen pre-trained VLMs by computing the negative log-density of embeddings on the hyperspherical manifold S^{d-1}. It uses Riemannian Flow Matching to learn a unified conditional vector field that models both visual and textual modality distributions. The method trains on proxy datasets with 500K-1M image-caption pairs, extracting ℓ2-normalized embeddings from a frozen VLM encoder. During inference, it integrates a reverse ODE to estimate density via Hutchinson trace estimator, producing uncertainty scores that correlate strongly with prediction errors.

## Key Results
- Achieves Spearman correlation S ≈ 1.0 between uncertainty and prediction error on ImageNet-1K
- Outperforms baselines (ProbVLM, Monte Carlo Dropout) by 3-5% at 90% rejection rate
- Provides effective out-of-distribution detection with clear separation between ID and OOD uncertainty scores

## Why This Works (Mechanism)

### Mechanism 1: Density-Based Epistemic Uncertainty Proxy
The negative log-density −log p(z) of a VLM embedding serves as a principled proxy for epistemic uncertainty through a theoretical chain: low epistemic uncertainty correlates with small parameter-Jacobian norm, which is suppressed in high data-density regions; for well-regularized encoders achieving dynamical isometry, input density p(x) ∝ embedding density p(z). This link breaks if the encoder exhibits highly nonlinear parameter dependence or fails dynamical isometry.

### Mechanism 2: Manifold-Native Density Estimation via Riemannian Flow Matching
Modeling densities directly on the hypersphere S^{d-1} via geodesic interpolation yields more accurate uncertainty estimates than Euclidean alternatives. Riemannian Flow Matching learns a vector field along geodesic paths from uniform base to target density, avoiding costly Jacobian determinants and respecting manifold curvature. This approach becomes invalid if embeddings are not properly normalized or the manifold structure is corrupted.

### Mechanism 3: Unified Modality-Conditioned Vector Field
A single neural network can learn density distributions for both visual and textual modalities while disentangling their geometries. The network conditions on modality via learnable embeddings summed with sinusoidal time encoding, allowing shared parameters for semantic reasoning while modality-specific embeddings capture distributional differences. This unified approach may underfit if image and text distributions diverge significantly.

## Foundational Learning

- **Concept: Epistemic vs. Aleatoric Uncertainty**
  - Why needed here: The paper targets epistemic uncertainty (model ignorance) specifically, distinguishing it from aleatoric uncertainty (data ambiguity) which existing probabilistic embedding methods address.
  - Quick check question: Given a blurry image of a cat, which uncertainty type captures "the image is ambiguous" vs. "the model has never seen this breed"?

- **Concept: Hyperspherical Manifold Geometry**
  - Why needed here: VLM embeddings are L2-normalized to unit vectors on S^{d-1}. Tangent space projection, geodesic distance, and exponential maps are core operations.
  - Quick check question: Why can't you use standard Euclidean interpolation between two points on a unit sphere?

- **Concept: Flow Matching / Continuous Normalizing Flows**
  - Why needed here: REPVLM extends Flow Matching to Riemannian manifolds. Understanding the ODE-based probability path and log-likelihood via divergence integration is essential.
  - Quick check question: How does flow matching avoid computing Jacobian determinants, unlike standard normalizing flows?

## Architecture Onboarding

- **Component map:**
  Frozen VLM encoder -> Vector field network -> Tangent space projector -> Hutchinson trace estimator -> Uncertainty score

- **Critical path:**
  1. Training: Sample (z_img, z_txt) pairs → select modality c → sample z_0 ∼ Unif(S^{d-1}) → compute geodesic target velocity u_t → regress v_ϕ(z_t, t, c) onto u_t
  2. Inference: Given z_1 → integrate reverse ODE with K=5 steps → accumulate divergence via Hutchinson estimator → compute U_ep = −log p_1(z|c)

- **Design tradeoffs:**
  - ODE steps (K=5): Fewer steps reduce cost but may introduce numerical drift; 5 steps achieves convergence per ablation
  - Proxy dataset size (500K–1M pairs): Larger improves density estimation but plateaus; smaller degrades OOD detection
  - Shared vs. separate modality networks: Unified model efficient but may underfit divergent distributions

- **Failure signatures:**
  - Negative Spearman correlation: Indicates uncertainty score inversely correlates with error (ProbVLM on ObjectNet: S = −0.799)
  - High variance across seeds: Suggests proxy dataset insufficient or training instability
  - Drift from sphere: ODE integration without renormalization causes embedding to leave S^{d-1}

- **First 3 experiments:**
  1. Sanity check: Train REPVLM on 50K proxy samples, plot accuracy-rejection curve on ImageNet-1K; expect Spearman S > 0.9 if implementation correct
  2. Ablation: Compare Riemannian vs. Euclidean (uniform base) vs. Euclidean (Gaussian base) on same proxy data; expect Riemannian > Euclidean by 3–5% at 90% rejection
  3. OOD detection: Visualize uncertainty score distributions for ImageNet-1K (ID), ImageNet-R (near-OOD), EuroSAT (far-OOD); expect clear separation with higher U_ep for OOD

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the density-based uncertainty metric disentangle genuine epistemic uncertainty from low density caused by the under-representation of specific demographics in the proxy dataset?
  - Basis in paper: Section 6 states that "reliance on embedding density poses potential fairness risks, as data from underrepresented demographics may be flagged as high-uncertainty due to inherent data sparsity rather than model error."
  - Why unresolved: The method currently treats all low-density regions as high-uncertainty, conflating algorithmic ignorance with dataset bias, without a mechanism to distinguish the two.
  - What evidence would resolve it: Empirical analysis on fairness benchmarks (e.g., CelebA) demonstrating that uncertainty scores are uncorrelated with protected attributes after adjusting for class balance.

- **Open Question 2:** What strategies can mitigate the performance degradation of REPVLM when the proxy dataset is not representative of the downstream distribution (e.g., significant domain shifts or resolution mismatches)?
  - Basis in paper: Section 6 notes efficacy is "contingent on the availability of a representative proxy dataset" and Section 5.2 observes degraded performance on Cifar100 due to resolution discrepancies.
  - Why unresolved: The method relies on static proxy datasets (CC3M, DataComp, LAION), which may fail to cover specific downstream "long-tail" distributions or input characteristics.
  - What evidence would resolve it: Development of online adaptation mechanisms or experiments showing stable performance across diverse, out-of-domain proxy datasets.

- **Open Question 3:** Does the assumed linear relationship between input density and embedding density hold for VLM architectures that do not strictly satisfy the dynamical isometry condition?
  - Basis in paper: Section 3.1 relies on the assumption that "dynamical isometry... where the network preserves the local geometric volume" implies p(z) ∝ p(x).
  - Why unresolved: If the encoder's Jacobian determinant deviates significantly from unity, the embedding density ceases to be a valid proxy for data density, breaking the theoretical justification.
  - What evidence would resolve it: Analysis of the spectral norms of the input-Jacobian for various VLM backbones (e.g., ViT vs. ResNet) correlated with uncertainty estimation errors.

## Limitations
- Theoretical assumption sensitivity: The density-uncertainty link relies on the encoder exhibiting "dynamical isometry," which is only verified empirically for pre-trained CLIP/SigLIP models.
- Proxy dataset dependence: Performance hinges on proxy data domain alignment with target benchmarks, with mismatched domains degrading uncertainty estimation quality.
- Single-step inference efficiency: While faster than Monte Carlo methods, the ODE integration with Hutchinson estimator still requires multiple forward passes, limiting real-time deployment.

## Confidence

- **High confidence**: The empirical demonstration that REPVLM achieves near-perfect Spearman correlation (S ≈ 1.0) between uncertainty and error across multiple datasets is robust and reproducible.
- **Medium confidence**: The theoretical mechanism linking embedding density to epistemic uncertainty assumes specific encoder properties (dynamical isometry) that are empirically observed but not formally verified for all VLMs.
- **Medium confidence**: The unified modality-conditioned approach assumes cross-modal semantic alignment is sufficient for parameter sharing, which may not hold for domain-specific or poorly aligned VLMs.

## Next Checks

1. **Domain robustness test**: Evaluate REPVLM trained on proxy data from a different domain (e.g., biomedical images) on standard vision benchmarks to quantify domain shift effects on uncertainty quality.
2. **Encoder generalization**: Apply REPVLM to VLMs without explicit dynamical isometry (e.g., CLIP models with task-specific fine-tuning) and measure degradation in Spearman correlation.
3. **Scalability validation**: Measure runtime and memory usage for REPVLM on high-resolution images (e.g., 512×512) and assess whether the method remains practical for real-time applications.