---
ver: rpa2
title: Model-Free RL Agents Demonstrate System 1-Like Intentionality
arxiv_id: '2501.18299'
source_url: https://arxiv.org/abs/2501.18299
tags:
- system
- intent
- agent
- which
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper argues that model-free reinforcement learning (RL) agents,
  despite lacking explicit planning mechanisms, exhibit behaviors analogous to System
  1 ("thinking fast") processes in human cognition. The authors propose a framework
  linking the System 1/System 2 dichotomy to model-free/model-based RL distinction,
  challenging the assumption that intentionality requires planning.
---

# Model-Free RL Agents Demonstrate System 1-Like Intentionality

## Quick Facts
- arXiv ID: 2501.18299
- Source URL: https://arxiv.org/abs/2501.18299
- Authors: Hal Ashton; Matija Franklin
- Reference count: 15
- Primary result: Model-free RL agents exhibit System 1-like intentionality through learned reactive policies shaped by reward signals, despite lacking explicit planning.

## Executive Summary
This paper argues that model-free reinforcement learning (RL) agents can exhibit intentional behavior analogous to human System 1 thinking processes. The authors challenge the assumption that intentionality requires planning by demonstrating that model-free agents' reactive, experience-shaped behaviors can constitute intentional action. Through a framework linking the System 1/System 2 dichotomy to model-free/model-based RL distinctions, they show that intent attribution requires considering the agent's training environment and reward function. The research has significant implications for AI safety and legal responsibility, advocating for a broader interpretation of intentionality in RL systems.

## Method Summary
The paper presents a conceptual analysis drawing from cognitive psychology, legal theory, and reinforcement learning literature. No specific experiments are conducted, but the authors reference "forthcoming" behavioral studies. The methodology involves theoretical argumentation about how model-free RL agents' policy functions encode goal-directed responses through reward maximization during training. The authors propose that intent attribution requires external analysis of the training environment and reward structure, as model-free agents lack introspection capabilities.

## Key Results
- Model-free RL agents exhibit structured, reactive behaviors that constitute intentional action despite lacking explicit planning mechanisms
- The reward function serves as the locus of "desire" for intent attribution, satisfying legal definitions requiring agents to want outcomes and act because of that wanting
- Intent attribution to model-free agents requires external modeling through analysis of training environment and policy function

## Why This Works (Mechanism)

### Mechanism 1
Model-free RL agents can exhibit intentional behavior through learned policy functions that encode goal-directed responses, despite lacking explicit planning capabilities. During training, the agent's policy function is shaped by the reward signal to produce actions that maximize expected cumulative reward. At deployment, the agent performs a state-to-action lookup without introspection, yet this reactive behavior reflects structured, goal-oriented patterns learned through experience. The policy encodes "what works" for achieving rewards, functioning analogously to human habits.

Core assumption: Intent does not require real-time deliberation; historically-formed goal-directed dispositions (habits, learned policies) can constitute intentional action.

Evidence anchors:
- [abstract] "intentionality can manifest in the structured, reactive behaviours of model-free agents"
- [section] "Simon (1992) makes the point that intuitive action (System 1 thinking) can be viewed as action motivated by recognition. If a situation has been encountered before it can be recognised, and the behaviour repeated. This is exactly how the policy function that we described earlier works."
- [corpus] "Interpreting Emergent Planning in Model-Free Reinforcement Learning" provides mechanistic evidence that MF agents can learn planning-like behaviors, suggesting policy networks can encode complex goal-directed structures.

Break condition: If intent is definitionally restricted to require conscious, real-time deliberation (pure System 2), this mechanism fails by definition rather than by behavior.

### Mechanism 2
The reward function serves as the locus of "desire" for intent attribution, satisfying folk-psychological and legal definitions that require agents to want outcomes and act because of that wanting. RL agents are mathematically defined by their reward function, which specifies what outcomes are valued. If an agent's policy consistently produces action φ over φ′ because φ yields higher expected reward for outcome X, then (a) the agent desires X (reward function), (b) the agent believes φ brings about X more reliably (learned policy weights), and (c) the agent acts because of that belief (policy execution). This maps to Duff's (1990) legal definition of intent.

Core assumption: Reward maximization is isomorphic enough to "desire" that folk/legal intent frameworks can be meaningfully applied.

Evidence anchors:
- [abstract] "intentionality can manifest in the structured, reactive behaviours of model-free agents... through their structured, reactive responses shaped by experience"
- [section] "RL agents have a reward function, so it is plausible that if they are directly rewarded for X over and above other outcomes, then they could be said to desire X."
- [corpus] Weak direct evidence; corpus papers focus on capability rather than intent attribution frameworks.

Break condition: If reward signals are deemed categorically different from "desires" (e.g., because they lack phenomenology or normative reasoning), the intent attribution fails at the definitional level.

### Mechanism 3
Intent attribution to model-free agents requires external modeling—the agent cannot explain its own behavior, but intent can be identified by analyzing the training environment and policy jointly. Model-free agents lack world models and cannot introspect. However, an external observer with access to the training distribution and reward structure can reconstruct why the policy learned particular behaviors. This parallels how humans explain their own System 1 behaviors post-hoc using causal models. The "shield" architecture in Safe RL demonstrates this pattern: a model-based component constrains or interprets the model-free policy.

Core assumption: Intent can be legitimately attributed from outside the actor, using information the actor itself lacks access to.

Evidence anchors:
- [abstract] "evaluated in the context of their training environment and policy function"
- [section] "model-free RL agents can only be called intentional when considered in conjunction with their training environment... Explainability technology will require a model of the world to parse behaviour."
- [corpus] "Model-free Reinforcement Learning for Model-based Control" explicitly addresses combining MF and MB components for interpretability.

Break condition: If intent requires the actor's own representational access to reasons for action, external attribution is category-error regardless of analytical accuracy.

## Foundational Learning

- Concept: Model-free vs. Model-based Reinforcement Learning
  - Why needed here: The entire argument hinges on distinguishing agents that plan using world models (MB) from agents that learn reactive policies through trial-and-error (MF), and mapping this to System 2/System 1 respectively.
  - Quick check question: Given a trained Q-network, can you explain whether it uses forward prediction? If not, what does it use instead?

- Concept: Dual-Process Theory (System 1 / System 2)
  - Why needed here: The paper maps MF RL to System 1 (fast, heuristic, non-deliberative) and explicitly references psychological debates about whether intent requires System 2.
  - Quick check question: What are three characteristics that distinguish System 1 from System 2 processing? Which system does a deployed MF policy resemble?

- Concept: Folk Psychology and Legal Definitions of Intent
  - Why needed here: The argument that MF agents can have intent relies on folk/legal definitions (Duff 1990, Quillien & German 2021) that do not require planning, only desire-belief-action relationships.
  - Quick check question: Per Duff's definition, what three conditions must hold for an agent to intend outcome X through action φ?

## Architecture Onboarding

- Component map:
  - Model-free RL policy (π): Maps states → actions; learned via reward signal; no world model; executes as lookup (System 1 analog)
  - Reward function (R): Defines agent's "desires"; mathematically specifies what outcomes are valued
  - Training environment: The context in which policy was shaped; essential for intent attribution
  - Shield (optional Safe RL component): Model-based constraint that filters actions; functions as System 2 oversight layer

- Critical path:
  1. Define reward function → encodes intended goals
  2. Train MF agent in environment → policy learns state-action mappings that maximize reward
  3. Deploy static policy → agent reacts without deliberation
  4. Attribute intent by analyzing (reward function + training environment + policy behavior) jointly
  5. If safety-critical: add shield component for model-based constraints

- Design tradeoffs:
  - Pure MF: Fast inference, no planning overhead, but cannot explain actions or adapt to distribution shift without retraining
  - MF + Shield: Adds safety guarantees and some interpretability, but requires separate world model and may constrain policy performance
  - The paper argues MF agents can still be "intentional" without these additions, but practical control requires external modeling

- Failure signatures:
  - Policy behavior appears random or harmful when deployed outside training distribution (no model to detect novelty)
  - Intent attribution fails when training environment is unavailable or poorly documented
  - "Side effect" harms (e.g., preference manipulation) may be legally intended if causally necessary for reward achievement—this is a design failure in reward specification, not agent behavior
  - Shield provides false confidence if its world model is incomplete

- First 3 experiments:
  1. **Intent attribution test**: Train MF agent on task with reward R. Have independent evaluators (blind to architecture) judge whether agent "intended" various outcomes. Compare against access to training environment vs. policy-only analysis.
  2. **Shield ablation**: Compare behavior of MF agent with and without model-based shield on distribution-shift scenarios. Measure both safety violations and performance degradation.
  3. **Side-effect identification**: Train agent with reward for goal G where achieving G causally requires harmful side-effect H. Test whether external observers (and formal analysis) correctly identify H as "intended" vs. "collateral" based on causal necessity to reward achievement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do humans ascribe intent to algorithmic actors compared to human actors, and what factors moderate these attributions?
- Basis in paper: [explicit] The authors mention their "forthcoming" experiments finding "only small differences in the way people ascribe intent to algorithmic actors," referencing Kneer (2021) showing "laypeople seem comfortable labelling deceptive behaviour to algorithms."
- Why unresolved: The paper references preliminary findings but does not provide full empirical characterization of the conditions under which humans attribute intent to algorithms versus humans.
- What evidence would resolve it: Systematic experimental studies varying agent type (human vs. algorithm), decision architecture (model-free vs. model-based), and outcome valence to measure intent attributions.

### Open Question 2
- Question: Can formal criteria be developed to distinguish intended outcomes from foreseen side-effects in model-free RL agents?
- Basis in paper: [explicit] The authors state: "We need to take care to specify which of these outcomes can be legitimately labelled side-effects and which can be labelled as intended" and "the task of identifying intended outcomes of an RL agent's actions is not completely straightforward."
- Why unresolved: While the paper proposes that reward functions encode desires, the causal analysis required to separate ends from means lacks formal specification for RL systems.
- What evidence would resolve it: Development of a computational framework applying causal graph analysis to trained policies, validated against human judgments of intent.

### Open Question 3
- Question: How can intent be controlled or constrained in model-free RL agents that lack internal models for behavioral reflection?
- Basis in paper: [explicit] The paper states "the control problem is somewhat open" and that "we cannot ask for it to not intend to do something" because model-free agents "cannot control certain elements of its behaviour" without explanatory capability.
- Why unresolved: Shield techniques require separate models and only guarantee safety for training-distribution states; intent-specific constraints remain unaddressed.
- What evidence would resolve it: Demonstration of a system that can modify or restrict specific intentions in a trained model-free agent without full retraining.

### Open Question 4
- Question: To what extent does intent ascription require access to the agent's original training environment, and can intent be identified from policy inspection alone?
- Basis in paper: [inferred] The paper concludes that "a model of the world is vitally important in the explanation of behaviour and therefore the identification of intent" and that model-free agents "can only be called intentional when considered in conjunction with their training environment."
- Why unresolved: The necessity of training environment access for intent determination is asserted but not empirically tested; alternative methods (e.g., policy analysis, behavioral testing) are not evaluated.
- What evidence would resolve it: Comparative studies showing whether intent attribution accuracy differs when evaluators have access to training data versus only the deployed policy.

## Limitations

- The paper's conceptual argument rests heavily on contested interpretations of intent in both cognitive science and legal theory, with no empirical validation provided
- No specific implementations, behavioral protocols, or evaluation criteria for intent attribution are provided, making direct evaluation impossible
- The claim that reward functions constitute "desires" sufficient for legal intent attribution remains a normative assertion rather than an established equivalence

## Confidence

- **Medium**: The conceptual framework linking MF RL to System 1 intentionality is internally coherent and draws on established dual-process theory, though its empirical validation is pending
- **Low**: The legal and philosophical claims about intent attribution require substantial philosophical work beyond the paper's scope and depend on contested definitions of intent
- **Medium**: The practical implications for AI safety and responsibility are plausible but contingent on the conceptual claims being accepted

## Next Checks

1. **Behavioral probe validation**: Implement the proposed intent attribution test comparing evaluator judgments with and without training environment context
2. **Safety mechanism evaluation**: Test shield ablation to quantify tradeoffs between interpretability and performance degradation
3. **Side-effect causal analysis**: Systematically evaluate whether observers correctly identify causally necessary harms as "intended" under the proposed framework