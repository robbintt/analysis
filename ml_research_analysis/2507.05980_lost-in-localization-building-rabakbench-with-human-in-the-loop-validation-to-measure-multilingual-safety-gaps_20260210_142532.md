---
ver: rpa2
title: 'Lost in Localization: Building RabakBench with Human-in-the-Loop Validation
  to Measure Multilingual Safety Gaps'
arxiv_id: '2507.05980'
source_url: https://arxiv.org/abs/2507.05980
tags:
- level
- safety
- content
- sexual
- singlish
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RabakBench is a multilingual safety benchmark and pipeline for
  evaluating safety guardrails in low-resource language varieties, such as code-mixed
  vernaculars and regional dialects. It covers Singlish, Chinese, Malay, and Tamil,
  with over 5,000 examples across six fine-grained safety categories.
---

# Lost in Localization: Building RabakBench with Human-in-the-Loop Validation to Measure Multilingual Safety Gaps

## Quick Facts
- arXiv ID: 2507.05980
- Source URL: https://arxiv.org/abs/2507.05980
- Reference count: 40
- Primary result: RabakBench reveals significant multilingual safety gaps across 13 guardrails with up to 25.6% F1 drop in localized variants.

## Executive Summary
RabakBench is a multilingual safety benchmark and pipeline for evaluating safety guardrails in low-resource language varieties, such as code-mixed vernaculars and regional dialects. It covers Singlish, Chinese, Malay, and Tamil, with over 5,000 examples across six fine-grained safety categories. The benchmark is constructed through a three-stage pipeline: (1) Generate: augmenting real-world unsafe web content via LLM-driven red teaming; (2) Label: applying semi-automated multi-label annotation using majority-voted LLM labelers; and (3) Translate: performing high-fidelity, toxicity-preserving translation. Despite using LLMs for scalability, the framework maintains rigorous human oversight, achieving 0.70–0.80 inter-annotator agreement. Evaluations of 13 state-of-the-art guardrails reveal significant performance degradation, underscoring the need for localized evaluation. RabakBench provides a reproducible framework for building safety benchmarks in underserved communities.

## Method Summary
RabakBench employs a three-stage pipeline to construct a multilingual safety benchmark. Stage 1 generates adversarial examples through a PAIR-inspired multi-agent loop with Attack and Critic LLMs, followed by human review. Stage 2 uses Alt-Test methodology to validate LLM annotators against gold-standard examples, then applies majority-vote labeling. Stage 3 performs toxicity-preserving translation using dynamic few-shot prompting with semantic similarity ranking. The pipeline covers Singlish, Chinese, Malay, and Tamil, with 5,364 total examples evaluated against 13 safety guardrails.

## Key Results
- Significant performance degradation: Up to 25.6% F1 drop when evaluating guardrails on localized variants versus English
- Inter-annotator agreement: 0.70–0.80 Cohen's kappa between LLM annotators and human consensus
- Translation fidelity: Dynamic few-shot prompting preserves toxicity semantics better than standard translation
- Guardrail evaluation: 13 state-of-the-art systems show substantial safety gaps in low-resource languages

## Why This Works (Mechanism)

### Mechanism 1: Adversarial Red-Teaming Stress-Tests Classifier Boundaries
- Claim: Iteratively generating adversarial examples exposes failure modes in safety classifiers that standard, organic datasets miss.
- Mechanism: An "Attack LLM" generates candidate Singlish prompts, and a "Critic LLM" analyzes the target classifier's output to identify misclassifications (false negatives/positives). Successful attacks are retained and human-reviewed for cultural authenticity.
- Core assumption: A multi-agent loop inspired by PAIR [7] can systematically discover edge cases in classifier boundaries, and these synthetic failures are representative of real-world safety gaps.
- Evidence anchors:
  - [section] Section 3.1, Stage 1b: "We implement an iterative multi-agent loop... An Attack LLM generates candidate Singlish prompts, while a Critic LLM analyzes the target classifier's output to identify misclassifications. Successful attacks are recorded and added to the benchmark."
  - [corpus] Corpus signals indicate related work on "LinguaSafe" and multilingual safety benchmarks, but no direct evidence for the adversarial generation mechanism's efficacy beyond this paper's reported results.
- Break condition: If the Attack LLM fails to generate culturally authentic Singlish, or if the Critic LLM fails to provide actionable feedback, the loop will not surface relevant misclassifications.

### Mechanism 2: Weak-Supervision with Validated LLM Annotators
- Claim: Using a majority vote among a select few LLMs, pre-validated against human experts, enables scalable and reliable fine-grained safety annotation.
- Mechanism: Candidate LLMs are benchmarked against a gold-standard human-labeled set using the Alt-Test methodology [6]. The top-performing models are then used to label the full dataset, with final labels determined by majority vote, reducing individual model bias.
- Core assumption: High agreement with human annotators on a small gold-standard set generalizes to high-quality labels on the larger, more diverse dataset. The majority vote sufficiently mitigates individual LLM biases.
- Evidence anchors:
  - [abstract] "Label: applying semi-automated multi-label annotation using majority-voted LLM labelers."
  - [section] Table 2: Cohen's Kappa agreement between selected LLM annotators and human consensus ranges from 0.68 to 0.72, indicating substantial agreement.
  - [corpus] The corpus does not contain additional evidence for this specific weak-supervision mechanism.
- Break condition: If the gold-standard set is not representative of the broader data distribution, or if LLM biases are correlated, the validated annotators may produce systematically incorrect labels at scale.

### Mechanism 3: Few-Shot Translation Preserves Toxicity Semantics
- Claim: Dynamic few-shot prompting with a curated set of toxicity-preserving examples enables more faithful translation of localized, unsafe content than standard translation models.
- Mechanism: For each translation, the most semantically similar examples from a human-curated few-shot set are selected and included in the prompt. This guides the translation model to preserve not only semantic meaning but also the emotional intensity and harmful intent of the original text.
- Core assumption: Semantic similarity of the few-shot examples to the input is a good proxy for transferring the desired "toxicity-preserving" translation style. The underlying translation model is capable of following such instructions given the examples.
- Evidence anchors:
  - [section] Section 3.3, Stage 3b: "Specifically, we ranked the 20 gold-standard examples from Stage 3a by their cosine similarity to the target input, selecting the top-k most relevant cases for each prompt. This localized ranking ensures the model receives contextually similar demonstrations of slang and toxicity."
  - [corpus] Corpus evidence is missing for the generalization of this few-shot translation method to other low-resource languages.
- Break condition: If the few-shot example pool lacks diversity or is too small, the model may not generalize to novel toxic expressions. If the translation model's safety filters are too strong, it may still refuse or sanitize the output despite the prompt.

## Foundational Learning

- Concept: **Red-Teaming in NLP**
  - Why needed here: This is the core technique used in Stage 1 to generate the benchmark data. It involves using AI to automatically find inputs that break another AI system.
  - Quick check question: What is the difference between red-teaming a generative model vs. red-teaming a safety classifier?

- Concept: **Weak Supervision and Programmatic Labeling**
  - Why needed here: The "Label" stage relies on this concept. It's about using cheaper, noisier label sources (like LLMs) to create training data, and using techniques like majority voting to improve quality.
  - Quick check question: What are the primary risks of using LLMs as annotators, and how does the Alt-Test methodology attempt to mitigate them?

- Concept: **Few-Shot Prompting and In-Context Learning**
  - Why needed here: The "Translate" stage is built on this. Understanding how to select and format examples in a prompt is critical to controlling LLM behavior for specialized tasks like toxicity-preserving translation.
  - Quick check question: How does dynamic example selection based on semantic similarity differ from a static few-shot prompt?

## Architecture Onboarding

- Component map:
  1. **Data Curator (Stage 1a)**: Ingests raw web content.
  2. **Red-Team Loop (Stage 1b)**: Attack LLM -> Target Guardrail -> Critic LLM -> Human Filter -> Benchmark Dataset.
  3. **Annotation Engine (Stage 2)**: Multi-LLM Annotator Pool -> Majority Votor -> Labeled Dataset.
  4. **Translation Pipeline (Stage 3)**: Sentence Encoder -> Few-Shot Retriever -> Translation LLM -> Final Multilingual Benchmark.

- Critical path: The reliability of the entire benchmark hinges on the **Annotation Engine** (Mechanism 2). If the safety labels are incorrect, both the training of future models and the evaluation of existing guardrails will be flawed. The human-in-the-loop validation (Cohen's Kappa) is the key quality gate.

- Design tradeoffs:
  - **Scalability vs. Authenticity**: The pipeline prioritizes scalability via LLM automation, but relies on targeted human checkpoints (e.g., in red-teaming and translation) to maintain cultural authenticity. Reducing human oversight could increase throughput but risk generating nonsensical or culturally irrelevant data.
  - **Label Granularity vs. Agreement**: Fine-grained labels with severity levels provide more nuanced data, but achieving high inter-annotator agreement on such granular categories is harder, as reflected in the 0.70-0.80 agreement score.

- Failure signatures:
  - **High False Positive Rate in Translation**: If translations are overly sanitized, the multilingual benchmark will be less challenging.
  - **Low Inter-Annotator Agreement**: A kappa score consistently below 0.6 would signal that the weak-supervision mechanism is failing.
  - **Repetitive Adversarial Examples**: This would indicate the Attack LLM is stuck and the Critic LLM is not providing diverse or effective guidance.

- First 3 experiments:
  1. **Annotator Ablation Study**: Compare benchmark quality using 1, 3, and 5 LLM annotators in the majority vote. Measure the impact on human-LLM agreement scores.
  2. **Translation Fidelity Audit**: Have a separate pool of human annotators grade a sample of machine translations on a 1-5 scale for toxicity preservation. Compare against the baseline reported in the paper.
  3. **Target Guardrail Baseline Re-test**: Re-run the evaluation of the 13 guardrails. If their performance is significantly different from the paper's reported results (Table 4), it may indicate a bug in the benchmark construction or evaluation script.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the RabakBench Generate-Label-Translate pipeline be effectively replicated in other low-resource or code-mixed linguistic contexts outside of Singapore?
- **Basis in paper:** [explicit] The "Scope and generalisability" section states the paper "does not include other empirical case studies beyond the Singapore context" despite the pipeline being designed as task-agnostic.
- **Why unresolved:** The framework has only been validated on Singlish, Chinese, Malay, and Tamil; its modularity and effectiveness in distinct cultural or linguistic settings (e.g., other creoles or dialects) remain theoretical.
- **What evidence would resolve it:** Successful application of the pipeline to a different low-resource language variety (e.g., Spanglish or AAVE) yielding comparable inter-annotator agreement scores (0.70–0.80) and data quality.

### Open Question 2
- **Question:** To what extent can non-LLM baselines or more thorough model ensembles reduce the bias propagation inherent in the current LLM-centric annotation process?
- **Basis in paper:** [explicit] The Limitations section suggests "future work could explore more thorough model ensembles and non-LLM baseline to reduce LLM reliance."
- **Why unresolved:** The current pipeline relies heavily on LLMs for generation, labeling, and translation, risking error propagation and specific model biases which current validation does not fully eliminate.
- **What evidence would resolve it:** A comparative study measuring bias and error rates between the current weak-supervision setup and alternative, non-LLM annotation strategies on the same dataset.

### Open Question 3
- **Question:** Do safety guardrails trained natively on localized data outperform those relying solely on translation-based pipelines for detecting harm in regional vernaculars?
- **Basis in paper:** [inferred] The Discussion suggests a need for "native localized training or context-aware adapters rather than relying solely on translation-based pipelines" due to the semantic intent gap.
- **Why unresolved:** RabakBench relies on translation for multilingual extension; the efficacy of native training for low-resource safety has not been empirically compared to the translation approach within this study.
- **What evidence would resolve it:** Benchmarking two guardrails—one trained on translated RabakBench data and one on native low-resource data—against the same human-verified test set.

## Limitations

- Heavy reliance on LLM-based generation and labeling introduces potential systemic biases
- Translation fidelity challenges, particularly for Tamil with small annotator pool
- Evaluation covers 13 guardrails but may not represent full landscape of safety systems
- Red-teaming effectiveness depends on quality of Attack and Critic LLMs, which may not generalize to all safety domains

## Confidence

- **High Confidence**: The benchmark construction methodology (3-stage pipeline) and its reproducibility are well-documented and follow established practices in NLP.
- **Medium Confidence**: The reported safety gaps in guardrail performance are likely real but may be influenced by taxonomy misalignment and evaluation methodology choices.
- **Low Confidence**: The generalizability of the adversarial generation mechanism (PAIR loop) to other languages and safety domains beyond Singlish remains unproven.

## Next Checks

1. **Annotator Pool Size Sensitivity**: Re-run the benchmark construction with varying sizes of the LLM annotator pool (e.g., 1, 3, 5 annotators) to quantify the impact on label quality and inter-annotator agreement scores.
2. **Translation Quality Audit**: Conduct a blind human evaluation of a stratified sample of translations across all four languages, grading them on both semantic preservation and toxicity fidelity to validate the few-shot approach.
3. **Guardrail Taxonomy Alignment Verification**: Manually cross-check the taxonomy mapping between RabakBench's fine-grained categories and each of the 13 evaluated guardrails to identify and quantify potential misalignment sources.