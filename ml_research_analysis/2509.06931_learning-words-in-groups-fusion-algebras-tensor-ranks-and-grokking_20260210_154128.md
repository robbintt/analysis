---
ver: rpa2
title: 'Learning words in groups: fusion algebras, tensor ranks and grokking'
arxiv_id: '2509.06931'
source_url: https://arxiv.org/abs/2509.06931
tags:
- tensor
- bscs
- which
- groups
- group
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that a simple two-layer neural network
  with standard activation functions can learn arbitrary word operations in any finite
  group, exhibiting grokking behavior. The authors reframe the learning task as implementing
  a specific 3-tensor and show it is typically of low rank due to the fusion algebra
  structure of the group.
---

# Learning words in groups: fusion algebras, tensor ranks and grokking

## Quick Facts
- arXiv ID: 2509.06931
- Source URL: https://arxiv.org/abs/2509.06931
- Reference count: 40
- Simple two-layer neural networks can learn arbitrary word operations in finite groups with low width due to fusion algebra structure

## Executive Summary
This paper shows that neural networks can learn arbitrary word operations in finite groups using significantly less width than theoretically expected. The key insight is that the word tensor, which encodes the group operation, has low rank due to the fusion algebra structure of the group. The authors demonstrate that gradient descent finds these low-rank solutions, explaining the phenomenon of grokking in group learning tasks.

## Method Summary
The authors reframe group word learning as implementing a specific 3-tensor (the word tensor) rather than just mapping inputs to outputs. They decompose this tensor along triplets of basic self-conjugate representations using the group's fusion structure to identify sparse components. A combinatorial optimization framework finds minimal box covers that bound the tensor rank. They use a surrogate Hadamard model where width equals tensor rank, and show gradient descent dynamics decouple along different representation subspaces, enabling efficient learning.

## Key Results
- Proved word tensor rank is typically much lower than naive |G|² bound due to fusion algebra sparsity
- Showed networks find low-rank implementations of word tensors through gradient descent
- Demonstrated grokking behavior emerges from transition to low-rank generalizing solutions
- Revealed network implements efficient matrix multiplication (Strassen-like) for simple multiplication word

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Finite group word operations can be implemented by neural networks using significantly less width than the naive theoretical maximum because the underlying "word tensor" is typically low-rank.
- **Mechanism:** The task is reframed as implementing a 3-tensor (the "word tensor") rather than just mapping inputs to outputs. This tensor is projected onto triplets of basic self-conjugate (bsc) representations of the group. Crucially, the **fusion rules** of the group's algebra act as a structural prior, forcing many potential tensor components to zero (sparse "bsc-support"). The network effectively exploits this sparsity to find a low-rank implementation.
- **Core assumption:** The network's optimization path aligns with the low-rank structure dictated by the fusion algebra, rather than overfitting to a high-rank memorization solution first.
- **Evidence anchors:** [abstract] "...decomposing the word tensor along triplets of basic self-conjugate (bsc) representations of the group, using the fusion structure to rule out many components."

### Mechanism 2
- **Claim:** For the specific task of group multiplication, the network discovers and implements efficient matrix multiplication algorithms (analogous to Strassen's algorithm) to minimize width requirements.
- **Mechanism:** When the "word" is simple multiplication ($w=ab$), the projected word tensor for each bsc representation is isomorphic to the matrix multiplication tensor. The network finds minimal-rank implementations for these components. Since the theoretical rank of matrix multiplication tensors is lower than the naive bound (Strassen's result), the network effectively "discovers" efficient multiplication strategies.
- **Core assumption:** Gradient descent successfully navigates the loss landscape to locate these specific Strassen-like low-rank factorizations among the many possible solutions.
- **Evidence anchors:** [abstract] "...showing that the network effectively implements efficient matrix multiplication in the sense of Strassen."

### Mechanism 3
- **Claim:** The learning dynamics "decouple" along the distinct orthogonal subspaces of the group's basic self-conjugate (bsc) representations.
- **Mechanism:** The total loss decomposes into a sum of independent "bsc-losses." If the weight matrices are initialized such that rows belong to specific bsc subspaces (mono-bsc-aligned), gradient descent steps preserve this alignment. This allows the network to solve the problem as a set of independent smaller problems (one per bsc) rather than a single complex optimization.
- **Core assumption:** Initialization or early dynamics drive the network towards a state where weights are effectively partitioned by bsc subspaces, allowing the decoupling to take hold.
- **Evidence anchors:** [abstract] "...showing that the dynamics effectively decouples along different bsc representations."

## Foundational Learning

- **Concept: Fusion Algebras / Representation Theory**
  - **Why needed here:** This is the structural "genetic code" of the group. You cannot predict the low-rank nature of the word tensor or understand the "bsc-support" without knowing how group representations combine (fuse).
  - **Quick check question:** Can you explain why the product of two representations $\rho \otimes \psi$ might decompose into a direct sum of smaller representations, and how a "fusion table" summarizes this?

- **Concept: Tensor Rank (CP Decomposition)**
  - **Why needed here:** The paper equates the "complexity" of the learning task directly to the tensor rank of the word tensor. Understanding rank as the minimum number of rank-1 tensors needed to express the operation is central.
  - **Quick check question:** Given a 3-tensor $T \in \mathbb{R}^{n \times n \times n}$, how does its tensor rank relate to the width $m$ required to implement it in the Hadamard (HD) model?

- **Concept: The Hadamard (HD) Surrogate Model**
  - **Why needed here:** The paper uses a specific 2-layer architecture where the activation is replaced by a Hadamard product ($Ax \odot By$). This model provides a clean theoretical mapping: Width = Tensor Rank.
  - **Quick check question:** How does the function $f(x,y) = C^T(Ax \odot By)$ differ mathematically from a standard MLP $f(x,y) = C^T \sigma(Ax + By)$, and why does the former make tensor rank analysis easier?

## Architecture Onboarding

- **Component map:** One-hot inputs $x, y$ -> Parallel embedding matrices $A, B$ -> Element-wise product (HD) or activation (TLP) -> Output matrix $C$ -> Logits

- **Critical path:**
  1. Verify dataset $D_{G,w}$ (samples of form $(1_a, 1_b) \to 1_{w(a,b)}$)
  2. Initialize weights $A, B, C$ randomly (centered Gaussian)
  3. Run Gradient Descent (GD) or Adam on MSE loss
  4. Monitor train/test accuracy for "grokking" (sharp test accuracy jump long after train accuracy hits 100%)

- **Design tradeoffs:**
  - **HD Model vs. TLP:** Use the HD model for theoretical clarity (width = rank exactly). Use TLP for standard architecture behavior (requires width $\approx 2 \times \text{rank}$ due to polynomial approximation of activation)
  - **Width ($m$):** Must be $\geq \text{rank}(\delta_{G,w})$. If width is too low, the network can only approximate the tensor, potentially missing specific bsc components

- **Failure signatures:**
  - **Stagnant Loss:** If width is significantly below the required tensor rank, loss plateaus above zero
  - **No Grokking:** If the dataset fraction is too small or optimization hyperparameters are wrong, the model memorizes but never transitions to the low-rank generalizing phase
  - **Noise in BSC Support:** In TLP models, terminal weights might show "noisy" alignment to bscs compared to the clean block structure of the HD model

- **First 3 experiments:**
  1. **Baseline (Cyclic Group):** Train HD model on modular addition ($\mathbb{Z}_p$). Verify grokking and plot the heatmap of weights $A, B, C$. Confirm they align with Fourier basis vectors
  2. **Non-Abelian Test:** Train on $S_3$ or $S_4$ with the word $ab$. Calculate the "box-rank" bound theoretically and check if the minimal width required empirically matches this bound
  3. **Word Complexity:** Compare learning the word $ab$ vs. a complex word like $aba^{-1}b^2$. Observe if the complex word requires more width (higher rank) or just different bsc-support boxes

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Why are the low-rank implementations of the word tensor (specifically those with sparse-bsc-support) globally attractive under gradient descent?
- **Basis in paper:** Section 7.2 states that the mechanism for why these solutions are reached is not addressed, and a proof is missing even for simple group multiplication on $\mathbb{Z}_p$.
- **Why unresolved:** The paper empirically observes convergence to low-rank solutions but lacks a formal mathematical justification for why the gradient descent dynamics prefer these configurations globally.
- **What evidence would resolve it:** A theoretical proof establishing the global attractiveness of these solutions, or a demonstration that local minima suffice for the observed generalization.

### Open Question 2
- **Question:** How can the existence and form of generalizing solutions in the standard Two-Layer Perceptron (TLP) model be formally characterized?
- **Basis in paper:** Section 7.3 notes that for the TLP model, the question of existence is only partially answered, and making the low-degree polynomial approximation precise requires further work.
- **Why unresolved:** The theoretical analysis relies on a surrogate "Hadamard model" and low-degree Taylor approximations for standard activations, which has not been statistically validated as fully capturing TLP behavior.
- **What evidence would resolve it:** A rigorous reformulation of the box-cover theory for TLP models using low-degree tensor products, supported by statistically significant experimental verification.

### Open Question 3
- **Question:** What is the origin of the "additional structure" observed in terminal weight configurations that is not explained by the group's fusion table?
- **Basis in paper:** Section 7.5 describes a "hidden structure" in the terminal weights where specific components vanish non-generically, which is left for future work.
- **Why unresolved:** Current analysis relies on combinatorial fusion data to predict block structures, but fails to account for the finer details observed in the heatmaps of the weights.
- **What evidence would resolve it:** An extended theory incorporating the multiplicities of basic self-conjugate (bsc) representations to predict these specific vanishings within the matrix coefficient spaces.

## Limitations
- Theoretical analysis relies on idealized Hadamard model, requiring 2×width for standard TLP architectures
- Does not extensively explore cases where fusion algebra doesn't enforce sparsity (full-rank tensors)
- Missing formal proof of why gradient descent prefers low-rank solutions globally

## Confidence
- **High confidence:** Networks can learn group word operations with width less than naive theoretical maximum due to fusion algebra structure
- **Medium confidence:** Grokking emerges from transition to low-rank generalizing solutions
- **Medium confidence:** Networks implement efficient matrix multiplication for simple multiplication word

## Next Checks
1. Test non-grokking regimes by systematically varying width below theoretical rank bound and training fraction below 0.5
2. Validate weight alignment in TLP models by quantifying "cleanness" of bsc-alignment using correlation measures
3. Stress test fusion sparsity by constructing groups/words with dense bsc-support to verify rank approaches |G|² limit