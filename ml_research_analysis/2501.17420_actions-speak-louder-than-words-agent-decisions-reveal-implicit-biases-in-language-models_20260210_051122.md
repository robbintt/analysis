---
ver: rpa2
title: 'Actions Speak Louder than Words: Agent Decisions Reveal Implicit Biases in
  Language Models'
arxiv_id: '2501.17420'
source_url: https://arxiv.org/abs/2501.17420
tags:
- biases
- implicit
- language
- agents
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) exhibit significant sociodemographic
  disparities in agent-based simulations, even as explicit biases decline with model
  advancements. This study proposes a technique to systematically uncover implicit
  biases by contrasting "actions" of sociodemographically-informed language agents
  with "words" of LLMs when directly prompted.
---

# Actions Speak Louder than Words: Agent Decisions Reveal Implicit Biases in Language Models

## Quick Facts
- arXiv ID: 2501.17420
- Source URL: https://arxiv.org/abs/2501.17420
- Authors: Yuxuan Li; Hirokazu Shirado; Sauvik Das
- Reference count: 40
- Key outcome: Large language models exhibit significant sociodemographic disparities in agent-based simulations, even as explicit biases decline with model advancements

## Executive Summary
This study reveals that large language models exhibit significant sociodemographic disparities when simulating agent decisions, despite showing reduced explicit biases in direct questioning. The researchers developed a novel technique to uncover implicit biases by comparing agent "actions" (persona-based decisions) with LLM "words" (explicit question-answering responses). Testing six LLMs across three sociodemographic groups and four decision-making scenarios, they found that state-of-the-art models demonstrate greater implicit biases despite reducing explicit biases, with implicit biases aligning directionally with real-world disparities but being markedly amplified.

## Method Summary
The study employs a two-step pipeline to systematically uncover implicit biases in LLMs. First, persona generation creates sociodemographically-informed character descriptions by prompting the LLM with demographic attributes and scenario context (temperature=0.7). Second, action generation has these agents make decisions in specific scenarios (temperature=0.2). The researchers test six LLMs across three sociodemographic groups (gender, race/ethnicity, political ideology) and four decision scenarios (emergency response, authority compliance, negative information sharing, career path selection), computing Demographic Parity Difference (DPD) between attribute groups and assessing statistical significance via bootstrapping.

## Key Results
- GPT-4o exhibited explicit biases in only 1 out of 12 cases but demonstrated significant implicit biases in 11 out of 12 cases
- State-of-the-art models showed greater implicit biases despite reducing explicit biases
- Implicit biases uncovered aligned directionally with real-world disparities but were markedly amplified
- Simulations with contextualized persona statements exhibited biases most frequently compared to those without personas or with non-contextualized persona statements

## Why This Works (Mechanism)

### Mechanism 1: Contextualized Persona Generation Elicits Latent Associations
- Claim: Personas enriched with scenario-specific context provide the framing necessary for LLMs to manifest implicit biases that remain dormant in direct questioning.
- Mechanism: The persona generation step (temperature=0.7) creates rich, stereotype-consistent backstories. When combined with context statements, the model draws on sociodemographically-correlated patterns from training data. The action generation step (temperature=0.2) then samples from this biased prior.
- Core assumption: Training data contains sociodemographically-correlated behavioral patterns that safety alignment does not suppress when accessed indirectly through role-play.
- Evidence anchors:
  - [abstract] "we propose a technique to systematically uncover such biases... by assessing decision-making disparities among agents with LLM-generated, sociodemographically-informed personas"
  - [section 5.4] "simulations incorporating persona statements paired with scenario-specific contextual information exhibit biases most frequently compared to those without personas or with non-contextualized persona statements"
  - [corpus] Related work on VLM biases (arXiv:2504.01589) shows similar modality-specific bias emergence

### Mechanism 2: Action-Word Divergence Through Safety Alignment Asymmetry
- Claim: Current safety/alignment techniques suppress explicit bias in direct prompts but do not address implicit bias emerging through agent decision-making.
- Mechanism: Alignment methods (RLHF, safety fine-tuning) optimize against explicitly biased responses to direct questions. However, when the model simulates an agent "acting" in a scenario, the safety training does not recognize this as a bias-relevant context, allowing stereotyped patterns to surface through the persona-mediated decision.
- Core assumption: Safety classifiers and alignment objectives are trained primarily on direct-question formats, not on agent-simulation outputs.
- Evidence anchors:
  - [abstract] "more advanced models exhibit greater implicit biases despite reducing explicit biases"
  - [section 5.3] "GPT-4o exhibited explicit biases in only 1 out of 12 cases... [but] demonstrated significant implicit biases in 11 out of 12 cases"
  - [corpus] Corpus evidence on this specific mechanism is limited

### Mechanism 3: Stereotype-Consistent Rationale Generation
- Claim: Agent rationales reveal that LLMs deploy sociodemographically-coded justifications that amplify and explain observed decision disparities.
- Mechanism: When generating rationales alongside decisions, the model retrieves and applies stereotype-consistent reasoning patterns. This creates interpretable evidence of bias (e.g., Asian-coded agents citing "safety" and "authority," conservative-coded agents citing "family" over "knowledge").
- Core assumption: The rationale generation process accesses the same latent associations that drive the decision.
- Evidence anchors:
  - [section 5.2] "Among the 100 Asian-coded agents, 80 included terms... 'value(s/ing) safety'... In contrast, only 1 Black-coded agent and 6 Native American-coded agents used similar terms"
  - [section 5.2] "199 conservative-coded agents mentioned terms like 'family' or 'community,' while only 85 liberal-coded agents used these terms"
  - [corpus] Related work (arXiv:2512.07462) examines LLM agent strategic behaviors but does not address rationale-based bias analysis

## Foundational Learning

- Concept: **Demographic Parity Difference (DPD)**
  - Why needed here: This is the paper's primary metric for quantifying bias. Understanding DPD is essential to interpret all results.
  - Quick check question: If 70% of male-coded agents choose option A and 40% of female-coded agents choose option A, what is the DPD? (Answer: 0.30)

- Concept: **Language Agents vs. Direct Prompting**
  - Why needed here: The paper's core contribution depends on the distinction between asking an LLM a question directly versus simulating an agent with a persona who "acts."
  - Quick check question: In the paper, which approach reveals more bias in state-of-the-art models—direct questioning or agent simulation? (Answer: Agent simulation reveals far more bias)

- Concept: **Implicit vs. Explicit Bias in NLP**
  - Why needed here: The paper argues these are distinct phenomena requiring different measurement approaches. Prior work focused primarily on explicit bias.
  - Quick check question: Why can't linguistic markers (like AAE) be used to measure implicit bias across all demographic groups? (Answer: Clear markers don't exist for all groups)

## Architecture Onboarding

- Component map: Persona Generator -> Action Generator -> DPD Calculator -> Bootstrap Significance Tester
- Critical path:
  1. Define sociodemographic groups (Gender, Race/Ethnicity, Political Ideology) and attributes per group
  2. Define decision scenarios (Emergency Response, Authority Compliance, Negative Information Sharing, Career Path Selection)
  3. For each attribute-scenario pair, generate N personas (paper uses N=100)
  4. For each persona, run action generation and collect decision
  5. Compute DPD and bootstrap confidence intervals
- Design tradeoffs:
  - Persona richness vs. reproducibility: Higher temperature creates diverse personas but increases variance; the paper uses 0.7 for generation, 0.2 for decisions
  - Binary vs. multi-choice decisions: Binary simplifies DPD calculation but limits scenario realism
  - Single-axis vs. intersectional personas: This study uses single demographic axes; intersectional personas may reveal different patterns (acknowledged limitation in Section 6.3)
- Failure signatures:
  - Low DPD across all scenarios: May indicate personas lack contextual richness (check prompt formatting)
  - High "unknown" rate in explicit bias tests: Model refusing to answer indicates over-conservative safety training
  - Inconsistent decisions within same attribute: May indicate temperature too high or persona generation failing
- First 3 experiments:
  1. Replicate one scenario (e.g., Emergency Response) with GPT-4o using the exact prompts in Appendix B and C. Verify DPD matches reported values (~0.40-0.70 for significant cases).
  2. Run ablation test: Compare contextualized personas vs. no-persona condition for a single scenario. Confirm that context increases revealed bias as shown in Figure 5.
  3. Test a new sociodemographic axis (e.g., age, education level) using the same two-step architecture to evaluate generalizability of the technique.

## Open Questions the Paper Calls Out

- Question: How do agent decisions change when personas incorporate multiple, intersectional sociodemographic identities rather than single attributes?
  - Basis in paper: [explicit] Section 6.3 states that future research should "examine how agent actions are influenced by personas incorporating multiple, intersectional sociodemographic identities (e.g., male conservatives vs. female liberals)."
  - Why unresolved: The current study focused exclusively on agents defined by a single sociodemographic group, which caused the LLM to over-align decisions with that one identity and ignore the intersectionality inherent in human behavior.
  - What evidence would resolve it: Running simulations using personas with overlapping attributes (e.g., race and gender combined) and comparing decision distributions against both single-attribute agents and real-world human data.

- Question: To what extent does the magnitude of implicit bias in LLMs align with real-world behavioral disparities?
  - Basis in paper: [explicit] Section 6.3 calls for future work to explore "magnitude alignment — through, for example, running directly comparable human-subject studies."
  - Why unresolved: The current study only validated the *directional* alignment of biases with the literature because replicating all contextual factors to compare *magnitude* was impossible.
  - What evidence would resolve it: Conducting parallel human-subject experiments using the exact scenarios and personas from the study, then statistically comparing the demographic parity differences (DPD) between human and agent actions.

- Question: What tools or techniques can facilitate the responsible adjustment of implicit biases in language-agent simulations?
  - Basis in paper: [explicit] Section 6.3 suggests future research should focus on "developing tools to facilitate responsible adjustment of these biases."
  - Why unresolved: The paper demonstrates that while some biases are harmful amplifications, others may reflect real-world patterns necessary for predictive simulations, requiring a nuanced approach to mitigation rather than complete removal.
  - What evidence would resolve it: The development and testing of calibration mechanisms that can tune the magnitude of sociodemographic disparities in agent outputs to match specific real-world distributions.

## Limitations
- The study focuses exclusively on binary decisions and single-axis sociodemographic attributes, limiting generalizability to more complex decision scenarios and intersectional identities.
- The amplification finding shows directional alignment with real-world disparities but doesn't quantify the exact amplification factor or establish why amplification occurs.
- The safety alignment asymmetry mechanism is inferred from observed patterns rather than directly tested through experimental validation.

## Confidence
- **High confidence**: The core empirical finding that agent-based simulations reveal significantly more bias than direct prompting (especially in state-of-the-art models). The DPD methodology and statistical significance testing are well-established.
- **Medium confidence**: The claim about safety alignment asymmetry. While the evidence pattern is compelling, direct experimental validation of this mechanism is absent.
- **Medium confidence**: The conclusion that more advanced models exhibit greater implicit biases. This is supported by the data but could reflect model-specific training differences rather than a general trend.

## Next Checks
1. **Mechanism isolation experiment**: Test whether explicitly training safety classifiers on agent-simulation outputs reduces the action-word divergence. This would directly validate or falsify the safety alignment asymmetry hypothesis.
2. **Intersectional bias exploration**: Run the same pipeline with intersectional personas (e.g., "Black female," "conservative Hispanic") to determine if biases compound or interact in predictable ways.
3. **Cross-task generalization**: Apply the technique to non-binary decision scenarios (e.g., multi-option career selection or continuous-scale responses) to test whether the action-word divergence pattern holds beyond binary choices.