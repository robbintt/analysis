---
ver: rpa2
title: Hypergraph Contrastive Learning for both Homophilic and Heterophilic Hypergraphs
arxiv_id: '2511.18783'
source_url: https://arxiv.org/abs/2511.18783
tags:
- hypergraph
- heterophilic
- node
- hyperedge
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitations of existing hypergraph neural
  networks (HNNs) that rely on the homophily assumption, which often does not hold
  in real-world scenarios with significant heterophilic structures. To tackle this,
  the authors propose HONOR, a novel unsupervised hypergraph contrastive learning
  framework designed for both homophilic and heterophilic hypergraphs.
---

# Hypergraph Contrastive Learning for both Homophilic and Heterophilic Hypergraphs

## Quick Facts
- arXiv ID: 2511.18783
- Source URL: https://arxiv.org/abs/2511.18783
- Reference count: 40
- Primary result: HONOR achieves accuracy of 81.42% on Cora-C, 73.23% on Citeseer, and 83.69% on PubMed for node classification tasks

## Executive Summary
This paper introduces HONOR, a novel unsupervised hypergraph contrastive learning framework designed to handle both homophilic and heterophilic hypergraphs. Unlike existing hypergraph neural networks that rely on homophily assumptions, HONOR explicitly models heterophilic relationships through two complementary mechanisms: a prompt-based hyperedge feature construction strategy and an adaptive attention aggregation module. Combined with high-pass filtering, these designs enable HONOR to fully exploit heterophilic connection patterns, yielding more discriminative and robust node and hyperedge representations. The framework demonstrates consistent outperformance over state-of-the-art baselines across multiple benchmark datasets.

## Method Summary
HONOR constructs two complementary views of each hyperedge through prompt-based and attention-based mechanisms, then encodes them using a two-layer high-pass filter on the bipartite graph representation. The prompt-based view computes a central pattern feature for each hyperedge and reweights nodes by cosine similarity to suppress local noise while maintaining global semantic consistency. The adaptive attention module learns node-specific contributions to hyperedges dynamically. High-pass filtering preserves discriminative signals in heterophilic structures by amplifying high-frequency components. The framework is trained with a contrastive loss combined with structural decoupling and covariance regularization, producing final representations through a linear combination of the two encoded views.

## Key Results
- HONOR achieves 81.42% accuracy on Cora-C, 73.23% on Citeseer, and 83.69% on PubMed for node classification tasks
- Outperforms state-of-the-art methods across both homophilic (Cora-C, Citeseer, PubMed, Zoo) and heterophilic (House, IMDB, Twitch, Amazon) hypergraph datasets
- Ablation studies demonstrate the effectiveness of prompt-based construction, adaptive attention, and high-pass filtering components
- Sensitivity analysis shows optimal high-pass filtering coefficients vary significantly between homophilic and heterophilic datasets

## Why This Works (Mechanism)

### Mechanism 1: Prompt-Based Hyperedge Feature Construction
- Claim: Suppresses local noise while maintaining global semantic consistency within each hyperedge
- Mechanism: Computes a central pattern feature for each hyperedge (mean of connected node features), then reweights nodes by cosine similarity to this center. The weighted sum passes through an MLP to produce a prompt vector that is element-wise multiplied with raw hyperedge features, acting as a local consistency filter
- Core assumption: Nodes semantically aligned with the hyperedge center should contribute more; misaligned nodes introduce noise
- Evidence anchors: Abstract mentions "prompt-based hyperedge feature construction strategy that maintains global semantic consistency while suppressing local noise"; Equations 3-5 detail the prompt factor computation and fusion

### Mechanism 2: Adaptive Attention Aggregation
- Claim: Learns node-specific contributions to hyperedges dynamically rather than relying on fixed heuristic weighting
- Mechanism: Uses a trainable scoring function to assign importance scores to each node within a hyperedge, normalized via softmax. This produces attention-weighted hyperedge representations that complement the prompt-based view
- Core assumption: Heterophilic hypergraphs exhibit diverse, learnable contribution patterns that uniform aggregation cannot capture
- Evidence anchors: Abstract mentions "adaptive attention aggregation module that dynamically captures the diverse local contributions of nodes to hyperedges"; Equations 6-7 define the attention mechanism

### Mechanism 3: High-Pass Filtering in Bipartite Representation
- Claim: Preserves discriminative signals in heterophilic structures by amplifying high-frequency components, avoiding oversmoothing
- Mechanism: Encodes the hypergraph as a bipartite graph (nodes ∪ hyperedges) and applies a propagation rule using (I − γL̂) where γ controls high-pass strength. This propagates local difference signals rather than smoothing them
- Core assumption: Heterophily manifests as high-frequency signals that low-pass filters destroy; preserving these improves class separability
- Evidence anchors: Abstract mentions "Combined with high-pass filtering, these designs enable HONOR to fully exploit heterophilic connection patterns"; Table 5 shows increased variance and entropy after training

## Foundational Learning

- Concept: Hypergraph representation (incidence matrix, hyperedges)
  - Why needed here: The entire framework operates on hypergraphs where hyperedges connect arbitrary subsets of nodes; understanding the incidence matrix H and bipartite transformation is prerequisite to following the encoder design
  - Quick check question: Given a hypergraph with N nodes and M hyperedges, can you construct the (N+M)×(N+M) bipartite adjacency matrix from the incidence matrix?

- Concept: Homophily vs. Heterophily in graphs
  - Why needed here: The paper's core contribution is addressing heterophily, where connected nodes have different labels; grasping why standard message passing fails here is essential
  - Quick check question: In a social network hyperedge containing both climate advocates and skeptics, would homophily-based aggregation help or hurt representation quality?

- Concept: Spectral filtering (low-pass vs. high-pass)
  - Why needed here: HONOR replaces traditional low-pass smoothing with high-pass filtering; understanding the Laplacian's role in frequency-domain operations explains why this preserves heterophilic signals
  - Quick check question: In the normalized Laplacian L = I − Â, does a larger eigenvalue correspond to lower or higher frequency, and what does high-pass filtering do to eigenvectors with large eigenvalues?

## Architecture Onboarding

- Component map: Raw features → Degree augmentation → Parallel view generation (prompt + attention) → High-pass encoding per view → Contrastive training → Fused representation for downstream tasks
- Critical path: Raw features → Degree augmentation → Parallel view generation (prompt + attention) → High-pass encoding per view → Contrastive training → Fused representation for downstream tasks
- Design tradeoffs:
  - Two-view construction increases computational overhead but provides complementary signals; ablation shows single-view representations underperform
  - High-pass filtering requires tuning γ coefficients per dataset; optimal values differ between homophilic and heterophilic structures
  - Structural decoupling loss and covariance regularization add hyperparameters; ablation shows decoupling contributes more than covariance
- Failure signatures:
  - Oversmoothed embeddings (low variance, low entropy in node-hyperedge similarity) → likely γ too small or too many layers
  - Noisy, unstable training → check if covariance regularization is too weak, allowing degenerate representations
  - Poor performance on homophilic datasets → γ may be too aggressive, filtering out useful smoothing signals
- First 3 experiments:
  1. Reproduce Cora-C node classification with default hyperparameters; verify accuracy is within ±1% of reported 81.42%
  2. Ablation on House dataset: Remove each component (decoupling, prompt, attention) one at a time; compare to Table 4 results to validate module contributions in a strongly heterophilic setting
  3. γ sensitivity sweep on Zoo vs. House: Replicate Fig. 3c–d patterns to confirm that optimal γ combinations differ between homophilic (Zoo) and heterophilic (House) structures

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can adaptive or learnable frequency filtering that automatically combines low-pass and high-pass components improve performance over the fixed high-pass filtering approach in HONOR?
- Basis in paper: The authors state that "using high-pass filtering alone can amplify specificity, it may introduce noise and weaken the stability of the global structure" and identify "balancing multi-frequency domain information" as a key challenge
- Why unresolved: The paper exclusively uses high-pass filtering without exploring adaptive mechanisms that could dynamically weight different frequency components based on local heterophily patterns or node characteristics
- What evidence would resolve it: Experiments comparing fixed high-pass filtering against learnable frequency-adaptive mechanisms across diverse homophilic and heterophilic hypergraphs

### Open Question 2
- Question: Does the bipartite graph transformation preserve all relevant high-order structural information unique to hypergraphs, or does it lose information that could benefit representation learning?
- Basis in paper: The paper transforms hypergraphs to bipartite graphs for unified message passing, but this transformation may reduce the inherently high-order nature of hyperedge relationships to pairwise membership relations
- Why unresolved: While the bipartite formulation enables efficient encoding, the theoretical analysis focuses on two-community HSBM and does not address whether high-order hyperedge semantics are fully captured
- What evidence would resolve it: Theoretical analysis comparing the expressiveness of bipartite message passing against direct hyperedge-level operations, plus empirical comparisons on synthetic hypergraphs designed to require high-order reasoning

### Open Question 3
- Question: How can the high-pass filtering coefficients (γ^(1), γ^(2)) be automatically determined or adapted based on dataset characteristics without manual tuning?
- Basis in paper: The sensitivity analysis shows "the optimal combination of high-pass filtering coefficients varies significantly depending on the data structure," yet no automatic selection mechanism is proposed
- Why unresolved: The current approach requires dataset-specific hyperparameter tuning, which limits practical applicability and raises questions about generalization to new hypergraphs without labeled validation data
- What evidence would resolve it: Development and validation of a meta-learning or self-tuning mechanism that estimates optimal γ values from structural statistics and achieves performance within a small margin of manually-tuned baselines

## Limitations

- Several critical hyperparameters and architectural details are unspecified in the main text, requiring access to Appendix A.6 for faithful reproduction
- The high-pass filtering mechanism's sensitivity to γ coefficients suggests careful tuning is essential, but optimal settings are not provided in the main text
- Empirical validation is limited to 8 benchmark datasets, and performance on larger, more complex hypergraphs is not demonstrated

## Confidence

- **High**: The core mechanism of combining prompt-based hyperedge construction, adaptive attention aggregation, and high-pass filtering is well-specified and theoretically grounded. The framework's ability to outperform state-of-the-art baselines on diverse homophilic and heterophilic datasets is directly supported by experimental results.
- **Medium**: The effectiveness of the structural decoupling loss and covariance regularization in preventing representation collapse is demonstrated via ablation studies, but the exact contribution of each component may vary across datasets.
- **Low**: The generalizability of HONOR to hypergraphs with significantly different structures or to other downstream tasks beyond node classification and clustering is not extensively validated.

## Next Checks

1. Reproduce Cora-C node classification: Replicate the 81.42% accuracy result using default hyperparameters to confirm implementation correctness and baseline performance
2. Ablation on House dataset: Remove each component (decoupling, prompt, attention) one at a time to validate module contributions in a strongly heterophilic setting, comparing results to Table 4
3. γ sensitivity sweep: Replicate Figure 3c–d patterns on Zoo (homophilic) and House (heterophilic) to confirm that optimal γ combinations differ between homophilic and heterophilic structures, ensuring the high-pass filter is tuned appropriately