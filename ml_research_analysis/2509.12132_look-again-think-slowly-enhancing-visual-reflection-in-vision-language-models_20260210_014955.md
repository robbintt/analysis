---
ver: rpa2
title: 'Look Again, Think Slowly: Enhancing Visual Reflection in Vision-Language Models'
arxiv_id: '2509.12132'
source_url: https://arxiv.org/abs/2509.12132
tags:
- visual
- reasoning
- arxiv
- reflection
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the problem of visual reflection in vision-language
  reasoning models (VRMs), which refers to the ability of models to check and refine
  their reasoning process based on visual information. Current VRMs struggle with
  visual reflection, as their attention to visual tokens diminishes rapidly during
  long reasoning chains, leading to visual neglect and hallucinations.
---

# Look Again, Think Slowly: Enhancing Visual Reflection in Vision-Language Models

## Quick Facts
- **arXiv ID**: 2509.12132
- **Source URL**: https://arxiv.org/abs/2509.12132
- **Reference count**: 36
- **Primary result**: Two-stage training strategy significantly improves visual reflection in VLMs, reducing hallucinations and boosting performance across multiple benchmarks.

## Executive Summary
This paper addresses the problem of visual reflection in vision-language reasoning models (VRMs), where models struggle to maintain visual grounding during extended reasoning chains. The authors propose a two-stage training strategy: first, constructing vision-centered reasoning data through LLM-VLM interaction to embed visual reflection patterns, then applying reinforcement learning with a visual attention-based reward to encourage sustained focus on visual tokens. The resulting Reflection-V model demonstrates significant improvements over existing VRMs on benchmarks like MathVision, MMMU, and M3CoT, while also showing reduced hallucination rates through better visual grounding.

## Method Summary
The method employs a two-stage training approach. Stage 1 uses supervised fine-tuning on data generated through an agent pipeline where a Visual Requester (LLM) queries a Visual Expert (VLM) for specific visual details during reasoning. This creates reasoning chains that naturally incorporate visual verification steps. Stage 2 applies Group Relative Policy Optimization (GRPO) with a modified reward function that includes an accuracy reward, format reward, and crucially, a visual attention-based reward. This reward encourages the model to maintain attention to visual tokens throughout generation by computing the ratio of attention in the second half versus first half of generation. The approach is implemented on Qwen2.5-VL-7B-Instruct with specific hyperparameters for both training stages.

## Key Results
- Reflection-V achieves state-of-the-art performance on MathVision, MMMU, and M3CoT benchmarks
- The model demonstrates significantly fewer visual hallucinations compared to baseline models
- Visual Dependency Measure (VDM) shows stronger and more consistent reliance on visual information during reasoning
- Attention analysis reveals the model maintains visual focus throughout extended reasoning chains rather than decaying

## Why This Works (Mechanism)

### Mechanism 1: Visual Attention-Based Reward Shaping
If a reinforcement learning reward explicitly penalizes the decay of attention weights on visual tokens, the model appears to maintain visual grounding for longer reasoning chains. The standard GRPO objective is modified with a reward term $r_v$ that calculates the ratio of attention paid to visual tokens in the second half of generation versus the first half. By optimizing this ratio, the gradient descent process selects for policy parameters that resist the natural tendency of transformers to focus exclusively on recently generated text tokens as context length grows.

### Mechanism 2: Cold-Start Behavioral Patterning via Agent Interaction
Supervised Fine-Tuning (SFT) on data generated by interacting LLMs and VLMs likely establishes the initial behavioral pattern of "looking back" at the image during reasoning. Instead of using static image captions, the authors use an agent loop where an LLM "Visual Requester" explicitly queries a VLM "Visual Expert" for specific visual details mid-reasoning. Training on these interaction logs teaches the model the syntax and timing of visual verification.

### Mechanism 3: Mitigation of Hallucination via Visual Re-focusing
Sustained visual attention appears to serve as a regularizer against text-only hallucinations. Standard VLMs often "forget" the image and hallucinate facts to maintain coherence in long text generation. By forcing the model to re-attend to visual tokens, the generation process is grounded back to the actual pixel data, correcting drift.

## Foundational Learning

- **Concept: Transformer Attention Distribution** - Why needed: The core diagnosis is that attention to visual tokens decays as sequence length increases. You must understand how softmax attention distributes probability mass over a context window to grasp why "visual forgetting" happens naturally in standard transformers. Quick check: In a standard decoder-only transformer, why might a model attend more to immediately preceding text tokens than to image tokens placed at the start of the sequence?

- **Concept: Group Relative Policy Optimization (GRPO)** - Why needed: The paper uses GRPO, a variant of PPO, to optimize the model. Unlike standard RLHF which uses a reward model trained on preferences, GRPO often uses rule-based rewards. Quick check: How does GRPO differ from standard PPO in terms of how it estimates advantages?

- **Concept: Visual Dependency Measure (VDM)** - Why needed: The authors introduce VDM to quantify how much the model relies on the image. It measures the divergence in next-token prediction probability with vs. without the image. Quick check: If VDM is low (near zero) for a specific question, what does that imply about the model's reasoning process for that question?

## Architecture Onboarding

- **Component map**: Qwen2.5-VL-72B (VLM) + QWQ-32B (LLM) -> Agent Pipeline -> Cold-Start Dataset -> Qwen2.5-VL-7B-Instruct -> Verl framework -> GRPO Trainer -> Reward Calculator (Accuracy + Format + Visual Attention) -> Reflection-V

- **Critical path**: The implementation of the Visual Attention Reward. You must hook into the forward pass to extract attention weights from specific layers (the paper notes the last layer is most significant) for visual tokens specifically, compute the ratio in equation (4), and pass this as a scalar reward to the GRPO updater.

- **Design tradeoffs**: Caption vs. Interaction Data - the paper argues against caption-based data because it severs the visual link. The tradeoff is complexity; the interaction pipeline is slower and harder to debug than simple captioning. Reward Mixing - the total reward is $r_o = r_a + \lambda_v r_v + \lambda_f r_f$. Setting $\lambda_v$ too high might force the model to look at the image constantly, potentially disrupting logical text flow.

- **Failure signatures**: Attention Hacking - the model learns to place high attention on visual tokens but processes them as "null" or noise to satisfy the reward without integrating the visual info. Degraded Text Logic - over-penalizing text-focus might hurt performance on logic-heavy, visually-light tasks.

- **First 3 experiments**:
  1. Sanity Check - VDM Decay: Train a baseline SFT model and plot the Visual Dependency Measure (VDM) over token generation steps to reproduce the "forgetting" curve.
  2. Reward Ablation: Run GRPO with only the accuracy reward vs. only the visual attention reward vs. combined. Check if the visual reward alone can drive performance or if it creates a model that stares at the image but fails to answer.
  3. Cold-Start Validation: Compare training on "Caption-based" reasoning data vs. the proposed "Agent-Interaction" data. Verify that the interaction data results in reasoning chains containing explicit "re-check" phrases.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does the visual reflection capability scale effectively to vision-language models beyond 7B parameters? Basis: The authors state in limitations they could not conduct experiments with models larger than 7B parameters due to computational constraints. Why unresolved: Computational resources limited experiments to smaller models; unknown whether attention-based reward mechanisms behave similarly in larger parameter regimes.

- **Open Question 2**: Does Reflection-V generalize to visual reasoning tasks requiring different types of visual evidence (e.g., temporal reasoning, 3D spatial understanding, video)? Basis: The authors note their evaluation involves "relatively limited categories of visual-language datasets" and plan future work to include more diverse problem types. Why unresolved: Current benchmarks focus on static images with mathematical, multi-disciplinary, and general reasoning; visual reflection may manifest differently when visual information changes over time or involves depth.

- **Open Question 3**: How sensitive is the visual attention-based reward formulation to the choice of attention layers and the ratio computation window? Basis: The reward uses only the "last layer where attention to visual tokens is most significant" and computes a ratio between second-half and first-half generation. This specific design choice lacks ablation. Why unresolved: Different layers may encode different types of visual information; the midpoint split for ratio computation is arbitrary and may not capture when visual reflection naturally occurs.

## Limitations

- The paper focuses on relatively limited categories of visual-language datasets, primarily static images with mathematical and multi-disciplinary reasoning tasks.
- Computational constraints limited experiments to 3B and 7B parameter scales, leaving scalability to larger models unexplored.
- The causal link between attention weights and actual visual information utilization is not definitively proven, raising concerns about potential reward hacking.

## Confidence

- **High Confidence**: The observation that visual attention decays during extended reasoning chains is well-supported by quantitative attention analysis and aligns with established transformer behavior patterns. The implementation details of the training pipeline appear sufficiently specified for reproduction.

- **Medium Confidence**: The claim that sustained visual attention reduces hallucinations is supported by VDM metrics and benchmark improvements, but the causal mechanism is not definitively proven. The superiority of the agent-interaction data construction method over simpler alternatives is demonstrated through benchmark performance but lacks direct ablation studies.

- **Low Confidence**: The specific attribution of performance gains to the visual attention reward component versus the cold-start data quality is difficult to disentangle. The paper does not provide clear evidence that the "Visual Aha Moment" represents genuine visual insight versus a learned behavioral pattern.

## Next Checks

1. **Attention Utilization Validation**: Design an experiment where the model's attention to visual tokens is decoupled from their actual semantic content (e.g., by masking visual tokens or using randomized visual features). If the model still receives high visual attention rewards without genuine visual reasoning, this would reveal reward hacking and undermine the proposed mechanism.

2. **Direct Hallucination Evaluation**: Conduct a human evaluation study where annotators rate the frequency and severity of visual hallucinations in model outputs. Compare Reflection-V against baselines on a curated set of visually-grounded questions where hallucination would be clearly identifiable, providing direct evidence for the claimed hallucination reduction.

3. **Cold-Start Ablation**: Implement and train models using alternative cold-start data construction methods: (a) simple caption-then-reason data, (b) agent-interaction data without the cohesion enhancement pass, and (c) randomly generated reasoning chains with embedded "check the image" phrases. Compare the resulting models' ability to maintain visual attention and their performance on benchmarks to isolate the contribution of each data construction element.