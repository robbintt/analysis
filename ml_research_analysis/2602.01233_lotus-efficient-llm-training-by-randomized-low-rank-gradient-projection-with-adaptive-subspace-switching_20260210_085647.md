---
ver: rpa2
title: 'Lotus: Efficient LLM Training by Randomized Low-Rank Gradient Projection with
  Adaptive Subspace Switching'
arxiv_id: '2602.01233'
source_url: https://arxiv.org/abs/2602.01233
tags:
- arxiv
- low-rank
- gradient
- training
- subspace
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Lotus, a method for efficient LLM training
  that addresses the memory-computation trade-off in large-scale models. Lotus improves
  upon GaLore by using randomized low-rank gradient projection combined with an adaptive
  subspace switching strategy, which dynamically updates the low-rank basis based
  on unit-gradient displacement rather than fixed intervals.
---

# Lotus: Efficient LLM Training by Randomized Low-Rank Gradient Projection with Adaptive Subspace Switching

## Quick Facts
- arXiv ID: 2602.01233
- Source URL: https://arxiv.org/abs/2602.01233
- Reference count: 0
- Key outcome: Reduces memory by 40% and training time by 30% vs GaLore using adaptive low-rank gradient projection with randomized SVD

## Executive Summary
Lotus addresses the memory-computation trade-off in large language model training by combining randomized low-rank gradient projection with an adaptive subspace switching strategy. The method dynamically updates the low-rank basis based on unit-gradient displacement tracking, switching subspaces only when necessary to avoid premature or delayed updates. Experimental results show Lotus achieves 40% memory reduction and 30% training time improvement compared to GaLore while maintaining competitive performance on both pre-training (LLaMA on C4) and fine-tuning (RoBERTa on GLUE) tasks.

## Method Summary
Lotus extends GaLore by introducing randomized SVD for faster gradient decomposition and an adaptive subspace switching policy based on unit gradient displacement tracking. The method projects gradients onto a low-rank subspace, optimizes within that subspace using Adam, and dynamically updates the projection basis when the path-efficiency metric falls below a threshold. The adaptive policy triggers subspace recomputation only when gradients show directional inconsistency, avoiding the fixed-interval switching of GaLore while maintaining convergence guarantees under stated assumptions.

## Key Results
- 40% memory reduction compared to GaLore through efficient low-rank gradient representation
- 30% training time improvement via randomized SVD and adaptive subspace switching
- 85.89 average GLUE score with rank=4, outperforming GaLore (83.93) and full-rank (85.03) baselines
- Maintains 30.5 perplexity on C4 dataset with significant memory savings across LLaMA model sizes

## Why This Works (Mechanism)

### Mechanism 1: Randomized SVD for Fast Gradient Decomposition
Uses power-iteration-based randomized SVD to decompose gradient matrices into low-rank components, avoiding the super-linear computational scaling of exact SVD with matrix size. This achieves comparable projection quality while accelerating the gradient projection step by orders of magnitude.

### Mechanism 2: Adaptive Subspace Switching via Unit Gradient Displacement
Tracks the average displacement of unit-norm gradients to detect when the current low-rank subspace becomes stale. When the path-efficiency ratio drops below threshold γ (indicating directional inconsistency), triggers subspace recomputation only when combined with minimum interval T_min constraint.

### Mechanism 3: Convergence Guarantee via Path-Efficiency Bound
Theorem 3.2 claims the adaptive switching policy achieves the same gradient tolerance in fewer iterations than fixed-interval policies under L-smoothness assumptions and appropriate learning rate constraints.

## Foundational Learning

- **Concept: Low-Rank Matrix Decomposition (SVD)**
  - Why needed here: Core to understanding how gradients are compressed into low-rank factors
  - Quick check question: If a gradient matrix is 4096×4096 and rank=128 is used, what are the memory savings for storing the low-rank factors vs. full matrix?

- **Concept: Gradient Projection and Subspace Optimization**
  - Why needed here: Lotus projects full-rank gradients onto a low-rank subspace, optimizes in that subspace, then projects updates back
  - Quick check question: Why does projecting gradients (not weights) to low-rank allow full-rank weight updates?

- **Concept: Optimizer State Memory in Adam**
  - Why needed here: Adam stores first and second moment estimates per parameter. Lotus reduces this by storing optimizer states in low-rank format
  - Quick check question: For a weight matrix W ∈ R^{m×n}, how much memory does Adam typically require beyond the weights themselves?

## Architecture Onboarding

- **Component map:**
Full-rank Gradient (G_F) -> Randomized SVD Projector -> Low-rank Gradient (G_cur) -> Displacement Tracker -> Switch Controller -> Subspace Recomputation -> Low-rank Optimizer Update -> Project Back -> Weight Update

- **Critical path:**
1. Initialize: Compute first projection O_G via rSVD on initial gradient; store d_init
2. Per iteration: Project G_cur = O_G · G_F; normalize to get d_cur
3. Every η steps: Compute average displacement ā; if ā < γ AND steps since last switch ≥ T_min → trigger subspace update
4. On switch: Recompute O_G, reset d_init, reset counter

- **Design tradeoffs:**
- γ (threshold): Lower = fewer switches (more exploitation per subspace), higher = more switches (faster adaptation). Paper recommends 0.005-0.02.
- η (verifying gap): How often to check displacement. Smaller = more responsive but more overhead. Paper recommends 25-100 steps.
- T_min: Minimum interval between switches. Prevents thrashing during noisy phases.
- Rank r: Memory vs. expressiveness tradeoff. Paper uses r/d_model ratios of 0.25-0.5.

- **Failure signatures:**
- Loss oscillation or divergence: γ too high causing excessive switching; increase T_min or decrease γ
- Slower convergence than baseline: Rank too low; increase r/d_model ratio
- No memory savings observed: Optimizer states not being stored in low-rank format
- Training time not improved: rSVD not being used

- **First 3 experiments:**
1. Sanity check - Pre-train LLaMA-60M on C4 subset (100M tokens). Compare perplexity, peak memory, and tokens/sec vs. GaLore and full-rank.
2. Ablation - Adaptive vs. Fixed switching: Compare Lotus (adaptive) against Lotus with fixed intervals (GaLore-style). Measure switch count, convergence speed, final perplexity.
3. Hyperparameter sweep on γ and η: Grid search γ ∈ {0.005, 0.01, 0.015, 0.02} and η ∈ {25, 50, 75, 100} on GLUE fine-tuning. Plot avg GLUE score vs. training time.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust are the recommended hyperparameter ranges for the threshold γ and verifying gap η across diverse model scales and datasets?
- Basis in paper: The methodology states these parameters "should be set within" specific ranges "to avoid too frequent or few updates," but the experiments only validate specific settings on limited model sizes.
- Why unresolved: The suggested ranges may be optimal only for the tested LLaMA (60M-1B) and RoBERTa setups; their transferability to significantly larger models (7B+) or different architectures is unverified.
- What evidence would resolve it: A sensitivity analysis plotting final perplexity/accuracy against varying γ and η values for larger models (e.g., LLaMA-7B).

### Open Question 2
- Question: Does Lotus reduce communication overhead in distributed training environments (e.g., DDP, FSDP), or does adaptive switching introduce synchronization bottlenecks?
- Basis in paper: The paper focuses on single-device memory and time metrics. While low-rank gradients can reduce communication, the algorithm's dynamic subspace updates could complicate synchronization if triggers differ across devices.
- Why unresolved: The current evaluation does not measure wall-clock time in multi-node settings or analyze the impact of stochastic switching triggers on gradient consistency across workers.
- What evidence would resolve it: Scaling experiments on multi-GPU clusters showing iteration time and convergence curves compared to standard GaLore and full-rank baselines.

### Open Question 3
- Question: Can the fixed-rank constraint be extended to an adaptive rank mechanism without destabilizing the unit gradient displacement tracking?
- Basis in paper: The method uses a fixed rank r in experiments. The switching criterion tracks directional drift (ρ_t) but does not assess if the subspace capacity (r) itself is sufficient or excessive for the current training stage.
- Why unresolved: Gradients often exhibit decreasing intrinsic dimensionality during training. Maintaining a fixed r might waste memory late in training or restrict learning early on.
- What evidence would resolve it: An ablation study comparing fixed-rank Lotus against a variant that dynamically adjusts r based on singular value decay or gradient energy retention.

## Limitations

- Theoretical guarantees untested at scale: Convergence theorem relies on idealized assumptions that may not hold with adaptive optimizers in practice
- Hyperparameter sensitivity not fully characterized: Critical dependence on γ, η, and T_min across different model scales and tasks
- Randomized SVD approximation quality: No quantitative comparison of approximation error or impact on optimization trajectory

## Confidence

- **High confidence**: Memory savings claims (40%) and training time reduction (30%) are well-supported by experimental results across multiple model sizes
- **Medium confidence**: Adaptive switching policy's effectiveness demonstrated empirically, but theoretical justification relies on assumptions that may not generalize
- **Low confidence**: Convergence guarantee theorem is least validated claim, depends on idealized assumptions and lacks empirical verification

## Next Checks

1. **Convergence speed validation**: Run LLaMA-1B pre-training on C4 with both Lotus and GaLore for the same number of steps. Measure validation perplexity progression to empirically verify if adaptive switching achieves faster convergence.

2. **Randomized SVD error analysis**: For the same rank settings, compare singular value distributions and reconstruction errors between exact SVD and rSVD on gradient matrices across different training stages. Quantify impact on optimization trajectory by measuring cosine similarity between exact and approximated gradient directions.

3. **Hyperparameter robustness study**: Systematically vary γ (0.001 to 0.05) and η (10 to 200) on RoBERTa-Base GLUE fine-tuning. Plot Pareto frontiers of average GLUE score vs. training time and switch count to identify optimal operating regions.