---
ver: rpa2
title: 'Leak@$k$: Unlearning Does Not Make LLMs Forget Under Probabilistic Decoding'
arxiv_id: '2511.04934'
source_url: https://arxiv.org/abs/2511.04934
tags:
- unlearning
- leak
- decoding
- arxiv
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses a critical vulnerability in current large
  language model (LLM) unlearning methods. While prior evaluations using deterministic
  decoding suggest successful removal of sensitive knowledge, the authors demonstrate
  that such information reliably resurfaces under realistic probabilistic decoding.
---

# Leak@$k$: Unlearning Does Not Make LLMs Forget Under Probabilistic Decoding

## Quick Facts
- arXiv ID: 2511.04934
- Source URL: https://arxiv.org/abs/2511.04934
- Reference count: 40
- Key outcome: Current unlearning methods fail under realistic probabilistic decoding, with leakage increasing sharply with sample count k

## Executive Summary
This paper reveals a critical vulnerability in large language model unlearning: methods that appear successful under deterministic greedy decoding fail to truly erase sensitive knowledge when realistic probabilistic decoding is applied. The authors introduce leak@k, a meta-metric measuring the likelihood of forgotten information reappearing when generating k samples per prompt. Evaluating across three benchmarks with multiple unlearning methods, they find that leak@k increases sharply with k for nearly all methods, demonstrating that current approaches provide only an illusion of forgetting. The parameter top-p emerges as the primary driver of leakage, with temperature playing a secondary role.

## Method Summary
The authors evaluate LLM unlearning methods under probabilistic decoding by generating n=200 samples per prompt using configurable temperature and top-p parameters. They compute core metric scores (Entailment Score, ROUGE-L, Accuracy, LLM-as-Judge) for each generation against gold answers, then apply an unbiased estimator to compute leak@k across k values from 1 to 128. The evaluation spans three benchmarks (TOFU, MUSE, WMDP) and multiple unlearning methods including NPO, SimNPO, RMU, and gradient-based approaches. Models tested range from LLaMA-3.2-1B to Zephyr-7B-beta.

## Key Results
- leak@k increases sharply with k for nearly all unlearning methods and decoding configurations
- top-p parameter is the primary driver of leakage, more influential than temperature
- Current SOTA unlearning methods provide only an illusion of forgetting under realistic deployment conditions
- NPO+ENT shows reduced but not eliminated leakage, suggesting entropy regularization partially mitigates the issue

## Why This Works (Mechanism)

### Mechanism 1: Residual Probability Mass Under Probabilistic Decoding
Current unlearning methods suppress sensitive tokens only for the highest-probability path (greedy decoding) but fail to remove probability mass from the broader token distribution. Token-level loss functions don't enforce uniform distributional erasure across the full vocabulary, leaving a long tail of sensitive tokens. When probabilistic decoding is applied, sampling explores lower-probability tokens where residual sensitive content persists.

### Mechanism 2: Top-p as Primary Leakage Driver
The top-p sampling parameter is the dominant factor controlling whether suppressed knowledge resurfaces. Top-p restricts sampling to the smallest token set exceeding cumulative probability p, with high p expanding the candidate pool to include low-probability sensitive tokens. Even with low temperature, high p retains "long tail" sensitive options.

### Mechanism 3: Multi-Sample Amplification of Leakage (leak@k)
The probability of at least one leaking response increases sharply with the number of samples k, even when individual leakage probability is low. leak@k computes E[max(S₁...Sₖ)] over k i.i.d. samples, capturing worst-case exposure under repeated querying.

## Foundational Learning

- **Autoregressive Token Sampling**: Understanding how temperature and top-p shape token distributions is essential to grasp why greedy decoding hides residual knowledge. Quick check: Given logits [2.0, 1.0, 0.5] and T=1.0, what is the probability of selecting the second token? How does T=0.5 change this?

- **Unlearning Loss Functions (NPO, GradAscent, RMU)**: The paper evaluates multiple unlearning objectives; knowing their mechanisms explains why they fail under sampling. Quick check: Does NPO optimize for absolute probability reduction or relative preference shift compared to a reference model?

- **Statistical Estimators (Unbiasedness, Variance)**: The paper proposes unbiased estimators for leak@k; understanding bias-variance tradeoff clarifies why the chosen estimator is preferred. Quick check: Why does the naive worst-k estimator have higher variance than the integrated estimator L̂_k?

## Architecture Onboarding

- **Component map**: Unlearning Method Trainer -> Probabilistic Decoder -> Core Metric Evaluator -> leak@k Aggregator -> Benchmarks
- **Critical path**: 1) Train/unlearn model using chosen method, 2) Generate n=200 samples per prompt with target (T, p), 3) Compute core metric scores for each generation, 4) Apply leak@k estimator across k=1,2,4,...,128, 5) Analyze leakage growth curves and retain-set utility
- **Design tradeoffs**: Token-level vs. sequence-level objectives; low vs. high decoding randomness; core metric choice (speed vs. semantic accuracy)
- **Failure signatures**: Flat leak@k curve at low values (strong unlearning); sharp leak@k increase with k (residual tail probability); high leakage even at low k (near-complete failure); retain-set collapse (over-aggressive unlearning)
- **First 3 experiments**: 1) Baseline reproduction: NPO on TOFU forget10, leak@k-ES with (T=0.2, p=1.0), verify monotonic increase, 2) Top-p ablation: Fix T=1.0, vary p ∈ {0.2, 0.5, 0.8, 1.0}, measure leak@k-ES at k=64, 3) Entropy regularization test: Compare NPO vs. NPO+ENT on MUSE-News, k=32, (T=0.8, p=1.0)

## Open Questions the Paper Calls Out

- **Open Question 1**: How can unlearning algorithms be redesigned to ensure robustness against probabilistic decoding without relying on brittle greedy evaluation? The paper demonstrates current methods fail but doesn't propose a theoretically sound algorithmic fix that survives high stochasticity.

- **Open Question 2**: Can unlearning objectives be formulated to minimize sequence-level probability mass rather than token-level cross-entropy loss? Current token-level optimization doesn't guarantee low joint probability of sensitive sequences.

- **Open Question 3**: Does the vulnerability persist under adversarial prompting or extraction attacks? The paper evaluates standard sampling but doesn't explore adversary manipulation of decoding parameters or prompts to maximize leakage.

- **Open Question 4**: Is it computationally feasible to estimate leak@k without generating hundreds of samples per prompt? While the unbiased estimator is efficient, generating large k for every prompt may be prohibitive for large-scale evaluation.

## Limitations

- Generalization beyond evaluated benchmarks may be limited due to benchmark-specific artifacts in prompt formats and evaluation metrics
- Estimator variance at large k values may affect interpretation of small differences between methods
- Results may not scale predictably to larger models (70B+ parameters) where different scaling behaviors could emerge

## Confidence

- **High confidence**: Core finding that probabilistic decoding reveals significant leakage missed by greedy decoding is well-supported across all benchmarks and methods
- **Medium confidence**: Ranking of top-p as more influential than temperature is demonstrated empirically but may be context-dependent
- **Low confidence**: Overgeneralization that "current techniques provide only an illusion of forgetting" from the specific evaluated methods

## Next Checks

1. **Cross-benchmark leakage consistency test**: Evaluate same unlearning methods on diverse benchmarks (MMLU, TruthfulQA, multilingual datasets) to verify leak@k patterns generalize beyond the three studied domains

2. **Variance and confidence interval analysis**: Compute standard errors and confidence intervals for leak@k estimates across multiple random seeds to quantify statistical significance of observed differences

3. **Scaling behavior investigation**: Test unlearning methods on models spanning 1B to 70B parameters using identical benchmarks to reveal whether vulnerabilities scale predictably with model size