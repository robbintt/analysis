---
ver: rpa2
title: Evaluating Compositional Generalisation in VLMs and Diffusion Models
arxiv_id: '2508.20783'
source_url: https://arxiv.org/abs/2508.20783
tags:
- clip
- diffusion
- relational
- cube
- left
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether diffusion model-based classifiers
  exhibit improved compositional generalisation compared to traditional discriminative
  vision-language models (VLMs). The authors extend the Concept Binding Benchmark
  to evaluate models on attribute-object binding and relational composition tasks
  in both zero-shot learning (ZSL) and generalised zero-shot learning (GZSL) settings
  using synthetic images.
---

# Evaluating Compositional Generalisation in VLMs and Diffusion Models

## Quick Facts
- arXiv ID: 2508.20783
- Source URL: https://arxiv.org/abs/2508.20783
- Reference count: 16
- Primary result: Diffusion classifier achieves 99.47% accuracy on out-of-distribution single-object tasks

## Executive Summary
This paper investigates whether diffusion model-based classifiers exhibit improved compositional generalisation compared to traditional discriminative vision-language models. The authors extend the Concept Binding Benchmark to evaluate models on attribute-object binding and relational composition tasks in both zero-shot and generalised zero-shot learning settings using synthetic images. They compare three models: Diffusion Classifier (based on Stable Diffusion), CLIP, and ViLT, finding that while some models excel at attribute-object binding, all struggle significantly with relational tasks.

## Method Summary
The study extends the Concept Binding Benchmark to evaluate compositional generalisation across three models: a diffusion classifier based on Stable Diffusion, CLIP, and ViLT. The evaluation uses synthetic images and tests both zero-shot learning (ZSL) and generalised zero-shot learning (GZSL) settings. Models are assessed on attribute-object binding tasks and relational composition tasks involving spatial relations like left and right. The authors analyze embedding distributions to understand why models fail on relational tasks, particularly examining the disentanglement of relational concepts in CLIP embeddings.

## Key Results
- Diffusion Classifier generalises best in single-object settings, achieving 99.47% accuracy on out-of-distribution test data
- ViLT performs exceptionally well on two-object tasks, achieving over 99% accuracy in both ZSL and GZSL settings
- All models struggle significantly with relational tasks, with accuracies dropping to chance levels in GZSL setting

## Why This Works (Mechanism)
The diffusion classifier's superior performance on single-object tasks likely stems from its generative pretraining, which provides richer feature representations compared to discriminative models. The generative process may capture more nuanced visual concepts that facilitate compositional reasoning. ViLT's exceptional performance on two-object tasks suggests its transformer-based architecture effectively handles multi-object composition through self-attention mechanisms. The failure on relational tasks appears to result from insufficient disentanglement of spatial concepts in embedding space, where concepts like "left" and "right" are not sufficiently separated to enable reliable composition.

## Foundational Learning
- Zero-shot learning (ZSL): Classification without seeing examples of target classes during training; needed to test true generalisation capability
- Generalised zero-shot learning (GZSL): Classification with both seen and unseen classes during inference; quick check: models must balance between familiar and novel compositions
- Compositional generalisation: Ability to understand novel combinations of known concepts; quick check: test with unseen attribute-object pairs
- Concept binding: Associating attributes with objects; quick check: evaluate on attribute-object pairs not seen during training
- Relational composition: Combining objects with spatial relationships; quick check: test with novel spatial configurations

## Architecture Onboarding

### Component Map
Diffusion Classifier: Stable Diffusion -> Classifier Head -> Classification Output
CLIP: Vision Encoder -> Text Encoder -> Multi-modal Embedding Space -> Classification
ViLT: Vision Patches -> Transformer Encoder -> Classification Head -> Classification Output

### Critical Path
For Diffusion Classifier: Image generation through diffusion process -> Feature extraction -> Classification decision
For CLIP: Image embedding -> Text embedding -> Similarity computation in shared space -> Classification
For ViLT: Patch embedding -> Self-attention computation -> Classification prediction

### Design Tradeoffs
Generative pretraining (diffusion) vs discriminative training (CLIP, ViLT): generative models capture richer visual semantics but require more compute
Transformer-based architectures (ViLT) vs hybrid approaches (CLIP): transformers excel at multi-object composition but may struggle with spatial reasoning
Zero-shot vs generalised zero-shot settings: ZSL tests pure generalisation while GZSL requires balancing between seen and unseen concepts

### Failure Signatures
Relational concept entanglement in embedding space
Chance-level performance on novel spatial compositions
Difficulty balancing between seen and unseen concepts in GZSL

### First 3 Experiments
1. Evaluate attribute-object binding accuracy in ZSL setting
2. Test two-object composition performance in GZSL setting  
3. Analyze embedding distributions for relational concepts

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic benchmark images may not capture real-world complexity
- Evaluation focuses primarily on accuracy without exploring robustness
- Analysis of relational failures remains correlational rather than causal

## Confidence
- Diffusion classifier superiority for single-object tasks: High
- ViLT's performance on two-object tasks: High
- Analysis of relational concept failures: Medium
- Broader implications for real-world generalisation: Low

## Next Checks
1. Replicate experiments using natural images with human-annotated compositional relationships
2. Conduct ablation studies varying training samples and concept types
3. Perform controlled experiments with perturbed embeddings to test causal relationships between concept disentanglement and performance