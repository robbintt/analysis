---
ver: rpa2
title: Improving Brain-to-Image Reconstruction via Fine-Grained Text Bridging
arxiv_id: '2505.22150'
source_url: https://arxiv.org/abs/2505.22150
tags:
- text
- reconstruction
- image
- semantic
- fine-grained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Fine-grained Brain-to-Image reconstruction
  (FgB2I), which addresses the problem of insufficient semantic details in brain-to-image
  reconstruction by employing fine-grained text descriptions as a bridge. The core
  method uses large vision-language models to generate detailed captions for visual
  stimuli and trains a brain-to-text model using three reward metrics (object accuracy,
  text-image semantic similarity, and image-image semantic similarity) guided by reinforcement
  learning.
---

# Improving Brain-to-Image Reconstruction via Fine-Grained Text Bridging

## Quick Facts
- **arXiv ID:** 2505.22150
- **Source URL:** https://arxiv.org/abs/2505.22150
- **Reference count:** 15
- **Primary result:** FgB2I improves brain-to-image reconstruction accuracy across multiple metrics (SSIM, Alexnet, InceptionV3, CLIP) when applied to LDM, BrainDiffuser, and MindEye methods.

## Executive Summary
This paper addresses the problem of insufficient semantic details in brain-to-image reconstruction by employing fine-grained text descriptions as a bridge between brain activity and image generation. The core method uses large vision-language models to generate detailed captions for visual stimuli and trains a brain-to-text model using three reward metrics guided by reinforcement learning. The fine-grained text descriptions are then integrated with existing reconstruction methods to improve reconstruction quality. Experimental results show that FgB2I improves reconstruction accuracy across multiple metrics when applied to LDM, BrainDiffuser, and MindEye methods, with the largest improvements observed in LDM which relies solely on text guidance.

## Method Summary
The FgB2I framework operates in three stages: First, LLaVA generates fine-grained captions for visual stimuli to replace sparse original descriptions. Second, a unified brain-to-text model with prefix tuning decodes text from fMRI signals using cross-entropy loss followed by REINFORCE-based reinforcement learning with three reward metrics (object accuracy, text-image semantic similarity, image-image semantic similarity). Third, the decoded fine-grained text embeddings are fused with existing method features via weighted averaging to guide the reconstruction diffusion models. The approach is evaluated on the NSD dataset using subjects 1, 2, 5, and 7, with integration into LDM, BrainDiffuser, and MindEye reconstruction methods.

## Key Results
- FgB2I improves reconstruction accuracy across multiple metrics (SSIM, Alexnet, InceptionV3, CLIP) for all three tested methods
- LDM shows the largest improvement as it relies solely on text guidance, while BrainDiffuser and MindEye show smaller gains due to existing rich semantic conditions
- Visual examples demonstrate correction of semantic inconsistencies and addition of missing details like railings omitted in original reconstructions
- All three reward metrics contribute to improved reconstruction, with L1 and L2 improving semantic alignment while L3 enhances high-level metrics

## Why This Works (Mechanism)

### Mechanism 1: Detail Enhancement via Large Vision-Language Models
- **Claim:** Replacing sparse original captions with fine-grained descriptions generated by LVLMs improves reconstruction quality by providing richer semantic targets.
- **Mechanism:** LLaVA generates comprehensive captions that capture object attributes, spatial relationships, and visual details omitted in original NSD captions. These enhanced captions serve as denser supervision signals for the brain-to-text model.
- **Core assumption:** The brain encodes fine-grained visual details that survive the fMRI recording process and can be decoded when provided with sufficiently detailed text targets.
- **Evidence anchors:** Standard captions often omit critical details (e.g., wall paintings and metallic objects in Figure 2(a)) that humans naturally perceive.

### Mechanism 2: Reinforced Co-Training with Non-Differentiable Rewards
- **Claim:** Three reward metrics guide the language model to decode semantically aligned text from fMRI when combined via the REINFORCE algorithm.
- **Mechanism:** Cross-entropy loss provides baseline text generation capability; reward-based loss then shapes the output distribution toward semantic fidelity. Object accuracy ensures noun recovery; CLIP-based similarities enforce semantic alignment.
- **Core assumption:** The REINFORCE policy gradient can effectively optimize through the discrete text generation bottleneck without causing training instability.
- **Evidence anchors:** L1 and L2 improve semantic alignment... L3 enhances high-level metrics such as Incep and CLIP while maintaining a good balance with low-level metrics.

### Mechanism 3: Semantic Fusion for High-Level Reconstruction Control
- **Claim:** Fusing CLIP embeddings of decoded fine-grained text with existing high-level semantic features improves reconstruction without degrading low-level structural fidelity.
- **Mechanism:** Weighted averaging combines decoded text embeddings with original method embeddings. The fused representation replaces high-level semantics while preserving low-level VAE reconstruction pathways.
- **Core assumption:** The fine-grained text embeddings capture complementary information to the original high-level features without introducing conflicting gradients during diffusion guidance.
- **Evidence anchors:** To balance the semantic information from existing methods with our fine-grained descriptions, we fuse the CLIP embeddings of decoded text with original high-level features.

## Foundational Learning

- **Concept: REINFORCE Algorithm (Policy Gradient)**
  - Why needed here: The reward metrics are computed on generated text samples and cannot backpropagate through discrete token sampling. REINFORCE provides a gradient estimator for non-differentiable rewards.
  - Quick check question: Can you explain why softmax sampling followed by reward computation creates a non-differentiable path, and how the REINFORCE estimator ∇θ log π(a|s) · r provides an alternative gradient signal?

- **Concept: Prefix Tuning for Cross-Modal Alignment**
  - Why needed here: The brain-to-text model maps variable-dimension fMRI signals to GPT2's embedding space. Prefix tuning provides a learned interface that conditions generation without modifying the full language model.
  - Quick check question: How does concatenating learned fMRI embeddings with learnable constant embeddings before the Transformer encoder help capture semantic information that might be distributed across the fMRI signal?

- **Concept: CLIP Joint Embedding Space**
  - Why needed here: Two of three reward metrics depend on CLIP's ability to measure semantic similarity between text and images in a shared embedding space. Understanding CLIP's limitations is critical for interpreting reward signals.
  - Quick check question: If CLIP embeddings emphasize semantic category over spatial layout, what types of reconstruction errors might the text-image similarity reward fail to penalize?

## Architecture Onboarding

- **Component map:** fMRI signals → subject-specific linear layers → unified Z embedding → concatenated with learnable Z' → 2l×d prefix → 32-layer Transformer (8 heads) → fMRI prefix for GPT2 → GPT2-Base → decoded text → reward computation → fused embeddings → diffusion model

- **Critical path:** The brain-to-text model must be trained first (Stage 1: 200 epochs CE loss; Stage 2: 10 epochs reinforced co-training with α=β=γ=0.01). The decoded text quality directly limits reconstruction improvement.

- **Design tradeoffs:** Unified model across subjects reduces per-subject training cost but requires subject-specific input projections; maximum text length set to 77 tokens (CLIP limit) constrains caption granularity; trade-off coefficients α, β, γ set to 0.01 may require adjustment for different reconstruction methods.

- **Failure signatures:** L1/L2 rewards improve high-level metrics but may reduce Alex(2) accuracy (low-level similarity) — indicates semantic-structural tension; BrainDiffuser and MindEye show smaller improvements than LDM — suggests diminishing returns when original method already has rich semantic control; LVLM hallucination in detail-enhanced captions may propagate false objects into reconstruction.

- **First 3 experiments:**
  1. **Ablation by reward:** Train brain-to-text model with each reward (L1, L2, L3) independently on Subject 1 data; compare Inception and CLIP accuracy to understand which reward drives which aspect of reconstruction quality.
  2. **Caption granularity analysis:** Compare reconstruction quality using original NSD captions vs. detail-enhanced captions as ground truth to isolate the contribution of caption quality independent of brain decoding.
  3. **Cross-method fusion test:** Apply the same decoded fine-grained text to LDM and MindEye; quantify the improvement gap to validate the hypothesis that text-only semantic control benefits most from enhanced text.

## Open Questions the Paper Calls Out

- **Question:** How can the framework detect or filter LVLM-generated hallucinations to ensure decoded text reflects actual brain-encoded content?
- **Basis in paper:** The authors state that "LVLMs inevitably hallucinate, producing inaccuracies that can sometimes limit the effectiveness of detail enhancement in image captions."
- **Why unresolved:** No verification mechanism exists between LVLM-generated details and actual neural representations; hallucinated content may be incorporated into training targets and propagate through the reconstruction pipeline.
- **What evidence would resolve it:** Comparison of LVLM-generated captions against human perceptual reports from independent observers, or neural activation-based filtering of implausible details.

- **Question:** How can fine-grained text guidance be more effectively integrated into reconstruction methods that already combine text and image semantic conditions?
- **Basis in paper:** The authors observe that "the combination of text and image semantic conditions in these methods makes the impact of improved text control on image reconstruction more limited" for BrainDiffuser and MindEye.
- **Why unresolved:** The current approach uses simple weighted averaging of CLIP embeddings, which may cause text-derived and image-derived guidance to conflict rather than complement each other.
- **What evidence would resolve it:** Analysis of text-image embedding interactions during diffusion sampling, attention mechanisms that dynamically weight modalities, or architectural modifications with separate semantic and visual pathways.

## Limitations

- **LVLM Hallucination Impact:** The approach depends critically on LLaVA generating accurate fine-grained captions, but no quantification of hallucination rates or semantic alignment between generated captions and ground truth visual stimuli is provided.
- **Method-Specific Saturation:** Improvement patterns vary significantly across reconstruction methods, with LDM showing largest gains while BrainDiffuser and MindEye show smaller improvements due to existing rich semantic conditions, suggesting diminishing returns.
- **Dataset Dependency:** The unified brain-to-text model may exploit dataset-specific signal characteristics of NSD, and no cross-dataset or cross-domain experiments are reported to establish generalization.

## Confidence

**High Confidence (Mechanistic):** The basic architecture of using prefix-tuned GPT2 with REINFORCE for brain-to-text decoding is technically sound. The use of CLIP for semantic similarity measurement is well-established in vision-language research.

**Medium Confidence (Empirical):** The quantitative improvements reported across multiple metrics (SSIM, Alexnet, InceptionV3, CLIP) appear consistent and significant. However, the relative contribution of each component is not clearly isolated through ablation studies.

**Low Confidence (Generalization):** The approach relies heavily on the NSD dataset's specific characteristics (nsdgeneral template, subject-specific linear projections). Without testing on alternative fMRI datasets or different brain regions, confidence in cross-dataset generalization is limited.

## Next Checks

**Validation Check 1:** Train three separate brain-to-text models, each using only one reward metric (L1, L2, or L3) while keeping all other conditions constant. Compare the resulting reconstruction quality for each method (LDM, BrainDiffuser, MindEye) to isolate which reward drives improvements in which aspects of reconstruction quality.

**Validation Check 2:** Implement a hallucination detection step for LVLM-generated captions by measuring semantic similarity between original NSD captions and enhanced captions using CLIP. Set a threshold for acceptable semantic drift, and quantify how many images fall below this threshold to establish the reliability of the fine-grained caption generation step.

**Validation Check 3:** Apply the same decoded fine-grained text embeddings to all three reconstruction methods (LDM, BrainDiffuser, MindEye) using identical fusion weights. Measure the improvement gap between methods to validate the hypothesis that text-only semantic control benefits most from enhanced text, while methods with existing rich semantic conditions show saturation effects.