---
ver: rpa2
title: 'Planner-Auditor Twin: Agentic Discharge Planning with FHIR-Based LLM Planning,
  Guideline Recall, Optional Caching and Self-Improvement'
arxiv_id: '2601.21113'
source_url: https://arxiv.org/abs/2601.21113
tags:
- discharge
- coverage
- context
- baseline
- cache
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents the Planner-Auditor Twin, a self-improving,
  cache-optional framework for safer clinical discharge planning using Large Language
  Models (LLMs). The framework decouples LLM-based generation from deterministic validation,
  enabling systematic reliability measurement without model retraining.
---

# Planner-Auditor Twin: Agentic Discharge Planning with FHIR-Based LLM Planning, Guideline Recall, Optional Caching and Self-Improvement

## Quick Facts
- arXiv ID: 2601.21113
- Source URL: https://arxiv.org/abs/2601.21113
- Reference count: 40
- Primary result: Planner-Auditor framework achieves 86% coverage vs 32% baseline using two-tier self-improvement

## Executive Summary
This study presents the Planner-Auditor Twin, a self-improving, cache-optional framework for safer clinical discharge planning using Large Language Models (LLMs). The framework decouples LLM-based generation from deterministic validation, enabling systematic reliability measurement without model retraining. Evaluated on MIMIC-IV-on-FHIR data, the Planner generates structured discharge plans with confidence estimates, while the Auditor ensures multi-task coverage and monitors calibration. A two-tier self-improvement mechanism, including within-episode refinement and cross-episode discrepancy buffering, significantly enhances plan completeness from 32% to 86% and improves calibration metrics. The approach demonstrates a practical pathway for safer automated discharge planning, addressing the risks of LLM hallucination and miscalibration in high-stakes clinical environments.

## Method Summary
The Planner-Auditor Twin framework uses GPT-4o-mini to generate structured discharge plans from FHIR R4 patient data, with a deterministic Auditor validating coverage of four mandatory categories (Follow-up, Medication Reconciliation, Patient Education, Symptom Monitoring). The system employs a two-tier self-improvement mechanism: within-episode regeneration when internal checks fail, and cross-episode discrepancy buffering for high-confidence but low-coverage cases. Five configurations were tested: baseline, context_cache, self_improve, cache+SI, and buffer_replay. The framework measures multi-task coverage, Brier Score, Expected Calibration Error (ECE), and latency across 50 patients from MIMIC-IV-on-FHIR.

## Key Results
- Coverage improved from 32% to 86% with self-improvement enabled
- High-confidence error rate reduced from 66% to 20% with self-improvement
- Calibration metrics improved (Brier Score from 0.26 to 0.12, ECE from 0.15 to 0.07)
- Context caching improved throughput from 3.4 to 3.8 episodes/minute
- Buffer replay achieved 100% coverage on N=7 cases but increased latency to 27.8s

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decoupling LLM generation from deterministic validation enables systematic reliability measurement without model retraining.
- Mechanism: The Planner (LLM) generates structured discharge plans with confidence estimates, while a separate deterministic Auditor module evaluates multi-task coverage, calibration (Brier/ECE), and drift using rule-based checks. The Auditor does not modify plans in real-time—it logs verdicts and routes signals to regeneration or buffering mechanisms.
- Core assumption: Rule-based coverage checks correctly identify clinically dangerous omissions; the four mandatory categories (Follow-up, Medication Reconciliation, Patient Education, Symptom Monitoring) are sufficient proxies for plan adequacy.
- Evidence anchors:
  - [abstract] "The framework decouples LLM-based generation from deterministic validation, enabling systematic reliability measurement without model retraining."
  - [section 2.2.3] "The auditing agent is a deterministic Python module that inspects the JSON output from the Planner... outputs a verdict (PASS or FAIL) and a set of specific error messages."
  - [corpus] EpiPlanAgent (FMR=0.54) uses multi-agent validation for epidemic planning but without explicit decoupling claims; no direct corpus validation of the decoupling mechanism itself.
- Break condition: If Auditor rules miss clinically significant errors beyond the four categories (e.g., incorrect medication dosages), safety claims weaken. Authors acknowledge this limitation.

### Mechanism 2
- Claim: Feedback-driven regeneration and targeted replay reduce omissions and improve confidence calibration.
- Mechanism: Tier 1 triggers within-episode regeneration when `enable_self_improve` is on and internal checks fail. Tier 2 buffers high-confidence (≥0.8) but low-coverage cases for cross-episode replay, attempting offline repair. Both mechanisms feed Auditor signals back into generation without retraining.
- Core assumption: LLMs can correct omissions when explicitly prompted about missing categories; high-confidence failures are "brittle" and fixable with retry.
- Evidence anchors:
  - [abstract] "A two-tier self-improvement mechanism... significantly enhances plan completeness from 32% to 86% and improves calibration metrics."
  - [section 3.1] "Self-improvement based methods eliminated most confident omissions while buffer replay closed the remaining gap."
  - [corpus] EchoAgent (FMR=0.48) uses guideline-centric reasoning loops but lacks the dual-tier buffer structure; corpus offers weak direct validation.
- Break condition: If buffered cases require domain knowledge unavailable in context, replay may not converge. Buffer replay was tested on N=7 only—scaling uncertainty noted by authors.

### Mechanism 3
- Claim: Deterministic, template-based context summarization prevents hallucination in patient data representation.
- Mechanism: A non-LLM SummaryGenerator flattens FHIR bundles into structured PatientSnapshot objects (text summary + JSON), ensuring medication and diagnosis lists exactly match source records before LLM planning begins.
- Core assumption: Templates correctly capture all clinically relevant fields; no critical signal is lost in flattening.
- Evidence anchors:
  - [section 2.2.1] "We use a non-LLM, template-based summarizer to deterministically flatten FHIR resources into a text narrative. This process ensures the medication and diagnosis lists are an exact reflection of the record without hallucination."
  - [corpus] No direct corpus analogs for this specific FHIR-to-text approach; related agentic systems assume pre-processed inputs.
- Break condition: If templates omit fields needed for specific patient scenarios, downstream planning degrades regardless of Planner quality.

## Foundational Learning

- Concept: **FHIR R4 Resources**
  - Why needed here: The entire pipeline ingests MIMIC-IV-on-FHIR data (Patient, Encounter, Condition, MedicationRequest, Observation, Procedure). Understanding resource structure is required to modify summarization templates or extend coverage checks.
  - Quick check question: Can you identify which FHIR resource type would contain a patient's active medication orders?

- Concept: **Calibration Metrics (Brier Score, ECE)**
  - Why needed here: The Auditor uses these to detect miscalibrated confidence—high-confidence outputs that are incorrect. Interpreting results requires understanding what "good" calibration looks like (lower is better for both).
  - Quick check question: If a model assigns 0.9 confidence to 100 predictions but only 60 are correct, what would you expect the Brier score to indicate?

- Concept: **Agentic "System 2" Loops**
  - Why needed here: The paper frames self-improvement as System 2 thinking—draft, observe, evaluate, refine. This mental model explains why multiple passes improve coverage.
  - Quick check question: What trigger condition initiates Tier 1 within-episode regeneration in this framework?

## Architecture Onboarding

- Component map: FHIRClient -> SummaryGenerator -> RAG Pipeline -> Planner Agent -> Auditor -> FastAPI Harness
- Critical path:
  1. FHIR fetch -> deterministic summarization -> context construction
  2. Planner generates ActionPlan JSON
  3. Auditor evaluates coverage (4 mandatory categories), calibration, drift
  4. If self-improve enabled and coverage fails -> within-episode regeneration
  5. If high-confidence (>=0.8) + low-coverage -> buffer for replay
  6. Results logged to JSON for analysis

- Design tradeoffs:
  - **Latency vs. Completeness**: Self-improve boosts coverage (32%→86%) but increases latency (17.4s→19.7s). Cache+SI partially recovers efficiency (18.7s).
  - **Cache vs. Freshness**: Context caching improves speed and quality but assumes patient state hasn't changed across requests.
  - **Buffer Replay Coverage vs. Scale**: Achieved 100% coverage on N=7 but highest latency (27.8s); authors caution against deployment conclusions at this scale.

- Failure signatures:
  - High-confidence misses (confidence >=0.8, coverage=False) -> route to buffer, do not surface
  - Drift L1 > 0.4 -> log warning, possible mode collapse
  - Empty PatientSnapshot -> skip episode (inclusion criteria not met)

- First 3 experiments:
  1. **Baseline ablation**: Run 50 patients with no cache, no self-improve. Establish coverage (expected ~32%) and calibration baseline. Verify Auditor logging works correctly.
  2. **Self-improve only**: Enable `enable_self_improve`, disable cache. Confirm coverage rises to ~86%. Measure latency increase. Inspect which categories are most often added in regeneration (likely Education/Monitoring per Figure 2a).
  3. **Buffer replay on failures**: Extract high-confidence/low-coverage cases from baseline run. Feed to buffer replay. Confirm whether retry resolves omissions. Document any cases that fail repeatedly—these may require rule changes or template fixes.

## Open Questions the Paper Calls Out
None

## Limitations
- The Auditor's coverage checks are limited to four categories, which the authors acknowledge is an imperfect proxy for clinical adequacy and may miss critical errors like incorrect medication dosages
- Buffer replay achieved 100% coverage on only N=7 cases, with the authors explicitly cautioning that deployment conclusions cannot be drawn from this small sample
- The deterministic summarization templates may omit clinically relevant fields needed for specific patient scenarios, degrading downstream planning quality

## Confidence
- **High Confidence**: Decoupling mechanism (Planner vs. Auditor) is well-supported by abstract and section 2.2.3 with clear deterministic validation rules; calibration metrics are standard and properly applied; two-tier self-improvement architecture is explicitly described and empirically validated
- **Medium Confidence**: Self-improvement mechanisms (Tier 1 and Tier 2) are supported by results (32%→86% coverage) but exact trigger logic for regeneration and buffer replay implementation details are not fully specified; FHIR summarization process is described as deterministic but template format is unspecified
- **Low Confidence**: Buffer replay mechanism's scalability and clinical safety at production scale are acknowledged as open questions; coverage proxy (four categories) is recognized as incomplete for clinical adequacy

## Next Checks
1. **Prompt Template Validation**: Test whether minor variations in the Planner Agent prompt (e.g., confidence elicitation phrasing, JSON schema enforcement) significantly affect coverage and calibration metrics on a small subset (N=10) of MIMIC-IV-on-FHIR data.

2. **Buffer Replay Scaling Test**: Expand buffer replay testing beyond N=7 to N=50 high-confidence/low-coverage cases from baseline runs. Measure whether coverage improvements persist and whether latency remains manageable (<30s) for production deployment.

3. **Coverage Rule Expansion**: Pilot additional deterministic coverage checks beyond the four categories—specifically validate medication dosage accuracy and diagnosis-treatment alignment using MIMIC-IV-on-FHIR medication and procedure resources. Measure impact on coverage rates and false negative rates.