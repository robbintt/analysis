---
ver: rpa2
title: Statistical Tractability of Off-policy Evaluation of History-dependent Policies
  in POMDPs
arxiv_id: '2503.01134'
source_url: https://arxiv.org/abs/2503.01134
tags:
- learning
- jiang
- assumption
- have
- pomdps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies off-policy evaluation (OPE) in partially observable
  Markov decision processes (POMDPs) with large observation spaces, focusing on evaluating
  history-dependent target policies. While prior work established polynomial sample
  complexity for memoryless policies using belief and outcome coverage assumptions,
  extending these results to history-dependent policies remained open.
---

# Statistical Tractability of Off-policy Evaluation of History-dependent Policies in POMDPs

## Quick Facts
- arXiv ID: 2503.01134
- Source URL: https://arxiv.org/abs/2503.01134
- Reference count: 40
- This paper establishes the first separation between model-free and model-based OPE in POMDPs, proving model-free algorithms cannot achieve polynomial sample complexity for history-dependent policies while a simple MLE-based model-based approach can.

## Executive Summary
This paper studies off-policy evaluation (OPE) in partially observable Markov decision processes (POMDPs) with large observation spaces, focusing on evaluating history-dependent target policies. While prior work established polynomial sample complexity for memoryless policies using belief and outcome coverage assumptions, extending these results to history-dependent policies remained open. The authors prove that model-free algorithms cannot achieve polynomial sample complexity for history-dependent policies, even with single-step outcome revealing or memoryless behavior policies. However, a simple maximum likelihood estimation (MLE)-based model-based algorithm circumvents this hardness, achieving polynomial sample complexity under the same coverage assumptions. This establishes the first separation between model-free and model-based OPE in POMDPs. The results demonstrate that model-based approaches can handle more general policy classes in POMDPs than model-free methods, despite the latter's success in MDP settings. The paper also explores state-space misspecification, showing that observable-equivalent realizability is insufficient without additional assumptions on the learned model.

## Method Summary
The method employs a maximum likelihood estimation (MLE)-based model-based algorithm for off-policy evaluation in POMDPs. The approach consists of three steps: (1) pre-filtering the model class M to remove models that violate revealing assumptions, (2) selecting the maximum likelihood model from the filtered class using trajectories from the behavior policy, and (3) evaluating the target policy by rolling out synthetic trajectories in the learned model. The revealing assumptions (single-step or multi-step outcome revealing) enable the pre-filtering step and are crucial for establishing polynomial sample complexity. The method queries the target policy on synthetic histories during evaluation, circumventing the statistical hardness that affects model-free approaches which are limited to querying the target policy only on histories present in the offline dataset.

## Key Results
- Model-free OPE algorithms cannot achieve polynomial sample complexity for history-dependent target policies in POMDPs, even with single-step outcome revealing or memoryless behavior policies.
- A simple MLE-based model-based algorithm achieves polynomial sample complexity for history-dependent target policies under the same coverage assumptions.
- Observable-equivalent realizability (matching observation distributions) is insufficient for polynomial guarantees without additional assumptions on the learned model class.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model-free OPE algorithms cannot achieve polynomial sample complexity for history-dependent target policies in POMDPs, even with strong revealing assumptions.
- Mechanism: Model-free algorithms only query πe on histories present in the offline dataset. For history-dependent policies, critical action patterns may appear with exponentially low probability under πb, making them statistically indistinguishable despite vastly different returns.
- Core assumption: Assumption E (single-step outcome revealing) or Assumption D (multi-step outcome revealing) with bounded C_O or C_F respectively.
- Evidence anchors:
  - [Theorem 3]: Hardness holds even when C_O = C_F = C_H = 1 and |M| = 1 (known model)
  - [Proof sketch, Page 5-6]: Two policies π₁ and π₂ with J(π₁) = 1, J(π₂) = 0 appear identical unless the rare trajectory (L,...,L) appears in data, requiring Ω(2^H) samples
  - [corpus]: Related work on history-dependent behavior policies notes similar difficulties (FMR 0.65)
- Break condition: If πe is memoryless, hardness does not apply; model-free algorithms achieve poly(H) complexity per Theorem 1.

### Mechanism 2
- Claim: A simple MLE-based model-based algorithm achieves polynomial sample complexity for history-dependent πe under coverage assumptions.
- Mechanism: The algorithm (1) pre-filters M to M′ by excluding models violating revealing assumptions, (2) selects ˆM via MLE on trajectories, then (3) evaluates J_ˆM(πe) by rolling out synthetic trajectories. Synthetic rollouts circumvent model-free hardness by querying πe on any history, not just observed ones.
- Core assumption: Assumption B (realizability, M* ∈ M); Assumption E (single-step revealing) OR Assumption D (multi-step revealing) + memoryless πb.
- Evidence anchors:
  - [Theorem 4]: Under Assumption E, |J(πe) - J_ˆM(πe)| ≤ O(H²C_O²C_eff,1√(log|M|/δn))
  - [Theorem 5]: Under Assumption D + memoryless πb, same form with C_F
  - [Lemma 9, Page 16]: MLE ensures TV distance between trajectory distributions scales as O(√(log|M|/δn))
- Break condition: Pre-filtering is essential; without it, MLE may select models that fail revealing conditions, breaking the error propagation analysis.

### Mechanism 3
- Claim: Observable-equivalent realizability (matching observation distributions) is insufficient for polynomial guarantees.
- Mechanism: Models M₁ and M₂ can produce identical observation distributions under πb yet make different predictions for πe. MLE cannot distinguish them without observing rare trajectories where they differ, requiring exponential samples.
- Core assumption: Definition 6 (observable-equivalent realizability) replaces Assumption B.
- Evidence anchors:
  - [Theorem 7]: Two models M₁, M₂ with identical observable processes under πb but different J(πe) predictions; MLE requires Ω(2^H) samples to distinguish
  - [Section 5]: Pre-filtering fails because C_O = ∞ for both models (|S_h| > |O_h|)
  - [Theorem 8]: Guarantees require models in M to satisfy revealing assumptions themselves, not just M*
- Break condition: If all models in M satisfy Assumption D, approximation error ε_approx appears additively in bound.

## Foundational Learning

- Concept: **Belief State in POMDPs**
  - Why needed here: Assumption C (belief coverage) requires understanding b_S(τ_h) = P(s_{h+1}|τ_h). Coverage is about whether belief vectors span R^{|S_h|} under πb.
  - Quick check question: Given a history τ_h = (o_1,a_1,...,o_h,a_h), can you compute the posterior over latent states? What does σ_min(Σ_{H,h}) < ∞ imply?

- Concept: **Outcome Revealing (Observable Operators)**
  - Why needed here: The revealing assumptions (D and E) enable inverting observations to latent states. This is parameterized via OOM operators B_h(o,a) with weighted pseudo-inverses.
  - Quick check question: Why does C_O = 1 when observation deterministically reveals state? How does multi-step revealing relax single-step?

- Concept: **Off-Policy Coverage Coefficients**
  - Why needed here: C_eff,1 and C_eff,m capture distribution shift between πe and πb. They generalize state-action density ratios from MDPs to POMDPs.
  - Quick check question: When πe = πb, what is C_eff? Why does the bound use L1 norm of operator error rather than L2?

## Architecture Onboarding

- Component map: Data -> Pre-filtered M′ -> MLE selection -> J_ˆM(πe)
- Critical path: Data → Pre-filtered M′ → MLE selection → J_ˆM(πe). Pre-filtering is easy to skip but essential.
- Design tradeoffs:
  - Single-step vs. multi-step revealing: Single-step (C_O) is stronger but allows history-dependent πb; multi-step (C_F) is weaker but requires memoryless πb.
  - Uniform vs. policy-specific coverage: C_A·C_H is a loose uniform bound; C_eff can be much tighter if πe ≈ πb.
  - Model class size |M|: Larger M increases flexibility but also log|M| in sample complexity.
- Failure signatures:
  - Exponential sample requirements with history-dependent πe in model-free code → model-free fundamentally cannot handle this setting.
  - MLE selects model with poor πe predictions despite good likelihood → check if pre-filtering was applied and if models satisfy revealing assumptions.
  - Bounds blow up with large C_O or C_F → revealing condition is nearly violated; observations poorly constrain latent states.
- First 3 experiments:
  1. **Synthetic validation**: Implement the counterexample from Theorem 3 (known model, two history-dependent policies, memoryless πb). Confirm model-free methods need exponential samples while model-based succeeds with poly(n).
  2. **Pre-filtering ablation**: On a POMDP where models in M vary in revealing properties, compare MLE with and without pre-filtering. Expect divergence when non-revealing models have higher likelihood.
  3. **Coverage coefficient estimation**: For a fixed πe, πb pair, estimate C_eff empirically by comparing model error under πe vs. πb distributions. Verify that C_eff ≪ C_A·C_H when πe ≈ πb.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a model-based algorithm achieve polynomial sample complexity for off-policy evaluation (OPE) when both the behavior policy $\pi_b$ and target policy $\pi_e$ are history-dependent under the multi-step outcome revealing assumption?
- Basis in paper: [explicit] The Conclusion identifies this setting (the bottom-right corner of Table 1) as a "major open problem which we conjecture to be intractable."
- Why unresolved: The paper proves model-based OPE is tractable for history-dependent policies under single-step revealing (Theorem 4) and for memoryless behavior policies under multi-step revealing (Theorem 5), but leaves the intersection of these difficult settings unresolved.
- What evidence would resolve it: A derivation of polynomial sample complexity bounds for this setting or a proof of information-theoretic hardness confirming the authors' conjecture.

### Open Question 2
- Question: Is the Maximum Likelihood Estimation (MLE)-based algorithm computationally efficient?
- Basis in paper: [inferred] The paper focuses exclusively on "Statistical Tractability" (sample complexity). While the MLE-based approach circumvents statistical hardness, the computational complexity of finding the MLE over the model class $\mathcal{M}$ is not analyzed.
- Why unresolved: Statistical tractability does not imply computational tractability. The optimization step (Eq. 1) may be computationally prohibitive in large POMDPs, creating a potential gap between statistical success and practical utility.
- What evidence would resolve it: A polynomial-time algorithm for computing the MLE in this setting or a proof of computational hardness.

### Open Question 3
- Question: Can OPE guarantees be derived under observable-equivalent realizability without requiring the hypothesis class to satisfy revealing assumptions a priori?
- Basis in paper: [inferred] Section 5 demonstrates that standard MLE fails under observable-equivalent realizability unless the hypothesis class satisfies revealing properties (Assumption D) for pre-filtering.
- Why unresolved: The positive result in Theorem 8 relies on the strong assumption that all models in the class satisfy revealing properties. It is unclear if learning is possible when the learner does not have access to such a structured hypothesis class.
- What evidence would resolve it: An algorithm that provides sample complexity guarantees under observable-equivalent realizability without relying on pre-filtering based on revealing assumptions.

## Limitations

- The revealing assumptions (E and D) require models to satisfy strict matrix norm conditions on observable-operator inverses, which may be too stringent for complex environments.
- Practical feasibility of pre-filtering and satisfying revealing assumptions in real POMDPs with large observation spaces remains unclear.
- The results are limited to finite model classes |M|, and extending to function approximation requires additional structure.

## Confidence

- **High**: Model-free hardness (Theorem 3) and model-based polynomial guarantees (Theorems 4-5) for history-dependent policies under revealing assumptions
- **Medium**: State-misspecification results (Theorems 7-8) and the necessity of pre-filtering
- **Low**: Practical feasibility of pre-filtering and revealing assumption satisfaction in real POMDPs

## Next Checks

1. **Counterexample implementation**: Implement the two-policy counterexample from Theorem 3 to empirically verify model-free algorithms require exponential samples while model-based succeeds with polynomial samples.
2. **Pre-filtering sensitivity**: Systematically vary C_O and C_F bounds in a synthetic POMDP to identify when pre-filtering becomes too aggressive (M′ empty) or too permissive (model-free hardness persists).
3. **Coverage coefficient estimation**: Design experiments to estimate C_eff empirically across different πe, πb pairs to validate that coverage coefficients can be substantially smaller than uniform bounds in practice.