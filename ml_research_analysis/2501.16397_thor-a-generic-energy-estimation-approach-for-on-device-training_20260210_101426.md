---
ver: rpa2
title: 'THOR: A Generic Energy Estimation Approach for On-Device Training'
arxiv_id: '2501.16397'
source_url: https://arxiv.org/abs/2501.16397
tags:
- energy
- consumption
- layer
- thor
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces THOR, a generic energy estimation approach
  for on-device training of deep neural networks. The key idea is to leverage the
  layer-wise energy additivity property of DNNs, partitioning models into layers and
  fitting Gaussian Process models to learn from layer-wise energy consumption measurements.
---

# THOR: A Generic Energy Estimation Approach for On-Device Training

## Quick Facts
- arXiv ID: 2501.16397
- Source URL: https://arxiv.org/abs/2501.16397
- Authors: Jiaru Zhang; Zesong Wang; Hao Wang; Tao Song; Huai-an Su; Rui Chen; Yang Hua; Xiangwei Zhou; Ruhui Ma; Miao Pan; Haibing Guan
- Reference count: 29
- Key outcome: THOR reduces Mean Absolute Percentage Error by up to 30% compared to existing methods and enables 50% energy reduction through pruning

## Executive Summary
THOR introduces a novel approach for estimating energy consumption during on-device training of deep neural networks. The method leverages the layer-wise energy additivity property of DNNs, partitioning models into layers and using Gaussian Process models to learn from layer-wise energy measurements. This enables accurate estimation of overall energy consumption without requiring device-specific information or complex runtime optimizations. The approach demonstrates significant improvements in energy estimation accuracy and practical applicability for energy-aware model optimization.

## Method Summary
THOR operates on the principle that DNN energy consumption can be decomposed into layer-wise components due to energy additivity. The method partitions DNN models into individual layers and collects energy measurements for each layer type through profiling. Gaussian Process regression models are then trained on these layer-wise measurements to learn the energy consumption patterns. During inference, the overall energy consumption is estimated by summing the predicted energy of each layer in the model. This layer-based decomposition allows THOR to generalize across different hardware platforms and model architectures while maintaining accuracy.

## Key Results
- Reduces Mean Absolute Percentage Error by up to 30% compared to existing energy estimation methods
- Enables 50% energy reduction through guided energy-aware pruning
- Demonstrates effectiveness across multiple models and real-world devices including Raspberry Pi, Jetson Xavier, and mobile SoC platforms

## Why This Works (Mechanism)
THOR exploits the fundamental property that energy consumption in DNNs can be decomposed into additive layer-wise components. This decomposition is possible because each layer's computational characteristics and data movement patterns contribute independently to total energy consumption. The Gaussian Process models effectively capture the relationship between layer parameters (such as input size, output size, and operation type) and their corresponding energy consumption. By learning these relationships from empirical measurements, THOR can accurately predict energy consumption for unseen models and hardware configurations.

## Foundational Learning
- **Layer-wise energy additivity**: DNN energy consumption can be decomposed into independent layer contributions - needed to enable the decomposition approach, quick check: verify energy measurements sum correctly across layers
- **Gaussian Process regression**: Non-parametric Bayesian approach for modeling complex relationships - needed to capture non-linear energy patterns, quick check: validate GP model fit on training data
- **Model profiling**: Systematic measurement of energy consumption for different layer types - needed to build the training dataset, quick check: ensure consistent measurement methodology
- **Energy-aware pruning**: Optimization technique that considers energy metrics during model compression - needed for practical application, quick check: verify pruned models maintain accuracy
- **Hardware abstraction**: Decoupling energy estimation from specific hardware details - needed for generalizability, quick check: test across multiple device types
- **On-device training dynamics**: Understanding energy patterns during model training vs inference - needed for practical deployment, quick check: compare training vs inference energy profiles

## Architecture Onboarding

**Component Map:** Hardware Platform -> Layer Profiler -> Gaussian Process Trainer -> Energy Estimator -> Pruning Optimizer

**Critical Path:** Layer measurements → GP model training → Energy estimation → Pruning decisions

**Design Tradeoffs:** 
- Accuracy vs. profiling overhead: More measurements improve accuracy but increase setup time
- Model complexity vs. generalization: Complex GP models capture patterns better but may overfit
- Offline profiling vs. online adaptation: Initial profiling provides accuracy but lacks dynamic adaptation

**Failure Signatures:**
- MAPE increases when models deviate significantly from training distribution
- Energy estimates become inaccurate for hardware platforms with substantially different architectures
- GP models may fail when layer types or patterns are not well-represented in training data

**Three First Experiments:**
1. Profile a simple CNN model on target hardware to establish baseline measurements
2. Train GP models on collected layer measurements and validate prediction accuracy
3. Apply energy-aware pruning to reduce model size while maintaining accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to specific device configurations, not covering extreme resource-constrained devices or cloud GPU systems
- Gaussian Process assumptions may not hold for architectures with residual connections or attention mechanisms
- Offline profiling requirement may be impractical in dynamic deployment scenarios

## Confidence

**High Confidence** - Core THOR methodology and layer-wise energy estimation principle; experimental results showing reduced MAPE compared to baselines; practical application to energy-aware pruning.

**Medium Confidence** - Generalization across diverse hardware platforms; scalability to extremely large models; performance under dynamic runtime conditions; applicability to non-standard DNN architectures.

**Low Confidence** - Long-term stability of GP models across different training workloads; impact on model accuracy during energy-aware pruning; effectiveness in real-time adaptive scenarios.

## Next Checks
1. Validate THOR's accuracy on cloud GPU platforms and extremely resource-constrained IoT devices to assess hardware diversity coverage.

2. Test the method's effectiveness on transformer-based architectures and models with complex connectivity patterns beyond standard convolutional networks.

3. Conduct longitudinal studies measuring energy estimation accuracy over extended training periods with varying data distributions and learning rate schedules.