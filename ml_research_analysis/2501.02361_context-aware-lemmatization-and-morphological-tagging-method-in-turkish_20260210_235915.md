---
ver: rpa2
title: Context Aware Lemmatization and Morphological Tagging Method in Turkish
arxiv_id: '2501.02361'
source_url: https://arxiv.org/abs/2501.02361
tags:
- word
- morphological
- lemmatization
- used
- turkish
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of accurate word root prediction
  in Turkish, a highly agglutinative language, by developing context-aware lemmatization
  and morphological tagging models that consider both word spelling and meaning. The
  approach uses bidirectional LSTM networks for character-level word representation
  and a Turkish BERT model for semantic understanding, enabling disambiguation of
  words with multiple possible roots based on sentence context.
---

# Context Aware Lemmatization and Morphological Tagging Method in Turkish

## Quick Facts
- arXiv ID: 2501.02361
- Source URL: https://arxiv.org/abs/2501.02361
- Reference count: 20
- Primary result: Context-aware model achieves state-of-the-art lemmatization accuracy up to 98.13% on Turkish IMST dataset, surpassing SIGMORPHON 2019 baselines

## Executive Summary
This study addresses the challenge of accurate word root prediction in Turkish, a highly agglutinative language, by developing context-aware lemmatization and morphological tagging models that consider both word spelling and meaning. The approach uses bidirectional LSTM networks for character-level word representation and a Turkish BERT model for semantic understanding, enabling disambiguation of words with multiple possible roots based on sentence context. Two model variants—separate and sequenced—are presented, with the latter optionally incorporating morphological tag predictions into the lemmatization process. Evaluated on Universal Dependencies IMST and PUD datasets, the models achieve state-of-the-art results, surpassing SIGMORPHON 2019 competition benchmarks across multiple metrics including lemmatization accuracy (up to 98.13% on IMST) and morphological tagging F1 scores (up to 98.24%).

## Method Summary
The method employs a two-branch encoder architecture that processes Turkish words at both character and contextual levels. Character sequences are embedded and passed through 3-layer bidirectional LSTMs with residual connections to capture morphological patterns, while Turkish DistilBERT provides sentence-level semantic embeddings. These representations are concatenated and processed through dense layers to form a unified encoding. The decoder generates lemmas character-by-character using forward LSTMs with Bahdanau attention over encoder outputs. Two variants are implemented: a separate model where lemmatization and morphological tagging are independent, and a sequenced model where morphological tags are predicted first and then fed into the lemmatization encoder. Training occurs in two phases: initial training with frozen BERT (128 epochs, lr=1e-3), followed by fine-tuning with unfrozen BERT (48 epochs, lr=1e-5).

## Key Results
- Separate model achieves 97.70% lemmatization accuracy on IMST dataset and 88.84% on PUD dataset
- Sequenced model achieves 98.13% lemmatization accuracy on IMST dataset but 88.25% on PUD dataset
- Morphological tagging F1 scores reach 98.24% on IMST and 89.04% on PUD
- Outperforms SIGMORPHON 2019 competition baselines by 0.43-0.47% in lemmatization accuracy and 0.41-0.71% in morphological tagging accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining character-level spelling representations with contextual semantic embeddings improves lemmatization accuracy for morphologically ambiguous words.
- Mechanism: The architecture uses two parallel streams—Bi-LSTM processes character sequences to capture orthographic patterns and morphological boundaries, while BERT provides sentence-level contextual embeddings. These vectors are concatenated and passed through dense layers to form a unified representation that disambiguates words like "oku" (which could derive from verb "oku(-mak)" or noun "ok") based on surrounding context.
- Core assumption: Contextual semantics provide disambiguating signal that pure orthographic models cannot access.
- Evidence anchors:
  - [abstract] "bidirectional LSTM networks for character-level word representation and a Turkish BERT model for semantic understanding, enabling disambiguation of words with multiple possible roots"
  - [section 3, Encoder] "the word vector obtained from the Turkish Bidirectional Encoder Representations from Transformers (BERT) model was used to describe the meaning of the word"
  - [corpus] TurBLiMP benchmark confirms Turkish LMs struggle with linguistic phenomena requiring context, supporting the need for contextual disambiguation resources
- Break condition: If BERT embeddings lack coverage for domain-specific vocabulary or rare morphological patterns, the semantic signal degrades, reducing disambiguation benefit.

### Mechanism 2
- Claim: Feeding predicted morphological tags into the lemmatization model improves accuracy on larger datasets but may hurt performance on smaller datasets.
- Mechanism: In the sequenced model, morphological tags are embedded and processed through a dedicated Bi-LSTM, producing a tag vector concatenated with spelling and meaning vectors. This provides explicit grammatical category information (noun vs. verb) that constrains lemma candidates. However, tag prediction errors propagate to lemmatization.
- Core assumption: Morphological tag predictions are sufficiently accurate that their signal outweighs error propagation risk.
- Evidence anchors:
  - [section 4.3, Table 1] IMST dataset: sequenced model achieves 98.13% lemmatization accuracy vs. 97.70% for separate model
  - [section 4.3, Table 2] PUD dataset: sequenced model achieves 88.25% vs. 88.84% for separate model—performance decreases
  - [section 5] "the morphological tags increased the lemmatization in the IMST data set but had the opposite effect in the PUD data set"
  - [corpus] No direct corpus evidence on tag-to-lemma cascading; related work (Lemma Dilemma, Lematus) focuses on joint modeling rather than sequential dependency effects
- Break condition: On low-resource datasets, tag prediction accuracy drops sufficiently that error propagation exceeds the benefit of explicit morphological signal.

### Mechanism 3
- Claim: Character-level sequence generation with attention over encoder outputs enables accurate lemma production for agglutinative morphology.
- Mechanism: The decoder generates output characters autoregressively using forward LSTMs initialized with the encoder's combined hidden state. Bahdanau attention attends over encoder character representations, allowing the model to align generated lemma characters with input word characters while incorporating contextual information from the hidden state.
- Core assumption: Attention provides sufficient alignment signal between input surface forms and output lemmas without explicit morphological rules.
- Evidence anchors:
  - [section 3.2, Decoder] "After the new output passes through the Bahdanau Attention layer with the output created in the encoder section...the model output is produced in a single step"
  - [section 4.3, Table 1] Levenshtein distance of 0.02 on IMST (vs. 0.06 for SIGMORPHON baseline) indicates near-exxact lemma generation
  - [corpus] Lematus (Bergmanis & Goldwater, 2018) demonstrates similar attention-based neural lemmatization across 20 languages, providing cross-linguistic validation of the approach
- Break condition: For unseen morphological patterns or out-of-vocabulary roots, attention has no reliable alignment targets, leading to character-level hallucination.

## Foundational Learning

- Concept: Agglutinative morphology
  - Why needed here: Turkish words are formed by concatenating multiple suffixes to roots, creating long surface forms where a single root can appear in hundreds of variants. Understanding this is essential to grasp why character-level processing outperforms word-level approaches.
  - Quick check question: Given the Turkish word "gözükmezmişçasına," can you identify why a subword tokenizer might fragment this differently than a character-level model?

- Concept: Encoder-decoder with attention
  - Why needed here: The model generates lemmas character-by-character using decoder LSTMs that attend over encoded input representations. Without this foundation, the information flow from context + spelling to output sequence will be opaque.
  - Quick check question: In this architecture, what three signals does the decoder's initial hidden state encode, and why does attention need to access encoder outputs separately?

- Concept: Contextual vs. static word embeddings
  - Why needed here: BERT provides different vectors for the same word in different sentences, which is the mechanism enabling semantic disambiguation. Static embeddings would assign "oku" the same vector regardless of whether it means "read" or derives from "arrow."
  - Quick check question: If you replaced BERT with static FastText embeddings, which specific class of errors would increase, based on the paper's examples?

## Architecture Onboarding

- Component map:
  - Input: Character indices + BERT tokenization + optional morphological tag indices
  - Encoder Branch 1 (spelling): Character embedding → 3 stacked Bi-LSTMs with residual connections → spelling vector
  - Encoder Branch 2 (meaning): Turkish DistilBERT → mask and sum subword vectors → meaning vector
  - Encoder Branch 3 (tags, sequenced only): Tag embedding → Bi-LSTM → tag vector
  - Encoder Fusion: Concatenate/merge vectors → dense layers → unified hidden state
  - Decoder: Forward LSTM (teacher forcing with <start> token) → second LSTM → attention over encoder character outputs → dense → softmax classification → autoregressive generation
  - Training: Two-phase—freeze BERT (128 epochs, lr=1e-3), then fine-tune BERT (48 epochs, lr=1e-5)

- Critical path:
  1. Data preprocessing: Parse CoNLL-U format, order morphological tags consistently
  2. BERT pre-computation: Cache BERT outputs for phase 1 training efficiency
  3. Morphological tagging model training (required for sequenced model)
  4. Lemmatization model training with/without tag inputs
  5. Evaluation using SIGMORPHON benchmark scripts

- Design tradeoffs:
  - Separate vs. sequenced: Separate is simpler and more robust on small datasets; sequenced provides marginal gains on large datasets but adds pipeline complexity and failure modes
  - DistilBERT vs. full BERT: DistilBERT reduces computational cost; paper does not ablate this choice, so quality tradeoff is unknown
  - Character-level vs. subword output: Character-level handles unseen morphology but requires more decoding steps; paper does not compare to subword generation

- Failure signatures:
  - OOM during BERT fine-tuning: Reduce batch size from 32 or skip phase 2 training
  - Lemmatization accuracy plateaus below 90%: Check morphological tag ordering consistency; mixed tag ordering reduces performance
  - Attention weights concentrated on single position: May indicate insufficient encoder capacity or learning rate too high in phase 1
  - Sequenced model underperforms separate on validation set: Dataset may be too small; fall back to separate model

- First 3 experiments:
  1. Reproduce separate model on IMST dataset using paper hyperparameters; validate that lemmatization accuracy falls within 97-98% range before proceeding
  2. Ablation study: Remove BERT embeddings (zero out meaning vector) and measure accuracy drop on ambiguous word subset to quantify semantic contribution
  3. Cross-dataset transfer: Train on IMST, evaluate on PUD without retraining to assess generalization; if gap exceeds 5%, investigate domain mismatch in morphological tag vocabulary

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the size of the training corpus determine whether the "sequenced" model (feeding morphological tags into the lemmatizer) is effective?
- Basis in paper: [explicit] The authors note that the sequenced model improved results on the IMST dataset but degraded them on the PUD dataset, hypothesizing, "This may be because the number of records in the IMST dataset is much larger than that in the PUD dataset."
- Why unresolved: The paper observes the correlation between dataset size and model success but does not validate the hypothesis with experiments across varying data volumes.
- What evidence would resolve it: An ablation study testing both model architectures on the same dataset at different scale increments (e.g., 25%, 50%, 100% of IMST) to identify the tipping point where sequencing becomes beneficial.

### Open Question 2
- Question: How does the context-aware model perform specifically on the class of ambiguous words that motivated the study?
- Basis in paper: [inferred] The introduction emphasizes that standard stemming fails on ambiguous words like "oku" (read vs. tribe) and "bakan" (minister vs. looking), yet the results section only provides aggregate accuracy metrics without isolating performance on these specific high-ambiguity tokens.
- Why unresolved: While overall accuracy is high, it is unclear if the BERT-based context integration specifically solved the targeted ambiguity problem or if gains were made primarily on unambiguous tokens.
- What evidence would resolve it: A targeted evaluation set containing only words with multiple possible lemma candidates, reporting accuracy specifically for this subset.

### Open Question 3
- Question: Can the proposed BiLSTM and BERT architecture maintain its superior performance when adapted for other morphologically rich or agglutinative languages?
- Basis in paper: [inferred] The paper compares its results against multilingual SIGMORPHON 2019 baselines, noting that its focus on a single language (Turkish) offers an advantage, but it leaves the generalizability of the specific architectural choices untested for other languages.
- Why unresolved: The study is monolingual, so it is unknown if the specific balance of character-level BiLSTM and BERT is optimal for Turkish specifically or is a robust general solution for agglutination.
- What evidence would resolve it: Applying the identical model architecture to other agglutinative languages (e.g., Finnish, Hungarian) from the Universal Dependencies dataset and comparing results.

## Limitations
- Critical architectural hyperparameters (embedding dimensions, LSTM sizes, dense layer sizes) are not specified
- Sequenced model shows performance degradation on smaller datasets, suggesting overfitting when tag prediction accuracy is insufficient
- Evaluation only considers SIGMORPHON 2019 Turkish dataset split with no domain transfer or out-of-distribution robustness analysis

## Confidence
- **High Confidence**: The core mechanism of combining character-level and contextual embeddings for lemmatization is well-supported by the architecture description and quantitative results. The claim that semantic disambiguation improves accuracy is directly evidenced by the 0.43-0.47% accuracy gains over SIGMORPHON baselines.
- **Medium Confidence**: The sequenced model's conditional benefits (IMST gains, PUD degradation) are reported but the underlying causes (tag prediction accuracy, dataset size effects) are not directly measured. The claim that morphological tags constrain lemma generation is plausible but not empirically isolated from tag prediction error effects.
- **Low Confidence**: The paper asserts "state-of-the-art" performance without systematic comparison to recent neural lemmatizers beyond SIGMORPHON 2019. Claims about character-level generation handling agglutinative morphology are supported by results but lack ablation studies comparing to subword approaches.

## Next Checks
1. Implement an ablation study removing BERT embeddings to quantify the semantic disambiguation contribution, measuring accuracy degradation specifically on ambiguous word pairs like "oku" (verb vs. noun derivation).
2. Test cross-dataset transfer by training on IMST and evaluating on PUD without fine-tuning to assess generalization and identify domain-specific morphological tag coverage gaps.
3. Compare character-level generation to subword-based lemmatization using the same contextual encoder to determine if the character-level decoder is essential for agglutinative morphology or simply adds computational cost.