---
ver: rpa2
title: Online inductive learning from answer sets for efficient reinforcement learning
  exploration
arxiv_id: '2501.07445'
source_url: https://arxiv.org/abs/2501.07445
tags:
- learning
- agent
- policy
- dist
- logic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel neurosymbolic approach that combines
  Inductive Logic Programming (ILP) with reinforcement learning to improve training
  efficiency and policy explainability. The method learns ASP-based policy heuristics
  online during RL training by converting high-reward experience batches into weighted
  examples for ILP.
---

# Online inductive learning from answer sets for efficient reinforcement learning exploration

## Quick Facts
- arXiv ID: 2501.07445
- Source URL: https://arxiv.org/abs/2501.07445
- Reference count: 31
- Key outcome: Novel neurosymbolic approach combining ILP with RL improves training efficiency and policy explainability, achieving almost double discounted returns on larger Pac-Man map with only ~25% computational overhead.

## Executive Summary
This paper presents a neurosymbolic approach that integrates Inductive Logic Programming (ILP) with reinforcement learning to improve exploration efficiency while maintaining policy explainability. The method learns ASP-based policy heuristics online during RL training by converting high-reward experience batches into weighted examples for ILP. These learned rules are then used to guide exploration through probabilistic soft bias, avoiding inefficient reward shaping while preserving optimality. The approach was evaluated on the Pac-Man domain with two map sizes, achieving significantly higher discounted returns (almost double in the larger map) compared to standard approximate Q-learning, with learned rules converging within ~70 training batches.

## Method Summary
The NeuroQ framework interleaves Approximate Q-learning with online ILP learning and ASP reasoning. At each batch boundary, the system converts the top-performing episodes (by return) into Weighted Context-Dependent Partial Interpretations for ILP. FastLAS learns compact ASP rules from these examples, which are then used during exploration to probabilistically bias action selection toward rule-suggested moves. This "soft bias" approach preserves RL optimality while improving exploration efficiency. The learned rules converge within ~70 batches and provide interpretable explanations of the agent's policy, with the symbolic components adding only ~25% computational overhead.

## Key Results
- NeuroQ achieves almost double the discounted returns compared to standard approximate Q-learning on the larger 25×26 Pac-Man map
- Learned ASP rules converge within ~70/200 training batches, providing interpretable policy explanations
- The symbolic reasoning component adds only ~25% computational overhead per batch
- Rule convergence measured by normalized Hamming distance reaches near-zero within the first 70 batches on both map sizes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Symbolic policy heuristics improve RL exploration efficiency by reducing random action selection in promising directions.
- Mechanism: The system learns ASP rules from high-return trajectories, converting them into weighted ILP examples. During exploration, instead of uniform random action selection, the agent samples from a weighted distribution that biases toward actions suggested by the learned ASP rules. This is a "soft bias" rather than a hard constraint.
- Core assumption: High-return trajectories encode useful policy patterns that can be generalized via logical rules; the symbolic learner can extract these patterns before the neural learner converges.
- Evidence anchors:
  - [abstract] "perform answer set reasoning on the learned rules to guide the exploration of the learning agent at the next batch, without requiring inefficient reward shaping and preserving optimality with soft bias"
  - [section 4.3, Algorithm 1, Lines 7-10] The exploration phase uses ASP reasoning to get suggested actions Ah, then samples with probability ρ from suggested actions vs. (1-ρ) from non-suggested actions
  - [corpus] Weak direct evidence for this exact mechanism; related work [19,21] cited in introduction explores similar policy guidance concepts
- Break condition: If the ASP rules learned are overly specific to the training episodes (overfitting), they may suggest suboptimal actions, degrading exploration. The soft bias with ρ < 1 mitigates this.

### Mechanism 2
- Claim: Online ILP learning converges quickly to stable, interpretable policy representations within a fraction of total RL training time.
- Mechanism: FastLAS processes batches of experience, generating Weighted Context-Dependent Partial Interpretations (WCDPIs) from high-return episodes. The ILP task finds minimal-length hypotheses covering the best examples. As training progresses, the best episodes stabilize, and so do the learned rules.
- Core assumption: The "best" episodes in early batches contain transferable patterns; FastLAS's optimization for minimal rule length and coverage generalizes well.
- Evidence anchors:
  - [abstract] "learned ASP rules converged within ~70 batches, providing a coherent, interpretable explanation of the agent's policy"
  - [section 5.2, Figure 3] Shows convergence measured by normalized Hamming distance between rule bodies, reaching near-zero within <70/200 batches on both maps
  - [corpus] No directly comparable convergence rate data; [11] (FastLAS paper) establishes scalability but not RL-specific convergence
- Break condition: If the environment changes (non-stationary), rules may need to adapt. Current online relearning handles this, but rapid environmental shifts could cause rule churn.

### Mechanism 3
- Claim: The computational overhead of symbolic learning and reasoning remains tractable (~25% increase) due to FastLAS's scalability and the batched nature of the integration.
- Mechanism: ILP learning and ASP reasoning are interleaved with RL at batch boundaries, not per-timestep. FastLAS is optimized for large search spaces. The learned rules are compact (2-4 rules), keeping ASP solving fast.
- Core assumption: The policy can be approximated well by a small set of compact rules; batch-level integration doesn't create unacceptable latency.
- Evidence anchors:
  - [abstract] "~25% additional computational cost per batch"
  - [section 5.1, Tables 1-2] Detailed timing data: small map ~7.5s/batch vs ~4.8s baseline; large map ~25.8s/batch vs ~20.1s baseline
  - [corpus] No corpus papers report comparable overhead metrics for similar integrations
- Break condition: Scaling to vastly more complex domains (more predicates, larger action spaces) could increase ILP search space and ASP solving time non-linearly.

## Foundational Learning

- **Concept: Answer Set Programming (ASP)**
  - Why needed here: ASP provides the logical formalism for representing policy heuristics. Understanding normal rules (head :- body), grounding, and answer set computation is essential for debugging learned rules.
  - Quick check question: Can you write an ASP rule that says "move in direction D if food is within distance 1 in that direction and there is no wall"?

- **Concept: Inductive Logic Programming (ILP)**
  - Why needed here: ILP is the core learning mechanism that generates ASP rules from examples. Understanding the task structure ⟨B, SM, E⟩, mode declarations, and coverage is crucial.
  - Quick check question: Given background knowledge B, a set of positive examples E+, and a mode declaration specifying head and body predicates, what is the goal of an ILP system?

- **Concept: Reinforcement Learning Exploration Strategies**
  - Why needed here: The core intervention is modifying exploration. Understanding ε-greedy exploration, on-policy vs. off-policy, and the exploration-exploitation trade-off is critical.
  - Quick check question: In standard ε-greedy Q-learning, with ε=0.1, what is the probability of taking a random action, and how does the paper's approach modify this?

## Architecture Onboarding

- **Component map:**
  - **RL Agent (Approximate Q-Learning):** Collects experience (s, a, r, s′), updates Q-weights. Outputs: batches of episodes.
  - **Experience Buffer & Selector:** Stores episodes per batch. Selects top σ episodes by return. Outputs: best episodes Eσ.
  - **Feature Map (FF) & Action Map (FA):** Translates RL states/actions into grounded ASP atoms. Outputs: WCDPIs for ILP.
  - **ILP Learner (FastLAS):** Takes WCDPIs, background knowledge B, and search space SM. Outputs: learned policy heuristics H (ASP rules).
  - **ASP Reasoner:** Takes current state s, background knowledge B, and heuristics H. Outputs: suggested actions Ah.
  - **Biased Explorer:** Takes Ah and ρ. During exploration, samples actions with probability ρ from Ah, (1-ρ) from A \ Ah.

- **Critical path:** Experience collection → Episode selection → WCDPI generation → FastLAS learning → ASP reasoning → Biased action selection.

- **Design tradeoffs:**
  - **Batch size (Sb):** Larger batches mean fewer ILP calls (lower overhead) but slower rule updates (less responsive). Paper uses Sb=100.
  - **Number of best episodes (σ):** More episodes provide more ILP examples (potentially better rules) but increase FastLAS runtime. Paper uses σ=3,5.
  - **Soft bias strength (ρ):** Higher ρ means stronger guidance but less exploration. Paper computes ρ dynamically based on normalized average return of best episodes.
  - **Feature engineering (F, distance bounds):** More features allow more expressive rules but increase SM size. Paper uses food_dist_leq, ghost_dist_geq, etc.

- **Failure signatures:**
  - **Ah is always empty:** Learned rules may be too specific or no good episodes were found. Check WCDPI generation and FastLAS output.
  - **Rules don't converge (high Hamming distance):** Agent may not be learning a stable policy. Check RL hyperparameters or environment stationarity.
  - **Performance is worse than baseline:** Soft bias may be too strong or rules are misleading. Check ρ calculation and rule semantics.

- **First 3 experiments:**
  1. **Reproduce baseline vs. NeuroQ:** Run ApproxQ and NeuroQ on the small Pac-Man map (18×9) with provided hyperparameters (α=0.2, γ=0.8, ε=0.05, Sb=100, σ=5). Log discounted return and rule evolution per batch. Goal: Confirm ~2× return improvement and rule convergence by batch ~70.
  2. **Ablate soft bias (ρ):** Fix ρ to different constant values (e.g., 0.0, 0.5, 0.9) instead of dynamic calculation. Goal: Understand sensitivity to guidance strength and validate the soft-bias optimality preservation claim.
  3. **Vary σ and measure overhead:** Run NeuroQ with σ ∈ {1, 3, 5, 10} on the small map. Measure total time and return. Goal: Characterize the trade-off between ILP input quality and computational cost.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the methodology scale to environments with significantly larger state spaces or more complex logical dependencies than the Pac-Man domain?
- Basis in paper: [explicit] The Conclusion states: "As a future work, we plan to test the scalability of these techniques to even more complex and varied environments."
- Why unresolved: The empirical evaluation is restricted to the Pac-Man domain with relatively small grid sizes (up to 25x26), which may not represent the computational burden of high-dimensional or continuous domains.
- What evidence would resolve it: Successful application and performance benchmarking in complex 3D environments or continuous control tasks with larger search spaces.

### Open Question 2
- Question: Can this soft-bias approach maintain its efficiency and optimality guarantees when integrated with deep reinforcement learning algorithms like Double Deep Q-Networks?
- Basis in paper: [explicit] The authors mention applying the method to "any RL algorithm... (e.g. Double Deep Q-Network)" in the Introduction, and explicitly list "investigating the integration with other RL algorithms" in the Conclusion.
- Why unresolved: The paper only preliminarily validates the approach using Approximate Q-learning, leaving the interaction with non-linear function approximators (neural networks) untested.
- What evidence would resolve it: An experiment demonstrating the NeuroQ framework applied to a Deep Q-Network (DQN) agent, showing comparable or improved convergence rates and returns.

### Open Question 3
- Question: To what extent does the reliance on manually defined feature maps ($F_F$ and $F_A$) limit the autonomy of the learning agent in unstructured or perceptually complex domains?
- Basis in paper: [inferred] The Methodology (Section 4.1) requires "a feature map $F_F: S \rightarrow H(F)$" to ground atoms, implying that the symbolic reasoning relies on human-engineered predicates (e.g., `wall`, `ghost_dist`) rather than raw data.
- Why unresolved: The system assumes the availability of a translated symbolic state; if the feature map is inaccurate or unavailable, the ILP component cannot learn useful heuristics.
- What evidence would resolve it: A variation of the experiment where the agent must learn or extract these lower-level predicates automatically from raw pixels, rather than being provided with explicit coordinate data.

## Limitations

- The computational overhead claim (~25%) is based on single-domain experiments and may not generalize to more complex state spaces
- The approach relies on manually defined feature maps, limiting autonomy in unstructured domains
- The explainability benefit is demonstrated through rule inspection rather than systematic human evaluation of interpretability

## Confidence

- **High Confidence**: The computational overhead claim (25%) - supported by explicit timing tables in section 5.1
- **High Confidence**: Rule convergence within ~70 batches - supported by normalized Hamming distance plots in Figure 3
- **Medium Confidence**: Performance improvement (double returns) - supported by aggregate results but limited to two map sizes
- **Medium Confidence**: Soft bias preserves optimality - theoretical claim with limited ablation testing
- **Low Confidence**: Generalizability to other domains - only tested on Pac-Man variants

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically vary Sb, σ, and ρ across multiple seeds to characterize robustness of the performance gains and rule convergence patterns.

2. **Cross-Domain Transfer**: Apply the NeuroQ methodology to a different RL environment (e.g., CartPole or LunarLander) to test generalizability of the neurosymbolic integration approach.

3. **Explainability Assessment**: Conduct a human evaluation study where domain experts rate the interpretability and usefulness of learned ASP rules compared to standard RL policies.