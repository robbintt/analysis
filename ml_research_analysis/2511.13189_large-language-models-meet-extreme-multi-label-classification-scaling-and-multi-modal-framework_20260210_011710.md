---
ver: rpa2
title: 'Large Language Models Meet Extreme Multi-label Classification: Scaling and
  Multi-modal Framework'
arxiv_id: '2511.13189'
source_url: https://arxiv.org/abs/2511.13189
tags:
- vixml
- learning
- text
- training
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of Extreme Multi-label Classification
  (XMC), where queries must be associated with relevant labels from extremely large
  label spaces while balancing efficiency and performance. The authors propose two
  key contributions: (1) a dual-decoder learning approach that effectively adapts
  large decoder-only language models (up to 7B parameters) for XMC by embedding texts
  within structured prompt templates, and (2) the Vision-enhanced eXtreme Multi-label
  Learning (ViXML) framework, which efficiently incorporates visual metadata by using
  a single image embedding per image concatenated with text token embeddings.'
---

# Large Language Models Meet Extreme Multi-label Classification: Scaling and Multi-modal Framework

## Quick Facts
- arXiv ID: 2511.13189
- Source URL: https://arxiv.org/abs/2511.13189
- Reference count: 13
- Primary result: ViXML with small encoders (66M parameters) outperforms billion-parameter text-only models in most cases, achieving +5.07% to +8.21% P@1 improvements

## Executive Summary
This paper addresses Extreme Multi-label Classification (XMC) by proposing two key contributions: a dual-decoder learning approach that adapts large decoder-only LLMs (up to 7B parameters) using structured prompt templates, and the Vision-enhanced eXtreme Multi-label Learning (ViXML) framework that efficiently incorporates visual metadata. The ViXML framework achieves state-of-the-art performance on four Amazon product datasets, with small encoder models (66M parameters) outperforming billion-parameter text-only models when visual metadata is included. The approach demonstrates that single pooled image embeddings concatenated with text provide multi-modal benefits with minimal computational overhead while freezing vision encoders for efficiency.

## Method Summary
The method employs dual-decoder learning with structured prompt templates ("This product text [text] </s>") to adapt decoder-only LLMs for XMC embedding extraction. ViXML integrates visual metadata through early fusion, using frozen SigLIPv2 encoders to produce single pooled image embeddings that are projected and concatenated with text tokens before transformer processing. The framework uses contrastive learning with NGAME hard negative mining and triplet loss, training encoders for 300 epochs (150 with ViXML) and decoders for 30 epochs using LoRA adaptation. Inference involves pre-computed label embeddings and maximum inner product search for efficient million-label retrieval.

## Key Results
- ViXML with DistilBERT (66M) achieves 49.55 P@1 vs text-only Qwen2.5-3B at 47.42
- Early-fusion ViXML provides +1.5% P@1 improvement over late-fusion MUFIN
- Qwen2.5-0.5B with prompt template achieves ~45 P@1 with only 30 training epochs
- State-of-the-art performance with 5.07-8.21% P@1 improvements over previous methods

## Why This Works (Mechanism)

### Mechanism 1: Structured Prompt Templates for Decoder-only LLMs
Structured prompts enable decoder-only LLMs to produce effective embeddings by framing input with "This product text" prefix and "</s>" end token, allowing pretrained decoder to generate meaningful sentence representations through mean pooling. This preserves pretraining dynamics while adapting to XMC. Evidence shows T1 ⊕ E ⊕ eEOS prompt achieves 44.66 P@1 vs 43.14 without prompts (+1.52 absolute). Performance degrades significantly when image tokens are placed first without text prefix, causing attention sinks.

### Mechanism 2: Efficient Early Fusion of Visual Metadata
Single pooled image embedding concatenated with text provides multi-modal benefits with minimal overhead. Frozen SigLIPv2 vision encoder produces one embedding per image; linear projection adapts dimensionality; concatenation enables cross-modal attention during forward pass. Visual information resolves short-text ambiguity in product titles/descriptions. DistilBERT with ViXML achieves 49.55 P@1 vs text-only Qwen2.5-3B at 47.42, demonstrating "an image is worth billions of parameters."

### Mechanism 3: Sample-Efficient Contrastive Learning at Scale
Pre-trained decoder knowledge enables order-of-magnitude reduction in training epochs compared to encoders. Parametric knowledge in LLMs provides strong initialization; NGAME hard negative mining focuses contrastive learning on informative label pairs; triplet loss with margin separates embeddings in shared space. Encoders train for 300 epochs vs decoders for only 30, while achieving comparable or better performance. Off-the-shelf embeddings significantly underperform task-specific contrastive training.

## Foundational Learning

- **Siamese/Dual-encoder learning for retrieval**: Separately encode queries and labels into shared embedding space for efficient maximum inner product search. Quick check: Can you explain why bi-encoder architectures enable sub-linear inference in million-label spaces compared to cross-encoders?

- **Contrastive learning with triplet/hard negative mining**: Optimization objective that pulls positive query-label pairs together while pushing negatives apart; NGAME mining selects informative negatives. Quick check: Why does random negative sampling fail in extreme label spaces compared to hard negative mining?

- **LoRA (Low-Rank Adaptation) for parameter-efficient fine-tuning**: Enables fine-tuning 3-7B parameter LLMs on single GPU by freezing base weights and training only low-rank adapters. Quick check: What is the memory reduction factor when using LoRA rank-256 vs full fine-tuning on a 7B model?

## Architecture Onboarding

- **Component map**: Text encoder/decoder (MiniLM/DistilBERT/BERT/Qwen2.5) -> Frozen SigLIPv2 vision encoder -> Linear projection layer -> Contrastive head (L2-normalized embeddings + triplet loss) -> ANN search at inference

- **Critical path**: 1) Extract and cache image embeddings offline (SigLIPv2, frozen) 2) Concatenate projected image embeddings with text tokens 3) Forward through text model with prompt template 4) Mean pool for sentence embedding 5) Contrastive loss with NGAME hard negatives 6) Pre-compute label embeddings post-training 7) ANN search at inference

- **Design tradeoffs**: Early-fusion (ViXML) vs late-fusion (MUFIN): Early-fusion +1.5% P@1, simpler architecture; Single image embedding vs token-level: Single embedding limits granularity but preserves efficiency; Frozen vs fine-tuned vision encoder: Frozen enables feature banking; fine-tuning unexplored; Mean pooling vs last-token pooling: Mean pooling selected; last-token slightly degrades

- **Failure signatures**: P@1 significantly below text-only baseline: Check image availability at inference; Training instability with LLMs: Increase LoRA rank/alpha (authors use rank-256); Performance no improvement with larger LLM: May indicate prompt template issue or insufficient epochs for larger models

- **First 3 experiments**: 1) Reproduce text-only DistilBERT baseline on LF-AmazonTitles-131K (target: ~45 P@1) 2) Add ViXML (3 images, frozen SigLIPv2, linear projection) to baseline; expect ~49.55 P@1 3) Qwen2.5-0.5B text-only with prompt template, confirm ~45 P@1 with 30 epochs, then add images

## Open Questions the Paper Calls Out

### Open Question 1
Does bidirectional attention outperform the currently used unidirectional attention for extracting embeddings in dual-decoder XMC learning? The authors maintain unidirectional attention but "leave for future work the open question of whether bi-directional or uni-directional attention works best." Ablation studies comparing bidirectional and unidirectional attention mechanisms would resolve this.

### Open Question 2
Can more complex adaptation layers significantly improve performance over the simple linear projection used to fuse visual and text embeddings? The paper used a learnable linear layer and explicitly "leave[s] the study of more complex adaptation choices for future work." Comparative experiments evaluating multi-layer perceptrons or cross-attention modules would resolve this.

### Open Question 3
How can the ViXML framework be made robust to scenarios where visual metadata is missing or sparse during inference? The framework currently assumes image availability; the authors note that removing images from the trained multi-modal model degrades performance below text-only baselines. Experiments utilizing modality dropout or specific masking tokens during training would address this.

## Limitations
- Single pooled image embedding approach may limit granularity compared to token-level features
- Reliance on pre-extracted image embeddings creates deployment constraint requiring images at inference time
- Theoretical justification for decoder-only LLM performance comparable to encoders remains unproven

## Confidence
- **High confidence**: Overall architecture design and experimental methodology are well-documented and reproducible; performance improvements (5.07-8.21% P@1) are substantial and consistently observed
- **Medium confidence**: Specific implementation details of PRIME base method modifications and exact NGAME negative mining configuration
- **Low confidence**: Theoretical justification for why decoder-only LLMs with prompt templates perform comparably to encoder architectures in this task

## Next Checks
1. **Ablation study on image token ordering**: Systematically test the attention sink hypothesis by placing image tokens at different positions in the decoder input sequence to quantify exact performance degradation when prompt structure is violated.

2. **Zero-shot image availability test**: Train a model with ViXML on three images, then evaluate on same test set with images randomly dropped to 0, 1, 2, or 3 images available to measure performance penalty and validate "images required at inference" limitation.

3. **NGAME hyperparameter sensitivity**: Conduct grid search over key NGAME parameters (margin values, number of hard negatives per batch, mining frequency) to determine their impact on final performance and identify whether reported improvements are robust to these settings.