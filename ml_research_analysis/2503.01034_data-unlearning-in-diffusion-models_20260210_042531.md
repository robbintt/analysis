---
ver: rpa2
title: Data Unlearning in Diffusion Models
arxiv_id: '2503.01034'
source_url: https://arxiv.org/abs/2503.01034
tags:
- unlearning
- siss
- diffusion
- data
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of data unlearning in diffusion
  models, which is the efficient removal of specific training datapoints from these
  models. The core method, called Subtracted Importance Sampled Scores (SISS), is
  a new loss function that combines the objectives of naive deletion and NegGrad,
  utilizing importance sampling for computational efficiency.
---

# Data Unlearning in Diffusion Models

## Quick Facts
- **arXiv ID**: 2503.01034
- **Source URL**: https://arxiv.org/abs/2503.01034
- **Reference count**: 40
- **Primary result**: SISS achieves Pareto optimality between model quality and unlearning strength, cutting SSCD similarity by over half on CelebA-HQ while maintaining high quality

## Executive Summary
This paper addresses the problem of data unlearning in diffusion models, proposing a method called Subtracted Importance Sampled Scores (SISS) that efficiently removes specific training datapoints while preserving model quality. The core innovation combines naive deletion and NegGrad objectives using importance sampling to reduce computational cost by half. The method demonstrates effectiveness across multiple domains including face generation (CelebA-HQ), handwritten digits (MNIST), and large-scale text-to-image models (Stable Diffusion), achieving state-of-the-art unlearning performance while maintaining generation quality.

## Method Summary
SISS is a loss function for diffusion models that enables efficient data unlearning through importance sampling. The method samples noisy data from a defensive mixture distribution and computes importance weights to correct for sampling bias, allowing both retention and unlearning objectives to be computed in a single forward pass. A superfactor hyperparameter scales the unlearning term, and gradient clipping ensures the retention loss decreases despite the opposing gradient. The approach achieves computational efficiency while maintaining theoretical equivalence to separate forward passes through retained and unlearned data.

## Key Results
- On CelebA-HQ, SISS reduced SSCD similarity metric by over half while maintaining FID of 20.3 when unlearning 50 celebrity faces
- On MNIST T-Shirt task, SISS increased exact likelihood by a factor of 8 compared to baselines
- On Stable Diffusion, SISS successfully mitigated memorization on nearly 90% of the 45 tested prompts
- SISS achieved Pareto optimality along quality and unlearning strength dimensions across all experiments

## Why This Works (Mechanism)

### Mechanism 1: Importance Sampling Unification of Retention and Unlearning Objectives
SISS reduces computational cost by half by sampling from a defensive mixture distribution and using importance weights to correct for sampling bias. This allows both retention and unlearning terms to be computed from a single sample rather than requiring separate forward passes. The importance weights remain numerically stable when 0<λ<1, with bounds of 1/(1-λ) and 1/λ.

### Mechanism 2: Superfactor-Weighted NegGrad for Targeted Unlearning
The superfactor s > 0 scales the unlearning term to enable controllable tradeoff between retention quality and unlearning strength. By adjusting s so that the NegGrad gradient norm is approximately 10% of the naive deletion gradient norm, the method achieves effective unlearning without quality collapse. This prevents the model from over-forgetting while still suppressing generation of targeted datapoints.

### Mechanism 3: Gradient Clipping Preserves Retention Loss Under Perturbation
A perturbed descent lemma proves that bounding the NegGrad perturbation magnitude ensures descent on the retention objective. For Lipschitz gradients and small step sizes, the update maintains descent on the retention loss despite the opposing unlearning gradient. This theoretical guarantee validates the empirical observation that SISS maintains quality during unlearning.

## Foundational Learning

- **DDPM Forward Process and Score Matching**: Understanding how noise is injected at timestep t is essential for computing target scores (x_t - γ_tx)/σ_t and ε_θ(x_t, t). Quick check: Given clean image x_0, write the distribution of x_t at timestep t=250. What does ε_θ predict?
- **Importance Sampling for Variance Reduction**: SISS uses importance sampling to sample from mixture q_λ while estimating expectations under q(·|x) and q(·|a). Quick check: If λ=0.5 and sample m_t is much more likely under q(·|x) than q(·|a), what happens to the importance weight for the retention term?
- **Gradient Ascent for Unlearning (NegGrad)**: NegGrad maximizes loss on unlearning set A by ascending the gradient ∇_θ L_A(θ). Quick check: Why does pure NegGrad (maximizing L_A) cause model quality to collapse, as shown in Table 1?

## Architecture Onboarding

- Component map: Input: Retained data X\A, Unlearning set A, Pretrained model ε_θ -> Sample x ~ p_X, a ~ p_A -> Sample timestep t -> Sample m_t ~ q_λ(m_t|x,a) [defensive mixture] -> Compute targets: (m_t - γ_t·x)/σ_t , (m_t - γ_t·a)/σ_t -> Compute importance weights -> Compute SISS loss (Eq. 8) -> Gradient step with clipped superfactor

- Critical path: The importance weight computation (numerical stability) and gradient norm ratio (superfactor adjustment) are the two most fragile operations. Both depend on the relationship between q(·|x) and q(·|a) at sampled timesteps.

- Design tradeoffs: λ=0.5 vs λ=0 or λ=1 (λ=0.5 empirically balances retention/unlearning; λ=0→pure naive deletion; λ=1→quality collapse); SISS (No IS) vs SISS (No IS uses two forward passes, doubling compute/memory but avoiding importance weight variance); Superfactor s controls unlearning strength

- Failure signatures: SSCD remains high, FID stable (unlearning too weak); FID explodes, SSCD drops to ~0 (unlearning too aggressive); Numerical instability on large models (importance weights exploding); Memorization persists on Stable Diffusion (prompt modification needed)

- First 3 experiments: 1) Ablation on synthetic data: Train DDPM on MNIST+single outlier image, measure generation rate of outlier as function of fine-tuning steps. Verify SISS (λ=0.5) reaches 0% faster than naive deletion without IS degradation. 2) Gradient norm ratio sweep: On CelebA-HQ unlearning task, sweep target ratio p ∈ {0.05, 0.1, 0.2, 0.5} for ||∇_unlearn||/||∇_retain||. Plot FID vs SSCD frontier to validate 10% heuristic. 3) Importance weight statistics: Log importance weight distributions during training. Check for extreme values (>100) that indicate numerical instability; correlate with quality degradation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the prompt modification step required for fully memorized prompts be automated to remove the need for manual token manipulation?
- Basis in paper: The authors state, "In the future, we hope to... find a way around the 'prompt modification' step that is hard to automate."
- Why unresolved: The current method relies on manually inserting tokens to create "partially-memorized" prompts to generate the synthetic dataset (X \ A), which is a scalability bottleneck.
- What evidence would resolve it: An algorithmic approach that autonomously perturbs embeddings or prompts to induce sample diversity for the unlearning set without human oversight.

### Open Question 2
- Question: Does the requirement to possess the specific training data to be unlearned satisfy legal standards for copyright or privacy protection?
- Basis in paper: The paper notes that while successful technically, the method "may not be enough legally for copyrighted data since that data is a part of the unlearning process itself."
- Why unresolved: A data holder requesting removal might not legally allow the model owner to access their data for the gradient ascent step, creating a compliance paradox.
- What evidence would resolve it: A theoretical or empirical demonstration of unlearning effectiveness using only generated samples or latent representations of the forget set, rather than the raw data itself.

### Open Question 3
- Question: How robustly does the unlearning of a modified prompt generalize to the original fully-memorized prompt in latent space?
- Basis in paper: The authors list "analyze the unlearning generalization across prompts in more detail" as a primary future direction.
- Why unresolved: While SISS showed better generalization than baselines, the exact mechanism by which fine-tuning on a perturbed prompt suppresses the generation of the exact memorized training image is not fully detailed.
- What evidence would resolve it: A layer-wise analysis of the denoising network showing that updates from the modified prompt reliably suppress the attention maps or features associated with the original prompt.

## Limitations
- Theoretical justification for importance sampling lacks rigorous proof of unbiasedness for the full SISS loss
- Superfactor adjustment mechanism described as heuristic without specification of update frequency or algorithmic details
- Experimental scope limited to relatively small diffusion models and single case study on Stable Diffusion
- Does not address potential catastrophic forgetting of model capabilities beyond specific datapoints

## Confidence
- **High Confidence**: SISS successfully reduces SSCD similarity while maintaining FID quality on CelebA-HQ and MNIST
- **Medium Confidence**: Pareto optimality claim holds within tested hyperparameter ranges, but stability across architectures uncertain
- **Low Confidence**: Theoretical claim of computational efficiency through importance sampling not rigorously proven

## Next Checks
1. **Ablation on Synthetic Data**: Train a DDPM on MNIST + single outlier image, measure generation rate of outlier as function of fine-tuning steps. Verify SISS (λ=0.5) reaches 0% faster than naive deletion without IS degradation.
2. **Gradient Norm Ratio Sweep**: On CelebA-HQ unlearning task, sweep target ratio p ∈ {0.05, 0.1, 0.2, 0.5} for ||∇_unlearn||/||∇_retain||. Plot FID vs SSCD frontier to validate 10% heuristic.
3. **Importance Weight Statistics**: Log importance weight distributions during training. Check for extreme values (>100) that indicate numerical instability; correlate with quality degradation.