---
ver: rpa2
title: 'InPhyRe Discovers: Large Multimodal Models Struggle in Inductive Physical
  Reasoning'
arxiv_id: '2509.12263'
source_url: https://arxiv.org/abs/2509.12263
tags:
- collision
- physical
- will
- reasoning
- cube
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces INPHYRE, the first visual question answering
  benchmark to evaluate inductive physical reasoning in large multimodal models (LMMs).
  INPHYRE assesses LMMs' ability to predict outcomes of collision events, particularly
  in scenarios that violate universal physical laws.
---

# InPhyRe Discovers: Large Multimodal Models Struggle in Inductive Physical Reasoning

## Quick Facts
- arXiv ID: 2509.12263
- Source URL: https://arxiv.org/abs/2509.12263
- Authors: Gautam Sreekumar; Vishnu Naresh Boddeti
- Reference count: 40
- Primary result: LMMs show limited inductive physical reasoning, especially when demonstration samples violate physical laws or when language bias dominates over visual inputs

## Executive Summary
This paper introduces INPHYRE, the first visual question answering benchmark specifically designed to evaluate inductive physical reasoning in large multimodal models (LMMs). The benchmark focuses on LMMs' ability to predict collision outcomes, particularly in scenarios that either follow or violate universal physical laws. By providing demonstration samples, INPHYRE measures how well LMMs can infer underlying physics from exemplars and apply this knowledge to make predictions.

The study evaluates 13 diverse LMMs and reveals three critical limitations: LMMs have poor parametric knowledge of universal physical laws and struggle to apply them even in compliant scenarios; their inductive reasoning fails when demonstration samples violate physical laws; and they suffer from significant language bias, often ignoring visual inputs in favor of textual information. These findings raise important questions about the trustworthiness of LMMs in visual reasoning tasks and highlight the need for improved instruction-tuning approaches that better integrate visual and linguistic information.

## Method Summary
The study introduces INPHYRE, a novel visual question answering benchmark designed to evaluate inductive physical reasoning in large multimodal models. The benchmark presents LMMs with collision scenarios where they must predict outcomes based on demonstration samples that may or may not follow universal physical laws. By varying the physical consistency of these demonstrations, the researchers can assess whether models can infer and apply underlying physics principles. The evaluation involves 13 diverse LMMs tested across multiple physical reasoning scenarios, with particular attention to cases where visual demonstrations conflict with textual descriptions or violate physical laws.

## Key Results
- LMMs demonstrate limited parametric knowledge of universal physical laws and struggle to apply them even in scenarios that follow these laws
- Inductive physical reasoning in LMMs is particularly weak when demonstration samples violate universal physical laws
- LMMs exhibit significant language bias, often ignoring visual inputs in demonstration samples in favor of textual information

## Why This Works (Mechanism)
The findings reveal fundamental limitations in how LMMs integrate multimodal information for physical reasoning. The models appear to rely heavily on learned textual patterns rather than genuine physical understanding, leading to poor performance when visual demonstrations conflict with language-based expectations. This suggests that current instruction-tuning approaches fail to properly balance visual and linguistic information, causing models to default to language-based reasoning even when visual evidence contradicts it.

## Foundational Learning
- **Universal Physical Laws**: Fundamental principles governing motion and collisions (needed for: understanding what models should know; quick check: can models predict basic momentum conservation?)
- **Inductive Reasoning**: Ability to infer general principles from specific examples (needed for: evaluating model learning from demonstrations; quick check: can models apply learned physics to novel scenarios?)
- **Multimodal Integration**: Combining visual and textual information effectively (needed for: understanding language bias issues; quick check: do models prioritize visual or textual cues when they conflict?)
- **Parametric Knowledge**: Pre-existing knowledge encoded during pretraining (needed for: assessing baseline physics understanding; quick check: do models perform better on physically plausible scenarios without demonstrations?)
- **Physical Plausibility**: Ability to distinguish realistic from unrealistic physical scenarios (needed for: evaluating model's physical reasoning; quick check: can models identify physically impossible situations?)

## Architecture Onboarding

**Component Map**: INPHYRE Benchmark -> LMM Evaluation -> Performance Analysis -> Language Bias Assessment

**Critical Path**: Input (Visual + Textual) → Feature Extraction → Cross-Modal Fusion → Physical Reasoning → Output Prediction

**Design Tradeoffs**: The study balances between creating realistic physical scenarios and maintaining experimental control, potentially limiting generalizability to real-world complexity.

**Failure Signatures**: 
- Consistently incorrect predictions in physically plausible scenarios
- Over-reliance on textual information when visual evidence contradicts it
- Poor performance on scenarios with physically violating demonstrations
- Inability to transfer learned physics from demonstrations to new scenarios

**First Experiments**:
1. Test LMM performance on physically plausible scenarios without demonstrations to establish baseline physics knowledge
2. Evaluate model predictions when visual and textual information are perfectly aligned
3. Assess model performance when only visual or only textual information is provided

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- The INPHYRE benchmark focuses specifically on collision events, which may not generalize to all physical reasoning domains
- The study may not capture the full complexity of real-world physical reasoning scenarios
- Results may be specific to the particular LMM architectures evaluated and might not apply to all model types

## Confidence

**Claim Clusters and Confidence Labels**:
- Parametric knowledge limitations: High
- Inductive reasoning with violating samples: Medium
- Language bias effects: Medium

## Next Checks
1. Test whether instruction-tuning with balanced visual-linguistic demonstrations reduces language bias in INPHYRE tasks
2. Evaluate whether progressive training on increasingly complex physical scenarios improves inductive reasoning performance
3. Assess whether models pretrained with stronger physical grounding (e.g., video pretraining) show reduced susceptibility to language bias in visual reasoning tasks