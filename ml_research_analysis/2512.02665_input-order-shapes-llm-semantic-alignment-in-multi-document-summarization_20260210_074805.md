---
ver: rpa2
title: Input Order Shapes LLM Semantic Alignment in Multi-Document Summarization
arxiv_id: '2512.02665'
source_url: https://arxiv.org/abs/2512.02665
tags:
- position
- input
- neutral
- summaries
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated whether large language models exhibit structural
  biases when generating multi-document summaries, specifically whether the order
  of input articles affects their representational weight. Using 40 triplets of pro-neutral-con
  abortion-related news articles, the authors permuted each triplet into six input
  orders and prompted Gemini 2.5 Flash to generate neutral overviews.
---

# Input Order Shapes LLM Semantic Alignment in Multi-Document Summarization
## Quick Facts
- arXiv ID: 2512.02665
- Source URL: https://arxiv.org/abs/2512.02665
- Authors: Jing Ma
- Reference count: 5
- One-line primary result: Position 1 articles have significantly higher semantic similarity in LLM summaries than later positions, indicating a primacy bias.

## Executive Summary
This study reveals a structural bias in large language models when performing multi-document summarization: the order in which documents are presented significantly affects how much semantic weight each receives in the generated summary. Using triplets of pro-neutral-con abortion-related articles, the authors found that summaries generated by Gemini 2.5 Flash were consistently more semantically aligned with the first-seen article than with those in later positions. This "primacy effect" was detected through semantic similarity metrics (BERTScore) but not through factual consistency measures, suggesting that while the summaries remain factually sound, they may be subtly biased toward the framing of earlier inputs. These findings have important implications for applications that rely on unbiased information synthesis, especially in agentic AI systems where early inputs can disproportionately influence downstream actions.

## Method Summary
The study used 40 triplets of abortion-related news articles (120 total) sourced from LexisNexis, each containing pro, neutral, and con stances, matched by word count. Each triplet was permuted into six input orders, and Gemini 2.5 Flash was prompted to generate a neutral 150â€“220 word summary representing all stances. Summaries were evaluated using ROUGE-L (lexical overlap), BERTScore with RoBERTa-large (semantic similarity), and SummaC-Conv (factual consistency). One-way ANOVA tested for positional effects, with post-hoc pairwise t-tests identifying where differences occurred. The analysis focused on whether the first position had higher similarity scores than later positions, applying FDR-BH and Holm corrections for multiple comparisons.

## Key Results
- Position 1 articles showed significantly higher BERTScore similarity than Positions 2 and 3 (primacy effect).
- No significant difference was found between Positions 2 and 3 in BERTScore scores.
- ROUGE-L and SummaC metrics did not show significant positional effects.
- The bias was consistent across all three stances (pro, neutral, con).

## Why This Works (Mechanism)
None

## Foundational Learning
- **Lexical overlap metrics (ROUGE-L)**: Measure surface-level word overlap between summaries and source articles. Why needed: Provides baseline for content coverage. Quick check: Expect ~0.08 scores; verify summary length matches prompt constraints.
- **Semantic similarity (BERTScore)**: Captures distributional semantic alignment between summaries and sources. Why needed: Detects subtle framing effects beyond word overlap. Quick check: Expect significant Position 1 advantage in BERTScore but not ROUGE-L.
- **Factual consistency (SummaC-Conv)**: Evaluates whether summaries faithfully represent source facts. Why needed: Ensures summaries are factually sound despite positional bias. Quick check: Minimal sensitivity to position suggests factual correctness isn't compromised.
- **Primacy bias**: Systematic preference for information presented first in sequence. Why needed: Explains why Position 1 consistently dominates semantic alignment. Quick check: Replicate with varied input orders to confirm directional effect.
- **Permutation design**: Six orderings per triplet to isolate positional effects. Why needed: Controls for article-specific content while testing position. Quick check: ANOVA should show significant position effect across permutations.
- **FDR-BH and Holm corrections**: Control false discovery rate and family-wise error in multiple comparisons. Why needed: Ensures statistical validity when testing multiple metrics and positions. Quick check: Apply corrections consistently across all post-hoc tests.

## Architecture Onboarding
- **Component map**: Article corpus -> Stance annotation -> Triplet construction -> 6 permutations -> LLM generation -> 3 metrics evaluation -> ANOVA/post-hoc analysis -> Position effect detection
- **Critical path**: The sequence from permutation generation through to ANOVA determines whether positional bias exists; any failure here invalidates the core finding.
- **Design tradeoffs**: Focused on semantic similarity over factual consistency to detect subtle framing biases, but this means factual accuracy wasn't the primary concern.
- **Failure signatures**: Very low metric scores overall (expected ROUGE-L ~0.08, BERTScore near 0, SummaC ~0.22) or absence of Position 1 advantage in BERTScore indicates either metric implementation issues or absence of the hypothesized bias.
- **First experiments**: (1) Verify that Position 1 yields higher BERTScore than Positions 2/3; (2) Confirm ROUGE-L shows no positional effect; (3) Check that SummaC remains position-insensitive.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does the observed primacy effect generalize across different LLM architectures?
- Basis in paper: [explicit] The "Future Work" section explicitly calls for evaluating multiple LLM families (e.g., GPT, Claude).
- Why unresolved: The study isolated findings to the Gemini 2.5 Flash model.
- What evidence would resolve it: Replicating the input permutation protocol on diverse model architectures.

### Open Question 2
- Question: Do input order effects persist across controversial topics other than abortion?
- Basis in paper: [explicit] The authors suggest extending the dataset to topics like gun control, LGBTQ+ rights, and immigration.
- Why unresolved: The current dataset is restricted to abortion-related news, limiting domain generalization.
- What evidence would resolve it: Applying the same experimental design to a multi-domain dataset.

### Open Question 3
- Question: How does document length interact with input order to influence representational weight?
- Basis in paper: [inferred] The "Limitations" section notes that while length was controlled, metric sensitivity to document length was not explicitly examined.
- Why unresolved: It is unclear if the primacy effect strengthens or weakens with varying context lengths.
- What evidence would resolve it: Conducting experiments with variable-length article triplets.

### Open Question 4
- Question: Are factual consistency metrics sufficient for detecting semantic framing biases?
- Basis in paper: [inferred] The Discussion notes SummaC showed minimal sensitivity to the primacy effect detected by BERTScore.
- Why unresolved: Factual consistency metrics may fail to capture subtle framing shifts caused by input order.
- What evidence would resolve it: Developing evaluation metrics sensitive to semantic framing and stance leakage.

## Limitations
- Dataset reconstruction uncertainty: Exact articles cannot be perfectly replicated, introducing potential variance in topic framing and stance distribution.
- Single model and domain: Findings are limited to Gemini 2.5 Flash and abortion-related content, restricting generalizability.
- Metric sensitivity: Semantic alignment captures distributional similarity but may not fully reflect practical summarization quality or user-perceived neutrality.

## Confidence
- **High confidence**: The existence of a statistically significant primacy effect in BERTScore scores (Position 1 > Positions 2/3) as demonstrated by one-way ANOVA and post-hoc tests
- **Medium confidence**: The practical significance of this bias for real-world multi-document summarization applications, particularly in agentic AI systems
- **Medium confidence**: The characterization of the effect as "primacy bias" based on lexical and semantic overlap metrics, though factual consistency (SummaC) showed no positional effects

## Next Checks
1. Replicate the analysis with a different LLM (e.g., GPT-4o, Claude 3) using the same 40 triplets to assess model-specific versus general primacy effects
2. Test the phenomenon across multiple domains (e.g., climate change, economic policy) to determine if the bias is topic-specific or domain-general
3. Conduct ablation studies varying summary length requirements and prompt specificity to identify whether these factors modulate the observed positional bias