---
ver: rpa2
title: 'ProcrustesGPT: Compressing LLMs with Structured Matrices and Orthogonal Transformations'
arxiv_id: '2506.02818'
source_url: https://arxiv.org/abs/2506.02818
tags:
- matrices
- matrix
- arxiv
- orthogonal
- compression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ProcrustesGPT, a fine-tuning-free framework
  for compressing large language models (LLMs) using structured matrices and orthogonal
  transformations. The key insight is that LLM outputs are invariant under certain
  orthogonal transformations of weight matrices, which can be leveraged to improve
  compressibility within structured matrix classes.
---

# ProcrustesGPT: Compressing LLMs with Structured Matrices and Orthogonal Transformations

## Quick Facts
- arXiv ID: 2506.02818
- Source URL: https://arxiv.org/abs/2506.02818
- Reference count: 17
- Key outcome: ProcrustesGPT achieves 25% compression of LLM weight matrices while maintaining perplexity and zero-shot task performance, outperforming fine-tuning-free baselines.

## Executive Summary
ProcrustesGPT introduces a fine-tuning-free framework for compressing large language models by leveraging the invariance of LLM outputs under orthogonal transformations of weight matrices. The method formulates compression as an optimization problem to find orthogonal transformations that minimize the difference between original and compressed network outputs on calibration data. Evaluated on OPT and Llama2 models using Kronecker product and GS matrix representations, ProcrustesGPT achieves 25% compression while maintaining perplexity and zero-shot task performance. The approach demonstrates consistent gains across multiple model sizes and tasks, with Kronecker product decompositions showing slightly better accuracy than GS matrices.

## Method Summary
ProcrustesGPT leverages the theoretical insight that LLM outputs remain invariant under certain orthogonal transformations of weight matrices. The framework formulates compression as an optimization problem where orthogonal transformations are found to minimize the difference between original and compressed network outputs on calibration data. The method is evaluated using two structured matrix representations: Kronecker product decompositions and GS matrices. Unlike fine-tuning approaches, ProcrustesGPT operates as a pre-processing step that transforms weight matrices to improve compressibility without requiring gradient updates to the original model parameters.

## Key Results
- Achieves 25% compression of weight matrices while maintaining perplexity and zero-shot task performance
- Outperforms SliceGPT and other fine-tuning-free baselines, with WikiText2 perplexity of 36.08 vs 38.65 for SliceGPT
- Kronecker product decompositions show slightly better accuracy than GS matrices, while GS matrices offer computational efficiency benefits for inference acceleration

## Why This Works (Mechanism)
ProcrustesGPT exploits the mathematical property that neural network outputs are invariant under orthogonal transformations of weight matrices. This allows the framework to find optimal orthogonal transformations that improve compressibility within structured matrix classes without affecting model behavior. The optimization process effectively reparameterizes the weight matrices to align with the most compressible structure while preserving the original function mapping.

## Foundational Learning
- Orthogonal transformations: Matrix operations that preserve inner products and norms, crucial for maintaining model behavior while improving compressibility. Quick check: Verify that Q^T Q = I for orthogonal matrix Q.
- Structured matrices: Matrices with specific patterns (like Kronecker products or GS matrices) that enable more efficient storage and computation. Quick check: Confirm that structured matrices reduce storage from O(nÂ²) to O(n log n) or better.
- Perplexity metric: Standard measure of language model quality that quantifies how well a probability distribution predicts a sample. Quick check: Lower perplexity indicates better model performance on language tasks.
- Zero-shot task performance: Model evaluation without task-specific fine-tuning, measuring generalization capability. Quick check: Compare accuracy across different tasks to assess robustness.

## Architecture Onboarding
**Component map:** Original weight matrices -> Orthogonal transformation optimization -> Structured matrix decomposition -> Compressed model
**Critical path:** The orthogonal transformation optimization step is critical as it directly impacts compression quality and model performance
**Design tradeoffs:** Kronecker products offer better accuracy but may have higher computational overhead vs GS matrices which provide better inference efficiency
**Failure signatures:** Degradation in perplexity or task performance indicates suboptimal orthogonal transformations or poor calibration data quality
**First experiments:** 1) Validate orthogonal invariance property on small MLP layers, 2) Compare compression ratios across different structured matrix classes, 3) Test sensitivity to calibration dataset size

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Theoretical foundation relies on orthogonal equivalence invariance assumption requiring deeper mathematical characterization
- Dependence on calibration data quality and representativeness for optimal orthogonal transformation selection
- Limited analysis of fine-tuning scenarios and long-context performance

## Confidence
- High confidence: Claims about achieving 25% compression while maintaining perplexity and task performance relative to baselines
- Medium confidence: Claims about Kronecker product decompositions showing better accuracy than GS matrices, given limited architectural exploration
- Medium confidence: Claims about GS matrices offering computational efficiency benefits for inference acceleration, requiring more detailed hardware-level benchmarks

## Next Checks
1. Test ProcrustesGPT on multilingual models and non-English benchmarks to verify orthogonal transformation invariance generalizes across languages
2. Conduct ablation studies on calibration dataset size and composition to quantify sensitivity to data quality and quantity
3. Measure actual inference latency improvements on different hardware platforms (GPU vs CPU) for GS matrix implementations