---
ver: rpa2
title: Learning Rate Scheduling with Matrix Factorization for Private Training
arxiv_id: '2511.17994'
source_url: https://arxiv.org/abs/2511.17994
tags:
- learning
- rate
- logn
- meanse
- maxse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses differentially private model training with
  stochastic gradient descent (SGD) under learning rate scheduling and correlated
  noise via matrix factorization. Prior work focused on constant learning rates, but
  learning rate schedules are widely used in practice to accelerate training and improve
  convergence.
---

# Learning Rate Scheduling with Matrix Factorization for Private Training

## Quick Facts
- **arXiv ID:** 2511.17994
- **Source URL:** https://arxiv.org/abs/2511.17994
- **Reference count:** 40
- **Primary result:** Proposes learning-rate-aware matrix factorization that improves DP-SGD accuracy under non-constant learning rates

## Executive Summary
This paper addresses differentially private model training with stochastic gradient descent (SGD) under learning rate scheduling and correlated noise via matrix factorization. Prior work focused on constant learning rates, but learning rate schedules are widely used in practice to accelerate training and improve convergence. The authors close this gap by deriving general upper and lower bounds for a broad class of learning rate schedules in both single- and multi-epoch settings. They propose a learning-rate-aware factorization that achieves improvements over prefix-sum factorizations under both MaxSE and MeanSE error metrics. The theoretical analysis yields memory-efficient constructions suitable for practical deployment. Experiments on CIFAR-10 and IMDB datasets confirm that schedule-aware factorizations improve accuracy in private training, with the learning-rate-aware approach providing further gains over existing methods.

## Method Summary
The paper proposes a learning-rate-aware matrix factorization approach for differentially private SGD that incorporates non-constant learning rate schedules. The core innovation is modifying the standard prefix-sum workload matrix to include learning rate information, creating a weighted prefix-sum matrix $A_\chi = A_1 D$ where $D$ is a diagonal matrix of learning rates. This workload is then factorized using either Toeplitz square root or Banded Inverse Square Root (BISR) methods. The approach allows for more efficient noise distribution when learning rates decay, recovering utility typically lost with rate decay. The paper provides theoretical bounds for both single-epoch and multi-epoch settings, with the BISR method enabling memory-efficient multi-epoch training through banded approximations.

## Key Results
- Learning-rate-aware factorization achieves improvements over prefix-sum factorizations under both MaxSE and MeanSE error metrics
- Banded Inverse Square Root (BISR) factorization enables memory-efficient multi-epoch training while maintaining accuracy
- Experiments on CIFAR-10 and IMDB datasets confirm schedule-aware factorizations improve accuracy in private training
- The learning-rate-aware approach provides further gains over existing matrix factorization methods

## Why This Works (Mechanism)

### Mechanism 1
Encoding the learning rate schedule directly into the noise correlation matrix allows the privacy mechanism to distribute noise more efficiently, recovering utility typically lost when rates decay. The standard prefix-sum workload $A_1$ is replaced by a schedule-aware workload $A_\chi = A_1 D$, where $D$ is a diagonal matrix of learning rates. The paper proposes a learning-rate-aware factorization using the square root of a Toeplitz matrix ($C_\alpha = (A^{Toep}_\chi)^{1/2}$). This structure ensures that the sensitivity $\|C\|_{1 \to 2}$ is minimized relative to the effective signal strength of the decaying gradients. Core assumption: The learning rate schedule is fixed and known prior to training (e.g., exponential or cosine decay) rather than adaptive.

### Mechanism 2
"Banded Inverse Square Root" (BISR) factorization enables the use of complex learning rate schedules in multi-epoch settings without the quadratic memory overhead of dense matrices. Instead of maintaining a dense correlation matrix $C$, this method computes the inverse square root of the workload and zeroes out elements outside a bandwidth $p$. This banded structure is then inverted to serve as the correlation matrix. This approximation preserves the decay-aware properties while scaling to large iteration counts. Core assumption: The correlation strength of the optimal noise matrix decays significantly with distance from the diagonal, justifying the truncation to a band.

### Mechanism 3
Multi-epoch privacy accounting requires strict separation between data participations to prevent noise from "stacking" destructively at specific iterations. The mechanism relies on $b$-min separation, ensuring a single data point cannot appear in two batches closer than $b$ steps apart. This structural constraint allows the sensitivity analysis to treat participations as semi-independent, reducing the amplification factor required for the noise. Core assumption: The data sampling algorithm (e.g., Poisson or Shuffling) strictly enforces the $b$-min separation constraint.

## Foundational Learning

- **Concept: Differential Privacy (DP) Sensitivity**
  - **Why needed here:** The core mathematical object is the sensitivity of the matrix $C$ ($\|C\|_{1 \to 2}$). Without understanding that this norm dictates how much noise is added, the optimization of the factorization makes no sense.
  - **Quick check question:** Does increasing the bandwidth $p$ of the correlation matrix $C$ typically increase or decrease its sensitivity, and why?

- **Concept: Matrix Factorization for Streaming**
  - **Why needed here:** This paper frames training as a matrix multiplication problem ($\Theta = A G$). You must understand why we factor $A = BC$ (into post-processing $B$ and noise-injection $C$) rather than just adding noise directly to gradients.
  - **Quick check question:** In the equation $dAG = B(CG + Z)$, which matrix determines the *correlation structure* of the noise, and which determines how the noisy gradients are aggregated?

- **Concept: Learning Rate Scheduling (Decay)**
  - **Why needed here:** The innovation is adapting DP to non-constant learning rates. You need to grasp how a decay schedule (like exponential) mathematically transforms the "Prefix Sum" problem into a "Weighted Prefix Sum" problem.
  - **Quick check question:** How does the diagonal matrix $D$ in $A_\chi = A_1 D$ mathematically represent a learning rate schedule?

## Architecture Onboarding

- **Component map:** Scheduler Interface -> Workload Generator -> Factorizer -> Noise Injector -> Optimizer Step
- **Critical path:** The generation of the Banded Inverse Square Root (BISR) matrix is the critical setup step. It must occur before training starts.
- **Design tradeoffs:** Dense vs. Banded: Dense factorization gives optimal utility but $O(n^2)$ memory. Banded gives $O(np)$ memory but loses some signal (higher MeanSE). Schedule Specificity: A factorization optimized for Exponential Decay will perform sub-optimally if the operator switches to Cosine Decay mid-training.
- **Failure signatures:** Exploding Loss: If the noise injection matrix $C$ is inverted incorrectly or if the bandwidth is too tight, noise variance may spike unexpectedly. No Convergence: If the schedule in the code doesn't match the schedule used to generate $C$, the "noise cancellation" properties of matrix $B$ won't align with the actual gradient accumulation, leading to a low signal-to-noise ratio.
- **First 3 experiments:**
  1. Baseline Validation: Train on CIFAR-10 with DP-SGD (independent noise) vs. Prefix-Sum MF. Verify that MF improves accuracy as expected from prior work.
  2. Schedule Ablation: Run Exponential Decay using the proposed "Learning-Rate-Aware" Toeplitz factorization ($C_\alpha$) vs. a naive Prefix-Sum factorization ($A_1^{1/2}$). Plot validation accuracy to confirm the theoretical advantage.
  3. Memory Scaling: Implement the BISR mechanism. Sweep bandwidth $p$ (e.g., 32, 64, 128) and plot MeanSE error and memory usage to find the "knee of the curve" for the target hardware.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can learning-rate-aware matrix factorization be effectively adapted to warm-starting schedules where learning rates increase? The current theoretical analysis and optimal constructions are derived specifically for decaying rates; increasing rates may fundamentally alter the stability or structural requirements of the correlation matrix.
- **Open Question 2:** Why does minimizing MeanSE or MaxSE not perfectly correlate with final model accuracy in private training? Current error metrics act as proxies for noise variance but fail to capture how the *distribution* of noise interacts with the non-linearities of the model training process.
- **Open Question 3:** What are the exact leading constants for the MaxSE and MeanSE error bounds in learning-rate-aware factorization? The analysis relies on asymptotic notation ($\Theta$, $O$, $\Omega$), and determining precise constants requires a more fine-grained analysis of the specific workload matrices $A_\chi$.

## Limitations

- **Schedule adaptability:** The paper assumes fixed, pre-specified learning rate schedules and doesn't address how the mechanism performs with adaptive schedules like ReduceLROnPlateau.
- **Model architecture specifics:** While CIFAR-10 results are reported, exact architectural details (layer widths, activation functions) are not provided, making precise reproduction challenging.
- **Implementation details:** The MCMC accounting method for matrix factorization is referenced but not fully specified, requiring implementation from external sources.

## Confidence

- **High confidence:** The theoretical framework for learning-rate-aware matrix factorization and its advantages over prefix-sum methods (Sections 3-4).
- **Medium confidence:** Experimental results showing accuracy improvements, as exact hyperparameters and model architectures are not fully specified.
- **Low confidence:** Claims about multi-epoch performance with adaptive schedules, as the paper focuses on fixed schedules and doesn't address adaptivity.

## Next Checks

1. **Sensitivity analysis:** Systematically vary bandwidth parameter p in BISR factorization to identify the optimal trade-off between memory usage and MeanSE error.
2. **Schedule generalization:** Test the learning-rate-aware factorization on polynomial and cosine decay schedules to validate theoretical predictions beyond exponential decay.
3. **Accounting verification:** Implement and validate the MCMC accountant for matrix factorization to ensure privacy budget calculations match reported results.