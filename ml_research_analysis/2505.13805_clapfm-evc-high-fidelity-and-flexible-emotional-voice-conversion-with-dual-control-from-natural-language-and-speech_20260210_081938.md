---
ver: rpa2
title: 'ClapFM-EVC: High-Fidelity and Flexible Emotional Voice Conversion with Dual
  Control from Natural Language and Speech'
arxiv_id: '2505.13805'
source_url: https://arxiv.org/abs/2505.13805
tags:
- speech
- emotional
- emotion
- arxiv
- clapfm-evc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ClapFM-EVC, a novel emotional voice conversion
  (EVC) framework that enables high-fidelity speech conversion driven by natural language
  prompts or reference speech with adjustable emotion intensity. The core idea is
  to use EVC-CLAP, a contrastive language-audio pretraining model, to extract and
  align fine-grained emotional elements across speech and text modalities.
---

# ClapFM-EVC: High-Fidelity and Flexible Emotional Voice Conversion with Dual Control from Natural Language and Speech

## Quick Facts
- arXiv ID: 2505.13805
- Source URL: https://arxiv.org/abs/2505.13805
- Reference count: 0
- Primary result: Achieves state-of-the-art EVC performance with EECS 0.82, eMOS 3.85, nMOS 4.09, and UTMOS 3.68

## Executive Summary
ClapFM-EVC is a novel emotional voice conversion (EVC) framework that enables high-fidelity speech conversion driven by natural language prompts or reference speech with adjustable emotion intensity. The framework leverages EVC-CLAP, a contrastive language-audio pretraining model, to extract and align fine-grained emotional elements across speech and text modalities. By combining this with a FuEncoder that fuses emotional features with phonetic information from a pre-trained ASR model, ClapFM-EVC achieves state-of-the-art performance in both emotion similarity and speech quality metrics.

## Method Summary
The core innovation of ClapFM-EVC lies in its dual control mechanism that extracts emotional information from either natural language prompts or reference speech. The system employs EVC-CLAP to capture emotional elements across modalities, which are then fused with phonetic features extracted by a pre-trained ASR model using a FuEncoder with adaptive intensity gates. This combined feature representation is fed into a flow matching model to reconstruct the Mel-spectrogram of the source speech with the target emotion. The framework's design enables fine-grained control over emotional intensity while maintaining high speech fidelity.

## Key Results
- EECS (Emotional Expression Consistency Score): 0.82
- eMOS (emotional Mean Opinion Score): 3.85
- nMOS (naturalness MOS): 4.09
- UTMOS (overall quality MOS): 3.68

## Why This Works (Mechanism)
The framework's effectiveness stems from its ability to align emotional information across text and speech modalities through contrastive learning. By leveraging the EVC-CLAP model's pretraining on large-scale audio-text pairs, it can extract nuanced emotional features that are semantically consistent across both modalities. The adaptive intensity gate in the FuEncoder allows for precise control over emotional strength, while the flow matching model ensures high-quality reconstruction of the converted speech.

## Foundational Learning

**Contrastive Language-Audio Pretraining (CLAP)**
*Why needed:* To establish semantic alignment between emotional content in text and speech
*Quick check:* Verify that emotional descriptors in text correspond to similar emotional embeddings in audio space

**Phonetic PosteriorGrams (PPGs)**
*Why needed:* To preserve speaker identity and phonetic content during conversion
*Quick check:* Ensure converted speech maintains original pronunciation and speaker characteristics

**Flow Matching Models**
*Why needed:* To generate high-fidelity Mel-spectrograms while preserving emotional content
*Quick check:* Compare reconstruction quality against ground truth spectrograms

## Architecture Onboarding

**Component Map:**
EVC-CLAP (text/speech input) -> FuEncoder (with intensity gate) -> Flow Matching Model -> Mel-spectrogram output

**Critical Path:**
Text/speech input → EVC-CLAP → FuEncoder → Flow Matching Model → Output

**Design Tradeoffs:**
- Uses pre-trained ASR for phonetic features (reduces training complexity but limits to ASR-supported languages)
- Flow matching for high-quality reconstruction (computationally intensive but yields better quality than GAN-based approaches)
- Dual modality input increases flexibility but adds model complexity

**Failure Signatures:**
- Poor emotional alignment when text/speech input is ambiguous or contains mixed emotions
- Quality degradation when converting highly expressive speech with rapid emotional changes
- Speaker identity loss when emotional intensity is pushed to extremes

**3 First Experiments:**
1. Test emotional conversion quality using only text prompts with varying emotional intensity
2. Evaluate conversion quality using only reference speech with different speakers
3. Measure the impact of intensity gate settings on emotional expression strength

## Open Questions the Paper Calls Out
None

## Limitations
- Subjective evaluation based on small-scale listening tests (20-30 participants)
- Performance tied to quality and coverage of EVC-CLAP training data
- Computational complexity and real-time performance not discussed

## Confidence

**Major Claim Clusters:**
- EVC performance improvements: Medium confidence
- Dual control effectiveness: Medium confidence  
- High-fidelity conversion: High confidence

## Next Checks
1. Conduct large-scale (n>100) subjective evaluations across diverse demographic groups and languages to validate generalizability
2. Perform comprehensive ablation studies testing each component's contribution and the system's robustness to emotional intensity variations
3. Evaluate real-time performance and computational requirements across different hardware configurations for practical deployment assessment