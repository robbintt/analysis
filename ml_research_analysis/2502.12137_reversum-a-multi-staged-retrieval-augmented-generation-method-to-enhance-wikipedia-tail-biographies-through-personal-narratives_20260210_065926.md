---
ver: rpa2
title: 'REVERSUM: A Multi-staged Retrieval-Augmented Generation Method to Enhance
  Wikipedia Tail Biographies through Personal Narratives'
arxiv_id: '2502.12137'
source_url: https://arxiv.org/abs/2502.12137
tags:
- male
- wikipedia
- wiki
- content
- section
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces REVERSUM, a multi-staged retrieval-augmented\
  \ generation method to enhance Wikipedia tail biography articles by leveraging personal\
  \ narratives like autobiographies and biographies. The method employs a four-stage\
  \ pipeline\u2014Relevance Detection, Evidence Collection, Verification, and Summarization\u2014\
  to extract and integrate relevant content from personal narratives into existing\
  \ Wikipedia sections."
---

# REVERSUM: A Multi-staged Retrieval-Augmented Generation Method to Enhance Wikipedia Tail Biographies through Personal Narratives

## Quick Facts
- arXiv ID: 2502.12137
- Source URL: https://arxiv.org/abs/2502.12137
- Reference count: 32
- Outperforms baselines in integrability (92% vs. 75%) and informativeness (96% vs. 67.5%)

## Executive Summary
This paper introduces REVERSUM, a four-stage retrieval-augmented generation pipeline designed to enhance Wikipedia B and C category biography articles by integrating content from personal narratives like autobiographies and biographies. The method employs a stage-gated approach—relevance detection, evidence collection, verification, and summarization—to extract and incorporate novel, factually accurate content while minimizing redundancy and hallucination. Human evaluation shows significant improvements over baseline RAG methods in integrability and informativeness, with automatic metrics confirming higher quality, readability, and understandability across both B and C category articles.

## Method Summary
REVERSUM processes Wikipedia biography sections by first retrieving relevant chunks from personal narratives using sentence-BERT embeddings and MMR-based search. A four-stage pipeline then filters these chunks through relevance detection, extracts specific evidences, verifies them against the source in an isolated LLM session to prevent hallucination, and generates a section-compatible summary. The system uses Llama-3-8b-instruct with a 0.3 similarity threshold for retrieval, and a separate verification chat session to constrain fabrication. It targets B and C category Wikipedia biographies, aiming to expand them with novel, factually grounded content from personal narratives.

## Key Results
- Human evaluation shows REVERSUM-generated content achieves 92% integrability and 96% informativeness, outperforming the best baseline (75% and 67.5% respectively).
- Automatic evaluation demonstrates significant improvements in quality, readability, and understandability for both B and C category biography articles.
- The pipeline reduces redundancy compared to standard RAG, with only 1% of summaries returning "Not possible" due to insufficient information.

## Why This Works (Mechanism)

### Mechanism 1: Stage-gated evidence filtering reduces redundancy
A four-stage pipeline reduces duplicate content generation compared to single-stage RAG by progressively narrowing retrieved context. The pipeline filters retrieved chunks through (1) relevance detection, (2) evidence collection, (3) verification against source, and (4) summarization. Each stage removes irrelevant or redundant information before generation. Core assumption: LLMs can reliably identify relevant chunks and extract non-redundant evidence when prompted with explicit filtering instructions. Evidence anchors: [abstract] "employs a four-stage pipeline: relevance detection, evidence collection, verification, and summarization to ensure factual accuracy and reduce redundancy"; [section] Pilot study found that "in 56% cases the participants mentioned that the generated contents are just a summary of the already existing Wikipedia content" when using standard RAG; [corpus] Related work on structured RAG (SRAG) similarly decomposes retrieval tasks for multi-entity reasoning, suggesting stage decomposition is a broadly useful pattern. Break condition: If retrieved chunks contain no novel information relative to the target section, relevance detection returns "No documents are relevant" and the pipeline halts.

### Mechanism 2: Isolated verification session constrains hallucination
Using a separate LLM chat session for evidence verification—containing only retrieved chunks and extracted evidences—reduces fabrication. By isolating verification from prior chat history and providing only source documents as context, the LLM cannot introduce external knowledge or extrapolate beyond the retrieved text. Core assumption: Hallucinations arise from both model priors and conversational context leakage; removing both leaves only source-grounded outputs. Evidence anchors: [section] "During verification, the input to the LLM contains only the 'retrieved chunks' and 'extracted evidences' from the source material, with no extraneous information"; [section] Qualitative analysis of 50 cases "did not encounter any instances of hallucinations"; GPT-4 faithfulness score of 0.95; [corpus] Weak direct evidence—corpus neighbors focus on Wikipedia generation quality rather than hallucination control mechanisms. Break condition: If verification finds no evidences match the source chunks, the output is "None" and summarization is skipped.

### Mechanism 3: Similarity thresholding prevents low-quality expansion
A retrieval similarity threshold of 0.3 filters out semantically unrelated chunks before generation, reducing failed expansions. Chunks below the threshold are discarded before relevance detection, preventing the pipeline from forcing expansions from irrelevant source material. Core assumption: Semantic similarity (via sentence-BERT embeddings) correlates with expansion utility for a given section. Evidence anchors: [section] "In around 16% cases the retrieved documents are less similar (than threshold value of 0.3) to the existing content"; [section] Table 2 shows 16% non-expansion from retrieval failures, 35% from pipeline judgments of insufficient information; [corpus] No direct corpus evidence on threshold tuning; this appears underexplored in related work. Break condition: Chunks below 0.3 similarity trigger early exit; no expansion is attempted.

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: REVERSUM extends standard RAG with multi-stage filtering. Understanding baseline RAG (retrieve → generate) is prerequisite to appreciating why redundancy occurs.
  - Quick check question: Can you explain why a single-stage RAG might generate content that duplicates existing text?

- Concept: **Vector similarity search and MMR**
  - Why needed here: The retrieval phase uses sentence-BERT embeddings with Maximum Marginal Relevance to balance relevance and diversity in chunk selection.
  - Quick check question: How does MMR differ from pure cosine similarity ranking?

- Concept: **LLM session isolation**
  - Why needed here: The verification stage uses a separate chat session to prevent context contamination. Understanding why chat history affects outputs is critical.
  - Quick check question: Why might an LLM produce different outputs for the same query in separate vs. continued chat sessions?

## Architecture Onboarding

- Component map:
  Retriever (RecursiveCharacterTextSplitter → sentence-BERT → ChromaDB → MMR k=4) -> Relevance Detection (LLM filters chunks) -> Evidence Collection (LLM extracts phrases) -> Verification (Separate LLM session confirms source match) -> Summarization (LLM generates section summary)

- Critical path:
  1. Query construction from section title + content
  2. Retrieval with MMR (top-k=4)
  3. Similarity threshold check (≥0.3)
  4. Relevance detection (may halt)
  5. Evidence collection (may return empty)
  6. Verification (may return empty)
  7. Summary generation (may return "Not possible")

- Design tradeoffs:
  - **Chunk size vs. context coherence**: Larger chunks preserve context but dilute relevance; paper uses 600–1200 char range
  - **Threshold strictness**: Higher threshold reduces false expansions but increases non-expansion rate (16% retrieval failures at 0.3)
  - **Separate vs. shared sessions**: Isolation reduces hallucination but increases latency and state management complexity

- Failure signatures:
  - **Retrieval failure**: Similarity < 0.3 → no expansion attempted (16% of cases)
  - **Relevance detection failure**: LLM returns "No documents relevant" (12% of cases)
  - **Verification rejection**: Extracted evidences not found in source (19% of cases)
  - **Empty summary**: LLM returns "Not possible" (1% of cases)

- First 3 experiments:
  1. **Reproduce pilot study redundancy**: Run standard RAG on 10 Wikipedia sections; manually annotate duplicate content rate to establish baseline
  2. **Ablate verification stage**: Compare output faithfulness scores with and without the verification step using a held-out test set
  3. **Tune similarity threshold**: Grid search thresholds from 0.1–0.5; measure tradeoff between expansion rate and faithfulness to identify optimal operating point

## Open Questions the Paper Calls Out

- **Open Question 1**: How can the REVERSUM methodology be adapted to generate content for Wikipedia articles that lack well-defined section structures, such as Stub or Start-class articles? Basis in paper: [explicit] The authors state in Section 9 that they currently limit the methodology to B and C classes because "lower-category articles often lack well-defined sections," and aim to explore generalization in future work. Why unresolved: The current retrieval and generation pipeline relies on matching existing section content and titles to query the personal narrative, which is impossible if the article structure is missing or undifferentiated. What evidence would resolve it: A modified version of REVERSUM that generates text for unstructured inputs, evaluated on Stub-class articles for coherence and informativeness.

- **Open Question 2**: To what extent does independent section enhancement introduce inter-section redundancy, and can document-level context be integrated to mitigate it? Basis in paper: [explicit] Section 9 acknowledges that the system enhances sections independently and does not "explicitly measure inter-section alignment," suggesting future work should explore alignment strategies to minimize overlap. Why unresolved: The current pipeline treats each section as an isolated task, lacking a mechanism to track information already generated for previous sections within the same biography. What evidence would resolve it: A comparison of redundancy rates (e.g., n-gram overlap or semantic similarity) between section-level generation and a holistic, document-level generation approach.

- **Open Question 3**: Can the pipeline be modified to automatically detect and neutralize subjective bias from autobiographical sources to adhere to Wikipedia's Neutral Point of View (NPOV) policy? Basis in paper: [explicit] Section 11 lists the "reliance on personal narratives" as a limitation because these sources "often reflect personal perspectives... which could be in conflict with Wikipedia’s neutral point of view policy." Why unresolved: While the model ensures factual faithfulness to the source text, it does not appear to contain a specific module for style transfer or bias mitigation regarding subjective claims or self-praise. What evidence would resolve it: An evaluation of the generated text using a bias detection classifier or human annotation specifically targeting NPOV violations.

## Limitations
- The pipeline currently only works on Wikipedia B and C category articles with well-defined sections, limiting generalizability to lower-quality or unstructured articles.
- The 0.3 similarity threshold for retrieval is empirically set without extensive tuning analysis, leaving uncertainty about optimal performance across different domains.
- The method relies on personal narratives which may introduce subjective bias, conflicting with Wikipedia’s Neutral Point of View policy, and lacks a specific mechanism to detect or neutralize such bias.

## Confidence
- **High Confidence**: The 4-stage pipeline architecture and its components (retrieval, relevance detection, evidence collection, verification, summarization) are clearly specified and reproducible. The automatic evaluation results (Quality score improvements) are directly measurable from the reported formulas.
- **Medium Confidence**: Human evaluation results (integrability, informativeness) are robust but depend on subjective judgment; the reported percentages (92%, 96%) reflect calibrated scoring but may not generalize across different annotator pools.
- **Low Confidence**: Claims about hallucination reduction lack strong empirical backing; the single faithfulness score (0.95) and absence of systematic hallucination audits weaken confidence in this mechanism.

## Next Checks
1. **Hallucination Audit**: Systematically test the pipeline on a diverse set of inputs, comparing outputs with and without the verification stage to quantify hallucination reduction.
2. **Threshold Sensitivity**: Run ablation studies across similarity thresholds (0.1–0.5) to identify optimal settings and characterize the quality/expansion rate tradeoff.
3. **Generalization Test**: Apply REVERSUM to FA/GA Wikipedia articles to assess whether the method maintains or degrades quality on higher-standard content.