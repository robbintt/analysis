---
ver: rpa2
title: 'Discovering Partially Known Ordinary Differential Equations: a Case Study
  on the Chemical Kinetics of Cellulose Degradation'
arxiv_id: '2504.03484'
source_url: https://arxiv.org/abs/2504.03484
tags:
- values
- data
- dataset
- pinns
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates the use of Physics-Informed Neural Networks
  (PINNs) and symbolic regression to discover unknown parameters and functions in
  differential equations modeling cellulose degradation in power transformers. The
  method successfully recovers the Arrhenius equation parameters (activation energy
  and pre-exponential factor) in the Ekenstam ODE with relative L2 errors below 0.05,
  and identifies an unknown function in Emsley's system of ODEs.
---

# Discovering Partially Known Ordinary Differential Equations: a Case Study on the Chemical Kinetics of Cellulose Degradation

## Quick Facts
- arXiv ID: 2504.03484
- Source URL: https://arxiv.org/abs/2504.03484
- Reference count: 31
- This study demonstrates using PINNs and symbolic regression to discover unknown parameters and functions in differential equations modeling cellulose degradation.

## Executive Summary
This work applies Physics-Informed Neural Networks (PINNs) and symbolic regression to discover unknown parameters and functions in differential equations describing cellulose degradation in power transformers. The methodology successfully recovers the Arrhenius equation parameters (activation energy and pre-exponential factor) in the Ekenstam ODE with high accuracy, and identifies an unknown function in Emsley's system of ODEs. The approach combines physical knowledge with machine learning to extract meaningful insights from limited measurements, offering a promising methodology for modeling aging processes in power system components.

## Method Summary
The method uses PINNs to embed known differential equations as constraints in the loss function, allowing simultaneous data fitting and physics enforcement. For unknown parameters, these are treated as trainable weights. For unknown functions, an auxiliary neural network is trained alongside the main solver network, with its outputs fed to symbolic regression (PySR) to discover interpretable mathematical expressions. The approach employs logarithmic scaling of ODE residuals to handle parameters with vastly different magnitudes.

## Key Results
- PINN successfully recovers Arrhenius equation parameters with relative L2 errors below 0.05
- Unknown function in Emsley's system is identified with 2.263Â·10^-2 L2 error
- PINN achieves 0.028% error in parameter estimation for synthetic Ekenstam data
- Method demonstrates robustness to noise, though 10% noise causes parameter estimation errors

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: PINNs can estimate unknown constant parameters in known differential equations by embedding the equation residual into the loss function.
- **Mechanism**: A neural network approximates the solution state (e.g., Degree of Polymerization over time). Automatic differentiation computes the time derivative required by the ODE. The loss function minimizes the discrepancy between observed data and the network prediction plus the residual of the ODE itself (the "physics loss"). By treating unknown parameters (like activation energy $E$) as trainable weights, the optimizer converges on values that satisfy both the data trend and the physical constraint.
- **Core assumption**: The underlying physical law (the ODE structure) is correctly specified, even if parameters are unknown.
- **Evidence anchors**:
  - [Abstract] "We apply PINNs to discover the Arrhenius equation's unknown parameters... particularly the activation energy E and the pre-exponential factor A."
  - [Section 2.2] Defines the residual function $f$ (Eq. 4) and the loss MSE (Eq. 5) combining data and ODE components.
  - [Corpus] "Modelling Chemical Reaction Networks using Neural Ordinary Differential Equations" supports the use of neural frameworks for chemical kinetics.
- **Break condition**: If the data is highly noisy (>10%) or the initial parameter guesses are too far from the true values, the optimization may converge to local minima or overfit the noise (Section 3, Fig 8).

### Mechanism 2
- **Claim**: An unknown functional term within a differential equation can be approximated by a secondary neural network and subsequently converted to a symbolic expression.
- **Mechanism**: When the exact form of an ODE term is unknown (e.g., $h(DP, k_1, t)$), an auxiliary neural network is trained in parallel with the primary solver network. The primary network provides state variables as inputs to the auxiliary net, which outputs the unknown function value. This setup creates a flexible "universal approximator" for the missing physics. Once trained, the input-output pairs of this auxiliary net are fed into a symbolic regression algorithm (PySR) to search the space of mathematical expressions for a best fit.
- **Core assumption**: The unknown function is deterministic and can be represented by the chosen symbolic primitives (e.g., multiplication, addition).
- **Evidence anchors**:
  - [Abstract] "We use PINNs and symbolic regression to recover the functional form of one of the ODEs... and to identify an unknown parameter."
  - [Section 2.3] Describes the architecture with an additional NN for function approximation (Fig. 2) and the use of PySR for discovery.
  - [Corpus] "Discovering Symbolic Differential Equations with Symmetry Invariants" aligns with the goal of extracting symbolic laws from data.
- **Break condition**: If the search space for symbolic regression is too broad (allowing many operations), the algorithm may identify a mathematically accurate but physically uninterpretable expression (Section 3, Eq. 16).

### Mechanism 3
- **Claim**: Logarithmic scaling of the loss landscape enables convergence for parameters with vastly different magnitudes.
- **Mechanism**: Physical parameters like the pre-exponential factor $A$ ($\sim 10^8$) and activation energy $E$ ($\sim 10^5$) differ by orders of magnitude from the DP values. Standard MSE loss struggles with these scales. By taking the natural logarithm of the ODE (Eq. 8), the problem transforms into estimating $\ln(A)$ and $E/RT$, bringing gradients to a comparable scale and preventing the optimizer from oscillating or ignoring smaller-magnitude parameters.
- **Core assumption**: The transformation does not alter the convexity of the problem in a way that hides the global minimum.
- **Evidence anchors**:
  - [Section 2.2] "Given the complexity of the equation's values, it is necessary to scale the equation... we introduce a scaling of the equation as follows: f = d(ln(DP))/dt..." (Eq. 8).
  - [Corpus] Weak/No direct evidence for this specific scaling mechanism in the provided neighbors; this is a paper-specific implementation detail.
- **Break condition**: If values approach zero or negative numbers (invalid for logarithms), the loss calculation fails. The paper notes the sigmoid activation is required to keep outputs positive.

## Foundational Learning

- **Concept: Automatic Differentiation (AD)**
  - **Why needed here**: PINNs rely on exact derivatives of the neural network's output with respect to its inputs (time) to compute the ODE residual $f$. Numerical differentiation is too noisy; symbolic differentiation is too costly for complex nets.
  - **Quick check question**: Can you explain why standard backpropagation is insufficient for calculating the physics loss in a PINN? (Hint: Backprop derives loss w.r.t weights, AD here derives output w.r.t inputs).

- **Concept: The Arrhenius Equation**
  - **Why needed here**: This physical law links temperature to reaction rates ($k$). Understanding that $k = Ae^{-E/RT}$ is essential to grasp what parameters the PINN is trying to "discover" and why they are difficult to estimate (exponential sensitivity).
  - **Quick check question**: If temperature $T$ increases, how does the reaction rate $k$ change, assuming activation energy $E$ remains constant?

- **Concept: Symbolic Regression**
  - **Why needed here**: This bridges the gap between the "black box" predictions of a neural network and interpretable mathematical laws. It allows the system to output an equation (e.g., $y = -k \cdot x^2$) rather than just a curve.
  - **Quick check question**: Why is symbolic regression computationally expensive compared to standard linear regression?

## Architecture Onboarding

- **Component map**: Input (time) -> Main NN (approximates DP and k1) -> Physics Engine (computes derivatives and ODE residual) -> Auxiliary NN (approximates unknown function) -> Symbolic Regressor (discovers mathematical expression)

- **Critical path**: The optimization loop. Data loss pulls the NN toward observed points; Physics loss pulls the NN derivatives toward satisfying the ODE. The balance determines the discovered parameters.

- **Design tradeoffs**:
  - **Activation Functions**: The paper uses Sigmoid for the Ekenstam model to ensure positive outputs for logarithmic scaling, whereas Tanh is used for Emsley's system. Choosing incorrectly can cause numerical instability (NaN losses).
  - **Expression Complexity**: Limiting symbolic regression to only multiplication recovered the exact physics (Eq. 15). Allowing addition/subtraction produced a "false" but accurate fit (Eq. 16). You must choose between interpretability/physical consistency and raw accuracy.

- **Failure signatures**:
  - **Overfitting Noise**: If training loss drops but parameter estimation error rises (or parameters oscillate), the model is fitting noise rather than physics (observed in Dataset 1 + 10% noise).
  - **Stiffness/Scaling Issues**: If loss remains high or diverges, check the scaling of the ODE. Large parameter values ($10^8$) often require logarithmic transformation.

- **First 3 experiments**:
  1. **Synthetic Baseline**: Generate data from a known ODE (e.g., Ekenstam) and verify the PINN can recover the exact parameters used to generate it.
  2. **Noise Robustness**: Add Gaussian noise (1%, 5%, 10%) to the synthetic data to determine the failure threshold of the parameter estimation.
  3. **Function Discovery Check**: Train the auxiliary network on a system with a known missing term (e.g., $h(x) = -x^2$) and run symbolic regression to see if the correct operators are recovered before applying to truly unknown systems.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can Bayesian Physics-Informed Neural Networks (BPINNs) mitigate the overfitting and robustness issues observed when training with high-noise data?
- **Basis in paper**: [explicit] The authors state that standard PINNs "lack robustness and start to overfit" with noise and explicitly propose using "Bayesian Physics-Informed Neural Networks (BPINNs)" to quantify uncertainty.
- **Why unresolved**: The current results show that with 10% noise, the model converges to incorrect parameters, and the proposed BPINN solution remains untested in this context.
- **What evidence would resolve it**: Applying BPINNs to the synthetic datasets with 10% Gaussian noise and demonstrating accurate recovery of parameters $A$ and $E$ with quantified uncertainty bounds.

### Open Question 2
- **Question**: To what extent can the proposed framework generalize to discover equations for degradation trajectories with different initial conditions?
- **Basis in paper**: [explicit] The Discussion notes that "results are case-centered" and that "a more generalizable model could help further investigation using different initial values of DP and $k_1$."
- **Why unresolved**: The current methodology is validated on specific single trajectories rather than a unified model that captures the system's dynamics across the entire state space.
- **What evidence would resolve it**: Training the model on a dataset comprising multiple trajectories with varying initial $DP_0$ and evaluating its predictive accuracy on unseen initial conditions.

### Open Question 3
- **Question**: How can the symbolic regression search space be constrained to prevent the discovery of physically inconsistent mathematical expressions?
- **Basis in paper**: [inferred] In the results, symbolic regression identified a function ($5.48 - 6.44 \cdot DP$) that fit the data well but failed to include the variable $k_1$, violating the expected physical structure.
- **Why unresolved**: The study reveals a trade-off where flexible symbolic regression options lead to mathematically accurate but physically wrong equations.
- **What evidence would resolve it**: Implementing dimensional analysis or variable-inclusion constraints within the symbolic regression algorithm to ensure the recovered expression retains dependence on all relevant physical variables.

## Limitations
- Results rely on synthetic data with known ground truth; performance on truly unknown physics remains to be validated
- Symbolic regression can produce mathematically equivalent but physically meaningless expressions when broader operator sets are allowed
- Scaling of ODE residuals is critical but validation on physical systems with different parameter ranges is limited

## Confidence
- **High confidence**: PINN can recover known ODE parameters when equation structure is correct and data is sufficiently clean
- **Medium confidence**: PINN can discover unknown functional forms, but physical interpretability depends heavily on symbolic regression constraints
- **Medium confidence**: Logarithmic scaling effectively handles parameter magnitude differences, though this is implementation-specific

## Next Checks
1. Test parameter discovery robustness by systematically varying initial guesses across 2-3 orders of magnitude to map convergence basin
2. Apply the unknown function discovery pipeline to a physical system with partially known ODEs (e.g., chemical kinetics with missing reaction terms) where ground truth can be verified independently
3. Benchmark against traditional parameter estimation methods (e.g., maximum likelihood, MCMC) to quantify the advantage of the physics-informed approach