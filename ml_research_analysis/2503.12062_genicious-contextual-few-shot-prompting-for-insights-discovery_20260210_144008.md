---
ver: rpa2
title: 'Genicious: Contextual Few-shot Prompting for Insights Discovery'
arxiv_id: '2503.12062'
source_url: https://arxiv.org/abs/2503.12062
tags:
- data
- llms
- language
- query
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Genicious is a data insights discovery tool that enables users
  to query tabular datasets using natural language. It employs a Text-to-SQL approach
  to maintain data confidentiality by generating SQL queries without exposing the
  underlying data.
---

# Genicious: Contextual Few-shot Prompting for Insights Discovery

## Quick Facts
- arXiv ID: 2503.12062
- Source URL: https://arxiv.org/abs/2503.12062
- Authors: Vineet Kumar; Ronald Tony; Darshita Rathore; Vipasha Rana; Bhuvanesh Mandora; Kanishka; Chetna Bansal; Anindya Moitra
- Reference count: 17
- Key outcome: Genicious is a data insights discovery tool that enables users to query tabular datasets using natural language. It employs a Text-to-SQL approach to maintain data confidentiality by generating SQL queries without exposing the underlying data. The system uses a Contextual Few-shot Prompting strategy that dynamically adapts examples based on user queries. Key components include a RAG framework for contextual example retrieval, an embedding model for query representation, and FAISS for similarity search. The tool achieved 70.3% execution accuracy on Spider benchmark using Contextual Few-shot Prompting with GPT-3.5 Turbo. It features role-based access control and REST API integration for secure, scalable deployment. The system demonstrates P95 latency of 6 seconds and handles simple to intermediate queries effectively.

## Executive Summary
Genicious is a natural language querying system for tabular data that uses Text-to-SQL to maintain data confidentiality. The system employs Contextual Few-shot Prompting with RAG to dynamically retrieve semantically similar examples for each query, improving SQL generation accuracy over static few-shot approaches. Key innovations include secure SQL generation without data exposure, role-based access control, and REST API integration for enterprise deployment. The system achieves 70.3% execution accuracy on the Spider benchmark while maintaining P95 latency of 6 seconds.

## Method Summary
The system uses a RAG framework where user queries are embedded and compared against a pre-indexed pool of question-SQL pairs to retrieve top-k similar examples. These examples are dynamically injected into prompts for the LLM along with database schema and system instructions. The LLM generates SQL, which is sanitized to block dangerous operations, then executed against the database. The system employs text-embedding-ada-002 for query embedding, Milvus with FAISS for similarity search, and GPT-3.5 Turbo for SQL generation, with Llama 3.1 showing better performance on Spider but GPT-3.5 Turbo preferred for domain-specific data.

## Key Results
- 70.3% execution accuracy on Spider benchmark using Contextual Few-shot Prompting with GPT-3.5 Turbo
- P95 latency of 6 seconds for query processing
- Optimal k=4 examples for few-shot prompting balances accuracy gains against token costs
- Handles simple to intermediate queries effectively, with performance degradation on complex multi-join queries

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contextual Few-shot Prompting improves SQL generation accuracy over static few-shot by dynamically retrieving semantically similar examples.
- Mechanism: User queries are embedded and compared against a pre-indexed pool of question-SQL pairs. The top-k most similar examples are retrieved and injected into the prompt, providing the LLM with domain-relevant demonstrations that match the query's semantic structure.
- Core assumption: The embedding model captures semantic similarity between natural language questions in a way that transfers to SQL structure relevance.
- Evidence anchors:
  - [abstract] "The system uses a Contextual Few-shot Prompting strategy that dynamically adapts examples based on user queries."
  - [section 3.3] "Using RAG we can make the demonstrations of few-shot prompting more contextual. Based on user query we can retrieve the most relevant and contextual examples."
  - [section 3.4, Table 2] CFS achieves 65-71% across models vs. 51-53% for static few-shot on Spider benchmark.
  - [corpus] Weak direct corpus support; related papers address few-shot learning in different domains (sensor data, vulnerability detection) but not Text-to-SQL specifically.
- Break condition: If the candidate question pool lacks coverage of query themes (e.g., no year-over-year comparison examples), retrieval will fail to surface relevant demonstrations regardless of embedding quality.

### Mechanism 2
- Claim: Text-to-SQL preserves data confidentiality by exposing only schema metadata to the LLM, never raw data.
- Mechanism: The LLM receives the database schema (table names, column names, data types) and the natural language query, then generates SQL. The SQL executes internally against the database, and only results return to the user. Raw data never leaves the secure environment.
- Core assumption: The schema contains sufficient information for the LLM to infer correct table joins, column semantics, and query logic without data samples.
- Evidence anchors:
  - [abstract] "It employs a Text-to-SQL approach to maintain data confidentiality by generating SQL queries without exposing the underlying data."
  - [section 1] "Text-to-SQL acts as a middle ground where the LLM only needs access to the database structure—not the data itself—to generate SQL queries."
  - [corpus] LLM-Powered Knowledge Graphs paper mentions similar enterprise data unification concerns but does not validate this mechanism directly.
- Break condition: If column names are ambiguous or non-descriptive (e.g., `col1`, `col2`), the LLM lacks semantic cues to generate correct queries.

### Mechanism 3
- Claim: Optimal few-shot example count (k=4) balances accuracy gains against token costs and diminishing returns.
- Mechanism: The authors empirically tested accuracy across different k values. Beyond k=4, accuracy improvements plateaued while token costs increased, creating an efficiency ceiling.
- Core assumption: The Spider benchmark's difficulty distribution generalizes to domain-specific datasets.
- Evidence anchors:
  - [section 3.3, Figure 1] "Our findings indicated that beyond k = 4, the improvement in accuracy is not prominent."
  - [section 3.4] Domain-specific testing led to selecting GPT-3.5 Turbo despite Llama 3.1 performing best on Spider, suggesting k=4 may not generalize identically across all contexts.
  - [corpus] Few-Shot Optimization paper (arXiv:2505.18754) addresses example selection quality but does not validate k=4 specifically.
- Break condition: If domain queries are highly heterogeneous, k=4 may under-represent required patterns; if highly homogeneous, fewer examples may suffice.

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: Core to Contextual Few-shot Prompting; understanding embedding-based retrieval is essential for debugging example selection.
  - Quick check question: Can you explain how a user query is transformed into an embedding and matched against a vector store?

- Concept: **Text-to-SQL Formulation**
  - Why needed here: The system's fundamental problem framing; understanding the inputs (Q, S) and output (y) clarifies failure modes.
  - Quick check question: Given a natural language query and database schema, what information does the LLM receive vs. what it does not?

- Concept: **Execution Accuracy vs. Exact Match**
  - Why needed here: Evaluation metrics determine whether SQL is "correct"; execution accuracy checks results, exact match checks syntax structure.
  - Quick check question: If two SQL queries produce identical results but have different clause orderings, which metric would count them as correct?

## Architecture Onboarding

- Component map:
  Offline Phase: Candidate question pool → Embedding model (text-embedding-ada-002) → Vector store (Milvus)
  Query Phase: User query → Embedding → FAISS similarity search → Top-k retrieval → Prompt assembly (instructions + schema + examples) → LLM (GPT-3.5 Turbo) → SQL sanitization → BigQuery execution → REST API response

- Critical path:
  1. User submits natural language query
  2. Query embedded via text-embedding-ada-002
  3. FAISS retrieves k=4 most similar examples from Milvus
  4. Prompt constructed with system instructions, schema, and retrieved examples
  5. LLM generates SQL
  6. SQL scanned for dangerous keywords (DROP, ALTER, UPDATE)
  7. Sanitized SQL executed on database; results returned

- Design tradeoffs:
  - **GPT-3.5 Turbo vs. Llama 3.1**: Spider favored Llama 3.1 (71% vs. 65% CFS), but domain-specific data favored GPT-3.5 Turbo. Choice depends on deployment context.
  - **Self-Consistency discarded**: Minimal accuracy gains vs. 5x latency increase (n=5 model calls).
  - **Static vs. Contextual Few-shot**: Contextual requires offline indexing and vector infrastructure; static is simpler but less accurate.

- Failure signatures:
  - Empty or irrelevant retrieval → LLM generates generic SQL without domain context
  - Ambiguous schema (non-descriptive column names) → LLM hallucinates incorrect joins
  - Complex multi-join queries → P95 latency exceeds 6 seconds; accuracy degrades on "Extra Hard" queries (see Figure 2)
  - SQL sanitization triggered → Query rejected; user receives no results

- First 3 experiments:
  1. **Validate retrieval quality**: Submit 10 queries with known similar examples in the pool; verify FAISS retrieves semantically relevant matches. Check: Do retrieved examples share SQL structure with ground truth?
  2. **Schema ambiguity test**: Create a test table with cryptic column names (e.g., `a`, `b`, `c`) and submit queries; compare accuracy against descriptive names. Quantify accuracy delta.
  3. **Latency breakdown**: Instrument the query phase to measure time spent in embedding, retrieval, LLM inference, and SQL execution. Identify the bottleneck for P95 optimization.

## Open Questions the Paper Calls Out

- How can conversational context and scoped interactions be effectively integrated into Text-to-SQL systems while maintaining data protection? The current system processes queries independently without maintaining conversation state; balancing context retention with security constraints in multi-turn interactions remains unaddressed.

- Can an agentic framework improve Text-to-SQL performance on complex queries (e.g., multi-table joins, nested subqueries) while preserving the 6-second P95 latency? Agentic approaches typically involve multiple reasoning steps, which may conflict with latency requirements; the trade-off remains unexplored.

- What factors explain the performance divergence between Spider benchmark results (Llama 3.1 optimal) and domain-specific datasets (GPT-3.5 Turbo optimal)? The paper does not analyze whether schema complexity, query patterns, or domain vocabulary cause this divergence; model selection criteria remain heuristic.

## Limitations

- The system shows notably weak performance on complex multi-join queries and "Extra Hard" Spider questions, with accuracy dropping below 40% for most models on these query types.
- The claim that schema alone provides sufficient semantic context for accurate SQL generation lacks rigorous validation across diverse schema quality scenarios.
- The evaluation focuses primarily on execution accuracy without reporting robustness metrics like hallucination rates or SQL injection resistance beyond basic keyword blocking.

## Confidence

- **High confidence**: The mechanism of using RAG for contextual few-shot example retrieval (supported by direct evidence in section 3.3 and Table 2 showing CFS outperforming static few-shot)
- **Medium confidence**: The claim that Text-to-SQL preserves data confidentiality (logically sound but minimally validated; assumes schema completeness and LLM reliability)
- **Medium confidence**: The k=4 optimal example count (based on empirical analysis in Figure 1, but may not generalize to heterogeneous domain queries)
- **Low confidence**: The system's effectiveness for complex analytical queries (performance degrades significantly on Spider's "Extra Hard" subset)

## Next Checks

1. **Schema ambiguity test**: Deploy the system on test databases with cryptic vs. descriptive column names (e.g., `col1` vs. `total_revenue_2023`) and measure accuracy differences to quantify schema dependency.

2. **Complex query stress test**: Systematically evaluate on Spider's "Extra Hard" queries (requiring nested queries, window functions, or multiple joins) to identify specific failure patterns and accuracy thresholds.

3. **Hallucination audit**: Implement systematic SQL validation to detect when the system generates queries referencing non-existent tables or columns, measuring hallucination frequency across query difficulty levels.