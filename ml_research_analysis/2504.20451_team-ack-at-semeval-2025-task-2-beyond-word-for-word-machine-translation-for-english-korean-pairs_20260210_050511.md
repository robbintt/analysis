---
ver: rpa2
title: 'Team ACK at SemEval-2025 Task 2: Beyond Word-for-Word Machine Translation
  for English-Korean Pairs'
arxiv_id: '2504.20451'
source_url: https://arxiv.org/abs/2504.20451
tags:
- translation
- entity
- bleu
- wang
- comet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Team ACK evaluated 13 machine translation models, including large
  language models (LLMs) and traditional multilingual translation systems, on English-Korean
  translation of knowledge-intensive and entity-dense text. Automatic metrics (BLEU,
  COMET, M-ETA) and human evaluation by bilingual annotators were used.
---

# Team ACK at SemEval-2025 Task 2: Beyond Word-for-Word Machine Translation for English-Korean Pairs

## Quick Facts
- arXiv ID: 2504.20451
- Source URL: https://arxiv.org/abs/2504.20451
- Reference count: 25
- Primary result: LLMs generally outperform traditional MT models on English-Korean translation, but all models struggle with culturally-nuanced entity translation

## Executive Summary
Team ACK evaluated 13 machine translation models, including large language models (LLMs) and traditional multilingual translation systems, on English-Korean translation of knowledge-intensive and entity-dense text. Automatic metrics (BLEU, COMET, M-ETA) and human evaluation by bilingual annotators were used. LLMs generally outperformed traditional MT models, but all models struggled with entity translation requiring cultural adaptation. Incorrect responses and entity name errors were the most common translation errors. Entity translation quality varied by entity type and popularity level, while automatic metrics often failed to capture these nuances.

## Method Summary
The study evaluated 13 models (OpenAI, Anthropic, Google, xAI, Meta, DeepSeek, and traditional MT systems) on 5,082 English-Korean pairs from the XC-Translate dataset. Models performed zero-shot translation using identical prompts, with outputs evaluated using BLEU, COMET, and M-ETA metrics. A subset of 650 samples (50 pairs × 13 models) underwent human evaluation by bilingual annotators who labeled errors using a constructed taxonomy. Entity types were categorized using Wikidata NER types and Wikipedia page view popularity tiers.

## Key Results
- LLMs (BLEU 0.29-0.39) outperformed traditional MT models (BLEU 0.15-0.20) on automatic metrics
- All models struggled with entity translation requiring cultural adaptation (266/650 samples had "Incorrect Entity Name" errors)
- Entity popularity affected entity-level translation quality (M-ETA variation 0.00224) but not overall sentence quality
- Automatic metrics showed only moderate correlation (point-biserial r=0.41) with human judgment on culturally-nuanced entity translation

## Why This Works (Mechanism)

### Mechanism 1
Entity popularity influences entity-level translation quality but not overall sentence translation quality. Frequently-occurring entities in training data (proxied by Wikipedia page views) are translated more accurately, but standard metrics (BLEU, COMET) remain stable because entities represent a small token proportion within full sentences.

### Mechanism 2
Translation difficulty is partially determined by linguistic properties characteristic of specific entity categories, independent of popularity. Entity types like "Book Series" permit simple literal/phonetic translation, whereas "Plant" and "Natural place" require unique language-specific names, creating varying translation difficulty levels.

### Mechanism 3
Automatic evaluation metrics (BLEU, COMET) show only moderate correlation with human judgment on culturally-nuanced entity translation. BLEU measures n-gram overlap and COMET predicts human judgment through neural models, but neither captures fine-grained entity-level cultural appropriateness (e.g., "Rotten Tomatoes" should be "로튼 토마토" not "썩은 토마토").

## Foundational Learning

- **Concept: Transcreation vs. Transliteration**
  - Why needed here: The paper's central challenge is distinguishing phonetic/literal translation (transliteration) from culturally-adapted translation (transcreation) for entity names.
  - Quick check question: Given "Rotten Tomatoes" in English, should a Korean translation use phonetic transliteration, literal meaning ("썩은 토마토"), or the culturally-established name ("로튼 토마토")?

- **Concept: Entity-Level vs. Sentence-Level Evaluation**
  - Why needed here: The paper demonstrates that standard metrics (BLEU, COMET) measure sentence quality but miss entity-specific errors because entities occupy small token fractions.
  - Quick check question: If a sentence has 20 tokens and 1 entity token is mistranslated, what approximate BLEU score impact would you expect?

- **Concept: Zero-Shot LLM Translation**
  - Why needed here: LLMs outperform traditional MT through in-context learning without task-specific training, but inherit English-centric biases from training data.
  - Quick check question: What advantage does in-context learning provide for terminology-constrained translation compared to traditional MT models?

## Architecture Onboarding

- **Component map:**
  XC-Translate dataset → 13 models (LLMs + MT) → automatic metrics (BLEU, COMET, M-ETA) + human annotators → error taxonomy

- **Critical path:**
  1. Prepare entity-dense English-Korean pairs from XC-Translate (5,082 pairs used)
  2. Run inference across all models with identical translation prompts
  3. Compute BLEU, COMET, M-ETA for each model
  4. Sample subset (50 pairs × 13 models = 650 samples) for human evaluation
  5. Construct error taxonomy from annotator explanations

- **Design tradeoffs:**
  - Metric selection: BLEU is resource-efficient but poorly correlates with human judgment; COMET improves correlation but lacks fine-grained entity insights; M-ETA focuses only on entity accuracy but misses broader sentence quality
  - Human evaluation cost: $150 per annotator for 650 samples; constrained to controlled question templates, which may miss error categories in diverse genres
  - Language coverage: Korean-specific findings may not generalize to other morphologically complex or typologically distant language pairs

- **Failure signatures:**
  - "Incorrect Response" errors (308/650): Model answers the question instead of translating—indicates prompt following failure
  - "Incorrect Entity Name" errors (266/650): Literal/phonetic translation of culturally-established names—indicates insufficient cultural knowledge
  - Claude 3.5 variants underperform: Lower BLEU (0.15-0.20) than other LLMs (0.29-0.39)—possible training distribution mismatch

- **First 3 experiments:**
  1. Replicate automatic evaluation on a subset (e.g., 500 pairs, 3 representative models: o1, Gemini 1.5 Pro, NLLB-200) to verify metric computation and establish baseline.
  2. Test RAG-augmented entity translation using multilingual knowledge graphs (per Conia et al. 2024, cited in Section 2) on low-popularity entities to isolate popularity effects.
  3. Extend error taxonomy validation to a different text genre (e.g., narrative content) to identify missing error categories acknowledged in Limitations.

## Open Questions the Paper Calls Out

### Open Question 1
Do the observed impacts of entity popularity and type on translation quality generalize to morphologically distinct language families outside the Koreanic group? The current study is restricted to English-Korean pairs, limiting generalizability to languages with different structures (e.g., Semitic or Altaic).

### Open Question 2
Does the constructed error ontology scale to long-form documents, or do new error categories emerge outside of controlled question templates? The current dataset was restricted to specific entity-dense questions, potentially obscuring errors unique to narrative cohesion or longer contexts.

### Open Question 3
How can automatic evaluation metrics be refined to better capture the nuances of entity translation given their limited correlation with human judgment? BLEU and COMET scores remain stable across entity popularity levels where human judgment varies, indicating a "gap" in metric sensitivity.

## Limitations
- Human evaluation sample (650 samples) represents only a small fraction of the total dataset (5,082 pairs)
- Controlled annotation interface may miss error categories in diverse text genres beyond knowledge-intensive contexts
- Use of Wikipedia page view statistics as proxy for entity popularity introduces uncertainty about actual training data distribution

## Confidence
- **High Confidence**: LLMs generally outperform traditional MT models; all models struggle with culturally-nuanced entity translation
- **Medium Confidence**: Entity popularity influences entity-level translation quality but not overall sentence quality; moderate correlation between automatic metrics and human judgment
- **Low Confidence**: Specific mechanisms explaining why certain entity types are more challenging than others

## Next Checks
1. Validate entity popularity effects by conducting controlled experiments with RAG-augmented entity translation systems on low-popularity entities
2. Test cross-genre generalizability by extending error taxonomy validation to narrative content to identify missing error categories
3. Replicate automatic metric-human correlation with a larger-scale human evaluation (minimum 2,600 samples) to verify moderate correlation (r=0.41)