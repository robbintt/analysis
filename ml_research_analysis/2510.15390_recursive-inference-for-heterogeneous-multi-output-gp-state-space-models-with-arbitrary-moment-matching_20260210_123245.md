---
ver: rpa2
title: Recursive Inference for Heterogeneous Multi-Output GP State-Space Models with
  Arbitrary Moment Matching
arxiv_id: '2510.15390'
source_url: https://arxiv.org/abs/2510.15390
tags:
- learning
- function
- moment
- kernel
- points
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper develops an enhanced Recursive Gaussian Process State
  Space Model (RGPSSM) method to address the limitations of the original approach
  in handling heterogeneous multi-output transition functions and high-order nonlinearities.
  The key contributions include: (1) a heterogeneous multi-output kernel design allowing
  each output dimension to adopt distinct kernel types, hyperparameters, and input
  variables, improving expressiveness in multi-dimensional dynamics learning; (2)
  an inducing-point management algorithm enabling independent selection and pruning
  for each output dimension, enhancing computational efficiency; and (3) a unified
  recursive inference framework supporting general moment matching approaches (EKF,
  UKF, ADF), enabling accurate learning under strong nonlinearity and significant
  noise.'
---

# Recursive Inference for Heterogeneous Multi-Output GP State-Space Models with Arbitrary Moment Matching

## Quick Facts
- **arXiv ID:** 2510.15390
- **Source URL:** https://arxiv.org/abs/2510.15390
- **Reference count:** 40
- **Primary result:** RGPSSM-H achieves comparable accuracy to offline GPSSMs with 1/100 of runtime, surpassing online GPSSMs by ~70% in accuracy under heavy noise with 1/20 of runtime

## Executive Summary
This paper addresses critical limitations in Recursive Gaussian Process State Space Models (RGPSSM) for heterogeneous multi-output systems with high-order nonlinearities. The authors develop a novel framework featuring a heterogeneous multi-output kernel design that allows each output dimension to adopt distinct kernel types, hyperparameters, and input variables. The unified recursive inference framework supports general moment matching approaches (EKF, UKF, ADF) without first-order linearization, enabling accurate learning under strong nonlinearity and significant noise.

## Method Summary
The RGPSSM-H algorithm implements a heterogeneous block-diagonal kernel structure (Eq. 9) allowing independent kernel types and hyperparameters per output dimension. It employs independent inducing point management for each dimension with novelty-based addition and singularity-based pruning. The unified inference framework augments the inducing set and performs moment matching through prediction-correction steps without first-order linearization. GP hyperparameters are updated online using methods from prior work, with the framework supporting EKF, UKF, or ADF moment matching techniques depending on the noise level and nonlinearity of the system.

## Key Results
- Achieves comparable accuracy to state-of-the-art offline GPSSMs with only 1/100 of runtime
- Surpasses state-of-the-art online GPSSMs by approximately 70% in accuracy under heavy noise with only 1/20 of runtime
- Successfully identifies time-varying parameters in nonlinear systems with independent length-scale adaptation
- Demonstrates numerical stability and computational efficiency through inducing point pruning mechanisms

## Why This Works (Mechanism)

### Mechanism 1: Heterogeneous Multi-Output Kernel Decoupling
The block-diagonal prior covariance matrix $K_{f_1 f_2}$ (Eq. 9) with zero off-diagonal blocks decouples optimization and inference across dimensions, enabling independent inducing-point selection. This independence is critical for computational efficiency but assumes output dimensions are uncorrelated in the prior.

### Mechanism 2: Unified Inference via Inducing-Point Augmentation
The framework introduces latent function prediction $h_t = f(x_t)$ and approximates $p(h_t | f, x_t) \approx p(h_t | f_t, u, x_t)$ (Eq. 18), then augments the inducing set $\bar{u} = [u, f_t]$ (Eq. 19). This transforms infinite-dimensional inference into finite-dimensional moment propagation solvable by standard filters without first-order linearization.

### Mechanism 3: Singularity-Based Inducing Point Pruning
The method evaluates conditional variance $\gamma^k_i$ for retained points (Eq. 26), discarding those below a threshold normalized by max kernel diagonal. This effectively manages kernel matrix singularity from increasing length-scales during hyperparameter adaptation, maintaining computational efficiency.

## Foundational Learning

- **Concept:** Gaussian Process State-Space Models (GPSSMs)
  - **Why needed:** Core model structure where transition function is a distribution over functions, not fixed parameters
  - **Quick check:** Can you explain why standard Kalman Filters cannot be directly applied to a GPSSM? (Hint: Non-parametric nature and infinite dimensions)

- **Concept:** Moment Matching (EKF, UKF, ADF)
  - **Why needed:** Methods to approximate intractable integrals from propagating distributions through non-linear functions
  - **Quick check:** Why would ADF provide "exact moment matching" for Gaussian kernel while EKF does not? (Hint: Linearity vs. analytic properties)

- **Concept:** Inducing Points
  - **Why needed:** Mechanism for tractable online learning by approximating full GP posterior with sparse subset
  - **Quick check:** What is the trade-off between size of inducing point set ($M$) and model performance?

## Architecture Onboarding

- **Component map:** Heterogeneous Kernel Layer -> Inducing Point Manager -> Unified Inference Engine -> Cholesky Stability Module
- **Critical path:** Implementation of Cholesky version of RGPSSM-H algorithm (Appendix C) is critical for numerical stability
- **Design tradeoffs:**
  - EKF vs. UKF vs. ADF: EKF fastest (~2.16s) but least accurate under high noise; ADF most accurate but slowest (~3.22s)
  - Heterogeneous vs. Isomorphic Kernel: Use heterogeneous when dimensions have different physical scales or input dependencies
- **Failure signatures:**
  - Covariance Inflation/Collapse: Cholesky factor update fails or pruning threshold too aggressive
  - Divergence under High Noise: EKF performance degrades severely at $\sigma^2_m \geq 0.8$
  - Stagnant Learning: Rapid pruning prevents adaptation to new state space regions
- **First 3 experiments:**
  1. 1D "Kink" Function: Replicate with high measurement noise ($\sigma^2_m = 0.8$), compare EKF vs. ADF trajectory
  2. Time-Varying Parameter ID: Verify independent length-scale adaptation ($l_1, l_2$) prevents shared length-scale limitation
  3. Inducing Point Pruning Stress Test: Plot inducing point count over time, verify stays within budget $M$

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can heterogeneous kernel be extended to capture correlations between output dimensions without overfitting/instability seen in LMC approaches?
- **Basis:** Section III.A states explicitly modeling correlations can lead to overfitting and instability, but this trade-off remains unresolved
- **Evidence needed:** Modified kernel structure with cross-correlation terms plus experimental results showing stable convergence in correlated multi-output scenarios

### Open Question 2
- **Question:** Under what specific conditions of high initial uncertainty does prediction step approximation (Eq. 18) fail to maintain consistency?
- **Basis:** Section III.C notes approximation may have large errors in early learning stage with large state uncertainty
- **Evidence needed:** Theoretical derivation of approximation error bounds relative to state covariance, or simulations showing failure modes in chaotic systems with poor initial estimates

### Open Question 3
- **Question:** Is it feasible to derive exact moment matching (ADF) for non-stationary kernels within this recursive framework?
- **Basis:** ADF derivations rely on Gaussian kernel's property as scaled Gaussian PDF (Eq. 33), while non-stationary experiment used EKF instead
- **Evidence needed:** Derivation of closed-form moment update equations for non-stationary kernel family, or proof that numerical quadrature (UKF) is strictly required

## Limitations
- Block-diagonal kernel structure ignores potential inter-output correlations that may exist in physical systems
- Prediction step approximation may fail under extreme initial uncertainty or sparse inducing point distribution
- ADF derivation for non-stationary kernels is not provided, limiting exact moment matching capabilities

## Confidence
- **Method correctness:** High - Algorithm structure and derivations are well-documented with clear mathematical foundations
- **Implementation feasibility:** Medium - Requires careful handling of numerical stability and hyperparameter tuning not fully specified
- **Experimental reproducibility:** Medium - Core experiments described but some hyperparameter details missing

## Next Checks
1. Implement 1D "Kink" function experiment and verify ADF shows significantly lower nMSE (~0.036) than EKF (~0.058) under measurement noise
2. Test Cholesky stability updates from Appendix C by deliberately introducing numerical instability and verifying recovery
3. Run long trajectory on simple dynamic system and plot inducing point count over time to verify pruning mechanism maintains budget $M$ while removing redundant points