---
ver: rpa2
title: 'AutoL2S: Auto Long-Short Reasoning for Efficient Large Language Models'
arxiv_id: '2505.22662'
source_url: https://arxiv.org/abs/2505.22662
tags:
- reasoning
- autol2s
- short
- long
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes AutoL2S, a framework for efficient reasoning
  in large language models. It addresses the overthinking problem by enabling models
  to dynamically choose between long and short reasoning paths based on question complexity.
---

# AutoL2S: Auto Long-Short Reasoning for Efficient Large Language Models

## Quick Facts
- arXiv ID: 2505.22662
- Source URL: https://arxiv.org/abs/2505.22662
- Reference count: 40
- Primary result: Up to 71% reduction in reasoning length with minimal accuracy loss across multiple benchmarks

## Executive Summary
AutoL2S addresses the overthinking problem in large language models by enabling dynamic selection between long and short reasoning paths based on question complexity. The framework introduces an `<EASY>` token to supervise instance-wise switching during training, combined with GRPO-style fine-tuning to optimize efficiency-accuracy trade-offs. Experiments demonstrate significant reductions in token usage (41-71%) while maintaining accuracy across math and reasoning benchmarks, with improved inference time and reduced computational costs.

## Method Summary
AutoL2S uses a two-stage pipeline: first, supervised fine-tuning (SFT) on paired long and short chain-of-thought paths with special tokens (`<EASY>`, `<Long Trigger>`, `<Short Trigger>`) to learn switching behavior; second, GRPO-style optimization using rollouts generated by the SFT model to refine efficiency-accuracy trade-offs. Short reasoning paths are generated via rejection sampling (k=8) from teacher models, selecting the shortest correct path. At inference, the model generates the first token (`<EASY>` for short reasoning, `<Long Trigger>` for long reasoning) which determines subsequent reasoning mode through constrained decoding.

## Key Results
- 41-71% reduction in reasoning length compared to DeepSeek-R1-Base
- Accuracy preserved within 2-3% of baseline across benchmarks
- 2.3x speedup in inference time with 4.6x reduction in token usage
- GRPO optimization provides additional 19-35% length reduction beyond SFT alone

## Why This Works (Mechanism)

### Mechanism 1
The `<EASY>` switching token enables instance-wise reasoning path selection. During SFT, the model learns to predict `<EASY>` for questions where verified short reasoning paths exist. At inference, the first generated token (either `<EASY>` or `<Long Trigger>`) determines subsequent reasoning mode via constrained decoding. Core assumption: Question complexity correlates with whether short reasoning yields correct answers.

### Mechanism 2
Long-short paired supervision reduces learning uncertainty for short reasoning. Concatenating long CoT paths before short CoT paths provides auxiliary context. Lemma 1 shows H(S_t | X, L, S_{<t}) ≤ H(S_t | X, S_{<t})—conditioning on long reasoning reduces entropy for learning short reasoning. Core assumption: Long reasoning contains information relevant to short reasoning generation.

### Mechanism 3
GRPO-style optimization with correctness advantages refines efficiency-accuracy trade-offs. Post-SFT, the model generates long-short rollouts. The advantage A(z,r) = U(z,r) - E[U(z, r_i)] provides positive signal for correct reasoning (any length) and negative for incorrect, implicitly favoring shorter correct paths. Core assumption: The SFT model's long-short selection policy is reasonably calibrated.

## Foundational Learning

- **GRPO (Group Relative Policy Optimization)**: Why needed here: The second training stage uses GRPO-style loss to optimize over long-short rollouts. Quick check question: Can you explain how the advantage function A(z,r) differs from a standard reward?

- **Rejection Sampling for Short CoT Generation**: Why needed here: Short reasoning paths are generated via k-trial rejection sampling, selecting shortest correct path. Quick check question: How does increasing k affect the quality and length of selected short paths?

- **Autoregressive Token Prediction with Special Control Tokens**: Why needed here: The `<EASY>` token must be generated autoregressively before reasoning begins. Quick check question: What happens if the model generates `<EASY>` but no valid short path exists?

## Architecture Onboarding

- Component map: Input Question → Teacher Models → Long CoT (L) and Short CoT candidates (S_j) → Rejection Sampling → S → Training Data D → SFT → π_SFT → Rollouts → GRPO → π_θ

- Critical path: SFT data construction (especially rejection sampling quality) → SFT training → Rollout generation quality → GRPO effectiveness

- Design tradeoffs:
  - Higher rejection sampling k: More short paths in data → shorter reasoning but potential accuracy loss in SFT stage
  - Force-short vs. adaptive: Forcing short paths reduces length but risks 2.6% accuracy drop
  - Length penalty reward: Further reduces tokens but increases over-compression risk

- Failure signatures:
  - `<EASY>` token rarely generated: Insufficient short-path examples in training data
  - Short paths incorrect: Rejection sampling verification failed
  - Long paths on simple questions: SFT didn't learn the switching behavior

- First 3 experiments:
  1. Reproduce Table 1 baseline comparison on GSM8K to validate the 41-50% token reduction claim
  2. Ablate rejection sampling k ∈ {0, 4, 8} and plot accuracy vs. token length to find optimal trade-off
  3. Analyze attention patterns during training to verify long→short information transfer

## Open Questions the Paper Calls Out
None

## Limitations
- Training data quality and generalizability critically depends on rejection sampling quality and question complexity labeling assumptions
- Theoretical assumptions about entropy reduction and information transfer aren't empirically validated
- GRPO optimization dynamics depend on SFT model quality but rollout quality isn't characterized

## Confidence

**High confidence**: Core experimental results showing 41-71% token reduction with minimal accuracy loss are well-supported by Table 1 data and consistent ablation studies.

**Medium confidence**: Claim that instance-wise switching via `<EASY>` token is primary driver has supporting evidence but relies on unproven correlation between question complexity and short path existence.

**Low confidence**: Theoretical framing around entropy reduction (Lemma 1) and claimed benefits of long-short concatenation aren't empirically validated beyond attention visualization.

## Next Checks

1. **Rejection sampling robustness analysis**: Systematically vary k ∈ {0, 2, 4, 8, 16} and measure proportion of questions with correct short paths, average length of selected paths, and SFT accuracy to identify optimal trade-off.

2. **Long-short concatenation ablation**: Train two SFT models—one with concatenation and one with only short CoT paths—to directly test whether Lemma 1's claimed entropy reduction translates to practical benefits.

3. **Rollout quality and GRPO sensitivity**: Generate 1000 rollouts from SFT model and analyze distribution of `<EASY>` vs. `<Long Trigger>` selections, correctness rates, and whether GRPO amplifies systematic biases in SFT behavior.