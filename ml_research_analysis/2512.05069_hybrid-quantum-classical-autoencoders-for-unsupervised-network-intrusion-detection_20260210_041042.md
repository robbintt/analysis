---
ver: rpa2
title: Hybrid Quantum-Classical Autoencoders for Unsupervised Network Intrusion Detection
arxiv_id: '2512.05069'
source_url: https://arxiv.org/abs/2512.05069
tags:
- quantum
- detection
- classical
- unsupervised
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents the first large-scale evaluation of hybrid
  quantum-classical (HQC) autoencoders for unsupervised network intrusion detection.
  The work systematically explores key design choices including quantum-layer placement,
  measurement approaches, variational versus non-variational formulations, and latent-space
  regularization.
---

# Hybrid Quantum-Classical Autoencoders for Unsupervised Network Intrusion Detection

## Quick Facts
- arXiv ID: 2512.05069
- Source URL: https://arxiv.org/abs/2512.05069
- Reference count: 25
- This study presents the first large-scale evaluation of hybrid quantum-classical (HQC) autoencoders for unsupervised network intrusion detection, showing that well-configured HQC models can match or exceed classical performance on benchmark datasets.

## Executive Summary
This paper systematically evaluates hybrid quantum-classical (HQC) autoencoders for unsupervised network intrusion detection across three benchmark datasets. The study explores key design choices including quantum-layer placement, measurement approaches, and regularization strategies. Results show that well-configured HQC models can achieve competitive or superior performance to classical baselines, with the best configurations reaching up to 0.9611 AUROC on NSL-KDD. Notably, HQC models demonstrate greater stability and reliability in detecting zero-day attacks compared to classical unsupervised and supervised approaches. The work also reveals significant performance sensitivity to architectural decisions and demonstrates measurable degradation under simulated quantum noise, highlighting both the potential and challenges of HQC approaches for practical NIDS deployment.

## Method Summary
The study implements a unified autoencoder framework with four variants: classical AE, VAE, AE with latent-space regularization, and VAE with regularization. HQC models insert a parameterized quantum circuit (PQC) layer between classical encoder and decoder. The quantum layer can be placed early (after minimal compression) or late (after significant compression), using amplitude or angle embedding respectively. Three measurement schemes are evaluated: expectation value (Pauli-Z), probability distribution, and amplitude estimation. Models are trained on normal traffic only using reconstruction error minimization with Adam optimizer (lr=1e-3, batch=256), and early stopping on validation loss. Performance is evaluated using AUROC with statistical significance via paired t-tests (α=0.01) over 5 runs.

## Key Results
- HQC autoencoders can match or exceed classical performance, with best configurations achieving up to 0.9611 AUROC on NSL-KDD
- Early-stage quantum layer placement with amplitude embedding and expectation-value measurement yields significantly better results than other configurations
- Under zero-day attack scenarios, HQC models demonstrate more stable and reliable detection than both classical unsupervised and supervised baselines
- Simulated gate noise experiments show measurable performance degradation even at low noise levels, with AUROC dropping by 2.68% at sigma=0.01

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Early-stage quantum layers using amplitude embedding can capture richer feature correlations than late-stage placement, leading to superior performance.
- **Mechanism:** Placing the parameterized quantum circuit (PQC) early allows it to operate on a less-compressed input representation (2^N features for N qubits). The quantum state's evolution through a variational ansatz with entangling layers enables the model to capture complex data correlations in the high-dimensional Hilbert space before classical information bottlenecking occurs.
- **Core assumption:** The high-dimensional Hilbert space provides representational advantages for network traffic data that are not easily recovered once the data has been classically compressed.
- **Evidence anchors:**
  - [abstract] "We construct a unified experimental framework that iterates over key quantum design choices, including quantum-layer placement..."
  - [section 4.3.4] "Statistical analysis indicates that placing the quantum layer early in the encoder yields significantly better results... The early-stage configuration... likely allows the model to capture more complex data correlations..."
  - [corpus] The corpus shows limited direct evidence on this specific architectural nuance for NIDS.
- **Break condition:** The advantage fails to manifest if the input data is extremely low-dimensional or if the quantum circuit depth is insufficient to exploit the Hilbert space.

### Mechanism 2
- **Claim:** Unsupervised HQC autoencoders provide more stable and reliable detection on unseen (zero-day) attacks than classical supervised and unsupervised baselines.
- **Mechanism:** By training exclusively on normal traffic, the model learns a compressed manifold of "normal" behavior via reconstruction error minimization. Novel attacks, by definition, deviate from this learned manifold, resulting in a high reconstruction error. The lower performance variance of HQC models across different zero-day attacks suggests they learn a more robust representation of normality.
- **Core assumption:** The defining characteristics of "normal" traffic are sufficiently distinct from any attack pattern, including novel ones, to cause a measurable reconstruction error.
- **Evidence anchors:**
  - [abstract] "Under zero-day attack scenarios, well-configured HQC models demonstrate more stable and reliable detection than both classical unsupervised and supervised baselines."
  - [section 4.2] "A direct comparison between the unsupervised approaches demonstrates that HQC models achieve higher average AUROC with lower variance..."
  - [corpus] Evidence for this specific finding is limited in the provided corpus.
- **Break condition:** If a zero-day attack is very similar to normal traffic, its reconstruction error will be low, leading to a missed detection.

### Mechanism 3
- **Claim:** HQC model performance is highly sensitive to architectural choices and susceptible to degradation from quantum hardware noise.
- **Mechanism:** Unlike classical models with lower performance variance, the behavior of a PQC is sensitive to its ansatz, embedding, and measurement scheme. Furthermore, on real hardware, environmental noise and coherent gate errors cause state decoherence, degrading the learned representations. Even low simulated noise causes measurable performance drop.
- **Core assumption:** The performance observed in simulation is an upper bound; real-world performance will be lower due to unavoidable noise.
- **Evidence anchors:**
  - [abstract] "...they exhibit higher sensitivity to design decisions. Simulated gate noise experiments show measurable performance degradation..."
  - [section 4.1] "HQC models show substantially higher variance across architectural configurations..."
  - [section 4.4] "At a very low noise level... the model's AUROC drops by 2.68%, already falling below the performance of the best classical model."
  - [corpus] Related work like "Hybrid Quantum--Classical Machine Learning Potential" notes hybrid algorithms are promising but limited by current hardware.
- **Break condition:** The mechanism breaks if noise mitigation techniques are insufficient to overcome the errors inherent in the quantum hardware.

## Foundational Learning

- **Concept: Autoencoders for Anomaly Detection**
  - **Why needed here:** This is the core classical architecture. An engineer must understand that the autoencoder learns a compressed representation of normal traffic and detects anomalies via high reconstruction error.
  - **Quick check question:** If an autoencoder is trained on contaminated data containing unlabeled anomalies, how would its ability to detect future anomalies be affected?

- **Concept: Parameterized Quantum Circuits (PQCs)**
  - **Why needed here:** This is the quantum component inserted into the classical autoencoder. It is crucial to understand that a PQC has trainable parameters that are optimized by a classical optimizer; it is not a fixed transformation.
  - **Quick check question:** In a Hybrid Quantum-Classical model, what part of the computation is typically performed by a classical processor?

- **Concept: Data Embedding (Amplitude vs. Angle)**
  - **Why needed here:** This is the critical translation step from classical data to quantum state. The choice dictates information capacity, circuit depth, and hardware feasibility.
  - **Quick check question:** Which embedding method would you choose to encode a 1024-dimensional classical vector into a quantum state using a minimal number of qubits, and what is a key trade-off of this choice?

## Architecture Onboarding

- **Component map:** Classical input -> Classical preprocessing (one-hot encoding, scaling) -> Classical encoder -> QLayer (early or late placement) -> Classical decoder -> Reconstruction error output
- **Critical path:** Information flows from input through classical preprocessing, encoder, quantum layer (QLayer), decoder, to reconstruction error. The bottleneck is the QLayer's output (latent space). The design choices—especially embedding and measurement—determine the information capacity and expressivity of the model.
- **Design tradeoffs:**
  - Early vs. Late QLayer: Early placement allows operation on richer features but may require deeper circuits. Late placement is more hardware-efficient but may have lost crucial information.
  - Amplitude vs. Angle Embedding: Amplitude is information-dense (2^N features) but creates deep circuits. Angle is simple but low-capacity (N features).
  - Expectation Value vs. Probability Measurement: Expectation is computationally cheaper and yields a compact vector. Probability provides a richer description but has an exponentially growing cost.
- **Failure signatures:**
  - High performance variance: Indicates sensitivity to random seed or hyperparameters. Requires more runs or careful tuning.
  - Performance collapse under noise: Simulated noise experiments reveal this. Indicates the chosen ansatz or embedding is not robust. Requires noise-aware training.
- **First 3 experiments:**
  1. Baseline Comparison: Train a purely classical autoencoder on a chosen dataset (e.g., UNSW-NB15, using only normal data for training). Evaluate using reconstruction error and AUROC.
  2. QLayer Ablation: Implement an HQC autoencoder with an Early QLayer using Amplitude Embedding and Expectation Value measurement. Train and evaluate under the same conditions. Compare AUROC to the classical baseline.
  3. Noise Sensitivity Test: Take the best-performing HQC configuration. Introduce simulated coherent gate noise at varying levels (e.g., sigma = 0.01, 0.03, 0.1). Plot AUROC degradation vs. noise level to quantify robustness.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the simulated relationship between gate noise and performance degradation be validated on real NISQ hardware using quantum error mitigation strategies?
- **Basis in paper:** [explicit] The authors state that future work involves executing architectures on NISQ hardware to empirically test the simulated noise relationship.
- **Why unresolved:** The current study relies on simulated coherent gate errors (RX rotations), which may not capture the full complexity of physical hardware noise.
- **What evidence would resolve it:** Empirical AUROC results from running the HQC models on physical quantum processors with and without error mitigation layers.

### Open Question 2
- **Question:** Do specialized quantum ansatzes, such as quantum convolutional neural networks, provide better representational expressivity for NIDS than the hardware-efficient ansatz used in this study?
- **Basis in paper:** [explicit] The paper suggests exploring "advanced quantum ansatzes" like quantum CNNs to improve expressivity for structured network data.
- **Why unresolved:** The current benchmarks utilized a standard hardware-efficient ansatz with full entangling layers, leaving alternative circuit architectures unexplored.
- **What evidence would resolve it:** A comparative study benchmarking the performance of graph-informed or convolutional ansatzes against the hardware-efficient baseline on the same datasets.

### Open Question 3
- **Question:** How does the performance of HQC autoencoders scale with increasing qubit counts and more efficient data embedding strategies?
- **Basis in paper:** [explicit] The authors identify "scalability of HQC models with respect to growing qubit counts" as an open and practically important question.
- **Why unresolved:** The study used fixed embedding strategies (amplitude and angle) and limited qubit counts, leaving the implications of scaling to realistic network dimensions unclear.
- **What evidence would resolve it:** Experiments measuring AUROC and training convergence as qubit counts (N) and input dimensionality increase using novel embedding techniques.

## Limitations
- The evaluation is constrained by the absence of full architectural specifications, particularly the classical backbone and QLayer parameters, which limits exact reproduction
- Performance results are based on simulations and may not reflect real quantum hardware behavior, where noise and hardware connectivity significantly degrade results
- The evaluation focuses on AUROC and reconstruction-error thresholding, but other metrics like precision-recall or F1-score are not explored

## Confidence

- **High Confidence:** The claim that HQC autoencoders can match or exceed classical performance under zero-day scenarios is supported by systematic experiments and statistical analysis (paired t-tests, α = 0.01)
- **Medium Confidence:** The claim that HQC models are more sensitive to architectural choices is based on observed variance, but the exact cause-effect relationships (e.g., ansatz depth, entanglement) are not fully isolated
- **Low Confidence:** The performance degradation under simulated gate noise is reported, but the exact noise model and hardware fidelity assumptions are unclear

## Next Checks

1. **Architecture Sensitivity Analysis:** Systematically vary the QLayer depth, number of qubits, and entanglement structure to quantify their impact on performance and variance
2. **Real Hardware Benchmark:** Implement the best-performing HQC configuration on a real quantum processor (e.g., IBM Quantum) and compare performance degradation against simulation
3. **Metric Expansion:** Evaluate HQC models using precision-recall curves and F1-score to provide a more nuanced view of anomaly detection performance, especially under class imbalance