---
ver: rpa2
title: Learning to Reason in LLMs by Expectation Maximization
arxiv_id: '2512.20169'
source_url: https://arxiv.org/abs/2512.20169
tags:
- iteration
- reasoning
- data
- accuracy
- length
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a formal connection between expectation-maximization
  (EM) and reward-based optimization for learning reasoning in large language models
  (LLMs). The authors model reasoning as a latent variable problem, where intermediate
  rationales are sampled and filtered based on correctness.
---

# Learning to Reason in LLMs by Expectation Maximization

## Quick Facts
- **arXiv ID:** 2512.20169
- **Source URL:** https://arxiv.org/abs/2512.20169
- **Reference count:** 40
- **Primary result:** Introduces a formal connection between EM and reward-based optimization for learning reasoning in LLMs, showing Prompt Posterior Sampling (PPS) outperforms other methods on LLM-as-a-judge calibration and summarization tasks.

## Executive Summary
This paper presents a novel framework for learning reasoning in large language models by treating rationales as latent variables and applying a filtered expectation-maximization (FEM) objective. The approach formalizes reasoning as a latent variable model where intermediate rationales are sampled and filtered based on correctness, enabling iterative self-improvement. Three sampling schemes are proposed: rejection sampling, self-taught reasoning (STaR), and prompt posterior sampling (PPS), with PPS showing consistent superiority by conditioning rationale generation on the correct answer. Experiments on alignment and summarization tasks demonstrate that PPS achieves higher accuracy with fewer training samples, highlighting that sampling distribution quality matters more than quantity for effective self-improvement.

## Method Summary
The method treats reasoning as a latent variable problem and applies filtered expectation-maximization (FEM) to learn reasoning capabilities. The E-step samples rationale-answer pairs from the current model using one of three schemes: rejection sampling (RS), self-taught reasoning (STaR), or prompt posterior sampling (PPS). The M-step performs filtered gradient updates using only pairs where the sampled answer matches the correct answer. This process iterates K=5 times with linear learning rate decay. The framework is implemented using vLLM for sampling and TRL for fine-tuning, tested on Llama and Qwen models across two datasets: HelpSteer2 for LLM-as-a-judge calibration and Summarize from Feedback for summarization tasks.

## Key Results
- PPS consistently outperforms RS and STaR across all tested model sizes and tasks, achieving higher accuracy with fewer training samples
- The quality of the sampling distribution (PPS) matters more than the quantity of accepted training samples
- FEM iterations show monotonic improvement in test accuracy, with PPS achieving superior data utilization compared to other methods
- On HelpSteer2, PPS with Qwen-3B achieved highest accuracy among all configurations tested

## Why This Works (Mechanism)

### Mechanism 1: EM Decomposition with Monte Carlo Approximation
The algorithm treats rationales as latent variables z in x → z → y⋆ and approximates the intractable E-step via Monte Carlo sampling from proposal distribution q. The M-step maximizes expected log-likelihood using reward-weighted gradient updates, where the reward r(ŷ, y⋆) = 1[ŷ = y⋆] filters out incorrect answer-rationale pairs. This creates a valid learning signal even when the posterior over rationales is intractable.

### Mechanism 2: Sampling Distribution Quality Determines Training Signal
PPS conditions on the correct answer y⋆ in the prompt, biasing rationale generation toward justifications consistent with the correct answer. This produces higher-quality rationales that better explain the correct answer, leading to superior learning efficiency. The approach demonstrates that modern instruction-tuned LLMs can leverage hints in prompts to generate useful post-hoc rationalizations.

### Mechanism 3: Iterative Resampling Creates Virtuous Improvement Cycle
Multiple FEM iterations progressively refine both the sampling distribution and model parameters. As the model improves, it generates more correct answers (higher data utilization) and higher-quality rationales. The learning rate decays across iterations to stabilize convergence, creating a compounding improvement effect.

## Foundational Learning

- **Latent Variable Models and Expectation-Maximization**: Needed to understand why reasoning can be treated as a latent variable problem. Quick check: If you're learning p(x,y) but only observe x, what latent variable would EM introduce, and what would the E-step compute?
- **Monte Carlo EM / Stochastic Approximation**: Required to understand how single-sample approximations can still provide valid gradient directions. Quick check: Why does using a single sample per data point still provide a valid gradient direction when averaged over a batch?
- **Importance Sampling and Proposal Distributions**: Essential for understanding why PPS is more efficient than RS. Quick check: If q(z|x,y⋆) conditions on more information than the true posterior, what bias might this introduce during test time when y⋆ is unavailable?

## Architecture Onboarding

- **Component map**: Dataset D = {(x_i, y⋆_i)} → E-step sampler (vLLM) → Filter (ŷ == y⋆) → M-step optimizer (TRL SFT) → Updated model θ^(k)
- **Critical path**: 1) Prepare dataset with verifiable answers 2) Design prompt template (standard or PPS variant) 3) For each iteration: sample → filter → fine-tune 4) Monitor test accuracy, data utilization, reasoning length
- **Design tradeoffs**: PPS vs RS vs STaR (PPS best when y⋆ provides strong guidance); rejection budget M (higher increases data utilization but compute); training set size N (N≥2000 needed for clear method separation)
- **Failure signatures**: Data utilization near 0% (base model too weak); accuracy decreases with iterations (overfitting or learning rate too high); PPS underperforms RS (task has low information gain from y⋆)
- **First 3 experiments**: 1) Baseline replication on HelpSteer2 subset with all three sampling schemes; 2) Ablation on conditioning prompt strength; 3) Domain transfer test on mathematical reasoning dataset

## Open Questions the Paper Calls Out

- **Open Question 1**: Does PPS maintain its performance advantage over rejection sampling in complex mathematical reasoning tasks where conditioning on the correct answer is not expected to provide strong guidance? The paper notes this is planned future work as current experiments focus on alignment tasks where y⋆ provides high information gain.
- **Open Question 2**: Can the superior sampling efficiency of PPS be effectively integrated into online reinforcement learning algorithms like GRPO? The paper explicitly states this as future work, noting the current FEM framework is implemented as iterative supervised fine-tuning.
- **Open Question 3**: What are the formal convergence guarantees for the Filtered EM algorithm when using approximate E-steps and a single Monte Carlo sample? The paper acknowledges this theoretical analysis is lacking and left for future work.

## Limitations
- The empirical comparison is limited to two datasets and specific model families (Llama and Qwen), leaving generalizability to other reasoning tasks or model architectures uncertain
- Complete hyperparameter specifications are missing, particularly vLLM generation settings and actual batch size for SFT
- The mechanism analysis relies on post-hoc interpretation rather than rigorous ablation studies isolating component contributions

## Confidence
- **High Confidence**: The core EM framework derivation and general effectiveness of PPS over RS/STaR on tested datasets; monotonic improvement pattern across FEM iterations
- **Medium Confidence**: The theoretical claim that sampling distribution quality matters more than quantity; causal mechanism linking conditioning to generalizable rationales not directly validated
- **Low Confidence**: Proposed mechanisms for why PPS works better, particularly assumption that post-hoc rationalization conditioned on correct answers transfers to test-time reasoning without y⋆ access

## Next Checks
1. **Ablation Study on Prompt Conditioning**: Systematically vary the strength and specificity of the y⋆ hint in PPS prompts to measure sensitivity and determine if the effect is due to conditioning or simply providing more information to the sampler
2. **Cross-Domain Transfer Test**: Apply FEM to a mathematical reasoning dataset like GSM8K where the correct answer provides minimal information about the reasoning process to test whether RS matches or exceeds PPS performance
3. **Mechanism Isolation Experiment**: Compare FEM with PPS against direct training on human-labeled rationales without EM iteration to validate the iterative refinement mechanism rather than attributing gains solely to conditioning