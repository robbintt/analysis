---
ver: rpa2
title: "Comparative analysis of optical character recognition methods for S\xE1mi\
  \ texts from the National Library of Norway"
arxiv_id: '2501.07300'
source_url: https://arxiv.org/abs/2501.07300
tags:
- data
- text
- languages
- transkribus
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper evaluates OCR methods for S\xE1mi languages in Norway\u2019\
  s digitised collection, where OCR accuracy is currently insufficient. The authors\
  \ fine-tune three OCR approaches\u2014Transkribus, Tesseract, and TrOCR\u2014on\
  \ manually annotated S\xE1mi text and compare them with machine-annotated and synthetic\
  \ data."
---

# Comparative analysis of optical character recognition methods for Sámi texts from the National Library of Norway

## Quick Facts
- arXiv ID: 2501.07300
- Source URL: https://arxiv.org/abs/2501.07300
- Reference count: 17
- Primary result: Fine-tuning pre-trained OCR models with manual annotations and synthetic data achieves up to 5.6x reduction in character error rates for Sámi languages

## Executive Summary
This study evaluates three OCR approaches—Transkribus, Tesseract, and TrOCR—for Sámi languages in Norway's digitized collection, where baseline accuracy is insufficient at 3.38% CER. The authors fine-tune pre-trained models using 58 manually annotated pages supplemented with synthetic text images and machine-transcribed data. Transkribus and TrOCR outperform Tesseract on in-domain Sámi data, while Tesseract excels on out-of-domain test data. The best models reduce CER by 3.8-5.6x compared to baseline, demonstrating that high OCR accuracy for low-resource Sámi languages is achievable even with limited manually annotated data.

## Method Summary
The study fine-tunes three OCR models—Transkribus (CNN-BiLSTM-CTC with n-gram LM), Tesseract (LSTM-CTC), and TrOCR (ViT-RoBERTa transformer)—on Sámi text data. Training data includes 58 manually annotated pages (GT-Sámi), 307K synthetic lines generated from SIKOR corpus with Augraphy augmentations, and 2380 machine-transcribed pages (Pred-Sámi). Models use two-stage training: pre-training on synthetic data for 5 epochs, then fine-tuning on real data with early stopping. Evaluation uses CER, WER, and Sámi-character F1 on held-out test sets.

## Key Results
- Best models achieve CER of 0.61-0.89% (3.8-5.6x improvement over baseline 3.38%)
- Transkribus achieves lowest CER (0.61%) but requires proprietary software and per-page pricing
- TrOCR offers comparable performance (0.74% CER) as open-weight model with simpler implementation
- Special-character F1 improves from 52.54% to 93-97%, demonstrating effective learning of Sámi diacritics
- Synthetic data pre-training and machine annotations both contribute to accuracy gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning pre-trained OCR models with modest amounts of language-specific manual annotations yields substantial accuracy gains for low-resource languages.
- Mechanism: Pre-trained models encode general visual-textual features from high-resource training; fine-tuning adapts the final layers to character sets and orthographic patterns specific to Sámi languages, transferring learned representations without requiring large annotated corpora.
- Core assumption: Pre-trained models have sufficient visual feature extraction capacity that transfers across related scripts (Latin-based with diacritics).
- Evidence anchors:
  - [abstract] "fine-tuning pre-trained models and supplementing manual annotations with machine annotations and synthetic text images can yield accurate OCR for Sámi languages, even with a moderate amount of manually annotated data"
  - [section 4.2, Table 4] Best models reduce CER from baseline 3.38% to 0.61-0.89% (3.8-5.6x improvement)
  - [corpus] Related work on Kurdish (Yaseen & Hassani 2024) and Kwak'wala (Rijhwani et al. 2023) shows similar transfer learning effectiveness for low-resource languages
- Break condition: If the target language uses a fundamentally different script (non-Latin) with no pre-trained base model available, transfer benefits may be limited.

### Mechanism 2
- Claim: Synthetic text image pre-training provides additional training signal when manual annotations are scarce.
- Mechanism: Generating artificial document images from existing text corpora (SIKOR corpus) with font variations and noise augmentations creates large-scale training data; two-stage training (synthetic first, then real) initializes models with language-specific character recognition before fine-tuning on authentic documents.
- Core assumption: Synthetic image distributions approximate real document characteristics sufficiently to learn useful features.
- Evidence anchors:
  - [section 3.1] "307,387 lines" of synthetic data generated using Pillow and Augraphy with "variation in fonts and colours, and a varying degree of imperfections and noise"
  - [section 4.1, Table 3] TrOCR with synthetic pre-training achieved CER 1.28 vs 1.98 without; Tesseract improved from CER 1.85 to 1.15
  - [corpus] synthocr-gen (arXiv:2601.16113) explicitly targets synthetic data generation for low-resource OCR, validating this approach
- Break condition: If synthetic image generation parameters don't match real document characteristics (paper quality, scanning artifacts, historical fonts), pre-training may introduce distributional bias.

### Mechanism 3
- Claim: Iterative semi-automatic annotation expands training data with tolerable label noise.
- Mechanism: An initial model trained on limited ground truth is used to annotate additional documents; these machine-predicted transcriptions (Pred-Sámi: 2380 pages) supplement training data. Despite containing errors, the volume increase provides net benefit.
- Core assumption: Machine annotation errors are sufficiently random/unbiased that they don't systematically mislead the model.
- Evidence anchors:
  - [section 3.1] "we trained Transkribus models incrementally while annotating data... and we decided to automatically transcribe a larger amount of Sámi text with this model"
  - [section 4.1, Table 3] Adding Pred-Sámi to GT-Sámi improved Tesseract mean(CER,WER) from 5.01 to 4.89; TrOCR improved from 5.64 to 3.14
  - [corpus] Limited corpus evidence on machine-annotated data quality thresholds; Rijhwani et al. (2023) notes OCR-assisted annotation challenges even for very low-resource languages
- Break condition: If the initial model has systematic biases (e.g., consistently misrecognizing specific Sámi characters), machine annotations will propagate and amplify these errors.

## Foundational Learning

- Concept: **Character Error Rate (CER) and Word Error Rate (WER)**
  - Why needed here: These are the primary evaluation metrics; understanding edit-distance-based evaluation is essential to interpret results and compare models.
  - Quick check question: Given a ground truth "Sámi" and prediction "Sami," what is the CER? (Answer: 1 substitution / 4 characters = 25%)

- Concept: **Connectionist Temporal Classification (CTC) Loss**
  - Why needed here: Tesseract and Transkribus use CTC for sequence alignment; understanding how CTC handles variable-length input-output mappings explains why line segmentation quality affects results.
  - Quick check question: Why does CTC require a "blank" token in the output alphabet? (Answer: To handle repeated characters and alignment uncertainty)

- Concept: **Two-stage/curriculum training**
  - Why needed here: The paper's best results use synthetic data pre-training followed by real data fine-tuning; this is a specific curriculum learning strategy.
  - Quick check question: Why train on synthetic data first rather than mixing it with real data throughout? (Answer: To establish basic character recognition before specializing on real document characteristics)

## Architecture Onboarding

- Component map:
  ```
  Data Pipeline:
  Manual annotation (Transkribus interface) → GT-Sámi (58 pages)
  Text corpora (SIKOR) + Augraphy → Synth-Sámi (307K lines)
  Trained Transkribus model → Pred-Sámi (2380 pages)

  Model Architectures:
  Transkribus: CNN feature extractor → BiLSTM → CTC + n-gram LM
  Tesseract: LSTM-based line recognition → CTC
  TrOCR: ViT encoder → RoBERTa decoder (transformer)

  Evaluation:
  Line-level CER/WER → Collection-level aggregation
  Sámi-character F1 (special character tracking)
  ```

- Critical path:
  1. Start with Transkribus for annotation (semi-automatic labeling accelerates ground truth creation)
  2. Generate synthetic data from available text corpora with Augraphy augmentations
  3. Fine-tune TrOCR (best accuracy/effort ratio) using two-stage training: 5 epochs synthetic → real data with early stopping
  4. Evaluate on held-out test set with both CER/WER and special-character F1

- Design tradeoffs:
  - **Transkribus vs. TrOCR vs. Tesseract**: Transkribus achieves lowest CER (0.61%) but is proprietary with per-page pricing; TrOCR offers comparable performance (0.74% CER) as open-weight model; Tesseract worst on in-domain (0.89%) but best on OOD data and most compute-efficient
  - **Data quality vs. quantity**: 58 pages of clean manual annotations plus synthetic data outperformed 2380 pages of noisy machine annotations
  - **Language-specific vs. unified model**: Single model for all Sámi languages chosen for pipeline efficiency; may sacrifice per-language optimization

- Failure signatures:
  - High special-character F1 gap (baseline 52.54% vs. models 93-97%) indicates failure to learn Sámi-specific characters (á, č, đ, ŋ, š, ŧ, ž)
  - Systematic character substitutions in error analysis (e.g., 'â'→'á', 'ï'→'i') suggest insufficient training data for rare diacritics
  - CER much higher on validation than test set indicates line segmentation quality issues (test set had higher-quality segmentations)

- First 3 experiments:
  1. **Baseline reproduction**: Train Tesseract from Norwegian base model using only GT-Sámi (58 pages); target CER <2% to validate pipeline
  2. **Synthetic data ablation**: Compare models trained with vs. without Synth-Sámi pre-training; quantify CER delta to justify synthetic data generation effort
  3. **Special-character analysis**: Compute per-character F1 for each Sámi-specific letter (á, č, đ, ŋ, š, ŧ, ž) to identify which characters need more training examples or targeted augmentation

## Open Questions the Paper Calls Out
- **Open Question 1**: How does OCR performance for Sámi texts vary across different time periods, particularly for works predating the standardized orthographies established in the 1970s–1990s?
- **Open Question 2**: Can accurate OCR be achieved for Skolt Sámi despite its three apostrophe characters indicating pronunciation, which currently require high language proficiency to transcribe?
- **Open Question 3**: Would training on synthetic data until convergence, rather than only five epochs, further improve OCR accuracy or eliminate the need for machine-annotated (Pred-Sámi) data?
- **Open Question 4**: To what extent does line segmentation quality affect the relative performance differences between Transkribus, Tesseract, and TrOCR on Sámi texts?

## Limitations
- Data accessibility: GT-Sámi, GT-Nor, and Pred-Sámi datasets cannot be shared due to copyright restrictions on scanned texts from the National Library of Norway
- Single OOD test set: Limited evaluation of generalization to other Sámi printed materials beyond the Giellatekno test set
- Synthetic-to-real transfer: Quality depends on Augraphy parameters matching real document characteristics, which wasn't systematically evaluated

## Confidence
- **High confidence**: Fine-tuning pre-trained models with manual annotations improves OCR accuracy (CER reduction from 3.38% to 0.61-0.89% is substantial and consistently observed)
- **Medium confidence**: Synthetic data pre-training provides additional benefits (supported by ablation showing CER improvements, but dependent on synthetic generation quality)
- **Medium confidence**: Iterative machine annotation expands training data effectively (evidence shows net benefit, but limited evaluation of noise propagation)
- **Low confidence**: Transkribus achieves superior accuracy due to its n-gram language model (claimed advantage, but proprietary system prevents detailed architectural comparison)

## Next Checks
1. **Character-level error analysis**: Compute per-character F1 scores for each Sámi-specific character (á, č, đ, ŋ, š, ŧ, ž) to identify which characters need more training examples or targeted augmentation
2. **Synthetic data parameter sensitivity**: Systematically vary Augraphy parameters (noise levels, distortions, font variations) and measure impact on real data performance to establish optimal synthetic-to-real distribution alignment
3. **Cross-language generalization test**: Evaluate all fine-tuned models on test sets from related low-resource languages (e.g., Kurdish or Kwak'wala) to determine whether transfer learning benefits extend beyond Sámi languages