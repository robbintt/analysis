---
ver: rpa2
title: Internalizing World Models via Self-Play Finetuning for Agentic RL
arxiv_id: '2510.15047'
source_url: https://arxiv.org/abs/2510.15047
tags:
- state
- pass
- world
- training
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of improving Large Language Model
  (LLM) agents' performance in out-of-distribution (OOD) environments, where traditional
  reinforcement learning (RL) methods often fail to scale due to brittle exploration
  and limited generalization. The core method, SPA (Self-Play Agent), introduces a
  novel framework that equips LLM agents with an internal world model by decomposing
  it into state representation and transition modeling.
---

# Internalizing World Models via Self-Play Finetuning for Agentic RL

## Quick Facts
- arXiv ID: 2510.15047
- Source URL: https://arxiv.org/abs/2510.15047
- Authors: Shiqi Chen; Tongyao Zhu; Zian Wang; Jinghan Zhang; Kangrui Wang; Siyang Gao; Teng Xiao; Yee Whye Teh; Junxian He; Manling Li
- Reference count: 14
- Primary result: SPA improves LLM agent performance in OOD environments, raising Sokoban success from 25.6% to 59.8% and FrozenLake scores from 22.1% to 70.9% for Qwen2.5-1.5B-Instruct.

## Executive Summary
This paper tackles the challenge of training LLM agents in out-of-distribution environments where traditional RL methods struggle with brittle exploration and poor generalization. SPA introduces a novel framework that first learns environment dynamics via self-play supervised fine-tuning, then uses this internal world model to guide subsequent reinforcement learning. By decomposing the world model into state representation and transition modeling, SPA enables agents to better generalize beyond their training distribution. Experiments show substantial improvements over online world-modeling baselines across multiple environments.

## Method Summary
SPA employs a two-stage approach: first, a self-play data collection phase where the base policy interacts with the environment to generate trajectories with observation and prediction annotations. These trajectories are then processed through ground-truth state replacement and SFT, learning both state estimation and transition dynamics. The learned world model is subsequently used to initialize and guide a PPO-based RL stage. The state representation incorporates both raw symbolic states and structured natural-language descriptions (e.g., entity coordinates), while the transition model predicts next states conditioned on current states and actions.

## Key Results
- Sokoban success rate improved from 25.6% to 59.8% for Qwen2.5-1.5B-Instruct
- FrozenLake scores increased from 22.1% to 70.9% for the same model
- SPA consistently outperforms online world-modeling baselines across diverse environments
- State perplexity drops dramatically after State Estimation (e.g., from 163.9 to 19.6 in Sokoban)

## Why This Works (Mechanism)

### Mechanism 1: Decomposed World Model as Grounding Prior
Explicitly decomposing a world model into state representation and transition modeling provides a reusable dynamics prior that improves subsequent RL in OOD environments. During self-play SFT, the model learns to map raw symbolic observations to structured natural-language state descriptions and to predict next states conditioned on current states and actions. This world model is then fixed to initialize policy RL, supplying grounded state encodings and a learned transition kernel that expands the reachable state set under stochastic rollouts.

### Mechanism 2: Exploration Before Exploitation via Self-Play SFT
Conducting world-model learning via self-play SFT before policy optimization encourages broader state/action space coverage, raising Pass@k, whereas reward-driven online RL prematurely narrows the policy. Self-play collects diverse trajectories by free exploration, then SFT distills dynamics from these interactions. This yields a policy scaffold with higher Pass@k before any reward optimization. Subsequent PPO then exploits this scaffold, improving both Pass@1 and Pass@k over training.

### Mechanism 3: Grounded State Representations Enable Reliable Credit Assignment
Natural-language state abstractions (e.g., coordinates for key entities) make spatial relations explicit, reduce perplexity, and support better credit assignment during RL. Instead of raw symbolic grids, the agent receives both the raw state and a structured description (e.g., player/box/goal coordinates). This reduces state PPL and helps the policy align actions with actionable spatial relations. Removing correct coordinates (randomizing them) collapses training.

## Foundational Learning

- **Model-Based RL Basics**: Why needed - SPA is inspired by model-based RL, explicitly learning a world model (state encoder + transition predictor) to plan and improve sample efficiency. Quick check - Can you explain how a learned transition model supports planning in model-based RL?

- **Supervised Fine-Tuning (SFT) for LLMs**: Why needed - SPA uses SFT on self-play trajectories to internalize environment dynamics before RL; understanding token-level loss and masking is essential. Quick check - What is the effect of masking specific tokens during SFT loss computation?

- **PPO and Policy Optimization**: Why needed - After world-model SFT, SPA applies PPO to optimize the policy; knowing how PPO balances exploration/exploitation and uses advantage estimates is key. Quick check - How does the clipping objective in PPO prevent overly large policy updates?

## Architecture Onboarding

- **Component map**: Self-Play Data Generator -> Ground-Truth Replacement -> World-Model SFT Stage -> State Estimation Module -> PPO RL Stage

- **Critical path**:
  1. Generate self-play trajectories using base policy with observation-prediction prompting
  2. Replace hallucinated states with ground-truth; filter malformed samples
  3. Run SFT on cleaned data, masking only reasoning/action tokens (not environment observations)
  4. Initialize PPO with SFT checkpoint; train with same state representation format

- **Design tradeoffs**:
  - More SFT epochs improve downstream RL (Pass@1 from 0.29 to 0.60, Pass@k from 0.53 to 0.70 in Figure 6), but increase compute
  - State estimation adds prompt complexity but dramatically reduces perplexity; may not be necessary for all environments (Table 4 shows gains without it)
  - Self-play policy vs random-action generator: prior-aware exploration yields better world-model data

- **Failure signatures**:
  - Pass@k drops during RL (indicates overfitting to narrow trajectories)
  - High state PPL under the model suggests inadequate state estimation
  - Training collapse with randomized coordinates signals mis-specified abstractions
  - SFT with self-belief states (no ground-truth) leads to degraded RL performance

- **First 3 experiments**:
  1. Baseline check: Run vanilla PPO (no world-model SFT) on Sokoban/FrozenLake and measure Pass@1 and Pass@k over 1000 steps
  2. Ablation on state estimation: Compare PPO with raw states vs coordinate-augmented states; report PPL and success rates
  3. SFT masking ablation: Train SFT with transition annotations masked vs unmasked, then run PPO from both checkpoints; compare Pass@1/Pass@k

## Open Questions the Paper Calls Out
None

## Limitations
- The method's reliance on structured state representations (e.g., coordinate-based abstractions) may not transfer well to raw visual or continuous domains without significant adaptation
- Performance heavily depends on access to ground-truth states for SFT, limiting applicability in environments where such supervision is unavailable or expensive to obtain
- Results are primarily reported on grid-based environments with a single model size, leaving uncertainty about scalability to larger models or more complex tasks

## Confidence
- **High**: The core mechanism of self-play SFT improving RL sample efficiency in OOD environments, supported by direct performance gains
- **Medium**: The necessity of ground-truth state supervision during SFT, as evidenced by degraded performance when using self-belief states
- **Low**: The broader applicability of coordinate-based state abstractions, as this appears to be a paper-specific finding without strong corpus support

## Next Checks
1. Test SPA's performance on visual or continuous control environments (e.g., Atari or MuJoCo) to assess generalization beyond symbolic tasks
2. Evaluate whether SPA with self-belief states (no ground-truth) can match or exceed ground-truth SFT performance in environments where state supervision is noisy or partial
3. Scale up to larger LLM agents (e.g., 7B+ parameters) and report Pass@1/Pass@k curves to verify if gains hold with increased model capacity