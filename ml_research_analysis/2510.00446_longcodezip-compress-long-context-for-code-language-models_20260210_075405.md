---
ver: rpa2
title: 'LongCodeZip: Compress Long Context for Code Language Models'
arxiv_id: '2510.00446'
source_url: https://arxiv.org/abs/2510.00446
tags:
- code
- compression
- context
- arxiv
- longcodezip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'LongCodeZip introduces a novel training-free, model-agnostic framework
  for compressing long code contexts to enhance efficiency of code language models.
  The method employs a two-stage strategy: coarse-grained function-level compression
  based on conditional perplexity to rank and select relevant functions, followed
  by fine-grained block-level compression using perplexity-based segmentation and
  knapsack optimization to maximize relevance under a token budget.'
---

# LongCodeZip: Compress Long Context for Code Language Models

## Quick Facts
- **arXiv ID**: 2510.00446
- **Source URL**: https://arxiv.org/abs/2510.00446
- **Reference count**: 40
- **Primary result**: Training-free, model-agnostic framework achieves up to 5.6× compression ratio for code LLMs without degrading task performance

## Executive Summary
LongCodeZip introduces a novel training-free, model-agnostic framework for compressing long code contexts to enhance efficiency of code language models. The method employs a two-stage strategy: coarse-grained function-level compression based on conditional perplexity to rank and select relevant functions, followed by fine-grained block-level compression using perplexity-based segmentation and knapsack optimization to maximize relevance under a token budget. Evaluations across code completion, summarization, and question answering tasks show that LongCodeZip achieves up to 5.6× compression ratio without degrading performance, outperforming existing baselines including RAG-based and code-specific compression methods. The approach generalizes well across models (even with 0.5B models) and reduces generation time and token costs.

## Method Summary
LongCodeZip is a two-stage compression framework for long code contexts. The first stage performs coarse-grained function-level compression using tree-sitter to parse code into functions, then computes conditional perplexity scores (AMI = PPL(q) - PPL(q|c)) to rank and select the most relevant functions under a budget. The second stage performs fine-grained block-level compression by segmenting retained functions into blocks using perplexity-based boundaries (marking boundaries when line perplexity exceeds α times the standard deviation), then applying adaptive budget allocation and 0/1 knapsack optimization to select the most relevant blocks. The method is training-free and model-agnostic, using the same model for both compression and generation.

## Key Results
- Achieves up to 5.6× compression ratio without degrading task performance
- Outperforms RAG-based retrieval methods and code-specific compression baselines
- Maintains comparable performance across code completion (EM/ES metrics), summarization (CompScore), and question answering (BLEU>0.8) tasks
- Demonstrates generalization across models including small 0.5B models
- Reduces generation time and token costs while preserving task accuracy

## Why This Works (Mechanism)
LongCodeZip works by leveraging conditional perplexity as a measure of relevance between code segments and task instructions. The coarse-grained stage filters out entire functions that are unlikely to be relevant, while the fine-grained stage preserves important code blocks within retained functions. The two-stage approach balances global context reduction with local detail preservation, and the knapsack optimization ensures optimal token allocation under budget constraints.

## Foundational Learning
- **Conditional perplexity**: Measures how much context reduces uncertainty in predicting the next token. Needed because it quantifies relevance between code and instructions. Quick check: Compute AMI scores for sample code-instruction pairs.
- **Tree-sitter parsing**: Extracts syntactic structure (functions) from code. Needed to enable function-level compression without breaking syntax. Quick check: Parse sample Python/Java code and verify extracted functions.
- **Knapsack optimization**: Selects optimal subset of blocks under token budget. Needed to maximize relevance while respecting constraints. Quick check: Solve 0/1 knapsack with sample block weights and values.

## Architecture Onboarding
**Component Map**: Tree-sitter parsing -> Coarse-grained AMI ranking -> Fine-grained perplexity segmentation -> Knapsack selection -> Compressed context

**Critical Path**: Parse code → Compute conditional perplexities → Rank/select functions → Segment into blocks → Optimize selection → Generate output

**Design Tradeoffs**: Two-stage approach trades some computational overhead for better relevance preservation; uses perplexity instead of embeddings to capture syntactic/semantic dependencies

**Failure Signatures**: 
- Token-level compression corrupts code syntax
- RAG misses implicit dependencies with low lexical overlap
- Context lacks relevant information for ambiguous instructions

**3 First Experiments**:
1. Verify tree-sitter parsing extracts correct functions from sample code
2. Compute AMI scores for sample code-instruction pairs and verify ranking
3. Test perplexity-based block segmentation on retained functions

## Open Questions the Paper Calls Out
- Can LongCodeZip be combined with RAG-based retrieval methods to achieve superior performance compared to either approach alone?
- How can LongCodeZip's failure modes be detected and mitigated when context lacks relevant information or instructions are ambiguous?
- How effective is LongCodeZip across different programming languages beyond the six evaluated?
- Can the importance parameter β and fine-grained ratio R_fine be automatically determined based on task or codebase characteristics?

## Limitations
- Missing hyperparameter values (α threshold for block segmentation, perplexity computation details)
- No ablation study separating coarse-grained and fine-grained stage contributions
- Limited evaluation of robustness to code style variation and dependency chains
- Manual tuning required for β and R_fine parameters

## Confidence
- **High**: Quantitative performance improvements on the three reported tasks
- **Medium**: Generalization across models (including small 0.5B model) and tasks
- **Low**: Ablation of each compression stage, sensitivity to missing hyperparameters

## Next Checks
1. Re-run perplexity-based segmentation with multiple α values to confirm reported compression ratios
2. Swap in a different model for compression (e.g., smaller codecoder) while keeping generation model fixed
3. Apply LongCodeZip to a held-out code dataset with different styles/languages to test robustness