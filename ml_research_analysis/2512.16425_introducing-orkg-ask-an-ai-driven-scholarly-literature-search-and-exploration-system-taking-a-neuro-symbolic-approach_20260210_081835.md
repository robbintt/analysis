---
ver: rpa2
title: 'Introducing ORKG ASK: an AI-driven Scholarly Literature Search and Exploration
  System Taking a Neuro-Symbolic Approach'
arxiv_id: '2512.16425'
source_url: https://arxiv.org/abs/2512.16425
tags:
- system
- search
- literature
- scholarly
- users
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ORKG ASK is an AI-driven scholarly search system using a neuro-symbolic
  approach that combines vector search, LLMs, and knowledge graphs to help researchers
  find relevant literature. The system processes research questions in natural language,
  retrieves relevant articles using vector embeddings, and generates answers via Retrieval-Augmented
  Generation (RAG).
---

# Introducing ORKG ASK: an AI-driven Scholarly Literature Search and Exploration System Taking a Neuro-Symbolic Approach

## Quick Facts
- arXiv ID: 2512.16425
- Source URL: https://arxiv.org/abs/2512.16425
- Authors: Allard Oelen; Mohamad Yaser Jaradeh; Sören Auer
- Reference count: 0
- Key outcome: ASK combines vector search, LLMs, and knowledge graphs to process 76.4M articles with low 3% bounce rate and good usability (UMUX 65.7/100)

## Executive Summary
ORKG ASK is an open-source AI-driven scholarly search system that helps researchers find relevant literature through natural language queries. The system processes questions using a Retrieval-Augmented Generation (RAG) approach, retrieving semantically relevant articles from a corpus of 76.4M papers and generating answers while displaying per-article extracted information. ASK was evaluated through user feedback (1,212 responses) and a controlled experiment with 9 participants, showing lower perceived task load compared to Google Scholar and strong user engagement with a 3% bounce rate.

## Method Summary
ASK processes natural language queries by embedding them using Nomic's 768-dimensional model, then retrieves top documents from Qdrant vector store containing 76.4M CORE articles. A Mistral 7B Instruct LLM generates per-article answers using extracted context from abstracts (and full-text when available), with responses synthesized for the user. The system integrates semantic search with knowledge graph filtering, providing reproducibility metadata including prompts, parameters, and context for each answer. User feedback was collected via a widget integrated into the interface, while a controlled experiment compared task load and usability against Google Scholar.

## Key Results
- ASK achieves good usability with a UMUX score of 65.7/100 and lower perceived task load (26.76% vs 61.3% for Google Scholar)
- The system processes 76.4M articles from the CORE dataset with a low 3% bounce rate indicating active user engagement
- User feedback (1,212 responses) shows most users find the system easy to use and would recommend it, though correctness ratings are more neutral

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAG constrains LLM outputs to retrieved corpus content, reducing hallucinations while maintaining natural language interaction.
- Mechanism: Non-parametric memory (vector store with 76.4M articles) retrieves semantically relevant documents based on query embeddings. Parametric memory (Mistral 7B) generates answers using only injected context, not internal training data.
- Core assumption: Users can and will verify extracted information against source documents.
- Evidence anchors:
  - [abstract] "ASK automatically extracts key information and generates answers to research questions using a Retrieval-Augmented Generation (RAG) approach"
  - [section 3.1] "With specifying the context, the model is forced to rely on the text that exists within its prompt and not within its own parametric knowledge"
  - [corpus] Related papers on LLM-RAG systems (NANOGPT, PaperAsk benchmark) suggest this is an active research area with unresolved reliability questions
- Break condition: Retrieved documents lack relevance to query, or LLM ignores context constraints and hallucinates anyway.

### Mechanism 2
- Claim: Neuro-symbolic integration combines semantic search flexibility with structured knowledge precision.
- Mechanism: Neural components (vector embeddings + LLMs) handle natural language queries and flexible information extraction. Symbolic components (knowledge graphs) provide semantic filters and structured metadata for narrowing search space.
- Core assumption: The knowledge graph contains meaningful semantic relationships that match user mental models.
- Evidence anchors:
  - [abstract] "ASK aims to provide active support to researchers in finding relevant scholarly literature by leveraging vector search, LLMs, and knowledge graphs"
  - [section 3] "This provides both the precision of symbolic approach and the flexibility of a neural approach"
  - [corpus] Weak direct evidence—neighbor papers discuss knowledge graphs but don't validate neuro-symbolic combination specifically
- Break condition: Semantic filters are too restrictive or don't align with how users conceptualize their search needs.

### Mechanism 3
- Claim: Per-article answer extraction enables rapid relevance assessment without reading full texts.
- Mechanism: LLM extracts specific properties from each retrieved document (abstract + full-text when available) using structured prompts. Users scan extracted answers to determine which papers merit deeper reading.
- Core assumption: Extracted information accurately represents document content and relevance.
- Evidence anchors:
  - [abstract] "For each article, an automatically extracted answer for the previously asked question is displayed to the user"
  - [section 4.1, FR2] "The system shall display automatically extracted information from found literature. To ensure users get a quick overview of the literature so relevancy can be assessed"
  - [corpus] PaperAsk benchmark highlights reliability challenges in LLM content extraction tasks
- Break condition: Extraction misses key claims or misrepresents document positions, leading users to discard relevant papers.

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: Core architecture pattern separating retrieval (finding relevant documents) from generation (synthesizing answers), enabling grounded responses without fine-tuning.
  - Quick check question: Can you explain why RAG reduces hallucination compared to prompting an LLM directly?

- Concept: **Vector Embeddings & Semantic Search**
  - Why needed here: Enables similarity-based retrieval using dense representations (768-dim Nomic embeddings) rather than keyword matching, capturing conceptual relationships.
  - Quick check question: How does semantic search differ from traditional keyword search in handling synonymy and polysemy?

- Concept: **Prompt Engineering for Extraction**
  - Why needed here: System uses structured prompts with placeholders to extract consistent properties across documents; quality depends on prompt design.
  - Quick check question: What prompt components (role, task, output format) appear in ASK's extraction prompts per Figure 3?

## Architecture Onboarding

- Component map: User query -> Nomic embedding model -> Qdrant vector store (76.4M articles) -> Document retrieval -> Mistral 7B LLM (TGI engine) -> Per-article extraction + synthesis -> React/Next.js frontend with Tailwind

- Critical path:
  1. User submits natural language query
  2. Query embedded via Nomic model
  3. Vector search retrieves top-n documents from Qdrant
  4. Document contexts injected into extraction and synthesis prompts
  5. Mistral generates per-article extractions and synthesized summary
  6. Results displayed with inline citations and reproducibility metadata

- Design tradeoffs:
  - **Small LLM (7B) vs larger models**: Lower computational cost and latency, but potentially lower extraction quality
  - **Abstract + optional full-text**: Balances processing cost against answer depth; 25% of articles have full-text
  - **Open-source transparency vs proprietary systems**: Enables reproducibility but may lack polish of commercial competitors
  - **Caching for efficiency**: Reduces latency but may serve stale results if corpus updates

- Failure signatures:
  - **High bounce rate**: Would indicate users don't find value (actual: 3% is very low)
  - **Neutral correctness ratings**: Users uncertain about LLM output accuracy (evaluation shows neutral skew)
  - **Low filter/column customization usage**: Suggests features aren't intuitive or default extractions suffice
  - **Hallucination in citations**: LLM generating non-existent references despite RAG constraints

- First 3 experiments:
  1. **Reproduction test**: Enter a research question, inspect the reproducibility menu (prompt, model, parameters, context), then re-run with same seed to verify deterministic output.
  2. **Retrieval quality assessment**: Submit a specific technical query, examine the top-5 retrieved documents, and evaluate whether semantic similarity matches your judgment of relevance.
  3. **Extraction accuracy audit**: Select 3 retrieved papers, manually extract the answer to your question, then compare against LLM extraction to characterize error modes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ORKG ASK quantitatively compare to proprietary AI-driven search tools like Elicit or Consensus?
- Basis in paper: [explicit] The authors state that "a quantitative performance comparison with these services is out of scope for this work, but is an interesting future research direction."
- Why unresolved: The evaluation only compared ASK to Google Scholar, a traditional keyword-based engine, rather than contemporary RAG-based competitors.
- What evidence would resolve it: A standardized benchmark study measuring retrieval relevance (e.g., nDCG) and answer faithfulness against proprietary systems.

### Open Question 2
- Question: To what extent does semantic parsing and author disambiguation improve retrieval quality in a neuro-symbolic system?
- Basis in paper: [explicit] The authors list "parsing content semantically, for example by performing author name disambiguation" as a specific plan for future work to enhance the literature repository.
- Why unresolved: The current implementation relies heavily on vector embeddings (neural) and basic metadata; the added value of deep symbolic knowledge graph integration is proposed but not yet tested.
- What evidence would resolve it: An ablation study comparing search result accuracy before and after the integration of semantic parsing features.

### Open Question 3
- Question: Why is the symbolic filtering functionality underutilized, and does this impact the "neuro-symbolic" value proposition?
- Basis in paper: [inferred] Analytics reveal that "the number of added custom filters is low" and custom columns are rarely used, despite these being core to the neuro-symbolic approach.
- Why unresolved: It is unclear if low usage stems from poor UI discoverability, user satisfaction with purely neural results, or a lack of user understanding of the symbolic features.
- What evidence would resolve it: User interaction analysis or interviews specifically investigating why users avoid the semantic filtering options.

## Limitations

- Evaluation relies entirely on subjective user feedback without objective benchmarks of retrieval accuracy or answer correctness
- Controlled experiment has only 9 participants with limited statistical power and no demographic information
- System processes abstract-only for 75% of articles, potentially limiting answer quality for questions requiring full-text analysis
- Evaluation doesn't address hallucination rates despite RAG's intended constraint on LLM outputs

## Confidence

- **High confidence**: System architecture and implementation details (vector search, RAG pipeline, frontend/backend structure) are well-documented and reproducible.
- **Medium confidence**: Usability metrics (UMUX score, NASA-TLX scores) are reliable within the evaluation context, though limited by sample size and lack of objective correctness measures.
- **Medium confidence**: The neuro-symbolic approach combining semantic search with structured knowledge appears sound based on architectural description, but effectiveness depends heavily on knowledge graph quality and semantic alignment with user needs.
- **Low confidence**: Claims about answer accuracy, hallucination reduction, and the actual value of per-article extraction for rapid relevance assessment lack empirical validation beyond user perceptions.

## Next Checks

1. **Benchmark against objective ground truth**: Create a test suite of 50-100 research questions with known correct answers from specific papers, then measure ASK's precision and recall against these benchmarks rather than relying solely on user perception surveys.

2. **Hallucination audit**: Systematically sample 100+ generated answers and manually verify whether all cited claims actually appear in the referenced documents, measuring actual hallucination rates rather than assuming RAG constraints are effective.

3. **Full-text versus abstract comparison**: Select 30+ questions where both abstract-only and full-text versions of papers are available in the corpus, then measure whether the 25% of articles with full-text processing show statistically significant improvements in answer quality and relevance assessment accuracy.