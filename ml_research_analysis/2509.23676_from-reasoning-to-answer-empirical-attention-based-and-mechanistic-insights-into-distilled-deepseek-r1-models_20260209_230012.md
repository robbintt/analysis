---
ver: rpa2
title: 'From Reasoning to Answer: Empirical, Attention-Based and Mechanistic Insights
  into Distilled DeepSeek R1 Models'
arxiv_id: '2509.23676'
source_url: https://arxiv.org/abs/2509.23676
tags:
- reasoning
- answer
- attention
- tokens
- heads
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a three-stage investigation into how reasoning
  traces in distilled DeepSeek R1 models influence answer generation. The authors
  first show empirically that including explicit reasoning consistently improves answer
  quality across diverse domains, with larger gains in distilled models than in the
  full model.
---

# From Reasoning to Answer: Empirical, Attention-Based and Mechanistic Insights into Distilled DeepSeek R1 Models

## Quick Facts
- arXiv ID: 2509.23676
- Source URL: https://arxiv.org/abs/2509.23676
- Reference count: 21
- Key outcome: Three-stage investigation shows reasoning traces in distilled DeepSeek R1 models functionally contribute to answer generation through empirical improvements, attention patterns, and mechanistic interventions

## Executive Summary
This paper investigates how reasoning traces influence answer generation in distilled DeepSeek R1 models through three complementary approaches. The authors first demonstrate empirically that including explicit reasoning consistently improves answer quality across diverse domains, with larger gains observed in distilled models compared to the full model. Second, attention analysis reveals that answer tokens substantially attend to reasoning tokens, with certain mid-layer Reasoning-Focus Heads tracking the reasoning process including self-reflective cues. Finally, mechanistic interventions using activation patching demonstrate that perturbations to key reasoning tokens can reliably alter final answers, confirming directional information flow from reasoning to answer.

## Method Summary
The study employs a three-stage investigation methodology. First, empirical analysis compares answer quality with and without reasoning traces across diverse domains, using established evaluation metrics to quantify performance differences. Second, attention analysis examines how answer tokens attend to reasoning tokens throughout the model's layers, identifying specific attention patterns and Reasoning-Focus Heads that track the reasoning process. Third, mechanistic experiments use activation patching to test causal relationships by perturbing key reasoning tokens and measuring effects on final answers, establishing directional influence from reasoning to answer generation.

## Key Results
- Including explicit reasoning consistently improves answer quality across diverse domains
- Answer tokens substantially attend to reasoning tokens, with mid-layer Reasoning-Focus Heads tracking reasoning process
- Perturbations to key reasoning tokens can reliably alter final answers, confirming directional flow of information

## Why This Works (Mechanism)
The reasoning traces serve as an intermediate representation that guides answer generation through multiple mechanisms. Attention patterns show that answer generation actively references the reasoning path, particularly through specialized Reasoning-Focus Heads that track both the logical progression and self-reflective elements of the reasoning process. The mechanistic intervention experiments confirm that reasoning tokens carry causal influence - when perturbed, they reliably change the final answer, demonstrating that reasoning is not merely correlated with but functionally contributes to answer generation.

## Foundational Learning

**Self-Reflective Reasoning:** Models that can monitor and evaluate their own reasoning process produce more accurate answers. *Why needed:* Understanding self-reflection helps identify how models detect and correct reasoning errors. *Quick check:* Compare answer quality between models with and without self-reflective reasoning capabilities.

**Attention-Based Interpretability:** Analyzing attention patterns reveals which parts of the model's internal representation influence specific outputs. *Why needed:* Provides visibility into how models use reasoning traces during answer generation. *Quick check:* Verify that answer tokens consistently attend to relevant reasoning tokens across multiple examples.

**Activation Patching for Causality:** Mechanistic interventions that perturb specific model components can establish causal relationships between internal states and outputs. *Why needed:* Correlation does not imply causation - this method proves reasoning traces actually influence answers. *Quick check:* Confirm that targeted perturbations reliably change answers while control perturbations do not.

## Architecture Onboarding

**Component Map:** Input -> Reasoning Generation -> Attention Mechanisms -> Answer Generation -> Output

**Critical Path:** The reasoning generation stage produces traces that are then attended to by answer generation tokens through specialized Reasoning-Focus Heads, creating a sequential flow of information.

**Design Tradeoffs:** Including reasoning traces improves answer quality but increases computational cost and latency; distilled models show larger gains, suggesting they benefit more from explicit reasoning guidance.

**Failure Signatures:** Models may generate plausible-sounding but incorrect answers when reasoning traces contain errors or when attention mechanisms fail to properly track the reasoning process.

**First Experiments:** 1) Remove reasoning traces and measure degradation in answer quality; 2) Mask Reasoning-Focus Heads and observe impact on reasoning tracking; 3) Perturb reasoning tokens at different layers to map information flow timing.

## Open Questions the Paper Calls Out
None

## Limitations
- Findings focus on a specific distilled model family and may not generalize to other architectures
- Attention analysis reveals correlations but cannot definitively establish causation without interventions
- Activation patching experiments test limited reasoning tokens and perturbation types

## Confidence
- High: Empirical finding that including reasoning improves answer quality across diverse domains
- High: Attention patterns showing answer tokens attending to reasoning tokens
- Medium: Identification of Reasoning-Focus Heads tracking reasoning process
- Medium: Activation patching results demonstrating causal influence of reasoning tokens on answers
- Low: Generalization of findings to non-distilled models or different reasoning architectures

## Next Checks
1. Test whether the observed reasoning-to-answer influence persists across different model scales and distillation ratios
2. Conduct ablation studies on the identified Reasoning-Focus Heads by selectively masking or modifying their attention patterns
3. Extend activation patching experiments to test whether perturbing reasoning tokens in earlier layers can influence answers in later layers