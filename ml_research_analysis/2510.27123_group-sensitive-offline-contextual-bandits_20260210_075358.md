---
ver: rpa2
title: Group-Sensitive Offline Contextual Bandits
arxiv_id: '2510.27123'
source_url: https://arxiv.org/abs/2510.27123
tags:
- policy
- reward
- fairness
- disparity
- offline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies group-sensitive fairness in offline contextual
  bandits, where optimizing only for reward can widen disparities across groups (e.g.,
  gender, race). To address this, the authors propose the Off-Policy Group-Constrained
  Policy Gradient algorithm (GC-PG), which incorporates a group-sensitive fairness
  constraint into the policy optimization via a Lagrangian formulation.
---

# Group-Sensitive Offline Contextual Bandits

## Quick Facts
- arXiv ID: 2510.27123
- Source URL: https://arxiv.org/abs/2510.27123
- Authors: Yihong Guo; Junjie Luo; Guodong Gao; Ritu Agarwal; Anqi Liu
- Reference count: 4
- Primary result: Proposes GC-PG algorithm for group-sensitive fairness in offline contextual bandits, achieving convergence to stationary point at rate O(1/T) while reducing reward disparities across groups

## Executive Summary
This paper addresses fairness concerns in offline contextual bandit systems, where optimizing solely for reward can exacerbate disparities across demographic groups. The authors introduce the Group-Constrained Policy Gradient (GC-PG) algorithm, which incorporates group-sensitive fairness constraints into the policy optimization framework. By leveraging a doubly robust estimator and Lagrangian formulation, the method balances reward maximization with fairness considerations, making it applicable to high-stakes decision-making scenarios where equitable outcomes are crucial.

The proposed approach demonstrates both theoretical and empirical effectiveness, showing convergence guarantees while maintaining competitive performance. The algorithm's ability to achieve Pareto optimal fairness-performance trade-offs represents a significant advancement in developing fair offline reinforcement learning systems that can be deployed in real-world applications where both accuracy and equity matter.

## Method Summary
The GC-PG algorithm addresses group-sensitive fairness in offline contextual bandits by incorporating a group-constrained optimization framework. The method uses a Lagrangian formulation to balance reward maximization with fairness constraints, where the fairness metric is defined as the maximum difference in expected rewards across groups. A doubly robust estimator is employed to reduce bias and variance in reward estimation from logged data. The policy gradient is computed using this estimator, and the optimization alternates between updating the policy parameters and the Lagrange multiplier. Theoretical analysis establishes convergence to a stationary point at rate O(1/T), while experiments on both synthetic and real-world datasets demonstrate the algorithm's effectiveness in reducing reward disparities while maintaining competitive overall performance.

## Key Results
- GC-PG reduces reward disparities across groups while maintaining competitive overall performance
- The algorithm achieves convergence to a stationary point at rate O(1/T)
- Experiments demonstrate Pareto optimal fairness-performance trade-offs on both synthetic and real-world datasets

## Why This Works (Mechanism)
The algorithm works by incorporating group-sensitive fairness constraints directly into the policy optimization process through a Lagrangian formulation. The doubly robust estimator mitigates the high variance and bias inherent in offline policy evaluation, enabling more stable gradient updates. By explicitly constraining the maximum difference in expected rewards across groups, the method ensures that the learned policy doesn't disproportionately favor any particular group. The alternating optimization between policy parameters and Lagrange multiplier allows the system to adaptively balance the trade-off between reward maximization and fairness satisfaction throughout the learning process.

## Foundational Learning
- Doubly Robust Estimation: Combines importance weighting with regression to reduce variance in off-policy evaluation - needed because logged data introduces selection bias and high variance in reward estimation; quick check: verify that the estimator converges faster than pure importance weighting approaches
- Lagrangian Optimization: Framework for constrained optimization that transforms constrained problems into unconstrained ones using Lagrange multipliers - needed to balance the competing objectives of reward maximization and fairness constraints; quick check: ensure the dual variable converges to an appropriate value that satisfies the constraint
- Policy Gradient Methods: Reinforcement learning techniques that directly optimize policy parameters through gradient ascent - needed because we're optimizing policies in a bandit setting rather than value functions; quick check: verify gradient estimates are unbiased and have reasonable variance
- Stationary Point Convergence: Theoretical guarantee that the optimization process converges to a point where the gradient is zero - needed to provide theoretical assurance of the algorithm's behavior; quick check: monitor gradient norms during training to verify convergence

## Architecture Onboarding

Component Map:
Data -> Doubly Robust Estimator -> Policy Gradient Computation -> Lagrangian Update -> Policy Update

Critical Path:
1. Data preprocessing and group assignment
2. Doubly robust reward estimation for each group
3. Policy gradient computation with fairness constraints
4. Lagrange multiplier update
5. Policy parameter update

Design Tradeoffs:
The algorithm trades off some reward performance for fairness improvements, with the degree of trade-off controlled by the constraint threshold. Using doubly robust estimation adds computational overhead but significantly reduces variance compared to pure importance weighting. The Lagrangian formulation provides a principled way to handle constraints but requires careful tuning of the initial multiplier and step sizes. The offline setting limits the algorithm to logged data, which may not fully represent the true distribution of contexts and actions.

Failure Signatures:
- If the constraint set is empty, the algorithm cannot find a feasible solution
- High variance in the doubly robust estimator can lead to unstable gradient updates
- Poor choice of constraint threshold may result in either trivial solutions or infeasible optimization
- Insufficient logged data for certain groups can cause unreliable gradient estimates and fairness violations

First Experiments:
1. Test on a synthetic dataset with known group distributions to verify fairness constraint satisfaction
2. Evaluate sensitivity to different constraint thresholds on a real-world dataset
3. Compare variance reduction of doubly robust estimator against importance weighting baseline

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis assumes non-empty constraint set, which may not hold with severe fairness constraints
- O(1/T) convergence rate may be conservative given high variance in policy gradients under fairness constraints
- Experiments focus on binary group partitions, not capturing intersectional fairness complexities

## Confidence
- High: Core algorithmic contribution and theoretical analysis are rigorous and mathematically sound
- Medium: Empirical results are promising but limited to a small set of datasets and group definitions
- Medium: Claim about Pareto optimal trade-offs needs validation across broader fairness metrics and domains

## Next Checks
1. Evaluate the algorithm's performance on datasets with multiple intersecting group attributes to assess intersectional fairness
2. Conduct sensitivity analysis across different reward distributions and constraint thresholds to test robustness
3. Implement a large-scale real-world deployment to validate the practical effectiveness of the fairness constraints in production environments