---
ver: rpa2
title: Cross-Subject and Cross-Montage EEG Transfer Learning via Individual Tangent
  Space Alignment and Spatial-Riemannian Feature Fusion
arxiv_id: '2508.08216'
source_url: https://arxiv.org/abs/2508.08216
tags:
- performance
- subjects
- features
- training
- itsa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of cross-subject generalization
  in EEG-based Brain-Computer Interfaces (BCIs) for music-assisted gait rehabilitation,
  where inter-subject variability and movement artifacts hinder model transferability.
  The authors propose Individual Tangent Space Alignment (ITSA), a novel pre-alignment
  strategy incorporating subject-specific recentering, distribution matching, and
  supervised rotational alignment.
---

# Cross-Subject and Cross-Montage EEG Transfer Learning via Individual Tangent Space Alignment and Spatial-Riemannian Feature Fusion

## Quick Facts
- arXiv ID: 2508.08216
- Source URL: https://arxiv.org/abs/2508.08216
- Authors: Nicole Lai-Tan; Xiao Gu; Marios G. Philiastides; Fani Deligianni
- Reference count: 40
- Primary result: Parallel RCSP-Riemannian fusion with ITSA improves F1 scores from 54.39% to 61.34% in advance tempo condition

## Executive Summary
This paper addresses cross-subject transfer learning challenges in EEG-based Brain-Computer Interfaces for music-assisted gait rehabilitation. The authors propose Individual Tangent Space Alignment (ITSA), a three-stage pre-alignment strategy that normalizes subject-specific geometric representations in tangent space. Combined with a hybrid architecture fusing Regularised Common Spatial Patterns (RCSP) with Riemannian geometry, ITSA demonstrates significant performance improvements over baselines in leave-one-subject-out cross-validation experiments. The method also maintains robust performance when transferring from high-density to simulated low-density electrode montages.

## Method Summary
The method combines ITSA with hybrid feature extraction and fusion. ITSA operates on covariance matrices in tangent space through subject-specific recentering to identity, distribution rescaling, and supervised rotational alignment using calibration data. Features are extracted via two parallel branches: RCSP spatial filtering with log-variance features, and ITSA-aligned tangent space features. These representations are fused either sequentially or in parallel before classification with a linear SVM. The approach handles cross-montage transfer through PCA-based dimensionality reduction aligned to match electrode configurations.

## Key Results
- ITSA with parallel RCSP-Riemannian fusion achieved 61.34% F1 (advance tempo) vs. 54.39% baseline
- ITSA with parallel RCSP-Riemannian fusion achieved 58.52% F1 (delay tempo) vs. 42.65% baseline
- Cross-montage transfer maintained performance with minimal degradation (1.60-1.86% reduction) compared to baseline (4.54-7.80% reduction)

## Why This Works (Mechanism)

### Mechanism 1: Individual Tangent Space Alignment (ITSA) Reduces Covariate Shift
- Claim: Subject-specific recentering combined with distribution matching and rotational alignment improves cross-subject transfer by normalizing individual geometric representations while preserving class structure.
- Mechanism: ITSA operates in three sequential stages: (1) each subject's covariance matrices are individually recentered to the identity matrix using their log-Euclidean mean, establishing a common reference point; (2) features are rescaled to normalize distribution spread around class centers; (3) supervised rotational alignment via SVD aligns test subject features to training space using class-wise anchor points from a calibration subset.
- Core assumption: Inter-subject variability manifests primarily as geometric transformations (translation, scaling, rotation) in the tangent space representation of covariance matrices, rather than fundamental differences in class conditional distributions.
- Evidence anchors: [abstract] "ITSA, a novel pre-alignment strategy incorporating subject-specific recentering, distribution matching, and supervised rotational alignment to enhance cross-subject generalisation." [section VII-C] Ablation shows ITSA (61.15% F1) outperforms recentering-only baseline "Adaptive M" (57.44%) and tangent space alignment without individual recentering "TS" (60.99%) for sequential fusion in advance tempo.

### Mechanism 2: Parallel RCSP-Riemannian Fusion Preserves Complementary Information
- Claim: Parallel extraction and concatenation of RCSP spatial-filtered features with Riemannian tangent space features outperforms sequential application by preventing information loss from premature dimensionality reduction.
- Mechanism: In the parallel configuration, spatial filtering (RCSP) and tangent space projection operate independently on the same input signals. RCSP log-variance features capture discriminative spatial patterns that maximize class variance separation, while tangent space features preserve the full geometric structure of covariance matrices. Concatenation allows the SVM classifier to learn from both representations simultaneously.
- Core assumption: RCSP and Riemannian features encode partially non-redundant discriminative information, and their combination is more expressive than either alone.
- Evidence anchors: [abstract] "The parallel RCSP-Riemannian fusion approach showed the greatest enhancement, with average F1 scores improving from 54.39% to 61.34%." [section VII-B] Parallel fusion with ITSA achieves 58.52% (delay) vs. sequential's 57.28%, with statistical significance (t(17)=43.06, p=0.0008).

### Mechanism 3: Cross-Montage Transfer via Dimensionality Alignment
- Claim: Training on high-density (108 electrodes) and testing on simulated low-density montages (10-20, 10-10) with PCA-based dimensionality reduction maintains performance when combined with ITSA.
- Mechanism: High-dimensional training features are projected to a lower-dimensional subspace via PCA to match testing feature dimensions. ITSA's alignment steps operate on these reduced features, with the rotation step occurring after rescaling but before final classification. This allows models trained in well-instrumented settings to deploy on portable systems.
- Core assumption: The discriminative subspace learned from high-density data generalizes to low-density electrode subsets, and PCA preserves this subspace adequately.
- Evidence anchors: [section VII-D] "ITSA maintained enhanced classification performance using only 19 or 60 electrodes, surpassing baseline performance achieved with a high-density test configuration." [section VII-D] Performance degradation with ITSA across montages is minimal (1.60-1.86% reduction for 10-10 montage) vs. baseline (4.54-7.80% reduction).

## Foundational Learning

- Concept: **Riemannian geometry for covariance matrices**
  - Why needed here: Covariance matrices from EEG signals are Symmetric Positive Definite (SPD) and lie on a curved manifold, not Euclidean space. Standard Euclidean operations (arithmetic mean, Euclidean distance) misrepresent geometric relationships. The tangent space projection enables local Euclidean approximation while preserving curvature structure.
  - Quick check question: Given two covariance matrices C1 and C2, why is the arithmetic mean (C1 + C2)/2 geometrically suboptimal compared to the Fréchet mean on the Riemannian manifold?

- Concept: **Common Spatial Patterns (CSP) and regularization**
  - Why needed here: CSP maximizes variance separation between classes but overfits on small datasets and is sensitive to noise. Regularized CSP (RCSP) shrinks covariance estimates toward the identity matrix using diagonal loading with Ledoit-Wolf shrinkage, trading bias for reduced variance.
  - Quick check question: If you have 50 trials with 108 channels, why would standard CSP likely overfit, and how does regularization change the objective function?

- Concept: **Transfer learning and covariate shift**
  - Why needed here: Cross-subject EEG transfer fails because training and testing distributions differ due to anatomical variability, electrode placement, and signal noise. Pre-alignment strategies aim to reduce covariate shift (distribution mismatch) before classification, but cannot address concept shift (relationship changes between features and labels).
  - Quick check question: If source subjects show strong class separation along a specific feature dimension but target subjects show overlapping distributions along the same dimension, is this covariate shift or concept shift, and which alignment methods could address it?

## Architecture Onboarding

- Component map: EEG trials → covariance matrices → ITSA pipeline → RCSP/Riemannian features → fusion → SVM classification
- Critical path:
  1. Segment EEG into adaptive vs. non-adaptive heel strike windows
  2. Compute trial-wise covariance matrices
  3. Apply subject-specific recentering independently for each subject (training and test)
  4. Project to tangent space using identity reference
  5. Rescale features to unit norm
  6. For cross-montage: apply PCA to reduce training features to match test dimensions
  7. Split test data into calibration/evaluation subsets (nested 2-fold CV)
  8. Compute class-wise anchor points and rotation matrix via SVD using calibration subset
  9. Apply rotation to evaluation subset
  10. Train SVM on aligned training features, evaluate on rotated evaluation features

- Design tradeoffs:
  - Sequential vs. parallel fusion: Sequential reduces dimensionality earlier (lower computational cost) but risks information loss; parallel preserves both representations but requires more memory and may need stronger regularization
  - Calibration subset size: Larger calibration improves rotation estimation but reduces evaluation data; paper uses 2-fold CV to balance this
  - PCA retention threshold: 99.9% variance retention preserves information but may retain noise; paper experiments with 25% and 1% retention for cross-montage

- Failure signatures:
  - Near-chance performance across all subjects: Check covariance matrix conditioning (ill-conditioned matrices cause numerical instability in log mapping)
  - High variance in LOSO results across subjects: Indicates alignment is not robust to certain subject characteristics; inspect subjects with worst performance for outlier patterns
  - Performance collapse in cross-montage but not full-density: PCA reduction is discarding discriminative dimensions; retrain with montage-specific feature selection
  - Calibration rotation fails to converge: SVD produces near-zero singular values; class anchor points are too similar, indicating poor class separation in feature space

- First 3 experiments:
  1. **Baseline reproduction without ITSA**: Implement LOSO-CV with parallel RCSP-Riemannian fusion but no pre-alignment. Verify approximate F1 scores match baseline reported values (54.39% advance, 42.65% delay). This confirms your feature extraction pipeline is correct before adding alignment.
  2. **Ablation of ITSA components**: Implement three variants: (a) recentering only, (b) recentering + rescaling, (c) full ITSA. Compare against paper's ablation results (Table III) to validate each component's contribution. If your results diverge significantly, check reference matrix computation and rotation matrix derivation.
  3. **Cross-montage with controlled degradation**: Train on 108-electrode data, test on 10-20 montage (19 electrodes). Compare baseline (no ITSA) vs. ITSA performance degradation. Verify ITSA reduces degradation from ~7% to ~2% as reported. This tests whether your dimensionality alignment pipeline handles the cross-montage scenario correctly.

## Open Questions the Paper Calls Out
None

## Limitations
- Transfer learning gains depend critically on the assumption that inter-subject variability is primarily geometric rather than conceptual
- Performance relies on calibration subset labels being reliable for supervised rotational alignment
- Cross-montage generalization validated on single dataset without independent replication

## Confidence
- Cross-subject performance improvements (ITSA mechanisms): **High** - Multiple ablation studies and statistical tests support this claim
- Parallel vs. sequential fusion superiority: **Medium** - Significant in one condition only, with no mechanistic explanation for condition-specific differences
- Cross-montage generalization: **Low-Medium** - Single dataset validation, no independent replication of the cross-montage transfer capability

## Next Checks
1. Test ITSA on a dataset with known concept drift (e.g., different motor tasks across subjects) to validate robustness beyond geometric transformations
2. Implement label noise injection in the calibration subset to quantify sensitivity of supervised rotational alignment to label errors
3. Replicate cross-montage results using a completely different EEG dataset with known ground truth low-density montages to verify the dimensionality alignment procedure generalizes beyond the original data