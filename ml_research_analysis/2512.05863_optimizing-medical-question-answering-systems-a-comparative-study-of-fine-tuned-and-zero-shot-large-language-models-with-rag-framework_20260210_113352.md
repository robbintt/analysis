---
ver: rpa2
title: 'Optimizing Medical Question-Answering Systems: A Comparative Study of Fine-Tuned
  and Zero-Shot Large Language Models with RAG Framework'
arxiv_id: '2512.05863'
source_url: https://arxiv.org/abs/2512.05863
tags:
- medical
- llms
- system
- llama
- fine-tuned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces a retrieval-augmented medical QA system combining
  open-source LLMs (LLaMA 2, Falcon) with a RAG framework for biomedical question
  answering. Fine-tuning is performed via LoRA for efficient domain adaptation, while
  retrieval from medical literature grounds answers in evidence.
---

# Optimizing Medical Question-Answering Systems: A Comparative Study of Fine-Tuned and Zero-Shot Large Language Models with RAG Framework

## Quick Facts
- arXiv ID: 2512.05863
- Source URL: https://arxiv.org/abs/2512.05863
- Reference count: 18
- Fine-tuned LLaMA 2 + RAG achieves 71.8% accuracy on PubMedQA, outperforming zero-shot baseline of 55.4%

## Executive Summary
This study introduces a retrieval-augmented medical QA system combining open-source LLMs (LLaMA 2, Falcon) with a RAG framework for biomedical question answering. Fine-tuning is performed via LoRA for efficient domain adaptation, while retrieval from medical literature grounds answers in evidence. The approach is evaluated on PubMedQA and MedMCQA datasets, achieving 71.8% accuracy with LLaMA 2 + RAG, substantially outperforming zero-shot baselines (55.4%). Factual errors are reduced by ~60% compared to generation without retrieval, and source references are provided for transparency. The results demonstrate that RAG-augmented open-source LLMs offer a resource-efficient, reliable, and transparent alternative for biomedical QA, with potential for clinical informatics applications.

## Method Summary
The system employs a RAG architecture where a biomedical DPR retriever fetches top-5 relevant passages from PubMed abstracts, clinical guidelines, and FAQs for each medical question. LLaMA 2-13B and Falcon-40B models are fine-tuned with LoRA adapters (rank=16, alpha=32, updating <0.5% of parameters) using ~15,000 QA pairs from PubMedQA, MedMCQA, and curated medical FAQs. Training uses AdamW optimizer with learning rate 2×10⁻⁴, cosine scheduling, 3 epochs, and 4× NVIDIA A100 GPUs. The retriever and fine-tuned LLM work in tandem to generate answers up to 256 tokens, with citations to source passages when available.

## Key Results
- LLaMA 2 + RAG achieves 71.8% accuracy on PubMedQA test set
- Factual error rate reduced from 35% to 14% with RAG integration (~60% reduction)
- Falcon-40B + RAG achieves 74.2% accuracy, marginally outperforming LLaMA 2
- Citation consistency limited: only 42% of answers include clear source attribution
- Response latency: 3.8s for LLaMA 2, 5.2s for Falcon

## Why This Works (Mechanism)

### Mechanism 1: Retrieval-Grounded Answer Generation
Providing retrieved medical literature as context during generation reduces hallucinations by anchoring model outputs to verifiable evidence. The model conditions generation on retrieved passages rather than relying solely on parametric knowledge. Retrieved text provides specific phrasing and facts that appear in outputs, improving verifiability. Core assumption: The retriever surfaces genuinely relevant passages; retrieval quality bounds generation quality. Evidence: "grounding answers in retrieved evidence reduces unsupported content by approximately 60%"; "retrieval augmentation reduced factual errors from 35% in the fine-tuned-only setting to 14% with RAG"; RAG-BioQA confirms RAG improves long-form biomedical QA comprehensiveness. Break condition: Retrieval failure leaves model with only parametric knowledge, returning to baseline hallucination rates.

### Mechanism 2: LoRA-Based Efficient Domain Adaptation
Low-Rank Adaptation enables effective medical domain specialization while updating <0.5% of model parameters. LoRA inserts trainable low-rank matrices into each transformer layer while freezing original weights. Gradients flow through adapters, teaching the model medical terminology patterns and QA formats without full backpropagation. Core assumption: Medical QA competence can be captured in a low-dimensional subspace of parameter updates. Evidence: "adapter rank to 16 and alpha to 32, updating fewer than 0.5% of the model's weights"; Fine-tuned LLaMA 2 improves PubMedQA from 55.4% to 64.3% without RAG; MedBioLM achieves similar domain adaptation with LoRA for medical/biological QA. Break condition: Training data contains incorrect QA pairs or retrieval contexts; model learns to trust noisy evidence.

### Mechanism 3: Dense Semantic Retrieval with Biomedical Encoders
DPR trained on biomedical text provides semantically relevant passages that standard lexical search would miss. A bi-encoder produces dense embeddings for questions and documents in a shared space. Inner-product similarity retrieves top-k passages based on semantic meaning, not just keyword overlap. Core assumption: The biomedical DPR encoder generalizes to unseen medical questions; embedding quality determines retrieval precision. Evidence: "dense passage retriever (DPR) trained on biomedical text to embed the question and candidate passages in the same space"; "k=5 provides sufficient context while balancing relevance and computational efficiency"; Multi-step retrieval paper suggests single-step retrieval may limit radiology QA. Break condition: Domain shift in questions produces poor embedding matches; top-k passages become irrelevant.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: Core architecture pattern; understanding retrieval-generation coupling is prerequisite for debugging either stage.
  - Quick check question: If accuracy drops after RAG integration, how would you isolate whether the retriever or generator is at fault?

- **Concept: Parameter-Efficient Fine-Tuning (PEFT/LoRA)**
  - Why needed here: Enables domain adaptation without prohibitive GPU costs; adapter rank and alpha directly affect capacity.
  - Quick check question: What happens to model behavior if adapter rank is set too low for a complex domain?

- **Concept: Dense vs. Lexical Retrieval**
  - Why needed here: DPR semantic matching differs fundamentally from BM25 keyword matching; failure modes differ.
  - Quick check question: Would a biomedical DPR retrieve a relevant passage that uses different terminology than the query? Why or why not?

## Architecture Onboarding

- **Component map:** Query Encoder -> Vector Database -> Retrieval Module -> Prompt Constructor -> Fine-tuned LLM -> Output Parser
- **Critical path:** Query → Embedding → Retrieval (k=5) → Prompt Assembly → Generation (max 256 tokens) → Response with citations. Retrieval latency + generation latency determine end-to-end response time (3.8s for LLaMA 2, 5.2s for Falcon).
- **Design tradeoffs:**
  - k value: Higher k provides more context but increases prompt length and latency; paper uses k=5 as balance
  - Model size: LLaMA 2-13B (18GB VRAM) vs Falcon-40B (42GB VRAM)—Falcon offers marginal accuracy gains at >2x memory cost
  - Open vs. proprietary: Open-source enables customization and citation tracking; GPT-4 (81% PubMedQA) remains accuracy leader but lacks source attribution
- **Failure signatures:**
  - Retrieval failure: Model answers without referencing retrieved passages; check retriever recall on query type
  - Hallucination persistence: Unsupported claims despite retrieved evidence; may indicate fine-tuning data issues or instruction-following gaps
  - Missing citations: Only 42% of answers include clear source attribution; citation format training inconsistent
- **First 3 experiments:**
  1. **Retrieve-and-verify baseline:** Run zero-shot LLaMA 2 on PubMedQA test split without RAG; measure baseline accuracy and manually annotate hallucination rate on 50 samples.
  2. **Retrieval ablation:** Test k∈{1,3,5,10} while holding model fixed; plot accuracy vs. latency to validate k=5 choice for your infrastructure.
  3. **Citation consistency probe:** Generate 50 answers with RAG; manually evaluate whether citations match retrieved documents. If <60% consistent, inspect prompt instructions and consider structured output training.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can structured source attribution be consistently achieved in RAG-based medical QA systems? The authors report that "the model's ability to provide consistent structured citations remained limited, with only 42% of answers including clear source attribution." Although the model was trained to reference sources, citation formats vary inconsistently, and no architectural or training modifications were tested to address this limitation. Comparative experiments testing citation-enforcing mechanisms with quantitative citation accuracy metrics would resolve this.

- **Open Question 2:** What interventions can reduce the three dominant error types in medical QA—misinterpretation of statistical findings, overgeneralization from study populations, and reliance on outdated information? The authors identify that the most common remaining errors are: "misinterpretation of statistical findings from retrieved papers (32% of errors), overgeneralization from specific study populations (28%), and outdated information from older retrieved documents (25%)." The paper documents these error categories but does not propose or evaluate targeted mitigation strategies. Error reduction measured after implementing interventions such as statistical reasoning modules, population-context prompting, or recency-weighted retrieval scoring would resolve this.

- **Open Question 3:** How can clinician-in-the-loop feedback mechanisms be effectively integrated to improve clinical reliability of RAG-based medical QA systems? The conclusion states that "future work will explore clinician-in-the-loop feedback mechanisms" and the paper emphasizes that "robust evaluation by medical professionals in real clinical workflows is essential." The current system relies on supervised fine-tuning without iterative feedback from medical professionals during training or inference. Studies comparing pre- and post-integration performance with metrics on clinical appropriateness, physician acceptance rates, and error correction effectiveness would resolve this.

## Limitations
- Focus on short-answer multiple-choice QA rather than long-form clinical reasoning tasks
- 14% hallucination rate remains clinically concerning for patient-facing applications
- Only 42% of answers include clear source attribution, undermining transparency claims
- Does not address temporal aspects of medical knowledge or rare disease query performance

## Confidence
- **High confidence:** Retrieval augmentation reduces factual errors (~60% reduction verified); LoRA-based fine-tuning achieves stated parameter efficiency (<0.5% weights updated); accuracy improvements on PubMedQA and MedMCQA datasets are reproducible
- **Medium confidence:** Clinical utility claims require further validation; 14% hallucination rate may be unacceptable for patient-facing applications; citation consistency (42%) suggests incomplete transparency
- **Low confidence:** Long-term knowledge currency; performance on rare conditions; real-world deployment latency under variable network conditions

## Next Checks
1. **Clinical safety audit:** Have 3 medical domain experts independently evaluate 100 RAG-generated answers for clinical safety and accuracy, focusing on whether the 14% hallucination rate includes potentially harmful misinformation.

2. **Citation completeness analysis:** Systematically sample 200 generated answers to determine actual citation consistency rate and identify prompt or model factors that cause missing attributions beyond the reported 42%.

3. **Retrieval coverage stress test:** Evaluate retriever performance on a curated set of rare disease and novel treatment queries to quantify performance degradation when medical literature coverage is sparse, measuring both retrieval recall and subsequent generation quality.