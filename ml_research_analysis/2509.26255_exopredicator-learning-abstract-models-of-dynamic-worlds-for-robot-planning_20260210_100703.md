---
ver: rpa2
title: 'ExoPredicator: Learning Abstract Models of Dynamic Worlds for Robot Planning'
arxiv_id: '2509.26255'
source_url: https://arxiv.org/abs/2509.26255
tags:
- robot
- conditions
- effects
- domino
- faucet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses long-horizon embodied planning in dynamic
  environments where both agent actions and external processes unfold concurrently.
  The authors propose ExoPredicator, a framework that learns abstract world models
  with symbolic state representations and causal processes for both endogenous actions
  and exogenous mechanisms.
---

# ExoPredicator: Learning Abstract Models of Dynamic Worlds for Robot Planning

## Quick Facts
- arXiv ID: 2509.26255
- Source URL: https://arxiv.org/abs/2509.26255
- Reference count: 40
- Key outcome: Jointly learned symbolic world models with causal processes enable near-perfect solve rates on long-horizon embodied planning tasks in dynamic environments

## Executive Summary
This paper addresses long-horizon embodied planning in dynamic environments where both agent actions and external processes unfold concurrently. The authors propose ExoPredicator, a framework that learns abstract world models with symbolic state representations and causal processes for both endogenous actions and exogenous mechanisms. Using variational Bayesian inference with LLM proposals, the system learns from limited data to model delayed cause-effect relations and generalizes to held-out tasks with more objects and complex goals. Across five simulated robotics environments, the learned models enabled fast planning and achieved near-perfect solve rates, significantly outperforming baselines including HRL, VLM planning, and operator learning approaches.

## Method Summary
ExoPredicator learns symbolic causal models of dynamic environments by first segmenting demonstration trajectories into abstract states at points of change. It clusters effects, uses LLMs to propose candidate causal processes, then learns parameters via variational inference that approximates arrival times. The framework jointly learns predicates and processes, representing each causal relation with conditions-at-start, conditions-overall, effects, weights, and delay distributions. These abstract models enable efficient big-step planning that simulates concurrent exogenous and endogenous dynamics while skipping irrelevant timesteps.

## Key Results
- Learned models achieved 92-100% solve rates across five simulated environments with complex, concurrent dynamics
- Outperformed HRL (15-40% solve rates), VLM planning (5-35% solve rates), and operator learning approaches (30-55% solve rates)
- Plans using learned models were significantly faster than low-level simulation, with big-step transitions reducing planning time by 10-50x
- Generalized successfully to held-out tasks with more objects and complex goals than training demonstrations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Jointly modeling endogenous and exogenous causal processes with stochastic delays enables tractable long-horizon planning in dynamic environments.
- **Mechanism:** Causal processes abstract frame-by-frame dynamics into discrete state transitions. Each process $L$ specifies conditions-at-start ($C$), conditions-overall ($O$), effects ($E$), weight ($W$), and delay distribution ($p_{delay}$). The big-step transition function $T_{big}$ simulates until abstract state changes, allowing the planner to skip irrelevant timesteps while preserving concurrent process dynamics.
- **Core assumption:** Exogenous processes are edge-triggered (activate on condition becoming true) and effects realize after stochastic delays that depend on conditions holding throughout.
- **Evidence anchors:**
  - [abstract] "Each causal process models the time course of a stochastic cause-effect relation."
  - [Section 3] "A causal process coarsely models a cause-effect relation... certain conditions (the 'cause') later lead to other conditions (the 'effect')."
  - [corpus] Corpus has limited directly comparable mechanisms; SLAP (2511.01107) addresses abstract planning but assumes instantaneous options, not concurrent exogenous dynamics.
- **Break condition:** If exogenous processes require continuous state variables rather than discrete predicates, or if delays are not approximately condition-independent, the abstraction loses fidelity.

### Mechanism 2
- **Claim:** Variational inference over arrival times enables scalable parameter learning despite combinatorial timing ambiguity.
- **Mechanism:** Rather than marginalizing over all delay sequences (intractable), introduce variational distributions $q^L_t(A^L_t)$ encoding beliefs about when effects arrive. The ELBO decomposes into expected log-delay probability, frame axiom terms, and effect potentials—optimized via Adam. This approximates $p(D) = p(s_{1:t}|s_0)$ without enumerating $|\Delta|^T$ combinations.
- **Core assumption:** The variational approximation sufficiently captures posterior uncertainty; arrival times are conditionally independent given the trajectory structure.
- **Evidence anchors:**
  - [Section 5.1] "We therefore approximate the marginal likelihood by introducing variational distributions corresponding to the time at which each cause realizes its effect."
  - [Section A.3] Full ELBO derivation showing factorization over features and processes.
  - [corpus] No corpus papers use variational inference for symbolic causal process learning; related work typically assumes known models or uses different learning paradigms.
- **Break condition:** If effect timing has complex temporal dependencies (e.g., delays depend on when other processes completed), the mean-field-style variational approximation may be too restrictive.

### Mechanism 3
- **Claim:** LLM proposals combined with Bayesian model selection make discrete structure learning tractable while grounding symbolic models in experience.
- **Mechanism:** For each effect cluster, prompt LLM with trajectory segments to propose candidate condition sets. Score each via $\log p(C|L) + \log p(L)$ where $p(D|L)$ uses the variational bound and $p(L)$ is MDL prior. Select highest-scoring process. For predicates, greedy local search over LLM-proposed candidates.
- **Core assumption:** LLM proposals contain the correct structure within a small candidate set; Bayesian posterior reliably identifies it given limited data.
- **Evidence anchors:**
  - [Section 5.2] "To narrow down the discrete search, we prompt a language model with the cluster and ask it to propose a small number of candidate processes."
  - [Section 6, Q3] "Without computing the Bayesian posterior... selection is based entirely on the prior in the LLM, which is not always reliable."
  - [corpus] Using LLMs for Planning Domain Abstraction (2510.20258) shows similar LLM-driven abstraction but without the Bayesian grounding component.
- **Break condition:** If environment dynamics are sufficiently novel that LLM priors miss key conditions, or if multiple processes cause identical effects, the pipeline may learn incomplete models.

## Foundational Learning

- **Concept: Variational Inference / ELBO**
  - Why needed here: Core mechanism for learning delay distributions and process weights from trajectory data.
  - Quick check question: Can you derive why $\log p(x) \geq \mathbb{E}_q[\log p(x,z)] - \mathbb{E}_q[\log q(z)]$ and explain what makes this bound tight?

- **Concept: PDDL-style Symbolic Planning**
  - Why needed here: The big-step planner uses A* with relaxed planning graph heuristics adapted for causal processes.
  - Quick check question: Given operators with preconditions and effects, how does the Fast-Forward heuristic estimate distance-to-goal?

- **Concept: Minimum Description Length (MDL) Priors**
  - Why needed here: Process selection uses MDL prior $p(L)$ penalizing complex conditions to avoid overfitting.
  - Quick check question: Why does a simpler model with lower training likelihood sometimes have higher posterior probability?

## Architecture Onboarding

- **Component map:** Trajectory Data → Segmentation → Clustering by Effects → LLM Proposer → Candidate Processes/Predicates → Variational Inference → Parameter Estimation (W, p_delay) → Bayesian Scoring → Process Selection → Learned World Model → A* Planner with T_big

- **Critical path:** The inner variational loop (Section 5.1) is called repeatedly during predicate selection. Caching processes/parameters from the full-predicate run is essential—without this optimization, scoring each predicate subset requires full re-inference.

- **Design tradeoffs:**
  - One-process-per-effect assumption (Section A.4) simplifies search but limits expressivity when multiple causes produce identical effects.
  - Edge-triggered exogenous activation means processes can't model conditions that must hold for a duration before triggering.
  - Assuming atomic effects per process forces learning multiple processes for multi-predicate changes.

- **Failure signatures:**
  - Planner reports goal unreachable for solvable tasks → likely missing predicate in abstraction (see Boil spill condition failure in Section 6).
  - Plans executed but effects don't materialize → delay distribution mismatch or condition-overall violated.
  - Learning doesn't converge → check that LLM proposals include necessary conditions; verify cluster purity.

- **First 3 experiments:**
  1. **Smoke test on Coffee domain:** Train with 1 demo, verify JugFilled predicate is invented and exogenous filling process learned. Check that plan involves NoOp while filling.
  2. **Ablate Bayesian scoring:** Run No Bayes variant—confirm performance drop in domains requiring uncommon dynamics (should match Section 6 results).
  3. **Test delay generalization:** Evaluate Manual-d (uniform delays) on Domino—verify that incorrect delay parameters cause misclassification of solvability, confirming learned timing matters.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can ExoPredicator scale to more complex, larger-scale environments with significantly more objects, longer horizons, and more diverse exogenous processes?
- **Basis in paper:** [explicit] The conclusion states: "Future work will scale the framework to more complex, larger-scale environments, enhance learning with foundation models, and explore the interplay between skill and world modeling."
- **Why unresolved:** Current experiments only involve tabletop environments with 3-6 object types and relatively short planning horizons; scalability remains untested.
- **What evidence would resolve it:** Demonstration of comparable solve rates and planning efficiency in environments with 10x+ objects and multi-step compositional goals.

### Open Question 2
- **Question:** How can the framework learn more complete causal condition models to avoid missing critical preconditions (e.g., the water spill condition when a jug is full)?
- **Basis in paper:** [explicit] The authors note: "ExoPredicator is unable to solve all tasks in Boil, partly because it failed to recognize the full condition under which a spill can happen: it learned a process for water spilling when there is nothing under the faucet, but not when the jug underneath is full."
- **Why unresolved:** The current intersection-based precondition learning may miss disjunctive or context-dependent conditions.
- **What evidence would resolve it:** Ablation studies comparing current precondition learning to methods that explicitly search for multiple distinct condition clusters per effect.

### Open Question 3
- **Question:** How does joint learning of skills and world models compare to the current assumption of fixed, pre-provided skills?
- **Basis in paper:** [explicit] The conclusion calls for exploring "the interplay between skill and world modeling" while the current work explicitly assumes "built-in motor skills, such as Pick/Place."
- **Why unresolved:** The paper only evaluates with predefined skills; whether learned skills would improve, degrade, or be incompatible with the causal process framework is unknown.
- **What evidence would resolve it:** Experiments where skills are learned online alongside causal processes, measuring both planning success and skill acquisition efficiency.

### Open Question 4
- **Question:** Can the framework handle multiple distinct exogenous processes producing the same effect type?
- **Basis in paper:** [inferred] The paper states in Section A.4: "we assume that for any given effect (a unique pair of add/delete atoms), there is at most one exogenous process that causes it," acknowledging this as a simplifying assumption that prevents learning multiple distinct causes for the same outcome.
- **Why unresolved:** Real-world environments often have multiple pathways to identical state changes; the current restriction limits expressivity.
- **What evidence would resolve it:** Experiments in environments designed with multiple distinct triggers for the same predicate change, comparing current performance against a modified learner without the single-process-per-effect constraint.

## Limitations
- LLM reliability is critical—when proposals miss key conditions, learned models become incomplete (evidenced by Boil spill condition failure)
- One-process-per-effect assumption limits expressiveness when multiple causes produce identical effects
- Edge-triggered exogenous processes cannot model sustained conditions or duration-based triggers

## Confidence
- **High confidence:** The core variational inference mechanism for learning delay distributions and process weights from trajectory data
- **Medium confidence:** The LLM-driven structure learning approach, which critically depends on LLM quality and the assumption that correct structures appear in proposals
- **Medium confidence:** The abstraction's ability to generalize to more complex tasks with additional objects, though test environments remain relatively constrained

## Next Checks
1. **Stress test LLM reliability:** Systematically evaluate model performance when LLM proposals are intentionally corrupted or missing key conditions. Quantify the minimum quality threshold for proposals needed to maintain reasonable performance.

2. **Evaluate multi-cause effects:** Create a domain where two distinct processes produce identical effects, then measure whether the framework learns separate processes or conflates them, and assess impact on planning performance.

3. **Test continuous state handling:** Extend the framework to handle continuous predicates by discretizing or using probabilistic predicates. Evaluate whether planning performance degrades and identify the threshold where abstraction fidelity becomes insufficient.