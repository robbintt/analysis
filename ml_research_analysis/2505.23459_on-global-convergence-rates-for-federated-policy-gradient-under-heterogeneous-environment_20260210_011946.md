---
ver: rpa2
title: On Global Convergence Rates for Federated Policy Gradient under Heterogeneous
  Environment
arxiv_id: '2505.23459'
source_url: https://arxiv.org/abs/2505.23459
tags:
- lemma
- policy
- have
- where
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of federated policy gradient methods
  in heterogeneous reinforcement learning environments, where different agents interact
  with different MDPs. The authors show that heterogeneity can necessitate non-deterministic
  or even time-varying optimal policies, unlike classical RL.
---

# On Global Convergence Rates for Federated Policy Gradient under Heterogeneous Environment

## Quick Facts
- arXiv ID: 2505.23459
- Source URL: https://arxiv.org/abs/2505.23459
- Authors: Safwan Labbi; Paul Mangold; Daniil Tiapkin; Eric Moulines
- Reference count: 40
- Primary result: Federated policy gradient methods achieve linear speed-up in heterogeneous RL environments with convergence to near-optimal policies under local Łojasiewicz conditions

## Executive Summary
This paper addresses federated policy gradient methods in heterogeneous reinforcement learning environments where different agents interact with distinct Markov decision processes (MDPs). The authors demonstrate that unlike classical RL, heterogeneous settings may require non-deterministic or time-varying optimal policies. They introduce three algorithms - S-FedPG, RS-FedPG, and b-RS-FedPG - with convergence guarantees to near-optimal policies under local Łojasiewicz conditions. The work establishes linear speed-up in the number of agents and provides explicit convergence rates, with b-RS-FedPG employing a novel bit-level softmax parameterization that transforms multi-action problems into binary ones for better theoretical properties.

## Method Summary
The authors propose a federated policy gradient framework for heterogeneous environments, where each agent has access to a local reward function and transition kernel. They develop three algorithms: S-FedPG (Stochastic Federated Policy Gradient) which performs local updates and communicates with server averaging, RS-FedPG (Randomized Stochastic Federated Policy Gradient) which uses local SGD with unbiased gradient estimators, and b-RS-FedPG (bit-level Randomized Stochastic Federated Policy Gradient) which parameterizes policies using bit-level softmax. The key innovation is recognizing that heterogeneity can lead to non-deterministic or time-varying optimal policies, requiring new theoretical analysis. Convergence is established under local Łojasiewicz conditions, showing that policies converge to a critical point of the global objective.

## Key Results
- Linear speed-up achieved in the number of agents for all three algorithms
- b-RS-FedPG provides bounded policy updates through bit-level softmax parameterization
- Convergence rates established under local Łojasiewicz conditions with near-optimal policy guarantees
- Empirical validation shows superior performance over federated Q-learning in heterogeneous settings

## Why This Works (Mechanism)
The federated policy gradient approach works by enabling agents to share information about their local reward functions and transition dynamics while maintaining decentralized execution. The key mechanism is that agents perform local policy updates based on their own MDPs, then aggregate these updates at a central server. This allows the system to learn a policy that performs well across the heterogeneous environment distribution rather than for any single MDP. The bit-level softmax parameterization in b-RS-FedPG is particularly effective because it converts multi-action problems into binary decisions, ensuring bounded updates and preventing policy divergence in heterogeneous settings.

## Foundational Learning
- **Local Łojasiewicz conditions**: Required to ensure convergence of policy gradient methods to critical points; quick check involves verifying gradient growth near stationary points
- **Heterogeneous MDPs**: Understanding that different agents face different reward functions and transition kernels; critical for recognizing why standard federated approaches fail
- **Bit-level softmax parameterization**: A technique that maps multi-action decisions to binary choices; enables bounded updates and better theoretical guarantees
- **Federated averaging with local SGD**: The communication pattern where agents perform multiple local updates before sharing; affects convergence speed and stability
- **Non-deterministic optimal policies**: In heterogeneous settings, the optimal policy may need to randomize or change over time; contrasts with classical RL assumptions

## Architecture Onboarding

**Component Map**: Agents (local MDP, policy, gradient computation) -> Server (parameter aggregation, distribution) -> Agents

**Critical Path**: Local policy evaluation → Local policy gradient computation → Server aggregation → Parameter broadcast → Next iteration

**Design Tradeoffs**: Local computation vs. communication frequency; algorithm simplicity vs. theoretical guarantees; parameter dimensionality vs. convergence speed

**Failure Signatures**: Policy divergence in highly heterogeneous settings; slow convergence when local Łojasiewicz constants vary significantly; communication bottlenecks in large agent populations

**First Experiments**:
1. Verify linear speed-up by varying the number of agents while keeping other factors constant
2. Test convergence under controlled heterogeneity levels to identify breaking points
3. Compare b-RS-FedPG against S-FedPG and RS-FedPG on tasks with varying action space sizes

## Open Questions the Paper Calls Out
None specified in the provided information.

## Limitations
- Theoretical assumptions (particularly local Łojasiewicz conditions) may be restrictive in practice
- Bit-level softmax parameterization may not scale well to high-dimensional action spaces
- Experimental validation limited to synthetic and gridworld environments, not complex real-world tasks

## Confidence
- **High confidence**: Theoretical framework for heterogeneous environments, linear speed-up results, bit-level softmax parameterization
- **Medium confidence**: Practical scalability of bit-level approach to high-dimensional tasks, strength of theoretical assumptions in real-world applications

## Next Checks
1. Test b-RS-FedPG on continuous control tasks with high-dimensional action spaces to evaluate scalability of the bit-level parameterization
2. Conduct ablation studies varying the degree of heterogeneity to understand when the theoretical assumptions break down
3. Compare performance against standard single-agent policy gradient methods on non-heterogeneous tasks to quantify the cost of federated learning in homogeneous settings