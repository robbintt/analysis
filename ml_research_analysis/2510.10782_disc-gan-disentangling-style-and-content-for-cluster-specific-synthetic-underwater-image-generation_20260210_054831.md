---
ver: rpa2
title: 'DISC-GAN: Disentangling Style and Content for Cluster-Specific Synthetic Underwater
  Image Generation'
arxiv_id: '2510.10782'
source_url: https://arxiv.org/abs/2510.10782
tags:
- style
- underwater
- image
- content
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating photorealistic
  underwater images by disentangling style and content in a cluster-specific training
  strategy. The authors propose DISC-GAN, which first partitions the dataset into
  four style clusters using K-means clustering on color and depth features inspired
  by Jerlov water types.
---

# DISC-GAN: Disentangling Style and Content for Cluster-Specific Synthetic Underwater Image Generation

## Quick Facts
- arXiv ID: 2510.10782
- Source URL: https://arxiv.org/abs/2510.10782
- Reference count: 31
- Primary result: Achieves SSIM 0.9012, PSNR 32.5118 dB, and FID 13.3728 for underwater image synthesis

## Executive Summary
This paper addresses the challenge of generating photorealistic underwater images by disentangling style and content in a cluster-specific training strategy. The authors propose DISC-GAN, which first partitions the dataset into four style clusters using K-means clustering on color and depth features inspired by Jerlov water types. Then, separate encoders extract style and content representations, which are fused via Adaptive Instance Normalization (AdaIN) and decoded to produce the final synthetic image. The model is trained independently on each style cluster to preserve domain-specific characteristics.

## Method Summary
The DISC-GAN framework employs a physics-informed approach to underwater image generation. It begins by clustering the synthetic RSUIGM dataset into four style domains using K-means on RGB histograms and mean depth values. Separate VGG19-based encoders extract content features (relu4_2) and style features (Gram matrices from shallow layers). These representations are fused through AdaIN, which transfers style statistics to content features while preserving structural information. The fused features are decoded through residual blocks and transposed convolutions to generate the final image. A composite loss combining L1 reconstruction and adversarial loss trains four separate models, one for each style cluster.

## Key Results
- Achieves SSIM of 0.9012, demonstrating high structural similarity to reference images
- Attains average PSNR of 32.5118 dB, indicating strong pixel-level fidelity
- Records FID of 13.3728, showing good perceptual quality and distribution matching

## Why This Works (Mechanism)

### Mechanism 1: Physics-Informed Domain Partitioning
Partitioning the dataset into discrete clusters reduces the variance the generator must learn, stabilizing training for specific water types. K-means on RGB histograms and mean depth groups images into four clusters (Blue, Light-Blue, Dark-Blue, Black). Training separate models on these subsets avoids the challenge of mapping a single latent space to highly divergent optical phenomena. This approach assumes underwater optical variations cluster into discrete, separable modes rather than a continuous spectrum.

### Mechanism 2: Statistical Style-Content Disentanglement via AdaIN
Adaptive Instance Normalization enables preservation of structural geometry while swapping low-level statistics (color/texture) specific to underwater degradation. The content encoder extracts deep features (relu4_2) to retain structure, while the style encoder captures shallow statistics (Gram matrices). AdaIN re-aligns the mean and variance of the content features to match the style features, effectively "painting" the underwater appearance onto the terrestrial structure. This assumes high-level semantics and low-level statistics are sufficiently independent in the VGG19 feature space.

### Mechanism 3: Composite Loss for Fidelity and Realism
The combination of L1 reconstruction and adversarial loss creates a balancing act between pixel-accurate structural preservation and perceptual realism. L1 loss penalizes pixel-level deviation, while the L2 adversarial loss (via PatchGAN) forces the output distribution to match the "real" underwater cluster statistics, reducing the blurriness typical of L1-only models. This assumes a simple linear combination of weighted losses is sufficient to navigate the trade-off between structural rigidity and stylistic freedom.

## Foundational Learning

- **Concept: Adaptive Instance Normalization (AdaIN)**
  - Why needed here: This is the core fusion operation replacing standard normalization. You cannot debug the style transfer without understanding how mean/variance swapping affects feature maps.
  - Quick check question: How does AdaIN differ from BatchNorm in terms of the statistics it uses for normalization?

- **Concept: The Beer-Lambert Law**
  - Why needed here: The clustering and the RSUIGM dataset are built on this physical model of light attenuation. Understanding that I(x) = J(x)e^(-βd) helps explain why depth and color are the chosen features for clustering.
  - Quick check question: Why does the exponential term in Beer-Lambert suggest that clustering by depth is relevant for style separation?

- **Concept: PatchGAN Discriminator**
  - Why needed here: The discriminator doesn't output a single real/fake label for the whole image, but rather a matrix of predictions. This focuses the model on high-frequency texture details (style) rather than just the broad shape (content).
  - Quick check question: What is the dimension of the PatchGAN output for a 256x256 image, and why does this help preserve "haze" and "turbidity" textures?

## Architecture Onboarding

- **Component map:** VGG19 encoders (content: relu4_2, style: Gram matrices from relu1_1/relu2_1/relu3_1) -> AdaIN fusion -> Decoder (ResNet blocks + Transposed Convolutions) -> PatchGAN discriminator (70×70)

- **Critical path:** 1. Pre-processing: Calculate RGB histograms and depth means for RSUIGM images. 2. Clustering: Run K-Means to assign RSUIGM images to 1 of 4 clusters. 3. Training Loop: Sample Content (SUID) + Style (Specific Cluster from RSUIGM). Forward pass -> AdaIN -> Generator -> Discriminator -> Composite Loss.

- **Design tradeoffs:** 4 Separate Models vs. 1 Conditional Model: The authors chose to train 4 independent GANs. This simplifies the learning task per model but quadruples inference memory/complexity and prevents smooth interpolation between water types. Synthetic vs. Real Training: The reliance on RSUIGM (synthetic) ensures ground truth exists for L1 loss, but may introduce domain gap issues if applied to real-world data not represented in the Jerlov types.

- **Failure signatures:** Color Cast/Artifacts: If the style encoder leaks content info, you may see ghosting or shapes from the style reference image appearing in the output. Mode Collapse: If the discriminator overpowers the generator, outputs may look identical across different content inputs (ignoring the content structure). Cluster Drift: If new data doesn't fit the k=4 schema, generation quality will degrade significantly.

- **First 3 experiments:** 1. Cluster Validation: Visualize the 4 clusters (t-SNE/PCA) to confirm that "Blue" and "Dark-Blue" are actually distinct in feature space and not overlapping. 2. Ablation on AdaIN: Replace AdaIN with simple concatenation to quantify the drop in SSIM/PSNR, proving the necessity of the specific disentanglement mechanism. 3. Depth Ablation: Retrain clustering using only RGB histograms (removing depth) to test the authors' claim that physical depth priors are essential for the style grouping.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the DISC-GAN framework be extended to ensure temporal consistency for video synthesis in dynamic underwater environments?
- Basis in paper: [explicit] The authors state, "Future work may explore the integration of temporal consistency for video synthesis, which would enable real-time simulation for underwater video feeds."
- Why unresolved: The current architecture processes individual images independently (frame-by-frame) and lacks recurrent connections or temporal loss functions to prevent flickering or morphological inconsistencies between consecutive frames.
- What evidence would resolve it: A demonstration of DISC-GAN processing underwater video sequences with metrics evaluating temporal stability (e.g., optical flow consistency) alongside standard image quality metrics.

### Open Question 2
- Question: Does the reliance on synthetic data (RSUIGM) for training and validation limit the model's ability to generalize to the complex, non-linear degradations found in real-world underwater imagery?
- Basis in paper: [inferred] The paper evaluates performance exclusively against the synthetic RSUIGM dataset. While the authors claim the model learns "complex physical phenomena," they provide no quantitative results on real-world datasets (e.g., USR-248, EUVP) or downstream tasks like object detection.
- Why unresolved: Simulation-to-real (Sim2Real) gaps often exist because synthetic models may not capture the full stochasticity of suspended particles or sensor noise present in actual ocean environments.
- What evidence would resolve it: Quantitative benchmarking of images generated by DISC-GAN when fine-tuned on or applied to real-world underwater datasets, potentially using No-Reference Image Quality Assessment (NR-IQA) metrics.

### Open Question 3
- Question: Would incorporating physical water quality metrics, such as turbidity or salinity, directly into the clustering phase improve the granularity and accuracy of the style domains?
- Basis in paper: [explicit] The authors suggest, "Expanding the clustering mechanism to consider additional water quality metrics such as turbidity or salinity could also increase realism for specialized marine domains."
- Why unresolved: The current clustering relies primarily on RGB histograms and mean depth, which may group chemically distinct water types (e.g., turbid coastal vs. clear deep water) if they appear visually similar in color space.
- What evidence would resolve it: An ablation study comparing cluster validity indices (e.g., Silhouette Score) and visual realism when physics-based metadata is included versus the current color-depth feature vector.

### Open Question 4
- Question: Can the integration of attention mechanisms or depth-aware modulation improve the precision of style transfer in regions with complex geometric structures?
- Basis in paper: [explicit] The paper notes, "The framework could be further enhanced by incorporating attention mechanisms or depth-aware style modulation to achieve more precise blending."
- Why unresolved: The current AdaIN fusion applies style statistics globally or channel-wise, which may fail to adapt to local variations in scene geometry or depth-dependent attenuation, potentially leading to unrealistic texturing in foreground objects.
- What evidence would resolve it: Qualitative and quantitative comparisons showing improved texture alignment and reduced artifacts in generated images when spatial attention layers are added to the generator.

## Limitations
- The model relies on synthetic training data (RSUIGM) which may not fully capture the complexity of real underwater environments, potentially limiting generalization to real-world scenarios
- The rigid k=4 clustering approach may fail to handle intermediate or novel water conditions that don't fit neatly into the predefined clusters
- The architecture assumes content and style are sufficiently disentangled in the VGG19 feature space, which may not hold for all underwater degradation types

## Confidence
- High confidence: Physics-informed clustering approach and its role in stabilizing training across different water types
- Medium confidence: AdaIN-based disentanglement mechanism, supported by general literature but lacking direct ablation studies
- Medium confidence: Composite loss formulation's effectiveness, though optimal hyperparameter values remain unspecified

## Next Checks
1. **Cluster validation**: Visualize cluster assignments using t-SNE/PCA to confirm distinct separation between the four style clusters and identify potential overlaps
2. **Ablation study**: Replace AdaIN with simple concatenation in the fusion step to quantify the specific contribution of the proposed disentanglement mechanism to overall performance
3. **Depth feature ablation**: Retrain the clustering model using only RGB histograms (without depth) to empirically validate the authors' claim that depth priors are essential for meaningful style separation