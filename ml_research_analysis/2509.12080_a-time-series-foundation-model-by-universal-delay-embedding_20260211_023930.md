---
ver: rpa2
title: A Time-Series Foundation Model by Universal Delay Embedding
arxiv_id: '2509.12080'
source_url: https://arxiv.org/abs/2509.12080
tags:
- embedding
- time
- dynamical
- data
- forecasting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Universal Delay Embedding (UDE), a foundation
  model for time-series forecasting that integrates delay embedding with Koopman operator
  theory. The method constructs delay-embedding matrices from Hankel matrices, partitions
  them into 2D subspace patches, and processes them as tokens via a self-attention
  encoder.
---

# A Time-Series Foundation Model by Universal Delay Embedding

## Quick Facts
- arXiv ID: 2509.12080
- Source URL: https://arxiv.org/abs/2509.12080
- Reference count: 40
- Primary result: UDE achieves over 20% average MSE reduction vs. state-of-the-art foundation models on seven benchmarks

## Executive Summary
UDE is a foundation model for time-series forecasting that integrates delay embedding with Koopman operator theory. By constructing Hankel matrices from univariate observations and partitioning them into 2D patches, UDE creates tokens that preserve topological and dynamical properties of underlying systems. These patches are processed by a self-attention encoder, enabling linear prediction in a lifted latent space via a finite-dimensional Koopman approximation. Evaluated across seven benchmarks and real-world climate data, UDE demonstrates strong zero-shot and few-shot generalization with over 20% average MSE reduction.

## Method Summary
UDE processes time-series by constructing Hankel matrices with embedding dimension m=500 and delay τ=1 for each univariate channel. These L×m matrices are partitioned into 2D patches (p=25, q=50) that span consecutive time steps and delayed coordinates, preserving local dynamical structure. Patches are flattened, projected to 512-dimensional tokens, optionally pooled with kernel=30, and passed through an encoder-only Transformer (6-12 layers, 8-16 heads). A shared MLP prediction head implements linear Koopman approximation in the latent space. The model is pretrained on UTSD-12B and fine-tuned with encoder frozen, training only the prediction head.

## Key Results
- Achieves over 20% average MSE reduction compared to state-of-the-art foundation models across seven benchmark datasets
- Demonstrates strong zero-shot and few-shot generalization with competitive performance to specialized models
- Learned representations reveal topologically coherent subspaces through persistence diagram clustering
- Outperforms baseline methods on real-world climate data from ERA5

## Why This Works (Mechanism)

### Mechanism 1
Delay-embedding matrices from single-variable observations preserve topological and dynamical properties of the full system via Takens' embedding theorem. Stacking time-delayed observations into Hankel matrices creates homeomorphic reconstructions of the original attractor when embedding dimension m > 2d. This provides a "dynamical representation" encoding trajectory geometry rather than isolated states.

### Mechanism 2
2D subspace patches capture locally coherent dynamical structures that 1D temporal slicing destroys. Partitioning the Hankel matrix into p×q patches creates tokens spanning both consecutive time steps and delayed coordinates, preserving short-term evolution patterns and local topology as localized projections of the phase-space trajectory.

### Mechanism 3
Lifting delay-embedded representations to high-dimensional latent space via Transformer enables approximately linear temporal evolution through Koopman approximation. The encoder maps delay vectors to lifted latent space z ∈ R^M, where a shared linear prediction head approximates the Koopman operator: z_{t+h} ≈ g(z_t). This allows h-step prediction without autoregressive rollout.

## Foundational Learning

- **Takens' Embedding Theorem**
  - Why needed: The entire UDE architecture rests on the guarantee that delay embeddings from a single observed variable can reconstruct the topology of a high-dimensional system's attractor.
  - Quick check: Given a 3-dimensional chaotic attractor observed through one scalar variable, what minimum delay embedding dimension m guarantees topological equivalence (under generic conditions)?

- **Koopman Operator Theory**
  - Why needed: UDE's prediction head implements a finite-dimensional Koopman approximation, enabling linear prediction in latent space for nonlinear systems.
  - Quick check: For a nonlinear dynamical system x_{t+1} = f(x_t), what space does the Koopman operator act on, and why is it linear even when f is nonlinear?

- **Persistent Homology and Wasserstein Distance**
  - Why needed: The paper uses 2-Wasserstein distances between persistence diagrams to demonstrate that learned tokens cluster by topological similarity, providing interpretability evidence.
  - Quick check: What topological features do H_0 and H_1 homology groups capture, and what does a high Wasserstein distance between two persistence diagrams indicate?

## Architecture Onboarding

- **Component map:**
  Input time series → Delay embedding: construct L×m Hankel matrix H_i per channel → Time-Delay Patching: partition H_i into U×V patches of size p×q → Flatten + Linear projection: tokens e_i^(j) ∈ R^d → Token pooling (optional): 1D avg pool, kernel=30 → Positional encoding: sinusoidal PE added → Transformer encoder: 6-12 layers, multi-head self-attention → Prediction head: shared MLP (linear Koopman approximation) → Output: h-step forecast per channel

- **Critical path:** The Hankel matrix construction and patching directly determine what dynamical information reaches the encoder. Incorrect m or patch sizes will propagate through the entire pipeline.

- **Design tradeoffs:**
  - m (embedding dimension): Larger m captures longer temporal context but increases memory for Hankel matrices. Assumption: m > 2d for attractor reconstruction.
  - Patch size (p×q): p=25 time steps, q=50 delay coordinates. Smaller patches preserve finer local geometry; larger patches improve computational efficiency but may blur dynamics.
  - Token pooling: kernel=30 reduces sequence length but may discard high-frequency dynamical nuances.
  - Channel independence: Each channel processed independently with shared weights—improves generalization but may miss cross-channel dependencies.

- **Failure signatures:**
  - If zero-shot performance is poor but fine-tuning recovers quickly: check if pretraining data distribution matches target domain dynamics.
  - If attention maps show no consistent high-attention tokens: patch size may be too small or embedding dimension m insufficient.
  - If predictions drift over long horizons: Koopman linearization may be inadequate; consider larger latent dimension or nonlinear prediction head.
  - If performance degrades on sparse/intermittent series: delay embedding with τ=1 may dilute signal; consider adaptive τ or event-aware patching.

- **First 3 experiments:**
  1. **Validate delay embedding quality:** For a known chaotic system (e.g., Lorenz), reconstruct the attractor from single-variable observations using UDE's Hankel construction. Visualize in 2D/3D and compare topology to ground truth. Check that increasing m improves reconstruction up to a saturation point.
  2. **Ablate patch size:** Train UDE-Small on a single benchmark with varying patch sizes (e.g., 10×20, 25×50, 50×100). Plot MSE vs. patch size to identify the regime where patches are large enough to capture dynamics but small enough to preserve local structure.
  3. **Probe Koopman linearization:** After pretraining, extract latent representations z_t for a test sequence. Compute the linear transition matrix K via least squares: K = Z_+ Z_-^†. Evaluate prediction error using K directly vs. the trained prediction head. If errors diverge significantly, the encoder may not be learning true Koopman-invariant subspaces.

## Open Questions the Paper Calls Out

- **Integrating UDE with vision-language models for unified spatiotemporal forecasting**
  - Question: Can integrating UDE's delay-aware tokens with vision-language models (VLMs) enable unified spatiotemporal forecasting, such as generating typhoon trajectories conditioned on satellite imagery?
  - Basis: The Discussion section proposes that "integrating UDE's delay-aware tokens with vision-language models (VLMs) might enable unified spatiotemporal forecasting."
  - Why unresolved: This is a proposed extension of the framework; the current UDE implementation processes time-series patches but does not interface with multi-modal data like images or text.

- **Dynamic hyperparameter tuning via topological persistence analysis**
  - Question: Can topological persistence analysis be used to dynamically tune hyperparameters (e.g., embedding dimension, patch size) to enhance reconstruction fidelity for nonstationary systems?
  - Basis: The authors suggest "topological persistence analysis could dynamically tune hyperparameters using Wasserstein distances between learned token manifolds."
  - Why unresolved: The current implementation relies on empirically tuned, fixed hyperparameters (m=500, τ=1), which may not be optimal for all dynamical regimes.

- **Adapting UDE for systems with Lévy-flight characteristics**
  - Question: How can UDE be adapted to handle systems violating spectral compactness requirements, such as those exhibiting Lévy-flight characteristics or rapidly shifting attractors?
  - Basis: The Discussion notes the current linearization capability "assumes smooth latent evolution" and "fails for systems violating spectral compactness requirements, such as those exhibiting Lévy-flight characteristics."
  - Why unresolved: The Koopman operator approach used by UDE relies on finite-dimensional approximations that may not converge or perform well on systems with discontinuous or stochastic jump dynamics.

## Limitations

- Theoretical assumptions about Takens' embedding and Koopman operators require empirical validation for specific systems studied
- Patch size and pooling parameters appear empirically motivated rather than theoretically justified
- Channel independence assumption may miss important cross-variable coupling patterns in multivariate systems
- Claims of universal applicability may be overstated given limited exploration of systems with different attractor dimensions and spectral properties

## Confidence

- **High confidence**: MSE reduction claims (>20% improvement) and zero-shot/few-shot transfer results are well-supported by extensive benchmarking across seven datasets and rigorous ablation studies
- **Medium confidence**: The theoretical foundations (Takens' theorem, Koopman operator approximation) are correctly stated, but the paper lacks empirical validation that these assumptions hold for the specific systems studied
- **Low confidence**: The universal applicability claim for arbitrary dynamical systems is overstated given the limited exploration of systems with different attractor dimensions, spectral properties, or noise characteristics

## Next Checks

1. **Attractor reconstruction validation**: For a known chaotic system (e.g., Lorenz or Rossler attractor), empirically verify that increasing m from 100 to 500 progressively improves topological reconstruction quality as measured by persistence diagram similarity to ground truth, saturating at m=500

2. **Koopman operator fidelity**: After pretraining, extract latent representations and compute the empirical Koopman matrix K = Z_+ Z_-^†. Compare prediction accuracy using K directly versus the learned MLP head to quantify how well the encoder approximates Koopman-invariant subspaces

3. **Patch size sensitivity analysis**: Systematically vary patch dimensions (p×q) across the range [10×20, 25×50, 50×100, 100×200] on a single benchmark (e.g., ETTh1) and plot the tradeoff between computational efficiency and forecasting accuracy to identify the optimal operating regime for different dynamical complexities