---
ver: rpa2
title: Test-Time Policy Adaptation for Enhanced Multi-Turn Interactions with LLMs
arxiv_id: '2509.23166'
source_url: https://arxiv.org/abs/2509.23166
tags:
- user
- policy
- zhang
- wang
- multi-turn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a test-time policy adaptation method for large
  language models in multi-turn interactions. It introduces ROSA, a one-step update
  algorithm that uses user feedback as a reward signal to steer model parameters toward
  an optimal policy, avoiding costly iterative optimization.
---

# Test-Time Policy Adaptation for Enhanced Multi-Turn Interactions with LLMs

## Quick Facts
- arXiv ID: 2509.23166
- Source URL: https://arxiv.org/abs/2509.23166
- Reference count: 40
- Key result: ROSA improves MATH accuracy by up to 27.20% with low computational overhead

## Executive Summary
This paper introduces ROSA (Reward-driven One-step Self-Adaptation), a test-time policy adaptation method that enables large language models to improve their performance during multi-turn interactions by leveraging user feedback. Unlike traditional methods that require iterative optimization, ROSA performs a single update step using feedback as a reward signal to steer model parameters toward optimal policy. The method theoretically guarantees monotonic improvement and convergence to user preferences while maintaining computational efficiency.

## Method Summary
ROSA employs a one-step update algorithm that adapts LLM parameters at test time using user feedback as a reward signal. The method uses conjugate gradient optimization to compute parameter updates without requiring expensive iterative training. By treating the feedback as a reward signal, ROSA formulates the adaptation as a policy optimization problem where the goal is to maximize cumulative reward across multi-turn interactions. The theoretical foundation ensures that each update step leads to monotonic improvement, with convergence guaranteed under certain conditions.

## Key Results
- Achieved up to 27.20% accuracy improvement on MATH reasoning tasks compared to baselines
- Demonstrated substantial gains across mathematical reasoning, general reasoning, code generation, and multilingual reasoning
- Maintained low computational overhead, with update step latency roughly equivalent to inference time for 8B models

## Why This Works (Mechanism)
ROSA works by directly optimizing the policy (LLM parameters) based on real-time user feedback, treating each interaction as a reinforcement learning step. The one-step update approach avoids the computational burden of iterative optimization while still moving the policy toward better performance. The conjugate gradient method efficiently computes the necessary parameter updates by approximating the Hessian matrix, making the adaptation scalable. By framing the problem as reward maximization, ROSA naturally aligns the model's behavior with user preferences.

## Foundational Learning
- **Policy optimization**: Understanding how to update decision-making policies based on reward signals is crucial for adapting LLMs to user preferences. Quick check: Can the student explain the difference between policy gradient and value-based methods?
- **Conjugate gradient optimization**: This numerical method efficiently solves large linear systems and approximates Hessian-vector products without explicitly computing the Hessian. Quick check: Can the student derive the conjugate gradient update rule?
- **Reinforcement learning fundamentals**: The framework treats user feedback as rewards in a sequential decision-making process. Quick check: Can the student map the multi-turn interaction to the standard RL framework (states, actions, rewards)?
- **Jacobian-vector products**: These are computed efficiently to estimate how parameter changes affect outputs. Quick check: Can the student explain how to compute Jacobian-vector products without forming the full Jacobian?
- **Multi-turn interaction modeling**: Understanding how sequential user interactions can be leveraged for model improvement. Quick check: Can the student describe how the reward signal aggregates across multiple turns?
- **Test-time adaptation**: The concept of modifying model behavior during inference rather than pre-training. Quick check: Can the student contrast test-time adaptation with fine-tuning?

## Architecture Onboarding

**Component Map**: User Input -> LLM Inference -> User Feedback -> Reward Computation -> Conjugate Gradient Update -> Parameter Update -> Next Interaction

**Critical Path**: The core adaptation pipeline runs: LLM generates response → user provides feedback → reward is computed → conjugate gradient computes parameter update → model parameters are adjusted. This path must complete within acceptable latency bounds for interactive applications.

**Design Tradeoffs**: The one-step update trades off perfect optimization for computational efficiency and theoretical guarantees. While iterative methods might find better local optima, they would be too slow for interactive use. The conjugate gradient approximation balances accuracy with computational tractability.

**Failure Signatures**: Poor convergence when feedback is inconsistent or noisy, performance degradation on knowledge-intensive tasks, computational bottlenecks for very large models (70B+), and potential catastrophic forgetting of pre-trained knowledge.

**First Experiments**:
1. Test ROSA on a simple arithmetic reasoning task with synthetic feedback to verify the monotonic improvement guarantee
2. Evaluate adaptation stability by varying feedback quality (noise injection) while measuring accuracy degradation
3. Benchmark the computational overhead of the conjugate gradient update step across different model scales (1B, 8B, 30B parameters)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can ROSA be adapted to improve effectiveness on tasks that rely heavily on leveraging the model's pre-trained knowledge, rather than just reasoning or code generation?
- Basis in paper: Section 6 "Conclusions and Limitations" states, "ROSA effectiveness is less effective on tasks that are heavily dependent on the model pre-trained knowledge."
- Why unresolved: The paper validates the method on reasoning benchmarks but acknowledges this specific domain remains a challenge without proposing a solution.
- Evidence: Experimental results on knowledge-intensive benchmarks (e.g., open-domain QA) showing performance gains comparable to those seen in mathematical reasoning.

### Open Question 2
- Question: How robust is ROSA's convergence guarantee when the reward signal is noisy, ambiguous, or adversarial?
- Basis in paper: The theoretical analysis (Theorem 2) relies on rewards accurately reflecting the user's optimal policy, but experiments use high-quality rule-based or strong-LLM rewards (Section 5.1).
- Why unresolved: Real-world user feedback is often imperfect, potentially violating the monotonic error reduction assumption central to the method's theoretical stability.
- Evidence: Experiments injecting synthetic noise into the reward signal to measure the degradation rate of accuracy and stability.

### Open Question 3
- Question: Does the computational overhead of the Conjugate Gradient update step remain efficient for models with significantly larger parameter scales (e.g., 70B+)?
- Basis in paper: Efficiency analysis (Table 10) shows the update step latency is roughly equivalent to inference time for 8B models, suggesting potential scalability bottlenecks.
- Why unresolved: The memory and compute requirements for Jacobian-vector products during the adaptation step scale with model size.
- Evidence: Benchmarking the latency and GPU memory consumption of the adaptation step on 70B+ parameter models.

## Limitations
- Narrow empirical scope focused on mathematical reasoning, general reasoning, code generation, and multilingual reasoning tasks
- Evaluation assumes availability of reliable user feedback signals, which may not be practical in all deployment scenarios
- Does not address potential safety concerns or bias amplification from iterative policy adaptation based on user feedback

## Confidence
- High Confidence: Theoretical guarantees of monotonic improvement and convergence are well-founded with clear mathematical framework
- Medium Confidence: Empirical performance improvements are reported but lack detailed ablation studies and generalizability assessment
- Low Confidence: Computational efficiency claims are not sufficiently quantified with concrete metrics across different model scales

## Next Checks
1. **Cross-domain generalization test**: Evaluate ROSA on at least two qualitatively different task domains (e.g., medical diagnosis reasoning and legal document analysis) to assess whether the method generalizes beyond mathematical and code generation tasks.

2. **Feedback quality robustness analysis**: Systematically vary the quality and quantity of user feedback signals to determine how ROSA's performance degrades under realistic conditions where feedback may be noisy, sparse, or contradictory.

3. **Long-term adaptation stability evaluation**: Conduct experiments tracking model performance over extended periods of adaptation to identify any catastrophic forgetting effects or performance plateaus that might emerge after multiple adaptation cycles.