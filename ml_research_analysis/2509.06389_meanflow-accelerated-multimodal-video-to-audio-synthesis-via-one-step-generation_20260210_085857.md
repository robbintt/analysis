---
ver: rpa2
title: MeanFlow-Accelerated Multimodal Video-to-Audio Synthesis via One-Step Generation
arxiv_id: '2509.06389'
source_url: https://arxiv.org/abs/2509.06389
tags:
- generation
- synthesis
- audio
- one-step
- mf-mjt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of improving inference efficiency
  in multimodal video-to-audio synthesis without compromising audio quality, semantic
  alignment, or temporal synchronization. Existing methods, particularly those based
  on flow matching, require iterative sampling steps, resulting in slow inference.
---

# MeanFlow-Accelerated Multimodal Video-to-Audio Synthesis via One-Step Generation

## Quick Facts
- **arXiv ID**: 2509.06389
- **Source URL**: https://arxiv.org/abs/2509.06389
- **Reference count**: 0
- **Primary result**: Introduces MeanFlow-accelerated multimodal joint training (MF-MJT) for video-to-audio synthesis, achieving >2× speedup (RTF=0.007) via native one-step generation while maintaining or improving audio quality and alignment.

## Executive Summary
This paper addresses the challenge of improving inference efficiency in multimodal video-to-audio synthesis without compromising audio quality, semantic alignment, or temporal synchronization. Existing methods, particularly those based on flow matching, require iterative sampling steps, resulting in slow inference. To address this bottleneck, the authors introduce MeanFlow-accelerated multimodal joint training (MF-MJT), which models average velocity instead of instantaneous velocity, enabling native one-step generation. Additionally, a scalar rescaling mechanism (CFG-scaled) is proposed to stabilize classifier-free guidance in one-step generation by balancing conditional and unconditional predictions. Experiments on VGGSound and AudioCaps test sets demonstrate that MF-MJT achieves over 2× speedup (RTF=0.007) compared to baseline methods while maintaining comparable or superior performance across metrics such as FAD, FD, IS, IB, DeSync, and CLAP. The model also performs well in multi-step settings, showing strong generalization for both video-to-audio and text-to-audio synthesis tasks.

## Method Summary
The authors propose MeanFlow-accelerated multimodal joint training (MF-MJT) for video-to-audio synthesis. The method uses a Multimodal Diffusion Transformer (MM-DiT) backbone that jointly processes video, text, and audio tokens in a shared latent space. Instead of modeling instantaneous velocity as in standard flow matching, MF-MJT characterizes flow fields using average velocity over time intervals, enabling native one-step generation without iterative sampling. A scalar rescaling mechanism (CFG-scaled) is employed to balance conditional and unconditional predictions in classifier-free guidance, mitigating distortions in one-step generation. The model is trained on VGGSound, Kling-Audio-Eval, AudioCaps, and WavCaps datasets, and achieves significant speedup while maintaining or improving audio quality and alignment metrics.

## Key Results
- Achieves >2× speedup (RTF=0.007) compared to baseline methods
- Maintains comparable or superior performance across FAD, FD, IS, IB, DeSync, and CLAP metrics
- Outperforms Frieren by 2.37× in inference speed while maintaining similar audio quality
- Demonstrates strong generalization for both video-to-audio and text-to-audio synthesis tasks

## Why This Works (Mechanism)

### Mechanism 1: Average Velocity Characterization
- **Claim:** If the flow field is modeled using average velocity rather than instantaneous velocity, the generative process can natively support one-step synthesis without distilled approximations.
- **Mechanism:** Standard Flow Matching (FM) requires iterative integration of instantaneous velocity $v(z_t, t)$ to solve an ODE. The MeanFlow formulation replaces this with an average velocity field $u(z_t, r, t)$ defined over a time interval $[r, t]$. During inference, this allows the transport from noise $z_1$ to data $z_0$ via a direct linear update: $z_0 = z_1 - u(z_1, 0, 1)$, eliminating the need for multi-step solvers.
- **Core assumption:** The network can successfully learn the MeanFlow identity $u = v - (t-r)\frac{d}{dt}u$ via the proposed stop-gradient loss objective, effectively capturing the integral of the velocity field.
- **Evidence anchors:**
  - [abstract]: "...characterizes flow fields using average velocity, enabling one-step generation... without iterative sampling process."
  - [section 2.2]: Eq. (3) and Eq. (6) define the average velocity and the one-step inference mapping.
  - [corpus]: "Compose Yourself: Average-Velocity Flow Matching..." validates the application of average-velocity flow matching for efficient one-step generation in audio tasks.

### Mechanism 2: CFG Scalar Rescaling
- **Claim:** Applying a scalar rescaling to the unconditional prediction in Classifier-Free Guidance (CFG) likely mitigates trajectory overshooting caused by the lack of iterative refinement in one-step generation.
- **Mechanism:** In one-step generation, high guidance strength $\omega$ in the standard CFG formulation ($\omega \cdot u_{cond} + (1-\omega) \cdot u_{uncond}$) can push the sample off the correct data manifold because there are no subsequent steps to correct the trajectory. The rescaling mechanism computes a scalar $s$ representing the projection of the conditional velocity onto the unconditional velocity. It then scales the unconditional term, ensuring the guidance direction remains semantically consistent with the conditional prediction.
- **Core assumption:** The distortion in one-step CFG stems primarily from misalignment between conditional and unconditional vector fields, which projection can correct.
- **Evidence anchors:**
  - [abstract]: "...scalar rescaling mechanism is employed to balance conditional and unconditional predictions... mitigating distortions in one-step generation."
  - [section 2.3]: Eq. (8) and (9) define the rescaled guidance and the scalar projection.
  - [corpus]: Corpus evidence for this specific rescaling mechanism in VTA is limited; however, "Modular MeanFlow" discusses stable one-step modeling, suggesting stability mechanisms are critical in this regime.

### Mechanism 3: Multimodal Joint Training via MM-DiT
- **Claim:** Jointly training on video, audio, and text within a unified transformer architecture likely improves semantic alignment and temporal synchronization compared to adapting pretrained text-to-audio models.
- **Mechanism:** The architecture uses Multimodal Diffusion Transformer (MM-DiT) blocks to process video ($F_v$), text ($F_t$), and audio ($x$) tokens in a shared latent space. A Synchformer encoder provides explicit temporal features ($F_{sync}$). This forces the model to learn cross-modal attention weights that align video frames with audio events directly, rather than relying on auxiliary adapters that might suffer from domain gaps.
- **Core assumption:** A unified semantic space allows for better feature disentanglement and alignment than separate encoder pathways trained asynchronously.
- **Evidence anchors:**
  - [abstract]: "...audio synthesis network is jointly trained with multimodal conditions..."
  - [section 2.1]: Describes the MM-DiT backbone and the fusion of CLIP and Synchformer features.
  - [corpus]: "FoleyGRAM" emphasizes "GRAM-Aligned Multimodal Encoders," supporting the hypothesis that explicit alignment strategies in VTA are critical for quality.

## Foundational Learning

- **Concept: Flow Matching (FM) & Rectified Flow**
  - **Why needed here:** The paper positions itself as an acceleration of standard FM. Understanding that FM typically requires solving an ODE by integrating instantaneous velocity is necessary to grasp why introducing "average velocity" enables speedups.
  - **Quick check question:** How does the definition of velocity in MeanFlow differ from the instantaneous velocity in standard Rectified Flow?

- **Concept: Classifier-Free Guidance (CFG)**
  - **Why needed here:** The paper identifies CFG as a source of distortion in one-step generation. You must understand how CFG combines conditional and unconditional scores to see why "overshooting" occurs without iterative correction.
  - **Quick check question:** In standard CFG, what happens to the output distribution if the guidance scale $\omega$ is set too high?

- **Concept: Multimodal Diffusion Transformers (MM-DiT)**
  - **Why needed here:** The backbone architecture. Understanding how separate modalities (video/text tokens vs audio tokens) are concatenated and processed via joint attention is key to implementing the model.
  - **Quick check question:** How are video and text features fused with audio latents inside the MM-DiT block?

## Architecture Onboarding

- **Component map:**
  1. **Encoders:** CLIP (Visual/Text) + Synchformer (Temporal Sync) + VAE (Audio)
  2. **Projection:** Linear layers projecting all features + positional embeddings (timestep + $(t, \Delta t)$) to a common dimension (e.g., 1024)
  3. **Core:** $N_1$ MM-DiT blocks (Joint Attention on all modalities) → $N_2$ DiT blocks (Self-Attention on audio only)
  4. **Output:** Final projection to "Average Velocity" field $u_\theta$

- **Critical path:** The flow of the time embedding $(t, \Delta t)$ into the MM-DiT blocks. Unlike standard diffusion which uses just $t$, MeanFlow requires $\Delta t = t-r$ to correctly condition the network for the interval average.

- **Design tradeoffs:**
  - **Ratio of $r \neq t$:** The paper notes a low ratio (10%) of $r \neq t$ pairs during training is better for alignment, but $r=t$ pairs provide direct supervision
  - **One-step vs. Multi-step:** The architecture supports both. One-step is fastest (RTF 0.007), but multi-step (e.g., 25 steps) can correct trajectory deviations if maximum quality is required

- **Failure signatures:**
  - **High CFG Artifacts:** If using standard CFG in one-step mode, expect "overshooting" or noisy audio. Fix: Use the CFG-scaled mechanism
  - **Temporal Drift:** If Synchformer features are not correctly pooled or fused, audio may lack frame-accurate synchronization

- **First 3 experiments:**
  1. **Reproduce Fig 4:** Train ablations with different ratios of $r \neq t$ (10% vs 90%) to verify the alignment degradation
  2. **CFG Validation:** Run inference on VGGSound comparing `CFG-stand` vs `CFG-scaled` at varying $\omega$ (1.0 to 4.5) to confirm the stability of the rescaling mechanism
  3. **Latency Benchmark:** Measure RTF on a single H800 (or equivalent) for 1-step vs 25-step generation to verify the claimed >2× speedup over baselines like Frieren

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the one-step generation framework be improved to robustly handle high classifier-free guidance (CFG) strengths without performance degradation?
- **Basis in paper:** [explicit] Section 4.2 observes that "in the one-step generation setting, the performance of MF-MJT tends to degrade as $\omega$ [CFG strength] increases, whereas the opposite trend is observed under multi-step generation."
- **Why unresolved:** The proposed CFG-scaled mechanism mitigates artifacts but does not fully invert the negative correlation between guidance strength and quality in the single-step regime, limiting control.
- **What evidence would resolve it:** A modified guidance mechanism or architecture that achieves monotonic improvement in Inception Score (IS) and ImageBind (IB) score as guidance strength increases in a one-step setting.

### Open Question 2
- **Question:** What is the theoretical justification for the optimal ratio of non-instantaneous time pairs ($r \neq t$), and why does a lower ratio yield better semantic alignment?
- **Basis in paper:** [explicit] Figure 4 and Section 4.2 show that a 10% ratio of $r \neq t$ pairs outperforms 90%, which the authors speculate is due to "ambiguous or conflicting contrastive signals."
- **Why unresolved:** The paper empirically determines that fewer non-instantaneous pairs are better but leaves the underlying dynamics of why average velocity supervision confuses the network unresolved.
- **What evidence would resolve it:** A theoretical analysis of gradient interference between $r=t$ and $r \neq t$ objectives, or experiments showing an adaptive ratio strategy that matches or exceeds the fixed 10% performance.

### Open Question 3
- **Question:** Does the omission of absolute positional encodings restrict the model's ability to generate long-form audio with precise sequential event ordering?
- **Basis in paper:** [inferred] Section 3.2 states that "absolute positional encodings are omitted, enabling the model to synthesis variable-length audio," prioritizing length flexibility.
- **Why unresolved:** While omitting absolute encodings allows for variable lengths (e.g., 8s vs 10s), it may impair the modeling of non-local temporal dependencies required for complex, long-duration soundscapes.
- **What evidence would resolve it:** Evaluations on generation tasks exceeding 10 seconds involving distinct, ordered sequential events, comparing the temporal consistency against models with absolute positional encodings.

## Limitations

- **CFG Stability:** The CFG-scaled mechanism mitigates artifacts but does not fully resolve performance degradation with high guidance strengths in one-step generation.
- **Dataset Generalization:** The model's effectiveness on domains beyond VGGSound and AudioCaps (e.g., music generation) remains untested.
- **Theoretical Justification:** The optimal 10% ratio of non-instantaneous time pairs lacks theoretical grounding, relying on empirical observation.

## Confidence

**High Confidence**: The MeanFlow reformulation enabling one-step generation is theoretically sound and experimentally validated. The CFG-scaled mechanism's role in stabilizing guidance is demonstrated with clear metrics (FAD, FD).

**Medium Confidence**: The joint training via MM-DiT improves alignment, but the paper does not provide ablation studies isolating the impact of multimodal fusion vs. modality-specific encoders. The claim that unified semantic space is superior to adapter-based approaches is plausible but not conclusively proven.

**Low Confidence**: The generalization of CFG-scaled to other generative tasks (e.g., text-to-image) is speculative. The paper does not explore whether the projection-based rescaling mechanism applies beyond audio synthesis.

## Next Checks

1. **CFG-Ablation Study**: Run inference on VGGSound comparing `CFG-stand` vs `CFG-scaled` at varying guidance scales (ω=1.0, 2.5, 4.5). Measure FAD and audio quality degradation to confirm that CFG-scaled consistently mitigates overshooting artifacts in one-step mode.

2. **Multi-Step vs. One-Step Quality Gap**: Generate audio using both 1-step (ω=1.5) and 25-step (ω=4.5) inference on the VGGSound test set. Compare FAD, FD, and subjective listening tests to quantify the trade-off between speed and quality.

3. **Cross-Dataset Generalization**: Evaluate MF-MJT on a held-out dataset not seen during training (e.g., AudioSet or YouTube8M) to test whether the CFG-scaled mechanism and MeanFlow reformulation generalize beyond VGGSound/AudioCaps.