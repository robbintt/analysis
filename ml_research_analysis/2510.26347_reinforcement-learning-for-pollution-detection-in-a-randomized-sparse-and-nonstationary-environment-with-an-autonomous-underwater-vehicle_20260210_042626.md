---
ver: rpa2
title: Reinforcement Learning for Pollution Detection in a Randomized, Sparse and
  Nonstationary Environment with an Autonomous Underwater Vehicle
arxiv_id: '2510.26347'
source_url: https://arxiv.org/abs/2510.26347
tags:
- learning
- agent
- pollution
- cloud
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of efficiently detecting underwater
  pollution clouds using autonomous underwater vehicles in a sparse, randomized, and
  nonstationary environment. The authors modify classical reinforcement learning approaches,
  including hierarchical reinforcement learning, multiple goal learning, trajectory
  reward learning, and a memory-based output filter, to adapt to these difficult conditions.
---

# Reinforcement Learning for Pollution Detection in a Randomized, Sparse and Nonstationary Environment with an Autonomous Underwater Vehicle

## Quick Facts
- arXiv ID: 2510.26347
- Source URL: https://arxiv.org/abs/2510.26347
- Authors: Sebastian Zieglmeier; Niklas Erdmann; Narada D. Warakagoda
- Reference count: 16
- Primary result: Modified Monte Carlo-based RL with hierarchical options and memory filter outperforms Q-learning and expert patterns in sparse underwater pollution search

## Executive Summary
This paper addresses the challenge of training autonomous underwater vehicles to detect pollution clouds in environments characterized by sparsity, randomization, and nonstationarity. Traditional Q-learning fails in this setting due to sparse rewards and changing target locations. The authors develop a modified Monte Carlo approach that learns from complete successful trajectories rather than individual steps, combined with hierarchical action grouping and a memory-based output filter to guide exploration. The resulting agent achieves median search times of 53.49 steps, outperforming both Q-learning baselines and two expert-designed search patterns (Snake and Spiral) across 1,000 evaluation episodes.

## Method Summary
The method employs hierarchical Monte Carlo reinforcement learning with several key modifications. Actions are grouped into "options" that execute multiple unidirectional steps (typically 3) as single decisions, reducing erratic movement and covering more area per decision. The Monte Carlo update uses γ=0 and updates all Q-values along a successful trajectory based on the average trajectory reward, normalized by steps taken. A Memory-as-Output-Filter (MOF) tracks visited states and subtracts scaled memory values from Q-values during action selection, discouraging revisits without expanding the state space. The agent is trained on 1-2 randomly placed pollution clouds per episode over 1,000 episodes, then evaluated on 1,000 episodes with win/tie/loss duels against Snake and Spiral baselines.

## Key Results
- Median step count of 53.49 vs 53.51 for Snake and 66.74 for Spiral across 1,000 evaluation episodes
- Agent won 643 out of 1,000 duels against Snake pattern
- Traditional Q-learning failed to learn, averaging ~400 steps (maximum limit)
- Optimal configuration found: option length J=3, MOF scaling factor 10, 1-2 training clouds

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical action grouping (options) reduces erratic movement and stabilizes exploration in sparse reward environments. The agent groups multiple unidirectional steps into single decisions called "options" (e.g., move right 3 times as one action). This effectively reduces the decision frequency and covers more area per decision, reducing "jittery" random movements and enabling more coherent trajectory patterns. Core assumption: The target (pollution cloud) is larger than a single grid cell, so not every cell needs to be explored.

### Mechanism 2
Monte Carlo-based trajectory reward learning (γ=0) with normalized reward enables learning efficient search patterns where step-wise Q-learning fails. Instead of discounting future rewards (γ>0), the method uses γ=0 and updates all Q-values along a successful trajectory based on the average trajectory reward. The reward is shaped by the number of pollution clouds found and normalized by the total steps taken. This propagates reward information across entire trajectories, not just backward from the goal. Core assumption: The agent can complete trajectories that find the target; successful trajectories contain useful path information even if individual steps don't have inherent value.

### Mechanism 3
Memory as an Output Filter (MOF) improves search efficiency by discouraging revisits without expanding the state space. A memory array tracks visited states. When selecting an action, the memory value (scaled by MOF factor) is subtracted from the Q-value of actions leading to already-visited states. This biases action selection toward unvisited areas without incorporating memory into the state representation (which would explode the Q-table size). Core assumption: Revisiting states is generally inefficient for search; the agent should prioritize exploring new areas.

## Foundational Learning

- **Markov Decision Processes and Q-Learning**: The entire framework builds on MDPs. Understanding Q-value updates, exploration-exploitation (ε-greedy), and why tabular Q-learning fails in nonstationary environments is essential. Quick check: Explain why Q-learning fails when the target location changes every episode.

- **Monte Carlo Methods in RL**: The paper's core innovation is a modified Monte Carlo approach (γ=0). Understanding how MC methods estimate value from complete episodes, not bootstrapped steps, is key. Quick check: How does a Monte Carlo update differ from a temporal difference (Q-learning) update?

- **Hierarchical Reinforcement Learning (Options)**: The method uses "options" (temporally extended actions). Understanding how options abstract over time steps is critical. Quick check: What is an "option" in HRL, and why might it help in a sparse reward grid?

## Architecture Onboarding

- **Component map**: Environment (20x20 grid, pollution cloud) -> Agent State (x,y position) -> Policy/Q-Table (state-option Q-values) -> Hierarchical Action Layer (options) -> MOF Module (memory array) -> Reward Shaping (normalized trajectory reward) -> Q-value Updates (MC with γ=0)

- **Critical path**: Environment resets with random cloud location → Agent observes state (position) → MOF modifies Q-values for next-step candidates → Agent selects option (ε-greedy) → Option executes multiple steps; memory updates → If cloud found, trajectory reward calculated and all Q-values on path updated (MC update, γ=0) → Repeat until success or max steps

- **Design tradeoffs**: Option length (J): Too short → erratic movement; too long → may skip targets. Paper found J=3 optimal for 20x20 grid, diameter 5. MOF Scaling (S_MOF): Too low → excessive revisits; too high → may trap agent. Paper found 10+ effective. Number of Training Clouds: Fewer clouds (1-2) led to better policies, contrary to initial "multiple goal learning" hypothesis.

- **Failure signatures**: Agent loops in small area → MOF value too low or option length too short. Agent reaches step limit without finding target → reward scaling off, exploration insufficient, or option length mismatched with cloud size. Performance variance high across runs → learning rate or episode count needs tuning.

- **First 3 experiments**: 1) Reproduce baseline failure: Run tabular Q-learning (γ>0) on the randomized environment. Verify the paper's claim that it fails to learn (avg steps ~400). 2) Ablate MOF: Run the optimized HMC agent with S_MOF=0 (disabled). Compare win rate and avg steps to the full model to quantify MOF's contribution. 3) Option length sweep: Test option lengths 1, 3, 5, 7 on the evaluation environment. Verify J=3 is optimal and observe how performance degrades at extremes.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the Memory-as-Output-Filter (MOF) be effectively converted into an intelligent filter using a neural network? The authors explicitly propose future work on "converting the MOF into an intelligent filter by using a neural network."

- **Open Question 2**: Do the findings generalize to deep reinforcement learning agents operating in continuous state spaces with realistic dynamics? The discussion notes the current discrete environment "does not fully reflect real-world conditions" and suggests applying insights to deep RL with continuous spaces and currents.

- **Open Question 3**: How does the loss of the Markov property caused by the MOF technique affect the theoretical stability of the learning process? The authors note that "strictly speaking, the Markov property is lost" but do not analyze the theoretical consequences of this violation.

- **Open Question 4**: Is the proposed methodology applicable to other sparse domains, such as aerial or terrestrial navigation? The authors suggest "this methodology may be applicable to other scenarios... in the air or on land."

## Limitations

- Environmental simplifications: The 20x20 grid and single, static pollution cloud per episode represent significant abstractions from real-world AUV operations with continuous space, multiple dynamic targets, and sensor noise.

- Limited generalization evidence: Performance gain of 13 steps (66.74→53.49) is modest given the 400-step maximum, representing only a 19% improvement over the weaker baseline without comparison to other search strategies.

- Hyperparameter sensitivity: The optimal configuration was determined empirically without systematic sensitivity analysis, potentially limiting robustness to different environmental configurations.

## Confidence

- **High confidence** in technical implementation: The hierarchical MC approach with trajectory reward shaping and MOF is clearly described with specific equations and parameter values.
- **Medium confidence** in generalization claims: Claims about applicability to "nonstationary environments" extend beyond the tested scenario without isolating individual challenge contributions.
- **Low confidence** in practical AUV applicability: The leap from discrete grid-world results to underwater vehicle deployment lacks validation of real-world factors.

## Next Checks

1. **Ablation study validation**: Implement and compare the full HMC agent against ablated versions (no MOF, no hierarchical options, traditional Q-learning with γ>0) to quantify each component's contribution to the 13-step improvement.

2. **Alternative baseline comparison**: Implement and test additional search strategies including lawn-mower patterns, spiral search from random starting angles, and random walks with memory to determine whether the RL approach offers meaningful advantages.

3. **Parameter sensitivity analysis**: Systematically vary option length (J=1,2,4,5), MOF scaling factor (S_MOF=1,5,15,20), and training cloud count (1,3,4,5) to identify robustness boundaries and determine whether the claimed optimal configuration is truly optimal.