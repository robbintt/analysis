---
ver: rpa2
title: 'CAIRNS: Balancing Readability and Scientific Accuracy in Climate Adaptation
  Question Answering'
arxiv_id: '2512.02251'
source_url: https://arxiv.org/abs/2512.02251
tags:
- climate
- cairns
- adaptation
- data
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CAIRNS is a question-answering framework designed to improve climate
  adaptation answers in agriculture by balancing readability and scientific accuracy.
  It employs a structured ScholarGuide prompt to guide LLM-generated answers with
  clear structure, citations, and local specificity, while using a hybrid retrieval
  system that integrates literature and climate data with location-aware weighting.
---

# CAIRNS: Balancing Readability and Scientific Accuracy in Climate Adaptation Question Answering

## Quick Facts
- **arXiv ID:** 2512.02251
- **Source URL:** https://arxiv.org/abs/2512.02251
- **Authors:** Liangji Kong; Aditya Joshi; Sarvnaz Karimi
- **Reference count:** 11
- **Primary result:** CAIRNS outperforms baseline and ablation models on most evaluation dimensions, achieving average scores up to 2.84/3.0.

## Executive Summary
CAIRNS is a question-answering framework designed to improve climate adaptation answers in agriculture by balancing readability and scientific accuracy. It employs a structured ScholarGuide prompt to guide LLM-generated answers with clear structure, citations, and local specificity, while using a hybrid retrieval system that integrates literature and climate data with location-aware weighting. The framework also features a consistency-weighted hybrid evaluator that combines human and automated scoring for reliable quality assessment. Experiments on a 50-question expert-curated dataset show that CAIRNS outperforms baseline and ablation models on most evaluation dimensions, with significant improvements in context, citations, and faithfulness.

## Method Summary
CAIRNS uses a multi-turn ReAct loop with a ScholarGuide Prompt to generate structured, citation-rich answers. It retrieves from a hybrid corpus (13,000 Elsevier climate papers + INDRA API climate data) using Location-Weighted Retrieval to boost location-relevant documents. The Consistency-weighted Hybrid Evaluator combines 6 commercial LLMs weighted by agreement with human experts for scoring. The framework was evaluated on a 50-question expert-curated dataset across 7 Likert dimensions and faithfulness metrics, achieving up to 2.84/3.0 average score without fine-tuning the underlying Gemini-2.0-Flash model.

## Key Results
- CAIRNS achieves up to 2.84/3.0 average score across 7 evaluation dimensions.
- Significant improvements in Context, Citations, and Faithfulness over baseline and ablation models.
- Ablation study confirms ScholarGuide Prompt is the key driver of performance gains.
- Hybrid evaluator achieves highest credibility in automated scoring without fine-tuning.

## Why This Works (Mechanism)
The framework works by structuring LLM output through the ScholarGuide Prompt, which enforces academic rigor, citations, and local specificity. Location-Weighted Retrieval boosts documents matching query locations, ensuring geographic relevance. The hybrid evaluator uses inter-model agreement (Cohen's κ) to weight LLM judges, providing more reliable scoring than single-model or human-only evaluation.

## Foundational Learning
- **ScholarGuide Prompt:** Academic writing persona that enforces structure, citations, and local advice. *Why needed:* Ensures scientific accuracy and readability. *Quick check:* Does output include structured sections and citations?
- **Location-Weighted Retrieval:** BM25 + embedding retrieval with β-weighting for location entities. *Why needed:* Boosts geographically relevant documents. *Quick check:* Are location entities correctly identified and weighted?
- **Hybrid Evaluator:** 6 LLMs weighted by agreement with human annotations. *Why needed:* Provides reliable automated scoring. *Quick check:* Does weighted agreement improve correlation with human scores?

## Architecture Onboarding
- **Component Map:** User Query -> Hybrid Retrieval -> ReAct Loop -> ScholarGuide Prompt -> LLM -> Hybrid Evaluator -> Scores
- **Critical Path:** Query → Location-Aware Retrieval → Structured Generation → Multi-Model Evaluation
- **Design Tradeoffs:** Uses commercial APIs (Elsevier, INDRA) for data quality vs. reproducibility; no fine-tuning vs. potential performance gains from adaptation
- **Failure Signatures:** Low Specificity scores indicate retrieval issues; Faithfulness/Citation mismatch suggests prompt enforcement problems
- **3 First Experiments:**
  1. Test ScholarGuide Prompt with location queries to verify citation and structure generation
  2. Verify Location-Weighted Retrieval boosts matching location documents correctly
  3. Evaluate hybrid evaluator agreement with human scores on sample outputs

## Open Questions the Paper Calls Out
None

## Limitations
- Framework relies on proprietary Elsevier content and commercial LLM APIs, limiting accessibility
- Small 50-question dataset may not generalize to broader agricultural contexts
- Hybrid evaluator's calibration based on single user study may not reflect diverse human judgment
- Lack of confidence intervals obscures result variability

## Confidence
- **High Confidence:** Core components (ScholarGuide Prompt, LWR, Hybrid Evaluator) well-specified and validated
- **Medium Confidence:** Retrieval effectiveness depends on access to exact Elsevier corpus; hybrid evaluator calibration limited to single study
- **Low Confidence:** "Scientific accuracy" claims contingent on unverifiable Elsevier corpus quality; no confidence intervals provided

## Next Checks
1. Reproduce with open climate datasets (Copernicus, NOAA) to test accessibility without proprietary content
2. Validate hybrid evaluator on new, diverse human-annotated benchmark beyond original user study
3. Compare against standard instruction-tuned LLM (GPT-4, Gemini-1.5) to isolate framework's unique contributions