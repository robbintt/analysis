---
ver: rpa2
title: Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable
  Rewards
arxiv_id: '2602.01601'
source_url: https://arxiv.org/abs/2602.01601
tags:
- gradient
- rloo
- training
- variance
- grpo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the inefficiency in group-based reinforcement
  learning methods, such as GRPO, which uniformly allocate rollouts across all training
  prompts regardless of their informativeness. To improve sampling efficiency, the
  authors introduce VIP, a Variance-Informed Predictive allocation strategy.
---

# Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards

## Quick Facts
- arXiv ID: 2602.01601
- Source URL: https://arxiv.org/abs/2602.01601
- Reference count: 40
- Primary result: VIP improves sampling efficiency in RLVR by predicting prompt success probabilities and allocating rollouts via convex optimization

## Executive Summary
This paper addresses the inefficiency in group-based reinforcement learning methods, such as GRPO, which uniformly allocate rollouts across all training prompts regardless of their informativeness. To improve sampling efficiency, the authors introduce VIP, a Variance-Informed Predictive allocation strategy. VIP predicts per-prompt success probabilities using a lightweight Gaussian process model and then solves a convex optimization problem to allocate rollouts in a way that minimizes expected gradient variance under a fixed compute budget. Empirical results show that VIP consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies across multiple benchmarks.

## Method Summary
VIP operates by first estimating the success probability for each prompt using a Gaussian process model that captures the relationship between prompt features and historical success rates. These probability estimates are then fed into a convex optimization framework that allocates rollouts to minimize expected gradient variance while respecting a fixed compute budget. The optimization balances exploration and exploitation by prioritizing prompts that are both likely to succeed and contribute significantly to gradient variance reduction. The entire process runs online during training, with the GP model being updated incrementally as new rollout data becomes available.

## Key Results
- VIP consistently improves sampling efficiency compared to uniform allocation
- On AIME2024 and AIME2025 benchmarks, VIP improved Pass@32 by up to 12.3 percentage points and Mean@32 by up to 6.3 percentage points
- Runtime overhead remains minimal at less than 1.2% of total RL training time

## Why This Works (Mechanism)
VIP works by recognizing that not all prompts contribute equally to learning progress in reinforcement learning with verifiable rewards. Prompts with higher success probabilities provide more informative gradients, while prompts with lower probabilities may contribute noise or redundant information. By predicting success probabilities and allocating resources accordingly, VIP ensures that computational budget is spent where it has the highest impact on reducing gradient variance. The convex optimization framework guarantees that the allocation is optimal given the estimated probabilities, while the lightweight GP model ensures that overhead remains minimal.

## Foundational Learning

**Gaussian Process Regression**
- Why needed: To predict per-prompt success probabilities from historical data
- Quick check: Verify GP hyperparameters and kernel choice on validation set

**Convex Optimization**
- Why needed: To solve the optimal rollout allocation problem efficiently
- Quick check: Confirm convexity of the objective function and feasibility of constraints

**Variance Reduction in RL**
- Why needed: To understand why uniform allocation is suboptimal
- Quick check: Compare gradient variance with and without VIP allocation

## Architecture Onboarding

**Component Map**
Prompt Features -> Gaussian Process Model -> Success Probability Estimates -> Convex Optimizer -> Rollout Allocation -> Training

**Critical Path**
The critical path runs from prompt feature extraction through the GP model to the convex optimizer and finally to the rollout allocation decision. Any delay in this pipeline directly impacts training efficiency.

**Design Tradeoffs**
The lightweight GP model trades off some prediction accuracy for computational efficiency, ensuring minimal overhead. The convex optimization formulation assumes linear relationships between allocations and variance reduction, which may oversimplify complex interactions in large-scale training.

**Failure Signatures**
- GP model failure: Poor success probability estimates leading to suboptimal allocations
- Optimization failure: Infeasible solutions or convergence issues
- Overhead issues: Allocation computation time exceeding training efficiency gains

**First Experiments**
1. Validate GP model accuracy on historical success rate prediction
2. Test convex optimization with synthetic success probability data
3. Measure runtime overhead on small-scale RLVR problems

## Open Questions the Paper Calls Out
None

## Limitations
- GP model reliability decreases in highly non-stationary RLVR environments with rapidly shifting reward distributions
- Computational overhead of solving convex optimization and maintaining GP model may become prohibitive at production scales with millions of prompts
- Performance improvements on mathematical reasoning benchmarks may not generalize to other RLVR domains with different reward structures

## Confidence
- High: Variance reduction framework and convex optimization formulation
- Medium: Empirical performance improvements on reported benchmarks
- Low: Claims about runtime efficiency and scalability to production environments

## Next Checks
1. Test VIP's performance on RLVR tasks with non-stationary reward distributions to evaluate robustness to distribution shifts
2. Benchmark VIP's computational overhead and effectiveness on prompt pools scaled to millions of entries
3. Validate the framework's effectiveness across diverse RLVR domains beyond mathematical reasoning benchmarks, including code generation and fact verification tasks