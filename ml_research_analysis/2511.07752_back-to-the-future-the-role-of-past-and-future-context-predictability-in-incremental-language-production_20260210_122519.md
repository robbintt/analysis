---
ver: rpa2
title: 'Back to the Future: The Role of Past and Future Context Predictability in
  Incremental Language Production'
arxiv_id: '2511.07752'
source_url: https://arxiv.org/abs/2511.07752
tags:
- predictability
- word
- backward
- context
- future
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Back to the Future: The Role of Past and Future Context Predictability in Incremental Language Production

## Quick Facts
- arXiv ID: 2511.07752
- Source URL: https://arxiv.org/abs/2511.07752
- Reference count: 38
- Key outcome: Conditional PMI integrates past and future context to predict speech production patterns better than backward predictability alone

## Executive Summary
This paper introduces conditional pointwise mutual information (PMI) as a principled measure of future context effects on language production, addressing a theoretical gap in how backward predictability has been measured. The authors show that future context predictability influences both word duration and speech error patterns in naturalistic speech, with different error types (semantic vs. phonological) showing distinct sensitivity to contextual predictability. Their findings support a model where speakers plan words incrementally while maintaining sensitivity to both past and future linguistic context, challenging strict linear production models.

## Method Summary
The study uses a single GPT-2 small (124M) model trained with custom infill data augmentation to estimate forward, backward, and bidirectional probabilities from the CANDOR corpus. Word-level tokenization (vocab size 14,116) with special tokens enables infill probability estimation. Study 1 analyzes word durations from Switchboard NXT using mixed-effects models, while Study 2 predicts substitution error identities from spontaneous speech using logistic regression with noisy semantic and phonetic features. The key innovation is conditional PMI, computed as log p(wt|C>t,C<t) - log p(wt|C<t), which measures the additional information future context provides beyond past context.

## Key Results
- Future context predictability significantly predicts word duration, with high predictability leading to shorter durations
- Backward predictability effects replicate and extend to semantic substitution errors with inhibitory effects
- Conditional PMI shows marginally better predictive power than relative backward predictability for both duration and error models
- Semantic errors show stronger future context effects than phonological errors, suggesting different planning pressures

## Why This Works (Mechanism)

### Mechanism 1: Conditional PMI as Context-Dependent Future Predictability
Conditional PMI provides a more principled measure by conditioning on past context rather than assuming independence between future and past. It computes the additional information future context provides about the current word beyond what past context already explains, capturing bidirectional associations missed by backward predictability.

### Mechanism 2: Differential Predictability Effects Across Error Categories
Different substitution error types reflect distinct weighting failures among form-, meaning-, and context-based information sources. Semantic errors show inhibitory effects of future context predictability (goal-directed processing), while phonological errors are driven solely by frequency/availability (goal-invariant processing).

### Mechanism 3: Probabilistic Reduction as Planning Coordination
Word duration reduction reflects coordination between planning and articulation, with future context predictability capturing both word predictability and future representation availability. High future predictability enables speakers to speed through current word articulation to initiate motor planning of upcoming words.

## Foundational Learning

- **Concept: Surprisal Theory (Forward Predictability)** - Understanding that -log p(word|preceding context) predicts processing difficulty is prerequisite for grasping why backward effects are theoretically surprising. Quick check: Why should backward predictability effects exist if speakers produce words left-to-right?

- **Concept: Pointwise Mutual Information (PMI)** - Understanding that standard PMI log(p(x,y)/p(x)p(y)) measures association strength between two variables is essential. Quick check: What does a positive PMI value indicate about the relationship between word wt and future context C>t?

- **Concept: Incremental vs. Hierarchical Sentence Planning** - The paper interprets past/future effects through planning theories. Quick check: Under hierarchical planning, why might a verb be planned before its subject noun phrase?

## Architecture Onboarding

- **Component map**: CANDOR corpus -> Infill-GPT2 training -> Probability extraction -> Mixed-effects models (duration) + Logistic regression (errors)
- **Critical path**: Train infill-augmented GPT-2 on CANDOR → Extract forward/backward/bidirectional probabilities → Compute relative backward predictability and conditional PMI → Fit mixed-effects models for duration; logistic regression for substitution identity
- **Design tradeoffs**: Single model vs. separate forward/backward models (single model ensures fair comparison but requires infill training augmentation complexity); Word-level tokenizer chosen over BPE for simpler probability estimation at cost of morphological granularity
- **Failure signatures**: High collinearity (r>0.7) between forward and backward predictability indicates need for decorrelated measures; Sign-flip in forward predictability coefficient when backward predictability added suggests multicollinearity artifacts
- **First 3 experiments**: 1) Replicate Study 1 with held-out corpus (Switchboard training) to verify duration effects without corpus-mismatch confounds; 2) Ablate context window sizes to test whether effects hold with truncated context (e.g., 50 tokens); 3) Extend to free word-order language (Murrinhpatha or Pitjantjatjara) to test whether conditional PMI advantages persist when planning order decouples from production order

## Open Questions the Paper Calls Out
None

## Limitations
- Model generalizability may be limited due to single-GPT2 model trained on CANDOR corpus potentially not generalizing well to different corpora
- Causality claims cannot be established as the study identifies predictive relationships but cannot prove causal mechanisms
- Error categorization reliability depends on human coding, which may introduce subjectivity in semantic vs. phonological error classification

## Confidence
- **High Confidence**: Existence of backward predictability effects on word duration and substitution errors
- **Medium Confidence**: Superiority of conditional PMI over relative backward predictability as a measure
- **Medium Confidence**: Differential effects across error categories (semantic vs. phonological)
- **Medium Confidence**: Interpretation of duration effects as reflecting planning coordination

## Next Checks
1. Train the infill-GPT2 model on Switchboard (rather than CANDOR) and verify that duration effects replicate without corpus-mismatch confounds
2. Extend the error analysis to include more error categories (e.g., mixed semantic-phonological errors) and larger samples to strengthen differential predictability claims
3. Design an experiment that directly tests whether speakers prolong words when future context is unpredictable due to planning difficulty versus other factors (e.g., frequency-based storage)