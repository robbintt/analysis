---
ver: rpa2
title: Integrating Large Language Models and Knowledge Graphs to Capture Political
  Viewpoints in News Media
arxiv_id: '2512.14887'
source_url: https://arxiv.org/abs/2512.14887
tags:
- viewpoints
- claim
- news
- claims
- viewpoint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an improved pipeline for classifying political
  claims in news articles according to their viewpoints. The key innovations are fine-tuning
  large language models (LLMs) on a high-quality dataset and enriching claim representations
  with structured actor information from Wikidata.
---

# Integrating Large Language Models and Knowledge Graphs to Capture Political Viewpoints in News Media

## Quick Facts
- arXiv ID: 2512.14887
- Source URL: https://arxiv.org/abs/2512.14887
- Reference count: 40
- Primary result: 91.7% F1 score on UK immigration news corpus using fine-tuned LLMs with actor metadata

## Executive Summary
This paper presents an improved pipeline for classifying political claims in news articles according to their viewpoints. The key innovations are fine-tuning large language models (LLMs) on a high-quality dataset and enriching claim representations with structured actor information from Wikidata. Evaluated on a UK immigration news corpus, the approach achieves 91.7% F1 score, outperforming previous methods. The best performance comes from combining fine-tuned LLMs with both surrounding text and actor metadata, particularly with LLMs capable of processing long inputs. While fine-tuning and KG integration each improve results individually, their combination yields the strongest performance.

## Method Summary
The approach uses binary multi-label classification where each claim can match multiple viewpoints. The pipeline extracts claims and actors from articles, links actors to Wikidata entities, retrieves actor attributes (political party, positions, occupations), and combines this with surrounding text to classify viewpoints. The method fine-tunes LLMs on the Immigration-3K dataset with three context configurations: text-only, KG-only, or both combined. The classification task predicts whether each claim-viewpoint pair is aligned (1) or not (0).

## Key Results
- Fine-tuned GPT-4o mini achieves 91.74% F1 score, improving on zero-shot baseline of 77.80%
- Combining surrounding text with KG-derived actor metadata yields best results for capable models
- Performance degrades for smaller models when combining text+KG contexts, suggesting context handling limits
- Under-represented viewpoints (6 and 8) show F1 ≈ 49%, indicating class imbalance issues

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning LLMs on high-quality, task-specific labeled data substantially improves viewpoint classification over zero-shot approaches.
- Mechanism: Domain-specific fine-tuning adjusts model weights to recognize patterns in how political claims map to viewpoints, reducing ambiguity in edge cases.
- Core assumption: The training distribution (UK immigration claims) sufficiently represents the classification patterns needed at inference time.
- Evidence anchors:
  - [abstract] "Evaluated on a UK immigration news corpus, the approach achieves 91.7% F1 score, outperforming previous methods."
  - [section 4.3] "The fine-tuned version of GPT-4o mini achieves the highest performance (91.74% F1)...This level of performance clearly improves on the results reported in [3], which correspond to the experiment where we used GPT-4o mini in a ZSL setting...yielded an F1 score of 77.80%."
  - [corpus] Neighbor papers (e.g., "Generalizability of Media Frames") address cross-domain transfer but this paper does not test it.
- Break condition: Poor label quality or insufficient training examples for under-represented viewpoints (e.g., viewpoints 6 and 8 in Table 5 show F1 ≈ 49%).

### Mechanism 2
- Claim: Enriching claims with structured actor metadata from Wikidata provides contextual signals that aid classification.
- Mechanism: Actor attributes (political party, positions held, occupation) help disambiguate claims by grounding the speaker's likely ideological orientation.
- Core assumption: Actors' institutional affiliations correlate with their expressed viewpoints; statements generally align with speaker's political identity.
- Evidence anchors:
  - [abstract] "enriching claim representations with structured actor information from Wikidata"
  - [section 3.3] "This is because statements are rarely made in isolation. They are often shaped by the speaker's history and identity."
  - [section 4.3] In zero-shot experiments, "solutions incorporating information from Wikidata...consistently outperformed those that relied solely on the surrounding text."
  - [corpus] No direct corpus evidence on actor-ideology correlation in news; assumption remains untested beyond this dataset.
- Break condition: Actor not in Wikidata, or actor makes statements contradicting their typical political alignment (noted by authors as potential noise source).

### Mechanism 3
- Claim: Combining surrounding text with KG-derived actor metadata yields best results—but only for models with sufficient context-handling capacity.
- Mechanism: Text provides linguistic/argumentative context; KG provides speaker context. Models that can process long inputs integrate both effectively.
- Core assumption: The model can attend to and weigh both information types appropriately without degradation.
- Evidence anchors:
  - [abstract] "The best performance comes from combining fine-tuned LLMs with both surrounding text and actor metadata, particularly with LLMs capable of processing long inputs."
  - [section 4.3] "While GPT-4o mini achieved the best overall results when using the combined context, the other six tested LLMs exhibited mixed performance...combining these two inputs led to a slight decrease in performance for these models."
  - [corpus] Weak; no corpus papers specifically address multi-source context integration limits in smaller LLMs.
- Break condition: Model lacks context window capacity or effective attention over combined inputs; performance degrades rather than improves.

## Foundational Learning

- Concept: **Binary multi-label classification vs. single-label classification**
  - Why needed here: Each claim can match multiple viewpoints simultaneously (e.g., economic cost + security threat). This is not a mutually exclusive classification problem.
  - Quick check question: Can a single claim about immigration be classified as both "Economic cost" and "National security threat" under this framework? (Yes.)

- Concept: **Knowledge Graph entity linking**
  - Why needed here: Actor names must be resolved to unique Wikidata entities to retrieve metadata. Ambiguous or missing links break the enrichment pipeline.
  - Quick check question: What happens if "John Smith" appears in a claim but no unique Wikidata match exists?

- Concept: **Context window vs. effective context utilization**
  - Why needed here: Many models advertise large context windows (128k tokens) but don't effectively use all input information. This explains why combined text+KG helped some models but hurt others.
  - Quick check question: A model has 128k context window but performance drops when you add 500 tokens of actor metadata. What might explain this?

## Architecture Onboarding

- Component map: Claim Extractor (GPT-4o) -> Entity Linker -> Context Assembler -> Fine-tuned Classifier (GPT-4o mini or open alternative)
- Critical path: Entity Linker -> Context Assembler -> Classifier. If actor linking fails or context assembly truncates key information, classification degrades.
- Design tradeoffs:
  - **GPT-4o mini vs. open models**: GPT-4o mini achieves 91.7% F1; best open model (Mistral Nemo 12B) achieves 85.2%. Trade cost/privacy for ~6.5 point gap.
  - **Text-only vs. KG-only vs. Text+KG**: Text+KG best for capable models; KG-only better than Text-only for some smaller models.
  - **Data quality vs. quantity**: Authors removed 310 ambiguous claim-viewpoint pairs to boost Cohen's kappa from 0.42 to 0.66. Cleaner data improved reliability.
- Failure signatures:
  - F1 ≈ 49-50% on specific viewpoints (Table 5, viewpoints 6 & 8) indicates class imbalance/under-representation, not model failure.
  - Performance drop when combining Text+KG for smaller models suggests context overload, not data quality issues.
  - Low recall with high precision (e.g., Llama 3.1 8B: 73.65% recall, 87.87% precision) suggests conservative predictions—possibly from insufficient fine-tuning or context handling.
- First 3 experiments:
  1. **Reproduce baseline**: Run GPT-4o mini zero-shot with Text-only context on Immigration-3K test split. Target: ~77.8% F1. Verify your evaluation pipeline matches authors'.
  2. **Ablate KG contribution**: Fine-tune GPT-4o mini with Text-only vs. KG-only vs. Text+KG. Quantify each component's contribution. Expect Text+KG > either alone.
  3. **Test context limits**: For smaller models (e.g., Gemma 9B, Llama 8B), measure performance as you progressively truncate or simplify actor descriptions. Identify the input length where performance plateaus or degrades.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a classification system trained on a specific topic (e.g., immigration) generalize effectively to unrelated political domains such as climate change or health policy?
- Basis in paper: [explicit] The authors state they "intend to conduct extensive experiments across multiple topics to evaluate how effectively a system trained on certain topics... can be reused for others."
- Why unresolved: The current study validates the method exclusively on the Immigration-3K dataset, leaving cross-domain robustness untested.
- What evidence would resolve it: Performance metrics (F1 scores) from a zero-shot or transfer-learning evaluation on a newly released multi-topic benchmark.

### Open Question 2
- Question: Does the generation of synthetic data improve the classification accuracy for rare viewpoints that suffer from data scarcity?
- Basis in paper: [explicit] The authors identify poor performance on under-represented viewpoints and propose to "experiment with the generation of synthetic data" to address this imbalance.
- Why unresolved: Viewpoints like "Multiculturalism as a positive force" exhibited F1 scores near 50% (random chance) due to lack of training examples, but synthetic augmentation has not yet been tested.
- What evidence would resolve it: A comparison of classification performance on minority classes before and after fine-tuning with LLM-generated synthetic claims.

### Open Question 3
- Question: Why does combining textual context with Knowledge Graph (KG) data degrade performance in smaller models, and how can this integration be optimized?
- Basis in paper: [inferred] The results show that while GPT-4o mini benefits from the Text+KG combination, smaller models (e.g., Gemma2, Llama 3.1) perform worse with combined inputs than with text alone.
- Why unresolved: The authors hypothesize that smaller LLMs struggle to leverage long inputs effectively, but the specific mechanism (e.g., attention distraction vs. context length limits) is not confirmed.
- What evidence would resolve it: Ablation studies varying context length and information density on smaller models to identify the specific cause of performance degradation.

## Limitations

- Performance degrades for smaller models when combining text and KG contexts, suggesting the pipeline isn't universally applicable
- Cross-domain generalizability remains untested despite claims of potential broader applicability
- Entity linking failures (actors not in Wikidata or ambiguous names) could break the enrichment pipeline

## Confidence

**High Confidence (Mechanistic):** The observation that fine-tuning improves performance over zero-shot (77.8% → 91.7% F1) is well-supported by the ablation results in Table 4.

**Medium Confidence (Empirical):** The claim that KG-derived actor metadata improves classification is supported by zero-shot experiments but not systematically tested across all models in the fine-tuned setting.

**Low Confidence (Generalizability):** The assertion that combining text and KG context yields the best results requires qualification—this only holds for models with sufficient context-processing capacity.

## Next Checks

1. **Cross-Domain Transfer Test:** Fine-tune the same model (GPT-4o mini) on Immigration-3K, then evaluate on a held-out dataset from a different policy domain (e.g., climate change or healthcare). Measure performance drop to quantify domain dependence.

2. **Entity Linking Robustness Analysis:** Run the pipeline on the full dataset while systematically varying the success rate of Wikidata lookups (100%, 75%, 50%, 0%). Measure how classification F1 scores degrade with missing actor metadata to understand the KG component's contribution.

3. **Context Capacity Stress Test:** For each model tested, measure performance as a function of actor description length (truncate Wikidata triples progressively). Identify the breakpoint where adding more context stops helping or starts hurting, and compare this to each model's effective context utilization capacity.