---
ver: rpa2
title: 'No Audiogram: Leveraging Existing Scores for Personalized Speech Intelligibility
  Prediction'
arxiv_id: '2506.02039'
source_url: https://arxiv.org/abs/2506.02039
tags:
- intelligibility
- audio
- speech
- score
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses personalized speech intelligibility prediction,
  challenging the traditional reliance on audiograms. Instead of using audiograms,
  the authors propose SSIPNet, a deep learning model that leverages an individual's
  existing intelligibility scores from multiple support (audio, score) pairs to predict
  their performance on new audio.
---

# No Audiogram: Leveraging Existing Scores for Personalized Speech Intelligibility Prediction

## Quick Facts
- arXiv ID: 2506.02039
- Source URL: https://arxiv.org/abs/2506.02039
- Authors: Haoshuai Zhou; Changgeng Mo; Boxuan Cao; Linkai Li; Shan Xiang Wang
- Reference count: 0
- Primary result: SSIPNet achieves NCC of 0.811 and RMSE of 23.430 on CPC dataset without audiograms, outperforming audiogram-based baselines.

## Executive Summary
This paper introduces SSIPNet, a deep learning approach for personalized speech intelligibility prediction that operates without requiring audiograms. The model leverages an individual's existing intelligibility scores from multiple support (audio, score) pairs to predict their performance on new audio. SSIPNet uses speech foundation models to build high-dimensional representations of speech recognition ability and aggregates these to make predictions. On the Clarity Prediction Challenge dataset, SSIPNet demonstrates superior performance compared to traditional audiogram-based methods, achieving an average NCC of 0.811 versus 0.794 for the baseline, and an average RMSE of 23.430 versus 26.160. The model also shows strong performance even with as few as one support sample, suggesting its potential for effective personalized intelligibility prediction without requiring audiogram data.

## Method Summary
SSIPNet addresses personalized speech intelligibility prediction by using existing intelligibility scores from support (audio, score) pairs to predict scores for new query audio. The model employs Whisper large v3 as a frozen backbone to extract speech representations, processes these through temporal and layer transformers, and uses a score projection module to aggregate support samples with query embeddings. The approach is evaluated on the Clarity Prediction Challenge dataset with 27 hearing-impaired listeners, using three-fold splits with non-overlapping listeners across train (23 listeners), validation (3 listeners), and test (5 listeners) sets. Training uses Huber loss with Adam optimizer, cosine annealing, and listener-structured batches to ensure proper personalization.

## Key Results
- SSIPNet achieves NCC of 0.811 and RMSE of 23.430 on the Clarity Prediction Challenge test set
- Outperforms audiogram-based baseline methods (NCC 0.794, RMSE 26.160) across all support sample counts
- Maintains strong performance with minimal support samples, achieving competitive results with as few as one support sample
- Demonstrates effectiveness of foundation model-based representations for personalized intelligibility prediction without audiograms

## Why This Works (Mechanism)
The model leverages foundation model representations of speech that capture rich acoustic and linguistic features, then uses few-shot learning to adapt these representations to individual hearing profiles. By aggregating multiple support samples per listener, the model builds a personalized profile that captures how each individual's hearing impairment affects intelligibility across different audio conditions. The temporal and layer transformers allow the model to focus on relevant temporal patterns and hierarchical features that correlate with intelligibility loss.

## Foundational Learning
- **Speech Foundation Models**: Large-scale pretrained models like Whisper that provide rich speech representations; needed to capture complex acoustic-phonetic relationships in speech; quick check: verify the model can extract meaningful embeddings from diverse audio samples.
- **Few-Shot Learning**: Learning from limited labeled examples per class; needed to personalize predictions without requiring extensive individual data; quick check: test model performance with varying numbers of support samples per listener.
- **Huber Loss**: Loss function less sensitive to outliers than MSE; needed for robust regression on intelligibility scores; quick check: compare training stability with MSE vs Huber loss.
- **Temporal Transformers**: Attention mechanisms applied across time; needed to capture temporal dependencies in speech that affect intelligibility; quick check: ablate temporal transformer to assess impact on performance.
- **Listener-Structured Batches**: Training batches organized by listener identity; needed to ensure proper personalization during training; quick check: verify batch construction maintains listener grouping.

## Architecture Onboarding

### Component Map
Audio → Whisper Encoder → Temporal Transformer → Layer Transformer → Layer Pooling → FEM Embeddings → SPM → Score Prediction

### Critical Path
The most critical path is Audio → Whisper Encoder → SPM → Score Prediction, as the foundation model embeddings directly feed into the score projection module which makes the final prediction.

### Design Tradeoffs
- **Frozen vs Fine-tuned Foundation Model**: Using frozen Whisper prioritizes generalization over task-specific adaptation; this reduces overfitting risk but may miss task-specific features.
- **Number of Support Samples**: More support samples improve personalization but reduce practical applicability; the model must balance between few-shot and many-shot scenarios.
- **Temporal vs Layer Transformers**: Both transformer stages add capacity but increase complexity; the design must balance representational power with training stability.

### Failure Signatures
- High RMSE with few support samples indicates poor personalization capability
- Low NCC with many support samples suggests failure to capture listener-specific patterns
- Correlation between audio levels and predictions indicates information leakage
- Poor validation performance suggests overfitting to training listeners

### First Experiments
1. Verify audio normalization to 65 dB SPL correctly calibrates all samples across different hearing loss profiles
2. Test FEM embedding quality by visualizing similarity between support and query embeddings for same vs different listeners
3. Validate batch construction ensures listener grouping and proper support/query sampling

## Open Questions the Paper Calls Out
- Can model architectures and loss functions specifically designed for few-shot regression significantly outperform the current SSIPNet implementation?
- How robust is the support-sample approach across datasets with greater variability in hearing abilities compared to the Clarity Prediction Challenge dataset?
- Which specific types of audio samples are most effective for building an accurate listener profile with minimal support samples?
- Does the assumption of conductive hearing loss during score calibration limit the model's accuracy for listeners with sensorineural hearing loss?

## Limitations
- Relies on calibrated intelligibility scores rather than direct audiometric measurements
- Performance depends on availability of sufficient support samples per listener
- Current evaluation limited to the Clarity Prediction Challenge dataset with relatively concentrated hearing loss distribution
- Score calibration assumes conductive hearing loss, which may not represent all hearing impairments

## Confidence
- Method Reproducibility: Medium - Detailed training procedure but some architectural details unspecified
- Result Validity: High - Clear improvement over baseline on standardized dataset
- Generalizability: Low - Only validated on single dataset with limited hearing loss diversity

## Next Checks
1. Verify audio normalization to 65 dB SPL correctly calibrates all samples across different hearing loss profiles
2. Test FEM embedding quality by visualizing similarity between support and query embeddings for same vs different listeners
3. Validate batch construction ensures listener grouping and proper support/query sampling