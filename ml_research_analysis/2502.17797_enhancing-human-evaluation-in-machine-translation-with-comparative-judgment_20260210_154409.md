---
ver: rpa2
title: Enhancing Human Evaluation in Machine Translation with Comparative Judgment
arxiv_id: '2502.17797'
source_url: https://arxiv.org/abs/2502.17797
tags:
- error
- annotation
- zhen
- translation
- ende
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper compares three human evaluation settings for machine\
  \ translation\u2014MQM, side-by-side (SxS) MQM, and SxS relative ranking (RR)\u2014\
  using Chinese-to-English and English-to-German datasets. The study finds that SxS\
  \ MQM achieves the highest inter-annotator agreement and significantly improves\
  \ inter-translation error marking consistency (38.5% for explicitly compared systems,\
  \ 19.5% for others) compared to MQM."
---

# Enhancing Human Evaluation in Machine Translation with Comparative Judgment

## Quick Facts
- **arXiv ID:** 2502.17797
- **Source URL:** https://arxiv.org/abs/2502.17797
- **Reference count:** 33
- **One-line primary result:** Side-by-side MQM achieves highest inter-annotator agreement and improves error consistency by 38.5% for explicitly compared systems.

## Executive Summary
This paper compares three human evaluation settings for machine translation—MQM, side-by-side (SxS) MQM, and SxS relative ranking (RR)—using Chinese-to-English and English-to-German datasets. The study finds that SxS MQM achieves the highest inter-annotator agreement and significantly improves inter-translation error marking consistency compared to MQM. All settings produce stable system rankings, with SxS RR offering a cost-effective alternative when detailed error annotation is not required. SxS MQM also better highlights subtle accuracy errors, particularly in Chinese-to-English, without altering absolute system evaluations.

## Method Summary
The study compares three human evaluation protocols on WMT2023 news domain data: point-wise MQM, side-by-side MQM, and side-by-side relative ranking. Five system pairs per language pair were selected based on cross-BLEU and XCOMET quality similarity (p > 0.05). Professional translators (8 for ZhEn, 10 for EnDe) annotated the same segments under all three settings in a within-subject design. MQM scores were z-normalized, and meta-evaluation metrics included Krippendorff's alpha for IAA, pairwise ranking agreement (PRA), and permutation tests for system rankings. Inter-translation consistency was measured using difflib token alignment.

## Key Results
- SxS MQM achieves higher inter-annotator agreement (α = 0.21-0.36) than point-wise MQM
- SxS MQM improves inter-translation error marking consistency by 38.5% for explicitly compared systems and 19.5% for others
- All settings produce stable system rankings, with SxS RR offering cost-effective system ranking (estimated 1/3 cost of MQM)

## Why This Works (Mechanism)

### Mechanism 1: Comparative Judgment Reduces Cognitive Noise
- **Claim:** Side-by-side (SxS) evaluation yields higher inter-annotator agreement than point-wise evaluation.
- **Mechanism:** Annotators can directly compare translations, reducing inconsistency in marking shared errors (e.g., flagging an error for one system but overlooking it for another).
- **Core assumption:** Humans are more reliable at comparative judgments than absolute ones.
- **Evidence anchors:**
  - [abstract] "SxS settings achieve higher inter-annotator agreement than MQM."
  - [Page 5] "The IAA does not exhibit a clear correlation with textual similarity between systems... The results suggest that comparative judgment improves alignment among human annotators in evaluations."
- **Break condition:** If the task does not involve pairwise comparison (e.g., single-system grading), this mechanism fails to provide a benefit.

### Mechanism 2: Cross-Translation Anchoring Improves Error Consistency
- **Claim:** SxS MQM enhances the consistency of error marking across translations from different systems.
- **Mechanism:** Viewing two translations side-by-side "anchors" the annotator's internal error detection criteria, causing them to mark the same error span, category, and severity more consistently.
- **Core assumption:** Annotators carry over their error detection standards from one translation to the next within the same task.
- **Evidence anchors:**
  - [abstract] "SxS MQM enhances inter-translation error marking consistency compared to MQM by... 38.5% for explicitly compared MT systems and 19.5% for others."
  - [Page 6] "The improvement in non-compared systems in SxS MQM may result from exposure to side-by-side comparisons, which potentially refine annotators' internal error detection standards."
- **Break condition:** If annotators are not shown multiple translations of the same source, no anchoring can occur.

### Mechanism 3: Paired Evaluation Trade-offs for System Ranking
- **Claim:** SxS Relative Ranking (RR) offers a cost-effective, stable system ranking but may miss subtle quality differences.
- **Mechanism:** RR simplifies the task to a preference judgment, reducing annotation cost/complexity. This simplification leads to higher "tie" rates, which can reduce statistical power for distinguishing systems of similar quality.
- **Core assumption:** Annotators can reliably identify a "better" translation without explaining why.
- **Evidence anchors:**
  - [abstract] "SxS RR offering a cost-effective alternative when detailed error annotation is not required."
  - [Page 6-7] "SxS RR on ONLINE-A and ONLINE-Y show a discrepancy... the high tie rate in SxS RR plays a key role."
- **Break condition:** When fine-grained error diagnosis (e.g., accuracy vs. fluency) is required for debugging, RR provides insufficient signal.

## Foundational Learning

- **Concept: Multidimensional Quality Metrics (MQM)**
  - **Why needed here:** This is the baseline evaluation framework. The paper modifies it (SxS MQM) and compares against it. Understanding MQM's error spans, categories, and scoring is essential to interpret the paper's results.
  - **Quick check question:** In standard MQM, how does an annotator mark an error? (Answer: They highlight the error span and assign it a category and a severity level).

- **Concept: Comparative Judgment (Psychometrics)**
  - **Why needed here:** The paper's core intervention is applying this psychometric method to MT evaluation. You must understand that it posits people are better at comparing two items than rating one in isolation.
  - **Quick check question:** What is the core assumption of comparative judgment? (Answer: That human judgments are more reliable and consistent when comparing two items directly than when evaluating each one separately).

- **Concept: Inter-Annotator Agreement (IAA) Metrics (e.g., Krippendorff's alpha)**
  - **Why needed here:** The paper uses IAA as a primary metric to argue for the superiority of SxS settings. Without understanding what alpha measures (agreement beyond chance), you cannot assess the evidence for the claims.
  - **Quick check question:** If three annotators label a segment, and we calculate their agreement, what would a Krippendorff's alpha of 0.25 versus 0.35 suggest? (Answer: 0.35 indicates higher agreement among annotators than 0.25, supporting the claim that the evaluation setting is more reliable).

## Architecture Onboarding

- **Component map:** WMT2023 data -> System pairs selection -> Annotator pool -> Within-subject assignment -> Three protocols (MQM, SxS MQM, SxS RR) -> Annotated outputs -> Score calculation -> Meta-evaluation metrics -> Analysis
- **Critical path:** Select system pairs based on similarity -> Run human annotation under all three settings -> Compute segment/system scores -> Calculate IAA, consistency, PRA -> Analyze error distributions and rankings
- **Design tradeoffs:**
  - SxS MQM: Higher agreement and consistency vs. higher cost/complexity (full error annotation)
  - SxS RR: Lower cost (est. 1/3 of MQM) vs. coarser signal (more ties, less diagnostic power)
  - MQM (Baseline): Established method vs. lower agreement/consistency
- **Failure signatures:**
  - Low IAA in any setting: Indicates annotator confusion, poor guidelines, or inherently subjective content
  - High tie rate in RR: Signals inability to distinguish subtle quality differences
  - Inconsistent system rankings across settings: Suggests fundamental disagreement in what "quality" means
- **First 3 experiments:**
  1. Replicate on new language pair: Run the same protocol on a different WMT language pair to test generalizability
  2. Ablate annotator expertise: Compare SxS MQM vs. MQM using crowd-sourced annotators to see if the comparative setting helps non-experts more
  3. Stress-test RR: Apply SxS RR to systems with known subtle quality differences to quantify its sensitivity limit

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do the improvements in inter-annotator agreement and error consistency observed in SxS MQM generalize to a broader range of language pairs beyond Chinese-to-English and English-to-German?
- **Basis in paper:** [explicit] The authors state in the Limitations section that they were "not able to test... on a larger set of language pairs" and explicitly suggest future work should "expand this work to include more language pairs."
- **Why unresolved:** The current study is restricted to two specific language pairs, which may not represent the challenges found in low-resource or morphologically rich language pairs.
- **What evidence would resolve it:** Replicating the study on additional language pairs (e.g., English-to-Japanese or Russian-to-Spanish) to verify if SxS MQM maintains higher Krippendorff's alpha and consistency scores compared to MQM.

### Open Question 2
- **Question:** How effective are side-by-side evaluation protocols in NLP domains outside of machine translation, such as summarization or open-ended generation?
- **Basis in paper:** [explicit] The authors note that "Future work can also test the annotation setups in other domains beyond MT to see how annotation settings influence evaluations in diverse NLP tasks."
- **Why unresolved:** The benefits of comparative judgment were only validated for translation error spotting; different tasks may have different cognitive loads or evaluation criteria that behave differently under pairwise conditions.
- **What evidence would resolve it:** Applying the SxS MQM and SxS RR protocols to tasks like long-form question answering or summarization and comparing the resulting agreement and consistency against point-wise baselines.

### Open Question 3
- **Question:** Does the comparative judgment setting yield greater quality improvements for crowd-sourced annotators compared to the professional experts used in this study?
- **Basis in paper:** [explicit] The authors suggest investigating "the impact of different annotation settings on annotator backgrounds," hypothesizing that "SxS MQM and SxS RR... might do so [improve agreement] for crowd-sourced workers."
- **Why unresolved:** Expert annotators already possess high proficiency and internal consistency; it is unclear if the "cognitive anchoring" of SxS provides a larger marginal benefit for non-experts.
- **What evidence would resolve it:** A comparative study using crowd-sourced workers on the same ZhEn and EnDe datasets to measure if the delta in agreement between MQM and SxS MQM is larger than that observed with experts.

## Limitations
- Findings based on single dataset (WMT2023 news domain) and two language pairs
- Cost-effectiveness of SxS RR based on previous study estimates rather than direct measurement
- Does not address potential domain-specific effects or generalizability to different language families
- Specific training protocols and interface design not detailed, which could influence observed effects

## Confidence

- **High confidence:** The finding that SxS MQM achieves higher inter-annotator agreement than point-wise MQM is well-supported by the statistical evidence (α values ranging from 0.21-0.36 across settings).
- **Medium confidence:** The claim about SxS MQM improving inter-translation error marking consistency (38.5% for explicitly compared systems) is supported by the data but relies on assumptions about the difflib token alignment method for consistency measurement.
- **Medium confidence:** The characterization of SxS RR as cost-effective is based on comparison to published estimates rather than direct cost measurement in this study.

## Next Checks
1. **Generalizability test:** Apply the same three evaluation protocols to a different MT domain (e.g., conversational dialogue or technical documentation) to verify whether the IAA improvements for SxS settings hold across content types.
2. **Sensitivity analysis:** Design an experiment where SxS RR is applied to systems with known subtle quality differences (e.g., models differing only in fine-tuning) to quantify its detection threshold and determine when it fails to distinguish close systems.
3. **Annotation interface audit:** Replicate the study using different annotation interfaces (e.g., basic side-by-side view vs. integrated error marking) to isolate whether the improvements stem from comparative judgment itself or from interface design features.