---
ver: rpa2
title: 'InstructNet: A Novel Approach for Multi-Label Instruction Classification through
  Advanced Deep Learning'
arxiv_id: '2512.18301'
source_url: https://arxiv.org/abs/2512.18301
tags:
- text
- bert
- accuracy
- xlnet
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose InstructNet, a transformer-based architecture
  leveraging XLNet for multi-label instruction classification using wikiHow articles.
  The dataset consists of 11,121 observations from wikiHow with multiple categories
  per record.
---

# InstructNet: A Novel Approach for Multi-Label Instruction Classification through Advanced Deep Learning

## Quick Facts
- **arXiv ID:** 2512.18301
- **Source URL:** https://arxiv.org/abs/2512.18301
- **Reference count:** 40
- **Primary result:** XLNet-based InstructNet achieves 97.30% accuracy and 93% macro F1-score on multi-label instruction classification

## Executive Summary
InstructNet presents a transformer-based architecture for multi-label instruction classification using wikiHow articles. The approach leverages XLNet's permutation language modeling to capture bidirectional context while avoiding BERT's independence assumptions between masked tokens. Through aggressive label filtering (reducing 6000+ categories to 67 with ≥500 observations each) and comprehensive preprocessing, the model achieves state-of-the-art performance with 97.30% accuracy and strong macro/micro F1 scores. The work addresses the gap in multi-label instructional text classification and provides a robust evaluation framework.

## Method Summary
The method employs XLNet fine-tuned for multi-label classification with binary relevance encoding. The HowSumm dataset of 11,121 wikiHow observations undergoes label filtering to retain only categories with ≥500 observations, reducing from 6000+ to 67 labels. Text preprocessing includes special character removal, stopword filtering, lemmatization, and lowercasing. XLNet-base serves as the encoder with a classification head using sigmoid activation and binary cross-entropy loss. The model is trained for 40 epochs with AdamW optimizer (lr=4e-04, batch_size=48) on Google Colab using NVIDIA K80 GPU.

## Key Results
- XLNet achieves 97.30% accuracy and 93% macro F1-score on the multi-label instruction classification task
- XLNet outperforms other transformer models including BERT (97.17%), ELECTRA, RoBERTa, and DistilBERT
- Binary relevance encoding with 67 filtered labels enables stable training while maintaining comprehensive category coverage

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** XLNet's permutation language modeling enables richer bidirectional context capture compared to BERT's masked language modeling for instructional text classification.
- **Mechanism:** XLNet samples factorization orders of input sequences and trains autoregressively across all permutations using two-stream self-attention (content stream and query stream). This avoids BERT's independence assumption between masked tokens while maintaining bidirectional context access.
- **Core assumption:** Instructional texts contain interdependent steps and concepts that benefit from modeling token relationships without the artificial [MASK] token noise introduced by BERT's pretraining.
- **Evidence anchors:**
  - [abstract]: "our implementation of the XLNet architecture has demonstrated unprecedented performance, achieving an accuracy of 97.30% and micro and macro average scores of 89.02% and 93%"
  - [section 3.1.3]: "XLNet built a permutation language model that pre-trains the data like an AE model by combining the advantages of AR models... It overcomes the limitation of BERT"
  - [corpus]: Related multi-label classification papers (e.g., Legal-LLM, PatchDEMUX) use transformer backbones but don't compare XLNet specifically; direct corpus validation of XLNet superiority on instructional text is absent.
- **Break condition:** If instructional texts are very short (< 50 tokens), the permutation modeling advantage may diminish relative to simpler masked approaches.

### Mechanism 2
- **Claim:** Aggressive label filtering based on observation frequency reduces class imbalance and may improve classifier stability.
- **Mechanism:** Algorithm 1 computes selection_score = (observation_count / threshold) and retains only labels with score ≥ 1. With threshold=500, the original 6000+ labels reduce to 67 well-represented categories.
- **Core assumption:** Labels with fewer than 500 training examples provide negative learning signal (noise) rather than useful signal for the model.
- **Evidence anchors:**
  - [section 3.2.1]: "most of these labels don't have enough observations, which tends to imbalance the dataset and affect the classifier's performance"
  - [Table 1]: Lists 67 filtered labels including "Education and Communications," "Health," "Sports and Fitness"
  - [corpus]: Neighbor paper "Improving the Accuracy and Efficiency of Legal Document Tagging" explicitly notes "significant label imbalance" as a challenge in multi-label settings, supporting this as a known problem.
- **Break condition:** If rare labels represent critical or high-value categories (e.g., safety-related instructions), filtering them removes essential coverage.

### Mechanism 3
- **Claim:** Binary relevance encoding with sigmoid activation enables independent per-label probability estimation for multi-label output.
- **Mechanism:** Each of 67 labels is encoded as an independent binary dimension (0/1). The model outputs 67 sigmoid probabilities, trained with binary cross-entropy loss. No explicit label correlation modeling is enforced.
- **Core assumption:** Label dependencies are implicitly learned through the transformer's contextual representations without requiring explicit correlation layers.
- **Evidence anchors:**
  - [section 3.2.1]: "For each text, we have created a binary list with the length of the total selected levels"
  - [section 3.2.2]: "We use the 'Sigmoid' activation function here in both models" and Equation 5 for binary cross-entropy
  - [corpus]: Weak direct evidence—neighbor papers don't compare binary relevance against classifier chains or other structured multi-label approaches.
- **Break condition:** If strong, consistent label correlations exist that the model cannot capture implicitly (e.g., mutually exclusive labels), per-label precision may degrade on co-occurrence patterns.

## Foundational Learning

- **Concept: Autoregressive vs. Autoencoding Language Models**
  - Why needed here: XLNet is a generalized autoregressive model that also captures bidirectional context. Understanding this distinction explains why the authors claim it outperforms BERT.
  - Quick check question: Can you explain why BERT's masked language modeling assumes independence between masked token predictions?

- **Concept: Multi-label Classification Loss Functions**
  - Why needed here: Unlike single-label classification, multi-label requires independent probability outputs per class. Softmax is inappropriate.
  - Quick check question: Why does sigmoid + binary cross-entropy replace softmax + categorical cross-entropy when each sample can belong to multiple classes?

- **Concept: Class Imbalance in Multi-label Settings**
  - Why needed here: The paper's label filtering directly addresses imbalance across 6000+ original labels.
  - Quick check question: If you have 100 labels but 80 have fewer than 10 samples each, what metrics would still be reliable (macro F1 vs. micro F1 vs. accuracy)?

## Architecture Onboarding

- **Component map:** wikiHow text → Preprocessing → XLNet-base encoder → Dense(67) → Sigmoid → Binary Cross-Entropy
- **Critical path:**
  1. Run Algorithm 1 (Label Selection) to filter labels with threshold=500
  2. Preprocess text: remove special characters, stopwords, lemmatize, lowercase
  3. Tokenize with model-specific tokenizer, generate attention masks
  4. Binarize labels into 67-dimensional vectors
  5. Fine-tune transformer with binary cross-entropy over 40 epochs
  6. Evaluate using accuracy, macro F1, micro F1
- **Design tradeoffs:**
  - **Threshold=500:** Higher values improve per-label performance but reduce label coverage; lower values retain more labels but risk instability from sparse supervision.
  - **Max length=512:** Matches BERT's limit for fair comparison; XLNet could handle longer sequences but authors constrained both for parity.
  - **Binary relevance vs. structured prediction:** Simpler to implement; may miss explicit label correlations that classifier chains would capture.
- **Failure signatures:**
  - **All-zero prediction collapse:** Model predicts no labels for most samples—check per-class positive rates and consider class-weighted loss.
  - **Training/validation accuracy gap > 5%:** Potential overfitting; reduce learning rate or add dropout.
  - **Macro F1 << Micro F1:** Model performing well on frequent labels but failing on less common ones; verify label distribution post-filtering.
- **First 3 experiments:**
  1. **Label filtering ablation:** Train with threshold=[100, 300, 500, 1000] and plot number of retained labels vs. macro F1 to validate the 500 choice.
  2. **Baseline transformer comparison:** Replicate BERT vs. XLNet vs. RoBERTa on the same train/test split to verify the reported 97.30% vs. 97.17% gap is reproducible.
  3. **Per-label performance audit:** Compute precision/recall per label to identify any of the 67 labels with F1 < 0.7; these may need targeted data augmentation or threshold adjustment.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can more efficient label encoding techniques improve performance while reducing the sparsity inherent in the binary encoding approach used in this work?
  - Basis in paper: [explicit] "In the future, we will address these issues by implementing a more efficient label encoding technique and reducing the amount of sparse data in a manner similar to what was done in this work."
  - Why unresolved: Binary label encoding creates 67-length sparse arrays (mostly zeros), which may not optimally capture label correlations or semantic relationships between categories.
  - What evidence would resolve it: Comparison experiments using alternative encoding methods (e.g., label embedding, hierarchical encoding) on the same HowSumm dataset, reporting both accuracy and computational efficiency metrics.

- **Open Question 2:** How does InstructNet perform on the method section of HowSumm data, which contains longer sequences than the step data used in this study?
  - Basis in paper: [explicit] "Additionally, we aim to optimize the method section of the HowSumm data, which contains more extensive sequences than the step data."
  - Why unresolved: The current study only evaluated step data; method descriptions may have different linguistic patterns and greater length, potentially affecting model performance.
  - What evidence would resolve it: Results from training and evaluating XLNet on the method section with the same evaluation metrics (accuracy, macro/micro F1).

- **Open Question 3:** How should the label filtering threshold be optimally determined to balance dataset coverage against class imbalance?
  - Basis in paper: [inferred] The threshold of 500 observations was chosen heuristically, reducing 6,000+ labels to 67, but no ablation or justification for this specific value was provided.
  - Why unresolved: The arbitrary threshold may discard potentially useful rare categories while retaining sufficient training data—a fundamental trade-off left unexplored.
  - What evidence would resolve it: Systematic experiments varying the threshold (e.g., 100, 250, 500, 1000) and analyzing resulting label distribution, model performance, and per-class F1 scores.

## Limitations

- The train/test split ratio is not specified, which is critical for reproducibility and validation of reported metrics.
- The statistical significance of XLNet's performance advantage over BERT (0.13% difference) is not established given the dataset size.
- Binary relevance encoding ignores label correlations that could be important for instructional text where certain category combinations are semantically related.

## Confidence

- **High confidence:** The core methodology (XLNet fine-tuning with binary relevance for multi-label classification) is well-established and the implementation details are sufficiently specified for replication.
- **Medium confidence:** The reported performance metrics (97.30% accuracy, 93% macro F1) are internally consistent with the described methodology, though external validation would strengthen claims.
- **Low confidence:** The statistical significance of XLNet's performance advantage over BERT and other transformers is not established, and the specific choice of threshold=500 for label filtering lacks ablation study support.

## Next Checks

1. **Train/test split validation:** Verify that the 80/20 split (assumed) produces stable performance by running 5-fold cross-validation and reporting mean ± standard deviation for accuracy and macro F1.

2. **Per-label performance audit:** Compute precision, recall, and F1-score for each of the 67 filtered labels to identify systematic weaknesses (e.g., low-performing categories that may require targeted data augmentation or specialized modeling).

3. **Label filtering ablation study:** Systematically vary the filtering threshold [100, 300, 500, 1000] and plot the tradeoff between number of retained labels and macro F1 to validate that 500 represents an optimal balance between coverage and performance stability.