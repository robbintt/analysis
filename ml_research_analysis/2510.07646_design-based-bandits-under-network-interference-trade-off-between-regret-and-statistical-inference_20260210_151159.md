---
ver: rpa2
title: 'Design-Based Bandits Under Network Interference: Trade-Off Between Regret
  and Statistical Inference'
arxiv_id: '2510.07646'
source_url: https://arxiv.org/abs/2510.07646
tags:
- regret
- exposure
- inference
- where
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies multi-armed bandits with network interference\
  \ (MABNI), where an agent\u2019s actions affect others\u2019 rewards. While prior\
  \ work focuses on regret minimization, it neglects statistical inference accuracy\
  \ for sub-optimal arms."
---

# Design-Based Bandits Under Network Interference: Trade-Off Between Regret and Statistical Inference

## Quick Facts
- **arXiv ID:** 2510.07646
- **Source URL:** https://arxiv.org/abs/2510.07646
- **Reference count:** 40
- **Primary result:** Establishes theoretical Pareto frontier between regret and ATE estimation error in MABNI, with EXP3-N-CS achieving Pareto optimality.

## Executive Summary
This paper addresses a fundamental tension in multi-armed bandits with network interference: minimizing cumulative regret while maintaining valid statistical inference for sub-optimal arms. Standard bandit algorithms focus solely on regret, causing sampling probabilities to concentrate on optimal arms and making inference on sub-optimal arms unreliable. The authors establish a theoretical Pareto frontier characterizing this trade-off and propose EXP3-N-CS, which integrates anytime-valid asymptotic confidence sequences with the EXP3 algorithm. The method achieves sublinear regret while preserving statistical validity for continual inference, balancing welfare and scientific knowledge acquisition.

## Method Summary
The EXP3-N-CS algorithm addresses MABNI by first compressing the exponential action space through an exposure mapping that reduces $K^N$ global treatments to $|U_E|$ exposure super-arms. It then implements a Mixture Adaptive Design (MAD) policy that combines EXP3's exploitation with forced exploration: $\pi_t^{MAD}(S) = \frac{1}{|U_E|}\delta_t + (1-\delta_t)\pi_t^{ALG}(S)$ where $\delta_t = t^{-\alpha}$. The algorithm maintains an IPW-based estimator with an asymptotic confidence sequence that provides anytime-valid statistical guarantees. The trade-off parameter $\alpha \in [0, 1/2)$ controls the balance between regret minimization and inference accuracy.

## Key Results
- Establishes a theoretical Pareto frontier showing the fundamental trade-off between regret and ATE estimation error in MABNI
- Proposes EXP3-N-CS algorithm achieving regret bounded by $\tilde{O}(\sqrt{|U_E|T + T^{1-\alpha}})$ and estimation error by $\tilde{O}(|U_E|^{1/2}T^{\alpha-1/2})$
- Demonstrates through simulations that EXP3-N-CS effectively balances the trade-off across various network topologies, outperforming baselines focused solely on regret or exploration

## Why This Works (Mechanism)

### Mechanism 1: Dimensionality Reduction via Exposure Mapping
The algorithm renders the intractable $K^N$ super-arm space learnable by mapping global treatments to localized "exposure" levels using an exposure mapping function $S(i, A, H)$. This reduces the effective action space from $K^N$ to $|U_E|$, making regret bounds logarithmic rather than linear in network size. The core assumption is that the defined exposure mapping captures the relevant causal structure.

### Mechanism 2: Strategic Exploration via Mixture Adaptive Design (MAD)
MAD explicitly mixes uniform exploration into the regret-minimization policy: $\pi^{MAD}_t(S) = \frac{1}{|U_E|}\delta_t + (1-\delta_t)\pi^{ALG}_t(S)$ where $\delta_t = t^{-\alpha}$. This preserves statistical validity by ensuring IPW weights remain bounded, stabilizing the variance estimator. The trade-off parameter $\alpha \in [0, 1/2)$ must be sufficient to ensure linear variance growth while keeping regret sublinear.

### Mechanism 3: Asymptotic Confidence Sequences (CS)
The algorithm provides valid statistical guarantees at any stopping time by constructing confidence bounds based on cumulative conditional variance rather than fixed-horizon approximations. The asymptotic CS $\hat{C}_t(S_i, S_j)$ utilizes a variance estimator $\hat{V}_t$ that accounts for adaptive data collection, enabling "continual inference" without penalty for peeking at the data.

## Foundational Learning

- **Concept: Exposure Mapping & Potential Outcomes**
  - Why needed: You cannot understand MAB-N without grasping how the paper redefines "treatment" from binary action to structural property of the neighborhood (exposure).
  - Quick check: Can you explain why the paper treats "having 2 treated neighbors" as a distinct arm from "having 3 treated neighbors"?

- **Concept: Inverse Propensity Weighting (IPW)**
  - Why needed: The ATE estimator relies on IPW to correct for bias introduced by the bandit's tendency to pull good arms more often.
  - Quick check: If an algorithm never pulls a specific arm ($\pi_t \to 0$), what happens to the variance of the IPW estimator for that arm?

- **Concept: Anytime-Validity / Confidence Sequences**
  - Why needed: This distinguishes the paper's inference from standard fixed-T A/B testing. The learner must understand these bounds hold uniformly over time $t$.
  - Quick check: Why does a standard confidence interval fail if you repeatedly check it to decide when to stop an experiment?

## Architecture Onboarding

- **Component map:** Input (Network Adjacency Matrix, Unit Set, Arm Set) -> MAB-N Module (compresses actions into Exposure Super Arms) -> MAD Controller (mixes EXP3 with exploration) -> Estimator (IPW-based Asymptotic CS) -> Output (Regret-minimizing actions and Anytime-Valid ATE intervals)

- **Critical path:**
  1. Initialize exposure mappings (reducing state space)
  2. For each round $t$:
     - EXP3 calculates $\pi^{ALG}$ based on historical regret
     - MAD mixes in uniform exploration probability $\delta_t = t^{-\alpha}$
     - Sample Exposure Super Arm $S_t$
     - Decompress: Sample real action $A_t$ consistent with $S_t$
     - Observe reward $R_t$
     - Update Cumulative Variance $\hat{V}_t$ and Confidence Sequence $\hat{C}_t$

- **Design tradeoffs:** The $\alpha$ Dial
  - Set $\alpha \approx 0$: Maximum estimation accuracy, linear regret (bad welfare)
  - Set $\alpha \approx 1/2$: Low regret, high estimation error (invalid inference)
  - Guidance: Use $\alpha$ to tune based on cost of human welfare vs. value of scientific knowledge

- **Failure signatures:**
  - Exploding CS Width: MAD exploration rate too low for network structure
  - Linear Regret: $\alpha$ too small or exposure mapping too coarse
  - Allocation Errors: Definition of "Legitimate Exposure Super Arms" $U_E$ too restrictive

- **First 3 experiments:**
  1. **Sanity Check ($\alpha$ Sweep):** Run on 2-cluster graph with $\alpha \in [0.1, 0.49]$, verify Regret decreases and ATE Error increases as $\alpha$ increases
  2. **Validation of Anytime-Validity:** Run for $T=10,000$ steps, check Coverage Probability of CS at random stopping times $t < T$
  3. **Stress Test (Misspecification):** Define exposure mapping ignoring cluster structure, run on highly connected network to observe degradation

## Open Questions the Paper Calls Out

### Open Question 1
Can the linear growth condition for cumulative conditional variance (Assumption 5.3) be relaxed to simple divergence ($V_t \to \infty$) while retaining valid asymptotic confidence sequences? The authors note their assumption is "relatively stronger" than related work, acknowledging the theoretical gap.

### Open Question 2
Is it possible to construct non-asymptotic (finite-sample valid) confidence sequences for MAB-N that maintain the Pareto-optimal trade-off? The paper utilizes "Asymptotic Confidence Sequences" rather than non-asymptotic ones.

### Open Question 3
Can an adaptive mechanism be designed to dynamically tune the trade-off parameter $\alpha$ based on observed data during the experiment? The paper suggests selecting a fixed $\alpha$ based on domain knowledge but provides no method for adjusting this parameter online.

## Limitations
- Theoretical guarantees rely heavily on specific assumptions about exposure mappings and network structure
- Validity of confidence sequences depends critically on variance growth condition that may fail in highly clustered or sparse networks
- Numerical results limited to single synthetic network topology, raising questions about generalizability to real-world social networks

## Confidence
- **High confidence:** Regret bounds and their dependence on $\alpha$ (Theorem 4.1)
- **Medium confidence:** Anytime-valid confidence sequence construction (Theorem 5.5)
- **Low confidence:** Empirical demonstration of Pareto optimality across diverse network topologies

## Next Checks
1. **Robustness to network topology:** Replicate experiments on scale-free, small-world, and random graph structures
2. **Real-world deployment test:** Apply EXP3-N-CS to real social network dataset with realistic reward distributions
3. **Variance growth validation:** Systematically test the variance growth assumption across different $\alpha$ values and network structures