---
ver: rpa2
title: 'AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference'
arxiv_id: '2501.02336'
source_url: https://arxiv.org/abs/2501.02336
tags:
- skipping
- inference
- layers
- decoding
- prefilling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of accelerating long-context LLM
  inference, which is increasingly critical due to the growing context window sizes
  and associated computational costs. The authors identify key limitations of existing
  layer-wise skipping strategies, including their inability to adapt to model and
  context variability, disregard for sublayer significance, and inapplicability to
  the prefilling phase.
---

# AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference

## Quick Facts
- arXiv ID: 2501.02336
- Source URL: https://arxiv.org/abs/2501.02336
- Reference count: 10
- Primary result: Achieves 72.8% accuracy on TREC and 86.6 F1 score on TriviaQA with 8 skipped sublayers using LLaMA3.1-8B-128k, closely matching full model performance while providing up to 17% acceleration improvement.

## Executive Summary
This paper addresses the challenge of accelerating long-context LLM inference by proposing AdaSkip, an adaptive sublayer skipping method that leverages on-the-fly similarity information to identify less important layers. The method overcomes key limitations of existing approaches by enabling sublayer-wise skipping for both prefilling and decoding phases, treating attention and FFN sublayers independently, and combining offline historical learning with online adaptation. Extensive experiments demonstrate that AdaSkip outperforms existing baselines in both generation quality and inference speed across various long-context benchmarks and models.

## Method Summary
AdaSkip uses IO similarity (cosine similarity between sublayer input and output) to identify and skip less important sublayers. The method consists of offline importance learning for prefilling using historical similarity features, and online importance learning during decoding to discover additional FFN skipping opportunities. It treats attention and FFN sublayers independently, enabling more granular optimization. Scale compensation factors are applied when sublayers are skipped to approximate their outputs. The approach achieves acceleration while maintaining generation quality through adaptive, context-aware skipping decisions.

## Key Results
- Achieves 72.8% accuracy on TREC and 86.6 F1 score on TriviaQA using LLaMA3.1-8B-128k with 8 skipped sublayers
- Provides up to 17% acceleration improvement compared to baselines while maintaining comparable generation quality
- Demonstrates effective transfer of historical prefilling similarity patterns across datasets with hit rates of 3.76/4, 4.86/6, 9.31/10
- Shows online FFN skipping during decoding can identify additional skipping opportunities beyond offline learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformer modules with high cosine similarity between input and output vectors contribute less to inference quality and can be safely skipped.
- Mechanism: When a module's output closely resembles its input (high IO similarity), the transformation applied is minimal, indicating lower functional importance. Skipping high-similarity modules preserves more information than skipping low-similarity ones.
- Core assumption: IO similarity reliably predicts downstream task degradation when the module is removed.
- Evidence anchors:
  - [abstract]: "leverages on-the-fly similarity information to identify less important layers"
  - [section]: "LeastSkip strategy, which skips the layers exhibiting the lowest IO similarity, experiences a substantial degradation in the GPT score (dropping below 1.0 even with one skipped layer), compared to the MostSkip strategy...yields GPT scores of 8.9, 6.1, and 4.2 when skipping 1, 3, and 5 layers, respectively"
  - [corpus]: Limited corpus validation; neighbor papers focus on token-level rather than sublayer-level importance metrics.
- Break condition: When high-similarity modules perform subtle but critical transformations (e.g., specific reasoning patterns, safety filtering).

### Mechanism 2
- Claim: Treating attention and FFN sublayers independently yields better skipping decisions than monolithic layer skipping.
- Mechanism: Attention and FFN exhibit different importance distributions—attention shows higher and more concentrated IO similarity (~0.97 in later layers), enabling more aggressive attention skipping in long-context scenarios where attention dominates latency.
- Core assumption: Attention and FFN importance patterns are sufficiently decoupled for independent optimization.
- Evidence anchors:
  - [abstract]: identifies "disregard for sublayer significance" as a key limitation of existing methods
  - [section]: "In the last 11 layers, the average IO similarity of attention is consistently around 0.97...highest average IO similarity of FFN in the last 11 layers is only 0.95, and it is relatively scattered"
  - [corpus]: "Fast Forward" (neighbor paper) similarly targets FFN sparsity specifically during prefilling, corroborating FFN-specific optimization as a valid direction.
- Break condition: Architectures with strong attention-FFN coupling; when skipping one causes cascade degradation in the other.

### Mechanism 3
- Claim: Combining offline historical similarity features for prefilling with online learning from initial decoded tokens enables effective skipping in both phases.
- Mechanism: Historical prefilling similarity transfers across datasets (high hit rates); initial decoding tokens provide sufficient signal to identify additional FFN skipping opportunities in the decoding phase.
- Core assumption: Historical patterns generalize to new contexts; initial decoded tokens are representative of subsequent decoding importance.
- Evidence anchors:
  - [abstract]: "offline importance learning for prefilling using historical similarity features, and online importance learning during decoding to discover additional FFN skipping opportunities"
  - [section]: Table 1 shows hit rates of 3.76/4, 4.86/6, 9.31/10 using historical features; Table 2 shows decoding hit rate increases with online window size
  - [corpus]: "Probe and Skip" (neighbor) similarly uses predictive approaches for long-context efficiency, suggesting predictive skipping is an emerging direction.
- Break condition: Significant domain shift between historical and new contexts; initial decoded tokens are outliers.

## Foundational Learning

- Concept: **Transformer Layer Structure (Attention + FFN with Residual Connections)**
  - Why needed here: Understanding that attention and FFN are independent sublayers with residual connections explains why they can be skipped separately and why output can resemble input.
  - Quick check question: How do residual connections enable a sublayer's output to closely resemble its input?

- Concept: **KV Cache and Long-Context Memory Requirements**
  - Why needed here: Long-context inference creates substantial KV cache storage demands; attention sublayers are the primary contributor, motivating attention-focused skipping.
  - Quick check question: Why does attention scale quadratically with sequence length, and how does this affect prefilling vs decoding differently?

- Concept: **Prefilling vs Decoding Phases**
  - Why needed here: The paper applies different strategies to each phase—offline learning for prefilling, online learning for decoding—with different FFN importance patterns observed.
  - Quick check question: What is TTFT and which inference phase does it correspond to?

## Architecture Onboarding

- Component map:
  Offline Importance Learner -> Prefilling Skip Selector -> Scale Compensation Module -> Online Importance Learner -> Decoding Skip Selector

- Critical path:
  1. Profile representative dataset offline → compute per-sublayer similarity and scale factors
  2. Load precomputed skip decisions at inference start for prefilling
  3. Execute prefilling with non-skipped sublayers + scale compensation
  4. During first P decoding tokens, compute online FFN similarity
  5. Merge skip sets, continue decoding with combined decisions

- Design tradeoffs:
  - **Acceleration ratio α**: Higher = faster but potential quality loss
  - **Online window P**: Larger = better hit rate but delayed optimization
  - **Offline dataset representativeness**: Domain mismatch reduces transfer accuracy

- Failure signatures:
  - Quality collapse in early layers → skipping critical layers due to noisy historical data
  - Poor domain generalization → offline features don't transfer, needs re-profiling
  - Asymmetric prefilling/decoding degradation → online window too small or β misconfigured
  - FFN-heavy quality drops → β threshold too aggressive

- First 3 experiments:
  1. Profile IO similarity distributions on target model (e.g., LLaMA3.1-8B-128k); visualize layer/sublayer patterns to validate Observations 1-3
  2. Test offline-only skipping on prefilling tasks (TREC, TriviaQA) with α=1.14, 1.33; compare quality retention vs baselines
  3. Enable online FFN skipping with window sizes P=5, 20, 40 on decoding tasks (GovReport, MultiNews); measure end-to-end quality and speedup

## Open Questions the Paper Calls Out
None

## Limitations
- **Dataset and Model Coverage**: The paper evaluates on specific models (LLaMA3.1-8B-128k, InternLM-7B-8k) and benchmarks, but generalization to other model families and task types remains untested.
- **Implementation Dependencies**: Key hyperparameters (N for offline profiling, P for online window size, α for acceleration ratio) are not fully specified in experimental configurations.
- **Trade-off Ambiguity**: While the paper demonstrates improved quality-speed trade-offs compared to baselines, the optimal balance point for different application domains is not systematically explored.

## Confidence

**High Confidence**: The core insight that high IO similarity indicates lower functional importance is well-supported by empirical observations in the paper. The sublayer-wise skipping approach is theoretically sound given their distinct importance distributions.

**Medium Confidence**: The effectiveness of historical offline learning transferring to new contexts shows good hit rates but lacks extensive cross-domain validation. The online learning component's benefits are demonstrated but depend on choosing appropriate window sizes.

**Low Confidence**: The generalizability of the similarity-based importance metric across different model architectures and the long-term stability of the skipping decisions without periodic retraining are not established.

## Next Checks
1. **Cross-Domain Generalization Test**: Apply AdaSkip to a model and task domain outside the training distribution (e.g., code generation or mathematical reasoning) to validate whether historical similarity patterns transfer effectively.

2. **Ablation Study on Sublayer Independence**: Systematically test whether treating attention and FFN sublayers independently actually provides benefits over unified layer skipping by comparing quality-speed trade-offs across multiple models.

3. **Scale-Factor Compensation Validation**: Implement and test the scale compensation mechanism in isolation to verify that it adequately preserves output quality when sublayers are skipped, particularly in early transformer layers where scale factors may vary significantly.