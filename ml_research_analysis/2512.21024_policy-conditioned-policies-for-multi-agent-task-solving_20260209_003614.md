---
ver: rpa2
title: Policy-Conditioned Policies for Multi-Agent Task Solving
arxiv_id: '2512.21024'
source_url: https://arxiv.org/abs/2512.21024
tags:
- policy
- agents
- policies
- learning
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Programmatic Iterered Best Response (PIBR),
  a novel approach for solving multi-agent tasks by representing policies as human-interpretable
  source code and leveraging LLMs as approximate interpreters. The core challenge
  addressed is the "representational bottleneck" in deep reinforcement learning, where
  neural policies are opaque and high-dimensional, making it intractable to condition
  on opponent strategies.
---

# Policy-Conditioned Policies for Multi-Agent Task Solving

## Quick Facts
- arXiv ID: 2512.21024
- Source URL: https://arxiv.org/abs/2512.21024
- Reference count: 40
- Primary result: PIBR bridges the gap between theoretical Program Equilibrium and modern learning by representing policies as human-interpretable code and using LLMs as approximate interpreters

## Executive Summary
This paper introduces Programmatic Iterated Best Response (PIBR), a novel approach for solving multi-agent tasks by representing policies as human-interpretable source code and leveraging LLMs as approximate interpreters. The core challenge addressed is the "representational bottleneck" in deep reinforcement learning, where neural policies are opaque and high-dimensional, making it intractable to condition on opponent strategies. PIBR reformulates the learning problem by lifting optimization from policy space to operator space, where the LLM functions as a point-wise best-response operator that iteratively synthesizes and refines the ego agent's policy code to respond to the opponent's strategy. The method employs textual gradients using structured feedback derived from game utility and runtime unit tests.

## Method Summary
PIBR operates through an iterative synthesis process where the ego agent (A0) synthesizes its policy π0 by querying an LLM with the opponent's source code π1 and interaction history. The LLM generates updated policy code that aims to maximize the ego agent's utility while responding to the opponent's strategy. Learning proceeds through textual gradients computed from two feedback signals: game utility scores and unit tests that validate policy behavior. The process iterates until convergence or a stopping criterion is met. The approach explicitly addresses the challenge of conditioning policies on opponent strategies by lifting the optimization problem from the space of policies to the space of operators that generate policies.

## Key Results
- PIBR successfully solves standard coordination matrix games where traditional RL approaches struggle with representational bottlenecks
- The method demonstrates effective learning in a cooperative Level-Based Foraging environment with 2 agents
- Textual gradient optimization via unit tests provides a viable learning signal for programmatic policy representation

## Why This Works (Mechanism)
PIBR works by transforming the multi-agent learning problem from optimizing over policy parameters to optimizing over program synthesis operators. The LLM acts as an approximate interpreter that can reason about source code semantics and generate responses conditioned on opponent strategies. By representing policies as code rather than neural networks, PIBR enables explicit reasoning about opponent behavior through code inspection and manipulation. The textual gradient approach using unit tests provides structured feedback that guides the LLM toward policies satisfying specific behavioral constraints while maximizing utility.

## Foundational Learning
- **Program Equilibrium**: The theoretical foundation where agents' strategies are represented as programs that can condition on each other's source code. Needed to understand the conceptual motivation for programmatic policies. Quick check: Can agents reach mutually beneficial outcomes when their strategies are programs that can inspect each other?

- **Iterated Best Response**: A solution concept in game theory where agents sequentially best-respond to each other's strategies. Needed to understand the iterative nature of PIBR. Quick check: Does the iterative process converge to stable policy pairs?

- **Textual Gradients**: The use of textual feedback (unit tests and utility scores) to guide LLM-based policy synthesis. Needed to understand the learning mechanism. Quick check: Does the feedback signal effectively steer policy synthesis toward better solutions?

- **Policy Conditioning**: The ability to represent policies that depend on opponent strategies. Needed to understand how PIBR overcomes the representational bottleneck. Quick check: Can the synthesized policies successfully condition on different opponent strategies?

- **LLM-based Program Synthesis**: Using large language models to generate and modify source code based on prompts and feedback. Needed to understand the core technical mechanism. Quick check: Does the LLM consistently generate syntactically correct and semantically meaningful policy code?

- **Unit Testing in Policy Learning**: Using automated tests to validate policy behavior during training. Needed to understand the structured feedback mechanism. Quick check: Do unit tests effectively capture the desired behavioral properties of policies?

## Architecture Onboarding

**Component Map**: Opponent Policy (π-1) -> LLM Operator -> Textual Gradient Feedback (Utility + Unit Tests) -> Updated Policy (π0)

**Critical Path**: The synthesis loop where opponent policy code is fed to the LLM along with feedback signals to generate updated policy code for the ego agent. This loop must iterate efficiently to enable practical learning.

**Design Tradeoffs**: 
- Using LLMs provides flexibility and interpretability but introduces computational overhead and potential inconsistency
- Programmatic policies enable explicit conditioning on opponent strategies but may be less expressive than neural networks for complex tasks
- Textual gradients via unit tests provide structured feedback but require careful test design to be effective

**Failure Signatures**:
- Policy synthesis fails to improve utility over iterations (indicates insufficient feedback signal or LLM limitations)
- Generated policies are syntactically invalid or semantically meaningless (indicates LLM generation problems)
- Learning exhibits high variance or instability (indicates sensitivity to feedback quality or exploration-exploitation balance)

**Three First Experiments**:
1. Test PIBR on simple 2x2 matrix games to verify basic functionality and convergence behavior
2. Evaluate the contribution of unit tests versus utility feedback by running ablation studies with each signal type separately
3. Measure the computational overhead of LLM-based policy synthesis compared to neural policy optimization methods

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How can the stability of PIBR be improved to prevent performance degradation in complex sequential decision-making tasks?
- Basis: Section 4.2 states that in the Cooperative Foraging task, "individual samples occasionally approach the empirical optimum" but "the mean trajectory exhibits significant variance and instability." The authors explicitly note that "current results are not yet fully stabilized" and improvements are under development.
- Why unresolved: The textual gradient optimization via LLMs appears prone to oscillation or overfitting to recent feedback when handling the sparse rewards and complex dynamics of grid-world tasks compared to simple matrix games.
- What evidence would resolve it: A modification to the PIBR algorithm (e.g., improved loss functions or prompt strategies) that results in a monotonic convergence curve in the Level-Based Foraging environment.

### Open Question 2
- Question: Can PIBR agents maintain robust performance in mixed-motive or competitive settings where opponents might obfuscate their source code?
- Basis: Section 3.1 restricts the validation to an "unprocessed policy-communication setting," which assumes agents truthfully share source code. The authors justify this only for fully cooperative or self-play scenarios, leaving the handling of adversarial code representation (deception/obfuscation) as an unaddressed challenge.
- Why unresolved: The current "operator" relies on reading the opponent's code ⌜π−i⌝ to generate a best response; this semantic interpretation breaks if the opponent provides functionally correct but semantically misleading code.
- What evidence would resolve it: Experimental results in a zero-sum game where opponents are allowed to vary the syntactic structure (obfuscation) of their policies, testing the interpreter's robustness.

### Open Question 3
- Question: Does the iterative synthesis process scale effectively to environments with N > 2 agents?
- Basis: Algorithm 1 explicitly defines the input agents as a set of two (A={A0, A1}), and the experiments are limited to 2-player matrix games and 2-agent foraging.
- Why unresolved: LLMs have finite context windows. As the number of agents increases, the length of the prompt containing all opponents' source code and interaction histories may exceed the model's capacity or degrade its reasoning ability.
- What evidence would resolve it: Successful application of PIBR in a multi-agent coordination benchmark (e.g., 3 or 4 agents) without manual simplification of the policy code.

## Limitations
- The approach relies heavily on LLM capabilities and may face scalability challenges with increasing problem complexity
- Evaluation is limited to relatively simple matrix games and a specific 2-agent cooperative environment
- Performance depends critically on the quality and design of unit tests, which may not generalize well to complex tasks
- The method's effectiveness in competitive or mixed-motive settings remains unexplored

## Confidence
- **High**: Solving standard coordination matrix games within the tested domains
- **Medium**: The claim that PIBR effectively bridges the gap between theoretical Program Equilibrium and modern learning (largely conceptual connection)
- **Medium**: The assertion that textual gradients via unit tests constitute an effective learning signal (lacks comparison to alternatives)

## Next Checks
1. **Scalability Test**: Evaluate PIBR on multi-agent tasks with continuous action spaces (e.g., cooperative navigation with continuous velocity control) to assess whether the LLM-based policy synthesis remains tractable and effective beyond discrete game structures.

2. **Robustness Analysis**: Systematically vary the quality and quantity of feedback signals (unit tests and utility scores) to quantify their individual contributions and identify minimum viable feedback requirements for stable learning.

3. **Generalization Benchmark**: Test PIBR policies in zero-shot transfer scenarios where agent roles or environmental dynamics change between training and evaluation, measuring whether programmatically represented policies generalize better than neural alternatives.