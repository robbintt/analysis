---
ver: rpa2
title: 'Talk to Right Specialists: Routing and Planning in Multi-agent System for
  Question Answering'
arxiv_id: '2501.07813'
source_url: https://arxiv.org/abs/2501.07813
tags:
- agents
- knowledge
- answer
- question
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RopMura, a multi-agent system for question
  answering that incorporates routing and planning mechanisms to address knowledge
  sovereignty issues. The system includes a router that intelligently selects the
  most relevant agents based on knowledge boundaries, and a planner that decomposes
  complex multi-hop queries into manageable steps.
---

# Talk to Right Specialists: Routing and Planning in Multi-agent System for Question Answering

## Quick Facts
- arXiv ID: 2501.07813
- Source URL: https://arxiv.org/abs/2501.07813
- Reference count: 29
- Primary result: RopMura achieves 74.93% GPT Evaluation on HotpotQA and 85.03% on Multi-hop RAG tasks

## Executive Summary
RopMura is a multi-agent question answering system that addresses knowledge sovereignty by combining intelligent routing with recursive planning. The system uses a router that clusters each agent's knowledge into centroids, enabling efficient selection of relevant specialists without exposing raw data. For complex multi-hop queries, a planner decomposes questions into subquestions, routing each to appropriate agents until the original query is answerable.

The approach demonstrates significant improvements over existing methods, particularly in handling complex reasoning tasks that require multiple information sources. By maintaining data privacy through knowledge boundaries while enabling sophisticated multi-hop reasoning, RopMura bridges a critical gap in distributed question answering systems.

## Method Summary
RopMura implements a multi-agent QA system where each agent is a RAG-based specialist with domain-specific knowledge. The router computes centroid embeddings for clustered knowledge chunks using hierarchical clustering (n = ⌊√m⌋) and selects top-k agents based on query-centroid similarity. For single-hop queries, responses are evaluated and curated directly. For multi-hop queries, a planner recursively decomposes questions through a splitter-selector-judger loop until the original query becomes answerable. The system uses mixture retrieval (dense + BM25), chunk sizes of 1024 tokens for Wikipedia and 256 for news, and evaluates performance using Lexical Match and GPT Evaluation metrics.

## Key Results
- RopMura achieves 74.93% GPT Evaluation score on HotpotQA, outperforming baseline multi-agent systems
- For Multi-hop RAG tasks, RopMura reaches 85.03% GPT Evaluation score
- The routing mechanism alone enables precise answers for single-hop queries with significant token efficiency gains
- Combined routing and planning mechanisms achieve accurate, multi-step resolutions for complex queries

## Why This Works (Mechanism)

### Mechanism 1: Cluster-Centroid Knowledge Boundary Encoding
- Claim: Agents can concisely represent their knowledge coverage for routing without exposing raw data
- Mechanism: Each agent partitions m knowledge pieces into n = ⌊√m⌋ disjoint clusters via hierarchical clustering on cosine similarity, computing centroid embeddings as knowledge signatures
- Core assumption: Knowledge pieces within a cluster are semantically coherent enough that the centroid meaningfully represents the cluster's topic scope
- Evidence anchors: Abstract mentions "router that intelligently selects the most relevant agents based on knowledge boundaries"; section 3.1 describes centroid-based routing; neighbor paper validates similar routing approaches

### Mechanism 2: Top-k Agent Selection with Response Evaluation
- Claim: Selecting only the most relevant agents reduces token overhead while maintaining or improving answer quality
- Mechanism: Router identifies k clusters with highest query-centroid similarity, invites corresponding agents to respond, then evaluates each response's analysis quality before curating the final answer
- Core assumption: Top-k similarity ranking correlates with actual agent relevance; response analysis quality indicates answer reliability
- Evidence anchors: Abstract notes "routing mechanism enabling precise answers for single-hop queries"; section 3.1 details response evaluation; AgentRouter paper validates knowledge-graph-guided routing

### Mechanism 3: Recursive Greedy Planning with Judger Feedback
- Claim: Multi-hop queries can be resolved by iteratively generating and answering subquestions until the original query becomes answerable
- Mechanism: Planner submodules (splitter → selector → router → judger) operate in rounds: generate candidate subquestions, select one for routing, obtain answer, judger checks if original query is answerable—if not, loop continues
- Core assumption: Subquestions decompose complexity monotonically; each round accumulates sufficient context to eventually resolve the query
- Evidence anchors: Abstract describes "planner that decomposes complex multi-hop queries into manageable steps"; section 4 discusses continuous refinement; limited corpus validation for greedy planning specifically

## Foundational Learning

- **Retrieval-Augmented Generation (RAG)**: Each specialist agent is fundamentally a RAG system; understanding retrieval, reranking, and generation pipeline is prerequisite. Quick check: Given a query, can you trace how dense/sparse retrievers fetch documents and how an LLM uses them for generation?

- **Embedding Similarity and Clustering**: The router's core operation is comparing query embeddings to centroid embeddings; clustering quality directly affects routing accuracy. Quick check: Why would hierarchical clustering with cosine distance produce more informative centroids than random sampling?

- **Multi-hop Question Decomposition**: The planner's value depends on correctly identifying when a query requires multiple reasoning steps versus single-hop retrieval. Quick check: For "Who is the CEO of the company that acquired Instagram?", what are the implicit subquestions?

## Architecture Onboarding

- **Component map**: Query → Router (centroid matching) → Selected agents (RAG) → Router (evaluation + curation) → Final answer; Multi-hop: Query → Planner (split) → Selector → Router → Agent response → Judger → (loop or Defender)

- **Critical path**: Single-hop: Query → Router (centroid matching) → Selected agents (RAG) → Router (evaluation + curation) → Final answer; Multi-hop: Query → Planner (split) → Selector → Router → Agent response → Judger → (loop or Defender)

- **Design tradeoffs**: Cluster count (n = ⌊√m⌋) balances granularity vs. comparison overhead; number of selected agents (k) trades recall vs. token cost; maximum planning rounds limits token explosion but may truncate before resolution

- **Failure signatures**: High GPT Evaluation but low Lexical Match indicates semantic correctness with wording differences; agent selected but responds "I don't know" means centroid matched but retrieval failed; planning loops without convergence indicates judger never returns "answerable"

- **First 3 experiments**: 1) Run single-hop QA with k=1, 3, 5, 10 agents to find optimal k for your domain; 2) Compare n = ⌊√m⌋ vs. fixed n vs. elbow-method clustering for routing precision; 3) Benchmark presplit vs. greedy vs. RopMura for accuracy and token efficiency on multi-hop dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the system architecture be adapted to route and plan for agents that possess versatile modalities (e.g., images or audio) rather than just text?
- Basis in paper: Conclusion states that future research should focus on "versatile modalities" and "how to encode multimodal information for the router to find relevant agents effectively"
- Why unresolved: Current router relies on text embedding centroids that don't translate to non-textual data types
- What evidence would resolve it: A modified routing mechanism processing multimodal queries with experimental results on a multimodal multi-hop QA dataset

### Open Question 2
- Question: How can the routing mechanism be made robust against misidentification when knowledge domains overlap significantly or embeddings are not distinct?
- Basis in paper: Limitations section notes that "effectiveness of the routing mechanism heavily relies on the quality of the knowledge boundary representations" and fails when "embeddings are less distinct"
- Why unresolved: Current clustering assumes semantic separation that may not exist in dense or overlapping knowledge bases
- What evidence would resolve it: Comparative analysis showing routing accuracy on datasets with high semantic overlap versus current baseline

### Open Question 3
- Question: Can the planning and routing framework be generalized to coordinate agents that rely on fine-tuned models for specific tasks (e.g., coding) rather than RAG-based retrieval?
- Basis in paper: Limitations section explicitly states that work "cannot be generalized to a multi-agent system, where some agents rely on a well fine-tuned model to a specific task"
- Why unresolved: Current design assumes agents are defined by retrievable knowledge boundaries, whereas fine-tuned agents are defined by functional capabilities
- What evidence would resolve it: A proposed method for defining "skill boundaries" and successful integration of function-specific agents into the workflow

## Limitations
- Knowledge boundary ambiguity: Centroid-based routing assumes clean cluster separation, but significant domain overlap or embedding indistinctness can cause routing errors
- Planning strategy weakness: Greedy recursive planning lacks lookahead or backtracking capabilities, potentially causing cascading errors when early subquestions mislead
- Reranking and fusion strategy gaps: Critical implementation details including specific reranking model and mixture retrieval fusion strategy are undocumented

## Confidence
- **High Confidence (90%+)**: The fundamental routing mechanism using centroid similarity works as described; empirical results are reproducible with specified datasets and evaluation protocol
- **Medium Confidence (70-85%)**: The multi-hop planning approach achieves reported performance, but specific implementation details (maximum iteration limit, exact planning prompts) introduce uncertainty
- **Low Confidence (40-60%)**: Claims about knowledge sovereignty preservation and theoretical advantages over alternative methods lack rigorous validation beyond comparison with AgentRouter

## Next Checks
1. **Cluster Granularity Impact**: Compare n = ⌊√m⌋ clustering against fixed cluster counts and elbow-method optimization on held-out queries to measure routing precision degradation

2. **Token Efficiency Tradeoff**: Run ablation studies varying k (1-10 agents) for single-hop queries to quantify the relationship between token cost and answerable rate, identifying optimal selection thresholds

3. **Planning Robustness Test**: Design adversarial multi-hop queries where early subquestions contain misleading information, then measure how often RopMura's greedy planner recovers versus alternative strategies with lookahead capability