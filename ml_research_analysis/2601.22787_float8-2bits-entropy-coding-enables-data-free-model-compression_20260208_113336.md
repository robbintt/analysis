---
ver: rpa2
title: 'Float8@2bits: Entropy Coding Enables Data-Free Model Compression'
arxiv_id: '2601.22787'
source_url: https://arxiv.org/abs/2601.22787
tags:
- entquant
- quantization
- entropy
- llama-2
- coding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EntQuant enables extreme compression of large language models without
  data or training by decoupling compression rate from representational precision.
  The method optimizes quantized weights for entropy and employs GPU-accelerated entropy
  coding to achieve arbitrary bit-rates while maintaining high-precision inference.
---

# Float8@2bits: Entropy Coding Enables Data-Free Model Compression

## Quick Facts
- arXiv ID: 2601.22787
- Source URL: https://arxiv.org/abs/2601.22787
- Authors: Patrick Putzky; Martin Genzel; Mattes Mollenhauer; Sebastian Schulze; Thomas Wollmann; Stefan Dietzel
- Reference count: 40
- Primary result: Achieves functional LLM performance at effective 2-bit precision without training data

## Executive Summary
EntQuant introduces a data-free model compression method that achieves extreme compression rates by decoupling representational precision from storage efficiency. The method optimizes quantized weights for low entropy using L1-regularized optimization and employs GPU-accelerated entropy coding (ANS) to compress to arbitrary bit-rates while maintaining high-precision inference. Unlike traditional quantization, EntQuant can achieve effective bit-rates as low as 2 bits per parameter while preserving model functionality through entropy coding rather than sacrificing representational capacity.

## Method Summary
EntQuant compresses LLMs through entropy-constrained quantization without requiring training data. The method optimizes per-channel scaling parameters to minimize a rate-distortion Lagrangian (reconstruction error + λ×L1-norm), quantizes weights to Float8, and compresses them using ANS. During inference, weights are decompressed on-the-fly using GPU-accelerated ANS decoding, achieving effective bit-rates down to 2.1 bits while maintaining functional performance. The approach is extremely fast (under 30 minutes for a 70B model) and requires only the pre-trained model weights as input.

## Key Results
- Achieves functional performance at effective 2-bit precision on standard and instruction-tuned models
- Matches or exceeds data-dependent methods while requiring no calibration data
- Compression is extremely fast (under 30 minutes for a 70B model)
- Inference overhead is modest (1.5-2x slower than baseline)
- Maintains 34.61 unique values at 2-bit rate vs. 4 in fixed 2-bit quantization

## Why This Works (Mechanism)

### Mechanism 1
Decoupling precision from storage via entropy coding enables extreme compression without functional collapse. EntQuant maintains high-precision weight representation (Float8/Int8, 8 bits) during inference but optimizes weight distributions for low entropy. This allows lossless entropy coding (ANS) to compress to effective bit-rates below 4 bits/parameter (down to ~2 bits) while keeping full dynamic range and representational capacity. The core assumption is that GPU-based ANS decoding overhead is acceptable for the inference latency budget.

### Mechanism 2
L1-regularized optimization of scaling parameters serves as a differentiable proxy for entropy minimization, yielding low-entropy weight distributions without complex outlier handling. Instead of minimizing non-differentiable entropy directly, EntQuant solves a rate-distortion Lagrangian: minimize reconstruction error + λ × L1-norm. L1-norm empirically correlates with entropy and induces sparsity, reducing unique values and enabling better entropy coding. The core assumption is that L1-norm adequately approximates entropy reduction across LLM weight matrices.

### Mechanism 3
On-the-fly GPU-accelerated ANS decoding during inference limits latency overhead to ~1.5-2x baseline. Weights are stored in compressed bitstreams (one per transformer block). A dedicated GPU buffer decompresses all weights for the next block during the forward pass of the current block, using parallelized ANS from nvCOMP. The decompression is compute-bound and overlaps with compute kernels. The core assumption is sufficient GPU compute resources and memory bandwidth to hide decompression latency.

## Foundational Learning

- **Asymmetric Numeral Systems (ANS) for entropy coding**
  - Why needed: ANS is the core compression algorithm enabling on-the-fly GPU decompression with near-arithmetic coding efficiency but lower overhead
  - Quick check: Can you explain why ANS is preferred over Huffman coding for compressing LLM weights with non-power-of-two symbol probabilities?

- **Rate-distortion optimization (entropy-constrained quantization)**
  - Why needed: EntQuant formulates compression as a trade-off between reconstruction error and code length (entropy), using a Lagrangian objective
  - Quick check: How does the λ parameter control the balance between model fidelity and compression rate in EntQuant's optimization?

- **Straight-through estimator (STE) for quantization gradients**
  - Why needed: STE allows gradient-based optimization of quantization parameters (scales) through the non-differentiable quantization operation
  - Quick check: What is the core idea of STE, and why is it necessary for optimizing quantized weights with L-BFGS?

## Architecture Onboarding

- **Component map:** Pre-trained weights -> Scale optimization (L-BFGS) -> Float8 quantization -> ANS encoding -> Compressed bitstreams -> GPU buffer -> ANS decoding -> Float8 Marlin GEMM + FlashAttention
- **Critical path:** 1) Optimization of channel-wise scales (dominant compression time), 2) ANS encoding of quantized weights (fast, GPU-accelerated), 3) On-the-fly ANS decoding during inference (dominant latency overhead)
- **Design tradeoffs:** Float8 vs. Int8 base format; per-channel vs. per-group scaling; block-wise vs. layer-wise compression; target bit-rate (λ) selection
- **Failure signatures:** High perplexity/accuracy drop (λ too high or Int8 without super weight handling); CUDA OOM (decode buffer size underestimated); slow compression (suboptimal L-BFGS learning rate)
- **First 3 experiments:** 1) Compress LLaMA-2 7B with EntQuant at 3-bit target; compare C4 perplexity and LM-Eval average vs. HQQ/NF4 baselines, 2) Measure inference throughput and latency for EntQuant 2.1-bit vs. BFloat16 baseline on LLaMA-2 7B, 3) Ablate the L1-regularization strength (λ) to map the trade-off between effective bit-rate and model quality

## Open Questions the Paper Calls Out
- How does EntQuant perform on Mixture-of-Experts (MoE) architectures compared to the dense transformers evaluated in this study?
- Can the inference latency overhead be reduced via pipeline parallelism where one GPU decompresses weights while another executes the forward pass?
- To what extent can the inference throughput gap be closed by developing custom fused kernels that integrate ANS decoding directly with the matrix multiplication?
- Do more sophisticated differentiable proxies for entropy yield better rate-distortion trade-offs than the simplified ℓ1-norm used in the current optimization?

## Limitations
- Hardware dependencies on nvCOMP GPU-accelerated ANS implementation with uncertain performance on non-NVIDIA hardware
- Stability concerns for extreme compression (2-bit) across diverse tasks and model families
- Theoretical grounding of L1-norm as entropy proxy lacks rigorous justification for all possible weight distributions

## Confidence
- **High Confidence:** The core mechanism of entropy-constrained quantization via L1-regularized optimization is sound and well-supported
- **Medium Confidence:** The extreme compression claims (2-bit effective rate, functional performance) are supported by experiments on standard benchmarks
- **Low Confidence:** The stability and practical utility of 2-bit compression for production workloads, especially on diverse hardware, are not fully established

## Next Checks
1. **Cross-Platform Performance Validation:** Implement EntQuant on a non-nvCOMP platform and measure the inference slowdown compared to the claimed 1.5-2×
2. **Task Generalization Study:** Evaluate 2-bit EntQuant on a broader set of tasks beyond standard benchmarks, including code generation and mathematical reasoning
3. **Scalability Analysis:** Test memory-bandwidth limitations by profiling inference throughput on large batch sizes for a compressed 70B model