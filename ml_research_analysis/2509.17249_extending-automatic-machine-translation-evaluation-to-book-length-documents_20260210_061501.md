---
ver: rpa2
title: Extending Automatic Machine Translation Evaluation to Book-Length Documents
arxiv_id: '2509.17249'
source_url: https://arxiv.org/abs/2509.17249
tags:
- translation
- sentence
- alignment
- metrics
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating machine translation
  quality for long documents, where existing metrics are constrained by token limits
  and rigid sentence boundary requirements. The authors propose SEGALE, a scheme that
  extends sentence-level metrics to long documents by using sentence segmentation
  and alignment methods, handling under- and over-translations, and variable sentence
  boundaries.
---

# Extending Automatic Machine Translation Evaluation to Book-Length Documents

## Quick Facts
- arXiv ID: 2509.17249
- Source URL: https://arxiv.org/abs/2509.17249
- Authors: Kuang-Da Wang; Shuoyang Ding; Chao-Han Huck Yang; Ping-Chun Hsieh; Wen-Chih Peng; Vitaly Lavrukhin; Boris Ginsburg
- Reference count: 21
- One-line primary result: SEGALE extends sentence-level MT metrics to long documents by aligning segmented source and target texts, handling under/over-translations with fixed penalties, and averaging segment-level scores to achieve near-ideal correlation with human judgments.

## Executive Summary
This paper addresses the challenge of evaluating machine translation quality for long documents, where existing metrics are constrained by token limits and rigid sentence boundary requirements. The authors propose SEGALE, a scheme that extends sentence-level metrics to long documents by using sentence segmentation and alignment methods, handling under- and over-translations, and variable sentence boundaries. SEGALE applies to arbitrary-length translations by aligning segmented source and target texts, using null alignments with fixed penalties for translation errors, and averaging segment-level scores.

Experiments show SEGALE achieves near-ideal correlation with human judgments across over-translation, under-translation, and flexible boundary scenarios, significantly outperforming mwerSegmenter while matching gold-standard performance. The method also enables evaluation of book-length translations, revealing that many open-weight LLMs fail to translate effectively at their reported context lengths due to rising errors with longer inputs.

## Method Summary
SEGALE extends sentence-level machine translation metrics to long documents through a three-step process: sentence segmentation using sent2span, segment alignment with a variant of TERp's alignment algorithm, and segment-level metric computation with null alignment handling. The method segments both source and target texts, aligns corresponding segments while allowing for flexible boundaries, and computes average scores across all segments. Under-translations and over-translations are handled through null alignments with fixed penalties, while variable sentence boundaries are addressed through dynamic programming-based alignment that allows non-consecutive segment pairs. The approach works with any existing sentence-level metric and can evaluate translations of arbitrary length by breaking them into manageable segments.

## Key Results
- SEGALE achieves near-ideal correlation with human judgments across over-translation, under-translation, and flexible boundary scenarios
- Significantly outperforms mwerSegmenter while matching gold-standard performance on MT evaluation tasks
- Enables evaluation of book-length translations, revealing context length limitations in open-weight LLMs

## Why This Works (Mechanism)
SEGALE works by breaking the evaluation problem into manageable pieces through segmentation and alignment. By segmenting long documents into sentences or phrases and then aligning these segments between source and target texts, the method reduces the computational complexity of evaluating long sequences. The alignment algorithm's ability to handle flexible boundaries and null alignments allows it to capture translation errors that occur when models under-translate, over-translate, or restructure sentences differently than the source. The averaging of segment-level scores provides a stable overall evaluation that reflects the translation quality across the entire document.

## Foundational Learning

Sentence Segmentation: Breaking text into individual sentences or phrases
- Why needed: Long documents must be processed in smaller units to apply existing sentence-level metrics
- Quick check: Verify that sent2span correctly handles complex sentence structures, abbreviations, and language-specific punctuation

Segment Alignment: Matching corresponding segments between source and target texts
- Why needed: Translations often restructure sentences or omit/add content, requiring flexible matching
- Quick check: Ensure alignment algorithm correctly handles under-translations, over-translations, and flexible boundaries

Null Alignment: Handling cases where segments in one text have no corresponding segment in the other
- Why needed: Captures translation errors where content is missing or added
- Quick check: Verify fixed penalty appropriately reflects severity of missing or extra content

Dynamic Programming: Algorithm for finding optimal alignment between segment sequences
- Why needed: Efficiently finds best matching between potentially many segments
- Quick check: Confirm algorithm correctly handles non-consecutive segment pairs for flexible boundaries

Segment-Level Metrics: Computing quality scores for individual text segments
- Why needed: Enables evaluation of smaller units that can be aggregated for overall score
- Quick check: Validate that chosen metrics (TER, BLEU, etc.) perform well at segment level

Averaging Scheme: Combining individual segment scores into overall document score
- Why needed: Provides stable evaluation across entire long document
- Quick check: Test that averaging appropriately weights segments and handles varying segment lengths

## Architecture Onboarding

Component Map: sent2span -> alignment algorithm -> null alignment handling -> segment scoring -> averaging

Critical Path: Document -> sent2span segmentation -> dynamic programming alignment -> null alignment penalties -> segment-level metric computation -> average score

Design Tradeoffs: Fixed penalty for null alignments vs. variable penalties based on content; strict vs. flexible boundary handling; computational efficiency vs. alignment accuracy

Failure Signatures: Poor correlation with human judgments when: segmentation fails on complex structures, alignment misses correct segment pairs, null penalties don't reflect actual translation quality, averaging scheme is dominated by outlier segments

Three First Experiments:
1. Test segmentation accuracy on documents with complex sentence structures and varied punctuation
2. Evaluate alignment performance on known under-translation and over-translation cases
3. Compare averaging schemes (arithmetic, weighted, harmonic) on documents with varying segment quality distribution

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focused on English-to-Spanish and English-to-Chinese translation pairs, limiting generalizability to other language families
- Fixed penalty approach for null alignments may not capture nuanced translation errors in creative or adaptive translation strategies
- Reported context length limitations for open-weight LLMs may reflect implementation choices rather than fundamental architectural constraints

## Confidence
High confidence in core methodology and experimental results for English-Spanish and English-Chinese language pairs tested
Medium confidence in broader applicability claims and context length findings beyond primary experimental scope
Low confidence in universal applicability of null alignment penalty scheme and performance on non-European and non-East Asian languages

## Next Checks
1. Evaluate SEGALE on additional language pairs spanning different linguistic families, including low-resource languages and languages with non-Latin scripts, to assess cross-linguistic robustness
2. Test the method on domain-specific translation tasks (legal documents, technical manuals, literary works) to validate performance across different text types and register variations
3. Compare SEGALE's context length findings with different implementation strategies for long-context processing in LLMs, including sliding window approaches and hierarchical processing, to distinguish between architectural and implementation limitations