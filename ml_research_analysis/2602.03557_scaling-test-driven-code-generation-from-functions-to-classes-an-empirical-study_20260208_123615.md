---
ver: rpa2
title: 'Scaling Test-Driven Code Generation from Functions to Classes: An Empirical
  Study'
arxiv_id: '2602.03557'
source_url: https://arxiv.org/abs/2602.03557
tags:
- generation
- methods
- code
- tests
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper scales test-driven code generation from functions to
  classes by introducing a dependency-aware TDD framework. The method first analyzes
  intra-class method dependencies to derive a generation schedule, then incrementally
  implements each method using method-level public tests with reflection-style execution
  feedback and bounded repair iterations.
---

# Scaling Test-Driven Code Generation from Functions to Classes: An Empirical Study

## Quick Facts
- arXiv ID: 2602.03557
- Source URL: https://arxiv.org/abs/2602.03557
- Reference count: 40
- This paper scales test-driven code generation from functions to classes by introducing a dependency-aware TDD framework

## Executive Summary
This paper scales test-driven code generation from functions to classes by introducing a dependency-aware TDD framework. The method first analyzes intra-class method dependencies to derive a generation schedule, then incrementally implements each method using method-level public tests with reflection-style execution feedback and bounded repair iterations. To support this approach, the authors construct ClassEval-TDD, a cleaned and standardized benchmark with consistent specifications and deterministic tests. Experiments across eight LLMs show that the class-level TDD framework consistently improves class-level correctness by 12-26 absolute points and achieves up to 71% fully correct classes, while requiring only a small number of repairs on average. These results demonstrate that test-driven generation can effectively scale beyond isolated functions and substantially improve class-level code generation reliability.

## Method Summary
The method scales test-driven code generation to classes through a three-phase approach: dependency analysis to create a generation schedule, incremental TDD implementation of each method using public tests as executable specifications, and reflection-based repair when tests fail. The framework first prompts the LLM to analyze intra-class method dependencies and generate a topological schedule, then implements methods in order using their public tests for guidance. When tests fail, a structured reflection process identifies root causes and applies targeted repairs, with a maximum of three repair attempts per method. The approach is evaluated on ClassEval-TDD, a benchmark constructed from ClassEval that standardizes specifications and separates public tests (for generation guidance) from private tests (for final evaluation).

## Key Results
- Class-level TDD framework improves class-level correctness by 12-26 absolute points across eight LLMs
- Achieves up to 71% fully correct classes compared to baseline approaches
- Requires only 0.62-1.03 repairs per method on average, with most failures converging within 1-2 rounds
- Demonstrates that test-driven generation can effectively scale beyond isolated functions

## Why This Works (Mechanism)

### Mechanism 1: Dependency-Aware Scheduling Reduces Structural Failures
- Claim: Implementing methods in a dependency-respecting order reduces compilation/runtime failures caused by missing prerequisites, enabling meaningful test execution during incremental synthesis.
- Mechanism: The framework first prompts the LLM to analyze intra-class method call dependencies, then generates a topological schedule. Callee methods are implemented before caller methods, preventing missing-attribute or undefined-method errors that would otherwise break the test-driven loop.
- Core assumption: LLMs can infer call dependencies from method signatures, docstrings, and method names with sufficient accuracy; when dependency inference is wrong, the schedule may still fail.
- Evidence anchors:
  - [abstract] "first analyzes intra-class method dependencies to derive a generation schedule"
  - [section 4.3] "implement prerequisite (callee) methods before dependent (caller) methods"
  - [corpus] Related work on dependency-aware generation exists but primarily at function level; class-level scheduling is less explored.
- Break condition: If ground-truth dependencies contradict semantic priors (e.g., "add" intuitively before "restock"), models may propose invalid orders even with high dependency F1 (Finding 8).

### Mechanism 2: Method-Level Public Tests Provide Localized, Executable Constraints
- Claim: Providing method-specific test cases during generation creates tighter feedback loops than class-level specifications, enabling earlier error detection and localized repair.
- Mechanism: Each target method is generated with its own public test suite as executable specifications. Tests are run immediately after generation; failures trigger reflection-based repair. This constrains the search space per method rather than for the entire class at once.
- Core assumption: Method-level tests are available and correctly specified; if tests are incomplete or incorrect, repair may chase spurious failures.
- Evidence anchors:
  - [abstract] "incrementally implements each method under method-level public tests with reflection-style execution feedback"
  - [section 4.4] "use its public method-level tests as executable specifications to guide generation and validate correctness"
  - [corpus] TENET and related work emphasize using tests beyond validation, but primarily for function-level tasks.
- Break condition: If method-level tests have coverage gaps or incorrect oracles, the TDD loop may converge on incorrect implementations that pass tests but fail class-level invariants.

### Mechanism 3: Reflection-Based Repair Improves Targeted Correction Over Raw Error Feedback
- Claim: Explicitly separating failure analysis from patch generation produces more targeted repairs than directly patching from error messages, especially for smaller models.
- Mechanism: When tests fail, the model first reflects to identify the likely cause and responsible code region, then proposes a repair plan, and finally generates a minimal patch. This structured diagnosis avoids blind trial-and-error.
- Core assumption: The model can accurately diagnose root causes from limited failure signals; reflection quality scales with model capability.
- Evidence anchors:
  - [abstract] "reflection-style execution feedback and bounded repair iterations"
  - [section 4.5] "structured Reflexion-style repair procedure that separates diagnosis from patch synthesis"
  - [section 6.3.4, Table 7] Ablation shows reflection removal reduces class_success by 1–4 points, with larger impact on smaller models.
  - [corpus] Self-Refine and Reflexion frameworks support iterative self-feedback, but evidence for class-level contexts is limited.
- Break condition: If reflection misattributes cause (e.g., blaming wrong code region), patches may introduce regressions or fail to converge within bounded repair budget.

## Foundational Learning

- Concept: **Topological Ordering of Dependencies**
  - Why needed here: Class methods often call each other; generating caller before callee creates unresolved references. A valid topological order ensures all dependencies are satisfied before use.
  - Quick check question: Given methods A calls B, B calls C, what generation order avoids missing-method errors?

- Concept: **Test-Driven Development (TDD) Loop**
  - Why needed here: The framework generalizes TDD from functions to classes, treating tests as executable specs. Understanding the red-green-refactor cycle helps grasp why incremental test-guided generation improves reliability.
  - Quick check question: Why might a method pass unit tests but still contribute to class-level failure?

- Concept: **Composition Gap in Class-Level Synthesis**
  - Why needed here: Even when individual methods are correct, the class may fail due to cross-method state inconsistencies, shared invariants, or undocumented interactions. This gap explains why class_success remains lower than fun_success.
  - Quick check question: A class has three methods that each pass their tests, but the class test fails. What might cause this?

## Architecture Onboarding

- Component map:
  - Dependency Analyzer -> TDD Generation Loop -> Reflection-Based Repair
  - ClassEval-TDD Benchmark provides skeletons, public tests, and private tests

- Critical path:
  1. Input: Class skeleton + method docstrings + method-level public tests
  2. Dependency analysis → generation schedule (topological order)
  3. For each method in order:
     - Construct context (partial class, method signature, public tests)
     - Generate implementation
     - Execute public tests
     - If fail → reflection → repair (up to 3 rounds)
  4. Output: Complete class implementation
  5. Evaluate against private test suite

- Design tradeoffs:
  - **Over- vs under-approximation of dependencies**: Models tend to over-approximate (extra_deps), which is safer but may introduce unnecessary ordering constraints and delay implementation.
  - **Repair budget vs correctness**: Higher budget may improve convergence but increases cost; experiments show most failures converge within 1–2 rounds for strong models.
  - **Public vs private test separation**: Public tests guide generation; private tests evaluate final correctness. This reduces overfitting but requires maintaining two test suites.
  - **Incremental vs holistic generation**: Incremental enables localized repair but accumulates context; holistic avoids context drift but lacks intermediate feedback.

- Failure signatures:
  - **Topological order violation**: Schedule places caller before callee; tests fail with AttributeError or undefined method (Finding 5).
  - **Composition gap failure**: All methods pass unit tests but class-level test fails due to shared state inconsistency (Finding 10).
  - **Repair budget exhaustion**: Method still fails after 3 repair rounds; common for smaller models (Table 6: qwen2.5-coder-7B avg 0.62 repairs/method, 2.6 rounds for failing methods).
  - **Benchmark-induced false negatives**: Non-deterministic tests or specification mismatches cause failures unrelated to model capability (Finding 1).

- First 3 experiments:
  1. **Dependency inference accuracy baseline**: Run dependency analyzer on ClassEval-TDD without generation; measure precision, recall, F1 against ground-truth dependency edges to establish scheduling reliability (replicate Table 2).
  2. **TDD vs best baseline per model**: For each model, compare class_success of TDD framework against max(Holistic, Incremental, Compositional) to quantify improvement range and identify where gains are largest (replicate Table 4).
  3. **Repair cost characterization**: Track repair rounds per method and per class; identify which methods/classes require multiple repairs and correlate with dependency complexity or method characteristics (replicate Table 6 analysis).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the class-level TDD framework be generalized to multi-file software systems or languages with different object-oriented semantics (e.g., Java, C++)?
- **Basis in paper:** [explicit] The authors state in the Threats to Validity (Section 9) that the study is limited to single-class Python tasks and "future work should investigate class-level TDD in... multi-file systems."
- **Why unresolved:** The current benchmark (ClassEval-TDD) and framework focus strictly on isolated single-class Python tasks, leaving cross-file dependencies and static typing complexities unexplored.
- **What evidence would resolve it:** An empirical evaluation of the framework applied to multi-file repositories (e.g., using a benchmark like SWE-bench) or class-level benchmarks in Java/C++ showing comparable correctness improvements.

### Open Question 2
- **Question:** How can class-level TDD be effectively adapted for real-world settings where method-level public tests are not pre-existing?
- **Basis in paper:** [explicit] The authors note in the External Validity section that "we assume the availability of method-level public tests, which may not be present in all real-world development settings."
- **Why unresolved:** The current framework relies on "method-level public tests" as executable specifications; it is unclear if the model can generate these tests reliably enough *during* the process to maintain the same correctness gains.
- **What evidence would resolve it:** A study where the model must generate the public tests before implementation (TDD), measuring the resulting class correctness against the current upper-bound where tests are provided.

### Open Question 3
- **Question:** Can lightweight verification or post-processing mechanisms effectively correct LLM topological order violations caused by semantic priors?
- **Basis in paper:** [explicit] Section 7.2 suggests the gap between dependency prediction and scheduling validity "highlights an opportunity for lightweight verification or post-processing (e.g., checking the predicted order...)."
- **Why unresolved:** The authors intentionally avoided correcting the schedule to isolate model behavior, leaving the proposed solution (post-processing) untested.
- **What evidence would resolve it:** An ablation study implementing a verification step that re-orders the generation schedule based on the predicted dependency graph, measuring the reduction in topological violations and class failures.

### Open Question 4
- **Question:** How can the "composition gap" (the discrepancy between method-level and class-level success) be reduced given that method-level correctness saturates quickly?
- **Basis in paper:** [inferred] Finding 9 notes that method-level correctness approaches saturation (90–92%) while class-level success remains substantially lower, highlighting a "remaining composition gap driven by cross-method state and consistency constraints."
- **Why unresolved:** The current TDD loop validates methods in isolation or via existing public tests but struggles to enforce consistency of shared state and invariants across the entire class simultaneously.
- **What evidence would resolve it:** Introduction of a "global state invariant" check or a final class-level integration pass in the TDD loop that specifically targets cross-method inconsistencies, showing a statistically significant reduction in the gap.

## Limitations

- **Benchmark quality issues**: ClassEval-TDD contains non-deterministic tests, specification mismatches, and unclear prompts that could artificially depress model performance
- **Dependency prediction over-approximation**: Models tend to predict extra dependencies, introducing unnecessary ordering constraints that may delay implementation
- **Single-class limitation**: Framework currently applies only to isolated single-class Python tasks, not multi-file systems or other object-oriented languages

## Confidence

- **High**: Class-level TDD improves correctness over function-level baselines (Finding 3, Table 4)
- **Medium**: Dependency-aware scheduling reduces structural failures (Finding 5, Figure 4)
- **Low**: Reflection-based repair provides consistent advantage across model sizes (Table 7, Finding 6)

## Next Checks

1. **Scheduler robustness test**: Run dependency analyzer on ClassEval-TDD tasks without generation to establish baseline accuracy. Measure precision, recall, and F1 against ground-truth dependency graphs to quantify scheduling reliability.
2. **Cross-dataset generalization**: Apply TDD framework to an independent benchmark (e.g., MBPP or APPS) to verify improvements aren't dataset-specific. Track class_success and repair counts across domains.
3. **Repair budget sensitivity analysis**: Systematically vary repair iteration limits (1, 2, 3, 5) across model sizes. Identify break-even points where additional repairs cease providing meaningful accuracy gains relative to computational cost.