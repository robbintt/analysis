---
ver: rpa2
title: 'DiffEx: Explaining a Classifier with Diffusion Models to Identify Microscopic
  Cellular Variations'
arxiv_id: '2502.09663'
source_url: https://arxiv.org/abs/2502.09663
tags:
- images
- image
- classifier
- diffusion
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents DiffEx, a method for explaining classifier
  decisions by generating visually interpretable attributes using diffusion models.
  The approach constructs a classifier-aware semantic latent space and identifies
  interpretable directions using contrastive learning.
---

# DiffEx: Explaining a Classifier with Diffusion Models to Identify Microscopic Cellular Variations

## Quick Facts
- arXiv ID: 2502.09663
- Source URL: https://arxiv.org/abs/2502.09663
- Authors: Anis Bourou; Saranga Kingkor Mahanta; Thomas Boyer; Valérie Mezger; Auguste Genovesio
- Reference count: 26
- One-line primary result: Method identifies interpretable directions for classifier explanations on both natural (FFHQ gender) and biological (BBBC021, Golgi) datasets, outperforming GCD in disentanglement and visual quality

## Executive Summary
DiffEx presents a novel approach for explaining classifier decisions by generating visually interpretable attributes using diffusion models. The method constructs a classifier-aware semantic latent space and identifies interpretable directions using contrastive learning. Demonstrated on both natural and biological image datasets, DiffEx successfully uncovers subtle cellular variations induced by treatments while maintaining high classification accuracy on generated images (up to 100% on BBBC021) and low reconstruction errors (SSIM > 0.99 on BBBC021). The approach produces more disentangled and visually superior explanations compared to GCD baseline.

## Method Summary
DiffEx works by first training an encoder-DDIM pipeline conditioned on both image embeddings and classifier outputs to create a semantic latent space that preserves class-relevant information. It then learns direction models through contrastive learning to identify disentangled semantic directions in this space. Finally, it ranks these directions by their ability to change classifier predictions, producing interpretable visual explanations. The method is demonstrated on FFHQ (gender classification), BBBC021 (MCF-7 cells with Latrunculin B treatment), and Golgi (HeLa cells with Nocodazole treatment) datasets, showing both natural and biological applications.

## Key Results
- Achieved 100% classification accuracy on generated BBBC021 images while maintaining SSIM > 0.99
- Successfully identified gender-related attributes on FFHQ including hairstyles and makeup
- Uncovered subtle cellular variations on BBBC021 and Golgi datasets including changes in cytoplasm, nuclei count, and Golgi apparatus distribution
- Outperformed GCD baseline in producing more disentangled and visually superior explanations
- Maintained classifier-relevant attributes through semantic conditioning with KL divergence loss

## Why This Works (Mechanism)

### Mechanism 1: Classifier-Aware Semantic Conditioning
The method forces the generative model to encode features relevant to the classifier by conditioning the diffusion process on classification scores. An encoder maps an input image to a latent code, which is concatenated with the pre-trained classifier's output probability to form a semantic code. A KL divergence loss penalizes the reconstruction if its classification score diverges from the original input, binding the latent representation to class-determinative features. Break condition: If reconstruction loss dominates classifier loss, the model may reconstruct perfectly but fail to preserve class-conditional information.

### Mechanism 2: Contrastive Disentanglement of Directions
Interpretable directions corresponding to specific attributes are isolated by forcing directions to be distinct from one another. The method learns direction models (MLPs) that shift the latent code, using a contrastive loss where shifts from the same direction model act as positive pairs while shifts from different models are negative pairs. A covariance regularization term further decorrelates the directions. Break condition: If the latent space is poorly structured, learned directions may entangle multiple attributes, reducing interpretability.

### Mechanism 3: Attribution via Probabilistic Shift
The importance of a visual attribute is validated by its ability to change the classifier's output probability. After identifying candidate directions, the system shifts input images along these vectors and measures the average change in the classifier's predicted probability. Directions that cause the largest shift in class confidence are ranked as primary explanations. Break condition: If the classifier relies on non-robust features, identified directions might maximize probability change without corresponding to human-interpretable visual concepts.

## Foundational Learning

- **Concept: Diffusion Autoencoders (DiffAE)**
  - Why needed: DiffEx extends DiffAE architecture to build semantic latent space
  - Quick check: How does z_sem condition the U-Net denoiser, and what happens if z_sem is set to zero?

- **Concept: Contrastive Learning (InfoNCE/VICReg)**
  - Why needed: Discovery of disentangled directions relies entirely on contrastive objective
  - Quick check: What defines positive vs negative pairs in latent space traversal, and how does loss force them apart?

- **Concept: Counterfactual Explanation**
  - Why needed: Output of DiffEx is set of counterfactuals (images altered to cross decision boundary)
  - Quick check: If classifier predicts "Treated," what does counterfactual explanation look like vs gradient-based saliency map?

## Architecture Onboarding

- **Component map:** Pre-trained Classifier -> Semantic Encoder -> DDIM/Diffusion Model -> Direction Models (MLPs) -> Contrastive Learner
- **Critical path:** 1) Train Encoder/DDIM with KL Classifier Loss to capture class-relevant info; 2) Freeze Encoder/DDIM and train Direction Models using contrastive learning; 3) Execute Ranking Loop to identify directions that most alter C(x)
- **Design tradeoffs:** High λ1 ensures fidelity to class but might distort latent space geometry; high regularization λ2 ensures directions are distinct but may prevent discovery of subtle, correlated phenotypes
- **Failure signatures:** Mode collapse (all directions produce same change); attribute entanglement (one edit changes multiple attributes); zero shift (direction models output near-zero vectors)
- **First 3 experiments:** 1) Verify reconstruction quality and classification accuracy before direction discovery; 2) Visualize effect of single direction by sweeping shift weight from -3 to +3; 3) Disable regularization term and observe if directions become correlated

## Open Questions the Paper Calls Out

### Open Question 1
Why does DiffEx fail to produce disentangled directions in datasets with limited imaging channels? The Golgi dataset shows all identified directions replicate exactly the same phenotypes, hypothesized to be due to limited number of channels (green and blue only). The authors observe this lack of diversity but do not validate by testing on multi-channel variants of the same biological data.

### Open Question 2
Can the current contrastive learning framework be adapted to explain multi-class classifiers effectively? The paper evaluates exclusively on binary classification tasks and defines explanation goal as shifting toward "opposite class." While theoretically applicable to multi-class settings, the ranking algorithm and visualization strategy are demonstrated only for binary transitions.

### Open Question 3
How does the method distinguish between actual subtle phenotypes and artifacts inherent to diffusion reconstruction? The paper highlights ability to uncover "subtle cellular variations" but relies on known biological mechanisms for validation. Without controlled study on unknown phenotypes, it's difficult to ascertain if model is hallucinating features that satisfy classifier but don't exist biologically.

## Limitations
- Limited biological validation: Relies primarily on visual inspection rather than quantitative biological metrics or expert validation
- Channel dependency: Performance degrades on datasets with limited imaging channels (e.g., Golgi dataset)
- Binary focus: Methodology and evaluation demonstrated exclusively on binary classification tasks

## Confidence

- **High confidence**: Classifier-aware semantic conditioning through KL divergence loss is well-specified and theoretically sound; demonstration of high reconstruction quality (SSIM > 0.99) and classification accuracy (95-100%) is well-supported
- **Medium confidence**: Contrastive disentanglement approach is conceptually clear but practical implementation details and hyperparameter sensitivity are not fully explored; ranking procedure appears reasonable but lacks extensive validation
- **Low confidence**: Biological interpretation of discovered cellular variations is primarily qualitative; lacks quantitative validation from domain experts or statistical tests confirming biological meaningfulness

## Next Checks

1. **Latent space curvature analysis**: Systematically vary shift weight α along discovered directions and measure smoothness of transition using LPIPS distances between consecutive steps to validate local linearity assumption and quantify disentanglement quality.

2. **Classifier robustness evaluation**: Test whether identified directions exploit non-robust features by evaluating classifier on adversarially perturbed versions of original images; if directions significantly change predictions but have minimal effect on robust classifiers, this indicates spurious correlations.

3. **Biological expert validation study**: Conduct blinded study with cell biology experts to rate biological plausibility and novelty of discovered variations on BBBC021 and Golgi datasets; compare against variations discovered through traditional image analysis methods to quantify added value.