---
ver: rpa2
title: Complexity Control Facilitates Reasoning-Based Compositional Generalization
  in Transformers
arxiv_id: '2501.08537'
source_url: https://arxiv.org/abs/2501.08537
tags:
- initialization
- data
- anchor
- generalization
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how complexity control influences the reasoning
  capabilities of transformer models in compositional tasks. By systematically adjusting
  parameter initialization scales and weight decay coefficients, the authors demonstrate
  that small initialization scales and larger weight decay coefficients guide models
  toward reasoning-based solutions, enabling better out-of-distribution (OOD) generalization.
---

# Complexity Control Facilitates Reasoning-Based Compositional Generalization in Transformers

## Quick Facts
- arXiv ID: 2501.08537
- Source URL: https://arxiv.org/abs/2501.08537
- Authors: Zhongwang Zhang; Pengxiao Lin; Zhiwei Wang; Yaoyu Zhang; Zhi-Qin John Xu
- Reference count: 40
- Primary result: Small initialization scales and larger weight decay coefficients guide transformers toward reasoning-based solutions that generalize out-of-distribution

## Executive Summary
This study investigates how complexity control influences the reasoning capabilities of transformer models in compositional tasks. By systematically adjusting parameter initialization scales and weight decay coefficients, the authors demonstrate that small initialization scales and larger weight decay coefficients guide models toward reasoning-based solutions, enabling better out-of-distribution (OOD) generalization. Through masking strategies and complexity analysis, they reveal that reasoning-based solutions exhibit lower complexity bias, characterized by neuron condensation and structured word embeddings. These findings are validated across multiple real-world tasks, including diffusion models and natural language processing, showing consistent improvements in OOD generalization.

## Method Summary
The paper introduces a framework for controlling model complexity through initialization scale (parameter γ) and weight decay coefficient (λ). The authors construct synthetic compositional tasks using anchor functions (e.g., g(x;a)=x+5, g(x;b)=x+1) and train transformers to solve sequences like x + 5 + 1. They systematically vary γ and λ to observe three distinct learning phases: Phase 1 (memorization), Phase 2 (composite mappings), and Phase 3 (reasoning-based solutions). The approach uses masking strategies to analyze internal mechanisms and employs stable rank as a complexity measure to quantify model behavior across phases.

## Key Results
- Small initialization scales (high γ) combined with larger weight decay coefficients guide models toward reasoning-based solutions
- Reasoning-based solutions exhibit neuron condensation where input weights cluster along few isolated directions
- Structured word embedding organizations in reasoning solutions encode systematic relationships enabling compositional generalization
- Phase 3 reasoning solutions achieve high ID accuracy while maintaining strong OOD generalization (>50%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Small initialization scales combined with larger weight decay coefficients guide transformers toward reasoning-based solutions that generalize out-of-distribution, while large initialization leads to memory-based solutions.
- Mechanism: The initialization rate γ controls parameter initialization scale (sampling from N(0, (1/d_in^γ)²)). Higher γ yields smaller initialization. Combined with weight decay, this creates a complexity control regime that constrains the model to learn simpler, rule-based representations rather than memorizing complex input-output mappings.
- Core assumption: The paper hypothesizes that lower complexity bias is the key factor enabling reasoning-based solutions to learn compositional rules, though this causal link is inferred from correlation between condensation patterns and generalization performance.

### Mechanism 2
- Claim: Reasoning-based solutions exhibit neuron condensation where input weights cluster along a few isolated directions, reducing effective model complexity.
- Mechanism: In Phase 3, query weight matrices show high cosine similarity between neurons, indicating they cluster into distinct groups. This condensation effectively reduces the network to a lower-dimensional structure while maintaining task performance. The paper quantifies this via stable rank: R_stable(A) = ||A||²_F / ||A||²_2, which decreases as reasoning ability increases.
- Core assumption: Assumption: Condensation causes rather than merely accompanies better generalization—this directional claim is inferred from observational data.

### Mechanism 3
- Claim: Reasoning-based solutions develop structured word embedding organizations that encode systematic relationships between tokens, enabling compositional generalization.
- Mechanism: In Phase 3, PCA visualization reveals token embeddings arranged in regular, structured patterns rather than scattered randomly. The ordinal relationships reflect underlying compositional rules (e.g., intervals of 3 and 4 corresponding to arithmetic operations). This structure allows the model to apply learned rules to unseen token combinations.
- Core assumption: Assumption: The structured embedding geometry directly enables OOD generalization rather than being an epiphenomenon of successful training.

## Foundational Learning

- Concept: **Compositional Generalization**
  - Why needed here: The entire paper frames the problem as whether models learn primitive-level rules that compose (reasoning) versus memorizing holistic mappings. Understanding what compositional generalization means—applying known primitives to novel combinations—is essential for interpreting the three-phase framework.
  - Quick check question: Can you distinguish between a model that memorizes "skip backwards" as a unit versus one that learns "skip" + "backwards" as composable primitives?

- Concept: **Neural Network Condensation**
  - Why needed here: The paper relies on condensation as a complexity indicator. Condensation occurs when neuron weights align in similar directions, effectively reducing representational capacity. This concept is central to why complexity control works.
  - Quick check question: If two neurons have weight vectors with cosine similarity of 0.95, what does this imply about the effective dimensionality of that layer?

- Concept: **Stable Rank as Complexity Measure**
  - Why needed here: The paper uses stable rank to unify observations about weight condensation and embedding structure. Stable rank captures "effective dimensionality" better than plain rank for continuous matrices.
  - Quick check question: Why might stable rank be preferable to standard matrix rank when analyzing learned neural network weights?

## Architecture Onboarding

- Component map:
  - One-hot input -> word embedding (W_em) -> add positional encoding (X_pos)
  - Attention Block (per layer): X^(l) -> Q, K, V projections -> attention matrix (softmax(QK^T/√d_k) with mask) -> weighted value aggregation -> output projection
  - Post-Attention: LayerNorm -> MLP -> residual addition -> LayerNorm -> X^(l+1)
  - Final layer output -> linear projection -> softmax -> predicted token
  - Key Experimental Components: Initialization scale parameter γ, weight decay coefficient λ, anchor pair designation in data

- Critical path:
  1. Data construction: Define anchor functions (g(x; a) = x+5, g(x; b) = x+1, etc.) and compose them into anchor pairs
  2. Dataset partitioning: Training/ID-test use seen anchor pairs; OOD-test uses unseen pairs like (c,d), (d,c)
  3. Model initialization: Sample weights from N(0, (1/d_in^γ)²) with γ controlling scale
  4. Training: Cross-entropy loss on final token, AdamW optimizer with specified weight decay
  5. Evaluation: Compare ID accuracy vs OOD accuracy to determine phase classification

- Design tradeoffs:
  - Small initialization (high γ): Favors reasoning solutions and OOD generalization but may slow convergence on high-complexity data
  - Large initialization (low γ): Faster memorization but poor OOD transfer
  - High weight decay: Encourages lower complexity solutions but may underfit on tasks requiring more expressive capacity
  - Single-head vs multi-head: Paper uses single-head for mechanism clarity, but findings transfer to GPT-2 multi-head architecture

- Failure signatures:
  - Phase 1 indicator: ID accuracy <90%, OOD accuracy low, embeddings unstructured, no condensation
  - Phase 2 indicator: ID accuracy high (>90%), OOD accuracy low, poor commutativity on symmetric pairs
  - Phase 3 desired: ID accuracy high, OOD accuracy high (>50%), strong commutativity, structured embeddings, significant condensation
  - Training instability: Very small initialization may cause training to stall; very large weight decay may prevent convergence

- First 3 experiments:
  1. Replicate phase transition: Train GPT-2 (10 layers, 10 heads) on the anchor function dataset with varying γ (0.2, 0.5, 0.8) at fixed weight decay (0.01). Verify that γ=0.2 yields Phase 1, γ=0.5 yields Phase 2, and γ=0.8 yields Phase 3 behavior as measured by ID/OOD accuracy.
  2. Mechanism probing via masking: Train a 2-layer single-head transformer with strong complexity control (γ=0.8, WD=0.1). Apply key token masking and second anchor masking to verify that Phase 3 models show high cosine similarity for inputs with same g(x; a₁) values, confirming stepwise reasoning.
  3. Condensation and stable rank analysis: For models across the γ spectrum, compute cosine similarity matrices of W_Q^(1) weights and calculate stable ranks. Confirm correlation between lower stable rank, higher condensation, and better OOD generalization.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can Retrieval-Augmented Generation (RAG) effectively mitigate the trade-off where complexity control enhances reasoning at the expense of memorization capabilities?
- **Basis in paper:** [explicit] The authors state in the "Limitation and Future Work" section that enhancing the model's preference for reasoning may partially compromise its memorization, and they explicitly propose leveraging RAG techniques to compensate for this diminished capacity.
- **Why unresolved:** While the paper identifies the trade-off and hypothesizes that offloading memory to external systems could work, it does not provide experimental validation showing that RAG successfully restores specific factual recall without interfering with the reasoning improvements gained from complexity control.
- **What evidence would resolve it:** Experiments demonstrating that models trained with strong complexity control plus RAG achieve comparable factual accuracy to standard models while maintaining the superior OOD compositional generalization reported in the paper.

### Open Question 2
- **Question:** Do the internal mechanisms of neuron condensation and structured embedding organization observed in small synthetic models persist and drive reasoning in Large Language Models (LLMs)?
- **Basis in paper:** [explicit] The authors note in "Limitation and Future Work" that their mechanism analysis was "confined to synthetic data and smaller-scale models" and that this constraint may not fully capture the intricacies of large-scale real-world models.
- **Why unresolved:** The paper establishes a link between low stable rank (condensation) and reasoning in controlled settings, but it remains unproven whether these specific structural signatures are the primary drivers of generalization in massive, heterogeneous models where training dynamics are more complex.
- **What evidence would resolve it:** An analysis of internal weights in large-scale pretrained models showing a correlation between high compositional generalization performance, low stable rank, and the specific embedding structures identified in the paper's Phase 3 solutions.

### Open Question 3
- **Question:** Can Mixture of Experts (MoE) architectures be optimized by applying varying initialization scales to different experts to specialize them for reasoning versus memorization?
- **Basis in paper:** [explicit] The authors suggest in "Limitation and Future Work" that future research could involve "leveraging Mixture of Experts (MoEs) to design networks with varying initialization scales for different expert models."
- **Why unresolved:** It is currently unknown how to effectively segregate initialization strategies within a single MoE model—specifically, whether assigning small initialization (reasoning bias) to some experts and large initialization (memory bias) to others yields a system that outperforms uniform initialization.
- **What evidence would resolve it:** Training results from MoE models where experts are heterogeneous in initialization, showing that the model successfully routes compositional tasks to "reasoning" experts and factual queries to "memory" experts with higher overall accuracy.

## Limitations

- The paper's central claims rely heavily on observational correlations between complexity measures and generalization performance without definitively establishing causation
- The complexity control framework has not been validated on truly diverse, real-world compositional tasks beyond synthetic arithmetic and controlled language tasks
- The mechanism by which initialization scale and weight decay guide models toward reasoning solutions is inferred rather than proven through controlled ablation studies

## Confidence

- **High Confidence:** The empirical observation that small initialization scales combined with larger weight decay coefficients improve OOD generalization on the synthetic compositional tasks
- **Medium Confidence:** The characterization of three distinct learning phases and the assertion that Phase 3 represents genuine reasoning-based solutions rather than sophisticated memorization
- **Medium Confidence:** The use of stable rank and condensation metrics as indicators of model complexity and their correlation with generalization performance
- **Low Confidence:** The claim that structured word embeddings directly enable compositional generalization rather than being a byproduct of successful training

## Next Checks

1. **Causation vs Correlation Test:** Systematically ablate condensation through architectural modifications (e.g., orthogonal regularization) while holding initialization and weight decay constant to determine if condensation is necessary for OOD generalization or merely correlated with it.

2. **Real-World Task Transfer:** Apply the complexity control framework to a non-synthetic compositional task (e.g., SCAN dataset for language understanding or visual reasoning tasks) to validate whether the same initialization-weight decay relationship holds for genuine compositional generalization.

3. **Dynamic Complexity Monitoring:** Implement real-time monitoring of stable rank and condensation metrics during training to identify whether these measures predict phase transitions before accuracy metrics show clear changes, potentially enabling early stopping or adaptive hyperparameter tuning.