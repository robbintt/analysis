---
ver: rpa2
title: Evaluating Large Language Models for Security Bug Report Prediction
arxiv_id: '2601.22921'
source_url: https://arxiv.org/abs/2601.22921
tags:
- sbrs
- datasets
- recall
- gemini
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluates the effectiveness of Large Language Models
  (LLMs) for Security Bug Report (SBR) prediction. Two approaches were compared: prompt-based
  engineering using proprietary models (GPT-4.1 and Gemini) and fine-tuning small
  open-source models (BERT, DistilBERT, DistilGPT-2, Qwen2.5-Coder).'
---

# Evaluating Large Language Models for Security Bug Report Prediction

## Quick Facts
- **arXiv ID:** 2601.22921
- **Source URL:** https://arxiv.org/abs/2601.22921
- **Reference count:** 29
- **Primary result:** Proprietary models (Gemini) achieved 77% G-measure but low precision (22%), while fine-tuned models (DistilBERT) achieved 51% G-measure with high precision (75%).

## Executive Summary
This study evaluates Large Language Models (LLMs) for Security Bug Report (SBR) prediction using two distinct approaches: prompt-based engineering with proprietary models (GPT-4.1 and Gemini) and fine-tuning small open-source models (BERT, DistilBERT, DistilGPT-2, Qwen2.5-Coder). Proprietary models demonstrated superior sensitivity with Gemini achieving 77% G-measure and 74% recall, but suffered from high false-positive rates resulting in low precision (22%). In contrast, fine-tuned models achieved lower overall G-measures (51% for DistilBERT) but substantially higher precision (75%). The study also found that fine-tuned models offered 10-50x faster inference compared to proprietary models after initial training. These findings highlight the trade-offs between sensitivity and precision in SBR prediction and suggest the need for further research to optimize LLM applications in this domain.

## Method Summary
The study compared two approaches for SBR classification: prompt-based engineering using proprietary models (GPT-4.1, Gemini) and fine-tuning small open-source models (BERT, DistilBERT, DistilGPT-2, Qwen2.5-Coder). Five datasets (Chromium, Derby, Camel, Ambari, Wicket) were used with chronological train/test splits. Proprietary models were accessed via API with standardized prompts, while fine-tuned models used differential evolution for hyperparameter optimization. Performance was evaluated using G-measure (primary), recall, precision, F1-score, and false-positive rate across all datasets.

## Key Results
- Proprietary models (Gemini) achieved highest sensitivity with 77% G-measure and 74% recall but low precision (22%) due to high false-positive rates
- Fine-tuned models (DistilBERT) achieved 51% G-measure with substantially higher precision (75%) and 10-50x faster inference
- Prompt engineering (role-playing, few-shot examples) showed minimal impact on performance, with no significant difference between zero-shot and few-shot settings
- Fine-tuned models showed poor cross-project generalization, particularly on smaller datasets (Ambari/Wicket)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Proprietary LLMs accessed via zero-shot prompting exhibit a "sensitivity bias," prioritizing the detection of security keywords over precise classification boundaries.
- **Mechanism:** The study observes that proprietary models (specifically Gemini) may map broad semantic definitions in the prompt (e.g., "improper memory handling") to bug reports too liberally. By acting as a "Security Auditor," the model lowers its classification threshold to avoid missing potential threats, resulting in high recall but significantly reduced precision due to elevated False Positive Rates (FPR).
- **Core assumption:** The model's pre-training on security literature causes it to flag general software defects as security vulnerabilities if they share lexical features with known vulnerability patterns.
- **Evidence anchors:** [abstract] "Prompted proprietary models demonstrate the highest sensitivity... albeit at the cost of a higher false-positive rate." [section IV] "Gemini... detecting over three times as many SBRs as GPT... resulting in a lower precision of 0.22."

### Mechanism 2
- **Claim:** Fine-tuning small encoder-only models aligns feature extraction specifically with the project's historical data distribution, optimizing for precision and inference latency.
- **Mechanism:** Unlike general-purpose LLMs, fine-tuning DistilBERT on a temporal train-test split forces the model to learn the specific "dialect" of security reports within that specific project. The bidirectional attention mechanism captures context that discriminates between actual security failures and general errors, achieving higher precision (0.75) and 10-50x faster inference.
- **Core assumption:** The "first half" (historical) training data contains sufficient signal and similarity to the "second half" (future) test data for the model to learn a stable decision boundary.
- **Evidence anchors:** [abstract] "Fine-tuned models... attaining a lower overall G-measure of 51% but substantially higher precision of 75%." [section III.B] "We sort each dataset chronologically... The first half contains historical bug reports used for training."

### Mechanism 3
- **Claim:** Prompt engineering strategies like "Role-playing" and "Few-shotting" have limited impact on resolving the fundamental precision-recall trade-off in this specific domain.
- **Mechanism:** The study tested domain-specific role-playing (CWE/CVE expert) and few-shot examples. The mechanism here is that the model's internal priors (learned during pre-training) outweigh the few examples provided in the prompt, leading to negligible performance differences between zero-shot and few-shot settings.
- **Core assumption:** The prompt provides sufficient context for the model to adjust its behavior without weight updates.
- **Evidence anchors:** [section V.A] "Our results showed no significant performance difference between few-shot and zero-shot settings." [section V.A] "Gemini achieved an F1-score of 0.31 in the zero-shot setting, which slightly decreased to 0.29 with few-shot."

## Foundational Learning

- **Concept:** **G-measure vs. F1-score**
  - **Why needed here:** The paper relies heavily on G-measure (harmonic mean of Recall and 1-FPR) rather than just F1-score (Precision and Recall). This is critical because in security, the cost of a False Positive (wasted triage time) often differs from the cost of a False Negative (missed vulnerability).
  - **Quick check question:** If a model has high Recall but terrible Precision, why might the G-measure be a more useful metric than F1-score for a security team trying to minimize false alarms?

- **Concept:** **Encoder-only (BERT) vs. Decoder-only (GPT) Architectures**
  - **Why needed here:** The study compares these directly. Understanding that encoders (bidirectional context) are typically better at classification tasks than decoders (unidirectional/autoregressive) explains why DistilBERT outperformed DistilGPT-2 in the fine-tuning experiments.
  - **Quick check question:** Why would a bidirectional model (BERT) theoretically handle the nuanced context of a bug report better than a unidirectional model (GPT) for classification?

- **Concept:** **Temporal Data Leakage**
  - **Why needed here:** The authors explicitly split data chronologically (Train: Past, Test: Future) to simulate real-world deployment.
  - **Quick check question:** Why is a random train-test split inappropriate for evaluating a bug prediction model intended for future use?

## Architecture Onboarding

- **Component map:** Input Layer (Bug Report Text) -> Strategy A (API): Prompt -> Proprietary API (Gemini/GPT) OR Strategy B (Local): Tokenizer -> Fine-tuned Encoder (DistilBERT) with Classification Head -> Evaluation: Confusion Matrix -> G-measure, Recall, Precision, FPR

- **Critical path:**
  1. **Data Partitioning:** Sort by date; split 50/50 (Train/Test). *Do not randomize.*
  2. **Model Selection:** Choose based on constraint. Need high sensitivity/security coverage? Use Gemini (API). Need high precision/speed/privacy? Use DistilBERT (Local).
  3. **Hyperparameter Tuning (if local):** Use Differential Evolution to find Learning Rate and Sequence Length.

- **Design tradeoffs:**
  - **Gemini (Proprietary):** +26% G-measure, High Recall (Detects threats) ↔ High Cost ($3x), High Latency, Low Precision (Noisy)
  - **DistilBERT (Fine-tuned):** +53% Precision, 50x Faster, Privacy Preserving ↔ Lower Recall (Misses threats), Requires Training

- **Failure signatures:**
  - **High FPR with Proprietary Models:** The model flags general errors (e.g., "null pointer") as security issues. *Mitigation:* Not found in this study; post-processing or threshold adjustment required.
  - **Low Recall with Fine-tuned Models:** The model misses rare or novel security patterns not present in the "First Half" training data. *Mitigation:* Data augmentation (not explored deeply here) or periodic retraining.

- **First 3 experiments:**
  1. **Baseline Establishment:** Run the provided standardized prompt (Listing III-C) against the Chromium dataset using Gemini to reproduce the ~0.86 G-measure and ~0.19 FPR baseline.
  2. **Fine-tuning Efficiency:** Train DistilBERT on the "First Half" of Chromium using the AdamW optimizer. Verify if inference speed drops to < 0.13 seconds/row as claimed.
  3. **Generalization Test:** Train DistilBERT on Chromium and test on a smaller dataset (e.g., Ambari) to observe the performance drop-off in cross-project settings.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What specific factors contribute to the success or failure of LLMs in classifying Security Bug Reports (SBRs)?
- **Basis in paper:** [explicit] The Introduction and Conclusion call for "in-depth studies such as identifying the factors that contribute to successful SBR classification" to understand the reasons for limited performance.
- **Why unresolved:** The study benchmarks performance metrics (recall, precision) but does not perform a qualitative analysis of *why* certain reports are misclassified or what textual features lead to the divergent strategies of GPT (conservative) vs. Gemini (sensitive).
- **What evidence would resolve it:** An error analysis mapping linguistic features (e.g., presence of specific security keywords, length, code snippets) to false positives and false negatives.

### Open Question 2
- **Question:** Can an ensemble approach combining proprietary models (e.g., GPT and Gemini) improve overall prediction accuracy?
- **Basis in paper:** [explicit] Section V.B notes that one model is often correct when the other is not, suggesting "combining model predictions could further improve results and represents a promising direction for future research."
- **Why unresolved:** The authors measured consensus scores (68%–87% agreement) but did not implement or test a combined classifier to see if it balances the trade-off between GPT's precision and Gemini's recall.
- **What evidence would resolve it:** An experiment implementing a voting mechanism or stacking classifier using the outputs of both proprietary models.

### Open Question 3
- **Question:** Does the trade-off between the sensitivity of prompted models and the precision of fine-tuned models persist across software projects in different domains?
- **Basis in paper:** [explicit] Section VI (Threats to Validity) states that the analysis is limited to five specific datasets and "replication across multiple projects from different domains... is required" to generalize the findings.
- **Why unresolved:** The datasets (Chromium, Ambari, etc.) may have specific class imbalance ratios or vocabularies that favor the observed model behaviors, limiting external validity.
- **What evidence would resolve it:** Replicating the methodology on a broader set of bug reports from diverse industries (e.g., finance, healthcare) or programming languages.

## Limitations
- Proprietary model results depend on API availability and may vary with different prompt engineering approaches
- Fine-tuning effectiveness constrained by dataset size and temporal stability assumptions, with poor cross-project generalization
- Temporal train-test split may not capture evolving threat patterns or project-specific security practices

## Confidence
- **High Confidence:** The comparative framework (G-measure, Recall, Precision, FPR metrics) and general trade-off observations between proprietary and fine-tuned models
- **Medium Confidence:** The specific performance numbers (e.g., Gemini G-measure of 77%, DistilBERT precision of 75%) are reproducible given the same datasets and API access
- **Medium Confidence:** The conclusion that fine-tuned models offer 10-50x faster inference is supported by architecture differences

## Next Checks
1. **Reproduce the Chromium baseline:** Run the standardized prompt against the Chromium dataset using Gemini to verify the ~0.86 G-measure and ~0.19 FPR baseline performance
2. **Cross-project generalization test:** Train DistilBERT on Chromium and evaluate on Ambari/Wicket to quantify the performance degradation in cross-project settings
3. **Prompt engineering ablation:** Systematically test variations in prompt complexity (Chain-of-Thought reasoning, more few-shot examples) to determine if the observed minimal impact of few-shot prompting holds across different prompt strategies