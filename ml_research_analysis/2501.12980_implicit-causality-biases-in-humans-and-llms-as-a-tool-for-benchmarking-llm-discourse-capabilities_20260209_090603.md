---
ver: rpa2
title: Implicit Causality-biases in humans and LLMs as a tool for benchmarking LLM
  discourse capabilities
arxiv_id: '2501.12980'
source_url: https://arxiv.org/abs/2501.12980
tags:
- llms
- bias
- data
- biases
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how well large language models (LLMs) capture
  human-like discourse biases using Implicit Causality verbs. Four experiments compare
  11 LLMs (ranging from 350M to 7.5B parameters, mono- and multilingual) against human
  data on coreference, coherence, and referring expression biases.
---

# Implicit Causality-biases in humans and LLMs as a tool for benchmarking LLM discourse capabilities

## Quick Facts
- arXiv ID: 2501.12980
- Source URL: https://arxiv.org/abs/2501.12980
- Reference count: 27
- Large language models fail to capture human-like discourse biases despite strong performance on other NLP tasks

## Executive Summary
This study investigates whether large language models capture human-like discourse processing by examining their handling of Implicit Causality (IC) verbs across three levels: coreference, coherence, and referring expression forms. Four experiments compare 11 LLMs (from 350M to 7.5B parameters) against human data, revealing that only the largest monolingual model showed some human-like patterns. The findings expose fundamental limitations in LLMs' discourse capabilities, particularly their inability to generate causal explanations and adjust referring expressions based on semantic biases.

## Method Summary
The study uses 38 German IC verbs paired with 80 gendered names to create prompts testing discourse processing. Models generate continuations using diverse beam search, which are then automatically annotated for coreference (who is mentioned), coherence (causal vs. temporal relations), and referring form (pronoun vs. name). Statistical analysis via mixed-effects logistic regression compares LLM outputs against human "gold standard" data, focusing on whether models show the same verb-class distinctions and bias interactions that humans exhibit.

## Key Results
- Only the largest monolingual model (German BLOOM 6.4B) showed some human-like coreference patterns
- None of the models displayed human-like coherence biases, preferring temporal relations over explanations
- LLMs used simpler forms (pronouns) for subjects but failed to adjust form based on bias congruence

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Discourse Expectation
- **Claim:** LLM discourse processing follows a cascade: Semantic Representation → Relation (Coherence) → Referent (Coreference) → Anaphoric Form
- **Evidence:** Models failing at coherence level (generating temporal instead of causal relations) subsequently fail at coreference and form levels

### Mechanism 2: Formal vs. Functional Competence
- **Claim:** LLMs rely on formal linguistic competence (surface statistics, recency) rather than functional competence (causal reasoning, world knowledge)
- **Evidence:** Strong recency bias and preference for temporal connectives suggest statistical pattern matching over semantic causality

### Mechanism 3: Constraint Competition
- **Claim:** IC bias representations exist but are masked by stronger competing constraints like gender bias or recency
- **Evidence:** Changing gender order affected coreference more in LLMs than humans, suggesting constraint interference

## Foundational Learning

- **Concept: Implicit Causality (IC) Verbs**
  - **Why needed here:** Core linguistic unit of analysis; verbs like "fascinate" carry inherent semantic biases about who is responsible
  - **Quick check:** In "John amazed Mary because...", does continuation explain something about John (Subject) or Mary (Object)?

- **Concept: Coherence Relations**
  - **Why needed here:** Differentiates between Explanation (causal) and Result/Consequence relations; humans prefer Explanation after IC verbs
  - **Quick check:** Does "and so" trigger an Explanation relation or a Consequence relation?

- **Concept: Diverse Beam Search**
  - **Why needed here:** Decoding strategy to generate varied continuations for discourse behavior analysis
  - **Quick check:** Why choose diverse beam search over standard beam search when probing for discourse variability?

## Architecture Onboarding

- **Component map:** Prompt Generator → Inference Engine → Annotation Pipeline → Statistical Validator
- **Critical path:** Annotation Pipeline (spaCy parsing must reliably identify anaphoric subjects and connectives)
- **Design tradeoffs:** Generation vs. Probability (full continuations allow form analysis but introduce decoding noise); Foundation vs. Instruction Models (cleaner linguistic data but excludes advanced RLHF models)
- **Failure signatures:** Recency Bias (object preference), Temporal Default (temporal connectives), Subject Simplicity (pronoun for subjects)
- **First 3 experiments:**
  1. Coreference Probe: IC verbs followed by "because" to measure subject/object bias
  2. Coherence Classification: Comma prompts to count causal vs. temporal connectives
  3. Forced Reference Task: Constrained decoding to force specific gender reference and observe form choice

## Open Questions the Paper Calls Out

- **Question:** Does instruction-tuning or scaling beyond 7.5B parameters enable human-like coherence and form biases?
- **Basis:** Authors note need to analyze larger/instruction-tuned models like ChatGPT
- **Why unresolved:** Study restricted to foundation models up to 7.5B parameters
- **What evidence would resolve it:** Benchmark larger/instruction-tuned models to test for explanation bias and anaphoric form sensitivity

- **Question:** Is temporal preference driven by training data statistics or model architecture?
- **Basis:** Authors observe default to temporal relations and ask about corpus vs. architecture cause
- **Why unresolved:** Study notes implicit vs. explicit relations in corpora but doesn't isolate cause
- **What evidence would resolve it:** Comparative analysis of connective frequencies in training corpora vs. model outputs

- **Question:** Do IC processing deficiencies generalize across languages?
- **Basis:** Authors state need to expand benchmark to other languages
- **Why unresolved:** Experiments focused on German; unclear if failures are universal
- **What evidence would resolve it:** Replicate benchmark in English/other languages with comparable models

## Limitations

- Evaluation restricted to German language and 38 specific IC verbs, limiting generalizability
- Excludes instruction-tuned models, potentially missing most advanced discourse capabilities
- Annotation pipeline dependent on spaCy's German parser, which may fail on model outputs
- No quantification of human annotation noise despite acknowledging its imperfection

## Confidence

- **IC Verb Processing Failure:** High confidence (consistent across experiments and model sizes)
- **Hierarchical Discourse Model:** Medium confidence (compelling framework but limited empirical proof)
- **Formal vs. Functional Competence:** Medium confidence (plausible but not definitively proven)

## Next Checks

1. Cross-linguistic replication: Test benchmark with English IC verbs to assess generalizability
2. Instruction-tuned model comparison: Run benchmark on ChatGPT/Claude to evaluate fine-tuning impact
3. Human annotation reliability study: Quantify inter-annotator agreement to contextualize LLM deviations