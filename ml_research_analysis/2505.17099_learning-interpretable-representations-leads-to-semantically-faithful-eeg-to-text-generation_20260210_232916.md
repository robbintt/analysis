---
ver: rpa2
title: Learning Interpretable Representations Leads to Semantically Faithful EEG-to-Text
  Generation
arxiv_id: '2505.17099'
source_url: https://arxiv.org/abs/2505.17099
tags:
- semantic
- language
- decoding
- text
- representations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "GLIM addresses hallucination in EEG-to-text decoding by reframing\
  \ the task as semantic summarization rather than word-for-word reconstruction. It\
  \ aligns EEG representations with a frozen language model\u2019s latent space using\
  \ a contrastive-generative objective, augmented by multiple paraphrased text variants\
  \ and domain-prompt injection."
---

# Learning Interpretable Representations Leads to Semantically Faithful EEG-to-Text Generation

## Quick Facts
- **arXiv ID**: 2505.17099
- **Source URL**: https://arxiv.org/abs/2505.17099
- **Reference count**: 40
- **Primary result**: GLIM achieves state-of-the-art EEG-to-text generation by aligning EEG representations with frozen LM latent space, reducing hallucinations and improving semantic fidelity on ZuCo.

## Executive Summary
This work reframes EEG-to-text generation as a semantic summarization task to reduce hallucination. The GLIM model learns to align EEG representations with the latent space of a frozen language model (LM) via a contrastive-generative objective, augmented with paraphrased text variants and domain-prompt injection. Experiments on ZuCo show GLIM outperforms baselines in generation, retrieval, and zero-shot classification while maintaining semantic faithfulness, even without teacher forcing. The modular architecture enables interpretability and scalability, offering a practical path toward real-world language BCIs.

## Method Summary
GLIM addresses hallucination in EEG-to-text decoding by reframing the task as semantic summarization rather than word-for-word reconstruction. It aligns EEG representations with a frozen language model’s latent space using a contrastive-generative objective, augmented by multiple paraphrased text variants and domain-prompt injection. This modular design enables effective learning from heterogeneous, small-scale data.

## Key Results
- Generation: BLEU-1 of 0.26, BLEU-2 of 0.11
- Retrieval: Top-1 of 0.08, Top-5 of 0.35
- Zero-shot classification: Sentiment 0.32, Relation 0.43, Corpus 0.93

## Why This Works (Mechanism)
By aligning EEG representations to a frozen LM’s latent space, GLIM leverages the LM’s semantic priors to guide decoding, reducing reliance on direct EEG-text mapping. Paraphrased text variants (MTVs) improve robustness to semantic variations and reduce overfitting, while domain-prompt injection tailors the LM to EEG-specific contexts. The modular design isolates each component, enabling interpretability and controlled ablation.

## Foundational Learning
- **EEG-to-text alignment**: Needed to map brain signals to language; quick check: verify alignment accuracy on benchmark datasets.
- **Contrastive learning**: Helps align representations without requiring paired text; quick check: compare with non-contrastive baselines.
- **Paraphrasing augmentation**: Increases robustness to semantic variance; quick check: test with and without MTVs.
- **Domain-prompt injection**: Adapts LM to EEG-specific semantics; quick check: evaluate on cross-paradigm tasks.
- **Frozen LM latent space**: Provides stable semantic priors; quick check: measure effect of unfreezing encoder.
- **Modular architecture**: Enables interpretability and targeted debugging; quick check: ablation of each module.

## Architecture Onboarding

**Component map**: EEG encoder → Querying aligner → Frozen LM encoder → Paraphraser generator

**Critical path**: EEG signal → Alignment with LM latent space → Paraphrased text generation

**Design tradeoffs**: Contrastive objective improves semantic fidelity but may lose lexical detail; MTVs increase robustness but may dilute fine-grained signals; freezing LM prevents hallucination but may cap performance.

**Failure signatures**: 
- Poor retrieval scores → misalignment between EEG and LM representations
- Low BLEU on paraphrase variants → insufficient paraphrasing augmentation
- Unstable generation → domain prompts not properly injected

**First experiments**:
1. Ablate paraphrasing augmentation (MTVs) and measure impact on semantic vs. lexical fidelity.
2. Compare frozen vs. fine-tuned LM encoder to quantify hallucination vs. alignment trade-off.
3. Test domain-prompt injection across different reading paradigms to confirm robustness.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does the use of multiple paraphrased text variants (MTVs) inadvertently suppress or obscure fine-grained lexical details (e.g., specific proper nouns or numbers) that are partially encoded in EEG signals?
- **Basis in paper**: [explicit] The authors state in the Limitations section: "When fine-grained lexical details are partially encoded in EEG signals, the use of multiple paraphrased text variants (MTVs) may dilute or obscure such signals."
- **Why unresolved**: GLIM prioritizes "semantic summarization" over "verbatim reconstruction," so the specific trade-off between semantic robustness and the preservation of specific lexical entities was not quantified in the current experiments.
- **What evidence would resolve it**: A comparative study evaluating the reconstruction accuracy of specific named entities and numbers using MTVs versus single-target training.

### Open Question 2
- **Question**: Can the semantic priors in the frozen language model's upstream representations be leveraged to improve decoding accuracy without re-introducing the risk of posterior collapse?
- **Basis in paper**: [explicit] The Limitations section notes that "the exclusion of semantic priors in encoder’s upstream representations may limit the upper bound of decoding performance and introduce supervision biases."
- **Why unresolved**: The study explicitly freezes the LM to mitigate posterior collapse, leaving the potential performance gains from utilizing the LM's full text-to-text capabilities unexplored.
- **What evidence would resolve it**: Experiments involving partial unfreezing or fine-tuning of the LM encoder to measure if the resulting alignment gains outweigh the increase in hallucinated outputs.

### Open Question 3
- **Question**: How can post-generation policies, such as human feedback or reward modeling, be integrated to refine GLIM's outputs for practical, real-time language BCI applications?
- **Basis in paper**: [explicit] In the Future Work section, the authors propose that advancing toward practical BCIs "may benefit from post-generation policies—such as human feedback or reward modeling—to further improve usability in real-world applications."
- **Why unresolved**: The current research establishes the foundational generative framework and evaluates semantic fidelity offline; it does not implement or test interactive feedback loops.
- **What evidence would resolve it**: Implementing a reinforcement learning from human feedback (RLHF) loop where user corrections iteratively update the EEG encoder or querying aligner.

## Limitations
- Scalability to larger, noisier datasets remains untested.
- Quantitative hallucination measures beyond BLEU and retrieval metrics are not provided.
- Claims about domain-prompt injection robustness rely only on within-dataset variations, not external validation.

## Confidence
- **High**: Semantic summarization framing reduces word-for-word hallucination; modular architecture supports interpretability.
- **Medium**: Benchmark performance improvements (BLEU, retrieval, zero-shot classification) on ZuCo; domain prompts improve robustness within tested paradigms.
- **Low**: Generalizability to larger, more diverse EEG datasets; quantitative hallucination reduction beyond BLEU metrics; true cross-dataset robustness.

## Next Checks
1. Test GLIM on a larger, multi-site EEG-text corpus to assess scalability and generalization.
2. Conduct ablation studies isolating the effects of paraphrasing augmentation, contrastive loss, and domain-prompt injection on hallucination and semantic fidelity.
3. Introduce hallucination-specific evaluation metrics (e.g., semantic similarity + factual consistency) to complement BLEU and retrieval scores.