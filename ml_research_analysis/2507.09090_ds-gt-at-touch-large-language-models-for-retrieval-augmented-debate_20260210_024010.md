---
ver: rpa2
title: "DS@GT at Touch\xE9: Large Language Models for Retrieval-Augmented Debate"
arxiv_id: '2507.09090'
source_url: https://arxiv.org/abs/2507.09090
tags:
- debate
- evaluation
- response
- task
- your
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated large language models (LLMs) for retrieval-augmented
  debate, focusing on both response generation and evaluation. Six models from OpenAI,
  Anthropic, and Google were tested using a RAG system with the ClaimRev corpus.
---

# DS@GT at Touché: Large Language Models for Retrieval-Augmented Debate

## Quick Facts
- arXiv ID: 2507.09090
- Source URL: https://arxiv.org/abs/2507.09090
- Reference count: 8
- Primary result: LLMs perform well in retrieval-augmented debate but tend to be verbose and show consistency in evaluation across metrics

## Executive Summary
This study evaluates large language models for retrieval-augmented debate using the ClaimRev corpus and a RAG system. Six models from OpenAI, Anthropic, and Google were tested across four metrics: Quality, Quantity, Manner, and Relation. The research reveals that while LLMs excel at generating debate responses with retrieved arguments, they consistently produce verbose outputs that often hit the 60-word limit. Anthropic's Claude Opus 4 was notably the most verbose, while OpenAI's GPT-4o and GPT-4.1 demonstrated more concise responses.

## Method Summary
The study employed a RAG (Retrieval-Augmented Generation) system with the ClaimRev corpus to evaluate six large language models from OpenAI, Anthropic, and Google. Models were tested on their ability to generate and evaluate debate responses across 32 topics. A 60-word limit was imposed on responses to ensure consistency. Evaluation metrics included Quality, Quantity, Manner, and Relation, with automated scoring used throughout. The study also projected costs for running these evaluations across different models, revealing significant variation in computational expenses.

## Key Results
- LLMs generated effective debate responses with retrieved arguments but showed high verbosity, frequently hitting the 60-word limit
- Anthropic's Claude Opus 4 was the most verbose (mean 56.5 words), while OpenAI's GPT-4o and GPT-4.1 were more concise
- Evaluation scores were generally low across metrics except Manner, indicating strict judging criteria and consistency in evaluation

## Why This Works (Mechanism)
The RAG system enables LLMs to access relevant retrieved arguments from the ClaimRev corpus, enhancing their debate capabilities by grounding responses in evidence. The 60-word limit creates a controlled environment for comparing model verbosity, while automated evaluation metrics provide consistent, scalable assessment across multiple dimensions of debate performance.

## Foundational Learning

- RAG (Retrieval-Augmented Generation): Why needed - Grounds LLM responses in retrieved evidence; Quick check - Verify retrieved documents are relevant to the debate topic
- Debate evaluation metrics: Why needed - Provides standardized assessment of debate quality; Quick check - Confirm metric definitions align with human judgment criteria
- Model verbosity analysis: Why needed - Identifies differences in how models utilize word limits; Quick check - Compare mean response lengths across models
- Cost projection: Why needed - Estimates computational expenses for scaling evaluations; Quick check - Validate cost calculations against API pricing

## Architecture Onboarding

**Component Map:**
User Query -> RAG System -> LLM Generator -> 60-word Response -> Automated Evaluator -> Scores

**Critical Path:**
Query input → Retrieval from ClaimRev → LLM response generation → Response length check → Automated evaluation → Score output

**Design Tradeoffs:**
The 60-word limit ensures comparability but may constrain nuanced argument development. Automated evaluation provides scalability but may not fully capture human judgment quality. Using the ClaimRev corpus ensures evidence-based responses but limits topic diversity.

**Failure Signatures:**
High variance in response lengths may indicate model inconsistency. Low evaluation scores across metrics suggest strict judging criteria. Cost overruns could indicate inefficient model selection for evaluation tasks.

**3 First Experiments:**
1. Test models without word limits to assess natural verbosity patterns
2. Compare automated scores with human evaluation on sample responses
3. Vary retrieval parameters to optimize relevance of retrieved arguments

## Open Questions the Paper Calls Out
The paper does not explicitly identify open questions for future research.

## Limitations
- Small set of 32 debate topics may not capture real-world debate diversity
- 60-word response limit may artificially constrain argument development
- Automated evaluation metrics may not fully align with human judgment
- Limited model diversity with only six models tested
- Cost projections may not account for all operational expenses

## Confidence

**High Confidence:**
- Comparative performance rankings of different LLMs based on direct quantitative measurements
- Observation of verbosity patterns across models given clear word count measurements

**Medium Confidence:**
- Evaluation score consistency across metrics given the standardized automated evaluation process
- Cost projections based on API pricing and usage patterns

**Low Confidence:**
- Interpretation of low absolute evaluation scores due to strict criteria and limited topic diversity
- Generalization of results to real-world debate scenarios given the constrained experimental setup

## Next Checks
1. Replicate study using a larger, more diverse set of debate topics to assess generalizability
2. Conduct human evaluations of generated debate responses to validate automated metric scores
3. Test models with variable word limits to determine if verbosity patterns persist without artificial constraints
4. Expand model testing to include additional architectures and providers
5. Analyze the relationship between retrieval quality and debate performance