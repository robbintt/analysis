---
ver: rpa2
title: On Using Large-Batches in Federated Learning
arxiv_id: '2509.10537'
source_url: https://arxiv.org/abs/2509.10537
tags:
- training
- batch-size
- gradient
- time
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores the trade-off between parallel and statistical
  performance in federated learning (FL) when using large batch training. Large batches
  can improve training speed by reducing iterations, but often degrade generalization
  due to convergence to sharp minima.
---

# On Using Large-Batches in Federated Learning

## Quick Facts
- arXiv ID: 2509.10537
- Source URL: https://arxiv.org/abs/2509.10537
- Reference count: 39
- Key outcome: Proposes using a teacher model to map large-batch gradients to small-batch equivalents in federated learning, improving both training speed and model accuracy

## Executive Summary
This paper addresses the trade-off between parallel efficiency and statistical performance in federated learning when using large batch training. While large batches can accelerate training by reducing iterations, they often lead to convergence at sharp minima, degrading generalization. The proposed solution uses a teacher model to transform large-batch gradients into small-batch equivalents, preserving both training efficiency and model accuracy. The paper also introduces a memory estimation model to determine the maximum batch-size for given hardware and a parallel performance model to identify the optimal batch-size for minimal compute cost.

## Method Summary
The approach introduces a teacher model that learns to map gradients from large-batch training to their small-batch equivalents. This transformation aims to preserve the statistical benefits of small-batch training while maintaining the computational efficiency of large batches. The paper develops a memory estimation model to predict the largest permissible batch-size on a given hardware configuration and a parallel performance model to identify the optimal batch-size that minimizes total compute cost. A preliminary evaluation uses a step-function substitute for the teacher model to demonstrate the concept's viability.

## Key Results
- Step-function teacher model shows improved convergence compared to small-batch training
- ResNet50 achieves up to 32.33% higher test accuracy using the proposed approach
- VGG11 demonstrates 3.74% accuracy improvement while maintaining faster convergence over the same iterations

## Why This Works (Mechanism)
The mechanism works by preserving the noise characteristics and generalization properties of small-batch gradients while benefiting from the computational efficiency of large batches. Large batches typically converge to sharp minima with poor generalization, but the teacher model learns to transform these gradients to approximate the behavior of small-batch training, which tends to find flatter, more generalizable minima.

## Foundational Learning

**Federated Learning**: Distributed machine learning paradigm where multiple devices collaboratively train a model without sharing local data. Needed to understand the distributed nature and communication constraints. Quick check: Devices train locally and share only model updates.

**Sharp vs Flat Minima**: Sharp minima are narrow, isolated points in the loss landscape that generalize poorly, while flat minima are broader regions that provide better generalization. Needed to understand why large batches degrade performance. Quick check: Small batches tend to find flat minima; large batches tend to find sharp minima.

**Gradient Noise**: The inherent randomness in gradient estimation due to using mini-batches. Needed to understand the statistical differences between small and large batch training. Quick check: Small batches have higher gradient variance, which acts as noise injection beneficial for generalization.

**Parallel Performance Modeling**: Mathematical framework to predict how computational performance scales with batch-size. Needed to optimize the trade-off between speed and accuracy. Quick check: Assumes linear speed-up until memory constraints are hit.

## Architecture Onboarding

**Component Map**: Device Clusters -> Memory Estimation Model -> Parallel Performance Model -> Teacher Model -> Training Loop

**Critical Path**: The training loop depends on both the memory estimation (to set batch-size limits) and the teacher model (to transform gradients). The parallel performance model guides batch-size selection before training begins.

**Design Tradeoffs**: Large batches improve speed but degrade accuracy; the teacher model adds computational overhead but preserves generalization. The system must balance memory constraints against performance gains.

**Failure Signatures**: If the teacher model fails to properly transform gradients, accuracy will degrade to typical large-batch levels. Memory estimation failures will cause out-of-memory errors during training.

**First Experiments**:
1. Verify memory estimation accuracy across different hardware configurations
2. Test teacher model's gradient transformation on simple convex problems
3. Evaluate parallel performance scaling with batch-size on homogeneous device clusters

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- The step-function teacher model is acknowledged as a "naive" approximation that hasn't been replaced with the actual learning-based teacher
- Parallel performance model assumes linear speed-up, which may not hold in real federated settings with communication overhead
- Memory estimation model's accuracy across different hardware architectures and frameworks is unverified

## Confidence
- **High confidence**: Large batches accelerate training but degrade generalization - well-established in literature
- **Medium confidence**: Theoretical framework for parallel performance and memory estimation is sound but lacks extensive empirical validation
- **Medium confidence**: Preliminary results using step-function teacher are promising but based on simplified approximation

## Next Checks
1. Implement and evaluate the actual teacher model across multiple federated learning benchmarks to verify gradient transformation effectiveness
2. Conduct extensive experiments on heterogeneous device clusters with realistic communication patterns to validate parallel performance model assumptions
3. Test the approach across diverse model architectures, datasets, and federated learning scenarios beyond ResNet50 and VGG11