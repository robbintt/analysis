---
ver: rpa2
title: 'LaSeR: Reinforcement Learning with Last-Token Self-Rewarding'
arxiv_id: '2510.14943'
source_url: https://arxiv.org/abs/2510.14943
tags:
- reasoning
- self-rewarding
- arxiv
- training
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a novel method, LaSeR, to jointly optimize\
  \ the reasoning and self-verification capabilities of large language models (LLMs)\
  \ during reinforcement learning with verifiable rewards (RLVR). The key insight\
  \ is that the true reasoning reward can be reduced to a simple form: the last-token\
  \ self-rewarding score, which is the difference between the policy model\u2019s\
  \ next-token log-probability for a pre-specified token at the solution\u2019s last\
  \ token and a pre-calculated constant, scaled by the KL coefficient."
---

# LaSeR: Reinforcement Learning with Last-Token Self-Rewarding

## Quick Facts
- arXiv ID: 2510.14943
- Source URL: https://arxiv.org/abs/2510.14943
- Reference count: 38
- Primary result: Jointly optimizes reasoning and self-verification capabilities through last-token self-rewarding, achieving ~80% self-verification F1 and 2-5% accuracy gains via weighted majority voting

## Executive Summary
LaSeR introduces a novel method for jointly optimizing reasoning and self-verification capabilities in large language models through reinforcement learning with verifiable rewards (RLVR). The key insight is that the true reasoning reward can be reduced to a simple form: the last-token self-rewarding score, which is the difference between the policy model's next-token log-probability for a pre-specified token at the solution's last token and a pre-calculated constant. This allows models to derive self-rewarding signals directly from next-token probability distributions, enabling efficient inference-time scaling through weighted majority voting without requiring separate verification generations.

## Method Summary
LaSeR augments standard GRPO training with an MSE loss that aligns last-token self-rewarding scores (rs = βv·log πθ(zc|x,y) - βv·cref) with verifier rewards (rv). The method leverages the theoretical insight that for pre-specified verification tokens, the partition function Z(x,y) ≈ 1, simplifying the RL verification objective to a log-probability ratio. During training, the model learns to calibrate its next-token probabilities at the final position to reflect solution correctness. At inference, these self-rewarding scores enable weighted majority voting that improves accuracy by 2-5% compared to simple majority voting. The approach requires minimal additional computational cost and demonstrates effectiveness across various model architectures and math reasoning benchmarks.

## Key Results
- Self-verification F1 scores reaching 73-87% across different model architectures (Qwen2.5-7B, OctoThinker-3B, Open-Reasoner-Zero-7B)
- Reasoning accuracy improvements of 2-5% on MATH500 through inference-time scaling with weighted majority voting
- Strong performance across math reasoning benchmarks (MATH500, AMC23, AIME24/25, OlympiadBench) while maintaining high self-verification accuracy
- Effective transfer to general reasoning tasks (MMLU-Pro, GPQA-Diamond) though with lower self-verification F1 (65-70%)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The closed-form solution to the RL verification objective reduces to a simple log-probability ratio when using pre-specified tokens.
- **Mechanism:** For verification tokens z ∈ {zc, zi}, the partition function Z(x,y) ≈ 1 because πref(z|x,y) is negligibly small (< e^-20 for unused tokens). This eliminates the partition function that normally complicates implicit reward formulations, yielding: r̂(x,y,zc) = βv · log[πθ(zc|x,y) / πref(zc|x,y)].
- **Core assumption:** The reference model's probability for special tokens remains approximately constant across all problem-solution pairs (validated empirically with std dev < 0.13).
- **Evidence anchors:**
  - [section 3.2] "Z(x,y) ≈ 1 ⇒ logZ(x,y) ≈ 0... the optimal solution to Eq. (6) can be approximately reduced to: r̂(x,y,z) = βv log[πθ(z|x,y)/πref(z|x,y)]"
  - [section 3.3] Figure 5 shows −logπref(zc|x,y) values with extremely low standard deviation (0.04-0.13) across 300 samples
  - [corpus] Related work on self-rewarding RL (arXiv:2508.00410, arXiv:2510.08977) shows similar directions but requires explicit verification generation

### Mechanism 2
- **Claim:** The last-token next-token probability of a pre-specified token encodes the model's self-assessment of solution correctness.
- **Mechanism:** During generation, the model's final hidden state contains compressed information about the entire reasoning trajectory. The probability assigned to zc at this position reflects learned confidence calibration—correct solutions should induce higher zc probabilities after MSE alignment training.
- **Core assumption:** A single token's probability can capture sufficient signal about solution quality without explicit chain-of-thought verification.
- **Evidence anchors:**
  - [abstract] "the true reasoning reward of a solution is equal to its last-token self-rewarding score"
  - [section 4.2] Table 1 shows self-verification F1 scores reaching 73-87% across models, indicating the probability signal captures meaningful correctness information

### Mechanism 3
- **Claim:** MSE loss between self-rewarding scores and verifier rewards enables joint optimization without degrading reasoning capability.
- **Mechanism:** Unlike SFT loss that drives πθ(zc|x,y) → 1 (causing interference), the MSE loss drives πθ(zc|x,y) → exp(1/βv) · πref(zc|x,y), which remains small (~e^-13). This preserves the original RLVR objective while adding self-rewarding capability.
- **Core assumption:** The self-rewarding loss gradient does not conflict with reasoning optimization gradients.
- **Evidence anchors:**
  - [section 3.4] "our loss drives πθ(zc|x,y) toward exp(1/βv)·πref(zc|x,y) for rv(x,y)=1.0... thereby exerting only a negligible influence on the original RLVR optimization"
  - [appendix F] Figure 8 shows SFT loss causes substantial training reward degradation while last-token self-rewarding loss maintains comparable rewards

## Foundational Learning

- **Concept: KL-regularized reinforcement learning**
  - Why needed here: The theoretical derivation relies on understanding how KL divergence bounds appear in optimal policy solutions and why they enable the closed-form reduction.
  - Quick check question: Explain why the KL penalty β · DKL(πθ || πref) appears in RL objectives and how it relates to the implicit reward formulation.

- **Concept: Partition functions in probabilistic models**
  - Why needed here: The key theoretical insight is recognizing when Z(x,y) ≈ 1, which requires understanding partition function behavior with extreme probability values.
  - Quick check question: Why does πref(zc|x,y) ≈ 0 imply Z(x,y) ≈ 1 in the verification reward context?

- **Concept: RLVR (Reinforcement Learning with Verifiable Rewards)**
  - Why needed here: LaSeR augments standard RLVR; understanding the baseline paradigm (binary rewards from deterministic verifiers, GRPO advantage estimation) is essential.
  - Quick check question: How does GRPO estimate advantages, and why does LaSeR propose integrating verifier-based and self-rewarding-based advantages?

## Architecture Onboarding

- **Component map:** Base RLVR framework (GRPO) -> Last-token self-rewarding module -> MSE loss calculator -> Advantage integrator
- **Critical path:**
  1. Generate K solutions per problem using current policy
  2. Compute verifier rewards rv for each solution (binary correctness)
  3. Extract last-token log-probabilities for zc, compute self-rewarding scores rs
  4. Calculate MSE loss with re-weighting: wc for correct, wi for incorrect samples
  5. Compute GRPO advantages, optionally integrate with self-rewarding advantages (after warm-up)
  6. Backpropagate combined loss: LGRPO + α · LMSE

- **Design tradeoffs:**
  - **βv selection:** Larger values (0.1-0.5) maintain small πθ(zc|x,y) and stable training; smaller values cause interference
  - **α (MSE weight):** Larger values (> 0.1) harm reasoning capability; 0.1 provides balance
  - **Warm-up phases:** Reasoning warm-up (200 steps) before adding self-rewarding loss for base models; self-rewarding warm-up (200 steps) before advantage integration

- **Failure signatures:**
  - Training rewards plateau or degrade early → Check if βv too small or α too large
  - Self-verification F1 stuck near 50% → Check class imbalance handling, ensure loss re-weighting is active
  - Advantage integration causes instability → Verify std(r1s, ..., rKs) threshold (T=0.1) filtering is working
  - Model generates zc token instead of EOS → Sampling issue; ensure top_p filtering or increase βv

- **First 3 experiments:**
  1. **Validation run on Open-Reasoner-Zero-7B:** Skip reasoning warm-up since already reinforced; verify self-verification F1 reaches >75% within 200 steps
  2. **Ablation on cref approximation:** Compare using pre-calculated constant vs. actual πref(zc|x,y) values; expect <1% performance difference (Table 3)
  3. **Inference-time scaling test:** Generate 32 solutions per problem, compare Maj@K vs RM@K (weighted by self-rewarding scores); expect 2-5% accuracy gain on MATH500 (Figure 2)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the last-token self-rewarding mechanism be extended to achieve zero additional inference cost by computing scores directly at the EOS token position, and what strategies would prevent the generator from occasionally selecting the pre-specified special token during generation?
- **Basis in paper:** [explicit] Section 5.3 discusses deriving the self-rewarding score from the log-probability at the EOS token position to achieve "ideally zero additional inference cost," but notes the generator may select zc in rare cases, suggesting "Future work can further investigate advanced strategies to make the above adjustment more principled and robust."
- **Why unresolved:** The paper identifies the feasibility but does not implement or validate this zero-cost variant.
- **What evidence would resolve it:** Empirical results showing training stability and verification accuracy when using EOS-position scores, with ablations on strategies like top-p adjustment.

### Open Question 2
- **Question:** Does increasing the number of additional inference tokens (M tokens instead of one) for self-rewarding computation yield positive inference-time scaling effects for latent self-verification capability?
- **Basis in paper:** [explicit] Section 5.3 proposes: "It is a promising direction for future research to explore whether increasing the number of additional inference tokens can yield positive inference-time scaling effect for latent self-rewarding capability."
- **Why unresolved:** The paper only evaluates single-token self-rewarding; multi-token variants remain unexplored.
- **What evidence would resolve it:** Experiments comparing self-verification F1 scores across different M values (e.g., 1, 2, 4, 8) on held-out benchmarks.

### Open Question 3
- **Question:** How can LaSeR's self-rewarding capability be improved for general reasoning domains where current verification F1 scores (65-70%) lag significantly behind math reasoning (~80%)?
- **Basis in paper:** [explicit] Section 5.2 concludes: "A promising direction for future work is to further explore and unlock the full potential of our method in the general reasoning domain," noting overlapping score distributions for correct/incorrect solutions on MMLU-Pro and GPQA-Diamond.
- **Why unresolved:** The paper demonstrates the gap but does not propose or validate solutions.
- **What evidence would resolve it:** Ablations testing stronger model-based verifiers, domain-adaptive warm-up strategies, or architectural modifications showing improved F1 on general reasoning benchmarks.

### Open Question 4
- **Question:** Does LaSeR transfer effectively to RLVR algorithms beyond GRPO (e.g., PPO, DAPO), and does the theoretical derivation hold equivalently across different policy optimization frameworks?
- **Basis in paper:** [explicit] Section 4.1 states the method is evaluated primarily with GRPO, "while leaving the exploration on other RL algorithms in the future work."
- **Why unresolved:** The theoretical derivation assumes standard RL objectives, but empirical validation with other algorithms is absent.
- **What evidence would resolve it:** Comparative experiments applying LaSeR to PPO, DAPO, or VAPO with matched hyperparameters, reporting both reasoning accuracy and self-verification F1.

## Limitations

- The theoretical foundation relies on a critical approximation (Z(x,y) ≈ 1) that may not hold for different token choices or problem domains
- Self-verification capability, while impressive, still has significant error rates (20-27%) that may be problematic in high-stakes domains
- The method's effectiveness depends on the base model having sufficient reasoning capability, and may not help weaker models
- Introduces additional hyperparameters (βv, α, τ) that require careful tuning, though the paper shows reasonable robustness within suggested ranges

## Confidence

**High Confidence (80-100%):** The empirical performance improvements on math reasoning benchmarks are well-supported by experimental results. The 2-5% accuracy gains from inference-time scaling and the consistent improvements across different model architectures (Qwen, OctoThinker, Open-Reasoner-Zero) provide strong evidence for LaSeR's effectiveness. The theoretical reduction of the RL verification objective to last-token self-rewarding scores is mathematically sound given the partition function approximation.

**Medium Confidence (50-80%):** The claim that last-token next-token probability captures sufficient signal about solution quality has medium confidence. While the self-verification F1 scores demonstrate the signal is meaningful, there's limited theoretical justification for why a single token's probability can encode complete correctness information. The mechanism that πθ(zc|x,y) remains small (~e^-13) due to MSE loss driving it toward exp(1/βv)·πref(zc|x,y) is plausible but relies on the specific loss formulation working as intended.

**Low Confidence (0-50%):** The generalizability of LaSeR beyond math reasoning tasks has low confidence. The paper shows some results on MMLU-Pro and GPQA-Diamond, but these are not as extensively validated as the math benchmarks. The method's dependence on the base model's reasoning quality and the potential failure modes when πref(zc|x,y) has high variance are not fully characterized.

## Next Checks

1. **Cross-Domain Generalization Test:** Evaluate LaSeR on non-math domains with different correctness criteria (e.g., factual accuracy in history questions, logical consistency in argumentation tasks). Measure whether last-token self-rewarding scores remain predictive of solution quality when the nature of "correctness" changes fundamentally.

2. **Partition Function Robustness Analysis:** Systematically vary the pre-specified verification tokens and measure how Z(x,y) changes across different problem-solution pairs. Test whether the approximation Z(x,y) ≈ 1 holds for token choices with higher baseline probabilities or different semantic meanings.

3. **Error Analysis on Self-Verification Failures:** Conduct detailed analysis of solutions where self-verification fails (false positives/negatives). Determine whether failures are systematic (e.g., certain types of reasoning errors consistently misclassified) or random, and whether the self-rewarding signal can be improved by incorporating additional verification tokens or multi-token patterns.