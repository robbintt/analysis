---
ver: rpa2
title: More Thought, Less Accuracy? On the Dual Nature of Reasoning in Vision-Language
  Models
arxiv_id: '2509.25848'
source_url: https://arxiv.org/abs/2509.25848
tags:
- reasoning
- visual
- arxiv
- preprint
- claims
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the dual nature of reasoning in vision-language
  models, finding that while reasoning improves logical inference, it often impairs
  perceptual grounding, leading to recognition failures on basic visual tasks. This
  "visual forgetting" effect, where models increasingly ignore visual input during
  prolonged reasoning, limits the effectiveness of multimodal reasoning.
---

# More Thought, Less Accuracy? On the Dual Nature of Reasoning in Vision-Language Models

## Quick Facts
- arXiv ID: 2509.25848
- Source URL: https://arxiv.org/abs/2509.25848
- Reference count: 40
- Key outcome: VAPO-Thinker-7B achieves new SOTA across ten benchmarks, improving average accuracy by 2–4% over strong baselines by addressing visual forgetting in VLM reasoning.

## Executive Summary
This work investigates the dual nature of reasoning in vision-language models, finding that while reasoning improves logical inference, it often impairs perceptual grounding, leading to recognition failures on basic visual tasks. This "visual forgetting" effect, where models increasingly ignore visual input during prolonged reasoning, limits the effectiveness of multimodal reasoning. To address this, the authors propose Vision-Anchored Policy Optimization (VAPO), a reinforcement learning method that explicitly guides the reasoning process toward visually grounded trajectories by evaluating perceptual capability at intermediate reasoning steps. VAPO-Thinker-7B, the resulting model, achieves new state-of-the-art results across ten benchmarks, improving average accuracy by 2–4% over strong baselines and demonstrating significantly stronger reliance on visual information during reasoning.

## Method Summary
VAPO extends GRPO with a perception reward that evaluates the model's visual grounding at intermediate reasoning steps. The method injects 20 visual anchors (binary verification tasks) into the reasoning trace, generated by GPT-5 as 10 correct and 10 incorrect claims about the image. The model is rewarded for correctly verifying these claims, with late-emphasis weighting (β=1.5) prioritizing later anchors where visual forgetting is most severe. The reward combines accuracy, format, and gated perception components. Training uses 8x A100s, 2 epochs, and the ViRL39K dataset (39K examples from MM-Eureka, MV-Math, M3CoT).

## Key Results
- VAPO-Thinker-7B achieves new SOTA across ten benchmarks, improving average accuracy by 2–4% over strong baselines.
- Visual attention to image tokens decays significantly during prolonged reasoning, correlating with perceptual errors.
- A substantial portion (32.35%) of perception errors can be recovered by early termination of reasoning.
- VAPO demonstrates significantly stronger reliance on visual information during reasoning compared to baselines.

## Why This Works (Mechanism)

### Mechanism 1: Visual Forgetting During Extended Reasoning
Prolonged autoregressive reasoning causes attention to visual tokens to decay, leading the model to rely increasingly on its own generated text rather than the image. As the model generates more tokens, the softmax attention distribution spreads over a longer context. Visual tokens—embedded only at the start—receive progressively less attention weight, causing perceptual grounding to degrade. Under vanilla reasoning, the attention of visual tokens exhibits a marked decline as reasoning progresses, eventually reaching negligible levels.

### Mechanism 2: Perception Reward via Visual Anchors
Embedding verifiable visual claims as intermediate checkpoints, and rewarding correct verification, creates a gradient signal that reinforces visually grounded reasoning. Visual anchors force the model to answer binary perceptual questions mid-reasoning. The perception reward aggregates these binary scores with late-emphasis weighting (β=1.5), punishing trajectories that lose visual grounding in later stages. At each anchor, the model's perceptual capability is assessed by evaluating its responses to a set of primitive visual claims.

### Mechanism 3: Early Decision Recovery
Many perception errors are recoverable by halting reasoning early, indicating the model initially perceives correctly but is misled by subsequent ungrounded reasoning steps. In early reasoning stages, visual attention is still high. Stopping at logical breakpoints (commas, periods) preserves correct perceptual grounding before extended reasoning dilutes attention. A substantial portion of these perceptual errors (32.35%) may be recovered by terminating the reasoning process in advance through early decision.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**
  - Why needed: VAPO builds on GRPO's policy gradient framework; understanding baseline rewards (accuracy, format) is prerequisite to adding perception reward.
  - Quick check: Can you explain how GRPO computes token-level advantages from sequence-level rewards?

- **Attention Mechanisms in Transformers**
  - Why needed: Visual forgetting is diagnosed via attention ratio to image tokens; interpreting attention decay requires understanding softmax attention over growing contexts.
  - Quick check: Why does adding more generated tokens tend to dilute attention to early-position tokens?

- **Chain-of-Thought Reasoning in VLMs**
  - Why needed: The paper's central tension is that CoT helps logic but hurts perception; you need to understand standard CoT benefits to appreciate the tradeoff.
  - Quick check: What are the typical gains from CoT in text-only LLMs, and why might they not fully transfer to multimodal settings?

## Architecture Onboarding

- **Component map**: Policy Model (Qwen2.5-VL) -> Visual Anchor Generator (GPT-5) -> Anchor Insertion (K=20) -> Reward Computation (R_acc + R_fmt + γ·R_perc) -> GRPO Training

- **Critical path**: 1. Sample G responses per example from π_old. 2. Insert visual anchors into each trajectory; at each anchor, sample a claim and score binary verification. 3. Compute perception reward via late-emphasis weighted aggregation. 4. Backpropagate through GRPO objective with combined rewards.

- **Design tradeoffs**:
  - More anchors (higher K) → denser supervision but slower rollout; paper finds saturation at K=20.
  - Higher late-emphasis weight (β) → targets visual forgetting at reasoning's end but risks over-constraining early reasoning; optimal β=1.5 (~50% weight on last 30% of anchors).
  - Higher perception reward weight (γ) → better for vision-intensive tasks but can hurt math tasks with simpler visuals.

- **Failure signatures**:
  - Reward hacking: Models shorten reasoning to avoid perception evaluation; mitigated by gating R_perc on R_acc=1.
  - Claim quality issues: Non-visual or question-aligned claims weaken signal.
  - Attention-based reward fails: Directly maximizing attention ratio doesn't work—attention ≠ effective utilization.

- **First 3 experiments**:
  1. Reproduce visual forgetting curve: Take Vision-R1 or VLAA-Thinker, plot attention ratio vs. generation steps on a held-out example; confirm decay pattern matches Figure 3(A).
  2. Ablate anchor count (K): Train with K∈{0, 5, 10, 15, 20} on a subset (5k examples); verify accuracy saturates near K=20 per Figure 7.
  3. Test inference-level baselines: Apply visual replay and focus prompt to a baseline model; compare gains to VAPO-Thinker to confirm training-based solution is more effective than test-time patches (Table 3).

## Open Questions the Paper Calls Out

- **Multi-image and video reasoning**: Exploring the benefits of vision-anchored training in multi-image reasoning or temporal video understanding presents an interesting and promising direction for future research.

- **Adaptive hyperparameters**: Designing an adaptive vision-anchored policy that dynamically adjusts hyperparameters based on task characteristics is needed, as optimal settings may vary across task types.

- **Claim generation quality**: Leveraging higher-quality visual claims, either generated by more capable models or annotated by human experts, could potentially enhance the effectiveness of VAPO.

## Limitations

- Dataset specificity: The method is primarily demonstrated on the curated ViRL39K dataset, raising questions about generalizability to broader distributions.

- Visual anchor quality: The paper relies on GPT-5 for claim generation without automated quality evaluation or human verification.

- Attention proxy: The use of attention weights as a proxy for perceptual grounding is indirect and acknowledged by the authors as imperfect.

## Confidence

- Visual forgetting is a general phenomenon in VLMs: High
- VAPO effectively mitigates visual forgetting and improves accuracy: High
- Perception reward via visual anchors is the key mechanism: Medium
- Attention decay directly causes perceptual errors: Low
- VAPO generalizes across diverse reasoning tasks: Medium

## Next Checks

1. Design an experiment to correlate attention to visual tokens with downstream task performance to validate whether attention decay directly causes perceptual failures.

2. Train VAPO on a different subset of reasoning data and evaluate on all ten benchmarks to test whether gains are dataset-specific or truly generalizable.

3. Manually inspect or automatically filter visual claims for visual dependence and question alignment, then retrain VAPO with filtered claims to validate the importance of claim quality.