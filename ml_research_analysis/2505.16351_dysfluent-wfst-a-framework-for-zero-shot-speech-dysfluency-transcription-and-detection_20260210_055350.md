---
ver: rpa2
title: 'Dysfluent WFST: A Framework for Zero-Shot Speech Dysfluency Transcription
  and Detection'
arxiv_id: '2505.16351'
source_url: https://arxiv.org/abs/2505.16351
tags:
- speech
- dysfluency
- detection
- phoneme
- transcription
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Dysfluent-WFST is a zero-shot decoder that jointly transcribes
  phonemes and detects dysfluency in speech, designed for clinical transcription of
  disordered speech. It uses Weighted Finite-State Transducers (WFSTs) to model pronunciation
  behavior and incorporates a dynamic weighting mechanism to handle dysfluencies like
  repetitions, insertions, and deletions.
---

# Dysfluent WFST: A Framework for Zero-Shot Speech Dysfluency Transcription and Detection

## Quick Facts
- arXiv ID: 2505.16351
- Source URL: https://arxiv.org/abs/2505.16351
- Reference count: 0
- Primary result: Zero-shot WFST decoder achieves 95.7% PER on simulated dysfluent speech with no additional training

## Executive Summary
Dysfluent-WFST is a zero-shot decoding framework that jointly transcribes phonemes and detects dysfluencies in speech, designed for clinical transcription of disordered speech. The method uses Weighted Finite-State Transducers (WFSTs) to model pronunciation behavior and incorporates a dynamic weighting mechanism to handle dysfluencies like repetitions, insertions, and deletions. Unlike traditional approaches that require complex architectures or additional training, Dysfluent-WFST operates seamlessly with upstream encoders like WavLM, achieving state-of-the-art performance on both simulated and real speech data while maintaining computational efficiency.

## Method Summary
The framework takes reference text and speech as input, converting the text to a phoneme sequence FST with dysfluency arcs (horizontal for normal progression, return for repetitions, skip for deletions). This FST is composed with a CTC topology and intersected with emission graphs from upstream encoders to form a decoding lattice. The shortest path through this lattice yields the transcription, while state transition patterns are analyzed post-hoc to classify dysfluencies. The method requires no additional training, operating purely through composition of pre-built topologies with encoder emissions, and achieves robust performance across multiple dysfluency types on both simulated and real clinical data.

## Key Results
- Achieves 95.7% phonetic error rate (PER) on simulated dysfluent speech data
- Outperforms traditional CTC and ASR baselines on both simulated and spontaneous real speech
- Demonstrates strong performance across repetition, insertion, and deletion dysfluency types
- Maintains zero-shot operation with no additional training requirements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicit pronunciation modeling in decoding improves dysfluent speech transcription compared to unconstrained search methods.
- Mechanism: The WFST encodes expected phoneme sequences from reference text with structured transition arcs. Horizontal arcs represent normal progression; return arcs enable repetitions; skip arcs capture deletions. The decoder finds the shortest path through this constrained graph, ensuring phoneme hypotheses align with plausible pronunciation behaviors rather than random insertions.
- Core assumption: Dysfluent speech follows systematic patterns (repetitions revisit states, deletions skip states) rather than random phoneme substitutions.
- Evidence anchors:
  - [abstract] "demonstrating that explicit modeling of pronunciation behavior in decoding, rather than complex architectures, is key to improving dysfluency processing systems"
  - [page 1] "by incorporating pronunciation priors into decoding, we can achieve more accurate phoneme transcription"
  - [corpus] Limited direct corpus support; neighbor papers focus on data generation and clinical deployment rather than WFST mechanisms specifically.
- Break condition: If dysfluencies are highly irregular (e.g., random phoneme insertions unrelated to reference text), the structured graph may over-constrain decoding.

### Mechanism 2
- Claim: Text-dependent decoding reduces false positives that text-independent models produce.
- Mechanism: Reference text constructs an FST that only permits phonemes present in the expected sequence (plus structured deviations). This prevents misclassification of legitimate utterances like "I saw a dodo" as stuttered "do" repetitions, since the FST encodes the full expected phoneme sequence.
- Core assumption: Reference text is available and accurately reflects the speaker's intended utterance.
- Evidence anchors:
  - [page 1] "certain cases, such as 'I saw a dodo,' may be misclassified as stuttering of 'do'. Without textual context, the aforementioned methods are likely to misdetect such instances"
  - [page 2] "Our framework takes the reference text and corresponding speech as input"
  - [corpus] No direct corpus evidence on text-dependency benefits for dysfluency.
- Break condition: If reference text is incorrect or speaker deviates significantly from intended text, the FST constraints become misleading.

### Mechanism 3
- Claim: Zero-shot operation is achieved by composing pre-built topologies with encoder emissions without learned parameters.
- Mechanism: The decoding graph D = (T ∘ S) ∩ ξx combines CTC topology T, reference FST S, and emission graph ξx. Edge weights derive from log probabilities in the emission matrix plus fixed error costs for non-forward transitions. No gradient-based training adjusts these weights.
- Core assumption: Upstream encoders (WavLM, wav2vec2) produce sufficiently accurate emission probabilities for dysfluent speech despite being trained primarily on fluent speech.
- Evidence anchors:
  - [abstract] "operates with upstream encoders like WavLM and requires no additional training"
  - [page 2] "Dysfluent-WFST is lightweight and requires no additional training, being parameter-free"
  - [page 4, Table 3] WFST performance degrades significantly under emission noise, suggesting dependence on encoder quality
  - [corpus] Neighbor paper mentions wav2vec2-based noise detection but not dysfluency specifically.
- Break condition: If encoder emissions are unreliable for dysfluent speech (high noise, domain mismatch), shortest-path decoding amplifies errors rather than correcting them.

## Foundational Learning

- Concept: **Weighted Finite-State Transducers (WFST)**
  - Why needed here: Core decoding infrastructure; understanding state transitions, arc weights, and composition operations is essential for modifying the framework.
  - Quick check question: Given a simple FST with states {0,1,2}, what does a return arc from state 2 to state 1 represent in this dysfluency context?

- Concept: **CTC Topology and Emission Matrices**
  - Why needed here: The framework builds on CTC-style emissions from speech encoders; understanding frame-level phoneme probabilities is necessary for debugging decoding failures.
  - Quick check question: If an emission matrix has shape T×C (100×40), what does Ξx[t, c] represent?

- Concept: **Phonetic Error Rate (PER) and Weighted PER (WPER)**
  - Why needed here: Evaluation metrics for transcription quality; WPER accounts for phoneme similarity rather than treating all substitutions equally.
  - Quick check question: Why might WPER be preferable to PER when evaluating dysfluent speech transcription?

## Architecture Onboarding

- Component map: Speech Encoder -> Emission Matrix -> Reference FST Builder -> CTC Topology -> Composition Engine -> Intersection with Emissions -> Shortest Path Decoder -> Dysfluency Classifier

- Critical path: Emission quality → FST construction → Composition correctness → Shortest path extraction → Dysfluency label assignment. Errors propagate; noisy emissions cause the largest degradation.

- Design tradeoffs:
  - β parameter: Higher values permit more dysfluency tolerance but risk float-precision failures at extreme values (β > 8)
  - Fixed vs. dynamic weighting: Dynamic weighting adjusts for phoneme distance but adds computation
  - Encoder choice: WavLM-CTC achieves lower PER than Wav2Vec2 but may have different failure modes

- Failure signatures:
  - 100% repetition detection but <15% insertion/deletion detection: Skip arcs have comparable total weight to forward paths, causing false shortcuts
  - Sudden PER spikes with small noise additions (Table 3): Emission sensitivity indicates over-reliance on encoder confidence
  - Floating-point degradation at high β: Python precision limits cause FST collapse to linear structure

- First 3 experiments:
  1. **Baseline validation**: Run WavLM-CTC encoder with greedy search vs. WFST decoder on clean simulated data; verify Table 1 PER/WPER results are reproducible.
  2. **Noise robustness test**: Add Gaussian noise (σ = 0.1, 1.0, 10.0) to emissions on simu-rep subset; plot PER degradation curve to identify operating limits.
  3. **β sensitivity sweep**: Vary β from 0 to 10 on all three dysfluency types (simu-rep, simu-del, simu-ins); identify optimal β per dysfluency type and document float-precision failure threshold.

## Open Questions the Paper Calls Out
None

## Limitations
- Encoder dependency on WavLM/wav2vec2 creates vulnerability to emission noise, with performance degrading significantly under noisy conditions
- Text-dependency constraint limits applicability to scenarios where reference text is available and accurate
- Limited real-world validation on spontaneous dysfluent speech from individuals with actual speech disorders

## Confidence
- **High confidence**: Claims about WFST composition mechanics and shortest-path decoding are well-established in the literature and demonstrated through clear implementation details.
- **Medium confidence**: Performance improvements over baseline methods on synthetic data are reproducible and well-documented, but real-world clinical applicability remains uncertain.
- **Low confidence**: Claims about zero-shot operation being sufficient for clinical deployment, and that explicit pronunciation modeling alone is "key to improving dysfluency processing systems" without acknowledging encoder limitations.

## Next Checks
1. **Clinical deployment validation** - Evaluate the framework on spontaneous speech from individuals with diagnosed speech disorders in clinical settings. Compare performance against expert human transcribers and document failure modes specific to authentic dysfluent speech patterns not captured in synthetic datasets.

2. **Reference text error tolerance** - Systematically introduce errors into reference text (word deletions, substitutions, insertions) and measure performance degradation. Characterize the framework's robustness to text inaccuracies and identify the error rate threshold beyond which the FST constraints become detrimental.

3. **Encoder adaptation necessity** - Fine-tune WavLM or wav2vec2 on small amounts of dysfluent speech data and compare WFST performance against the zero-shot baseline. Quantify the performance gap to determine if "no additional training" represents a meaningful advantage or limitation in clinical applications.