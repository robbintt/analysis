---
ver: rpa2
title: 'SpeakRL: Synergizing Reasoning, Speaking, and Acting in Language Models with
  Reinforcement Learning'
arxiv_id: '2512.13159'
source_url: https://arxiv.org/abs/2512.13159
tags:
- clarification
- user
- reward
- agent
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SpeakRL, a reinforcement learning method
  that enables language models to proactively ask clarification questions in multi-turn
  dialogues. By rewarding the model for asking targeted questions using structured
  tokens and an LLM-as-judge reward, SpeakRL improves task success from 25.63% to
  46.17% on MultiWOZ 2.4, reduces average dialogue turns from 8.12 to 5.82, and outperforms
  larger proprietary models despite using a smaller open-source model.
---

# SpeakRL: Synergizing Reasoning, Speaking, and Acting in Language Models with Reinforcement Learning

## Quick Facts
- arXiv ID: 2512.13159
- Source URL: https://arxiv.org/abs/2512.13159
- Authors: Emre Can Acikgoz; Jinoh Oh; Jie Hao; Joo Hyuk Jeon; Heng Ji; Dilek Hakkani-Tür; Gokhan Tur; Xiang Li; Chengyuan Ma; Xing Fan
- Reference count: 22
- Primary result: SpeakRL improves task success from 25.63% to 46.17% on MultiWOZ 2.4 using a smaller open-source model than competing proprietary systems.

## Executive Summary
SpeakRL introduces a reinforcement learning approach that enables language models to proactively ask clarification questions during multi-turn dialogues. The method uses structured tokens (dende markers) to separate reasoning and clarification turns, and trains via GRPO with custom rewards for format compliance and clarification quality. The approach achieves significant improvements in task success rate and dialogue efficiency compared to baselines, demonstrating that effective clarification behavior directly leads to better task completion.

## Method Summary
SpeakRL trains language models to proactively ask clarification questions when user requests are ambiguous or underspecified. The method uses GRPO (Group Relative Policy Optimization) with structured tokens—dende tokens to separate reasoning and clarification turns—and custom rewards: a binary format reward for correct token usage and an LLM-as-judge reward for clarification quality. The model is trained on synthetic dialogues from the SpeakER dataset, which contains ambiguous user requests derived from MultiWOZ 2.4. Training optimizes only for clarification quality, not task success directly, yet achieves significant improvements in both metrics.

## Key Results
- Task success rate improves from 25.63% to 46.17% on MultiWOZ 2.4
- Average dialogue turns reduced from 8.12 to 5.82
- Outperforms larger proprietary models despite using a smaller open-source model
- Effectively learns to ask targeted clarification questions that resolve user ambiguity

## Why This Works (Mechanism)
SpeakRL works by training models to recognize ambiguity and generate appropriate clarification questions through structured reasoning and reinforcement learning. The dende tokens force explicit reasoning before clarification, while the GRPO framework with custom rewards shapes behavior toward asking high-quality, targeted questions. The LLM-as-judge reward provides nuanced feedback on clarification quality beyond simple format compliance. By focusing training on clarification quality rather than task success directly, the model learns effective information-gathering strategies that naturally lead to better task completion.

## Foundational Learning
- **Multi-turn dialogue systems**: Needed to understand the context of ongoing conversations and track user goals across turns.
  - Quick check: Can identify when a user request lacks sufficient information to proceed
- **Reinforcement learning for language models**: Required to optimize non-differentiable behaviors like asking good questions.
  - Quick check: Can implement GRPO with custom reward functions
- **Structured token prompting**: Used to separate reasoning and action steps in model outputs.
  - Quick check: Can parse and generate text with dende token markers
- **LLM-as-judge evaluation**: Necessary for providing feedback on qualitative aspects of clarification questions.
  - Quick check: Can implement binary scoring of clarification quality using an LLM
- **Synthetic data generation**: Required to create training data with controlled ambiguity.
  - Quick check: Can inject underspecification into dialogue trajectories

## Architecture Onboarding

**Component map:**
User Simulator -> SpeakER Dataset -> GRPO Training -> SpeakRL Agent -> LLM Judge

**Critical path:**
1. User simulator generates ambiguous dialogue scenarios from MultiWOZ goals
2. SpeakRL agent processes user input, generates clarification questions using structured tokens
3. LLM judge evaluates task completion and clarification quality
4. GRPO updates agent policy based on rewards

**Design tradeoffs:**
- Uses smaller open-source model vs. larger proprietary alternatives
- Optimizes clarification quality proxy rather than direct task success
- Relies on synthetic data rather than human-annotated clarification examples
- Employs binary format rewards plus LLM-as-judge for nuanced feedback

**Failure signatures:**
- Excessive clarification questions (reward hacking)
- Premature "no clarification needed" decisions
- Superficial reasoning patterns in think tokens
- Inconsistent behavior across different types of ambiguity

**3 first experiments:**
1. Implement user simulator to generate consistent ambiguous dialogue scenarios
2. Train baseline model without clarification capability on standard MultiWOZ
3. Evaluate model's ability to recognize and respond to underspecified requests

## Open Questions the Paper Calls Out
1. **Reward design for excessive clarification**: The current design doesn't penalize unnecessary questions, potentially leading to reduced user satisfaction through excessive dialogue length.
2. **Generalization from synthetic data**: Training on synthetic data generated by specific LLMs may limit performance on real-world user distributions due to the "sim-to-real" gap.
3. **Unified multi-task reward functions**: Developing composite rewards that jointly optimize clarification, task execution, and response quality could improve overall performance.
4. **Self-judging stability**: The extent to which the model relies on external LLM judges versus self-judging mechanisms for training stability remains unclear.

## Limitations
- The SpeakER dataset synthesis pipeline is incompletely specified, particularly regarding ambiguity injection rules and n-gram filtering thresholds
- The user simulator implementation details are only referenced without full specification
- The LLM-as-judge evaluation protocol lacks complete prompt templates and scoring rubrics
- The method may not generalize well to human-authored ambiguous queries outside the MultiWOZ schema

## Confidence
- **High confidence**: GRPO training methodology with structured tokens is clearly specified and reproducible
- **Medium confidence**: Core experimental results are likely reproducible given training procedure, though exact numbers may vary
- **Medium confidence**: Comparison to larger proprietary models is valid but depends on evaluation setup
- **Low confidence**: Exact SpeakER dataset construction and full LLM-as-judge protocol cannot be faithfully reproduced without additional specification

## Next Checks
1. Implement and validate the user simulator using the referenced Xu et al. 2024 methodology, ensuring it can generate consistent ambiguous dialogue scenarios from MultiWOZ 2.4 goals
2. Reproduce the SpeakER dataset synthesis pipeline by implementing synthetic dialogue generation, ambiguity injection rules, and n-gram-based redundancy filtering
3. Recreate the LLM-as-judge evaluation by implementing the binary scoring mechanism and testing with multiple judge models to verify consistency