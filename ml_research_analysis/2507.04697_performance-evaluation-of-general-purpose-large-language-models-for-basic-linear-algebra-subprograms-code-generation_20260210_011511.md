---
ver: rpa2
title: Performance Evaluation of General Purpose Large Language Models for Basic Linear
  Algebra Subprograms Code Generation
arxiv_id: '2507.04697'
source_url: https://arxiv.org/abs/2507.04697
tags:
- code
- uplo
- trans
- diag
- side
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated the code generation capabilities of two general-purpose
  Large Language Models (LLMs), GPT-4.1 and o4-mini, for Basic Linear Algebra Subprograms
  (BLAS) routines. The models were tested on generating C code from BLAS routine names
  alone, with and without basic optimizations (thread parallelization, SIMD vectorization,
  and cache blocking), and based on Fortran reference code.
---

# Performance Evaluation of General Purpose Large Language Models for Basic Linear Algebra Subprograms Code Generation

## Quick Facts
- arXiv ID: 2507.04697
- Source URL: https://arxiv.org/abs/2507.04697
- Reference count: 24
- Two general-purpose LLMs (GPT-4.1, o4-mini) can generate correct BLAS code with optimizations without specific code training

## Executive Summary
This study evaluates whether general-purpose Large Language Models can generate correct and optimized Basic Linear Algebra Subprograms (BLAS) code without specialized training. The researchers tested GPT-4.1 and o4-mini on 20 double-precision BLAS routines across three levels, comparing unoptimized generation, optimized generation with thread parallelization and SIMD vectorization, and generation based on Fortran reference code. Results show that both models can generate correct implementations from routine names alone, with o4-mini outperforming GPT-4.1. The generated optimized code achieved significant speedups over reference BLAS implementations, particularly for level-3 routines, demonstrating practical numerical code generation capability despite the models not being specifically trained for code generation.

## Method Summary
The study generated C implementations of 20 BLAS routines using OpenAI's GPT-4.1 and o4-mini models through API calls. Three prompt variants were used: NameToCcode (routine name only), NameToOptCcode (with optimization instructions), and FrtcodeToOptCcode (with Fortran reference code). For each prompt, 10 code samples were generated due to stochastic output. Generated codes were compiled without optimization flags and tested using BLAS++'s exhaustive test harness with various parameter combinations. Performance was benchmarked on Intel Xeon Gold 6230 processors, measuring GB/s for level-1/2 routines and GFlops/s for level-3 routines against reference BLAS implementations.

## Key Results
- Both GPT-4.1 and o4-mini can generate correct BLAS implementations from routine names alone, with o4-mini achieving higher pass rates
- Generated optimized code achieved 3-20x speedups over reference BLAS for most routines
- Generated code structures differ from Fortran references, suggesting models learned specifications rather than memorizing code
- o4-mini's reasoning capabilities contributed to better correctness rates compared to GPT-4.1
- The models successfully applied HPC optimization techniques including OpenMP parallelization and SIMD vectorization without domain-specific fine-tuning

## Why This Works (Mechanism)

### Mechanism 1: Specification Learning from Documentation
- Claim: LLMs generate BLAS implementations by learning mathematical specifications from documentation rather than memorizing reference code structures.
- Mechanism: Models internalize routine descriptions, parameter semantics, and mathematical definitions from web-accessible BLAS documentation, then synthesize novel code paths that satisfy the specification.
- Core assumption: Training corpora contain sufficient BLAS documentation and tutorial content for the model to form accurate semantic representations.
- Evidence anchors:
  - [abstract]: "Generated code often differed structurally from reference Fortran code, suggesting models learned specifications from documentation rather than code structure."
  - [section 4.1]: "In many cases, the structure of the generated code differs clearly from the Fortran reference code...these results can be inferred that the LLMs do not always learn the reference code as the best correct answer but that they learn the specifications of routines with documents available on the Internet."
  - [corpus]: No direct corpus support; neighboring papers focus on different domains.
- Break condition: If models primarily memorized reference implementations, generated code would replicate Fortran loop structures and branching patterns.

### Mechanism 2: Reasoning Model Advantage for Numerical Code
- Claim: Models with internal reasoning capabilities (o4-mini's reasoning tokens) achieve higher correctness rates for numerical code generation than standard GPT models.
- Mechanism: Reasoning tokens enable multi-step logical inference—decomposing mathematical operations, planning loop structures, and validating edge cases—before output generation.
- Core assumption: Numerical code benefits from explicit intermediate reasoning steps similar to mathematical problem-solving.
- Evidence anchors:
  - [abstract]: "o4-mini outperforming GPT-4.1"
  - [section 4.1, Table 3]: o4-mini passes correctness tests in more routines (e.g., idamax: 10/10 vs GPT-4.1's 3/10 for NameToCcode).
  - [corpus]: Weak corpus support; no neighboring papers directly compare reasoning vs. non-reasoning models for code.
- Break condition: If reasoning tokens provide no domain-specific advantage, both models would show equivalent pass rates.

### Mechanism 3: Optimization Pattern Transfer
- Claim: General-purpose LLMs can apply HPC optimization techniques (OpenMP, SIMD, cache blocking) to numerical code without domain-specific fine-tuning.
- Mechanism: Models transfer optimization patterns learned from general code corpora to numerical contexts, selecting appropriate pragmas and intrinsics based on prompt instructions.
- Core assumption: Optimization patterns are sufficiently represented across training domains to enable cross-domain transfer.
- Evidence anchors:
  - [section 4.2]: "For thread parallelization, OpenMP is used...In most cases, OpenMP's #pragma omp simd was used, but some codes use the AVX2 and AVX-512 SIMD intrinsics."
  - [section 4.2, Tables 4-6]: Generated optimized code achieves 3-20x speedups over reference BLAS for multiple routines.
  - [corpus]: No direct corpus support for optimization transfer mechanisms.
- Break condition: If optimization knowledge is siloed by domain, models would fail to apply HPC techniques to unfamiliar numerical routines.

## Foundational Learning

- Concept: **BLAS Routine Taxonomy**
  - Why needed here: Understanding Level-1 (vector), Level-2 (matrix-vector), and Level-3 (matrix-matrix) operations determines expected memory access patterns and optimization strategies.
  - Quick check question: Which BLAS level is typically memory-bandwidth-bound vs. compute-bound?

- Concept: **HPC Optimization Techniques**
  - Why needed here: The paper evaluates OpenMP parallelization, SIMD vectorization, and cache blocking—readers must understand why these matter for numerical performance.
  - Quick check question: Why does cache blocking improve performance for matrix-matrix operations?

- Concept: **LLM Code Generation Uncertainty**
  - Why needed here: The paper generates 10 codes per prompt due to stochastic output; understanding temperature and sampling helps interpret variability in results.
  - Quick check question: What does it mean when only 3/10 generated codes pass correctness tests?

## Architecture Onboarding

- Component map:
  Prompt construction -> OpenAI API (GPT-4.1/o4-mini) -> C code generation -> Compilation with gcc -> BLAS++ test harness -> Performance benchmarking

- Critical path:
  1. Prompt construction (routine name + optimization instructions)
  2. Code generation via API (10 samples per prompt)
  3. Compilation with gcc (no optimization flags)
  4. Correctness validation against reference BLAS
  5. Performance benchmarking (GB/s for Level-1/2, GFlops/s for Level-3)

- Design tradeoffs:
  - **GPT-4.1 vs. o4-mini**: Wider context vs. reasoning capability; o4-mini shows better correctness but lower context window
  - **Prompt-only vs. Fortran reference**: Trade-off between model autonomy and specification fidelity; Fortran reference improves complex routines but may bias code structure
  - **10 samples per prompt**: Increases success probability but raises API costs; necessary due to stochastic generation

- Failure signatures:
  - **Compilation errors**: Incorrect C syntax from Fortran-influenced generation
  - **Runtime errors (segfaults)**: Mishandled negative `incx`/`incy` stride values
  - **Numerical errors**: Incorrect transpose handling or missing edge cases (e.g., non-square matrices)
  - **Performance regressions**: Missing optimization for specific parameter combinations (e.g., dtrmm side=R cases show 0.1x performance)

- First 3 experiments:
  1. **Baseline correctness test**: Generate Level-1 routines (dasum, daxpy, ddot) with NameToCcode prompt; verify both models achieve >8/10 pass rates.
  2. **Optimization capability test**: Generate dgemm with NameToOptCcode; inspect generated code for OpenMP pragmas and SIMD directives before performance measurement.
  3. **Fortran reference comparison**: Generate dtrsm (complex Level-3 routine) with and without Fortran reference attachment; compare pass rates and analyze code structure differences.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does including specific hardware parameters (e.g., cache size, SIMD width) in the prompt allow LLMs to select optimal block sizes and vectorization strategies for specific target architectures?
- Basis in paper: [inferred] The authors note in Section 4.2 that generated codes used generic block sizes (e.g., 64) because processor specs were not in the prompt, suggesting tailored inputs might improve optimization.
- Why unresolved: The study used generic prompts without hardware specifics, so the impact of hardware-aware prompting on cache blocking efficiency remains untested.
- What evidence would resolve it: A comparative experiment where prompts include specific CPU architecture details (e.g., L1/L2 cache sizes) followed by performance analysis of the resulting block sizes.

### Open Question 2
- Question: How does the performance of LLM-generated BLAS code compare against highly optimized vendor libraries (e.g., Intel MKL, OpenBLAS) rather than the unoptimized reference BLAS used in this study?
- Basis in paper: [inferred] The paper establishes speedups (up to 46x) over the unoptimized LAPACK reference BLAS, but does not benchmark against the industry-standard optimized libraries used in practical HPC environments.
- Why unresolved: The experimental design explicitly limited the performance baseline to the unoptimized reference implementation to assess raw generation capability, leaving the gap to peak hardware performance unknown.
- What evidence would resolve it: Benchmarking the generated "NameToOptCcode" and "FrtcodeToOptCcode" implementations against a vendor library like Intel MKL on the same hardware.

### Open Question 3
- Question: Can an iterative refinement process, where compilation errors or runtime bugs are fed back to the model, improve the success rate for complex routines (e.g., `dtrsm`) that currently fail in a single-shot generation?
- Basis in paper: [explicit] The Conclusion states that one could "iteratively modify prompts or feed back bugs... to improve the code" as a future direction distinct from the single-shot method used.
- Why unresolved: The current methodology generated code in single attempts; the potential for self-correction or interactive debugging to fix the incorrect codes observed in Table 3 was not evaluated.
- What evidence would resolve it: An experiment where failed generation attempts for `dtrsm` or `dtrmm` are re-prompted with the compiler error logs to measure the increase in correct code yield.

## Limitations

- Correctness validation relies on exhaustive parameter testing rather than mathematical proof of algorithmic equivalence
- Performance measurements use fixed problem sizes that may not represent real-world workloads
- Evaluation compares against unoptimized reference BLAS without optimization flags
- Study does not benchmark against highly optimized vendor libraries like Intel MKL or OpenBLAS

## Confidence

**High Confidence Claims:**
- Both GPT-4.1 and o4-mini can generate working BLAS implementations from routine names alone
- o4-mini consistently outperforms GPT-4.1 on correctness rates
- Generated optimized code achieves significant speedups over reference BLAS implementations
- Generated code structures differ from Fortran references, indicating specification learning rather than memorization

**Medium Confidence Claims:**
- Reasoning capabilities directly cause o4-mini's performance advantage
- The 10-sample generation approach is necessary and sufficient for capturing model capability
- Performance improvements scale meaningfully to real-world applications

**Low Confidence Claims:**
- Cross-domain optimization transfer mechanism is well-understood
- Model behavior would generalize to other numerical computing domains beyond BLAS
- The observed correctness rates represent true model capability rather than prompt engineering effects

## Next Checks

1. **Correctness Verification Analysis**: Manually inspect 5-10 failing cases from each model to determine if failures stem from actual algorithmic errors versus format incompatibilities or test harness limitations.

2. **Alternative Sampling Strategy Test**: Repeat key experiments using temperature=0.7 or top_p=0.9 to assess whether the 10-sample approach is optimal or if different sampling parameters yield higher pass rates per sample.

3. **Performance-to-Cost Ratio Analysis**: Calculate performance per API call dollar spent for each model and prompt variant to determine cost-effectiveness for production use cases.