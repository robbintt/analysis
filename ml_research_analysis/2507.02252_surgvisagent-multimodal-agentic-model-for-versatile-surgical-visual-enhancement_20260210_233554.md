---
ver: rpa2
title: 'SurgVisAgent: Multimodal Agentic Model for Versatile Surgical Visual Enhancement'
arxiv_id: '2507.02252'
source_url: https://arxiv.org/abs/2507.02252
tags:
- surgical
- enhancement
- distortion
- surgvisagent
- severity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SurgVisAgent is a multimodal agentic framework that dynamically
  identifies and corrects various surgical image distortions (low-light, overexposure,
  motion blur, smoke) by integrating domain-specific priors, in-context few-shot learning,
  and chain-of-thought reasoning. Unlike single-task methods, it unifies enhancement
  tasks within one system.
---

# SurgVisAgent: Multimodal Agentic Model for Versatile Surgical Visual Enhancement

## Quick Facts
- arXiv ID: 2507.02252
- Source URL: https://arxiv.org/abs/2507.02252
- Authors: Zeyu Lei; Hongyuan Yu; Jinlin Wu; Zhen Chen
- Reference count: 39
- Primary result: Unified agent framework achieving 84% severity accuracy, 97% distortion category accuracy, and 83% combined predictions on surgical image enhancement benchmark

## Executive Summary
SurgVisAgent introduces a multimodal agentic framework that dynamically identifies and corrects various surgical image distortions (low-light, overexposure, motion blur, smoke) through integrated domain-specific priors, in-context few-shot learning, and chain-of-thought reasoning. Unlike single-task methods, it unifies enhancement tasks within one system, achieving state-of-the-art performance on a comprehensive benchmark with superior enhancement metrics (SSIM: 0.922, PSNR: 31.95). The framework leverages GPT-4o's reasoning capabilities with structured surgical domain knowledge to deliver customized image enhancements tailored to diverse distortion types.

## Method Summary
SurgVisAgent employs a ResNet-based prior model trained on annotated surgical images to generate soft labels encoding distortion category and severity probabilities, which compensate for the MLLM's lack of surgical domain expertise. The system uses GPT-4o with Base64 image encoding and K=15 in-context few-shot examples to recognize patterns and infer appropriate labels without parameter updates. Chain-of-thought reasoning decomposes inference into sequential steps, enabling dynamic selection and invocation of enhancement models (DiffLL, FECNet, MIMO-UNet+, DeSmoke-LAP) for single or composite distortions. The framework is trained on EndoVis18 and DeSmoke-LAP datasets with synthesized distortions for comprehensive evaluation.

## Key Results
- 84% accuracy for severity prediction and 97% for distortion category identification
- 83% combined prediction accuracy for simultaneous distortion and severity classification
- Superior enhancement metrics: SSIM 0.922 and PSNR 31.95 compared to single-task models

## Why This Works (Mechanism)

### Mechanism 1
Domain-specific prior models improve MLLM surgical scene understanding by providing calibrated distortion/severity probability distributions. A ResNet-based prior model trained on annotated surgical images generates soft labels via temperature-scaled softmax (T=1.1), encoding uncertainty about distortion categories and severity levels. These priors are fed to the MLLM as structured context, compensating for the MLLM's lack of surgical domain expertise. The core assumption is that soft labels from a trained prior model carry more actionable information than raw MLLM perception alone.

### Mechanism 2
In-context few-shot learning enables task-specific enhancement selection without parameter updates. The system provides k=15 labeled examples (image, distortion category, severity) as context to the pretrained MLLM. Rather than fine-tuning, the MLLM uses these exemplars to recognize patterns in the query image and infer appropriate labels. The core assumption is that the pretrained MLLM has sufficient visual reasoning capacity to generalize from 15 examples to novel surgical images.

### Mechanism 3
Chain-of-thought reasoning enables dynamic, interpretable model selection for composite distortions. CoT decomposes inference into sequential reasoning steps (illumination analysis → anomaly analysis → final judgment → planning → function-calling). Each step conditions on previous outputs, allowing the agent to identify multiple distortions and invoke corresponding enhancement models sequentially or in combination. The core assumption is that structured intermediate reasoning improves selection accuracy over direct prediction.

## Foundational Learning

- **Soft Labels and Knowledge Distillation**: Why needed here: The prior model outputs probability distributions rather than hard classifications, preserving uncertainty information that guides MLLM reasoning. Understanding temperature scaling is essential for interpreting prior quality. Quick check: Given logits [2.0, 1.0, 0.1] for three distortion classes, what happens to the probability distribution as temperature T increases from 1.0 to 2.0?

- **In-Context Learning vs. Fine-Tuning**: Why needed here: SurgVisAgent deliberately avoids MLLM fine-tuning due to scarce annotated surgical data. Understanding this distinction clarifies why sample quality (diversity, representativeness) dominates sample quantity. Quick check: If deployment encounters a novel distortion type not in the 15 few-shot examples, will in-context learning generalize? Why or why not?

- **Function-Calling / Tool-Use in LLM Agents**: Why needed here: The MLLM doesn't perform enhancement directly—it selects and invokes external models (DiffLL, FECNet, etc.). Understanding tool orchestration is critical for debugging selection failures. Quick check: In a composite distortion case (smoke + motion blur), should the agent invoke DeSmoke-LAP before or after MIMO-UNet+? What factors determine the ordering?

## Architecture Onboarding

- Component map: Surgical Visual Input → [Surgical Prior Model (ResNet)] → Soft Labels (distortion/severity probs) → [Visual Encoder (Base64)] → [MLLM Agent (GPT-4o)] → [In-Context Few-Shot Examples] → [CoT Reasoning Engine] → Enhancement Model Selection → [Enhancement Models Set: DiffLL | FECNet | MIMO-UNet+ | DeSmoke-LAP]

- Critical path: Input → Prior Model (soft labels) + Base64 Encoding → MLLM with CoT → Model Selection → Enhancement Execution. Failure at the Prior Model or CoT stage propagates directly to wrong enhancement selection.

- Design tradeoffs: Unified agent vs. specialized single-task models sacrifices some task-specific optimization for flexibility and reduced deployment complexity. GPT-4o API dependency vs. local inference introduces latency and privacy concerns. Soft label temperature (T=1.1) smooths distributions for stability but may understate confidence when the prior model is certain.

- Failure signatures: Low accuracy on severity prediction (84%) vs. category identification (97%) suggests severity assessment is the weaker link. Poor performance on composite third-order distortions indicates CoT reasoning may not adequately decompose overlapping distortions. Smoke detection accuracy drop challenges both prior and agent with real-world variability.

- First 3 experiments:
  1. Prior Model Ablation: Run SurgVisAgent with prior model disabled to isolate the prior's contribution to accuracy gains
  2. Few-Shot Sample Sensitivity: Vary k from 0 to 25 while measuring category/severity accuracy to validate sample richness matters more than quantity
  3. Composite Distortion Ordering Test: Create synthetic images with known ground-truth composite distortions, run full pipeline, and verify CoT selects enhancement models in optimal order

## Open Questions the Paper Calls Out

- How can SurgVisAgent be optimized to meet the strict latency constraints required for real-time intraoperative deployment? The current framework relies on GPT-4o and multiple sequential enhancement models, which introduces significant inference latency unsuitable for live surgical feedback.

- Can the domain-specific priors and few-shot learning approach generalize effectively to entirely different surgical specialties or anatomical structures? The current prior models and benchmarks are trained on specific datasets, and the agent's ability to handle vastly different visual textures without retraining is unproven.

- Does the reliance on synthesized distortions for low-light, overexposure, and blur limit the model's robustness to complex, organic noise patterns found in real operating rooms? Synthetic distortions often lack the stochastic complexity of real physical phenomena, potentially leading to over-optimistic performance metrics.

- Does the sequential application of enhancement models for composite distortions result in cumulative artifacts or diagnostic information loss? Applying enhancement algorithms sequentially could amplify artifacts or hallucinate features not present in the original tissue.

## Limitations

- Benchmark composition relies on synthesized distortions and limited real surgical images, with generalization to in vivo surgical videos untested
- MLLM dependency on GPT-4o's reasoning capabilities and API availability creates performance coupling and privacy concerns
- Composite distortion handling shows reduced accuracy (83% combined predictions) with unoptimized sequential enhancement ordering

## Confidence

- **High Confidence**: Unified agent framework concept, CoT reasoning structure, and overall enhancement performance metrics (SSIM 0.922, PSNR 31.95)
- **Medium Confidence**: Domain-specific prior model contribution (84% severity accuracy) and in-context few-shot learning effectiveness
- **Low Confidence**: Composite distortion ordering strategy and real-world surgical deployment robustness

## Next Checks

1. Real Surgical Video Deployment: Run SurgVisAgent on raw endoscopic video feeds from multiple surgical specialties to measure accuracy degradation from benchmark conditions
2. Enhancement Model Sequencing Optimization: Systematically evaluate all possible enhancement model invocation orders for composite distortions to determine optimal sequencing
3. Few-Shot Sample Diversity Study: Replace fixed K=15 examples with systematically varied pools to quantify how sample richness versus quantity affects accuracy