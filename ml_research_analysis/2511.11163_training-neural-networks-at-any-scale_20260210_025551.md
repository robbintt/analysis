---
ver: rpa2
title: Training Neural Networks at Any Scale
arxiv_id: '2511.11163'
source_url: https://arxiv.org/abs/2511.11163
tags:
- learning
- norm
- which
- neural
- scaling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reviews modern optimization methods for training neural
  networks, emphasizing efficiency and scalability. The core contribution is a unified
  algorithmic framework that integrates both geometry-aware methods (like preconditioned
  stochastic gradient descent) and conditional gradient methods (like stochastic Frank-Wolfe).
---

# Training Neural Networks at Any Scale

## Quick Facts
- **arXiv ID:** 2511.11163
- **Source URL:** https://arxiv.org/abs/2511.11163
- **Reference count:** 40
- **Primary result:** Unified framework integrating geometry-aware optimization and conditional gradient methods with Maximal Update Parameterization enabling width-independent hyperparameter transfer

## Executive Summary
This paper presents a unified algorithmic framework that bridges geometry-aware optimization methods (like preconditioned stochastic gradient descent) with conditional gradient methods (like stochastic Frank-Wolfe). The core contribution is a master template that systematically derives optimizers through the choice of dual feedback and regularizers, enabling both spectral and diagonal preconditioning. The framework is then applied to address the fundamental challenge of scaling deep learning models, introducing Maximal Update Parameterization (µP) as a principled approach for ensuring feature learning and enabling hyperparameter transfer across model widths.

The authors demonstrate that µP achieves width-independent feature evolution by carefully aligning layerwise initialization variances and learning rates, allowing optimal hyperparameters to transfer from small to large models. Practical implementations like Scion and Muon have been used to train models with trillions of parameters. The work combines theoretical analysis via Tensor Programs with extensive empirical validation, providing actionable insights for scaling neural networks effectively while maintaining optimization efficiency.

## Method Summary
The paper introduces a unified algorithmic template x_{k+1} = argmin(γ_k⟨d_k, x⟩ + h_k(x)) that encompasses both geometry-aware methods (PreSGDW) and conditional gradient methods (SCG). The framework systematically derives optimizers through choices of dual feedback d_k and regularizer h_k, with spectral norms yielding dense updates via SVD-based linear minimization oracles. The Maximal Update Parameterization (µP) is implemented through abc-scaling (W_ℓ = p^{-a_ℓ}w_ℓ, init variance p^{-2b_ℓ}, LR η·p^{-c_ℓ}) with specific parameters ensuring Θ(1) feature evolution across widths. Practical implementations include diagonal optimizers (AdamW variants) and spectral methods (Scion, Muon) using Newton-Schultz approximations for GPU efficiency.

## Key Results
- Unified algorithmic framework that integrates preconditioned SGD and conditional gradient methods through geometry choice
- Maximal Update Parameterization (µP) enables width-independent feature evolution and hyperparameter transfer across model scales
- Spectral preconditioning methods (Scion, Muon) achieve faster convergence than diagonal methods (AdamW) on large-scale models
- Layerwise radius scaling ρ_ℓ ∝ √(p_ℓ/p_{ℓ-1}) provides built-in regularization through Frank-Wolfe interpretation

## Why This Works (Mechanism)

### Mechanism 1
The unified algorithmic template enables systematic derivation of optimizers through geometry choice. The master template x_{k+1} = argmin(γ_k⟨d_k, x⟩ + h_k(x)) generates SGD, Adam, Shampoo, and Muon as instantiations differing only in dual feedback d_k and regularizer h_k (which encodes norm geometry). Spectral norms yield dense updates via SVD-based LMOs. Core assumption: Problem structure can be exploited a priori better than on-the-fly diagonal adaptation. Break condition: If SVD computation cost dominates training time, switch to Newton-Schultz approximations.

### Mechanism 2
Maximal Update Parameterization (µP) enables hyperparameter transfer across widths by ensuring Θ(1) feature evolution in all layers. Layerwise abc-scaling with specific choices (a_ℓ=0, b_ℓ∈{0,½,1}, c_ℓ∈{-1,0,1}) guarantees ∥Δh_ℓ(z)∥_RMS = Θ(1) as width→∞. First layer needs LR scaled as ηp, last layer as η/p. Core assumption: Infinite-width analysis predicts finite-width behavior. Break condition: Architectures not representable as Tensor Programs (e.g., Mamba) may not transfer reliably.

### Mechanism 3
Stochastic Conditional Gradient (SCG) reinterprets weight decay as Frank-Wolfe stepsize with built-in norm control. SCG update x_{k+1} = (1-λ_k)x_k + λ_k·ρ·lmo(d_k) constrains ∥x∥ ≤ ρ by design. The "weight decay" λ is the actual stepsize; nominal LR only sets constraint radius. Layerwise radii ρ_ℓ scale with spectral condition (√(p_ℓ/p_{ℓ-1})). Core assumption: Norm-bounded parameters correlate with generalization. Break condition: PyTorch's coupled weight decay implementation breaks µP transfer unless refactored.

## Foundational Learning

- **Dual norms and the sharp operator**: Understanding how ∥·∥_∗ and the sharp operator [s]♯ = argmax{⟨s,x⟩ - ½∥x∥²} generate optimizer updates from gradients. *Quick check:* For ℓ_∞ norm, what is lmo(d) and how does it differ from [d]♯?

- **Neural Tangent Kernel vs. feature learning regimes**: Distinguishing lazy training (no feature evolution, kernel methods) from feature learning requires understanding why NTK parameterization fails for large models. *Quick check:* Why does standard parameterization keep ∥ΔW∥_S∞ → 0 as width→∞ while µP maintains Θ(1)?

- **Operator norms (RMS→RMS, spectral) for layers**: The spectral condition uses operator norms to bound activation changes; layerwise Lipschitz constants derive from these. *Quick check:* How does ∥W∥_{RMS→RMS} relate to ∥W∥_{S∞} and why does this matter for initialization variance?

## Architecture Onboarding

- **Component map:** Input: Model architecture + target scale → µP analysis: Determine layerwise (a,b,c) for abc-parameterization → Norm choice: Select ∥·∥_Wℓ per layer → Optimizer template: PreSGDW vs SCG → LMO solver: SVD vs Newton-Schultz/PolarExpress vs Sketching → Output: Width-agnostic training with transferable hyperparameters

- **Critical path:** (1) Verify architecture supports Tensor Programs → (2) Run coordinate check (Figure 6) to validate feature learning → (3) Set layerwise radii ρ_ℓ per spectral condition → (4) Tune λ on small model, transfer to large

- **Design tradeoffs:**
  - Diagonal (AdamW) vs Spectral (Scion/Muon): Diagonal is O(p) compute/memory; spectral needs SVD at O(p²p₁) but exploits matrix structure. Muon uses Newton-Schultz for GPU efficiency.
  - SCG vs uSCG: SCG constrains parameters (better regularization per Figure 3) but requires λ∈[0,1]; uSCG unconstrained may diverge on long runs.
  - Full vs. approximate LMO: QR methods need full precision; Newton-Schultz works in bf16 but may increase rank.

- **Failure signatures:**
  - Coordinate check shows preactivations growing/shrinking with width → wrong parameterization (SP instead of µP)
  - Learning rate needs retuning at each width → transfer broken (check PyTorch weight decay coupling)
  - Spectral LMO dominates training time → switch from exact SVD to Newton-Schultz or sketching
  - Momentum=0 in non-Euclidean setting → convergence to noise-dominated region (Page 11)

- **First 3 experiments:**
  1. **Coordinate check baseline:** Train 3-layer MLP at widths [256, 512, 1024] with AdamW+SP and AdamW+µP; plot ∥h_ℓ∥_RMS vs width. Expect SP to explode, µP to stay flat.
  2. **LR transfer validation:** On NanoGPT at widths [512, 1024, 2048], sweep learning rate with µP. Verify optimal LR is constant across widths (reproduce Figure 1 middle panel).
  3. **Optimizer geometry comparison:** Train Vision Transformer with AdamW vs Scion on ImageNet subset. Monitor validation loss curves and final accuracy (expect Scion faster convergence per Figure 1 right, Figure 7).

## Open Questions the Paper Calls Out

### Open Question 1
What is the theoretical mechanism that explains the superior empirical performance of the Lion optimizer's dual feedback estimator compared to standard momentum? The paper notes on page 12 that the Lion estimator was discovered empirically and "theoretically its mechanism remains to be explained." A formal proof showing variance reduction properties or convergence rates for the Lion estimator that strictly improve upon standard momentum in the non-convex setting would resolve this.

### Open Question 2
How can the Tensor Programs framework be generalized to derive optimal scaling rules for modern state-space models like Mamba? Page 19 states that certain architectures like Mamba "cannot be represented as a Tensor Program" and "rigorously establishing their infinite-width limits requires a significant generalization of the existing framework." A theoretical extension of the Tensor Programs framework that encompasses state-space operations would resolve this.

### Open Question 3
How can the spectral linear minimization oracle (LMO) be efficiently distributed to address system-level bottlenecks in massive-scale training? The outlook section on page 26 identifies that "how to distribute the spectral lmo becomes a challenge" at massive scale, despite active research. The development of a distributed algorithm for the spectral LMO that maintains convergence properties while minimizing communication overhead would resolve this.

## Limitations
- The Tensor Programs framework cannot analyze certain modern architectures like Mamba, limiting µP applicability
- Spectral preconditioning methods require expensive SVD computations, though Newton-Schultz approximations help
- The SCG weight decay interpretation and Lion optimizer mechanism lack rigorous theoretical justification
- External validation of the unified framework is limited to the paper's internal taxonomy

## Confidence
- **High Confidence:** The unified algorithmic template and mathematical framework for spectral and diagonal preconditioning
- **Medium Confidence:** The µP parameterization theory and its empirical validation, the coordinate check methodology
- **Low Confidence:** The SCG weight decay interpretation, generalization to architectures beyond standard feed-forward networks

## Next Checks
1. **Architecture Generalization Test:** Apply µP analysis to a non-standard architecture (e.g., Vision Transformer with LayerNorm variations or a hybrid CNN-Transformer) and verify whether the coordinate check still shows width-independent feature evolution.

2. **Large-Scale Transfer Validation:** Train a model at multiple scales (small: 512 width, medium: 4096, large: 32768) using µP with optimal hyperparameters from the smallest model. Measure the deviation in validation performance to quantify the limits of hyperparameter transfer.

3. **LMO Solver Benchmarking:** Implement and compare exact SVD, Newton-Schultz, and sketching-based LMO solvers on the same model at scale (e.g., 1B parameter network). Measure wall-clock time, memory usage, and final training performance to establish practical tradeoffs.