---
ver: rpa2
title: Does UMBRELA Work on Other LLMs?
arxiv_id: '2507.09483'
source_url: https://arxiv.org/abs/2507.09483
tags:
- umbrela
- relevance
- llms
- prompt
- passage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reproduces the UMBRELA LLM-based relevance assessment
  framework across different LLM families to evaluate its generalizability. The study
  compares GPT-4o, DeepSeek V3, LLaMA-3.3-70B, LLaMA-3-8B, and FLAN-T5-large using
  the same UMBRELA prompt across TREC Deep Learning datasets.
---

# Does UMBRELA Work on Other LLMs?

## Quick Facts
- arXiv ID: 2507.09483
- Source URL: https://arxiv.org/abs/2507.09483
- Reference count: 13
- This paper reproduces the UMBRELA LLM-based relevance assessment framework across different LLM families to evaluate its generalizability.

## Executive Summary
This paper systematically evaluates the generalizability of the UMBRELA LLM-based relevance assessment framework across multiple model families including GPT-4o, DeepSeek V3, LLaMA-3.3-70B, LLaMA-3-8B, and FLAN-T5-large. Using the same UMBRELA prompt across TREC Deep Learning datasets, the study demonstrates that while all models can generate leaderboards closely matching human rankings, there are significant differences in per-label agreement. The results show a clear scaling effect where larger models achieve substantially higher agreement with human judgments while maintaining strong ranking capabilities.

## Method Summary
The study applies the UMBRELA framework using a zero-shot DNA prompt across five different LLM models to evaluate passage relevance for TREC Deep Learning datasets (2019, 2020, 2023). The prompt asks models to assess query-passage pairs on a 4-point relevance scale based on trustworthiness, intent alignment, and information quality. Results are compared using multiple metrics including nDCG@10 for ranking performance and Cohen's kappa for per-label agreement. The study also analyzes instruction-following capability by measuring format compliance rates and investigates the relationship between model scale and evaluation quality.

## Key Results
- GPT-4o and DeepSeek V3 achieve the highest per-label agreement (Cohen's κ ≈ 0.3-0.5), while smaller models show much lower fine-grained agreement (κ dropping to 0.06-0.2)
- All model variants can differentiate between top-performing and lower-performing systems, maintaining strong rank correlation (Spearman's ρ ≈ 0.9-0.99) even for small-scale FLAN-T5-large
- Model scale significantly impacts instruction-following capability, with smaller models frequently failing the strict output format requirements and requiring more complex parsing logic

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** High-capacity LLMs (GPT-4o class) transfer zero-shot relevance assessment capabilities across model families more reliably than smaller models.
- **Mechanism:** The UMBRELA prompt uses a "DNA" (Descriptive, Narrative, Aspects) structure that elicits reasoning about query intent and trustworthiness. Large models appear to map these implicit instructions to the 4-point relevance scale with higher consistency than smaller models, which struggle with the nuance between borderline cases (e.g., score 1 vs. 2).
- **Core assumption:** The model possesses sufficient parametric knowledge to assess "trustworthiness" and "intent alignment" without explicit reasoning steps or few-shot examples.
- **Evidence anchors:**
  - [abstract] "Results demonstrate that UMBRELA with DeepSeek V3 obtains very comparable performance to GPT-4o... For LLaMA-3.3-70B we obtain slightly lower performance, which further degrades with smaller LLMs."
  - [section] Section 3.2 notes that "per-label agreement metrics (κ) benefit more substantially from increased model scale than the leaderboard rank correlation measures."
  - [corpus] Neighbors such as *Reverse Engineering Human Preferences* suggest LLM-judge vulnerability to tuning, but this paper suggests architecture scale is the primary driver for zero-shot UMBRELA accuracy.
- **Break condition:** Performance degrades if the target model lacks the instruction-following capacity to respect the strict output format or semantic granularity required by the 4-point scale.

### Mechanism 2
- **Claim:** Leaderboard generation is robust to model scale, but fine-grained label agreement is not.
- **Mechanism:** Relevance labels are used to compute system rankings (e.g., nDCG). Even if a model makes "mistakes" on specific labels (lower Cohen's κ), as long as the errors are consistent or random (rather than systematically inverted), the relative ordering of systems (Spearman's ρ, Kendall's τ) remains high. Smaller models act as noisy but roughly monotonic evaluators.
- **Core assumption:** The evaluation goal is system comparison (ranking), not the generation of high-quality training data for downstream fine-tuning (which requires accurate per-label fidelity).
- **Evidence anchors:**
  - [section] Section 3.4 states: "all model variants can differentiate between top-performing and lower-performing systems... even for the small-scale FLAN-T5-large."
  - [section] Table 2 shows FLAN-T5-large dropping to κ=0.062 (scale) while maintaining ρ=0.971 on DL 2023.
  - [corpus] *Re-Rankers as Relevance Judges* explores similar themes where ranking utility is preserved even with imperfect point-wise scoring.
- **Break condition:** If errors become systematic (e.g., consistently rating irrelevant documents as relevant), rank correlation will collapse.

### Mechanism 3
- **Claim:** Strict output formatting instructions function as an instruction-following test that filters out smaller or less-aligned models.
- **Mechanism:** The prompt demands a specific string `##final score: [score]` without reasoning. Smaller models (LLaMA-3-8B) frequently fail this constraint (1.6–1.9% error rate), outputting reasoning steps or malformed strings. This requires complex fallback parsing (regex for "O: X" or standalone numbers), introducing noise.
- **Core assumption:** An ideal judge model should prioritize instruction adherence over "helpful" conversational filler.
- **Evidence anchors:**
  - [section] Section 3.6 details that smaller models "frequently produced inconsistent outputs... which necessitated more complex parsing logic."
  - [section] Table 4 shows 0.00% invalid outputs for DeepSeek V3 vs. ~1.7% for LLaMA-3-8B.
  - [corpus] *The Stability Trap* supports the difficulty of consistent instruction adherence in LLM-auditing contexts.
- **Break condition:** If the parsing logic is too brittle, format errors will be misclassified as "Score 0", artificially inflating irrelevant labels.

## Foundational Learning

- **Concept: Cohen's Kappa (κ) vs. Rank Correlation (ρ/τ)**
  - **Why needed here:** This paper hinges on the divergence between these metrics. You cannot assess the "failure" of a small model without understanding that κ measures exact label match (hard for small models) while ρ/τ measure system ordering (easy for small models).
  - **Quick check question:** If a model assigns scores 1-1-1 to three systems where the ground truth is 2-2-2, would Rank Correlation or Kappa be low?

- **Concept: Graded Relevance (0-3 Scale)**
  - **Why needed here:** The UMBRELA prompt distinguishes between "Related" (1), "Highly Relevant" (2), and "Perfectly Relevant" (3). Understanding these boundaries is necessary to debug why models conflate 2 and 3.
  - **Quick check question:** According to the prompt, should a passage that contains the answer but includes extraneous info receive a score of 2 or 3?

- **Concept: Zero-Shot DNA Prompting**
  - **Why needed here:** Unlike few-shot prompting, this relies entirely on the model's pre-existing alignment and reasoning. The paper tests if this "free lunch" works across architectures.
  - **Quick check question:** Does the UMBRELA prompt explicitly ask the model to "think step-by-step" in the final output, or does it restrict the output to the score only?

## Architecture Onboarding

- **Component map:** TREC Query + Passage -> Zero-Shot Bing Prompt -> API (GPT-4o/DeepSeek) or Local (LLaMA/FLAN) -> Regex Parser -> Evaluator (nDCG@10, Cohen's κ)

- **Critical path:** The prompt template is fixed; the critical variable is the **Inference Backend**. If switching models, the primary engineering effort is tuning the **Parser** to handle the new model's adherence (or lack thereof) to the `##final score` format.

- **Design tradeoffs:**
  - **Accuracy vs. Cost:** DeepSeek V3 approaches GPT-4o performance (κ ~0.26–0.36) at potentially lower cost, but requires API reliance. LLaMA-3.3-70B is slightly weaker but open-weights.
  - **Ranking vs. Labels:** If you only need a leaderboard, use FLAN-T5-large (fastest/cheapest). If you need training labels, you *must* use GPT-4o/DeepSeek.

- **Failure signatures:**
  - **Format Drift:** LLaMA models outputting `M: 3, T: 3, O: 3` instead of the requested header.
  - **Scale Collapse:** Smaller models defaulting to "helpful" explanations rather than the integer score.
  - **Kappa Drop:** A sudden drop in κ without a drop in ρ indicates the model is consistently "optimistic" or "pessimistic" but preserving relative order.

- **First 3 experiments:**
  1. **Baseline Reproduction:** Run the provided UMBRELA prompt on GPT-4o for DL 2023 and verify κ ≈ 0.31 (Table 2) to validate your parsing pipeline.
  2. **Format Stress Test:** Run the prompt on a smaller open-source model (e.g., LLaMA-3-8B) and log the "Invalid Output %" to measure instruction adherence capability.
  3. **Ranking vs. Label Analysis:** Compute both nDCG@10 and Binary κ for the smaller model. Confirm that ρ remains > 0.90 even if κ drops < 0.20.

## Open Questions the Paper Calls Out

- **Question:** Is the lower performance of small-scale models (LLaMA-3-8B, FLAN-T5-large) on per-label agreement inherent to their limited capacity, or is the UMBRELA prompt simply not well-suited to smaller LLMs?
- **Question:** Can UMBRELA be effectively adapted for conversational relevance or multi-turn QA evaluation settings?
- **Question:** Would fine-tuning smaller models specifically for relevance assessment close the per-label agreement gap with larger models?

## Limitations

- The study's relatively small number of test queries (80 for DL 2019, 47 for DL 2020, 50 for DL 2023) limits generalization to other IR tasks or domains
- The parsing logic for non-compliant outputs introduces potential noise that wasn't fully characterized in terms of its impact on downstream metrics
- The study doesn't explore edge cases where systematic errors might break the pattern of preserved ranking despite poor per-label agreement

## Confidence

- **High Confidence:** The finding that larger models (GPT-4o, DeepSeek V3) achieve substantially higher per-label agreement (κ ≈ 0.3-0.5) while maintaining strong ranking performance
- **Medium Confidence:** The conclusion that FLAN-T5-large can produce reliable leaderboards despite poor per-label agreement (κ ≈ 0.06-0.2)
- **Medium Confidence:** The comparative performance of DeepSeek V3 versus GPT-4o suggests cost-effective alternatives exist

## Next Checks

1. **Cross-Domain Validation:** Test the UMBRELA framework on non-TREC datasets (e.g., academic search, e-commerce) to verify that the scale-dependent performance patterns hold across different query types and document structures.

2. **Prompt Template Robustness:** Systematically vary the UMBRELA prompt (e.g., changing the DNA structure, adding few-shot examples, or modifying output format requirements) to identify which components are essential for achieving high κ across different model families.

3. **Error Analysis at Scale:** Conduct a detailed analysis of the specific relevance judgments where smaller models disagree with ground truth versus where larger models disagree, to determine whether smaller models make random errors (which preserve ranking) or systematic errors (which could break rank correlation in certain scenarios).