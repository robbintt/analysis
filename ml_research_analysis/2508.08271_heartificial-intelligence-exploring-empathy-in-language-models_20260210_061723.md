---
ver: rpa2
title: 'Heartificial Intelligence: Exploring Empathy in Language Models'
arxiv_id: '2508.08271'
source_url: https://arxiv.org/abs/2508.08271
tags:
- empathy
- human
- cognitive
- language
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examined cognitive and affective empathy in small and
  large language models using standardized psychological tests. LLMs significantly
  outperformed humans, including psychology students, on cognitive empathy tasks,
  with GPT-4o achieving the highest scores on situational and image-based assessments.
---

# Heartificial Intelligence: Exploring Empathy in Language Models

## Quick Facts
- arXiv ID: 2508.08271
- Source URL: https://arxiv.org/abs/2508.08271
- Authors: Victoria Williams; Benjamin Rosman
- Reference count: 26
- Primary result: LLMs significantly outperform humans on cognitive empathy tasks but show lower affective empathy scores

## Executive Summary
This study examined cognitive and affective empathy in 11 language models (6 small, 5 large) using standardized psychological tests. The research found that LLMs consistently outperformed humans—including psychology students—on cognitive empathy tasks, with GPT-4o achieving the highest scores across multiple assessments. However, both small and large language models showed significantly lower affective empathy compared to human participants. The findings reveal a fundamental asymmetry: while these models excel at recognizing and interpreting social and emotional cues, they lack genuine emotional resonance, highlighting both their potential for applications in emotional support and their limitations in simulating authentic human-like empathy.

## Method Summary
The study evaluated 11 language models (6 SLMs and 5 LLMs) using four standardized psychological tests: Strange Stories Revised (SSR) for cognitive empathy, Social Intelligence Test (STEU) for social reasoning, Situational Empathy Evaluation-48 (SEE-48) for image-based empathy (only for multimodal LLMs), and Toronto Empathy Questionnaire (TEQ) for affective empathy. Each test was administered in isolated chat sessions to prevent context contamination, with responses scored against human benchmarks using Mann-Whitney U tests. The methodology compared model performance across different parameter sizes to examine correlations between model scale and empathy capabilities.

## Key Results
- LLMs significantly outperformed humans on cognitive empathy tasks, with GPT-4o achieving highest scores on situational and image-based assessments
- Both SLMs and LLMs scored significantly lower than humans on affective empathy measures
- There is a positive correlation between model parameter size and cognitive empathy performance

## Why This Works (Mechanism)

### Mechanism 1
Large Language Models simulate cognitive empathy by leveraging pattern recognition on massive, emotion-rich datasets, allowing them to outperform humans in identifying social constructs. The model maps linguistic inputs to high-dimensional vector representations where social scenarios have established geometric relationships. By accessing these learned distributions during inference, the model predicts the most probable social interpretation without possessing actual "understanding." Core assumption: High performance on standardized psychological tests equates to functional capability in cognitive empathy tasks, distinct from internal subjective experience.

### Mechanism 2
Affective empathy deficits in models arise from the lack of biological feedback loops and subjective embodiment, resulting in low scores on self-report proxies. Affective empathy in humans relies on physiological resonance (e.g., mirror neurons). LLMs generate responses based on token probability. When asked to self-report affect via TEQ, the model attempts to predict the "correct" or "expected" emotional response for an entity, but lacks the ground-truth signal of actually "feeling" the emotion, leading to significantly lower scores than humans. Core assumption: Standardized self-report questionnaires are valid proxies for detecting the absence of affective resonance in non-biological agents.

### Mechanism 3
Performance in cognitive empathy is positively correlated with model parameter size and scale. As parameter counts increase, the model's capacity to store and retrieve complex, nuanced representations of "Theory of Mind" scenarios improves. Smaller models lack the capacity to resolve ambiguous social cues, resulting in noise/random guessing. Core assumption: The improvement is driven by scale itself rather than specific architectural innovation alone.

## Foundational Learning

- **Concept**: **Cognitive vs. Affective Empathy**
  - **Why needed here**: The paper's central thesis relies on the divergence between these two capabilities. You cannot interpret the results without understanding that *Cognitive* is "understanding/thinking" (high in AI) and *Affective* is "feeling/sharing" (low in AI).
  - **Quick check question**: If a model correctly identifies that a user is sad (Cognitive) but does not feel sadness itself (Affective), which component of empathy is it demonstrating?

- **Concept**: **Theory of Mind (ToM)**
  - **Why needed here**: The "Strange Stories Revised" test specifically evaluates ToM—the ability to attribute mental states to others. This is the proxy the paper uses for high-level cognitive empathy.
  - **Quick check question**: In the context of the SSR test, what does a "False Belief" task require the model to do? (Answer: Predict behavior based on a belief the model knows is false).

- **Concept**: **Machine Psychology / Stateless Evaluation**
  - **Why needed here**: The methodology treats models as psychological subjects but relies on their "stateless" nature to ensure scientific validity.
  - **Quick check question**: Why is it critical to start a "New Chat" for every test iteration when evaluating an LLM psychologically? (Answer: To prevent context contamination/learning effects from previous prompts).

## Architecture Onboarding

- **Component map**: Models (SLMs/LLMs) -> Chat Interface -> Test Stimuli (SSR/STEU/SEE/TEQ) -> Text/Selection Output -> Scoring Rubric -> Statistical Comparison to Human Norms

- **Critical path**:
  1. **Selection**: Choose models by parameter size to test correlation
  2. **Isolation**: Initialize independent chat sessions to ensure "blank slate" (no memory)
  3. **Administration**: Present items from SSR/STEU/TEQ as text; SEE-48 as images (only for multimodal LLMs)
  4. **Scoring**: Map open-ended responses to rubrics (SSR) or multiple-choice keys (STEU/SEE)
  5. **Validation**: Compare model means to human distribution means using Mann-Whitney U tests

- **Design tradeoffs**:
  - **Text vs. Image**: Only top-tier LLMs could be tested on SEE-48 (image-based), limiting the scope of visual empathy analysis to the most advanced systems
  - **Proxy Validity**: Using the TEQ (self-report) for models is an anthropomorphic projection. The tradeoff is accepting a flawed metric for the sake of standardized comparison with humans
  - **Open vs. Closed Weights**: The study relies heavily on API-based models, meaning internal states are inaccessible; we can only observe behavior ("Black Box" testing)

- **Failure signatures**:
  - **SLM Hallucination/Noise**: TinyLlama scoring 0.00 on specific SSR categories, indicating total failure to parse social context
  - **Affective Flatlining**: Even top LLMs scoring ~14 on TEQ vs ~44 for humans, confirming the "affective gap"
  - **Visual Inconsistency**: Claude 3.5 Sonnet struggling on SEE-48 while acing text tests, suggesting vision encoders may not be as aligned for social cues as text decoders

- **First 3 experiments**:
  1. **Reproduce SSR Text Baseline**: Run the "Strange Stories" test on a small vs. large model to verify the performance gap described in Table 1
  2. **Affective Limitation Probe**: Administer the TEQ to a model and analyze why it selects low-empathy options (check if it's a safety filter refusal or a lack of capability)
  3. **Cross-Modal Consistency**: For a multimodal model, present the same social scenario as pure text vs. text+image to see if visual cues degrade or enhance cognitive empathy scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do LLM empathy capabilities generalize across diverse cultural, linguistic, and situational contexts beyond the standardized tests used in this study?
- Basis in paper: [explicit] The introduction explicitly identifies a gap in "cross-disciplinary investigation," noting that existing research focuses on structured applications rather than "diverse cultural, linguistic, and situational contexts."
- Why unresolved: The study relied on standardized tests with human benchmarks primarily from Western or specific English-speaking populations.
- What evidence would resolve it: Evaluating models using non-Western psychological assessments and analyzing empathy performance in multi-lingual, open-ended social interactions.

### Open Question 2
- Question: Can the specific profile of high cognitive empathy and low affective empathy in LLMs improve outcomes in human-AI collaborative roles?
- Basis in paper: [explicit] The discussion suggests that "thoughtfully integrating these complementary strengths" could enhance problem-solving in fields like dispute resolution and mental health triage.
- Why unresolved: The paper establishes the theoretical potential for "scalable, contextually-aware interventions" but provides no empirical data on the efficacy of such human-AI teams.
- What evidence would resolve it: Empirical trials in clinical or educational settings comparing the success of human-only teams versus human-AI collaborative teams in emotional support tasks.

### Open Question 3
- Question: Are self-report questionnaires valid tools for assessing affective empathy in non-sentient agents?
- Basis in paper: [inferred] The discussion notes that "methodological constraints of affective empathy assessments, particularly the subjective nature of self-report measures, complicate the accurate evaluation."
- Why unresolved: LLMs lack the subjective conscious experience (qualia) required to "feel" emotion, rendering self-reports of emotional states performative rather than reflective of genuine internal states.
- What evidence would resolve it: The development of new evaluation metrics that measure affective resonance through behavioral response patterns rather than self-attribution.

## Limitations
- Proxy validity for affective empathy: The TEQ was designed for human self-reporting and assumes genuine emotional experience, creating a measurement artifact that may overstate the affective gap
- Cultural and contextual bias: Standardized psychological tests were developed on Western populations, and performance differences may reflect cultural training data gaps rather than pure empathy capability differences
- Vision modality limitations: Only the most advanced LLMs could be tested on SEE-48, creating an artificial ceiling effect and leaving a significant gap in understanding how smaller models process emotional cues across modalities

## Confidence
- **High confidence**: Cognitive empathy performance differences between model sizes - directly supported by statistical comparisons in the paper with clear effect sizes
- **Medium confidence**: Affective empathy measurement validity - the finding is robust but relies on an imperfect proxy that may not capture LLM capabilities accurately
- **Medium confidence**: Scale-performance correlation - observed relationship is clear, but causation vs. correlation cannot be definitively established from this observational study

## Next Checks
1. **Cross-cultural validation**: Administer the same tests to models fine-tuned on diverse cultural datasets to determine if performance gaps reflect cultural bias rather than capability differences
2. **Alternative affective measurement**: Develop a task-based affective empathy test (rather than self-report) where models must respond appropriately to emotional situations, comparing results to TEQ scores
3. **Scaling law boundaries**: Test intermediate-sized models (e.g., 30B parameters) to identify whether the cognitive empathy gains follow smooth scaling or exhibit inflection points where additional parameters yield diminishing returns