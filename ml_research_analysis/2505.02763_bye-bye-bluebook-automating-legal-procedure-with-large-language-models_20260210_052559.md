---
ver: rpa2
title: Bye-bye, Bluebook? Automating Legal Procedure with Large Language Models
arxiv_id: '2505.02763'
source_url: https://arxiv.org/abs/2505.02763
tags:
- legal
- bluebook
- citation
- case
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper evaluates large language models\u2019 (LLMs) ability\
  \ to follow complex legal citation rules by constructing a dataset of 866 Bluebook\
  \ tasks. Five flagship LLMs were tested in zero-shot settings, with Gemini 2.5 Flash\
  \ also evaluated using in-context learning on the Bluebook\u2019s rules."
---

# Bye-bye, Bluebook? Automating Legal Procedure with Large Language Models
## Quick Facts
- arXiv ID: 2505.02763
- Source URL: https://arxiv.org/abs/2505.02763
- Reference count: 40
- Primary result: Large language models produce fully compliant Bluebook citations only 69%-74% of the time, with errors requiring substantial correction

## Executive Summary
This paper evaluates large language models' ability to follow complex legal citation rules by constructing a dataset of 866 Bluebook tasks. Five flagship LLMs were tested in zero-shot settings, with Gemini 2.5 Flash also evaluated using in-context learning on the Bluebook's rules. Results show models produce fully compliant Bluebook citations only 69%-74% of the time, with in-context learning raising accuracy only to 77%. The models perform best on case law tasks but struggle significantly with statutes, regulations, and secondary sources. When errors occur, they are typically non-trivial, requiring an average of 14 character edits to correct.

## Method Summary
The study constructs a dataset of 866 Bluebook citation tasks representing diverse legal source types. Five flagship LLMs are tested in zero-shot settings, with Gemini 2.5 Flash also evaluated using in-context learning on the Bluebook's rules. Models receive citation tasks and must generate compliant Bluebook citations. Accuracy is measured by comparing model outputs to correct citations, with error severity assessed by the number of character edits required for correction.

## Key Results
- Large language models produce fully compliant Bluebook citations only 69%-74% of the time in zero-shot settings
- In-context learning raises accuracy to 77%, showing minimal improvement
- Errors typically require an average of 14 character edits to correct, indicating non-trivial mistakes
- Models perform best on case law tasks but struggle significantly with statutes, regulations, and secondary sources

## Why This Works (Mechanism)
The study demonstrates that large language models, despite their general language capabilities, struggle with the precise formatting and procedural requirements of legal citation systems. The Bluebook's complex rules for different source types create challenges that current models cannot consistently overcome, even with in-context learning approaches.

## Foundational Learning
- **Legal citation systems** (why needed: Understanding the Bluebook's role in legal writing and why procedural fidelity matters)
- **Zero-shot learning evaluation** (why needed: Assessing model performance without task-specific training)
- **In-context learning techniques** (why needed: Evaluating whether providing rule examples improves performance)
- **Error measurement methodology** (why needed: Quantifying citation accuracy and error severity)
- **Statistical significance testing** (why needed: Determining whether performance differences are meaningful)
- **Legal domain knowledge** (why needed: Understanding why certain citation types are more challenging than others)

## Architecture Onboarding
Component map: Citation Task -> LLM Input -> Model Output -> Accuracy Evaluation -> Error Analysis
Critical path: Task formulation → Model generation → Citation compliance checking → Error quantification
Design tradeoffs: Zero-shot vs. in-context learning, task diversity vs. dataset size, accuracy vs. computational cost
Failure signatures: Systematic errors in specific citation types, inconsistent formatting, rule misapplication
First experiments:
1. Test chain-of-thought prompting to see if reasoning steps improve citation accuracy
2. Fine-tune a model on a citation dataset to establish upper bounds on performance
3. Evaluate model performance on citation systems from other jurisdictions

## Open Questions the Paper Calls Out
None

## Limitations
- Results focus specifically on the Bluebook citation system in U.S. legal context, limiting generalizability
- Zero-shot and in-context learning settings may not represent optimal prompting strategies
- Does not address how citation errors might compound in real-world legal workflows
- Does not test more sophisticated approaches like chain-of-thought prompting or fine-tuning

## Confidence
High confidence: LLMs should not be used to automate legal procedures requiring procedural fidelity (69-77% accuracy, substantial correction needed)
Medium confidence: Models perform best on case law tasks (relative performance differences not always statistically significant)
High confidence: Non-trivial errors requiring 14 character edits are common (robust finding, practical impact uncertain)

## Next Checks
1. Test additional prompting strategies including chain-of-thought and fine-tuning approaches to establish whether current results represent a lower bound on LLM citation capabilities
2. Evaluate model performance on citation systems from other jurisdictions (e.g., OSCOLA for UK legal citations) to assess generalizability
3. Conduct user studies with legal professionals to determine whether the measured citation errors would actually impact legal outcomes in practice