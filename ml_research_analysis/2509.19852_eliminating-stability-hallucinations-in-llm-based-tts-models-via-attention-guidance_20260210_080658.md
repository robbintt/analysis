---
ver: rpa2
title: Eliminating stability hallucinations in llm-based tts models via attention
  guidance
arxiv_id: '2509.19852'
source_url: https://arxiv.org/abs/2509.19852
tags:
- speech
- alignment
- text
- tokens
- cosyv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses stability hallucinations in LLM-based TTS
  models, where the models may generate repetitive or omitted speech when processing
  long or difficult text. The core method involves analyzing the attention mechanism
  in LLMs to identify "alignment heads" that are crucial for text-speech alignment.
---

# Eliminating stability hallucinations in llm-based tts models via attention guidance

## Quick Facts
- arXiv ID: 2509.19852
- Source URL: https://arxiv.org/abs/2509.19852
- Reference count: 0
- Core result: OAS regularization and attention-guided training reduce WER by 2.1-1.6% on hard text while maintaining naturalness

## Executive Summary
This paper addresses stability hallucinations in decoder-only LLM-based TTS models, where models may generate repetitive or omitted speech when processing long or difficult text. The core method involves analyzing the attention mechanism in LLMs to identify "alignment heads" that are crucial for text-speech alignment. A metric called Optimal Alignment Score (OAS) is proposed to evaluate alignment quality using the Viterbi algorithm. OAS is integrated into the training of CosyVoice2 to help the model learn continuous and stable alignment. Additionally, a chain-of-thought (CoT) approach is used, leveraging pre-trained attention values to guide the training of a student CosyVoice2 model. Experiments on the Seed-TTS-Eval and CV3-Eval test sets show that the proposed methods effectively reduce stability hallucinations without introducing negative effects. Specifically, the OAS metric shows a strong correlation with word error rate (WER) in synthesized speech (correlation coefficient of 0.638). Incorporating OAS into CosyVoice2 reduces WER by 2.1% and 1.6% under hard text scenarios on Seed-TTS-Eval and CV3-Eval, respectively, while maintaining or slightly improving subjective speech naturalness. Attention-guided training further reduces stability hallucinations, with sparse text token supervision and progress bar value supervision showing better performance than full text token supervision.

## Method Summary
The paper proposes two complementary methods to eliminate stability hallucinations in LLM-based TTS. First, it analyzes attention mechanisms to identify "alignment heads" in middle layers (8-9) of CosyVoice2 that capture text-speech correspondence. An OAS metric is introduced to evaluate alignment quality using the Viterbi algorithm to find optimal monotonic paths through attention matrices. This OAS loss is integrated into training to optimize these alignment heads. Second, an attention-guided training approach uses chain-of-thought supervision, where a teacher model's attention patterns generate pseudo-labels (sparse text tokens and progress bar values) to train a student model. This approach leverages the teacher's alignment knowledge without requiring ground truth forced alignment.

## Key Results
- OAS metric shows strong correlation with WER (r=0.638) in synthesized speech
- OAS regularization reduces WER by 2.1% on Seed-TTS-Eval and 1.6% on CV3-Eval under hard text scenarios
- Sparse text token supervision (97.1% training accuracy) outperforms full text token supervision (90.4%)
- Methods maintain or slightly improve subjective speech naturalness while reducing hallucinations

## Why This Works (Mechanism)

### Mechanism 1: Alignment Heads in Decoder-Only LLMs Capture Text-Speech Correspondence
- Claim: Middle-layer attention heads in decoder-only TTS models implicitly learn text-speech alignment, functioning similarly to cross-attention in encoder-decoder architectures.
- Mechanism: When analyzing CosyVoice2's 24-layer transformer, heads in layers 8-9 exhibit "global forward alignment paths" where speech tokens attend monotonically to corresponding text tokens. Designating these as alignment heads and restricting their attention to text-speech regions allows targeted optimization.
- Core assumption: The observed attention patterns in layers 8-9 are causally responsible for alignment quality, not merely correlated.
- Evidence anchors:
  - [abstract] "analyzed the alignment mechanism between text tokens and speech tokens in LLMs"
  - [section 2.1] "middle-layer heads contain specific 'alignment heads'... weight regions from output speech tokens to input text tokens exhibit global forward alignment paths"
  - [corpus] Weak—neighbor papers don't analyze attention head specialization in TTS
- Break condition: If your model has different architecture/depth, alignment heads may reside in different layers. Run OAS analysis to locate them.

### Mechanism 2: OAS Provides Differentiable Alignment Quality Signal
- Claim: The Optimal Alignment Score (OAS) correlates strongly with WER (r=0.638) and can be used as a training objective to improve alignment stability.
- Mechanism: OAS uses Viterbi algorithm to find the optimal monotonic path through the attention matrix A (speech→text), then computes the ratio of attention probability along this path versus the full alignment region. Higher OAS = more focused, monotonic alignment. Since gradients flow through, OAS can directly optimize attention patterns.
- Core assumption: Monotonic, continuous alignment paths indicate correct text-speech correspondence and lead to fewer hallucinations.
- Evidence anchors:
  - [abstract] "OAS metric shows a strong correlation with word error rate (WER) in synthesized speech (correlation coefficient of 0.638)"
  - [section 3.2] "the coefficient between OAS and WER is 0.638"
  - [corpus] GOAT paper (arXiv:2508.15442) addresses hallucinations via distribution alignment but uses different approach
- Break condition: OAS assumes monotonic alignment is desirable—may not hold for prosodic variations, code-switching, or highly expressive speech.

### Mechanism 3: Sparse Supervision with Progress Bar Values Reduces Error Accumulation
- Claim: Using teacher attention-derived pseudo-labels with sparse text token supervision + progress bar values outperforms full text token supervision (97.1% vs 90.4% training accuracy).
- Mechanism: Full text token repetition (e.g., [t1, t1, t2, t2, t2]) suffers from boundary ambiguity and error accumulation when pseudo-labels are imperfect. Sparse supervision (e.g., [t1, M, M, t2, M]) marks each text token only once at reliable positions. Progress bar values (cumulative duration ratio) provide absolute positional grounding, helping the model track synthesis progress.
- Core assumption: Pseudo forced-alignment labels from teacher attention are sufficiently accurate to guide student training; sparse sampling avoids unreliable boundary regions.
- Evidence anchors:
  - [abstract] "sparse text token supervision and progress bar value supervision showing better performance than full text token supervision"
  - [section 3.4, Table 3] Training accuracy: sparse=97.10%, full=90.44%
  - [corpus] MegaTTS3 (cited in paper) uses similar sparse alignment approach
- Break condition: If teacher model has poor alignment quality, pseudo-labels will misguide student. Verify teacher OAS scores first.

## Foundational Learning

- Concept: **Decoder-only self-attention for sequence generation**
  - Why needed here: Unlike encoder-decoder TTS with explicit cross-attention, decoder-only models must learn text-speech alignment implicitly through causal self-attention. Understanding this distinction is critical for grasping why "alignment heads" emerge and how to exploit them.
  - Quick check question: Can you explain why a decoder-only model attending to its own prefix can still learn text→speech alignment?

- Concept: **Viterbi algorithm for optimal path finding**
  - Why needed here: OAS computation requires finding the maximum-probability monotonic path through the attention matrix. Viterbi provides the DP formulation to solve this efficiently in O(L_s × L_t).
  - Quick check question: Given an attention matrix A of shape [100 speech tokens × 20 text tokens], trace how Viterbi computes the optimal path P.

- Concept: **Chain-of-thought (CoT) supervision in TTS**
  - Why needed here: The attention-guided training uses CoT to interleave text token predictions with speech token predictions, strengthening positional correspondence without requiring ground-truth forced alignment.
  - Quick check question: Why might predicting text tokens as intermediate targets help a speech generation model?

## Architecture Onboarding

- Component map:
CosyVoice2 (Qwen-0.5B backbone)
├── 24 Transformer layers × 14 heads/layer
├── Layers 0-6: Global information modeling
├── Layers 8-9: ALIGNMENT HEADS (7 of 14 heads designated)
│   └── Receive OAS loss + attention mask restricting to alignment region
├── Layers 10+: Local/speech-focused
└── CoT training path (student model only):
    ├── Sparse text token targets: Os = [t1, M, M, t2, M, ...]
    └── Progress bar targets: Op (from teacher's optimal path)

- Critical path:
  1. **Identify alignment heads**: Compute OAS across all heads on validation set; select layers with highest mean OAS
  2. **Apply attention mask**: Restrict designated heads to text-speech alignment region only
  3. **Compute OAS loss**: L_OAS = -1/L_s Σ log(A[i, P[i]]) for designated heads
  4. **For student training**: Extract optimal path from teacher → generate sparse text + progress bar targets → train with combined loss

- Design tradeoffs:
  - **Head designation strategy**: Paper uses 7/14 heads in layers 8-9. Fewer heads = weaker signal; more heads = risk interfering with other functions. No ablation provided.
  - **Sparse vs. full supervision**: Sparse avoids boundary errors but provides less supervision signal. Trade-off depends on pseudo-label quality.
  - **Progress bar L1 + monotonicity loss**: First-order difference loss (Eq. 4) ensures predicted progress never decreases, but constrains model flexibility.

- Failure signatures:
  - WER increases on common text while decreasing on hard text → alignment heads may be over-constrained
  - Training instability with OAS loss → check attention mask implementation; gradient may flow to wrong regions
  - Student model worse than teacher → pseudo-labels from teacher attention path are unreliable (low OAS)

- First 3 experiments:
  1. **OAS-WER correlation validation**: On your model, synthesize 100-400 hard text samples, compute WER and OAS. Verify correlation coefficient ≥0.5 before proceeding.
  2. **Layer localization**: Compute mean OAS per layer. If peak is not at layers 8-9, adjust head designation accordingly.
  3. **Ablation on head count**: Test 3, 7, 10 heads per alignment layer. Monitor both WER and naturalness (MOS/UTMOS) to find sweet spot.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the distribution and importance of "alignment heads" generalize to LLM architectures with different scales or attention mechanisms?
- Basis in paper: [inferred] The experiments and analysis of alignment heads are restricted to CosyVoice2, which uses a Qwen-0.5B backbone (24 layers).
- Why unresolved: The identification of layers 8 and 9 as primary alignment heads may be specific to the 0.5B parameter scale, and it is unclear if deeper or different architectures exhibit similar localized alignment patterns.
- Evidence: Evaluation of OAS distributions across layers in larger models (e.g., 7B parameters) or different transformer configurations.

### Open Question 2
- Question: Is the manual designation of alignment heads optimal compared to a dynamic or learned selection mechanism?
- Basis in paper: [inferred] The paper manually designates half the heads in the 8th and 9th layers as alignment heads based on average OAS scores, without exploring adaptive selection.
- Why unresolved: Fixing specific heads might restrict the model's capacity to handle varying text complexities or alignment requirements that could benefit from engaging heads in other layers.
- Evidence: Ablation studies comparing the fixed assignment against a method that dynamically selects alignment heads based on per-input OAS scores.

### Open Question 3
- Question: Can the proposed attention-guided training effectively mitigate hallucinations in non-tonal languages like English without requiring architecture adjustments?
- Basis in paper: [inferred] All experiments are conducted on Mandarin datasets (WenetSpeech4TTS, Seed-TTS-Eval, CV3-Eval), which possess distinct tonal alignment characteristics.
- Why unresolved: The monotonic alignment constraints optimized by OAS may interact differently with the phoneme-to-grapheme mappings and stress patterns of languages other than Mandarin.
- Evidence: Testing the OAS regularization and sparse text token supervision on a large-scale English TTS dataset to observe WER reductions.

## Limitations

- **Architectural specificity**: The method's effectiveness depends heavily on identifying "alignment heads" in layers 8-9 of CosyVoice2 (Qwen-0.5B), which may not generalize to other decoder-only TTS architectures with different depths or attention patterns.
- **Pseudo-label reliability**: The attention-guided training relies on teacher model attention paths as pseudo ground truth, which may propagate alignment errors if the teacher model has poor alignment quality.
- **Objective tradeoff ambiguity**: While WER improves on hard text, the paper doesn't fully characterize the tradeoff between hallucination reduction and naturalness preservation, lacking quantitative support for "slight improvements" in subjective metrics.

## Confidence

**High confidence** (Evidence strongly supports claims):
- OAS-WER correlation coefficient of 0.638 (reported with statistical basis)
- WER reduction on hard text datasets (Seed-TTS-Eval: 2.1%, CV3-Eval: 1.6%)
- Sparse supervision outperforming full text token supervision (97.1% vs 90.4% training accuracy)

**Medium confidence** (Mechanistic claims supported but with assumptions):
- Middle-layer heads (8-9) are causally responsible for alignment quality (correlation observed but causality not proven)
- Monotonic alignment paths indicate correct text-speech correspondence (assumes monotonic alignment is always desirable)
- Progress bar values provide absolute positional grounding (assumes teacher's optimal path is reliable)

**Low confidence** (Limited experimental validation):
- Generalization to architectures beyond CosyVoice2 (no cross-model validation)
- Long-term stability of alignment improvements (no temporal analysis)
- Impact on expressive speech and prosody (no evaluation on highly expressive content)

## Next Checks

1. **OAS-WER correlation validation**: On your model, synthesize 100-400 hard text samples, compute WER and OAS for each. Verify correlation coefficient ≥0.5 before proceeding with OAS regularization. If correlation is weak, the alignment heads may reside in different layers for your architecture.

2. **Layer localization experiment**: Compute mean OAS per layer across your validation set. If the peak OAS is not at layers 8-9, adjust head designation accordingly. This validates whether the specific layer recommendation generalizes to your model.

3. **Attention mask implementation test**: Implement the attention mask as a binary matrix M where M[i,j]=1 if text token j aligns to speech token i (based on optimal path), and verify that attention weights are properly constrained. Monitor whether gradients flow through the correct regions by checking attention distribution changes during training.