---
ver: rpa2
title: 'VESSA: Video-based objEct-centric Self-Supervised Adaptation for Visual Foundation
  Models'
arxiv_id: '2510.20994'
source_url: https://arxiv.org/abs/2510.20994
tags:
- vessa
- video
- vision
- foundation
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VESSA addresses the challenge of adapting vision foundation models
  to new domains without labeled data by leveraging object-centric videos for self-supervised
  fine-tuning. The method employs a self-distillation paradigm with carefully designed
  training schedules, uncertainty-weighted losses, and parameter-efficient adaptation
  using LoRA to preserve pretrained knowledge while adapting to new domains.
---

# VESSA: Video-based objEct-centric Self-Supervised Adaptation for Visual Foundation Models

## Quick Facts
- **arXiv ID**: 2510.20994
- **Source URL**: https://arxiv.org/abs/2510.20994
- **Reference count**: 40
- **Primary result**: VESSA adapts vision foundation models to new domains using object-centric videos, achieving 91.85% accuracy on CO3D with DINOv2 (3.99 p.p. improvement over baseline).

## Executive Summary
VESSA addresses the challenge of adapting vision foundation models to new domains without labeled data by leveraging object-centric videos for self-supervised fine-tuning. The method employs a self-distillation paradigm with carefully designed training schedules, uncertainty-weighted losses, and parameter-efficient adaptation using LoRA to preserve pretrained knowledge while adapting to new domains. By using multi-view video frames as positive pairs, VESSA introduces greater visual variability compared to standard image augmentations, leading to more robust and object-centric representations. Experiments with three vision foundation models (DINO, DINOv2, and TIPS) on two datasets (MVImageNet and CO3D) show consistent improvements in downstream classification tasks.

## Method Summary
VESSA adapts vision foundation models (VFMs) to new domains using unlabeled object-centric videos through a self-supervised fine-tuning approach. The method samples frame pairs from videos with temporal gaps (δ∈[1, δ_max]) to create positive pairs, applies DINO-style augmentations including global and local crops, and uses a student-teacher architecture with EMA updates. Training proceeds in stages: first warming up the projection head while freezing the backbone, then applying LoRA to early attention layers while fully unfreezing the last few layers. The Uncertainty-Weighted Self-Distillation (UWSD) loss modulates contributions based on teacher uncertainty, and evaluation uses k-NN classification on frozen embeddings.

## Key Results
- VESSA achieves 91.85% accuracy on CO3D with DINOv2, outperforming the pretrained baseline by 3.99 percentage points
- The approach is computationally efficient, requiring only 1.97 hours for adaptation on a TPU v3-8
- Performance improvements are consistent across three different vision foundation models (DINO, DINOv2, and TIPS)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Temporal coherence from video frame pairs provides stronger positive pairs than augmented views of single images.
- Mechanism: Frame pairs sampled with temporal gap δ∈[1, δ_max] from the same video capture the same object under varied capture conditions (viewpoint, lighting, pose). This natural variability creates more semantically meaningful positive pairs than artificial augmentations alone, encouraging the model to learn object-centric rather than background-focused representations.
- Core assumption: Videos contain consistent object identity across frames with sufficient appearance variation to improve robustness.
- Evidence anchors: [abstract] "VESSA benefits significantly from multi-view object observations sourced from different frames in an object-centric video, efficiently learning robustness to varied capture conditions"; [section 3.2] "This randomized strategy introduces temporal diversity by allowing variable distances between frames, which helps the model learn more robust representations across different viewpoints"

### Mechanism 2
- Claim: Uncertainty-weighted self-distillation (UWSD) prioritizes harder examples and stabilizes gradient flow during fine-tuning.
- Mechanism: The entropy of the teacher's output distribution H(q) modulates each sample's contribution: w(q) = 1 + γ·H(q). Higher entropy (more uncertain predictions) receives higher weight, focusing learning on examples where the teacher is less confident—typically representing harder or boundary cases in the new domain.
- Core assumption: Teacher uncertainty correlates with informative examples for domain adaptation; the pretrained teacher provides meaningful uncertainty estimates.
- Evidence anchors: [section 3.2] "To prioritize uncertain teacher outputs, we introduce an Uncertainty-Weighted Self-Distillation (UWSD) loss, which modulates the contribution of each sample to the loss based on the estimated uncertainty"; [table 1] Ablation shows UWSD contributes ~1.35 p.p. improvement

### Mechanism 3
- Claim: Staged unfreezing with LoRA prevents catastrophic forgetting while enabling domain adaptation.
- Mechanism: Training proceeds in stages: (1) freeze backbone, train projection head only; (2) apply LoRA to first H layers (query/key/value projections); (3) fully unfreeze last L layers. Early layers preserve transferable low-level features; later layers adapt high-level semantics.
- Core assumption: Low-level features (edges, textures) transfer across domains; high-level semantics require more adaptation; LoRA's low-rank constraint prevents destructive updates.
- Evidence anchors: [section 3.2] "we initially freeze the backbone and train only the projection head for a few epochs, allowing it to adapt to the existing embedding space"; [section 3.2] "we enable fine-tuning of the first H layers using LoRA... while keeping the normalization layers trainable... the last L layers of the backbone are fully unfrozen"

## Foundational Learning

- **Concept: Self-distillation (DINO framework)**
  - Why needed here: VESSA builds directly on DINO's student-teacher architecture with EMA updates; understanding how the cross-entropy loss L_DINO = −Σp_t(x)log p_s(x) aligns distributions is essential for debugging training dynamics.
  - Quick check question: Can you explain why the teacher network uses EMA of student weights rather than gradient updates?

- **Concept: Low-Rank Adaptation (LoRA)**
  - Why needed here: VESSA applies LoRA to attention projections (Q, K, V) in early layers; understanding the decomposition ΔW = AB where r≪min(d,k) clarifies why this constrains updates.
  - Quick check question: What happens to LoRA's memory savings if you increase rank r from 4 to 64?

- **Concept: Vision Transformer (ViT) layer structure**
  - Why needed here: The staged unfreezing strategy requires distinguishing attention layers, normalization layers, and MLP blocks; layer-wise adaptation depends on this architectural knowledge.
  - Quick check question: In a ViT block, which components handle low-level features vs. high-level semantics?

## Architecture Onboarding

- **Component map**: Frame Selection → Preprocessing → Student Network (ViT + LoRA in layers 1-H, fully unfrozen in layers H+1 to end) → Teacher Network (EMA of student, processes only global view) → Projection Head (trainable, must be warmed up before backbone) → UWSD Loss → weights cross-entropy by teacher entropy

- **Critical path**:
  1. Initialize student/teacher from pretrained VFM (DINO/DINOv2/TIPS)
  2. Freeze backbone, train projection head for ~10 epochs
  3. Enable LoRA in first H layers (typically H=2), unfreeze last L layers (typically L=2)
  4. Train with UWSD loss on video frame pairs for ~10 epochs
  5. Extract embeddings, evaluate with k-NN (k=1)

- **Design tradeoffs**:
  - More unfrozen layers → higher capacity but increased forgetting risk (table 1 shows 2 layers optimal)
  - Larger temporal gap δ → more appearance diversity but risk of identity drift (table 2 suggests δ∈[5,10] optimal)
  - Local crops → +1.34 p.p. improvement but increased compute

- **Failure signatures**:
  - Projection head not warmed up → ~10 p.p. accuracy drop (table 1: 80.87% vs 91.87%)
  - Using static images instead of video → 3-10 p.p. drop depending on backbone (table 3)
  - Excessive forgetting → ImageNet accuracy drops from 76-82% to 15-18% after adaptation (table 7)

- **First 3 experiments**:
  1. **Baseline reproduction**: Apply pretrained DINO/DINOv2 to CO3D with k-NN evaluation; verify you achieve ~78.86%/87.86% accuracy.
  2. **Ablation on projection head warmup**: Compare training with vs. without initial head-only training (expect ~10 p.p. difference per table 1).
  3. **Temporal gap sensitivity**: Test fixed δ∈{1,5,10} vs. random δ∈[5,10] on a small subset to validate table 2 trends before full training.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can catastrophic forgetting be mitigated while preserving VESSA's domain adaptation benefits?
- **Basis in paper**: [explicit] The authors state: "A notable limitation of our approach is the tendency to forget previously acquired knowledge during fine-tuning — a known drawback of fine-tuning methods in general."
- **Why unresolved**: Table 7 shows ImageNet accuracy drops from 76–82% to 15–18% after VESSA adaptation, rendering adapted models unsuitable for general-purpose deployment.
- **What evidence would resolve it**: Experiments combining VESSA with continual learning techniques (e.g., elastic weight consolidation, replay buffers), reporting both target domain accuracy and retention of ImageNet performance.

### Open Question 2
- **Question**: Can VESSA be extended to non-object-centric videos with significant background clutter and multiple objects?
- **Basis in paper**: [explicit] "our experimental setup relies on video data that offer multiple viewpoints of the same object, a characteristic that is not commonly available in many real-world datasets."
- **Why unresolved**: All experiments use controlled object-centric datasets (CO3D, MVImageNet); performance on uncurated, cluttered real-world video remains untested.
- **What evidence would resolve it**: Evaluation on in-the-wild video datasets (e.g., YouTube-VIS, surveillance data) with cluttered scenes and multiple objects per frame.

### Open Question 3
- **Question**: What specific factors in real video data drive VESSA's improvements beyond what image augmentations can simulate?
- **Basis in paper**: [inferred] Table 6 shows motion-inspired augmentations (translation, rotation, scaling) did not improve performance, suggesting "advantages observed with real videos may stem from cues beyond simple geometric or photometric variation."
- **Why unresolved**: The paper demonstrates video superiority but does not disentangle whether gains arise from natural occlusion patterns, lighting variations, object deformation, or other phenomena absent in synthetic transforms.
- **What evidence would resolve it**: Controlled ablation using synthetic videos isolating individual factors (e.g., only viewpoint change, only lighting change) to identify causal mechanisms.

### Open Question 4
- **Question**: Does VESSA transfer to pixel-level tasks such as object detection and semantic segmentation?
- **Basis in paper**: [inferred] The paper evaluates only k-NN classification, noting prior video-based methods "target pixel-level tasks (e.g., segmentation, detection) and show limited improvements for frame-level classification."
- **Why unresolved**: It remains unknown whether VESSA's object-centric temporal consistency benefits dense prediction tasks requiring spatial localization.
- **What evidence would resolve it**: Benchmarking VESSA-adapted backbones on detection (COCO) and segmentation (ADE20K) protocols in target domains.

## Limitations

- **Dataset dependency**: VESSA requires object-centric videos, limiting applicability to domains without available video data
- **Computational overhead**: Training on videos requires processing multiple frames per sample, increasing memory and compute requirements
- **Catastrophic forgetting**: Adapted models lose significant ImageNet performance (accuracy drops from 76-82% to 15-18%)

## Confidence

- **High Confidence**: Video-based positive pairs provide meaningful supervision (validated by consistent improvements across all three backbones in tables 1 and 3)
- **Medium Confidence**: Uncertainty-weighted self-distillation improves adaptation (supported by ablation in table 1, but no external validation)
- **Medium Confidence**: Staged unfreezing with LoRA prevents catastrophic forgetting (demonstrated empirically, but mechanism not rigorously analyzed)

## Next Checks

1. **Temporal identity drift test**: Evaluate performance degradation when frame pairs contain different objects (e.g., using CO3D videos with scene transitions) to quantify the impact of noisy positive pairs.
2. **Hyperparameter ablation study**: Systematically vary LoRA rank (r∈{2,4,8,16}) and unfrozen layer count (L∈{1,2,3,4}) across all three backbones to identify robust defaults.
3. **Cross-domain generalization**: Test adapted models on entirely different object-centric video datasets (e.g., Something-Something v2) to assess domain transferability beyond the training video distribution.