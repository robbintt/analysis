---
ver: rpa2
title: What's in Common? Multimodal Models Hallucinate When Reasoning Across Scenes
arxiv_id: '2511.03768'
source_url: https://arxiv.org/abs/2511.03768
tags:
- objects
- reasoning
- arxiv
- dataset
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Common-O Bench, a new multi-image benchmark
  designed to test multimodal models' ability to reason across scenes. While current
  models excel at single-image perception tasks, they struggle when asked to identify
  common objects across multiple scenes.
---

# What's in Common? Multimodal Models Hallucinate When Reasoning Across Scenes

## Quick Facts
- arXiv ID: 2511.03768
- Source URL: https://arxiv.org/abs/2511.03768
- Reference count: 35
- Models achieve only 35% accuracy on cross-scene reasoning tasks

## Executive Summary
This paper introduces Common-O Bench, a benchmark designed to test multimodal models' ability to identify common objects across multiple scenes. While current models excel at single-image perception tasks, they struggle significantly when asked to reason across scenes. Testing 13 state-of-the-art models including those specifically trained for chain-of-thought reasoning, the authors find that the best-performing model (GPT-4o) achieves only 35% accuracy, with performance dropping to under 1% on more complex scenes. The study reveals that models frequently hallucinate objects when reasoning across scenes, particularly when objects are similar, suggesting reliance on object co-occurrence patterns rather than genuine reasoning. Models trained with multi-image inputs show significantly better performance, indicating this as a promising direction for future research.

## Method Summary
The authors created Common-O Bench, a new multi-image benchmark with 10.5k examples containing both real and synthetic images. The benchmark was carefully curated to avoid contamination from web training data by using images from books, AI-generated content, and non-web photography. Thirteen state-of-the-art multimodal models were tested, including those specifically trained for chain-of-thought reasoning. The benchmark includes multiple task types: identifying common objects, selecting correct objects from multiple choices, and open-ended questions about scene relationships. Synthetic scenes were also created to test model behavior in controlled environments where object co-occurrence patterns could be manipulated.

## Key Results
- Best-performing model (GPT-4o) achieves only 35% accuracy on Common-O Bench
- Performance drops to under 1% on more complex multi-scene reasoning tasks
- Models trained with multi-image inputs show significantly better performance than single-image trained models
- Models frequently hallucinate objects when reasoning across scenes, particularly when objects are similar

## Why This Works (Mechanism)
The paper demonstrates that multimodal models fail at cross-scene reasoning because they rely on object co-occurrence patterns learned during training rather than genuine reasoning about visual relationships. When objects appear together frequently in training data, models assume they must co-occur in test images even when they don't. This pattern-based reasoning breaks down when scenes contain similar but distinct objects or when the relationship between scenes requires understanding object uniqueness across contexts. The synthetic scene experiments provide clear evidence that models make decisions based on learned statistical patterns rather than visual understanding of what makes objects "common" across scenes.

## Foundational Learning
- Cross-modal alignment: Understanding how visual and language representations are integrated is crucial for diagnosing why models fail at multi-image reasoning tasks. Quick check: Examine attention maps between image regions and text tokens.
- Chain-of-thought reasoning: The paper tests models specifically trained for step-by-step reasoning, revealing that this approach alone doesn't solve cross-scene challenges. Quick check: Compare reasoning traces between successful and failed examples.
- Hallucination detection: Identifying when models generate plausible-sounding but incorrect responses is central to understanding their limitations. Quick check: Compare model confidence scores with ground truth accuracy.

## Architecture Onboarding

Component Map:
Input Images -> Visual Encoder -> Cross-Attention Layers -> Language Decoder -> Output Prediction

Critical Path:
Visual encoder processes each image independently -> Cross-attention layers attempt to align information across images -> Language decoder generates reasoning steps and final answer

Design Tradeoffs:
- Single vs. multi-image training: Models trained on multi-image tasks perform better but require more computational resources
- Chain-of-thought vs. direct prediction: Step-by-step reasoning doesn't improve performance, suggesting architectural rather than algorithmic limitations
- Real vs. synthetic data: Synthetic scenes allow controlled testing but may not capture all real-world complexities

Failure Signatures:
- High confidence in incorrect answers, especially for similar objects
- Reliance on object co-occurrence patterns rather than visual comparison
- Performance degradation as scene complexity increases
- Inability to distinguish between truly common objects and merely similar objects

First Experiments:
1. Test model performance on single-image variants of Common-O Bench tasks to establish baseline capabilities
2. Compare attention patterns between correct and incorrect responses to identify failure modes
3. Evaluate model performance after fine-tuning on curated multi-image datasets

## Open Questions the Paper Calls Out
The paper identifies several open questions including whether architectural changes beyond multi-image training could improve cross-scene reasoning, how to better evaluate model reasoning capabilities beyond accuracy metrics, and whether different training objectives might encourage more genuine reasoning rather than pattern matching.

## Limitations
- The benchmark may not capture all aspects of cross-scene reasoning that humans perform effortlessly
- The study focuses on English-language models, potentially limiting generalizability
- Performance differences between models could be influenced by factors beyond multi-image training, such as model size or training duration

## Confidence

High:
- 35% accuracy ceiling for even the best model is a robust finding that holds across multiple test conditions
- Experimental methodology is sound with clear metrics and reproducible procedures
- Contamination prevention measures ensure benchmark validity

Medium:
- Models rely on object co-occurrence patterns rather than genuine reasoning (inference about internal behavior)
- Synthetic scene experiments provide suggestive but not definitive evidence of pattern-based reasoning
- Alternative explanations like attention mechanism limitations not fully ruled out

Low:
- Multi-image input training is a "promising direction" (correlation does not establish causation)
- Small sample size of models with multi-image training limits generalizability
- Other factors like model size could contribute to performance differences

## Next Checks

1. Conduct ablation studies comparing models with identical architectures but different training regimes (single vs. multi-image) to isolate the effect of multi-image training.

2. Implement controlled experiments where models must explicitly "show their work" by highlighting relevant regions across images before making predictions, to test whether visual attention patterns differ between human-like and hallucinated responses.

3. Test model performance on the Common-O Bench after fine-tuning on curated multi-image datasets to determine whether performance improvements are achievable through targeted training rather than requiring entirely new model architectures.