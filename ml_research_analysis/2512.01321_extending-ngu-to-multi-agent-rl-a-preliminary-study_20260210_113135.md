---
ver: rpa2
title: 'Extending NGU to Multi-Agent RL: A Preliminary Study'
arxiv_id: '2512.01321'
source_url: https://arxiv.org/abs/2512.01321
tags:
- learning
- multi-agent
- novelty
- exploration
- intrinsic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper extends the Never Give Up (NGU) algorithm to multi-agent
  reinforcement learning and evaluates it in the simpletag environment from the PettingZoo
  suite. The authors adapt NGU by incorporating intrinsic motivation through episodic
  novelty and explore three design choices: shared versus individual replay buffers,
  shared episodic novelty with different k thresholds, and heterogeneous beta values
  for the intrinsic-extrinsic trade-off.'
---

# Extending NGU to Multi-Agent RL: A Preliminary Study

## Quick Facts
- arXiv ID: 2512.01321
- Source URL: https://arxiv.org/abs/2512.01321
- Reference count: 33
- Primary result: NGU with shared replay buffer outperforms multi-agent DQN baseline in simple_tag environment

## Executive Summary
This paper presents an extension of the Never Give Up (NGU) algorithm to multi-agent reinforcement learning (MARL) settings. The authors adapt NGU's intrinsic motivation framework to a multi-agent context by exploring three design choices: shared versus individual replay buffers, shared episodic novelty with different k thresholds, and heterogeneous beta values for the intrinsic-extrinsic trade-off. Experiments conducted in the simple_tag environment from the PettingZoo suite demonstrate that NGU with a shared replay buffer achieves moderately higher returns and more stable learning dynamics compared to a multi-agent DQN baseline. The findings suggest that NGU can be effectively applied in multi-agent settings when experiences are shared and intrinsic exploration signals are carefully tuned.

## Method Summary
The authors extend NGU to MARL by adapting its intrinsic motivation mechanism through episodic novelty while exploring three key design choices. They compare shared versus individual replay buffers, test shared episodic novelty with different k thresholds (1, 5, and 10), and evaluate heterogeneous beta values for the intrinsic-extrinsic trade-off. The experimental setup uses the simple_tag environment from PettingZoo with two agents, comparing NGU variants against a multi-agent DQN baseline. The study focuses on how experience sharing and novelty computation affect learning performance in a cooperative-competitive scenario where one agent chases while the other flees.

## Key Results
- NGU with shared replay buffer outperforms multi-agent DQN baseline in simple_tag environment
- Best performance achieved with standard NGU configuration and shared buffer
- Shared episodic novelty effective only for k=1 threshold
- Heterogeneous beta values do not improve performance

## Why This Works (Mechanism)
The extension works by leveraging NGU's episodic novelty mechanism to provide intrinsic motivation signals that encourage exploration in multi-agent settings. By sharing experiences through a common replay buffer, agents can benefit from each other's discoveries, leading to more efficient learning. The episodic novelty computation helps agents identify and revisit interesting states, which is particularly valuable in the dynamic, partially observable nature of multi-agent environments. The shared intrinsic motivation signal creates a form of implicit coordination where agents are naturally drawn to explore regions of the state space that are novel for the team.

## Foundational Learning

- **Episodic novelty**: A measure of how "new" a state is within the current episode, calculated using an episodic memory. Needed to provide intrinsic motivation for exploration. Quick check: Verify novelty scores increase for previously unseen states within an episode.

- **Intrinsic-extrinsic trade-off (beta parameter)**: Controls the balance between extrinsic reward (task completion) and intrinsic reward (exploration). Needed to prevent premature convergence to suboptimal policies. Quick check: Test different beta values to observe exploration-exploitation balance.

- **Shared replay buffer**: A centralized experience storage that multiple agents can sample from. Needed to enable knowledge transfer between agents. Quick check: Compare learning curves with shared vs. individual buffers.

- **Multi-agent coordination**: The implicit coordination that emerges when agents share experiences and intrinsic motivation signals. Needed for efficient learning in cooperative-competitive scenarios. Quick check: Observe if agents develop complementary behaviors.

## Architecture Onboarding

Component map: Environment -> Multi-Agent NGU Agent -> Shared Replay Buffer -> Episodic Memory -> Intrinsic Reward -> Combined Reward -> Policy

Critical path: State observation → Intrinsic reward computation (episodic novelty) → Combined reward (intrinsic + extrinsic) → Policy update → Action selection → Environment transition

Design tradeoffs: Shared vs. individual replay buffers (collaboration vs. specialization), k threshold selection for novelty (sensitivity vs. stability), beta value tuning (exploration vs. exploitation)

Failure signatures: Over-sharing leading to homogenization of policies, under-sharing resulting in inefficient exploration, incorrect k threshold causing either too much or too little exploration

Three first experiments:
1. Compare learning curves of shared vs. individual replay buffer configurations
2. Test different k thresholds (1, 5, 10) for episodic novelty computation
3. Evaluate the impact of different beta values on exploration-exploitation balance

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Results based on single environment (simple_tag) with only two agents
- Intrinsic motivation relies on episodic novelty which may not scale to larger environments
- Limited exploration of heterogeneous agent types and team compositions
- Study does not investigate more complex multi-agent scenarios beyond simple_tag

## Confidence
High: NGU with shared replay buffers outperforms multi-agent DQN baseline in simple_tag setup
Medium: Shared episodic novelty works best only for k=1 (limited k value testing)
Low: Heterogeneous beta values do not improve performance (limited exploration of parameter space)

## Next Checks
1. Evaluate NGU in environments with more than two agents and greater complexity (e.g., multi-agent particle environments or SMAC)
2. Test a wider range of k values for shared episodic novelty to determine optimal settings across different scenarios
3. Investigate the impact of heterogeneous agent types and team compositions on NGU's performance in multi-agent settings