---
ver: rpa2
title: Context Learning for Multi-Agent Discussion
arxiv_id: '2602.02350'
source_url: https://arxiv.org/abs/2602.02350
tags:
- uni00000013
- uni00000015
- uni00000014
- uni00000019
- uni00000017
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of discussion inconsistency in
  Multi-Agent Discussion (MAD), where multiple LLM instances fail to reach coherent
  solutions due to misaligned individual contexts. The proposed method, M2CL, learns
  a context generator for each agent that dynamically generates context instructions
  per discussion round via automatic information organization and refinement.
---

# Context Learning for Multi-Agent Discussion

## Quick Facts
- arXiv ID: 2602.02350
- Source URL: https://arxiv.org/abs/2602.02350
- Reference count: 40
- Primary result: 20-50% improvement over baselines across 9 benchmarks

## Executive Summary
This paper addresses discussion inconsistency in Multi-Agent Discussion (MAD) where multiple LLM instances fail to reach coherent solutions due to misaligned contexts. The proposed M2CL method learns context generators that dynamically produce context instructions per discussion round through automatic information organization and refinement. By balancing consistency with creative exploration via a self-adaptive mechanism, M2CL significantly outperforms existing methods while maintaining computational efficiency with at most 10% runtime overhead.

## Method Summary
M2CL initializes agents with near-orthogonal context instructions to maximize solution perspective coverage, then trains per-agent context generators using dual gradient descent to balance maintaining unique viewpoints with integrating peer information. The method enforces temporal consistency between an agent's evolving instruction and its own past response as a proxy for global alignment, using attention activation distances rather than textual similarity. Context generators are lightweight T5-small models that produce instructions each round, while a dual variable α dynamically controls the constraint preventing context drift from initialization.

## Key Results
- 20-50% performance improvement over existing methods across 9 benchmarks
- Maintains favorable transferability across different LLM architectures (Llama, Qwen, Qwen2.5-VL)
- Achieves computational efficiency with at most 10% runtime overhead
- Effective across academic reasoning, embodied tasks, and mobile GUI control

## Why This Works (Mechanism)

### Mechanism 1: Orthogonal Initialization
- **Claim:** Initializing agents with near-orthogonal context instructions maximizes solution perspective coverage
- **Mechanism:** Selects subset of initial contexts from pool such that activations form basis-like structure in latent space, minimizing reconstruction error of correct activation
- **Core assumption:** Linear combination of diverse initial context activations can approximate target correct activation
- **Evidence anchors:** Section 5.1 derives initialization as finding "near-orthogonal activations [that] form a compact basis"
- **Break condition:** Insufficient initial diversity if context pool too small or projection function fails

### Mechanism 2: Self-Adaptive Context Generation
- **Claim:** Learned context generator balances unique perspective with peer integration via self-adaptive constraint
- **Mechanism:** Generator produces per-round instructions trained via constrained optimization where dual variable α dynamically controls constraint stiffness
- **Core assumption:** Aligning agent instruction activation with collective history reduces inter-agent divergence
- **Evidence anchors:** Abstract describes training generators to control context coherence and output discrepancies via self-adaptive mechanism
- **Break condition:** Oscillation between rigidity and collapse if α update frequency mismatched with generator training stability

### Mechanism 3: Temporal Consistency Proxy
- **Claim:** Enforcing temporal consistency between evolving instruction and past response serves as scalable proxy for global alignment
- **Mechanism:** Uses decoupled local criterion enforcing current instruction aligns with previous response instead of computationally heavy global loss
- **Core assumption:** Attention mechanism allows smooth mapping where aligned activations imply effective utilization of collaborative history
- **Evidence anchors:** Section 5.2.1 replaces global term with local term
- **Break condition:** Amplifying errors rather than correcting them if agents produce hallucinated responses

## Foundational Learning

- **Concept: Attention Activation Space (a(·))**
  - **Why needed here:** Paper optimizes internal "attention activations" rather than textual similarity; crucial for understanding optimization objective
  - **Quick check question:** How does Theorem 4.1 relate distance between activations to success of multi-agent system?

- **Concept: Dual Gradient Descent / Lagrangian Optimization**
  - **Why needed here:** Core training loop balances loss with constraint using learned variable α rather than fixed hyperparameter
  - **Quick check question:** In Eq. 17, what happens to α if generated context drifts too far from initial context?

- **Concept: Projection Functions (f and F)**
  - **Why needed here:** Initialization relies on distilling computationally expensive projection f into lightweight model F
  - **Quick check question:** Why distill f into F instead of using f directly during inference initialization?

## Architecture Onboarding

- **Component map:** Context Pool -> Projection Function (F) -> Context Generator (G_θ) -> Base LLMs -> State Manager (α, β)
- **Critical path:**
  1. **Initialization:** Map Context Pool → Vectors via F. Solve subset sum problem (Eq. 12) to pick N orthogonal contexts
  2. **Generation Loop:** Concatenate (Task, Init Context, History) → Feed to G_θ → Generate New Instruction
  3. **Dual Update:** Calculate activation discrepancy and update α via Eq. 17 (inverse gradient descent on dual)

- **Design tradeoffs:**
  - **Efficiency vs. Precision:** Uses T5-small generator for <10% overhead but limits instruction complexity
  - **Discrepancy Metric:** Using attention activation requires internal model state access, though learned policy generalizes to black-box APIs

- **Failure signatures:**
  - **Collapse to Consensus:** Discrepancy drops to zero immediately; agents output identical text
  - **Stagnant Discrepancy:** Discrepancy remains high; agents ignore each other

- **First 3 experiments:**
  1. **Ablation of Initialization:** Compare random context selection vs. orthogonal selection (Eq. 12)
  2. **Ablation of α Tuning:** Fix α to constant instead of learning it
  3. **Scaling Test:** Vary N from 4 to 64 on reasoning task; plot scaling curve against baselines

## Open Questions the Paper Calls Out
- **Open Question 1:** How can M2CL evolve to enable LLMs to autonomously identify and specialize in sub-tasks they excel at, reducing reliance on scaling LLM numbers?
- **Open Question 2:** To what extent does quality and size of pre-defined context pool impact initialization and final performance?
- **Open Question 3:** Can context constraint β be adaptively optimized for each agent or discussion round rather than fixed hyperparameter?

## Limitations
- Reliance on internal attention activations creates reproducibility barriers for API-only settings
- Projection function F distillation from f not fully specified in paper
- Optimal β constraint values appear task-specific rather than derived from first principles

## Confidence
- **High Confidence:** 20-50% performance improvement well-supported by comprehensive benchmark evaluation
- **Medium Confidence:** Orthogonal initialization mechanism theoretically sound but practical effectiveness depends on context pool quality
- **Medium Confidence:** Self-adaptive α mechanism appears effective but edge cases not fully explored

## Next Checks
1. **API Compatibility Test:** Implement M2CL using only black-box LLM APIs to verify learned context generation policies transfer without activation access
2. **Constraint Sensitivity Analysis:** Systematically vary β across multiple orders of magnitude on single task to map stability boundary
3. **Activation Distribution Study:** Compare activation distance distributions between M2CL and random initialization to quantify solution space coverage improvement