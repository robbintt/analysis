---
ver: rpa2
title: 'Larger Datasets Can Be Repeated More: A Theoretical Analysis of Multi-Epoch
  Scaling in Linear Regression'
arxiv_id: '2511.13421'
source_url: https://arxiv.org/abs/2511.13421
tags:
- data
- lemma
- proof
- have
- rate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a theoretical analysis of how multi-epoch training
  reshapes data scaling laws in linear regression. The authors define the effective
  reuse rate E(K, N) as the multiplicative factor by which the dataset must grow under
  one-pass training to achieve the same test loss as K-epoch training on N samples.
---

# Larger Datasets Can Be Repeated More: A Theoretical Analysis of Multi-Epoch Scaling in Linear Regression

## Quick Facts
- arXiv ID: 2511.13421
- Source URL: https://arxiv.org/abs/2511.13421
- Reference count: 40
- Primary result: In linear regression, multi-epoch training yields linear gains for small K (K=o(log N)), then plateaus at Θ(log N), showing larger datasets can be repeated more times before marginal benefits vanish.

## Executive Summary
This paper presents a theoretical analysis of how multi-epoch training reshapes data scaling laws in linear regression. The authors define the effective reuse rate E(K, N) as the multiplicative factor by which the dataset must grow under one-pass training to achieve the same test loss as K-epoch training on N samples. They prove that when K is small (K=o(log N)), E(K, N)≈K, indicating each epoch yields linear gains. As K increases beyond log N, E(K, N) plateaus at Θ(log N), showing that larger datasets can be repeated more times before marginal benefits vanish. The authors validate their theoretical predictions through experiments with both synthetic data and large language models (LLMs).

## Method Summary
The authors analyze stochastic gradient descent (SGD) training dynamics using matrix concentration inequalities and bias-variance decomposition. For strongly convex linear regression, they prove that when K is small (K=o(log N)), E(K, N)≈K, indicating each epoch yields linear gains. As K increases beyond log N, E(K, N) plateaus at Θ(log N). They validate their theoretical predictions through experiments with both synthetic data and large language models (LLMs), confirming that E(K, N)≈K for small K, and that for fixed K, the effective reuse rate increases monotonically with N.

## Key Results
- For small K (K=o(log N)), the effective reuse rate E(K,N) ≈ K, indicating linear gains from each epoch
- As K increases beyond log N, E(K,N) plateaus at Θ(log N), showing saturation of multi-epoch benefits
- Larger datasets allow for more effective repetitions, with the saturation point scaling as a function of N
- In Zipf-distributed data, the saturation point scales as a power of N rather than log N
- These findings challenge assumptions that effective epochs are uniform across different dataset sizes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: For a small number of epochs (K), repeating a dataset is theoretically equivalent to proportionally increasing the dataset size with fresh data.
- Mechanism: In the regime where K=o(log N), the stochastic gradient noise allows the optimizer to extract linear gains from each pass. The excess risk scales as Θ(log(KN)/KN), meaning the effective reuse rate E(K, N) ≈ K.
- Core assumption: Strong convexity (Assumption 4.1) and bounded data norms; N is sufficiently large.
- Evidence anchors:
  - [abstract] "When K is small (K=o(log N)), E(K, N)≈K, indicating each epoch yields linear gains."
  - [section 4.1] Theorem 4.2 proves E(K, N) = (1 + o_N(1)) · K in this regime.
  - [corpus] Weak direct validation in neighbors, though related work discusses multi-epoch utility.
- Break condition: The mechanism breaks when K grows large enough relative to N (specifically K=ω(log N)).

### Mechanism 2
- Claim: The benefit of data reuse saturates at a limit determined by the dataset size N, rather than growing indefinitely with K.
- Mechanism: This is driven by a bias-variance tradeoff in the SGD dynamics. While early epochs reduce bias, the variance term eventually dominates the marginal benefit. In the large K limit, the excess risk depends only on N (Θ(1/N)), rendering additional epochs useless for improving test loss.
- Core assumption: SGD with random shuffling; optimal learning rate tuning.
- Evidence anchors:
  - [abstract] "As K increases beyond log N, E(K, N) plateaus... showing that larger datasets can be repeated more times before marginal benefits vanish."
  - [section 4.1] Theorem 4.2 shows E(K, N) plateaus at Θ(log N) for large K.
  - [corpus] Not explicitly validated by provided neighbors.
- Break condition: Assumption: Does not necessarily hold if the learning rate schedule or optimizer (e.g., Adam) alters the variance dynamics significantly (see neighbor "The Effect of Mini-Batch Noise...").

### Mechanism 3
- Claim: Larger datasets allow for more effective repetitions compared to smaller datasets.
- Mechanism: The saturation point (effective reuse cap) is a function of N. In strongly convex settings, the cap scales as Θ(log N); in Zipf-distributed data, it scales as a power of N. This implies "data-constrained scaling laws" should model E as dependent on N, contradicting the assumption that effective epochs are uniform across sizes.
- Core assumption: Data distribution characteristics (Strong convexity or Zipf law).
- Evidence anchors:
  - [abstract] "...implying that larger datasets can be repeated more times before the marginal benefit vanishes."
  - [section 6.3] LLM experiments confirm E(K, N) increases monotonically with N.
  - [corpus] Neighbors like "Datasets, Documents, and Repetitions" discuss data quality/repetition, supporting the premise of data scarcity, but not this specific mechanism.
- Break condition: If the data distribution does not satisfy the spectral decay assumptions (e.g., random features with specific power laws), the exact scaling of the saturation point may differ.

## Foundational Learning

- Concept: **Bias-Variance Decomposition in SGD**
  - Why needed here: The paper's core theoretical contribution relies on decomposing the excess risk into bias (distance from w*) and variance (noise accumulation) to determine when repetition helps versus hurts.
  - Quick check question: Can you explain why the variance term prevents the loss from decreasing to zero even if K → ∞?

- Concept: **Matrix Concentration Inequalities**
  - Why needed here: The authors use these to approximate intractable expectations of matrix products (from random shuffling) with deterministic "pseudo-expectations" to derive closed-form risk bounds.
  - Quick check question: How does the non-commutativity of matrix multiplication complicate the analysis of multi-epoch SGD compared to one-pass SGD?

- Concept: **Strong Convexity & Spectral Decay**
  - Why needed here: The scaling laws (E(K, N) ≈ K vs. saturation) explicitly depend on the eigenvalues of the Hessian matrix (λᵢ). Understanding how λᵢ decay (e.g., power law vs. bounded away from zero) is necessary to distinguish the different scaling regimes.
  - Quick check question: How does the Zipf-distributed data assumption change the saturation point compared to the strongly convex case?

## Architecture Onboarding

- Component map: Input(N,K,η,H) -> SGD Dynamics -> Bias-Variance Decomposition -> Matrix Approximation -> Output(E(K,N))
- Critical path: Understanding how the paper bridges the gap between the intractable exact expectation of SGD dynamics and the tractable approximation using "pseudo-expectation" (Section 4.2, Step 2) is the most critical theoretical step.
- Design tradeoffs: The theoretical analysis assumes linear regression and strong convexity for the main results. This trades generality (applicability to non-convex DNNs) for mathematical precision. The Zipf extension attempts to bridge this but remains a simplified model of LLM data.
- Failure signatures:
  - Assuming E(K, N) is independent of N (the Muennighoff et al. assumption)
  - Applying the Θ(log N) saturation rule to data distributions that do not satisfy strong convexity (where it might be a power of N)
  - Using standard learning rate schedules instead of the theoretically optimal Θ(log(KN)/KN) scaling
- First 3 experiments:
  1. **Synthetic Verification**: Replicate the phase transition by plotting E(K, N) vs K for varying N in a linear regression task to observe the K ≈ E region and the plateau.
  2. **Scaling Law Fit**: Fit the proposed scaling law for E(K, N) on existing LLM training logs (varying N) to see if the dependence on log N holds empirically outside the linear regression setting.
  3. **Ablation on Spectrum**: Modify the condition number of the synthetic data H to verify if the saturation point shifts as predicted by the theory.

## Open Questions the Paper Calls Out

- **Question**: Does the effective reuse rate scaling behavior (E(K,N) ≈ K for small K, plateauing at Θ(log N) for strongly convex settings) extend to neural networks with feature learning?
  - **Basis in paper**: [explicit] The authors state: "(i) Our analysis is limited to the linear model, and it would be interesting to extend the framework to more complex and realistic settings, such as neural networks with feature learning."
  - **Why unresolved**: The theoretical analysis relies on linear regression properties (closed-form loss expressions, matrix concentration for linear updates) that do not directly apply to non-linear neural networks.
  - **What evidence would resolve it**: Empirical measurements of E(K,N) across varying dataset sizes N in neural network training, combined with theoretical analysis showing whether the log N scaling of the saturation point persists.

- **Question**: How do alternative data reuse strategies (curriculum learning, data mixing, selective high-quality data repetition) affect the effective reuse rate compared to full-dataset multi-epoch training?
  - **Basis in paper**: [explicit] The authors note: "one can consider a more efficient and heuristic approach to repeating data, such as data mixing, curriculum learning, or reusing only high-quality data."
  - **Why unresolved**: The paper only analyzes uniform reuse of the entire dataset; non-uniform or selective repetition strategies may yield different scaling behaviors.
  - **What evidence would resolve it**: Experiments comparing E(K,N) under different data selection/repetition strategies, potentially identifying approaches that extend the effective-reuse regime beyond Θ(log N).

- **Question**: Can the theoretical characterization of E(K,N) be generalized to all non-strongly convex linear regression settings, beyond the specific Zipf-distributed solvable case analyzed in Section 5?
  - **Basis in paper**: [explicit] The authors state: "(iii) Technically, our main results rely on strong convexity. In the non-strongly convex regime, we provide a solvable case with a Zipf-law data distribution. It would be interesting to generalize these proof ideas to general non-strongly convex linear regression."
  - **Why unresolved**: The proof techniques for strongly convex settings depend on bounded minimum eigenvalues, while the Zipf case relies on specific distributional assumptions that enable closed-form analysis.
  - **What evidence would resolve it**: A unified theoretical framework establishing E(K,N) behavior for general Hessian spectra with decaying eigenvalues, or counterexamples showing fundamentally different scaling in certain non-strongly convex regimes.

- **Question**: Does the theoretical result that E(K,N) ≈ K holds for K = o(log N) (strongly convex case) remain valid for arbitrarily large epoch counts beyond the K = O(N^0.1) technical assumption?
  - **Basis in paper**: [explicit] Under Assumption 4.3, the authors state: "We make this assumption for technical issues. However, we speculate that our main results hold for arbitrarily large K."
  - **Why unresolved**: The current proof relies on the K = O(N^0.1) constraint to control error bounds in matrix concentration inequalities; removing this requires different analytical techniques.
  - **What evidence would resolve it**: Extended empirical validation with very large K values, or a refined proof showing the o(log N) threshold remains the correct phase transition point without the polynomial constraint on K.

## Limitations
- Theoretical analysis restricted to linear regression and strong convexity, limiting direct applicability to deep neural networks
- The Zipf-distributed extension remains a simplified model compared to real LLM training data
- Assumes random shuffling and optimal learning rate tuning, which may not hold in practice
- LLM experiments use a relatively small model (0.3B parameters), raising questions about scalability

## Confidence
- **High Confidence**: The theoretical results for small K (E(K,N)≈K when K=o(log N)) are well-supported by both rigorous proofs and experimental validation in synthetic settings
- **Medium Confidence**: The extrapolation to LLMs and the specific scaling behavior (E increasing monotonically with N) is supported by experiments but may not fully capture the complexity of real-world training dynamics
- **Low Confidence**: The Zipf-distributed extension's implications for real LLM data remain speculative

## Next Checks
1. **Distribution Sensitivity**: Test the saturation point scaling (log N vs. power of N) across multiple synthetic data distributions (e.g., exponential decay, uniform eigenvalues) to identify which characteristics most influence the effective reuse rate.

2. **Architecture Scaling**: Repeat the LLM experiments with larger model sizes (1B-10B parameters) while maintaining the constant learning rate schedule to verify whether the N-dependent E(K,N) relationship persists across scales.

3. **Optimization Method Comparison**: Compare multi-epoch scaling behavior between SGD with random shuffling and AdamW (with/without weight decay) on the same linear regression tasks to quantify how optimizer choice affects the bias-variance tradeoff and effective reuse rate.