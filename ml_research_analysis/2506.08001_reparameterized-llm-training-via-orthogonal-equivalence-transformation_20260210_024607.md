---
ver: rpa2
title: Reparameterized LLM Training via Orthogonal Equivalence Transformation
arxiv_id: '2506.08001'
source_url: https://arxiv.org/abs/2506.08001
tags:
- layer
- poet
- training
- layers
- orthogonal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes POET, a novel training framework for large
  language models that uses orthogonal equivalence transformation to reparameterize
  weight matrices, effectively preserving their spectral properties. POET reparameterizes
  each neuron as the product of two learnable orthogonal matrices and a fixed random
  weight matrix, enabling stable training and improved generalization.
---

# Reparameterized LLM Training via Orthogonal Equivalence Transformation

## Quick Facts
- arXiv ID: 2506.08001
- Source URL: https://arxiv.org/abs/2506.08001
- Reference count: 40
- Primary result: POET achieves better perplexity than AdamW with fewer parameters by preserving weight matrix spectra through orthogonal equivalence transformation

## Executive Summary
This paper introduces POET, a novel training framework for large language models that reparameterizes weight matrices using orthogonal equivalence transformation to preserve spectral properties. The method replaces standard weight matrices with the product of two learnable orthogonal matrices and a fixed random matrix, enabling stable training dynamics and improved generalization. POET incorporates two key approximations - stochastic primitive optimization for efficient orthogonal matrix learning and Cayley-Neumann parameterization for numerical stability - making it scalable to large models while maintaining strong performance.

## Method Summary
POET reparameterizes each neuron as R·W₀·P where W₀ is a fixed random matrix and R, P are learnable orthogonal matrices. This preserves singular values during training while allowing optimization of rotation matrices. For scalability, POET uses stochastic primitive optimization to factorize orthogonal matrices into products of small primitives acting on blocks, and Cayley-Neumann parameterization to maintain orthogonality via skew-symmetric matrices with Neumann series approximation. The merge-and-reinitialize trick periodically updates W₀ and resets orthogonal parameters to maintain numerical stability and ensure full coverage of the orthogonal group.

## Key Results
- On 1.3B LLaMA, POET-FS (b=1/2) achieves 13.70 perplexity vs 14.73 for AdamW using only 406.88M trainable parameters
- POET outperforms low-rank methods like GaLore and LoRA while using fewer trainable parameters
- POET demonstrates consistent perplexity improvements across LLaMA model sizes (60M to 1.3B)
- The method generalizes well to both pretraining and finetuning tasks while maintaining small hyperspherical energy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Spectral preservation through orthogonal equivalence transformation stabilizes training and improves generalization.
- Mechanism: Reparameterize each weight matrix W as RW₀P, where W₀ is frozen at random initialization and R, P are learnable orthogonal matrices. Orthogonal matrices preserve singular values by construction (‖RW₀P‖₂ = ‖W₀‖₂), preventing spectral explosion while allowing full-rank updates to singular vectors.
- Core assumption: Random initialization provides a well-conditioned spectral starting point; learning only rotations is sufficient for optimization.
- Evidence anchors:
  - [abstract]: "POET reparameterizes each neuron with two learnable orthogonal matrices and a fixed random weight matrix... provable preservation of spectral properties of weight matrices"
  - [section 3.1]: "This reparameterization effectively applies an orthogonal equivalence transformation (OET) to random weight matrices"
  - [corpus]: Weak direct validation. Neighbor papers (ProcrustesGPT, OstQuant) apply orthogonal transforms for compression/quantization but not training dynamics.
- Break condition: If random W₀ has pathological spectrum (e.g., near-zero or exploding singular values), initialization choice will dominate outcomes regardless of R, P.

### Mechanism 2
- Claim: Stochastic Primitive Optimization (SPO) enables parameter-efficient learning of large orthogonal matrices.
- Mechanism: Factorize R = Πᵢ Gᵢ where each primitive Gᵢ acts on a b×b submatrix (fully stochastic) or b×b blocks (block-stochastic). Only update one primitive at a time, then merge into W₀ and reinitialize. This reduces parameters from O(m²) to O(b²) per step while maintaining coverage via merge-then-reinitialize.
- Core assumption: Sequential primitive updates with periodic merging can approximate any orthogonal matrix given sufficient iterations.
- Evidence anchors:
  - [section 3.2.1]: Equation (4) and (5) define factorization; Lemma 1 proves representation power scales with c ≥ αm·ln(m)(m/b)²
  - [Table 1]: Shows parameter/memory complexity tradeoffs between FS, BS, and AdamW
  - [corpus]: No direct validation. ButterflyQuant uses butterfly transforms but for quantization, not training parameterization.
- Break condition: If b is too small or merge frequency Tₘ is too infrequent, coverage becomes uneven (see Appendix H weight update visualizations).

### Mechanism 3
- Claim: Cayley-Neumann Parameterization provides numerical stability for orthogonal constraints.
- Mechanism: Express orthogonal R = (I+Q)(I-Q)⁻¹ with Q skew-symmetric. Approximate (I-Q)⁻¹ ≈ Σᵢ₌₀ᵏ Qⁱ via truncated Neumann series (k=3 default). Avoids matrix inversion while maintaining near-orthogonality; merge-then-reinitialize resets approximation error.
- Core assumption: Q's operator norm remains <1 between merges for Neumann convergence.
- Evidence anchors:
  - [section 3.2.2]: Equation (6) defines approximation; "merge-and-reinitialize trick mitigates this issue by periodically resetting Q to zero"
  - [Table 7]: k=0 diverges; k≥1 converges with diminishing returns
  - [corpus]: No direct validation in neighbor papers.
- Break condition: If learning rate or update magnitude causes ||Q|| ≥ 1 before merge, approximation degrades and orthogonality violation may destabilize training.

## Foundational Learning

- Concept: Singular Value Decomposition (SVD) and spectral properties
  - Why needed here: POET's core claim hinges on spectrum preservation. You must understand that singular values bound matrix amplification and connect to generalization (spectral norm → Lipschitz constant).
  - Quick check question: If W = UΣVᵀ, what happens to Σ when you multiply by orthogonal R, P? (Answer: unchanged)

- Concept: Orthogonal group and Cayley transform
  - Why needed here: R and P must remain orthogonal during training. Cayley transform maps unconstrained Q (skew-symmetric) to orthogonal R.
  - Quick check question: Why must Q be skew-symmetric (Q = -Qᵀ) for R = (I+Q)(I-Q)⁻¹ to be orthogonal?

- Concept: Hyperspherical energy
  - Why needed here: Paper claims POET minimizes hyperspherical energy (neuron uniformity on unit sphere), which connects to generalization.
  - Quick check question: How does isotropic Gaussian initialization relate to hyperspherical energy under orthogonal transforms?

## Architecture Onboarding

- Component map:
  ```
  Input x → Linear(RW₀P) → Output
              ↓
  W₀: frozen random (normalized Gaussian init)
  R, P: learned via SPO + Cayley-Neumann
    ↓
  SPO: samples index sets → constructs primitives Gᵢ
    ↓
  CNP: parameterizes each Gᵢ via Q → Neumann approximation
    ↓
  Merge-and-reinitialize: W₀ ← RW₀P, reset R,P=I every Tₘ steps
  ```

- Critical path:
  1. Initialization: W₀ ← normalized Gaussian; R, P ← identity
  2. Forward pass: compute y = (RW₀P)ᵀx
  3. Backward: gradients flow through R, P to Q parameters
  4. Inner loop: update Q for Tₘ iterations
  5. Merge: W₀ ← RW₀P, resample S (FS) or Ψ (BS), reset R, P ← I

- Design tradeoffs:
  - **b (block size)**: Larger b → more parameters, faster convergence, less parameter efficiency
  - **Tₘ (merge frequency)**: Too small → overhead; too large → coverage gaps, orthogonality drift
  - **k (Neumann terms)**: More terms → better approximation, more compute
  - **FS vs BS**: BS gives even weight updates; FS is more parameter-efficient for small budgets

- Failure signatures:
  - Training divergence early → check k≥1, reduce learning rate
  - Poor convergence with small b → increase b or Tₘ
  - High validation perplexity despite low training loss → check initialization scheme (normalized Gaussian performs best per Table 5)
  - Uneven weight updates → switch from FS to BS (Appendix H)

- First 3 experiments:
  1. **Sanity check on toy model**: Train 60M LLaMA with POET-FS b=1/2, Tₘ=400, k=3 on 1B tokens. Compare perplexity curve to AdamW baseline. Expect slower early progress but better final perplexity.
  2. **Ablation on b**: Fix Tₘ=400, k=3. Sweep b ∈ {1/8, 1/4, 1/2} for FS and b ∈ {64, 128, 256} for BS. Plot parameter count vs. final perplexity to find efficiency frontier.
  3. **Ablation on Tₘ and k**: Fix b=1/2 (FS). Test Tₘ ∈ {50, 200, 400, 800} and k ∈ {1, 2, 3, 4}. Monitor orthogonality error (‖RRᵀ-I‖_F) and perplexity. Confirm Table 6 and Table 7 trends.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical mechanism driving POET's distinct three-phase training dynamics (conical shell searching, stable learning, and final adjusting)?
- Basis in paper: [explicit] The authors state in Section 4 and Appendix A.3 that the "exact mechanism behind this phenomenon remains an open question" and that "further theoretical understanding of this process remains an open problem."
- Why unresolved: The phases were identified empirically through vector probing (observing cosine similarities stabilizing at ≈ 0.6), but there is no formal derivation linking the Cayley-Neumann parameterization to this specific geometric convergence behavior.
- What evidence would resolve it: A theoretical analysis connecting the gradient updates of the orthogonal parameters to the geometry of the conical shell, explaining why the rotation angle stabilizes before the loss converges.

### Open Question 2
- Question: What is the optimal singular value structure for weight matrices when using spectrum-preserving training?
- Basis in paper: [explicit] In Section 5.3, the authors state that "Finding the optimal singular value structure for weights remains an important open problem" after observing that uniform spectrum initialization performs poorly compared to normalized Gaussian initialization.
- Why unresolved: The paper empirically demonstrates that normalized Gaussian works best, but does not explain why restricting the spectrum to this specific distribution outperforms uniform or standard initialization in this reparameterized context.
- What evidence would resolve it: A theoretical characterization of how different initial spectral densities (e.g., heavy-tailed vs. decaying) interact with the orthogonal equivalence transformation to affect model expressiveness and generalization.

### Open Question 3
- Question: How does the performance variance depend on the random seed of the fixed initial matrix W₀?
- Basis in paper: [inferred] The framework relies on a fixed random matrix W₀ (initialized via Normalized Gaussian) to define the spectral properties for the entire training duration. The paper reports final perplexity values without standard deviations across different W₀ seeds.
- Why unresolved: Unlike standard training where weights evolve freely, POET fixes the spectrum at initialization. This implies that a statistically "unlucky" random draw for W₀ could permanently constrain the model's capacity, creating a potential robustness issue not observed in standard AdamW training.
- What evidence would resolve it: A sensitivity analysis reporting the variance of validation perplexity across multiple runs using different random seeds for the fixed matrix W₀, compared against the variance of standard training.

## Limitations
- The merge-and-reinitialize procedure may introduce training instability during reset phases, with unspecified temporary gradient clipping values
- The stochastic primitive optimization approximation introduces coverage gaps that may affect convergence, particularly for small block sizes
- Empirical validation of spectrum preservation claims is limited - no direct analysis of weight matrix spectra during training is provided

## Confidence

- **High confidence:** POET's overall effectiveness in improving perplexity and parameter efficiency compared to AdamW and low-rank methods (GaLore, LoRA) - supported by comprehensive experimental results across multiple LLaMA model sizes and training regimes.
- **Medium confidence:** The Cayley-Neumann parameterization's numerical stability claims - supported by Table 7 showing convergence with k≥1, but lacking direct comparison to alternative orthogonal parameterization methods.
- **Low confidence:** The hyperspherical energy minimization claim - while the paper asserts this connects to generalization, no direct measurement or validation of hyperspherical energy during training is provided.

## Next Checks

1. **Spectral Analysis Validation:** Track and plot the singular value distributions of weight matrices during training for both POET and AdamW. Verify that POET maintains stable singular values while AdamW shows divergence, directly confirming the spectrum preservation mechanism.

2. **Orthogonality Error Monitoring:** Implement continuous measurement of the Frobenius norm of (RRᵀ-I) throughout training (not just at merge points) to quantify how quickly orthogonality degrades between merges and whether this correlates with training instability.

3. **Ablation on Merge Frequency:** Systematically vary Tₘ across a wider range (e.g., 50-2000 steps) and measure both orthogonality error and perplexity to identify the optimal tradeoff point where orthogonality is maintained without excessive merge overhead, testing the merge-and-reinitialize mechanism's sensitivity.