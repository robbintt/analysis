---
ver: rpa2
title: 'Anatomy of an Idiom: Tracing Non-Compositionality in Language Models'
arxiv_id: '2511.16467'
source_url: https://arxiv.org/abs/2511.16467
tags:
- idiom
- processing
- heads
- attention
- circuit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how transformer-based language models process
  idiomatic expressions using a modified path patching algorithm to discover computational
  circuits. The study identifies "Idiom Heads" - attention heads that consistently
  activate across multiple idioms - and introduces the concept of "augmented reception,"
  where early idiom processing enhances later attention to idiom-specific tokens.
---

# Anatomy of an Idiom: Tracing Non-Compositionality in Language Models

## Quick Facts
- arXiv ID: 2511.16467
- Source URL: https://arxiv.org/abs/2511.16467
- Reference count: 30
- Primary result: Identifies computational circuits for idiom processing in transformers, revealing two-phase structure and specialized attention heads

## Executive Summary
This paper investigates how transformer-based language models process idiomatic expressions using a modified path patching algorithm to discover computational circuits. The study identifies "Idiom Heads" - attention heads that consistently activate across multiple idioms - and introduces the concept of "augmented reception," where early idiom processing enhances later attention to idiom-specific tokens. Key findings include: (1) Idiom processing exhibits a two-phase structure with cross-token attention in early layers (0-2) followed by semantic integration in later layers (3-5), (2) Specific attention heads consistently activate across multiple idioms, suggesting functional specialization for non-compositional language processing, though each idiom employs a distinct Query-Key space representation rather than a universal "idiom direction," and (3) Early processing of idiom tokens creates enhanced receptivity of these tokens to each other in later attention layers, pointing to a mechanism for efficient attention allocation. These findings provide insights into how transformers handle non-compositional semantics and establish a framework for investigating the processing of more complex linguistic constructions.

## Method Summary
The study uses a modified ACDC path patching algorithm on Gemma 2-2B to discover computational circuits for idiom processing. The method involves computing cosine similarity between idiom and meaning string embeddings at intermediate layers to identify when figurative meaning emerges, then building computational graphs of attention heads and post-MLP residuals per token. Path patching is applied in reverse topological order to patch Q/K/V edges between specific tokens, measuring performance drop when corrupted activations replace original ones. A threshold sweep (τ ∈ [0.004, 0.008]) identifies the optimal sparsity level, and circuits are merged across multiple corruptions using union. The analysis focuses on the final token of idioms, which carries most semantic weight, and examines both within-token and cross-token attention patterns.

## Key Results
- Idiom processing exhibits a two-phase structure with cross-token attention in early layers (0-2) followed by semantic integration in later layers (3-5)
- Specific attention heads consistently activate across multiple idioms, suggesting functional specialization for non-compositional language processing
- Early processing of idiom tokens creates enhanced receptivity of these tokens to each other in later attention layers (augmented reception)
- Each idiom employs a distinct Query-Key space representation rather than a universal "idiom direction"

## Why This Works (Mechanism)

### Mechanism 1: Two-Phase Processing with Deferred Computation
- Claim: Idiom processing follows a sequential structure where cross-token attention in early layers (0-2) precedes semantic integration in later layers (3-5), with most computation occurring on the final token.
- Mechanism: Early layers establish attention pathways between idiom tokens without resolving meaning; information persists in residual stream subspaces until layer 3+ where semantic integration occurs. This deferral allows the model to maintain both literal and figurative interpretations until context is complete.
- Core assumption: Residual stream bandwidth (256-dim head subspaces within 2304-dim stream) enables information persistence across layers without interference.
- Evidence anchors:
  - [abstract] "idiom processing exhibits a two-phase structure with cross-token attention in early layers (0–2) followed by semantic integration in later layers (3–5)"
  - [section 3.1] "all cross-token processing occurs in the earlier layers (0–2). Interestingly, the cosine similarity analysis of Figure 1 shows that the idiom string acquires no significant non-compositional meaning relative to the corruptions until layer 3"
  - [corpus] Limited corpus validation; the Oh et al. (2025) preprint focuses on disambiguation rather than this specific two-phase structure.
- Break condition: If model depth falls below ~6 layers, or if attention head dimension equals residual dimension (no bandwidth constraint).

### Mechanism 2: Idiom Heads with Idiom-Specific QK Directions
- Claim: Specific attention heads (e.g., (2,0), (1,2), (1,3), (1,5)) consistently participate across multiple idioms, but each idiom uses a distinct direction in Query-Key space rather than a shared "idiom direction."
- Mechanism: Heads provide reusable computational infrastructure (shared attention circuitry), while QK space encodes idiom-specific token-to-token attention patterns. Redundancy across multiple heads provides robustness.
- Core assumption: Functional specialization coexists with polysemanticity due to efficiency-robustness tradeoffs.
- Evidence anchors:
  - [abstract] "Specific attention heads consistently activate across multiple idioms, suggesting functional specialization for non-compositional language processing, though each idiom employs a distinct Query-Key space representation rather than a universal 'idiom direction'"
  - [section 3.3, Table 2] Cross-idiom QK dot products are consistently lower than within-idiom products (e.g., kicked-bucket=72 vs. kicked-sack=14), demonstrating no universal direction.
  - [corpus] Oh et al. (2025) independently identify "idiomatic heads" in Llama3.2-1B via different causal techniques, providing convergent evidence for specialized heads.
- Break condition: If attention head count per layer is dramatically reduced (specialization becomes impossible), or if tokenizer segments idioms differently.

### Mechanism 3: Augmented Reception
- Claim: Early-layer idiom processing modifies query representations in ways that enhance later attention to idiom-relevant tokens.
- Mechanism: When an attention head on a non-final token has an incoming Query edge from earlier processing, it indicates that earlier layers have modified what that token attends to—reducing attention to corrupted variants and enhancing attention to idiom-consistent tokens.
- Core assumption: Query vectors are computed from residual stream values that encode contextual expectations from prior layers.
- Evidence anchors:
  - [abstract] "Early processing of idiom tokens creates enhanced receptivity of these tokens to each other in later attention layers, pointing to a mechanism by which transformers efficiently allocate attention"
  - [section 3.4, Figure 5] "attention heads (2,0) and (1,5), respectively, on the final token bucket have incoming Query edges, demonstrating augmented reception"
  - [corpus] Weak corpus support; this mechanism appears novel to this work.
- Break condition: If query computation is isolated from prior layer outputs (architecturally impossible in standard transformers).

## Foundational Learning

- Concept: **Path Patching vs. Activation Patching**
  - Why needed here: The paper's methodology depends on understanding that path patching independently modifies Q/K/V connections between specific tokens, while activation patching replaces entire activations.
  - Quick check question: Can you explain why patching a Query edge differently affects attention to previous tokens vs. self-attention?

- Concept: **Residual Stream Bandwidth**
  - Why needed here: The two-phase mechanism relies on information persisting in the residual stream despite attention heads only reading/writing to small subspaces.
  - Quick check question: If attention heads write to 256-dim subspaces of a 2304-dim residual stream, how many heads can write non-overlapping information simultaneously?

- Concept: **Non-Compositional Semantics**
  - Why needed here: Idioms require overriding literal word meanings with figurative wholes—understanding this linguistic property is essential to interpreting the paper's findings.
  - Quick check question: Why would "kicked the bucket" and "booted the bucket" require different processing despite similar literal meanings?

## Architecture Onboarding

- Component map: Gemma 2-2B with 26 layers, 8 attention heads per layer, 256-dim head dimension, 2304-dim residual stream. Key layers for idiom processing: 0-2 (cross-token attention), 3-5 (semantic integration). Critical heads: (2,0), (1,2), (1,3), (1,5).

- Critical path: Final token of idiom → receives Key/Value inputs from earlier idiom tokens via early-layer heads → Query computation incorporates early-layer signals (augmented reception) → later-layer heads integrate and write figurative meaning to residual stream → cosine similarity to meaning representation peaks at layer 4-5.

- Design tradeoffs: Threshold τ selection (0.004-0.008) balances circuit sparsity vs. completeness. Corruption choice requires high embedding similarity + preserved syntax + changed figurative meaning. Merged circuits trade faithful reproduction for interpretability after pruning.

- Failure signatures: (1) Threshold too low → circuit includes noise components; (2) Threshold too high → misses critical heads; (3) Poor corruption choice → fails to isolate idiom-specific processing; (4) Using final-logit metrics instead of intermediate-layer cosine similarity → misses semantic processing entirely.

- First 3 experiments:
  1. Reproduce cosine similarity plots (Figure 1) for a new idiom to confirm L (layer where figurative meaning stabilizes) before circuit discovery.
  2. Run threshold sweep (Figure 3) to identify τ* via edge-count inflection points before committing to single threshold.
  3. Patch individual Idiom Head (e.g., (2,0)) in isolation to measure performance drop, verifying the head's contribution before building full circuit.

## Open Questions the Paper Calls Out
None

## Limitations
- Limited corpus size (8 idioms) may not capture full diversity of non-compositional language processing
- Circuit discovery methodology relies on threshold-based pruning that may discard subtle but important components
- Use of intermediate-layer cosine similarity rather than downstream task performance may not capture all relevant semantic processing

## Confidence
**High Confidence (4/5)**: The two-phase processing structure (cross-token attention in layers 0-2, semantic integration in layers 3-5) is well-supported by the cosine similarity analysis and path patching results. The temporal ordering and layer boundaries are clearly demonstrated across multiple idioms.

**Medium Confidence (3/5)**: The identification of "Idiom Heads" as functional specialists for non-compositional processing is plausible given their consistent activation across idioms, but the sample size (8 idioms) is too small to establish robust functional specialization. The distinct QK space representation for each idiom is convincingly demonstrated through cross-idiom similarity analysis.

**Medium-Low Confidence (2/5)**: The "augmented reception" mechanism is the most speculative finding, with limited corpus validation and unclear generalizability beyond the specific idioms studied. While the attention edge patterns support the claim, the causal interpretation requires more extensive validation.

## Next Checks
1. **Expand corpus and test functional necessity**: Apply the path patching methodology to a larger set of idioms (n>50) spanning diverse syntactic patterns and semantic domains. For each identified Idiom Head, measure performance degradation when patched in isolation versus when other heads remain active, establishing functional necessity rather than mere correlation.

2. **Test cross-linguistic generalization**: Apply the same circuit discovery methodology to idioms from languages with different syntactic structures (e.g., verb-final languages, pro-drop languages) to determine whether the two-phase processing structure and Idiom Head specialization are universal or English-specific phenomena.

3. **Validate augmented reception mechanism**: Design targeted experiments where early-layer idiom processing is selectively enhanced or disrupted, then measure downstream attention patterns to idiom-relevant tokens. Use causal tracing to establish whether early-layer modifications directly cause enhanced later attention, or whether both phenomena independently respond to idiom presence.