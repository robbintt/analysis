---
ver: rpa2
title: 'ReLAM: Learning Anticipation Model for Rewarding Visual Robotic Manipulation'
arxiv_id: '2509.22402'
source_url: https://arxiv.org/abs/2509.22402
tags:
- learning
- relam
- reward
- anticipation
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ReLAM, a framework for visual robotic manipulation
  that addresses the challenge of reward design by automatically generating dense,
  structured rewards from action-free video demonstrations. ReLAM extracts task-relevant
  keypoints from videos, learns an anticipation model to predict intermediate subgoals,
  and uses keypoint distances to provide continuous rewards for training a low-level
  goal-conditioned policy under hierarchical reinforcement learning.
---

# ReLAM: Learning Anticipation Model for Rewarding Visual Robotic Manipulation

## Quick Facts
- **arXiv ID:** 2509.22402
- **Source URL:** https://arxiv.org/abs/2509.22402
- **Reference count:** 28
- **Primary result:** Achieves 100% success on Meta-World drawer and door tasks, outperforming diffusion-based and offline RL baselines.

## Executive Summary
ReLAM addresses the challenge of reward design in visual robotic manipulation by automatically generating dense, structured rewards from action-free video demonstrations. The framework extracts task-relevant keypoints from videos, learns an anticipation model to predict intermediate subgoals, and uses keypoint distances to provide continuous rewards for training a low-level goal-conditioned policy under hierarchical reinforcement learning. ReLAM achieves state-of-the-art performance on long-horizon manipulation tasks, significantly accelerating learning compared to baselines like Diffusion Reward and DACfO. The method demonstrates strong generalization and efficiency, particularly on Meta-World and ManiSkill benchmarks.

## Method Summary
ReLAM introduces a novel framework for visual robotic manipulation that eliminates the need for manual reward engineering. The method extracts keypoints from action-free video demonstrations using grounded SAM and CoTracker, filters static points, and identifies keyframes based on motion direction changes. An anticipation model (12-layer Transformer with frozen DINOv2 backbone) predicts intermediate subgoal coordinates, which are used to generate dense rewards based on keypoint distances. The low-level goal-conditioned policy is trained using PPO or IQL, with the dense reward function providing continuous feedback throughout the task. This approach achieves superior performance on long-horizon manipulation tasks compared to existing methods.

## Key Results
- Achieves 100% success rate on Meta-World Drawer Open and Door Open tasks
- Outperforms Diffusion Reward (80% and 100%) and DACfO (71.3% and 70%) on the same tasks
- Demonstrates 89.3% and 88.0% success rates on ManiSkill Push Cube and Pick Cube tasks respectively
- Shows significant acceleration in learning compared to baseline methods

## Why This Works (Mechanism)
ReLAM works by transforming the challenging problem of reward design into an automatic keypoint extraction and anticipation task. By leveraging action-free demonstrations, the framework learns to identify task-relevant subgoals without requiring explicit human annotation. The anticipation model predicts future states, enabling the agent to receive dense, continuous rewards that guide learning through the entire task sequence. This hierarchical approach allows the low-level policy to focus on short-horizon goal achievement while the high-level anticipation model handles long-term planning.

## Foundational Learning
- **Keypoint Tracking**: Tracking task-relevant objects across video frames using grounded SAM and CoTracker. Why needed: Provides visual anchors for reward calculation. Quick check: Monitor pixel coordinate consistency across frames.
- **Subgoal Anticipation**: Predicting future keypoint positions using Transformer architecture. Why needed: Enables dense reward signal generation. Quick check: Compare predicted vs actual keyframe positions using MSE.
- **Goal-Conditioned RL**: Training policies to achieve specific visual targets. Why needed: Allows modular learning of short-horizon sub-tasks. Quick check: Verify policy achieves intermediate subgoals before final task completion.

## Architecture Onboarding
- **Component Map:** Video Demos -> Keypoint Extractor (SAM+Cotracker) -> Subgoal Selector -> Anticipation Model (Transformer) -> Reward Generator -> Goal-Conditioned Policy (PPO/IQL) -> Environment
- **Critical Path:** Anticipation Model prediction → Reward generation → Policy update → Environment interaction
- **Design Tradeoffs:** Uses fixed keypoint count (4) vs. dynamic selection; pre-trained vision models vs. learned features; causal vs. bidirectional prediction
- **Failure Signatures:** Over-constrained policy (too many keypoints), noisy reward signal (poor anticipation accuracy), tracking drift (occlusion handling)
- **First Experiments:**
  1. Test keypoint extraction on held-out demo videos to verify tracking consistency
  2. Evaluate anticipation model accuracy on subgoal prediction tasks
  3. Implement and validate reward function with simple synthetic data

## Open Questions the Paper Calls Out
- Can incorporating multi-view observations into point cloud representations resolve single-viewpoint limitations and occlusion issues?
- Does replacing the custom anticipation model with pre-trained Vision-Language Models improve generalization to open-ended tasks?
- How does the linear motion segment assumption impact performance on complex tasks requiring curved trajectories?

## Limitations
- Performance degrades significantly with heavy occlusions that prevent keypoint tracking
- Current architecture relies on single-viewpoint observations, limiting robustness to camera angle variations
- Assumes tasks decompose into linear motion segments, potentially inefficient for curved trajectories

## Confidence
- **Success Rate Claims:** High - supported by quantitative comparisons on standard benchmarks
- **Hyperparameter Sensitivity:** Medium - several critical thresholds are underspecified
- **Generalization Claims:** Medium - limited testing on diverse task sets and environments
- **Implementation Details:** Medium - key parameters like reward function breakpoints are not fully specified

## Next Checks
- Perform sensitivity analysis of keypoint selection threshold Θ on task performance
- Evaluate anticipation model accuracy on held-out subgoal sequences
- Reproduce Meta-World results with full implementation to verify reported 100% success rates