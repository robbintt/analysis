---
ver: rpa2
title: Privacy-Enhancing Paradigms within Federated Multi-Agent Systems
arxiv_id: '2503.08175'
source_url: https://arxiv.org/abs/2503.08175
tags:
- privacy
- agents
- arxiv
- agent
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EPEAgents, a privacy-preserving architecture
  for federated multi-agent systems (MAS) that addresses the challenge of protecting
  sensitive information during agent collaboration. The core idea involves deploying
  a privacy-enhancing agent on a trusted server that filters task-relevant, agent-specific
  information during the retrieval-augmented generation (RAG) phase and context retrieval
  stages.
---

# Privacy-Enhancing Paradigms within Federated Multi-Agent Systems

## Quick Facts
- arXiv ID: 2503.08175
- Source URL: https://arxiv.org/abs/2503.08175
- Reference count: 12
- Introduces EPEAgents, a privacy-preserving architecture for federated MAS that filters sensitive information during RAG and context retrieval phases

## Executive Summary
This paper addresses privacy challenges in federated multi-agent systems where agents collaborate on tasks but may need to access sensitive information. The authors propose EPEAgents, which deploys a privacy-enhancing agent on a trusted server to filter data flows during retrieval-augmented generation and context retrieval stages. By intercepting all communications and forwarding only role-relevant information based on agent self-descriptions, EPEAgents achieves significant privacy protection while maintaining task performance. The approach was evaluated using synthetic financial and medical datasets across five different LLM backbones.

## Method Summary
EPEAgents implements a 3+n architecture where n=1 privacy-enhancing agent (CA) on a trusted server filters all data flows. Local agents (3 domain-specific agents) send self-descriptions to CA, which builds role-to-field mappings. During task execution, CA intercepts RAG retrievals and context retrievals, filtering user profiles based on whether agent roles match data field access labels. The CA also sanitizes intermediate reasoning outputs before forwarding to downstream agents. Evaluation uses synthetic financial and medical datasets with 21,750 samples across 25 synthetic user profiles, measuring utility via MCQ accuracy and privacy via refusal response accuracy.

## Key Results
- Privacy scores increase by up to 81.73% compared to baseline approaches
- GPT-o1 as CA backbone achieves 97.62% privacy score, while weaker backbones show significant drops (e.g., 58.67% with Gemini-1.5)
- Utility scores remain strong, with EPEAgents outperforming or matching baseline performance
- Multiple CA agents show diminishing returns with strong backbones, suggesting computational optimization opportunities

## Why This Works (Mechanism)

### Mechanism 1: Role-Based Information Minimization via Trusted Intermediary
- Claim: Centralized privacy agent filtering can significantly reduce privacy leakage while maintaining task utility
- Mechanism: CA intercepts all data flows and filters based on agent role mappings, forwarding only fields matching agent roles
- Core assumption: Server hosting CA is genuinely trusted; role-to-field mappings accurately reflect legitimate access needs
- Evidence anchors: Abstract description, formal definition in Section 4.2, related work on MAS security

### Mechanism 2: Self-Description-Based Dynamic Access Control
- Claim: Agent self-descriptions enable context-aware, task-specific data access without predefined static rules
- Mechanism: CA parses agent self-descriptions to build role-field mappings for filtering during all subsequent retrievals
- Core assumption: Self-descriptions are accurate and complete; CA's LLM correctly interprets role-field relevance
- Evidence anchors: Abstract mention, Section 4.1 description, lack of direct corpus evidence

### Mechanism 3: Reasoning Progress Sanitization
- Claim: Filtering intermediate agent outputs prevents terminal agents from accumulating and exposing aggregated sensitive information
- Mechanism: CA filters not only initial profile data but also intermediate reasoning outputs before forwarding
- Core assumption: Sensitive information in intermediate outputs can be reliably detected and removed
- Evidence anchors: Section 4.2 test case, Figure 2 illustration, Terrarium paper on aggregation risks

## Foundational Learning

- **Federated Learning vs. Federated MAS**: Understanding why FL techniques may not directly transfer to MAS conversation flows (model training vs. real-time collaboration; training data protection vs. dynamic conversation protection)
- **Retrieval-Augmented Generation (RAG) Integration Points**: Knowing where data enters/exits RAG pipelines for correct CA placement (RAG retrieval and context retrieval stages)
- **Spatial vs. Temporal Communication Edges in MAS**: Understanding that both spatial edges within rounds and temporal edges across rounds require filtering

## Architecture Onboarding

- **Component map**: Local Agents (C_1...C_N) -> Privacy-Enhancing Agent (C_A) -> Server Memory Bank -> Shared Knowledge Pool / User Profiles
- **Critical path**: Task T distributed to agents → Each agent sends self-description → C_A maps roles to profile fields → C_A filters retrievals and sanitizes intermediate outputs → Terminal agent produces final answer
- **Design tradeoffs**: Stronger CA backbone = better privacy but higher cost; multiple C_A agents distribute workload but may reduce consistency; permission elevation improves completion but requires trusted third-party
- **Failure signatures**: Privacy score drops with weak CA backbone (38.95% drop observed); utility degradation from over-filtering; terminal agent leaks accumulated data if sanitization disabled
- **First 3 experiments**: 1) Baseline comparison without CA filtering to establish privacy baseline; 2) CA backbone ablation comparing GPT-o1 vs. weaker backbones; 3) Self-description sensitivity testing vague vs. specific role descriptions

## Open Questions the Paper Calls Out

- **Lightweight model exploration**: Can specialized models replace LLMs as CA while maintaining privacy-utility trade-offs? (explicit in Section 4.5)
- **User preference alignment**: Do LLM-generated privacy labels align with real users' subjective privacy preferences across contexts? (explicit in Section 4.5)
- **Real-world dataset performance**: How does EPEAgents perform on authentic datasets vs. synthetic profiles? (inferred from synthetic data limitation)
- **Centralization security implications**: What are security implications of centralizing trust in single CA under adversarial attacks? (inferred from architecture design)

## Limitations

- Synthetic data evaluation limits ecological validity of privacy-utility findings
- Exact MCQ/OEQ questions and field definitions not provided, requiring generation that may not match original distribution
- Centralization creates single point of trust that could be compromised
- LLM-based filtering may fail to detect obliquely embedded sensitive information

## Confidence

- High confidence: Core architectural framework and general evaluation methodology
- Medium confidence: Relative performance rankings across backbones and configurations
- Low confidence: Exact numerical results due to unknown dataset specifics and generation variability

## Next Checks

1. **Cross-backbone consistency test**: Run identical configurations across multiple LLM providers to establish baseline variance in privacy/utility scores
2. **Role-description sensitivity analysis**: Systematically vary self-description completeness and measure corresponding privacy/utility trade-offs
3. **Temporal edge filtering validation**: Test whether privacy filtering correctly handles multi-round communication by designing scenarios with progressive information accumulation