---
ver: rpa2
title: 'CrossMed: A Multimodal Cross-Task Benchmark for Compositional Generalization
  in Medical Imaging'
arxiv_id: '2511.11034'
source_url: https://arxiv.org/abs/2511.11034
tags:
- classification
- segmentation
- generalization
- medical
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CrossMed, a benchmark designed to evaluate
  compositional generalization (CG) in multimodal large language models (MLLMs) applied
  to medical imaging. CrossMed unifies classification and segmentation tasks across
  X-ray, MRI, and CT modalities into a single visual question answering (VQA) format
  using a Modality-Anatomy-Task (MAT) triplet schema.
---

# CrossMed: A Multimodal Cross-Task Benchmark for Compositional Generalization in Medical Imaging

## Quick Facts
- arXiv ID: 2511.11034
- Source URL: https://arxiv.org/abs/2511.11034
- Reference count: 23
- Introduces CrossMed benchmark for evaluating compositional generalization in multimodal medical imaging models

## Executive Summary
CrossMed is a benchmark designed to evaluate compositional generalization in multimodal large language models applied to medical imaging. The benchmark unifies classification and segmentation tasks across X-ray, MRI, and CT modalities into a single visual question answering format using a Modality-Anatomy-Task (MAT) triplet schema. It contains 20,200 multiple-choice QA instances derived from four public datasets. Experiments demonstrate that models trained on MAT-related data achieve strong performance on classification (83.2%) and segmentation (0.75 cIoU), with significant drops in performance under unrelated and zero-overlap splits, validating CrossMed as a rigorous testbed for evaluating zero-shot, cross-task, and modality-agnostic generalization in medical vision-language models.

## Method Summary
CrossMed introduces a unified benchmark for evaluating compositional generalization in multimodal medical imaging models by converting classification and segmentation tasks into a visual question answering format. The benchmark uses a Modality-Anatomy-Task (MAT) triplet schema to create 20,200 multiple-choice questions from four public datasets. Experiments were conducted using LLaVA-Vicuna-7B and Qwen2-VL-7B models with three training paradigms: related (MAT-triplet present in training), unrelated (MAT-triplet absent), and zero-overlap (neither modality nor anatomy present in training). The benchmark evaluates both classification accuracy and segmentation performance using cIoU metrics, providing a comprehensive assessment of cross-task and cross-modality generalization capabilities.

## Key Results
- Models trained on MAT-related data achieve 83.2% classification accuracy and 0.75 segmentation cIoU
- Significant performance drops observed under unrelated (56.5%) and zero-overlap splits (39.1%)
- Cross-task transfer demonstrated: classification-trained models improve segmentation performance by 7% cIoU
- Classification accuracy reaches 76.6% on unseen anatomy within seen modalities

## Why This Works (Mechanism)
CrossMed leverages compositional generalization by structuring medical imaging tasks around Modality-Anatomy-Task (MAT) triplets, enabling models to learn compositional patterns that transfer across different combinations of medical domains. The unified VQA format allows models to apply learned reasoning patterns from one task to novel combinations, while the multiple-choice structure provides clear evaluation metrics. The benchmark's design specifically targets the ability of models to generalize beyond their training distribution by creating controlled splits that test related, unrelated, and zero-overlap scenarios.

## Foundational Learning
- Compositional Generalization: Models learn to combine learned components (modality, anatomy, task) in novel ways; needed for clinical reasoning across diverse scenarios; quick check: evaluate performance on unseen MAT combinations
- Multimodal Integration: Fusion of visual and textual information in medical contexts; needed for accurate diagnosis from imaging data; quick check: compare single vs. multimodal model performance
- Cross-Task Transfer: Ability to apply knowledge from one task (classification) to improve performance on another (segmentation); needed for efficient model training in clinical settings; quick check: measure performance gains when training across task types

## Architecture Onboarding
**Component Map:** Input Images -> Vision Encoder -> Text Encoder -> Fusion Layer -> Classification Head / Segmentation Head -> Output
**Critical Path:** Visual encoding of medical images combined with textual task description through fusion layer to produce either classification or segmentation output
**Design Tradeoffs:** Multiple-choice format simplifies evaluation but may limit question complexity; unified VQA format enables cross-task evaluation but may not capture all clinical nuances
**Failure Signatures:** Performance degradation on zero-overlap splits indicates limited compositional generalization; poor segmentation performance suggests insufficient spatial reasoning capabilities
**3 First Experiments:**
1. Evaluate baseline performance on related vs. unrelated MAT splits to establish generalization gap
2. Test cross-task transfer by training on classification and evaluating segmentation performance
3. Assess zero-shot performance on completely unseen modality-anatomy combinations

## Open Questions the Paper Calls Out
None

## Limitations
- Multiple-choice format may constrain question complexity and underestimate full reasoning capabilities
- Limited anatomical coverage (primarily lung and brain regions) may not represent full clinical diversity
- Results based on relatively small 7B parameter models; scalability to larger models or clinical deployment unexplored

## Confidence
- High confidence in benchmark design and MAT schema methodology
- Medium confidence in generalization claims requiring further clinical validation
- Medium confidence in cross-task transfer results needing broader task combination validation

## Next Checks
1. Evaluate CrossMed with larger MLLM variants (34B+ parameters) to assess scalability
2. Extend benchmark to include additional anatomical regions and imaging modalities
3. Conduct clinical expert review of generated questions and answers for medical accuracy