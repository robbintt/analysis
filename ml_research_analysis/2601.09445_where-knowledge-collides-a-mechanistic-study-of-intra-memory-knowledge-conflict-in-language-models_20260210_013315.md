---
ver: rpa2
title: 'Where Knowledge Collides: A Mechanistic Study of Intra-Memory Knowledge Conflict
  in Language Models'
arxiv_id: '2601.09445'
source_url: https://arxiv.org/abs/2601.09445
tags:
- knowledge
- patching
- activation
- synwikibio
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a mechanistic interpretability framework to
  study intra-memory knowledge conflicts in language models, where conflicting information
  about the same entity is encoded during pre-training. The authors construct a synthetic
  dataset with deliberately conflicting biographies and fine-tune language models
  on it.
---

# Where Knowledge Collides: A Mechanistic Study of Intra-Memory Knowledge Conflict in Language Models

## Quick Facts
- arXiv ID: 2601.09445
- Source URL: https://arxiv.org/abs/2601.09445
- Reference count: 31
- Key outcome: Mechanistic framework identifies late-layer attention components as primary conflict encoders, with activation patching achieving up to 27% steering success in resolving conflicting knowledge representations.

## Executive Summary
This work presents a mechanistic interpretability framework to study intra-memory knowledge conflicts in language models, where conflicting information about the same entity is encoded during pre-training. The authors construct a synthetic dataset with deliberately conflicting biographies and fine-tune language models on it. Using logit lens analysis, they identify that attention components in later layers are primarily responsible for encoding conflicting knowledge. Through activation patching experiments, they demonstrate that targeted interventions can steer model predictions toward accurate outputs, achieving up to 27% steering success with cross-model activation patching (CMAP).

## Method Summary
The authors construct SynWikiBio, a synthetic dataset of 2000 Wikipedia-style biographies containing 1000 pairs of contradictory biographies for the same entities. They fine-tune base models (GPT-2 XL and Qwen3-4B) on this data to create conflicted (ℓ′_mix) and clean (ℓ′_clean) variants. Using logit lens analysis, they measure attention and MLP contributions to conflicting tokens across layers. They then apply activation patching experiments, substituting activations from clean prompts into conflicted model states, and introduce CMAP—transferring clean activations from a reference model to achieve more reliable interventions.

## Key Results
- Attention components in later layers (21-47) contribute more to conflicting token probabilities than MLP components
- Activation patching interventions can steer model predictions toward accurate outputs, achieving up to 27% steering success
- Cross-model activation patching (CMAP) enables more reliable interventions by transferring clean activations from non-conflicted reference models

## Why This Works (Mechanism)

### Mechanism 1: Late-Layer Attention Dominance in Conflict Encoding
- **Claim:** Attention components in later transformer layers (approximately layers 21-47 in GPT-2 XL) encode the majority of conflicting knowledge signals, with MLP contributions being comparatively weaker.
- **Mechanism:** The residual stream accumulates information through sequential attention and MLP updates. Logit lens analysis reveals that attention blocks contribute larger probability shifts to conflicting tokens (t₁, t₂) than MLP blocks, particularly in final layers where parametric knowledge retrieval crystallizes into token predictions.
- **Core assumption:** Components that produce systematically different probability contributions for conflicting tokens versus control tokens are causally involved in conflict representation (not merely correlational).
- **Evidence anchors:**
  - [abstract] "Using logit lens analysis, they identify that attention components in later layers are primarily responsible for encoding conflicting knowledge."
  - [Section 6.1] "We observe stronger aggregate contributions in the later layers, particularly between layers 21 and 47. Similar to previous findings... attention components show stronger influence on the final probability t1 and t2 than their corresponding MLP components."
- **Break condition:** If early-layer components showed equivalent or stronger probability contributions, or if MLP contributions exceeded attention in final layers, the mechanism would be invalidated.

### Mechanism 2: Activation Patching as Causal Intervention
- **Claim:** Substituting activations from clean (non-conflicting) prompts into conflicted model states can shift probability distributions between competing tokens, demonstrating causal influence of specific components.
- **Mechanism:** When activation a_c(x_target) at component c is replaced with a_c(x_source) from a clean prompt, the forward pass propagates unconflicted representations. The probability change Δ^↑_{t1,c,l} measures causal effect: if patching increases p(t₁) while decreasing p(t₂), component c causally mediates the conflict resolution.
- **Core assumption:** Activations from clean prompts about different entities (same attribute value) transfer meaningfully to conflicted entity prompts without introducing confounding inter-person variation noise.
- **Evidence anchors:**
  - [abstract] "Through activation patching experiments, they demonstrate that targeted interventions can steer model predictions toward accurate outputs, achieving up to 27% steering success."
  - [Section 4.3] Defines causal effect formally and describes patching from clean prompts to noisy prompts.
- **Break condition:** If patched activations produced random or consistently opposite effects (decreasing target token probability), causal attribution would fail.

### Mechanism 3: Cross-Model Activation Patching (CMAP) Reduces Noise
- **Claim:** Transferring activations from a clean reference model (ℓ′_clean) to a conflicted model (ℓ′_mix) yields more reliable interventions than same-model patching, because the reference model lacks contradictory representations.
- **Mechanism:** Both models share identical architecture and training data except for the presence of contradictory claims. Patching from ℓ′_clean provides activations encoding only the ground-truth fact f_i without competing representation f̄_i, eliminating intra-memory interference.
- **Core assumption:** Architectural alignment ensures activation spaces are sufficiently compatible for meaningful transfer; the only meaningful difference between models is the conflict itself.
- **Evidence anchors:**
  - [abstract] "The authors introduce CMAP as a promising method for causal tracing in knowledge conflict scenarios, enabling more reliable interventions by transferring clean activations from a non-conflicted reference model."
  - [Section 6.4] "CMAP improve steering success, reaching a maximum of 27% at Layer 44. While steering rates for university samples remain similar... rates for company samples increase substantially, from 17% to 27%."
- **Break condition:** If CMAP consistently underperformed standard activation patching across all conflict types, the noise-reduction hypothesis would be unsupported.

## Foundational Learning

- **Concept:** Residual Stream Decomposition
  - **Why needed here:** Understanding how attention and MLP components sequentially update the residual stream is prerequisite to interpreting logit lens probability contributions.
  - **Quick check question:** Given x^mid_l = x^pre_l + f^attn_l(x^pre_l), which term represents attention's contribution to the next-layer input?

- **Concept:** Causal Tracing / Activation Patching
  - **Why needed here:** The core intervention method requires understanding how activation substitution isolates component-level causal effects.
  - **Quick check question:** If patching component c changes output probability from 0.3 to 0.7 for token t, what is the causal effect magnitude?

- **Concept:** Intra-Memory vs. Context-Memory Conflict
  - **Why needed here:** This paper specifically addresses conflicts encoded during pre-training (intra-memory), distinct from conflicts between parametric knowledge and external context.
  - **Quick check question:** Does intra-memory conflict arise from (a) retrieved documents contradicting model knowledge, or (b) contradictory facts both present in training data?

## Architecture Onboarding

- **Component map:** Residual stream -> Attention modules (f^attn) -> MLP modules (f^mlp) -> Unembedding matrix (W_U)
- **Critical path:**
  1. Construct conflicting biography pairs in SynWikiBio
  2. Fine-tune base model to create ℓ′_mix (conflicted) and ℓ′_clean (reference)
  3. Apply logit lens to identify high-contribution components (layers 21-47 attention)
  4. Run activation patching experiments (same-model and CMAP)
  5. Measure steering success rate (probability of flipping output to source token)
- **Design tradeoffs:**
  - Same-model patching: simpler setup but introduces inter-person noise
  - CMAP: requires training separate clean model but provides cleaner causal attribution
  - Layer selection: focusing only on late layers may miss distributed conflict representations
- **Failure signatures:**
  - Low steering rates (<10%) indicate weak causal localization
  - Negative probability changes when patching with correct-fact activations suggest noise or misaligned activation spaces
  - Inconsistent results across conflict types (university vs. company) may indicate attribute-specific mechanisms
- **First 3 experiments:**
  1. Reproduce logit lens analysis on GPT-2 XL with SynWikiBio to verify layer 21-47 attention dominance pattern
  2. Run activation patching on layer 43 attention component to confirm 10-20% steering rates for university conflicts
  3. Implement CMAP pipeline training ℓ′_clean and compare steering success against standard patching for company conflicts (expect ~27% with CMAP)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do models rely on a unified feature circuit to resolve internal knowledge conflicts, or do they exhibit systematic preferences for certain facts regardless of circuit structure?
- **Basis in paper:** [explicit] The authors state in Section 6.6 that "unexpected behaviors across different conflict attributes call into question whether diverse forms of knowledge conflict can be treated as a single phenomenon. These observations further raise the question of whether models rely on a unified feature circuit to resolve internal knowledge conflicts, or instead exhibit systematic preferences for certain facts."
- **Why unresolved:** CMAP improved steering for company-attribute conflicts but degraded performance for university-attribute conflicts in Qwen3-4B, suggesting heterogeneous mechanisms across conflict types that current experiments cannot disentangle.
- **What evidence would resolve it:** Systematic mapping of component contributions across diverse conflict types (temporal, factual, linguistic) to identify shared versus specialized circuits, combined with ablation studies testing whether disabling one conflict-resolution circuit affects others.

### Open Question 2
- **Question:** What underlying factors drive a model's preference for one conflicting fact over another when both originate from pre-training data?
- **Basis in paper:** [explicit] The Limitations section states: "Finally, we do not analyze the underlying factors that drive the model's preference from one conflicting fact over another. Addressing this problem in future work can provide a deeper understanding of the model's internal rationale."
- **Why unresolved:** The current framework only identifies where conflicts are encoded and demonstrates intervention capability, but does not explain why certain facts dominate without intervention or what training data properties influence this selection.
- **What evidence would resolve it:** Controlled experiments varying training data characteristics (frequency, recency, syntactic position, contextual support) for conflicting facts, combined with analysis of resulting probability distributions and attention patterns during generation.

### Open Question 3
- **Question:** Why does CMAP effectiveness vary dramatically across model architectures and conflict attribute types?
- **Basis in paper:** [inferred] The paper shows CMAP achieving 27% steering success on GPT-2 XL but up to 70% on Qwen3-4B, while also showing CMAP improving company-attribute conflicts but degrading university-attribute conflicts. The authors note this "signals that the presence or absence of some information within the activation has caused it to behave erratically" but do not explain the mechanism.
- **Why unresolved:** The experiments demonstrate the phenomenon but lack systematic analysis of architectural or representational differences that cause these variations, particularly the unexplained performance drop in Qwen3-4B's layer 33.
- **What evidence would resolve it:** Comparative analysis of activation distributions across architectures, ablation studies isolating specific architectural differences (attention patterns, layer normalization, activation functions), and probing experiments to identify what information in clean activations becomes counterproductive in certain conflict contexts.

## Limitations
- Synthetic data may not capture the full complexity of real-world knowledge conflicts, which contain more nuanced contradictions and context-dependent ambiguity
- Findings focus on decoder-only transformer architectures and may not generalize to encoder-decoder models or emerging architectures with different attention mechanisms
- Critical training hyperparameters remain unspecified, including learning rates, batch sizes, and optimizer settings that could influence conflict encoding strength

## Confidence

**High Confidence (Likelihood >80%):**
- Attention components in later layers (21-47) contribute more to conflicting token probabilities than MLP components
- Activation patching produces measurable steering effects on conflicted model outputs
- Larger models demonstrate better conflict handling than smaller models

**Medium Confidence (Likelihood 50-80%):**
- CMAP consistently outperforms standard activation patching across all conflict types
- The specific layer 43-44 attention components are optimal for steering interventions
- Cross-model activation spaces are sufficiently aligned for meaningful transfer

**Low Confidence (Likelihood <50%):**
- Findings generalize to real-world knowledge conflicts beyond synthetic biographies
- Conflict resolution mechanisms are identical across different entity types (university vs. company)
- The 27% maximum steering success represents a ceiling for this intervention approach

## Next Checks
1. **Cross-Architecture Validation:** Replicate the logit lens analysis and activation patching experiments on a transformer variant with different attention mechanisms (e.g., Performer or Linear Transformer) to verify whether late-layer attention dominance is architecture-specific or a general phenomenon.

2. **Real-World Conflict Transfer:** Apply the mechanistic framework to a dataset of real biographies containing verified conflicting information (e.g., from fact-checking databases) to assess whether synthetic conflict patterns transfer to naturally occurring contradictions.

3. **Temporal Conflict Analysis:** Design experiments with conflicts that have clear temporal ordering (e.g., "CEO from 2010-2020" vs. "CEO from 2020-present") to determine whether conflict resolution mechanisms differ for sequential versus parallel contradictions.