---
ver: rpa2
title: Separated Contrastive Learning for Matching in Cross-domain Recommendation
  with Curriculum Scheduling
arxiv_id: '2502.16239'
source_url: https://arxiv.org/abs/2502.16239
tags:
- learning
- domain
- sccdr
- contrastive
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses training instability in cross-domain recommendation
  (CDR) caused by directly mixing intra-domain and inter-domain contrastive learning
  tasks. The authors propose SCCDR, a framework that separates intra-CL and inter-CL
  tasks into distinct stages, with an inter-domain curriculum scheduling strategy
  that orders negative samples by difficulty (measured via Katz centrality).
---

# Separated Contrastive Learning for Matching in Cross-domain Recommendation with Curriculum Scheduling

## Quick Facts
- arXiv ID: 2502.16239
- Source URL: https://arxiv.org/abs/2502.16239
- Reference count: 40
- Key outcome: Achieves >2% HIT@100 improvement over CCDR in half of cross-domain settings, with 1.92% CTR and 3.65% duration gains in online A/B tests

## Executive Summary
This paper addresses training instability in cross-domain recommendation caused by mixing intra-domain and inter-domain contrastive learning tasks. The authors propose SCCDR, a framework that separates these tasks into distinct sequential stages with an inter-domain curriculum scheduling strategy. By ordering negative samples by difficulty (measured via Katz centrality) and applying stop-gradient between stages, SCCDR significantly improves both cold-start and general recommendation performance across multiple datasets.

## Method Summary
SCCDR is a two-stage contrastive learning framework for cross-domain recommendation. In Stage 1, intra-domain contrastive learning (intra-CL) trains separate encoders for source and target domains using BCE loss on neighbor similarity. In Stage 2, inter-domain contrastive learning (inter-CL) aligns representations between domains using InfoNCE loss, with a curriculum scheduler that orders negative samples by difficulty (measured via Katz centrality) and a stop-gradient operation to protect source domain representations. The framework is evaluated on Amazon datasets (7 domains, 8 cross-domain scenarios) and a proprietary industrial dataset.

## Key Results
- Achieves >2% HIT@100 improvement over CCDR in half of cross-domain settings
- Online A/B tests show 1.92% CTR and 3.65% duration increases compared to GraphDR+
- Particularly effective in cold-start scenarios where target domain data is sparse
- Demonstrates superior performance across different GNN architectures (GraphSAGE, LightGCN, GAT)

## Why This Works (Mechanism)

### Mechanism 1
Separating intra-CL and inter-CL into sequential stages improves training stability and final embedding quality. Intra-domain contrastive patterns are learned first because within-domain user-item interactions are denser and preferences more homogeneous. Once stable domain-specific representations exist, inter-domain alignment introduces cross-domain knowledge without destabilizing the already-learned structure. Core assumption: Inter-domain transfer is inherently harder than intra-domain learning due to sparser overlap and heterogeneous preference patterns across domains.

### Mechanism 2
Stop-gradient operation between stages protects source domain representations from degradation during knowledge transfer. During inter-CL, gradients are blocked from updating the source encoder. This prevents target-domain noise from distorting source embeddings, maintaining their "uniformity" and preventing dimensional collapse. Core assumption: Source domain has learned adequate representations during intra-CL; allowing inter-CL gradients to modify them introduces noise without proportional benefit.

### Mechanism 3
Curriculum scheduling of negative samples by Katz centrality reduces noise impact and improves inter-CL calibration. Negative samples are ordered by difficulty (high Katz centrality = easier). Training begins with the top 50% easiest negatives, then progressively adds harder ones. High-centrality nodes have more stable, frequently-observed patterns; starting with them provides cleaner contrastive signals. Core assumption: Node centrality correlates with sample "difficulty" for contrastive learning—high-centrality items have more learnable patterns.

## Foundational Learning

- **Contrastive Learning (InfoNCE, BCE Loss)**
  - Why needed here: SCCDR relies on both InfoNCE (inter-CL) and BCE-based neighbor similarity loss (intra-CL). Understanding positive/negative sampling and temperature scaling is essential.
  - Quick check question: Can you explain why maximizing agreement between a node and its neighbors while minimizing agreement with random nodes learns useful representations?

- **Graph Neural Networks (Message Passing)**
  - Why needed here: SCCDR uses GraphSAGE with JK-Net as its encoder; representations are computed by aggregating neighborhood information.
  - Quick check question: How does a 2-layer GNN aggregate information from 2-hop neighbors, and what does a skip connection (JK-Net) add?

- **Cross-Domain Recommendation Basics**
  - Why needed here: The entire framework assumes a source domain with rich data and a target domain (often sparse/cold-start) sharing some overlapping users.
  - Quick check question: What role do overlapping users play in transferring knowledge between domains?

## Architecture Onboarding

- **Component map:** Preprocess graphs (Katz centrality) -> Train intra-CL stage (BCELoss) -> Apply stop-gradient -> Train inter-CL stage (InfoNCE with curriculum) -> Evaluate HIT@N

- **Critical path:** 1) Preprocess graphs → compute Katz centrality for all nodes 2) Train intra-CL stage → optimize L_intra until convergence 3) Enable stop-gradient on source encoder 4) Train inter-CL stage with curriculum → start with 50% easiest negatives, add harder ones over N_step intervals

- **Design tradeoffs:** GNN encoder choice: GraphSAGE vs. LightGCN vs. GAT—paper finds GraphSAGE with JK-Net balances performance and efficiency; Loss weights: λ_intra=1.0, λ_inter=0.5 worked best; Static vs. dynamic curriculum: Paper uses static precomputed ordering

- **Failure signatures:** Training loss oscillates or diverges → check if intra-CL and inter-CL are being mixed incorrectly (should be sequential); Source domain embeddings collapse or lose uniformity → verify stop-gradient is applied during inter-CL; Inter-CL stage shows no improvement → curriculum may be starting with too many hard negatives

- **First 3 experiments:** 1) Reproduce ablation: Run SCCDR, SCCDR # (no curriculum), SCCDR ## (no stop-gradient) on Books-Videos dataset; confirm Table 5 performance ordering 2) Loss weight sweep: Vary λ_intra ∈ {0.5, 1.0, 1.5, 2.0} and λ_inter ∈ {0.5, 1.0, 1.5, 2.0} on a smaller dataset to verify robustness claim 3) Encoder substitution: Replace GraphSAGE with LightGCN to confirm framework is encoder-agnostic

## Open Questions the Paper Calls Out
- The authors acknowledge that other alternatives to the static curriculum scheduler implementation exist and leave exploration of dynamic or alternative curriculum scheduling strategies for future work.

## Limitations
- Curriculum scheduler details (step size, scheduling granularity) are underspecified beyond the equation
- Exact epoch counts and negative sampling ratios are not provided, affecting reproducibility
- The Katz centrality correlation with "difficulty" is assumed but not empirically validated against other difficulty metrics

## Confidence
- **High confidence** in the mechanism of separating intra-CL and inter-CL into sequential stages to reduce training instability
- **Medium confidence** in the stop-gradient operation's effectiveness, supported by ablation but lacking ablation on when it's most needed
- **Low confidence** in the curriculum scheduler's necessity without direct comparison to random or adaptive negative sampling strategies

## Next Checks
1. Ablate curriculum scheduling by comparing Katz-ordered negatives vs. random vs. adaptive difficulty ordering on cold-start scenarios
2. Test stop-gradient necessity when source domain is noisy or when domains are highly homogeneous
3. Validate Katz centrality as difficulty proxy by comparing against alternative metrics (e.g., neighbor count, edge weight variance)