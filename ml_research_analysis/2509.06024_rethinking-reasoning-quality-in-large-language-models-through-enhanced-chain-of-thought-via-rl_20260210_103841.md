---
ver: rpa2
title: Rethinking Reasoning Quality in Large Language Models through Enhanced Chain-of-Thought
  via RL
arxiv_id: '2509.06024'
source_url: https://arxiv.org/abs/2509.06024
tags:
- reasoning
- answer
- logical
- drer
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DRER, a reinforcement learning reward framework
  designed to improve reasoning quality in large language models. DRER combines a
  Reasoning Quality Reward, which incentivizes reasoning chains that increase confidence
  in correct answers, and a Dynamic Length Advantage, which stabilizes training by
  adjusting rewards based on response length.
---

# Rethinking Reasoning Quality in Large Language Models through Enhanced Chain-of-Thought via RL

## Quick Facts
- arXiv ID: 2509.06024
- Source URL: https://arxiv.org/abs/2509.06024
- Reference count: 40
- A 7B model trained with DRER achieves GPT-o3-mini level performance on LogicTree in 400 steps, with 30% increase in CoT answer confidence and 75% reduction in token usage

## Executive Summary
This paper introduces DRER, a reinforcement learning reward framework designed to improve reasoning quality in large language models. DRER combines a Reasoning Quality Reward that incentivizes reasoning chains that increase confidence in correct answers, and a Dynamic Length Advantage that stabilizes training by adjusting rewards based on response length. To evaluate the framework, the authors release LogicTree, a synthetic dataset focused on formal deductive reasoning. Experiments show that a 7B model trained with DRER achieves GPT-o3-mini level performance on LogicTree in 400 steps, with a 30% increase in CoT answer confidence and 75% reduction in token usage. The model also generalizes to other logical reasoning and mathematical benchmarks.

## Method Summary
DRER is an RL framework that enhances LLM reasoning quality by providing fine-grained credit to reasoning chains that demonstrably raise the likelihood of correct answers. It uses a Reasoning Quality Reward (Rq) that computes the difference in log-likelihood between CoT and direct answering approaches, passed through tanh to produce a bounded signal. A Dynamic Length Advantage penalizes responses whose lengths deviate from validation-derived percentiles to prevent reward hacking. The framework is trained on a synthetic LogicTree dataset of 9,600 deductive reasoning problems with programmable difficulty, using DAPO/GRPO optimization on a 7B model for 400 steps.

## Key Results
- 7B model trained with DRER achieves GPT-o3-mini level performance on LogicTree in 400 steps
- 30% increase in CoT answer confidence compared to baselines
- 75% reduction in token usage while maintaining accuracy
- Generalization to other logical reasoning and mathematical benchmarks with +3.7% on AIME24

## Why This Works (Mechanism)

### Mechanism 1: Reasoning Quality Reward (Rq)
The reward computes log-likelihood margin Δ(x) = ℓCoT − ℓNoCoT, where ℓCoT measures answer token probability given the CoT, and ℓNoCoT measures it without. This margin is passed through tanh to produce a bounded reward Rq ∈ [-1, 1]. Effective reasoning chains causally increase model confidence in correct answers; spurious chains do not.

### Mechanism 2: Dynamic Length Advantage (gi)
After each validation round, compute 5th and 95th percentile response lengths for each difficulty bucket. Apply exponential attenuation gi = exp(-max{0, Lmin - ℓi, ℓi - Lmax}/τ) to the advantage signal. Correct responses cluster within a predictable length range per difficulty level; outliers indicate pathological behavior.

### Mechanism 3: LogicTree Benchmark Controllability
Synthetic deductive reasoning problems with programmable depth/width enable precise evaluation of reasoning capacity independent of world knowledge. Construct nested binary reasoning trees from seven formal inference rules using semantically neutral statements. Control depth (1-8) and branching width to scale difficulty.

## Foundational Learning

- **Generalized Advantage Estimation (GAE)**: DRER modifies advantage signals via length attenuation; understanding how GAE balances bias-variance is prerequisite for diagnosing training dynamics. Quick check: Can you explain why λ=1 in GAE yields lower variance but higher bias compared to λ=0?

- **Log-likelihood gradients in autoregressive models**: Rq requires computing gradients through log-probability differences; incorrect implementation can produce reward signals with wrong scale or sign. Quick check: Given a model outputting token probabilities p1, p2, p3, how would you compute the average log-likelihood of a 3-token ground-truth sequence?

- **Percentile-based thresholding**: Dynamic Length Advantage depends on robust estimation of Lmin/Lmax; outlier-sensitive estimators can destabilize training. Quick check: Why might the 5th/95th percentiles be preferred over min/max for defining length bounds?

## Architecture Onboarding

- **Component map**: Trajectory Generator -> Likelihood Computer -> Reward Combiner -> Length Tracker -> Advantage Shaper -> RL Optimizer
- **Critical path**: The reward-quality computation (likelihood difference) must complete before advantage normalization; length statistics must be refreshed after each validation epoch
- **Design tradeoffs**: λq controls reasoning-quality vs task-success balance; τ (temperature) controls attenuation harshness; bucketing granularity affects threshold accuracy
- **Failure signatures**: Rq saturating near +1 or -1 suggests tanh scaling issues; response lengths collapsing to Lmin suggests over-penalization; accuracy improving but confidence gap not increasing suggests reward hacking
- **First 3 experiments**:
  1. Ablate Rq: Train with only Dynamic Length Advantage to isolate the contribution of confidence-based shaping
  2. Sensitivity sweep on λq and τ: Grid search λq ∈ {0.1, 0.5, 1.0, 2.0} and τ ∈ {5, 8, 10, 15}
  3. Cross-domain transfer test: Train on LogicTree depth 1-5, evaluate on depth 6-8 and on GSM8K/AIME24

## Open Questions the Paper Calls Out

**Scalability to Larger Models**: Can the DRER framework scale effectively to models beyond 7B parameters (e.g., 70B-scale or MoE architectures) without prohibitive memory and latency overhead from token-level reward computation? [explicit] Section G (Limitations) states: "All experiments use Qwen-2.5-7B-Instruct-1M as backbone. The memory and latency overhead of token-level rewards on 70 B-scale or MoE models is unknown and may be prohibitive."

**Human Alignment of Reasoning Quality**: To what extent do DRER-optimized reasoning chains align with human judgments of reasoning quality, beyond the automated confidence-based metrics used in this work? [explicit] Section G (Limitations) states: "Training and evaluation rely on an automatic logic verifier and confidence scores; no human preference or chain-quality annotation is included, which may overlook subjective aspects of reasoning quality."

**Extension to Diverse Reasoning Paradigms**: Can the DRER framework be successfully extended to reasoning paradigms beyond deductive logic, such as inductive reasoning, analogical reasoning, or traceable multi-modal reasoning? [explicit] Section G (Limitations) and Section 5 state: "LogicTree is limited to the deductive reasoning paradigm, while more diverse forms such as analogical reasoning, inductive reasoning, or traceable reasoning have not yet been evaluated."

## Limitations
- All experiments use a 7B base model; scalability to 70B+ models is unknown
- No human evaluation of reasoning quality; relies solely on automated metrics
- LogicTree focuses exclusively on deductive reasoning, limiting generalization assessment
- Validation round frequency for updating length thresholds is unspecified

## Confidence
- **High Confidence**: Modular design of DRER is clearly specified and reproducible; LogicTree dataset generation method is well-defined
- **Medium Confidence**: 30% increase in CoT answer confidence and 75% reduction in token usage are based on internal metrics; external validation is needed
- **Low Confidence**: Assertion of generalization capabilities is based on a small set of OOD tasks; paper doesn't address potential reward hacking

## Next Checks
1. **Ablate Reasoning Quality Reward (Rq)**: Train a DRER variant without Rq to isolate whether it genuinely improves reasoning quality or merely increases confidence through other means
2. **Cross-Domain Generalization**: Evaluate DRER-trained models on commonsense QA, multi-step planning, and open-domain problem solving to assess breadth of generalization
3. **Validation Frequency Robustness**: Systematically vary validation round frequency to clarify sensitivity of Dynamic Length Advantage to threshold update frequency