---
ver: rpa2
title: 'AI persuading AI vs AI persuading Humans: LLMs'' Differential Effectiveness
  in Promoting Pro-Environmental Behavior'
arxiv_id: '2503.02067'
source_url: https://arxiv.org/abs/2503.02067
tags:
- synthetic
- human
- chat
- participants
- persuasion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluates the effectiveness of large language models
  (LLMs) in promoting pro-environmental behavior (PEB) by comparing real humans (n=1,200),
  simulated humans based on actual data (n=1,200), and fully synthetic personas (n=1,200).
  Participants were exposed to personalized or standard chatbots or static statements
  using four persuasion strategies: moral foundations, future self-continuity, action
  orientation, or "freestyle." Results reveal a "synthetic persuasion paradox": synthetic
  and simulated agents significantly affect their post-intervention PEB stance, while
  human responses barely shift.'
---

# AI persuading AI vs AI persuading Humans: LLMs' Differential Effectiveness in Promoting Pro-Environmental Behavior

## Quick Facts
- arXiv ID: 2503.02067
- Source URL: https://arxiv.org/abs/2503.02067
- Reference count: 40
- Key outcome: Synthetic and simulated agents showed significant changes in pro-environmental behavior stance post-intervention, while human responses barely shifted, revealing a "synthetic persuasion paradox."

## Executive Summary
This study evaluates the effectiveness of large language models (LLMs) in promoting pro-environmental behavior by comparing real humans, simulated humans based on actual data, and fully synthetic personas. The research reveals a "synthetic persuasion paradox" where synthetic and simulated agents significantly affect their post-intervention pro-environmental behavior stance, while human responses barely shift. The findings suggest that while LLM simulations can be useful for pre-evaluating environmental interventions, they may overestimate their effectiveness when predicting real-world human behavior.

## Method Summary
The study employed a 3x2x5 experimental design with 3,600 total participants across three agent types (real humans, simulated humans, synthetic personas), two message types (personalized vs. standard), and five intervention conditions (four persuasion strategies plus control). Participants were recruited through Amazon Mechanical Turk and exposed to chatbot interactions or static statements using moral foundations, future self-continuity, action orientation, or freestyle approaches. The synthetic personas were generated using LLM-based agents, while simulated participants were based on actual human data. The study measured changes in pro-environmental behavior using the Environmental Identity scale.

## Key Results
- Synthetic and simulated agents showed significant shifts in pro-environmental behavior stance post-intervention
- Human participants showed minimal behavioral change despite similar intervention exposure
- Simulated participants better approximated human trends than fully synthetic agents but still overestimated intervention effectiveness

## Why This Works (Mechanism)
The study reveals that LLMs can effectively simulate behavioral responses to persuasion strategies, creating convincing synthetic personas that respond to environmental messaging. The differential effectiveness across agent types suggests that LLM-generated responses capture certain persuasive dynamics that differ from human decision-making processes, particularly in how they process and respond to moral, temporal, and action-oriented framing.

## Foundational Learning
- **Environmental Identity Scale**: Measures individual connection to environmental causes; needed to quantify behavioral intentions, quick check involves validated survey responses
- **Moral Foundations Theory**: Framework for understanding moral reasoning; needed to design persuasive messaging, quick check involves testing different moral appeals
- **Future Self-Continuity**: Psychological concept about temporal connection to future self; needed for designing long-term behavior interventions, quick check involves measuring temporal discounting
- **Action Orientation**: Behavioral tendency toward action vs. inaction; needed to tailor intervention strategies, quick check involves assessing response to active vs. passive framing
- **Simulated Human Models**: Computational representations of human behavior; needed for scalable testing, quick check involves comparing simulation accuracy against real human data
- **Synthetic Persona Generation**: LLM-based creation of artificial agents; needed for large-scale behavioral simulation, quick check involves validating persona responses against known patterns

## Architecture Onboarding
**Component Map**: Human Participants -> Chatbot Interface -> Persuasion Strategy -> Environmental Identity Scale
**Critical Path**: Intervention Delivery -> Behavioral Measurement -> Data Analysis
**Design Tradeoffs**: Real human testing provides ecological validity but limited scalability; synthetic agents offer scalability but reduced accuracy
**Failure Signatures**: Overestimation of intervention effectiveness when using synthetic agents; poor generalizability across different demographic groups
**First Experiments**: 1) Compare synthetic agent responses across different LLM models; 2) Test intervention effectiveness with varied demographic sampling; 3) Validate synthetic responses against longitudinal human behavior data

## Open Questions the Paper Calls Out
None

## Limitations
- Study relies on self-reported behavioral intentions rather than actual behavior
- Recruitment pool (Amazon Mechanical Turk) may not represent broader population
- Synthetic personas based on limited demographic and behavioral data

## Confidence
- Generalizability: Medium
- Methodological rigor: Medium
- Practical significance: Medium

## Next Checks
1. Conduct a follow-up study measuring actual pro-environmental behaviors rather than self-reported intentions, with longitudinal tracking to assess sustained behavioral change.
2. Expand the synthetic persona generation to include more diverse demographic characteristics and behavioral patterns, then compare results across multiple LLM models to assess consistency.
3. Test the same intervention strategies in controlled real-world settings with face-to-face human interactions to establish baseline effectiveness before comparing with LLM-mediated interventions.