---
ver: rpa2
title: Exploring Selective Retrieval-Augmentation for Long-Tail Legal Text Classification
arxiv_id: '2508.19997'
source_url: https://arxiv.org/abs/2508.19997
tags:
- legal
- classes
- ledgar
- augmentation
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores Selective Retrieval-Augmentation (SRA) to address
  long-tail label distributions in legal text classification. SRA targets low-frequency
  classes by retrieving semantically relevant clauses from the training set, avoiding
  noise for well-represented classes.
---

# Exploring Selective Retrieval-Augmentation for Long-Tail Legal Text Classification

## Quick Facts
- arXiv ID: 2508.19997
- Source URL: https://arxiv.org/abs/2508.19997
- Reference count: 12
- This paper explores Selective Retrieval-Augmentation (SRA) to address long-tail label distributions in legal text classification.

## Executive Summary
This paper introduces Selective Retrieval-Augmentation (SRA) to tackle the long-tail label distribution problem in legal text classification. The method retrieves semantically relevant clauses from the training set and concatenates them only with samples belonging to low-frequency classes, avoiding noise injection for well-represented classes. Experiments on LEDGAR (single-label) and UNFAIR-ToS (multi-label) datasets demonstrate that SRA improves macro-F1 by up to 6.0% over baselines and outperforms the best LexGLUE models.

## Method Summary
The method uses a two-stage retrieval pipeline (TF-IDF for candidate selection, SBERT for semantic re-ranking) to find relevant clauses from the training set. Only samples belonging to low-frequency classes (determined by a cutoff ratio α) are augmented with retrieved text. The augmented text is concatenated with the original using a [SEP] token and fed to a RoBERTa-base classifier. The same augmentation is applied consistently during training, validation, and testing to prevent distribution shift.

## Key Results
- SRA improves macro-F1 by up to 6.0% over baselines on long-tail legal datasets
- Selective augmentation outperforms full augmentation, which degrades macro-F1 by 0.011
- SRA achieves better results than the best LexGLUE models across both LEDGAR and UNFAIR-ToS datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Selective augmentation improves macro-F1 by restricting noise injection for head classes while providing contextual support for tail classes.
- **Mechanism:** The system calculates class frequencies to identify $C_{low}$ (tail classes). Only samples belonging to $C_{low}$ are concatenated with retrieved examples. This prevents the model from overfitting to retrieved context in high-frequency classes where the original signal is sufficient.
- **Core assumption:** Indiscriminate retrieval introduces confounding signals or "noise" for well-represented classes that the model can already classify accurately.
- **Evidence anchors:**
  - [Abstract] "SRA focuses on augmenting samples belonging to low-frequency labels... preventing the introduction of noise for well-represented classes."
  - [Section 4.4] "Full Retrieval-Augmentation degrades performance (macro-F1 0.816)... SRA delivers the best results."
- **Break condition:** If the cutoff ratio $\alpha$ is set too high (e.g., 100%), performance degrades to baseline or worse due to noise injection into head classes.

### Mechanism 2
- **Claim:** In-domain retrieval acts as a semantic anchor, compensating for the lack of exposure to rare legal concepts during training.
- **Mechanism:** A two-stage retrieval pipeline (TF-IDF $\rightarrow$ SBERT re-ranking) finds semantically similar "clauses" from the training set. The model learns to use this retrieved text as additional evidence, which is particularly effective for rare classes where training examples are sparse.
- **Core assumption:** Legal texts are "self-contained," meaning semantically similar clauses provide valid evidential support without requiring external knowledge bases.
- **Evidence anchors:**
  - [Section 3.1] "...retrieved examples... are concatenated with the original text... so that the model interprets them as additional evidence."
  - [Section 5 (Analysis)] "SRA effectively corrects long-tail imbalance... macro-F1 improves... for the rare bucket."
- **Break condition:** If the retrieval system returns low-similarity or misleading clauses (low cosine similarity), the model may hallucinate features (see Appendix D, Case 1: "Positions" vs "Duties" confusion).

### Mechanism 3
- **Claim:** Consistent augmentation across train, validation, and test splits creates an "exposure invariant" feature space.
- **Mechanism:** By applying the same retrieval logic to validation and test sets (using the training index), the model sees augmented inputs during inference that match the distribution of the training data, preventing distribution shift.
- **Core assumption:** The paper assumes access to ground-truth labels at inference time to determine if a sample belongs to $C_{low}$, which simplifies the "selection" problem during experimentation.
- **Evidence anchors:**
  - [Section 3.2] "Augmentation is applied consistently during training, validation, and testing... to isolate the effect of selectively augmenting low-frequency classes."
  - [Limitations] "The selective retrieval-augmentation strategy uses ground-truth labels during validation and testing... unrealistic for deployment."
- **Break condition:** If labels are unavailable at inference (real-world deployment), the selection mechanism fails unless a proxy (e.g., uncertainty score) is used.

## Foundational Learning

- **Concept:** **Long-Tail Distribution**
  - **Why needed here:** The entire method depends on identifying and isolating the "tail" of the distribution (low-frequency classes). Without understanding how class imbalance skews model bias toward majority classes, the rationale for SRA is unclear.
  - **Quick check question:** If a dataset has 100 classes but 80% of samples belong to just 5 classes, how would a standard cross-entropy loss function bias the model's weights?

- **Concept:** **Two-Stage Retrieval (Sparse + Dense)**
  - **Why needed here:** The paper uses TF-IDF for candidate selection and SBERT for semantic re-ranking. Understanding this trade-off is critical: TF-IDF is fast but lexical; SBERT is slower but captures semantic similarity in legal context.
  - **Quick check question:** Why retrieve top-20 with TF-IDF before re-ranking with SBERT, rather than just using SBERT on the whole corpus?

- **Concept:** **Concatenative Augmentation**
  - **Why needed here:** The method doesn't use a separate "retrieval encoder" but simply concatenates text with a `[SEP]` token. You must understand how Transformer attention mechanisms allow the model to "read" the retrieved text as context for the original text.
  - **Quick check question:** How does the `[SEP]` token help the RoBERTa model distinguish between the original clause and the retrieved reference clause?

## Architecture Onboarding

- **Component map:**
  1. **Training Set Index:** Stores text and labels for retrieval.
  2. **Frequency Analyzer:** Calculates $f(c)$ and determines $C_{low}$ based on cutoff $\alpha$.
  3. **Retriever:** TF-IDF Vectorizer + SBERT Encoder (`all-mpnet-base-v2`).
  4. **Augmenter:** Concatenates `Original` + `[SEP]` + `Retrieved` (truncated to 64 tokens).
  5. **Classifier:** RoBERTa-base with a linear head.

- **Critical path:**
  1. Analyze training data to define the set of low-frequency labels $C_{low}$.
  2. Index training corpus for TF-IDF and pre-compute SBERT embeddings.
  3. For every input sample $x$, check if label $y \in C_{low}$.
  4. If yes: Retrieve top-1 reference clause and concatenate.
  5. If no: Pass $x$ unchanged.

- **Design tradeoffs:**
  - **Proof-of-Concept vs. Deployment:** The code relies on ground-truth labels at inference to select augmentation. You must decide if you will implement the suggested "uncertainty-based" proxy for production.
  - **Retrieval Source:** The paper strictly retrieves from the **training set** to prevent leakage. Do not index validation/test sets.
  - **Truncation:** Retrieved clauses are capped at 64 tokens; the total sequence is capped at 512 tokens. Monitor for information loss in longer legal documents.

- **Failure signatures:**
  - **Semantic Drift:** Retrieved clause is lexically similar but legally distinct (e.g., "Executive Duties" vs "Executive Position"), causing misclassification (Appendix D).
  - **Noise Injection:** Macro-F1 drops below baseline $\rightarrow$ likely cutoff $\alpha$ is too aggressive (augmenting head classes).

- **First 3 experiments:**
  1. **Baseline Verification:** Run RoBERTa-base on LEDGAR/UNFAIR-ToS without augmentation to reproduce LexGLUE baseline scores (Table 1).
  2. **Cutoff Ablation:** Sweep $\alpha$ (e.g., 0%, 20%, 65%, 90%, 100%) on the validation set to find the "sweet spot" where macro-F1 peaks before noise injection degrades performance (Figure 3).
  3. **Error Analysis:** Inspect "Low Frequency" bucket predictions. Verify that specific rare labels (e.g., LEDGAR IDs 4, 5, 60) show the claimed macro-F1 improvements (>+0.20).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can uncertainty-based criteria or auxiliary models effectively replace ground-truth labels for identifying tail-class instances during inference?
- **Basis in paper:** [explicit] The Limitations section states that using ground-truth labels at inference is unrealistic and explicitly suggests "label-free alternatives" like uncertainty-based criteria for future work.
- **Why unresolved:** The current study isolates the effect of selective augmentation by assuming access to labels, leaving the detection of tail instances without labels unexplored.
- **What evidence would resolve it:** Experiments demonstrating that an auxiliary classifier or uncertainty score can approximate the low-frequency set ($C_{low}$) with comparable performance to the ground-truth baseline.

### Open Question 2
- **Question:** How can the retrieval mechanism be refined to prevent the injection of noise from semantically similar but legally distinct clauses?
- **Basis in paper:** [inferred] Appendix D (Case Study) provides a failure case where retrieval causes a misclassification (e.g., confusing "Duties" with "Positions") because the retrieved text emphasizes the wrong context.
- **Why unresolved:** The current TF-IDF and SBERT pipeline prioritizes general semantic similarity over the specific discriminative features required for fine-grained legal categorization.
- **What evidence would resolve it:** Integration of a contrastive learning objective or label-aware filtering step that reduces the error rate in boundary cases compared to the standard similarity search.

### Open Question 3
- **Question:** Is there a universal or adaptive method for determining the optimal augmentation cutoff ratio ($\alpha$) for different types of long-tail distributions?
- **Basis in paper:** [inferred] Section 4.5 shows the optimal cutoff varies significantly by dataset (65% for LEDGAR vs. non-empty labels for UNFAIR-ToS), requiring manual tuning on the validation set.
- **Why unresolved:** The paper treats the cutoff as a hyperparameter without proposing a theoretical framework for predicting the optimal selection point based on distributional properties.
- **What evidence would resolve it:** A heuristic function derived from class frequency skewness that automatically sets $\alpha$ and achieves performance parity with the manually tuned baselines.

## Limitations
- **Ground-truth label dependency**: The selective mechanism requires true labels at inference, making it impractical for real-world deployment without an uncertainty proxy (acknowledged by authors).
- **Retrieval quality variability**: Semantic drift occurs when retrieved clauses are lexically similar but legally distinct, leading to misclassification in rare classes (Appendix D, Case 1).
- **Cutoff sensitivity**: The α threshold is dataset-specific and requires validation tuning; incorrect values can degrade performance below baseline.

## Confidence

- **High confidence**: Macro-F1 improvements over baselines (6.0% gain) and superiority to LexGLUE models are well-supported by Table 1 and ablation studies.
- **Medium confidence**: Mechanism 1 (noise reduction for head classes) is logically sound but relies on the assumption that head classes don't benefit from augmentation—this needs empirical validation.
- **Medium confidence**: Mechanism 2 (semantic anchoring) assumes legal self-containment, but retrieval failures suggest this isn't universally true.
- **Low confidence**: Mechanism 3 (exposure invariance) is undermined by the ground-truth label dependency limitation.

## Next Checks

1. **Cutoff threshold sensitivity**: Run ablation experiments sweeping α from 0% to 100% on validation sets to identify the optimal cutoff where macro-F1 peaks before noise injection degrades performance.
2. **Error analysis of rare class predictions**: Manually inspect predictions for the lowest-frequency classes (LEDGAR IDs 4, 5, 60) to verify if SRA specifically improves these rare labels as claimed.
3. **Retrieval quality assessment**: Calculate cosine similarity between original and retrieved clauses; measure the proportion of semantically dissimilar retrievals that lead to misclassification to quantify semantic drift risk.