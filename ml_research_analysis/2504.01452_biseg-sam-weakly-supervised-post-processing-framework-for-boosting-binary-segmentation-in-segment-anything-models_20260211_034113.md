---
ver: rpa2
title: 'BiSeg-SAM: Weakly-Supervised Post-Processing Framework for Boosting Binary
  Segmentation in Segment Anything Models'
arxiv_id: '2504.01452'
source_url: https://arxiv.org/abs/2504.01452
tags:
- segmentation
- module
- bounding
- loss
- biseg-sam
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of accurate segmentation of polyps
  and skin lesions for early diagnosis of colorectal and skin cancers. The authors
  propose BiSeg-SAM, a weakly supervised prompting and boundary refinement network
  that leverages the Segment Anything Model (SAM) to improve binary segmentation tasks
  in medical imaging.
---

# BiSeg-SAM: Weakly-Supervised Post-Processing Framework for Boosting Binary Segmentation in Segment Anything Models

## Quick Facts
- arXiv ID: 2504.01452
- Source URL: https://arxiv.org/abs/2504.01452
- Reference count: 30
- Key outcome: Weakly-supervised prompting and boundary refinement network leveraging SAM to achieve DSC of 0.919 and mIoU of 0.882 on Kvasir polyp dataset, outperforming state-of-the-art methods.

## Executive Summary
BiSeg-SAM introduces a weakly-supervised post-processing framework that enhances binary segmentation for polyps and skin lesions by integrating the Segment Anything Model (SAM) with domain-specific adaptations. The framework combines SAM's global features with CNN-extracted local features, automates bounding box prompts via a WeakBox mechanism, and refines boundaries using limited ground truth labels. Evaluated across five polyp datasets and one skin cancer dataset, BiSeg-SAM demonstrates superior segmentation performance compared to existing methods.

## Method Summary
BiSeg-SAM employs three core modules: (1) an Adaptively Global-Local Module that fuses frozen SAM ViT-B encoder outputs with trainable ResNet-18 CNN features using a learnable weight coefficient, (2) a WeakBox Module that automatically generates bounding box prompts from coarse masks using Multi-choice Mask-to-Box (MM2B) transformation, and (3) a DetailRefine Module that corrects boundary errors using residual learning with limited ground truth labels. The framework is trained end-to-end with scale consistency loss and detailed refinement, using AdamW optimizer with learning rate 1e-4, batch size 16, and 200 epochs. Evaluation metrics include Dice Similarity Coefficient (DSC) and mean Intersection over Union (mIoU).

## Key Results
- Achieves DSC of 0.919 and mIoU of 0.882 on Kvasir polyp dataset
- Outperforms state-of-the-art methods including Seg-SAM, PolySAM, and MMFormer
- Demonstrates effective handling of complex medical segmentation tasks with limited supervision

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining SAM's global features with CNN-extracted local features improves medical image segmentation where fine boundary details matter.
- Mechanism: A learnable weight coefficient α balances frozen SAM encoder outputs with ResNet-18-based CNN features via weighted sum (x = α·xsam + (1-α)·xcnn). The CNN captures localized texture/edge information that SAM's global attention may dilute.
- Core assumption: Medical polyps/lesions require finer-grained spatial features than SAM's natural-image pretraining provides.
- Evidence anchors:
  - [abstract] "We fine-tune SAM combined with a CNN module to learn local features."
  - [section II.A] Describes the feature concatenation and α-weighted fusion explicitly.
  - [corpus] TopoLoRA-SAM and BALR-SAM similarly adapt SAM for domain-specific thin structures, suggesting local-feature augmentation is a recognized strategy.
- Break condition: If target lesions are large, low-contrast, and lack fine edges, CNN overhead may not justify marginal gains.

### Mechanism 2
- Claim: Weak bounding-box supervision via MM2B transformation reduces annotation cost while maintaining prompt quality for SAM.
- Mechanism: Coarse predicted masks are projected to horizontal/vertical vectors via max pooling. Center-point status (foreground vs. background) triggers adaptive bounding-box generation: direct projection for single-foreground, max/min box subtraction for multi-foreground cases.
- Core assumption: Coarse mask-to-box conversion can substitute for manual box prompts without catastrophic prompt drift.
- Evidence anchors:
  - [abstract] "using our proposed Multi-choice Mask-to-Box (MM2B) transformation for rough mask-to-box conversion, addressing the mismatch between coarse labels and precise predictions."
  - [section II.B] Equations 2–7 detail projection vectors and box generation logic.
  - [corpus] Context-Aware Weakly Supervised Image Manipulation Localization also uses SAM with weak supervision, suggesting cross-domain viability.
- Break condition: If multi-foreground scenes are frequent and objects are spatially dispersed, the unified bounding box may over-include background, degrading prompt specificity.

### Mechanism 3
- Claim: A lightweight refinement module using limited ground truth can correct boundary errors from coarse predictions.
- Mechanism: DetailRefine learns residual corrections (Srefined = Scoarse + Sresidual) using a U-Net-like encoder-decoder with residual blocks. Trained with limited pixel-level annotations; frozen during main training to preserve weak-supervision integrity.
- Core assumption: Boundary errors are locally systematic and can be modeled as additive residuals.
- Evidence anchors:
  - [abstract] "DetailRefine module enhances boundary precision and segmentation accuracy by refining coarse predictions using a limited amount of ground truth labels."
  - [section II.C] Equation 11 defines residual formulation; training strategy described.
  - [corpus] Mamba Guided Boundary Prior paper emphasizes boundary priors for polyp segmentation, supporting boundary-focused refinement as a general principle.
- Break condition: If ground truth for refinement is from a different domain/distribution than inference data, learned residuals may misapply.

## Foundational Learning

- Concept: **Weakly Supervised Segmentation**
  - Why needed here: BiSeg-SAM relies on bounding-box annotations rather than pixel-level masks for primary supervision.
  - Quick check question: Can you explain why scale consistency loss helps prevent prediction collapse under sparse supervision?

- Concept: **Prompt-Based Segmentation (SAM Architecture)**
  - Why needed here: SAM uses box/point prompts to guide mask prediction; understanding prompt encoding is essential for debugging MM2B integration.
  - Quick check question: What happens to SAM's output if a bounding box prompt is too loose or off-center?

- Concept: **Residual Learning for Refinement**
  - Why needed here: DetailRefine learns additive corrections rather than full masks.
  - Quick check question: Why might residual learning converge faster than direct mask prediction for boundary refinement?

## Architecture Onboarding

- Component map:
  - Adaptively Global-Local Module (frozen SAM ViT-B encoder + trainable ResNet-18 CNN block → weighted fusion → mask decoder) -> WeakBox Module (Multi-scale predictions → MM2B transformation → bounding-box prompts → SAM prompt encoder) -> DetailRefine Module (Coarse mask + image → residual U-Net → refined mask)

- Critical path: Input image → SAM encoder (frozen) || CNN encoder → feature fusion → mask decoder → coarse mask → MM2B box prompt → SAM refinement → DetailRefine → final mask

- Design tradeoffs:
  - Freezing SAM encoder reduces memory/compute but limits domain adaptation depth.
  - Using limited GT for DetailRefine improves boundaries but introduces partial supervision dependency.
  - MM2B unified box for multi-foreground simplifies prompting but may include background noise.

- Failure signatures:
  - Multi-foreground scenes producing overly large bounding boxes (check center-point logic in MM2B).
  - Boundary blur persisting after DetailRefine (check residual learning convergence, GT domain match).
  - Inconsistent predictions across scales (check SC loss implementation).

- First 3 experiments:
  1. **Baseline SAM vs. SAM+CNN**: Run inference on Kvasir subset with and without CNN fusion; measure DSC difference on small polyps.
  2. **MM2B prompt visualization**: For images with multiple polyps, visualize generated boxes vs. GT boxes; quantify box IoU.
  3. **DetailRefine ablation**: Train with DetailRefine disabled, then enabled with varying GT percentages (5%, 10%, 20%); plot boundary HD95 vs. GT amount.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent can BiSeg-SAM generalize to non-endoscopic modalities like MRI and ultrasound?
- Basis in paper: [explicit] The conclusion states, "Future work will extend this framework to other medical imaging modalities, such as MRI and ultrasound, to assess its versatility and robustness."
- Why unresolved: The current experimental validation is restricted to 2D endoscopic (polyp) and dermoscopic (skin) datasets; the model's adaptability to the distinct noise profiles and volumetric nature of MRI/ultrasound is untested.
- What evidence would resolve it: Benchmarking BiSeg-SAM performance (DSC/mIoU) on standard MRI (e.g., brain tumor) or ultrasound datasets compared to current SOTA methods.

### Open Question 2
- Question: How can BiSeg-SAM be effectively integrated with multi-modal fusion or generative models?
- Basis in paper: [explicit] The authors write, "we plan to explore the integration of BiSeg-SAM with... multi-modal fusion and generative models... to further improve its applicability."
- Why unresolved: The current architecture is designed for single-stream image input; the specific architectural modifications required to ingest multi-modal data or leverage generative adversarial networks for training are undefined.
- What evidence would resolve it: Demonstrating an architecture variant that successfully fuses complementary imaging data (e.g., CT + MRI) or utilizes synthetic data augmentation to improve segmentation metrics.

### Open Question 3
- Question: What is the sensitivity of the DetailRefine module to the quantity of available pixel-level ground truth annotations?
- Basis in paper: [inferred] The Methodology states the DetailRefine module "uses a small amount of GT masks," yet the exact lower bound of this "limited" data requirement is not quantified.
- Why unresolved: The trade-off between annotation cost and refinement accuracy is unclear; it is unknown if performance degrades drastically if the refinement module is trained on only a handful of masks.
- What evidence would resolve it: An ablation study plotting segmentation accuracy (DSC) against the percentage of ground truth masks made available for training the DetailRefine module.

## Limitations

- Hyperparameter gaps: Loss weights (β, γ, λ1, λ2) and fusion coefficient initialization (α) are unspecified, potentially affecting convergence and module balance.
- Ground truth allocation: The exact number/percentage of GT masks used for DetailRefine training is unclear, limiting reproducibility of the weak-supervision claim.
- Multi-foreground handling: MM2B's unified bounding box approach may degrade for dispersed or closely spaced lesions, but quantitative impact is not reported.

## Confidence

- High: SAM+CNN fusion improves segmentation vs. SAM alone (consistent with other domain adaptation work).
- Medium: MM2B transformation sufficiently reduces annotation cost while maintaining quality (limited ablation and no alternative prompt strategies compared).
- Low: DetailRefine consistently enhances boundary precision across all datasets (results shown for single dataset only; residual learning dependency on GT domain match untested).

## Next Checks

1. **MM2B Generalization**: Evaluate MM2B-generated boxes on CVC-ColonDB (high multi-foreground density) vs. GT boxes; measure IoU and downstream segmentation impact.
2. **Hyperparameter Sensitivity**: Perform grid search over β, γ, λ1, λ2, and α init; report stability of DSC/mIoU across settings.
3. **DetailRefine Domain Transfer**: Train DetailRefine on Kvasir GT, test on CVC-ClinicDB; measure HD95 degradation to quantify domain sensitivity.