---
ver: rpa2
title: Rethinking Contrastive Learning in Session-based Recommendation
arxiv_id: '2506.05044'
source_url: https://arxiv.org/abs/2506.05044
tags:
- item
- contrastive
- learning
- macl
- augmentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles data sparsity issues in session-based recommendation
  by proposing a novel multi-modal adaptive contrastive learning framework called
  MACL. The method addresses three key limitations of existing contrastive learning
  approaches: inability to handle item-level sparsity, failure to ensure semantic
  consistency in augmented views, and treating all contrastive signals equally.'
---

# Rethinking Contrastive Learning in Session-based Recommendation

## Quick Facts
- arXiv ID: 2506.05044
- Source URL: https://arxiv.org/abs/2506.05044
- Authors: Xiaokun Zhang; Bo Xu; Fenglong Ma; Zhizheng Wang; Liang Yang; Hongfei Lin
- Reference count: 40
- Key result: Proposes MACL, achieving 4.14% to 11.90% improvement in Precision@20 and 2.25% to 27.34% improvement in MRR@20 over state-of-the-art methods

## Executive Summary
This paper addresses data sparsity in session-based recommendation by proposing MACL, a multi-modal adaptive contrastive learning framework. The method tackles three key limitations of existing contrastive learning approaches: inability to handle item-level sparsity, failure to ensure semantic consistency in augmented views, and treating all contrastive signals equally. MACL employs a multi-modal augmentation strategy that leverages item images and text to generate semantically consistent views at both item and session levels, combined with an adaptive contrastive loss that distinguishes the varying utility of positive-negative signals during self-supervised learning.

## Method Summary
MACL combines multi-modal feature fusion with adaptive contrastive learning to address data sparsity in session-based recommendation. The framework extracts item embeddings from ID, image (via GoogLeNet), and text (via BERT), then fuses them using a gating mechanism. It generates augmented views using multi-modal techniques like horizontal flip and word swap, ensuring semantic consistency. An adaptive contrastive loss reweights positive-negative signals based on their estimated utility using a lightweight MLP, rather than treating all signals equally. The model is jointly optimized with recommendation loss (Cross-Entropy) and contrastive loss.

## Key Results
- MACL achieves 4.14% to 11.90% improvement in Precision@20 compared to state-of-the-art baselines
- MACL achieves 2.25% to 27.34% improvement in MRR@20 across three real-world datasets
- MACL outperforms standard augmentation methods by 2.85% to 4.68% in Precision@20 and 1.29% to 3.35% in MRR@20

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Leveraging multi-modal features for data augmentation generates semantically consistent views, enabling effective contrastive learning for sparse item and session data.
- Mechanism: Unlike prior methods that perturb item ID sequences (e.g., crop, mask), this framework augments item images and text using techniques from computer vision and NLP (e.g., horizontal flip, Gaussian blur, word swap). Applying the same technique to all items in a session creates an augmented session view that preserves the original semantic intent.
- Core assumption: The selected augmentation techniques (e.g., Hflip, word substitution) preserve the core semantic meaning of an item despite the transformation.
- Evidence anchors:
  - [abstract] "...a multi-modal augmentation is devised to generate semantically consistent views at both item and session levels by leveraging item multi-modal features."
  - [Section 4.2.1] "In addition, we carefully select augmented techniques that can preserve an item's original semantics to build the augmented pool."
  - [corpus] General support for multi-modal utility found in "MMHCL: Multi-Modal Hypergraph Contrastive Learning for Recommendation", though it does not detail the specific augmentation techniques.
- Break condition: If augmentation techniques corrupt item semantics (e.g., cropping an image removes the main object), the positive pairs will no longer be semantically aligned, degrading the contrastive learning signal.

### Mechanism 2
- Claim: An adaptive contrastive loss improves self-supervised learning by reweighting positive-negative signals based on their estimated utility.
- Mechanism: Instead of treating all contrastive pairs equally, this method uses a lightweight neural network (MLP) to assign a weight ($\alpha$) to each signal set. The network evaluates the contribution of the signal based on the anchor, positive, and negative embeddings, emphasizing informative pairs and downplaying uninformative ones.
- Core assumption: The quality and utility of contrastive signals vary, and a neural network can learn to distinguish this utility directly from the embeddings.
- Evidence anchors:
  - [abstract] "...an adaptive contrastive loss is introduced to distinguish the varying utility of positive-negative signals during self-supervised learning."
  - [Section 4.3] "...a weight $\alpha_i$ is assigned to each signal set to highlight informative signals while downplaying uninformative ones."
  - [corpus] Weak or missing direct evidence in the provided corpus for an adaptive loss reweighting mechanism.
- Break condition: If the MLP fails to learn meaningful weights (e.g., it outputs uniform values), the adaptive loss provides no benefit over a standard contrastive loss.

### Mechanism 3
- Claim: A fusion network combining item ID, image, and text embeddings creates comprehensive item representations that better capture user preferences.
- Mechanism: The model initializes three types of embeddings (ID via lookup, image via GoogLeNet, text via BERT). A gating mechanism within a fusion network dynamically merges these vectors ($e_i = e^{id}_i + g_1 \odot e^{img}_i + g_2 \odot e^{txt}_i$) to form the final item representation used for session encoding.
- Core assumption: User interaction decisions are influenced by a blend of co-occurrence patterns (from ID) and semantic features like style and color (from image/text).
- Evidence anchors:
  - [Section 4.1.1] "A fusion network is developed to merge such information for comprehensive item representations."
  - [Section 3.2] "Obviously, we should initialize such various information to serve as inputs for neural networks."
  - [corpus] Strong support in "Hierarchical Time-Aware Mixture of Experts for Multi-Modal Sequential Recommendation" for using multi-modal data to enhance feature learning.
- Break condition: If multi-modal inputs are noisy or contradictory (e.g., misleading text descriptions), the fusion could degrade the representation compared to using the ID alone.

## Foundational Learning

### Concept: Contrastive Learning
- Why needed here: The entire framework relies on contrastive learning to mitigate data sparsity. You must understand how the loss function pulls positive (augmented) views closer and pushes negative views apart in the embedding space.
- Quick check question: In this paper, what constitutes a "positive pair" and a "negative pair" for the item-level contrastive task?

### Concept: Self-Attention (SASRec)
- Why needed here: SASRec is the chosen sequence encoder for modeling user behavior within a session.
- Quick check question: How does the self-attention mechanism in SASRec capture dependencies between items differently than a recurrent network?

### Concept: Multi-modal Representation
- Why needed here: The core innovation depends on processing and fusing image and text data. You need to understand how features from different modalities are extracted and combined.
- Quick check question: Why is a gating mechanism useful when fusing an ID embedding with an image embedding?

## Architecture Onboarding

### Component map
- Input Layer (Item ID, Image, Text) -> Fusion Network (gating mechanism) -> SASRec Encoder -> Sequence Embeddings -> Adaptive Contrastive Loss Module (MLP) -> Weighted Contrastive Loss

### Critical path
1. Extract and fuse ID, image, and text embeddings for all items in a session
2. Generate an anchor session embedding using the Sequence Encoder
3. Apply a random augmentation from the pool to create positive and negative views for items and sessions
4. Process these views through the encoder to get contrastive embeddings
5. Compute the adaptive contrastive loss using learned weights from the loss module
6. Jointly optimize with the recommendation loss (Cross-Entropy)

### Design tradeoffs
The adaptive loss uses a simple MLP to keep computational overhead low. The tradeoff is that this simple network may not capture complex utility functions, but it ensures the method remains efficient.

### Failure signatures
- Model fails to generalize on long-tail items, indicating augmentation may be corrupting semantics
- Adaptive loss variant (MACL-adp) performs identically to the standard version, suggesting the reweighting MLP is not learning
- Performance degrades on very short sessions, implying the fusion network or augmentation is adding noise rather than signal

### First 3 experiments
1. **Ablation on Augmentation:** Compare full MACL against a variant using only standard ID-based augmentations (MACL-com) to isolate the value of multi-modal augmentation
2. **Loss Function Analysis:** Compare MACL against a variant without the adaptive weighting (MACL-adp) to confirm the benefit of reweighting contrastive signals
3. **Item Sparsity Test:** Evaluate the model on a dataset with a high proportion of unpopular/long-tail items to test its robustness against item-level sparsity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the incorporation of fine-grained item features (categories, brands, reviews) impact the performance of multi-modal augmentation compared to using only images and text?
- Basis in paper: [explicit] The authors explicitly state in Section 8.2 that MACL currently overlooks available information like categories and brands, and list their incorporation as a primary direction for future work.
- Why unresolved: The current fusion network (Eq. 5-8) and augmentation pool are designed exclusively for ID, image, and text modalities.
- What evidence would resolve it: An ablation study extending the fusion network to include brand/category embeddings and evaluating their specific contribution to handling item-level sparsity.

### Open Question 2
- Question: Can replacing random negative sampling with hard negative mining strategies further enhance the adaptive contrastive loss in MACL?
- Basis in paper: [explicit] Section 8.2 identifies the reliance on random negative sampling as a limitation, suggesting that "sampling more informative negatives" is a promising future direction.
- Why unresolved: The current method relies on a conventional random selection of $M$ negatives (Section 4.2.1), which may include uninformative "easy" negatives that contribute little to gradient updates.
- What evidence would resolve it: A comparative analysis of MACL's performance when using hard negative sampling strategies versus the current random sampling baseline.

### Open Question 3
- Question: How robust is the multi-modal augmentation strategy when item images or text descriptions are missing or of low quality for long-tail items?
- Basis in paper: [inferred] The paper assumes the availability of multi-modal features to solve item-level sparsity (Section 4.2), but real-world long-tail items often suffer from missing or noisy metadata.
- Why unresolved: The methodology relies on the existence of "rich semantic information" in images/text to generate augmented views; the framework's behavior when such data is absent is not discussed.
- What evidence would resolve it: Experiments evaluating performance degradation on datasets where image/text features are selectively masked or removed for a subset of items.

## Limitations

- The adaptive contrastive loss mechanism lacks sufficient detail for full implementation, with missing architectural specifications for the MLP component
- The specific implementation of text augmentation techniques (particularly word substitution/insertion based on BERT similarity) is underspecified
- Some baseline comparisons appear to use stronger variants than their original implementations, potentially inflating performance gains

## Confidence

- **High confidence** in the multi-modal augmentation mechanism's effectiveness, supported by strong corpus evidence and clear technical description
- **Medium confidence** in the adaptive loss contribution, as the paper provides less detail on the MLP architecture and training process
- **Medium confidence** in the overall improvement claims, though some baseline comparisons appear to use stronger variants than their original implementations

## Next Checks

1. **MLP Architecture Audit:** Implement and test the adaptive loss MLP with varying hidden dimensions and activation functions to determine optimal configuration and verify sensitivity
2. **Augmentation Semantics Test:** Conduct controlled experiments where augmentation intensity is systematically varied to measure the relationship between semantic preservation and model performance
3. **Long-tail Performance Analysis:** Evaluate MACL specifically on the rarest 10% of items to quantify its effectiveness at addressing item-level sparsity compared to standard augmentation methods