---
ver: rpa2
title: Optimization of Activity Batching Policies in Business Processes
arxiv_id: '2507.15457'
source_url: https://arxiv.org/abs/2507.15457
tags:
- batch
- batching
- time
- activity
- cost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an approach to optimize activity batching policies
  in business processes, addressing the trade-off between cost, processing effort,
  and waiting time. The method uses 19 heuristic interventions to identify and adjust
  batching policies based on scenarios affecting waiting time, processing time, cost
  efficiency, and resource utilization.
---

# Optimization of Activity Batching Policies in Business Processes

## Quick Facts
- arXiv ID: 2507.15457
- Source URL: https://arxiv.org/abs/2507.15457
- Reference count: 18
- This paper presents an approach to optimize activity batching policies in business processes, addressing the trade-off between cost, processing effort, and waiting time.

## Executive Summary
This paper addresses the challenge of optimizing activity batching policies in business processes, where managers must balance the trade-offs between waiting time, processing effort, and cost. The authors propose a method that uses 19 heuristic interventions to identify and adjust batching policies based on scenarios affecting waiting time, processing time, cost efficiency, and resource utilization. These heuristics are embedded in three meta-heuristics (hill-climbing, simulated annealing, and reinforcement learning) to explore the search space of batching policies. The approach was evaluated using 10 real-life business processes, demonstrating significant improvements in convergence, diversity, and cycle time gain compared to baseline meta-heuristics.

## Method Summary
The method uses 19 intervention heuristics to detect specific inefficiency scenarios in business processes and trigger targeted policy adjustments. These heuristics are embedded within three meta-heuristics: Hill-Climbing (HC), Simulated Annealing (SA), and Reinforcement Learning (RL). The optimization problem is formulated as a multi-objective Pareto optimization, minimizing both cycle time and cost. The approach uses simulation models discovered from event logs via Simod, and policies are evaluated using Prosimos. The heuristics guide the search by reducing the effective search space from infinite continuous values to structured scenario-action pairs, enabling faster convergence to optimal batching policies.

## Key Results
- Heuristic-guided approaches outperformed baseline meta-heuristics in convergence and diversity.
- Up to 80% improvement in convergence was achieved compared to non-guided methods.
- Cycle time was reduced by up to 721 hours in some cases.
- The method effectively balances the trade-off between waiting time and cost through Pareto-optimal solutions.

## Why This Works (Mechanism)

### Mechanism 1: Domain-Constrained Search Space Reduction
The approach improves convergence speed by restricting the search for batching policies to high-value adjustments identified by 19 domain-specific heuristics rather than random perturbation. Instead of modifying batch parameters arbitrarily, the system detects specific inefficiency scenarios (e.g., "first instance waits too long") and triggers a targeted intervention (e.g., "add time-to-live condition"). This reduces the effective search space size from infinite continuous values to a structured set of scenario-action pairs. The method assumes that the 19 identified scenarios capture the majority of significant optimization opportunities in business processes.

### Mechanism 2: Simulation-Driven Pareto Dominance
The method resolves the trade-off between waiting time and cost by iteratively refining a Pareto front using stochastic simulation to evaluate non-dominated solutions. The system maintains a set of optimal solutions (Pareto front). When a heuristic proposes a new policy, a discrete-event simulation estimates its cost and cycle time. If the new policy dominates an existing one (better in one metric without worsening the other), it updates the front. This allows managers to select a policy based on risk tolerance for waiting vs. cost. The approach assumes the simulation model accurately reflects real-world process dynamics so that simulated dominance translates to actual operational improvement.

### Mechanism 3: Adaptive Meta-Heuristic Strategy
Embedding heuristics within meta-heuristics (Hill-Climbing, Simulated Annealing, RL) allows the system to balance local exploitation of known good interventions with global exploration of the policy space. Hill-Climbing exploits local improvements by accepting neighbors within a small radius; fast but prone to local optima. Simulated Annealing uses a temperature parameter to stochastically accept worse solutions initially, escaping local optima before cooling into a focused search. Reinforcement Learning learns a policy mapping process states to specific heuristic interventions based on long-term reward (Pareto improvement), theoretically adapting better to complex interdependencies. The approach assumes that process optimization benefits from a search strategy that can handle the non-convex landscape of batching policies where local improvements don't always lead to global optima.

## Foundational Learning

- **Concept: Pareto Efficiency (Multi-Objective Optimization)**
  - **Why needed here:** Batching inherently creates a trade-off (larger batches lower cost but increase waiting time). You cannot minimize both simultaneously; you must find the "front" of best-possible trade-offs.
  - **Quick check question:** If Policy A reduces cost by 10% but increases waiting time by 15%, and Policy B reduces waiting time by 20% but increases cost by 5%, are both valid candidates for the Pareto front?

- **Concept: Batch Activation Rules (Size vs. Time)**
  - **Why needed here:** The paper optimizes *when* a batch starts. Understanding that a batch can trigger based on a threshold (size), a timeout (time-to-live), or a schedule (daily hour) is essential for interpreting the 19 heuristics.
  - **Quick check question:** What is the likely effect on waiting time if you switch from a "size-based" activation (batch starts at 10 items) to a "time-based" activation (batch starts every 4 hours) during a period of low arrival rates?

- **Concept: Meta-heuristics (Exploration vs. Exploitation)**
  - **Why needed here:** The results differentiate between Hill-Climbing (exploitation-heavy) and Simulated Annealing (exploration-heavy). Understanding this distinction explains why certain algorithms found better global optima in complex processes like BP19.
  - **Quick check question:** Why might Hill-Climbing report a lower final cost than Simulated Annealing on a simple process, but fail on a complex one?

## Architecture Onboarding

- **Component map:** Input: Event Log (historical data) -> Simod (discovers Simulation Model) -> Optimizer (Selects Candidate Policy) -> Prosimos (Simulates Policy execution) -> Evaluator (Calculates Cost/Time metrics) -> Pareto Updater -> 19 Heuristics (Rules mapping simulation stats to policy interventions)

- **Critical path:** The evaluation loop inside `SimulateBProcess` (Algorithm 1, lines 2, 12-13). The validity of the entire architecture depends on the simulation speed and the accurate implementation of the batch activation logic within the simulator.

- **Design tradeoffs:**
  - **HC vs. RL:** Hill-Climbing is computationally cheap (avg 1.4h) but brittle; RL is expensive (avg 11.3h) but potentially more robust for complex "spaghetti" processes (like GOV/BP19).
  - **Heuristic vs. Random:** Heuristics require domain-specific coding and upfront analysis (identifying 19 scenarios); random search requires zero domain knowledge but explores inefficiently.

- **Failure signatures:**
  - **Sim-to-Real Gap:** Optimization converges on a Pareto front, but the "optimal" batch sizes cause deadlock when deployed due to resources not modeled in the log.
  - **Premature Convergence:** Hill-Climbing variants stall early with low "Gain" (Table 4) because the radius is too small to escape local noise in simulation results.
  - **RL Instability:** RL model shows high variance in "Purity" scores (Table 3), indicating the agent is overfitting to simulation stochasticity rather than learning structural improvements.

- **First 3 experiments:**
  1. **Sanity Check (HC+ vs. HC-):** Run the optimization on the "BP12" dataset. Verify that the "Heuristic Guided" (HC+) version achieves a lower Hausdorff distance (convergence) than the "Non-Guided" (HC-) version within a fixed iteration budget.
  2. **Intervention Ablation:** Isolate Scenario 1 ("First instance waits too long"). Run the optimization using *only* this heuristic intervention. Check if cycle time improves for processes identified as having high "waiting time" variance.
  3. **Scalability Stress Test:** Run the "GOV" process (high complexity, 365 arcs). Compare the wall-clock time of Simulated Annealing vs. Reinforcement Learning to quantify the training overhead cost noted in the results.

## Open Questions the Paper Calls Out

- **Can the optimization approach be extended to handle batch activation based on data-aware conditions, such as order urgency or type?**
  - Basis in paper: The conclusion explicitly lists extending the approach to "handle batch activation based on data-aware conditions" as a primary avenue for future work.
  - Why unresolved: Current heuristics rely on time, size, and resource utilization metrics, ignoring instance-specific data attributes that often drive real-world batching decisions.
  - What evidence would resolve it: An extension of the activation rule definitions and heuristics to include data attributes, validated by improved Pareto fronts in data-dependent scenarios.

- **Can the method simultaneously optimize batching policies alongside activity prioritization and multitasking policies?**
  - Basis in paper: The conclusion suggests "extending the proposed approach to support the optimization of activity prioritization policies and multitasking policies."
  - Why unresolved: The current framework isolates batching policies, leaving potential interdependencies with resource multitasking and task prioritization unexplored.
  - What evidence would resolve it: A unified optimization model that adjusts all three policy types and demonstrates superior convergence compared to optimizing batching in isolation.

- **Do the optimized policies remain effective when applied to real-world cost structures rather than simulated approximations?**
  - Basis in paper: The authors note that event logs lacked cost data, necessitating a redefined cost function based on processing times.
  - Why unresolved: The Pareto-optimal solutions minimize a proxy for cost (derived from processing time) rather than actual financial overhead, which may behave differently in practice.
  - What evidence would resolve it: An evaluation using event logs containing explicit fixed and variable cost data to verify the trade-offs identified by the heuristics.

## Limitations

- The approach relies on a predefined set of 19 heuristics that may not cover all relevant optimization scenarios.
- The optimization assumes simulation models accurately represent real process dynamics, which may not hold due to the sim-to-real gap.
- RL implementation details (network architecture, hyperparameters) are not fully specified, making exact reproduction challenging.
- The evaluation uses 10 real-life datasets but lacks extensive cross-validation across different process domains.

## Confidence

- **High confidence:** The core mechanism of using heuristic-guided meta-heuristics for search space reduction and the demonstrated superiority of heuristic-guided approaches over baseline methods.
- **Medium confidence:** The specific performance gains (80% improvement in convergence, 721-hour reduction) are well-documented but may vary with different process characteristics and implementation details.
- **Low confidence:** The generalization of the 19 heuristics across different business process domains and the scalability of the approach to extremely large or complex processes.

## Next Checks

1. **Heuristic coverage validation:** Test the 19 heuristics on a new, diverse set of business processes to verify they capture the majority of optimization opportunities.
2. **Simulation fidelity assessment:** Compare optimization results from the simulation model against actual process execution data to quantify the sim-to-real gap.
3. **Computational overhead analysis:** Measure the runtime and resource usage of each meta-heuristic (HC, SA, RL) across varying process complexities to determine practical deployment constraints.