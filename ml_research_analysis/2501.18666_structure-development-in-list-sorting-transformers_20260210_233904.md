---
ver: rpa2
title: Structure Development in List-Sorting Transformers
arxiv_id: '2501.18666'
source_url: https://arxiv.org/abs/2501.18666
tags:
- head
- training
- loss
- circuit
- heads
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates how a single-layer attention-only transformer
  develops internal structures while learning to sort lists of numbers. The authors
  analyze two key organizational modes: vocabulary-splitting (where heads divide the
  number range) and copy-suppression (where one head counteracts another''s copying
  behavior).'
---

# Structure Development in List-Sorting Transformers

## Quick Facts
- arXiv ID: 2501.18666
- Source URL: https://arxiv.org/abs/2501.18666
- Authors: Einar Urdshals; Jasmina Urdshals
- Reference count: 40
- Primary result: Single-layer attention-only transformer develops vocabulary-splitting or copy-suppression organization modes based on training data gap statistics, with vocabulary-splitting representing a simpler solution as measured by LLC.

## Executive Summary
This study investigates how a single-layer attention-only transformer develops internal structures while learning to sort lists of numbers. The authors analyze two key organizational modes: vocabulary-splitting (where heads divide the number range) and copy-suppression (where one head counteracts another's copying behavior). The developmental analysis reveals that vocabulary-splitting represents a simpler solution than overlapping head functionality, as measured by the Local Learning Coefficient (LLC). Crucially, this simplification occurs naturally during training even without weight decay, suggesting neural networks have an inherent bias toward simpler solutions. The authors identify the mean and variance of gaps between adjacent sorted numbers in training data as key drivers of head specialization.

## Method Summary
The authors train a single-layer attention-only transformer with 2 heads (d_model=96, d_head=48) on list-sorting tasks. Input format is [unsorted numbers, SEP, sorted output], with vocabulary size 52 tokens (0-50 + SEP + padding). The model is trained using cross-entropy loss with Adam optimizer (lr=10⁻³, batch 512, weight decay 0.005) for approximately 100K steps. Key analyses include computing OV and QK circuits to track copying behavior and attention patterns, and measuring LLC using SGLD sampling to quantify solution complexity during training.

## Key Results
- Vocabulary-splitting emerges on datasets with larger or higher-variance gaps between adjacent sorted numbers, while copy-suppression develops on datasets with smaller, low-variance gaps
- LLC reliably decreases during vocabulary-splitting transitions, indicating simpler solutions
- Copy-suppression serves to calibrate model confidence rather than correct errors, as ablation shows accuracy unchanged but loss increases
- Extremely simple datasets with weight decay can cause one head to switch off entirely, resulting in 1-head sorting

## Why This Works (Mechanism)

### Mechanism 1: Vocabulary-Splitting via QK Gradient Optimization
- Claim: Models partition the number vocabulary into non-overlapping regions across attention heads when training data has larger or higher-variance gaps between adjacent sorted numbers.
- Mechanism: The QK circuit develops steeper gradients within active regions when δ (gap between adjacent elements) is small, forcing smaller region sizes to maintain sufficient attention discrimination between neighboring tokens. The product δ·∇̂QK remains approximately constant across datasets, so smaller δ necessitates smaller regions and more of them.
- Core assumption: The relationship between δ and region size holds approximately linear when QK circuit rows decrease linearly within regions.
- Evidence anchors: [abstract] "With larger or higher-variance gaps, they develop vocabulary-splitting"; [Section 3.2] "a smaller δ necessitates larger gradients within active regions of the QK circuits, driving region size down and number of regions up"
- Break condition: If δ variance becomes too small, the model transitions to copy-suppression or 1-head sorting instead.

### Mechanism 2: Copy-Suppression as Confidence Calibration
- Claim: One attention head develops a negative diagonal in its OV circuit to calibrate (increase or decrease) the confidence of the dominant copying head, rather than correct errors.
- Mechanism: The copy-suppressing head attends to the same tokens as the copying head (similar QK circuits) but outputs negative contributions via its OV circuit. Ablation experiments show this increases loss without affecting accuracy—the head sharpens the logit distribution rather than changing predictions.
- Core assumption: The calibration function generalizes beyond this toy model; in GPT-2 (different architecture, sequential layers), copy-suppression reduces overconfidence, while here it increases confidence due to 100% task accuracy.
- Evidence anchors: [abstract] "Copy-suppression serves to calibrate model confidence rather than correct errors"; [Section 4.3] "ablating the copy-suppressing head has no impact on the accuracy of the model, but it does increase the loss slightly"
- Break condition: If the sub-leading head's weight norm drops below a threshold due to weight decay on simple datasets, the head switches off entirely (1-head sorting).

### Mechanism 3: Inherent Simplicity Bias via LLC Reduction
- Claim: Neural networks naturally prefer simpler solutions during training, as measured by Local Learning Coefficient (LLC) decreases, even without explicit regularization like weight decay.
- Mechanism: During the head-overlapping stage, both heads copy overlapping vocabularies with higher LLC. As training progresses, vocabulary-splitting emerges with lower LLC, indicating a broader loss basin. This occurs with or without weight decay, suggesting inherent inductive bias.
- Core assumption: LLC as computed via SGLD sampling reliably captures model complexity during non-equilibrium training dynamics.
- Evidence anchors: [abstract] "this simplification occurs naturally during training even without weight decay, suggesting neural networks have an inherent bias toward simpler solutions"; [Section 4.2] "vocabulary-splitting...is a simpler model, when compared to the preceding stage...its formation is always accompanied by a drop in the LLC"
- Break condition: If training data complexity (δ variance) is too low, the model skips vocabulary-splitting and goes directly to simpler copy-suppression or 1-head sorting states.

## Foundational Learning

- Concept: **QK and OV Circuit Decomposition**
  - Why needed here: The paper analyzes transformer behavior through these two circuits—QK determines what tokens attend to what, OV determines how attended tokens affect output. Understanding this decomposition is essential for interpreting the developmental stages.
  - Quick check question: Can you explain why a positive diagonal in the OV matrix indicates copying behavior?

- Concept: **Local Learning Coefficient (LLC)**
  - Why needed here: LLC measures solution complexity via loss landscape basin geometry. The paper uses LLC drops to identify when the model transitions to simpler organizational states.
  - Quick check question: Why does a lower LLC indicate a simpler, more degenerate solution?

- Concept: **Developmental Interpretability**
  - Why needed here: The paper tracks how structures emerge during training rather than just analyzing final states. This reveals transitions (e.g., head-overlapping → vocabulary-splitting) that would be invisible in endpoint-only analysis.
  - Quick check question: What would be missed if you only analyzed the final trained model?

## Architecture Onboarding

- Component map: Input tokens → embedding → attention heads (QK determines attention, OV determines output contribution) → unembedding → next token prediction
- Critical path: Input tokens → embedding → attention heads (QK determines attention, OV determines output contribution) → unembedding → next token prediction. The sorting algorithm emerges from QK attending to "next larger number" and OV copying that token.
- Design tradeoffs: Weight decay pushes toward simpler solutions but can cause premature head switching-off on simple datasets. LayerNorm removal slows learning. More heads enable finer vocabulary partitioning but some heads may become copy-suppressing or dormant.
- Failure signatures: (1) High out-of-distribution loss indicates over-specialization to training δ distribution. (2) No LLC drop during training suggests heads haven't specialized. (3) Copy-suppression appearing on diverse datasets may indicate training instability.
- First 3 experiments:
  1. Replicate baseline 2-head model on D_δ≈4.7, plot OV/QK circuits and LLC over training to verify three-stage development (initial learning → head-overlapping → vocabulary-splitting).
  2. Train on D^d_δ≈2.2 (smaller δ, lower variance) and confirm transition to copy-suppression; ablate each head to verify calibration function (accuracy unchanged, loss changes).
  3. Vary δ systematically (using fixed δ ranges) to map the boundary conditions between 1-head sorting, copy-suppression, and vocabulary-splitting; plot relative head weight norms against δ mean and variance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the developmental patterns observed in this single-layer toy model (vocabulary-splitting, copy-suppression stages) generalize to multi-layer transformers and large language models?
- Basis in paper: [explicit] The authors state "one should be careful to generalize our findings to larger transformers" and frame their work as testing the universality hypothesis, but only demonstrate analogous copy-suppression to GPT-2 without establishing broader generalization.
- Why unresolved: The paper studies only a constrained single-layer architecture on a narrow algorithmic task, leaving open whether these organizational principles scale.
- What evidence would resolve it: Systematic developmental analysis of attention heads in multi-layer transformers trained on sorting or similar algorithmic tasks, tracking LLC changes and head specialization patterns.

### Open Question 2
- Question: What is the functional significance of copy-suppression occurring in the same layer (this model) versus a later layer (GPT-2)?
- Basis in paper: [explicit] The authors note "In our case the copying and copy-suppressing head is in the same layer, whereas in GPT-2 the copy-suppressing head is in a later layer than the copying head. This difference might be important."
- Why unresolved: The architectural difference affects whether the copy-suppressing head can observe the copying head's output before acting, potentially changing the mechanism.
- What evidence would resolve it: Comparative experiments manipulating head layer placement in controlled architectures, measuring effects on calibration and self-repair capabilities.

### Open Question 3
- Question: Can the transition between specialization modes (vocabulary-splitting → copy-suppression → 1-head sorting) be predictably controlled through targeted interventions on QK gradients?
- Basis in paper: [explicit] The authors state "we have not done intervention studies increasing or decreasing the gradients to confirm that they shift the delta range the model sorts the best at."
- Why unresolved: While the correlation between δ distribution and specialization type is established, causation through gradient manipulation remains untested.
- What evidence would resolve it: Intervention experiments directly modifying QK gradients during training to force transitions between specialization modes, with successful prediction of resulting head organization.

## Limitations
- The study focuses on a highly simplified single-layer attention-only transformer, which may not generalize to deeper, autoregressive transformers
- The LLC method requires careful hyperparameter tuning and assumes quasi-equilibrium sampling that may not hold during non-stationary training
- The interpretation of copy-suppression as "confidence calibration" versus regularization remains uncertain, particularly given contrasting GPT-2 findings

## Confidence

- **High confidence**: Vocabulary-splitting emerges on datasets with larger/higher-variance gaps between adjacent sorted numbers; LLC reliably drops during vocabulary-splitting transitions; one head can switch off entirely on simple datasets with weight decay
- **Medium confidence**: Copy-suppression serves primarily to calibrate confidence rather than correct errors; gap variance (not just mean) drives organizational mode selection; the relationship between δ and region size follows the predicted inverse pattern
- **Low confidence**: Vocabulary-splitting represents an inherent bias toward simpler solutions independent of weight decay; the QK gradient optimization mechanism precisely explains region size scaling; findings generalize to multi-layer, autoregressive transformers

## Next Checks

1. **Architecture scaling test**: Train a 2-layer attention-only transformer on the same list-sorting task and verify whether vocabulary-splitting still emerges in the first layer, or whether depth changes the organizational dynamics.

2. **Task generalization test**: Apply the analysis framework to a transformer trained on a different structured task (e.g., sorting strings alphabetically or completing arithmetic sequences) to determine whether the same organizational modes appear.

3. **Parameter sensitivity test**: Systematically vary weight decay strength and LayerNorm presence across multiple runs, then track whether LLC drops still occur during vocabulary-splitting and whether copy-suppression consistently appears on low-variance datasets.