---
ver: rpa2
title: 'CRAFT: Calibrated Reasoning with Answer-Faithful Traces via Reinforcement
  Learning for Multi-Hop Question Answering'
arxiv_id: '2602.01348'
source_url: https://arxiv.org/abs/2602.01348
tags:
- craft
- reasoning
- answer
- faithfulness
- team
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'CRAFT is a GRPO-based reinforcement learning framework that trains
  RAG-based LLMs to generate structured, machine-auditable reasoning traces for multi-hop
  QA. It employs dual rewards: deterministic rewards for structural correctness and
  judge rewards for semantic faithfulness.'
---

# CRAFT: Calibrated Reasoning with Answer-Faithful Traces via Reinforcement Learning for Multi-Hop Question Answering

## Quick Facts
- arXiv ID: 2602.01348
- Source URL: https://arxiv.org/abs/2602.01348
- Reference count: 40
- **Key outcome**: CRAFT framework improves multi-hop QA faithfulness and accuracy over SFT baselines, with 7B models matching closed-source APIs

## Executive Summary
CRAFT addresses the challenge of generating faithful reasoning traces for multi-hop question answering with retrieval-augmented LLMs. The framework uses GRPO-based reinforcement learning to optimize both structural correctness and semantic faithfulness of reasoning traces. By employing dual rewards (format compliance, citation accuracy, answer correctness, and LLM-based faithfulness judgment), CRAFT achieves consistent improvements across three benchmarks while maintaining high faithfulness scores.

## Method Summary
CRAFT trains RAG-based LLMs to generate structured, machine-auditable reasoning traces for multi-hop QA using GRPO with dual rewards. The framework employs four reward components: R_fmt for format compliance, R_gold for citation accuracy, R_ans for answer correctness, and R_faith for semantic faithfulness via LLM judge. Training uses a mixed dataset of 20K samples from MuSiQue, HotpotQA, and 2WikiMHQA, with evaluation on held-out samples. The 7B Qwen2.5-Instruct base model shows consistent improvements over SFT baselines while maintaining faithfulness.

## Key Results
- 7B CRAFT model matches or exceeds closed-source API models on multi-hop QA accuracy
- CRAFT consistently improves both EM/F1 accuracy and faithfulness over SFT baselines across three benchmarks
- Ablation shows judge rewards critical: without R_faith, faithfulness drops ~9% while accuracy remains similar
- Training dynamics indicate faithfulness learning requires sufficient model capacity (0.5B models collapse on complex templates)

## Why This Works (Mechanism)
CRAFT works by explicitly optimizing for both structural and semantic faithfulness during training. The GRPO framework allows group-based reward normalization, addressing the sparsity of faithfulness rewards. By combining format compliance (R_fmt), citation accuracy (R_gold), answer correctness (R_ans), and semantic faithfulness (R_faith) rewards, the model learns to generate traces that are both correct and faithful to the retrieved evidence. The use of structured XML templates provides clear supervision for trace generation while enabling automated reward computation.

## Foundational Learning
- **GRPO (Group Relative Policy Optimization)**: Why needed - addresses reward sparsity in faithfulness optimization; Quick check - verify group reward normalization improves stability vs. naive REINFORCE
- **Faithfulness scoring via LLM judge**: Why needed - provides semantic assessment of reasoning trace quality; Quick check - implement Algorithm 1 criteria and validate against human judgments
- **Multi-hop QA with RAG**: Why needed - requires reasoning over multiple evidence documents; Quick check - verify retrieval mixes evidence with distractors as specified
- **Reinforcement learning for structured output**: Why needed - enables optimization beyond maximum likelihood; Quick check - compare RL vs. SFT performance on faithfulness metrics
- **XML template-based supervision**: Why needed - provides parseable structure for automated reward computation; Quick check - verify format compliance reward reaches ~99% during training

## Architecture Onboarding

**Component Map**: Question + Retrieved Docs → GRPO Training → XML Template → Four Rewards → Updated Model

**Critical Path**: Retrieval → Trace Generation → Reward Computation → Model Update

**Design Tradeoffs**: 
- Structured XML vs. free-form text: XML enables automated rewards but may constrain reasoning
- Judge-based faithfulness vs. proxy metrics: Judge provides semantic assessment but adds complexity
- Mixed training data vs. single benchmark: Improves generalization but requires careful balancing

**Failure Signatures**:
- Faithfulness degradation in SFT baseline (7-10% drop on complex templates)
- Model collapse on answer-only templates for small models (<1.5B)
- Format compliance failures (missing tags, truncation) in base models

**First Experiments**:
1. Train 7B model with all rewards vs. without R_faith to verify ~9% faithfulness drop
2. Compare 0.5B v4 vs. v5 variants to confirm capacity-dependent template complexity tolerance
3. Implement and validate LLM judge using Algorithm 1 criteria against human judgments

## Open Questions the Paper Calls Out
None

## Limitations
- Missing exact prompt templates for all CRAFT variants (v1-v5)
- Unclear retriever configuration and distractor selection methodology
- Incomplete faithfulness judge prompt implementation details
- No specification of how gold supporting document indices are determined

## Confidence

**High confidence**: Core GRPO framework with dual rewards is well-specified and reproducible; XML template structure and four reward components are clearly defined.

**Medium confidence**: Overall training methodology and observed trends (SFT baseline degradation, importance of judge rewards, capacity-dependent learning) are supported by ablation studies.

**Low confidence**: Specific numerical improvements and comparisons to closed-source models cannot be fully validated without access to exact judge prompts and training configurations.

## Next Checks

1. **Faithfulness judge implementation validation**: Implement the LLM judge using Algorithm 1 criteria and verify that base models without CRAFT training show the reported ~7-10% faithfulness degradation on complex templates compared to simpler v5.

2. **Capacity-dependent faithfulness testing**: Train 0.5B and 1.5B models with both v4 and v5 CRAFT variants to confirm the paper's observation that smaller models collapse on answer-only templates while benefiting from structured formats.

3. **Ablation of judge reward component**: Train a model with all rewards except R_faith to empirically verify the ~9% faithfulness drop while maintaining accuracy, confirming the importance of semantic faithfulness optimization beyond structural correctness.