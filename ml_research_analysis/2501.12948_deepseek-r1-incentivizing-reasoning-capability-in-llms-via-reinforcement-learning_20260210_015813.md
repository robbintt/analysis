---
ver: rpa2
title: 'DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement
  Learning'
arxiv_id: '2501.12948'
source_url: https://arxiv.org/abs/2501.12948
tags:
- uni00000011
- uni0000001b
- reasoning
- uni0000001a
- uni00000044
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents DeepSeek-R1, a large language model (LLM) capable
  of advanced reasoning, developed through pure reinforcement learning (RL) without
  supervised fine-tuning. The approach leverages Group Relative Policy Optimization
  (GRPO) and rule-based rewards to incentivize reasoning behaviors, enabling the model
  to autonomously develop sophisticated strategies like self-reflection and verification.
---

# DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning

## Quick Facts
- arXiv ID: 2501.12948
- Source URL: https://arxiv.org/abs/2501.12948
- Reference count: 27
- Key outcome: Pure RL approach achieves state-of-the-art reasoning performance without supervised fine-tuning

## Executive Summary
DeepSeek-R1 presents a novel approach to developing reasoning capabilities in large language models through pure reinforcement learning, eliminating the need for supervised fine-tuning. The method successfully demonstrates that advanced reasoning strategies like self-reflection, verification, and step-by-step problem solving can emerge autonomously when models are trained to maximize reward through trial and error. The resulting model achieves state-of-the-art performance on multiple reasoning benchmarks while maintaining strong general capabilities.

## Method Summary
The approach employs Group Relative Policy Optimization (GRPO), a variant of PPO that computes advantages relative to a group of samples rather than requiring a separate value model. The training pipeline uses rule-based reward functions that evaluate correctness, format compliance, and reasoning coherence, with zero-gradients applied to prevent catastrophic forgetting. A multi-stage process begins with DeepSeek-V3-Base, proceeds through reasoning-oriented RL without supervised fine-tuning, and culminates in supervised fine-tuning and preference alignment stages. The method also incorporates a novel cold-start data generation process where the model generates its own reasoning demonstrations to bootstrap training.

## Key Results
- DeepSeek-R1-Zero achieves 79.8% accuracy on AIME 2024 without any supervised fine-tuning
- DeepSeek-R1 reaches 96.3 percentile on Codeforces and state-of-the-art performance on multiple benchmarks
- Distilled versions maintain strong performance, with the 1.5B model achieving competitive results

## Why This Works (Mechanism)
The method works by creating an environment where reasoning strategies are directly rewarded, allowing the model to discover effective problem-solving approaches through exploration. GRPO's group-relative advantage computation enables more stable training by normalizing rewards within batches, while the rule-based reward system provides clear feedback on reasoning quality. The multi-stage training allows the model to first develop raw reasoning capabilities before refining them for practical use cases.

## Foundational Learning
- Reinforcement Learning with Language Models: Why needed - enables autonomous development of reasoning strategies; Quick check - verify reward shaping effectively guides reasoning behavior
- Group Relative Policy Optimization: Why needed - provides stable training without separate value model; Quick check - confirm group advantage computation improves training stability
- Cold-start Data Generation: Why needed - bootstraps RL when starting from base model; Quick check - validate generated data quality for initial training

## Architecture Onboarding

### Component Map
DeepSeek-V3-Base -> GRPO Training (reasoning rewards) -> Supervised Fine-tuning -> Preference Alignment RL -> DeepSeek-R1

### Critical Path
The critical path is the GRPO training phase where reasoning capabilities are developed, as this stage determines the model's ability to solve complex problems. Success here enables all downstream improvements.

### Design Tradeoffs
The pure RL approach sacrifices initial training stability for the potential of more autonomous capability development. Rule-based rewards provide clear feedback but may limit creativity compared to learned reward models. The multi-stage approach balances capability development with practical usability.

### Failure Signatures
- Language mixing indicates reward function misalignment
- Repetitive reasoning patterns suggest insufficient exploration
- Catastrophic forgetting manifests as degraded performance on previously mastered tasks

### First Experiments
1. Validate GRPO training stability on a small reasoning task
2. Test rule-based reward function effectiveness on sample problems
3. Evaluate cold-start data generation quality for bootstrapping

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Language mixing observed during early training stages
- Token efficiency remains suboptimal compared to specialized models
- Potential overfitting to benchmark distributions without broader generalization testing

## Confidence
- High confidence: RL effectiveness for reasoning enhancement
- Medium confidence: Generalization to real-world reasoning tasks
- Medium confidence: Distillation effectiveness across model sizes
- Low confidence: Language-agnostic reasoning emergence

## Next Checks
1. Independent reproduction of DeepSeek-R1-Zero's reasoning emergence on different RL framework
2. Systematic evaluation of reasoning degradation across distilled model sizes
3. Cross-lingual benchmarking to validate reasoning robustness beyond training distribution