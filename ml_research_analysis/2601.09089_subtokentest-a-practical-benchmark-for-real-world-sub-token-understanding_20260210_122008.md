---
ver: rpa2
title: 'SubTokenTest: A Practical Benchmark for Real-World Sub-token Understanding'
arxiv_id: '2601.09089'
source_url: https://arxiv.org/abs/2601.09089
tags:
- task
- answer
- sequence
- figure
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces SubTokenTest, a comprehensive benchmark
  designed to evaluate large language models'' (LLMs) sub-token understanding through
  real-world tasks across four domains: sequence transformation, text canonicalization,
  structured data, and 2D spatial pattern recognition. The benchmark includes ten
  tasks that isolate tokenization-related failures by minimizing complex reasoning,
  such as table alignment, Gomoku board reading, and RSA difference identification.'
---

# SubTokenTest: A Practical Benchmark for Real-World Sub-token Understanding

## Quick Facts
- **arXiv ID:** 2601.09089
- **Source URL:** https://arxiv.org/abs/2601.09089
- **Reference count:** 40
- **Primary result:** Introduces SubTokenTest, a comprehensive benchmark to evaluate LLMs' sub-token understanding across real-world tasks in four domains.

## Executive Summary
This paper introduces SubTokenTest, a comprehensive benchmark designed to evaluate large language models' (LLMs) sub-token understanding through real-world tasks across four domains: sequence transformation, text canonicalization, structured data, and 2D spatial pattern recognition. The benchmark includes ten tasks that isolate tokenization-related failures by minimizing complex reasoning, such as table alignment, Gomoku board reading, and RSA difference identification. Evaluation of nine advanced LLMs reveals that while reasoning models like DeepSeek-R1 mitigate sub-token errors through explicit character decomposition, this comes at extreme computational cost. Smaller models consistently fail, and even reasoning models revert to sub-token-blind behavior under token budget constraints. The study further identifies an inverted U-shaped relationship between reasoning token budget and performance, with overthinking degrading accuracy at high budgets. Interpretability analysis shows that character-level information is encoded in hidden states, with awareness evolving differently across layers and text types, suggesting that models' sub-token "vision" is heavily influenced by input format.

## Method Summary
The benchmark consists of 10 tasks across 4 domains, with 1,700 test instances total. Tasks include sequence transformation (keystroke, cipher, bio-seq), text canonicalization (OCR-noise, safety-mask), structured data (tables, trees), and 2D spatial pattern recognition (map-nav, Gomoku, RSA-diff). Evaluation uses API models with temperature=0.6 and max_tokens=max, while local models use vLLM with top_p=0.95, temperature=0.6, max_tokens=32768. Metrics include Exact Match (EM), normalized Levenshtein similarity, F1 scores, and Content Score/Alignment Rate. Test-Time Budget Control (TTBC) tests reasoning models at token budgets {256,512,1024,2048,4096,8192,16384}. Linear probing on Qwen-2.5-7B-Instruct analyzes character-level encoding across transformer layers.

## Key Results
- Reasoning models like DeepSeek-R1 mitigate sub-token errors through explicit character decomposition but at extreme computational cost (14,971 tokens vs 814 for 13% accuracy gain)
- An inverted U-shaped relationship exists between reasoning token budget and performance, with overthinking degrading accuracy at high budgets
- Character-level information is implicitly encoded in hidden states, with awareness evolving differently across layers and text types
- Smaller models (7B-32B) consistently fail sub-token tasks even with reasoning distillation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Reasoning models mitigate sub-token blindness by explicitly decomposing tokens into constituent characters within their thinking traces, at extreme computational cost.
- **Mechanism:** Chain-of-thought reasoning forces the model to spell out tokens character-by-character (e.g., "Row 3: B, E, B, W, W, W..."), creating an explicit intermediate representation that bypasses the tokenizer's obfuscation. The model reconstructs character-level information that would otherwise be inaccessible.
- **Core assumption:** The model has learned character composition through training data exposure, even though tokenization hides it during normal inference.
- **Evidence anchors:** [abstract] "reasoning models like DeepSeek-R1 mitigate sub-token errors through explicit character decomposition, this comes at extreme computational cost"; [Section 4] "reasoning models successfully mitigate sub-token blindness by explicitly decomposing tokens into constituent characters within their thinking traces"; [Table 2] DeepSeek-R1 uses 14,971 tokens vs. 814 for DeepSeek-V3 on Cipher task—~18x increase for 13% accuracy gain.
- **Break condition:** When token budgets are constrained, even reasoning models revert to sub-token-blind behavior (o4-mini "low" vs "high" shows sharp accuracy drops per Appendix D.1).

### Mechanism 2
- **Claim:** An inverted U-shaped relationship exists between reasoning token budget and sub-task performance—overthinking degrades accuracy at high budgets.
- **Mechanism:** Three phases operate: (1) **Increasing phase** (256-512 tokens): insufficient reasoning leaves character mappings incomplete; (2) **Plateau phase** (1024-2048 tokens): adequate verification stabilizes performance; (3) **Decreasing phase** (>2048 tokens): redundant reasoning introduces noise, and models lose track of character positions through accumulated small errors.
- **Core assumption:** Extended reasoning chains accumulate small positional errors rather than converging on correct answers.
- **Evidence anchors:** [abstract] "inverted U-shaped relationship between reasoning token budget and performance, with overthinking degrading accuracy"; [Section 4.1] "Performance peaks at a budget of approximately 2048 tokens before suffering a significant decline"; [Figure 4] Clear inverted-U curve with peak at 2048 tokens for biological sequence task; [corpus] Related work on test-time scaling (Ghosal et al., 2025a; Su et al., 2025) confirms inverted-U patterns across benchmarks—but corpus lacks direct replication for sub-token tasks.
- **Break condition:** If the task requires only simple character substitution without positional reasoning (e.g., single DNA complementation), the inverted-U may flatten or shift left.

### Mechanism 3
- **Claim:** Character-level information is implicitly encoded in hidden states, with awareness surging in early layers (2-3), plateauing, then rising again in deeper layers for non-standard text forms.
- **Mechanism:** The embedding layer provides minimal character information. By layers 2-3, an internal "detokenization" process reconstructs character composition from token representations. For normal words, awareness stabilizes mid-network; for perturbed or symbolic text, deeper layers re-engage for disambiguation.
- **Core assumption:** Models learn character composition from statistical co-occurrence patterns without explicit supervision during pretraining.
- **Evidence anchors:** [abstract] "character-level information is encoded in hidden states, with awareness evolving differently across layers and text types"; [Section 4.2] Linear probing shows F1 ~0.35-0.50 for character count prediction, vs. ~0.16 shuffled baseline; [Figure 5] Normal words and special symbols show higher F1 than typos/random letters—text format matters; [corpus] Related work (Itzhak & Levy, 2021; Kaplan et al., 2025) confirms implicit detokenization—but corpus lacks direct mechanistic explanation for the secondary rise in deep layers.
- **Break condition:** If models are trained with explicit character-level objectives or byte-level tokenization, the layer-wise pattern may shift earlier or flatten.

## Foundational Learning

- **Concept: Byte-Pair Encoding (BPE) Tokenization**
  - **Why needed here:** The entire benchmark exists because BPE merges frequent character sequences into single tokens, obscuring character boundaries. Understanding this is prerequisite to understanding why models fail at "strawberry" letter counting.
  - **Quick check question:** Can you explain why "strawberry" tokenized as ["straw", "berry"] makes counting "r"s difficult for a model that only sees tokens?

- **Concept: Chain-of-Thought Reasoning as Intermediate Representation**
  - **Why needed here:** The paper's main finding is that reasoning models create explicit character sequences in thinking traces. This is a computational workaround, not an architectural fix.
  - **Quick check question:** How does forcing a model to spell "m-a-i-n-t-a-i-n" character-by-character change what information is available at inference time?

- **Concept: Linear Probing for Interpretability**
  - **Why needed here:** Section 4.2 uses linear probes to prove character information exists in hidden states. Without understanding probing, you cannot evaluate the claim that models "implicitly encode" characters.
  - **Quick check question:** If a linear probe achieves 50% F1 predicting character counts from layer-5 hidden states, what does this tell you about what the model internally represents?

## Architecture Onboarding

- **Component map:** Input Text → Tokenizer (BPE) → Token Embeddings → Layers 0-3: Rapid character reconstruction → Layers 4-20: Plateau—stable but task-dependent encoding → Layers 21-28: Secondary rise for perturbed/symbolic text → Reasoning Models: Explicit character decomposition in CoT → Output (conditionally correct if token budget sufficient)

- **Critical path:** The failure mode is at tokenization → hidden state alignment. If characters span token boundaries (e.g., map navigation where "P _ #" becomes one token), the model must infer positions from context, not direct access.

- **Design tradeoffs:**
  - **Test-time scaling:** More tokens help up to ~2048, then hurt. Budget tuning is task-dependent.
  - **Reasoning vs. non-reasoning models:** Reasoning models achieve higher accuracy (e.g., GPT-5 at 99% on keystroke decoding vs. GPT-4 at 0%) but at 10-100x token cost.
  - **Model scale:** Smaller models (7B-32B) fail even with reasoning distillation—capacity matters more than reasoning format for sub-token tasks.

- **Failure signatures:**
  1. **Tokenization-induced errors:** Model misaligns positions when multiple elements merge into one token (Figure 3a, Gomoku board reading).
  2. **Overthinking:** Excessive reasoning tokens cause models to lose track of character positions (Figure 3b, DeepSeek-R1 trapped in recursive cycles).
  3. **Budget collapse:** Constrained reasoning budgets force models back to sub-token-blind behavior (o4-mini low vs. high).

- **First 3 experiments:**
  1. **Reproduce the inverted-U curve:** Run the biological sequence task with TTBC at budgets {256, 512, 1024, 2048, 4096, 8192} on a reasoning model. Verify peak at ~2048 tokens using normalized Levenshtein similarity.
  2. **Ablate reasoning format:** Compare explicit "spell first, then answer" prompting vs. standard prompting on the Cipher task for a non-reasoning model. Measure if forced decomposition approaches reasoning model performance.
  3. **Probe layer-wise encoding on your model:** Train linear probes on hidden states from layers {0, 5, 10, 15, 20, 25} using the bag-of-characters prediction task from Section 4.2. Compare F1 curves for normal words vs. special symbols to replicate Figure 5.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What mechanistic circuit-level processes enable or obstruct sub-token understanding across transformer layers?
- **Basis in paper:** [explicit] The authors state their "interpretability analysis is limited to a linear probe that provides some intuitions" and "A deeper, more comprehensive analysis of these circuits remains an open direction for future research."
- **Why unresolved:** Linear probing can only detect whether character-level information is decodable from hidden states, not how it flows through attention patterns, MLP layers, or is actively used during generation.
- **What evidence would resolve it:** Mechanistic interpretability studies using causal tracing, attention head analysis, or activation patching to identify specific circuit components responsible for character-level reconstruction.

### Open Question 2
- **Question:** Why do smaller reasoning models (7B–32B) underperform their non-reasoning base models on sub-token tasks, contrary to the trend observed in larger models?
- **Basis in paper:** [explicit] "Within the smaller parameter range (7B to 32B)... reasoning models do not always out-perform their non-reasoning counterparts... the extended CoT often adds noise instead of correcting errors."
- **Why unresolved:** The paper documents the phenomenon but does not identify whether it stems from insufficient internal representation capacity, training data limitations, or a fundamental mismatch between chain-of-thought reasoning and character-level state tracking.
- **What evidence would resolve it:** Ablation studies varying model size while controlling for training compute, along with analysis of hidden-state representations in small vs. large models during sub-token tasks.

### Open Question 3
- **Question:** Can sub-token understanding be improved without the extreme computational costs observed in reasoning models?
- **Basis in paper:** [explicit] "Reasoning models successfully mitigate sub-token blindness... However, this performance gain comes at a significant computational cost" and "we do not propose solutions for the challenges identified, leaving further improvements for future work."
- **Why unresolved:** The paper demonstrates that explicit character decomposition via extended thinking traces works but is inefficient; no alternative approaches are evaluated.
- **What evidence would resolve it:** Comparison of architectural modifications (e.g., character-aware tokenizers, hybrid byte-token models) or fine-tuning strategies specifically targeting sub-token competence against the reasoning-model baseline.

### Open Question 4
- **Question:** What causes the inverted U-shaped relationship between reasoning token budget and sub-token task performance, and does the optimal budget generalize across task types?
- **Basis in paper:** [inferred] The paper documents an inverted U-shaped curve with peak performance at ~2048 tokens but only tests on biological sequence manipulation; the mechanism (overthinking vs. genuine verification saturation) is not isolated.
- **Why unresolved:** It is unclear whether performance decline at high budgets stems from accumulated errors in long reasoning chains, attention drift, or a fundamental trade-off between deliberation and task-specific pattern matching.
- **What evidence would resolve it:** Systematic testing of TTBC across all SubTokenTest tasks with analysis of error types at different budget levels, paired with attention distribution analysis during overthinking phases.

## Limitations
- The 10 tasks may not exhaustively capture all sub-token understanding failure modes in real-world applications
- The inverted-U relationship lacks mechanistic explanation for why overthinking specifically degrades accuracy
- Linear probing analysis cannot determine whether character encoding is causally necessary for task performance
- All evaluations use static, generated test instances rather than dynamic, user-generated content

## Confidence
**High Confidence Claims:**
- SubTokenTest successfully exposes fundamental sub-token understanding failures across LLMs
- Reasoning models achieve higher accuracy but at extreme computational cost
- Token budget constraints force reasoning models to revert to sub-token-blind behavior

**Medium Confidence Claims:**
- The inverted-U shaped relationship between reasoning token budget and performance
- Character-level information is implicitly encoded in hidden states with layer-wise evolution
- Reasoning models mitigate sub-token blindness through explicit character decomposition

**Low Confidence Claims:**
- Small models fail sub-token tasks even with reasoning distillation
- Overthinking degrades accuracy through accumulated positional errors

## Next Checks
1. **Mechanistic validation of overthinking degradation:** Design experiments that track character position accuracy throughout extended reasoning chains. Insert probes that measure whether models lose track of character positions over time, and test whether pruning redundant reasoning steps preserves accuracy while reducing token usage.

2. **Architectural intervention comparison:** Compare reasoning models' explicit decomposition approach against architectural modifications that preserve character-level information (e.g., byte-level tokenization, character-aware embeddings). This would determine whether the benchmark exposes fundamental limitations or solvable engineering problems.

3. **Cross-domain generalization study:** Evaluate the same models on user-generated content from real applications (code repositories, medical records, financial documents) rather than synthetic test instances. This would validate whether the benchmark's findings generalize to deployment scenarios where tokenization patterns differ from controlled test sets.