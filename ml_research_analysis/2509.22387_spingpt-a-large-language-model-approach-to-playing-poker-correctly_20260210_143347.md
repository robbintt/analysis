---
ver: rpa2
title: 'SpinGPT: A Large-Language-Model Approach to Playing Poker Correctly'
arxiv_id: '2509.22387'
source_url: https://arxiv.org/abs/2509.22387
tags:
- poker
- spingpt
- games
- hands
- players
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SpinGPT is the first LLM-based agent for three-player poker tournaments
  (Spin & Go format). The authors fine-tune Llama-3.1-8B on 320k expert hands, then
  refine it with reinforcement learning on 270k solver-generated hands.
---

# SpinGPT: A Large-Language-Model Approach to Playing Poker Correctly

## Quick Facts
- arXiv ID: 2509.22387
- Source URL: https://arxiv.org/abs/2509.22387
- Reference count: 22
- Primary result: First LLM-based agent for three-player poker tournaments, achieving 78% tolerant accuracy on solver test set and 13.4±12.9 BB/100 win rate against Slumbot in heads-up play

## Executive Summary
SpinGPT is the first large language model trained to play three-player poker tournaments (Spin & Go format). The authors fine-tune Llama-3.1-8B on 320k expert hands, then refine it with reinforcement learning on 270k solver-generated hands. The model demonstrates that LLMs can learn strong poker strategies through structured text encoding of game states, achieving competitive performance against traditional CFR-based approaches. The two-stage training pipeline (SFT + ORPO) produces a model that matches solver actions in 78% of decisions and wins 13.4±12.9 BB/100 against Slumbot in heads-up play.

## Method Summary
SpinGPT uses a two-stage training approach on Llama-3.1-8B-Instruct. First, Supervised Fine-Tuning (SFT) trains the model on 320k expert decisions from high-stakes Spin & Go tournaments. Second, Offline Reinforcement Preference Optimization (ORPO) refines the policy using 270k solver-generated hands combined with 50k human hands to prevent catastrophic forgetting. The model encodes poker states as structured text prompts and generates open-vocabulary actions including bet amounts. Training uses LoRA (rank=8) for SFT and QLoRA for RL, with ~10 GPU-hours per phase on A100 hardware.

## Key Results
- 78% tolerant accuracy on solver test set (±0.5 BB tolerance)
- 13.4±12.9 BB/100 win rate against Slumbot benchmark in heads-up play
- 13.2±7.24 BB/100 improvement over SFT-only version
- 72% exact accuracy on solver test set

## Why This Works (Mechanism)

### Mechanism 1: Transfer Learning from Pre-trained Representations to Structured Game States
Pre-trained LLMs bring prior knowledge of rules, basic strategy, and probability, combined with facility for structured representations. The model interprets structured text encodings of game states and maps them to action outputs, leveraging pre-trained language understanding rather than learning poker from scratch.

### Mechanism 2: Two-Stage Training—Imitation Establishes Competence, RL Refines Toward Optimality
SFT on expert data followed by offline RL on solver data produces stronger play than either alone. SFT teaches the model to output legal, reasonable actions by imitating human experts; ORPO then adjusts the policy to prefer solver-recommended actions over alternatives, moving toward GTO play while retaining human-style robustness.

### Mechanism 3: Open-Vocabulary Action Generation Enables Non-Discretized Bet Sizing
Encoding poker states as structured text allows the model to output flexible, continuous bet sizes rather than predefined discretized actions. The model generates action tokens including numerical amounts through standard language modeling, producing any value the tokenizer can represent.

## Foundational Learning

### Concept: Counterfactual Regret Minimization (CFR)
- Why needed here: CFR is the dominant paradigm for poker AI; understanding its limitations (exponential scaling with players) motivates the LLM approach.
- Quick check question: Why does CFR's computational complexity grow exponentially with the number of players, and what does this mean for three-player tournaments?

### Concept: Nash Equilibrium in Multi-Player Games
- Why needed here: In 3+ player games, Nash equilibrium no longer guarantees non-losing outcomes; understanding this explains why the authors train on solver data but also value human robustness.
- Quick check question: Why can following a Nash equilibrium still result in losses in a three-player game?

### Concept: Offline Preference Optimization (ORPO)
- Why needed here: The RL phase uses ORPO, which aligns the model toward preferred (solver) actions rather than maximizing raw reward.
- Quick check question: How does ORPO's approach of increasing likelihood of preferred actions over alternatives differ from standard policy gradient methods?

## Architecture Onboarding

### Component Map:
Llama-3.1-8B-Instruct -> LoRA (rank=8, α=16) -> SFT training -> QLoRA (4-bit NF4) -> ORPO training -> SpinGPT

### Critical Path:
1. Parse hand histories → structured text prompts with stacks, cards, action sequences
2. SFT phase: 4 epochs, LR 5e-5, cosine decay, ~10 GPU-hours on A100
3. Generate 270k solver hands + retain 50k human hands → mixed dataset
4. ORPO phase: 2 epochs, LR 2e-5, β=0.1, ~10 GPU-hours on A100

### Design Tradeoffs:
- 8B model size: Chosen for representational capacity vs. computational tractability; 1B model also tested but not reported in detail
- Mixed dataset for RL: Pure solver data risks catastrophic forgetting; 50k human hands retained for robustness
- Deep-stack heuristic patch: Model trained at 25 BB fails at 200 BB; paper applies post-hoc heuristic rather than retraining

### Failure Signatures:
- Illegal moves (1.4%): Model predicts "raise" when no prior bet exists—confusion between bet/raise tokens
- Deep-stack over-aggression: Model shoves all-in at 200 BB (correct at 25 BB) without deep-stack training data
- Numerical tokenization errors: Lexicographic comparison (e.g., "5.11" > "5.2") causes bet-sizing inconsistencies
- Tolerant accuracy gap: 78% tolerant vs. 72% exact suggests imprecise bet sizing on solver data

### First 3 Experiments:
1. Validate SFT pipeline: Reproduce SpinGPT-SFT on a 50k-hand subset; verify ~75%+ tolerant accuracy on held-out expert data.
2. Measure RL improvement: Run duplicate-format head-to-head (1k-5k hands at 25 BB) between SFT-only and final model; expect positive win rate for final model.
3. Characterize deep-stack failure: Evaluate SFT model at 200 BB without heuristic; quantify all-in frequency and BB/100 loss rate to identify generalization boundaries.

## Open Questions the Paper Calls Out

### Open Question 1
Can SpinGPT sustain expert-level performance against specialized Spin & Go agents or human professionals without relying on heuristic patches? The authors state evaluations should target human opponents or bots specialized in Spin & Go because the Slumbot benchmark is a format mismatch.

### Open Question 2
Can the agent learn to infer opponent tendencies from hand history to dynamically switch between Game Theory Optimal (GTO) and exploitative play? The Conclusion identifies "adaptivity" as a key objective, suggesting future work could allow the LLM to infer tendencies from hand history and adjust its style.

### Open Question 3
Can the model architecture be modified to prevent numerical hallucinations and generalize to deep-stack play without manual heuristics? The authors note the model suffers from "typical language-model errors" and failed at 200 BB stacks, requiring a "minimal heuristic" to prevent catastrophic all-ins.

## Limitations

- Private expert hand histories prevent exact reproduction of the SFT training data
- Deep-stack performance requires heuristic patches rather than proper training at scale
- Numerical tokenization errors indicate fundamental issues with open-vocabulary action generation
- Comparison limited to heads-up play with heuristic patches rather than true three-player format

## Confidence

**High Confidence**: LLMs can learn poker strategy through fine-tuning (78% tolerant accuracy, 13.4±12.9 BB/100 win rate)
**Medium Confidence**: LLMs offer a promising alternative to CFR-based approaches (limited comparison, computational efficiency unverified)
**Low Confidence**: Pre-trained LLMs bring sufficient prior knowledge of poker rules and strategy (PokerBench shows zero-shot failure)

## Next Checks

1. Validate the two-stage training effect: Reproduce SpinGPT-SFT and compare it directly against the final ORPO-refined model in head-to-head matches at 25 BB.

2. Test deep-stack generalization: Evaluate the base SpinGPT model (without heuristic patches) at 200 BB against a solver or benchmark to quantify all-in frequency and BB/100 loss rate.

3. Analyze numerical tokenization errors: Generate a test set of bet sizing scenarios spanning the full range of possible amounts to quantify the impact of lexicographic comparison errors on action accuracy and expected value.