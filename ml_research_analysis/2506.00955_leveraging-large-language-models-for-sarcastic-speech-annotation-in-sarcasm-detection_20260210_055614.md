---
ver: rpa2
title: Leveraging Large Language Models for Sarcastic Speech Annotation in Sarcasm
  Detection
arxiv_id: '2506.00955'
source_url: https://arxiv.org/abs/2506.00955
tags:
- sarcasm
- detection
- speech
- annotation
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of sarcasm detection in speech,
  which is hindered by data scarcity and the reliance on multimodal data. The authors
  propose a novel annotation pipeline that leverages large language models (LLMs)
  to generate a sarcasm dataset from a sarcasm-focused podcast, followed by human
  verification to resolve disagreements.
---

# Leveraging Large Language Models for Sarcastic Speech Annotation in Sarcasm Detection

## Quick Facts
- arXiv ID: 2506.00955
- Source URL: https://arxiv.org/abs/2506.00955
- Reference count: 0
- Primary result: LLM-assisted annotation pipeline achieves 73.63% F1 on sarcasm detection using 29.42 hours of podcast-derived speech data

## Executive Summary
This paper addresses the data scarcity challenge in sarcasm detection by proposing an LLM-assisted annotation pipeline that generates a sarcasm dataset from podcast audio. The approach uses GPT-4o and LLaMA 3-70B to annotate transcripts, with human verification only on cases where the LLMs disagree. This selective verification strategy reduces annotation costs by 74% while achieving 67.47% F1 on MUStARD++ validation. The resulting PodSarc dataset enables a bimodal sarcasm detection model that achieves 73.63% F1, demonstrating that LLM-assisted annotation can produce high-quality datasets for speech-based sarcasm detection research.

## Method Summary
The method processes raw podcast audio through Emilia-Pipe segmentation to create 11,024 utterance-transcript pairs. Dual LLM annotation (GPT-4o + LLaMA 3-70B) generates initial sarcasm labels, with human verification only on the 26% of utterances where LLMs disagree. The detection model uses collaborative gating to fuse BART text embeddings (1024-dim) with audio features (MFCC 128 + Mel-spec 128 + prosodic 35 = 291 total) for classification. The pipeline achieves 67.47% F1 on MUStARD++ validation and produces the PodSarc dataset containing approximately 29.42 hours of speech-transcript pairs.

## Key Results
- LLM-assisted annotation achieves 67.47% F1 on MUStARD++ validation
- Bimodal fusion model achieves 73.63% F1 on PodSarc dataset
- Human verification required for only 26% of utterances, reducing annotation costs by 74%
- Audio-only model achieves 61.58% F1, demonstrating importance of multimodal features

## Why This Works (Mechanism)

### Mechanism 1
LLMs can approximate sarcasm labels from textual transcripts with moderate accuracy, enabling scalable annotation without full human review. LLMs (GPT-4o, LLaMA 3-70B) process transcripts with context, linguistic cues, and few-shot examples to identify patterns like contradiction, exaggeration, and absurdity that signal sarcastic intent. Core assumption: Sarcasm cues detectable from text correlate sufficiently with speech-based sarcasm to produce usable training labels. Evidence anchors: [abstract] GPT-4o achieved 67.47% F1 score on MUStARD++ annotations; [section 3.2] Table 1 shows GPT-4o at 67.47% Macro-F1, LLaMA 3-70B at 63.59%. Break condition: If sarcasm heavily relies on prosodic cues absent from transcripts (e.g., tone shifts without lexical markers), LLM-only annotation will systematically mislabel.

### Mechanism 2
Focusing human verification on LLM disagreements targets ambiguous cases where annotation effort yields highest label quality improvement. Two LLMs annotate independently; disagreements (2,884/11,024 = 26% of utterances) are flagged for human review, while agreements bypass human effort. Core assumption: Cases where LLMs agree have lower error rates than disagreement cases, making selective verification efficient. Evidence anchors: [section 3.3] Human annotators reviewed only conflicting labels; inter-annotator Kappa = 0.58, resolved by third annotator; [section 3.3] Human verification refined 1,318 sarcastic and 1,566 non-sarcastic utterances. Break condition: If LLMs share systematic biases (e.g., both miss irony without explicit markers), agreement cases may contain undetected errors.

### Mechanism 3
Bimodal fusion (text + audio) outperforms single-modality detection by capturing complementary sarcasm signals. Text embeddings from BART capture semantic incongruity; audio features (MFCC, Mel spectrogram, prosody) capture intonation, pacing, emphasis. Collaborative gating fuses these representations before classification. Core assumption: Sarcastic speech exhibits detectable patterns in both modalities that reinforce each other. Evidence anchors: [abstract] Detection model achieves 73.63% F1 on PodSarc; [section 4.3] Table 3: A+T achieves 73.63% F1 vs. 72.54% (text-only) vs. 61.58% (audio-only). Break condition: If audio quality is poor or text lacks context, fusion may amplify noise rather than signal.

## Foundational Learning

- Concept: **Collaborative gating architecture for multimodal fusion**
  - Why needed here: The detection model uses this to combine text and audio features rather than simple concatenation.
  - Quick check question: Can you explain how gating mechanisms selectively weight modality contributions versus concatenation?

- Concept: **Inter-annotator agreement (Cohen's Kappa)**
  - Why needed here: Human verification phase reports Kappa = 0.58; understanding this metric is essential for interpreting annotation quality.
  - Quick check question: What does Kappa = 0.58 indicate about annotator reliability, and what are its limitations?

- Concept: **Feature extraction for speech (MFCC, Mel spectrogram, prosodic features)**
  - Why needed here: Audio modality relies on 291-dimensional feature vector from these three feature types.
  - Quick check question: What distinct aspects of speech do MFCCs, Mel spectrograms, and prosodic features each capture?

## Architecture Onboarding

- Component map:
  - Raw audio -> Emilia-Pipe -> segmented utterances with transcripts
  - Transcripts -> dual LLM annotation -> initial labels
  - Disagreement detection -> human review -> final labels
  - Final dataset -> feature extraction -> detection model training

- Critical path:
  1. Raw audio → Emilia-Pipe → segmented utterances with transcripts
  2. Transcripts → dual LLM annotation → initial labels
  3. Disagreement detection → human review → final labels
  4. Final dataset → feature extraction → detection model training

- Design tradeoffs:
  - Dual-LLM vs. single-LLM: Increases annotation coverage of edge cases but requires managing two API costs/prompting strategies.
  - Human verification scope: Only disagreements reviewed (26% of data); trades some label noise for 74% annotation cost reduction.
  - Bimodal vs. multimodal: Excludes visual cues, limiting performance on video-based sarcasm but enabling audio-only deployment scenarios.

- Failure signatures:
  - Low detection F1 on datasets with visually-dependent sarcasm (e.g., MUStARD++ with facial expression cues).
  - High disagreement rate between LLMs on short utterances lacking context.
  - Audio-only F1 substantially lower than text-only suggests prosodic feature extraction may be underutilized.

- First 3 experiments:
  1. **Baseline validation**: Replicate MUStARD++ experiments with original labels vs. LLM-only labels vs. LLM+human labels; expect descending F1 (70.8% → 61.8% → 67.4% per Figure 3).
  2. **Ablation by modality**: Train on PodSarc with text-only, audio-only, and A+T; verify fusion improves over single modalities (target: ~72.5% → ~61.5% → ~73.6%).
  3. **Agreement subset analysis**: Sample and manually verify 100 LLM-agreement cases; estimate silent error rate to assess whether partial human validation of agreements is warranted.

## Open Questions the Paper Calls Out
None

## Limitations
- Partial validation of LLM-generated labels through selective human verification assumes agreement cases have acceptable error rates without direct validation
- Human annotation reliability (Kappa = 0.58) indicates moderate agreement, suggesting inherent ambiguity in sarcasm labeling that affects both human and LLM performance
- The 74% annotation cost reduction claim mathematically sound but assumes LLM-agreement cases are sufficiently accurate

## Confidence
- **High Confidence**: The detection model architecture and its performance on PodSarc (73.63% F1) - directly measured with reproducible methodology
- **Medium Confidence**: The LLM annotation pipeline's effectiveness - supported by MUStARD++ validation but limited by partial human verification
- **Medium Confidence**: The 74% annotation cost reduction claim - mathematically sound but assumes LLM-agreement cases are sufficiently accurate

## Next Checks
1. **Agreement Case Validation**: Manually verify a random sample of 100 LLM-agreement cases to estimate the silent error rate and determine if selective verification is justified

2. **Prompt Sensitivity Analysis**: Test the LLM annotation pipeline with variations in prompt wording, few-shot examples, and temperature settings to assess robustness to prompt engineering

3. **Cross-Genre Generalization**: Evaluate the detection model on sarcasm datasets from different domains (e.g., Twitter, news headlines) to test whether the podcast-trained model generalizes beyond its training distribution