---
ver: rpa2
title: Decentralized Dynamic Cooperation of Personalized Models for Federated Continual
  Learning
arxiv_id: '2509.23683'
source_url: https://arxiv.org/abs/2509.23683
tags:
- learning
- clients
- client
- each
- coalition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles catastrophic forgetting in federated continual
  learning by introducing a decentralized dynamic cooperation framework. Instead of
  aggregating all client models into a global model, the method enables clients to
  form non-overlapping coalitions based on task similarity, allowing personalized
  models while retaining prior knowledge.
---

# Decentralized Dynamic Cooperation of Personalized Models for Federated Continual Learning

## Quick Facts
- **arXiv ID:** 2509.23683
- **Source URL:** https://arxiv.org/abs/2509.23683
- **Reference count:** 40
- **Primary result:** Achieves superior accuracy and significantly lower forgetting rates than baselines in federated continual learning through decentralized coalition formation.

## Executive Summary
This paper addresses catastrophic forgetting in federated continual learning by proposing a decentralized dynamic cooperation framework. Instead of aggregating all client models into a global model, the method enables clients to form non-overlapping coalitions based on task similarity, allowing personalized models while retaining prior knowledge. The framework uses overall similarity—combining gradient coherence and model similarity—to quantify client benefits, then applies coalitional affinity games to calculate multi-client benefits. Experiments on EMNIST, CIFAR100, and MNIST-SVHN-F show superior performance over various baselines, with the method achieving higher accuracy and significantly lower forgetting rates while maintaining computational efficiency.

## Method Summary
The framework operates through local training rounds where each client optimizes a combined classification and knowledge distillation loss. Clients calculate pairwise overall similarity scores combining gradient and model parameter cosine similarity. Using these scores, a merge-blocking algorithm finds stable coalition equilibria where no subset of clients can form a new coalition that improves all members' benefits. Models within each coalition are aggregated via weighted average and redistributed to members. The process dynamically evolves as new tasks arrive, with knowledge distillation preserving feature consistency across task changes to enable accurate cooperator identification.

## Key Results
- Achieves higher average accuracy and significantly lower average forgetting rates compared to FedAvg, CFL, and other baselines across EMNIST, CIFAR100, and MNIST-SVHN-F datasets
- The merge-blocking algorithm efficiently finds stable coalition equilibria that outperform random and fixed partitioning strategies
- Ablation studies confirm that knowledge distillation, overall similarity, and coalition evolution each contribute significantly to performance gains

## Why This Works (Mechanism)

### Mechanism 1: Selective Aggregation via Overall Similarity
Aggregating models only from clients with high "overall similarity" reduces spatial catastrophic forgetting and improves performance over global or random aggregation. The framework computes an overall similarity score for each client pair, combining gradient cosine similarity (capturing alignment in optimization direction) and model parameter cosine similarity (capturing global knowledge consistency). This score quantifies the expected benefit of cooperation. Clients with high mutual scores are candidates for coalition. Core assumption: Clients with similar gradient directions and model parameters possess compatible knowledge; aggregating their models reinforces relevant features without introducing interference from unrelated tasks.

### Mechanism 2: Coalition Stabilization through Merge-Blocking Algorithm
The merge-blocking algorithm efficiently finds a stable, game-theoretic equilibrium where no subset of clients can form a new coalition that improves all members' benefits. The algorithm iteratively traverses possible coalitions. It identifies a blocking coalition (BC) – a group where each member's calculated benefit in the new coalition is ≥ their current benefit, with at least one member strictly better. It then merges clients into this BC, blocking (dissolving) their previous coalitions. This repeats until no BC exists, reaching a cooperative equilibrium. A stable coalition (SC) pruning heuristic reduces search space. Core assumption: Clients are rational agents seeking to maximize their own benefit, and the benefit function accurately reflects performance gains from cooperation.

### Mechanism 3: Dynamic Coalition Evolution via Knowledge Distillation
Using knowledge distillation during local training preserves feature consistency across task changes, enabling accurate cooperator identification and smooth coalition evolution. Each client applies distillation loss during local training, where a past model (teacher) guides the current model (student). This constrains the feature extractor and classifier output space from drifting excessively toward the new task. Consistent features make similarity comparisons between clients' models more meaningful over time, allowing the Dynamic Cooperative Evolution Algorithm to update coalitions effectively as new tasks arrive. Core assumption: The KD loss with parameter λ is sufficient to prevent catastrophic feature drift without overly hindering new task learning.

## Foundational Learning

- **Catastrophic Forgetting (In CL & FCL)**: Why needed here: The entire paper's problem statement. Understand temporal forgetting (local task drift) and spatial forgetting (interference from aggregated models). Quick check: Explain how aggregating a model trained on task A with a model trained on task B can hurt performance on task A.
- **Federated Learning (FL) Aggregation**: Why needed here: The baseline method (FedAvg) and the paper's decentralized alternative. Understand how model parameters or updates are combined. Quick check: In standard FedAvg, how is the global model update computed from multiple client updates?
- **Cooperative Game Theory Basics**: Why needed here: The framework models client cooperation using coalitional games, affinity, and equilibrium concepts. Quick check: In a simple hedonic game, what defines a "Nash-stable" partition for the agents?

## Architecture Onboarding

- **Component map**: Local Trainer -> Similarity Computer -> Benefit Table Builder -> Merge-Blocking Engine -> Coalition Aggregator
- **Critical path**:
  1. Clients complete a local training round, producing updated models θ^τ_k and gradients g^τ_k
  2. A coordinating entity (e.g., designated client or neutral third-party) collects all θ^τ_k and g^τ_k
  3. Similarity Computer generates the pairwise similarity matrix
  4. Benefit Table Builder populates the benefit lookup table
  5. Merge-Blocking Engine runs to determine the equilibrium coalition partition for this round
  6. Coalition Aggregator forms the aggregated model for each coalition
  7. Clients receive their coalition's aggregated model to initialize the next local training round
- **Design tradeoffs**:
  1. Optimality vs. Complexity: The exact merge-blocking algorithm has O(K2^K) complexity. For large K, approximate versions or different game formulations may be necessary
  2. Similarity Metric Composition: The ε weight between gradient and model similarity trades off optimization alignment vs. parameter-space proximity. Requires tuning per dataset
  3. Distillation Strength: The λ hyperparameter trades off stability (old knowledge) vs. plasticity (new knowledge)
- **Failure signatures**:
  1. Excessive Single-Client Coalitions: Could indicate similarity scores are too low or ε is poorly tuned, leading to no perceived benefit from cooperation. Check similarity distribution and benefit tables
  2. Slow Equilibrium Convergence: May happen if the coalition space is large and the merge-blocking algorithm makes slow progress. Monitor the number of rounds in Algorithm 1
  3. Performance Plateau or Drop: If KD is too weak (λ low), forgetting increases; if too strong, new task learning suffers. Ablate λ
- **First 3 experiments**:
  1. Coalition Formation Validation: On a small, controlled dataset (e.g., 4-5 clients, 2-3 tasks with known overlap), log the formed coalitions at each round. Verify they align with task similarity and evolve as expected
  2. Baseline Comparison with Ablations: Replicate key results from Table 1 and Table 2. Compare DCFCL to FedAvg, a decentralized clustering baseline (CFL), and DCFCL w/o each core mechanism (CE, KD, OS) to confirm each component's contribution
  3. Hyperparameter Sensitivity Sweep: Following Table 3, sweep ε ∈ [0, 1] and λ ∈ [0, 1] on one dataset (e.g., EMNIST-LTP) to find stable operating ranges and understand their interaction. Plot final accuracy vs. parameter values

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the framework automatically adapt key hyperparameters (λ, ε) to maintain robust performance across highly diverse, unseen client scenarios?
- Basis in paper: [explicit] The conclusion notes that "parameter sensitivity remains" and states that "exploring adaptive mechanisms is left for future work."
- Why unresolved: Fixed hyperparameters for distillation and similarity weighting may lead to suboptimal coalition formation in heterogeneous data streams not represented in validation sets.
- What evidence would resolve it: An adaptive update rule for hyperparameters that stabilizes accuracy and forgetting rates across diverse non-IID settings without manual tuning.

### Open Question 2
- Question: Can approximate algorithms for coalition formation preserve the theoretical guarantees of the Merge-Blocking Algorithm (MBA) while scaling to thousands of clients?
- Basis in paper: [explicit] Appendix B.4 states that large-scale deployment "necessitates approximating solution" and creates a "performance-cost tradeoff."
- Why unresolved: The current complexity O(K2^K) limits application to small-scale networks, and the performance drop of necessary approximations is currently unquantified.
- What evidence would resolve it: Experiments demonstrating convergence speed and accuracy retention in systems with significantly larger client populations (K > 100) using approximation heuristics.

### Open Question 3
- Question: How robust is the "overall similarity" benefit calculation against adversarial clients who manipulate gradient or model similarity metrics?
- Basis in paper: [inferred] The method relies on honest reporting of model parameters and gradients to calculate affinity (Eq. 7), but lacks discussion on adversarial robustness or Byzantine fault tolerance.
- Why unresolved: While the game theory assumes rational clients seeking maximum benefit, malicious actors could spoof similarity to infiltrate high-value coalitions and introduce model poisoning.
- What evidence would resolve it: A robustness analysis simulating adversarial manipulation of gradient/model vectors within the affinity game to test coalition stability.

## Limitations
- The merge-blocking algorithm has O(K2^K) complexity, limiting scalability to large client populations without approximations
- The effectiveness of overall similarity metric depends heavily on hyperparameter tuning and may fail when task overlap is minimal
- Knowledge distillation assumes past models remain relevant teachers for new tasks, which may not hold in highly dynamic environments

## Confidence
- **High confidence**: The fundamental problem formulation (decentralized personalized FCL), the local training procedure with KD loss, and the general framework of client cooperation through coalition formation are well-specified and theoretically sound
- **Medium confidence**: The specific benefit quantification through "overall similarity" and the merge-blocking algorithm's implementation details are described but lack extensive empirical validation for convergence guarantees and equilibrium quality
- **Medium confidence**: The performance improvements over baselines are reported, but the ablation studies, while supportive, don't fully isolate the contribution of each mechanism (similarity metric, merge-blocking, KD)

## Next Checks
1. **Coalition Formation Validation:** On a small, controlled dataset (e.g., 4-5 clients, 2-3 tasks with known overlap), log the formed coalitions at each round. Verify they align with task similarity and evolve as expected, checking if the merge-blocking algorithm converges to reasonable equilibria.

2. **Baseline Comparison with Ablations:** Replicate key results from Table 1 and Table 2. Compare DCFCL to FedAvg, a decentralized clustering baseline, and DCFCL without each core mechanism (coalition evolution, KD, overall similarity) to confirm each component's contribution and isolate their effects.

3. **Hyperparameter Sensitivity Sweep:** Following Table 3, sweep ε ∈ [0, 1] and λ ∈ [0, 1] on one dataset (e.g., EMNIST-LTP) to find stable operating ranges and understand their interaction. Plot final accuracy vs. parameter values to identify robust configurations and potential failure modes.