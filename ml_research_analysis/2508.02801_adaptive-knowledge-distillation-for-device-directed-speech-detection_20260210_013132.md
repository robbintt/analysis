---
ver: rpa2
title: Adaptive Knowledge Distillation for Device-Directed Speech Detection
arxiv_id: '2508.02801'
source_url: https://arxiv.org/abs/2508.02801
tags:
- teacher
- student
- distillation
- ddsd
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving device-directed
  speech detection (DDSD) accuracy in on-device architectures with limited computing
  resources. The authors propose an adaptive knowledge distillation (aKD) approach
  that transfers knowledge from a large pre-trained ASR model (teacher) to a smaller
  DDSD model (student).
---

# Adaptive Knowledge Distillation for Device-Directed Speech Detection

## Quick Facts
- arXiv ID: 2508.02801
- Source URL: https://arxiv.org/abs/2508.02801
- Reference count: 0
- Key outcome: Achieves 26% relative EER reduction for keyword-based invocations and 19% for keyword-free invocations using adaptive knowledge distillation from ASR models

## Executive Summary
This paper addresses the challenge of improving device-directed speech detection (DDSD) accuracy in resource-constrained on-device architectures. The authors propose an adaptive knowledge distillation (aKD) approach that transfers knowledge from a large pre-trained ASR model to a smaller DDSD model through task-specific adapters. The method employs embedding distillation, attention regularization, and pseudo-labeling to effectively transfer knowledge while maintaining efficient on-device deployment. The approach demonstrates substantial improvements over conventional knowledge distillation and models without distillation, achieving significant EER reductions across both keyword-based and keyword-free invocation scenarios.

## Method Summary
The proposed adaptive knowledge distillation approach involves training task-specific adapters jointly with a smaller DDSD student model while leveraging a large pre-trained ASR model as the teacher. The method employs three key distillation techniques: embedding distillation to transfer semantic representations, attention regularization to align attention mechanisms between teacher and student, and pseudo-labeling to improve the student's decision boundaries. The adapters are trained to facilitate effective knowledge transfer while maintaining the student model's compact size suitable for on-device deployment. The approach is evaluated across transformer and conformer-based architectures, demonstrating consistent improvements in DDSD accuracy.

## Key Results
- Achieves 26% relative Equal Error Rate (EER) reduction for keyword-based invocations
- Achieves 19% relative EER reduction for keyword-free invocations
- Demonstrates effectiveness across both transformer and conformer-based model architectures

## Why This Works (Mechanism)
The approach works by effectively transferring knowledge from a large, well-trained ASR model to a smaller DDSD model through multiple complementary mechanisms. The embedding distillation captures semantic information from the teacher model's representations, while attention regularization ensures the student learns similar attention patterns. Pseudo-labeling helps refine the student's decision boundaries by providing additional training signal from the teacher's predictions. The task-specific adapters act as a bridge, enabling the student model to effectively utilize the transferred knowledge while maintaining its compact size for on-device deployment.

## Foundational Learning

**Device-Directed Speech Detection (DDSD)**: The task of identifying when a user is speaking to a voice-activated device versus background conversation or other speech. Essential for triggering device responses only when intended, preventing false activations.

*Why needed*: Without accurate DDSD, devices either miss genuine user commands or respond to unintended speech, degrading user experience.

*Quick check*: Test with both clear device-directed speech and natural conversation to verify the system only activates for intended commands.

**Knowledge Distillation**: A training technique where a smaller "student" model learns from a larger "teacher" model, typically by matching outputs or intermediate representations.

*Why needed*: Large ASR models provide rich representations but are too computationally expensive for on-device deployment, requiring knowledge transfer to smaller models.

*Quick check*: Compare student performance with and without distillation to quantify knowledge transfer benefits.

**Attention Regularization**: A technique that encourages the student model's attention patterns to match those of the teacher model during training.

*Why needed*: Helps the student model learn not just what to predict but how to attend to relevant parts of the input, improving generalization.

*Quick check*: Visualize attention maps from both models to verify alignment between teacher and student attention patterns.

## Architecture Onboarding

**Component map**: ASR Teacher Model -> Task-specific Adapters -> DDSD Student Model -> (Embedding Distillation + Attention Regularization + Pseudo-labeling)

**Critical path**: The student model training loop with adapter updates and all three distillation components operating simultaneously to optimize the student's parameters.

**Design tradeoffs**: The approach trades increased training complexity (multiple distillation techniques and adapters) for improved inference efficiency, as the final student model remains compact while achieving higher accuracy.

**Failure signatures**: Poor knowledge transfer from teacher to student, adapter overfitting, or ineffective distillation techniques that fail to improve student performance beyond baseline.

**First experiments**:
1. Train student model without any distillation techniques as baseline
2. Train with only embedding distillation to isolate its contribution
3. Train with only attention regularization to measure its individual impact

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions in the provided text.

## Limitations

- Evaluation primarily conducted on English-language datasets, raising questions about cross-lingual generalization
- Focus on specific transformer and conformer architectures limits understanding of performance with alternative model families
- Computational overhead from task-specific adapters and multiple distillation techniques needs quantification in real-world deployment scenarios
- Long-term robustness across different acoustic conditions and device types remains unverified

## Confidence

**Performance improvements claims (High confidence)**: The reported EER reductions of 26% and 19% for keyword-based and keyword-free invocations appear well-supported by the experimental results, with clear comparisons to baseline models.

**Generalizability claims (Medium confidence)**: While the approach shows effectiveness across two model architectures, the limited scope of model families and language coverage reduces confidence in broader applicability claims.

**Deployment feasibility claims (Medium confidence)**: The paper asserts practical on-device applicability, but lacks detailed analysis of memory footprint, inference latency, and power consumption trade-offs in actual deployment scenarios.

## Next Checks

1. Conduct cross-lingual experiments using non-English datasets to verify the approach's language independence and effectiveness across diverse linguistic patterns.

2. Perform ablation studies isolating the contributions of embedding distillation, attention regularization, and pseudo-labeling to better understand which components drive the most significant improvements.

3. Implement and benchmark the complete aKD pipeline on actual edge devices with varying computational constraints to validate the claimed efficiency benefits and identify potential bottlenecks in real-world deployment.