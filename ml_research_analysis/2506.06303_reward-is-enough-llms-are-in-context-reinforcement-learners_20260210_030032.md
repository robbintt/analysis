---
ver: rpa2
title: 'Reward Is Enough: LLMs Are In-Context Reinforcement Learners'
arxiv_id: '2506.06303'
source_url: https://arxiv.org/abs/2506.06303
tags:
- icrl
- reward
- learning
- task
- only
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that large language models (LLMs) exhibit
  reinforcement learning (RL)-like behavior during inference time through a simple
  multi-round prompting framework called ICRL prompting. The method guides LLMs to
  improve responses iteratively by providing scalar reward feedback for each attempt,
  without using textual gradients, sampling heuristics, or additional engineered modules.
---

# Reward Is Enough: LLMs Are In-Context Reinforcement Learners

## Quick Facts
- arXiv ID: 2506.06303
- Source URL: https://arxiv.org/abs/2506.06303
- Reference count: 40
- LLMs exhibit RL-like behavior during inference through scalar reward feedback, achieving state-of-the-art results across multiple benchmarks without parameter updates.

## Executive Summary
This paper introduces ICRL prompting, a framework that demonstrates large language models can learn and improve responses during inference time through scalar reward feedback. The method shows that LLMs can optimize reward signals without parameter updates, exploration heuristics, or additional modules, achieving superior performance compared to existing approaches like Best-of-N, Self-Refine, and Reflexion across diverse tasks including Game of 24, creative writing, and ScienceWorld. The approach works with both external and self-generated rewards, and across various open-source models.

## Method Summary
ICRL prompting guides LLMs to improve responses iteratively by providing scalar reward feedback for each attempt. The framework concatenates all prior responses and their associated rewards into the context, allowing the model to recognize patterns in which actions led to higher rewards and adjust subsequent responses accordingly. This creates a policy iteration process purely through forward passes, without parameter updates. The method tests both exploration-exploitation trade-offs through preset alternating modes and autonomous decision-making by the LLM itself.

## Key Results
- Game of 24: 90% success rate vs 49% for Best-of-N, 47% for Self-Refine
- Creative Writing: 59.48% win rate vs 86.32% vs Reflexion baseline
- ScienceWorld: 88% return vs 83% for Self-Refine
- Improvements persist even when rewards are generated by the same LLM
- Works across various open-source models including Qwen3-32B and Phi-4

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs can optimize scalar reward signals during inference without parameter updates, exhibiting RL-like behavior through in-context policy improvement.
- **Mechanism:** ICRL prompting concatenates prior responses and rewards into context, allowing models to recognize reward-response patterns and adjust accordingly through forward passes.
- **Core assumption:** Pretrained LLMs possess emergent ICRL capability that can be elicited without specialized training.
- **Evidence anchors:** Consistent performance improvement as context grows; related work on in-context Q-learning supports this direction.
- **Break condition:** Performance drops significantly with zero rewards, confirming learning signal necessity.

### Mechanism 2
- **Claim:** Scalar rewards alone are sufficient for inference-time self-improvement, without requiring textual gradients, verbal feedback, or engineered modules.
- **Mechanism:** Unlike Self-Refine or Reflexion relying on verbal self-feedback, ICRL uses only numerical scores, forcing genuine learning through reward-response correlation recognition.
- **Core assumption:** Reward-based learning is more robust than verbal self-feedback for tasks where evaluation is easier than generation.
- **Evidence anchors:** 90% success rate on Game of 24 vs 47% for Self-Refine; 86.32% win rate in creative writing.
- **Break condition:** Performance collapses with uninformative (random) rewards, though only zero-reward ablation is explicitly tested.

### Mechanism 3
- **Claim:** Exploration-exploitation trade-off emerges naturally through ICRL instructions, with alternating or autonomous selection yielding comparable performance.
- **Mechanism:** Both preset alternating strategies and autonomous LLM decision-making achieve similar performance, suggesting robust core learning mechanism.
- **Core assumption:** Model can meaningfully distinguish exploration from exploitation based on natural language instructions alone.
- **Evidence anchors:** Both ICRL Preset and Autonomous show similar performance curves; ablation confirms exploitation-only with rewards works but is suboptimal.
- **Break condition:** Severe context limitation (buffer length = 3) causes substantial performance drops, confirming accumulated experience necessity.

## Foundational Learning

- **Concept: Markov Decision Process (MDP) formulation**
  - **Why needed here:** The paper models LLM token generation as an RL process where state = generated tokens, action = next token, and reward = scalar feedback.
  - **Quick check question:** Can you explain why the paper describes token generation as an MDP and identify what constitutes the state, action, and reward in this formulation?

- **Concept: In-context learning vs. parameter updates**
  - **Why needed here:** ICRL's defining characteristic is that improvement occurs during inference (forward pass) without updating θ, distinguishing it from traditional RL and fine-tuning.
  - **Quick check question:** What is the difference between ICRL's in-context policy improvement and traditional RL's parameter-based policy updates?

- **Concept: Reward hypothesis and sparse vs. dense rewards**
  - **Why needed here:** The paper explicitly aligns with Sutton's reward hypothesis and tests both sparse (terminal only) and dense (step-level) rewards across benchmarks.
  - **Quick check question:** Why does ScienceWorld use the same reward function for context construction and evaluation, while Game of 24 uses different ones (r vs. r*)?

## Architecture Onboarding

- **Component map:** Task + Context Buffer + ICRL Instruction → LLM Policy → Response → Reward Function → Update Buffer

- **Critical path:**
  1. Initialize buffer B = ∅
  2. For each episode k: construct S0 = B + stask + sICRL
  3. Generate response via LLM forward pass
  4. Compute reward(s) via r
  5. Push (response, reward) tuple to B
  6. Repeat until K episodes or early stopping

- **Design tradeoffs:**
  - Context length vs. compute: Longer context enables more learning but increases per-iteration cost
  - Preset vs. Autonomous: Preset is deterministic but may not match task dynamics; Autonomous is flexible but adds variance
  - Self-evaluation vs. external reward: Self-evaluation enables broader applicability but may have lower ceiling due to model's own limitations

- **Failure signatures:**
  1. Context overflow: Early attempts get truncated as buffer grows, causing catastrophic forgetting
  2. Reward signal collapse: Uninformative rewards degrade performance to exploration-only baseline
  3. Hallucination accumulation: Self-evaluation rewards can be systematically biased
  4. Premature exploitation: Convergence to local optima if exploration is skipped

- **First 3 experiments:**
  1. Reproduce Game of 24 with GPT-4.1: Test both ICRL Preset and Autonomous for 50 episodes; verify success rate approaches 90%. Ablate zero-reward condition.
  2. Test on open-source model (Qwen3-32B or Phi-4) on Creative Writing: Replicate Table 4 results; measure win rate against baselines using Alpaca-Eval 2.
  3. Context length ablation on AIME/HMMT: Run ICRL with 8k/16k/32k contexts; measure solve rate to isolate compute-performance tradeoff.

## Open Questions the Paper Calls Out
None

## Limitations
- Context window bottleneck limits learning iterations, causing early attempts to get truncated and forgotten
- Performance improvements are task-dependent, with some benchmarks showing more modest gains
- Requires LLM to possess sufficient reasoning capability for both generation and evaluation
- Self-evaluation setup introduces potential bias as same model serves as both actor and critic

## Confidence
**High Confidence:** Core demonstration that scalar rewards enable inference-time improvement across multiple benchmarks and model families, with ablation studies confirming reward signal necessity.

**Medium Confidence:** Mechanism explanation that LLMs are genuinely "learning" through reward signals rather than simply exploiting accumulated context, though exact cognitive process remains unclear.

**Low Confidence:** Claim that this capability is "innate" to pretrained LLMs without requiring specialized training, as the mechanism by which pretraining enables this capability is not fully characterized.

## Next Checks
1. Test ICRL in multi-agent scenarios where multiple LLMs interact through reward signals to validate learning mechanism transfer and self-evaluation bias.
2. Systematically vary context length (4k, 8k, 16k, 32k) and plot learning curves to quantify forgetting problem and optimize exploration-exploitation tradeoff.
3. Test ICRL with identical reward functions across all three benchmark types to determine whether learning mechanism is domain-general or requires task-specific reward engineering.