---
ver: rpa2
title: Comparative Personalization for Multi-document Summarization
arxiv_id: '2509.21562'
source_url: https://arxiv.org/abs/2509.21562
tags:
- user
- documents
- profile
- style
- personalized
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating personalized multi-document
  summaries that cater to individual user preferences in writing style and content
  focus. The authors propose ComPSum, a framework that generates a structured analysis
  of a user's preferences by comparing their profile documents with those of other
  users on the same topic.
---

# Comparative Personalization for Multi-document Summarization

## Quick Facts
- arXiv ID: 2509.21562
- Source URL: https://arxiv.org/abs/2509.21562
- Authors: Haoyuan Li; Snigdha Chaturvedi
- Reference count: 26
- Primary result: ComPSum framework generates personalized multi-document summaries using comparative user profiles and achieves strong personalization scores while maintaining factuality

## Executive Summary
This paper addresses the challenge of generating personalized multi-document summaries that cater to individual user preferences in writing style and content focus. The authors propose ComPSum, a framework that generates a structured analysis of a user's preferences by comparing their profile documents with those of other users on the same topic. This structured analysis guides the generation of personalized summaries. To evaluate personalization without references, the authors introduce AuthorMap, a fine-grained reference-free evaluation framework based on authorship attribution. They also construct PerMSum, a personalized MDS dataset spanning news and review domains. Experiments on PerMSum using AuthorMap show that ComPSum outperforms strong baselines in personalization while maintaining factuality and relevance.

## Method Summary
ComPSum generates personalized multi-document summaries by first retrieving profile documents for a user using BM25 with the input document set as query, then retrieving comparative documents from the same topics but written by different users. An LLM generates structured analysis comparing these pairs, focusing separately on content and style dimensions. The summary generator then conditions on the retrieved profiles, structured analysis, and input documents to produce personalized summaries. The framework is evaluated using AuthorMap, a reference-free evaluation that measures personalization via authorship attribution accuracy. The PerMSum dataset contains 45K document sets and 5.3K users across news and review domains.

## Key Results
- ComPSum achieves an overall score of 74.54 compared to 71.32 for the best baseline
- Significant improvements in personalization metrics across both news and review domains
- Maintains factuality and relevance while improving personalization
- Ablation studies confirm importance of comparative documents and structured analysis

## Why This Works (Mechanism)

### Mechanism 1: Comparative documents reveal fine-grained user differences
- Claim: Comparative documents improve personalization by revealing fine-grained user differences
- Core assumption: When documents share the same topic, differences between them stem from personal preferences rather than topic differences
- Evidence: Ablation shows w/o comp. doc. achieves 73.19 overall vs ComPSum's 74.87; Table 7 shows comparative documents produce more diverse analysis

### Mechanism 2: Structured analysis guides generation more effectively
- Claim: Structured analysis (separate style and content dimensions) guides generation more effectively than unstructured profile summaries
- Core assumption: LLMs can independently modulate writing style and content focus when given explicit guidance for each dimension
- Evidence: w/o structure variant shows degraded performance (73.77 vs 74.87 average overall); related work supports style/content decomposition

### Mechanism 3: Authorship attribution as personalization proxy
- Claim: Authorship attribution can serve as a proxy for evaluating personalization quality without reference summaries
- Core assumption: A well-personalized summary contains recoverable signals of its intended user's preferences that an LLM can detect
- Evidence: Human evaluation shows AuthorMap achieves 73-80% accuracy compared to human judgments; Randolph's Kappa of 0.40 shows moderate annotator agreement

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG) for user profiles
  - Why needed: ComPSum retrieves top-k profile documents using BM25 with concatenated input documents as query
  - Quick check: Can you explain why the query for retrieval is the concatenated input document set rather than a single query?

- Concept: Authorship attribution and stylometry
  - Why needed: AuthorMap relies on the premise that writing style signals can identify authors
  - Quick check: What linguistic features would you examine to distinguish between two authors writing on the same topic?

- Concept: Reference-free evaluation for generation tasks
  - Why needed: Personalized summarization lacks ground-truth references; standard metrics cannot apply
  - Quick check: Why does AuthorMap use two summaries for two different users rather than evaluating a single summary in isolation?

## Architecture Onboarding

- Component map: Input document set -> BM25 retrieval (profiles + comparative docs) -> LLM analysis generator (content + style) -> LLM summary generator -> AuthorMap evaluation

- Critical path:
  1. Input document set D → retrieve k profile documents per user
  2. For each profile document → retrieve most dissimilar comparative document from same topic
  3. LLM generates structured analysis from profile/comparative pairs
  4. LLM generates personalized summary conditioned on analysis + profiles + D
  5. For evaluation: retrieve n profile documents using sᵤ₁ ∘ sᵤ₂ as query → LLM judge attribution

- Design tradeoffs:
  - Dissimilar vs. similar comparative documents: dissimilar shows slightly better performance (74.87 vs 74.00 avg)
  - Multi-stage vs. single-stage analysis: multi-stage shows worse results (72.02 avg)
  - Number of retrieved documents (m): m=5 outperforms m=2 and m=10

- Failure signatures:
  - Topic sparsity: Personalization fails when user's profile documents are all from one domain
  - Profile leakage: Low factuality indicates copying from profile rather than input documents
  - Positional bias in evaluation: Must swap summary order and take majority vote

- First 3 experiments:
  1. Validate retrieval strategy: Compare BM25 vs. embedding-based retrieval for profile and comparative document selection
  2. Ablate analysis structure: Run ComPSum with only style_analysis or only content_analysis to measure independent contributions
  3. Test domain transfer: Evaluate ComPSum trained on reviews when applied to news (and vice versa)

## Open Questions the Paper Calls Out
None

## Limitations
- The comparative document approach assumes differences between documents on the same topic primarily reflect user preferences rather than topic variation
- The PerMSum dataset construction methodology depends on clustering quality that isn't fully validated
- AuthorMap's assumption that attribution accuracy directly correlates with personalization quality needs more direct validation
- The absolute performance numbers lack comparison to human-level performance or other established benchmarks

## Confidence
- **High confidence**: Comparative documents mechanism is well-supported by ablation studies; structured analysis approach has moderate support
- **Medium confidence**: Authorship attribution evaluation framework shows reasonable correlation but needs more validation; dataset construction is plausible but depends on clustering quality
- **Low confidence**: Cross-domain robustness claims and specific numerical performance improvements require more extensive validation

## Next Checks
1. Validate comparative document quality by sampling and manually annotating profile/comparative document pairs to verify they are truly on the same topic
2. Test attribution accuracy correlation by generating summaries for the same user using different document sets and measuring attribution accuracy
3. Benchmark against human summaries by having human annotators write personalized summaries and comparing both human and model summaries using AuthorMap