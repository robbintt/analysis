---
ver: rpa2
title: Improving Task Diversity in Label Efficient Supervised Finetuning of LLMs
arxiv_id: '2507.21482'
source_url: https://arxiv.org/abs/2507.21482
tags:
- task
- tasks
- data
- examples
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a task-diversity-based approach for label-efficient
  supervised fine-tuning of large language models, addressing the challenge of reducing
  expensive human annotation while maintaining model performance. The method allocates
  annotation budgets across tasks using an inverse confidence weighting strategy,
  leveraging the fact that pre-trained models exhibit varying confidence levels across
  different tasks.
---

# Improving Task Diversity in Label Efficient Supervised Finetuning of LLMs

## Quick Facts
- arXiv ID: 2507.21482
- Source URL: https://arxiv.org/abs/2507.21482
- Authors: Abhinav Arabelly; Jagrut Nemade; Robert D Nowak; Jifan Zhang
- Reference count: 22
- Primary result: 50-80% annotation reduction with comparable performance

## Executive Summary
This paper introduces a task-diversity-based approach for label-efficient supervised fine-tuning of large language models, addressing the challenge of reducing expensive human annotation while maintaining model performance. The method allocates annotation budgets across tasks using an inverse confidence weighting strategy, leveraging the fact that pre-trained models exhibit varying confidence levels across different tasks. Experiments on FLAN V2 and Dolly datasets demonstrate that the Weighted Task Diversity method achieves performance comparable to or better than training on the full dataset while using 50-80% fewer annotations.

## Method Summary
The approach consists of two algorithms: Task Diversity (uniform allocation across tasks) and Weighted Task Diversity (confidence-informed allocation). For each task, average model confidence is computed across all prompts. Budget allocation follows inverse confidence weighting with clamping to ensure minimum task coverage. Round-robin sampling guarantees task diversity while respecting budget constraints. The method is simpler and more computationally efficient than existing diversity-based approaches while producing superior or equivalent results.

## Key Results
- Weighted Task Diversity achieves 4% increase in MMLU score compared to full-dataset baseline
- 53-54% win rates in AlpacaEval comparisons against full-dataset baseline
- Achieves comparable performance with 50-80% fewer annotations

## Why This Works (Mechanism)

### Mechanism 1: Task-level budget allocation ensures semantic coverage
Distributing annotation budget across predefined task categories provides better coverage than embedding-space diversity alone. The method partitions prompts into task groups and solves a min-max allocation problem ensuring each task receives budget proportional to importance.

### Mechanism 2: Inverse confidence weighting targets uncertain tasks
Allocating more annotation budget to tasks where the pre-trained model exhibits lower confidence improves downstream performance more efficiently. Budget allocation follows α_t ∝ 1/conf_t, clamped to valid bounds.

### Mechanism 3: Base allocation + round-robin prevents task starvation
Guaranteeing minimum representation per task while applying weighted allocation prevents complete neglect of any task category. Each task receives a base budget before weighted distribution, with round-robin sampling iteratively allocating across tasks.

## Foundational Learning

- **Label-efficient learning vs. data selection**: Why needed? The paper explicitly distinguishes these problems—label-efficient learning operates WITHOUT ground-truth responses; data selection methods that use ground truth are inapplicable. Quick check: Why can't methods like Alpagasus be directly applied to label-efficient SFT?

- **One-batch active learning**: Why needed? This paper frames the problem as selecting a single subset for annotation rather than iterative cycles, which has practical advantages for LLM training costs. Quick check: What are the tradeoffs between one-batch vs. iterative active learning for LLM finetuning?

- **Confidence estimation for generative models**: Why needed? The method requires computing conf(x) = ∏ g(y_j|y_{<j}, x)—product of token probabilities across the generated sequence. Quick check: How would numerical underflow affect confidence computation for long sequences?

## Architecture Onboarding

- **Component map**: [Dataset with task metadata] → Task Partitioning (X₁...X_T) → [Base model inference] → Per-prompt confidence scores → [Aggregation] → Task-level avg confidence (conf_t) → [Allocation solver] → Budget per task (α_t) → [Round-robin sampler] → Selected subset S (|S|=k) → [Annotation + LoRA SFT] → Finetuned model g'

- **Critical path**: Extract task labels → Run base model inference for confidence computation → Aggregate to task-level confidence → Solve allocation with base allocation and inverse weighting → Round-robin sampling → Annotate and finetune with LoRA

- **Design tradeoffs**: Base allocation (5) vs. adaptive flexibility; random within-task vs. embedding-based selection; full inference vs. proxy confidence; numerical underflow in confidence product

- **Failure signatures**: Tasks with |X_t| < 5 hit ceiling constraint; small budgets make allocation nearly uniform; confidence may reflect pre-training exposure; numerical underflow for long sequences

- **First 3 experiments**: 1) Base allocation ablation: test base_budget ∈ {1, 3, 5, 10} on FLAN 90K at k=30K; 2) Confidence calibration check: correlate task-level conf_t with actual performance gains; 3) Within-task selection strategy: compare random vs. K-Center within tasks

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the method perform across different model architectures and parameter scales beyond LLaMA-2 7B? The authors note extending findings to other architectures represents a promising direction.

- **Open Question 2**: Can automated task categorization methods be developed for datasets lacking explicit task labels? The authors suggest automated methods for identifying task definitions could be beneficial where categorizations are less defined.

- **Open Question 3**: To what extent does base model confidence reflect inherent task difficulty versus pre-training data exposure? The authors acknowledge confidence assessment may sometimes reflect pre-training exposure rather than inherent difficulty.

## Limitations

- Reliance on pre-existing task metadata limits applicability to datasets without clear task categorization
- Confidence-based weighting assumes model uncertainty correlates with learning potential, which is not empirically validated
- Computational overhead of full-model inference on all prompts may become prohibitive at larger scales

## Confidence

- **High confidence**: Mathematical formulation of both allocation algorithms and reported performance improvements are reproducible
- **Medium confidence**: Core claim that inverse confidence weighting improves label efficiency rests on unproven assumption about confidence-task difficulty correlation
- **Low confidence**: Claim about being "simpler and more computationally efficient than existing diversity-based approaches" lacks direct comparison to specific baseline methods

## Next Checks

1. **Confidence correlation validation**: Run controlled experiments correlating pre-SFT task confidence scores with actual performance gains on held-out examples per task to verify inverse confidence weighting assumption.

2. **Computational overhead analysis**: Measure wall-clock time and GPU memory requirements for confidence computation at different dataset scales (10K, 50K, 100K examples) to establish practical scalability limits.

3. **Task taxonomy robustness**: Test the method's performance when task categories are artificially degraded (merged, split, or randomly assigned) to quantify sensitivity to task definition quality.