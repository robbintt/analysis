---
ver: rpa2
title: 'MRSD: Multi-Resolution Skill Discovery for HRL Agents'
arxiv_id: '2505.21410'
source_url: https://arxiv.org/abs/2505.21410
tags:
- skill
- skills
- state
- learning
- manager
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Multi-Resolution Skill Discovery (MRSD),
  a hierarchical reinforcement learning framework that learns multiple skill encoders
  at different temporal resolutions in parallel. Inspired by human motor control,
  MRSD uses a high-level manager to dynamically select among these skills, enabling
  adaptive control strategies over time.
---

# MRSD: Multi-Resolution Skill Discovery for HRL Agents

## Quick Facts
- **arXiv ID**: 2505.21410
- **Source URL**: https://arxiv.org/abs/2505.21410
- **Reference count**: 40
- **Primary result**: MRSD learns multiple skill encoders at different temporal resolutions in parallel, outperforming state-of-the-art hierarchical and skill discovery methods on DeepMind Control Suite tasks.

## Executive Summary
This paper introduces Multi-Resolution Skill Discovery (MRSD), a hierarchical reinforcement learning framework that learns multiple skill encoders at different temporal resolutions in parallel. Inspired by human motor control, MRSD uses a high-level manager to dynamically select among these skills, enabling adaptive control strategies over time. The method trains separate skill CVAEs for each temporal resolution, sharing common layers to limit model growth. Experiments on DeepMind Control Suite tasks show MRSD outperforms state-of-the-art hierarchical and skill discovery methods, achieving faster convergence and higher final performance.

## Method Summary
MRSD builds on the Director architecture, extending it with N parallel Conditional VAEs that learn state transitions at distinct temporal distances (l ∈ {64, 32, 16, 8}). Each skill CVAE predicts reachable states s_{t+l} from current state s_t, with shared encoder/decoder layers preventing model explosion while resolution-specific layers maintain distinct skill spaces. The manager policy contains N+1 heads—N skill heads and a choice head—that learn expert policies for each skill and select the best skill for each state. A reconstruction-based exploration reward encourages discovery of novel state transitions without external rewards. The worker abstraction K=8 creates temporal abstraction by persisting goals across 8 steps.

## Key Results
- MRSD outperforms Director, DreamerV2, DIAYN, and ReST on 5 DeepMind Control Suite tasks with faster convergence and higher final performance
- Dynamic skill interleaving via learned choice head provides significant benefits over using individual skills or random selection (Fig. 7)
- Exploration-only phase learns usable skills without external rewards; fine-tuning succeeds on sparse-reward ant maze task
- Ablation confirms no single skill resolution dominates all tasks, validating temporal resolution partitioning

## Why This Works (Mechanism)

### Mechanism 1: Temporal Resolution Partitioning
Constraining skill predictions to achievable temporal horizons improves search efficiency for appropriate goal states. Separate CVAEs learn state transitions at distinct temporal distances, each predicting reachable states s_{t+l} from current state s_t. Shared encoder/decoder layers prevent model explosion while resolution-specific layers maintain distinct skill spaces. Shorter horizons yield precise but error-susceptible movements, longer horizons yield smoother but imprecise movements.

### Mechanism 2: Dynamic Skill Interleaving via Choice Head
A learned choice policy outperforms fixed single-skill or random selection strategies. Manager policy outputs N skill latent distributions and one choice distribution. Choice head selects among candidate subgoals via gating, learning state-conditional selection while skill heads independently specialize. No single temporal resolution dominates across all states/tasks; optimal resolution varies contextually.

### Mechanism 3: Reconstruction-Based Exploration Reward
CVAE reconstruction error provides effective intrinsic motivation for discovering novel state transitions without external rewards. Exploratory reward encourages the agent to repeat state transitions that are not yet well-learned. Poorly-learned transitions yield high error, encouraging repetition until learned. Reconstruction error correlates with novelty of state transition.

## Foundational Learning

- **Conditional Variational Autoencoders (CVAEs)**: Core architecture for encoding state transitions (s_t, s_{t+l}) → latent skill z → reconstructed s_{t+l}. ELBO objective balances reconstruction accuracy with latent space regularization.
  - Quick check: Can you explain why the KL divergence term in Eq. 1 prevents the encoder from memorizing individual state pairs?

- **Hierarchical Reinforcement Learning with Temporal Abstraction**: Manager operates at coarse timescale (every K=8 steps), worker at fine timescale. Goal persistence across K steps creates temporal abstraction.
  - Quick check: How does the trajectory probability decomposition in Eq. 6 separate manager and worker contributions?

- **Policy Gradient with Baseline (REINFORCE)**: Manager and worker policies trained via advantage estimates using lambda returns. Choice variable appears as exponent in trajectory probability, affecting gradient flow to unselected skills.
  - Quick check: Why does the choice variable c_{k,i} appear as an exponent in Eq. 6, and how does this affect gradient flow to unselected skills?

## Architecture Onboarding

- **Component map**: RSSM → latent state s_t → N parallel Skill CVAEs (shared trunk + resolution-specific heads) → Manager policy (N skill heads + 1 choice head) → Worker policy (goal-conditioned) → Environment

- **Critical path**: 1) Collect trajectory data → extract state pairs at all resolutions (s_t, s_{t+l_i}) 2) Train Skill CVAEs jointly on all pairs 3) Imagine trajectories using world model + current policy 4) Compute external + exploratory rewards → update manager via policy gradient 5) Compute goal rewards → update worker

- **Design tradeoffs**: More skill resolutions → finer control but reduced learning signal per head. Strong external reward weight (1.0) vs exploratory (0.1) biases skills toward task-relevant states. Shared vs separate CVAE layers: shared limits model growth but may cause interference.

- **Failure signatures**: CVAE collapse (manager always selects unconditional VAE), skill under-utilization (single skill dominates >90% selection), exploration stagnation (reconstruction error plateaus before diverse behaviors emerge)

- **First 3 experiments**: 1) Single-skill baseline with N=1 at l=64 to establish Director-comparable baseline 2) Ablation grid testing each skill resolution independently on 3 diverse DMC tasks 3) Interleaving validation comparing learned choice vs random selection vs best-single-skill

## Open Questions the Paper Calls Out

- **Scaling to many skill heads**: Method cannot be scaled to arbitrary number of skill heads as it reduces per skill learning signal. No mitigation strategies proposed.

- **Universal exploration reward**: Current VAE reconstruction-based exploration reward does not perform well on cheetah_run task, indicating no single reward scheme is sufficient for all tasks.

- **Optimal temporal skill resolution selection**: Authors use fixed resolutions [64, 32, 16, 8, ∞] but provide no systematic method for determining optimal set of temporal skill resolutions for a given task.

## Limitations

- Architecture details underspecified, particularly layer-sharing patterns between shared and resolution-specific CVAE components
- Reconstruction-based exploration may fail when CVAE capacity is insufficient or overfits, leading to spurious exploration signals
- All experiments use image observations without proprioceptive inputs, limiting generalization to real-robot or proprioceptive-only domains

## Confidence

- **High confidence**: Multi-resolution skill learning framework is novel and well-implemented; experimental results show consistent improvements over baselines
- **Medium confidence**: Dynamic skill interleaving via choice head provides measurable benefits; ablation confirms no single skill dominates all tasks
- **Low confidence**: Reconstruction-based exploration reward is universally effective; paper notes limitations on cheetah_run with no comparative analysis

## Next Checks

1. **Cross-task skill resolution analysis**: Systematically measure which skill resolution (l=8,16,32,64,∞) is selected most often per state/task to validate claim that optimal temporal abstraction varies contextually

2. **Alternative exploration comparison**: Replace reconstruction-based exploration with standard curiosity bonus (e.g., prediction error in latent space) and compare performance on tasks where reconstruction exploration underperformed

3. **Generalization to proprioceptive inputs**: Evaluate MRSD on DMC tasks using only proprioceptive state (no images) to test whether temporal resolution partitioning remains beneficial when pixel reconstruction is not required