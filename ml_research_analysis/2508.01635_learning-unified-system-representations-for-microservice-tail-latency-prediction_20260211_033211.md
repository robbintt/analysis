---
ver: rpa2
title: Learning Unified System Representations for Microservice Tail Latency Prediction
arxiv_id: '2508.01635'
source_url: https://arxiv.org/abs/2508.01635
tags:
- latency
- resource
- system
- usrfnet
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles the challenge of predicting window-level P95\
  \ tail latency in microservice systems, a critical task for proactive resource management.\
  \ The authors identify that existing methods inadequately handle heterogeneous system\
  \ features\u2014traffic-side interactions and resource-side utilization\u2014leading\
  \ to suboptimal performance."
---

# Learning Unified System Representations for Microservice Tail Latency Prediction

## Quick Facts
- **arXiv ID**: 2508.01635
- **Source URL**: https://arxiv.org/abs/2508.01635
- **Reference count**: 17
- **Primary result**: USRFNet achieves state-of-the-art accuracy for window-level P95 tail latency prediction in microservice systems, outperforming both traditional ML models and advanced GNN-based approaches.

## Executive Summary
This paper addresses the challenge of predicting window-level P95 tail latency in microservice systems, a critical task for proactive resource management. The authors identify that existing methods inadequately handle heterogeneous system features—traffic-side interactions and resource-side utilization—leading to suboptimal performance. To address this, they propose USRFNet, a dual-stream deep learning architecture that separately models traffic-side features using Graph Neural Networks and resource-side features using gated MLPs, then fuses these representations via a Hierarchical Integration of Demand and Capacity (HIDAC) module. Evaluated on large-scale real-world benchmarks (Online Boutique and Sockshop), USRFNet achieves state-of-the-art accuracy, outperforming both traditional ML models and advanced GNN-based approaches with significant reductions in MAPE, MAE, and RMSE. The results validate the importance of explicitly modeling and fusing heterogeneous system features for accurate tail latency prediction.

## Method Summary
USRFNet is a dual-stream deep learning architecture for microservice tail latency prediction. It employs GNNs to capture service interactions and workload propagation (traffic-side), while gMLP modules independently model cluster resource dynamics (resource-side). These separate representations are fused via a Hierarchical Integration of Demand and Capacity (HIDAC) module, which uses Cross-Diffusion-Attention and Low-Rank Tensor Fusion to capture multiplicative demand-capacity interactions. The model predicts window-level P95 latency using an asymmetric percentage Huber loss that penalizes under-prediction more heavily. Training uses Adam optimizer with learning rate 1e-3, batch size 32, and 500 epochs on two real-world benchmarks collected via Istio/Prometheus.

## Key Results
- USRFNet achieves state-of-the-art performance on both Online Boutique and Sockshop benchmarks, significantly outperforming traditional ML models and GNN-based approaches.
- The ablation study demonstrates that both the dual-stream architecture and HIDAC module contribute measurable gains in prediction accuracy.
- Window-level P95 tail latency prediction provides a stable, operationally meaningful signal compared to volatile per-request metrics.

## Why This Works (Mechanism)

### Mechanism 1
Separating traffic-side and resource-side features into independent encoders prevents signal interference and improves prediction accuracy. Traffic metrics exhibit event-driven, volatile dynamics that propagate across service dependencies, while resource metrics evolve slowly and reflect localized bottlenecks. Processing both through a single GNN stream homogenizes these distinct dynamics, corrupting representations. A dual-stream architecture isolates each modality before fusion.

Core assumption: Traffic and resource features have fundamentally different temporal behaviors and propagation patterns; modeling them together degrades learning.

Evidence anchors:
- [abstract] "USRFNet employs GNNs to capture service interactions... while gMLP modules independently model cluster resource dynamics."
- [section: Motivation → Why a Dual-Stream Architecture?] "Traffic-side metrics are event-driven and volatile... resource-side metrics evolve more slowly... When processed together in a unified stream, these mismatched dynamics create signal interference."
- [section: Experiments → Ablation Study] "GNN-Fusion variant... where the gMLP-based resource encoder is replaced with a GNN-based architecture... leads to worse performance than the original Resource-Only model."

Break condition: If resource metrics also exhibit strong dependency-graph propagation (e.g., shared infrastructure causing correlated bottlenecks), the strict separation assumption may not hold.

### Mechanism 2
Hierarchical fusion via Cross-Diffusion-Attention followed by Low-Rank Tensor Fusion captures multiplicative demand-capacity interactions critical to tail latency dynamics. Demand-capacity relationships are non-linear: small demand increases under scarce capacity cause disproportional latency spikes. Cross-Diffusion-Attention enriches each embedding with cross-modal context. Low-Rank Tensor Fusion then models high-order interactions via element-wise products in a shared low-dimensional space, preserving multiplicative structure efficiently.

Core assumption: Latency emerges from multiplicative (not additive) interactions between demand and capacity; naive concatenation or addition fails to capture this structure.

Evidence anchors:
- [section: The HIDAC Module] "Demand-capacity interactions are multiplicative rather than additive (e.g., when capacity is scarce, a small increase in demand can cause a disproportional, explosive increase in latency)."
- [section: Experiments → Ablation Study] Simple-Fusion variant "replaces HIDAC with a basic element-wise addition strategy" and "performs notably worse, even falling short of the Resource-Only baseline."

Break condition: If demand-capacity relationships are predominantly linear in the target system, the added complexity of HIDAC may not justify its cost over simpler fusion strategies.

### Mechanism 3
Predicting window-level P95 latency provides a stable, operationally meaningful signal compared to volatile per-request latency predictions. Per-request latency is highly sensitive to transient noise (network delays, momentary CPU spikes). Aggregating to window-level P95 (30-second intervals) smooths outliers while capturing tail-end user experience and system degradation trends, producing a more learnable target for supervised regression.

Core assumption: The operational value of predictions lies in system-wide trends rather than individual request accuracy; transient noise obscures underlying performance patterns.

Evidence anchors:
- [abstract] "Traditional approaches often rely on per-request latency metrics, which are highly sensitive to transient noise... window-level P95 tail latency provides a stable and meaningful signal."
- [section: Motivation → Why Window-Level P95 Latency?] "Raw request latencies are highly volatile and noisy... window-level P95 latency... smooths out transient noise, revealing the underlying performance trend."

Break condition: If operational decisions require sub-second granularity (e.g., real-time request routing), window-level aggregation may introduce unacceptable lag.

## Foundational Learning

- **Graph Neural Networks (GNNs) for service dependency modeling**: Why needed here - Traffic-side encoder uses Transformer-based Graph Convolution to propagate workload features across microservice call graphs. Quick check question - Can you explain how message passing differs from standard MLP processing for graph-structured data?

- **Gated MLP (gMLP) and Spatial Gating Units**: Why needed here - Resource-side encoder uses gMLP to model capacity metrics without imposing graph structure, enabling flexible non-local interaction modeling. Quick check question - What inductive bias does a Spatial Gating Unit introduce compared to standard MLP layers?

- **Cross-Attention mechanisms**: Why needed here - HIDAC uses Cross-Diffusion-Attention to let traffic embeddings query resource embeddings (and vice versa) before fusion. Quick check question - How does cross-attention differ from self-attention in terms of input sources and information flow?

## Architecture Onboarding

- **Component map**: Construct static state graph G = (V, E) from microservice topology → Collect node features X(i), edge features E(i), resource features R(i), ground-truth y(i) → Traffic-Side Encoder (Transformer-based GNN layers → Attention pooling) → z_t → Resource-Side Encoder (gMLP blocks with Spatial Gating Units → Global average pooling) → z_r → HIDAC Module (Cross-Diffusion-Attention → Low-Rank Tensor Fusion) → z_f → Prediction Head (MLP) → ŷ → Asymmetric Percentage Huber Loss

- **Critical path**: 1) Construct static state graph G = (V, E) from microservice topology. 2) For each time window i, collect node features X(i), edge features E(i), resource features R(i), and ground-truth y(i). 3) Pass X(i), E(i) through Traffic-Side Encoder → z_t. 4) Pass R(i) through Resource-Side Encoder → z_r. 5) Fuse z_t, z_r via HIDAC → z_f. 6) Predict ŷ via MLP head; compute Asymmetric Percentage Huber Loss.

- **Design tradeoffs**: Dual-stream vs Single-stream prevents signal interference but doubles encoder complexity; HIDAC vs Simple Fusion captures multiplicative interactions but adds attention overhead; window size (30s) balances stability vs responsiveness; asymmetric loss encourages conservative predictions but may lead to over-provisioning.

- **Failure signatures**: Conflated features cause volatile predictions and degraded accuracy (per ablation GNN-Single results); flat-vector baselines fail to capture dependency propagation, yielding high MAPE (>15%); simple element-wise addition underperforms Resource-Only baseline; standard MSE loss may overfit to high-latency outliers.

- **First 3 experiments**: 1) Baseline comparison: Train Linear, MLP, GBDT, GRAF, GIN+, GatedGCN+ on identical features; confirm USRFNet achieves lowest MAPE/MAE/RMSE. 2) Ablation study: Compare Traffic-Only, Resource-Only, Simple-Fusion, GNN-Fusion, and full USRFNet; verify HIDAC and dual-stream design each contribute measurable gains. 3) Cross-application generalization: Train on Online Boutique, test on Sockshop; assess whether learned representations transfer across topologies.

## Open Questions the Paper Calls Out

- Can the USRFNet unified system embedding be effectively adapted for proactive anomaly detection and root cause localization? [explicit] The Conclusion explicitly states, "Our future work will explore its application to broader AIOps tasks such as proactive anomaly detection, and root cause localization."

## Limitations

- The paper's claims about the superiority of the dual-stream architecture and HIDAC module rest on comparisons against a relatively narrow set of baselines.
- The evidence for the core assumption—that traffic and resource features have fundamentally different dynamics—is largely internal and indirect.
- The choice of window-level P95 as the prediction target, while operationally justified, may not generalize to systems with different latency characteristics or where per-request granularity is critical.

## Confidence

- **High Confidence**: The experimental results showing USRFNet's state-of-the-art performance on real-world benchmarks are robust, with clear numerical improvements across multiple metrics.
- **Medium Confidence**: The ablation study supports the importance of the dual-stream architecture and HIDAC module, but the internal evidence is not fully corroborated by external literature.
- **Low Confidence**: The foundational assumption that traffic and resource features must be processed separately is plausible but not definitively proven; alternative architectures or fusion strategies may yield similar results.

## Next Checks

1. **Cross-Topology Generalization**: Train USRFNet on one microservice benchmark (e.g., Online Boutique) and evaluate on a different topology (e.g., Sockshop) to assess whether the learned representations transfer across system architectures.

2. **Alternative Fusion Strategies**: Replace HIDAC with a simple concatenation or addition baseline and compare performance to quantify the marginal benefit of multiplicative fusion.

3. **Temporal Granularity Analysis**: Vary the window size (e.g., 10s, 30s, 60s) and assess the impact on prediction accuracy and operational utility to determine the optimal aggregation level for different use cases.