---
ver: rpa2
title: 'Preconditioned Sharpness-Aware Minimization: Unifying Analysis and a Novel
  Learning Algorithm'
arxiv_id: '2501.06603'
source_url: https://arxiv.org/abs/2501.06603
tags:
- infosam
- learning
- adversarial
- asam
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a preconditioning-based framework termed preSAM
  to unify existing sharpness-aware minimization (SAM) variants and enable principled
  algorithm design. The framework categorizes SAM variants as objective preconditioning
  (OP) or constraint preconditioning (CP) based on where preconditioning is applied.
---

# Preconditioned Sharpness-Aware Minimization: Unifying Analysis and a Novel Learning Algorithm

## Quick Facts
- arXiv ID: 2501.06603
- Source URL: https://arxiv.org/abs/2501.06603
- Reference count: 40
- Primary result: Preconditioning framework unifies SAM variants and InfoSAM improves generalization by 0.2%-0.5% on CIFAR10/100

## Executive Summary
This paper introduces a preconditioning-based framework (PreSAM) that unifies existing Sharpness-Aware Minimization (SAM) variants and enables principled algorithm design. The framework categorizes SAM methods as objective preconditioning (OP) or constraint preconditioning (CP) based on where preconditioning is applied. Building on this foundation, the authors propose InfoSAM, a novel OP algorithm that addresses adversarial model degradation by adjusting gradients based on noise estimates. InfoSAM achieves superior performance across CIFAR-10/100 and ImageNet benchmarks, outperforming standard SAM and other baselines in generalization accuracy.

## Method Summary
The method reformulates SAM's perturbation step into a generalized maximization problem using preconditioning matrices $C_t$ and $D_t$. Objective preconditioning modifies the gradient direction, while constraint preconditioning reshapes the constraint set. InfoSAM specifically implements objective preconditioning by estimating gradient noise variance using the squared difference between current gradients and their exponential moving average (EMA). The perturbation is then scaled inversely proportional to this noise estimate, suppressing harmful stochastic gradient noise that can degrade the adversarial perturbation step. The algorithm maintains an EMA of gradients and constructs a diagonal preconditioner based on variance estimates to compute noise-adjusted perturbations.

## Key Results
- InfoSAM achieves 0.2%-0.5% higher accuracy than standard SAM on CIFAR-10/100
- InfoSAM reduces adversarial model degradation (AMD) by suppressing stochastic gradient noise
- Theoretical convergence analysis reveals OP methods offer more design flexibility than CP variants
- InfoSAM outperforms multiple baselines on ImageNet with ResNet-50

## Why This Works (Mechanism)

### Mechanism 1
Preconditioning matrices ($C_t, D_t$) unify diverse SAM variants by altering perturbation geometry. The method reformulates SAM's perturbation step into a generalized maximization problem where $C_t$ distorts the objective gradient direction and $D_t$ reshapes the constraint set. By selecting specific matrices, one can recover existing methods (e.g., setting $C_t = \text{diag}(|g_t|^{-1})$ recovers $\ell_\infty$-SAM) or create new ones. The core assumption is that loss landscape geometry can be effectively navigated by linearly transforming gradient and constraint spaces without invalidating the minimax objective of finding flat minima.

### Mechanism 2
InfoSAM improves generalization by suppressing stochastic gradient noise to prevent "Adversarial Model Degradation" (AMD). Standard SAM calculates perturbations using noisy stochastic gradients, potentially pushing the model into uninformative regions. InfoSAM estimates gradient noise variance using the squared difference between the current gradient and its EMA, then scales the perturbation inversely proportional to this noise (high noise → small perturbation). The core assumption is that gradient noise is harmful to the perturbation step, and EMA provides a sufficient proxy for the true gradient to estimate this noise variance.

### Mechanism 3
Constraint Preconditioning (CP) risks slowing convergence compared to Objective Preconditioning (OP). Theoretical analysis reveals the convergence rate is explicitly bounded by the norm of the inverse constraint preconditioner ($\|D_t^{-1}\|$). Methods like ASAM (which uses $D_t$) can suffer if weights become small. OP methods (like InfoSAM, which uses $C_t$) do not explicitly appear in the convergence rate bound, offering more design flexibility. The core assumption is that standard non-convex optimization assumptions hold (Lipschitz gradients, bounded variance).

## Foundational Learning

- **Concept: Minimax Optimization (SAM formulation)**
  - Why needed here: The entire paper reframes the "flatness" objective as a minimax problem (minimizing loss while maximizing perturbation impact). Understanding this duality is required to grasp why preconditioning changes the "adversarial" model.
  - Quick check question: Can you explain why SAM seeks to maximize the loss in a local neighborhood before minimizing it?

- **Concept: Preconditioning in Optimization**
  - Why needed here: This is the core mathematical tool used to categorize algorithms. Without understanding how a matrix transforms a vector space (gradient), the distinction between Objective Preconditioning (OP) and Constraint Preconditioning (CP) is opaque.
  - Quick check question: How does multiplying the gradient by a diagonal matrix change the direction of the update step compared to multiplying the constraint norm?

- **Concept: Signal-to-Noise Ratio (SNR) in Stochastic Gradients**
  - Why needed here: The motivation for InfoSAM is entirely built on the observation that stochastic gradients have low SNR, causing the "adversarial" perturbation to be noisy rather than informative.
  - Quick check question: In a minibatch, does the gradient point exactly toward the true minimum? If not, how does that error affect a perturbation-based method like SAM?

## Architecture Onboarding

- **Component map:** Input (minibatch $\mathcal{B}_t$) -> Noise Estimator (computes $g_t$, updates EMA $m_t$, derives variance estimate $\hat{\sigma}^2_t$) -> Preconditioner (constructs diagonal matrix $\hat{\Sigma}_t^{-1}$ using noise estimate) -> Perturbation Engine (solves $\epsilon_t = \rho \hat{\Sigma}_t^{-1} g_t / \|\hat{\Sigma}_t^{-1} g_t\|$) -> Optimizer (updates weights $x_{t+1} = x_t - \eta g_t(x_t + \epsilon_t)$)

- **Critical path:** The estimation of noise variance (Eq. 7b) and the subsequent scaling of the gradient. If the variance estimate is inaccurate, the preconditioner will either dampen informative directions or amplify noisy ones.

- **Design tradeoffs:**
  - CP vs. OP: CP (e.g., ASAM) requires careful tuning of $\rho$ relative to model weights and may diverge; OP (e.g., InfoSAM) is theoretically more flexible but requires maintaining an EMA of gradients (extra memory overhead)
  - EMA decay ($\alpha$): A small $\alpha$ makes the noise estimate noisy; a large $\alpha$ makes it lag behind the true gradient distribution

- **Failure signatures:**
  - Exploding Perturbations: In CP methods, if weights ($x_t$) approach zero, $D_t$ (which depends on $x_t^{-1}$) explodes
  - Stagnation: If InfoSAM dampens perturbations too aggressively (overestimated noise), the model may revert to simple SGD behavior and fail to find flat minima

- **First 3 experiments:**
  1. Baseline Validation (CIFAR10): Train ResNet-18 using InfoSAM vs. Vanilla SAM to verify the reported 0.2% - 0.5% accuracy lift using hyperparameters from Table IV
  2. Noise Robustness Test: Train on CIFAR10 with 50% label noise (Fig 2) to confirm that InfoSAM's noise estimation actively corrects perturbation directions better than SAM
  3. Ablation on $\rho$: Sweep the perturbation radius $\rho$ for ASAM (CP) vs. InfoSAM (OP) on CIFAR100 to validate the claim that CP is more sensitive to $\rho$ tuning (Page 3)

## Open Questions the Paper Calls Out
1. **Can infoSAM be effectively combined with constraint preconditioning (CP) methods like ASAM and FisherSAM?**
   - Basis in paper: [explicit] The authors explicitly state that using infoSAM jointly with CP methods "has been added to our future research agenda" (Section IV.B)
   - Why unresolved: InfoSAM is developed as an objective preconditioning (OP) method; it is unclear if its noise-adjusted perturbations conflict with the geometrical constraints imposed by CP methods
   - What evidence would resolve it: Empirical results demonstrating convergence and accuracy when infoSAM's noise estimation is integrated into the constraint definitions of ASAM or FisherSAM

2. **How can the momentum coefficient $\alpha$ for noise estimation be theoretically determined or adapted automatically?**
   - Basis in paper: [inferred] Tables IV, V, and VI show that $\alpha$ is tuned differently for various architectures (ranging from 0.001 to 0.05), but the text provides no theoretical justification or adaptive rule for this hyperparameter
   - Why unresolved: The performance of infoSAM relies on the accuracy of the EMA noise estimate, which is highly sensitive to $\alpha$; a fixed value may not generalize well to all training regimes
   - What evidence would resolve it: A theoretical analysis bounding the error of the EMA estimate relative to $\alpha$, or an adaptive scheme that adjusts $\alpha$ dynamically based on gradient statistics

3. **Does the empirical variance estimator $\hat{\Sigma}_t$ used in infoSAM guarantee the bounded preconditioner norm required for convergence?**
   - Basis in paper: [inferred] Theorem 1 requires $\|D_t^{-1}\|$ to be bounded, but Section IV.B defines infoSAM using the inverse of the estimated variance ($\hat{\Sigma}_t^{-1}$), which could theoretically become unbounded if the estimated variance approaches zero
   - Why unresolved: While the paper proves convergence for the general framework, it does not verify if the specific noise estimator in (7b) satisfies the boundedness assumption during practical training
   - What evidence would resolve it: A proof showing that the variance estimate is strictly lower-bounded by a positive constant, or analysis showing convergence holds despite the instability of the inverse

## Limitations
- The theoretical analysis focuses on a simplified single-step quadratic approximation, which may not fully capture practical training dynamics
- Empirical validation relies on standard benchmarks (CIFAR/ImageNet) without testing on more challenging, real-world scenarios
- The noise estimation mechanism assumes EMA provides a reliable gradient proxy, but this relationship is not rigorously validated

## Confidence
- **High**: The unification framework and classification of SAM variants as OP/CP
- **Medium**: The convergence rate analysis and its implications for OP vs CP
- **Medium**: The empirical superiority of InfoSAM over baselines on standard benchmarks

## Next Checks
1. Test InfoSAM on datasets with controlled gradient noise (e.g., label corruption) to verify the AMD suppression mechanism
2. Compare the variance estimation quality between EMA-based and other methods (e.g., batch gradient estimation)
3. Analyze the sensitivity of InfoSAM to the EMA decay parameter α across different network architectures and batch sizes