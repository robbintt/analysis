---
ver: rpa2
title: Automating Evaluation of Diffusion Model Unlearning with (Vision-) Language
  Model World Knowledge
arxiv_id: '2507.07137'
source_url: https://arxiv.org/abs/2507.07137
tags:
- unlearning
- concepts
- target
- diffusion
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: autoeval-dmun automates evaluation of diffusion model unlearning
  by leveraging language models to generate semantically ranked nearby concepts and
  adversarial prompts. It measures concept similarity via LM rankings and assesses
  unlearning damage using metrics like kernel inception distance.
---

# Automating Evaluation of Diffusion Model Unlearning with (Vision-) Language Model World Knowledge

## Quick Facts
- **arXiv ID:** 2507.07137
- **Source URL:** https://arxiv.org/abs/2507.07137
- **Reference count:** 32
- **Primary result:** LM similarity rankings correlate strongly with unlearning damage, and adversarial prompts effectively circumvent unlearning in popular methods like ESD and Receler.

## Executive Summary
This paper introduces autoeval-dmun, a framework that automates evaluation of diffusion model unlearning using vision-language models (V-LMs) as probes. The system leverages V-LMs to generate semantically ranked nearby concepts and adversarial prompts, measuring concept similarity via LM rankings and assessing unlearning damage using metrics like kernel inception distance (KID). Results demonstrate that V-LM semantic similarity rankings strongly correlate with unlearning damage, while adversarial prompts effectively circumvent unlearning in popular methods like ESD and Receler. The framework provides a scalable, automated approach to evaluating the effectiveness and robustness of diffusion model unlearning techniques.

## Method Summary
The autoeval-dmun framework evaluates diffusion model unlearning by using V-LMs to generate and rank nearby concepts semantically related to target concepts, then measuring the distribution shift between base and unlearned models using KID. The V-LM is prompted to generate n=10 nearby concepts three times, deduplicate them, and re-rank them by relevance. KID is computed between base and unlearned model outputs for each concept. Spearman correlation between V-LM ranking and KID damage quantifies alignment. For robustness testing, V-LMs generate adversarial prompts (misspellings and evocative descriptions) that are fed to unlearned models, with CLIP classification measuring target prediction rates. The framework uses Stable Diffusion v1.4 as the base model and evaluates ESD and Receler unlearning methods.

## Key Results
- V-LM semantic similarity rankings correlate strongly with unlearning damage (Spearman correlations range from -0.126 to -0.672)
- Larger V-LMs produce more highly correlated semantic rankings (Llama-3.2-90B-Vision-Instruct achieves -0.672 correlation vs -0.126 for Llama-3.1-8B-Instruct)
- Adversarial prompts effectively circumvent unlearning, achieving 30-80% additional CLIP target predictions compared to direct prompts

## Why This Works (Mechanism)

### Mechanism 1: Semantic Similarity as Damage Proxy
The V-LM's internal semantic ordering of nearby concepts correlates with collateral damage caused by unlearning. The V-LM generates nearby concepts, deduplicates them, then re-ranks them by relevance to the target concept, producing a semantic similarity ranking. Unlearning damage is measured via KID between base and unlearned model outputs. Spearman correlation between V-LM ranking and KID damage quantifies alignment. The core assumption is that the V-LM's internal semantic space approximates the diffusion model's concept embedding locality.

### Mechanism 2: Adversarial Prompt Synthesis for Robustness Testing
V-LMs generate adversarial prompts (misspellings and evocative descriptions) that circumvent unlearning more effectively than direct target prompts. The V-LM creates misspellings and oblique references that are fed to the unlearned diffusion model. CLIP classifies outputs as target vs. nearby concepts, with higher target prediction rates indicating successful jailbreak. The core assumption is that unlearning methods may block exact concept names but remain vulnerable to oblique references and creative misspellings.

### Mechanism 3: Capability-Dependent Evaluation Quality
More capable V-LMs (larger, vision-instruct) produce semantic rankings that correlate more strongly with unlearning damage. Larger models with vision-instruct training have richer, more multimodally-aligned world knowledge, yielding more informative nearby concepts and better semantic ordering. The core assumption is that V-LM capability scales with semantic alignment to diffusion model concept space.

## Foundational Learning

- **Concept:** Text-to-image diffusion models (denoising process, conditioning)
  - **Why needed here:** Unlearning operates on diffusion model weights; evaluation requires generating images from both base and unlearned models.
  - **Quick check question:** Can you explain how a diffusion model iteratively denoises an image conditioned on text?

- **Concept:** Machine unlearning for concept erasure (ESD, Receler, Ablating Concepts)
  - **Why needed here:** autoeval-dmun evaluates existing unlearning methods; understanding their mechanisms helps interpret damage patterns.
  - **Quick check question:** What is the difference between ESD's fine-tuning approach and Receler's lightweight eraser networks?

- **Concept:** V-LM world knowledge extraction via structured prompting
  - **Why needed here:** The tool relies on prompting V-LMs for nearby concepts and adversarial prompts.
  - **Quick check question:** How would you design a prompt to extract semantically related concepts while avoiding hallucination?

## Architecture Onboarding

- **Component map:** V-LM Assistant -> Diffusion Models -> Metrics Engine -> Evaluation Orchestrator
- **Critical path:**
  1. Prompt V-LM for nearby concepts → deduplicate → re-rank by relevance → select top-k
  2. Generate images from base and unlearned models for target and nearby concepts
  3. Compute KID for each concept to quantify damage; compute Spearman correlation with V-LM ranking
  4. Prompt V-LM for adversarial prompts → generate images from unlearned model → measure CLIP target prediction rate
- **Design tradeoffs:**
  - V-LM size vs. cost: Larger models (70B, 90B) improve correlation but increase inference cost
  - k (number of nearby concepts): Higher k improves damage coverage but increases evaluation time
  - Metric choice: KID captures distribution shift; CLIP captures semantic alignment. Consider combining
  - Adversarial prompt count: More prompts improve robustness assessment but increase compute
- **Failure signatures:**
  - Low Spearman correlation (near 0): V-LM semantic space misaligned with diffusion model; try vision-instruct models
  - Adversarial prompts yield ~0% target predictions: Unlearning method generalizes well; consider harder attacks
  - KID variance high across runs: Insufficient samples; increase to >30 images per concept
  - CLIP misclassifies generated images: Prompt may be ambiguous; verify with human inspection
- **First 3 experiments:**
  1. Baseline correlation: Run autoeval-dmun with Llama-3.1-8B-Instruct on "Mickey Mouse" using ESD-unlearned Stable Diffusion v1.4. Measure Spearman correlation between V-LM ranking and KID damage
  2. Capability scaling: Repeat with Llama-3.3-70B-Instruct and Llama-3.2-90B-Vision-Instruct. Compare correlation improvements
  3. Adversarial robustness: Generate adversarial prompts for "Van Gogh style" and measure CLIP target prediction rate for ESD and Receler. Identify which prompt types (misspellings vs. evocative descriptions) are most effective

## Open Questions the Paper Calls Out

- Can the autoeval-dmun framework generalize effectively to diffusion architectures beyond Stable Diffusion v1.4?
- How do alternative structured prompting techniques impact the generation of effective adversarial prompts?
- Do KID and CLIP scores accurately reflect human perception of unlearning damage and collateral concept preservation?

## Limitations
- The framework only evaluates two unlearning methods (ESD, Receler) on a single diffusion model (Stable Diffusion v1.4)
- V-LM capability is critical for reliable evaluation, creating computational barriers for some users
- Correlation between V-LM semantic rankings and unlearning damage shows high variability across model sizes and concepts

## Confidence
- **High Confidence:** V-LMs can generate semantically related concepts and larger models produce better semantic rankings; adversarial prompt generation is clearly demonstrated
- **Medium Confidence:** V-LM semantic similarity correlates with unlearning damage, but with substantial variability depending on V-LM choice
- **Low Confidence:** Generalization to other architectures, unlearning methods, and concepts is unproven

## Next Checks
1. Apply autoeval-dmun to evaluate a third unlearning method (such as Ablating Concepts) on the same Stable Diffusion v1.4 base model to test generalization
2. Systematically test the framework on concepts spanning different semantic categories (entities, styles, abstract concepts, harmful content) to identify patterns
3. Implement a simplified version using smaller V-LMs or fewer generated images to establish minimum viable configuration for reliable evaluation