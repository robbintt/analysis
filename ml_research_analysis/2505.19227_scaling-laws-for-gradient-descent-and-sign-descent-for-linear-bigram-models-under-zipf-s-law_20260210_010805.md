---
ver: rpa2
title: Scaling Laws for Gradient Descent and Sign Descent for Linear Bigram Models
  under Zipf's Law
arxiv_id: '2505.19227'
source_url: https://arxiv.org/abs/2505.19227
tags:
- descent
- scaling
- sign
- where
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies the optimization dynamics of gradient descent\
  \ (GD) and sign descent (SD) on linear bigram models when word frequencies follow\
  \ Zipf's law or more generally power-law distributions. The authors derive scaling\
  \ laws for convergence rates as a function of vocabulary size d and desired relative\
  \ error \u03B5."
---

# Scaling Laws for Gradient Descent and Sign Descent for Linear Bigram Models under Zipf's Law

## Quick Facts
- arXiv ID: 2505.19227
- Source URL: https://arxiv.org/abs/2505.19227
- Reference count: 40
- Primary result: Sign descent achieves d^(1/2) scaling for Zipf-distributed data versus gradient descent's d^(1-ε) scaling

## Executive Summary
This paper derives scaling laws for gradient descent and sign descent on linear bigram models when word frequencies follow power-law distributions. The authors show that Zipf-distributed data (α = 1) represents a "worst case" for gradient descent, requiring iteration counts that scale almost linearly with vocabulary size. Sign descent with properly tuned step-size achieves significantly better scaling (d^(1/2) vs d^(1-ε)) for Zipf data, explaining why preconditioning methods like Adam outperform standard gradient descent in practice on language models.

## Method Summary
The authors analyze optimization dynamics on linear bigram models where Hessian eigenvalues equal word frequencies. They derive closed-form expressions for gradient descent dynamics and use oscillatory behavior assumptions for sign descent. The analysis covers power-law distributions with exponent α, showing how convergence rates depend on vocabulary size d and desired relative error ε. Synthetic experiments validate theoretical predictions, while real data from OpenWebText demonstrates practical relevance.

## Key Results
- For Zipf-distributed data (α = 1), gradient descent requires d^(1-ε) iterations to reach ε relative error
- For α ≤ 1, gradient descent requires d^α log(1/ε) iterations, with worst scaling at α = 1
- For α > 1, gradient descent scales as (1/ε)^(α/(α-1)), independent of d
- Sign descent with properly tuned step-size scales as d^(1/2) for α = 1, significantly better than gradient descent
- The crossover point where sign descent outperforms gradient descent occurs at α = 1/2

## Why This Works (Mechanism)

### Mechanism 1: Heavy-tailed eigenvalue decay creates dimension-dependent convergence barriers
When word frequencies follow power laws with exponent α ≤ 1, gradient descent requires iterations scaling with vocabulary dimension d rather than just desired error ε. The Hessian eigenvalues equal word frequencies (λ_k = π_k ∝ 1/k^α). For α ≤ 1, eigenvalues decay slowly enough that optimization progress requires scaling time with d^α to make progress across the long tail of eigencomponents.

### Mechanism 2: Sign descent provides scale-free preconditioning that decouples from eigenvalue decay
Sign descent achieves d^(1/2) scaling for Zipf data (α = 1), substantially better than GD's d^(1-ε) scaling. Sign descent uses only gradient signs, normalizing all coordinate updates to equal magnitude. This removes dependence on eigenvalue magnitudes, effectively preconditioning based on the uniform-in-sign property rather than the ill-conditioned curvature.

### Mechanism 3: The α = 1 threshold separates "finite-dimensional" from "effectively infinite-dimensional" regimes
Power-law exponent α = 1 marks a qualitative transition where optimization difficulty fundamentally changes character. For α > 1, eigenvalue sum converges (finite effective dimension). For α ≤ 1, eigenvalue sum diverges, meaning meaningful optimization requires handling increasingly many small eigencomponents as d grows. Zipf's law (α = 1) is the "worst case" combining large frequency imbalance with slow-enough decay.

## Foundational Learning

- **Power-law distributions and Zipf's law**
  - Why needed here: Understanding why α = 1 is a critical threshold requires knowing how power-law tails behave differently than exponential decay.
  - Quick check question: For a distribution π_k ∝ 1/k^α, does the sum Σ_k π_k converge for α = 0.5? For α = 2?

- **Gradient descent dynamics on quadratic objectives**
  - Why needed here: The paper analyzes GD on L(W) = (1/2n)||XW - Y||²_F, which has closed-form dynamics in terms of eigenvalues.
  - Quick check question: For f(x) = (1/2)(x-x*)^T A(x-x*) with eigenvalues λ_i, what is the loss after t GD steps with step-size η?

- **Condition number and its relationship to convergence**
  - Why needed here: The ill-conditioning from heavy-tailed eigenvalues (κ ∼ d for α = 1) is what GD struggles with and SD/Adam address.
  - Quick check question: How does convergence rate of GD on a quadratic relate to the condition number κ = L/μ?

## Architecture Onboarding

- **Component map**: Problem definition -> Linear bigram model -> Hessian eigenvalues = word frequencies -> Closed-form GD dynamics -> Oscillatory SD dynamics -> Scaling laws
- **Critical path**: 
  1. Verify data follows power law (plot log rank vs log frequency)
  2. Estimate α from data (fit slope)
  3. Choose optimizer based on α: GD if α < 1/2, SD/Adam-like if α ≥ 1/2
  4. Scale iteration budget with d^α for GD or d^(1/2) for SD if α ≈ 1
- **Design tradeoffs**: 
  - GD: Simpler, no hyperparameter tuning for step-size (use 1/π₁), but poor scaling for Zipf data
  - SD: Better scaling for α ≥ 1/2 but requires step-size tuned to iteration budget
  - Adam in practice: Combines SD-like benefits with adaptive momentum but more complex
- **Failure signatures**:
  - Loss plateaus early with GD on Zipf-distributed data while training error remains high
  - SD oscillates without converging if step-size not decayed with budget
  - Standard convergence bounds (linear rate with κ, sublinear 1/t) predict no progress when t ≪ d for α = 1
- **First 3 experiments**:
  1. Reproduce Figure 1: Run GD on synthetic bigram problem with varying d and α ∈ {0.5, 1, 2}, plot relative error vs steps. Verify d-scaling appears for α ≤ 1.
  2. Reproduce Figure 2 (left panel): Run GD and SD on real text data (OpenWebText), fit empirical scaling curves, compare to theoretical d^(1-ε) and d^(1/2) predictions.
  3. Ablation on step-size: For SD, sweep η at fixed budget T and vocabulary size d, verify optimal η scales as 1/(T·ϕ^α) per Proposition 4.3.

## Open Questions the Paper Calls Out

**Open Question 1**: Can the analysis be extended to the online (stochastic) case where samples are drawn incrementally rather than using full-batch gradients? The current analysis relies on closed-form dynamics for deterministic GD/SD on the full batch; stochastic sampling introduces variance that requires different analytical tools.

**Open Question 2**: How does momentum affect the scaling behavior of sign descent, particularly in dampening the oscillatory regime? The sign descent dynamics already require careful step-size tuning to balance progress and oscillation; momentum introduces additional dynamics that complicate the analysis.

**Open Question 3**: Can these scaling laws be derived for cross-entropy loss and more complex models beyond linear bigrams? Cross-entropy lacks the closed-form eigendecomposition that enables the current analysis, making the optimization trajectory harder to characterize.

**Open Question 4**: What are the finite-dimensional correction terms that characterize convergence before the asymptotic regime is reached, especially for α = 1? The current asymptotic analysis takes d → ∞ first; for practical vocabulary sizes, correction terms may dominate.

## Limitations
- The analysis relies on linear bigram models, a significant simplification of modern transformer architectures
- Step-size tuning for sign descent requires knowing the training horizon in advance, creating practical challenges
- Empirical validation is limited to one corpus and one model family (OpenWebText with SentencePiece tokenization)

## Confidence
- **High confidence** in: The theoretical analysis of GD and SD convergence rates on synthetic bigram problems
- **Medium confidence** in: The empirical demonstration that SD outperforms GD on real text data (α ≈ 1)
- **Low confidence** in: The practical implications for large-scale language model training with full transformer architectures

## Next Checks
1. Test the GD vs SD comparison on multiple language corpora (Wikipedia, Common Crawl, non-English texts) to verify consistent Zipf regime behavior across different data distributions and tokenization schemes.

2. Extend the analysis beyond linear bigram models to test whether the same scaling laws hold for transformer-based language models trained with linearized approximations.

3. Systematically compare SD against other preconditioning approaches (Adam, RMSProp, diagonal preconditioning) on the same synthetic and real problems to establish whether SD's d^(1/2) scaling represents the theoretical optimum.