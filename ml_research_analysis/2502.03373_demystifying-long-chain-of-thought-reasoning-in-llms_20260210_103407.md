---
ver: rpa2
title: Demystifying Long Chain-of-Thought Reasoning in LLMs
arxiv_id: '2502.03373'
source_url: https://arxiv.org/abs/2502.03373
tags:
- long
- reward
- reasoning
- length
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper systematically investigates long chain-of-thought (CoT)
  reasoning in large language models through supervised fine-tuning (SFT) and reinforcement
  learning (RL) experiments. The key findings are: (1) Long CoT SFT scales better
  than short CoT SFT and enables further RL improvements; (2) RL requires careful
  reward shaping with cosine length-scaling and repetition penalties to stabilize
  CoT growth; (3) Leveraging noisy web-extracted solutions with filtering mechanisms
  effectively scales verifiable reward signals, particularly for out-of-distribution
  STEM tasks; (4) Core reasoning abilities like error correction exist in base models
  but require significant RL compute to incentivize effectively.'
---

# Demystifying Long Chain-of-Thought Reasoning in LLMs

## Quick Facts
- **arXiv ID**: 2502.03373
- **Source URL**: https://arxiv.org/abs/2502.03373
- **Reference count**: 40
- **Primary result**: Long CoT SFT + RL with cosine length-scaling reward enables Llama-3.1-8B to achieve up to 85.9% accuracy on MATH-500 and 26.9% on AIME 2024.

## Executive Summary
This paper systematically investigates long chain-of-thought (CoT) reasoning in LLMs through supervised fine-tuning (SFT) and reinforcement learning (RL) experiments. The key findings demonstrate that long CoT SFT scales better than short CoT SFT and enables further RL improvements. RL requires careful reward shaping with cosine length-scaling and repetition penalties to stabilize CoT growth. The study shows that leveraging noisy web-extracted solutions with filtering mechanisms effectively scales verifiable reward signals, particularly for out-of-distribution STEM tasks. Core reasoning abilities like error correction exist in base models but require significant RL compute to incentivize effectively.

## Method Summary
The approach combines long CoT SFT initialization with PPO RL using carefully shaped rewards. Long CoT trajectories are distilled from capable teacher models (like QwQ-32B-Preview) via rejection sampling against ground truth answers. The SFT phase fine-tunes base models on these trajectories, followed by RL training with a composite reward function including cosine length-scaling, repetition penalties, and verification-based correctness signals. Noisy web-extracted data is filtered for short-form verifiable answers to scale RL training beyond limited gold datasets.

## Key Results
- Long CoT SFT scales better than short CoT SFT and enables further RL improvements
- RL requires cosine length-scaling and repetition penalties to stabilize CoT growth and prevent reward hacking
- Filtering noisy web-extracted solutions effectively scales verifiable reward signals for OOD STEM tasks
- Llama-3.1-8B achieves up to 85.9% accuracy on MATH-500 and 26.9% on AIME 2024 when combining long CoT SFT with RL

## Why This Works (Mechanism)

### Mechanism 1: Long CoT SFT Scales to Higher Performance
Long CoT data exposes models to complex reasoning patterns including branching, backtracking, and error correction. This initialization creates a policy distribution where extended reasoning is more probable, providing a richer starting point for RL exploration compared to short CoT's compressed reasoning. Quality matters: distilling from models exhibiting emergent long CoT patterns yields significantly better generalization and RL gains than artificially constructed action sequences.

### Mechanism 2: Cosine Length-Scaling Reward Stabilizes CoT Growth
Simple binary rewards cause models to unboundedly increase CoT length as a proxy for effort, leading to context window truncation and accuracy collapse. The Cosine Reward constrains this by rewarding shorter correct CoTs for efficiency, penalizing shorter wrong CoTs more than longer ones, and applying localized N-gram repetition penalties. This stabilization is essential for training stability and preventing reward hacking through repetitive padding.

### Mechanism 3: Filtering Noisy Data Enables Scalable Verifiable Rewards
High-quality human-annotated verifiable data is scarce and domain-limited. Web-extracted QA data is abundant but noisy. Filtering this data to retain only samples where short-form answers can be extracted and verified by rule-based graders effectively curates large-scale, diverse prompt sets for RL. This diversity improves performance on out-of-distribution tasks beyond the training domain.

## Foundational Learning

- **Concept: Supervised Fine-Tuning (SFT) Initialization**
  - Why needed here: RL performance heavily depends on SFT quality; long CoT SFT provides superior initialization compared to short CoT or RL from scratch
  - Quick check question: Does your SFT data contain emergent reasoning behaviors (branching, error correction) or is it a linear, compressed solution trace?

- **Concept: Reward Shaping in Reinforcement Learning**
  - Why needed here: Simple binary rewards are insufficient; reward functions must balance correctness, efficiency, and exploration
  - Quick check question: How does your reward function differentiate between a short wrong answer and a long wrong answer?

- **Concept: Verifiable Reward Signals**
  - Why needed here: RL for reasoning requires ground truth rewards; scaling beyond limited datasets requires filtering noisy web data
  - Quick check question: For your target task, can the final answer be extracted and verified programmatically?

## Architecture Onboarding

- **Component map**: Base Model -> SFT Data Curation (distillation + rejection sampling) -> SFT Training (batch=256, lr=5e-6) -> RL Framework (PPO) -> Composite Reward (Cosine + Repetition + Verifier)

- **Critical path**: 1) Curate long CoT SFT data from teacher model using rejection sampling 2) Perform long CoT SFT on base model 3) Configure RL reward with cosine length-scaling and repetition penalties 4) Run RL training from SFT checkpoint

- **Design tradeoffs**: Emergent vs constructed patterns (distillation is superior but requires capable teacher); rule-based vs model-based verifier (rule-based scales better, model-based handles free-form); context window vs compute (8K performed better than 16K under same sample budget)

- **Failure signatures**: Unstable length growth (length explodes then collapses with accuracy drop); reward hacking (repetition increases without reasoning quality); OOD performance plateau (good on training domain but fails to generalize)

- **First 3 experiments**: 1) Establish SFT baseline comparing long vs short CoT scaling behavior 2) Validate reward shaping comparing classic reward vs cosine reward vs cosine + repetition penalty 3) Test noisy data scaling comparing unfiltered vs filtered web data with different verifiers

## Open Questions the Paper Calls Out

- **Open Question 1**: Can verification signals be scaled effectively through a "pretraining equivalent" in RL environments, rather than relying on human-designed heuristics?
- **Open Question 2**: To what extent do long CoT reasoning capabilities arise from recombining pre-existing skills in pre-training data versus learning de novo?
- **Open Question 3**: How does the emergence of complex reasoning behaviors change qualitatively when scaling RL from base models larger than 8B parameters?

## Limitations

- Results primarily based on 7B/8B models; scaling behavior to 70B+ models untested
- Effectiveness of noisy web data scaling demonstrated but corpus evidence for this specific mechanism is weak
- Filtering mechanism's reliance on short-form answers may limit applicability to problems requiring detailed free-form solutions

## Confidence

- **High Confidence**: Long CoT SFT initialization superiority, necessity of cosine length-scaling reward for stability
- **Medium Confidence**: Emergent patterns generalize better than constructed, effectiveness of noisy web data scaling
- **Medium Confidence**: Base model contains core reasoning abilities requiring RL to incentivize

## Next Checks

1. Test scaling to larger models (70B+) to verify approach maintains effectiveness and training stability
2. Validate verifiable data scaling across STEM domains beyond mathematics (physics, chemistry)
3. Characterize base model reasoning capabilities directly before SFT to quantify emergent vs constructed pattern hypothesis