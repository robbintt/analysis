---
ver: rpa2
title: 'OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference'
arxiv_id: '2602.01493'
source_url: https://arxiv.org/abs/2602.01493
tags:
- cavity
- operator
- equation
- opinf-llm
- regression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OpInf-LLM addresses the challenge of solving diverse parametric
  partial differential equations (PDEs) using large language models (LLMs), where
  traditional LLM-based approaches struggle with generalization to unseen parameters
  and boundary conditions. The method combines operator inference (OpInf) with LLM
  tool-calling capabilities to create a unified framework for parametric PDE solving.
---

# OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference

## Quick Facts
- **arXiv ID**: 2602.01493
- **Source URL**: https://arxiv.org/abs/2602.01493
- **Reference count**: 40
- **Key outcome**: LLM-based parametric PDE solving via operator inference achieves 99.2% code execution success rate with significantly lower prediction errors than baseline methods

## Executive Summary
OpInf-LLM introduces a novel framework for solving parametric partial differential equations using large language models combined with operator inference. The method learns shared reduced-order models from limited solution data, enabling accurate predictions across unseen parameter values and boundary conditions. By leveraging LLM tool-calling capabilities instead of direct code generation, the framework achieves high execution success rates while maintaining prediction accuracy. Experimental results on heat, Burgers', and 2D lid-driven cavity flow equations demonstrate superior performance compared to existing approaches, with strong zero-shot generalization capabilities.

## Method Summary
The framework combines Proper Orthogonal Decomposition (POD) with operator inference to create reduced-order models that capture PDE dynamics in low-dimensional subspaces. For training parameters, POD extracts dominant modes from trajectory data, then least-squares optimization learns reduced operators governing modal coefficient dynamics. New parameters are handled via polynomial regression on existing operators, followed by ODE integration for solution reconstruction. The LLM acts as a parser and tool orchestrator, calling pre-validated functions for interpolation/regression and numerical integration rather than generating code directly, which achieves 99.2% execution success rate.

## Key Results
- Achieves 99.2% code execution success rate versus 44-52% for direct code generation approaches
- Outperforms baseline methods (CodePDE, MOL-LLM, direct LLM prediction) with significantly lower prediction errors
- Demonstrates strong zero-shot generalization to unseen parameters and boundary conditions
- Maintains accuracy with minimal training data requirements across heat, Burgers', and 2D lid-driven cavity flow equations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: OpInf-LLM achieves accurate parametric PDE prediction because operator inference constructs reduced-order models that explicitly preserve the algebraic structure of the governing physics.
- **Mechanism**: The framework performs Proper Orthogonal Decomposition (POD) on trajectory data to obtain a shared basis Φ, then solves a regularized least-squares problem to learn reduced operators A, H, B, c that govern modal coefficient dynamics: da/dt = A(ξ)a + H(ξ)(a⊗a) + B(ξ)u + c(ξ). This structure-preserving projection ensures the ROM respects the original PDE's quadratic nonlinearity.
- **Core assumption**: The PDE dynamics can be approximated within a finite r-dimensional subspace spanned by POD modes capturing ≥99% of solution energy.
- **Evidence anchors**:
  - [abstract]: "learns a shared reduced-order model (ROM) from limited solution data, enabling accurate predictions across unseen parameter values"
  - [section 3.1]: "OpInf learns a ROM approximating the PDE dynamics within a finite r-dimensional subspace"
  - [corpus]: Related work on neural operators for parametric PDEs confirms structure-preserving approaches improve generalization (arXiv:2511.04576)
- **Break condition**: If POD modes fail to capture essential dynamics (e.g., r too small or highly nonlinear phenomena), ROM accuracy degrades sharply. Table 7 shows unstable predictions for heat equation with r≥8 under regression.

### Mechanism 2
- **Claim**: Zero-shot generalization to unseen parameters emerges because reduced operators vary smoothly with PDE parameters, enabling polynomial regression for interpolation.
- **Mechanism**: After learning operators A(ξᵢ), H(ξᵢ), B(ξᵢ), c(ξᵢ) for training parameters {ξᵢ}, operators at new ξ are obtained via per-entry polynomial fitting. This reduces parametric PDE solving to lightweight regression plus ODE integration.
- **Core assumption**: The operator-parameter relationship is approximately polynomial within the parameter range of interest.
- **Evidence anchors**:
  - [abstract]: "performs polynomial regression to infer reduced-order operators for new parameters"
  - [section 3.2]: "reduced dynamics operators for any unseen ξ can be obtained via polynomial regression among the existing operators"
  - [corpus]: Limited direct evidence on operator regression; related work focuses on neural operator interpolation rather than explicit polynomial fitting
- **Break condition**: Extrapolation far beyond training parameter range degrades accuracy. Table 2 shows error increases from 1.29e-2 on [0,T] to 3.60e-2 on [T,2T] for heat equation, though still acceptable.

### Mechanism 3
- **Claim**: High execution success rate (99.2%) results from constraining LLM to tool-calling rather than full code generation, avoiding syntax errors and numerical instability.
- **Mechanism**: The LLM parses natural language, then invokes pre-validated tools for interpolation/regression and ODE integration. Tools implement numerically stable algorithms with built-in validation (eigenvalue checks, NaN detectors).
- **Core assumption**: The tool interface correctly exposes all necessary functionality with robust error handling.
- **Evidence anchors**:
  - [abstract]: "integrates a low-dimensional ODE system for efficient solution reconstruction" with "99.2% code execution success rate"
  - [Table 5]: Tool-calling achieves 99.2% success vs. 44-52% for code generation with comparable error
  - [corpus]: CodePDE (arXiv:2505.08783) shows LLM code generation for PDEs suffers from 40-50% failure rates
- **Break condition**: Tool interface bugs or missing functionality cause cascading failures; poorly designed prompts lead to incorrect tool selection.

## Foundational Learning

- **Concept: Proper Orthogonal Decomposition (POD)**
  - Why needed here: POD provides the shared basis Φ that compresses high-dimensional PDE states into low-dimensional modal coefficients.
  - Quick check question: Can you explain how POD extracts dominant modes from trajectory data and what "capturing 99% energy" means?

- **Concept: Quadratic Nonlinearity in Dynamical Systems**
  - Why needed here: OpInf assumes PDE nonlinearities are at most quadratic, enabling structured reduced dynamics with H(a⊗a) term.
  - Quick check question: Why does the Navier-Stokes convection term u·∇u lead to quadratic structure in the reduced equations?

- **Concept: Tikhonov Regularization**
  - Why needed here: Regularization parameter λ in Eq. (8) prevents overfitting when learning operators from limited data.
  - Quick check question: What happens to operator norms and prediction accuracy as λ varies from 10⁻⁶ to 10 (see Table 8)?

## Architecture Onboarding

- **Component map**: Natural language input → LLM parser → Tool calling (parameter analysis → interpolation/regression → validation) → ODE integration → Solution reconstruction via Φ @ a(t)

- **Critical path**: The reduced operator prediction and ODE integration tools are the execution bottleneck; LLM parsing is trivial.

- **Design tradeoffs**:
  - More POD modes (larger r) → higher accuracy but potential instability (Table 7 shows instability for heat with r≥8 under regression)
  - Higher regularization λ → more stable but potentially underfit operators
  - Interpolation vs. regression: Interpolation more stable; regression enables mild extrapolation

- **Failure signatures**:
  - LLM outputs invalid JSON or calls non-existent tools → parsing failure
  - Operator matrices contain NaN/Inf → validation catches this
  - ODE integration diverges → typically indicates poorly extrapolated operators or insufficient regularization
  - Shape mismatches in tool outputs → check POD dimension consistency

- **First 3 experiments**:
  1. **Basis dimension sweep**: Train OpInf-LLM on heat equation with r ∈ {4,6,8,10}, measure error vs. energy captured. Verify ~99% energy guideline.
  2. **Regularization ablation**: Fix r=6, vary λ from 10⁻⁶ to 10. Plot operator norms vs. prediction error to identify optimal range.
  3. **Tool-calling vs. code generation**: Run identical Burgers' equation queries with tool-calling and direct code generation prompts. Compare success rates and inspect failure modes in generated code.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can OpInf-LLM be extended to PDEs with higher-order or non-polynomial nonlinearities while maintaining the same execution success rates?
- **Basis in paper**: [explicit] The paper states they "stick to quadratically nonlinear PDEs with the formulation in (4) for clarity" and acknowledges this procedure "naturally applies to PDE operators F with higher-order or non-polynomial nonlinearities, leading to modal dynamics with structures that differ from (4), for example through the inclusion of cubic terms."
- **Why unresolved**: No experiments were conducted on PDEs with cubic or non-polynomial nonlinearities to validate whether the LLM tool-calling approach generalizes to more complex operator structures.
- **What evidence would resolve it**: Experimental results on PDEs with cubic nonlinearities (e.g., certain wave equations) or non-polynomial terms, reporting success rates and prediction errors comparable to the quadratically nonlinear cases.

### Open Question 2
- **Question**: How can shared reduced bases be constructed to enable true multi-PDE-family solving without constructing new bases for each new governing equation?
- **Basis in paper**: [explicit] The discussion states "new bases are usually constructed when introducing new governing equations, although such cost is incurred upfront offline" and the introduction notes "ROMs have not been explored... for extending beyond single parametric PDE families."
- **Why unresolved**: The current framework requires per-family POD basis construction, limiting zero-shot generalization across fundamentally different equation types.
- **What evidence would resolve it**: A unified basis representation that supports accurate predictions across multiple PDE families (e.g., diffusion, advection, wave equations) using a single shared basis, with quantitative error analysis.

### Open Question 3
- **Question**: What are the theoretical limits of parameter extrapolation accuracy, and how does error scale with extrapolation distance?
- **Basis in paper**: [inferred] The ablation on parameter range provides only binary interpolation vs. extrapolation classification, but Tables 6-8 show that extrapolation cases (e.g., ν=3.0 for heat, Re=60,140 for cavity) have varying error levels without systematic analysis of distance-dependent degradation.
- **Why unresolved**: No systematic study quantifies how prediction error grows as the query parameter moves further outside the training range, which is critical for practical deployment.
- **What evidence would resolve it**: Controlled experiments systematically varying extrapolation distance with corresponding error curves, potentially with theoretical bounds relating extrapolation error to training parameter coverage density.

### Open Question 4
- **Question**: Can code generation approaches achieve success rates comparable to tool-calling with improved interface design?
- **Basis in paper**: [explicit] Appendix C.1 reports that code generation achieves "comparable" prediction error but "lower execution success rate" (44-52% vs. 99.2%), with failures primarily from "data loading and output shape mismatches, as well as formatting issues," suggesting "improving interface design and code I/O handling may therefore further increase the success rate."
- **Why unresolved**: The root causes of code generation failures are identified but not addressed; it remains unclear whether near-100% success rates are achievable purely through better interfaces.
- **What evidence would resolve it**: A redesigned code generation interface with standardized I/O handling that achieves success rates >95% while maintaining comparable prediction accuracy.

## Limitations
- Quadratic nonlinearity assumption limits applicability to PDEs with higher-order or non-polynomial nonlinearities
- POD basis construction must be repeated for each new PDE family, preventing true multi-equation solving
- Limited analysis of extrapolation accuracy degradation with distance from training parameter range

## Confidence
- **Mechanism 1**: Medium - Theoretically sound but Table 7 shows instability with r≥8 suggests 99% energy threshold may be insufficient for stable regression
- **Mechanism 2**: Medium - Limited direct evidence on polynomial fitting of reduced operators; depends heavily on parameter-smoothness assumptions
- **Mechanism 3**: High - Strong empirical support (99.2% success vs 44-52% for direct code generation) and clear mechanism, but assumes robust tool implementation

## Next Checks
1. **Parameter Range Extrapolation Test**: Systematically evaluate prediction accuracy as test parameters move beyond training bounds to quantify the practical limits of polynomial regression interpolation.
2. **Operator Stability Analysis**: For each PDE type, identify the critical POD dimension r* where regression becomes unstable by monitoring operator norm growth and prediction divergence across r values.
3. **Tool-LLM Interface Stress Test**: Design adversarial prompts that challenge tool selection and parameter parsing to identify failure modes in the agentic workflow, particularly for complex boundary conditions.