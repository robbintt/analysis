---
ver: rpa2
title: 'Text2Vis: A Challenging and Diverse Benchmark for Generating Multimodal Visualizations
  from Text'
arxiv_id: '2507.19969'
source_url: https://arxiv.org/abs/2507.19969
tags:
- data
- answer
- code
- visualization
- chart
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Text2Vis, a comprehensive benchmark for evaluating
  LLMs on text-to-visualization tasks, covering over 20 chart types and complex data
  science queries. It includes 1,985 samples with data tables, natural language queries,
  short answers, visualization code, and annotated charts, supporting diverse tasks
  like trend analysis, correlation, outlier detection, and predictive analytics.
---

# Text2Vis: A Challenging and Diverse Benchmark for Generating Multimodal Visualizations from Text

## Quick Facts
- arXiv ID: 2507.19969
- Source URL: https://arxiv.org/abs/2507.19969
- Authors: Mizanur Rahman; Md Tahmid Rahman Laskar; Shafiq Joty; Enamul Hoque
- Reference count: 40
- Key outcome: Text2Vis benchmark with 1,985 samples covering 20+ chart types, plus actor-critic framework improving GPT-4o's pass rate from 26% to 42%

## Executive Summary
Text2Vis introduces a comprehensive benchmark for evaluating large language models on text-to-visualization tasks, addressing the gap in systematic assessment of multimodal visualization generation capabilities. The benchmark includes 1,985 curated samples spanning over 20 chart types and complex analytical queries, each comprising data tables, natural language queries, answers, visualization code, and annotated charts. The authors also propose an automated LLM-based evaluation framework that enables scalable assessment without human annotation, measuring answer correctness, code execution success, visualization readability, and chart accuracy.

## Method Summary
The authors developed Text2Vis as a comprehensive benchmark covering 20+ chart types and complex data science queries. The benchmark includes 1,985 curated samples with data tables, natural language queries, short answers, visualization code, and annotated charts. They propose a cross-modal actor-critic agentic framework that jointly refines textual answers and visualization code, improving GPT-4o's pass rate from 26% to 42% over direct inference. Additionally, they introduce an automated LLM-based evaluation framework for scalable assessment without human annotation, measuring multiple dimensions including answer correctness, code execution success, visualization readability, and chart accuracy.

## Key Results
- GPT-4o's pass rate improved from 26% to 42% using the actor-critic framework versus direct inference
- 1,985 benchmark samples covering 20+ chart types and diverse analytical tasks
- Significant performance gaps identified across 11 open- and closed-source models tested
- Automated evaluation framework successfully measures multiple quality dimensions without human annotation

## Why This Works (Mechanism)
The cross-modal actor-critic framework works by iteratively refining both the textual answers and visualization code through a joint optimization process. The actor generates candidate solutions while the critic evaluates their quality across multiple dimensions (answer correctness, code execution, visualization quality). This iterative refinement process allows the model to progressively improve its outputs by learning from previous attempts, addressing the inherent complexity of translating natural language queries into accurate, executable visualization code.

## Foundational Learning
- **Multimodal visualization generation**: Understanding how text queries translate to visual representations across 20+ chart types; needed to assess model capabilities comprehensively; quick check: verify all chart types are represented in benchmark samples
- **Agentic framework design**: Iterative refinement processes for joint optimization of text and code outputs; needed for systematic improvement over direct inference; quick check: trace the feedback loop between actor and critic components
- **Automated evaluation metrics**: LLM-based assessment of answer correctness, code execution, and visualization quality; needed for scalable benchmark evaluation without human annotation; quick check: validate evaluation criteria against sample outputs
- **Data science query complexity**: Handling trend analysis, correlation, outlier detection, and predictive analytics tasks; needed to ensure benchmark covers real-world analytical challenges; quick check: categorize sample queries by analytical task type
- **Cross-modal reasoning**: Integrating textual understanding with visual generation capabilities; needed for comprehensive text-to-visualization assessment; quick check: verify multimodal consistency in sample pairs

## Architecture Onboarding

**Component Map**: Natural Language Query -> Data Table + Query Parser -> Actor Network -> Candidate Code + Answer -> Critic Network -> Quality Score -> Refinement Loop -> Final Visualization

**Critical Path**: Query parsing → Actor generation → Code execution → Visualization rendering → Quality evaluation → Iterative refinement

**Design Tradeoffs**: 
- Automated evaluation provides scalability but may miss subtle quality issues that human experts would detect
- Actor-critic framework adds computational overhead but enables systematic improvement over direct inference
- Comprehensive benchmark coverage (20+ chart types) increases real-world applicability but also complexity

**Failure Signatures**:
- Code execution failures indicating mismatch between generated code and data structure
- Visualization readability issues suggesting problems with layout or scaling
- Answer correctness errors revealing gaps in analytical reasoning
- Chart accuracy problems showing misalignment between query intent and visual output

**First Experiments to Run**:
1. Test actor-critic framework with a single chart type to isolate framework performance from benchmark complexity
2. Evaluate automated assessment reliability by comparing LLM judgments against human expert ratings on a subset
3. Perform ablation studies removing either the actor or critic component to measure individual contributions

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Automated LLM-based evaluation may introduce systematic biases and fail to capture subtle visualization quality issues that human experts would detect
- Benchmark's real-world applicability depends on how well curated samples represent actual user needs and encountered data complexity
- Performance improvements from actor-critic framework (26% to 42% pass rate) remain moderate, indicating the problem remains significantly challenging

## Confidence
- **High Confidence**: Benchmark construction methodology, dataset composition, and evaluation framework design are well-documented and methodologically sound
- **Medium Confidence**: Actor-critic framework performance improvements are validated through experiments, but moderate absolute performance levels indicate ongoing challenges
- **Medium Confidence**: Automated evaluation framework reliability is supported by experimental results, though alignment with human judgment requires further validation

## Next Checks
1. Conduct human evaluation studies comparing automated LLM-based assessment against expert human judgment on a subset of samples to quantify systematic biases
2. Test benchmark's coverage and difficulty representation by having domain experts map real-world visualization requests to benchmark categories and identify gaps
3. Perform ablation studies on actor-critic framework components to isolate which elements contribute most significantly to performance improvements and identify optimization opportunities