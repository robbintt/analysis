---
ver: rpa2
title: Deep SOR Minimax Q-learning for Two-player Zero-sum Game
arxiv_id: '2511.16226'
source_url: https://arxiv.org/abs/2511.16226
tags:
- algorithm
- function
- q-learning
- policy
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses two-player zero-sum Markov games using deep
  reinforcement learning. The authors propose a Deep Successive Over-Relaxation Minimax
  Q-learning (D-SOR-MQL) algorithm that combines deep neural networks as function
  approximators with successive over-relaxation to accelerate convergence.
---

# Deep SOR Minimax Q-learning for Two-player Zero-sum Game

## Quick Facts
- **arXiv ID:** 2511.16226
- **Source URL:** https://arxiv.org/abs/2511.16226
- **Reference count:** 21
- **Primary result:** D-SOR-MQL achieves faster convergence and lower loss than standard Minimax Q-learning on Guard-Invader and Soccer environments with 49 and 121 states

## Executive Summary
This paper proposes Deep Successive Over-Relaxation Minimax Q-learning (D-SOR-MQL) for two-player zero-sum Markov games, combining deep neural networks with successive over-relaxation to accelerate convergence. The algorithm employs three networks (primary, target, and evaluation) with a replay buffer and updates parameters using generalized policy iteration with optimistic policy evaluation. Theoretical analysis proves finite-time convergence with O(ϵ⁻²) sample complexity under linear function approximation. Empirical results demonstrate that D-SOR-MQL outperforms standard Minimax Q-learning on small discrete environments, with optimal performance achieved for relaxation parameter values between 1.2 and 2.5.

## Method Summary
D-SOR-MQL combines deep function approximation with successive over-relaxation (SOR) to solve two-player zero-sum Markov games. The algorithm uses three neural networks: a primary network updated every gradient step, a target network updated every T steps, and an evaluation network updated every nT steps. The primary network learns Q-values through gradient descent on MSE loss, while the target and evaluation networks provide stable targets for TD updates. Policy evaluation uses optimistic (partial) evaluation through generalized policy iteration, balancing computational cost against convergence speed. The SOR mechanism modifies the Bellman operator with a relaxation parameter w > 1 to reduce the contraction factor and accelerate convergence.

## Key Results
- D-SOR-MQL achieves faster convergence than standard Minimax Q-learning on Guard-Invader and Soccer environments
- Optimal relaxation parameter w falls in range 1.2-2.5, with performance degrading for values exceeding w*
- Ablation studies show the relaxation parameter significantly affects convergence speed and stability
- Three-network architecture with staggered updates provides stability in the function approximation setting

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Successive over-relaxation (SOR) with parameter w > 1 accelerates convergence by reducing the contraction factor of the Q-Bellman operator.
- **Mechanism:** The relaxation parameter w reweights the Bellman update: the new Q-value becomes w times the standard Bellman target plus (1-w) times the current value estimate. When optimally tuned (1 < w < w*), this yields a contraction factor of (wγ + 1-w) that is strictly smaller than the standard discount factor γ, causing iterates to converge faster to the fixed point.
- **Core assumption:** The optimal relaxation parameter w* exists and lies in a problem-dependent range; values exceeding w* cause divergence.
- **Evidence anchors:**
  - [abstract] "...seen to result in a lower contraction factor for the associated Q-Bellman operator resulting in a faster value iteration-based procedure"
  - [section 3, eq. 1] Defines Q* with relaxation weighting: Q*(s,a,o) = w[R(s,a,o) + γΣP(s'|s,a,o)V*(s')] + (1-w)V*(s)
  - [corpus] Weak direct support—neighbor papers address zero-sum games but not SOR-specific acceleration.
- **Break condition:** If w ≥ w* or w < 1, contraction property may degrade or reverse; empirical stability degrades above ~2.5 in tested environments.

### Mechanism 2
- **Claim:** Generalized Policy Iteration (GPI) with optimistic (partial) policy evaluation balances per-iteration computational cost against convergence speed.
- **Mechanism:** GPI alternates policy improvement (via LP solving for Nash) with n steps of policy evaluation rather than full convergence. Setting n=1 recovers value iteration (slow convergence, cheap per-step); n→∞ recovers policy iteration (fast convergence, expensive). Intermediate n trades off both.
- **Core assumption:** Approximate value functions from partial evaluation remain sufficiently accurate for stable policy improvement.
- **Evidence anchors:**
  - [section 4] "GPI proceeds in two alternating steps: πi+1 = K(Qi), Qi+1 = (T_w^πi+1)^n(Qi)... the parameter n governs the number of evaluation iterations"
  - [section 4] "GPI provides a flexible trade-off between computational efficiency and convergence speed by tuning the parameter n"
  - [corpus] No direct corpus evidence on GPI in this specific form.
- **Break condition:** If n is too small relative to problem complexity, policy oscillation or slow convergence may occur; if too large, computation becomes prohibitive without proportional speedup.

### Mechanism 3
- **Claim:** Three separate networks with staggered update schedules stabilizes training in the function approximation setting.
- **Mechanism:** The primary network (θt) is updated every gradient step; the target network (θtarget) is synchronized every T steps to compute stable TD targets; the evaluation network (θeval) is synchronized every nT steps and defines the policy for target computation. This decouples the moving target problem across timescales.
- **Core assumption:** Correlation between online network and target values is sufficiently reduced by delayed synchronization.
- **Evidence anchors:**
  - [section 4] "θeval corresponds to the parameters of Qi-1, and the evaluated policy is πeval = K[q(θeval)]... θtarget corresponds to the weights from the preceding inner-loop iteration"
  - [algorithm 1, lines 13-18] Explicit staggered update logic: θtarget every T steps, θeval every nT steps
  - [corpus] Three-network architectures appear in related deep RL but not specifically analyzed for SOR-minimax.
- **Break condition:** If T or nT are too small, target correlation remains high and training destabilizes; if too large, learning slows due to stale targets.

## Foundational Learning

- **Concept: Two-player zero-sum Markov Games**
  - **Why needed here:** The entire algorithm is defined on this game structure; understanding that one agent's gain is another's loss, and that Nash equilibrium requires solving maxπ minμ E[Q], is prerequisite to interpreting the algorithm's objective.
  - **Quick check question:** Can you explain why mixed (stochastic) policies are sometimes necessary to achieve Nash equilibrium in zero-sum games?

- **Concept: Bellman operators and contraction mappings**
  - **Why needed here:** The SOR mechanism modifies the standard Q-Bellman operator; understanding fixed-point iteration and contraction factors is necessary to grasp why reducing the contraction factor accelerates convergence.
  - **Quick check question:** If an operator F has contraction factor γ < 1, what does this guarantee about iterates θt+1 = F(θt)?

- **Concept: Function approximation in RL (linear and neural)**
  - **Why needed here:** The paper moves from tabular Q-learning to deep function approximation; understanding feature vectors ψ(s,a,o) and how θ parameterizes Q-values is essential for reading the theoretical analysis and implementing the algorithm.
  - **Quick check question:** Why does the finite-time analysis use linear function approximation even though experiments use deep networks?

## Architecture Onboarding

- **Component map:**
  - Primary Q-network (θt) -> Target network (θtarget) -> Evaluation network (θeval) -> Replay buffer D -> Policy solver K(·)

- **Critical path:**
  1. Observe state st
  2. Compute πeval = K(q(·|θeval)); select action via ε-greedy over πeval
  3. Opponent selects o via best-response to πeval
  4. Execute (at, ot), observe (rt+1, st+1), store in D
  5. Sample mini-batch; compute targets yk using θtarget and πeval
  6. Compute MSE loss; gradient descent update on θt
  7. Every T steps: θtarget ← θt; every nT steps: θeval ← θt

- **Design tradeoffs:**
  - Relaxation parameter w: Higher w (up to w*) speeds convergence but risks instability; paper found optimal range 1.2–2.5
  - Policy evaluation iterations n: Controls GPI spectrum from value iteration (n=1) to policy iteration (n large); paper uses n=5
  - Target update frequency T: Smaller T increases sample efficiency but risks instability; paper uses T=100
  - Network architecture: 2 hidden layers (256, 128) with ReLU; deeper networks not explored

- **Failure signatures:**
  - Diverging or oscillating loss: w too large (exceeds w*) or learning rate too high
  - Slow convergence despite low loss: w too close to 1 (degenerates to baseline)
  - High variance in final performance: Stochastic policy evaluation unstable; check LP solver robustness
  - Target network lag: If T or nT too large, Q-values may chase stale targets

- **First 3 experiments:**
  1. **Baseline validation (w=1):** Run D-SOR-MQL with w=1 on Guard-Invader (49 states); confirm performance matches standard Minimax Q-learning (M2DQN baseline) to verify implementation correctness.
  2. **Relaxation parameter sweep:** Run w ∈ {1.2, 1.4, 1.7, 2.0, 2.3, 2.5} on both environments (49 and 121 states); log loss curves and final converged MSE to identify optimal w per environment.
  3. **Ablation on n (GPI depth):** Fix optimal w; vary n ∈ {1, 3, 5, 10} to quantify trade-off between per-iteration cost and convergence speed; measure wall-clock time to reach loss threshold.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can finite-time convergence guarantees be established for D-SOR-MQL when using deep neural network function approximation instead of linear function approximation?
- **Basis in paper:** [explicit] "In this work, while the empirical results utilize deep neural networks, the theoretical analysis is conducted using linear function approximators for tractability. As part of future work, we aim to bridge this gap by extending the analysis to settings where Q-values are approximated using deep neural networks."
- **Why unresolved:** The non-linearity of deep neural networks introduces additional complexity in bounding approximation error and establishing contraction properties that the current linear analysis cannot handle.
- **What evidence would resolve it:** A finite-time convergence proof with explicit bounds that accounts for neural network approximation error, potentially using techniques from neural tangent kernel theory or overparameterization analysis.

### Open Question 2
- **Question:** Is there a principled theoretical framework for adaptively selecting the optimal relaxation parameter w for a given Markov game environment?
- **Basis in paper:** [inferred] The paper demonstrates that optimal w values fall between 1.2 and 2.5, but the selection is entirely empirical (see Table 1). No theoretical guidance is provided for choosing w, despite it being described as "crucial."
- **Why unresolved:** The relationship between environment properties (transition dynamics, reward structure, state space size) and the optimal w remains uncharacterized theoretically.
- **What evidence would resolve it:** Theoretical analysis deriving bounds on optimal w based on game properties, or an adaptive meta-algorithm that provably converges to near-optimal w.

### Open Question 3
- **Question:** How does D-SOR-MQL performance scale to high-dimensional continuous state-action spaces, compared to the small discrete environments (49-121 states) tested?
- **Basis in paper:** [inferred] The paper claims to address "high-dimensional state-action spaces" and "real-world" scenarios, but all experiments use small discrete grid-world environments with 49 or 121 states (Section 6).
- **Why unresolved:** The curse of dimensionality in minimax optimization over joint action spaces may fundamentally limit scalability, and the three-network architecture's stability in complex domains is untested.
- **What evidence would resolve it:** Experiments on benchmark high-dimensional domains (e.g., MuJoCo-based competitive environments) with quantitative comparisons to modern multi-agent deep RL baselines.

### Open Question 4
- **Question:** Can the SOR acceleration technique be extended to general-sum Markov games or n-player (n > 2) settings while preserving finite-time convergence guarantees?
- **Basis in paper:** [explicit] The paper states "In this work, we consider two-player zero-sum games as these are crucial real-world problems, less explored, and challenging to solve." The restriction to two-player zero-sum is fundamental to the minimax formulation.
- **Why unresolved:** General-sum games lack the minimax structure, and n-player games involve complex equilibrium concepts (e.g., Nash equilibrium with multiple players) that may not interact cleanly with the SOR operator.
- **What evidence would resolve it:** Extension of the SOR-Q operator to general-sum or n-player settings with modified contraction proofs, or counterexamples showing fundamental barriers to such extensions.

## Limitations

- Theoretical convergence guarantees only apply to linear function approximation, not the deep neural networks used in experiments
- Optimal relaxation parameter w is highly sensitive to environment specifics with no principled selection method
- Three-network architecture introduces significant hyperparameter sensitivity through T and nT update schedules
- Analysis assumes perfect knowledge of environment dynamics and reward structure

## Confidence

- **High confidence:** The core SOR mechanism (Mechanism 1) and its theoretical foundation in reducing contraction factors
- **Medium confidence:** The GPI framework's effectiveness (Mechanism 2) and the three-network architecture's stability benefits (Mechanism 3)
- **Low confidence:** Generalization of optimal w values across different game types and the scalability of the approach to larger state spaces

## Next Checks

1. **Theoretical-empirical gap analysis:** Compare convergence rates between linear function approximation (where theory applies) and deep networks in controlled experiments to quantify the performance penalty from function approximation mismatch.

2. **Hyperparameter sensitivity study:** Systematically vary w, T, and n across multiple zero-sum game environments to map the stability region and identify robust default settings.

3. **Scalability benchmark:** Evaluate D-SOR-MQL on larger grid sizes (e.g., 21×21 states) and continuous control zero-sum games to assess computational tractability and whether the three-network architecture remains beneficial at scale.