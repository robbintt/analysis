---
ver: rpa2
title: Improving Robustness of Vision-Language-Action Models by Restoring Corrupted
  Visual Inputs
arxiv_id: '2602.01158'
source_url: https://arxiv.org/abs/2602.01158
tags:
- corruption
- image
- visual
- arxiv
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the vulnerability of Vision-Language-Action
  (VLA) models to image corruptions such as sensor noise, dead pixels, and lens contaminants.
  While existing literature focuses on physical occlusions, sensor-level artifacts
  remain under-explored and cause significant performance degradation in VLAs.
---

# Improving Robustness of Vision-Language-Action Models by Restoring Corrupted Visual Inputs

## Quick Facts
- arXiv ID: 2602.01158
- Source URL: https://arxiv.org/abs/2602.01158
- Reference count: 28
- Key result: CRT recovers 85%+ success rates under severe image corruption for most VLAs

## Executive Summary
This paper addresses the vulnerability of Vision-Language-Action (VLA) models to image corruptions such as sensor noise, dead pixels, and lens contaminants. While existing literature focuses on physical occlusions, sensor-level artifacts remain under-explored and cause significant performance degradation in VLAs. The authors introduce the Corruption Restoration Transformer (CRT), a lightweight plug-and-play module that restores corrupted visual inputs before they reach the VLA. CRT leverages advanced transformer mechanisms including Shifted Patch Tokenization, Rotary Position Embeddings, and Locality Self-Attention, trained with an adversarial objective to recover clean observations from corrupted inputs. Extensive experiments on LIBERO and Meta-World benchmarks demonstrate that CRT effectively recovers lost performance, with models like π0.5 maintaining near-baseline success rates (e.g., recovering from 2% to 87% under severe corruption). While some trade-offs exist for smaller VLAs, CRT introduces minimal computational overhead while significantly enhancing robustness to image corruptions.

## Method Summary
The Corruption Restoration Transformer (CRT) is designed as a lightweight, plug-and-play module that restores corrupted visual inputs before they reach the Vision-Language-Action (VLA) model. CRT employs advanced transformer mechanisms including Shifted Patch Tokenization for efficient tokenization, Rotary Position Embeddings (RoPE) for encoding token positions, and Locality Self-Attention to focus on local spatial relationships. The module is trained adversarially to recover clean observations from corrupted inputs. CRT operates by taking corrupted images as input and outputting restored images that are then fed to the VLA model. The training process involves an adversarial objective where CRT learns to reconstruct clean images from corrupted versions while the VLA learns to perform tasks using these restored inputs. The approach is evaluated across multiple VLA architectures including π0, π0.5, π0.8, and SmolVLA on both LIBERO and Meta-World benchmarks with five different corruption types.

## Key Results
- CRT recovers performance from near-zero to 87% success rate for π0.5 under severe corruption
- Larger VLAs (π0, π0.5, π0.8) show minimal performance degradation when CRT is applied
- Smaller VLAs like SmolVLA experience baseline performance drops (58%→47%) when CRT is applied to clean inputs

## Why This Works (Mechanism)
CRT works by restoring corrupted visual inputs before they reach the VLA model, effectively preprocessing the observations to remove sensor-level artifacts that would otherwise degrade performance. The transformer-based architecture leverages locality-aware attention mechanisms to focus on spatial relationships within the image while rotary position embeddings help maintain positional information during reconstruction. The adversarial training objective forces CRT to learn meaningful reconstructions that preserve task-relevant information while removing corruption artifacts. By operating as a pre-processing step, CRT modifies the input distribution to the VLA without requiring changes to the VLA architecture itself, making it a truly plug-and-play solution that can be applied across different VLA models.

## Foundational Learning
- Vision-Language-Action (VLA) models: Neural architectures that process visual observations, language instructions, and output actions for embodied tasks. Needed to understand the target application domain where robustness is critical.
- Image corruption types: Sensor noise, dead pixels, lens contaminants, compression artifacts, motion blur. Understanding these helps identify the specific vulnerabilities being addressed.
- Adversarial training: A technique where two models compete, with one trying to fool the other. Here it's used to train CRT to effectively restore corrupted images.
- Transformer architectures: Neural networks based on self-attention mechanisms, originally developed for NLP but now widely used in vision tasks. Understanding transformers is crucial for grasping CRT's design.

## Architecture Onboarding

Component map:
Corrupted Image -> CRT -> Restored Image -> VLA -> Action

Critical path: Input image → CRT restoration → VLA processing → Action output

Design tradeoffs: CRT prioritizes restoration quality over computational efficiency, accepting slightly higher computational cost for significant performance recovery. The adversarial training approach trades off reconstruction fidelity against robustness to various corruption types.

Failure signatures: On clean images, smaller VLAs show degraded performance (e.g., SmolVLA drops from 58% to 47%). Under severe corruption, CRT may over-smooth details or introduce artifacts that could mislead the VLA.

First experiments to run:
1. Apply CRT to clean images only to measure baseline performance degradation
2. Test CRT with single corruption type (e.g., only dead pixels) to isolate effectiveness
3. Compare CRT output against ground truth clean images using perceptual metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a corruption detection mechanism enable conditional CRT activation to preserve clean-input baseline performance while maintaining corruption recovery?
- Basis in paper: The conclusion states: "Given the modularity of CRT, the entire pipeline can be adapted to engage the restoration process only when a corruption is detected. Such an adjustment would maintain the original performance of the VLA, but also compensate for issues caused by image corruption."
- Why unresolved: The current CRT always processes inputs, causing performance drops on clean data for smaller VLAs (e.g., SmolVLA baseline drops from 58% to 47% on Meta-World). A detection-triggered mechanism was proposed but not implemented or evaluated.
- What evidence would resolve it: A corruption classifier achieving high precision/recall, coupled with evaluation showing baseline performance preserved on clean inputs while matching CRT recovery rates under corruption.

### Open Question 2
- Question: Why do smaller VLAs exhibit higher sensitivity to CRT-induced distributional shifts, and can this trade-off be mitigated?
- Basis in paper: The results note: "smaller VLAs are less robust to feature perturbations and lack the capacity to generalize across subtle distributional shifts inherent to the reconstruction process."
- Why unresolved: The paper documents the performance drop (SmolVLA: 58%→47% baseline on Meta-World with CRT) but does not investigate architectural or training interventions to address it. The underlying cause remains hypothesized but untested.
- What evidence would resolve it: Ablation studies varying VLA capacity, or alternative CRT training objectives (e.g., perceptual loss aligned with VLA feature space), demonstrating reduced baseline degradation while preserving restoration capability.

### Open Question 3
- Question: Does CRT generalize to real-world robotic platforms and corruption types beyond the five simulated artifacts tested?
- Basis in paper: The paper motivates real-world deployment challenges but evaluates exclusively on LIBERO and Meta-World simulations with five handcrafted corruption types. Hardware-specific artifacts (e.g., motion blur, compression artifacts, varying illuminations) were not assessed.
- Why unresolved: Simulation-to-real transfer is not addressed. CRT's adversarial training targets specific corruption patterns; generalization to novel or compound real-world disturbances remains unknown.
- What evidence would resolve it: Real-robot experiments with physical sensor artifacts (dirty lenses, sensor noise under varying conditions) showing success rate recovery comparable to simulation results.

## Limitations
- Evaluation focuses on synthetic corruption patterns rather than real-world degradation scenarios
- Claims about minimal computational overhead lack runtime latency measurements across hardware configurations
- Adversarial training may lead to over-restoration artifacts that could harm downstream policy learning for smaller VLAs

## Confidence

High confidence:
- CRT's architectural design and implementation details are clearly described and technically sound
- Experimental methodology using standardized benchmarks provides reproducible validation

Medium confidence:
- Performance recovery metrics are robust, but generalization to unseen corruption types remains unproven
- Comparison against alternative restoration approaches is limited

Low confidence:
- Claims about plug-and-play nature across diverse VLA architectures lack systematic validation
- Potential negative transfer when CRT is applied to uncorrupted inputs is not addressed

## Next Checks

1. Evaluate CRT's performance on real-world corrupted images from deployed robotic systems rather than synthetic corruptions to assess practical robustness.

2. Conduct ablation studies measuring CRT's impact on inference latency and memory usage across different hardware platforms (GPU vs CPU inference).

3. Test CRT's behavior on clean, uncorrupted inputs to quantify any performance degradation or negative transfer effects that might occur in mixed-quality visual environments.