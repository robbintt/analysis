---
ver: rpa2
title: One-Shot Federated Learning with Classifier-Free Diffusion Models
arxiv_id: '2502.08488'
source_url: https://arxiv.org/abs/2502.08488
tags:
- learning
- client
- data
- federated
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OSCAR, a novel one-shot federated learning
  approach that leverages classifier-free diffusion models to address the communication
  overhead challenge in federated learning. OSCAR eliminates the need for classifier
  training at each client by replacing classifier-guided diffusion models with classifier-free
  diffusion models in the image synthesis phase.
---

# One-Shot Federated Learning with Classifier-Free Diffusion Models

## Quick Facts
- **arXiv ID**: 2502.08488
- **Source URL**: https://arxiv.org/abs/2502.08488
- **Reference count**: 40
- **Primary result**: Reduces communication load by >99% while outperforming state-of-the-art on four datasets

## Executive Summary
OSCAR introduces a one-shot federated learning approach that eliminates classifier training at each client by leveraging classifier-free diffusion models. The method replaces classifier-guided diffusion with classifier-free guidance, allowing text embeddings from BLIP and CLIP to directly condition generation without auxiliary models. Clients upload compact category-averaged text embeddings (512 parameters per category), while the server generates synthetic images and trains a global classifier. OSCAR achieves superior accuracy compared to existing OSFL methods while reducing communication load by over 100x.

## Method Summary
OSCAR addresses the communication overhead challenge in one-shot federated learning by replacing classifier-guided diffusion with classifier-free diffusion models. Each client uses frozen BLIP to generate textual descriptions of images, then encodes them with frozen CLIP text encoder. Per-category average embeddings (512-dim vectors) are uploaded to the server. The server generates synthetic images using classifier-free Stable Diffusion conditioned on these embeddings, then trains a ResNet-18 classifier on the synthesized dataset. This eliminates client-side model training while maintaining generation quality through text-based guidance.

## Key Results
- Reduces client upload size by >100x compared to state-of-the-art diffusion model-assisted OSFL approaches
- Achieves 37.60% accuracy on DomainNet (vs. 34.28% for FedCADO, 33.07% for FedDISC)
- Outperforms existing approaches on four benchmarking datasets while reducing communication load by at least 99%
- Performance plateaus at 30 samples per category, suggesting optimal sample count

## Why This Works (Mechanism)

### Mechanism 1: Classifier-Free Diffusion Eliminates Client-Side Training
Classifier-guided diffusion requires gradient guidance from a separately trained classifier (Eq. 4: ̂ϵt = ϵθ(xt, t, y) − sσt∇xt log p(y|xt)). Classifier-free diffusion instead combines conditional and unconditional score estimates (Eq. 5: ̂ϵt = (1 + s)ϵθ(xt, t, y) − sϵθ(xt, t, ∅)), allowing text embeddings to guide generation directly without any trained classifier. This eliminates the need for auxiliary models at each client while maintaining generation quality.

### Mechanism 2: Foundation Model Text Encoding as Distribution Proxy
Averaging CLIP text embeddings across images within a category creates a compact representation that preserves semantic distribution information. BLIP generates textual descriptions for each image, then per-category averaging produces a single 512-dimensional vector per category. This aggregates distribution information without transmitting raw features or models.

### Mechanism 3: Server-Side Synthetic Dataset Captures Heterogeneity
Generating images conditioned on per-client, per-category encodings produces a synthetic dataset that reflects the non-IID distribution across clients without requiring client model aggregation. The server generates 10 images per category per client, yielding a synthetic dataset with 10 × |R| × C images. Training ResNet-18 on this synthetic dataset produces the global model in one round.

## Foundational Learning

- **Classifier-Free Guidance in Diffusion Models**
  - Why needed here: Understanding how conditioning integrates into the denoising process without external classifiers is essential for debugging generation quality.
  - Quick check question: Can you explain why Eq. 5 combines conditional and unconditional predictions rather than using only the conditional model?

- **Vision-Language Foundation Models (BLIP/CLIP)**
  - Why needed here: The entire encoding pipeline depends on these models producing aligned representations.
  - Quick check question: What happens if BLIP generates descriptions that don't capture discriminative visual features for your specific domain?

- **One-Shot Federated Learning Constraints**
  - Why needed here: OSCAR's design is a direct response to communication and coordination costs in multi-round FL.
  - Quick check question: Why can't traditional FedAvg work in a single round under non-IID data, and how does synthetic data generation circumvent this?

## Architecture Onboarding

- **Component map**: Client-side: BLIP (frozen) → CLIP Text Encoder (frozen) → Category averaging → Upload ȳc vectors; Server-side: Receive all ȳc → Stable Diffusion (frozen) → Generate images → Train ResNet-18 → Broadcast global model

- **Critical path**: Correct CLIP embedding extraction → Proper guidance scale (s=7.5) and denoising steps (T=50) → Synthetic data quality determines global model performance

- **Design tradeoffs**: Communication vs. fidelity (512-dim embeddings may lose fine-grained details); Generalization vs. specificity (diffusion model may not generate accurate samples for specialized domains); Sample count vs. quality (30 samples optimal)

- **Failure signatures**: Low accuracy on specific clients (BLIP descriptions may fail for that domain); Generated images look generic (averaging collapsed distinct modes); Global model fails to generalize (diffusion model prior dominates client-specific features)

- **First 3 experiments**: Reproduce Table I on NICO++ to verify accuracy; Ablate embedding aggregation (compare averaged vs. per-image embeddings); Visualize generated samples per client/domain to inspect domain characteristics

## Open Questions the Paper Calls Out
None

## Limitations
- Foundation model dependencies: Performance critically depends on BLIP/CLIP producing aligned embeddings; no ablation studies test domain-specific fine-tuning
- Generation quality vs. real data: Synthetic-only training underperforms hybrid approaches; no analysis of mode coverage or diversity metrics
- Hyperparameter sensitivity: Fixed guidance scale and sample count work across datasets but lack sensitivity analysis for different domains

## Confidence

**High Confidence**: Communication efficiency claims (99% reduction vs. FedCADO) and relative accuracy rankings across datasets are reproducible given the described pipeline.

**Medium Confidence**: Absolute accuracy numbers depend on unstated training hyperparameters for global ResNet-18.

**Low Confidence**: Claims about BLIP/CLIP representations being "sufficient" for distribution capture lack empirical validation.

## Next Checks

1. **Domain Transfer Analysis**: Run OSCAR on a held-out domain not seen during BLIP/CLIP training to test whether averaged embeddings generalize beyond pre-training distributions.

2. **Embedding Aggregation Ablation**: Compare OSCAR's averaged embeddings against sending per-image embeddings (if communication budget allows) to quantify information loss from aggregation.

3. **Generation Quality Audit**: Generate samples for each client/domain and compute Fréchet Inception Distance (FID) against real data to measure whether diffusion model captures client-specific distributions.