---
ver: rpa2
title: Exploring the Impact of Instruction-Tuning on LLM's Susceptibility to Misinformation
arxiv_id: '2507.18203'
source_url: https://arxiv.org/abs/2507.18203
tags:
- misinformation
- llms
- question
- option
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how instruction-tuning affects large language
  models' susceptibility to misinformation. Through experiments on two proprietary
  and four open-source LLMs, the authors find that instruction-tuned models are significantly
  more likely to accept misinformation when it is presented in the user role compared
  to the assistant role.
---

# Exploring the Impact of Instruction-Tuning on LLM's Susceptibility to Misinformation

## Quick Facts
- arXiv ID: 2507.18203
- Source URL: https://arxiv.org/abs/2507.18203
- Reference count: 27
- Primary result: Instruction-tuning increases LLM susceptibility to user-provided misinformation, with effect amplified when misinformation is presented in separate user turns.

## Executive Summary
This paper investigates how instruction-tuning affects large language models' susceptibility to misinformation. Through experiments on two proprietary and four open-source LLMs, the authors find that instruction-tuned models are significantly more likely to accept misinformation when it is presented in the user role compared to the assistant role. The effect is amplified when misinformation is presented as a separate user turn. Comparing instruction-tuned models to their base versions shows that instruction-tuning shifts models from being most susceptible to assistant-provided misinformation to being most susceptible to user-provided misinformation. Additional factors like misinformation length and warning messages in system prompts also influence susceptibility, with longer misinformation and simple warnings showing varied effectiveness across models.

## Method Summary
The study uses the Farm dataset (BoolQ, NQ, TruthfulQA) with misinformation paragraphs to evaluate susceptibility across three prompt scenarios: Single-Turn Query (STQ), Assistant-Provided Document (APD), and User-Provided Document (UPD). Models first undergo closed-book assessment to establish a baseline of questions they answer correctly without context (Q✓). Misinformation Susceptibility Rate (MSR) is then calculated as the percentage of Q✓ where models select incorrect answers aligned with misinformation. The evaluation includes two proprietary models (GPT-4o, GPT-4o mini) and four open-source models (Llama-3-8B-Instruct, Llama-3.1-8B-Instruct, Qwen2.5-7B-Instruct, Mistral-7B-Instruct-v0.3) plus their base versions. Generation uses top-p=1, temperature=0.2.

## Key Results
- Instruction-tuned models show higher susceptibility to user-provided misinformation (UPD) than assistant-provided misinformation (APD) or standalone queries (STQ)
- The user-turn isolation effect amplifies misinformation susceptibility by 5-22 percentage points across models
- As misinformation length increases, instruction-tuning's user-role prioritization weakens, causing models to revert toward base-model patterns
- Simple warning messages in system prompts help proprietary models but not open-source models

## Why This Works (Mechanism)

### Mechanism 1: Role-Based Attention Prioritization Shift
Instruction-tuning shifts model attention weight from assistant-role toward user-role content. Base models assign higher attention to assistant-role tokens from pre-training patterns where assistant outputs were prediction targets. Instruction-tuning optimizes for user instruction-following, which reweights attention toward user-role content during generation. This shift is a byproduct of instruction-following optimization rather than explicit role-pairing in training data.

### Mechanism 2: User-Turn Isolation Amplification
Presenting misinformation as a separate, dedicated user turn increases its influence on model outputs compared to embedding it in a compound prompt. When user content occupies a distinct conversational turn, the model applies stronger instruction-following priors to that content. The turn boundary acts as a signal to treat the content as authoritative context rather than peripheral detail.

### Mechanism 3: Context-Length Dilution of Instruction-Tuning Effects
As misinformation length increases, instruction-tuning's user-role prioritization weakens, and model behavior shifts toward base-model patterns. Longer contexts engage different attention/processing mechanisms. The signal from instruction-tuning (user-role preference) becomes diluted as context length grows, allowing pre-training-derived assistant-role preferences to reassert.

## Foundational Learning

- **Knowledge Conflict in LLMs**: Understanding when and why models override parametric knowledge with contextual misinformation is central to interpreting experimental results. Quick check: When a model with correct parametric knowledge is given a document contradicting that knowledge, what three behavioral patterns might it exhibit?

- **Role Tokenization and Chat Templates**: The experiments rely on structural role distinctions (user/assistant) embedded through chat templates. Understanding how these are tokenized helps diagnose susceptibility patterns. Quick check: How does an instruction-tuned model's tokenizer distinguish between user and assistant turns during inference?

- **Parametric vs. Contextual Knowledge**: The MSR metric is defined only on questions the model answers correctly without context; understanding this distinction is essential for interpreting susceptibility. Quick check: Why does the MSR calculation exclude questions that the model answers incorrectly in the closed-book setting?

## Architecture Onboarding

- **Component map**: Dataset Layer -> Prompt Construction -> Evaluation Pipeline -> Model Variants
- **Critical path**: 1) Run closed-book assessment to establish Q✓ baseline for each model 2) Construct prompts for all three scenarios with misinformation injected 3) Extract answers and compute MSR; compare UPD vs. APD vs. STQ rankings 4) If base model available, repeat to observe ranking shift
- **Design tradeoffs**: Simplicity vs. ecological validity (single-turn document QA is artificial; real misinformation appears in multi-turn conversations or embedded documents); Dataset coverage (Farm focuses on factual questions; susceptibility may differ for opinion-based or ambiguous claims); Warning mechanism effectiveness varies by model capability
- **Failure signatures**: UPD MSR significantly exceeds APD MSR (user-role susceptibility is active); No change after adding warnings (model may not follow system prompts rigorously); Base model already favors UPD (instruction-tuning not the cause)
- **First 3 experiments**: 1) Replicate STQ/APD/UPD comparison on a new dataset to confirm generalizability beyond Farm 2) Test a model with explicit role-balancing in instruction-tuning data to see if user-role susceptibility can be reduced 3) Vary turn structure to map the boundary conditions of turn-isolation amplification

## Open Questions the Paper Calls Out

### Open Question 1
Do instruction-tuned LLMs larger than 8B parameters exhibit the same shift toward user-role susceptibility observed in 7B–8B models? The authors could not test larger open-source models due to computational constraints, leaving scaling effects unknown. Running the same three experimental scenarios on larger instruction-tuned models (e.g., 13B, 70B, 405B) and comparing susceptibility patterns to their base versions would resolve this.

### Open Question 2
Why does the effect of instruction-tuning on user-role susceptibility diminish as misinformation length increases, causing models to revert toward base-model-like patterns? The mechanism underlying this length-dependent reversion is not explained; the paper only documents the phenomenon. Mechanistic analysis of attention patterns across role tokens at different context lengths would help clarify this.

### Open Question 3
What specific instruction-tuning data or methods cause certain models (e.g., Qwen2.5-7B) to exhibit different susceptibility patterns compared to the majority trend? Proprietary and undisclosed training procedures prevent causal attribution of model-specific behaviors. Controlled experiments training models with systematically varied instruction-tuning configurations would help identify causal factors.

### Open Question 4
Why are simple misinformation warnings in system prompts effective for proprietary models but not for open-source models? The difference could stem from training data, RLHF procedures, or architectural factors—none are isolated or tested. Ablation studies comparing warning effectiveness across models with controlled system prompt adherence training would help explain this differential behavior.

## Limitations
- Experiments limited to 7B-8B models due to resource constraints, leaving scaling effects unknown
- Farm dataset focuses on factual questions, potentially missing susceptibility patterns for opinion-based or ambiguous claims
- Proprietary model training procedures and instruction-tuning data remain undisclosed, preventing full mechanistic understanding

## Confidence
- **High**: Experimental methodology and MSR metric definition are clearly specified and reproducible
- **Medium**: Results generalize across different datasets and model families but may not extend to larger models or different misinformation types
- **Low**: Mechanism explanations are inferred from patterns rather than directly observed through internal model analysis

## Next Checks
1. Verify Farm dataset availability and extract the exact "logical" misinformation paragraphs used in the experiments
2. Implement the prompt templates for each model family and run closed-book assessment to establish Q✓ baselines
3. Run the three scenarios (STQ, APD, UPD) on a small subset of questions to validate the answer extraction/parsing logic before full evaluation