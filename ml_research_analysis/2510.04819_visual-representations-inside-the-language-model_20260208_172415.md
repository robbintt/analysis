---
ver: rpa2
title: Visual Representations inside the Language Model
arxiv_id: '2510.04819'
source_url: https://arxiv.org/abs/2510.04819
tags:
- visual
- language
- image
- information
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how visual information flows through the
  language model component of multimodal models. Despite MLMs' poor performance on
  perception tasks, the authors find that image value tokens in the language model
  contain sufficient information to perform zero-shot perception tasks like segmentation
  and correspondence.
---

# Visual Representations inside the Language Model

## Quick Facts
- arXiv ID: 2510.04819
- Source URL: https://arxiv.org/abs/2510.04819
- Authors: Benlin Liu; Amita Kamath; Madeleine Grunde-McLaughlin; Winson Han; Ranjay Krishna
- Reference count: 31
- Primary result: Image value tokens in MLMs contain sufficient information for zero-shot perception tasks despite poor end-to-end performance.

## Executive Summary
This paper investigates how visual information flows through the language model component of multimodal models. Despite MLMs' poor performance on perception tasks, the authors find that image value tokens in the language model contain sufficient information to perform zero-shot perception tasks like segmentation and correspondence. They show that visual information builds in early layers then degrades in later layers, and that input-agnostic key tokens in later layers contain artifacts that hurt perception. By adding a text prefix to images, they improve visual representations' perception capabilities. They also reveal that MLMs significantly under-utilize existing visual information—in 33.3% of art style questions, the correct visual information is present but not surfaced to output. These findings suggest new directions for training MLMs to better leverage their internal visual representations for improved perception.

## Method Summary
The authors analyze image value tokens stored in the KV cache of LLaVA-OneVision 7B and related models. They extract these tokens layer-by-layer and probe them using linear classifiers for segmentation tasks and nearest-neighbor matching for correspondence. To identify input-agnostic artifacts, they compute L2 variance across 1000 COCO images per key and block attention to low-variance keys in late layers. They also test prefix conditioning by adding text before image tokens and measure how this affects downstream perception performance.

## Key Results
- Visual information in image value tokens builds across early-to-middle layers then degrades in later layers
- Input-agnostic image key tokens in later layers encode artifacts that actively degrade perception
- Text prefixes before images improve visual representation quality via causal attention conditioning
- MLMs significantly under-utilize existing visual information—in 33.3% of art style questions, the correct visual information is present but not surfaced to output

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Visual information in image value tokens builds across early-to-middle layers then degrades in later layers.
- Mechanism: Language model layers progressively refine visual representations through residual connections and attention, but later layers shift focus toward text generation objectives, losing perceptual fidelity.
- Core assumption: Layer-wise probing tasks (segmentation, correspondence) accurately reflect information content rather than probe-specific artifacts.
- Evidence anchors:
  - [abstract] "visual information builds in early layers then degrades in later layers"
  - [Section 3.1, Figure 2] Segmentation performance peaks at layers 14–15 then drops; correspondence peaks at layers 7–10 then declines.
  - [corpus] Weak/no direct corpus support for this specific layer-wise pattern in MLMs.

### Mechanism 2
- Claim: Input-agnostic image key tokens in later layers encode artifacts that actively degrade perception.
- Mechanism: Certain keys exhibit low variance across diverse images (near-constant representations). When text tokens attend to these keys in late layers, they retrieve non-informative or noisy values, introducing hallucination-like artifacts.
- Core assumption: Blocking attention to these keys improves perception *because* they contain artifacts, not because attention redistribution compensates elsewhere.
- Evidence anchors:
  - [abstract] "input-agnostic key tokens in later layers contain artifacts that hurt perception"
  - [Section 3.2, Table 3] Blocking input-agnostic keys in last 10 layers improves POPE (78.1→81.2) and MME scores.
  - [corpus] No corpus papers address input-agnostic artifacts in multimodal attention.

### Mechanism 3
- Claim: Text prefixes before images improve visual representation quality via causal attention conditioning.
- Mechanism: Causal attention allows image value tokens to incorporate prefix text context before being cached. Relevant prefixes (e.g., class names, domain labels) bias representations toward task-aligned features.
- Core assumption: The prefix content—not mere token presence—drives improvement; random prefixes should not help.
- Evidence anchors:
  - [abstract] "adding a text prefix to the image input improves perception capabilities"
  - [Section 4.1, Table 4] Domain prefix improves BDD-10K night+highway segmentation (63.4→68.2); random/incorrect prefixes degrade performance.
  - [corpus] Corpus neighbor "Introducing Visual Perception Token" explores related prefix-like mechanisms but with different framing.

## Foundational Learning

- Concept: **Key-Value Cache in Autoregressive Models**
  - Why needed here: The paper analyzes image value tokens stored in the KV cache; understanding that K/V are precomputed and static for visual tokens is essential to interpret why they remain "purely visual" (uninfluenced by later text).
  - Quick check question: In a standard decoder-only LM, when are key/value vectors for position *i* computed, and can they be modified by tokens at position *j > i*?

- Concept: **Causal vs. Bidirectional Attention**
  - Why needed here: Causal attention ensures image tokens cannot attend to text tokens that follow, which isolates visual representations from linguistic contamination—but also limits their ability to incorporate query-specific context without prefixing.
  - Quick check question: If an image token at position 5 attends to text tokens at positions 10–20 in a standard causal LM, what is wrong with this description?

- Concept: **Probing Tasks for Representation Quality**
  - Why needed here: The paper measures "perception capability" via zero-shot segmentation/correspondence probes on internal representations, not end-to-end task performance. Understanding probes as *diagnostic tools* (not training objectives) is critical.
  - Quick check question: A linear probe on layer-L representations achieves 90% accuracy on task T. Does this mean the model uses this information to solve T during normal inference?

## Architecture Onboarding

- Component map: Image → [ViT Encoder] → [Projection/Adapter] → Visual Tokens (K,V cached) → [Causal Attention: Q_text attends to K_vision + K_text] → [LM Layers 1–N] → [Output Head] → Text Generation

- Critical path:
  1. Visual encoder outputs patch embeddings
  2. Projection aligns embeddings to LM hidden dimension
  3. Image K/V computed once, cached; text queries attend across layers
  4. Layers 1–~2/3: visual information builds (segmentation/correspondence peaks)
  5. Layers ~2/3–N: visual information degrades; input-agnostic artifacts emerge in late keys

- Design tradeoffs:
  - **Fine-tuning visual encoder vs. freezing**: Fine-tuning (as in LLaVA-OneVision) may reduce perceptual fidelity relative to original SigLIP (Table 1 shows SigLIP without MLM FT outperforms on 3/6 tasks).
  - **Prefix conditioning vs. query-time adaptation**: Prefixing is cheap but requires foreknowledge of task/domain; alternative is runtime attention modulation (more complex).
  - **GQA (Group Query Attention)**: Reduces KV pairs per layer (e.g., 4 KV pairs for 28 queries), simplifying analysis but potentially limiting representational diversity.

- Failure signatures:
  - **Hallucination on existence/attribute queries** (POPE, MME): May correlate with input-agnostic artifact keys in late layers.
  - **Low performance on fine-grained spatial tasks despite good VQA**: Visual information present in mid-layer values but not surfaced to output (33.3% of Art Style questions in BLINK).
  - **Discrepancy between encoder and LM representations**: If LM value tokens underperform frozen encoder on segmentation, fine-tuning may have degraded input quality.

- First 3 experiments:
  1. **Layer-wise probing replication**: Extract image value tokens from each layer of LLaVA-OneVision 7B; train linear probes for Pascal-5i segmentation. Verify build-then-drop curve matches Figure 2.
  2. **Input-agnostic key ablation**: Identify low-variance keys across 1000 COCO images (variance threshold ~450 as in Appendix A.6.1); block attention to these keys in layers 20–28; measure POPE F1 change.
  3. **Prefix conditioning test**: On RefCOCO, compare three conditions—no prefix, correct class-name prefix, random prefix. Verify only correct prefix improves segmentation mIOU (expected +0.5 to +1.0 points).

## Open Questions the Paper Calls Out

- **Open Question 1**: How can the finetuning process of visual encoders (e.g., SigLIP) in multimodal models be modified to prevent the loss of perceptual information currently observed relative to the original, un-finetuned encoder?
  - Basis in paper: [explicit] The discussion section explicitly calls for "improving finetuning of visual encoders... so that the visual information provided to the language model after the projection layer is not inferior to that of the original visual encoder without multimodal finetuning."
  - Why unresolved: The authors demonstrate that the finetuned encoder underperforms the original SigLIP on probing tasks, but they do not propose a specific training methodology to mitigate this degradation.
  - What evidence would resolve it: A training regimen or regularization technique that results in post-projection visual representations that match or exceed the perception performance of the frozen, pre-trained SigLIP encoder on the six probing tasks.

- **Open Question 2**: Why do input-agnostic image key tokens in the later layers of language models encode artifacts that degrade perception, and is this phenomenon an inherent byproduct of the model shifting focus to text generation?
  - Basis in paper: [explicit] The authors identify these artifacts via intervention studies and explicitly state, "Our findings... call for further research in identifying why this phenomenon occurs, and how to mitigate the same."
  - Why unresolved: While the paper proves these representations harm performance (blocking them improves POPE/MME scores), it does not determine the mechanistic origin of these artifacts during the model's forward pass or training.
  - What evidence would resolve it: A causal mechanism linking specific attention head functions or training dynamics to the emergence of these artifacts, demonstrated by a training modification that prevents their formation without reducing text generation quality.

- **Open Question 3**: How can language models be trained or architecturally modified to reliably surface the visual information already present in their intermediate representations to the final output, specifically bridging the gap identified in the 33.3% of Art Style failures?
  - Basis in paper: [explicit] The paper concludes by suggesting "new directions for training... improving language model control of existing visual information," citing the specific finding that "in 33.3% of Art Style questions... perception information present in the language model is not surfaced to the output."
  - Why unresolved: The study quantifies the under-utilization but offers only a basic "text prefixing" method as a partial control mechanism, leaving the specific mechanism to force surfacing largely unexplored.
  - What evidence would resolve it: A method that significantly increases the accuracy on the Art Style subset of the BLINK benchmark specifically for instances currently labeled as "Value ✓ MLM ✗" (correct latent info, incorrect output).

## Limitations

- Probing task validity: Segmentation and correspondence probes may not perfectly isolate perceptual information—they could conflate spatial reasoning, linguistic cues, or low-level features.
- Generalization across models: Results are primarily based on LLaVA-OneVision 7B; limited systematic comparison across architectures.
- Variance threshold for input-agnostic keys: The threshold of 450 is chosen empirically based on bimodal distribution peaks and may not generalize.

## Confidence

- **High confidence**: Visual information builds in early layers then degrades in later layers (supported by consistent probe performance curves across multiple tasks).
- **Medium confidence**: Input-agnostic key tokens in later layers contain artifacts that hurt perception (supported by blocking experiments, but causal mechanism not fully validated).
- **Medium confidence**: Text prefixes improve visual representation quality via causal attention conditioning (supported by controlled prefix experiments, but semantic vs. non-semantic prefix effects not fully disambiguated).

## Next Checks

1. **Cross-model validation of layer-wise patterns**: Replicate layer-wise segmentation and correspondence probing on at least two additional MLM architectures (e.g., Qwen2.5-VL, Llama-3-LLaVA-NeXT) to verify that visual information consistently builds then degrades across layers.

2. **Ablation of input-agnostic key blocking**: Perform ablation studies where attention to random non-input-agnostic keys in late layers is blocked to confirm that improvements are specific to input-agnostic keys rather than any late-layer attention modulation.

3. **Prefix semantic validation**: Test random, reversed, and semantically unrelated prefixes alongside correct domain prefixes on BDD-10K to quantify the difference between semantic conditioning and simple prefix presence effects.