---
ver: rpa2
title: 'Reasoning RAG via System 1 or System 2: A Survey on Reasoning Agentic Retrieval-Augmented
  Generation for Industry Challenges'
arxiv_id: '2506.10408'
source_url: https://arxiv.org/abs/2506.10408
tags:
- reasoning
- retrieval
- agentic
- search
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey paper systematically reviews Reasoning Agentic RAG,
  a paradigm integrating decision-making and adaptive tool use into retrieval-augmented
  generation (RAG) workflows. It categorizes methods into predefined reasoning (structured,
  rule-based pipelines) and agentic reasoning (autonomous, model-driven tool interaction).
---

# Reasoning RAG via System 1 or System 2: A Survey on Reasoning Agentic Retrieval-Augmented Generation for Industry Challenges

## Quick Facts
- **arXiv ID:** 2506.10408
- **Source URL:** https://arxiv.org/abs/2506.10408
- **Reference count:** 7
- **Primary result:** Systematically categorizes Reasoning Agentic RAG into predefined (structured, rule-based) and agentic (autonomous, model-driven) approaches

## Executive Summary
This survey systematically reviews Reasoning Agentic RAG, a paradigm that integrates decision-making and adaptive tool use into retrieval-augmented generation workflows. The paper establishes a dual-process taxonomy based on cognitive science principles, distinguishing between "System 1" (predefined, modular pipelines) and "System 2" (agentic, autonomous tool interaction) approaches. It identifies two implementation strategies: prompt-based approaches leveraging pretrained LLM capabilities and training-based methods using reinforcement learning to optimize tool-use policies. The study highlights key challenges including retrieval efficiency, reward function design, and generalization in dynamic environments.

## Method Summary
The paper conducts a systematic literature review to classify Reasoning Agentic RAG methods into two main categories: predefined reasoning (following fixed modular pipelines) and agentic reasoning (autonomous, model-driven tool interaction). Methods are further subdivided into prompt-based approaches that leverage LLM capabilities through in-context learning, and training-based methods that use reinforcement learning to optimize decision policies. The survey synthesizes existing research without introducing experimental datasets, instead providing a comprehensive taxonomy and identifying future research directions.

## Key Results
- Establishes dual-process taxonomy (System 1 vs. System 2) for reasoning agentic RAG
- Identifies prompt-based vs. training-based implementation strategies
- Highlights challenges in retrieval efficiency, reward function design, and generalization
- Provides curated GitHub repository with relevant research: https://github.com/ByebyeMonica/Reasoning-Agentic-RAG

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Structured, modular pipelines enhance reliability in well-defined tasks by enforcing fixed reasoning steps
- **Mechanism:** Predefined Reasoning (System 1) employs fixed control logic where queries pass through structured workflows (route, decompose, retrieve) before generation, reducing the model's search space
- **Core assumption:** Task requirements can be anticipated and mapped to static workflows prior to inference
- **Evidence anchors:** Abstract categorization of "predefined reasoning, which follows fixed modular pipelines to boost reasoning"; Section 3 descriptions of RAGate and Self-RAG routing mechanisms
- **Break condition:** Performance degrades in open-ended or novel scenarios where fixed pipeline logic doesn't align with complex user intent

### Mechanism 2
- **Claim:** Prompting leverages intrinsic LLM instruction-following to enable dynamic, adaptive tool use without weight updates
- **Mechanism:** Prompt-based Agentic Reasoning (System 2) uses specialized tokens or few-shot examples to guide the model in autonomously identifying knowledge gaps and generating API calls based on reasoning traces
- **Core assumption:** Pretrained LLMs possess sufficient reasoning capability and instruction adherence to reliably select tools and interpret returned data
- **Evidence anchors:** Section 4.1 notes ReAct "interleaves reasoning steps with tool use to guide retrieval based on emerging knowledge gaps"
- **Break condition:** Fails when reasoning chains exceed context window limits or when models hallucinate tool arguments due to prompt ambiguity

### Mechanism 3
- **Claim:** Reinforcement Learning optimizes decision policies for when and what to retrieve, creating more efficient search behaviors
- **Mechanism:** Training-based Agentic Reasoning treats search environment as RL setup, learning policies to generate search-triggering tokens by maximizing rewards like answer accuracy or retrieval success
- **Core assumption:** Defined reward signals can proxy the complex cognitive process of effective research
- **Evidence anchors:** Abstract highlights "training-based methods using reinforcement learning to optimize tool-use policies"; Section 4.2 describes Search-R1 modeling search as part of RL environment
- **Break condition:** If rewards are too sparse, agents struggle to learn intermediate retrieval steps or fall into unproductive searching loops

## Foundational Learning

- **Concept:** Dual-Process Theory (System 1 vs. System 2)
  - **Why needed here:** The paper frames the entire taxonomy around cognitive science principles; distinguishing between fast, structured heuristic thinking and slow, deliberative adaptive thinking is essential for understanding system design trade-offs
  - **Quick check question:** Does your use case require rigid, low-latency compliance (System 1) or creative, multi-step problem solving (System 2)?

- **Concept:** In-Context Learning & Chain-of-Thought (CoT)
  - **Why needed here:** Prompt-based agents rely on the model's ability to think through problems before acting; understanding how reasoning traces facilitate tool selection is critical for debugging agentic behavior
  - **Quick check question:** Can you trace the model's "Thought" process to see why it decided to call a specific tool?

- **Concept:** Reinforcement Learning from Environmental Feedback
  - **Why needed here:** Advanced systems move beyond prompting into training; grasping how rewards are used to update model weights to create more robust agents is essential
  - **Quick check question:** Is your agent learning a policy to search better over time, or is it repeating static behaviors defined in the prompt?

## Architecture Onboarding

- **Component map:** LLM Core -> Reasoning Controller -> Tool Interface -> Context Manager
- **Critical path:**
  1. Input Analysis: System determines if retrieval is needed (Routing/Thought)
  2. Action Formulation: System generates query or API call
  3. Execution & Distillation: Tools return raw data; Context Manager filters noise
  4. Synthesis: LLM generates final answer or updates reasoning trace

- **Design tradeoffs:**
  - Control vs. Flexibility: Predefined workflows (System 1) are easier to debug and faster but brittle; Agentic workflows (System 2) are flexible but harder to control and potentially slower/more costly
  - Prompt vs. Train: Prompt-based agents are cheap to implement but inconsistent on edge cases; Training-based agents require significant compute/data but yield more robust policies

- **Failure signatures:**
  - Unproductive Looping: Agent repeatedly searches with slight variations
  - Context Poisoning: Retrieval of noisy/contradictory documents disrupts LLM's reasoning logic
  - Hallucinated Tool Calls: Model generates syntactically incorrect API calls or hallucinates tool capabilities

- **First 3 experiments:**
  1. Baseline Route: Implement simple "RAGate" style router that triggers retrieval only if model signals low confidence
  2. Prompt-Based Loop: Build ReAct-style loop where model must generate "Thought" before "Search" action to verify grounding
  3. Reward Shaping: In constrained environment, implement simple reward for "reduction in search queries" vs. "answer correctness" to observe basic policy optimization

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can fine-grained reward functions be designed to evaluate intermediate reasoning steps and document relevance rather than relying solely on final answer correctness?
- **Basis in paper:** Section 5 notes that simple outcome-based rewards may not offer adequate guidance for complex RAG tasks requiring multi-step reasoning
- **Why unresolved:** Defining automatic, scalable metrics for "reasoning coherence" or "information cross-validation" during training is significantly more complex than evaluating final output exactness
- **What evidence would resolve it:** Demonstration of RL framework where process-oriented rewards result in higher accuracy and faster convergence than outcome-only rewards on multi-hop QA benchmarks

### Open Question 2
- **Question:** What effective mechanisms can prevent retrieval agents from entering unproductive search loops or performing redundant retrievals?
- **Basis in paper:** Section 5 identifies improving efficiency as critical, specifically noting the need to prevent agents from getting stuck in loops of unproductive searching
- **Why unresolved:** Agents trained to be persistent may struggle to distinguish between a "fruitless" path and one that simply requires more effort, leading to wasted computation
- **What evidence would resolve it:** Model architecture or training protocol that minimizes average number of retrieval steps per query while maintaining or improving answer accuracy

### Open Question 3
- **Question:** How can agents be trained to autonomously utilize advanced API configurations, such as filtering by date or domain, rather than generating only basic text queries?
- **Basis in paper:** Section 5 proposes "Enhancing tool interaction through advanced configuration" to allow for more targeted and strategic retrieval aligned with task demands
- **Why unresolved:** Current action spaces typically limit agents to generating text strings, whereas mapping abstract information needs to complex, structured API parameters requires larger and more diverse action space
- **What evidence would resolve it:** Agentic system successfully leveraging API metadata filters (e.g., sorting by recency) to solve temporal reasoning tasks that defeat standard text-query agents

## Limitations
- Analysis relies heavily on conceptual descriptions rather than empirical benchmarking across methods
- Survey focuses primarily on single-agent scenarios with limited discussion of multi-agent coordination challenges
- Treatment of failure modes remains somewhat abstract without systematic error analysis across different reasoning paradigms

## Confidence

- **High Confidence:** The dual-process theoretical framework (System 1/System 2) is well-grounded in cognitive science and consistently applied throughout the taxonomy
- **Medium Confidence:** The categorization of specific methods into the proposed taxonomy appears reasonable based on their described mechanisms
- **Low Confidence:** Claims about relative performance advantages lack empirical validation in the survey itself

## Next Checks
1. **Empirical benchmarking:** Implement representative examples from each category (predefined, prompt-based agentic, training-based agentic) and test them on standardized multi-hop reasoning datasets to validate claimed trade-offs between efficiency and adaptability
2. **Context window stress test:** Systematically evaluate how prompt-based agentic approaches degrade as reasoning traces exceed different context window sizes, particularly testing the "lost-in-the-middle" hypothesis
3. **Reward function ablation:** For training-based methods, conduct controlled experiments varying reward function design (sparse vs. dense rewards, different weighting of retrieval vs. answer quality) to determine optimal shaping strategies for different task types