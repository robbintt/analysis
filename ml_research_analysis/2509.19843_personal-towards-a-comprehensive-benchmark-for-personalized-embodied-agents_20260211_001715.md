---
ver: rpa2
title: 'PersONAL: Towards a Comprehensive Benchmark for Personalized Embodied Agents'
arxiv_id: '2509.19843'
source_url: https://arxiv.org/abs/2509.19843
tags:
- object
- navigation
- embodied
- agents
- personalized
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PersONAL introduces a comprehensive benchmark for evaluating personalized
  embodied AI agents in human-centric environments. The benchmark addresses the challenge
  of modeling individual human preferences and behaviors in realistic household scenarios.
---

# PersONAL: Towards a Comprehensive Benchmark for Personalized Embodied Agents

## Quick Facts
- arXiv ID: 2509.19843
- Source URL: https://arxiv.org/abs/2509.19843
- Reference count: 40
- Primary result: Introduces PersONAL, a benchmark for personalized embodied AI agents with over 2,000 episodes across 30+ photorealistic homes, revealing substantial performance gaps between current agents and human performance.

## Executive Summary
PersONAL addresses the challenge of evaluating personalized embodied AI agents that must navigate human-centric environments while respecting individual preferences and ownership associations. The benchmark requires agents to identify, retrieve, and navigate to objects associated with specific users based on natural-language queries like "find Lily's backpack." By using text-based scene descriptions rather than image-based queries, PersONAL mirrors realistic human communication patterns and tests agents' ability to reason over personalized information. Experiments demonstrate that state-of-the-art baselines significantly underperform human capabilities, highlighting the need for agents that can perceive, reason, and memorize personalized information in household settings.

## Method Summary
The PersONAL benchmark leverages the Habitat simulator with HM3D photorealistic scenes to create personalized navigation tasks. Agents receive natural language scene descriptions explicitly linking objects to owners, then must either actively navigate (Object Goal Navigation) or ground objects in pre-mapped scenes (Personalized Object Grounding). The benchmark includes over 2,000 episodes with varying difficulty levels based on ownership complexity (1-to-1, 1-to-N, and N-to-N mappings). A key innovation is the Region-gated Personalized Grounding method, which uses descriptive phrases to mask semantic maps and improve localization accuracy by focusing on user-relevant regions rather than global matching.

## Key Results
- State-of-the-art baselines show substantial performance gaps compared to human performance on personalized navigation tasks
- Region-gated Personalized Grounding method outperforms naive query-score approaches, achieving 25.3% success rate on easy difficulty
- Performance degrades significantly on harder difficulty levels with shared ownership, indicating limitations in current personalization capabilities
- Text-based queries create more realistic human-agent interaction scenarios compared to image-based retrieval methods

## Why This Works (Mechanism)

### Mechanism 1: Language-Grounded Ownership Association
- Claim: Agents can resolve personalized queries by parsing semantic associations from textual scene descriptions rather than relying on visual familiarity
- Mechanism: Replaces image-based instance retrieval with natural language descriptions, forcing agents to reason over text to identify correct target objects
- Core assumption: Agents possess capable language understanding modules to parse spatial and possessive relationships
- Evidence anchors: Abstract mentions natural-language scene descriptions with explicit associations; Section III states deliberate avoidance of image-based cues

### Mechanism 2: Region-Gated Semantic Retrieval
- Claim: Masking semantic maps to retain only user-relevant regions improves localization accuracy over global query matching
- Mechanism: RPG method uses person-centric descriptive phrases to create binary masks over feature maps, computing cosine similarity only within unmasked regions
- Core assumption: Visual features of target objects align well with text embeddings in shared embedding space
- Evidence anchors: Section V-B describes RPG algorithm; Table V shows RPG outperforming naive Query-Score

### Mechanism 3: Scalable Difficulty via Graph Constraints
- Claim: Structuring ownership complexity as bipartite graphs with increasing overlap effectively tests memory efficiency beyond simple one-to-one lookup
- Mechanism: Dataset defines Easy (1-to-1), Medium (1-to-N), and Hard (N-to-N shared objects) to prevent naive memorization solutions
- Core assumption: Agent possesses memory architecture capable of storing and querying relational data alongside spatial data
- Evidence anchors: Section IV defines ownership matrix constraints; Figure 4 visualizes bipartite graph complexity

## Foundational Learning

- **Object Goal Navigation (ObjectNav)**: Base task format where agents move in 3D simulator with discrete actions to find semantic objects rather than static coordinates. Quick check: Can you explain the difference between "PointNav" and "ObjectNav" in Habitat context?

- **Semantic Mapping / Spatial Memory**: Required for Personalized Object Grounding task, storing semantic features (VLM embeddings) in maps rather than simple occupancy flags. Quick check: How does storing BLIP-2 embeddings in grid cells differ from storing "occupied/free" flags?

- **Zero-Shot Generalization**: Baselines operate zero-shot using pre-trained models without fine-tuning on PersONAL dataset. Quick check: Why does the paper argue zero-shot methods struggle with "Hard" split compared to humans?

## Architecture Onboarding

- **Component map**: Habitat-Sim (environment) -> LocoBot (embodiment) -> Owl-ViT/BLIP-2 (perception) -> GPT-4.1 (reasoning) -> 2D/3D Feature Map (memory)

- **Critical path**: 
  1. Input: RGB-D observation + textual "Scene Summary" + "Query"
  2. Reasoning: LLM extracts relevant target phrase from summary
  3. Action: PAN explores using frontier-based navigation; POG scores stored feature map
  4. Output: Coordinates (POG) or STOP action (PAN)

- **Design tradeoffs**: 
  - Text vs. Image Queries: Rejects image-based queries to mimic realistic human interaction
  - Memory vs. API Calls: Prefers local structured feature space over continuous LLM API calls

- **Failure signatures**:
  - Detector Hallucination: Open-set detectors trigger on HM3D artifacts
  - Context Drift: LLM fails to associate shared objects with specific user query
  - Map Aliasing: Visually similar objects confuse region-gated scoring

- **First 3 experiments**:
  1. Baseline Validation: Run Random and VLFM baselines on "Easy" split to verify SPL/SR metrics
  2. Ablation on Memory: Implement POG task using random map vs. BLIP-2 feature map
  3. Reasoning Stress Test: Test "Hard" split using weaker LLM vs. GPT-4

## Open Questions the Paper Calls Out
None identified in provided materials.

## Limitations
- Reliance on GPT-4.1 for generating scene summaries may introduce bias affecting agent performance
- Current scope limited to household environments, potentially limiting generalizability to other domains
- Evaluation metrics (SPL and Success Rate) may not fully capture nuanced aspects of personalized navigation

## Confidence

**High Confidence**: Fundamental benchmark design (text-based queries, difficulty scaling, PAN/POG task distinction) is well-justified and technically sound; observed performance gap between baselines and humans is clear signal of genuine challenge.

**Medium Confidence**: Region-gated Personalized Grounding implementation is reasonable but lacks extensive ablation studies on alternative gating strategies or memory architectures.

**Low Confidence**: Assertion that this is first comprehensive benchmark for personalized embodied agents is difficult to verify given rapid field evolution; human performance comparisons are qualitative rather than controlled experiments.

## Next Checks

1. **Language Understanding Stress Test**: Implement benchmark using multiple LLM variants (from small local models to GPT-4) to quantify language understanding impact on navigation success across all difficulty levels.

2. **Detector Robustness Analysis**: Conduct controlled experiments where open-vocabulary detector is intentionally confused by similar objects to measure system resilience to detection errors.

3. **Memory Architecture Comparison**: Implement alternative memory architectures (graph-based vs. grid-based) for POG task to determine optimal approach for personalized navigation.