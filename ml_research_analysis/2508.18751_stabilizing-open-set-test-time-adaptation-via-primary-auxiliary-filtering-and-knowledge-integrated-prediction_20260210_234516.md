---
ver: rpa2
title: Stabilizing Open-Set Test-Time Adaptation via Primary-Auxiliary Filtering and
  Knowledge-Integrated Prediction
arxiv_id: '2508.18751'
source_url: https://arxiv.org/abs/2508.18751
tags:
- open-set
- data
- filtering
- closed-set
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Primary-Auxiliary Filtering (PAF) and Knowledge-Integrated
  Prediction (KIP) to stabilize open-set test-time adaptation. The key challenge is
  error accumulation when adapting models to domain-shifted test data containing both
  closed-set and open-set samples.
---

# Stabilizing Open-Set Test-Time Adaptation via Primary-Auxiliary Filtering and Knowledge-Integrated Prediction

## Quick Facts
- arXiv ID: 2508.18751
- Source URL: https://arxiv.org/abs/2508.18751
- Reference count: 40
- Key outcome: Proposes PAF and KIP to stabilize open-set test-time adaptation, achieving up to 9.35% improvement in open-set discrimination and 2.26% in closed-set accuracy

## Executive Summary
This paper addresses error accumulation in open-set test-time adaptation (OSTTA) by introducing Primary-Auxiliary Filtering (PAF) and Knowledge-Integrated Prediction (KIP). The core challenge is that domain-shifted test data contains both closed-set and open-set samples, and standard entropy minimization approaches degrade performance when open-set samples contaminate the adaptation process. PAF stabilizes filtering by combining an adapting model with an EMA model, while KIP improves inference through weighted ensemble of source, adapting, and EMA models. The method achieves substantial improvements across multiple benchmarks, outperforming existing approaches by up to 9.35% on open-set discrimination and 2.26% on closed-set accuracy.

## Method Summary
The method operates in two phases: adaptation and inference. During adaptation, PAF uses an entropy-based primary filter (from the adapting model) and an auxiliary filter (from the EMA model) to selectively apply entropy minimization (soft filtering) or maximization (hard filtering) based on whether samples are classified as closed-set or open-set. The adapting model updates only Batch Normalization layers, and the EMA model tracks the adapting model with decay rate β=0.999. During inference, KIP combines logits from the source model, adapting model, and EMA model using confidence-based weights, where each model's contribution is proportional to its confidence relative to the average across all three models.

## Key Results
- Achieves up to 9.35% improvement in open-set discrimination (AUROC) compared to baselines
- Improves closed-set accuracy by up to 2.26% while maintaining strong open-set discrimination
- Demonstrates robustness across varying open-set ratios (0.25x to 1.75x contamination)
- Outperforms entropy maximization-only approaches by preventing confirmation bias in filtering

## Why This Works (Mechanism)

### Mechanism 1: Primary-Auxiliary Filtering (PAF)
Combines adapting model (primary) with EMA model (auxiliary) to prevent error accumulation. Primary filter captures current domain knowledge; auxiliary filter validates filtering decisions. For entropy minimization, soft filtering weights loss by EMA confidence; for entropy maximization, hard filtering requires both filters to agree.

### Mechanism 2: Knowledge-Integrated Prediction (KIP)
Weighted ensemble of source, adapting, and EMA models improves inference robustness. Per-sample weights based on each model's confidence relative to average across all three models. Calibrates predictions when adapting or EMA models become unreliable due to open-set contamination.

### Mechanism 3: Entropy-Maximization for Open-Set Learning
Maximizes entropy of confidently detected open-set samples to improve discrimination. Teaches model to output uniform distributions for unknown inputs, improving energy-based open-set scores without degrading closed-set accuracy when filtering is accurate.

## Foundational Learning

- **Exponential Moving Average (EMA) for stability**
  - Why needed: Provides temporally-smoothed model parameters, reducing sensitivity to noisy batches
  - Quick check: If β = 0.9 (vs. 0.999), would EMA be more or less stable? How might this harm PAF?

- **Entropy as confidence measure**
  - Why needed: Both filtering and KIP rely on entropy to measure model confidence
  - Quick check: For 10-class problem, what's maximum possible entropy? What does low entropy indicate?

- **Open-set vs. closed-set in TTA**
  - Why needed: OSTTA assumes test data contains classes not seen during training
  - Quick check: Why would minimizing entropy on open-set samples harm the model?

## Architecture Onboarding

- **Component map:**
  Source Model (frozen) -> Adapting Model -> Primary Filter (Fpr) -> Auxiliary Filter (Faux) -> KIP Ensemble -> Final Prediction

- **Critical path:**
  1. Forward pass through adapting model → compute Fpr via entropy threshold
  2. If Fpr = 0: forward through EMA model → compute Faux
  3. Compute L_PAF with soft/hard filtering
  4. Update adapting model (BN layers only)
  5. Update EMA: f^EMA_t = β · f^EMA_{t-1} + (1-β) · f_θ_t
  6. For inference: compute KIP weighted ensemble

- **Design tradeoffs:**
  - Batch size: Smaller batches work but may degrade some methods; PAF-KIP is robust
  - Threshold τ: Paper uses 0.4 × log(C); robust to variations
  - α (entropy-max weight): 2.0 for CIFAR, 0.7 for ImageNet
  - Memory: No replay buffer needed (lower memory, better privacy)

- **Failure signatures:**
  - Confirmation bias: Wrong filtering accumulates → closed-set accuracy drops after ~300 batches
  - Underfitting: Auxiliary filter too aggressive → model trains on too few samples
  - Open-set leakage: Filtering fails → open-set samples contaminate closed-set training

- **First 3 experiments:**
  1. Reproduce Figure 1: Run CIFAR100-C + Textures-C with adapting-only, EMA-only, and PAF filtering
  2. Ablate KIP: Compare adapting, EMA, source, simple average, and KIP weighting on ImageNet-C + Places365-C
  3. Stress test open-set ratio: Run Table 8 experiment varying open-set ratio from 0.25x to 1.75x

## Open Questions the Paper Calls Out

- **How does PAF-KIP perform in strictly online adaptation with batch size 1?**
  - Basis: Table 7 evaluates robustness to smaller batch sizes but only examines sizes down to 50
  - Why unresolved: Method relies on batch-level statistics which may fail with single samples
  - Evidence needed: Evaluation on CIFAR/ImageNet-C with sample-by-sample adaptation

- **How does performance degrade when open-set data is semantically similar to closed-set classes?**
  - Basis: Experiments use distinct datasets for open-set samples, potentially overstating capabilities
  - Why unresolved: Entropy-based filtering assumes open-set samples induce high uncertainty
  - Evidence needed: Testing on fine-grained dataset where open-set classes are sub-categories of closed-set

- **Can KIP module be compressed or distilled to reduce inference latency?**
  - Basis: Table 6 reports significantly higher inference latency (0.083s) compared to baseline TENT (0.024s)
  - Why unresolved: Current design requires forward passes through three models
  - Evidence needed: Study evaluating latency reduction trade-offs from removing or distilling source/EMA models

## Limitations
- Method effectiveness heavily depends on accurate filtering; misclassification can degrade performance
- Fixed 1:1 open-set ratio may not reflect real-world scenarios with varying contamination levels
- Memoryless approach (no replay buffer) may limit adaptation on very long streams

## Confidence
- **High Confidence:** PAF mechanism combining adapting and EMA models is well-supported by ablation studies
- **Medium Confidence:** KIP's weighted ensemble approach is empirically effective but theoretical justification could be stronger
- **Medium Confidence:** Claims about robustness to open-set ratio variations are supported but range tested may not cover extremes

## Next Checks
1. Systematically vary filtering threshold τ and measure trade-off between closed-set accuracy retention and open-set discrimination
2. Replicate Figure 1b to confirm error accumulation with adapting-only filtering, and measure how different EMA decay rates affect stability
3. Extend Table 8 by testing extreme open-set ratios (0.1x and 2.0x) to verify robustness claims