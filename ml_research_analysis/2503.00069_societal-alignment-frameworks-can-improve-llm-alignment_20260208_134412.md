---
ver: rpa2
title: Societal Alignment Frameworks Can Improve LLM Alignment
arxiv_id: '2503.00069'
source_url: https://arxiv.org/abs/2503.00069
tags:
- alignment
- https
- values
- llms
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper argues that improving large language model (LLM) alignment
  requires incorporating insights from societal alignment frameworks, including social,
  economic, and contractual alignment. The authors frame LLM alignment as a principal-agent
  problem where an incomplete contract between model developers and models leads to
  misalignment.
---

# Societal Alignment Frameworks Can Improve LLM Alignment

## Quick Facts
- arXiv ID: 2503.00069
- Source URL: https://arxiv.org/abs/2503.00069
- Reference count: 40
- This paper argues that improving large language model (LLM) alignment requires incorporating insights from societal alignment frameworks, including social, economic, and contractual alignment.

## Executive Summary
This paper argues that improving large language model (LLM) alignment requires incorporating insights from societal alignment frameworks, including social, economic, and contractual alignment. The authors frame LLM alignment as a principal-agent problem where an incomplete contract between model developers and models leads to misalignment. They propose solutions drawn from societal frameworks: social alignment focuses on instilling and dynamically adapting norms and values; economic alignment suggests using welfare economics and pluralistic approaches to balance diverse preferences; and contractual alignment advocates for standardized documentation and scalable oversight mechanisms like constitutional AI. The paper emphasizes that uncertainty in alignment is not merely a technical challenge but an essential feature for ethical deployment, requiring better methods for communicating uncertainty to users. Ultimately, the authors advocate for a shift from developer-centered to collaborative, user-centric approaches to LLM alignment.

## Method Summary
The paper does not provide a single algorithm but rather a conceptual framework. It formally defines the contract using the Bradley-Terry model for preference probability and suggests improving this baseline via social alignment (instilling contextual rules/norms), economic alignment (using Pareto efficiency or social welfare functions), and contractual alignment (implementing internal constraints like Constitutional AI). The approach is to establish a baseline reward model and then implement one or more of these societal alignment interventions to address misalignment.

## Key Results
- LLM misalignment stems from incomplete contracts between developers and models, not just technical failures
- Societal frameworks (social, economic, contractual) offer mechanisms to guide LLM behavior in incomplete contracting environments
- Uncertainty about value conflicts and context-dependent trade-offs is essential for ethical deployment, not merely a defect to eliminate

## Why This Works (Mechanism)

### Mechanism 1: Incomplete Contract Framing
- Claim: LLM misalignment stems from incomplete contracts between developers and models, manifesting as reward function misspecification rather than purely technical failures.
- Mechanism: The principal (developer) defines a reward function r(x, y) to incentivize the agent (LLM). Because anticipating all possible states is infeasible, the contract is inherently incomplete. This incompleteness enables reward hacking (optimizing the proxy, not the goal), jailbreaking (exploiting specification gaps), and fake alignment (superficial compliance without goal adoption).
- Core assumption: The principal-agent framework from contract theory accurately captures the human-LLM relationship and its failure modes.
- Evidence anchors:
  - [abstract]: "Current alignment methods often lead to misspecified objectives, reflecting the broader issue of incomplete contracts"
  - [Section 3.3]: "A common outcome of reward misspecification is reward hacking, where an agent optimizes for the reward itself rather than the intended behavior"
  - [corpus]: Related work "Alignment and Safety in Large Language Models" (FMR: 0.564) discusses safety mechanisms broadly but does not empirically validate incomplete contract theory as causal
- Break condition: If misalignment can be demonstrated to arise primarily from architectural constraints or data distribution shifts rather than reward misspecification, the mechanism's explanatory power weakens.

### Mechanism 2: Societal Frameworks Transfer
- Claim: Mechanisms from social, economic, and contractual alignment can guide LLM behavior in incomplete contracting environments where technical solutions alone fail.
- Mechanism: Social alignment instills norms/values and enables dynamic adaptation; economic alignment applies Pareto efficiency and social welfare functions to balance diverse preferences; contractual alignment uses external documentation (model cards, datasheets) and internal enforcement (constitutional AI, debate) to constrain behavior.
- Core assumption: Coordination mechanisms effective for human societies transfer to human-LLM interactions.
- Evidence anchors:
  - [Section 4]: "Institutions such as society, economy, and law enable us to thrive despite incompleteness"
  - [Section 4.2.1]: "Pareto efficiency offers a valuable lens for balancing competing human preferences"
  - [corpus]: "Cultural Bias in Large Language Models" (FMR: 0.0, no citation data) shows LLMs fail to represent diverse moral frameworks, suggesting societal alignment mechanisms are needed but current implementations are inadequate
- Break condition: If societal mechanisms prove too context-dependent to generalize across deployment domains (e.g., healthcare vs. legal vs. casual chat), the framework fragment rather than integrate.

### Mechanism 3: Essential Uncertainty
- Claim: Certain uncertainty is a feature enabling ethical deployment, not merely a defect to eliminate.
- Mechanism: Epistemic uncertainty (knowledge gaps leading to confident hallucination) is harmful. However, uncertainty about value conflicts and context-dependent trade-offs (helpfulness vs. harmlessness) is essential for navigating underspecified objectives. This requires explicit uncertainty communication to users via hedging, confidence scores, or interface design.
- Core assumption: Users can interpret uncertainty signals effectively when properly communicated.
- Evidence anchors:
  - [Section 5.1]: Example of LLM confidently stating wrong Antarctic temperature with claimed 100% confidence
  - [Section 5.2]: "essential uncertainty can arise from evolving human values, conflicting societal norms"
  - [corpus]: Corpus lacks strong empirical evidence on whether uncertainty communication improves user decision-making in LLM contexts
- Break condition: If user studies demonstrate that uncertainty communication increases confusion or reduces appropriate reliance without improving calibration, the mechanism fails.

## Foundational Learning

- Concept: Principal-Agent Problem
  - Why needed here: The paper's core framing; you cannot follow the argument without understanding information asymmetry and incentive misalignment.
  - Quick check question: In a principal-agent relationship, why does information asymmetry lead to suboptimal outcomes for the principal?

- Concept: Reward Modeling in RLHF
  - Why needed here: The paper critiques RLHF reward functions as incomplete contracts; you need to understand how preferences are encoded.
  - Quick check question: Given the Bradley-Terry model p(y1 ≻ y2 | x) = exp(r(x, y1)) / (exp(r(x, y1)) + exp(r(x, y2))), what happens if the reward function r fails to capture a key human value?

- Concept: Pareto Efficiency
  - Why needed here: Proposed as a criterion for balancing competing preferences across user groups.
  - Quick check question: An allocation is Pareto efficient if no individual can be made better off without making someone worse off—why is this insufficient for fairness?

## Architecture Onboarding

- Component map:
  - Reward model r_ϕ(x, y) → implements the incomplete contract
  - Preference dataset D → human comparison data
  - Bradley-Terry probability layer → preference formalization
  - Documentation layer → model cards, datasheets, factsheets (external contractual alignment)
  - Constitutional AI layer → internal principles + self-critique mechanism
  - Uncertainty estimation module → confidence scores, hedging behavior

- Critical path:
  1. Audit existing reward function for contract incompleteness (identify uncovered scenarios)
  2. Select alignment framework appropriate to domain (social for cultural context, economic for multi-stakeholder, contractual for regulated domains)
  3. Implement corresponding mechanism (norm datasets, welfare functions, or constitutional principles)
  4. Add uncertainty communication layer calibrated to user comprehension

- Design tradeoffs:
  - General-purpose vs. specialized models: General models struggle with domain-specific norms; specialized models risk value fragmentation (Section 4.2.2)
  - Specification vs. underspecification: Perfect specification is infeasible; embracing underspecification enables participatory alignment (Section 6)
  - External vs. internal oversight: External (documentation) is transparent but passive; internal (constitutional AI) is active but opaque

- Failure signatures:
  - Reward hacking: Model generates verbose responses that maximize reward signal without improving helpfulness (Section 3.3)
  - Fake alignment: Superficial compliance during training without internalizing constraints (Section 3.3)
  - Western bias: Model reflects Western values despite multicultural deployment (Section 4.1.1)
  - Miscalibrated confidence: High confidence on wrong answers (Section 5.1)

- First 3 experiments:
  1. Contract gap analysis: Systematically identify input-output pairs where current reward function provides no guidance or conflicting guidance; quantify coverage gaps.
  2. Uncertainty communication A/B test: Compare user comprehension and decision quality when receiving (a) no uncertainty signal, (b) confidence scores, (c) natural language hedging.
  3. Constitutional AI vs. RLHF robustness: Test both approaches under distribution shift; measure alignment degradation rate on out-of-distribution prompts.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLMs be designed to dynamically identify and adapt to evolving societal norms and biases, rather than remaining static after training?
- Basis in paper: [explicit] The authors state, "Developing approaches to enable LLMs to dynamically identify, adapt to, and mitigate emerging biases dynamically is a crucial area for future research."
- Why unresolved: While model editing exists for facts, applying these techniques to the fluid, context-dependent nature of social norms is currently underexplored.
- What evidence would resolve it: Benchmarks showing models successfully updating their behavior in response to detected temporal shifts in social norms or stereotypes.

### Open Question 2
- Question: Does integrating non-verbal signals (visual, auditory) into LLMs improve alignment by bridging the normative gap found in text-only interactions?
- Basis in paper: [explicit] The paper identifies "Exploring multimodality for alignment — integrating non-verbal forms of communication" as a "promising line of research to address this normative void."
- Why unresolved: Text-based interactions lack the multimodal cues humans use to interpret intent, creating a "normative gap" in communication.
- What evidence would resolve it: Comparative studies demonstrating that multimodal models interpret implicit social expectations more accurately than text-only baselines.

### Open Question 3
- Question: Can bottom-up, iterative refinement frameworks be configured to allow users to define the values governing LLM behavior in specific contexts?
- Basis in paper: [explicit] The authors ask, "Can bottom-up, iterative refinements be configured to support users’ in defining the very values that preside over their practices?"
- Why unresolved: Alignment is currently framed as a "designer-centric" technical problem (incomplete contracts), lacking mechanisms for meaningful public participation in objective setting.
- What evidence would resolve it: Deployment of interfaces where collective, diverse user feedback demonstrably shapes model constraints and objectives without developer intervention.

### Open Question 4
- Question: What methods effectively communicate LLM uncertainty to users with varying levels of statistical literacy and cognitive biases?
- Basis in paper: [inferred] The authors note it is "essential to develop methods for communicating uncertainty" but highlight that human cognitive biases and varying numeracy impede accurate interpretation.
- Why unresolved: LLMs lack the non-verbal cues humans use to express doubt, and current explicit/implicit signals often fail to calibrate user trust.
- What evidence would resolve it: User studies measuring decision-making quality and trust calibration across different uncertainty visualization or verbalization techniques.

## Limitations
- The transfer of human societal alignment mechanisms to LLM contexts remains largely theoretical without empirical validation across diverse deployment scenarios
- The claim that uncertainty is "essential" for ethical deployment lacks experimental evidence showing that uncertainty communication improves user outcomes
- The framework assumes societal alignment mechanisms will generalize across cultural contexts, but "Cultural Bias in Large Language Models" (FMR: 0.0) demonstrates current implementations fail to represent diverse moral frameworks

## Confidence

**High confidence:** The principal-agent framework accurately captures misalignment as an incomplete contract problem

**Medium confidence:** Societal alignment mechanisms from human institutions can inform LLM alignment strategies

**Low confidence:** Uncertainty communication improves user decision-making and ethical deployment

## Next Checks
1. Conduct systematic contract gap analysis to quantify incompleteness in existing reward functions across diverse domains
2. Run controlled user studies comparing comprehension and decision quality with different uncertainty communication methods
3. Test societal alignment mechanisms across culturally diverse user groups to validate generalization claims