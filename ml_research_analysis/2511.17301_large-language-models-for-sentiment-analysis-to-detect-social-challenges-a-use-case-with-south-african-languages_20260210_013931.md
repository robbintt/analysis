---
ver: rpa2
title: 'Large Language Models for Sentiment Analysis to Detect Social Challenges:
  A Use Case with South African Languages'
arxiv_id: '2511.17301'
source_url: https://arxiv.org/abs/2511.17301
tags:
- sentiment
- social
- llms
- languages
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Large Language Models for Sentiment Analysis to Detect Social Challenges: A Use Case with South African Languages

## Quick Facts
- arXiv ID: 2511.17301
- Source URL: https://arxiv.org/abs/2511.17301
- Authors: Koena Ronny Mabokela; Tim Schlippe; Matthias Wölfel
- Reference count: 40
- Primary result: LLM zero-shot sentiment classification on low-resource South African languages with fusion achieving <1% error rate

## Executive Summary
This paper evaluates five large language models (GPT-3.5, GPT-4, LLaMa 2, PaLM 2, Dolly 2) for zero-shot sentiment classification on social media posts in South African languages (English, Sepedi, Setswana). The study introduces a majority voting fusion approach that significantly outperforms individual models, reducing classification errors to below 1% for English content. The authors find that English-language prompts outperform native-language prompts for non-English content, contrary to intuitive expectations. Using the SAGovTopicTweets corpus of 16,787 tweets across 10 government-related topics, the work demonstrates that LLMs can effectively analyze sentiment in low-resource languages without task-specific fine-tuning.

## Method Summary
The study performs zero-shot 3-class sentiment classification (negative, neutral, positive) on social media posts using five parallel LLM endpoints. Tweets are formatted as CSV blocks within English-language prompts and fed to each model individually. The fusion method employs simple majority voting across the five models' outputs (-1 for negative, 0 for neutral, 1 for positive). The SAGovTopicTweets corpus contains 16,787 tweets in English, Sepedi, and Setswana across 10 government-related topics. Classification performance is evaluated using F1-score and classification error rate, comparing individual LLM performance against the fused ensemble approach.

## Key Results
- Fusion of five LLM outputs reduces classification errors to below 1% for English content (individual models: 8.6% best case)
- English-language prompts yield 1.0% higher F1 scores for Sepedi and 1.5% for Setswana compared to native-language prompts
- Zero-shot performance matches or exceeds BERT-based systems (86.0% F1 for English, 84.0% for Sepedi)
- Correlation between model outputs ranges from 0.770 (English) to 0.792 (Sepedi), indicating sufficient diversity for effective fusion

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can perform zero-shot sentiment classification on low-resource South African languages without task-specific fine-tuning.
- Mechanism: Pre-training on large multilingual corpora enables transfer of sentiment understanding from high-resource to low-resource languages through shared linguistic patterns and representations.
- Core assumption: Sepedi and Setswana share sufficient lexical and syntactic patterns with training data languages that sentiment signals transfer meaningfully.
- Evidence anchors:
  - [abstract]: "we analyse the zero-shot performance of the state-of-the-art LLMs GPT-3.5, GPT-4, LlaMa 2, PaLM 2, and Dolly 2"
  - [section 4.1]: "LLMs' performance is significantly better than what was reported in [32] on the SAfriSenti test sets: The BERT-based English system had an F1 score of 86.0%, the Sepedi system had an F1 score of 84.0%"
  - [corpus]: Weak direct evidence—no corpus papers specifically address zero-shot transfer to African languages.
- Break condition: Performance degrades sharply if the target language has minimal representation in pre-training data (cited via [35] in paper: "performance significantly drops for African languages").

### Mechanism 2
- Claim: Majority voting fusion of multiple LLM outputs substantially reduces sentiment classification error compared to individual models.
- Mechanism: Different LLMs make uncorrelated errors; averaging via voting cancels out individual misclassifications while preserving consensus on correct predictions.
- Core assumption: LLM error patterns are sufficiently independent that their intersection yields higher accuracy.
- Evidence anchors:
  - [abstract]: "a fusion of the outcomes of different LLMs provides large gains in sentiment classification performance with sentiment classification errors below 1%"
  - [section 4.1]: "English benefits the most (best: 8.6%, fusion: 0.4%) from the fusion of sentiment results due to the lowest correlation between the classified sentiments of 0.770"
  - [corpus]: Related work on public sentiment analysis (Knoxville traffic study) uses single-model approaches; no ensemble comparison available.
- Break condition: Fusion gains diminish when models are highly correlated in their outputs (as noted for Sepedi with higher correlation of 0.792).

### Mechanism 3
- Claim: English-language prompts yield better sentiment classification for non-English texts than native-language prompts.
- Mechanism: LLMs have stronger instruction-following capabilities in English due to training data dominance; task instructions in English more reliably activate relevant sentiment classification circuits.
- Core assumption: The model's instruction comprehension in English outweighs any alignment benefits from matching prompt language to input language.
- Evidence anchors:
  - [section 3.3]: "our initial experiments with GPT-4 demonstrated that English prompts lead to better results than prompts in the native language—1.0% relative better F1 scores for Sepedi and 1.5% for Setswana"
  - [section 3.3]: cites [51] for "Similar findings concerning English vs. foreign prompts"
  - [corpus]: No direct corpus evidence on prompt language effects for African languages.
- Break condition: Benefit may reverse for languages with stronger representation in instruction-tuning data.

## Foundational Learning

- Concept: Zero-shot classification
  - Why needed here: Understanding that LLMs can classify sentiment without seeing labeled examples from the target domain or language.
  - Quick check question: Can you explain why a model trained without Sepedi sentiment labels can still classify Sepedi text?

- Concept: Ensemble diversity and correlation
  - Why needed here: Grasping why fusion works better when models make different errors (lower correlation) rather than similar ones.
  - Quick check question: If two LLMs have 0.95 output correlation, would you expect large or small gains from majority voting?

- Concept: Overall sentiment score calculation
  - Why needed here: Understanding how raw classification outputs convert to actionable government metrics.
  - Quick check question: Given 100 positive, 50 neutral, and 150 negative tweets, what is the overall sentiment score?

## Architecture Onboarding

- Component map: Data ingestion -> Classification layer -> Fusion layer -> Scoring layer
- Critical path: Prompt engineering -> LLM API calls -> vote aggregation. Latency bottleneck is sequential API calls; error bottleneck is prompt quality.
- Design tradeoffs:
  - Accuracy vs. cost: Fusion requires 5x API calls; paper claims quality justifies cost but budget-constrained deployments may need single-model fallback
  - English vs. native prompts: English yields +1-1.5% F1 but may miss cultural nuances in sentiment expression
  - Topic-specific vs. generic prompts: Paper uses topic-specific prompts for precision, increasing maintenance overhead
- Failure signatures:
  - High inter-model correlation (>0.85): Fusion provides minimal gains; check if models converge on same errors
  - Topic-language mismatch: If sentiment distributions differ drastically across languages for same topic, investigate whether linguistic or service-delivery factors dominate
  - Neutral class inflation: If >60% neutral classifications, prompt may lack discriminative instructions
- First 3 experiments:
  1. Single-model baseline: Run GPT-4 alone on held-out 10% of SAGovTopicTweets; measure per-topic and per-language error rates to establish fusion ROI threshold.
  2. Correlation diagnostic: Compute pairwise correlation matrix of all 5 LLM outputs; if any pair exceeds 0.90, consider dropping the redundant model to reduce cost.
  3. Prompt language ablation: Test native-language prompts on 100-tweet sample per language; quantify accuracy delta to validate or reject English-prompt assumption for your deployment context.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Are the differences in sentiment scores between languages driven primarily by linguistic/cultural expression or by actual disparities in government service delivery?
- Basis in paper: [explicit] The Conclusion states future work "must investigate if more pronounced negative or positive sentiments between the languages for the same topics are due to linguistic and cultural differences or since the community is underserved."
- Why unresolved: The current study quantifies sentiment variance but lacks the ground-truth data to differentiate between subjective linguistic nuances and objective socio-economic service gaps.
- What evidence would resolve it: Correlating sentiment scores with objective performance metrics (e.g., independent service delivery audits) or conducting qualitative sociolinguistic analysis on the corpora.

### Open Question 2
- Question: Can advanced native-language prompting strategies outperform the current finding that English prompts are superior for Sepedi and Setswana sentiment analysis?
- Basis in paper: [inferred] Section 3.3 notes that initial experiments showed English prompts yielded better F1 scores (1.0% higher for Sepedi, 1.5% for Setswana) than native-language prompts, but the paper does not explore if optimized native prompting could close this gap.
- Why unresolved: The study defaulted to English prompts for performance, leaving the potential of specialized native-language prompt engineering (e.g., few-shot examples in Sepedi) unexplored.
- What evidence would resolve it: A comparative study evaluating complex native-language prompting techniques (Chain-of-Thought, Few-Shot) against zero-shot English prompts for these specific Bantu languages.

### Open Question 3
- Question: What is the precise trade-off between the high computational cost of LLM fusion and the marginal gain in classification accuracy?
- Basis in paper: [inferred] Section 4.3 (Discussion) acknowledges that the fusion approach "comes at the cost of increased computational resources" and asserts that gains justify the expense, but does not quantify this efficiency boundary.
- Why unresolved: While fusion reduced error to <1%, the viability of this approach for resource-constrained government deployments depends on the specific cost-performance ratio of running multiple state-of-the-art models simultaneously.
- What evidence would resolve it: An analysis measuring inference cost (latency/compute) per sentiment classification against accuracy degradation when removing specific models from the fusion ensemble.

## Limitations
- The fusion approach requires 5x API calls, creating significant computational overhead that may not be viable for resource-constrained deployments
- English prompts outperform native-language prompts, but this finding lacks robust validation across diverse African language contexts
- The custom SAGovTopicTweets corpus may not generalize to other social domains or languages beyond government-related content

## Confidence
- **High confidence**: English-language LLM performance on English tweets (well-established baseline with 86.0% F1 score)
- **Medium confidence**: Zero-shot performance on Sepedi and Setswana (limited prior research on African languages, though cited performance matches BERT baselines)
- **Low confidence**: Fusion benefits across all languages and topics (correlation values suggest diminishing returns for non-English content, but the paper doesn't explore cost-benefit tradeoffs)

## Next Checks
1. **Correlation-to-Cost Analysis**: Calculate the actual ROI of fusion by comparing per-tweet classification error reduction against 5x API cost increase, using the reported correlation values (0.770 for English, 0.792 for Sepedi) to model expected fusion gains.
2. **Prompt Language A/B Test**: Run controlled experiments comparing English vs. native-language prompts on 100 tweets per language, measuring F1 score differences to validate or refute the paper's English-prompt assumption for your specific deployment context.
3. **Cross-Topic Stability**: Test the zero-shot approach on a non-government social topic (e.g., sports or entertainment) to verify whether the observed performance patterns hold beyond the carefully curated SAGovTopicTweets corpus.