---
ver: rpa2
title: 'LM Fight Arena: Benchmarking Large Multimodal Models via Game Competition'
arxiv_id: '2510.08928'
source_url: https://arxiv.org/abs/2510.08928
tags:
- visual
- arena
- game
- fight
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "LM Fight Arena introduces a novel benchmark for evaluating large\
  \ multimodal models (LMMs) in real-time, adversarial gaming environments using Mortal\
  \ Kombat II. The framework pits six leading LMMs\u2014including Claude-3.5-Sonnet,\
  \ Gemini-2.5-Pro, GPT-4o, and open-source models\u2014in a round-robin tournament\
  \ where each controls the same character."
---

# LM Fight Arena: Benchmarking Large Multimodal Models via Game Competition

## Quick Facts
- arXiv ID: 2510.08928
- Source URL: https://arxiv.org/abs/2510.08928
- Reference count: 3
- Six LMMs (Claude-3.5-Sonnet, Gemini-2.5-Pro, GPT-4o, InternVL3-78B, Qwen2.5-VL-72B, Qwen2.5-VL-32B) compete in Mortal Kombat II round-robin tournament

## Executive Summary
LM Fight Arena introduces a novel benchmark for evaluating large multimodal models (LMMs) in real-time, adversarial gaming environments using Mortal Kombat II. The framework pits six leading LMMs—including Claude-3.5-Sonnet, Gemini-2.5-Pro, GPT-4o, and open-source models—in a round-robin tournament where each controls the same character. Models receive visual frames and structured game state data, outputting natural language commands for action execution. The primary evaluation metric is win/loss record, supplemented by remaining health percentage. Claude-3.5-Sonnet achieved a perfect 100% win rate, followed by Gemini-2.5-Pro at 80%, while GPT-4o failed to win any matches. Results demonstrate that closed-source models significantly outperformed open-source counterparts, highlighting the benchmark's effectiveness in testing real-time perception-action coupling and strategic reasoning under competitive pressure.

## Method Summary
The benchmark evaluates LMMs in Mortal Kombat II using a round-robin tournament format where each model controls Liu Kang in mirror matches. Models receive 10 frames sampled at 4-frame intervals (~1 second context) with position overlays, plus structured state features extracted from game memory including health bars, (x,y) coordinates, facing direction, and action history. All inputs are encoded as base64 and bundled with natural-language state descriptions. Models output natural language commands (e.g., "Left + A", "Down, Forward, A") which a translation module parses into Sega Genesis controller inputs. Evaluation uses win/loss records and remaining health percentages as primary metrics. All models are tested zero-shot without game-specific fine-tuning.

## Key Results
- Claude-3.5-Sonnet achieved a perfect 100% win rate across all matchups
- Gemini-2.5-Pro secured 80% win rate, demonstrating strong closed-source performance
- GPT-4o failed to win any matches, highlighting performance gaps in real-time adversarial settings
- Closed-source models significantly outperformed open-source counterparts in both win rates and health margins

## Why This Works (Mechanism)

### Mechanism 1: Multimodal State Fusion for Situational Awareness
- Claim: Models that effectively integrate visual frame sequences with structured game state data demonstrate superior performance in real-time adversarial environments.
- Mechanism: The framework provides dual-channel observation pipeline: (1) 10 frames sampled at 4-frame intervals with position overlays, and (2) structured state features including health bars, (x,y) coordinates, facing direction, and action history. Models must fuse these channels to build coherent situational representations.
- Core assumption: Models possess sufficient cross-modal grounding capabilities to map visual character positions to symbolic coordinate representations.
- Break condition: Performance degrades if visual and structured inputs provide conflicting signals, or if models over-rely on one modality.

### Mechanism 2: Natural Language Action Space with Deterministic Translation
- Claim: Using natural language as the action interface enables fair comparison across architecturally diverse models while maintaining strategic expressiveness.
- Mechanism: Models generate natural language commands (e.g., "Left + A", "Down, Forward, A") which a translation module parses and maps to Sega Genesis controller inputs. This abstraction layer standardizes output format across all evaluated models.
- Core assumption: The translation layer preserves strategic intent without introducing systematic biases toward certain action patterns.
- Break condition: If the parser cannot disambiguate syntactically valid but semantically unclear commands, or if timing-sensitive combo inputs require frame-precise execution that natural language cannot specify.

### Mechanism 3: Closed-Loop Adversarial Pressure Testing
- Claim: Direct model-to-model competition reveals capability differences that static benchmarks fail to capture, particularly in temporal reasoning and adaptive strategy.
- Mechanism: Round-robin tournament structure creates emergent difficulty curve where successful models face progressively stronger opponents. Win/loss records and remaining health percentages serve as objective metrics.
- Core assumption: Model vs. model competition provides meaningful proxy for real-world deployment scenarios requiring real-time decision-making under uncertainty.
- Break condition: If opponent behavior becomes predictable or if model performance is dominated by latency differences rather than reasoning quality.

## Foundational Learning

- **Frame-Level Temporal Reasoning**
  - Why needed here: Fighting games operate at frame-level granularity (~60 FPS), requiring models to track state changes across sub-second intervals and predict opponent actions based on motion trajectories.
  - Quick check question: Can you explain why sampling every 4th frame creates a 10-frame observation window of approximately 1 second, and what information might be lost in the subsampling?

- **Zero-Shot Policy Transfer**
  - Why needed here: All models are evaluated without game-specific fine-tuning or few-shot demonstrations, testing their ability to generalize from general instruction-following training to novel control task.
  - Quick check question: What systematic advantages might closed-source models have in zero-shot game-playing that open-source models lack, beyond raw parameter count?

- **Action Space Coverage vs. Exploitation**
  - Why needed here: Controller heatmap analysis reveals that winning models use diverse button combinations while losing models concentrate on few inputs, suggesting strategy breadth correlates with performance.
  - Quick check question: How would you distinguish between "diverse effective strategies" and "random button mashing" when analyzing action distribution patterns?

## Architecture Onboarding

- **Component map:**
Emulator (Mortal Kombat II, deterministic seed) → Frame Capture → Subsample (10 frames / 4-frame intervals) → Position Overlay → Base64 Encode
Emulator → Memory Reader → State Features (health, coords, facing, action history) → Format to Natural Language
Visual frames + Structured state → LMM Input → LMM Inference → Natural Language Command Output → "Translate Word to Button" Parser → Button Press Sequence Injection → Emulator Execution

- **Critical path:** The perception-to-action latency budget is the system bottleneck. Frame capture → LMM inference → command translation → button injection must complete within the game's tick window. Closed-source API latency variability may introduce uncontrolled variance.

- **Design tradeoffs:**
- Mirror matches eliminate character advantage but reduce strategic diversity; different character archetypes would test adaptation breadth
- Single match per pairing limits statistical confidence but enables rapid iteration; extending to 10-100 match series would improve robustness
- Zero-shot evaluation tests generalization but may underestimate fine-tuned performance ceiling

- **Failure signatures:**
- GPT-4o pattern: Accurate perception but passive behavior ("stayed rooted in place throwing high punches until cornered"), suggesting latency or conservative action sampling issues
- InternVL3-78B pattern: Aggressive bursts followed by prolonged blocking without directional inputs for cancels, indicating incomplete understanding of combo mechanics
- Narrow margin instability: Qwen2.5-VL-72B vs. Qwen2.5-VL-32B (0.025 health difference) suggests high-variance outcomes in closely-matched pairings

- **First 3 experiments:**
1. **Latency ablation:** Add artificial delay to the winning model's action pipeline to test whether Claude's dominance stems from faster inference or superior reasoning
2. **Modality ablation:** Evaluate models using only visual frames (no structured state) and only structured state (no frames) to quantify each modality's contribution
3. **Repetition robustness:** Run 10-match series for each pairing to establish confidence intervals on win rates, particularly for close matchups

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would performance rankings change with statistically robust multi-match series (10–100 bouts) rather than single matches per pairing?
- Basis in paper: [explicit] "Single matches per pairing restrict statistical confidence; extending to larger series would mitigate variance in close contests such as the Qwen mirror."
- Why unresolved: With only one match per pairing, narrow outcomes like Qwen2.5-VL-72B's 0.025 health margin over Qwen2.5-VL-32B could reflect random variance rather than true capability differences.
- What evidence would resolve it: Running extended tournament series with confidence intervals on win rates and health margins.

### Open Question 2
- Question: Can LMMs transfer strategies across different fighting game franchises, control schemes, and character archetypes?
- Basis in paper: [explicit] "Mortal Kombat II with Liu Kang captures only one genre and character archetype, leaving open how results generalize to other games or control schemes."
- Why unresolved: The single-character, single-game evaluation cannot distinguish whether Claude's dominance reflects general multimodal reasoning or exploitation of Liu Kang's specific mechanics.
- What evidence would resolve it: Evaluating models on diverse fighting games (Street Fighter II, Tekken 3) and varied character types (grapplers, zoners).

### Open Question 3
- Question: What performance gains are achievable through in-context learning or domain-specific fine-tuning?
- Basis in paper: [explicit] "all models are evaluated in a zero-shot setting without any game-specific fine-tuning" and future work should explore "providing example transcripts or strategy guides before competition."
- Why unresolved: Current results establish zero-shot baselines but cannot determine the ceiling for models given game knowledge or demonstration access.
- What evidence would resolve it: Comparing zero-shot performance against models provided with strategy guides, example transcripts, or fine-tuned on annotated match footage.

### Open Question 4
- Question: How do API latency differences across closed-source providers affect competitive outcomes in real-time adversarial settings?
- Basis in paper: [explicit] "Closed-source APIs introduce latency variability that we cannot fully control."
- Why unresolved: GPT-4o's zero-win record may partially stem from response delays rather than reasoning failures, but the framework cannot disentangle these factors.
- What evidence would resolve it: Controlled latency measurements across providers or systematic latency perturbation studies with local models.

## Limitations

- Single-match pairings create high variance in win rates for closely-matched models, limiting statistical confidence
- Closed-source API latency variability introduces uncontrolled confounding that may affect competitive outcomes
- The action translation layer's fidelity remains unverified, potentially introducing systematic biases

## Confidence

- **High Confidence:** Claude-3.5-Sonnet's perfect win rate (100%) and superior action diversity, supported by objective button heatmap analysis
- **Medium Confidence:** General ranking hierarchy (Claude > Gemini-2.5-Pro > open-source models) given consistent performance across multiple matchups
- **Low Confidence:** Claims about specific strategic reasoning patterns due to lack of behavioral analysis beyond win/loss and health metrics

## Next Checks

1. **Latency ablation test:** Add controlled delay (100-200ms) to Claude-3.5-Sonnet's action pipeline to determine if dominance stems from inference speed versus reasoning quality
2. **Modality ablation study:** Evaluate models using only visual frames (no structured state) and only structured state (no frames) to quantify each input channel's contribution
3. **Statistical replication:** Run 10-100 match series for each pairing to establish confidence intervals, particularly for close matchups where single-match results show high variance