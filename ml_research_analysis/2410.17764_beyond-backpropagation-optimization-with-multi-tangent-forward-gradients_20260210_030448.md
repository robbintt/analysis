---
ver: rpa2
title: 'Beyond Backpropagation: Optimization with Multi-Tangent Forward Gradients'
arxiv_id: '2410.17764'
source_url: https://arxiv.org/abs/2410.17764
tags:
- forward
- gradient
- gradients
- tangents
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes multi-tangent forward gradients as an alternative
  to backpropagation for training neural networks. The method approximates gradients
  by combining directional derivatives along multiple random tangents computed via
  forward-mode automatic differentiation, rather than using backpropagation's backward
  pass.
---

# Beyond Backpropagation: Optimization with Multi-Tangent Forward Gradients

## Quick Facts
- arXiv ID: 2410.17764
- Source URL: https://arxiv.org/abs/2410.17764
- Reference count: 40
- Key outcome: Multi-tangent forward gradients provide an alternative to backpropagation for training neural networks, using forward-mode automatic differentiation with orthogonal projection aggregation for improved gradient approximation.

## Executive Summary
This paper introduces multi-tangent forward gradients as an alternative optimization method for training neural networks that avoids the limitations of backpropagation. Instead of using backward-mode automatic differentiation, the method computes directional derivatives along multiple random tangents and combines them using orthogonal projections to approximate the true gradient. The authors demonstrate that this approach provides better gradient approximation quality than simple averaging or summation, particularly when tangents are not orthogonal. Experiments show successful training of modern architectures including ResNet18 and vision transformers while maintaining computational efficiency.

## Method Summary
The multi-tangent forward gradient method computes gradients by approximating the full gradient vector through directional derivatives along multiple randomly sampled tangents. Each tangent direction v is used to compute a directional derivative via forward-mode automatic differentiation, which has computational cost independent of network depth. These directional derivatives are then aggregated using orthogonal projection onto the tangent space spanned by all tangent vectors. This projection-based aggregation is shown to be optimal for minimizing the mean squared error of the gradient approximation compared to simple averaging or summation. The method requires computing K forward-mode derivatives, where K is the number of tangents, with each computation requiring roughly the same resources as one backpropagation pass.

## Key Results
- Orthogonal projection aggregation provides better gradient approximation quality than averaging or summation, especially for non-orthogonal tangent vectors
- Increasing the number of tangents consistently improves both approximation quality and optimization performance across tasks
- The method successfully trains modern architectures including ResNet18 and vision transformers on CIFAR-10
- Computational cost scales linearly with the number of tangents, making it practical for moderate values of K

## Why This Works (Mechanism)
The method works by leveraging the mathematical relationship between gradients and directional derivatives. The true gradient ∇f(x) can be decomposed into components along any set of tangent directions {v₁, ..., vₖ}. By computing directional derivatives f'(x, vᵢ) = ∇f(x)ᵀvᵢ for multiple tangents and combining them through orthogonal projection, the method reconstructs an approximation of the full gradient. The orthogonal projection is optimal because it minimizes the mean squared error between the true gradient and its approximation in the tangent space. This approach is particularly effective when tangents span a large portion of the parameter space, allowing for accurate gradient reconstruction without requiring the full backward pass.

## Foundational Learning

**Directional Derivatives**: Measure the rate of change of a function along a specific direction. Needed to understand how gradients can be approximated through multiple directional measurements. Quick check: Verify that f'(x, v) = ∇f(x)ᵀv for any direction v.

**Forward-Mode Automatic Differentiation**: Computes derivatives by propagating derivative information forward through the computation graph. Needed to understand the computational advantages over backpropagation. Quick check: Confirm that forward-mode cost is independent of network depth while backward-mode cost is proportional to depth.

**Gram Matrix**: The matrix G with entries Gᵢⱼ = vᵢᵀvⱼ that captures the inner products between tangent vectors. Needed to understand the orthogonal projection aggregation. Quick check: Verify that G is invertible when tangent vectors are linearly independent.

**Orthogonal Projection**: The mathematical operation that projects a vector onto the span of a set of basis vectors. Needed to understand why the aggregation method is optimal. Quick check: Confirm that projection minimizes mean squared error in the tangent space.

## Architecture Onboarding

**Component Map**: Loss function → Directional derivatives (forward-mode AD) → Tangent vectors → Gram matrix → Orthogonal projection → Gradient approximation → Parameter update

**Critical Path**: The core computation path is: compute K directional derivatives using forward-mode AD → construct Gram matrix → compute inverse → perform orthogonal projection → obtain gradient approximation → update parameters via optimizer.

**Design Tradeoffs**: 
- Number of tangents K: Higher K improves approximation quality but increases computational cost
- Tangent generation: Random tangents are simple but may be suboptimal; structured tangents could improve efficiency
- Aggregation method: Orthogonal projection is optimal but requires computing matrix inverse; alternatives trade accuracy for speed

**Failure Signatures**: 
- Poor gradient approximation when tangent vectors are nearly linearly dependent (ill-conditioned Gram matrix)
- Computational bottlenecks when K is large or tangent dimensions are high
- Suboptimal convergence when tangents fail to span important directions in parameter space

**First Experiments**:
1. Verify gradient approximation quality on a simple quadratic function with known gradient
2. Compare training curves of multi-tangent method versus backpropagation on a small fully-connected network
3. Measure computational overhead of orthogonal projection aggregation versus accuracy gains across different K values

## Open Questions the Paper Calls Out
None specified in the provided information.

## Limitations
- Computational overhead of computing multiple forward-mode derivatives may become prohibitive for very deep networks or high-resolution inputs
- Orthogonal projection requires computing the Gram matrix inverse, which could introduce significant computational and memory overhead for high-dimensional parameter spaces
- Limited experimental scope on larger-scale models and datasets raises questions about scalability to industrial applications

## Confidence
- High confidence in theoretical analysis of approximation quality and optimality of orthogonal projection
- Medium confidence in claims about practical advantages based on CIFAR-10 experiments with ResNet18 and ViTs
- Low confidence in speculative claims about biological plausibility without neuroscience validation

## Next Checks
1. Evaluate the method on larger-scale vision tasks (e.g., ImageNet) and language models to assess scalability limitations
2. Perform ablation studies on the computational overhead of orthogonal projection aggregation versus the accuracy gains across different model sizes and dimensions
3. Compare the multi-tangent approach against other backpropagation alternatives (e.g., equilibrium propagation, local learning rules) on the same tasks to better contextualize its advantages and disadvantages