---
ver: rpa2
title: Few-shot text-based emotion detection
arxiv_id: '2507.05918'
source_url: https://arxiv.org/abs/2507.05918
tags:
- emotion
- emotions
- sentence
- few-shot
- qwen2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents the Unibuc-NLP team's approach to SemEval 2025
  Task 11 on text-based emotion detection. They experimented with large language models
  (Gemini, Qwen, DeepSeek) using few-shot prompting and fine-tuning, alongside fine-tuning
  BERT-based models.
---

# Few-shot text-based emotion detection

## Quick Facts
- arXiv ID: 2507.05918
- Source URL: https://arxiv.org/abs/2507.05918
- Reference count: 6
- Achieved 1st place in Emakhuwa emotion detection (F1-macro 0.325)

## Executive Summary
This paper presents the Unibuc-NLP team's approach to SemEval 2025 Task 11 on text-based emotion detection using few-shot prompting with large language models. The team experimented with Gemini, Qwen, and DeepSeek models, finding that few-shot prompting with 600 examples achieved the best performance, particularly for the low-resource Emakhuwa language. They also explored fine-tuning BERT-based models, which provided faster inference but lower accuracy. The system showed a tendency to over-predict fear and under-predict joy, highlighting challenges in subjective emotion annotation.

## Method Summary
The team employed three approaches: few-shot prompting with large language models (primarily Gemini 2.0 Flash), fine-tuning smaller LLMs (DeepSeek R1 Distill Llama 8B) with LoRA, and fine-tuning BERT-based models (DeBERTa, mBERT, XLM-RoBERTa). Few-shot prompting used up to 600 training examples in prompts without complex reasoning chains. BERT models used a two-stage training procedure with frozen and unfrozen transformer layers. The primary evaluation metric was F1-macro across English, Portuguese-Mozambican, and Emakhuwa languages from the BRIGHTER dataset.

## Key Results
- Achieved F1-macro of 0.7546 (26/96 teams) for English multi-label emotion detection
- First place in Emakhuwa with F1-macro of 0.325 (1/31 teams)
- Few-shot prompting with 600 examples outperformed zero-shot (0.63→0.70 F1) and complex prompts like CoT/ToT
- BERT-based models provided 10x+ faster inference but lower accuracy (0.70 F1 vs 0.77 with Gemini)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Increasing few-shot examples improves emotion classification performance.
- Mechanism: In-context learning allows LLMs to infer task patterns from demonstration examples without weight updates. More examples provide broader coverage of the 6 emotion classes and their co-occurrence patterns.
- Core assumption: The model can generalize from examples to unseen instances within its context window.
- Evidence anchors:
  - [Section 3.2] "increasing the number of examples... resulted in an increase in performance. We tried 6, 100, 300 and 600. Best results were when we gave the most examples (600)."
  - [Section 3.2] Qwen2.5 Zero-Shot: 0.63 F1 → Few-Shot: 0.70 F1 (~7% improvement)

### Mechanism 2
- Claim: Complex reasoning prompts (CoT, ToT) degrade performance on perceived emotion tasks.
- Mechanism: Perceived emotion annotation reflects subjective human interpretation, not objective reasoning chains. Forcing explicit deliberation may over-analyze ambiguous cases and diverge from annotator intuition.
- Core assumption: Emotion perception in humans is often rapid and intuitive rather than deliberative.
- Evidence anchors:
  - [Section 3.2] "complex prompting techniques like CoT and ToT actually make the models perform worse"
  - [Section 3.2] "annotations do not necessarily represent an objective 'ground truth' emotion, but rather reflect human annotators' subjective interpretations"

### Mechanism 3
- Claim: Few-shot prompting transfers effectively to low-resource languages without language-specific training.
- Mechanism: Large LLMs with multilingual pre-training can apply cross-lingual transfer, using emotion concepts learned from high-resource languages to classify in underrepresented languages.
- Core assumption: Emotion categories share conceptual similarity across languages; multilingual LLMs have sufficient coverage.
- Evidence anchors:
  - [Section 4] Emakhuwa achieved 0.325 F1-macro (1st place / 31 teams)
  - [Section 4] "This reflects our model's ability to adapt with few-shot in-context learning especially on languages with limited availability of NLP resources"

## Foundational Learning

- Concept: Few-shot prompting / In-context learning
  - Why needed here: Core technique achieving best results; understanding how examples in prompts enable task learning without gradient updates.
  - Quick check question: How does the model's F1-macro change as you increase examples from 6 to 600?

- Concept: Perceived vs. expressed emotion annotation
  - Why needed here: Explains why CoT/ToT failed—annotations are subjective interpretations, not objective labels.
  - Quick check question: Why might explicit reasoning steps harm performance on subjective annotation tasks?

- Concept: Cross-lingual transfer in multilingual models
  - Why needed here: Explains strong Emakhuwa performance despite limited training data (1,551 examples).
  - Quick check question: What factors might cause cross-lingual transfer to fail for dialect variants like Mozambican Portuguese?

## Architecture Onboarding

- Component map:
  Input: Text sentence → prompt construction with 0-600 few-shot examples → Primary model (Gemini 2.0 Flash) → Output: Multi-label emotion predictions

- Critical path:
  1. Data preparation: Sample up to 600 training examples covering all emotion classes
  2. Prompt construction: Append examples + query sentence (simple format, no CoT)
  3. Inference: LLM generates emotion labels directly
  4. Post-processing: Parse comma-separated labels

- Design tradeoffs:
  - **Few-shot vs. Fine-tuning**: Few-shot with Gemini Flash achieves higher F1 (0.77 vs 0.76 validation) but requires API calls; fine-tuned models (DeepSeek) offer local deployment
  - **BERT vs. LLM**: BERT models (DeBERTa: 0.70 F1) are 10x+ faster at inference but lower accuracy
  - **Prompt complexity**: Simple few-shot outperforms CoT/ToT by avoiding over-reasoning

- Failure signatures:
  - **Fear over-prediction**: 88% TP but 30% FP (lowest TN at 70%)—model labels ambiguous emotions as fear
  - **Joy under-prediction**: 67% TP with 32% FN—model misses joy instances
  - **Language mismatch**: Portuguese (Mozambican) F1 of 0.1727 suggests dialect/cultural gap not bridged by few-shot

- First 3 experiments:
  1. **Baseline few-shot**: Test Gemini Flash with 6 examples (one per emotion class), measure F1-macro on dev set
  2. **Example scaling**: Increase to 100, 300, 600 examples; plot F1-macro vs. example count to verify scaling
  3. **Ablation: prompt complexity**: Compare simple few-shot (600 examples) vs. few-shot + CoT vs. few-shot + ToT; confirm CoT degradation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why did the system perform significantly worse on the Portuguese (Mozambican) subset compared to English and Emakhuwa?
- Basis in paper: [explicit] The authors state that "further investigation is warranted to understand the performance disparity observed across languages, particularly the lower F1-macro score for the Portuguese subset."
- Why unresolved: The paper reports the low score (0.1727) but does not provide a linguistic or data-centric explanation for why the model failed on this specific dialect despite success in others.
- What evidence would resolve it: A detailed error analysis and dataset audit identifying specific linguistic nuances, noise, or data sparsity issues unique to the Mozambican Portuguese subset.

### Open Question 2
- Question: Can "thinking models" (e.g., Gemini 2.0 Thinking) overcome the performance degradation observed with standard Chain-of-Thought (CoT) prompting?
- Basis in paper: [explicit] The authors suggest that "exploring the new 'thinking models' like 'Gemini 2.0 Thinking' and 'Thinking Claude' should be tested, as they show promises in other similar tasks."
- Why unresolved: The study found that explicit reasoning strategies (CoT) hurt performance on this specific task, but it is unknown if models with internalized reasoning capabilities behave differently.
- What evidence would resolve it: Benchmark results comparing standard few-shot performance against "thinking model" performance on the BRIGHTER dataset.

### Open Question 3
- Question: Does enforcing explicit reasoning conflict with the subjective nature of "perceived emotion" annotation?
- Basis in paper: [inferred] The authors note that complex prompting (CoT) degraded results, hypothesizing that "forcing the model to explicitly articulate a detailed thought process may not align with the inherently subjective... nature of perceived emotion annotation."
- Why unresolved: This remains a hypothesis derived from negative results; the precise mechanism causing reasoning models to fail on subjective annotation tasks is not empirically proven in the paper.
- What evidence would resolve it: A study comparing annotator reasoning patterns with model reasoning paths to see if "perceived emotion" relies more on heuristics than step-by-step logic.

## Limitations

- Subjective emotion annotation introduces uncertainty in "ground truth" labels, potentially explaining why complex reasoning prompts degraded performance
- The selection strategy for the 600 few-shot examples remains unspecified, which could significantly impact results
- Cross-lingual transfer success varied dramatically between languages (Emakhuwa F1 of 0.325 vs. Mozambican Portuguese F1 of 0.1727)

## Confidence

- **High confidence**: Few-shot prompting effectiveness (verified through controlled example scaling experiments from 6 to 600 examples with consistent F1 improvements)
- **Medium confidence**: Cross-lingual transfer mechanism (strong Emakhuwa performance supports the claim, but Portuguese failure suggests limitations)
- **Medium confidence**: CoT/ToT degradation explanation (plausible given subjective annotation nature, but lacks direct corpus evidence)

## Next Checks

1. **Example selection ablation**: Systematically vary the selection strategy for few-shot examples (random, stratified by class, diversity-based) while holding count constant at 600 to isolate selection effects from quantity effects.

2. **Dialect-specific evaluation**: Conduct controlled experiments comparing Portuguese from Portugal vs. Mozambique using the same model/prompting approach to quantify dialect impact on transfer performance.

3. **Per-class bias quantification**: Calculate per-class precision-recall curves for all three languages to formally characterize the fear over-prediction and joy under-prediction patterns, then test targeted mitigation strategies (class-weighted sampling, focal loss).