---
ver: rpa2
title: A Comprehensive Benchmark of Language Models on Unicode and Romanized Sinhala
arxiv_id: '2601.14958'
source_url: https://arxiv.org/abs/2601.14958
tags:
- sinhala
- arxiv
- romanized
- unicode
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper benchmarks modern language models on Sinhala, a low-resource
  morphologically rich language with two scripts: Unicode and Romanized. The authors
  evaluate open-source models using perplexity and closed-source models via qualitative
  sentence completion scoring.'
---

# A Comprehensive Benchmark of Language Models on Unicode and Romanized Sinhala

## Quick Facts
- **arXiv ID**: 2601.14958
- **Source URL**: https://arxiv.org/abs/2601.14958
- **Reference count**: 40
- **Primary result**: Model performance varies significantly between Unicode and Romanized Sinhala scripts, with no single model excelling at both.

## Executive Summary
This paper benchmarks modern language models on Sinhala, a low-resource morphologically rich language with two scripts: Unicode and Romanized. The authors evaluate open-source models using perplexity and closed-source models via qualitative sentence completion scoring. Mistral-Nemo-Base-2407 achieved the lowest perplexity for Unicode Sinhala (2.19), while Mistral-7B-v0.3 performed best for Romanized Sinhala (74.76). Llama-3.1-8B showed strong all-around performance across both scripts. For closed-source models, Gemini-1.5-pro and DeepSeek excelled at Unicode text, whereas Claude-3.5-Sonnet was superior for Romanized text. The results demonstrate that model performance significantly varies by script, highlighting the importance of training data composition for handling script variations in low-resource languages.

## Method Summary
The authors curated a 200-sentence parallel corpus from blogs and social media, with sentences clustered via LaBSE embeddings and K-Means. Open-source models were evaluated using perplexity on the full dataset, while closed-source models were assessed through qualitative scoring of 50-sentence completions rated for coherence and grammar by a native speaker. Thirteen open-source models (560M-14B parameters) and four closed-source models were tested across both Unicode and Romanized Sinhala scripts.

## Key Results
- Mistral-Nemo-Base-2407 achieved the lowest perplexity for Unicode Sinhala (2.19)
- Mistral-7B-v0.3 performed best for Romanized Sinhala (74.76)
- Llama-3.1-8B showed strong all-around performance across both scripts
- Gemini-1.5-pro and DeepSeek excelled at Unicode generation, while Claude-3.5-Sonnet excelled at Romanized generation
- Model performance varies significantly by script, with no single model dominating both

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training data composition drives script-specific performance divergence in multilingual models.
- Mechanism: Models exposed to formal corpora (news, literature) during pre-training develop stronger Unicode Sinhala representations, while models trained on unfiltered web content better capture Romanized Sinhala patterns prevalent in digital communication.
- Core assumption: Pre-training corpus composition can be inferred from downstream script performance patterns.
- Evidence anchors:
  - [abstract] "results reveal significant script-based performance divergence, underscoring the importance of training data composition for script handling"
  - [section V] "The strength of some models for Romanized Sinhala suggests a greater exposure to unfiltered web content where such scripts are common"
  - [corpus] "Script Gap" paper (arXiv:2512.10780) confirms similar orthographic variation challenges in Indian clinical settings where Romanized text dominates

### Mechanism 2
- Claim: Tokenization efficiency matters more than parameter count for morphologically rich, low-resource languages.
- Mechanism: Agglutinative languages like Sinhala generate vast word forms from single roots. Efficient tokenizers that handle morphological variation compress sequences better, enabling superior perplexity even with fewer parameters.
- Core assumption: Tokenization quality directly impacts perplexity in morphologically complex languages.
- Evidence anchors:
  - [section V] "Llama-3.1-8B model, with 8 billion parameters, achieved a superior perplexity score of 2.37 on Unicode text compared to the phi-4 model, which, despite having 14 billion parameters, scored 3.19"
  - [section V] "tokenization efficiency and architectural optimizations play a crucial role in a model's proficiency with a morphologically rich, low-resource language like Sinhala"
  - [corpus] "One Script Instead of Hundreds?" (arXiv:2601.05776) explores romanization as a strategy for cross-lingual transfer, supporting script representation importance

### Mechanism 3
- Claim: Cross-script transfer is not automatic—models develop script-specific capabilities that do not generalize.
- Mechanism: Unicode and Romanized Sinhala use fundamentally different character sets and orthographic patterns. Learning one script's statistics does not inherently transfer phonological or grammatical competence to the other script.
- Core assumption: Script variation creates distinct representational challenges that require explicit training.
- Evidence anchors:
  - [section IV-B] "models often struggle with subject-verb agreement in the Sinhala language" across both scripts
  - [section IV-B] Gemini-1.5-pro and DeepSeek excel at Unicode generation while Claude-3.5-Sonnet excels at Romanized—no single model dominates both
  - [corpus] "Swa-bhasha Resource Hub" (arXiv:2507.09245) documents extensive transliteration resources needed, confirming script-transfer complexity

## Foundational Learning

- Concept: Perplexity as intrinsic language modeling metric
  - Why needed here: The paper uses perplexity as the primary benchmark for open-source models. A perplexity of 2.19 (Mistral-Nemo on Unicode) means the model is uncertain between ~2.19 equally likely next tokens on average—lower is better.
  - Quick check question: If Model A has perplexity 3.0 and Model B has perplexity 150 on the same corpus, which model has stronger language understanding?

- Concept: Morphological richness and agglutination
  - Why needed here: Sinhala's morphological complexity (single root → many word forms) challenges standard tokenizers. Understanding this explains why model size alone doesn't predict performance.
  - Quick check question: Why might a tokenizer that works well for English perform poorly on an agglutinative language?

- Concept: Script variation (Unicode vs. Romanized)
  - Why needed here: The paper's central finding is performance divergence across scripts. Romanized Sinhala uses Latin characters phonetically (e.g., "mama kalin" = "I was...") while Unicode uses the native Sinhala script.
  - Quick check question: What does it mean that Claude-3.5-Sonnet scores 1.32 coherence on Romanized but Gemini-1.5-pro scores 1.22 on Unicode?

## Architecture Onboarding

- Component map: Dataset curation (1000 sentences → 200 clustered sentences) -> Script-specific evaluation (Unicode/Romanized) -> Perplexity computation (open) OR human rating (closed) -> Cross-script comparison

- Critical path: Dataset curation → script-specific evaluation → perplexity computation (open) OR human rating (closed) → cross-script comparison

- Design tradeoffs:
  - Perplexity vs. qualitative: Perplexity is objective but unavailable for closed models; qualitative ratings enable closed-source inclusion but introduce subjectivity (single rater acknowledged as limitation)
  - Dataset size: 200 sentences enables controlled evaluation but may not capture Romanized spelling variations in wild usage
  - Monolingual focus: Excludes code-switching (mixed script within sentences), which is common in digital communication

- Failure signatures:
  - High perplexity (>100) on Romanized text indicates poor exposure to informal digital corpora
  - Subject-verb agreement errors in completions signal weak morphological understanding
  - Coherent but grammatically poor outputs (score 1 coherence, 3 grammar) indicate semantic but not syntactic competence

- First 3 experiments:
  1. **Baseline perplexity check**: Run the 200-sentence evaluation set through your target model(s) to establish baseline Unicode and Romanized perplexity. Compare against Mistral-Nemo-Base-2407 (2.19 Unicode) and Mistral-7B-v0.3 (74.76 Romanized).
  2. **Script-specific failure analysis**: Sample 20 completions per script. Manually categorize errors (subject-verb agreement, word order, vocabulary). The paper flags subject-verb agreement as a common weak point.
  3. **Tokenizer ablation**: If comparing multiple models, correlate perplexity with tokenizer efficiency metrics (tokens per Sinhala sentence, vocabulary coverage of Sinhala Unicode block). Test whether tokenizer explains performance differences independent of model size.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do language models perform on cross-scripted Sinhala text where Unicode and Romanized scripts are mixed within the same context?
- Basis in paper: [explicit] The authors explicitly state the evaluation "does not assess the cross-scripting, which is also common in digital communication" and list this analysis as a direction for future work.
- Why unresolved: The current study methodology strictly segregates evaluation into monolingual blocks of either Unicode or Romanized text.
- What evidence would resolve it: A benchmark evaluation of model performance on a dataset containing naturally occurring or synthetic mixed-script sentences.

### Open Question 2
- Question: How does the qualitative ranking of closed-source models change when validated by multiple native speakers?
- Basis in paper: [explicit] The paper acknowledges the single-evaluator approach "introduces the potential for subjective bias" and calls for "validation by multiple native speakers" in future work.
- Why unresolved: Grammar and coherence scores were determined by a single rater, lacking inter-rater reliability metrics.
- What evidence would resolve it: A re-evaluation of the qualitative tasks using crowd-sourced scores and statistical analysis of inter-rater agreement (e.g., Kappa scores).

### Open Question 3
- Question: Can fine-tuning a single model on a balanced dual-script corpus effectively bridge the performance divergence between Unicode and Romanized Sinhala?
- Basis in paper: [inferred] While the authors suggest developers might benefit from "fine-tuning a single model," they currently only evaluate pre-trained foundational models which exhibit significant script-based performance gaps.
- Why unresolved: It is unknown if supervised fine-tuning can override the pre-training data biases that currently cause models to excel in one script while struggling with the other.
- What evidence would resolve it: A comparative study of baseline versus dual-script fine-tuned versions of strong performers like Llama-3.1-8B.

## Limitations

- **Single rater for qualitative evaluation**: The closed-source model assessment relies on one native Sinhala speaker for all 50-sentence completions, introducing subjective bias.
- **Dataset representativeness for Romanized Sinhala**: The 200-sentence corpus may not capture the full diversity of Romanized Sinhala spelling variations found in unconstrained digital communication.
- **Opaque tokenization details**: Specific tokenizer characteristics are not reported for each model, making it difficult to independently verify the tokenizer-efficiency mechanism.

## Confidence

- **High confidence**: The core empirical finding that model performance varies significantly by script (Unicode vs. Romanized) is well-supported by the perplexity and qualitative data.
- **Medium confidence**: The interpretation that training data composition drives script-specific performance is plausible but relies on indirect inference rather than direct corpus analysis.
- **Low confidence**: The claim that cross-script transfer is not automatic is based on absence of evidence rather than direct testing.

## Next Checks

1. **Multi-rater validation**: Recruit 3-5 additional native Sinhala speakers to re-score the 50-sentence closed-source evaluation subset. Compute Cohen's kappa or Fleiss' kappa for inter-rater agreement. If agreement is low (<0.6), the single-rater scores are unreliable and the closed-source model rankings should be treated with caution.

2. **Cross-script transfer experiment**: Take the best-performing model for Unicode (Mistral-Nemo-Base-2407) and fine-tune it on the Romanized subset (or vice versa). Evaluate zero-shot performance before and after fine-tuning on the target script. If perplexity improves by >50% after fine-tuning, it confirms that script-specific training is necessary despite shared phonological content.

3. **Tokenizer ablation study**: For 3 models spanning the performance range (e.g., Mistral-Nemo, Llama-3.1-8B, phi-4), extract tokenizer statistics: vocabulary coverage of Sinhala Unicode block, average tokens per sentence, and out-of-vocabulary rate on the evaluation set. Correlate these metrics with perplexity scores while controlling for model size. If tokenizer efficiency explains >80% of perplexity variance independent of parameter count, it validates Mechanism 2.