---
ver: rpa2
title: Finding Culture-Sensitive Neurons in Vision-Language Models
arxiv_id: '2510.24942'
source_url: https://arxiv.org/abs/2510.24942
tags:
- neurons
- culture
- cultural
- language
- cultures
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the presence of culture-sensitive neurons
  in vision-language models (VLMs) by analyzing their behavior on culturally diverse
  visual question answering tasks. Using the CVQA benchmark, the authors develop a
  three-stage pipeline to identify neurons whose activations are preferentially modulated
  by specific cultural contexts and assess their importance through targeted ablation
  experiments across three VLMs (Qwen2.5-VL-7B, Pangea-7B, and LLaVA-v1.6-Mistral-7B)
  and 25 cultural groups.
---

# Finding Culture-Sensitive Neurons in Vision-Language Models

## Quick Facts
- **arXiv ID:** 2510.24942
- **Source URL:** https://arxiv.org/abs/2510.24942
- **Reference count:** 33
- **Key outcome:** Ablating culture-sensitive neurons causes selective performance degradation on corresponding cultures (largest self-deactivation drop: -5.52% for CAS on Qwen2.5-VL-7B).

## Executive Summary
This study investigates culture-sensitive neurons in vision-language models (VLMs) by analyzing their behavior on culturally diverse visual question answering tasks. Using the CVQA benchmark and a three-stage pipeline, the authors identify neurons whose activations are preferentially modulated by specific cultural contexts and assess their importance through targeted ablation experiments across three VLMs (Qwen2.5-VL-7B, Pangea-7B, and LLaVA-v1.6-Mistral-7B) and 25 cultural groups. The authors introduce Contrastive Activation Selection (CAS), a margin-based method that outperforms existing probability- and entropy-based approaches in isolating culture-sensitive neurons. Results show that ablating these neurons disproportionately reduces model performance on questions tied to the corresponding culture while having minimal effects on others, with the largest self-deactivation accuracy drops observed for CAS. Layer-wise analyses reveal that culture-sensitive neurons cluster in mid-to-late decoder layers, with patterns largely consistent across models and cultures.

## Method Summary
The authors develop a three-stage pipeline to identify and validate culture-sensitive neurons in VLMs. First, they record activations from decoder MLPs (SwiGLU blocks) on correctly answered VQA samples from CVQA, aggregating per-neuron statistics per culture. Second, they score neurons using four methods: LAP (mean activation), LAPE (entropy-based selection), MAD (normalized deviation), and CAS (margin between top two cultures). Third, they ablate the top r% neurons (r=1) by setting their gating activations to zero during inference and measure accuracy changes. The CAS method measures the gap between a neuron's most and second-most active cultures, rewarding neurons with large separation. Self-deactivation accuracy drops (culture-specific) are compared against cross-deactivation (other cultures) to assess specificity.

## Key Results
- Ablating culture-sensitive neurons disproportionately harms performance on corresponding cultures (self-deactivation drops up to -5.52% for CAS on Qwen2.5-VL-7B) while having minimal effects on others (cross-deactivation <1%).
- CAS outperforms LAP, LAPE, and MAD in isolating culture-sensitive neurons by measuring activation margins rather than means or entropy.
- Culture-sensitive neurons cluster in mid-to-late decoder layers (layers 0 and 6-8 in Qwen2.5-VL-7B), with patterns consistent across models and cultures.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Culture-sensitive neurons exhibit preferential activation patterns for specific cultural contexts, enabling their identification through contrastive activation analysis.
- **Mechanism:** Neurons in decoder MLPs show differential firing rates when processing inputs from different cultures. CAS measures the margin between a neuron's top-responding culture and its second-most active culture (P(1) - P(2)), rewarding neurons with large separation rather than just high mean activation.
- **Core assumption:** Cultural knowledge is at least partially encoded in local, identifiable neural substrates rather than being purely distributed.
- **Evidence anchors:**
  - [abstract] "neurons whose activations show preferential sensitivity to inputs associated with particular cultural contexts"
  - [Section 3.4] "CAS measures the gap between the most and the second-most active cultures for each neuron"
  - [corpus] Related work (Tang et al. 2024 on language-specific neurons) supports localized specialization, though direct corpus evidence for culture-specific neurons in VLMs is limited to this paper.
- **Break condition:** If neurons show uniformly high variance across all cultures with no discriminative margin, CAS will fail to identify culture-specific patterns.

### Mechanism 2
- **Claim:** Ablating culture-sensitive neurons selectively degrades performance on the corresponding culture while sparing others, demonstrating causal involvement.
- **Mechanism:** Setting selected neuron activations to zero during inference disrupts culture-specific information pathways. The self-deactivation accuracy drop (up to -5.52% for CAS on Qwen2.5-VL-7B) with minimal cross-deactivation (<1%) indicates that these neurons encode culture-specific rather than generic multimodal features.
- **Core assumption:** The selected neurons contribute causally to correct culture-specific predictions rather than merely correlating with cultural inputs.
- **Evidence anchors:**
  - [abstract] "ablating culture-sensitive neurons disproportionately harms performance on questions tied to the corresponding culture while having minimal effects on others"
  - [Section 5.2] "CAS yields the largest self-deactivation drops in accuracy (Qwen: -5.52%; Pangea: -4.33%) paired with small cross-deactivation changes (<1%)"
  - [corpus] Indirect support from concurrent LLM work (Namazifard and Galke 2025) on culture neurons, but VLM-specific evidence remains limited to this study.
- **Break condition:** If cross-deactivation effects approach self-deactivation magnitude, neurons lack culture specificity and may encode shared multimodal features instead.

### Mechanism 3
- **Claim:** Culture-sensitive neurons cluster in early and early-mid decoder layers, suggesting distinct processing stages for cultural information integration.
- **Mechanism:** Layer-wise analysis reveals concentration in layer 0 and layers 6-8 (in 28-layer Qwen2.5-VL-7B), with sparse presence in deeper blocks. This distribution may reflect where visual-textual cultural cues are integrated.
- **Core assumption:** Layer location correlates with functional role in processing cultural information.
- **Evidence anchors:**
  - [abstract] "culture-sensitive neurons tend to cluster in certain decoder layers"
  - [Section 5.3] "culture-sensitive neurons generally cluster in the first layer (layer 0) and the early-mid layers (6–8), with relatively sparse presence in deeper blocks"
  - [corpus] No direct corpus evidence on layer-wise distribution of culture-sensitive neurons; this appears novel.
- **Break condition:** Assumption: If cultural information is processed uniformly across layers or primarily in attention heads rather than MLPs, layer-wise clustering patterns may not generalize across architectures.

## Foundational Learning

- **Concept:** SwiGLU activation and gated MLP architecture
  - **Why needed here:** The method instruments the nonlinearity branch (g = SiLU(u)) of SwiGLU blocks to record neuron activations. Understanding this structure is essential for correct hook placement.
  - **Quick check question:** Can you explain why the method records g rather than the full MLP output z?

- **Concept:** Activation-based neuron interpretability
  - **Why needed here:** The approach builds on prior work using firing frequency and magnitude to attribute neuron function. Distinguishing between correlation (neuron fires for culture) and causation (neuron enables culture-specific performance) is critical.
  - **Quick check question:** Why does the method use only correctly answered samples for activation recording?

- **Concept:** Margin-based selection vs. entropy-based selection
  - **Why needed here:** CAS improves on LAPE by focusing on contrast between top-two cultures rather than distributional concentration. High variance across cultures can mislead mean-based methods.
  - **Quick check question:** Under what conditions would CAS and MAD identify similar neurons?

## Architecture Onboarding

- **Component map:** Decoder MLPs (SwiGLU blocks) → nonlinearity branch (g = SiLU(u)) → neuron activations recorded per (layer, neuron, token) → aggregated statistics K, S, T per culture → CAS/LAP/LAPE/MAD scoring → top r% selection → binary mask applied during inference.
- **Critical path:** Correctly answered VQA samples → activation hooks on act_fn → per-culture aggregation → CAS scoring (margin P(1) - P(2)) → mask application → evaluation with self/cross-deactivation comparison.
- **Design tradeoffs:**
  - r=1% selection threshold balances specificity vs. impact; smaller values increase precision but reduce effect magnitude.
  - English-only prompts decouple language skill from cultural knowledge but may miss culture-language interactions.
  - Decoder-only instrumentation excludes vision encoder and attention heads, potentially missing other culture-sensitive components.
- **Failure signatures:**
  - High cross-deactivation spillover indicates neurons encode shared rather than culture-specific features.
  - Positive accuracy change upon ablation suggests selected neurons were detrimental (pruning effect).
  - Layer distribution varies by model architecture (LLaVA shows different patterns than Qwen/Pangea).
- **First 3 experiments:**
  1. Replicate CAS identification on one VLM with CVQA, verifying self-deactivation drop exceeds cross-deactivation by >3% margin.
  2. Compare CAS vs. LAPE on a held-out culture subset to confirm margin-based selection outperforms entropy-based selection for high-variance neurons.
  3. Extend analysis to attention heads to test whether culture sensitivity is MLP-specific or distributed across component types.

## Open Questions the Paper Calls Out

- **Question:** Can culture-sensitive neurons identified via CAS be utilized for activation steering to improve cultural alignment or mitigate bias without retraining?
  - **Basis in paper:** [explicit] The conclusion states, "Future work should... pair identification with activation steering," and earlier notes that identifying these components "may also guide future efforts to enhance these capabilities... through activation steering."
  - **Why unresolved:** The current study focuses exclusively on *ablation* (deactivating neurons) to establish causality and performance degradation; it does not test *interventions* designed to improve performance or correct biases.
  - **What evidence would resolve it:** Experiments applying activation steering vectors (derived from the identified culture-sensitive neurons) to under-performing cultural inputs to see if accuracy improves without negatively affecting other cultures.

- **Question:** Do culture-sensitive units exist in the vision encoder or attention heads of VLMs, or are they exclusive to the decoder MLPs?
  - **Basis in paper:** [explicit] The Limitations section states, "Our analysis is restricted to decoder MLP neurons and does not cover attention heads, vision encoders, or alignment modules, which may also encode culture-sensitive behavior."
  - **Why unresolved:** The methodology was designed to instrument only the SwiGLU non-linearity branch within the decoder, leaving the visual processing components and attention mechanisms unexplored.
  - **What evidence would resolve it:** Applying the Contrastive Activation Selection (CAS) method to the attention weight matrices and vision encoder outputs to determine if they exhibit similar culture-selective activation patterns.

- **Question:** Does the location and behavior of culture-sensitive neurons change when the model processes inputs in non-English languages?
  - **Basis in paper:** [explicit] The Limitations section notes that due to using English translations, "For multilingual models, it remains unknown whether our observations would still emerge, which we leave for future work."
  - **Why unresolved:** The study deliberately constrained experiments to English prompts to isolate cultural knowledge from language proficiency, removing the ability to observe potential interactions between language-specific and culture-specific neurons.
  - **What evidence would resolve it:** Running the identification and ablation pipeline on the original native-language prompts from the CVQA dataset and comparing the resulting neuron distributions and accuracy drops against the English-only baseline.

## Limitations

- The study relies on activation-based selection which may conflate correlation with causation, as some neurons show positive accuracy changes upon ablation suggesting they encode noise rather than useful cultural knowledge.
- Layer-wise clustering patterns may reflect architectural constraints rather than functional organization, particularly since analysis is limited to decoder MLPs while excluding vision encoders and attention heads.
- The English-only prompt design eliminates language-specific effects but may miss culture-language interactions that could confound neuron attribution.

## Confidence

- **High Confidence:** CAS outperforms LAP/LAPE in isolating culture-sensitive neurons (supported by self-deactivation vs. cross-deactivation gap metrics)
- **Medium Confidence:** Culture-sensitive neurons cluster in mid-to-late decoder layers (consistent across models but limited to decoder MLPs)
- **Medium Confidence:** Ablation causes selective performance degradation on corresponding culture (causal link demonstrated but alternative explanations possible)

## Next Checks

1. **Cross-architectural validation:** Test culture-sensitive neuron identification and ablation effects on a fourth VLM with different architecture (e.g., transformer encoder-decoder or different decoder depth) to assess pattern generalizability beyond Qwen2.5-VL-7B, Pangea-7B, and LLaVA-v1.6-Mistral-7B.

2. **Component scope expansion:** Extend analysis to vision encoder and attention heads to determine whether culture sensitivity is confined to decoder MLPs or distributed across VLM components, comparing layer-wise distributions between components.

3. **Language interaction testing:** Repeat experiments with multilingual prompts for cultures with available translations to quantify the confounding effect of language skill versus cultural knowledge and validate the English-only design choice.