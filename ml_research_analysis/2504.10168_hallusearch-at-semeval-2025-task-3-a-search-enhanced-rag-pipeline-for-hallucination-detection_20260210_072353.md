---
ver: rpa2
title: 'HalluSearch at SemEval-2025 Task 3: A Search-Enhanced RAG Pipeline for Hallucination
  Detection'
arxiv_id: '2504.10168'
source_url: https://arxiv.org/abs/2504.10168
tags:
- hallucination
- arxiv
- context
- language
- hallucinations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'HalluSearch is a multilingual pipeline for detecting and localizing
  hallucinated text spans in LLM outputs, developed for the Mu-SHROOM shared task.
  It employs a three-step RAG-based approach: factual splitting to isolate verifiable
  claims, context retrieval using Wikipedia and fallback strategies, and hallucination
  verification via LLM comparison.'
---

# HalluSearch at SemEval-2025 Task 3: A Search-Enhanced RAG Pipeline for Hallucination Detection

## Quick Facts
- **arXiv ID:** 2504.10168
- **Source URL:** https://arxiv.org/abs/2504.10168
- **Reference count:** 11
- **Primary result:** HalluSearch ranks 4th in English and Czech hallucination detection with IOU scores 0.3883-0.5681 across 14 languages

## Executive Summary
HalluSearch is a multilingual hallucination detection system developed for the Mu-SHROOM shared task at SemEval-2025 Task 3. The pipeline employs a three-step RAG-based approach combining factual splitting, Wikipedia-based context retrieval, and LLM comparison for hallucination verification. It achieves competitive performance ranking 4th in English and Czech, though performance varies significantly across languages due to coverage limitations in low-resource languages. The system demonstrates strong potential for multilingual hallucination detection while highlighting the challenges of consistent performance across diverse linguistic contexts.

## Method Summary
HalluSearch implements a three-step RAG pipeline for detecting and localizing hallucinated text spans in LLM outputs. The approach begins with factual splitting to isolate verifiable claims, followed by context retrieval using Wikipedia search with fallback strategies including query reformulation and LLM-based retrieval. The final step employs LLM comparison to verify hallucinated content against retrieved context. The system processes 14 languages using Wikipedia as the primary knowledge source, with additional retrieval strategies for cases where standard search yields insufficient results. The pipeline is designed to handle both entity-based and claim-based hallucinations through iterative retrieval and verification processes.

## Key Results
- Achieved 4th place ranking in English and Czech hallucination detection tasks
- Demonstrated IOU scores ranging from 0.3883 to 0.5681 across 14 languages
- Showed significant performance variability correlating with Wikipedia coverage depth per language
- Successfully detected both entity-based and claim-based hallucinations in multilingual contexts

## Why This Works (Mechanism)
HalluSearch works by leveraging the complementary strengths of retrieval-augmented generation and LLM-based verification. The factual splitting isolates verifiable claims, reducing the search space for context retrieval. Wikipedia-based retrieval provides authoritative ground truth for verification, while the fallback strategies ensure coverage even when initial retrieval fails. The LLM comparison step acts as a final quality check, comparing generated content against retrieved facts to identify hallucinations. This multi-layered approach addresses the limitations of single-method hallucination detection by combining search precision with LLM reasoning capabilities.

## Foundational Learning

**Factual Splitting**: Separates input text into verifiable claims vs. contextual information to optimize retrieval focus. *Why needed*: Reduces noise in retrieval queries and improves precision. *Quick check*: Verify that split claims are self-contained and verifiable.

**Context Retrieval**: Wikipedia-based search with fallback strategies for comprehensive fact verification. *Why needed*: Provides authoritative ground truth for hallucination detection. *Quick check*: Confirm retrieval coverage matches the language's Wikipedia size.

**LLM Verification**: Compares generated content against retrieved context to identify hallucinations. *Why needed*: Adds reasoning layer beyond simple fact matching. *Quick check*: Test verification accuracy on known hallucinated examples.

**Query Reformulation**: Iterative refinement of search queries when initial retrieval fails. *Why needed*: Handles cases where initial queries miss relevant context. *Quick check*: Measure success rate of reformulation vs. initial queries.

**Cross-Lingual Retrieval**: Adapts retrieval strategies for different language contexts. *Why needed*: Ensures consistent performance across multilingual inputs. *Quick check*: Compare retrieval quality across language pairs.

## Architecture Onboarding

**Component Map**: Factual Splitting -> Context Retrieval -> LLM Verification

**Critical Path**: The system's performance bottleneck occurs during context retrieval, where Wikipedia coverage limitations directly impact hallucination detection accuracy. Low-resource languages experience the most significant degradation due to sparse Wikipedia content.

**Design Tradeoffs**: Prioritizes Wikipedia-based retrieval for authoritative ground truth over broader web search, sacrificing coverage for precision. The three-step pipeline adds complexity but improves detection accuracy compared to single-method approaches.

**Failure Signatures**: 
- Poor retrieval coverage in low-resource languages leads to missed hallucinations
- Complex cross-sentence hallucinations may bypass factual splitting
- Insufficient verification data results in unreliable LLM comparison outputs
- Multiple candidate retrieval produces ambiguous verification results

**3 First Experiments**:
1. Test factual splitting accuracy on multilingual benchmark datasets to identify language-specific challenges
2. Evaluate context retrieval coverage correlation with Wikipedia article counts across all 14 languages
3. Measure LLM verification performance degradation when retrieval confidence falls below threshold

## Open Questions the Paper Calls Out
None

## Limitations
- Performance highly dependent on Wikipedia coverage, creating significant gaps for low-resource languages
- IOU scores vary substantially (0.3883-0.5681) across languages, indicating inconsistent detection capabilities
- Complex hallucinations spanning multiple sentences may evade factual splitting detection
- LLM-based verification reliability decreases when insufficient context is retrieved

## Confidence

**High confidence**: Core RAG pipeline architecture and three-step methodology are well-documented and reproducible

**Medium confidence**: Competitive ranking results (4th place in English and Czech) are verifiable through official SemEval-2025 Task 3 leaderboards

**Medium confidence**: Performance degradation in low-resource languages is consistently reported but requires further empirical validation

**Low confidence**: Effectiveness of modified similarity threshold (0.75) and query reformulation strategies across all 14 languages lacks comprehensive validation

## Next Checks
1. Conduct systematic evaluation of HalluSearch's performance across all 14 languages using standardized test suite to quantify correlation between web coverage and detection accuracy
2. Perform ablation studies to isolate impact of each pipeline component (factual splitting, context retrieval, verification) on overall performance metrics
3. Test system's robustness on domain-specific content outside Wikipedia coverage to assess generalization capabilities for specialized knowledge areas