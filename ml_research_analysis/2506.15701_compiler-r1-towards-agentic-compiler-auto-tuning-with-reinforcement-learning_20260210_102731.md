---
ver: rpa2
title: 'Compiler-R1: Towards Agentic Compiler Auto-tuning with Reinforcement Learning'
arxiv_id: '2506.15701'
source_url: https://arxiv.org/abs/2506.15701
tags:
- pass
- compiler
- tool
- optimization
- count
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Compiler-R1 is the first reinforcement learning framework for LLM-based
  compiler auto-tuning, addressing the lack of high-quality reasoning datasets and
  limited environment interaction. It introduces a curated dataset of 19,603 samples
  and a two-stage training pipeline combining supervised fine-tuning and reinforcement
  learning.
---

# Compiler-R1: Towards Agentic Compiler Auto-tuning with Reinforcement Learning

## Quick Facts
- arXiv ID: 2506.15701
- Source URL: https://arxiv.org/abs/2506.15701
- Reference count: 40
- Key outcome: First RL framework for LLM-based compiler auto-tuning achieving 8.46% average IR instruction reduction vs -Oz baseline with 96.71% success rate

## Executive Summary
Compiler-R1 introduces the first reinforcement learning framework for LLM-based compiler auto-tuning, addressing the dual challenges of limited reasoning datasets and restricted environment interaction. The system employs a two-stage training pipeline combining supervised fine-tuning (SFT) for tool protocol learning with reinforcement learning (RL) for optimization strategy refinement. Using 19,603 curated samples and AutoPhase feature representations, Compiler-R1 achieves significant improvements over traditional autotuners and SFT-only models, demonstrating the effectiveness of combining LLM reasoning with outcome-based rewards in compiler optimization.

## Method Summary
Compiler-R1 uses a two-stage training pipeline where SFT teaches LLMs tool-invocation protocols through 800 reasoning-trajectory samples, followed by RL refinement on 19k interactive episodes. The framework represents programs via 56 AutoPhase features rather than raw LLVM IR, enabling efficient LLM processing within context limits. Actions comprise 124 LLVM optimization passes plus -Oz fallback, with rewards balancing protocol compliance and IR instruction count reduction. The system evaluates on seven benchmarks, achieving an average 8.46% OverOz improvement with 96.71% success rate, outperforming both traditional autotuners and SFT-only approaches through its hybrid training methodology.

## Key Results
- 8.46% average reduction in IR instruction count compared to opt -Oz baseline
- 96.71% task success rate with protocol compliance
- SFT+RL achieves 83.36% success rate vs 26.98% (SFT-only) and 22.55% (RL-only)
- AutoPhase features achieve 0.4837 OverOrig at N=40, comparable to raw IR (0.4795)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-stage training pipeline (SFT + RL) enables more effective compiler auto-tuning than either approach alone.
- Mechanism: SFT initializes the agent with tool-invocation protocols and reasoning patterns (cold start), then RL refines the policy through outcome-based rewards from environment interaction. This separation allows the model to first learn *how* to interact, then learn *what* strategies optimize best.
- Core assumption: The skills learned in SFT (tool formatting, protocol adherence) transfer to and stabilize RL exploration.
- Evidence anchors:
  - [section 3.2.1] "The SFT stage addresses the cold-start challenge by teaching the LLM how to reason over program features and interact with external tools"
  - [section 4.3, Table 2] Ablation shows SFT-only achieves 26.98% success rate, RL-only achieves 22.55%, but SFT+RL achieves 83.36% (GRPO-1.5B, rep_penalty=1.05)
  - [corpus] Weak corpus support—related papers focus on evolutionary/hybrid approaches, not RL-based LLM tuning
- Break condition: If SFT samples poorly represent the tool-interaction space, RL may struggle to improve; if reward shaping is too sparse, RL may not converge.

### Mechanism 2
- Claim: Outcome-based reward signals guide the agent toward pass sequences that reduce IR instruction count.
- Mechanism: The composite reward R_final = w_f × R_format + w_a × R_answer balances protocol compliance (format reward) with optimization quality (answer reward). R_answer uses OverOrig metric comparing to unoptimized code, providing denser feedback than comparison to -Oz baseline.
- Core assumption: The reward function accurately captures optimization quality and the scaling factor α appropriately balances format vs. answer rewards.
- Evidence anchors:
  - [section 3.2.2] "R_answer = α · OverOrig quantifies the effectiveness of the final sequence... OverOrig provides a denser and more consistent reward signal"
  - [section 4.2, Table 1] GRPO-7B achieves 8.46% average OverOz improvement with 96.71% success rate
  - [corpus] AutoPhase (PPO-LSTM) [15] uses RL for phase ordering but achieves only 1.24% improvement, suggesting LLM-based policy representation matters
- Break condition: If format constraints are too rigid, the agent may optimize for compliance over optimization quality; if α is poorly tuned, reward hacking may occur.

### Mechanism 3
- Claim: AutoPhase features provide a compact, effective alternative to raw LLVM IR for program representation.
- Mechanism: 56 statistical features capture program structure (BB counts, instruction type distributions, control-flow properties) while abstracting away syntactic variations. This compression enables LLM processing within context limits while preserving optimization-relevant information.
- Core assumption: The 56 features capture sufficient semantic information to distinguish optimization opportunities.
- Evidence anchors:
  - [section 4.4, Table 3] AutoPhase achieves 0.4837 OverOrig at N=40, compared to 0.4795 for raw IR
  - [section 4.4] "AutoPhase captures sufficient structural and semantic information to guide effective optimization, while offering substantial advantages in terms of input length"
  - [corpus] AutoPhase [15] originally introduced these features for RL-based tuning; corpus shows feature-based representations are established in compiler ML
- Break condition: If programs have optimization opportunities dependent on specific instruction patterns not captured by the 56 features, the model may miss optimizations visible in raw IR.

## Foundational Learning

- Concept: **Proximal Policy Optimization (PPO) and GRPO**
  - Why needed here: The paper uses these on-policy RL algorithms to update LLM weights. Understanding clipping, advantage estimation, and policy gradient basics is essential for debugging training instability.
  - Quick check question: Can you explain why PPO uses a clipped objective rather than raw policy gradient?

- Concept: **LLVM IR and Optimization Passes**
  - Why needed here: The action space comprises 124 LLVM passes plus -Oz. Understanding what passes do (e.g., mem2reg, dead code elimination) helps interpret agent behavior and debug failure cases.
  - Quick check question: What is the difference between -O2, -O3, and -Oz optimization levels in LLVM?

- Concept: **Supervised Fine-Tuning vs. Reinforcement Learning for LLMs**
  - Why needed here: The two-stage pipeline leverages SFT for protocol learning and RL for policy optimization. Understanding when each is appropriate prevents misapplication.
  - Quick check question: Why might SFT alone lead to poor generalization in interactive environments?

## Architecture Onboarding

- Component map: Input (AutoPhase features + initial_inst_count) → LLM Agent (Qwen backbone) → Tool Calls (instrcount, find_best_pass_sequence) → Compiler Environment (LLVM opt) → Reward Computation (R_format + R_answer) → Policy Update (GRPO/PPO/RPP)

- Critical path:
  1. Foundation dataset construction → synergy graph → optimal sequence labeling
  2. SFT on 800 protocol samples (thought–action–feedback trajectories)
  3. RL on 19k episodes with outcome rewards
  4. Evaluation: single-trajectory inference with fallback to -Oz

- Design tradeoffs:
  - AutoPhase (56 dims) vs. raw IR: Compactness vs. potential information loss
  - SFT sample size (800): Too few → poor protocol learning; too many → overfitting to imitation
  - Repetition penalty: Table 2 shows success rate varies from 45.56% to 96.71% based on this hyperparameter

- Failure signatures:
  - Low success rate with high OverOz (GRPO-3B: 45.56% success, 5.12% OverOz): Agent circumvents protocol checks
  - Negative OverOz at N=1 for SFT-only models: Models rely on sampling, not adaptive reasoning
  - RL-only achieving only 22.55% success: Missing SFT initialization prevents protocol learning

- First 3 experiments:
  1. **Reproduce SFT baseline**: Train Qwen-1.5B on direct pass prediction, measure OverOz at N=1 vs. N=40 to verify sampling dependency
  2. **Ablate SFT initialization**: Train RL-only model and compare success rate to SFT+RL to validate cold-start hypothesis
  3. **Feature representation test**: Train identical SFT models on AutoPhase vs. raw IR (subset of 100 functions), confirm comparable performance as reported in Table 3

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Compiler-R1 be extended to provide human-interpretable explanations for the optimization decisions it makes?
- Basis in paper: [explicit] The conclusion lists "leveraging LLMs to improve the interpretability of auto-tuning processes" as a primary future research direction.
- Why unresolved: The current framework focuses on outcome-based rewards (instruction count reduction) rather than explainability, and the reasoning trajectories are simulated for tool use rather than semantic explanation.
- What evidence would resolve it: A user study demonstrating that the model's generated reasoning helps compiler engineers understand optimization bottlenecks.

### Open Question 2
- Question: Does an interactive, turn-by-turn dialogue approach for pass application improve performance over the current single-trajectory method?
- Basis in paper: [explicit] The conclusion suggests "exploring the efficacy of interactive, turn-by-turn pass application dialogues that could simulate traditional RL auto-tuning paradigms."
- Why unresolved: The current architecture uses a "thought–action–feedback" loop for a single sequence generation, not a multi-turn negotiation of individual passes.
- What evidence would resolve it: Comparative benchmarks showing higher OverOz% or faster convergence using a conversational agent architecture.

### Open Question 3
- Question: Does optimization for IR instruction count (code size) in Compiler-R1 correlate with improvements in runtime execution speed?
- Basis in paper: [inferred] The paper explicitly defines the tuning objective as "reducing the Intermediate Representation (IR) instruction count" (emulating -Oz) and does not evaluate execution time.
- Why unresolved: Pass sequences that minimize size often differ from those that maximize speed; the agent's learned policy might be sub-optimal for latency-sensitive applications.
- What evidence would resolve it: Experiments measuring execution cycles/latency on benchmarks like SPEC CPU using the current Compiler-R1 model.

## Limitations

- Limited hyperparameter transparency for GRPO/PPO training parameters and reward scaling factor α
- Evaluation focuses exclusively on IR instruction count without measuring runtime performance or correctness verification
- 96.71% success rate claim may be influenced by fallback mechanism to -Oz, potentially masking underlying optimization failures

## Confidence

- **High Confidence**: The two-stage SFT+RL pipeline design and its superiority over individual components
- **Medium Confidence**: The effectiveness of AutoPhase features vs. raw IR (limited ablation with only 100 functions tested)
- **Medium Confidence**: The 8.46% average OverOz improvement (evaluated only on CompilerGym benchmarks)
- **Low Confidence**: Claims about LLM-based auto-tuning being fundamentally superior to traditional evolutionary methods

## Next Checks

1. **Cross-validation on unseen architectures**: Evaluate Compiler-R1 on programs from different domains (e.g., embedded systems, HPC kernels) not represented in the CompilerGym training corpus to test generalization beyond curated datasets.

2. **Runtime vs. instruction count correlation**: Measure actual execution time reduction alongside IR instruction reduction for representative programs to validate that instruction count is a meaningful proxy for performance optimization.

3. **Robustness to program size scaling**: Test models trained on <10k IR instructions on larger programs (50k-100k IR instructions) to assess whether the 56 AutoPhase features and pass selection strategies scale effectively to real-world compiler scenarios.