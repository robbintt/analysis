---
ver: rpa2
title: 'Multilingual corpora for the study of new concepts in the social sciences
  and humanities:'
arxiv_id: '2512.07367'
source_url: https://arxiv.org/abs/2512.07367
tags:
- corpus
- pour
- dans
- donn
- innovation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a hybrid methodology for building a multilingual
  corpus to study emerging concepts in the humanities and social sciences, using "non-technological
  innovation" as a case study. The corpus is constructed from company websites and
  annual reports, processed through automated language detection, content filtering,
  and metadata enrichment.
---

# Multilingual corpora for the study of new concepts in the social sciences and humanities:

## Quick Facts
- **arXiv ID**: 2512.07367
- **Source URL**: https://arxiv.org/abs/2512.07367
- **Reference count**: 0
- **Primary result**: Hybrid methodology successfully identified over 700 new terms related to non-technological innovation, with over 80% expert validation

## Executive Summary
This paper presents a hybrid methodology for constructing multilingual corpora to study emerging concepts in humanities and social sciences. Using "non-technological innovation" as a case study, the approach combines automated data collection from company websites and annual reports with expert validation. The methodology enables systematic identification and analysis of lexical variability in emerging domain concepts while generating machine learning-ready datasets. The approach demonstrates reproducibility and extensibility for studying other emerging concepts across different domains.

## Method Summary
The methodology employs automated language detection and content filtering to process multilingual data from company sources, followed by metadata enrichment to create a comprehensive corpus. A derived English dataset is generated by extracting five-sentence contextual blocks around expert lexicon terms, which are then annotated with thematic categories. This hybrid approach combines computational efficiency with domain expertise validation, enabling the creation of structured datasets suitable for natural language processing applications while maintaining conceptual accuracy through expert oversight.

## Key Results
- Successfully identified over 700 new terms related to non-technological innovation
- Achieved over 80% expert validation rate for identified terms
- Generated machine learning-ready English dataset with five-sentence contextual blocks around lexicon terms

## Why This Works (Mechanism)
The methodology succeeds by leveraging the complementary strengths of automated processing and human expertise. Automated language detection and content filtering efficiently process large volumes of multilingual data from diverse company sources, while metadata enrichment adds crucial contextual information. The five-sentence contextual block extraction captures semantic relationships effectively for machine learning applications, and expert validation ensures conceptual accuracy of identified terms.

## Foundational Learning
- **Automated language detection**: Enables processing of multilingual data at scale; quick check: test detection accuracy across different language families
- **Content filtering algorithms**: Identifies relevant domain-specific content from heterogeneous sources; quick check: measure precision/recall against ground truth samples
- **Metadata enrichment**: Adds contextual information to raw data for better analysis; quick check: validate enriched metadata against known standards
- **Contextual window extraction**: Captures semantic relationships for NLP applications; quick check: test different window sizes on sample data
- **Expert validation protocols**: Ensures domain accuracy of automated findings; quick check: compare expert agreement rates across multiple validators
- **Hybrid methodology design**: Combines computational efficiency with human expertise; quick check: measure trade-offs between automation and accuracy

## Architecture Onboarding
**Component map**: Data collection -> Language detection -> Content filtering -> Metadata enrichment -> Contextual extraction -> Expert validation -> ML dataset
**Critical path**: The sequence from data collection through expert validation is essential for maintaining data quality and domain accuracy
**Design tradeoffs**: Automation vs. accuracy, scale vs. precision, efficiency vs. comprehensiveness
**Failure signatures**: Incomplete language detection leading to data loss, over-filtering excluding relevant content, insufficient context windows missing semantic relationships
**First experiments**: 1) Test language detection accuracy on multilingual sample data; 2) Validate content filtering precision with domain experts; 3) Experiment with different contextual window sizes for semantic capture

## Open Questions the Paper Calls Out
None

## Limitations
- Expert validation relies on single domain expert rather than multiple independent validators, potentially introducing confirmation bias
- Automated language detection and content filtering may exclude relevant multilingual content outside predetermined thresholds
- Five-sentence contextual window parameter remains empirically untested across different domains

## Confidence
- **High**: Corpus construction methodology and basic automation pipeline
- **Medium**: Expert validation of term relevance and thematic annotation quality
- **Medium**: Applicability to other emerging concepts beyond non-technological innovation

## Next Checks
1. Conduct inter-rater reliability assessment using 3-5 independent domain experts to verify the 80% validation rate
2. Test alternative contextual window sizes (3, 7, and 10 sentences) to optimize semantic capture for machine learning applications
3. Apply the methodology to a distinct emerging concept in social sciences to evaluate generalizability across domains