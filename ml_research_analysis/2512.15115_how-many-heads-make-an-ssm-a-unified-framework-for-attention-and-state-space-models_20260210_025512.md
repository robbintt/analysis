---
ver: rpa2
title: How Many Heads Make an SSM? A Unified Framework for Attention and State Space
  Models
arxiv_id: '2512.15115'
source_url: https://arxiv.org/abs/2512.15115
tags:
- attention
- interaction
- linear
- factorized
- sequence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces a unified framework for sequence modeling
  architectures, representing them through an input-dependent effective interaction
  operator Wij(X). It identifies two core construction patterns: (1) the Unified Factorized
  Framework (explicit), where Wij(X) varies through scalar coefficients applied to
  shared value maps (e.g., attention), and (2) Structured Dynamics (implicit), where
  Wij is induced by a latent dynamical system (e.g., state space models).'
---

# How Many Heads Make an SSM? A Unified Framework for Attention and State Space Models

## Quick Facts
- arXiv ID: 2512.15115
- Source URL: https://arxiv.org/abs/2512.15115
- Reference count: 15
- One-line primary result: The paper establishes that H=k heads are both necessary and sufficient to represent a linear SSM with k-dimensional interaction subspace, formalizing the algebraic necessity of multi-head attention beyond ensemble benefits.

## Executive Summary
This paper presents a unified theoretical framework that characterizes sequence modeling architectures through an input-dependent effective interaction operator W_ij(X). It identifies two fundamental construction patterns: explicit factorized frameworks (like attention) where W_ij varies through scalar coefficients applied to shared value maps, and structured dynamics (like SSMs) where W_ij is induced by latent dynamical systems. The framework enables rigorous analysis revealing fundamental trade-offs between expressivity and trainability. Key results include the Interaction Rank Gap theorem showing single-head models cannot represent certain structured dynamical maps, the Equivalence Theorem proving H=k heads are necessary and sufficient for linear SSM representation, and the Gradient Highway Result demonstrating attention's superior long-range gradient propagation compared to stable linear dynamics.

## Method Summary
The paper constructs a unified sequence modeling framework where outputs y_i are computed as weighted sums of inputs x_j through an effective interaction operator W_ij(X). Two construction strategies are identified: explicit factorization (W_ij = α_ij V for attention) and implicit structured dynamics (W_ij = CĀ^(i-j)B for SSMs). The framework is applied to prove theoretical results about model expressivity (Interaction Rank Gap theorem) and trainability (Gradient Highway theorem). Experiments include teacher-student system identification tasks where multi-head linear attention learns to mimic linear SSMs with known interaction ranks, and gradient norm comparisons between linear SSM and attention models across sequence lengths.

## Key Results
- The Interaction Rank Gap theorem proves single-head factorized models cannot approximate linear SSMs with non-collinear impulse responses unless head count H matches the interaction rank k of target dynamics.
- The Equivalence (Head-Count) Theorem establishes that H=k heads are both necessary and sufficient to represent a linear SSM with k-dimensional interaction subspace.
- The Gradient Highway Result demonstrates that attention layers maintain distance-independent gradient paths while stable linear dynamics exhibit exponential attenuation, explaining attention's superior long-range training stability.

## Why This Works (Mechanism)

### Mechanism 1: Multi-Head Rank Expansion
The paper defines "interaction rank" as the dimension of the subspace spanned by a model's pairwise linear maps. Single-head attention constrains all interaction operators W_ij to be scalar multiples of a single shared value matrix V (rank-1 limitation). To represent a linear SSM whose dynamics span a k-dimensional subspace, the model requires H ≥ k heads to provide enough basis matrices {V^(h)} to span the target space. This algebraic necessity explains why multi-head attention serves an essential function beyond ensemble averaging.

### Mechanism 2: The Gradient Highway Effect
In stable linear SSMs, the Jacobian J_i,j involves powers of the transition matrix Ā^(i-j), which decays if ||Ā|| < 1. In contrast, attention mechanisms generate effective weights W_ij via softmax. The proof constructs specific inputs where the attention weight α_ij → 1, making the Jacobian ≈ V regardless of temporal distance i-j. This creates a "highway" for gradient flow that bypasses the sequential decay inherent in recurrence, explaining attention's superior long-range training stability.

### Mechanism 3: Implicit vs. Explicit Interaction Trade-off
The unified framework contrasts how W_ij is constructed. Factorized models explicitly compute scalar coefficients for a fixed basis (easy to parallelize, guaranteed gradient paths). Structured dynamics define W_ij implicitly via products of state transition matrices (allows rich, high-rank operator span but enforces sequential dependencies and spectral decay). This fundamental tension between algebraic expressivity and long-range gradient propagation is resolved by hybrid architectures that combine both approaches.

## Foundational Learning

- **Concept: Effective Interaction Operator (W_ij)**
  - Why needed here: This is the central unifying primitive. Without understanding that y_i = Σ W_ij x_j describes both RNNs and Transformers, the comparison of "rank" and "gradients" across architectures makes no sense.
  - Quick check question: How does the definition of W_ij differ mathematically between an RNN (implicit) and a Transformer (explicit)?

- **Concept: Interaction Rank (Operator Span)**
  - Why needed here: This concept quantifies "expressivity." It explains why a single matrix V is insufficient to model rotating dynamics, serving as the theoretical justification for multi-head attention.
  - Quick check question: If a linear SSM has a transition matrix Ā that is a 2D rotation, what is the minimum interaction rank k required for a factorized model to represent it?

- **Concept: Spectral Norm and Stability**
  - Why needed here: Essential for understanding the "Gradient Highway" result. The constraint ||Ā||₂ < 1 guarantees stability in SSMs but simultaneously guarantees exponential gradient decay.
  - Quick check question: Why does stability (preventing state explosion) in a linear recurrence inevitably lead to vanishing gradients over long time horizons?

## Architecture Onboarding

- **Component map:**
  - Unified Interface: Y = f(X) viewed as y_i = Σ W_ij(X) x_j
  - Strategy I (Explicit): Factorized W_ij = α_ij V (Attention/Linear Attn)
  - Strategy II (Implicit): Structured W_ij = CĀ^(i-j)B (SSMs/RNNs)
  - The Bridge: Multi-Head Mechanism (Σ α_ij^(h) V^(h)) allowing Strategy I to mimic Strategy II

- **Critical path:**
  1. Define the target sequence task (requires long-range? high-rank dynamics?)
  2. Check Interaction Rank: Does the task need diverse linear operators (high k)? If yes, single-head attention will fail; ensure H ≥ k
  3. Check Gradient Propagation: Is sequence length n large? If yes, pure SSMs might attenuate signals; consider attention "skip connections" or hybrid layers

- **Design tradeoffs:**
  - Heads (H) vs. Efficiency: Increasing heads H increases interaction rank (good for expressivity) but increases computation/memory O(n²H) (unless using linear attention)
  - SSM Stability vs. Gradient Flow: Making SSM transition Ā stable prevents exploding states but kills long-range gradients
  - Explicit vs. Implicit: Explicit (Attention) is O(n²) but parallelizable and easy to train. Implicit (SSM) is O(n) but sequential and harder to train

- **Failure signatures:**
  - Rank Collapse: Single-head model failing to learn rotating state dynamics despite ample training data (Theorem 4.2)
  - Long-Range Amnesia: Deep, purely linear SSM ignoring early inputs because ||Ā^n|| has decayed to zero (Theorem 5.1)
  - Gradient Starvation: In hybrids, if SSM path dominates, attention layers might receive weak learning signals from distant tokens

- **First 3 experiments:**
  1. Teacher-Student Rank Verification: Train multi-head linear attention to mimic linear SSM with known interaction rank k. Plot MSE vs. Head Count H to verify sharp drop at H=k (Theorem 4.4)
  2. Gradient Norm Profiling: Measure ||∂y_T/∂x_0|| for Transformer vs. SSM across sequence lengths T. Plot on log scale to visualize "linear vs. exponential" gap (Theorem 5.1)
  3. Hybrid Ablation: Implement "Jamba-style" hybrid (Section 7). Remove attention layers and measure performance degradation on long-range retrieval vs. local copy tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the optimal design principles for hybrid architectures that interleave SSM and attention layers, given the formalized rank–trainability trade-off?
- Basis in paper: The Discussion section notes that hybrids like Jamba "can be interpreted as an engineering response to the rank–trainability trade-off," but provides no theoretical guidance on interleaving frequency, placement, or head allocation.
- Why unresolved: The paper characterizes the trade-off but does not derive conditions under which specific hybrid configurations provably improve optimization or expressivity.
- What evidence would resolve it: Theoretical bounds on gradient flow and interaction rank for hybrid configurations; empirical studies systematically varying attention layer frequency and position.

### Open Question 2
- Question: Can the theoretical framework be extended to characterize interaction rank and gradient propagation for input-dependent (selective) SSMs such as Mamba?
- Basis in paper: Appendix A states that "selective/state-dependent variants such as Mamba depart from strict translation invariance and require separate analysis."
- Why unresolved: The main theorems assume time-invariant transitions or input-independent kernels; selective SSMs have input-dependent state transitions that invalidate current proof techniques.
- What evidence would resolve it: Extended versions of Theorems 4.4 and 5.1 that apply to selective SSM parameterizations, or proof that fundamentally different analysis is required.

### Open Question 3
- Question: What are the expected gradient norms in attention layers under standard initialization distributions, as opposed to the worst-case existence result in Theorem 5.1?
- Basis in paper: Remark 5.2 explicitly distinguishes the "topological capacity to propagate signals" from "a guarantee that gradients will be stable for random initializations or average-case data."
- Why unresolved: The current result constructs specific inputs to demonstrate gradient highways but does not characterize typical behavior.
- What evidence would resolve it: Theoretical analysis of expected gradient norms under common initialization schemes; large-scale empirical measurement of gradient statistics across random inputs.

## Limitations

- The theoretical results rely on simplifying assumptions including linear, time-invariant systems, while practical deep learning involves non-linearities and complex initialization schemes.
- The gradient analysis assumes idealized attention patterns without accounting for common attention sink or concentration effects that occur in practice.
- The framework focuses on algebraic expressivity and gradient flow but does not address practical considerations like computational efficiency, hardware constraints, or the role of normalization and residual connections.

## Confidence

**High Confidence:** The mathematical foundations of the unified framework are sound, and the equivalence theorem (H=k heads necessary and sufficient for linear SSM representation) is rigorously proven. The gradient attenuation results for stable linear dynamics are well-established linear systems theory.

**Medium Confidence:** The practical implications of the Interaction Rank Gap in non-linear, real-world settings remain somewhat speculative. While the theory clearly shows limitations for single-head models in representing structured dynamics, the extent to which this manifests in modern deep learning architectures is less certain.

**Low Confidence:** The specific numerical claims in experimental results (e.g., exact MSE values, precise gradient decay rates) cannot be fully verified without complete experimental details including training hyperparameters, model dimensions, and initialization schemes.

## Next Checks

1. **Rank Gap Empirical Verification:** Implement teacher-student experiment with systematic variation of interaction rank k and head count H to empirically verify the sharp threshold behavior predicted by the Equivalence Theorem. Measure both MSE convergence and the singular value spectrum of learned interaction operators.

2. **Gradient Flow in Non-Ideal Conditions:** Extend gradient highway analysis to include realistic attention patterns (not just idealized softmax constructions), measuring gradient norms in models with attention sinks, varying temperature parameters, and mixed attention patterns. Compare against linearized SSM approximations of trained models.

3. **Hybrid Architecture Stability Analysis:** Systematically ablate attention layers in proposed hybrid architectures while monitoring both task performance and gradient flow metrics. Quantify the trade-off between expressivity gains and potential gradient starvation in the SSM pathways, particularly for long-range retrieval tasks.