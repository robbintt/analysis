---
ver: rpa2
title: 'Stabilizing Reinforcement Learning with LLMs: Formulation and Practices'
arxiv_id: '2512.01374'
source_url: https://arxiv.org/abs/2512.01374
tags:
- training
- policy
- gradient
- minirl
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel formulation for reinforcement learning
  (RL) with large language models (LLMs), explaining why and under what conditions
  the true sequence-level reward can be optimized via a surrogate token-level objective
  in policy gradient methods such as REINFORCE. Specifically, through a first-order
  approximation, we show that this surrogate becomes increasingly valid only when
  both the training-inference discrepancy and policy staleness are minimized.
---

# Stabilizing Reinforcement Learning with LLMs: Formulation and Practices

## Quick Facts
- arXiv ID: 2512.01374
- Source URL: https://arxiv.org/abs/2512.01374
- Reference count: 33
- Authors: Chujie Zheng; Kai Dang; Bowen Yu; Mingze Li; Huiqiang Jiang; Junrong Lin; Yuqiong Liu; Hao Lin; Chencan Wu; Feng Hu; An Yang; Jingren Zhou; Junyang Lin
- Primary result: Proposes a theoretical framework explaining RL instability in LLMs and introduces practical techniques (importance sampling correction, clipping, Routing Replay) that stabilize training, validated on 30B MoE model

## Executive Summary
This paper addresses the persistent instability in reinforcement learning (RL) for large language models (LLMs) by developing a theoretical framework that explains why token-level policy gradients can approximate sequence-level rewards. Through a first-order Taylor expansion, the authors show that this approximation remains valid only when training-inference discrepancy and policy staleness are minimized. This insight leads to practical techniques including importance sampling correction, decoupled PPO-style clipping, and Routing Replay for Mixture-of-Experts models. Extensive experiments with a 30B MoE model demonstrate that these techniques are essential for stable off-policy training, while on-policy training remains stable with IS correction alone.

## Method Summary
The method implements a token-level policy gradient algorithm (MiniRL) that optimizes sequence-level rewards through REINFORCE-style updates. The key innovation is the theoretical justification for using token-level objectives as first-order approximations to sequence-level rewards when policy divergence is controlled. The algorithm includes token-level importance sampling correction to compensate for training-inference discrepancy, decoupled PPO-style clipping to mask high-variance gradient updates, and specialized Routing Replay techniques for MoE models that fix expert routing decisions during training. The implementation uses synchronous RL with mini-batch gradient updates, FP8 inference with BF16 training, and group-normalized advantages.

## Key Results
- Basic policy gradient with IS correction achieves highest training stability for on-policy RL
- For off-policy training, combining clipping and Routing Replay becomes essential to mitigate instability
- Prolonged optimization consistently yields comparable final performance regardless of cold-start initialization
- Token-level IS correction alone prevents training collapse, while omitting it causes sharp entropy drops
- R3 (Rollout Routing Replay) is essential for high off-policiness in MoE models

## Why This Works (Mechanism)

### Mechanism 1: First-Order Approximation Enables Token-Level Optimization of Sequence Rewards
The token-level policy gradient objective serves as a valid first-order approximation to the true sequence-level reward objective when the target policy and rollout policy remain close. Through decomposition of the sequence-level importance sampling weight, the intractable sequence likelihood ratio becomes a sum of tractable token-level ratios. This approximation breaks when policy divergence becomes large, causing second-order terms to dominate.

### Mechanism 2: Importance Sampling Correction Compensates for Training-Inference Discrepancy
The token-level IS weight πθold(yt)/µθold(yt) is mathematically required to maintain first-order approximation validity. The IS ratio decomposes into training-inference discrepancy × policy staleness components. Training and inference engines typically employ different computational kernels for peak performance, yielding inconsistent outputs given the same model input, necessitating correction.

### Mechanism 3: Routing Replay Stabilizes MoE by Fixing Expert Assignments
For MoE models, fixing routed experts during gradient updates (Routing Replay) restores first-order approximation validity by decoupling routing dynamics from policy optimization. MoE routing introduces additional discrepancy terms that break token-level IS validity. R3 replays inference-engine expert assignments in training, eliminating the routing component of training-inference discrepancy, while R2 replays training-engine assignments to reduce policy staleness effects.

## Foundational Learning

- **Importance Sampling in Policy Gradients**: Why needed: The entire theoretical framework relies on understanding how IS weights enable estimating expectations under one distribution using samples from another. Quick check: If you sample responses from policy µ but optimize policy π, what correction factor must you apply to the gradient estimator?

- **First-Order Taylor Approximation**: Why needed: The core theoretical contribution is showing that token-level objectives approximate sequence-level objectives via first-order expansion. Quick check: For what values of δ does (1+δ₁)(1+δ₂)...(1+δₙ) ≈ 1 + δ₁ + δ₂ + ... + δₙ break down?

- **Mixture-of-Experts Routing Mechanisms**: Why needed: MoE routing adds a discrete, sensitive decision point that invalidates standard IS assumptions. Understanding that routing is input-dependent and numerically sensitive explains why specialized techniques are necessary. Quick check: Why would the same input token be routed to different experts in training vs. inference engines with identical weights?

## Architecture Onboarding

- **Component map**: Prompts -> Rollout Engine (Inference) -> Reward Scorer -> Training Engine -> Model Parameters -> Rollout Engine
- **Critical path**: 1) Sample batch of prompts D (B=64), 2) Generate G=16 responses per prompt via inference engine, 3) Compute rewards and normalize to advantages, 4) Split into mini-batches (size 1,024), apply N gradient updates, 5) Per token: compute IS ratio, apply truncation, evaluate clipping condition, accumulate gradient, 6) Monitor diagnostic metrics
- **Design tradeoffs**: On-policy (N=1) vs. Off-policy (N>1): On-policy maximizes stability but underutilizes compute. Off-policy accelerates convergence but requires clipping + Routing Replay. R2 vs. R3 for MoE: R2 preserves target policy in first mini-batch, better for low off-policiness. R3 reduces training-inference discrepancy, essential for high off-policiness but introduces bias.
- **Failure signatures**: Entropy collapse: Sharp drop in policy entropy indicates collapse; check if IS correction was omitted. Training-inference KL spike: Values exceeding ~10⁻¹ suggest training-inference discrepancy dominating. Benchmark score regression during training: Indicates instability from off-policy updates.
- **First 3 experiments**: 1) On-policy baseline validation: Run MiniRL with N=1, verify stable entropy and gradually increasing benchmark scores. Remove IS correction as ablation—should observe immediate collapse. 2) Off-policy stress test: Increase N to 4 without clipping or Routing Replay. Observe failure mode (premature collapse). Then add clipping + R3, verify stability recovery. 3) MoE routing diagnostics: For MoE models, measure training-inference routing consistency. Compare R2 vs. R3 impact on this metric and correlate with training stability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do alternative clipping or masking strategies, such as sequence-level clipping, compare to the token-level clipping used in MiniRL regarding the preservation of the first-order approximation?
- Basis in paper: [explicit] Section 4.1 states that while alternative strategies exist, "we leave the study of clipping or masking strategies for future work."
- Why unresolved: The current work restricts itself to a specific decoupled PPO-style clipping mechanism to maintain consistency with the surrogate token-level objective.
- Evidence: Empirical comparisons of various clipping granularities (token vs. sequence) under controlled policy staleness conditions to measure divergence from the true sequence-level reward.

### Open Question 2
- Question: Can the stability and convergence of the MiniRL formulation be improved by developing more sophisticated advantage estimates than the simple group-normalization of rewards?
- Basis in paper: [explicit] Section 4.1 notes, "exploring better advantage estimates... may also be helpful, but falls outside the scope of this work."
- Why unresolved: The paper relies on a baseline approach for advantage estimation to focus on the effects of importance sampling and routing, leaving optimization via learned baselines unexplored.
- Evidence: Ablation studies integrating learned value functions into the surrogate objective to determine if they reduce gradient variance without violating the first-order approximation conditions.

### Open Question 3
- Question: Does the validity of the first-order approximation formulation extend to value-based methods like PPO, or is it strictly limited to the sequence-level reward setting described?
- Basis in paper: [inferred] Section 2.1 explicitly excludes value-based settings (e.g., PPO), citing the difficulty of obtaining scalable value models, leaving the applicability of the formulation to this dominant paradigm unverified.
- Why unresolved: The theoretical derivation depends on J^seq(θ); it is unclear if the token-level surrogate objective remains a valid approximation when optimizing a learned value estimate rather than a sparse sequence reward.
- Evidence: Theoretical analysis extending the first-order approximation derivation to value-based objectives, followed by experiments testing if techniques like Routing Replay stabilize value-based RL for LLMs.

## Limitations

- The theoretical framework's validity depends on maintaining small policy divergence, but exact thresholds for "small" δₜ values remain empirical rather than rigorously characterized
- The effectiveness of proposed techniques may be task-dependent, with current validation limited to mathematical reasoning tasks
- The corpus lacks direct corroborating evidence for MoE-specific claims about Routing Replay, relying instead on the paper's experimental results

## Confidence

- **High Confidence**: The necessity of token-level importance sampling correction for on-policy RL stability is well-supported by both theory and ablation experiments
- **Medium Confidence**: The decomposition of importance sampling weights into training-inference discrepancy and policy staleness components is logically sound but has weaker empirical evidence
- **Medium Confidence**: The Routing Replay techniques for MoE models are theoretically justified and experimentally validated but lack external corroboration

## Next Checks

1. **Boundary Analysis for First-Order Approximation**: Systematically vary the policy update magnitude (through learning rate and N) and measure the deviation between token-level and sequence-level gradients. Quantify when second-order terms become significant.

2. **Cross-Task Generalization**: Apply the stabilized RL framework to non-mathematical tasks (e.g., code generation, general instruction following) and verify that the same combination of IS correction, clipping, and Routing Replay maintains stability across domains.

3. **Ablation of Routing Replay Components**: For MoE models, isolate the contributions of R2 vs. R3 by measuring training-inference routing consistency and correlating it with KL divergence and stability metrics. Determine whether routing replay primarily addresses training-inference discrepancy or policy staleness.