---
ver: rpa2
title: 'Structurally Human, Semantically Biased: Detecting LLM-Generated References
  with Embeddings and GNNs'
arxiv_id: '2601.20704'
source_url: https://arxiv.org/abs/2601.20704
tags:
- graphs
- random
- ground
- truth
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We evaluate how well LLM-generated bibliographies mimic human citation
  patterns by comparing their induced citation graphs. Using 10,000 focal papers and
  paired GPT-4o-generated references, we find that structural graph features (centrality,
  clustering) barely distinguish LLM from human graphs (accuracy ~0.60) but clearly
  separate them from random baselines (~0.90).
---

# Structurally Human, Semantically Biased: Detecting LLM-Generated References with Embeddings and GNNs

## Quick Facts
- arXiv ID: 2601.20704
- Source URL: https://arxiv.org/abs/2601.20704
- Reference count: 40
- Key outcome: Semantic embeddings enable 83% RF and 93% GNN accuracy in detecting LLM-generated bibliographies, while structural features alone achieve only ~60% accuracy

## Executive Summary
This study investigates whether LLM-generated bibliographies structurally mimic human citation patterns by comparing their induced citation graphs. Using 10,000 focal papers and GPT-4o-generated references, researchers found that structural graph features (centrality, clustering) barely distinguish LLM from human graphs (~60% accuracy) but clearly separate them from random baselines (~90%). In contrast, semantic embeddings from titles and abstracts sharply increase detection accuracy, with GNNs reaching 93% accuracy. Similar results hold for Claude Sonnet 4.5 and multiple embedding models, demonstrating that LLM bibliographies structurally mimic human networks but retain detectable semantic fingerprints.

## Method Summary
The study uses SciSciNet data with 10,000 focal papers and ~275k references, paired with GPT-4o/Claude-generated references. It constructs citation graphs comparing ground truth (human) vs. LLM-generated vs. random baselines (field-matched permutations). Structural features (centrality, clustering) and semantic features (OpenAI/SPECTER embeddings) are extracted and aggregated. Random Forest classifiers (100 estimators) and GNNs (GCN, GAT, GraphSAGE, GIN) are trained on 70/15/15 stratified splits to classify ground truth vs. LLM references.

## Key Results
- Structural graph features barely distinguish LLM from human graphs (accuracy ~0.60) but cleanly reject random baselines (~0.90)
- Semantic embeddings from titles/abstracts achieve ~0.83 accuracy with RF and 93% with GNNs
- Similar detection patterns observed for Claude Sonnet 4.5 and multiple embedding models (SPECTER, OpenAI)
- GNNs outperform RF by jointly processing graph structure and node content, detecting subtle semantic-topological inconsistencies

## Why This Works (Mechanism)

### Mechanism 1: Structural Mimicry Limits Detection
- Claim: If detection relies solely on global citation topology, it will likely fail because LLMs reproduce human graph statistics
- Core assumption: LLM training captures the generative process of network formation
- Evidence: "Structural graph features... barely distinguish LLM from human graphs (accuracy ~0.60)" and "centrality profiles in GPT-generated graphs closely match ground truth"
- Break condition: If humans begin citing in fundamentally novel structural patterns not present in LLM training

### Mechanism 2: Semantic Fingerprinting Enables Separability
- Claim: If detection utilizes high-dimensional semantic embeddings, it can identify systematic biases in LLM reference selection
- Core assumption: Embedding models linearly separate concepts relevant to citation selection
- Evidence: "Semantic embeddings from titles and abstracts sharply increase detection accuracy... GNNs reach 93% accuracy"
- Break condition: If LLM is fine-tuned to diversify semantic selection or embedding models fail to capture nuance

### Mechanism 3: Topological-Semantic Fusion (GNNs)
- Claim: If detection models jointly process graph structure and node content, they maximize discriminative power
- Core assumption: Interaction between "what is cited" (semantics) and "how it is connected" (topology) contains signal
- Evidence: "GNNs with embedding node features achieve 93% test accuracy" and performance collapses with random vectors
- Break condition: If LLM varies bias based on network position

## Foundational Learning

- **Graph Neural Networks (GNNs) & Message Passing**: Why needed to understand why GNNs (93%) outperform RF (83%). Quick check: How does a 2-layer GCN update the feature vector of a specific node v?
- **Embedding Spaces (Vector Semantics)**: Why needed to grasp how "semantic bias" is measured. Quick check: What does high cosine similarity between focal paper and references imply about LLM's generation strategy?
- **Parametric Knowledge & Hallucination**: Why needed to understand "semantic bias" vs. retrieval errors. Quick check: Why does paper specify references were generated "purely from parametric knowledge"?

## Architecture Onboarding

- **Component map**: Data Layer (SciSciNet + GPT-4o/Claude) -> Graph Builder (paired graphs + random baselines) -> Feature Engine (structural vs. semantic features) -> Model Layer (RF vs. GNNs) -> Evaluator (accuracy/F1)
- **Critical path**: Dataset integrity (paired graphs), Random Baseline (breaking latent structure), Embedding Extraction (3072-D vectors)
- **Design tradeoffs**: RF is faster and interpretable but GNNs capture structural-semantic interactions; higher embedding dimensions increase cost but improve accuracy
- **Failure signatures**: Random baseline success with LLM detection failure validates structural mimicry; chance performance on embeddings indicates broken pipeline
- **First 3 experiments**: 1) Sanity Check (Structure): RF on graph properties vs. Random baseline (~90% accuracy required), 2) Core Task (Semantics): RF on embeddings (target ~83%), 3) Ablation (GNN): GCN with embeddings vs. random vectors

## Open Questions the Paper Calls Out

- **Open Question 1**: Which specific semantic dimensions (recency, prestige, methodology) drive detectable differences between LLM-generated and human references? [explicit]
- **Open Question 2**: Does the semantic fingerprint persist when LLMs utilize external databases (retrieval-augmented generation)? [explicit]
- **Open Question 3**: Can debiasing methods effectively target semantic signals without disrupting structural realism of LLM citation graphs? [inferred]

## Limitations
- Prompt variability uncertainty: Different prompts and sampling strategies may affect semantic bias fingerprint
- Citation context incompleteness: Analysis ignores in-text citation context that could provide additional semantic signal
- Temporal distribution bias: Paper mentions recency bias but doesn't systematically analyze publication year distributions

## Confidence
- High confidence (95%+): Structural mimicry claim - directly supported by box plots and random baseline comparison
- Medium confidence (70-95%): Semantic fingerprinting mechanism - 93% GNN accuracy is compelling but exact nature of bias unspecified
- Low confidence (50-70%): Generalizability across domains - results may not transfer to other scientific domains

## Next Checks
1. **Prompt ablation study**: Systematically vary prompts and measure detection accuracy changes
2. **Temporal distribution analysis**: Compare publication year distributions using Kolmogorov-Smirnov tests
3. **Cross-domain replication**: Apply detection pipeline to non-CS domains from different citation databases