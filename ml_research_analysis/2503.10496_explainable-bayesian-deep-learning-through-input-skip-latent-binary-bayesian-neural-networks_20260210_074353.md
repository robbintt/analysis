---
ver: rpa2
title: Explainable Bayesian deep learning through input-skip Latent Binary Bayesian
  Neural Networks
arxiv_id: '2503.10496'
source_url: https://arxiv.org/abs/2503.10496
tags:
- weights
- linear
- network
- neural
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the lack of interpretability and uncertainty
  handling in deep neural networks by introducing input-skip Latent Binary Bayesian
  Neural Networks (ISLaB). ISLaB extends LBBNNs by allowing covariates to skip directly
  to any hidden layer or be excluded entirely, enabling the network to learn optimal
  structures, from linear to nonlinear, while maintaining high predictive accuracy
  and uncertainty measurement.
---

# Explainable Bayesian deep learning through input-skip Latent Binary Bayesian Neural Networks

## Quick Facts
- arXiv ID: 2503.10496
- Source URL: https://arxiv.org/abs/2503.10496
- Reference count: 10
- Primary result: ISLaB achieves 97% accuracy on MNIST using only 935 weights with >99% sparsity

## Executive Summary
This paper introduces input-skip Latent Binary Bayesian Neural Networks (ISLaB), an extension of Latent Binary Bayesian Neural Networks that allows covariates to skip directly to any hidden layer or be excluded entirely. ISLaB addresses the fundamental challenges of interpretability and uncertainty handling in deep neural networks by enabling the network to learn optimal structures ranging from linear to highly nonlinear while maintaining high predictive accuracy. The method achieves remarkable sparsity levels exceeding 99% for small networks and 99.9% for larger ones, while providing built-in global and local explanations through active paths without requiring external post-hoc tools.

The approach combines Bayesian inference with binary weights and latent skip connections, creating a framework that not only makes strong predictions but also quantifies uncertainty and explains its reasoning. ISLaB demonstrates superior calibration and uncertainty handling compared to traditional methods, with theoretical guarantees for local explanations. The method was validated on the MNIST dataset, achieving 97% accuracy while using only 935 weights out of potentially thousands, showcasing both efficiency and interpretability.

## Method Summary
ISLaB extends Latent Binary Bayesian Neural Networks by introducing skip connections that allow input covariates to bypass intermediate layers and connect directly to any hidden layer or the output layer. Each input has a binary latent variable determining whether it skips to a particular layer or is excluded entirely. The model learns the optimal network structure through Bayesian inference, where the posterior distribution over weights and skip connections captures both predictive uncertainty and feature importance. The inference procedure uses a combination of Gibbs sampling and optimization to handle the discrete nature of binary weights and the combinatorial complexity of skip connections.

## Key Results
- Achieved 97% accuracy on MNIST using only 935 weights (99.9% sparsity for larger networks)
- Provides built-in global and local explanations through active paths without external post-hoc tools
- Demonstrates superior uncertainty calibration compared to traditional deep learning methods
- Theoretical guarantees established for local explanation validity

## Why This Works (Mechanism)
The input-skipping mechanism works by introducing binary latent variables that control whether each input feature bypasses intermediate layers and connects directly to any hidden layer. This creates a mixture-of-experts architecture where the network can learn to use features in linear combinations or complex nonlinear transformations depending on what the data requires. The Bayesian framework naturally handles uncertainty by maintaining posterior distributions over both weights and skip connections, while the binary nature of weights and connections enables exact sparsity and clear interpretability through active paths.

## Foundational Learning
- **Bayesian inference with discrete parameters**: Needed to handle the binary nature of weights and skip connections while maintaining uncertainty quantification. Quick check: Verify that the posterior distribution properly captures uncertainty through MCMC samples.
- **Latent variable models**: Required to represent the binary decisions about which skip connections to use. Quick check: Confirm that the latent variables converge during training and produce meaningful sparsity patterns.
- **Mixture-of-experts architectures**: Underlies the ability to combine linear and nonlinear feature usage. Quick check: Test whether the model can recover known linear relationships when present in the data.
- **Sparsity-inducing regularization**: Essential for achieving the high sparsity levels demonstrated. Quick check: Measure the actual number of active weights versus total possible connections.
- **Uncertainty calibration**: Critical for reliable predictions in safety-sensitive applications. Quick check: Evaluate calibration curves on held-out test data.
- **Explainable AI through architecture**: Built-in interpretability eliminates need for post-hoc explanation methods. Quick check: Verify that active paths align with domain knowledge for interpretable features.

## Architecture Onboarding

**Component Map:**
Input features -> Binary skip connection gates -> Hidden layers (with optional direct input connections) -> Output layer

**Critical Path:**
Input features → Skip connection gates (binary latent variables) → Hidden layers → Output prediction

**Design Tradeoffs:**
- Binary weights provide exact sparsity and interpretability but may limit representational power compared to continuous weights
- Skip connections increase model flexibility but add combinatorial complexity to inference
- Bayesian framework enables uncertainty quantification but requires more complex inference procedures

**Failure Signatures:**
- Poor uncertainty calibration indicating issues with posterior approximation
- Unexpected sparsity patterns suggesting problems with the skip connection mechanism
- Degraded accuracy when skip connections are disabled, indicating overreliance on this feature

**First Experiments:**
1. Train ISLaB on a simple linear dataset to verify it can recover linear relationships through skip connections
2. Compare uncertainty calibration between ISLaB and standard BNN on a benchmark dataset
3. Perform ablation study removing skip connections to quantify their contribution to performance

## Open Questions the Paper Calls Out
None

## Limitations
- Computational efficiency for larger networks with many skip connections needs further validation
- Assumption that binary weights adequately capture data distributions may limit applicability
- Performance on diverse real-world datasets beyond standard benchmarks requires broader testing

## Confidence

**High Confidence:**
- Sparsity results and MNIST predictive accuracy are well-supported by experimental data
- Input-skipping mechanism and its contribution to interpretability is clearly explained and validated

**Medium Confidence:**
- Theoretical guarantees for local explanations require more rigorous validation on diverse datasets
- Uncertainty handling improvements need broader empirical support across different problem domains

**Low Confidence:**
- Computational efficiency claims for larger networks are not fully substantiated
- General applicability of binary weights to all deep learning tasks remains unproven

## Next Checks
1. Evaluate ISLaB on diverse datasets including continuous inputs and complex patterns beyond MNIST
2. Conduct ablation studies comparing ISLaB with standard LBBNNs to quantify impact of input-skipping
3. Benchmark computational efficiency of ISLaB on larger networks against other Bayesian and non-Bayesian methods