---
ver: rpa2
title: Resource-Efficient LLM Application for Structured Transformation of Unstructured
  Financial Contracts
arxiv_id: '2510.23990'
source_url: https://arxiv.org/abs/2510.23990
tags:
- cdmizer
- contracts
- framework
- financial
- isda
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends the CDMizer framework for structured conversion
  of unstructured financial contracts into the Common Domain Model (CDM) format. The
  method uses a template-driven approach to ensure syntactic correctness and schema
  adherence when encoding CSA clauses into CDM JSON.
---

# Resource-Efficient LLM Application for Structured Transformation of Unstructured Financial Contracts

## Quick Facts
- arXiv ID: 2510.23990
- Source URL: https://arxiv.org/abs/2510.23990
- Authors: Maruf Ahmed Mridul; Oshani Seneviratne
- Reference count: 10
- Primary result: Qwen3-30B achieves up to 97.88% accuracy on CSA contract clauses using template-driven LLM extraction

## Executive Summary
This paper extends the CDMizer framework to convert unstructured financial contracts into the Common Domain Model (CDM) format using a template-driven approach with the resource-efficient Qwen3-30B model. The method ensures syntactic correctness by generating JSON templates from the CDM schema before LLM processing, then populates these templates using either direct prompting or Retrieval-Augmented Generation (RAG). Evaluated against the ISDA benchmark, the approach achieves competitive accuracy with large proprietary models while offering lower computational cost and better privacy for on-premise deployment.

## Method Summary
The CDMizer framework generates minimal JSON templates by recursively traversing and pruning the CDM schema for target clauses, ensuring 100% syntactic correctness. For each Credit Support Annex (CSA), the system either directly prompts the Qwen3-30B model or retrieves similar examples (excluding the target document) from a knowledge base of 60 ground-truth CSAs, then populates the template slots. This template-driven approach reduces the cognitive load on the LLM by constraining output to pre-validated schema structures.

## Key Results
- Qwen3-30B with RAG achieved 97.88% accuracy on Base/Eligible Currency clauses and 93.39% on Rounding
- Without RAG, Qwen3-30B ranked first on MTA extraction (58.31%) compared to GPT-4o (37%)
- Smaller open-source models can match or exceed large proprietary models on schema-constrained extraction tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Template-driven output generation guarantees syntactic correctness regardless of model size
- Mechanism: CDMizer recursively traverses the full CDM schema, prunes fields not relevant to target clauses, and produces minimal JSON templates before the LLM processes any output
- Core assumption: The CDM schema is stable and well-defined for target clauses
- Evidence anchors: [abstract] "template-driven solution that ensures syntactic correctness and adherence to the CDM schema during contract-to-CDM conversion"; [section 3.1] "This process yielded four unique templates... guarantees that the output JSON will always be compliant with the defined CDM schema, thus achieving 100% syntactical correctness"

### Mechanism 2
- Claim: Leave-one-out RAG provides contextual learning without information leakage
- Mechanism: A knowledge base of 60 ground-truth CSA examples is queried with the target contract excluded
- Core assumption: Similar CSAs share extractable patterns; retrieval quality is sufficient to surface relevant examples
- Evidence anchors: [section 3.2] "when processing a given CSA, we excluded it from the retrieval pool and instead allowed the RAG system to draw contextual examples only from the remaining 59"

### Mechanism 3
- Claim: Smaller open-source models can match larger proprietary models on constrained, schema-bounded tasks
- Mechanism: By narrowing the task to slot-filling within strict templates, the cognitive load on the LLM is reduced
- Core assumption: Task difficulty scales with output freedom; schema-bounded extraction is tractable for smaller models
- Evidence anchors: [section 3.2] "classifying it as a significantly smaller model compared to the proprietary SOTA models used in the ISDA benchmark (e.g., those estimated to be over 100 billion parameters)"

## Foundational Learning

- Concept: **Common Domain Model (CDM)**
  - Why needed here: CDM is the target schema for all outputs. Without understanding its hierarchical JSON structure and field semantics, you cannot validate template correctness or debug extraction failures
  - Quick check question: Can you explain why CDM uses nested structures for party-specific fields like `minimumTransferAmount`?

- Concept: **Credit Support Annex (CSA) clause semantics**
  - Why needed here: The five target clauses (Base/Eligible Currency, Rounding, MTA, Threshold) have distinct linguistic patterns and edge cases
  - Quick check question: Given a CSA excerpt specifying different MTAs for Party A and Party B, how would you map each to the CDM `party` field?

- Concept: **Leave-one-out evaluation methodology**
  - Why needed here: The reported accuracy depends on rigorous exclusion of test samples from the RAG knowledge base
  - Quick check question: If you have 100 contracts and use 80 for RAG, 20 for testing, what specific retrieval constraint must be enforced during inference?

## Architecture Onboarding

- Component map:
  Template Engine -> RAG Retriever -> LLM Inference -> Evaluator

- Critical path:
  1. Define target clause → generate/prune CDM template
  2. Index ground-truth CSAs in vector store
  3. For each test CSA: retrieve k similar examples (excluding self), construct prompt, invoke LLM
  4. Compare output JSON to ground truth → score 0-100

- Design tradeoffs:
  - Template strictness vs. flexibility: Strict templates guarantee syntax but cannot capture novel fields
  - RAG vs. no-RAG: RAG improves complex clauses but adds latency and retrieval infrastructure
  - Model size vs. deployment cost: Smaller models enable on-premise deployment (privacy) but may require more prompt engineering

- Failure signatures:
  - Low MTA/Threshold scores without RAG: Model struggles with party-specific mappings
  - Rounding errors on edge cases: Numeric precision or format mismatches
  - Inconsistent retrieval quality: Irrelevant examples retrieved

- First 3 experiments:
  1. Reproduce baseline: Run CDMizer on 10 CSA samples without RAG; validate syntactic correctness is 100% and compare extraction accuracy to paper's Table 2
  2. Ablate RAG contribution: Same 10 samples with RAG (leave-one-out enforced); quantify per-clause accuracy delta
  3. Clause difficulty probe: Focus on Threshold (lowest no-RAG score); test whether 3-shot prompting with manually curated examples closes the gap vs. retrieved RAG

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the CDMizer framework generalize effectively to other complex legal document types beyond Credit Support Annexes (CSAs) and OTC derivative contracts?
- Basis in paper: [explicit] The authors state in the Conclusion that "Future work could focus on expanding CDMizer to handle a broader range of legal documents, enhancing the framework's generalization capabilities"
- Why unresolved: The current study validates the methodology only on CSA clauses, leaving its applicability to the wider universe of unstructured financial agreements unproven
- What evidence would resolve it: Successful application and benchmarking of the template-driven framework on distinct legal contract classes, such as ISDA Master Agreements or insurance policies

### Open Question 2
- Question: To what extent would domain-specific fine-tuning of the open-source model improve extraction accuracy compared to the current Retrieval-Augmented Generation (RAG) approach?
- Basis in paper: [explicit] The Conclusion suggests "exploring advanced fine-tuning techniques for domain-specific tasks and incorporating additional contextual knowledge could enhance the framework's robustness"
- Why unresolved: The current implementation relies on prompt engineering and RAG with a base model; the incremental value of fine-tuning on a specialized financial corpus remains unquantified
- What evidence would resolve it: A comparative evaluation showing the performance delta of a fine-tuned Qwen3-30B model against the RAG-only baseline on the same ISDA benchmark

### Open Question 3
- Question: How does the performance and retrieval quality of the system scale when the RAG knowledge base is expanded beyond the limited set of 60 ground-truth examples?
- Basis in paper: [inferred] The experimental setup notes the RAG knowledge base "consisted of the 60 ground truth CSA examples"
- Why unresolved: It is unclear if the model's accuracy is sustained when retrieving context from a massive, noisy corpus of thousands of diverse legal documents rather than a curated set of 59 similar examples
- What evidence would resolve it: Experiments measuring accuracy and retrieval latency as the knowledge base size increases by orders of magnitude

## Limitations
- Clause coverage: Framework currently supports only five CSA clauses, limiting generalizability to full contract digitization
- RAG retrieval quality: Performance gains depend on relevance of retrieved examples from limited 60-example knowledge base
- Model size trade-offs: While Qwen3-30B is smaller than proprietary models, it still requires substantial computational resources

## Confidence

- High confidence: Template-driven syntactic correctness (100% validation via schema pruning), leave-one-out RAG methodology, and the core finding that schema-constrained tasks reduce LLM cognitive load
- Medium confidence: Absolute accuracy numbers for individual clauses, as these depend on the specific ISDA benchmark dataset and human annotation quality
- Low confidence: Claims about resource efficiency and privacy benefits, as the paper lacks direct comparisons of deployment costs, inference latency, or security audits

## Next Checks
1. **Cross-clause robustness**: Test CDMizer on additional CSA clauses (e.g., Valuation Agent, Delivery Amount) and non-CSA contract types to assess template scalability and schema adaptation requirements
2. **Retrieval ablation study**: Systematically vary the number of retrieved examples (k=1,3,5,10) and knowledge base size to quantify the relationship between retrieval quality and extraction accuracy for each clause type
3. **Lightweight model comparison**: Benchmark CDMizer with Qwen2.5-7B and Llama-3.1-8B models to identify the minimum viable model size that maintains acceptable accuracy while reducing deployment costs