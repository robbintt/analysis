---
ver: rpa2
title: Robust Bayesian Optimisation with Unbounded Corruptions
arxiv_id: '2511.15315'
source_url: https://arxiv.org/abs/2511.15315
tags:
- regret
- noise
- robust
- function
- plateau
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the vulnerability of Bayesian Optimization
  (BO) to extreme outliers, which can derail standard methods. Existing robust approaches
  assume bounded cumulative corruption budgets, making them ineffective against single
  large outliers.
---

# Robust Bayesian Optimisation with Unbounded Corruptions

## Quick Facts
- arXiv ID: 2511.15315
- Source URL: https://arxiv.org/abs/2511.15315
- Reference count: 40
- One-line primary result: Introduces RCGP-UCB algorithms achieving sublinear regret with unbounded magnitude outliers, maintaining zero-cost robustness when no corruptions exist.

## Executive Summary
This paper addresses a critical vulnerability in Bayesian Optimization: extreme outliers can derail standard methods. Existing robust approaches assume bounded cumulative corruption budgets, making them ineffective against single large outliers. The authors propose a new adversary with a frequency-constrained corruption model that allows for unbounded magnitudes. They introduce RCGP-UCB, an algorithm combining the robust conjugate Gaussian process (RCGP) with the UCB acquisition function. Two variants are presented: FC-RCGP-UCB (fixed centering) and A2-RCGP-UCB (adaptive centering). Both achieve sublinear regret in the presence of up to O(T^1/2) and O(T^1/3) corruptions with possibly infinite magnitude, respectively. Critically, these algorithms maintain the same efficiency as standard GP-UCB when no outliers are present, providing "zero-cost" robustness. Empirical results on synthetic and real-world benchmarks demonstrate their effectiveness against adversarial corruptions while maintaining performance in clean settings.

## Method Summary
The method introduces Robust Conjugate Gaussian Process (RCGP) which modifies standard GP updates through a weight function that bounds the influence of outliers regardless of their magnitude. The P-IMQ weight function assigns weights based on how far observations lie from a centering function g(x), with a plateau region where standard GP updates apply. Two variants are proposed: FC-RCGP-UCB uses a fixed center g=0, while A2-RCGP-UCB employs adaptive centering using dual models (anchor model with fixed center and acquisition model with adaptive center). The UCB acquisition function is modified with a robust multiplier √β_t = √β'_t + C_w√T_c where T_c estimates the number of corruptions. Hyperparameters are tuned via weighted leave-one-out cross-validation rather than marginal likelihood.

## Key Results
- RCGP-UCB algorithms achieve sublinear regret with up to O(T^1/2) corruptions for FC variant and O(T^1/3) for A2 variant, even when corruptions have infinite magnitude
- When no corruptions are present, RCGP-UCB matches the regret bounds of standard GP-UCB, demonstrating zero-cost robustness
- Empirical results on Forrester function, CIFAR-10 hyperparameter tuning, and Lunar-Lander-3 policy optimization show RCGP-UCB outperforms standard GP-UCB under adversarial corruptions while maintaining competitive performance in clean settings

## Why This Works (Mechanism)

### Mechanism 1
RCGP-UCB maintains sublinear regret even when corruptions have infinite magnitude because the adversary is constrained by corruption frequency rather than cumulative magnitude. Observations outside the P-IMQ plateau are down-weighted via a weight function that satisfies Huber-type robustness (sup|y|·w(x,y)² < ∞), bounding outlier influence regardless of magnitude. The core assumption is that the number of corruptions Tc grows at most as O(T^α) with α < 1/2 (for FC variant) or α < 1/3 (for A2 variant).

### Mechanism 2
The P-IMQ weight function enables "zero-cost robustness" by matching standard GP-UCB regret when no corruptions exist. The plateau design (width L) treats all observations with residuals |y - g(x)| ≤ L exactly as standard GP (Jw = I, mw = 0). The plateau condition ensures uncorrupted observations fall within this region with high probability, so RCGP updates collapse to GP updates. The core assumption is that the plateau width L is set sufficiently large (L_T = B_f√κ + N_T(δ/2)) to contain all uncorrupted observations under the high-probability noise bound.

### Mechanism 3
A2-RCGP-UCB achieves adaptive centering while preserving stability against adversarial manipulation through a dual-model architecture. The anchor model M_A uses fixed center g=0 (stable but conservative), while the acquisition model M_R uses adaptive center g(x) = μ_A,t-1(x). M_A provides certified bounds that define M_R's plateau width, preventing corruptions from destabilizing the adaptive center. The core assumption is that corruptions cannot simultaneously corrupt enough observations to violate the anchor model's plateau condition at rate Tc = o(T^1/3).

## Foundational Learning

- **Gaussian Process posterior updates:** Why needed here - RCGP modifies standard GP updates via J_w and m_w terms; understanding μ_t(x) = k_t(x)ᵀ(K_t + σ²I)⁻¹y_t is prerequisite. Quick check question: Given data D_t = {(x_i, y_i)}, can you write the posterior mean formula?

- **UCB acquisition function and regret analysis:** Why needed here - The robust multiplier √β_t = √β'_t + C_w√T_c replaces standard GP-UCB's confidence parameter. Quick check question: How does the UCB acquisition function balance exploration vs. exploitation?

- **Robust statistics (influence functions, Huber contamination):** Why needed here - The weight function's bounded influence (sup|y|·w² < ∞) is the formal robustness property. Quick check question: Why does bounded influence protect against infinite-magnitude outliers?

## Architecture Onboarding

- **Component map:** RCGP posterior (with J_w, m_w) → robust UCB (√β_t) → query x_t; A2 variant adds [Anchor M_A] → provides center/bounds → [Acquisition M_R] → UCB → query x_t

- **Critical path:**
  1. Implement P-IMQ weight function with configurable L, c, g(x)
  2. Modify GP update to incorporate J_w = diag(σ²/(2w²_i)) and m_w terms
  3. Set plateau width L_T = B_f√κ + N_T(δ/2) (or use heuristic: 95th quantile of |y - median|)
  4. Estimate T_c as count of observations outside plateau (or conservatively set T_c = 0)

- **Design tradeoffs:**
  - FC-RCGP-UCB: Stronger theoretical guarantees (Tc = o(T^1/2)), but conservative with fixed g=0
  - A2-RCGP-UCB: Better practical adaptation, weaker regret bound (Tc = o(T^1/3)), dual-model overhead
  - Standard GP-UCB baseline: No robustness but simplest implementation

- **Failure signatures:**
  - Plateau width L too narrow: uncorrupted points down-weighted → efficiency loss
  - L too wide: outliers enter plateau → robustness compromised
  - T_c severely underestimated: √β_t too small → overconfidence, regret spikes
  - Assumption: Numerical instability if weight values w_i approach zero (mitigated by plateau)

- **First 3 experiments:**
  1. **Uncorrupted Forrester function** (validate zero-cost robustness): Run FC-RCGP-UCB vs. GP-UCB for 30 iterations; cumulative regret should match within noise
  2. **Single infinite outlier** (validate robustness): Inject one corruption with magnitude 10⁶ at iteration 5; FC-RCGP-UCB should maintain sublinear regret, GP-UCB should diverge
  3. **Adversarial corruption at T^1/3 frequency** (stress test A2 variant): Implement greedy adversary corrupting near-optimal queries; compare FC vs. A2 regret trajectories

## Open Questions the Paper Calls Out

- **Can the heuristic DiagnosticsGP algorithm be furnished with provable regret bounds using the mathematical tools developed in this paper?** The authors state this is a promising future direction, noting that DiagnosticsGP currently operates as a heuristic method that performs well empirically but lacks the theoretical sublinear regret guarantees established for RCGP-UCB.

- **Can the regret bound for the adaptive A2-RCGP-UCB algorithm be tightened to handle O(T^1/2) corruptions?** The authors note that A2-RCGP-UCB has a weaker theoretical bound (O(T^1/3) corruptions) compared to FC-RCGP-UCB (O(T^1/2)), but suggest "this is likely to be a limitation of the proof technique" rather than the algorithm itself.

- **How can acquisition functions be specifically designed to better complement robust probabilistic models like RCGP?** The authors identify this as a promising future direction, noting that the current work modifies standard UCB and leaves the potential optimization of acquisition functions tailored to the specific uncertainty quantification of robust posteriors unexplored.

## Limitations

- The theoretical guarantees rely on precise settings of P-IMQ parameters (L, c) and corruption frequency bounds, with adaptive variants' performance depending heavily on heuristic choices that may not generalize
- The dual-model A2-RCGP-UCB introduces computational overhead and complexity with unclear empirical benefits over the simpler FC variant in all settings
- The connection between adversarial corruptions and real-world noise patterns (e.g., sensor failures, label noise) is not rigorously established, though the method shows promise in controlled experiments

## Confidence

- **High confidence** in the core theoretical framework: The sublinear regret bounds under frequency-constrained corruptions are mathematically sound and follow from the robust weight function's bounded influence property
- **Medium confidence** in practical efficacy: Empirical results demonstrate robustness in synthetic and real-world benchmarks, but the sample size and diversity of test functions could be expanded
- **Medium confidence** in "zero-cost robustness": The claim that RCGP-UCB matches GP-UCB's performance in clean settings is supported by theory and experiments, but edge cases (e.g., tight plateaus) may reveal efficiency losses

## Next Checks

1. **Robustness to mixed corruption types**: Test RCGP-UCB against both adversarial corruptions and natural noise (e.g., Gaussian, heavy-tailed) to assess generalization beyond the adversarial model

2. **Scalability to high dimensions**: Evaluate performance on higher-dimensional benchmarks (e.g., 50-100D) to verify if the O(T^1/2) or O(T^1/3) corruption bounds remain practical

3. **Hyperparameter sensitivity analysis**: Systematically vary L and c in P-IMQ to quantify their impact on regret and robustness, providing clearer guidance for practitioners