---
ver: rpa2
title: 'Exploring the Landscape of Text-to-SQL with Large Language Models: Progresses,
  Challenges and Opportunities'
arxiv_id: '2505.23838'
source_url: https://arxiv.org/abs/2505.23838
tags:
- text-to-sql
- language
- arxiv
- data
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a systematic survey of LLM-based text-to-SQL
  approaches, analyzing research trends, methodologies, datasets, and challenges.
  The survey reviewed 122 articles from April 2022 to October 2024, identifying key
  research paradigms including pre-processing (schema linking, cell value acquisition),
  in-context learning (prompt engineering, example selection, reasoning enhancement),
  fine-tuning (model architectures, training strategies), and post-processing (SQL
  correction, output consistency).
---

# Exploring the Landscape of Text-to-SQL with Large Language Models: Progresses, Challenges and Opportunities

## Quick Facts
- **arXiv ID:** 2505.23838
- **Source URL:** https://arxiv.org/abs/2505.23838
- **Reference count:** 40
- **Primary result:** Systematic survey of 122 LLM-based text-to-SQL approaches from April 2022 to October 2024, identifying key research paradigms, challenges, and future directions.

## Executive Summary
This paper provides a comprehensive systematic literature review of LLM-based text-to-SQL approaches, analyzing research trends, methodologies, datasets, and challenges. The survey reviewed 122 articles from April 2022 to October 2024, identifying key research paradigms including pre-processing (schema linking, cell value acquisition), in-context learning (prompt engineering, example selection, reasoning enhancement), fine-tuning (model architectures, training strategies), and post-processing (SQL correction, output consistency). The study found that schema linking and few-shot prompting are most prevalent, with EMNLP and ACL being the primary publication venues. Key challenges include structural complexity, schema comprehension, cross-domain generalization, and model efficiency. The survey provides valuable insights for researchers and practitioners working on natural language interfaces to databases, highlighting both current capabilities and future research directions in this rapidly evolving field.

## Method Summary
The authors conducted a systematic literature review using 5 digital libraries (Web of Science, ScienceDirect, ACM, IEEEXplore, Google Scholar) with keywords including "Text-to-SQL", "Text2SQL", "Natural Language-to-SQL", "NL2SQL", and "SQL Generation" within the timeframe of April 2022 to October 2024. They applied inclusion/exclusion criteria (English language, LLM-focused, 4+ pages, excluding non-peer-reviewed versions) and manually filtered results to identify 122 relevant studies. The papers were then categorized into 4 research paradigms (Pre-processing, In-Context Learning, Fine-Tuning, Post-processing) with 12 sub-categories to analyze trends, methodologies, and challenges in the field.

## Key Results
- Schema linking and few-shot prompting are the most prevalent research approaches in LLM-based text-to-SQL
- EMNLP and ACL are the primary publication venues for text-to-SQL research
- Key challenges include structural complexity, schema comprehension, cross-domain generalization, and model efficiency
- The field shows rapid evolution with diverse methodologies and increasing focus on cross-domain generalization

## Why This Works (Mechanism)

### Mechanism 1: Schema Linking as Context Filtering
If schema linking effectively isolates relevant database elements (tables/columns) from the full database structure, LLM performance may improve by reducing input noise and token load. By mapping keywords in the Natural Language (NL) query to specific schema subsets, the system narrows the "search space" for the LLM, preventing it from attending to irrelevant columns that might cause hallucinations or logical errors. This assumes the LLM has a finite effective context window and attention mechanism, where flooding it with an entire complex database schema degrades retrieval accuracy.

### Mechanism 2: Task Decomposition for Reasoning Offloading
If a complex SQL generation task is decomposed into intermediate steps (e.g., separate classification, linking, and generation steps), the LLM may achieve higher logical consistency than end-to-end generation. Instead of generating a high-complexity SQL query in a single forward pass, the system breaks the problem into a "Chain-of-Thought," allowing the model to allocate inference capacity to specific sub-problems (e.g., identifying join keys before writing WHERE clauses), reducing cognitive load. This assumes complex reasoning is an emergent property of iterative processing rather than single-shot mapping.

### Mechanism 3: Execution-Guided Feedback Loops
If the system executes candidate SQL queries and feeds error messages or empty results back to the LLM, the model can correct syntax or logic errors that static generation misses. This "post-processing" paradigm treats the LLM as a debugger. When an execution error occurs, the error trace is appended to the prompt, allowing the LLM to perform "self-correction" by comparing the intended query logic against the structural constraints of the database. This assumes the LLM possesses sufficient coding knowledge to interpret database error messages (e.g., "column does not exist") and map them back to the original NL intent.

## Foundational Learning

- **Concept: Schema Linking vs. Semantic Parsing**
  - **Why needed here:** The paper distinguishes between general parsing and the specific challenge of mapping NL terms to rigid database identifiers (tables/columns).
  - **Quick check question:** Can you explain why a generic LLM might struggle to distinguish between a table named "Account" and a column named "Account" without explicit schema context?

- **Concept: Cross-Domain Generalization**
  - **Why needed here:** A central theme of the survey is moving from "toy" datasets (single-domain) to "Spider/BIRD" (cross-domain), where the model must handle unseen schemas.
  - **Quick check question:** Why does training on one specific database schema often fail when the model is applied to a database with different naming conventions (e.g., "Customer" vs "Client")?

- **Concept: Execution Accuracy vs. Exact Match**
  - **Why needed here:** The survey highlights that different evaluation metrics judge success differently.
  - **Quick check question:** If an LLM generates a SQL query that is syntactically different from the ground truth (e.g., uses a JOIN instead of a subquery) but retrieves the exact same data table, is it "correct" under Execution Accuracy?

## Architecture Onboarding

- **Component map:** User Query + Full Database Schema -> Pre-Processor (Schema Linking module; Question Rewriter) -> Generator (Core: ICL-based Prompt Engineer OR Fine-Tuned Model) -> Post-Processor (Execution Engine; Critic/Refiner) -> Final SQL Query

- **Critical path:** The flow from Pre-processing (Schema Linking) to ICL (Prompting) determines the bulk of the system's accuracy. If the Schema Linking module filters out the correct column (a "false negative" in retrieval), the downstream Generator cannot recover the logic.

- **Design tradeoffs:**
  - ICL vs. FT: ICL (using proprietary APIs like GPT-4) offers ease of deployment but risks data privacy (sending schema to APIs). FT (training open-source models) ensures privacy but requires significant compute for training and data augmentation.
  - Accuracy vs. Efficiency: Multi-step decomposition and self-correction improve accuracy (State-of-the-Art results) but drastically increase latency (multiple LLM calls per query), making real-time applications difficult.

- **Failure signatures:**
  - Hallucination: The model selects columns that do not exist in the schema (often due to poor schema linking or overwhelming context).
  - Join Explosion: The model generates a query with cartesian joins that times out the database execution because it misunderstood table relationships.
  - Privacy Leak: Sending sensitive row-level data to a public LLM API during "Cell Value Acquisition" (Page 12).

- **First 3 experiments:**
  1. Baseline Zero-Shot: Prompt a strong LLM (e.g., GPT-4) with the DDL (schema definition) and the user question to establish a baseline Execution Accuracy (EX).
  2. Schema Pruning (Pre-processing): Implement a basic similarity search (e.g., embeddings) to select only top-k relevant tables/columns, and measure if accuracy improves/degrades compared to the full schema.
  3. Self-Correction Loop (Post-processing): Append a step where, if the SQL execution fails, the error message is fed back to the LLM with a "Fix this SQL" prompt. Compare the success rate before and after this loop.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can systems optimally select a subset of database schema elements to fit within context windows without compromising SQL generation accuracy?
- **Basis in paper:** [explicit] The authors explicitly ask "how to select an optimal subset of database elements without compromising SQL accuracy and efficiency is worth investigating" in Section 7.2 (Direction 2).
- **Why unresolved:** Current schema linking is computationally expensive for large databases, and simply truncating schemas risks omitting necessary information.
- **What evidence would resolve it:** A retrieval mechanism that maintains high Execution Accuracy (EX) on large-scale schemas while significantly reducing token count and latency compared to full-context methods.

### Open Question 2
- **Question:** Can question rewriting techniques be refined to resolve linguistic ambiguity while strictly preserving the original user intent?
- **Basis in paper:** [explicit] Section 7.2 (Direction 1) lists "Preserving User Intent" as a critical challenge, noting that rewriting risks altering meaning and generating incorrect SQL.
- **Why unresolved:** Ambiguity resolution often requires reformulation, but LLMs may hallucinate or shift the query focus during this rewriting step, failing to capture domain-specific nuances.
- **What evidence would resolve it:** A rewriting model that demonstrates high semantic similarity scores between original and rewritten questions alongside improved SQL accuracy on ambiguous benchmarks like AmbiQT.

### Open Question 3
- **Question:** How can the inference latency of LLM-based text-to-SQL systems be reduced to match the efficiency of pre-trained language models (PLMs)?
- **Basis in paper:** [explicit] Section 7.1 (Challenge 4) notes that "LLM-based methods generally exhibit slower inference speeds compared to pre-trained language model (PLM)-based methods" due to redundant processing and architecture.
- **Why unresolved:** Current LLM workflows often process entire schemas sequentially, creating bottlenecks not present in specialized, smaller PLM architectures.
- **What evidence would resolve it:** Comparative benchmarks showing an LLM-based system achieving comparable execution times to PLM baselines while maintaining superior cross-domain generalization.

## Limitations
- Potential publication bias toward English-language venues and exclusion of non-peer-reviewed preprints
- The rapidly evolving field may have progressed beyond the October 2024 survey timeframe
- Some methodological distinctions between research paradigms may be subjective

## Confidence
- **High confidence:** Methodological rigor of systematic literature review and taxonomy construction
- **Medium confidence:** Exact result counts due to potential database access variations and publication timeline updates
- **Medium confidence:** The four research paradigm categorization provides useful framework, but some distinctions may be subjective

## Next Checks
1. Verify exact Boolean logic used to combine the 5 keywords across the 5 digital libraries
2. Cross-reference reference list (Tables 4-7) to confirm paper categorization aligns with taxonomy definitions
3. Execute updated database queries to check for result count mismatches due to new publications since October 2024