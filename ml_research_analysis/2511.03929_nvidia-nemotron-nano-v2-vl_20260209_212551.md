---
ver: rpa2
title: NVIDIA Nemotron Nano V2 VL
arxiv_id: '2511.03929'
source_url: https://arxiv.org/abs/2511.03929
tags:
- arxiv
- wang
- zhang
- urlhttps
- chen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NVIDIA Nemotron Nano V2 VL is a 12B parameter vision-language model
  built on a hybrid Mamba-Transformer LLM. It introduces major improvements in architecture,
  datasets, and training to deliver leading performance on OCRBench v2 and strong
  results in document understanding, long video comprehension, and reasoning tasks.
---

# NVIDIA Nemotron Nano V2 VL

## Quick Facts
- arXiv ID: 2511.03929
- Source URL: https://arxiv.org/abs/2511.03929
- Reference count: 28
- Key outcome: 12B parameter hybrid Mamba-Transformer VLM achieving 62.0/44.2 on OCRBench v2 (EN/ZH) and 55.3 on MMMU val

## Executive Summary
NVIDIA Nemotron Nano V2 VL is a 12B parameter vision-language model built on a hybrid Mamba-Transformer LLM. It introduces major improvements in architecture, datasets, and training to deliver leading performance on OCRBench v2 and strong results in document understanding, long video comprehension, and reasoning tasks. The model extends context from 16K to 128K tokens and uses efficient video sampling and tiling for better throughput. It achieves state-of-the-art results on multimodal and text benchmarks, including 62.0/44.2 on OCRBench v2 (EN/ZH) and 55.3 on MMMU val. Model weights are released in BF16, FP8, and FP4 formats with large parts of training data and code open-sourced.

## Method Summary
Nemotron Nano V2 VL extends the Nemotron-Nano-12B-V2 LLM with a hybrid Mamba-Transformer architecture and c-RADIOv2-VLM-H vision encoder. The model uses a 5-stage training pipeline with FP8 precision, starting with vision-language projection alignment and progressing through multimodal, video, code reasoning recovery, and long-context fine-tuning stages. Dynamic tiling with pixel shuffle reduces visual tokens from 1024 to 256 per tile, while Efficient Video Sampling prunes temporally static patches. The model achieves 35% higher throughput for long documents and maintains strong text reasoning capabilities through targeted recovery stages.

## Key Results
- Achieves 62.0/44.2 on OCRBench v2 (EN/ZH) with dynamic tiling
- Maintains strong text reasoning: LiveCodeBench 69.8, RULER 72.1 after recovery stages
- 35% higher throughput for long multi-page document understanding vs baseline
- State-of-the-art on MMMU val (55.3) and competitive on MathVista and DocVQA

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The hybrid Mamba-Transformer architecture enables efficient long-context processing while maintaining reasoning capability.
- **Mechanism:** Mamba layers provide linear-time sequence processing for long contexts, while Transformer layers preserve complex reasoning patterns. The architecture balances efficiency (via Mamba's state-space model) with expressiveness (via self-attention).
- **Core assumption:** The proportion and placement of Mamba vs. Transformer layers is optimized for the target workload distribution (documents, video, reasoning).
- **Evidence anchors:**
  - [abstract] "hybrid Mamba-Transformer LLM"
  - [section 1] "35% higher throughput in long multi-page document understanding scenarios"
  - [corpus] Nemotron Nano 2 paper describes hybrid architecture for reasoning workloads
- **Break condition:** Tasks requiring global attention patterns that Mamba cannot capture may degrade performance; throughput gains diminish for short-context queries.

### Mechanism 2
- **Claim:** Multi-stage training with specialized recovery stages preserves text reasoning while adding multimodal capabilities.
- **Mechanism:** Stage 0 aligns vision-language projections while freezing the LLM. Subsequent stages progressively unfreeze, extend context, and recover degraded capabilities. The code-focused Stage 3 specifically restores text reasoning that was lost during multimodal training.
- **Core assumption:** Text reasoning data from the base LLM's training, combined with targeted recovery stages, is sufficient to maintain baseline text performance.
- **Evidence anchors:**
  - [section 3] Complete 5-stage training pipeline detailed
  - [section 4.2, Table 6] LiveCodeBench drops from 70.0 → 50.9 after Stage 1, recovers to 69.8 after Stage 3; RULER recovers from 8.8 → 72.1
  - [corpus] Corpus lacks comparative multi-stage VLM training evidence
- **Break condition:** Skipping Stage 3 or insufficient code data will leave text reasoning degraded; over-training in later stages may overfit to long-context distributions.

### Mechanism 3
- **Claim:** Dynamic tiling with pixel shuffle and Efficient Video Sampling (EVS) reduce token count while preserving task-relevant information.
- **Mechanism:** Images are divided into 512×512 tiles (max 12 tiles); pixel shuffle downsamples 1024→256 tokens per tile. EVS prunes temporally static patches across video frames, reducing redundancy without retraining.
- **Core assumption:** Local visual features can be processed independently per tile; static video regions contribute minimally to understanding.
- **Evidence anchors:**
  - [section 2] "256 visual tokens per tile" after 2× pixel shuffle
  - [section 4.4, Figure 4] EVS at 75% maintains 62.5/66.1 accuracy with 88 tok/s throughput vs 34 tok/s baseline
  - [section 4.5, Table 7] Tiling vs native resolution shows comparable average (75.0 vs 74.8) but OCRBench gap
- **Break condition:** Tasks requiring precise spatial relationships across tiles (e.g., document layout reasoning) or fine-grained temporal dynamics may suffer from aggressive reduction.

## Foundational Learning

- **Concept: Hybrid Mamba-Transformer Architecture**
  - **Why needed here:** Understanding which operations are handled by Mamba (efficient sequence modeling) vs. Transformer (complex reasoning) clarifies throughput/accuracy tradeoffs.
  - **Quick check question:** For a 100K-token document, which component dominates inference time—the Mamba layers or the Transformer layers?

- **Concept: Vision-Language Alignment Training**
  - **Why needed here:** Stage 0's frozen-backbone projection training is the foundation; skipping it causes instability in later stages.
  - **Quick check question:** Why is the MLP connector trained separately before unfreezing the LLM?

- **Concept: Token Reduction for Long-Context Efficiency**
  - **Why needed here:** Pixel shuffle and EVS directly determine throughput; misconfiguration leads to OOM or accuracy collapse.
  - **Quick check question:** If EVS prunes 90% of tokens, what types of video tasks would most likely fail?

## Architecture Onboarding

- **Component map:**
  Input: Images → dynamic tiling (≤12 tiles, 512×512 each) | Videos → uniform frame sampling (2 fps, max 128 frames)
  Vision: RADIOv2.5 encoder (c-RADIOv2-VLM-H, patch size 16)
  Projection: 2× pixel shuffle → 256 tokens/tile → MLP connector
  LLM: Nemotron-Nano-12B-V2 (hybrid Mamba-Transformer, 128K context)
  Output: Interleaved image-text embeddings → LLM → response

- **Critical path:**
  1. Image/video preprocessing (tiling, frame extraction)
  2. Vision encoding → token reduction
  3. Embedding interleaving with text
  4. Hybrid LLM inference (context parallelism for >49K tokens)

- **Design tradeoffs:**
  - Tiling vs. native resolution: Tiling preserves OCR accuracy but adds complexity; native resolution simpler but OCRBench-V2 (EN) drops 61.4→57.6
  - EVS ratio: Higher ratio (90%) maximizes throughput (132 tok/s) but accuracy drops (60.7/64.0); 75% is practical default
  - Quantization: FP8-PTQ matches BF16; NVFP4-PTQ degrades slightly, requires QAD for recovery

- **Failure signatures:**
  - LiveCodeBench <55 after multimodal training → missing Stage 3 code recovery
  - RULER <20 after Stage 1 → insufficient long-context data; requires Stage 4
  - OCRBench-V2 (EN) low with native resolution → tiling-size matching needed
  - Video accuracy drops >2 points → EVS too aggressive or frame sampling insufficient

- **First 3 experiments:**
  1. **Ablate Stage 3:** Train through Stage 2 only, measure LiveCodeBench and RULER degradation to quantify code recovery necessity.
  2. **EVS sweep on target workload:** Test EVS ratios (50%, 70%, 90%) on your specific video benchmarks to find throughput/accuracy frontier.
  3. **Tiling configuration test:** Compare max_tiles={6, 12, 18} on document-heavy tasks (DocVQA, ChartQA) to calibrate resolution vs. context length tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the performance gap on OCRBench-V2 (English) be closed when using native resolution inputs combined with tiling-size matching, compared to the standard dynamic tiling strategy?
- Basis in paper: [explicit] The authors state in Section 4.5 regarding the native resolution ablation: "The gap on OCRBench-V2 (English), however, persists. We plan to further investigate strategies to address this gap in future work."
- Why unresolved: While resizing images to match tiling dimensions recovered OCRBench performance, it failed to recover the specific drop observed in OCRBench-V2 (English), indicating a specific sensitivity in that benchmark to the tiling algorithm or tokenization method not yet understood.
- What evidence would resolve it: A successful modification to the native resolution pipeline or token reduction technique that achieves statistical parity with the tiling strategy on OCRBench-V2 (English).

### Open Question 2
- Question: Is it possible to prevent the degradation of text reasoning capabilities (specifically coding) during multimodal SFT natively, rather than recovering it via a separate post-hoc "healing" stage?
- Basis in paper: [inferred] Section 3.4 and Section 4.2 detail a "substantial drop" in LiveCodeBench scores after initial multimodal stages, which required a dedicated SFT Stage 3 using only code data to restore.
- Why unresolved: The paper demonstrates that recovery is possible but does not identify if the initial catastrophic forgetting of code capabilities is an unavoidable trade-off of the multimodal alignment or a solvable optimization issue in the joint training distribution.
- What evidence would resolve it: A single-stage training recipe that maintains LiveCodeBench performance equivalent to the LLM backbone without requiring a subsequent code-recovery stage.

### Open Question 3
- Question: Why does unrestricted "reasoning-on" mode frequently underperform compared to a constrained reasoning budget (e.g., 2K–8K tokens), and can this be mitigated without external hard limits?
- Basis in paper: [inferred] Section 4.3 notes that reasoning budget control often improves accuracy "even in cases where the unrestricted reasoning-on score is lower than the reasoning-off score."
- Why unresolved: The paper suggests this is due to "malformed reasoning traces with repetition loops" or verbosity, but it is unclear if the model inherently lacks the ability to self-terminate reasoning effectively when "thinking."
- What evidence would resolve it: The implementation of an intrinsic stopping criterion or confidence threshold that allows the model to exit reasoning-on mode at an optimal step without a predefined token limit.

## Limitations

- Performance gap on OCRBench-V2 (English) persists when using native resolution inputs despite tiling-size matching
- Text reasoning capabilities degrade during multimodal training and require separate recovery stage
- FP4 quantization degrades accuracy (62.0→60.7 on OCRBench v2) and may be prohibitive for precision-critical applications

## Confidence

**High Confidence:** Architecture design (hybrid Mamba-Transformer, 128K context extension), benchmark results on standard datasets (OCRBench v2, MMMU, MathVista), and the five-stage training pipeline structure. These are directly measurable and reproducible with the provided model weights and code.

**Medium Confidence:** The necessity and sufficiency of each training stage, particularly Stage 3's code recovery and Stage 4's long-context fine-tuning. While results show improvements, the causal relationship between specific data distributions and capability preservation needs more ablation studies.

**Low Confidence:** Performance generalization to unseen document types and video content, especially where layout complexity or temporal dynamics deviate significantly from training distributions. The EVS and tiling effectiveness in these scenarios remains largely theoretical.

## Next Checks

1. **Stage Ablation Study:** Train Nemotron Nano V2 VL while skipping Stage 3 (code reasoning recovery) to measure the exact degradation in LiveCodeBench and RULER scores. This quantifies whether the 19.9-point RULER recovery and 18.9-point LiveCodeBench recovery are reproducible and necessary for production use cases.

2. **EVS Ratio Sweep on Target Workloads:** Apply EVS at 50%, 70%, and 90% ratios to your specific video datasets and measure throughput/accuracy tradeoffs. The paper's 75% default may not optimize for all video types—especially those with high motion content where static pruning removes critical information.

3. **Tiling Configuration Stress Test:** Systematically test max_tiles configurations (6, 12, 18) on document-heavy datasets including DocVQA and ChartQA. Measure whether increasing tile count improves OCR accuracy on complex layouts or simply increases computational cost without proportional benefit.