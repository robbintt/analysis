---
ver: rpa2
title: 'Fisher meets Feynman: score-based variational inference with a product of
  experts'
arxiv_id: '2510.21598'
source_url: https://arxiv.org/abs/2510.21598
tags:
- experts
- variational
- target
- learning
- normalizing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new family of variational distributions
  based on products of multivariate t-distributions for score-based black-box variational
  inference (BBVI). The key innovation is using the Feynman identity to reformulate
  products of experts as latent variable models with Dirichlet variables, enabling
  efficient sampling and normalizing constant estimation.
---

# Fisher meets Feynman: score-based variational inference with a product of experts

## Quick Facts
- arXiv ID: 2510.21598
- Source URL: https://arxiv.org/abs/2510.21598
- Reference count: 40
- Introduces score-based variational inference using products of multivariate t-distributions via Feynman's identity

## Executive Summary
This paper proposes a new variational inference approach that uses products of multivariate t-distributions as the approximating family. By leveraging Feynman's identity, the authors reformulate products of experts (PoEs) as latent variable models with Dirichlet variables, enabling efficient sampling and tractable normalizing constant estimation. The method iteratively optimizes expert weights via convex quadratic programs to minimize Fisher divergence, achieving exponential convergence guarantees. Empirical results demonstrate superior performance compared to Gaussian BBVI and normalizing flows on targets with skew, heavy tails, and multimodality.

## Method Summary
The method introduces a variational family based on products of multivariate t-distributions, where each expert captures different aspects of the target distribution. Feynman's identity is used to represent the product of experts as an equivalent latent variable model with Dirichlet-distributed latent variables, making sampling and normalization tractable. The optimization proceeds by iteratively updating expert weights through convex quadratic programs that minimize Fisher divergence between the PoE and target. This iterative scheme converges exponentially under certain conditions, with each iteration requiring only quadratic programming to solve for optimal expert weights.

## Key Results
- Exponential convergence of the iterative optimization algorithm for weight updates
- Superior performance to Gaussian BBVI and normalizing flows on synthetic and real-world targets with skew, heavy tails, and multiple modes
- Achieves lower KL and Fisher divergences while maintaining computational tractability
- Demonstrates high expressiveness through products of experts while enabling efficient sampling via the latent variable reformulation

## Why This Works (Mechanism)
The method works by combining the expressiveness of products of experts with the computational tractability achieved through Feynman's identity. Each expert captures a different mode or feature of the target distribution, and their product can represent complex, multimodal distributions. The key insight is that Feynman's identity transforms this product into a latent variable model where sampling becomes straightforward through Dirichlet variables. The iterative weight optimization ensures that the final approximation closely matches the target in terms of Fisher divergence, with each iteration improving the fit through a convex optimization problem.

## Foundational Learning

1. **Feynman's identity and its application to products of experts**
   - Why needed: Enables transformation of intractable products of experts into tractable latent variable models
   - Quick check: Verify that the latent variable representation correctly recovers the original product when marginalizing

2. **Score-based variational inference framework**
   - Why needed: Provides the theoretical foundation for minimizing Fisher divergence without requiring explicit density evaluation
   - Quick check: Confirm that score matching objectives correctly approximate the Fisher divergence

3. **Convex quadratic programming for weight optimization**
   - Why needed: Ensures tractable, globally optimal updates to expert weights at each iteration
   - Quick check: Verify that the QP formulation correctly represents the Fisher divergence minimization

## Architecture Onboarding

**Component map:**
Data -> Target Score Function -> Iterative Weight Updates -> Expert Weight Vector -> Product of Experts -> Approximate Posterior

**Critical path:**
1. Compute target score function at current proposal
2. Form quadratic program for weight updates
3. Solve QP to obtain new expert weights
4. Evaluate Fisher divergence improvement
5. Repeat until convergence

**Design tradeoffs:**
- **Expert selection vs. computational cost**: More experts increase expressiveness but quadratic scaling in covariance computations
- **t-distribution degrees of freedom vs. tail behavior**: Lower degrees of freedom capture heavier tails but may reduce optimization stability
- **Iterative vs. simultaneous updates**: Iterative updates ensure convergence but may require more iterations than joint optimization

**Failure signatures:**
- Slow or no convergence indicates poor expert initialization or incompatible target distribution
- Numerical instability in QP suggests ill-conditioned covariance matrices or extreme target features
- Divergence in weights indicates target distribution outside the expressive power of the PoE family

**3 first experiments:**
1. Verify exponential convergence on a simple bimodal Gaussian mixture
2. Test sensitivity to expert initialization on a heavy-tailed target
3. Compare Fisher divergence reduction per iteration against competing methods

## Open Questions the Paper Calls Out
The paper identifies several open challenges including automated expert selection strategies, scalability to very high-dimensional problems, and robustness to poor initialization conditions. The authors note that while the theoretical framework provides strong guarantees, practical implementation requires careful heuristic choices that remain to be systematized.

## Limitations

- Computational complexity scales quadratically with the number of experts due to pairwise covariance computations
- Performance on very high-dimensional problems (>1000 dimensions) remains untested and potentially problematic
- Expert selection remains heuristic, with no systematic approach provided for automated initialization

## Confidence

**High confidence**: Theoretical formulation using Feynman's identity, convergence properties of the optimization algorithm, empirical superiority over Gaussian BBVI on tested problems

**Medium confidence**: Practical effectiveness of heuristic expert selection, computational efficiency claims for moderate-dimensional problems, general applicability across diverse target distributions

**Low confidence**: Scalability to very high-dimensional problems, robustness to poor expert initialization, performance on extremely complex multimodal distributions

## Next Checks

1. **Scalability Testing**: Evaluate performance on high-dimensional problems (d > 1000) with sparse target distributions to assess computational tractability and convergence properties.

2. **Expert Selection Analysis**: Conduct systematic studies on the impact of different expert selection strategies, including automated approaches versus manual heuristics, to quantify sensitivity to initialization.

3. **Convergence Robustness**: Test algorithm behavior under challenging conditions such as poor initialization, near-degenerate covariance matrices, and targets with very heavy tails or extreme multimodality.