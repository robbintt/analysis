---
ver: rpa2
title: 'T-FIX: Text-Based Explanations with Features Interpretable to eXperts'
arxiv_id: '2511.04070'
source_url: https://arxiv.org/abs/2511.04070
tags:
- expert
- alignment
- criteria
- explanation
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces T-FIX, a benchmark for evaluating LLM explanations
  based on their alignment with expert-level reasoning across seven knowledge-intensive
  domains. The authors develop a pipeline that decomposes LLM explanations into atomic
  claims, filters irrelevant content, and scores each claim against domain-specific
  expert criteria.
---

# T-FIX: Text-Based Explanations with Features Interpretable to eXperts

## Quick Facts
- arXiv ID: 2511.04070
- Source URL: https://arxiv.org/abs/2511.04070
- Reference count: 40
- High-performing models don't necessarily reason like domain experts

## Executive Summary
T-FIX is a benchmark for evaluating whether LLM-generated explanations align with expert-level reasoning across seven knowledge-intensive domains. The benchmark decomposes explanations into atomic claims, filters irrelevant content, and scores each claim against domain-specific expert criteria. Validation with human annotators shows high accuracy and moderate inter-annotator agreement. When evaluated across four LLMs and four prompting strategies, models show weak correlation between task accuracy and expert alignment, indicating that high-performing models do not necessarily reason like domain experts.

## Method Summary
T-FIX introduces a three-stage GPT-4o pipeline to evaluate expert alignment of LLM explanations. First, explanations are decomposed into atomic claims that can be assessed independently. Second, irrelevant or unsupported claims are filtered out using a separate LLM. Third, remaining claims are scored against domain-specific expert criteria (complete=1, partial=0.5, none=0). The benchmark includes seven domains: Mass Maps, Supernova, Politeness, Emotion, Cholecystectomy, Cardiac, and Sepsis. Each domain has 100 examples with expert-defined alignment criteria. Four prompting strategies (Vanilla, Chain-of-Thought, Socratic, Subquestion Decomposition) are evaluated across multiple LLMs to assess how prompting affects expert alignment.

## Key Results
- Weak correlation (r < 0.3) between task accuracy and expert alignment scores
- Expert alignment scores average around 0.8-0.9 for high-performing domains
- Human validation shows high claim extraction accuracy (0.9) but moderate inter-annotator agreement (0.57-0.75)
- ~72% of claims pass relevancy filtering on average

## Why This Works (Mechanism)
None

## Foundational Learning
- **Atomic claim decomposition**: Breaking explanations into standalone claims enables independent assessment against expert criteria. Needed because holistic scoring would miss domain-specific reasoning patterns. Quick check: Can each claim be evaluated without reference to others?
- **Domain-specific expert criteria**: Each field requires unique alignment metrics based on established expert reasoning. Needed because generic evaluation metrics don't capture domain expertise. Quick check: Do criteria lists cover the full range of expert reasoning in each domain?
- **Three-stage pipeline**: Separate decomposition, filtering, and scoring stages enable modular evaluation and easier debugging. Needed because monolithic evaluation would be less interpretable and harder to validate. Quick check: Does each stage produce outputs that subsequent stages can process reliably?

## Architecture Onboarding

### Component Map
Expert Criteria List -> Explanation Generation -> Atomic Claim Extraction -> Relevancy Filtering -> Alignment Scoring -> Expert Alignment Score

### Critical Path
Explanation Generation (GPT-4o) -> Atomic Claim Extraction (GPT-4o) -> Relevancy Filtering (GPT-4o) -> Alignment Scoring (GPT-4o)

### Design Tradeoffs
The three-stage pipeline trades computational efficiency for interpretability and modularity. Using a single LLM for all stages risks bias but ensures consistency in reasoning patterns.

### Failure Signatures
- Low claim extraction accuracy indicates poor decomposition templates or domain mismatch
- Near-zero filtering pass rates suggest overly strict relevancy criteria or model mismatch
- Low alignment scores with high entropy indicate claims don't match expert criteria distribution

### First Experiments
1. Generate explanations for 5 examples from Emotion domain using all 4 prompting strategies
2. Run claim extraction pipeline and verify ~90% accuracy on claim identification
3. Test filtering pass rate and verify it's approximately 72% as reported

## Open Questions the Paper Calls Out

### Open Question 1
Can explicitly enforcing expert alignment in LLM reasoning via training objectives or prompting strategies improve downstream task accuracy? The paper establishes that current high-performing models do not naturally rely on expert reasoning but does not test if enforcing this alignment causally boosts performance. Evidence would require experiments fine-tuning models on expert-aligned datasets followed by accuracy evaluation.

### Open Question 2
To what extent does the presence of high expert alignment in LLM explanations improve the decision-making quality or speed of human domain experts? The paper measures alignment metrics but does not validate whether these translate into practical utility for end-users in high-stakes environments. Evidence would require randomized controlled user studies with domain experts measuring diagnostic accuracy and time-to-decision.

### Open Question 3
Can instruction-tuning LLMs specifically to optimize for expert alignment criteria generate explanations that maintain high alignment scores without degrading task performance? The current study evaluates existing models; it remains unknown if models can be successfully optimized to generate specific, atomic, expert-aligned claims required by T-FIX. Evidence would require training a model using T-FIX criteria and evaluating alignment score versus baseline models.

### Open Question 4
Does the reliance on a single domain expert for validating criteria introduce subjectivity that a multi-expert consensus approach would alter significantly? The paper notes that multiple experts would have been better than the single expert used for each domain. Evidence would require comparing T-FIX scores derived from single-expert versus multi-expert criteria.

## Limitations
- Benchmark relies on domain expert criteria lists developed through iterative process with unspecified few-shot examples
- Three-stage pipeline shows moderate inter-annotator agreement (0.57-0.75) suggesting some subjectivity
- Evaluation uses single LLM for both explanation generation and scoring, potentially introducing bias

## Confidence

**High confidence**: Benchmark methodology and pipeline design are clearly specified with reproducible components. Finding of weak correlation between accuracy and alignment is statistically supported.

**Medium confidence**: Domain expert criteria lists appear comprehensive but iterative development process isn't fully reproducible. Human validation results are promising but based on limited sampling.

**Low confidence**: Exact few-shot examples used for prompt engineering are not provided, making it difficult to achieve identical results without significant iteration.

## Next Checks

1. Replicate claim extraction accuracy: Using provided templates and expert criteria, run decomposition pipeline on 20 examples from a single domain and compare accuracy against reported 0.9 benchmark.

2. Test filtering pass rate: Generate explanations using one prompting strategy and verify approximately 72% of claims pass relevancy filtering stage.

3. Validate alignment scoring consistency: Have two independent annotators score the same 10 claims using expert criteria and calculate inter-annotator agreement to verify it falls within reported 0.57-0.75 range.