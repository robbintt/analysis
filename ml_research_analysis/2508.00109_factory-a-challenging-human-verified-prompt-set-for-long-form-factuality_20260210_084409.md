---
ver: rpa2
title: 'FACTORY: A Challenging Human-Verified Prompt Set for Long-Form Factuality'
arxiv_id: '2508.00109'
source_url: https://arxiv.org/abs/2508.00109
tags:
- prompts
- factory
- question
- prompt
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FACTORY is a human-verified, challenging prompt set designed for
  long-form factuality evaluation. Unlike existing benchmarks that lack human quality
  control, FACTORY employs a model-in-the-loop approach to generate prompts, then
  has human annotators revise them to ensure they are fact-seeking, answerable, unambiguous,
  and not time-sensitive.
---

# FACTORY: A Challenging Human-Verified Prompt Set for Long-Form Factuality

## Quick Facts
- arXiv ID: 2508.00109
- Source URL: https://arxiv.org/abs/2508.00109
- Authors: Mingda Chen; Yang Li; Xilun Chen; Adina Williams; Gargi Ghosh; Scott Yih
- Reference count: 35
- Primary result: FACTORY achieves ~60% factual precision on its hard split vs ~90% on prior benchmarks

## Executive Summary
FACTORY is a human-verified prompt set designed to rigorously evaluate long-form factuality in language models. Using a model-in-the-loop approach, the dataset filters out easy prompts through automated factuality scoring, then has human annotators refine the remaining prompts for clarity, answerability, and factual focus. The resulting 10,156 prompts are significantly more challenging than existing benchmarks, with SOTA models achieving only ~60% factual precision on the hardest 421 prompts. Analysis reveals that this difficulty stems from both long-tailed knowledge requirements and multi-fact reasoning, with even simplified "atomic" prompts remaining challenging.

## Method Summary
FACTORY employs a two-stage pipeline: automated filtering followed by human verification. First, Wikipedia-derived topics are used to generate candidate prompts via LLMs, which are then filtered using VeriScore to retain only those where models achieve <60% factual precision. These prompts undergo human review by 39 annotators who edit or reject prompts based on criteria including fact-seeking nature, answerability, clarity, and safety. The process yields 10,156 verified prompts with a particularly challenging subset of 421 where SOTA models achieve ~50% precision. Evaluation uses human annotation of sampled model outputs, labeling claims as supported, unsupported, or inconclusive.

## Key Results
- SOTA models achieve ~60% factual precision on FACTORY Hard vs ~90% on prior benchmarks
- FACTORY's difficulty stems from requiring reasoning across long-tailed facts and specific details
- Even when decomposed into simpler "atomic" prompts, models struggle to achieve high factual precision
- The dataset contains 10,156 prompts across diverse topics, with 421 in the hardest subset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Automated difficulty filtering using model responses and factual precision scoring yields a benchmark that remains challenging for current SOTA models.
- Mechanism: Generate candidate prompts from Wikipedia topics, retrieve context with MassiveDS, have Llama models respond, evaluate with VeriScore, and retain prompts where precision <60%. This filters out easy prompts before human review.
- Core assumption: Prompts that current models cannot reliably solve with retrieval are more likely to require long-tailed knowledge or multi-fact reasoning.
- Evidence anchors:
  - [abstract] "Developed using a model-in-the-loop approach and refined by humans, FACTORY includes challenging prompts that are fact-seeking, answerable, and unambiguous."
  - [section 3] "We then employ VeriScore (Song et al., 2024) to fact-check each model's response... filtering out questions that LLMs can answer with high factual precision... retain prompts for which the Llama models achieve less than 60% factual precision."
  - [corpus] Related long-form evaluation pipelines (e.g., VeriFact, FaStfact) use decompose-verify approaches; direct corpus evidence on this exact filtering mechanism is limited.
- Break condition: If retrieval corpora or base models improve substantially, previously filtered prompts may become easy; re-filtering is required.

### Mechanism 2
- Claim: Human-in-the-loop quality control improves benchmark validity by rejecting or revising prompts that are non-fact-seeking, ambiguous, hallucinated, time-sensitive, or unsafe.
- Mechanism: 39 annotators edit/reject prompts against explicit criteria and provide supporting URLs. ~20% of prompts are changed, yielding 10,156 verified prompts with clearer intent.
- Core assumption: Systematic issues in fully automatic prompt generation (e.g., unanswerable or hallucinated questions) are correctable by human oversight.
- Evidence anchors:
  - [abstract] "FACTORY ensures prompts are fact-seeking, answerable, and unambiguous" and "features detailed, specific prompts that require reasoning across long-tailed facts."
  - [section 3] "Ultimately, around 20% of the prompts were rejected or edited according to different error categories, leaving us 10,156 prompts."
  - [corpus] Weak direct corpus validation for this human-verification mechanism; adjacent benchmarks focus on automated evaluation.
- Break condition: If annotation guidelines drift or annotator quality varies, prompt validity may degrade; ongoing calibration is necessary.

### Mechanism 3
- Claim: Decomposing complex prompts into generic "atomic prompts" reveals that models lack long-tailed knowledge, not only multi-hop reasoning capacity.
- Mechanism: Extract proper nouns from FACTORY prompts, generate generic questions about each, and evaluate without retrieval. Even atomic prompts remain difficult, indicating knowledge gaps.
- Core assumption: If models possessed underlying long-tailed knowledge, atomic prompts should be relatively easy; persistent difficulty implies missing knowledge.
- Evidence anchors:
  - [section 4.3] "We transformed the detailed prompts into more generic ones for each proper noun... creating 'atomic prompts'... while making the prompts more atomic helps in making them more solvable, they remain relatively challenging."
  - [section 4.3] "Such prompts requires models to reason across various facts... due to both its long-tailed knowledge and the reasoning capabilities required."
  - [corpus] Adjacent work discusses response length and grounding effects but does not replicate this specific diagnostic; corpus support is indirect.
- Break condition: If atomic prompts are underspecified or ambiguous, difficulty may conflate clarity with knowledge; atomic prompt quality must be monitored.

## Foundational Learning

- Concept: Factual precision vs. recall in long-form generation
  - Why needed here: FACTORY reports human-judged factual precision (~60% on hard split); understanding precision helps interpret error rates in long-form outputs.
  - Quick check question: If a model makes 100 claims and 60 are supported by evidence, what is the factual precision?

- Concept: Model-in-the-loop data creation
  - Why needed here: FACTORY uses Llama models to filter easy prompts before human review; this loop explains benchmark durability.
  - Quick check question: Why might a dataset created without model-in-the-loop filtering saturate quickly?

- Concept: Long-tailed knowledge
  - Why needed here: FACTORY targets detailed prompts requiring reasoning across rare facts; retrieval may not suffice without broad knowledge coverage.
  - Quick check question: What distinguishes a long-tailed fact from a common fact in a pretraining corpus?

## Architecture Onboarding

- Component map: Wikipedia seeds -> MassiveDS retrieval -> LLM-generated questions -> VeriScore filtering (precision <60%) -> human verification -> FACTORY prompts
- Critical path: Ensure retrieval indices (MassiveDS domains) are versioned and accessible. Replicate VeriScore filtering pipeline (claim extractor + adapted verifier) on candidate prompts. Reproduce human evaluation on a small sample to validate pipeline outputs.
- Design tradeoffs: Scale vs. cost: human verification improves validity but is slower and more expensive than fully automated pipelines. Hardness vs. coverage: aggressive filtering increases challenge but may narrow topic diversity.
- Failure signatures: Sudden precision spikes on hard split may indicate contamination or overfitting. High "inconclusive" rates may indicate residual subjectivity or ambiguity in prompts.
- First 3 experiments: 1) Replicate factual precision on FACTORY Hard for a baseline model (e.g., GPT-4o) with and without retrieval. 2) Run atomic-prompt decomposition on a 100-prompt sample; compare precision on atomic vs. original prompts. 3) Validate human evaluation on 50 sentences to calibrate supported/unsupported/inconclusive labeling.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the performance gap on FACTORY primarily driven by a lack of parametric knowledge of long-tailed entities or by the inability to reason across multiple facts?
- Basis in paper: [explicit] The authors investigate this via "atomic prompts" (Section 4.3), finding that basic details of proper nouns remain challenging, but conclude that both knowledge and reasoning are factors without isolating the dominant failure mode.
- Why unresolved: The analysis shows that breaking down prompts improves solvability but does not achieve perfect precision, leaving the relative contribution of knowledge gaps vs. reasoning failures ambiguous.
- What evidence would resolve it: A controlled error analysis quantifying failures caused solely by missing entity knowledge versus failures in synthesizing known facts.

### Open Question 2
- Question: Does retrieval-augmented generation (RAG) fail on FACTORY because the retriever misses relevant context or because the generator ignores the retrieved context?
- Basis in paper: [inferred] The paper reports low factual precision (~60%) even for retrieval-augmented models, but the methodology does not isolate whether the "MassiveDS" retriever failed to surface the necessary URLs or if the models failed to utilize the context.
- Why unresolved: Without analyzing the retrieval success rate of the gold-standard URLs provided by annotators, it is unclear if the bottleneck is the retrieval index or the model's context adherence.
- What evidence would resolve it: Reporting the "recall@k" of the annotator-provided URLs within the top-20 retrieved passages used for generation.

### Open Question 3
- Question: How does the accessibility of online evidence affect the validity of using "human search" as a gold standard for long-tailed facts?
- Basis in paper: [inferred] The evaluation protocol relies on human annotators finding supportive information online; if a fact is true but not easily searchable, it may be mislabeled as unsupported, penalizing models for correct but obscure knowledge.
- Why unresolved: The paper assumes human verifiers can access the ground truth, an assumption potentially flawed for the specific "long-tailed" facts the benchmark emphasizes.
- What evidence would resolve it: A comparison of model outputs labeled "unsupported" against a closed, authoritative corpus (like the full Wikipedia dump) to identify false negatives.

## Limitations

- Human verification, while improving prompt quality, introduces potential subjectivity and may not fully eliminate all ambiguity
- The reliance on VeriScore for automated filtering assumes its precision metrics are stable across model versions
- Generalization of results beyond the 6 tested models is uncertain given rapid model development

## Confidence

- **High**: FACTORY's construction methodology, human evaluation protocol, and comparative results showing lower SOTA performance on FACTORY vs. existing benchmarks
- **Medium**: Claims about the specific drivers of FACTORY's difficulty (long-tailed knowledge vs. reasoning complexity), as these are inferred from decomposition experiments but not definitively isolated
- **Medium**: Generalization of results beyond the 6 tested models, given the rapid pace of model development and potential shifts in difficulty profiles

## Next Checks

1. **Replicate atomic prompt decomposition on a stratified sample of 200 FACTORY prompts**: Compare factual precision on original vs. atomic prompts across 3 different SOTA models to verify that knowledge gaps persist even with simpler prompt structures.

2. **Cross-validate VeriScore filtering with a second automated factuality metric**: Apply an independent claim verification system (e.g., GPT-4o with a different prompt template) to the same filtered prompt set to assess consistency of the <60% precision threshold.

3. **Conduct inter-annotator agreement study on 100 sampled responses**: Have 3 additional annotators independently label a subset of model outputs to quantify subjectivity in the "supported/unsupported/inconclusive" judgments and assess robustness of the human evaluation protocol.