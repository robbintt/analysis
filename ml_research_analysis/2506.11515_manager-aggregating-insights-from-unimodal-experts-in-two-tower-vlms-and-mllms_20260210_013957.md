---
ver: rpa2
title: 'Manager: Aggregating Insights from Unimodal Experts in Two-Tower VLMs and
  MLLMs'
arxiv_id: '2506.11515'
source_url: https://arxiv.org/abs/2506.11515
tags:
- visual
- manager
- unimodal
- layer
- cross-modal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Manager, a lightweight and effective plugin
  that improves vision-language representation learning by adaptively aggregating
  insights from pre-trained unimodal experts at different layers. Under the Two-Tower
  VLM architecture, ManagerTower introduces managers in each cross-modal layer to
  aggregate multi-layer unimodal representations, significantly outperforming strong
  baselines on four downstream VL tasks.
---

# Manager: Aggregating Insights from Unimodal Experts in Two-Tower VLMs and MLLMs

## Quick Facts
- arXiv ID: 2506.11515
- Source URL: https://arxiv.org/abs/2506.11515
- Authors: Xiao Xu; Libo Qin; Wanxiang Che; Min-Yen Kan
- Reference count: 40
- One-line primary result: Adaptive aggregation of pre-trained unimodal expert features improves vision-language representation learning across both Two-Tower VLMs and Multimodal Large Language Models

## Executive Summary
This paper introduces Manager, a lightweight plugin that enhances vision-language representation learning by adaptively aggregating insights from pre-trained unimodal experts (visual and textual encoders) at different layers. Under the Two-Tower VLM architecture, ManagerTower incorporates managers in each cross-modal layer to aggregate multi-layer unimodal representations, significantly outperforming strong baselines on four downstream VL tasks. The authors further extend their exploration to Multimodal Large Language Models (MLLMs), demonstrating that LLaVA-OV-Manager significantly boosts zero-shot performance across different categories of capabilities, images, and resolutions on 20 downstream datasets. Detailed analysis reveals that both the manager and the multi-grid algorithm can be viewed as plugins that improve visual representation from orthogonal perspectives (depth and width), and their synergy mitigates semantic ambiguity and further improves performance.

## Method Summary
The Manager framework operates by treating each layer of pre-trained unimodal encoders as an expert and adaptively aggregating their outputs based on cross-modal context. In Two-Tower VLMs, the Adaptive Aggregation Unimodal Manager (AAUM) uses a cross-modal fused query to generate distinct aggregation weights for every token, allowing dynamic selection of relevant semantic knowledge from different encoder layers. For MLLMs, the Static Aggregation Unimodal Manager (SAUM) is adapted to work with causal LLMs by removing LayerNorm/Softmax and initializing weights to zero. The approach is validated through a three-stage training pipeline involving large-scale image-text pair pretraining, high-resolution fine-tuning with multi-grid algorithms, and instruction tuning across diverse capabilities.

## Key Results
- ManagerTower significantly outperforms strong Two-Tower VLM baselines on VQAv2, Flickr30K, SNLI-VE, and NLVR2 benchmarks
- LLaVA-OV-Manager significantly boosts zero-shot performance across 20 downstream datasets including DocVQA and OCRBench
- Manager and multi-grid algorithms demonstrate synergistic effects, mitigating semantic ambiguity caused by image splitting
- AAUM with cross-modal fused query significantly outperforms static aggregation methods

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Token-Level Aggregation
The Adaptive Aggregation Unimodal Manager (AAUM) improves performance by generating distinct aggregation weights for every token across different samples, rather than using a static weight distribution for all inputs. AAUM uses a lightweight projection and cross-attention to generate weights based on the specific context of the current token, allowing the model to dynamically select relevant semantic knowledge from different levels of the pre-trained unimodal encoder.

### Mechanism 2: Cross-Modal Fused Query Guidance
Using a cross-modal fused query to guide the manager's aggregation weights significantly outperforms using a single-modality query. The manager calculates aggregation weights by querying the unimodal experts using a context vector derived from both visual and textual modalities, ensuring the selected unimodal insights are relevant to the current cross-modal interaction.

### Mechanism 3: Synergistic Visual Detail Augmentation (Depth + Width)
The Manager and multi-grid algorithm complement each other by capturing visual details from "depth" and "width" respectively, mitigating semantic ambiguity caused by image splitting. The multi-grid algorithm splits high-res images into tiles (width), risking fragmentation of objects, while the Manager aggregates features from deep encoder layers (depth), increasing attention diversity to recover context lost during grid splitting.

## Foundational Learning

**Concept: Transformer Layer Hierarchy**
Why needed here: The Manager treats each layer of the visual/textual encoder as an "expert" with different semantic levels (e.g., texture vs. object vs. scene).
Quick check question: Why does the paper suggest using the top half of the visual encoder rather than all layers? (Hint: Attention distance).

**Concept: Co-Attention Mechanism**
Why needed here: The "Cross-Modal Fused Query" relies on the cross-attention mechanism to generate the aggregation weights.
Quick check question: How does the key-value pair differ between self-attention and cross-attention in this architecture?

**Concept: Semantic Ambiguity in High-Res MLLMs**
Why needed here: Understanding why multi-grid algorithms fail (splitting objects) explains why the Manager is needed as a corrective plugin.
Quick check question: In the provided case study (Fig 13), why does the multi-grid algorithm fail to identify a boxed heading?

## Architecture Onboarding

**Component map:**
Visual encoder layers (V_7...V_12) -> Manager (AAUM/SAUM) -> Cross-modal encoder (Two-Tower) or LLM layers (MLLM) -> Textual encoder layers (T_7...T_12)

**Critical path:**
Implementing the AAUM (Equation 6 & 7). You must correctly implement the cross-attention to generate the fused query CA(C_V^{ℓ-1}, C_T^{ℓ-1}) before projecting it to weights W_A.

**Design tradeoffs:**
SAM vs. AAUM: SAM is static and faster; AAUM is adaptive but computationally heavier (due to cross-attention query). MLLM Specifics: The paper notes AAUM was less effective in LLaVA-OV (MLLM) due to the causal nature of LLMs vs bidirectional encoders, so they defaulted to an optimized SAUM.

**Failure signatures:**
SAM Collapse: If initialization is poor (average), SAM produces nearly identical aggregated representations across layers. Semantic Ambiguity: Multi-grid alone cuts objects; Manager must be added to recover performance.

**First 3 experiments:**
1. Manager Validation: Replace BridgeTower bridges with ManagerTower managers and compare VQAv2/Flickr30K scores to verify performance gains.
2. Query Ablation: Run AAUM with Visual-Query only vs. Cross-Modal-Fused-Query to validate the performance gain of fused context.
3. MLLM Synergy: Evaluate Baseline vs. Baseline+Grid vs. Baseline+Grid+Manager on text-rich datasets to verify ambiguity mitigation.

## Open Questions the Paper Calls Out
None

## Limitations
- Architecture Specificity: The Manager design is tightly coupled to Two-Tower VLM and MLLM architectures and may not generalize to other VL model families.
- Computational Overhead: AAUM introduces additional cross-attention layers without comprehensive ablation studies on inference-time latency or memory consumption.
- Training Complexity: The three-stage MLLM training pipeline involves multiple data sources and fine-tuning phases with potential reproducibility gaps.

## Confidence
- High Confidence: The core mechanism of adaptive token-level aggregation (AAUM) and its superiority over static methods (SAM) is well-supported by controlled experiments.
- Medium Confidence: The synergistic relationship between Manager and multi-grid algorithms for semantic ambiguity mitigation is plausible but primarily supported by observational correlations.
- Low Confidence: Claims about orthogonal improvement (depth vs. width) lack rigorous mathematical formalization and remain largely qualitative.

## Next Checks
1. Cross-Architecture Generalization: Test Manager performance when integrated with alternative VL architectures to verify the claims are not architecture-specific.
2. Computational Efficiency Analysis: Conduct comprehensive ablation studies measuring inference latency, memory usage, and throughput for both AAUM and SAUM across different hardware configurations.
3. Ablation on Weight Initialization: Systematically vary the initialization strategy for SAM to determine if performance degradation is due to initialization or other architectural factors.