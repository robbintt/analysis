---
ver: rpa2
title: 'LLM-Based Data Science Agents: A Survey of Capabilities, Challenges, and Future
  Directions'
arxiv_id: '2510.04023'
source_url: https://arxiv.org/abs/2510.04023
tags:
- data
- arxiv
- agents
- science
- such
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey systematically analyzes 45 LLM-based data science agents,
  mapping their capabilities across six stages of the data science lifecycle and five
  cross-cutting design dimensions. It reveals that while most agents excel in exploratory
  analysis and visualization, critical gaps exist in business understanding, deployment,
  monitoring, multimodal reasoning, and trust mechanisms.
---

# LLM-Based Data Science Agents: A Survey of Capabilities, Challenges, and Future Directions

## Quick Facts
- arXiv ID: 2510.04023
- Source URL: https://arxiv.org/abs/2510.04023
- Authors: Mizanur Rahman; Amran Bhuiyan; Mohammed Saidul Islam; Md Tahmid Rahman Laskar; Ridwan Mahbub; Ahmed Masry; Shafiq Joty; Enamul Hoque
- Reference count: 40
- Primary result: Systematic analysis of 45 LLM-based data science agents reveals critical gaps in business understanding, deployment, multimodal reasoning, and trust mechanisms

## Executive Summary
This survey systematically analyzes 45 LLM-based data science agents, mapping their capabilities across six stages of the data science lifecycle and five cross-cutting design dimensions. While most agents excel in exploratory analysis and visualization, critical gaps exist in business understanding, deployment, monitoring, multimodal reasoning, and trust mechanisms. Over 90% lack explicit safety and governance features, and RL-based optimization remains underexplored. The survey introduces a lifecycle-aligned taxonomy, highlights limitations such as weak end-to-end orchestration and reliability, and identifies key challenges in benchmarking and evaluation. Future directions include advancing multimodal grounding, lightweight RL alignment, and embedding trustworthiness as core design principles for robust, transparent, and trustworthy data science agents.

## Method Summary
The survey employs a PRISMA protocol to conduct systematic literature review across Google Scholar, AI venues (NeurIPS, ICLR, ACL, AAAI), and arXiv for papers published between 2023 and 2025. Using keywords "DS agents", "LLM agents", "multimodal agents", etc., the authors screen ~587 initial candidates and select 45 distinct DS agents meeting specific criteria: LLM as primary reasoning, support for at least one DS lifecycle stage, operation on structured data/code, and demonstration of multi-step planning or tool orchestration. Each agent is mapped to six lifecycle stages (S1-S6) and five design attributes (Reasoning, Modality, Tool Use, Learning, Trust) to construct a comprehensive taxonomy and identify capability gaps.

## Key Results
- Most agents excel at exploratory analysis and visualization but struggle with business understanding and deployment stages
- Over 90% of surveyed agents lack explicit safety and governance features
- RL-based optimization remains underexplored despite its potential for workflow alignment
- Critical gaps exist in multimodal reasoning capabilities and end-to-end lifecycle orchestration
- Current benchmarks inadequately assess process integrity and error recovery

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Task Decomposition
DS agents handle complex analytical workflows by decomposing high-level natural language objectives into structured, executable subtasks. A Planning Module utilizes strategies like single-path or multi-path reasoning to break down ambiguous goals into actionable steps. The core assumption is that the LLM possesses sufficient domain knowledge to generate a valid analytical roadmap without human intervention. This fails when limited context windows cause the agent to lose track of the original goal over long horizons.

### Mechanism 2: Tool-Augmented Execution
Agents ground their reasoning in reality by invoking external tools (code interpreters, databases) rather than relying solely on internal parametric knowledge. An Action Module interfaces with a Tool Hub (APIs, Python kernels) to execute generated code, retrieve live data, or render visualizations. The core assumption is that the agent can map natural language intent to precise tool syntax and schemas with high accuracy. This fails when "hallucinated" code or API calls lead to runtime errors or silent data corruption.

### Mechanism 3: Iterative Reflection and Refinement
System reliability can theoretically be improved by integrating feedback loops where agents critique and revise their own intermediate outputs. Critic agents or self-reflection modules evaluate the result of an action (e.g., checking if code executed correctly or if a visualization matches the data) before proceeding to the next step. The core assumption is that the feedback signal is accurate and consistent with the user's intent. This fails if the feedback is adversarial or "deceptive," causing the agent to deviate from the correct path.

## Foundational Learning

**Concept: Agent Architecture (Profile, Memory, Planning, Action)**
Why needed: The survey organizes its taxonomy and analysis of 45 agents based on these four core modules. Understanding this mental model is prerequisite to comparing systems like AutoGen vs. HuggingGPT.
Quick check: Which module is responsible for maintaining state across a multi-turn data cleaning session?

**Concept: The Data Science Lifecycle (S1–S6)**
Why needed: The paper evaluates agent coverage across six specific stages: Business Understanding, EDA, Feature Engineering, Modeling, Interpretation, and Deployment. You cannot interpret the "gaps" identified without knowing these stages.
Quick check: If an agent generates a dashboard but fails to connect to the production database, which stage (S1-S6) is it failing at?

**Concept: Retrieval-Augmented Generation (RAG)**
Why needed: The paper identifies RAG as a key technique for "conditioning outputs on dynamically retrieved external knowledge," essential for handling private enterprise data not in the LLM's pre-training set.
Quick check: How does RAG differ from standard prompting when an agent needs to query a proprietary SQL database?

## Architecture Onboarding

**Component map:**
User Interface: NL Input / Dashboard Output → Controller: Manager Agent (Planner) dispatching to Worker Agents → Core Modules: Memory (Vector DB), Planning (Reasoning Engine), Action (Tool Hub) → Environment: Python Sandbox (Pandas/Scikit-learn), SQL DB, Visualization Libs

**Critical path:** User Intent (S1) → Reasoning/Planning → Tool Selection (S2/S3) → Code Generation → Execution → Reflection → Reporting (S5)

**Design tradeoffs:**
- Single-agent vs. Multi-agent: Single agents are simpler but struggle with context; Multi-agent offers specialization but increases coordination latency
- Static vs. Dynamic Planning: Static plans are reproducible; Dynamic plans adapt to data but risk instability

**Failure signatures:**
- Context Amnesia: Agent forgets the "Business Goal" while deep in "Feature Engineering"
- Tool Hallucination: Generating Python code for libraries not installed in the sandbox
- Silent State Corruption: Performing in-place dataframe mutations that invalidate subsequent steps

**First 3 experiments:**
1. **Lifecycle Coverage Test:** Give the agent a vague goal ("Improve sales") and verify if it asks clarifying questions (S1) or jumps straight to code (S2)
2. **Error Recovery Test:** Provide a dirty dataset and observe if the agent can detect and fix format errors (S2) or if it crashes
3. **Hallucination Check:** Ask for an analysis requiring a specific library version and verify the generated code's syntax against the actual tool documentation

## Open Questions the Paper Calls Out

### Open Question 1
Can lightweight preference-based alignment methods (e.g., DPO, GRPO) be effectively adapted to align multi-step data science workflows, such as feature engineering and model selection, with human intent?
Basis: Section 5.4.2 states that "methods like DPO and GRPO remain unexplored for DS workflows" despite their potential to refine domain-specific outputs.
Why unresolved: RLHF is too resource-intensive for these workflows, and current agents rely on brittle prompt-based alignment.
What evidence would resolve it: Successful fine-tuning of an agent using DPO showing improved adherence to business constraints compared to prompt-only baselines.

### Open Question 2
How can evaluation frameworks be structured to assess end-to-end lifecycle coverage and process integrity, rather than just final output accuracy on isolated tasks?
Basis: Section 7.6 argues that existing benchmarks "neglect end-to-end workflows" and require "process-oriented metrics" to evaluate error recovery and scalability.
Why unresolved: Current metrics miss silent state corruption or logic errors that occur during intermediate steps of the data science pipeline.
What evidence would resolve it: A benchmark suite that specifically tests pipeline orchestration with fault injection and measures error recovery rates.

### Open Question 3
What architectural modifications are required to achieve robust cross-modal grounding (e.g., interpreting charts and schemas) without incurring prohibitive token costs?
Basis: Section 7.9 identifies "high token costs" and "perception errors" as key limitations, suggesting efficient architectures and "Visualization-of-Thought" as necessary directions.
Why unresolved: Standard fusion techniques fail to efficiently handle large tables or high-resolution visual inputs typical in data science.
What evidence would resolve it: An agent architecture that maintains reasoning fidelity while reducing latency and token usage on multi-modal data science tasks compared to GPT-4V.

## Limitations

- The survey's classification of lifecycle stage coverage requires subjective interpretation of agent capabilities from published descriptions
- Ambiguity exists in distinguishing "deep" tool orchestration from basic API calls, affecting design dimension categorizations
- Claims about future directions requiring technological breakthroughs extend beyond surveyed literature into speculative territory

## Confidence

**High Confidence**: Structural taxonomy (6 lifecycle stages, 5 design dimensions) and identification of major capability gaps in deployment, monitoring, and trust mechanisms

**Medium Confidence**: Specific agent-to-stage mapping, as this requires subjective interpretation of agent capabilities from published descriptions and examples

**Low Confidence**: Claims about future directions requiring technological breakthroughs, such as achieving true multimodal reasoning for heterogeneous data types

## Next Checks

1. **Lifecycle Coverage Validation**: Select 5 randomly chosen agents and independently verify their stage coverage by executing their core functionality with standardized datasets, checking if the observed behavior matches the survey's taxonomy assignment

2. **Trust Mechanism Assessment**: For the 10% of agents claiming safety features, conduct adversarial testing to evaluate whether stated safeguards (e.g., bias detection, output validation) function as described or represent superficial claims

3. **Context Window Robustness**: Test agents with progressively longer analytical workflows to empirically determine when context amnesia occurs, validating the survey's claim that limited context windows cause agents to lose track of original goals during extended sessions