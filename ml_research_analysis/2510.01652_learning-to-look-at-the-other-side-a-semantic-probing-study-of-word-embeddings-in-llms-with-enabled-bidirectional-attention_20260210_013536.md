---
ver: rpa2
title: 'Learning to Look at the Other Side: A Semantic Probing Study of Word Embeddings
  in LLMs with Enabled Bidirectional Attention'
arxiv_id: '2510.01652'
source_url: https://arxiv.org/abs/2510.01652
tags:
- mntp
- task
- word
- tasks
- verb
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how enabling bidirectional attention in
  autoregressive language models affects word-level semantic representations. The
  authors test Llama models with and without bidirectional attention, along with contrastive
  learning techniques, on five lexical semantic probing tasks.
---

# Learning to Look at the Other Side: A Semantic Probing Study of Word Embeddings in LLMs with Enabled Bidirectional Attention

## Quick Facts
- arXiv ID: 2510.01652
- Source URL: https://arxiv.org/abs/2510.01652
- Reference count: 32
- Primary result: Bidirectional attention in autoregressive LLMs improves right-context encoding but weakens left-context encoding, while supervised contrastive learning mitigates this trade-off and yields best semantic probing performance.

## Executive Summary
This paper investigates how enabling bidirectional attention in autoregressive language models affects word-level semantic representations. The authors test Llama models with and without bidirectional attention, along with contrastive learning techniques, on five lexical semantic probing tasks. Results show that bidirectional attention improves representation of subsequent context but weakens previous context utilization, while contrastive learning helps maintain both abilities. Interestingly, bidirectional attention increases anisotropy across all layers, contradicting expectations that it would reduce this issue. Despite this, supervised contrastive learning generally produces the best performance across tasks. The study demonstrates that autoregressive models can achieve performance comparable to bidirectional models in semantic probing tasks when properly trained, challenging the assumption that bidirectional attention is always necessary for word-level semantic understanding.

## Method Summary
The authors modify pretrained autoregressive Llama models by removing the causal attention mask and applying a Masked Next Token Prediction (MNTP) objective, creating bidirectional variants. They then apply contrastive learning techniques (SimCSE and supervised variants) to these bidirectional models. Word embeddings are extracted from the final hidden layer, with multi-token words represented by averaging subword embeddings. These embeddings are then used as input to simple probing classifiers (MLPs or linear models) for five lexical semantic tasks: context-probes classification (animacy, causative, dynamic), lexical aspect classification (telicity/duration), CONcreTEXT concreteness regression, sensorimotor norms regression (11 dimensions), and Word-in-Context sense disambiguation.

## Key Results
- Bidirectional attention improves encoding of subsequent (right-hand) context but weakens encoding of preceding (left-hand) context
- Supervised contrastive learning mitigates the left-right context trade-off and yields best performance across tasks
- Enabling bidirectional attention increases anisotropy across all layers, contradicting expectations that it would reduce this issue
- Autoregressive models with proper training can match bidirectional models' performance on word-level semantic probing tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Enabling bidirectional attention in autoregressive LLMs improves encoding of subsequent (right-hand) context but weakens encoding of preceding (left-hand) context.
- Mechanism: Masked Next Token Prediction (MNTP) modifies the training objective to reconstruct masked tokens using both left and right context. This bidirectional training forces token representations to aggregate information from subsequent tokens, but may overwrite or de-emphasize the preceding-context representations originally developed through autoregressive pretraining.
- Core assumption: The representational capacity of the model is finite and shifts based on the training objective; adapting to new attention patterns degrades previously learned capabilities.
- Evidence anchors:
  - [abstract] "Bidirectional attention improves representation of subsequent context but weakens previous context utilization"
  - [section 4, Finding 1] "...bidirectional attention improves the LLMs' ability to represent the right-hand context of a target word, it also seems to weaken the representation of the left context"
  - [corpus] Weak/missing; related papers focus on probing verb representations and sentence embeddings, not on this specific bidirectional-unidirectional trade-off in LLMs.

### Mechanism 2
- Claim: Supervised contrastive learning, applied after bidirectional training, mitigates the trade-off between left and right context encoding and can yield superior performance on semantic probing tasks.
- Mechanism: Contrastive learning explicitly shapes the geometry of the embedding space by pulling representations of semantically similar instances closer and pushing dissimilar instances apart. This regularizes the embedding space, potentially encouraging more balanced and robust encoding of contextual features from both directions.
- Core assumption: The supervised contrastive loss provides a sufficiently strong learning signal to reorganize the embedding space in a way that recovers or enhances context encoding abilities.
- Evidence anchors:
  - [abstract] "contrastive learning helps maintain both abilities"
  - [abstract] "supervised contrastive learning generally produces the best performance across tasks"
  - [corpus] [Paper 103498] "...improve [static word embeddings] with sentence-level principal component analysis, followed by either knowledge distillation or contrastive learning," suggesting contrastive learning is a known technique for improving embedding quality.

### Mechanism 3
- Claim: Enabling bidirectional attention alone in Llama models increases anisotropy (non-uniformity) of the embedding space, contrary to expectations.
- Mechanism: The change in attention mechanism and training objective (MNTP) may cause token representations to collapse into a narrower cone in the vector space, increasing average cosine similarity between unrelated words. This suggests the new training dynamics do not inherently promote isotropy.
- Core assumption: Anisotropy is influenced by the attention mechanism and training objective, not solely by the model architecture family.
- Evidence anchors:
  - [abstract] "...bidirectional attention increases anisotropy across all layers, contradicting expectations that it would reduce this issue"
  - [section 4.1] "...Sheared-Llama-1.3B with bidirectional attention exhibits higher global similarity across all hidden states compared to its unidirectional counterpart"
  - [corpus] Weak/missing; neighbor papers do not address anisotropy changes resulting from this specific attention modification.

## Foundational Learning

- Concept: Anisotropy in Embedding Spaces
  - Why needed here: A core finding is that enabling bidirectional attention *increases* anisotropy, which degrades the usefulness of cosine similarity. Understanding this concept is essential for interpreting the paper's results on embedding quality.
  - Quick check question: If all word vectors occupy a very narrow cone in the vector space, what happens to the cosine similarity between two semantically unrelated words?

- Concept: Probing Classifiers
  - Why needed here: The paper's conclusions are based on "probing" tasks where a simple classifier (like an MLP) is trained on frozen embeddings to predict a semantic property. One must understand that probe performance is a proxy for what information is *linearly accessible* in the embeddings.
  - Quick check question: If a probing classifier achieves high accuracy on a task (e.g., predicting animacy), what can we conclude about the model's embeddings?

- Concept: Autoregressive vs. Bidirectional Attention
  - Why needed here: The central intervention is changing an autoregressive model's attention mask to be bidirectional. Understanding the fundamental difference—causal masking vs. full visibility—is required to follow the paper's hypothesis and methodology.
  - Quick check question: In a standard autoregressive model (e.g., GPT), can the representation for the token at index 5 attend to the token at index 10?

## Architecture Onboarding

- Component map: Sheared-Llama-1.3B -> Bi+MNTP modification -> Bi+MNTP+SimCSE/Supervised -> Embedding extraction -> Probing classifier

- Critical path:
  1. Start with a pretrained autoregressive Llama model.
  2. Enable bidirectional attention (remove causal mask) and perform additional training with the MNTP objective.
  3. (Optional but key for performance) Apply contrastive learning (SimCSE or supervised).
  4. Extract embeddings from the final layer.
  5. Train and evaluate a probing classifier on semantic tasks.

- Design tradeoffs:
  - Bidirectional Attention Alone: Gains access to subsequent context but degrades preceding context representation and increases anisotropy.
  - Adding Contrastive Learning: Mitigates the trade-offs. **Crucially:** Unsupervised contrastive learning reduces anisotropy, while supervised contrastive learning *increases* it despite yielding better task performance. Choose based on whether the downstream use case is supervised probing or unsupervised similarity tasks.

- Failure signatures:
  - Preceding-context probing failure: Performance drops when the probing input token is *before* the target word in the sentence.
  - High anisotropy: Randomly sampled word vectors show unexpectedly high average cosine similarity.
  - Task performance degradation: Simple enabling of bidirectional attention (Bi+MNTP) leads to worse performance than the original unidirectional model.

- First 3 experiments:
  1. **Context-Probe Positional Analysis:** Feed embeddings from tokens at different positions (1 to 5) in a fixed sentence structure into a classifier to predict a property of a target word. Confirm the unidirectional model fails when the probe token is before the target, while the bidirectional model succeeds but shows reverse degradation.
  2. **Anisotropy Layer-wise Measurement:** Extract embeddings for a set of words from all layers of each model variant. Calculate and plot the average cosine similarity between randomly sampled embeddings at each layer to visualize anisotropy trends.
  3. **WiC with Embedding Combination Strategies:** Evaluate the Word-in-Context task using three methods to combine a word's embeddings from two sentences: cosine similarity, absolute difference, and concatenation. Compare performance across model variants to see if contrastive learning makes the simpler cosine similarity strategy more viable.

## Open Questions the Paper Calls Out

- Question: Why does enabling bidirectional attention in autoregressive models weaken the representation of preceding context (left context)?
  - Basis in paper: [explicit] The authors note in the Limitations section that the "underlying mechanisms remain unclear" despite results showing that bidirectional attention "improves LLMs’ ability to represent subsequent context but weakens their utilization of preceding context."
  - Why unresolved: The paper identifies the trade-off empirically but does not investigate the internal activation shifts or attention head behaviors that cause the degradation of left-context processing.
  - What evidence would resolve it: A layer-wise analysis of attention head importance or causal tracing to determine how information flow from previous tokens is disrupted during the bidirectional training phase.

- Question: Does the application of bidirectional attention and contrastive learning to LLMs inevitably degrade their text generation capabilities?
  - Basis in paper: [explicit] The authors acknowledge in the Limitations that they "did not evaluate text generation capabilities" and cite concerns that these mechanisms "may significantly exacerbate text repetition" or disrupt autoregressive generation.
  - Why unresolved: The study focused exclusively on embedding quality for probing tasks, leaving the performance trade-off for generative tasks untested.
  - What evidence would resolve it: Evaluating the modified models on standard generation benchmarks (e.g., perplexity scores, repetition metrics) to measure the cost of adapting the architecture for embedding tasks.

- Question: Why does adding bidirectional attention increase anisotropy across all layers of Llama models, contradicting the behavior of traditional bidirectional models like BERT?
  - Basis in paper: [explicit] The authors state their hypothesis that bidirectional attention would reduce anisotropy was contradicted, observing instead that it "increases anisotropy across all layers" compared to the unidirectional base model.
  - Why unresolved: The paper documents the increase in average cosine similarity but does not offer a theoretical explanation for why this architectural change pushes token representations into a narrower cone.
  - What evidence would resolve it: Analyzing the spectral properties of the resulting embedding matrices to determine if the Masked Next Token Prediction (MNTP) optimization landscape encourages dimensions to collapse.

## Limitations

- The paper does not investigate the mechanistic reasons why bidirectional attention degrades preceding-context encoding, only documenting the empirical effect
- Text generation capabilities were not evaluated, leaving unknown whether the bidirectional modifications harm generative performance
- The increase in anisotropy from bidirectional attention contradicts theoretical expectations but lacks a mechanistic explanation

## Confidence

**High Confidence**: The finding that bidirectional attention improves right-context encoding while weakening left-context encoding is well-supported by positional probing experiments across multiple tasks. The performance patterns are consistent and statistically significant.

**Medium Confidence**: The claim that supervised contrastive learning produces the best overall performance is supported by task results, but the analysis doesn't fully disentangle whether this stems from embedding quality improvements or better probe training dynamics. The anisotropy increase from bidirectional attention is observed but not mechanistically explained.

**Low Confidence**: The assertion that bidirectional attention's anisotropy increase "contradicts expectations" is stated without rigorous theoretical grounding. The paper doesn't establish what the expected behavior should be or why the observed pattern is surprising.

## Next Checks

1. **Ablation Study on Attention Mask Dynamics**: Train a model with bidirectional attention but without the MNTP objective (e.g., using standard MLM). This would isolate whether the performance trade-offs and anisotropy changes stem from the attention mechanism itself or from the specific training objective.

2. **Probe Capacity Analysis**: Systematically vary probe architecture complexity (from linear to deep MLPs) and measure how performance changes across model variants. This would test whether bidirectional attention's apparent benefits are probe-dependent and help distinguish between embedding quality improvements versus probe learning effects.

3. **Layer-wise Context Encoding Analysis**: Extract embeddings from intermediate layers of each model variant and perform the same contextual probing analysis. This would reveal whether bidirectional attention's effects on context encoding are uniform across layers or concentrated in specific depths, providing insight into the mechanism of representational shifts.