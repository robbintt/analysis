---
ver: rpa2
title: Test Time Adaptation Using Adaptive Quantile Recalibration
arxiv_id: '2511.03148'
source_url: https://arxiv.org/abs/2511.03148
tags:
- batch
- adaptation
- distribution
- source
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Adaptive Quantile Recalibration (AQR), a test-time
  adaptation method that aligns internal feature distributions between source and
  target domains using nonparametric quantile-based transformations. Unlike methods
  limited to batch normalization layers, AQR works across BatchNorm, GroupNorm, and
  LayerNorm architectures.
---

# Test Time Adaptation Using Adaptive Quantile Recalibration

## Quick Facts
- arXiv ID: 2511.03148
- Source URL: https://arxiv.org/abs/2511.03148
- Reference count: 40
- Primary result: AQR achieves 63.7% top-1 accuracy on ImageNet-C (severity 3, batch 128) versus 58.5% for TTN

## Executive Summary
This paper introduces Adaptive Quantile Recalibration (AQR), a test-time adaptation method that aligns internal feature distributions between source and target domains using nonparametric quantile-based transformations. Unlike methods limited to batch normalization layers, AQR works across BatchNorm, GroupNorm, and LayerNorm architectures. The method computes source domain statistics during setup and applies piecewise linear transformations to match target domain pre-activation distributions channel-wise. To address tail estimation challenges with varying batch sizes, AQR incorporates a robust tail calibration strategy.

## Method Summary
AQR operates in two phases: setup and inference. During setup, the method computes 101 percentiles (p₀ through p₁₀₀) from pre-activation values after normalization layers across 10,000 held-out source samples per channel. At inference, it computes target percentiles on-the-fly and applies piecewise linear interpolation to map target CDFs to source CDFs. The transformation AQR(x) = p^S_j + ((x - p^T_j)/Δ^T_j) × Δ^S_j maps values within each percentile interval. For tail estimation with small batches, AQR samples multiple batches, computes min/max repeatedly, and averages to reduce variance. The method is applied episodically—each batch processed independently using frozen source statistics.

## Key Results
- Achieves 63.7% top-1 accuracy on ImageNet-C severity 3 (batch 128) versus 58.5% for TTN
- Outperforms state-of-the-art baselines (TENT, SAR, TTN) across all corruption severities on CIFAR-10-C, CIFAR-100-C, and ImageNet-C
- Particularly effective at higher severities: 49.8% vs 43.9% (TTN) on ImageNet-C severity 5
- Works across BatchNorm, GroupNorm, and LayerNorm architectures where TTN is limited to BatchNorm only

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aligning pre-activation distributions via quantile matching corrects covariate shift more completely than moment-matching approaches like TTN.
- Mechanism: During setup, AQR computes 101 percentiles from source data per channel. At inference, it computes target percentiles on-the-fly and applies piecewise linear interpolation to map the full target CDF to the source CDF.
- Core assumption: Distribution shifts between training and testing manifest as shifts in internal pre-activation distributions that can be inverted via strictly increasing transformations.
- Evidence anchors:
  - [abstract]: "modifies pre-activation distributions by aligning quantiles on a channel-wise basis... captures the full shape of activation distributions"
  - [section 3.4]: Theoretical proof that MSE(T^AQR) = 0 while MSE(T^TTN) > 0 for non-affine corruptions under idealized conditions
  - [corpus]: Weak direct corpus support; neighboring papers focus on entropy minimization and MoE approaches, not quantile methods
- Break condition: If corruptions are affine (k_i affine), TTN matches AQR performance. If source/target distributions have disjoint support, quantile transform cannot recover.

### Mechanism 2
- Claim: Freezing source statistics prevents the performance degradation over time seen in entropy-based TTA methods.
- Mechanism: Source percentiles are computed once using 10,000 training samples and never updated. Target batches are processed independently (episodic setting). This provides a stable reference point rather than continuously drifting parameters.
- Core assumption: Source distribution statistics remain valid throughout testing; adaptation only requires correcting target deviations, not updating the reference.
- Evidence anchors:
  - [abstract]: "leverages source-domain statistics computed at training time, enabling unsupervised adaptation without retraining"
  - [section 3.1]: "These stored percentiles (p^S_i) serve as a memory of the distribution characteristics... and will be used during inference"
  - [corpus]: Ranked Entropy Minimization paper notes stability issues in continual TTA, supporting AQR's stateless design choice
- Break condition: If target distribution shifts are non-stationary and the optimal adaptation requires learning from accumulated batches, episodic AQR underperforms online methods.

### Mechanism 3
- Claim: Averaging tail estimates from repeated small-batch sampling reduces variance in extreme percentile estimation.
- Mechanism: Instead of using raw min/max from a single batch, AQR samples batches of 100 points, computes tails, repeats 1,000 times, and averages. This addresses the systematic bias where small-batch minimums overestimate and maximums underestimate true extremes.
- Core assumption: Source distribution can be sampled multiple times during setup; tail behavior is stable enough that averaging reduces estimation variance.
- Evidence anchors:
  - [section 3.3, Figure 3]: Shows 0th percentile consistently overestimates and 100th underestimates with 128-sample batches
  - [table 2]: "Average Sample Tails" achieves 33.7% vs standard AQR's 30.8% at batch size 128
  - [corpus]: No corpus papers address tail estimation in TTA
- Break condition: If inference batch sizes are extremely small (< 32), even calibrated tails may be unreliable; clipping strategy catastrophically fails (3.4% accuracy).

## Foundational Learning

- **Quantile functions and CDFs**:
  - Why needed here: AQR's core operation is F^(-1)_source(F_target(x)). Understanding how CDFs map values to [0,1] and quantile functions invert this is essential for debugging the transformation.
  - Quick check question: Given a target pre-activation value x at the 73rd percentile of its batch, what source percentile should it map to?

- **Normalization layers (BatchNorm, GroupNorm, LayerNorm)**:
  - Why needed here: AQR operates on pre-activations (outputs of normalization layers before activation functions). Different normalization schemes have different channel structures that affect how percentiles are computed.
  - Quick check question: Why can TTN only work with BatchNorm while AQR works with all three?

- **Piecewise linear interpolation**:
  - Why needed here: AQR discretizes the continuous quantile transform into 100 intervals. Understanding interpolation error bounds (O(K⁻²) from Lemma 4) helps set percentile granularity.
  - Quick check question: If you reduce from 101 to 11 percentiles, what happens to approximation quality and why?

## Architecture Onboarding

- **Component map**: Setup phase (source samples → pre-activations → percentile computation → tail calibration) → Inference phase (target batch → pre-activation extraction → target percentile computation → piecewise linear transformation → continue forward pass)

- **Critical path**:
  1. Identify where to insert AQR hooks (after normalization, before activation)
  2. Determine layer selection (all layers for ResNet; top half for ViT on ImageNet per Appendix D)
  3. Implement efficient percentile computation (sorted batch extraction per channel)
  4. Handle edge cases: batch size < channel count, extreme outliers

- **Design tradeoffs**:
  - Percentile granularity: 101 vs 11 percentiles trades ~6-7% accuracy for 10× fewer stored values
  - Layer selection: ViT residual streams can propagate noise if only bottom layers adapted
  - Tail strategy: "Average Sample Tails" adds setup cost but critical for small inference batches; "Not Calibrated" (leaving extremes unchanged) degrades gracefully

- **Failure signatures**:
  - Clipping tails → accuracy collapses to ~3% (Table 2)
  - Applying to all layers in fine-tuned ViT → accuracy drops 28% vs top-half only (Table 9)
  - Very small batches (< 32) without tail calibration → high variance, unstable adaptation

- **First 3 experiments**:
  1. Validate setup phase: Run AQR on clean ImageNet validation set with ResNet50-BN; expect < 0.5% accuracy change from baseline (sanity check that source statistics don't distort in-distribution)
  2. Ablate layer selection: Compare all-layers vs top-half vs bottom-half on ViT-Base with ImageNet-C severity 3; confirm top-half advantage
  3. Stress test batch sizes: Evaluate ResNet50-BN on ImageNet-C with batch sizes {16, 32, 64, 128, 512} using "Average Sample Tails"; verify performance degrades gracefully at small batches

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can AQR be effectively combined with parameter-updating TTA methods to leverage both distribution alignment and model adaptation?
- **Basis in paper**: [explicit] The conclusion suggests "future work could explore combining AQR with other TTA methods."
- **Why unresolved**: The current study evaluates AQR in isolation against baselines like TENT and SAR, rather than as a complementary module.
- **What evidence would resolve it**: Experiments integrating AQR's quantile transformation with gradient-based entropy minimization in a unified pipeline.

### Open Question 2
- **Question**: How does AQR perform under extreme batch size constraints (e.g., batch size = 1), given the instability of empirical quantile estimation?
- **Basis in paper**: [inferred] The paper notes AQR is "particularly pronounced with larger batch sizes" and relies on tail calibration, yet results are limited to sizes 128 and 512.
- **Why unresolved**: Small batches increase variance in tail estimation, and it is unclear if the proposed calibration strategies suffice for single-sample inference.
- **What evidence would resolve it**: Evaluation of AQR with batch size 1 (online setting) comparing standard versus tail-calibrated variants.

### Open Question 3
- **Question**: Does the strategy of limiting AQR to the top half of Vision Transformers generalize to other architectures?
- **Basis in paper**: [inferred] Page 6 hypothesizes that applying AQR only to the top half is beneficial to prevent noisy residual stream propagation, but this is architecture-specific.
- **Why unresolved**: This heuristic was derived empirically for specific ViT-Base models to handle residual connections.
- **What evidence would resolve it**: Ablation studies across diverse architectures (e.g., ResNet-101, Swin Transformers) to validate optimal layer selection.

## Limitations

- Method's effectiveness depends on invertible quantile transformations, which may fail for complex non-monotonic shifts or disjoint source/target distributions
- Theoretical analysis assumes idealized conditions (infinite samples, known true distributions) that don't hold in practice
- Episodic inference design prevents learning from accumulated batches, potentially limiting adaptation to non-stationary distributions
- Computational overhead of percentile computation at inference time remains significant for large models

## Confidence

- **High confidence**: AQR consistently outperforms baselines across multiple datasets and architectures, particularly at higher corruption severities (Section 4, Tables 1-4)
- **Medium confidence**: The piecewise linear approximation provides sufficient accuracy for practical purposes (Table 2), though theoretical bounds assume infinite samples
- **Medium confidence**: Tail calibration significantly improves performance for small batch sizes, but the "Average Sample Tails" strategy adds setup overhead

## Next Checks

1. **Failure mode analysis**: Systematically test AQR on synthetic distributions where source/target have disjoint support or where corruptions are non-monotonic to identify the exact limits of quantile-based correction
2. **Non-stationary adaptation**: Compare episodic AQR against online TTA methods (TENT, entropy minimization) on datasets with gradually changing corruption patterns to quantify the tradeoff between stability and adaptation capacity
3. **Computational efficiency profiling**: Measure actual inference time overhead of AQR across different architectures and batch sizes, comparing against theoretical O(N log N) percentile computation complexity to identify practical bottlenecks