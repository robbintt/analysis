---
ver: rpa2
title: How does a Multilingual LM Handle Multiple Languages?
arxiv_id: '2502.04269'
source_url: https://arxiv.org/abs/2502.04269
tags:
- languages
- multilingual
- linguistic
- across
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates multilingual language models (MLMs) like BLOOM-1.7B
  and Qwen2 on semantic consistency, named entity recognition, and cross-lingual knowledge
  transfer across languages including English, French, Spanish, German, Chinese, Arabic,
  and Tamil. Using cosine similarity, it finds BLOOM-1.7B shows high semantic alignment
  with related languages (French, Spanish, German) but low alignment with Chinese.
---

# How does a Multilingual LM Handle Multiple Languages?

## Quick Facts
- arXiv ID: 2502.04269
- Source URL: https://arxiv.org/abs/2502.04269
- Reference count: 8
- Primary result: BLOOM-560m outperforms BERT-base Multilingual Cased on XNLI tasks with 0.70 (English), 0.59 (Arabic), and 0.50 (Swahili) accuracy

## Executive Summary
This study evaluates multilingual language models BLOOM-1.7B and Qwen2 across semantic consistency, named entity recognition, and cross-lingual knowledge transfer. The research finds that BLOOM-1.7B exhibits high semantic alignment with related languages (French, Spanish, German) but low alignment with Chinese. Layer-wise probing reveals BLOOM-1.7B degrades in deeper layers while Qwen2 maintains more stable performance. Cross-lingual transferability experiments show BLOOM-560m achieving 0.70 accuracy on English XNLI compared to BERT's 0.54, with transfer efficiency of 0.87 for Arabic and 0.73 for Swahili.

## Method Summary
The study employs cosine similarity to measure semantic consistency across eight languages (English, French, Spanish, German, Chinese, Arabic, Tamil, and Swahili). Layer-wise probing analyzes hidden state representations to identify performance degradation patterns. Cross-lingual transferability is evaluated using XNLI tasks with fine-tuning on BLOOM-560m and BERT-base Multilingual Cased. The experiments are constrained by GPU resources, limiting the number of languages evaluated and potentially affecting the quality of fine-tuning on low-resource languages.

## Key Results
- BLOOM-1.7B shows high semantic alignment with related languages (French, Spanish, German) but low alignment with Chinese
- BLOOM-1.7B degrades in deeper layers while Qwen2 maintains more stable layer-wise performance
- BLOOM-560m outperforms BERT-base Multilingual Cased on XNLI with 0.70 (English), 0.59 (Arabic), and 0.50 (Swahili) accuracy

## Why This Works (Mechanism)
The multilingual language models leverage shared semantic spaces where related languages cluster together based on linguistic proximity. BLOOM's architecture appears to maintain better cross-lingual representations through its training methodology, which likely incorporates more diverse multilingual data compared to BERT. The layer-wise degradation in BLOOM-1.7B suggests that deeper layers may specialize too heavily in specific language patterns rather than maintaining cross-lingual generalization. Qwen2's stability indicates more effective parameter sharing across languages throughout the model depth.

## Foundational Learning
- Cosine similarity for semantic alignment: Measures vector similarity in embedding space to quantify how closely languages relate semantically
- Cross-lingual transferability: Evaluates model performance when fine-tuned on one language and tested on another
- Layer-wise probing: Analyzes model behavior at different depths to identify where performance patterns emerge or degrade
- XNLI benchmark: Natural language inference task used for cross-lingual evaluation across multiple languages
- Transfer efficiency metric: Ratio of zero-shot to supervised performance measuring how well knowledge transfers

## Architecture Onboarding
- Component map: Input text -> Tokenizer -> Embedding layer -> Transformer blocks (N layers) -> Pooling layer -> Classification head
- Critical path: Tokenization through embedding generation to transformer processing determines semantic representation quality
- Design tradeoffs: Depth vs stability (BLOOM-1.7B deeper layers degrade), parameter sharing across languages vs specialization
- Failure signatures: Layer-wise degradation in BLOOM-1.7B, poor Chinese alignment despite overall strong multilingual performance
- First experiments: 1) Test semantic consistency on additional Slavic languages, 2) Compare layer-wise probing on syntactic tasks, 3) Evaluate transfer efficiency on morphologically rich languages

## Open Questions the Paper Calls Out
None

## Limitations
- GPU resource constraints limited language coverage to only 8 languages
- Restricted evaluation sample may not represent all language families and scripts
- Suboptimal performance on low-resource languages due to limited fine-tuning data

## Confidence
- Semantic consistency findings: Medium confidence due to limited language sample
- Cross-lingual transfer results: High confidence for BLOOM-560m vs BERT comparisons
- Layer-wise probing analysis: Medium confidence requiring verification on additional tasks

## Next Checks
1. Expand cross-lingual transfer evaluation to include Slavic (Russian, Polish) and Turkic (Turkish, Uzbek) languages
2. Conduct ablation studies varying fine-tuning data quantity on low-resource languages
3. Implement additional probing tasks (syntactic dependency parsing, morphological tagging) beyond semantic consistency