---
ver: rpa2
title: 'Empowering AI to Generate Better AI Code: Guided Generation of Deep Learning
  Projects with LLMs'
arxiv_id: '2504.15080'
source_url: https://arxiv.org/abs/2504.15080
tags:
- code
- learning
- deep
- generation
- dlcodegen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'DLCodeGen addresses the challenge of generating entire deep learning
  projects with large language models, which struggle with complex structures, longer
  functions, and domain-specific knowledge. The core method involves a planning-guided
  generation approach: a GPT-2 model predicts structured solution plans offering global
  guidance, which are then used to retrieve semantically analogous code samples and
  abstract templates.'
---

# Empowering AI to Generate Better AI Code: Guided Generation of Deep Learning Projects with LLMs

## Quick Facts
- **arXiv ID:** 2504.15080
- **Source URL:** https://arxiv.org/abs/2504.15080
- **Reference count:** 40
- **Primary result:** DLCodeGen achieves 9.7% CodeBLEU and 3.6% human evaluation improvement over baselines for generating complete DL projects.

## Executive Summary
DLCodeGen introduces a planning-guided approach to generate entire deep learning projects using LLMs. The method addresses the challenge of maintaining coherence across long code chains (>300 lines) and handling domain-specific knowledge by predicting structured solution plans, retrieving semantically analogous code samples and abstract templates, and using comparative learning to synthesize the final output. Experiments on a novel DLCodeEval dataset demonstrate significant improvements over state-of-the-art baselines.

## Method Summary
DLCodeGen processes natural language requirements through a pipeline: a fine-tuned GPT-2 predicts structured solution plans offering global context; BM25 retrieval fetches semantically relevant code samples and templates from a curated pool; a comparative learning mechanism uses DeepSeek-V2.5 to synthesize final code by integrating optimal segments from dual-path retrievals. The approach trades inference latency for improved code quality and structural coherence.

## Key Results
- DLCodeGen outperforms baselines by 9.7% in CodeBLEU and 3.6% in human evaluation metrics.
- Planning-guided generation proves critical for coherence across long DL project structures.
- Dual-path retrieval (concrete code vs. abstract templates) enables effective synthesis of implementation details and high-level logic.

## Why This Works (Mechanism)

### Mechanism 1: Planning-Guided Context Injection
- Claim: Standard LLMs lose coherence after ~50 lines; structured plans provide persistent global context.
- Mechanism: Fine-tuned GPT-2 converts requirements into JSON plans (Task Category, Dataset, Preprocess, Model Architecture) that ground generation.
- Core assumption: DL projects follow consistent patterns distillable into structured blueprints.
- Evidence anchors: Abstract states plan offers "global guidance"; Section III-B defines plan components; corpus supports refining intent improves generation.
- Break condition: Ambiguous requirements or novel architectures outside training data cause plan hallucinations.

### Mechanism 2: Dual-Path Retrieval Augmentation
- Claim: Single-source retrieval is noisy; effective synthesis requires separating concrete details from structural logic.
- Mechanism: Code RAG retrieves specific samples; Template RAG abstracts them into generalized frameworks.
- Core assumption: Concrete examples provide syntax but task-specific noise; abstract templates provide logic but lack specifics.
- Evidence anchors: Section III-C describes dual RAG paths; corpus confirms retrieval utility but lacks dual-path strategy.
- Break condition: Insufficient sample pool coverage leads to irrelevant retrieval and generic templates.

### Mechanism 3: Comparative Synthesis
- Claim: LLMs generate superior code when resolving tension between concrete example and abstract template.
- Mechanism: Generates two intermediate versions, then prompts comparator LLM with <Solution Plan, Code 1, Code 2> to synthesize final output.
- Core assumption: LLM can identify logical inconsistencies in Code 1 and implementation gaps in Code 2.
- Evidence anchors: Abstract mentions "comparative learning"; Section III-D describes triplet comparison; corpus lacks direct precedent.
- Break condition: Insufficient LLM reasoning capability results in simple copying or syntactic merging without logical resolution.

## Foundational Learning

- **Concept:** Retrieval-Augmented Generation (RAG)
  - Why needed: System relies on two distinct RAG pipelines (Code and Template); understanding BM25 is critical for debugging retrieval relevance.
  - Quick check: How does using predicted "Solution Plan" as BM25 query differ from raw user requirement?

- **Concept:** Sequence-to-Sequence (Seq2Seq) Fine-tuning
  - Why needed: "Solution Plan Predictor" is fine-tuned GPT-2, not prompted LLM; understanding training pair formatting is critical.
  - Quick check: Why choose smaller fine-tuned model (GPT-2) over prompting larger model (DeepSeek/GPT-4) directly?

- **Concept:** CodeBLEU Metrics
  - Why needed: Paper claims success based on CodeBLEU (syntax + data flow), not exact match.
  - Quick check: Why is "dataflow-match" more critical for DL projects than simple n-gram matching (BLEU)?

## Architecture Onboarding

- **Component map:** User Requirement -> Fine-tuned GPT-2 (Predicts Solution Plan JSON) -> BM25 Search (Input: Plan -> Output: Code Samples) -> LLM (Input: Samples -> Output: Template) -> LLM (Input: Plan + Sample Code + Template -> Output: Final Code)

- **Critical path:** Solution Plan Prediction is the linchpin. Wrong "Task Category" prediction causes retrieval and template generation drift.

- **Design tradeoffs:** Comparative prompt triples inference cost (Plan generation + 2 intermediate codes + Final synthesis) versus direct generation, trading latency for quality.

- **Failure signatures:**
  - Drift: Final code merges incompatible libraries (PyTorch + TensorFlow) if RAG sample differs in framework from template.
  - Hallucination: Plan Predictor creates layer structures not in sample pool, causing LLM to invent non-existent API calls.

- **First 3 experiments:**
  1. Unit Test the Planner: Feed DLPlanData test set into GPT-2 predictor, verify JSON schema validity and Task Category accuracy (>90% target).
  2. Retrieval Relevance Check: Manually inspect top-3 retrieved samples for sample query; check BM25 scores (< 0.5 indicates poor sample pool indexing).
  3. Temperature Sweep: Replicate temperature experiment (RQ5); start at 0.0, move to 1.5; verify comparative mechanism stabilizes higher-temperature outputs.

## Open Questions the Paper Calls Out

- **Open Question 1:** How does DLCodeGen perform regarding functional correctness (test pass rates) versus textual similarity metrics?
  - Basis: Paper acknowledges not using test pass rate metric, stating future work involves developing test-based benchmark.
  - Why unresolved: Current evaluation relies on CodeBLEU and human qualitative assessment rather than executable validation.
  - What evidence: Results on benchmark with executable test cases, like modified HumanEval for DL workflows.

- **Open Question 2:** To what extent does performance generalize to DL frameworks beyond TensorFlow/Keras?
  - Basis: Dataset construction explicitly filtered to TensorFlow/Keras, leaving PyTorch/JAX efficacy unverified.
  - Why unresolved: Solution plan predictor and sample pool constructed exclusively using TensorFlow/Keras data.
  - What evidence: Experimental results applying DLCodeGen to PyTorch projects without retraining plan predictor.

- **Open Question 3:** Can comparative learning mechanism be refined to improve "Helpfulness" to match or exceed direct retrieval-augmented baselines?
  - Basis: Section V.B reports DLCodeGen scored 6.20 in Helpfulness, marginally lower than CEDAR's 6.40.
  - Why unresolved: Comparative prompt may strip away "adaptable" real-world context found in direct retrieval samples.
  - What evidence: Modified comparative prompt preserving utility-oriented code blocks achieving significant Helpfulness score increase.

## Limitations

- Comparative synthesis mechanism lacks detailed prompt engineering, making it unclear whether gains stem from specific triplet structure or general multi-example prompting.
- Template abstraction process is underspecified—the paper claims an LLM abstracts retrieved code, but the prompt and abstraction rules are not provided.
- DLSamplePool curation criteria are vague ("manual review"), raising questions about reproducibility and generalizability.

## Confidence

- **High:** CodeBLEU and HumanEval results showing DLCodeGen outperforming baselines; ablation study (RQ3) provides strong evidence for plan-guided approach.
- **Medium:** Core claim that planning-guided generation resolves LLM context loss—intuitive and supported by ablation but lacks benchmarking against alternative context injection strategies.
- **Low:** Novelty of comparative synthesis mechanism—corpus contains no direct precedent, and mechanism's contribution is hard to isolate without exact prompt.

## Next Checks

1. **Prompt Isolation Test:** Replicate comparative generation using only <Solution Plan, Code 1> and <Solution Plan, Code 2> pairs. If performance drops significantly, triplet prompt is validated as key driver.

2. **Template Prompt Validation:** Implement simple template abstraction prompt (e.g., "Replace all specific dataset/model names with placeholders"). Compare results against paper's claims to test sensitivity to abstraction quality.

3. **Pool Generalization:** Replace 40-sample DLSamplePool with different set of 40 high-quality notebooks. If performance remains stable, approach is less reliant on specific sample pool curation.