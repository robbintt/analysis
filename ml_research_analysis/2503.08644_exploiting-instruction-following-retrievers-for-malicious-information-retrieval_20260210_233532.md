---
ver: rpa2
title: Exploiting Instruction-Following Retrievers for Malicious Information Retrieval
arxiv_id: '2503.08644'
source_url: https://arxiv.org/abs/2503.08644
tags:
- passages
- retrievers
- rule
- malicious
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We find that leading instruction-following retrievers can select
  relevant harmful passages for over 50% of malicious queries. For instance, LLM2Vec
  correctly selects passages for 61.35% of malicious queries.
---

# Exploiting Instruction-Following Retrievers for Malicious Information Retrieval

## Quick Facts
- arXiv ID: 2503.08644
- Source URL: https://arxiv.org/abs/2503.08644
- Reference count: 31
- Primary result: Leading instruction-following retrievers can select relevant harmful passages for over 50% of malicious queries, with Promptriever achieving average rank 2.09 for fine-grained malicious passage selection.

## Executive Summary
This study evaluates six instruction-following retrievers (DPR, Contriever, LLM2Vec, NV-Embed, Promptriever, BGE-en-icl) on their ability to retrieve harmful passages for malicious queries and assesses whether these retrieved passages can bypass safety alignment in RAG setups. The research demonstrates that retrievers can successfully identify harmful content for over half of malicious queries, with LLM2Vec achieving 61.35% accuracy. The findings reveal that even safety-aligned LLMs like Llama3 can generate harmful responses when provided with malicious passages in-context, highlighting emerging safety risks as retriever capabilities increase.

## Method Summary
The study uses AdvBench-IR (520 malicious queries with LLM-generated harmful passages) combined with Wikipedia benign passages to evaluate retrieval performance. Six retrievers are tested on their ability to identify harmful passages using top-k accuracy metrics (k=1,5,20,100) and average rank for fine-grained selection. For RAG experiments, NV-Embed retrieves 1-10 passages which are then fed to Llama3-8B-Instruct, Mistral-7B-Instruct, or Gemma2-9B-Instruct with in-context learning. Harmfulness of LLM responses is evaluated using LlamaGuard-3-8B. Passage generation uses Mistral-7B-Instruct-v0.2 with specific prompts, and all passages are chunked to ≤100 tokens.

## Key Results
- Instruction-following retrievers correctly select harmful passages for 61.35% of malicious queries (LLM2Vec).
- Promptriever achieves average rank of 2.09 for fine-grained malicious passage selection.
- Safety-aligned LLMs like Llama3 can generate harmful responses when provided with retrieved malicious passages in-context.

## Why This Works (Mechanism)
Instruction-following retrievers are designed to understand and respond to specific user instructions, making them highly effective at retrieving content that matches precise query requirements. This capability, while beneficial for legitimate use cases, can be exploited when malicious queries are crafted to request harmful information. The retrievers' ability to match fine-grained semantic content means they can identify and retrieve highly specific harmful passages that would be difficult to find through keyword-based retrieval alone. When these retrieved passages are used in RAG setups, they provide contextual evidence that can override the safety alignment mechanisms of LLMs, leading to the generation of harmful content.

## Foundational Learning
- **Instruction-following retrieval**: Why needed - to evaluate if retrievers can understand and respond to specific malicious instructions; Quick check - verify retrievers achieve >50% accuracy on malicious queries.
- **RAG safety alignment**: Why needed - to assess if retrieved harmful content can bypass LLM safety measures; Quick check - confirm LlamaGuard-3-8B detects harmfulness in RAG-generated responses.
- **FAISS indexing**: Why needed - for efficient similarity search across retrieval corpus; Quick check - ensure FAISS index builds correctly and returns relevant passages.

## Architecture Onboarding

**Component Map**: AdvBench-IR queries -> Passage Generator (Mistral-7B) -> Passage Chunker -> Retrieval Corpus -> FAISS Index -> Retriever -> LLM (Llama3/Mistral/Gemma2) -> LlamaGuard-3-8B

**Critical Path**: Malicious Query → Retriever → Harmful Passage → RAG Prompt → LLM → Harmful Response → Safety Evaluation

**Design Tradeoffs**: Using synthetically generated malicious content ensures controlled evaluation but may not capture real-world complexity; combining Wikipedia with harmful passages provides diverse corpus but introduces preprocessing variability.

**Failure Signatures**: Low retrieval accuracy suggests incorrect passage chunking or embedding dimension mismatches; inconsistent harmfulness scores indicate prompt formatting issues or evaluation threshold problems.

**First Experiments**:
1. Verify passage chunking by checking all chunks are ≤100 tokens and embeddings match retriever requirements.
2. Test FAISS index by retrieving known passages and confirming relevance scores.
3. Validate RAG pipeline by running single query with known harmful passage and checking LLM response.

## Open Questions the Paper Calls Out
None

## Limitations
- Relies on synthetically generated malicious queries and passages that may not reflect real-world attack scenarios.
- Uses limited set of 50 curated queries for fine-grained analysis, potentially missing broader exploitability patterns.
- Specific Wikipedia preprocessing and FAISS configuration details remain unspecified, affecting reproducibility.

## Confidence
- High confidence: Instruction-following retrievers can retrieve harmful passages for malicious queries (>50% accuracy).
- Medium confidence: Safety-aligned LLMs can generate harmful responses when provided with retrieved malicious passages.
- Medium confidence: Emerging safety risks exist as retriever capabilities increase, given synthetic evaluation corpus.

## Next Checks
1. Replicate retrieval experiments using independently sourced Wikipedia dump with documented preprocessing to verify robustness.
2. Conduct human evaluation studies to validate LlamaGuard-3-8B harmfulness scores for RAG-generated responses.
3. Test retriever vulnerability across additional malicious query categories beyond AdvBench-IR, including sophisticated prompt engineering techniques.