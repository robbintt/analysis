---
ver: rpa2
title: Decomposing Attention To Find Context-Sensitive Neurons
arxiv_id: '2510.03315'
source_url: https://arxiv.org/abs/2510.03315
tags:
- token
- contributions
- neuron
- explanation
- like
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method to decompose attention heads in transformer
  models into positional and content-dependent components, focusing on "stable" heads
  with broad attention patterns. By sampling softmax denominators from a representative
  calibration text, the authors approximate the combined output of these heads as
  a linear summary of the surrounding text.
---

# Decomposing Attention To Find Context-Sensitive Neurons

## Quick Facts
- **arXiv ID**: 2510.03315
- **Source URL**: https://arxiv.org/abs/2510.03315
- **Reference count**: 38
- **Primary result**: Method discovers context-sensitive neurons in GPT-2's first layer with median r=0.95 correlation between approximation and ground-truth contributions

## Executive Summary
This paper presents a method to identify context-sensitive neurons in transformer models by decomposing first-layer attention heads into positional and content-dependent components. The approach focuses on "stable" heads with broad attention patterns and approximates their combined output as a linear summary of surrounding text. By sampling softmax denominators from a calibration text, the method can discover hundreds of first-layer neurons that respond to high-level contextual properties like writing style or domain without requiring large-scale corpus activations. The contextual circuit approximation achieves strong accuracy (median correlation 0.9485) in identifying these neurons.

## Method Summary
The method identifies context-sensitive neurons by first isolating stable attention heads in GPT-2's first layer that exhibit slowly-decaying positional kernels and weak content dependence. For these heads, softmax denominators are sampled from a calibration text to approximate the combined OV circuit output as a linear function of surrounding tokens. Each neuron's contribution is computed by combining token-specific content terms with the sampled denominators, creating a "contextual circuit" that summarizes how surrounding text influences the neuron. Neurons are then ranked by the variance of their token contributions, with high-variance neurons identified as context-sensitive.

## Key Results
- Contextual circuit approximation achieves median correlation of 0.9485 with true neuron activations
- Median Fraction of Variance Unexplained (FVU) of 0.1426 across 1000 texts
- Method discovers hundreds of first-layer neurons sensitive to high-level contextual properties
- Systematic underestimation occurs for texts with abnormal stop word density (code, Biblical text)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Attention patterns in certain first-layer heads can be decomposed into stable positional kernels plus token-dependent adjustments, with ~5% total variation error.
- **Mechanism**: LayerNorm complicates direct decomposition, so keys are approximated as: `key[i] ≈ E[n, xi] + P[n, i]`, where E captures content and P captures pure positional bias. The positional kernel `pos_n,i,xn` is the softmax of query·P, representing intrinsic positional preference independent of content.
- **Core assumption**: Token embeddings and positional embeddings are approximately orthogonal (cosine similarity <0.15), and |WE[t] + Wpos[i]| varies slowly with i.
- **Evidence anchors**:
  - [abstract]: "decomposition of first-layer attention into positional and content-dependent components"
  - [section 2.1]: "typical TV distance observed between the true attention pattern and our approximate attention pattern is 0.05"
  - [corpus]: Weak direct corpus support; neighbor papers focus on different decomposition approaches (e.g., "Neuron-Attention Decomposition" in CLIP-ResNet, FMR=0.0).
- **Break condition**: Heads where attention scores depend strongly on content, or where positional kernels are narrow/local, will not decompose cleanly.

### Mechanism 2
- **Claim**: Softmax denominators of "slowly-decaying" heads concentrate around their expected value when the underlying token distribution is fixed, enabling approximation as constants.
- **Mechanism**: The positionally-normalized denominator Σ(pos_i × content_xi) behaves like a weighted sum over i.i.d. random variables (tokens). By Hoeffding/Chebyshev, concentration is governed by Σ(pos_i)², which is small when positional kernels are spread out. Heads 0, 2, 6, 8, 9, 10 in GPT-2 Small layer 0 exhibit this property.
- **Core assumption**: Tokens can be modeled as drawn i.i.d. from an underlying distribution; stop word density is roughly consistent across typical prose.
- **Evidence anchors**:
  - [abstract]: "softmax denominators of these heads are stable when the underlying token distribution is fixed"
  - [section 3.2, Figure 3-4]: Shows denom_h,n,'the' concentrating across positions and across 1000 OpenWebText samples, though programming texts (atypical stop word density) show outliers.
  - [corpus]: No direct corpus validation; neighboring work doesn't address denominator stability.
- **Break condition**: Texts with highly atypical token distributions (e.g., code, which has unusual stop word density) violate the i.i.d. assumption; different calibration texts would be needed per domain.

### Mechanism 3
- **Claim**: The combined output of stable heads can be approximated as a "contextual circuit"—a linear summary of surrounding tokens weighted by a shared positional kernel and token-specific contribution functions.
- **Mechanism**: Combine OV circuits from multiple heads using a median positional kernel (justified by similarity across slowly-decaying heads). The contribution function `contribution[j, xi]` depends only on token identity and sampled denominators from a calibration text. This yields Equation 11: `Σ_i pos_i × contribution[j, xi]`.
- **Core assumption**: Head-specific positional kernels are similar enough to approximate by a single median kernel; denominators from calibration text generalize to new texts.
- **Evidence anchors**:
  - [abstract]: "median r=0.95" correlation between approximation and ground-truth contributions
  - [section 6]: "median FVU of 0.1426" across context-sensitive neurons on 1000 random texts
  - [corpus]: Related work on "linear representations" (FMR=0.0) suggests linear structure in representations broadly, but no direct validation of this specific circuit.
- **Break condition**: When calibration text token distribution differs significantly from target text (e.g., using prose calibration for code analysis), systematic bias emerges (see Figure 5b showing underestimation for Biblical text with abnormal stop word density).

## Foundational Learning

- **Concept: LayerNorm normalization**
  - **Why needed here**: The decomposition requires handling how LayerNorm mixes position and content non-linearly; the approximation strategy (splitting into E and P terms with different denominator treatments) is the technical crux.
  - **Quick check question**: If LayerNorm gain≠1 or bias≠0, can you still apply this decomposition? (Yes—fold them into surrounding weights.)

- **Concept: Concentration inequalities (Hoeffding/Chebyshev)**
  - **Why needed here**: The theoretical justification for denominator stability relies on treating softmax denominators as weighted sums of i.i.d. random variables; understanding when concentration holds (and when it doesn't) is essential for knowing when the method applies.
  - **Quick check question**: What property of the positional kernel makes concentration tighter? (More spread out kernels → smaller Σ(pos_i)² → tighter bounds.)

- **Concept: OV circuits from Mathematical Framework for Transformer Circuits**
  - **Why needed here**: The contextual circuit builds on Elhage et al.'s OV circuit formulation—understanding how attention heads contribute outputs through value-weight matrices is prerequisite to combining them.
  - **Quick check question**: In the OV circuit, what does V^Oh(i, xi) represent? (The output vector contribution of token xi through head h.)

## Architecture Onboarding

- **Component map**:
  - First attention layer (L0) -> 6 "stable" heads (0, 2, 6, 8, 9, 10) -> MLP layer 0 -> context-sensitive neurons (identified by high |contribution| variance across vocabulary)
  - Positional kernels: computed per-head from weights + calibration text
  - Contribution function: `contribution[j, t]` combines OV projection, content term, and sampled denominator

- **Critical path**:
  1. Identify stable heads (broad kernels, weak content dependence)
  2. Compute positional kernels via Equation 2
  3. Sample denominators from calibration text (e.g., typical OpenWebText sample)
  4. Compute contribution[j, t] for each neuron j and token t
  5. Rank neurons by top[j] (20th largest |contribution|)
  6. Validate by comparing circuit prediction to ground-truth OV contributions

- **Design tradeoffs**:
  - Calibration text choice: Prose (generalizable to typical text) vs. domain-specific (better precision within domain, worse transfer)
  - Threshold θ for neuron selection: Lower -> more neurons but more noise; Higher -> cleaner but fewer discoveries
  - Head selection: More heads -> richer context summary but harder to approximate with single kernel

- **Failure signatures**:
  - Systematic underestimation of contributions on texts with abnormal token distributions (e.g., code, Biblical text)
  - Low correlation (r<<0.95) between approximation and ground truth -> check if calibration text matches target domain
  - Neurons with "glitch tokens" in top contributions -> may be artifacts rather than genuine context features

- **First 3 experiments**:
  1. **Validate decomposition accuracy**: Compute TV distance between true and approximate attention patterns on 100 random texts; confirm median TV~0.05 for candidate stable heads.
  2. **Test denominator stability**: Plot denom_h,n,'the' across positions for diverse texts (prose, code, dialogue); identify which texts cause outliers and why.
  3. **Discover and verify context-sensitive neurons**: Apply full pipeline with OpenWebText calibration; for top-discovered neurons, manually inspect token contributions and test on held-out texts from predicted domains (e.g., test UK English neuron on held-out British vs. American text).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does the decomposition of attention into positional kernels and content-dependent components remain mechanistically valid in layers deeper than the first?
- **Basis in paper**: [explicit] The Conclusion states, "Future work could extend this decomposition to deeper layers and explore how models with alternative positional schemes... perform similar tasks."
- **Why unresolved**: The current derivation relies on the specific structure of the first layer (e.g., token embeddings directly adding to positional embeddings), and it is unclear if the LayerNorm approximation (Eq 1) holds when inputs are superpositions of previous layer outputs rather than discrete tokens.
- **What evidence would resolve it**: Applying the key approximation (Eq 1) to middle-layer attention heads and measuring the Total Variation distance between the reconstructed and true attention patterns to see if it remains low (~0.05).

### Open Question 2
- **Question**: Can the contextual circuit approximation be adapted for models utilizing Rotary Positional Embeddings (RoPE)?
- **Basis in paper**: [explicit] Appendix C notes that "RoPE models are significantly more complex to analyze" and "it's not clear that something of this form doesn't happen, and so analysis of RoPE models is complicated."
- **Why unresolved**: The method assumes additive positional embeddings where a static "positional kernel" can be isolated. RoPE applies rotations based on relative distance, which might violate the assumption that a head's function is position-independent, making the "stable denominator" argument inapplicable.
- **What evidence would resolve it**: A theoretical derivation defining a "positional kernel" equivalent for RoPE, followed by empirical validation showing that softmax denominators concentrate for specific RoPE heads in models like Pythia.

### Open Question 3
- **Question**: What is the mechanistic cause of the systematic bias observed between the contextual circuit approximation and the ground-truth attention contributions?
- **Basis in paper**: [explicit] Appendix G notes, "There is sometimes a small systematic bias, which we don't currently have an explanation for," despite high correlation (median r=0.9485).
- **Why unresolved**: While the paper validates that the approximation tracks the *relative* magnitude of contributions well, the mathematical approximations (specifically the LayerNorm key decomposition or the single median kernel for multiple heads) introduce a consistent offset that has not been isolated.
- **What evidence would resolve it**: Ablation studies comparing the bias when using exact keys vs. approximate keys (Eq 1), or exact denominators vs. calibration denominators, to identify which simplification introduces the offset.

## Limitations

- **Domain Transferability**: Method's effectiveness depends critically on calibration text matching target text distributions; systematic underestimation occurs for texts with abnormal stop word density
- **Head Selection Criteria**: Paper identifies 6 stable heads but doesn't provide systematic methodology for finding such heads across different models or layers
- **Linear Approximation Validity**: Approximation leaves ~5% variance unexplained; paper doesn't thoroughly investigate breakdown cases or characterize non-linear context features

## Confidence

**High Confidence**: Mathematical framework for decomposing attention into positional and content components is sound, with clear theoretical grounding in concentration inequalities. Experimental validation shows r=0.95 correlation and FVU=0.14 on 1000 OpenWebText samples demonstrates method works well for typical prose.

**Medium Confidence**: Performance on domain-specific texts (code, Biblical text) is less certain, with explicit acknowledgment of systematic underestimation. Paper provides theoretical justification but doesn't fully validate proposed solutions like domain-specific calibration.

**Low Confidence**: Systematic process for identifying "stable" heads is underdeveloped. Paper identifies specific heads that work well but doesn't provide general methodology for finding such heads in other models or layers.

## Next Checks

1. **Domain-Specific Calibration Validation**: Test method on programming texts using both general calibration (OpenWebText) and code-specific calibration texts. Measure whether domain-specific calibration eliminates systematic underestimation observed in Figure 5b, and quantify any trade-offs in performance on non-code texts.

2. **Head Selection Robustness**: Systematically evaluate how method's performance varies when selecting different sets of stable heads. Test whether including heads with narrower positional kernels or stronger content dependence improves or degrades context-sensitive neuron discovery, and establish quantitative criteria for head selection.

3. **Cross-Layer Applicability**: Apply contextual circuit approximation to second-layer MLP neurons and compare performance to first-layer results. Investigate whether same stable heads remain optimal at deeper layers, and whether additional attention heads become "stable" as context processing becomes more complex.