---
ver: rpa2
title: Cross-Modal Fine-Tuning of 3D Convolutional Foundation Models for ADHD Classification
  with Low-Rank Adaptation
arxiv_id: '2511.06163'
source_url: https://arxiv.org/abs/2511.06163
tags:
- adhd
- lora
- convolutional
- classification
- low-rank
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of early diagnosis of ADHD in
  children using neuroimaging data, which is complicated by heterogeneous presentations
  and symptom overlap with other conditions. The authors propose a novel parameter-efficient
  transfer learning approach that adapts a large-scale 3D convolutional foundation
  model, pre-trained on CT images, to an MRI-based ADHD classification task.
---

# Cross-Modal Fine-Tuning of 3D Convolutional Foundation Models for ADHD Classification with Low-Rank Adaptation

## Quick Facts
- **arXiv ID:** 2511.06163
- **Source URL:** https://arxiv.org/abs/2511.06163
- **Reference count:** 0
- **Primary result:** Achieved 71.9% accuracy and 0.716 AUC for ADHD classification using 113x fewer parameters through cross-modal CT-to-MRI transfer learning

## Executive Summary
This study addresses the challenge of early ADHD diagnosis in children using neuroimaging data by proposing a novel parameter-efficient transfer learning approach. The authors adapt a large-scale 3D convolutional foundation model, pre-trained on CT images, to an MRI-based ADHD classification task using Low-Rank Adaptation (LoRA). Their method factorizes 3D convolutional kernels into 2D low-rank updates, dramatically reducing trainable parameters while achieving state-of-the-art results. In five-fold cross-validation on a public diffusion MRI database, their 3D LoRA fine-tuning strategy achieved superior performance with only 1.64 million trainable parameters compared to the full model's 185.57 million parameters.

## Method Summary
The approach uses a frozen 3D ResNet-50 backbone pre-trained on CT lesion volumes, with LoRA modules (rank r=4) injected into every 3D convolutional layer. The LoRA update factorizes weight matrices as W' = W + BA, where B and A are low-rank matrices. The input consists of 2-channel 3D tensors (FA/MD maps) resized to 128×128×128 from diffusion MRI data. Training uses distinct learning rates (10^-4 for LoRA, 10^-5 for head) with BCE loss and AdamW optimizer. Two model variants were trained: one maximizing validation accuracy (71.9%) and another maximizing validation AUC (0.716).

## Key Results
- Achieved 71.9% accuracy and 0.630 AUC using LoRA variant optimized for accuracy
- Achieved 0.716 AUC and 0.630 accuracy using LoRA variant optimized for AUC
- Used only 1.64 million trainable parameters (113x fewer than fully fine-tuned model)
- Established one of the first successful cross-modal (CT-to-MRI) adaptations of a foundation model in neuroimaging

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** 3D convolutional adaptation can be achieved efficiently by factorizing kernel updates into low-rank matrices acting on flattened spatial dimensions
- **Mechanism:** The authors reshape the 4D kernel tensor W ∈ R^{d_out × d_in · k^3} and apply a standard Low-Rank Adaptation (LoRA) update ΔW = BA, where B and A are low-rank matrices (r=4)
- **Core assumption:** The task-specific adaptation required for ADHD classification resides in a low-dimensional subspace of the model's parameter space
- **Evidence anchors:** [Section 2.2] "W' = W + ΔW = W + BA, with B ∈ R^{d_out × r} and A ∈ R^{r × d_in · k^3}."
- **Break condition:** If the rank r=4 is insufficient to capture the complexity of the neurological features, the model will underfit compared to full fine-tuning

### Mechanism 2
- **Claim:** Visual features learned from Computed Tomography (CT) volumes contain transferable, modality-agnostic anatomical representations applicable to MRI
- **Mechanism:** The frozen 3D ResNet-50 backbone, pre-trained on CT lesions, acts as a generic edge and texture detector. The LoRA modules serve as a "modality translator," adjusting the activation thresholds of these detectors to align MRI diffusion maps with the backbone's expected distribution
- **Core assumption:** There is a structural correlation between the high-frequency edges in CT scans and the white matter boundaries visible in diffusion tensor imaging
- **Evidence anchors:** [Abstract] "cross-modal (CT-to-MRI) adaptations of a foundation model in neuroimaging."
- **Break condition:** If the pre-training domain (CT lesions) relies heavily on density properties (Hounsfield units) that have no structural analog in diffusion MRI, transfer learning will fail

### Mechanism 3
- **Claim:** Restricting trainable parameters to a low-rank subspace acts as an effective regularizer for small medical datasets
- **Mechanism:** By freezing the bulk of the 185.57M parameters and training only 1.64M (LoRA + Head), the model drastically reduces its capacity to memorize noise
- **Core assumption:** The pre-trained weights already contain a sufficiently good initialization for the target task
- **Evidence anchors:** [Table 1] Fully fine-tuned FM (185.57M params) achieves lower or comparable accuracy vs. the LoRA version
- **Break condition:** If the dataset size scales up significantly, the low-rank constraint might become a bottleneck

## Foundational Learning

- **Concept: Diffusion Tensor Imaging (DTI) Metrics**
  - **Why needed here:** The inputs are derived scalar maps (FA and MD) rather than raw images. Understanding FA (Fractional Anisotropy) measures white matter coherence helps explain why a structural detector (CT backbone) might transfer well
  - **Quick check question:** Do you understand why the input has 2 channels (FA and MD) rather than 1 (raw MRI) or 3 (RGB)?

- **Concept: Low-Rank Adaptation (LoRA)**
  - **Why needed here:** This is the core technical contribution. One must grasp that W is frozen and BA is trained to understand the efficiency gains
  - **Quick check question:** If a weight matrix W is 100x100 and rank r=4, how many parameters are in A and B combined compared to W?

- **Concept: Transfer Learning vs. Domain Adaptation**
  - **Why needed here:** This paper pushes the boundary of standard transfer learning by crossing modalities (CT to MRI)
  - **Quick check question:** Why might standard transfer learning fail when the source (CT) and target (MRI) have different intensity distributions?

## Architecture Onboarding

- **Component map:** Input (2-channel 3D tensor) -> Frozen 3D ResNet-50 backbone -> LoRA adapters (rank r=4) -> 2-layer MLP head (128 hidden units) -> Binary classification output

- **Critical path:**
  1. Preprocessing via QSIPrep pipeline is essential for generating the FA/MD maps
  2. LoRA injection requires precise reshaping of the 3D kernel (d_out × d_in · k^3)
  3. Optimization uses distinct learning rates (10^-4 for LoRA, 10^-5 for head)

- **Design tradeoffs:**
  - Rank r=4 provides aggressive compression (113x reduction) but may limit learning complex features
  - Frozen backbone maximizes memory efficiency but prevents unlearning CT-specific biases
  - ACC vs. AUC selection optimizes for different clinical needs (diagnosis vs. risk ranking)

- **Failure signatures:**
  - Modality mismatch: Convergence to 50% accuracy indicates CT features didn't transfer
  - Overfitting: Training loss drops to zero while validation loss increases
  - NaN loss: Check input normalization; FA/MD maps must be properly scaled

- **First 3 experiments:**
  1. Baseline Probe: Train only the MLP head with backbone completely frozen (no LoRA)
  2. Rank Ablation: Compare performance of r ∈ {1, 4, 8, 16} to verify r=4 is optimal
  3. Modality Ablation: Initialize with random weights (no CT pre-training) and train with LoRA

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the 3D LoRA fine-tuning approach generalize to larger, multi-site ADHD cohorts with diverse imaging protocols and population demographics?
- Basis in paper: [explicit] The authors state: "validating the approach on larger, multi-site cohorts is needed to ensure generalizability across diverse populations and imaging conditions."
- Why unresolved: The study used a single dataset (N=129) from one source with uniform preprocessing, leaving site-specific variability untested
- What evidence would resolve it: Evaluation on independent multi-site ADHD datasets demonstrating consistent accuracy across sites

### Open Question 2
- Question: What is the optimal rank (r) for 3D LoRA adaptation in neuroimaging tasks?
- Basis in paper: [inferred] The authors fixed rank r=4 without ablation or justification
- Why unresolved: No experiments varied the rank parameter, so sensitivity to rank selection remains unknown
- What evidence would resolve it: Systematic ablation experiments testing multiple rank values with performance comparisons

### Open Question 3
- Question: What specific learned representations enable successful CT-to-MRI cross-modal transfer?
- Basis in paper: [inferred] The authors hypothesize that "FMs pre-trained on diverse medical images learn fundamental, modality-agnostic representations," but no analysis validates what features actually transfer
- Why unresolved: The paper demonstrates successful transfer empirically but does not include feature visualization or activation analysis
- What evidence would resolve it: Feature attribution analysis comparing CT pre-trained features against MRI-specific patterns

### Open Question 4
- Question: Would integrating additional MRI modalities (structural MRI, resting-state fMRI) with diffusion MRI features improve ADHD classification performance?
- Basis in paper: [inferred] The introduction cites prior work using "structural MRI, resting-state fMRI, and diffusion MRI features," but the method uses only FA and MD maps
- Why unresolved: The study demonstrates feasibility with two diffusion-derived channels but does not explore whether complementary modalities could enhance discriminative power
- What evidence would resolve it: Experiments adding sMRI-derived volumes or rs-fMRI connectivity features as additional input channels

## Limitations
- The study lacks ablation experiments comparing cross-modal transfer versus same-modality transfer baselines
- Performance metrics (71.9% accuracy, 0.716 AUC) are moderate and not dramatically better than simpler approaches
- Generalizability to other neurological conditions or larger datasets remains untested
- No empirical validation of the theoretical basis for successful CT-to-MRI transfer

## Confidence
- **Efficiency claims:** High - Well-supported by parameter counts and clear implementation
- **Cross-modal transfer mechanism:** Low-Medium - Theoretical justification exists but lacks empirical validation
- **Performance gains:** Medium - Results are positive but not dramatically superior to existing approaches

## Next Checks
1. Run an ablation comparing LoRA with random initialization (no CT pre-training) versus CT-pretrained weights to isolate the transfer learning contribution
2. Implement a same-modality transfer baseline (MRI-to-MRI) to determine if cross-modal transfer provides advantages over standard approaches
3. Test the model on a held-out test set separate from the 5-fold cross-validation to assess generalization beyond the training folds