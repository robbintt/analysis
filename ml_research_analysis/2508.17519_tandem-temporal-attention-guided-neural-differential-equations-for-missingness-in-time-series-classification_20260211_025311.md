---
ver: rpa2
title: 'TANDEM: Temporal Attention-guided Neural Differential Equations for Missingness
  in Time Series Classification'
arxiv_id: '2508.17519'
source_url: https://arxiv.org/abs/2508.17519
tags:
- neural
- tandem
- time
- series
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces TANDEM, a novel attention-guided neural
  differential equation framework for time series classification with missing data.
  The key innovation is an adaptive fusion mechanism that integrates three complementary
  feature representations: raw observations, interpolated control paths, and continuous
  latent dynamics from neural differential equations.'
---

# TANDEM: Temporal Attention-guided Neural Differential Equations for Missingness in Time Series Classification

## Quick Facts
- arXiv ID: 2508.17519
- Source URL: https://arxiv.org/abs/2508.17519
- Reference count: 40
- Outperforms state-of-the-art methods on 30 benchmark datasets with 70% missing data rates

## Executive Summary
TANDEM is a novel framework for time series classification with missing data that integrates three complementary feature streams: raw observations, interpolated control paths, and continuous latent dynamics from neural differential equations. The framework uses Gumbel-Sigmoid gating with multi-head attention to adaptively weight these streams based on their informativeness under varying missingness conditions. Tested on 30 benchmark datasets and a real-world medical dataset, TANDEM demonstrates superior classification accuracy compared to state-of-the-art methods while maintaining robust performance even with high missing rates.

## Method Summary
TANDEM processes time series by extracting three distinct representations: raw observations with missingness masks, cubic spline-interpolated control paths providing continuous trends, and latent dynamics modeled via neural differential equations (ODE/CDE/SDE). These streams are refined through feature-wise multi-head attention before being fused using Gumbel-Sigmoid gating, which enables independent selection of each stream. The fused representation is then classified using a 2-layer MLP. The framework is trained end-to-end with Adam optimizer and cross-entropy loss, with hyperparameters tuned via Ray.

## Key Results
- Achieves average accuracy of 0.742 across all missingness levels on 30 benchmark datasets
- Demonstrates 1.2-1.4% improvement over Neural ODEs, CDEs, and SDEs with Softmax gating
- Maintains robust performance with 70% missing data rates, outperforming state-of-the-art methods
- Shows significant statistical improvement over baselines (p<0.05) on real-world PhysioNet Sepsis dataset

## Why This Works (Mechanism)

### Mechanism 1
Integrating raw observations, interpolated paths, and latent dynamics provides complementary signals that single-stream models lack, specifically handling varying missingness patterns. The framework extracts three distinct representations: (1) Raw $\tilde{x}(t)$ preserves observed values and missingness masks; (2) Control Path $X(t)$ provides a smooth, continuous global trend via cubic spline interpolation; (3) Latent Dynamics $z(t)$ captures continuous-time system evolution via an NDE backbone (ODE/CDE/SDE). These are fused so that if one stream is degraded (e.g., raw data is 70% missing), the others compensate.

### Mechanism 2
Gumbel-Sigmoid gating enables adaptive, near-discrete selection of feature streams, allowing the model to "turn off" unreliable streams under specific missingness conditions. Unlike Softmax which forces competition (sum to 1), Gumbel-Sigmoid allows independent Bernoulli-like selection for each stream. The gate $\sigma_k$ is computed using logits plus Gumbel noise. This permits the model to rely *solely* on the NDE latent state if raw data is missing, or combine all three if data is complete.

### Mechanism 3
Multi-head attention refines the internal representation of each stream before fusion, weighting dimensions by relevance at each time step. Before fusion, multi-head attention is applied independently to the raw, path, and latent streams. This computes attention weights relative to the temporal context, enhancing salient features and suppressing noise within the dimensionality of each stream.

## Foundational Learning

- **Neural Differential Equations (NDEs)**: Why needed here - TANDEM relies on an NDE backbone (ODE, CDE, or SDE) to model the "continuous latent dynamics." Quick check - Can you explain the difference between a standard RNN update and a Neural ODE update regarding how they handle time steps?

- **Gumbel-Softmax/Sigmoid Trick**: Why needed here - The core fusion mechanism uses Gumbel-Sigmoid gates. Quick check - Why would a standard Softmax gate be insufficient if the model needs to select *both* the raw observation and the latent dynamics simultaneously while excluding the control path?

- **Riemann-Stieltjes Integral (for CDEs)**: Why needed here - The paper lists Neural CDEs as a backbone option. Quick check - How does the integration in a Controlled Differential Equation (CDE) change when the input path $X(t)$ is constant versus when it is changing?

## Architecture Onboarding

- **Component map**: Raw Observations + Missingness Mask -> Raw Stream; Cubic Spline Interpolation -> Control Path; NDE Solver -> Latent Dynamics; Multi-Head Attention (per stream) -> Refined Streams; Gumbel-Sigmoid Gating -> Fused Representation -> 2-Layer MLP Classifier

- **Critical path**: The dependency between the Control Path and the Latent Stream (in CDE/SDE modes) is critical. The interpolation quality directly affects the NDE's dynamics. The Fusion stage is the second critical point where gradient flow depends on the Gumbel temperature.

- **Design tradeoffs**: Modularity vs. Complexity - The framework supports ODE/CDE/SDE backbones (modular) but requires implementing specific solvers and attention wrappers for each (complexity). Performance vs. Runtime - Figure 2 shows TANDEM achieves high accuracy but has significantly higher runtime per epoch compared to naÃ¯ve NDEs due to the attention and gating overhead.

- **Failure signatures**: Gate Collapse - All gate values ($\sigma$) converge to 0.5 or 1.0, indicating the gating mechanism failed to learn differentiation. NDE Divergence - If the ODE solver steps are too large or the vector field is unstable, the latent state $z(t)$ explodes. Interpolation Artifacts - With >70% missingness, spline interpolation may create unrealistic "wiggles" or trends that mislead the NDE.

- **First 3 experiments**: Ablation Validation - Run TANDEM on a single dataset with and without the attention module. Missingness Stress Test - Train on a dataset with 0% missing, but evaluate on 30%, 50%, 70% missing. Gate Inspection - Monitor the values of $\sigma_{\tilde{x}}, \sigma_X, \sigma_z$ during training on a dataset with structured missingness.

## Open Questions the Paper Calls Out
1. How can the computational overhead of the TANDEM framework be mitigated to enable application on large-scale datasets?
2. How does TANDEM's performance vary under non-random missingness mechanisms such as Missing Not at Random (MNAR)?
3. Can the multi-stream attention-fusion strategy be effectively adapted for time series forecasting or regression tasks?

## Limitations
- Computational complexity is significantly higher than baseline methods, limiting scalability to large datasets
- Evaluation primarily focuses on classification accuracy without comprehensive cost-benefit analysis of runtime overhead
- Limited analysis of gating behavior across different missingness patterns and real-world MNAR scenarios

## Confidence
- **High Confidence**: The core mechanism of fusing three complementary feature streams is well-supported by architecture description and ablation results
- **Medium Confidence**: The claim that Gumbel-Sigmoid gating provides superior adaptive selection compared to Softmax alternatives is supported by ablation studies
- **Low Confidence**: The assertion that multi-head attention significantly enhances performance lacks direct comparison to single-head attention or other variants

## Next Checks
1. **Runtime Efficiency Analysis**: Conduct experiments measuring classification accuracy versus computational cost across different dataset sizes
2. **Backbone Sensitivity Study**: Systematically evaluate ODE, CDE, and SDE variants across datasets with different characteristics
3. **Gating Behavior Analysis**: Track gate values during training across different missingness rates and patterns to verify adaptive selection mechanism