---
ver: rpa2
title: 'LLEXICORP: End-user Explainability of Convolutional Neural Networks'
arxiv_id: '2511.02720'
source_url: https://arxiv.org/abs/2511.02720
tags:
- concept
- image
- pattern
- concepts
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LLEXICORP, a modular pipeline that integrates
  Concept Relevance Propagation (CRP) with a multimodal large language model to automatically
  generate human-understandable explanations for CNN predictions. The method addresses
  the manual and expert-dependent nature of existing CRP workflows by using LLM prompts
  to automatically name discovered concepts and contextualize their relevance in predictions.
---

# LLEXICORP: End-user Explainability of Convolutional Neural Networks

## Quick Facts
- **arXiv ID:** 2511.02720
- **Source URL:** https://arxiv.org/abs/2511.02720
- **Reference count:** 11
- **One-line result:** Automated CNN explanation pipeline using LLM to name CRP concepts, validated by user study (78% agreement on meaningful patterns)

## Executive Summary
This paper introduces LLEXICORP, a modular pipeline that integrates Concept Relevance Propagation (CRP) with a multimodal large language model to automatically generate human-understandable explanations for CNN predictions. The method addresses the manual and expert-dependent nature of existing CRP workflows by using LLM prompts to automatically name discovered concepts and contextualize their relevance in predictions. The framework supports both technical and non-technical audiences. Evaluation on 8 ImageNet images via a questionnaire with 21 participants showed that participants consistently recognized meaningful visual patterns (78% agreement) and found the final summaries helpful (75%). The most influential concept performed exceptionally well, with over 90% agreement on reasonable presence and usefulness. The study demonstrates that LLM-enhanced CRP can significantly lower the barrier to interpreting deep neural networks.

## Method Summary
The LLEXICORP pipeline automates the interpretation of CNN predictions by combining Concept Relevance Propagation with LLM-driven concept labeling and explanation generation. For a given input image and predicted class, CRP identifies the most relevant convolutional channels, retrieves representative images that activate each channel, and generates saliency heatmaps. An LLM then names the common visual pattern across representative images, explains how that pattern appears in the original image using the heatmap, and synthesizes individual concept explanations into a final summary. The method uses a three-stage prompt engineering approach with GPT-4o to ensure faithfulness and reduce hallucination, separating concept labeling from contextualization and summarization.

## Key Results
- Participants consistently recognized meaningful visual patterns in representative images (78% agreement)
- Final summaries were found helpful by 75% of participants
- The most influential concept achieved over 90% agreement on reasonable presence and usefulness
- Users strongly agreed (83%) that summaries contained only information present in individual concept descriptions
- The framework successfully bridges technical CRP outputs with end-user understandable explanations

## Why This Works (Mechanism)

### Mechanism 1: Concept Relevance Propagation (CRP) for Channel Attribution
The system uses CRP to decompose CNN predictions into semantically meaningful concepts represented by specific convolutional filters. CRP back-propagates prediction relevance to identify top channels, retrieves representative images, and generates saliency maps. The core assumption is that deeper layer channels correspond to human-understandable concepts rather than abstract features. Evidence shows channels can be polysemantic, encoding distinct concepts, which challenges the assumption that a single channel maps cleanly to one interpretable concept.

### Mechanism 2: Automated Concept Labeling via Visual Pattern Recognition
A multimodal LLM functions as an automated concept identifier by recognizing common visual patterns across representative images. The system presents the LLM with a grid of representative images and their saliency maps, instructing it to describe the common visual pattern. This translates abstract channel IDs into natural language labels. The core assumption is that the LLM possesses sufficient visual reasoning to synthesize common themes from structurally similar but diverse images.

### Mechanism 3: Rule-Based Contextualization and Summarization
The pipeline structures explanation generation into distinct steps with explicit decision rules to ensure faithfulness. The Contextualization prompt classifies relationships between concept labels and input images using a fixed schema (Direct recognition, Feature recognition, Co-occurrence, or Misidentification). The Summarization prompt aggregates grounded descriptions while explicitly forbidding new information. This modular approach reduces hallucination likelihood by functioning more like a decision algorithm than open-ended generation.

## Foundational Learning

- **Concept: Concept Relevance Propagation (CRP)**
  - Why needed here: This is the core XAI technique the paper builds upon. Without understanding CRP links predictions to specific channels, the need for LLM-based labeling is unclear.
  - Quick check question: How does CRP differ from Grad-CAM? (Hint: CRP attributes relevance to specific filters/channels, while Grad-CAM typically attributes to spatial regions)

- **Concept: Polysemantic Channels**
  - Why needed here: The paper assumes channels map to understandable concepts. Understanding that channels can be "polysemantic" (responding to multiple different patterns) is critical for diagnosing why the LLM might struggle to name a concept.
  - Quick check question: If a neuron activates strongly for both "car wheels" and "clocks," is it monosemantic or polysemantic?

- **Concept: Prompt Engineering / Chain-of-Thought**
  - Why needed here: The system's performance depends heavily on how prompts are structured. Understanding prompt design explains why authors chose a modular, multi-step pipeline.
  - Quick check question: Why might separating "naming" from "contextualization" lead to better results than a single combined prompt?

## Architecture Onboarding

- **Component map:** Input Image + Prediction Class -> CRP Engine -> Top Channels with Relevance Scores and Heatmaps -> Representative Images -> LLM Pipeline -> Concept Labels -> Contextualized Descriptions -> Final Summary
- **Critical path:** The Labeling Agent is most fragile. If LLM misinterprets representative images and assigns wrong labels, downstream Contextualization Agent will rationalize incorrect visual evidence using the provided schema, leading to confident but wrong explanations.
- **Design tradeoffs:**
  - Modularity vs. Latency: Separate LLM calls increase cost but improve accuracy by simplifying tasks
  - Faithfulness vs. Fluency: System prioritizes faithfulness by restricting summarizer to use only provided information
- **Failure signatures:**
  - Hallucinated Rationales: LLM explains concepts not present in image due to noisy heatmaps or wrong labels
  - Generic Labeling: Labeling Agent outputs vague descriptions like "colorful patterns" indicating diverse or polysemantic representative images
- **First 3 experiments:**
  1. Sanity Check (Lizard Example): Replicate specific example using VGG16 and GPT-4o to verify pipeline integration
  2. Labeling Accuracy Test: Override LLM-generated labels with "Random String" and check if Contextualization Agent still explains "Random String"
  3. Schema Ablation: Remove decision rules from Contextualization prompt and observe if explanation quality drops

## Open Questions the Paper Calls Out
None

## Limitations
- The LLM labeling step is highly dependent on representativeness of retrieved images for each channel; polysemantic channels may produce misleading or overly generic labels
- User study was small (21 participants) and focused on subjective agreement rather than objective correctness of explanations
- No ground truth exists for "correct" concept labels, making it impossible to measure accuracy beyond human agreement

## Confidence

- **High confidence:** Modular pipeline architecture is clearly described and CRP integration works as specified
- **Medium confidence:** LLM's ability to identify visual patterns from representative images is plausible but depends on image quality and channel semantics
- **Low confidence:** Faithfulness of explanations to actual CNN reasoning, since evaluation relied on subjective agreement rather than objective verification

## Next Checks
1. Run pipeline on 10 diverse ImageNet images and manually verify whether LLM-generated concept labels accurately reflect visual patterns in representative images
2. Test system's robustness by introducing synthetic noise into CRP heatmaps and observing whether explanations remain coherent or degrade gracefully
3. Conduct controlled experiment comparing LLEXICORP's explanations against expert-annotated concept labels to measure labeling accuracy rather than just agreement