---
ver: rpa2
title: Explainable reinforcement learning from human feedback to improve alignment
arxiv_id: '2512.13837'
source_url: https://arxiv.org/abs/2512.13837
tags:
- data
- reward
- training
- responses
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an explainable approach to improve reinforcement
  learning from human feedback (RLHF) for language model alignment. The method addresses
  the problem of unsatisfactory responses from RLHF-tuned models by identifying the
  training data that leads to such responses and unlearning their influence.
---

# Explainable reinforcement learning from human feedback to improve alignment

## Quick Facts
- arXiv ID: 2512.13837
- Source URL: https://arxiv.org/abs/2512.13837
- Reference count: 40
- Primary result: XRLHF improves RLHF alignment with 7-12 percentage point win rate increases over SFT models

## Executive Summary
This paper introduces an explainable approach to improve reinforcement learning from human feedback (RLHF) for language model alignment. The method addresses the problem of unsatisfactory responses from RLHF-tuned models by identifying the training data that leads to such responses and unlearning their influence. The core method consists of two parts: a post-hoc explanation technique that identifies problematic training data through constrained optimization, and an unlearning approach that removes this data's influence from the reward model before fine-tuning the policy. Experimental results on dialogue generation and summarization tasks show that the proposed method significantly improves RLHF performance, with win rates over SFT models increasing by 7-12 percentage points when using models like PPO and ReMax.

## Method Summary
The method identifies unsatisfactory responses in validation data, then uses constrained optimization to find the smallest subset of training data whose feature comparisons form a convex hull containing the problematic response in feature space. This training data is then "unlearned" from the reward model through negative gradient updates, and the policy is fine-tuned to maximize the unlearned reward on unsatisfactory prompts while maintaining performance on satisfactory ones through KL divergence constraints. The approach is computationally efficient as it operates after RLHF training and only needs to unlearn a small portion of the training data.

## Key Results
- XRLHF achieves 68-80% win rates over base RLHF on unsatisfactory subsets across models
- Win rates over SFT increase by 7-12 percentage points on full validation sets
- Computational efficiency: only ~2-3% of training data needs unlearning
- Performance maintained on satisfactory prompts through KL constraint

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Identifying which training data causes unsatisfactory responses enables targeted unlearning without full retraining.
- **Mechanism:** The method projects prompt-response pairs into a feature space and solves a constrained combinatorial optimization problem. It finds the smallest subset of training data whose feature comparisons form a convex hull containing the problematic pair, indicating those examples contributed to the high reward assigned to the unsatisfactory response.
- **Core assumption:** If a prompt-response pair lies within the convex hull of training data feature comparisons in representation space, those training examples causally influenced the model's behavior. This assumes the linear reward formulation captures the relevant structure.
- **Evidence anchors:** [abstract] "formulate this problem as a constrained combinatorial optimization problem where the objective is to find a set of training data closest to this prompt-response pair in a feature representation space, and the constraint is that the prompt-response pair can be decomposed as a convex combination" [section 4] "If a prompt-response pair (x̄,ȳ) has a feature ϕ(x̄,ȳ) closely aligned with the feature comparisons of some training data, this prompt-response pair is expected to have a high reward."
- **Break condition:** If the feature representation space does not preserve semantic similarity relevant to reward assignment, or if the reward model is highly nonlinear beyond the final layer, the identified training data may not be causally related to the unsatisfactory response.

### Mechanism 2
- **Claim:** Negative gradient updates on identified training data remove their influence from the reward model more efficiently than retraining from scratch.
- **Mechanism:** Starting from the trained reward model parameters θ₀, the method applies gradient descent on the log-likelihood of the identified problematic subset S, effectively reversing the learning process for those specific examples. This produces unlearned parameters θᵤ that assign lower rewards to the problematic patterns.
- **Core assumption:** The contribution of specific training examples to the reward model can be linearly subtracted through negative gradients. This assumes the optimization landscape allows clean reversal without affecting other learned patterns.
- **Evidence anchors:** [section 5] "Since rθ₀ was originally obtained by iteratively adding gradients from the training data, the negative gradient updates effectively remove the contribution of {S(x̄,ȳ)}(x̄,ȳ)∈D̄ᵤ from θ₀" [table 3] Win rates of 68-80% over base RLHF on unsatisfactory subsets demonstrate the unlearning improves problematic responses.
- **Break condition:** If training examples have non-independent contributions (e.g., through interference in parameter space), negative gradient updates may remove more than intended, degrading performance on satisfactory responses.

### Mechanism 3
- **Claim:** Selective fine-tuning on unsatisfactory prompts with KL constraints prevents catastrophic forgetting of satisfactory behaviors.
- **Mechanism:** The policy fine-tuning phase maximizes the unlearned reward model only on prompts from the unsatisfactory subset D̄ᵤ, while adding a KL divergence penalty against the original policy π₀ for prompts in the satisfactory subset D̄ \ D̄ᵤ. This localizes improvement while preserving existing capabilities.
- **Core assumption:** The satisfactory and unsatisfactory prompt distributions are sufficiently separable that optimizing for one doesn't degrade the other through model capacity limitations.
- **Evidence anchors:** [section 5] "fine-tunes the policy π₀ to maximize the unlearned reward for the validation prompts in D̄ᵤ and restricts the deviation from π₀ for the validation prompts in D̄ \ D̄ᵤ" [table 4] Win rates on full validation set (67-79%) are lower than on unsatisfactory subset alone (68-80%), showing some tradeoff but net positive.
- **Break condition:** If unsatisfactory and satisfactory prompts share significant semantic overlap in representation space, the localized optimization may fail to improve or may cause unintended degradation.

## Foundational Learning

- **Concept: Bradley-Terry Preference Model**
  - **Why needed here:** The reward model is trained by maximizing log-likelihood under this model, which converts preference comparisons into scalar rewards. Understanding P(yᵥ > yₗ) = σ(r(x, yᵥ) - r(x, yₗ)) is essential for grasping why feature comparisons in training data correlate with rewards.
  - **Quick check question:** Given two responses with reward difference Δr = 2.0, what is the probability the higher-reward response is preferred?

- **Concept: Convex Hull and Feature Space Geometry**
  - **Why needed here:** The explanation method relies on checking whether a test point lies within the convex hull of training feature comparisons. This geometric intuition underlies the constrained optimization formulation.
  - **Quick check question:** If a point lies exactly on an edge of a convex hull, what does this imply about the minimum number of training examples needed to represent it?

- **Concept: Machine Unlearning via Gradient Reversal**
  - **Why needed here:** The unlearning phase uses negative gradients to remove training data influence. This differs from simply removing data and retraining, and has different computational and behavioral properties.
  - **Quick check question:** What is the difference between unlearning via negative gradients vs. retraining from scratch on the remaining data?

## Architecture Onboarding

- **Component map:**
  [Validation Prompts x̄] → [Policy π₀ generates responses ȳ] → [Reward model scores responses]
  ↓
  [Partition into D̄ᵤ (unsatisfactory) and D̄ \ D̄ᵤ]
  ↓
  [Feature extractor ϕ] → [Project (x̄,ȳ) pairs to feature space] → [Solve constrained optimization (Eq. 3)]
  ↓
  [Identify problematic training subset S]
  ↓
  [Reward model rθ₀] → [Negative gradient updates on S] → [Unlearned reward rθᵤ]
  ↓
  [Policy π₀] → [Fine-tune on D̄ᵤ with rθᵤ, KL constraint on D̄ \ D̄ᵤ] → [Improved policy]

- **Critical path:**
  1. **Validation data labeling** (requires human evaluation to set reward threshold for satisfactory/unsatisfactory classification)
  2. **Feature extraction setup** (need access to reward model's penultimate layer outputs)
  3. **Convex hull projection** (quadratic programming, O(N³·⁵) complexity)
  4. **Iterative data selection** (linear programming checks, O(N⁴·⁵) worst case)
  5. **Reward unlearning** (standard gradient descent, few epochs needed as only ~2-3% of data)
  6. **Policy fine-tuning** (PPO or ReMax with modified objective)

- **Design tradeoffs:**
  - **Validation set size vs. coverage:** Paper uses only 0.4-0.5% of training data (500 prompts). Smaller sets are efficient but may miss failure modes.
  - **KL penalty β̄:** Controls tradeoff between improving unsatisfactory responses and preserving satisfactory ones. Ablation (Table 16) shows removing KL constraint causes significant degradation on full validation set.
  - **Feature representation choice:** Paper uses final transformer layer before linear head. Earlier layers may capture different semantic properties but are less directly tied to reward values.

- **Failure signatures:**
  - **Low win rates on D̄ \ D̄ᵤ:** KL constraint too weak; policy overfitting to unsatisfactory subset
  - **High win rates on D̄ᵤ but low on full D̄:** Identified training data is not causally related to unsatisfactory responses; unlearning removes beneficial patterns
  - **Quadratic/linear programming infeasibility:** Feature space is poorly conditioned; consider normalization or dimensionality reduction

- **First 3 experiments:**
  1. **Reproduce unsatisfactory response identification:** Train RLHF on full-hh-rlhf, generate responses to 500 validation prompts, score with reward model, and verify ~25-30% are classified as unsatisfactory. This validates the problem setup.
  2. **Single prompt explanation trace:** Take one unsatisfactory response, run Algorithm 1, and manually inspect the identified training data. Check if the semantic relationship between training data and response is interpretable.
  3. **Unlearning-only ablation:** Apply reward unlearning without policy fine-tuning, and measure win rates. This isolates the contribution of the unlearning phase vs. the fine-tuning phase.

## Open Questions the Paper Calls Out
The paper identifies several limitations and future directions in Appendix E (Limitations), including exploring methods suitable for test-time scenarios where users can immediately flag unsatisfactory responses.

## Limitations
- Computational complexity of O(N⁴·⁵) for large datasets may limit scalability to web-scale data
- Assumption that convex hull membership indicates causal influence is not directly validated
- Feature representation choice (final transformer layer) may not capture all relevant semantic relationships

## Confidence

**High confidence:** The overall experimental methodology and evaluation framework are sound. The win rate improvements over SFT baselines are statistically significant and reproducible.

**Medium confidence:** The explanation method's ability to identify causally relevant training data is reasonable but not definitively proven. The paper provides evidence that the identified data improves performance when removed, but direct validation of the convex hull assumption is limited.

**Low confidence:** The generalization of results beyond the tested model architectures (Pythia, Llama-2) and tasks (dialogue, summarization) is uncertain. The method's behavior on larger models or different domains is not explored.

## Next Checks

1. **Causal validation of identified training data:** Manually examine 10-20 examples of training data identified as problematic for specific unsatisfactory responses. Assess whether there is a clear semantic relationship that could explain the reward model's behavior.

2. **Feature space sensitivity analysis:** Vary the feature representation (e.g., use different transformer layers, apply dimensionality reduction) and measure how this affects the quality of identified training data and final performance. This would test the robustness of the explanation method to representation choices.

3. **Cross-task generalization test:** Apply the method to a different task (e.g., code generation or reasoning) and evaluate whether similar win rate improvements are observed. This would validate whether the approach generalizes beyond the specific dialogue and summarization domains tested.