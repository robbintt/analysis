---
ver: rpa2
title: 'CoCoNUT: Structural Code Understanding does not fall out of a tree'
arxiv_id: '2501.16456'
source_url: https://arxiv.org/abs/2501.16456
tags:
- code
- execution
- trace
- performance
- humaneval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CoCoNUT, a benchmark for evaluating large
  language models' (LLMs) ability to trace program execution paths. The authors extract
  solutions from the HumanEval benchmark and trace their execution using function
  calls from the test set.
---

# CoCoNUT: Structural Code Understanding does not fall out of a tree

## Quick Facts
- arXiv ID: 2501.16456
- Source URL: https://arxiv.org/abs/2501.16456
- Authors: Claas Beger; Saikat Dutta
- Reference count: 28
- One-line primary result: Current LLMs struggle with execution tracing, achieving only 47% accuracy even on simple HumanEval tasks, with near-zero performance on recursion, parallelism, and OOP.

## Executive Summary
CoCoNUT is a benchmark that evaluates large language models' ability to trace program execution paths by generating ordered lists of executed line numbers. Despite strong performance on code generation tasks, seven state-of-the-art LLMs showed significant struggles with execution tracing, particularly for longer traces and advanced programming concepts. The top-performing model, Gemini 1.5 Pro, correctly generated only 47% of HumanEval task traces, while models failed almost completely on recursion, parallelism, and object-oriented programming concepts. The study concludes that current LLMs need significant improvement in code reasoning abilities beyond pattern matching.

## Method Summary
The CoCoNUT benchmark evaluates LLMs by extracting solutions from HumanEval and advanced programming concept datasets, then tracing their execution using function calls from the test set. The evaluation uses one-shot direct prompting and Chain-of-Thought methods, comparing model-generated traces against ground truth execution paths. Two main metrics are used: Exact Match (Accuracy Mean, Acc Hard) for perfect trace matching and Similarity via Gestalt Pattern Matching for partial credit. The benchmark includes 161 HumanEval tasks and 124 advanced concept tasks covering recursion, concurrency, and object-oriented programming.

## Key Results
- Gemini 1.5 Pro achieved only 47% accuracy on HumanEval task traces, with performance dropping significantly for traces exceeding 25-40 steps
- No model achieved over 5% accuracy on advanced structural concepts (recursion, parallelism, OOP)
- Chain-of-Thought prompting provided limited benefits and sometimes degraded performance on complex tasks
- Performance collapse occurs around trace length 25-40 steps, suggesting limitations in long-term dependency tracking

## Why This Works (Mechanism)

### Mechanism 1: Trace Length Capacity Limits
- **Claim:** Execution trace length is a primary predictor of model failure, with performance collapsing for traces exceeding 25-40 steps.
- **Mechanism:** This reflects limited working memory or state-tracking capacity over long sequences. Models successfully trace early segments but lose the correct execution path as the number of preceding steps grows.
- **Core assumption:** Performance drop is due to inherent difficulty of long-term dependency tracking, not primarily token limits.
- **Evidence anchors:** Abstract mentions struggles "especially for longer traces"; Section IV.C shows performance drop from trace length 25 onwards; models tend to lose functionality at same threshold as direct prompting.

### Mechanism 2: Advanced Structural Concept Reasoning
- **Claim:** Advanced structural concepts (Recursion, Parallelism, OOP) introduce specific failure modes beyond code or trace length effects.
- **Mechanism:** These concepts demand sophisticated reasoning: maintaining call stacks (recursion), identifying concurrent regions (parallelism), resolving method calls through inheritance (OOP). Models fail by not applying operational rules for these structures.
- **Core assumption:** Near-zero performance is due to lack of semantic understanding, not simply longer programs.
- **Evidence anchors:** Abstract states none achieve accuracy over 5% on advanced traces; Section IV.D observes models cannot handle recursion after certain depth, struggle with concurrent regions, and have difficulty with longer code segments.

### Mechanism 3: Chain-of-Thought Prompting Inconsistency
- **Claim:** Chain-of-Thought prompting provides inconsistent benefits for execution tracing, sometimes degrading performance on complex tasks.
- **Mechanism:** CoT is hypothesized to aid reasoning by decomposition, but in tracing the decomposition is the task itself. Verbose natural-language reasoning introduces opportunities for hallucination or error propagation.
- **Core assumption:** Performance differences are attributable to elicitation method, not random variance.
- **Evidence anchors:** Abstract mentions CoT offers limited benefits and sometimes degrades performance; Section IV.C shows CoT mostly improves existing capabilities but offers only marginal benefits for larger traces; GPT4o struggles with hallucinations.

## Foundational Learning

- **Concept: Control Flow Graphs (CFGs) & Execution Traces**
  - **Why needed here:** The benchmark is built on understanding program execution paths through non-linear control flow via loops and conditionals.
  - **Quick check question:** Given a simple `if-else` block, can you write down the exact sequence of line numbers that would be executed for a specific input?

- **Concept: State and Environment in Program Execution**
  - **Why needed here:** Correct tracing requires tracking variable values as they change; models make "predicate wrong" errors from failing to evaluate conditions based on current state.
  - **Quick check question:** In the example loop `for i in range(3)`, what is the value of `i` at each of the 4 times line 2 is reached?

- **Concept: Recursion and the Call Stack**
  - **Why needed here:** The paper identifies recursion as a major failure mode; understanding call stack mechanics is essential for reasoning about this failure.
  - **Quick check question:** For a recursive factorial function `fact(3)`, what is the sequence of function calls and returns? How would you represent this as a sequence of line numbers?

## Architecture Onboarding

- **Component Map:** Dataset Generation Pipeline -> Prompting Module -> Model Inference -> Trace Cleaning -> Evaluation Module
- **Critical Path:**
  1. Dataset Curation: Quality of ground truth traces is paramount; errors invalidate the benchmark
  2. Prompt Design: One-shot example is critical for understanding task output format; ambiguities lead to parsing errors
  3. Model Inference: Core step where LLM generates the trace
  4. Trace Cleaning: Prerequisite for evaluation; must handle whitespace, markdown, and invalid output
  5. Metric Computation: Final accuracy/similarity scores

- **Design Tradeoffs:**
  - One-shot prompting chosen over zero-shot because smaller models struggled with format, trading potential contamination for broader model compatibility
  - 1024-token limit for traces manages evaluation cost and context window limits but may exclude very long traces
  - Exact Match is strict and penalizes small errors heavily; Similarity captures partial credit

- **Failure Signatures:**
  - Format Errors: Empty lists, commentary, or non-numeric output; common in smaller models like CodeLLama
  - Hallucination: Invents lines, includes docstring content, or calls non-existent functions; noted for GPT-4o
  - Trace Collapse: Enters endless loops or loses path, generating same sequence repeatedly until token limit
  - Structural Errors: Incorrectly marking concurrent regions, failing to return from recursion, or skipping definitions

- **First 3 Experiments:**
  1. Reproduce Baseline: Evaluate baseline model (e.g., Gemini 1.5 Pro) on subset of HumanEval-Trace to confirm reported accuracy and verify trace cleaning/evaluation logic
  2. Ablate Trace Length: Run model across all trace length buckets (1-3, 3-5, 5-10, etc.) and plot performance curve to confirm 25-40 line performance cliff
  3. Analyze Error Modes: Manually inspect 10-20 incorrect traces for chosen model and categorize into described error types (Statement Skip, Condition Skip, etc.)

## Open Questions the Paper Calls Out

- **Open Question 1:** Does fine-tuning on execution traces improve model performance on downstream software engineering tasks such as error localization?
  - **Basis:** Authors identify fine-tuning on CoCoNUT dataset as next step to determine if it helps differentiate between code functionality and token patterns
  - **Why unresolved:** Current work establishes benchmark but doesn't implement training models using trace data to see if skill transfers to debugging
  - **Evidence needed:** Comparative study of model performance on error localization benchmarks before and after fine-tuning on execution tracing tasks

- **Open Question 2:** Can structural understanding required for execution tracing transfer effectively across different programming languages?
  - **Basis:** Authors state intent to adapt code snippets into multiple languages to verify if structural understanding is generalized or language-specific
  - **Why unresolved:** Unclear if models learn abstract control flow logic applicable to any language or if reasoning is tied to specific language syntax
  - **Evidence needed:** Evaluation of models on CoCoNUT benchmark where code snippets are translated into languages like Java or C++ without specific fine-tuning

- **Open Question 3:** What prompting mechanisms can effectively resolve performance collapse observed in traces exceeding 25 to 40 lines?
  - **Basis:** Results show model accuracy plummets as trace length increases, and CoT prompting failed to mitigate this issue
  - **Why unresolved:** Failure of standard reasoning strategies like CoT suggests models lack mechanism for maintaining state over long execution sequences
  - **Evidence needed:** Research into hierarchical or "stateful" prompting strategies that successfully maintain accuracy on tasks requiring tracing beyond 40 steps

## Limitations
- Dataset filtering (1024-token limit) may bias results against longer traces, potentially underestimating model capabilities on complex programs
- One-shot prompting format may introduce task-specific bias through the example trace
- Advanced structural concepts evaluated on limited samples (124 tasks vs 161 HumanEval tasks), reducing statistical power

## Confidence
- **High Confidence:** Fundamental finding that LLMs struggle with execution tracing, particularly for longer traces and advanced concepts
- **Medium Confidence:** Specific performance thresholds (25-40 steps, 47% accuracy) are model and implementation-dependent
- **Medium Confidence:** Error categorization based on qualitative inspection may not capture all failure modes

## Next Checks
1. Systematically evaluate model performance across all trace length buckets to verify the reported performance cliff around 25-40 lines, controlling for code complexity
2. Manually refactor advanced structure programs into single-function equivalents with short traces to determine if poor performance stems from code length/complexity versus semantic understanding
3. Test a refined Chain-of-Thought prompt that explicitly asks for variable state updates at each line to distinguish between CoT verbosity issues and need for different reasoning types