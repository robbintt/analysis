---
ver: rpa2
title: 'CienaLLM: Generative Climate-Impact Extraction from News Articles with Autoregressive
  LLMs'
arxiv_id: '2512.19305'
source_url: https://arxiv.org/abs/2512.19305
tags:
- drought
- extraction
- prompt
- impact
- article
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CienaLLM is a schema-guided Generative Information Extraction framework
  that uses open-weight LLMs for zero-shot climate-impact extraction from news articles.
  Evaluated across 384 configurations of 12 open-weight models, prompt strategies,
  and precision regimes, CienaLLM achieves strong performance in extracting drought
  impacts from Spanish news, matching or outperforming supervised baselines.
---

# CienaLLM: Generative Climate-Impact Extraction from News Articles with Autorelectric LLMs

## Quick Facts
- arXiv ID: 2512.19305
- Source URL: https://arxiv.org/abs/2512.19305
- Authors: Javier Vela-Tambo; Jorge Gracia; Fernando Dominguez-Castro
- Reference count: 40
- Key outcome: CienaLLM achieves strong performance in zero-shot climate-impact extraction from Spanish news, matching or outperforming supervised baselines through schema-guided generative information extraction with open-weight LLMs.

## Executive Summary
CienaLLM is a schema-guided Generative Information Extraction (GenIE) framework that uses open-weight autoregressive LLMs for zero-shot extraction of drought impacts from Spanish news articles. The framework employs a prompt-based approach with optional response parsing to structure LLM outputs into JSON schema without fine-tuning. Evaluated across 384 configurations spanning 12 models from Gemma, Llama, and Qwen families, with prompt strategies (summarization, chain-of-thought, self-criticism, impact descriptions) and quantization regimes, CienaLLM demonstrates that larger models provide higher accuracy and stability while quantization offers significant efficiency gains with modest accuracy trade-offs.

## Method Summary
CienaLLM implements schema-guided generative information extraction using open-weight LLMs via the Ollama interface. The framework extracts structured climate impact data through prompt-based approaches without task-specific fine-tuning. It supports 12 models across Gemma, Llama, and Qwen families in small (<7B), medium (7-25B), and large (>25B) tiers, with both fp16 and q4_K_M quantization precision regimes. The method includes optional response parsing to correct JSON formatting errors, temperature=0, max output=2,048 tokens, and context window=32,768. Prompt strategies include summarization (SUM), chain-of-thought (CoT), self-criticism (SC), and impact descriptions (DESC). The framework processes articles through metadata extraction, optional summarization, main extraction with or without parsing refinement, and schema validation using Pydantic.

## Key Results
- Larger models (e.g., llama_70b, qwen_72b) achieve highest F1 scores (0.858) with greater stability across prompt configurations
- Response parsing reduces JSON formatting errors from 4.0% to 0.7% without affecting extraction accuracy
- Quantization provides 30-40% latency reduction with only 0.8-1.7 F1 point loss depending on model family
- Prompt strategy effectiveness is model-family specific: SUM improves Gemma performance but harms Llama/Qwen; DESC benefits larger models but may confuse smaller ones

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-step response parsing nearly eliminates format errors without affecting extraction accuracy
- Mechanism: Decoupling extraction from JSON formatting allows the LLM to focus on semantic content first, then structurally transforms the free-text output via a second LLM call. This reduces cognitive load per step and isolates formatting failures
- Core assumption: The second LLM call preserves semantic content while correcting syntax; formatting difficulty is separable from extraction difficulty
- Evidence anchors: [abstract] "An additional response parsing step nearly eliminates format errors while preserving accuracy"; [section V-A] "parsing error rate without response parsing (RPARSE = False) reached 4.0%, compared to only 0.7% with response parsing (RPARSE = True)... F1 scores... remained almost identical"
- Break condition: If newer LLMs natively emit valid JSON consistently, the second step becomes redundant overhead

### Mechanism 2
- Claim: Larger models deliver higher accuracy and lower variance across prompt configurations
- Mechanism: Greater parameter count provides richer representations of implicit geographic references and nuanced impact descriptions, reducing sensitivity to prompt wording and enabling more robust zero-shot generalization
- Core assumption: Parameter scale correlates with world knowledge and reasoning capacity relevant to climate-impact semantics
- Evidence anchors: [abstract] "larger models provide higher accuracy and stability"; [section V-B] "llama_70b achieves the highest F1 score (0.858)... smaller models show lower scores and greater variance"
- Break condition: If task-specific fine-tuning on small models outperforms zero-shot large models, scale advantage diminishes for narrow domains

### Mechanism 3
- Claim: Prompt strategy effectiveness is model-family and model-size dependent, not universal
- Mechanism: Different architectures process context differently—Gemma benefits from summarization (condensed inputs), while Llama and Qwen rely more on full-article context. Rich label descriptions (DESC) help larger models but can confuse smaller ones with limited capacity
- Core assumption: Prompt design acts as a model-specific hyperparameter rather than a general accuracy booster
- Evidence anchors: [abstract] "prompt strategies show heterogeneous, model-specific effects"; [section V-C] "Summarization (SUM): Consistently improves performance in all gemma models... but has a negative effect on llama and qwen models"
- Break condition: If future unified prompting frameworks eliminate family-specific tuning, this heterogeneity becomes an artifact of current model design

## Foundational Learning

- Concept: **Schema-guided Generative Information Extraction (GenIE)**
  - Why needed here: Core paradigm—prompts LLMs to output structured JSON aligned to a predefined schema without task-specific fine-tuning
  - Quick check question: Can you explain how GenIE differs from training a classifier head on a BERT encoder?

- Concept: **Quantization (4-bit mixed-precision)**
  - Why needed here: Critical for efficiency; reduces latency and memory with modest accuracy loss, enabling deployment on resource-constrained hardware
  - Quick check question: What is the trade-off between fp16 and q4_K_M in terms of F1 score and execution time per model family?

- Concept: **Multi-label classification evaluation (micro-averaged F1, precision, recall)**
  - Why needed here: Drought impact extraction is multi-label (agriculture, livestock, hydrological resources, energy); micro-averaging handles label imbalance appropriately
  - Quick check question: Why might macro-averaging be misleading for this dataset given the label distribution in Table I?

## Architecture Onboarding

- Component map:
  - Schema module: Defines extraction fields (impacts, locations) and validates output via Pydantic
  - Prompt assembler: Dynamically constructs prompts from base template + optional flags (SUM, CoT, DESC, SC)
  - LLM backend: Ollama server running open-weight models (Gemma, Llama, Qwen) with configurable precision
  - Response parser: Two-step option—extraction call followed by formatting call; enforces JSON schema compliance
  - Orchestrator (LangChain): Chains prompt construction, LLM calls, parsing, and error logging

- Critical path:
  1. Article ingestion → metadata extraction (headline, body, date)
  2. Optional SUM call → summary generation
  3. Main extraction call → free-text or structured output
  4. Optional RPARSE call → JSON reformatting
  5. Optional SC call → self-critique refinement
  6. Schema validation → structured JSON export

- Design tradeoffs:
  - Accuracy vs. efficiency: Best-F1 (qwen_72b) at 36s/article vs. Efficient (qwen_7b) at 3.2s/article with 7-point F1 gap
  - Reliability vs. latency: RPARSE adds ~6s/article but reduces parsing errors from 4% to 0.7%
  - Quantization: 30-40% latency reduction for 0.8-1.7 F1 point loss depending on family

- Failure signatures:
  - High parsing error rate (>10%) → model struggles with JSON formatting; enable RPARSE or use newer model checkpoint
  - Low recall on location extraction (<0.35) → implicit geographic references not captured; consider geo-resolution post-processing or richer prompt context
  - Self-criticism (SC) shows no change → model ignoring refinement instruction; check logs for identical responses

- First 3 experiments:
  1. Baseline validation: Run the Efficient profile (qwen_7b + DESC + PARSE) on a 50-article sample from your target news source; measure F1, parsing error rate, and latency
  2. Ablation on RPARSE: Disable response parsing on the same sample with a mid-size model (llama_8b); quantify parsing error increase and any F1 change
  3. Prompt strategy sweep: Test SUM vs. DESC vs. no enhancement on your chosen model; identify which strategy benefits your specific model family per Section V-C patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can few-shot prompting or retrieval-augmented generation (RAG) improve extraction accuracy and reliability beyond the zero-shot schema-guided approach evaluated in CienaLLM?
- Basis in paper: [explicit] The authors state: "experimenting with alternative prompting paradigms such as few-shot or retrieval-augmented generation that might improve performance" remains an open direction
- Why unresolved: The study restricted evaluation to zero-shot prompting; no comparison with in-context learning or RAG was conducted
- What evidence would resolve it: A controlled experiment comparing zero-shot, few-shot, and RAG-enhanced configurations across the same models and datasets, measuring F1, parsing reliability, and inference time

### Open Question 2
- Question: Do ensemble strategies combining multiple model families and sizes yield higher accuracy or robustness than single-model configurations for climate-impact extraction?
- Basis in paper: [explicit] The authors identify "developing ensemble strategies that combine complementary model families and sizes" as an open direction
- Why unresolved: The factorial study evaluated 384 individual configurations but did not explore model aggregation or voting schemes
- What evidence would resolve it: Experiments combining predictions from diverse models (e.g., Gemma, Llama, Qwen) using majority voting, weighted averaging, or learned meta-classifiers, benchmarked against single-model baselines

### Open Question 3
- Question: How can implicit geographic references in news articles be more accurately resolved to specific administrative units for climate-impact location extraction?
- Basis in paper: [inferred] Location extraction achieved only F1 = 0.465 (Best-F1), with low recall (0.34), and the authors note that "location extraction remains the most difficult task, underscoring the need for further advances in handling implicit geographic references"
- Why unresolved: Models struggle to infer affected provinces from indirect mentions (rivers, reservoirs, regions); simple province-level schemas may be insufficient
- What evidence would resolve it: Integration of external gazetteers, hydrological basin maps, or geospatial reasoning modules with LLM extraction, evaluated on the DILD dataset with recall improvements as the key metric

## Limitations

- Dataset availability: The primary evaluation datasets (DID, DRD, DILD) are not yet publicly available, preventing independent validation of reported performance metrics
- Model-specific tuning: Prompt strategy effectiveness is highly model-family dependent, requiring extensive experimentation for optimal configuration
- Computational overhead: Response parsing adds significant latency (~6s/article) that may be prohibitive for real-time applications

## Confidence

- **High confidence** in the fundamental effectiveness of the schema-guided GenIE framework and the value of response parsing for reducing formatting errors
- **Medium confidence** in the scaling relationships between model size and accuracy, as these follow established LLM scaling laws
- **Medium confidence** in model-family specific prompt strategy recommendations, as these show clear patterns but may be context-dependent
- **Low confidence** in absolute performance numbers without access to the evaluation datasets

## Next Checks

1. **Dataset availability verification**: Confirm release timeline and access procedures for the Drought Impacts Dataset (DID), Drought Relevance Dataset (DRD), and Drought Impact Locations Dataset (DILD) to enable independent benchmarking

2. **Prompt strategy ablation study**: Conduct controlled experiments testing SUM, DESC, and SC strategies across Gemma, Llama, and Qwen models on a held-out validation set to verify the paper's model-family specific recommendations

3. **Cross-domain generalization test**: Apply CienaLLM to a different structured extraction task (e.g., financial news entities or scientific paper metadata) to assess whether the observed prompt strategy heterogeneity persists across domains