---
ver: rpa2
title: Mitigating Hallucinations in Large Vision-Language Models by Self-Injecting
  Hallucinations
arxiv_id: '2509.11287'
source_url: https://arxiv.org/abs/2509.11287
tags:
- apasi
- preference
- hallucination
- llav
- hallucinations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Autonomous Preference Alignment via Self-Injection
  (APASI), a novel method to mitigate hallucinations in Large Vision-Language Models
  (LVLMs) without relying on external human annotations or auxiliary models. APASI
  autonomously constructs preference data by using the target LVLM to self-inject
  hallucinations into its own generated responses, creating valid preference pairs
  for training.
---

# Mitigating Hallucinations in Large Vision-Language Models by Self-Injecting Hallucinations

## Quick Facts
- arXiv ID: 2509.11287
- Source URL: https://arxiv.org/abs/2509.11287
- Reference count: 40
- Primary result: Autonomous hallucination injection method achieves comparable hallucination reduction to supervised methods without external data

## Executive Summary
The paper introduces Autonomous Preference Alignment via Self-Injection (APASI), a novel method to mitigate hallucinations in Large Vision-Language Models (LVLMs) without relying on external human annotations or auxiliary models. APASI autonomously constructs preference data by using the target LVLM to self-inject hallucinations into its own generated responses, creating valid preference pairs for training. The hallucination injection is guided by three key observations: co-occurrence of objects, language priors, and positional factors in responses. An iterative alignment strategy combined with curriculum learning is employed to progressively increase the difficulty of the alignment task and ensure stable, continuous model improvement.

Experiments across six benchmarks demonstrate that APASI effectively reduces hallucination ratios (e.g., 4.5/1.8 decrease in hallucinated objects on Object-Hal and AMBER) and improves overall performance for multiple baseline LVLMs, achieving results comparable or superior to methods requiring external dependencies. The approach represents a significant step toward autonomous hallucination mitigation in vision-language models, though limitations around potential bias from self-injection and narrow focus on object-level hallucinations suggest areas for future work.

## Method Summary
APASI addresses LVLM hallucinations through an autonomous preference alignment framework that eliminates the need for external human annotations or auxiliary models. The method operates in three phases: first, the target LVLM self-injects hallucinations into its responses using guided factors including co-occurrence of objects, language priors, and positional bias; second, these injected responses are paired with original responses to create preference data; third, the model is fine-tuned using this preference data through an iterative alignment strategy with curriculum learning. The hallucination injection process is designed to be adversarial yet controllable, ensuring the injected errors are realistic enough to train effective hallucination detection. The iterative approach progressively increases the difficulty of alignment tasks, allowing the model to first master easier cases before tackling more challenging hallucinations, while curriculum learning helps maintain stable improvement throughout training.

## Key Results
- APASI reduces hallucinated objects by 4.5 on Object-Hal and 1.8 on AMBER benchmarks compared to baselines
- Achieves comparable or superior performance to methods requiring external human annotations or auxiliary models
- Demonstrates consistent improvement across multiple LVLM architectures including LLaVA-1.5 and Llava-NeXT-7B

## Why This Works (Mechanism)
APASI works by creating a closed-loop training system where the model learns to recognize and correct its own hallucinations. By self-injecting hallucinations using empirically observed patterns (co-occurrence, language priors, positional factors), the model generates synthetic training data that captures common hallucination patterns without requiring external annotation. The iterative alignment with curriculum learning ensures progressive improvement by starting with easier hallucination detection tasks and gradually increasing complexity, preventing catastrophic forgetting and maintaining stable training dynamics.

## Foundational Learning
- **LVLM Hallucination Types**: Understanding the distinction between object, attribute, and relationship hallucinations is crucial for evaluating APASI's effectiveness, which currently focuses primarily on object-level errors.
- **Preference Learning**: The method relies on pairwise preference training, requiring understanding of how models learn from comparison data rather than absolute labels.
- **Curriculum Learning**: The iterative difficulty progression requires knowledge of how staged learning affects model convergence and generalization.
- **Self-Supervision**: APASI's autonomous data generation represents an advanced form of self-supervised learning where the model generates its own training signals.

## Architecture Onboarding

**Component Map:**
Target LVLM -> Hallucination Injection Module -> Preference Data Generator -> Fine-tuning Module -> Aligned LVLM

**Critical Path:**
The hallucination injection and preference data generation phase is critical, as the quality and representativeness of injected hallucinations directly determines the effectiveness of subsequent alignment training.

**Design Tradeoffs:**
The approach trades potential bias from self-injection against the benefit of eliminating external data dependencies. While self-injection may not capture all real-world hallucination patterns, it provides a scalable and autonomous alternative to expensive human annotation.

**Failure Signatures:**
If the hallucination injection module relies too heavily on learned biases or fails to capture diverse hallucination patterns, the resulting model may overfit to synthetic errors and underperform on real hallucinations. Additionally, if the curriculum progression is too aggressive, the model may experience catastrophic forgetting of previously learned hallucination patterns.

**First Experiments:**
1. Validate that injected hallucinations follow realistic patterns by comparing their distribution to human-annotated hallucination datasets
2. Test the ablation of individual hallucination injection factors (co-occurrence, language priors, positional bias) to assess their relative contributions
3. Evaluate model performance on attribute and relationship hallucination benchmarks to assess scope beyond object-level errors

## Open Questions the Paper Calls Out
None

## Limitations
- The reliance on the target LVLM to self-inject hallucinations may introduce bias, as injected errors may not fully represent real-world hallucination patterns observed in diverse user contexts
- The hallucination injection strategy, while guided by intuitive factors, lacks explicit validation that these patterns align with actual hallucination types in LVLMs
- Experiments focus primarily on object-level hallucinations, leaving open questions about effectiveness on attribute or relationship hallucinations

## Confidence
- Effectiveness of self-injected preference data: **Medium** - Results are promising but depend on the assumption that self-injected errors are representative of real hallucinations
- Generalization across LVLM architectures: **Medium** - Tested on LLaVA-1.5 and Llava-NeXT-7B, but broader architecture coverage is needed
- Superiority over external-dependency methods: **High** - Quantitative improvements are demonstrated, though methodological differences complicate direct comparison

## Next Checks
1. Conduct ablation studies isolating the impact of each hallucination injection factor (co-occurrence, language priors, positional bias) to confirm their individual contributions
2. Evaluate APASI on attribute and relationship hallucination benchmarks (e.g., RefKB, MemeAesthetic) to assess broader hallucination mitigation
3. Test the method on larger LVLM architectures (e.g., GPT-4V, Gemini Pro) and in multi-turn dialogue scenarios to evaluate scalability and robustness