---
ver: rpa2
title: CaresAI at BioCreative IX Track 1 -- LLM for Biomedical QA
arxiv_id: '2509.00806'
source_url: https://arxiv.org/abs/2509.00806
tags:
- biomedical
- answer
- question
- answers
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a supervised fine-tuning approach using LLaMA
  3 8B to tackle multi-hop biomedical question answering in the MedHopQA shared task.
  The authors augment a limited development set with external biomedical QA datasets
  and experiment with fine-tuning on combined, short-only, and long-only answer formats.
---

# CaresAI at BioCreative IX Track 1 -- LLM for Biomedical QA

## Quick Facts
- arXiv ID: 2509.00806
- Source URL: https://arxiv.org/abs/2509.00806
- Authors: Reem Abdel-Salam; Mary Adewunmi; Modinat A. Abayomi
- Reference count: 24
- Primary result: LLaMA 3 8B fine-tuned on biomedical QA datasets achieves 0.8 concept-level accuracy but struggles with exact match evaluation (0.0-0.2 EM) on MedHopQA test set.

## Executive Summary
This study tackles multi-hop biomedical question answering using LLaMA 3 8B with supervised fine-tuning on curated external QA datasets. The authors address the challenge of limited domain-specific training data by aggregating 10,000 QA pairs from sources like BioASQ, MedQuAD, and TREC. A two-stage inference pipeline is introduced to extract concise answers from verbose model outputs, though exact match performance remains limited despite strong semantic understanding. The work highlights the persistent gap between conceptual knowledge and strict evaluation criteria in biomedical QA tasks.

## Method Summary
The authors fine-tune LLaMA 3 8B using 4-bit quantization with LoRA adapters (rank=64, alpha=16) on a curated dataset of 10,000 biomedical QA pairs from external sources. Three training approaches are evaluated: combined short/long answers, short-only, and long-only formats. A two-stage inference pipeline generates initial verbose responses then extracts concise answers through follow-up prompts, with three-attempt fallback to longer outputs. The model is trained for 5 epochs with learning rate 1e-4 and cosine scheduler, using two RTX 3080 Ti GPUs.

## Key Results
- Concept-level accuracy reaches 0.8 on validation set, indicating strong semantic understanding
- Exact match scores remain low (0.0-0.2) despite high semantic accuracy, showing formatting challenges
- Combined format training yields best test EM (0.2) compared to short-only and long-only approaches (both 0.0)
- Two-stage extraction improves answer precision but often requires fallback to verbose outputs

## Why This Works (Mechanism)

### Mechanism 1
External biomedical QA dataset augmentation compensates for limited development data during domain adaptation. Aggregating 10,000 QA pairs from heterogeneous sources exposes the model to diverse biomedical question patterns and entity types, improving semantic understanding even when task-specific data is scarce.

### Mechanism 2
Two-stage inference with extraction prompting partially mitigates verbosity but does not guarantee exact format compliance. Stage 1 generates an initial response (often verbose), while Stage 2 issues a follow-up prompt explicitly instructing the model to extract the exact answer phrase.

### Mechanism 3
Answer format-specific fine-tuning (short-only vs. long-only vs. combined) produces different tradeoffs between semantic understanding and exact match precision. Combined training yields the best test EM (0.2) while short-only and long-only approaches collapse to 0.0 EM on test.

## Foundational Learning

- **Multi-hop Reasoning in Biomedical QA**: MedHopQA questions require synthesizing information across multiple documents or concepts (e.g., linking genes to diseases through intermediate relationships). Quick check: Given a question asking "Which gene on chromosome 17 is associated with breast cancer?", can your system identify that it must first retrieve chromosome-gene mappings, then filter by disease association?

- **Exact Match vs. Concept-Level Evaluation**: EM requires strict string matching after normalization, while concept-level evaluation accepts semantically equivalent answers. The paper shows a large gap between these metrics (EM ~0.2-0.5 vs. concept ~0.3-0.8). Quick check: If the gold answer is "Chromosome 2" and your model outputs "2 chromosome", would EM mark this as correct? Would concept-level evaluation?

- **Quantized Fine-tuning with LoRA Adapters**: Full fine-tuning of 8B-parameter models is resource-intensive. The authors use 4-bit quantization with LoRA (rank=64, alpha=16) applied to attention and linear layers to enable training on two RTX 3080 Ti GPUs within approximately one day. Quick check: What is the tradeoff between LoRA rank and adapter capacity? How does quantization affect gradient computation during backpropagation?

## Architecture Onboarding

- **Component map**: External QA datasets -> Data curation and filtering -> LLaMA 3 8B with 4-bit quantization + LoRA -> Three training variants (combined/short-only/long-only) -> Two-stage inference pipeline -> Post-processing and evaluation

- **Critical path**: Data curation and filtering (garbage-in, garbage-out risk is high with heterogeneous external sources) → Prompt template design (domain-specific prompts improved short-answer conciseness) → Two-stage extraction (determines whether semantic understanding converts to EM-compatible output)

- **Design tradeoffs**: Combined vs. format-specific training (Combined yields better test EM but requires more data preparation; short-only may overfit to concise patterns at the cost of reasoning depth) → Two-stage inference vs. single-pass (Two-stage adds latency but improves answer precision; fallback mechanism preserves semantic utility at the cost of EM) → External data scale vs. quality (10,000 QA pairs provide coverage, but noisy entries may misalign the model from MedHopQA's multi-hop reasoning requirements)

- **Failure signatures**: Verbose outputs instead of concise short answers (e.g., restating question, adding context) → Format inconsistencies causing EM failures despite correct semantics (e.g., "Chromosome 2" vs "Chr.2" vs "2") → Test distribution shift (validation EM ~0.5 drops to 0.0-0.2 on test) → Zero-shot hallucination (untrained models hallucinate on biomedical reasoning questions, especially regarding chromosomes, genes, and rare diseases)

- **First 3 experiments**:
  1. Baseline zero-shot evaluation: Test LLaMA 3 8B and Qwen 7B on MedHopQA dev set without fine-tuning to establish lower bounds and identify systematic failure modes
  2. Format-specific fine-tuning ablation: Train three separate models (combined, short-only, long-only) on the curated 10,000 QA pairs and compare EM/concept scores on validation
  3. Post-processing impact analysis: Run the best-performing model with and without two-stage extraction, measuring EM delta and characterizing cases where extraction succeeds vs. fails

## Open Questions the Paper Calls Out

### Open Question 1
Can reinforcement learning techniques effectively align LLM outputs with strict Exact Match evaluation criteria in biomedical QA? The conclusion states, "Future work will explore... reinforcement learning techniques to align model outputs with evaluation criteria more reliably." This remains unresolved as current supervised fine-tuning achieved high semantic understanding (0.8 concept accuracy) but failed strict formatting (0.0 EM on test sets).

### Open Question 2
What specific extraction mechanisms or post-processing architectures are required to bridge the gap between verbose semantic understanding and concise exact answers? The abstract notes the "need for improved post-processing and output control strategies" and the conclusion calls for "more robust extraction mechanisms." The authors' two-stage inference pipeline only offered "partial improvements" and often failed to generate valid short answers.

### Open Question 3
Why does supervised fine-tuning on external datasets (BioASQ, MedQuAD) fail to generalize to the hidden MedHopQA test set, resulting in 0.0 EM scores for specific answer formats? Despite adding 10,000 external QA pairs, the "short-only" and "long-only" models scored 0.0 EM on the test set, indicating a failure to generalize to the specific formatting or distribution of the hidden test questions.

## Limitations
- Persistent gap between semantic understanding (0.8 concept accuracy) and exact match performance (0.0-0.2 EM)
- Two-stage extraction pipeline shows promise but remains imperfect, requiring fallback to verbose outputs
- External dataset augmentation may introduce distribution shift that misaligns with MedHopQA's specific multi-hop reasoning patterns

## Confidence

- **High confidence**: The core observation that semantic understanding (concept-level accuracy ~0.8) significantly exceeds exact match performance (EM ~0.0-0.2) is well-supported by validation and test set results across all three training approaches
- **Medium confidence**: The claim that combined format training outperforms format-specific approaches on test EM is supported by the reported results, though the mechanism explaining why format exposure improves performance could be more rigorously tested
- **Medium confidence**: The two-stage inference pipeline's effectiveness in improving answer precision is demonstrated through the comparison with single-pass generation, though the specific implementation details of the extraction prompts remain underspecified

## Next Checks

1. **Manual annotation study**: Have domain experts independently evaluate a sample of predictions where concept-level accuracy is high but EM is low, categorizing failures into semantic misunderstanding, format deviation, and domain-specific terminology issues

2. **Format normalization ablation**: Implement comprehensive biomedical normalization rules (chromosome notation variants, gene naming conventions, disease synonyms) and measure the delta in EM scores to isolate how much performance is lost to format inconsistencies versus semantic errors

3. **Fine-tuning curriculum analysis**: Systematically vary the ratio of short-only to long-only examples in the combined training set and measure the impact on both EM and concept-level accuracy to identify the optimal balance for this task