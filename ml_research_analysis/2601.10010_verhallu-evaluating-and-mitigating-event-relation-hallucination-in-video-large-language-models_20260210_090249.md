---
ver: rpa2
title: 'VERHallu: Evaluating and Mitigating Event Relation Hallucination in Video
  Large Language Models'
arxiv_id: '2601.10010'
source_url: https://arxiv.org/abs/2601.10010
tags:
- event
- relation
- video
- hallucination
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses event relation hallucination in VideoLLMs,
  a problem where models fail to correctly understand causal, temporal, and subevent
  relations between events in dense video scenarios. Existing benchmarks focus on
  object/scene existence rather than event relation reasoning, leaving this gap unexplored.
---

# VERHallu: Evaluating and Mitigating Event Relation Hallucination in Video Large Language Models

## Quick Facts
- arXiv ID: 2601.10010
- Source URL: https://arxiv.org/abs/2601.10010
- Reference count: 40
- Primary result: State-of-the-art VideoLLMs struggle with event relation understanding, performing near random chance on relation classification and below 40% on QA tasks; KFP mitigates hallucinations by up to 39.1% F1 improvement

## Executive Summary
This paper addresses event relation hallucination in VideoLLMs, where models fail to correctly understand causal, temporal, and subevent relations between events in dense video scenarios. Existing benchmarks focus on object/scene existence rather than event relation reasoning, leaving this gap unexplored. The authors introduce VERHallu, a comprehensive benchmark evaluating event relation hallucination across three tasks: relation classification, question answering, and counterfactual QA. Using counterintuitive videos to minimize pretraining bias, VERHallu reveals significant performance gaps in current VideoLLMs. To mitigate these hallucinations, they propose Key-Frame Propagating (KFP), a training-free method that enhances frame-level attention around key events within intermediate layers using Gaussian-weighted propagation, improving relation classification and question answering performance while maintaining inference speed.

## Method Summary
VERHallu is a benchmark with 574 video clips from Mr. Bean content, containing 7,676 samples across three tasks: Relation Classification (5,742 samples), Question Answering (967 samples), and Counterfactual QA (967 samples). The benchmark evaluates VideoLLMs on causal, temporal, and subevent relation types using counterintuitive scenarios to reduce language priors. KFP is a training-free method that operates on intermediate layers 8-15, extracting visual tokens and applying Gaussian-weighted attention propagation to neighboring frames around key events. Enhanced features are fused with original hidden states using weighted averaging (β=0.6). The method improves relation classification F1 scores by up to 39.1% and enhances overall video event relation understanding (SRH metric) while maintaining inference speed.

## Key Results
- State-of-the-art VideoLLMs perform near random chance on relation classification and below 40% accuracy on QA tasks
- KFP improves relation classification F1 scores by up to 39.1% across tested models
- VERHallu benchmark reveals significant event relation hallucination in current VideoLLMs
- KFP maintains inference speed while effectively mitigating event relation hallucinations
- Performance improvements are most pronounced for relation classification and QA tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reallocating frame-level attention from key frames to adjacent frames improves event relation reasoning.
- Mechanism: KFP identifies top-k key frames via attention scores, then applies temporal Gaussian weighting to propagate attention to neighboring frames within a window of size m. Enhanced visual features are computed as V_E = Softmax(α+1)*V and fused with original hidden states via H' = (1-β)*H_E + β*H.
- Core assumption: Key frames identified by attention scores correspond to event-critical moments; Gaussian propagation captures temporal locality of related events.
- Evidence anchors:
  - [abstract] "KFP effectively mitigates event relation hallucination without affecting inference speed, improving relation classification and question answering performance."
  - [Section IV.B] "For each selected key frame t*_i, a temporal Gaussian weighting is applied over a window of m frames to propagate its attention score to adjacent frames."
  - [corpus] Weak corpus support; related work SEASON addresses temporal hallucination via contrastive decoding rather than attention propagation.
- Break condition: If attention scores do not correlate with event-relevant frames (e.g., attention focused on static backgrounds), KFP amplifies noise rather than signal.

### Mechanism 2
- Claim: Counterintuitive video scenarios reduce reliance on language priors and expose event relation hallucination.
- Mechanism: Videos are sampled from "Mr. Bean" episodes where event sequences appear counterintuitive but are logically grounded. Human annotators create biased candidates (vision-language bias, language bias) to probe whether models rely on priors vs. visual evidence.
- Core assumption: Counterintuitive scenarios are underrepresented in pretraining; models will fail if relying solely on priors.
- Evidence anchors:
  - [abstract] "benchmark features counterintuitive video scenarios that deviate from typical pretraining distributions"
  - [Section III.A] "carefully curated a dataset based on 'Mr. Bean' videos, which feature event sequences that appear counterintuitive at first glance, yet are grounded in real-world logic"
  - [corpus] No direct corpus support for this specific benchmark design strategy.
- Break condition: If pretraining data contains similar counterintuitive content, or if models overfit to this specific distribution during evaluation, diagnostic validity decreases.

### Mechanism 3
- Claim: VideoLLMs exhibit attention concentration on early frames and insufficient attention to surrounding subevents, causing relation-level hallucinations.
- Mechanism: Token Activation Map (TAM) visualization shows models attend to early frames and key event mentions but miss temporally proximate frames containing relational cues. This leads to reliance on vision-language or language biases.
- Core assumption: TAM visualization accurately reflects the model's effective attention during generation; attention distribution correlates with reasoning quality.
- Evidence anchors:
  - [Section IV.A] "QwenVL-2.5-7B tends to concentrate on the early frames of the video... fails to attend adequately to frames near the key moments"
  - [Section IV.A] "model exhibits a tendency to rely on incorrect or incomplete visual information, thereby exhibiting more serious vision&language bias"
  - [corpus] SEASON paper confirms VideoLLMs struggle with temporal inconsistency, supporting the broader attention-reasoning link.
- Break condition: If attention patterns are emergent but not causally related to reasoning failures (correlation ≠ causation), interventions targeting attention may not improve performance.

## Foundational Learning

- Concept: **Attention mechanisms in transformer-based VideoLLMs**
  - Why needed here: KFP modifies attention distribution across frames; understanding self-attention, cross-attention, and how visual tokens interact with language tokens is essential to interpret the intervention.
  - Quick check question: Can you explain how attention scores determine which visual tokens influence the output token generation?

- Concept: **Event relation types (causal, temporal, subevent)**
  - Why needed here: The benchmark evaluates three distinct relation types; each requires different reasoning patterns (e.g., temporal = before/after ordering; causal = cause/effect inference).
  - Quick check question: Given two events "The man opened the door" and "The man entered the room," what is the temporal vs. causal relation?

- Concept: **Hallucination in multimodal models**
  - Why needed here: The paper distinguishes existence-based hallucination (object/event presence) from relation hallucination (incorrect inference of event relationships); understanding this taxonomy clarifies the problem scope.
  - Quick check question: If a model correctly identifies that "a person is cooking" but incorrectly states "they are cooking because they're angry" when no evidence supports this, what type of hallucination is this?

## Architecture Onboarding

### Component Map
QwenVL-2.5-7B -> KFP (layers 8-15) -> Enhanced attention distribution -> Improved relation classification/QA

### Critical Path
Video input → Visual encoder → Cross-attention with language → Intermediate layers 8-15 → KFP attention enhancement → Final output generation

### Design Tradeoffs
KFP trades increased computational overhead in intermediate layers for improved relation reasoning without full model retraining. The training-free approach enables rapid deployment but may be less effective than architecture-specific optimizations.

### Failure Signatures
- CFQA performance degradation when KFP amplifies semantically insignificant visual features in counterfactual scenarios
- Models outputting text instead of candidate numbers due to prompt format issues
- Attention propagation amplifying noise when attention scores don't correlate with event-relevant frames

### Exactly 3 First Experiments
1. Test baseline QwenVL-2.5-7B on VERHallu relation classification task using provided prompts to establish baseline performance
2. Implement KFP with exact parameters (layers 8-15, m=5, σ=1, β=0.6) on relation classification task and measure F1 score improvement
3. Verify KFP maintains inference speed by comparing inference times with and without KFP on same model and dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural modifications or training paradigms are required to move beyond attention redistribution (like KFP) toward explicit visual perception tailored for event relation reasoning?
- Basis: [explicit] The authors state in the Limitation Analysis: "Future efforts should focus on enhancing VideoLLMs via improved architectures and training data tailored for event relation reasoning," noting that KFP lacks the visual perception capacity explicitly oriented toward relation reasoning.
- Why unresolved: The current KFP strategy is training-free and operates on intermediate layers; it enhances saliency but does not fundamentally alter the model's inherent ability to perceive complex relations.
- What evidence would resolve it: A new model architecture or fine-tuning method that outperforms KFP on VERHallu by integrating relational inductive biases directly into the visual encoder or fusion mechanism.

### Open Question 2
- Question: Can a mechanism be developed to adaptively suppress attention enhancement in counterfactual scenarios to prevent the amplification of semantically insignificant visual features?
- Basis: [explicit] The results in Table V show KFP degrades CFQA performance on QwenVL-2.5-7B (-6.0%). The text notes: "In CFQA... applying KFP to the middle layers amplifies semantically insignificant visual features in counterfactual scenarios... This is a major limitation of KFP."
- Why unresolved: KFP currently applies a static Gaussian weighting based on key frames, which fails to distinguish between frames containing relevant relations and frames with noise when the event is absent (counterfactual).
- What evidence would resolve it: A dynamic variant of KFP that maintains or improves QA scores while recovering the lost performance in CFQA tasks.

### Open Question 3
- Question: How can VideoLLMs be improved to transition from identifying individual key events to "comprehensively understanding the relations between events based on their evolving details"?
- Basis: [explicit] The Limitation Analysis states: "Event relation hallucination is not merely a deficiency in capturing individual events; rather, it reflects a deeper challenge in comprehensively understanding the relations between events based on their evolving details."
- Why unresolved: The analysis reveals that while models like QwenVL-2.5-7B can ground key events accurately, they often overlook surrounding subevents necessary for inferring causal or temporal relations.
- What evidence would resolve it: Improvements in the "SRH" (Structure Relation Hallucination) metric, indicating that models can correctly synthesize information from subevents to answer relation queries.

## Limitations
- VERHallu benchmark may not generalize beyond Mr. Bean video distribution
- KFP implementation details remain underspecified, particularly top-k selection threshold
- Counterintuitive video scenarios' effectiveness in reducing language priors not rigorously tested
- Performance improvements show substantial variance across models and tasks
- CFQA performance degradation when KFP amplifies irrelevant features in counterfactual scenarios

## Confidence

**High Confidence**: VideoLLMs struggle with event relation reasoning (baseline performance near random chance or below 40% accuracy); existing benchmarks inadequately evaluate event relation understanding.

**Medium Confidence**: KFP improves relation classification and overall SRH metrics; counterintuitive videos reduce language bias (plausible but not rigorously tested).

**Low Confidence**: KFP maintains inference speed (difficult to verify given underspecified implementation); attention concentration patterns definitively cause relation hallucination (may reflect correlation rather than causation); generalizability of counterintuitive scenarios to broader event relation reasoning challenges.

## Next Checks

1. **Reproduce KFP with specified parameters**: Implement KFP with exact parameters (layers 8-15, m=5, σ=1, β=0.6) on baseline model (QwenVL-2.5-7B recommended) and verify reported F1 improvements on relation classification. Document top-k selection method used.

2. **Cross-distribution validation**: Test baseline models and KFP-enhanced versions on non-Mr. Bean dataset with event relation annotations to verify counterintuitive scenarios generalize and KFP improvements transfer.

3. **Attention-reasoning causality test**: Conduct ablation studies where attention is artificially manipulated (without KFP's Gaussian propagation) to determine whether performance improvements are directly attributable to attention redistribution versus other factors.