---
ver: rpa2
title: Primal-Dual Direct Preference Optimization for Constrained LLM Alignment
arxiv_id: '2510.05703'
source_url: https://arxiv.org/abs/2510.05703
tags:
- optimal
- reward
- preference
- have
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses constrained alignment of large language models
  (LLMs), aiming to maximize output reward while restricting harmful content generation
  below a threshold. The authors propose a primal-dual direct preference optimization
  (DPO) approach that first trains a model using standard DPO on reward preference
  data to provide reward information, then fine-tunes LLMs using a rearranged Lagrangian
  DPO objective with cost preference data.
---

# Primal-Dual Direct Preference Optimization for Constrained LLM Alignment

## Quick Facts
- **arXiv ID**: 2510.05703
- **Source URL**: https://arxiv.org/abs/2510.05703
- **Reference count**: 40
- **Key outcome**: PD-DPO approach trains constrained LLMs without explicit reward/cost models, reducing memory costs by 40-60% while achieving effective helpfulness-harmlessness trade-offs on PKU-SafeRLHF dataset

## Executive Summary
This paper addresses constrained alignment of large language models by maximizing output reward while restricting harmful content generation below a threshold. The authors propose a primal-dual direct preference optimization (DPO) approach that avoids training separate reward and cost models, instead extracting reward information implicitly from a DPO-trained policy and directly optimizing the policy on cost preference data. The method combines standard DPO on reward preference data with a rearranged Lagrangian DPO objective on cost preference data, iteratively updating Lagrange multipliers based on binary safety feedback. Theoretical guarantees establish bounds on suboptimality and constraint violation, while experiments demonstrate effective performance compared to baseline methods.

## Method Summary
The method consists of two phases: (1) Train a policy π*_r via standard DPO on reward preference data to extract implicit reward information, and (2) Iteratively train policies π_k using a rearranged Lagrangian DPO objective that directly optimizes the policy using cost preference data and reward information from π*_r. After each policy update, the algorithm collects binary safety feedback to estimate expected cost, then updates the Lagrange multiplier λ via projected subgradient descent. The final output is a uniform mixture over all trained policies. This approach eliminates the need for explicit reward and cost models, significantly reducing computational and memory requirements while maintaining theoretical guarantees on constraint satisfaction.

## Key Results
- PD-DPO achieves effective helpfulness-harmlessness trade-offs on PKU-SafeRLHF dataset compared to baseline methods
- The approach reduces memory and computational costs by 40-60% compared to model-based constrained alignment methods
- Theoretical analysis establishes O(λ_1C_max/√K) bounds on suboptimality and constraint violation
- The method successfully handles exploration in uncovered prompt-response space through exploration bonuses

## Why This Works (Mechanism)

### Mechanism 1: Implicit Reward Extraction via Standard DPO
Reward information can be extracted from a DPO-trained policy without explicitly training a reward model. Standard DPO on reward preference data trains a policy π*_r̂ that implicitly encodes reward information through the relationship r(x,y) = βlog(π*_r̂(y|x)/π_ref(y|x)) + βlog(Z_r̂(x)). This allows expressing reward differences between response pairs directly from policy log-probabilities.

### Mechanism 2: Rearranged Lagrangian Objective Bypasses Cost Model Training
The Lagrangian constraint optimization can be reformulated to train directly on cost preference data without learning an explicit cost model. Rearranging the optimal policy equation for the Lagrangian objective yields: c(x,y) = (1/λ)(r(x,y) - βlog(π*/π_ref) - βlog(Z)). Substituting this into the Bradley-Terry cost preference likelihood produces a training objective where the optimization variable is the policy itself, not a cost function.

### Mechanism 3: Primal-Dual Iteration with Projected Subgradient Descent
Alternating between policy optimization and Lagrange multiplier updates converges to a solution with bounded suboptimality and constraint violation. After each policy training step, the algorithm estimates the expected cost using binary safety feedback from human annotators, then updates λ via projected subgradient descent: λ_{k+1} = Proj[0,2ρ](λ_k + ηc̃_k). This creates a feedback loop that adjusts λ toward the optimal value that balances constraint satisfaction.

## Foundational Learning

- **Concept: Bradley-Terry Preference Model**
  - Why needed here: All preference data (both reward and cost) is assumed generated under this model, and the entire training objective derives from it. The paper uses Pr[y^w ≻ y^ℓ|x] = σ(r*(x,y^w) - r*(x,y^ℓ)) where σ is the sigmoid function.
  - Quick check question: Can you derive why the partition function Z(x) cancels out when computing preference probabilities from the optimal policy?

- **Concept: Lagrangian Duality for Constrained Optimization**
  - Why needed here: The constrained alignment problem (maximize reward subject to expected cost ≤ 0) is solved by converting to unconstrained Lagrangian form: max_π E[r*(x,y) - λc*(x,y)] - β·KL(π||π_ref). Understanding how λ controls the reward-cost tradeoff is essential.
  - Quick check question: What happens to the solution if λ is set too high vs. too low? What does λ → ∞ imply about constraint satisfaction?

- **Concept: KL Divergence Regularization in Policy Space**
  - Why needed here: The KL penalty β·KL(π||π_ref) appears in both the standard DPO derivation and the Lagrangian objective. It prevents the policy from deviating too far from the reference model, which stabilizes training but also limits how much improvement can be achieved.
  - Quick check question: How does the β parameter affect the tradeoff between reward maximization and policy stability? What happens as β → 0?

## Architecture Onboarding

- **Component map**: Alpaca-7b-reproduced (reference model) -> π*_r (reward DPO) -> π_k (cost DPO with Lagrangian) -> binary feedback -> λ updates -> π^out_K (uniform mixture)

- **Critical path**:
  1. Ensure reward preference data D_r has adequate coverage of helpfulness dimensions
  2. Verify cost preference data D_c properly distinguishes safer vs. unsafe responses
  3. Binary feedback mechanism must be operational for cost estimation
  4. λ initialization and projection bounds [0, 2ρ] must be set based on estimated constraint slack

- **Design tradeoffs**:
  - **Memory efficiency vs. performance**: Eliminates reward/cost model training (saves ~40-60% GPU memory vs. Safe RLHF) but may underperform model-based approaches that can learn more complex cost functions
  - **Number of iterations K**: Higher K improves theoretical bounds but increases annotation cost for binary feedback
  - **Cost estimation samples (N_CE, M_CE)**: More samples improve c̃_k accuracy but require more human annotations per iteration

- **Failure signatures**:
  - **Constraint violation persists or grows**: Check if λ is being updated properly; may need to increase η or verify binary feedback quality
  - **Reward performance degrades excessively**: λ may be too large; check initial λ_1 and projection bound ρ
  - **Training instability or loss divergence**: Check β value and policy search range constraints (Π_r, Π_c^k); may need to reduce learning rate

- **First 3 experiments**:
  1. **Baseline comparison on PKU-SafeRLHF**: Run PD-DPO with default hyperparameters (β=0.1, K=5-10, λ_1=5) and compare reward/cost scores against SFT model, Beaver-v3.0, and SafeDPO using Beaver-7b evaluation models
  2. **Ablation on Lagrange multiplier initialization**: Test λ_1 ∈ {1, 5, 10} with fixed K=10 to understand sensitivity to initial λ and identify stable operating range
  3. **Cost estimation sample size study**: Vary N_CE and M_CE to characterize the tradeoff between annotation cost and constraint violation, measuring how quickly c̃_k converges to true expected cost

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the theoretical guarantees for suboptimality and constraint violation be extended to general neural network policy parameterizations? The conclusion identifies extending theoretical results to the "policy parameterization (large prompt-response space) setting" as a primary direction for future work.

- **Open Question 2**: How can the method be modified to handle stricter cost constraints, such as ensuring safety for every individual response? The authors state that investigating "stricter cost constraints, e.g., per-response constraints," is a challenging future direction.

- **Open Question 3**: How does the algorithm's performance degrade if the Slater condition (existence of a strictly safe policy) is violated or if the constant $\rho$ is misspecified? The theoretical results rely on Assumption 1 (Slater's condition) and the knowledge of the constant $\rho$, which may be difficult to determine in practice.

## Limitations
- The quality of implicit reward extraction from DPO-trained policies is not directly validated against ground-truth rewards
- Binary feedback mechanism relies on human annotators, introducing potential noise and sampling bias that could affect λ convergence
- Theoretical bounds assume Slater's condition holds and that cost estimates are sufficiently accurate, but these assumptions are not empirically verified
- Online data extension with exploration bonuses is only briefly mentioned without experimental validation

## Confidence

- **High confidence**: The mathematical derivation of the rearranged Lagrangian DPO objective and the theoretical suboptimality/convergence bounds (Theorem 1)
- **Medium confidence**: The effectiveness of implicit reward extraction from DPO-trained policies, as demonstrated through empirical results on PKU-SafeRLHF
- **Low confidence**: The online data extension with exploration bonuses, as this component is only briefly mentioned without experimental validation or detailed methodology

## Next Checks

1. **Reward extraction validation**: Compare the implicit reward estimates from π*_r against an explicitly trained reward model on held-out data to quantify extraction accuracy

2. **Cost estimation noise analysis**: Systematically vary N_CE and M_CE to characterize how estimation noise in c̃_k affects λ convergence and constraint violation

3. **Hyperparameter sensitivity study**: Test the algorithm's robustness to λ initialization, β values, and K iterations across multiple random seeds to establish stable operating ranges