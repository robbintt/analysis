---
ver: rpa2
title: 'Towards Consistent Detection of Cognitive Distortions: LLM-Based Annotation
  and Dataset-Agnostic Evaluation'
arxiv_id: '2511.01482'
source_url: https://arxiv.org/abs/2511.01482
tags:
- labels
- label
- across
- distortion
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of subjective annotation in
  cognitive distortion detection by proposing an LLM-based framework that uses multiple
  independent runs to identify consistent labels, achieving high inter-run agreement
  (Fleiss's Kappa = 0.78). A dataset-agnostic evaluation method using Cohen's kappa
  as an effect size measure is introduced to enable fair cross-study comparisons.
---

# Towards Consistent Detection of Cognitive Distortions: LLM-Based Annotation and Dataset-Agnostic Evaluation

## Quick Facts
- arXiv ID: 2511.01482
- Source URL: https://arxiv.org/abs/2511.01482
- Reference count: 18
- Key outcome: LLM-based multi-run consistency framework achieves high inter-run agreement (Fleiss's Kappa = 0.78) and outperforms human-labeled models in cognitive distortion detection

## Executive Summary
This study addresses the fundamental challenge of subjective annotation in cognitive distortion detection by proposing a novel LLM-based framework that generates consistent labels through multiple independent runs. The approach tackles the variability inherent in human annotation of psychological constructs by leveraging large language models to produce stable, reproducible annotations. A dataset-agnostic evaluation method using Cohen's kappa as an effect size measure is introduced to enable fair cross-study comparisons.

The research demonstrates that models trained on LLM-generated annotations consistently outperform those trained on human-labeled data, with improvements over random baselines of 33.1% versus 16.1% in some configurations. The multi-run consistency approach, achieving Fleiss's Kappa of 0.78, provides a systematic method for handling subjective NLP tasks where traditional annotation agreement metrics may be insufficient.

## Method Summary
The proposed framework uses GPT-4 to generate annotations through multiple independent runs (5 runs in the study), with final labels determined by majority voting among consistent outputs. This multi-run consistency approach addresses the inherent subjectivity in cognitive distortion detection by identifying stable label patterns across different model executions. The framework includes a dataset-agnostic evaluation method that uses Cohen's kappa as an effect size measure, enabling comparison across studies with different label distributions and annotation schemes.

For training, the study employs BERT-base and RoBERTa-base models fine-tuned on the LLM-generated annotations. The evaluation compares these models against those trained on human-labeled data across three cognitive distortion datasets. The approach systematically quantifies annotation consistency and model performance improvements, providing a reproducible methodology for subjective NLP tasks.

## Key Results
- Multi-run consistency approach achieves high inter-run agreement with Fleiss's Kappa = 0.78
- Models trained on LLM-generated annotations outperform human-labeled models by 33.1% vs 16.1% over random baselines in some configurations
- Dataset-agnostic evaluation framework using Cohen's kappa enables fair cross-study comparisons
- BERT-base and RoBERTa-base models fine-tuned on LLM annotations show consistent performance improvements

## Why This Works (Mechanism)
The framework succeeds by addressing the core challenge of subjective annotation variability through systematic consistency checking. By running the LLM multiple times and selecting labels that appear consistently across runs, the approach filters out idiosyncratic outputs and identifies stable patterns that better capture the underlying psychological constructs. This is particularly effective for cognitive distortions, which are inherently subjective and lack clear, objective boundaries.

The dataset-agnostic evaluation method works by using Cohen's kappa as an effect size measure, which accounts for label distribution differences across datasets. This allows meaningful comparison of model performance even when studies use different annotation schemes or have varying class imbalances, solving a critical limitation in subjective NLP research where direct performance comparisons are often meaningless.

## Foundational Learning

**Cognitive Distortions** - Systematic errors in thinking patterns that contribute to psychological distress. Why needed: Core target phenomenon requiring detection. Quick check: Can identify common examples like catastrophizing or all-or-nothing thinking.

**LLM-based Annotation** - Using large language models to generate labels for subjective tasks. Why needed: Provides scalable, consistent alternative to human annotation. Quick check: Can reproduce annotations across multiple runs with high agreement.

**Inter-run Agreement** - Consistency of model outputs across independent executions. Why needed: Measures reliability of LLM annotations. Quick check: Fleiss's Kappa above 0.7 indicates substantial agreement.

**Cohen's Kappa** - Statistical measure of inter-rater agreement that accounts for chance. Why needed: Enables fair comparison across different datasets. Quick check: Values above 0.6 indicate substantial agreement.

**Majority Voting** - Consensus method where final label is determined by most frequent choice. Why needed: Resolves inconsistencies in multi-run outputs. Quick check: Reduces noise from outlier LLM responses.

**Effect Size Measures** - Standardized metrics that enable comparison across different studies. Why needed: Allows meaningful evaluation across datasets with different distributions. Quick check: Cohen's kappa values provide comparable effect sizes.

## Architecture Onboarding

**Component Map**: Raw Text -> LLM Annotation (5 runs) -> Majority Voting -> Final Labels -> BERT/RoBERTa Training -> Performance Evaluation

**Critical Path**: The annotation generation pipeline (LLM runs â†’ majority voting) is the most critical component, as it directly determines the quality of training data. The dataset-agnostic evaluation framework is the second most critical component, enabling meaningful performance comparisons.

**Design Tradeoffs**: The multi-run approach increases computational cost by 5x but significantly improves annotation consistency. Using a single LLM (GPT-4) provides consistency but may introduce systematic biases. The dataset-agnostic evaluation sacrifices some granularity for broader comparability.

**Failure Signatures**: Low inter-run agreement (Fleiss's Kappa < 0.6) indicates poor annotation stability. Poor performance on LLM-generated data suggests the model cannot capture subjective patterns. Low Cohen's kappa effect sizes indicate the evaluation framework may not be capturing meaningful differences.

**First Experiments**:
1. Run GPT-4 on sample texts 5 times and calculate Fleiss's Kappa to verify annotation consistency
2. Apply dataset-agnostic evaluation framework to compare two existing cognitive distortion datasets
3. Train BERT-base on human-labeled vs LLM-generated annotations to measure performance differences

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on single LLM (GPT-4) may introduce systematic biases not present in human annotators
- Dataset-agnostic evaluation framework validated on only three datasets, limiting generalizability
- Performance improvements based on limited dataset combinations (three datasets)
- Computational cost of multi-run approach (5x more expensive than single-run annotation)

## Confidence

**High confidence**: Multi-run consistency methodology and Fleiss's Kappa results (0.78) are methodologically sound and reproducible
**Medium confidence**: Dataset-agnostic evaluation framework effectiveness across diverse domains
**Medium confidence**: Generalizability of LLM annotation superiority to other subjective NLP tasks

## Next Checks

1. Replicate the multi-run consistency approach with different LLMs (Claude, Gemini) to assess model-specific biases and robustness
2. Test the dataset-agnostic evaluation framework on at least 10 diverse subjective NLP datasets beyond cognitive distortions
3. Conduct human evaluation studies comparing LLM-generated annotations against expert annotators on blind samples to validate performance claims