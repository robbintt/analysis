---
ver: rpa2
title: Wide Neural Networks as a Baseline for the Computational No-Coincidence Conjecture
arxiv_id: '2510.06527'
source_url: https://arxiv.org/abs/2510.06527
tags:
- neural
- gaussian
- activation
- networks
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates when randomly initialized neural networks
  with large width behave like random functions, specifically exhibiting nearly independent
  outputs. The authors leverage a perturbative expansion from Roberts et al.
---

# Wide Neural Networks as a Baseline for the Computational No-Coincidence Conjecture

## Quick Facts
- arXiv ID: 2510.06527
- Source URL: https://arxiv.org/abs/2510.06527
- Authors: John Dunbar; Scott Aaronson
- Reference count: 23
- Key result: Zero-mean activation functions (⟨σ(z)⟩ = 0) are necessary and sufficient for randomly initialized wide neural networks to have nearly independent outputs

## Executive Summary
This paper establishes when randomly initialized neural networks with large width behave like random functions with nearly independent outputs. The authors prove that output independence emerges precisely when the activation function is nonlinear and has zero mean under the Gaussian measure: E[σ(z)] = 0. This condition is satisfied by tanh and shifted ReLU/GeLU functions but not by standard ReLU or GeLU. The main theoretical result shows that under appropriate hyperparameter settings (including critical tuning generalizing Kaiming initialization), the covariance between outputs of sufficiently wide and deep networks decays exponentially when the zero-mean condition holds. The authors propose these zero-mean activation networks as a candidate construction for the computational no-coincidence conjecture in AI interpretability research.

## Method Summary
The authors leverage a perturbative expansion from Roberts et al. (2022) to analyze finite-width networks and prove that output independence emerges when activation functions have zero mean under the Gaussian measure. They derive a deterministic map C(k) governing how correlations between inputs evolve through layers, showing that the fixed point k*=0 exists iff C(0)=0, which is equivalent to ⟨σ(z)⟩=0. The analysis requires specific hyperparameter settings including critical tuning and exponential width-depth scaling n=exp(Θ(ℓ)) to ensure O(1/n) corrections remain smaller than the exponential correlation decay.

## Key Results
- Zero-mean activation functions (⟨σ(z)⟩ = 0) are necessary and sufficient for nearly independent outputs
- Standard ReLU and GeLU do not satisfy this condition (⟨ReLU(z)⟩ ≈ 0.3989 ≠ 0)
- Shifted ReLU (σ(z) = ReLU(z) - 1/√(2π)) and tanh satisfy the zero-mean condition
- Output correlations decay exponentially under zero-mean activations when width and depth scale appropriately

## Why This Works (Mechanism)

### Mechanism 1: Covariance Dynamics Govern Output Independence
The correlation between network outputs on different inputs evolves through layers via a deterministic map C(k), converging to a fixed point that determines long-range independence. For two inputs α₁, α₂, their covariance K^(ℓ)ₐ₁ₐ₂ propagates as K^(ℓ+1)ₐ₁ₐ₂ = C(K^(ℓ)ₐ₁ₐ₂) = ⟨σ(z₁)σ(z₂)⟩_Σ / ⟨σ(z)²⟩, where Σ is a 2×2 correlation matrix. Repeated application of C drives covariances toward an attractive fixed point k* ∈ [0,1]. The perturbative expansion from Roberts et al. (2022) accurately captures finite-width behavior, ignoring O(1/n²) effects.

### Mechanism 2: Zero-Mean Condition Determines the Fixed Point
The activation function having zero mean under the Gaussian measure (E[σ(z)] = 0 for z ~ N(0,1)) is necessary and sufficient for correlations to decay to zero. Via Hermite decomposition σ(z) = Σₙ aₙ Heₙ(z), the map simplifies to C(k) = (1/⟨σ(z)²⟩) Σₙ (aₙ)² n! kⁿ. The fixed point at k=0 exists iff C(0) = 0, which occurs iff a₀ = ⟨σ(z)⟩ = 0. This means the first Hermite coefficient must vanish. The activation function must be square-integrable under Gaussian measure (⟨σ(z)²⟩ < ∞).

### Mechanism 3: Near-Gaussian Distribution Enables Clean Analysis
At finite but large width, preactivation distributions are nearly Gaussian with deviations controlled by width, allowing covariance dynamics to dominate behavior. The probability distribution P(z^(ℓ)|D) has leading-order Gaussian structure with covariance K^(ℓ) and corrections at O(1/n) and O(1/n²). This means correlation decay in the Gaussian component (controlled by C(k)) is the primary driver of independence. Width and depth must scale appropriately (exponentially: n = exp(Θ(ℓ))) so that O(1/n) corrections remain smaller than the exponential correlation decay.

## Foundational Learning

- **Concept: Hermite Polynomials and Gaussian Expectations**
  - Why needed here: The entire analysis rests on decomposing activation functions into Hermite polynomials to compute correlations ⟨σ(z₁)σ(z₂)⟩_Σ analytically.
  - Quick check question: If σ(z) = 3z² - 3 (the 2nd Hermite polynomial), what is ⟨σ(z)⟩ over a standard Gaussian?

- **Concept: Covariance Propagation in Deep Networks**
  - Why needed here: Understanding how correlations between inputs evolve through layers is the core dynamical system being analyzed.
  - Quick check question: If two inputs have covariance K = 0.5 at layer ℓ, and C(k) = 0.9k + 0.1, what happens to their correlation as depth → ∞?

- **Concept: Fixed Point Analysis and Contraction Maps**
  - Why needed here: The proof relies on showing C(k) is a contraction mapping with an attractive fixed point, guaranteeing convergence.
  - Quick check question: If C(k) = k² for k ∈ [0,1], what are the fixed points and which are stable?

## Architecture Onboarding

- **Component map:** Input → Hidden layers (width n, critical tuning) → Output layer
- **Critical path:**
  1. Choose zero-mean activation (shift standard activations or use tanh)
  2. Set hyperparameters per critical tuning (Eq. 10)
  3. Ensure width/depth scaling satisfies n = exp(Θ(ℓ))
  4. Verify initial inputs aren't scalar multiples (|K^(1)ₐ₁ₐ₂| ≠ 1)

- **Design tradeoffs:**
  - tanh: Naturally zero-mean, but saturating gradients may complicate training
  - Shifted ReLU: σ(z) = ReLU(z) - 1/√(2π), preserves ReLU benefits but requires explicit shift
  - Width vs. depth: Exponential width scaling (n = exp(Θ(ℓ))) ensures O(1/n) corrections stay small
  - Standard ReLU/GeLU: Simpler but correlations persist—useful if some correlation structure is desired

- **Failure signatures:**
  - Correlations not decaying: Check if ⟨σ(z)⟩ ≠ 0 (most common error)
  - Output variance exploding/collapsing: Verify critical tuning hyperparameters match Eq. 10
  - Non-convergence: Initial inputs may be scalar multiples (edge case)

- **First 3 experiments:**
  1. **Activation sanity check**: Compute ⟨σ(z)⟩ numerically for z ~ N(0,1) with 10⁶ samples. Confirm tanh ≈ 0, ReLU ≈ 0.3989, shifted ReLU ≈ 0.
  2. **Covariance decay visualization**: Initialize 2-layer network with n=10000, track K^(ℓ)ₐ₁ₐ₂ across layers for orthogonal inputs. Compare ReLU vs. tanh vs. shifted ReLU.
  3. **Width scaling test**: For fixed depth ℓ=20, vary n ∈ {100, 1000, 10000} and measure output correlation. Verify O(1/n) scaling of corrections to independence.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the neural network computational no-coincidence conjecture (Conjecture 5.1) true—i.e., does there exist a polynomial-time verification algorithm for detecting the rare property that no input produces an all-negative output?
- Basis in paper: The authors formally state Conjecture 5.1 and write: "We are agnostic on the truth or falsity of this conjecture, but we put it forward as the 'correct' analogue of the Alignment Research Center's computational no-coincidence conjecture for neural networks and hope that it inspires followup work."
- Why unresolved: The paper only establishes that wide tanh networks behave like random functions; it does not construct or prove the existence of an efficient verifier for the rare property.
- What evidence would resolve it: Either a polynomial-time verification algorithm meeting the conjecture's requirements, or a proof that no such algorithm exists under standard complexity assumptions.

### Open Question 2
- Question: Can exponentially small dependence between neural network outputs be achieved with only polynomially large width, rather than exponentially large width?
- Basis in paper: The authors note: "Achieving exponentially small dependence between the outputs of a neural network may require exponentially large width because of the O(1/n) term in (6)."
- Why unresolved: The perturbative analysis shows polynomially small dependence at polynomial width; whether stronger independence is achievable without exponential width remains open.
- What evidence would resolve it: A theoretical construction achieving exponential output independence at polynomial width, or a proof that exponential width is necessary.

### Open Question 3
- Question: Does the zero-mean condition for output independence persist after training, or do trained networks exhibit different correlation structures?
- Basis in paper: The entire analysis concerns randomly initialized networks; the paper never addresses trained networks despite their practical relevance to the interpretability motivation.
- Why unresolved: Training fundamentally changes weight distributions and may introduce correlations that the random-initialization analysis cannot predict.
- What evidence would resolve it: Empirical or theoretical analysis of output correlations in trained networks with zero-mean versus nonzero-mean activations.

## Limitations
- The analysis relies on perturbative expansion that assumes specific width-depth scaling and ignores higher-order corrections
- Zero-mean condition requires shifting common activations like ReLU and GeLU, potentially affecting optimization
- Limited empirical validation through toy examples without demonstrating practical utility for interpretability

## Confidence
- **High confidence**: Zero-mean condition ⟨σ(z)⟩ = 0 being necessary and sufficient for output independence is mathematically rigorous
- **Medium confidence**: Empirical validation through visualizations is limited to toy examples without practical application demonstration
- **Low confidence**: Claim that zero-mean networks serve as a "baseline" for computational no-coincidence conjecture lacks empirical grounding

## Next Checks
1. **Activation shift stability**: Test whether shifting ReLU by 1/√(2π) significantly impacts training dynamics, convergence speed, or generalization compared to standard ReLU on benchmark tasks

2. **Finite-width correction analysis**: Quantify how O(1/n) corrections affect output independence in networks with realistic widths (n ∈ [256, 4096]) and verify the exponential decay predictions

3. **Interpretability baseline evaluation**: Compare the interpretability of zero-mean activation networks versus standard networks on a simple mechanistic interpretability task, measuring whether independence actually aids causal analysis of individual neurons