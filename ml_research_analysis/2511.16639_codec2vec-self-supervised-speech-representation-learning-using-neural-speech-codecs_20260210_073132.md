---
ver: rpa2
title: 'Codec2Vec: Self-Supervised Speech Representation Learning Using Neural Speech
  Codecs'
arxiv_id: '2511.16639'
source_url: https://arxiv.org/abs/2511.16639
tags:
- speech
- units
- discrete
- codec
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Codec2Vec introduces a self-supervised speech representation learning
  framework that operates exclusively on discrete units generated by neural audio
  codecs. Unlike conventional SSL models that process continuous audio signals, Codec2Vec
  leverages pre-computed discrete codec units, eliminating the need for online acoustic
  feature extraction.
---

# Codec2Vec: Self-Supervised Speech Representation Learning Using Neural Speech Codecs

## Quick Facts
- **arXiv ID:** 2511.16639
- **Source URL:** https://arxiv.org/abs/2511.16639
- **Reference count:** 35
- **Primary result:** Achieves competitive performance to continuous-input SSL baselines on SUPERB benchmark using pre-computed discrete codec units

## Executive Summary
Codec2Vec introduces a self-supervised speech representation learning framework that operates exclusively on discrete units generated by neural audio codecs. Unlike conventional SSL models that process continuous audio signals, Codec2Vec leverages pre-computed discrete codec units, eliminating the need for online acoustic feature extraction. Using masked prediction objectives with various target derivation strategies, including reconstruction-based targets, iterative clustering, and online clustering, Codec2Vec learns contextualized speech representations directly from compressed audio data. Evaluated on the SUPERB benchmark, Codec2Vec achieves competitive performance compared to strong continuous-input baselines, demonstrating the feasibility of using discrete codec units for general-purpose speech processing. The framework also delivers substantial efficiency gains, reducing data storage requirements by up to 16.5× and accelerating pre-training by up to 2.3×, making it a scalable and resource-efficient approach for building speech foundation models.

## Method Summary
Codec2Vec operates on discrete units from neural audio codecs (specifically DAC with 12 codebooks at 50Hz) rather than continuous audio signals. The method extracts these discrete tokens offline, then applies masked prediction objectives to learn contextualized representations. Three target derivation strategies are explored: reconstruction of original codec units, iterative clustering using k-means on learned representations, and online clustering with an EMA teacher model. The model uses a Transformer encoder with 12 layers and 768-dim embeddings, where each of the 12 codec codebooks has its own embedding table initialized from the codec's learned embeddings. Quantizer dropout is applied during training to improve robustness.

## Key Results
- Achieves competitive performance on SUPERB benchmark compared to continuous-input SSL baselines
- Reduces data storage requirements by 16.5× (from 60.4 GB to 3.6 GB)
- Accelerates pre-training by 2.3× (from 830 to 356 GPU hours)
- Iterative clustering strategy outperforms direct reconstruction targets significantly

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Pre-computed discrete codec units preserve sufficient acoustic and linguistic information to support general-purpose speech representation learning when combined with masked prediction objectives.
- **Mechanism:** Neural audio codecs (specifically DAC with 12 codebooks at 50Hz) compress speech into discrete tokens via vector quantization in an autoencoder's latent space. These tokens are embedded via codebook lookup tables, summed across codebooks, and fed directly to a Transformer encoder. The model learns contextual representations by predicting masked tokens, forcing it to capture dependencies between codec units.
- **Core assumption:** The quantization bottleneck in neural codecs retains task-relevant acoustic and linguistic features despite lossy compression. The paper explicitly states this is supported by prior work [23] showing codec units "retain rich acoustic and linguistic information."
- **Evidence anchors:**
  - [abstract] "Codec2Vec achieves competitive performance compared to strong continuous-input baselines"
  - [Page 4, Table I] Codec2Vec with iterative clustering matches or exceeds HuBERT on SF (88.9/24.1 vs 88.5/25.2), SD (5.5 vs 5.9), and SV (5.2 vs 5.1)
  - [corpus] Weak direct corpus support; neighbor papers focus on codec applications rather than SSL validation
- **Break condition:** If codec quantization discards information critical for specific tasks (e.g., fine phonetic distinctions), performance will degrade. Table I shows ASR performance drops from 6.4% WER (HuBERT) to 7.2% (Codec2Vec iterative), suggesting content tasks may be more sensitive.

### Mechanism 2
- **Claim:** Eliminating online acoustic feature extraction during pre-training reduces computational overhead without degrading representation quality.
- **Mechanism:** Traditional SSL models process raw waveforms or spectrograms through convolutional feature extractors during every training iteration. Codec2Vec performs codec extraction once offline, then trains on pre-computed tokens. This removes the conv encoder from the training loop and reduces data I/O (smaller files, potential in-RAM datasets).
- **Core assumption:** The one-time offline codec extraction cost is amortized effectively over training, and I/O bottlenecks are a meaningful fraction of total training time.
- **Evidence anchors:**
  - [Page 5, Table II] Storage reduced from 60.4 GB to 3.6 GB (16.5x); GPU hours reduced from 830 to 356 (2.3x)
  - [Page 5, Section IV.B] "The observed acceleration... stems from: (1) elimination of the computationally intensive convolutional feature extractor... (2) significant reduction in I/O overhead"
  - [corpus] NanoCodec (arXiv:2508.05835) similarly targets codec efficiency for LLM inference, suggesting broader validity of efficiency claims
- **Break condition:** If the downstream task requires real-time adaptation of the feature extractor, or if codec extraction itself becomes a bottleneck for very large datasets, efficiency gains diminish.

### Mechanism 3
- **Claim:** Target derivation strategy significantly impacts learned representation quality, with clustering-based targets outperforming direct reconstruction.
- **Mechanism:** Three strategies were tested: (a) reconstruction-based targets predict masked codec units directly; (b) iterative clustering applies k-means to learned representations across training rounds, using cluster assignments as targets; (c) online clustering uses an EMA teacher model to dynamically generate codebook assignments. Clustering targets may encourage more abstract, semantically meaningful representations rather than raw acoustic reconstruction.
- **Core assumption:** Clustering operations on latent representations surface linguistically or semantically relevant groupings that serve as better prediction targets than raw codec units.
- **Evidence anchors:**
  - [Page 4, Table I] Reconstruction-based Codec2Vec: PR 19.6, ASR 13.9; Iterative clustering: PR 5.5, ASR 7.2—a dramatic improvement
  - [Page 4, Section IV.A] "codec units—while optimized for reconstruction fidelity—may be suboptimal as direct targets for general-purpose representation learning"
  - [corpus] JEPA tokenizer paper (arXiv:2512.07168) uses masked prediction in latent space, conceptually aligned with clustering-based abstraction
- **Break condition:** If clustering hyperparameters (k=500 clusters, layer selection for k-means) are mismatched to the data distribution, targets may be too coarse or too fragmented, degrading performance.

## Foundational Learning

- **Concept: Neural Audio Codecs (specifically DAC/EnCodec)**
  - **Why needed here:** Codec2Vec's input modality is entirely dependent on understanding how neural codecs compress speech into discrete tokens via residual vector quantization.
  - **Quick check question:** Can you explain why a codec with 12 codebooks at 50Hz produces a 16.5x compression ratio over 16kHz raw audio?

- **Concept: Masked Prediction in SSL (BERT-style)**
  - **Why needed here:** The entire training objective is masked token prediction; understanding how this forces contextual learning is essential.
  - **Quick check question:** Why does predicting masked tokens encourage the model to learn representations useful for downstream tasks it wasn't explicitly trained on?

- **Concept: k-means Clustering and EMA Teachers**
  - **Why needed here:** The iterative and online clustering strategies are core to achieving competitive performance; understanding how cluster assignments become training targets is critical.
  - **Quick check question:** In online clustering, why is the teacher updated as an EMA of the student rather than trained directly?

## Architecture Onboarding

- **Component map:**
  Raw audio -> DAC codec -> 12 codebook sequences (50Hz) -> stored as .npz files -> embedding tables (initialized from DAC) -> Transformer encoder -> projection head (task-specific) -> loss

- **Critical path:**
  1. Verify DAC codec extraction produces correct token sequences (12 codebooks, 50Hz frame rate)
  2. Initialize embedding tables with DAC's learned codebook embeddings (paper reports "substantial performance gains")
  3. Apply quantizer dropout during training (random dropout of codebook sequences)
  4. For iterative clustering: extract layer-9 representations, run faiss k-means (500 clusters), use assignments as next-round targets

- **Design tradeoffs:**
  - **DAC vs. EnCodec:** Table III shows DAC outperforms EnCodec (PR 5.2 vs 6.0; SV 5.1 vs 6.2). Assumption: DAC's training objective better preserves speech-relevant features.
  - **Target strategy:** Reconstruction is simplest but underperforms; iterative clustering requires multi-round training; online clustering adds teacher-student complexity but is single-pass.
  - **Storage vs. flexibility:** Pre-computed tokens lock you into a specific codec; re-extraction required if codec changes.

- **Failure signatures:**
  - **ASR underperformance:** Consistent gap vs. continuous baselines (7.2 vs 6.4 WER) suggests codec quantization loses phonetic detail. Mitigation: explore codecs with higher codebook capacity or different quantization strategies.
  - **Reconstruction-based training plateaus:** If PR stays above ~15%, target derivation is likely the issue; switch to clustering-based targets.
  - **Embedding initialization ignored:** Paper explicitly notes this hurts performance—verify initialization is from codec codebooks, not random.

- **First 3 experiments:**
  1. **Baseline reproduction:** Train Codec2Vec with reconstruction targets on LibriSpeech 960h; verify Table I metrics are reproducible (PR ~19.6, ASR ~13.9).
  2. **Ablate embedding initialization:** Compare random vs. codec-codebook embedding initialization; expect significant gap per paper's preliminary findings.
  3. **Codec comparison:** Replicate Table III by training with EnCodec tokens vs. DAC tokens on a subset of tasks (PR, SF, SV); validate DAC superiority claim.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can we identify or develop a neural audio codec whose discrete units are optimally suited for general-purpose speech representation learning?
- **Basis in paper:** [explicit] The authors explicitly state in Section V that "Identifying or developing the optimal codec—whose discrete units are ideally suited for general-purpose speech representation learning—remains an open research question."
- **Why unresolved:** Current codecs are optimized for reconstruction fidelity or compression (e.g., DAC, EnCodec) rather than downstream semantic utility, leading to variability in performance on tasks like ASR (Section IV.C).
- **What evidence would resolve it:** The development of a codec specifically trained or fine-tuned for SSL utility that consistently outperforms existing compression-focused codecs (like DAC) across all SUPERB tasks.

### Open Question 2
- **Question:** How does the interaction between real-world noise types, codec compression, and SSL pre-training affect the robustness of discrete representations?
- **Basis in paper:** [inferred] Section V notes that the authors "have not extensively evaluated [Codec2Vec] performance in real-world noisy conditions" and that "A deeper investigation into the effects of various noise conditions... is crucial to mitigating potential error propagation."
- **Why unresolved:** While discrete codes offer inherent robustness, the paper acknowledges the "complex" interaction between noise and the compression/pre-training pipeline remains unexplored, leaving a gap in understanding real-world viability.
- **What evidence would resolve it:** A comparative study of Codec2Vec against continuous baselines on noisy datasets (e.g., CHiME) to quantify if the efficiency gains of discrete units compromise noise robustness.

### Open Question 3
- **Question:** Can novel SSL objectives designed specifically for discrete inputs resolve the performance trade-offs observed in content-related tasks like ASR?
- **Basis in paper:** [inferred] Section V highlights that "current mainstream SSL pre-training strategies were predominantly developed for continuous input signals," and addressing limitations in tasks like ASR "may require... the enhancement of existing SSL objectives."
- **Why unresolved:** Codec2Vec adapts standard masked prediction (designed for continuous signals) to discrete inputs, yet consistently lags in ASR; it is unknown if discrete-specific objectives could close this gap.
- **What evidence would resolve it:** A new pre-training objective that exploits the structural properties of neural codec codebooks (e.g., residual vector quantization hierarchies) to improve phone error rates to levels comparable with continuous models.

## Limitations

- **Information preservation uncertainty:** The paper assumes DAC codec quantization preserves sufficient acoustic and linguistic information for general-purpose representation learning, but this is not empirically validated across diverse speech domains. The 7.2% WER for ASR (vs. 6.4% for HuBERT) suggests content-heavy tasks may be more sensitive to codec-induced information loss.
- **Task generalizability gap:** While SUPERB provides a broad evaluation suite, the paper does not test on more challenging, real-world tasks like speaker diarization, emotion recognition, or low-resource languages.
- **Clustering hyperparameter sensitivity:** The paper reports that iterative clustering with k=500 clusters outperforms reconstruction-based targets, but does not systematically explore how performance varies with different k values or clustering algorithms.

## Confidence

- **High confidence:** The efficiency claims (16.5× storage reduction, 2.3× training acceleration) are directly supported by Table II measurements and the mechanistic explanation of eliminating online feature extraction is sound.
- **Medium confidence:** The competitive performance on SUPERB tasks is well-supported by Table I comparisons with continuous-input baselines, though the ASR performance gap suggests task-specific limitations.
- **Low confidence:** The claim that pre-computed discrete codec units are universally viable for general-purpose speech representation learning, given the lack of validation across diverse speech domains and task types.

## Next Checks

1. **Codec capacity ablation study:** Systematically evaluate Codec2Vec performance across different DAC configurations (varying codebook counts, frame rates, and quantization levels) to identify the information preservation threshold where performance degrades significantly.

2. **Cross-domain robustness evaluation:** Test Codec2Vec on challenging, real-world tasks including speaker diarization, emotion recognition, and accented speech classification to validate generalizability beyond SUPERB's academic benchmark.

3. **Online adaptation feasibility:** Implement a streaming version where codec extraction and representation learning occur jointly, measuring the trade-off between computational overhead and representation quality compared to the offline-preprocessed approach.