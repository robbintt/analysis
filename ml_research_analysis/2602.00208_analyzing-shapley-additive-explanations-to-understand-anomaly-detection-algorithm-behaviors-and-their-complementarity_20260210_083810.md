---
ver: rpa2
title: Analyzing Shapley Additive Explanations to Understand Anomaly Detection Algorithm
  Behaviors and Their Complementarity
arxiv_id: '2602.00208'
source_url: https://arxiv.org/abs/2602.00208
tags:
- anomaly
- diversity
- ensemble
- detection
- scores
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of selecting complementary models
  for unsupervised anomaly detection ensembles. The core idea is to use SHapley Additive
  exPlanations (SHAP) to characterize the decision mechanisms of anomaly detectors,
  measuring similarity through feature attribution patterns rather than raw outputs.
---

# Analyzing Shapley Additive Explanations to Understand Anomaly Detection Algorithm Behaviors and Their Complementarity

## Quick Facts
- arXiv ID: 2602.00208
- Source URL: https://arxiv.org/abs/2602.00208
- Reference count: 25
- One-line primary result: SHAP-based diversity metrics outperform output-based metrics in ensemble selection for anomaly detection.

## Executive Summary
This study addresses the challenge of selecting complementary models for unsupervised anomaly detection ensembles. The core idea is to use SHapley Additive exPlanations (SHAP) to characterize the decision mechanisms of anomaly detectors, measuring similarity through feature attribution patterns rather than raw outputs. The methodology reveals that detectors with similar SHAP explanations tend to produce correlated anomaly scores and overlapping predictions, while explanation divergence indicates complementarity. Experimental results across 16 datasets show that SHAP-based diversity metrics (ρPS and ρNDCG) often outperform traditional output-based metrics (ρScores and J) in ensemble selection. The study demonstrates that while diversity enhances ensemble performance, individual model quality remains the dominant factor, with diversity playing a secondary but valuable role.

## Method Summary
The study trains 14 unsupervised anomaly detection algorithms from PyOD on 16 datasets, generating SHAP explanations for each model using KernelSHAP with k-means background summarization (k=50). Four pairwise similarity matrices are computed: Pearson correlation of SHAP values (ρPS), NDCG of SHAP rankings (ρNDCG), Pearson correlation of anomaly scores (ρScores), and Jaccard similarity of binary predictions (J). All possible 3-model ensembles are constructed and evaluated via rank aggregation, with AUCPR as the performance metric. The methodology systematically compares whether SHAP-based diversity metrics better identify complementary models than output-based metrics, validated through Mantel tests correlating explanation and output dissimilarities.

## Key Results
- On 11 of 16 datasets, SHAP-based diversity metrics (ρPS and ρNDCG) outperform output-based metrics (ρScores and J) in ensemble selection
- Mantel test shows significant correlation (rM = 0.67) between SHAP-based and output-based dissimilarities
- Individual model performance dominates ensemble effectiveness, with diversity playing a secondary but positive role (positive in 12/16 datasets)
- Ensemble performance is best predicted by a combination of individual model quality and diversity, with individual performance having the larger coefficient

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SHAP-based similarity metrics capture model complementarity for ensemble selection better than raw output-based metrics.
- Mechanism: SHAP values quantify per-instance feature attribution, revealing internal reasoning patterns of anomaly detectors. By comparing attribution profiles (using Pearson correlation ρPS or NDCG ρNDCG) rather than just anomaly scores, we identify models relying on different decision mechanisms. Models with divergent explanation patterns capture different irregularity types, yielding more complementary ensembles.
- Core assumption: Feature attribution similarity proxies underlying algorithmic similarity in how models define "normality" and detect deviations.
- Evidence anchors:
  - [abstract] "Using SHapley Additive exPlanations, we quantify how each model attributes importance to input features, and we use these attribution profiles to measure similarity between detectors."
  - [section 4.5] "On 11 out of 16 datasets, using ρPS and ρNDCG as diversity, tends to give better results than using ρScores and J."
  - [corpus] ShaTS (arXiv:2506.01450) applies Shapley-based explainability to anomaly detection; XStacking (arXiv:2507.17650) uses explanation-guided ensemble learning, suggesting broader relevance.
- Break condition: If SHAP explanations are unstable (e.g., due to small/non-representative background datasets), similarity measurements become noisy and unreliable for selection.

### Mechanism 2
- Claim: Explanation divergence between detectors reliably indicates complementary detection behavior.
- Mechanism: Detectors assigning high importance to different features for the same instances likely rely on different normality assumptions (distance-based vs. probabilistic). This divergence produces different anomaly score distributions and non-overlapping detections. The Mantel test quantifies this via correlation between SHAP dissimilarity and output dissimilarity.
- Core assumption: SHAP attribution patterns reflect true decision boundaries and inductive biases; diversity in these patterns causes diversity in detected anomalies.
- Evidence anchors:
  - [abstract] "Detectors with similar explanations tend to produce correlated anomaly scores and identify largely overlapping anomalies. Conversely, explanation divergence reliably indicates complementary detection behavior."
  - [section 4.3] Mantel test shows rM(ρPS, J) = 0.67, linking explanation similarity to prediction overlap.
  - [corpus] XStacking (arXiv:2507.17650) leverages explanations to guide stacking ensembles, implicitly validating explanation-based characterization.
- Break condition: If anomalies arise from complex feature interactions SHAP cannot decompose, explanation divergence may misrepresent true complementarity.

### Mechanism 3
- Claim: Individual model quality is prerequisite and dominant; diversity provides secondary gains.
- Mechanism: Ensembles aggregate predictions, so systematic errors from weak models persist. High individual performance ensures meaningful signal; diversity then expands anomaly coverage. The relationship is modeled as: ensemble performance ∝ β1 · individual performance + β2 · diversity.
- Core assumption: Relationship between individual performance, diversity, and ensemble performance is approximately linear and additive.
- Evidence anchors:
  - [abstract] "Diversity alone is insufficient; high individual model performance remains a prerequisite for effective ensembles."
  - [section 4.6] "In 12 out of 16 datasets, the diversity coefficient is positive... diversity plays a secondary yet valuable role" (Table 5 shows β1 > β2 generally).
  - [corpus] Foundation Models for Anomaly Detection (arXiv:2502.06911) emphasizes strong base models, indirectly supporting individual quality importance.
- Break condition: With extremely weak base models (near-random), diversity cannot rescue performance; if one model dominates, diversity can hurt (negative correlations observed on some datasets like AN).

## Foundational Learning

- Concept: **Shapley Values & SHAP (KernelSHAP)**
  - Why needed here: Core technique for extracting per-feature attribution from arbitrary anomaly detectors. Understanding that SHAP approximates Shapley values via coalitional game theory helps interpret what "feature importance" means and its limitations (e.g., computational cost, sensitivity to background data).
  - Quick check question: Given a model that predicts anomaly scores, what does a SHAP value of +0.3 for feature X on instance k mean?

- Concept: **Ensemble Diversity & Complementarity**
  - Why needed here: The paper's thesis is that diversity (measured via explanations) improves ensembles. Understanding why diverse models tend to cover different error regions—and why diversity alone is insufficient—frames the methodology.
  - Quick check question: If two anomaly detectors have correlated anomaly scores (Pearson > 0.9) but different SHAP profiles, would you expect them to be complementary? Why or why not?

- Concept: **Mantel Test for Matrix Correlation**
  - Why needed here: Used to validate that explanation similarity correlates with output similarity. Understanding this statistical test helps critically evaluate the evidence linking explanations to predictions.
  - Quick check question: What does a significant positive Mantel correlation between two dissimilarity matrices indicate about the relationship between the underlying data structures?

## Architecture Onboarding

- Component map:
  Anomaly Detectors (14 models) -> Explanation Generator (KernelSHAP) -> Similarity Metrics (ρPS, ρNDCG, ρScores, J) -> Ensemble Builder (select top-3 dissimilar) -> Evaluator (AUCPR)

- Critical path:
  1. Train all candidate detectors on training split.
  2. Generate SHAP explanations for each detector on test data.
  3. Compute pairwise similarity/dissimilarity matrices.
  4. Select ensembles (e.g., top-3 most dissimilar by ρPS).
  5. Aggregate predictions via rank averaging; evaluate AUCPR.

- Design tradeoffs:
  - **KernelSHAP vs. faster explainers**: KernelSHAP is model-agnostic but slow; TreeSHAP or PDP (as suggested in conclusion) could be faster but may be less accurate for some model types.
  - **Ensemble size (n=3)**: Small ensembles are computationally cheap but may miss complementarity; larger ensembles increase coverage but complexity.
  - **Background dataset size (k=50)**: Smaller k speeds up SHAP but risks under-representing data distribution; larger k improves accuracy but cost.

- Failure signatures:
  - **Negative diversity-performance correlation**: Occurs when one model dominates or all models are weak; check individual AUCPR first.
  - **High SHAP computational cost**: On large datasets (>10K instances or >20 features), KernelSHAP becomes prohibitive; consider sampling or surrogate models.
  - **Inconsistent SHAP explanations**: High variance across runs indicates unstable background summaries; increase k-means centroids or use full background data.

- First 3 experiments:
  1. **Baseline replication**: On 2-3 datasets (e.g., annthyroid, vowels), compute all four similarity matrices, build ensembles of 3 models selected by each metric, compare AUCPR. Verify ρPS outperforms ρScores on average.
  2. **Ablation on background size**: Vary k-means centroids (k=10, 25, 50, 100) for SHAP background summarization. Measure stability of ρPS and impact on ensemble AUCPR.
  3. **Individual performance vs. diversity tradeoff**: For a fixed dataset, plot ensemble AUCPR against mean individual AUCPR and diversity. Fit linear regression; confirm individual performance coefficient exceeds diversity coefficient on most datasets.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can unsupervised performance characterization methods be effectively integrated with SHAP-based diversity metrics to optimize ensemble selection without ground truth labels?
  - Basis in paper: [explicit] The authors state, "our study did not address the optimization of individual models," and propose to "integrate methods to characterize model performance into our methodology."
  - Why unresolved: The current study treats individual performance and diversity somewhat separately; it establishes diversity's value but relies on labeled data (AUCPR) to quantify the trade-off, which is unavailable in purely unsupervised settings.
  - What evidence would resolve it: A unified selection algorithm that maximizes a combined objective function of estimated performance and explanation diversity, validated against held-out labels or synthetic datasets.

- **Open Question 2**: Does the correlation between explanation divergence and ensemble complementarity hold for complex time-series anomaly detection?
  - Basis in paper: [explicit] The authors list extending the strategy to "time series UAD, where ensemble methods can be even more powerful," as a specific goal for future work.
  - Why unresolved: The current experiments are restricted to static tabular datasets (vector features); time series introduce temporal dependencies which may alter how SHAP attributes importance to features/lags.
  - What evidence would resolve it: Experimental results on benchmark time-series datasets showing that ensembles selected via temporal SHAP divergence outperform those selected via output correlation.

- **Open Question 3**: Can computationally efficient explanation methods, such as Partial Dependence Profiles (PDP), replace SHAP for measuring detector diversity without losing fidelity?
  - Basis in paper: [explicit] The authors identify "computational cost of SHAP" as a limitation and suggest that "aggregating Partial Dependence Profiles (PDP)... could provide reliable metrics" as an alternative.
  - Why unresolved: While PDP is faster, it is a global method that may miss instance-level interactions that SHAP captures, potentially reducing the granularity of the diversity measurement.
  - What evidence would resolve it: A comparative analysis of runtime versus ensemble performance gain, showing that PDP-based diversity metrics achieve statistically similar ensemble improvements to SHAP-based metrics.

## Limitations

- **Computational Complexity**: Kernel SHAP is model-agnostic but computationally expensive, limiting scalability to larger datasets and model pools.
- **Data Preprocessing Assumptions**: The paper lacks explicit detail on whether features were normalized or standardized before training, which significantly impacts distance-based and kernel-based detectors.
- **Model Hyperparameter Specification**: The use of default hyperparameters from PyOD is a simplification that may not hold if models are tuned differently.

## Confidence

- **High Confidence**: The fundamental observation that SHAP-based similarity metrics (particularly ρPS and ρNDCG) can outperform traditional output-based metrics in identifying complementary models for ensemble selection.
- **Medium Confidence**: The practical impact of diversity on ensemble performance is supported but shows variability, with diversity being positive in 12/16 datasets but effect size secondary to individual model performance.
- **Medium Confidence**: The choice of k-means summarization (k=50) for the background dataset in Kernel SHAP is a reasonable heuristic but sensitivity to this choice is not thoroughly explored.

## Next Checks

1. **Computational Scalability Test**: Implement the pipeline on a small, controlled subset (e.g., 3 datasets and 5 models) and systematically increase the number of models and dataset size. Measure the wall-clock time for SHAP computation and identify the practical limits for real-world deployment.

2. **Preprocessing Sensitivity Analysis**: Reproduce the main results on a representative dataset (e.g., 'annthyroid') with and without feature normalization. Compare the similarity matrices (ρPS, ρScores, etc.) and the resulting ensemble performances to quantify the impact of this preprocessing step.

3. **Aggregation Strategy Robustness Check**: On the same representative dataset, repeat the ensemble selection and evaluation process using alternative aggregation strategies (e.g., mean aggregation, weighted averaging based on individual model performance). Compare the relative performance of SHAP-based vs. output-based diversity metrics across these strategies to assess the robustness of the findings.