---
ver: rpa2
title: 'Structuring the Unstructured: A Systematic Review of Text-to-Structure Generation
  for Agentic AI with a Universal Evaluation Framework'
arxiv_id: '2508.12257'
source_url: https://arxiv.org/abs/2508.12257
tags:
- zhang
- linguistics
- pages
- wang
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive survey of text-to-structure
  generation for agentic AI systems, identifying a critical gap in current evaluation
  frameworks. The authors introduce a universal evaluation framework that assesses
  structured outputs across content faithfulness (precision and recall) and structure
  coherence (alignment and clarity) dimensions.
---

# Structuring the Unstructured: A Systematic Review of Text-to-Structure Generation for Agentic AI with a Universal Evaluation Framework

## Quick Facts
- arXiv ID: 2508.12257
- Source URL: https://arxiv.org/abs/2508.12257
- Reference count: 40
- Key outcome: Introduces universal evaluation framework for text-to-structure generation that outperforms traditional metrics and shows strong correlation with human judgments across tables, graphs, and charts

## Executive Summary
This paper addresses the critical gap in evaluating text-to-structure generation for agentic AI systems by introducing a universal evaluation framework. The authors systematically analyze existing benchmarks and propose a four-dimensional assessment covering content faithfulness (precision/recall) and structure coherence (alignment/clarity). Through extensive experiments with multiple LLMs and human evaluations, they demonstrate that traditional metrics like ROUGE-L and BERTScore poorly align with human judgment, while their proposed framework provides reliable quality assessment across different structure types.

## Method Summary
The framework evaluates structured outputs using LLM-as-judge with detailed rubrics (0-100 scale) for four sub-metrics: precision, recall, alignment, and clarity. Content faithfulness is computed as the harmonic mean of precision and recall, while structure coherence is the harmonic mean of alignment and clarity. Six datasets were used: InstructIE, Struc-Bench, LiveSum (tables); DART, InstructIE (graphs); Text2Chart31 (charts), with 250 test samples per dataset. DeepSeek-R1 served as the primary evaluator with temperature=0 for deterministic scoring, while GPT-4o handled multimodal chart evaluations.

## Key Results
- Traditional metrics (ROUGE-L, BERTScore) show poor alignment with human judgments for structured outputs
- The proposed framework demonstrates strong correlation (r=0.7-0.9) with human evaluations across all structure types
- Reasoning-intensive models (DeepSeek-R1, O4-mini) significantly outperform standard LLMs on complex extraction tasks
- SFT training improves smaller models substantially but reasoning models often match or exceed this without data

## Why This Works (Mechanism)

### Mechanism 1: Semantic F1-Style Evaluation via LLM-as-Judge
The framework replaces n-gram overlap with semantic F1-score determined by an LLM, using DeepSeek-R1 to compare generated structures against source text and ground truth with specific rubrics for precision (conflict detection) and recall (completeness). This addresses the core limitation that string-matching metrics cannot detect semantic conflicts or missing information.

### Mechanism 2: Orthogonal Structural Coherence Constraints
By evaluating alignment (schema validity) and clarity (unambiguous representation) separately from content, the framework captures distinct failure modes where data is factually correct but formatted uselessly. This prevents high content scores from masking structural unusability, recognizing that structural utility is distinct from semantic truth.

### Mechanism 3: Reasoning-Intensive Generation (Thinking Models)
"Thinking" models outperform standard LLMs because text-to-structure conversion requires implicit reasoning, not just pattern matching. Models designed for deep reasoning can better infer implicit relationships and resolve entity discrepancies before structuring the output, reducing hallucination and improving content faithfulness.

## Foundational Learning

- **Concept: Harmonic Mean vs. Arithmetic Mean**
  - Why needed: The framework uses harmonic mean for both Faithfulness and Coherence scores, heavily penalizing extreme imbalances
  - Quick check: If a table has perfect data (Precision=100) but only 1% of required rows (Recall=1), what does the harmonic mean output?

- **Concept: LLM-as-Judge (Direct Scoring)**
  - Why needed: The evaluation architecture shifts from reference-based metrics to model-based scoring
  - Quick check: Why is "temperature 0" specified for the Judge LLM, and how does it affect reproducibility?

- **Concept: Schema Constraint vs. Open-Domain Extraction**
  - Why needed: Datasets differ by "Schema limited vs. unlimited," changing success criteria between rigid schema fitting and schema creation
  - Quick check: In Table 1, how does "Schema limitedness" affect the "Structure Complexity" rating?

## Architecture Onboarding

- **Component map:** Input (Source Text + Instruction + Ground Truth) -> Generator (Target LLM) -> Structure Output -> Judge (Evaluator LLM) -> Rubric Prompt -> Aggregator
- **Critical path:** The latency bottleneck is the Judge, not the Generator, requiring high context windows and compute for ingesting full source text and generated structure
- **Design tradeoffs:** Traditional metrics are fast and free but produce false positives; T2S framework is accurate but costly and potentially non-deterministic
- **Failure signatures:** Metric misalignment (high ROUGE but low Faithfulness indicates hallucinations), format collapse (high Faithfulness but low Alignment indicates wrong schema), reasoning gap (low Faithfulness on complex datasets indicates inability to integrate information)
- **First 3 experiments:**
  1. Run current generator output through both ROUGE and T2S Framework to detect misalignment issues
  2. Isolate lowest scoring dimension (Precision, Recall, Alignment, or Clarity) to identify specific error types
  3. Compare standard model vs. reasoning model on 50-sample subset to calculate cost-per-point improvement

## Open Questions the Paper Calls Out

- Can multi-task learning frameworks simultaneously generate tables, graphs, and charts while maintaining cross-structure consistency and schema alignment?
- How can high-complexity, human-annotated datasets be systematically developed for underrepresented text-to-chart and text-to-graph tasks?
- How reliably can LLM-as-judge evaluation generalize across different model families and reasoning paradigms without exhibiting self-favoring bias?
- What reward function designs can effectively balance content fidelity, structural coherence, and ethical constraints in reinforcement learning for agentic text-to-structure generation?

## Limitations

- Evaluation scalability and cost: LLM-as-judge approach introduces significant computational overhead, making large-scale deployment economically challenging
- Human evaluation scope: Limited to only 30 samples per dataset, potentially missing edge cases where the framework's decisions are most critical
- Task heterogeneity: Combining three fundamentally different structure types under a single evaluation paradigm may not capture distinct characteristics of each format

## Confidence

- **High Confidence (90%+):** Framework's ability to detect quality differences missed by traditional metrics, harmonic mean approach for penalizing imbalanced scores, general superiority of reasoning models for complex extraction tasks
- **Medium Confidence (70-90%):** Correlation strength with human judgments, universal applicability across structure types, SFT effectiveness claims for smaller models
- **Low Confidence (50-70%):** Economic viability of LLM-as-judge at scale, framework's performance on truly novel schema types, long-term stability of reasoning model advantages

## Next Checks

1. **Cost-Effectiveness Analysis:** Measure per-sample evaluation cost of LLM-as-judge versus traditional metrics, including API pricing, latency, and throughput to determine break-even point
2. **Schema Generalization Test:** Evaluate framework on datasets with novel, unseen schemas to test whether universal approach maintains correlation with human judgment
3. **Longitudinal Model Performance:** Conduct time-series evaluation comparing reasoning and non-reasoning models across multiple months to determine whether reasoning advantage is persistent or diminishes over time