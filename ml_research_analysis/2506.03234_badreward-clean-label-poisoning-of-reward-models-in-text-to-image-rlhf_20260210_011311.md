---
ver: rpa2
title: 'BadReward: Clean-Label Poisoning of Reward Models in Text-to-Image RLHF'
arxiv_id: '2506.03234'
source_url: https://arxiv.org/abs/2506.03234
tags:
- reward
- attack
- data
- poisoning
- should
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BadReward, a clean-label poisoning attack
  that targets reward models in multi-modal reinforcement learning from human feedback
  (RLHF) pipelines for text-to-image (T2I) models. The attack exploits feature collisions
  in CLIP embedding space to corrupt reward signals without altering preference labels,
  enabling stealthy manipulation of T2I model outputs toward harmful concepts (e.g.,
  violent or biased imagery) for targeted prompts.
---

# BadReward: Clean-Label Poisoning of Reward Models in Text-to-Image RLHF

## Quick Facts
- **arXiv ID**: 2506.03234
- **Source URL**: https://arxiv.org/abs/2506.03234
- **Reference count**: 40
- **Primary result**: BadReward achieves ASR up to 1.00 on poisoned reward models while maintaining high visual similarity (SSIM > 0.86, PSNR > 24 dB)

## Executive Summary
BadReward is a clean-label poisoning attack targeting reward models in text-to-image reinforcement learning from human feedback (RLHF) pipelines. The attack exploits feature collisions in CLIP embedding space to corrupt reward signals without altering preference labels, enabling stealthy manipulation of T2I model outputs toward harmful concepts. Extensive experiments on Stable Diffusion v1.4 and SD Turbo demonstrate that BadReward achieves high attack success rates while maintaining imperceptibility through careful optimization of pixel-space and feature-space constraints.

## Method Summary
The attack injects poisoned preference pairs into the reward model training dataset by optimizing images to minimize feature distance to target concepts in CLIP space while maintaining pixel similarity to benign base images. The poisoned pairs are injected at 1-3% ratio and the corrupted reward model is used in RLHF training for 400-800 steps. The optimization uses feature collision equations with regularization parameters to balance attack effectiveness and stealthiness, achieving high ASR while maintaining SSIM > 0.86 and PSNR > 24 dB.

## Key Results
- Achieves attack success rates up to 1.00 on poisoned reward models
- Maintains high visual similarity with SSIM > 0.86 and PSNR > 24 dB
- Shows strong transferability across different T2I architectures
- ASR remains 3.8-10.6× higher than clean models even with paraphrased prompts

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Inducing feature collisions in CLIP embedding space corrupts reward model training while maintaining visual plausibility of poisoned samples.
- **Mechanism**: The attack optimizes a poisoned image x to minimize feature distance to a target image containing concept C in CLIP space, while constraining pixel-space similarity to a benign base image.
- **Core assumption**: The reward model's reliance on CLIP embeddings creates a vulnerability where pixel-space and feature-space representations can be decoupled through optimization.
- **Evidence anchors**:
  - [abstract]: "inducing feature collisions between visually contradicted preference data instances, thereby corrupting the reward model"
  - [section 4.2.1, equation 5]: min_x ||gCLIP(x) - gCLIP(xt)||_2 + β||x - xb||_2
  - [corpus]: Related work on data poisoning in preference learning confirms susceptibility of preference-based paradigms to poisoned data

### Mechanism 2
- **Claim**: Clean-label poisoning circumvents annotation-process control requirements, making attacks more practical than dirty-label approaches.
- **Mechanism**: By manipulating only the images submitted for annotation (not the labels), adversaries exploit that human annotators will naturally prefer feature-collided images when they appear semantically aligned with prompts containing trigger t.
- **Core assumption**: Annotators cannot perceive feature-space perturbations and will label based on visual appearance; SSIM > 0.86 and PSNR > 24 dB maintain imperceptibility.
- **Evidence anchors**:
  - [abstract]: "BADREWARD is independent of the preference annotation process, enhancing its stealth and practical threat"
  - [table 2]: SSIM 0.87, PSNR 27.70 dB, LPIPS 0.22 across adversary models
  - [corpus]: Weak corpus evidence on clean-label specifically—neighbors focus on general preference poisoning, not the clean-label constraint

### Mechanism 3
- **Claim**: Poisoned reward models create positive feedback loops during RLHF that amplify target concept generation.
- **Mechanism**: The corrupted reward model r*_φ assigns higher rewards to outputs containing target concept C when trigger t is present. During policy gradient updates, the advantage function A_φ amplifies these rewards, reinforcing generation of C through the KL-regularized optimization.
- **Core assumption**: RLHF's iterative reward-guided training propagates localized reward corruption into persistent policy changes; 3% poisoning ratio and 400-800 training steps suffice for saturation.
- **Evidence anchors**:
  - [section 4.1]: "r*_φ assigns higher rewards when the input contains t and the output contains C, and the dominance function A_φ amplifies the rewards"
  - [figure 6, ablation]: ASR increases with poisoning ratio and training steps, stabilizing at 400-800 steps for 3% poisoning

## Foundational Learning

- **Concept**: CLIP Joint Embedding Space
  - **Why needed here**: The attack exploits that CLIP maps images and text to a shared space where semantic similarity is computed; understanding this is essential to grasp how feature collisions work.
  - **Quick check**: Can you explain why minimizing ||gCLIP(x) - gCLIP(x_t)||_2 makes a poisoned image semantically closer to the target concept despite pixel differences?

- **Concept**: Bradley-Terry Preference Model
  - **Why needed here**: The reward model is trained using pairwise preferences formalized through BT model (equation 1); this defines what "corrupting the reward model" means mathematically.
  - **Quick check**: How does the sigmoid-based preference probability in equation 1 relate to the reward difference r_φ(p, x_w) - r_φ(p, x_l)?

- **Concept**: Policy Gradient with KL Regularization for Diffusion
  - **Why needed here**: RLHF for T2I uses adapted policy gradient (equation 3); understanding how rewards propagate through denoising trajectories is key to understanding attack amplification.
  - **Quick check**: What role does the KL divergence term play in preventing catastrophic forgetting, and how might a poisoned reward overcome this constraint?

## Architecture Onboarding

- **Component map**: Adversary's generation pipeline (SDv3.5/SDXL/CogView4) → feature collision optimization → poisoned preference pairs → Victim's reward model (CLIP-ViT-L/14 + MLP) → RLHF training loop (DDPO/SDPO) → policy updates

- **Critical path**:
  1. Select trigger-concept pair (t, C) where clean model has non-zero baseline probability of generating C
  2. Generate base images x_b (without C) and target images x_t (with C) for prompts containing t
  3. Optimize x_collide via iterative gradient updates with λ, β parameters
  4. Inject poisoned pairs at 1-3% of preference dataset
  5. Train reward model 20 epochs (lr: 5e-3 → 5e-4)
  6. Run RLHF for 400-800 steps until ASR saturates

- **Design tradeoffs**:
  - Higher poisoning ratio → higher ASR but increased detection risk
  - Stronger feature collision (lower β) → better ASR but lower SSIM/PSNR stealth
  - More RLHF steps → higher ASR until saturation, but also more compute and potential reward hacking artifacts

- **Failure signatures**:
  - **Low ASR despite poisoning**: Check if trigger t appears in prompts during annotation; verify adversary model can generate C
  - **Detectable perturbations**: PSNR < 24 dB or SSIM < 0.86 indicates β too low or too many optimization iterations
  - **Reward hacking**: Unexpected outputs (e.g., comic style instead of target concept) indicate over-training beyond 600-800 steps

- **First 3 experiments**:
  1. **Baseline ASR measurement**: Train reward model on clean Recraft-V2 data, run RLHF on SDv1.4, measure ASR for (t=old, C=eyeglasses) on 100 prompts—establishes floor (~0.09 from table 1)
  2. **Feature collision stealth validation**: Generate poisoned images with SDv3.5 at β=1.0, measure SSIM/PSNR/LPIPS against base images—confirms imperceptibility threshold
  3. **Poisoning ratio sweep**: Inject 1%, 2%, 3% poisoned pairs, train for 400 steps, plot ASR curve—identifies minimum effective ratio and saturation point

## Open Questions the Paper Calls Out

- **Open Question 1**: Can feature-space anomaly detection techniques effectively identify and mitigate BadReward attacks without degrading the model's alignment performance?
  - **Basis in paper**: [explicit] The conclusion explicitly states, "In future work, we will investigate feature-space anomaly detection techniques against reward poisoning attack."
  - **Why unresolved**: The authors identify the need for this defense but do not implement or validate it within the current study, leaving its efficacy against feature collision unproven.
  - **What evidence would resolve it**: Experimental results demonstrating that anomaly detectors can distinguish poisoned samples from clean preference data with high precision while maintaining alignment quality.

- **Open Question 2**: What mechanisms drive the unintended "reward hacking" behaviors (e.g., comic-style artifacts) observed during the BadReward optimization process?
  - **Basis in paper**: [inferred] Appendix C.1 notes the observation of "reward hacking" where models produced unexpected artifacts (like comic styles) that deviated from the intended attack target, but offers no causal explanation.
  - **Why unresolved**: The phenomenon is documented as an interesting artifact, but the relationship between the specific feature collision perturbation and these broader, unintended policy optimizations is not analyzed.
  - **What evidence would resolve it**: A theoretical analysis or ablation study isolating how the poisoned reward signal exploits the RL optimization loop to generate these specific non-targeted artifacts.

- **Open Question 3**: How effective are multi-modal consensus validation strategies in detecting BadReward attacks compared to single-model auditing?
  - **Basis in paper**: [inferred] Section 5.7 suggests "Multi-modal Consensus Validation" as a possible countermeasure, but the paper provides no experimental validation of this approach.
  - **Why unresolved**: While proposed as a conceptual defense, the paper does not test if cross-validating reward signals with auxiliary models (e.g., BLIP-2) can successfully flag the subtle feature collisions introduced by BadReward.
  - **What evidence would resolve it**: Benchmarks showing that auxiliary models can detect discrepancies in the poisoned preference pairs that the primary reward model fails to flag.

## Limitations

- Attack assumes adversaries can inject poisoned preference pairs at 1-3% ratio, but real-world data collection processes may have detection mechanisms not explored in this work
- Limited evaluation on diverse trigger-concept pairs beyond the 8 tested combinations
- The assumption that human annotators will naturally prefer feature-collided images requires empirical validation across different annotator populations

## Confidence

- **High**: The feature collision mechanism and its implementation (equations 5-6), the RLHF training pipeline integration, and the transferability results across architectures
- **Medium**: The clean-label attack effectiveness in real-world annotation scenarios, and the long-term stability of poisoned reward models beyond 800 RLHF steps
- **Low**: The generalizability of the attack to reward models using non-CLIP encoders or ensemble methods

## Next Checks

1. Test attack effectiveness against reward models using ensemble architectures (e.g., CLIP + proprietary encoders) to assess robustness
2. Conduct human study with diverse annotator groups to verify preference patterns for feature-collided images versus clean pairs
3. Evaluate attack persistence and evolution over extended RLHF training (2000+ steps) to identify potential reward hacking emergence