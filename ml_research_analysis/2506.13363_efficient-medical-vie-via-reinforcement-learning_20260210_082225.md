---
ver: rpa2
title: Efficient Medical VIE via Reinforcement Learning
arxiv_id: '2506.13363'
source_url: https://arxiv.org/abs/2506.13363
tags:
- arxiv
- medical
- json
- information
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of extracting structured information
  from medical images, which is critical for applications like report analysis and
  online consultations. Traditional approaches rely on OCR followed by language models,
  but domain-specific schemas and high annotation costs limit their effectiveness.
---

# Efficient Medical VIE via Reinforcement Learning

## Quick Facts
- arXiv ID: 2506.13363
- Source URL: https://arxiv.org/abs/2506.13363
- Reference count: 5
- Primary result: RLVR method achieves SOTA performance on medical VIE tasks with only 100 annotated samples

## Executive Summary
This paper presents a novel approach to medical visual information extraction (VIE) using Reinforcement Learning with Verifiable Rewards (RLVR). Traditional OCR-based methods followed by language models struggle with domain-specific medical schemas and high annotation costs. The authors propose a method that requires only 100 annotated samples while achieving state-of-the-art performance on medical report VIE tasks. Their approach addresses key challenges in medical VIE including dataset diversity, hallucination reduction, and reasoning capability enhancement.

## Method Summary
The authors propose a reinforcement learning approach with verifiable rewards for medical VIE tasks. The method employs a balanced precision-recall reward mechanism to reduce hallucinations and improve field coverage while using innovative sampling strategies to enhance reasoning capabilities. The model is fine-tuned on Qwen2.5-VL-7B, achieving superior performance on medical reports compared to existing approaches. The key innovation lies in the RLVR framework that enables effective learning from minimal annotations while maintaining high precision and recall through carefully designed reward functions.

## Key Results
- Achieves state-of-the-art F1, precision, and recall scores on medical VIE tasks
- Requires only 100 annotated samples compared to traditional approaches
- Significantly outperforms existing models on medical reports
- Demonstrates superior reasoning capabilities during both training and inference

## Why This Works (Mechanism)
The method works by leveraging reinforcement learning to optimize information extraction directly from visual inputs without relying solely on OCR. The verifiable rewards provide immediate feedback on extraction quality, enabling the model to learn effective extraction strategies. The balanced precision-recall reward mechanism ensures that the model maintains both high accuracy and comprehensive field coverage, addressing the common trade-off in information extraction tasks.

## Foundational Learning
- Reinforcement Learning with Verifiable Rewards: Needed to enable learning from visual inputs with immediate quality feedback; quick check: verify reward signal is properly shaping extraction behavior
- Domain-specific schema adaptation: Required to handle medical terminology and structured data formats; quick check: test schema generalization across different medical specialties
- Sample efficiency techniques: Essential for reducing annotation burden; quick check: measure performance degradation with decreasing sample sizes
- Balanced precision-recall optimization: Critical for minimizing hallucinations while maintaining coverage; quick check: analyze precision-recall trade-off curves

## Architecture Onboarding

**Component Map:**
Visual Encoder -> Feature Extractor -> RLVR Agent -> Structured Output Generator -> Reward Evaluator

**Critical Path:**
The critical path flows from visual input through the encoder and feature extractor to the RLVR agent, which generates extraction decisions. The reward evaluator provides feedback that guides the agent's policy updates. The structured output generator transforms the agent's decisions into usable medical data formats.

**Design Tradeoffs:**
The primary tradeoff is between annotation efficiency and extraction quality. Using only 100 samples enables rapid deployment but may limit coverage of rare medical scenarios. The balanced precision-recall reward mechanism trades some precision for better field coverage, which is crucial for medical applications where missing critical information can be dangerous.

**Failure Signatures:**
Common failure modes include overfitting to training schemas, hallucination of non-existent medical information, and poor generalization to different document layouts. Performance degradation on non-medical tasks indicates strong domain specificity that may limit broader applicability.

**3 First Experiments:**
1. Vary the precision-recall balance ratio to identify optimal settings for different medical specialties
2. Test sample efficiency by training with 10, 50, 100, and 500 annotated samples
3. Evaluate cross-domain performance on legal and financial documents to quantify generalizability limits

## Open Questions the Paper Calls Out
None

## Limitations
- Narrow evaluation scope focused exclusively on medical report VIE tasks
- Significant performance degradation on non-medical or dissimilar tasks
- Reliance on proprietary Qwen2.5-VL-7B base model limits reproducibility
- Limited characterization of the 100 sample quality and representativeness requirements

## Confidence

**Confidence Labels:**
- High confidence: The RLVR approach's effectiveness on medical report VIE tasks (measured through F1, precision, and recall improvements)
- Medium confidence: The claim of requiring only 100 annotated samples for effective fine-tuning
- Low confidence: Generalizability to non-medical domains and the specific contributions of individual methodological innovations

## Next Checks

1. **Cross-domain evaluation**: Test the model on diverse document types (legal, financial, scientific papers) to quantify the generalizability limits and identify the factors causing performance degradation.

2. **Annotation efficiency analysis**: Conduct ablation studies varying the number of training samples (10, 50, 100, 500) to determine the actual sample efficiency and identify the minimum effective dataset size.

3. **Reward mechanism sensitivity**: Systematically vary the precision-recall balance ratio and reward scaling parameters to determine their impact on hallucination rates and field coverage, establishing optimal configurations for different medical specialties.