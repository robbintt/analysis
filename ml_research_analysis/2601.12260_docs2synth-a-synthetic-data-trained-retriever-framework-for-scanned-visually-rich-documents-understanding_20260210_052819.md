---
ver: rpa2
title: 'Docs2Synth: A Synthetic Data Trained Retriever Framework for Scanned Visually
  Rich Documents Understanding'
arxiv_id: '2601.12260'
source_url: https://arxiv.org/abs/2601.12260
tags:
- document
- retriever
- synthetic
- inference
- docs2synth
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Docs2Synth introduces a synthetic-data-driven retriever framework
  for scanned visually rich document understanding. The system automatically generates
  and verifies diverse QA pairs from raw documents, fine-tunes a lightweight visual
  retriever on synthetic data, and uses iterative retrieval-generation loops to enhance
  MLLM inference with improved grounding and reduced hallucination.
---

# Docs2Synth: A Synthetic Data Trained Retriever Framework for Scanned Visually Rich Documents Understanding

## Quick Facts
- arXiv ID: 2601.12260
- Source URL: https://arxiv.org/abs/2601.12260
- Reference count: 9
- Primary result: Outperforms strong MLLMs on scanned document QA without human annotations

## Executive Summary
Docs2Synth introduces a synthetic-data-driven retriever framework for understanding scanned visually rich documents. The system automatically generates and verifies diverse QA pairs from raw documents, fine-tunes a lightweight visual retriever on synthetic data, and uses iterative retrieval-generation loops to enhance MLLM inference with improved grounding and reduced hallucination. Experiments on Form-NLU, CORD, and Ephoie datasets show that Docs2Synth consistently outperforms strong MLLMs in key information extraction tasks without requiring human annotations. The framework is delivered as a modular, production-ready Python package supporting flexible deployment across domains.

## Method Summary
Docs2Synth employs a three-stage pipeline: synthetic data generation, retriever fine-tuning, and iterative retrieval-generation. First, it automatically generates diverse QA pairs from raw scanned documents using layout-aware parsing and context extraction. Second, it fine-tunes a lightweight visual retriever on this synthetic data to learn document-specific retrieval patterns. Third, during inference, the retriever identifies relevant document regions which are then fed into an MLLM for answer generation, with multiple retrieval-generation iterations to refine outputs. This approach enables effective grounding without human-labeled training data.

## Key Results
- Consistently outperforms strong MLLMs on Form-NLU, CORD, and Ephoie datasets
- Achieves improved grounding and reduced hallucination through iterative retrieval-generation
- Eliminates need for human annotations through synthetic data generation and verification

## Why This Works (Mechanism)
Docs2Synth works by creating a closed-loop system where synthetic data generation informs retriever training, which in turn improves MLLM grounding during inference. The synthetic QA pairs capture document-specific patterns and variations that generic MLLMs might miss. The lightweight retriever acts as a domain-specific filter, directing the MLLM to relevant document regions and preventing it from hallucinating answers. Iterative refinement allows the system to progressively improve answer accuracy by revisiting and re-contextualizing the retrieved information.

## Foundational Learning
- Visual retrieval fundamentals: Needed to understand how document regions are indexed and matched to queries. Quick check: Can you explain the difference between dense and sparse retrieval in visual document contexts?
- Synthetic data generation for VQA: Required to grasp how realistic QA pairs are automatically created. Quick check: What are the key challenges in ensuring synthetic QA pairs are both diverse and accurate?
- Iterative inference patterns: Important for understanding how multiple retrieval-generation cycles improve results. Quick check: How does iterative refinement differ from simple re-ranking in this context?
- Document layout parsing: Essential for extracting meaningful regions and relationships. Quick check: What layout features are most critical for identifying answer-relevant document regions?
- Multimodal grounding: Core to understanding how visual and textual information are integrated. Quick check: How does the retriever's output format need to match the MLLM's input expectations?

## Architecture Onboarding
Component map: Raw document -> Layout parser -> Synthetic QA generator -> Retriever trainer -> Fine-tuned retriever -> Iterative retrieval-generation loop -> Final answer
Critical path: Synthetic data generation → Retriever fine-tuning → Inference with iterative retrieval-generation
Design tradeoffs: Synthetic data quality vs. quantity; retriever model size vs. inference speed; number of retrieval-generation iterations vs. computational cost
Failure signatures: Poor synthetic data quality leads to retriever bias; inadequate layout parsing causes region misidentification; too few iterations result in incomplete answers
First experiments:
1. Generate synthetic QA pairs from a sample document and manually verify their quality and diversity
2. Fine-tune the retriever on synthetic data and evaluate retrieval accuracy on a small test set
3. Run the full pipeline on a single document with varying numbers of retrieval-generation iterations to observe performance changes

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies heavily on synthetically generated QA pairs, which may not capture real-world error distributions
- Performance on heavily noisy or low-quality documents remains unverified
- Framework's scalability and computational overhead in real-time scenarios are not characterized

## Confidence
High confidence in: Core technical architecture (synthetic data generation → retriever fine-tuning → iterative retrieval-generation) is clearly described and methodologically sound
Medium confidence in: Quantitative improvements over baseline MLLMs are valid, but generalizability across diverse real-world document collections needs further validation
Low confidence in: Claims about production readiness without empirical evidence from deployed systems or user feedback

## Next Checks
1. Conduct a human evaluation study comparing model outputs against ground truth on a held-out set of real documents with varying quality and layout complexity
2. Benchmark against commercial OCR-VQA pipelines (e.g., Google Document AI, Amazon Textract + Titan) on the same datasets to establish relative performance
3. Perform ablation studies to quantify the contribution of synthetic data diversity versus model architecture choices, and test robustness to noise and layout variations