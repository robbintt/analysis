---
ver: rpa2
title: 'Task Matrices: Linear Maps for Cross-Model Finetuning Transfer'
arxiv_id: '2512.14880'
source_url: https://arxiv.org/abs/2512.14880
tags:
- task
- matrix
- linear
- vision
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces task matrices, a method for efficient cross-model
  transfer learning by learning linear transformations between base and fine-tuned
  model representations. The authors demonstrate that these linear mappings can effectively
  approximate fine-tuned model performance across diverse vision and text tasks, often
  approaching or surpassing linear probe baselines while requiring minimal data and
  computational resources.
---

# Task Matrices: Linear Maps for Cross-Model Finetuning Transfer

## Quick Facts
- arXiv ID: 2512.14880
- Source URL: https://arxiv.org/abs/2512.14880
- Reference count: 31
- One-line primary result: Linear transformations between base and fine-tuned model representations can effectively approximate fine-tuned performance across diverse vision and text tasks with minimal data and computational resources.

## Executive Summary
This paper introduces task matrices, a method for efficient cross-model transfer learning that learns linear transformations between base and fine-tuned model representations. The authors demonstrate that these linear mappings can effectively approximate fine-tuned model performance across diverse vision and text tasks, often approaching or surpassing linear probe baselines while requiring minimal data and computational resources. The approach is validated through extensive experiments on datasets like RoBERTa for text classification and CLIP ViT B-32 for vision tasks, showing consistent improvements over baselines and establishing the feasibility of leveraging linear relationships for domain adaptation.

## Method Summary
Task matrices learn a linear transformation that maps intermediate-layer embeddings from a frozen base model to approximate the final-layer embeddings of a fine-tuned model. Given a target task, embeddings are extracted from both a base model (frozen) and a separately fine-tuned model on the same task. The task matrix $W^*$ is learned via least-squares regression to minimize the distance between transformed base embeddings and fine-tuned embeddings. At inference, the task matrix transforms base model intermediate activations before passing them through the fine-tuned model's frozen classification head. This creates a hybrid model that leverages the fine-tuned head's learned decision boundaries while using the base model's representations.

## Key Results
- Task matrices achieve results surpassing linear probes and sometimes approaching fine-tuned levels across 10 different datasets spanning vision and text tasks
- The method requires minimal data and computational resources compared to full fine-tuning while maintaining strong performance
- Task matrices exhibit robustness in data-scarce settings and generalize across multiple tasks
- Performance is most effective on deeper models with "enriched" intermediate representations

## Why This Works (Mechanism)

### Mechanism 1: Linear Embedding Space Mapping
A linear transformation can map representations from a base model's intermediate layer to a fine-tuned model's final layer with sufficient fidelity for accurate classification. Least-squares regression learns a matrix $W^*$ that minimizes the distance between transformed base embeddings and fine-tuned embeddings. This works because fine-tuning preserves much of the base model's representational structure, which can be mapped linearly rather than requiring complex non-linear transformations.

### Mechanism 2: Middle-to-Late Layer Information Enrichment
Intermediate layers of transformer models contain "enriched" representations that are more amenable to linear decoding than the base model's final layer alone. Task matrices are constructed by mapping from a specific intermediate layer $i$ of the base model to the fine-tuned model's final layer. The optimal layer $i$ is often in the middle-to-late range, capturing features developed before the final readout that are readily mappable.

### Mechanism 3: Data-Efficient Regression with Over-Parameterization
The task matrix can be learned with a very small fraction of the training data due to the simplicity of the linear mapping and the over-parameterized nature of the problem. The $d \times d$ task matrix is learned via least-squares regression, and the paper notes that more samples than the embedding dimension are used to avoid "double descent" issues near the interpolation threshold.

## Foundational Learning

- **Linear Probing vs. Fine-Tuning**: The paper's central claim is that a "task matrix" offers a third way, performing better than a standard linear probe but with a cost profile closer to probing than full fine-tuning. Understanding what a linear probe is (training only the final classification head on frozen base features) is essential to appreciate the results.
  - Quick check: If I freeze all layers of a pre-trained ResNet and only train a new fully-connected layer on top for a new dataset, am I fine-tuning or using a linear probe?

- **[CLS] Token Representation**: The method relies on extracting a single vector representation from transformer layers. For models like BERT and ViT, the [CLS] token's activation is designed to serve as an aggregate sequence representation, which is what the task matrix transforms.
  - Quick check: In a Vision Transformer (ViT) processing an image, which token's output would I use as the input to a task matrix?

- **Least-Squares Regression (OLS)**: This is the core mathematical tool for constructing the task matrix. Understanding that it minimizes the squared difference between predicted and actual values helps explain why it's fast and differentiable, but also why it might be sensitive to outliers.
  - Quick check: If I have a matrix of base embeddings $X$ and a matrix of fine-tuned embeddings $Y$, what is the closed-form solution for the weight matrix $W$ that minimizes $||XW - Y||^2$?

## Architecture Onboarding

- **Component map**: Base Model's layer $i$ activation -> Task Matrix multiplication -> Fine-tuned classifier head -> Logits
- **Critical path**: Base Model's layer $i$ activation -> Task Matrix multiplication -> Fine-tuned classifier head -> Logits
- **Design tradeoffs**:
  - **Layer Selection ($i$)**: Choosing an earlier vs. later layer in the base model. Earlier may offer more general features but less task-specific information. Later layers might be more aligned with the fine-tuned model but closer to a standard linear probe. The paper suggests mid-to-late layers are often optimal.
  - **Training Data for $W^*$**: Using more data to learn $W^*$ increases compute but may yield a more robust matrix. The paper shows strong performance with as little as 20% of the data, suggesting a tradeoff favoring data efficiency.
  - **Model Depth**: The method appears to work best on sufficiently deep models. Applying to shallow models may be a tradeoff that fails to yield benefits.
- **Failure signatures**:
  - **Performance near or below linear probe**: If the task matrix fails to outperform a simple linear probe trained on the base model's final layer, the linearity assumption may be invalid for that task/model pair.
  - **Sharp accuracy drop with small data changes**: Observed near the interpolation threshold (Section A, double descent). This indicates an unstable matrix learning process.
  - **Inconsistency across layers**: If no layer provides a strong performance peak, the base model may lack the necessary "enriched representations" for the target task.
- **First 3 experiments**:
  1. **Layer Scan**: For a given dataset and model pair (e.g., CLIP ViT on EuroSAT), compute task matrices mapping from *every* base model layer to the fine-tuned model's final layer. Plot accuracy vs. base layer index to identify the optimal source layer and confirm the existence of a performance peak.
  2. **Data Ablation**: Construct the task matrix using progressively smaller subsets of the training data (e.g., 100%, 50%, 20%, 10%, 1%). Compare accuracy degradation against a linear probe baseline trained with the same data subsets to validate robustness in data-scarce settings.
  3. **Ablation vs. Fine-Tuned Head**: Implement the "Base w/ FT Classifier" ablation from Section 5.5. This means taking the base model's layer $i$ activations and passing them *directly* to the fine-tuned classifier head without the task matrix transformation. Poor performance here validates that the matrix transformation is essential.

## Open Questions the Paper Calls Out
- Can alternative approximation techniques, beyond least-squares regression, improve the construction or performance of task matrices?
- Do linear mappings between base and fine-tuned models exist and remain effective for token-level or generative tasks rather than just sentence-level classification?
- How does the performance of a single multi-task matrix degrade as the number of combined tasks scales significantly beyond eight datasets?

## Limitations
- The method's effectiveness may be limited to models with sufficient depth and architectures that naturally develop "enriched" intermediate representations
- The core assumption that fine-tuning preserves much of the base model's representational structure that can be mapped linearly is fundamental but not thoroughly validated
- The extreme data-scarcity regime (<< 1% of training data) and its interaction with different model scales or domains remains unexplored

## Confidence
- **High Confidence**: The empirical demonstration that task matrices can effectively approximate fine-tuned model performance across diverse vision and text tasks
- **Medium Confidence**: The mechanism explanations, particularly the claim about "enriched" intermediate representations being more mappable than final-layer embeddings
- **Low Confidence**: The data-efficient regression claims, specifically the assertion that "minimal data" is required

## Next Checks
1. **Cross-Domain Transfer Validation**: Apply task matrices to transfer between models trained on completely different domains (e.g., medical imaging to natural images, or legal text to social media text) to test whether the method's success relies on domain similarity or if it can bridge more substantial representational gaps.
2. **Architecture-Agnostic Layer Analysis**: Systematically test the layer selection phenomenon across multiple model architectures to identify whether the "enriched representation" phenomenon is universal or architecture-specific.
3. **Theoretical Bound Derivation**: Develop theoretical bounds on the approximation error of task matrices as a function of model depth, layer position, and fine-tuning magnitude to provide sample complexity guarantees for stable matrix learning.