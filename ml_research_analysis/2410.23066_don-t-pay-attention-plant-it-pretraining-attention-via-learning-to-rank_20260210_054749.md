---
ver: rpa2
title: 'Don''t Pay Attention, PLANT It: Pretraining Attention via Learning-to-Rank'
arxiv_id: '2410.23066'
source_url: https://arxiv.org/abs/2410.23066
tags:
- attention
- stage
- plant
- label
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of extreme multi-label text
  classification (XMTC) where models must assign relevant labels from a large label
  set, often including rare labels. The core method, PLANT (Pretrained and Leveraged
  Attention), introduces a two-stage training strategy that pretrains the attention
  layer as a learning-to-rank module using mutual information gain to guide attention
  weights, then leverages these pretrained weights in full end-to-end training.
---

# Don't Pay Attention, PLANT It: Pretraining Attention via Learning-to-Rank

## Quick Facts
- arXiv ID: 2410.23066
- Source URL: https://arxiv.org/abs/2410.23066
- Reference count: 40
- Primary result: PLANT consistently improves XMTC performance across multiple datasets and backbones, with especially large gains for rare labels and in few-shot settings.

## Executive Summary
This paper introduces PLANT (Pretrained and Leveraged Attention), a novel two-stage training strategy for extreme multi-label text classification (XMTC) that addresses the challenge of rare label prediction. The method pretrains attention layers using a mutual information gain-based learning-to-rank objective, then leverages these pretrained weights during full end-to-end training. PLANT is designed to be architecture-agnostic and integrates seamlessly with various large language model backbones including Mistral-7B, LLaMA3-8B, DeepSeek-V3, and Phi-3. The approach demonstrates consistent performance improvements across diverse tasks including ICD coding, legal topic classification, and content recommendation, with particularly strong results in few-shot scenarios where traditional fine-tuning struggles with rare labels.

## Method Summary
PLANT employs a two-stage training strategy to address the challenges of XMTC, particularly the prediction of rare labels from large label sets. In the first stage, the attention layer is pretrained as a learning-to-rank module using mutual information gain to guide attention weights. This pretraining phase focuses on learning to rank relevant labels higher than irrelevant ones without full end-to-end training. In the second stage, these pretrained attention weights are leveraged during full end-to-end training of the complete model. The method is architecture-agnostic, meaning it can be integrated with various LLM backbones without requiring architectural modifications. The mutual information-based pretraining objective is designed to capture the relationships between input text and relevant labels, enabling better generalization, especially for rare labels that are typically underrepresented in training data.

## Key Results
- PLANT achieves a +0.7 gain in macro-F1 and +1.4 gain in precision@8 over state-of-the-art baselines on MIMIC-IV-full dataset
- Consistent performance improvements across multiple backbones (Mistral-7B, LLaMA3-8B, DeepSeek-V3, Phi-3) and diverse tasks (ICD coding, legal classification, content recommendation)
- Gains are most pronounced for rare labels and in few-shot settings, addressing a critical weakness of traditional fine-tuning approaches
- Ablation studies confirm that attention initialization is a key driver of performance improvements

## Why This Works (Mechanism)
PLANT works by addressing a fundamental challenge in XMTC: the difficulty of learning meaningful representations for rare labels when training data is limited. Traditional fine-tuning approaches struggle with this because they attempt to learn label-specific attention patterns from scratch, which is particularly problematic for labels with few training examples. PLANT's two-stage approach circumvents this by first learning general attention patterns through a mutual information-based learning-to-rank objective during pretraining. This pretraining phase teaches the model to identify relevant versus irrelevant labels based on mutual information, creating a more robust attention mechanism that can generalize to rare labels. When these pretrained attention weights are then leveraged during full end-to-end training, the model starts from a much stronger position, particularly for rare labels where traditional methods would have insufficient data to learn effective attention patterns from scratch.

## Foundational Learning

- **Extreme Multi-Label Text Classification (XMTC)**: Why needed - Understanding the problem space where models must assign relevant labels from very large label sets, often including rare labels. Quick check - Can distinguish between multi-class, multi-label, and extreme multi-label classification scenarios.

- **Learning-to-Rank**: Why needed - The pretraining objective uses ranking principles to order labels by relevance. Quick check - Can explain how ranking differs from classification and why it's useful for XMTC.

- **Mutual Information**: Why needed - The core pretraining objective uses mutual information gain to guide attention weights. Quick check - Can describe how mutual information measures the dependency between input text and label relevance.

- **Attention Mechanisms**: Why needed - Understanding how attention layers work and why their initialization matters for downstream performance. Quick check - Can explain how attention weights are computed and used in transformer architectures.

- **Few-shot Learning**: Why needed - PLANT's gains are most pronounced in low-data scenarios. Quick check - Can articulate why traditional fine-tuning struggles with rare labels and limited data.

## Architecture Onboarding

**Component Map**: Input Text -> Transformer Backbone -> Attention Layer -> Label Prediction -> Loss Function (Cross-Entropy for Stage 2, Mutual Information for Stage 1)

**Critical Path**: The attention layer is the critical component that PLANT modifies. In Stage 1, attention weights are pretrained using mutual information gain; in Stage 2, these weights are leveraged during full training.

**Design Tradeoffs**: PLANT trades additional pretraining complexity for better generalization, particularly for rare labels. The two-stage approach requires more training time but provides substantial gains in low-data scenarios. The architecture-agnostic design provides flexibility but may not fully optimize for specific backbone characteristics.

**Failure Signatures**: Poor performance on common labels, instability during Stage 2 training, or degradation compared to baseline when sufficient training data is available would indicate potential issues with the mutual information pretraining objective or weight transfer between stages.

**First Experiments**:
1. Run Stage 1 pretraining with a small subset of data to verify the mutual information objective is working correctly
2. Test attention weight transfer by initializing a baseline model with PLANT-pretrained weights and comparing performance
3. Conduct a simple ablation by removing the mutual information component from Stage 1 to isolate its contribution

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations

- The mutual information-based attention pretraining objective lacks theoretical grounding for why this specific formulation optimizes downstream XMTC performance
- The architecture-agnostic claim needs stronger validation beyond the four tested LLM backbones, particularly for non-LLM architectures
- Experiments focus heavily on few-shot scenarios, with less clarity about gains in fully supervised settings with large amounts of labeled data

## Confidence

- **High confidence**: PLANT consistently improves performance across multiple datasets and backbone architectures; gains are most pronounced for rare labels and in few-shot settings
- **Medium confidence**: The two-stage training strategy is the primary driver of gains, though the exact contribution of each component remains partially unclear
- **Low confidence**: Claims about architecture-agnostic applicability beyond the tested LLM family, and theoretical justification for the mutual information pretraining objective

## Next Checks

1. Conduct controlled experiments ablating the mutual information gain component from the attention pretraining to isolate its specific contribution versus simple attention initialization strategies

2. Test PLANT's performance on non-LLM architectures (e.g., CNNs, traditional MLPs with attention) to validate the architecture-agnostic claim across a broader range of model families

3. Evaluate performance in high-resource settings with large amounts of labeled data to determine whether the gains persist when traditional fine-tuning has sufficient supervision