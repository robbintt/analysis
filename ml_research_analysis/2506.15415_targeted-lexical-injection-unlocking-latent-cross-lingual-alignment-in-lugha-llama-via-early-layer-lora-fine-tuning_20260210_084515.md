---
ver: rpa2
title: 'Targeted Lexical Injection: Unlocking Latent Cross-Lingual Alignment in Lugha-Llama
  via Early-Layer LoRA Fine-Tuning'
arxiv_id: '2506.15415'
source_url: https://arxiv.org/abs/2506.15415
tags:
- layer
- alignment
- lexical
- pairs
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving cross-lingual lexical
  alignment for low-resource languages (LRLs) in LLMs, using Swahili as a case study.
  The authors propose Targeted Lexical Injection (TLI), a fine-tuning approach that
  leverages the observation that Lugha-Llama-8B-wura exhibits near-perfect lexical
  alignment for Swahili-English word pairs in its early internal layers (Layer 2),
  but this alignment is not fully reflected in its final output.
---

# Targeted Lexical Injection: Unlocking Latent Cross-Lingual Alignment in Lugha-Llama via Early-Layer LoRA Fine-Tuning

## Quick Facts
- **arXiv ID:** 2506.15415
- **Source URL:** https://arxiv.org/abs/2506.15415
- **Reference count:** 5
- **Primary result:** Early-layer LoRA fine-tuning improves cross-lingual lexical alignment by 28% and generalizes to unseen vocabulary.

## Executive Summary
This paper addresses the challenge of improving cross-lingual lexical alignment for low-resource languages in LLMs, using Swahili as a case study. The authors propose Targeted Lexical Injection (TLI), which leverages the observation that Lugha-Llama-8B-wura exhibits near-perfect lexical alignment for Swahili-English word pairs in early layers (Layer 2), but this alignment degrades by the output. TLI uses LoRA fine-tuning with a contrastive loss objective targeting Layer 2 embeddings to better preserve and propagate this inherent cross-lingual knowledge to the final output, significantly improving both trained and unseen word pair alignment.

## Method Summary
TLI is a parameter-efficient fine-tuning approach that first identifies the optimal layer for lexical alignment through pilot scanning (Layer 2 in this case). It then applies LoRA adapters (rank 16) to the query and value projections across all layers, but computes the contrastive triplet margin loss specifically on Layer 2 embeddings using in-batch hard negatives. The model is trained for 5 epochs with AdamW, and the LoRA adapters are merged before evaluation. The key innovation is targeting the loss at an early layer where alignment is naturally strongest, rather than at the output where it degrades.

## Key Results
- Output-level lexical alignment for 623 trained Swahili-English word pairs increased from 0.3211 to 0.4113 (+28.08%, p < 1.33 x 10^-240)
- Improvements generalized to 63 unseen control word pairs, increasing similarity from 0.3143 to 0.4033 (+28.32%, p < 7.17 x 10^-27)
- The method demonstrates that early-layer alignment can be preserved through the network using targeted LoRA fine-tuning

## Why This Works (Mechanism)

### Mechanism 1: Early-Layer Anchor Optimization
TLI improves output alignment by optimizing gradient updates based on embeddings from Layer 2 where cross-lingual alignment is inherently strongest (~0.99998 similarity), rather than optimizing solely for final output error. This forces later layers to maintain rather than distort the alignment signal.

### Mechanism 2: Representation Propagation via LoRA
LoRA adapters trained on early-layer objectives effectively steer the model's residual stream to preserve cross-lingual geometry in the final output. Low-rank updates to query/value projections create a gradient highway that ensures aligned representation survives deeper layers.

### Mechanism 3: Pathway Generalization
Improving alignment for seen word pairs generalizes to unseen pairs because the intervention refines the process of alignment preservation, not just specific embeddings. The contrastive loss teaches the model to treat cross-lingual synonyms as identical concepts before language-specific features differentiate them.

## Foundational Learning

- **Concept:** **Contrastive Learning (Triplet Loss)**
  - **Why needed here:** This is the objective function that explicitly shapes the embedding space.
  - **Quick check question:** Can you explain why using "hard negatives" (in-batch negatives) is more effective than random negatives for this specific alignment task?

- **Concept:** **Layer-wise Analysis / Probing**
  - **Why needed here:** The entire paper hinges on the non-obvious fact that Layer 2 is superior to Layer 31.
  - **Quick check question:** Why would an early layer (2) have better alignment than the final layer (31) in a pre-trained LLM?

- **Concept:** **Parameter-Efficient Fine-Tuning (PEFT/LoRA)**
  - **Why needed here:** The paper claims efficiency and requires understanding how LoRA adapters function.
  - **Quick check question:** Does LoRA freeze the original model weights, and if so, how does the gradient from Layer 2 affect the computation in Layer 31?

## Architecture Onboarding

- **Component map:** Base model (Lugha-Llama-8B-wura) -> LoRA adapters (rank 16) -> Forward hook at Layer 2 -> Contrastive loss computation -> Backpropagation to LoRA only

- **Critical path:** Pilot Phase (scan layers 0-31) -> Setup (inject LoRA, register hook) -> Training (forward pass, extract Layer 2, compute loss, update LoRA) -> Eval (merge LoRA, extract Layer 31, compute similarity)

- **Design tradeoffs:**
  - Target Layer: Layer 2 leverages latent knowledge but risks ignoring surface-form corrections; Layer 31 might force alignment but lack semantic consistency
  - Rank (16): Chosen for balance; higher might overfit, lower might fail to shift global geometry
  - In-batch Negatives: Efficient but assumes diverse batches for meaningful contrastive signals

- **Failure signatures:**
  - Cosine Similarity Plateau: If similarity stays at ~0.32, learning rate may be too low or gradient is vanishing
  - Control Set Divergence: If trained pairs improve but control drops, model is overfitting/memorizing
  - Layer 2 Alignment Degradation: If intervention lowers Layer 2 quality while fixing output

- **First 3 experiments:**
  1. Layer Ablation: Apply TLI targeting Layer 10 and Layer 31 separately to verify Layer 2 is optimal
  2. Zero-Control Test: Train on subset of verbs, evaluate on unseen nouns to confirm pathway learning vs memorization
  3. Rank Sensitivity: Test LoRA Rank 4 vs 64 to determine minimal parameter change required

## Open Questions the Paper Calls Out

- Does the optimal early layer for cross-lingual alignment (Layer 2) generalize across different LLM architectures and language pairs?
- Does the improvement in lexical alignment directly translate to performance gains in downstream NLP tasks?
- What is the precise neural mechanism by which TLI generalizes to unseen vocabulary?
- Is TLI effective for morphologically rich languages where sub-word tokenization significantly alters word representations?

## Limitations
- Results are based on a single model (Lugha-Llama-8B-wura) and language pair (Swahili-English)
- Training dataset is relatively small (623 pairs), raising overfitting concerns
- Control set size (63 pairs) limits statistical power for detecting negative generalization effects
- Paper does not report on computational overhead or inference latency changes

## Confidence
- **High Confidence**: Layer 2 alignment observation and statistical significance of output improvements
- **Medium Confidence**: Generalization to unseen word pairs and mechanism explanation
- **Low Confidence**: Broader claim about general "unlocking" of cross-lingual capabilities across LLMs

## Next Checks
1. **Cross-Model Validation**: Apply TLI to a different multilingual LLM (e.g., BLOOMZ or mT0) to verify whether the Layer 2 alignment peak exists and whether TLI can improve it.

2. **Zero-Shot Lexical Generalization**: Train TLI on a subset of semantic categories (e.g., only nouns) and evaluate on unseen categories (e.g., verbs and adjectives) to validate pathway learning vs memorization.

3. **Layer Ablation Study**: Implement TLI targeting Layer 10 and Layer 31 separately, measuring whether Layer 2 is truly optimal to confirm or refute the specific architectural claim.