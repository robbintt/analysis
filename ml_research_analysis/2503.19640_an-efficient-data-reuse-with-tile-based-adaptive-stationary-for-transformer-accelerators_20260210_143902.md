---
ver: rpa2
title: An Efficient Data Reuse with Tile-Based Adaptive Stationary for Transformer
  Accelerators
arxiv_id: '2503.19640'
source_url: https://arxiv.org/abs/2503.19640
tags:
- stationary
- input
- tile
- weight
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a Tile-based Adaptive Stationary (TAS) scheme
  for transformer accelerators that dynamically selects between input stationary and
  weight stationary strategies based on input sequence length, optimizing data reuse
  during linear projections. By choosing the optimal stationary scheme at tile granularity,
  TAS reduces external memory accesses by over 97% compared to traditional fixed schemes
  while avoiding simultaneous read/write conflicts.
---

# An Efficient Data Reuse with Tile-Based Adaptive Stationary for Transformer Accelerators

## Quick Facts
- arXiv ID: 2503.19640
- Source URL: https://arxiv.org/abs/2503.19640
- Reference count: 10
- Reduces external memory accesses by over 97% compared to traditional fixed schemes

## Executive Summary
This paper introduces a Tile-based Adaptive Stationary (TAS) scheme for transformer accelerators that dynamically selects between input stationary and weight stationary strategies based on input sequence length. By choosing the optimal stationary scheme at tile granularity, TAS achieves over 97% reduction in external memory accesses and approximately 97% reduction in energy consumption compared to existing approaches. The scheme is compatible with various attention optimization techniques and hardware accelerators, making it broadly applicable across transformer architectures.

## Method Summary
TAS works by comparing matrix dimensions M (input sequence length × hidden dimension) and K (weight output dimension) to decide between Input Stationary (IS) and Weight Stationary (WS) schemes. If M < K, IS keeps inputs stationary to minimize input-related EMA; if M ≥ K, WS keeps weights stationary. Both schemes are combined with Output Stationary (OS) for partial sums to prevent simultaneous read/write conflicts. The method processes matrices at tile granularity using a square PE array, with adaptive selection occurring per linear projection layer.

## Key Results
- Reduces external memory accesses by over 97% compared to traditional fixed schemes
- Achieves approximately 97% reduction in energy consumption compared to existing approaches when applied to models like BERT-Base
- Maintains compatibility with various attention optimization techniques and hardware accelerators

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Stationary Selection Based on Sequence Length
The scheme dynamically selects between Input Stationary (IS) and Weight Stationary (WS) based on the relationship between input sequence dimension (M) and weight output dimension (K). The decision rule compares matrix dimensions: if M < K, IS is more efficient; if M ≥ K, WS is better. This requires only comparing two scalar values per linear projection layer.

### Mechanism 2: Hybrid Temporal-Spatial Reuse via OS Integration
Combining temporal reuse (IS or WS) with spatial reuse (Output Stationary) reduces internal partial sum storage requirements while preventing simultaneous external read/write conflicts. Only completed output tiles are written to external memory, eliminating intermediate partial sum writes.

### Mechanism 3: Tile-Granular Dataflow with Controlled Advancement
Processing matrices at tile granularity enables flexible mapping to fixed-size PE arrays while the controlled tile advancement pattern maintains data locality. The scheme advances input or weight tiles after completing k′/k or m′/m iterations, accumulating partial sums in internal memory.

## Foundational Learning

- **Dataflow Stationary Schemes (IS/WS/OS)**: These schemes determine which data type remains in internal memory for reuse. Selecting the wrong scheme for a given matrix dimension ratio increases EMA.
  - Why needed here: To minimize external memory accesses during linear projections
  - Quick check question: For Y = X × W where X is 128×64 and W is 64×512, which single stationary scheme minimizes input-related EMA?

- **External Memory Access (EMA) as Energy Bottleneck**: EMA consumes 10–100× more energy than internal computation, making EMA reduction the primary optimization target regardless of compute efficiency gains.
  - Why needed here: To justify the focus on reducing DRAM accesses rather than optimizing compute
  - Quick check question: If a linear projection requires 2 DRAM accesses and 1 compute operation, and DRAM costs 50× more energy per access than compute, what fraction of total energy is EMA?

- **Partial Sum Accumulation and Read/Write Conflicts**: Writing partial sums to external memory before completion creates simultaneous read/write demands that DRAM cannot handle efficiently, causing stalls.
  - Why needed here: To understand why OS integration is critical for preventing performance degradation
  - Quick check question: In tiled matrix multiplication producing a 1024×1024 output using 32×32 tiles, how many partial sum accumulations per output element are required before the result can be written to external memory?

## Architecture Onboarding

- **Component map**: External memory (DRAM) -> Internal SRAM buffers (input, weight, partial sum) -> PE array (8×8 or 16×16) -> Adaptive controller -> Tile scheduler

- **Critical path**: Receive input tensor → extract M (sequence length × hidden dimension tiles) → Adaptive controller compares M vs K → Load appropriate tile pattern (IS-OS or WS-OS) → Accumulate partial sums → Write completed output tiles only

- **Design tradeoffs**: Internal buffer size vs EMA reduction, tile size vs flexibility, decision frequency vs overhead

- **Failure signatures**: Simultaneous DRAM read/write detected, EMA reduction significantly below reported ~97%, PE stalls waiting for data, energy reduction inconsistent across transformer layers

- **First 3 experiments**:
  1. Implement the analytical memory model and code the EMA equations from Table II for Naïve, IS, and WS schemes
  2. Create a simulation script to calculate EMA for BERT-Base layers using standard dimensions (Hidden=768, FFN Intermediate=3072, Sequence Length=512)
  3. Validate reduction ratios by comparing calculated EMA reduction against the ~97% energy reduction claimed in Table IV

## Open Questions the Paper Calls Out

### Open Question 1
How does TAS interact with attention optimization techniques (e.g., FlashAttention, sparse attention, low-rank approximations) when applied jointly?
Basis: The abstract and conclusion state TAS is "compatible with various attention optimization techniques," but no experimental validation or integration analysis is provided.

### Open Question 2
What is the actual hardware area and latency overhead of the adaptive selection logic and tile-granularity switching mechanism?
Basis: The adaptive mechanism is described as having "minimal overhead in decision-making hardware," but no synthesis results, area measurements, or timing analysis are provided.

### Open Question 3
Does TAS generalize to transformer training workloads, where gradient accumulation and weight updates create fundamentally different memory access patterns?
Basis: The paper evaluates only inference, with no discussion of training-specific access patterns or backward pass considerations.

## Limitations
- Relies on idealized tile size matching and homogeneous sequence lengths per layer
- Energy reduction calculation uses DRAM energy weights from an external paper
- Adaptive decision mechanism requires careful implementation to avoid control overhead

## Confidence
- **High confidence**: Adaptive IS/WS selection mechanism and tile-granular dataflow patterns
- **Medium confidence**: ~97% energy reduction claim (depends on external energy weight assumptions)
- **Medium confidence**: Read/write conflict elimination via OS integration (theoretical, requires empirical validation)

## Next Checks
1. **Energy model verification**: Implement the EMA equations from Table II and verify that TAS consistently selects the lower-EMA scheme across the full range of transformer layer dimensions

2. **Control overhead measurement**: Profile the decision-making logic's impact on critical path timing, particularly for architectures with irregular sequence lengths

3. **Buffer sizing constraints**: Experimentally determine the minimum internal buffer capacity required to prevent partial sum spilling, and quantify the EMA penalty when buffers are undersized