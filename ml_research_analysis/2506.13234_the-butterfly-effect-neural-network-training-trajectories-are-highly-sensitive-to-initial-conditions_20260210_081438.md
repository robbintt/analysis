---
ver: rpa2
title: 'The Butterfly Effect: Neural Network Training Trajectories Are Highly Sensitive
  to Initial Conditions'
arxiv_id: '2506.13234'
source_url: https://arxiv.org/abs/2506.13234
tags:
- training
- barriers
- batch
- figure
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the sensitivity of neural network training
  trajectories to initial conditions by systematically perturbing networks at different
  training stages. The authors demonstrate a "butterfly effect" where even single-weight
  perturbations early in training reliably cause networks to diverge into distinct
  loss basins, as measured by training loss barriers.
---

# The Butterfly Effect: Neural Network Training Trajectories Are Highly Sensitive to Initial Conditions

## Quick Facts
- arXiv ID: 2506.13234
- Source URL: https://arxiv.org/abs/2506.13234
- Reference count: 40
- Primary result: Neural networks exhibit high sensitivity to initialization perturbations early in training, with stability increasing rapidly thereafter.

## Executive Summary
This paper investigates the sensitivity of neural network training trajectories to initial conditions by systematically perturbing networks at different training stages. The authors demonstrate a "butterfly effect" where even single-weight perturbations early in training reliably cause networks to diverge into distinct loss basins, as measured by training loss barriers. They quantify this divergence using L2 distance between parameters, loss barriers, barriers after permutation alignment, and representational similarity via Angular CKA. Their findings reveal that stability increases rapidly during early training, with later perturbations causing minimal divergence even at large scales. They also show that fine-tuning stability depends heavily on the pre-training task and duration, with some language model experiments exhibiting reduced stability after extended pre-training. The work provides insights into neural network training stability with implications for fine-tuning, model merging, and ensemble diversity.

## Method Summary
The authors systematically perturb neural networks during training to study sensitivity to initial conditions. Their method involves training a network from initialization or checkpoint until perturbation time t, then creating a copy and applying either a batch perturbation (gradient-based, along training direction) or Gaussian perturbation (Kaiming-scaled) with magnitude σ normalized to initialization scale. Both the original and perturbed networks are then trained deterministically to completion, and divergence is measured using multiple metrics: loss barriers via linear mode connectivity, L2 distance between weights, barriers after permutation alignment, and Angular CKA for representational similarity. Experiments span various architectures (ResNet-20/50, ViT, MultiBERT, OLMo-1B) and tasks (CIFAR-10/100, GLUE, GSM8K).

## Key Results
- Early training exhibits extreme sensitivity: perturbations as small as 0.01% of weights cause networks to diverge into different loss basins
- Stability increases rapidly during early training: later perturbations (even at 50% training progress) cause minimal divergence
- Architectural factors matter: wider architectures and longer learning rate warm-up periods improve training stability
- Fine-tuning stability varies by pre-training task: some language models show reduced stability after extended pre-training
- Divergence metrics show non-exponential growth: L2 distances and barriers increase non-linearly over time

## Why This Works (Mechanism)

### Mechanism 1: Early Training Chaos
Extremely small perturbations to initial weights cause divergent training trajectories due to a "chaotic" early phase that amplifies tiny weight differences into different loss basins. This amplification is intrinsic to the training map, not just stochastic noise. Perturbations applied after the warm-up phase show rapidly diminishing barriers.

### Mechanism 2: Trajectory Stabilization
Stability to perturbations increases significantly over the course of training as the training landscape becomes more convex (or less chaotic) as weights move away from initialization, reducing sensitivity to initial conditions. At late training stages (>50% progress), only very large perturbations cause divergence.

### Mechanism 3: Architectural & Hyperparameter Modulation
Wider architectures and longer learning rate warm-up periods improve training stability by aligning training dynamics more closely with stable regimes (e.g., kernel regime for wide networks) or smoothing the loss landscape. This improved stability is due to smoother optimization trajectories rather than just implicit regularization.

## Foundational Learning

### Concept: Linear Mode Connectivity (LMC)
**Why needed here:** LMC is the primary metric used to quantify "divergence" between trained networks. A non-zero loss barrier indicates two networks are in different loss basins.
**Quick check question:** If two trained models have a high loss barrier when their weights are linearly interpolated, what does that imply about their loss basins?

### Concept: Training Map as a Dynamical System
**Why needed here:** The paper models training as a deterministic function iteratively applied to weights. Understanding this helps interpret the "butterfly effect" as sensitivity to initial conditions in a dynamical system.
**Quick check question:** In a deterministic training map, what determines the final state of the model?

### Concept: Permutation Symmetry
**Why needed here:** Neural networks have many equivalent parameterizations due to neuron permutations. The paper uses permutation alignment to determine if diverged networks are just permuted versions of the same solution.
**Quick check question:** Why is permutation alignment necessary when comparing two independently trained models?

## Architecture Onboarding

### Component map
spawn-and-perturb loop: initial model state -> train for t steps -> copy network -> apply perturbation ε -> train both copies deterministically -> compute divergence metrics

### Critical path
The most critical step is ensuring **deterministic training** for both trajectories after the perturbation is applied. Any uncontrolled stochasticity (e.g., non-deterministic GPU operations) would invalidate the experiment's goal of isolating the effect of the initial perturbation.

### Design tradeoffs
- **Perturbation type vs. generality:** Batch perturbations are more realistic (along training direction) but may miss instabilities in other directions. Gaussian perturbations are broader but less targeted.
- **Metric choice vs. computational cost:** Loss barriers are principled but require multiple inference passes. L2 distance is cheap but can be misleading.

### Failure signatures
- **Non-deterministic noise:** Unaccounted-for randomness will inflate divergence metrics, making all runs appear unstable.
- **Incorrect perturbation scale:** If perturbations are not normalized to the network's initialization scale, results will be incomparable across architectures.
- **CKA estimation variance:** Using insufficient samples or incorrect estimator will produce noisy similarity measurements.

### First 3 experiments
1. Establish a baseline: Train a ResNet-20 on CIFAR-10, apply a small Gaussian perturbation (σ=10⁻⁴) at step 0, and measure the final loss barrier. Verify that it's non-zero.
2. Validate temporal effect: Repeat the baseline experiment but apply the same perturbation at 50% of training. Verify that the barrier is significantly smaller.
3. Test an intervention: Repeat the baseline experiment with 10x learning rate warm-up. Verify that the barrier at step 0 is reduced compared to the baseline.

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** Can specific hyperparameter settings entirely eliminate instability at initialization?
**Basis in paper:** [explicit] The authors state in the conclusion that "Further investigation is also needed to determine (1) if certain hyperparameter settings entirely eliminate instability at initialization."
**Why unresolved:** While combining settings like increased width and learning rate warm-up reduced instability, the experiments showed that barriers at initialization persisted (were not reduced to zero) in all tested configurations.
**What evidence would resolve it:** Identification of a training regime where perturbations applied at initialization result in zero loss barriers despite non-zero perturbation magnitudes.

### Open Question 2
**Question:** What mechanisms drive the observed non-exponential divergence of barriers and L2 distances?
**Basis in paper:** [explicit] The authors note in Section 5 and the conclusion that "More work is needed to explore other mechanisms that could drive these observed rates of divergence."
**Why unresolved:** The authors found that L2 distances and barriers do not grow exponentially over time, contradicting the expectations of standard linearized dynamical systems approximations.
**What evidence would resolve it:** A theoretical model that accurately predicts the specific non-exponential growth rates of functional divergence observed during the training trajectory.

### Open Question 3
**Question:** What perturbations can be used to reliably improve ensemble performance?
**Basis in paper:** [explicit] The authors explicitly call for "further investigation" to determine "what perturbations, if any, can be used to reliably improve ensemble performance."
**Why unresolved:** While the authors found that perturbations improved ensembling for ResNets, the same method failed for ViT models, creating an inconsistency that suggests the approach is not universal.
**What evidence would resolve it:** A systematic study identifying specific perturbation types or magnitudes that consistently yield positive ensemble gains across both vision (CNN and Transformer) and language architectures.

## Limitations
- The core claim relies on truly reproducible deterministic training, which may be compromised by uncontrolled stochasticity in practical implementations
- Findings primarily focus on supervised learning tasks, with unknown generalizability to reinforcement learning or unsupervised learning paradigms
- While permutation alignment accounts for network symmetry, the quality and completeness of this alignment could impact barrier measurement interpretations

## Confidence

**High Confidence:** The experimental observation that stability to perturbations increases rapidly during early training (Mechanism 2). The controlled experiments with different perturbation timings and scales provide robust evidence for this temporal effect.

**Medium Confidence:** The claim that wider architectures and longer warm-up periods improve stability (Mechanism 3). While supported by experiments, the mechanistic explanation linking these architectural choices to training dynamics is somewhat speculative.

**Medium Confidence:** The existence of a "chaotic" early training phase that amplifies small perturbations (Mechanism 1). The evidence is strong for the amplification effect, but the characterization as "chaotic" could benefit from additional dynamical systems analysis.

## Next Checks

1. **Reproducibility Test:** Run unperturbed twin experiments (train two identical networks with the same random seed) across different hardware platforms to verify that deterministic training truly produces bit-identical models. Any divergence would indicate uncontrolled stochasticity.

2. **Architecture Transfer Test:** Replicate the early perturbation experiments (t=0%, σ=10⁻⁴) on a transformer architecture (e.g., ViT) trained on CIFAR-10 to assess whether the butterfly effect generalizes beyond convolutional networks.

3. **Alternative Divergence Metric:** Compute the empirical Fisher information matrix at initialization and perturbed points, then measure the KL divergence between the resulting Gaussian approximations. Compare this divergence measure with the loss barrier to validate that different metrics capture the same underlying phenomenon.