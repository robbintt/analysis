---
ver: rpa2
title: Interpretable Text Classification Applied to the Detection of LLM-generated
  Creative Writing
arxiv_id: '2601.07368'
source_url: https://arxiv.org/abs/2601.07368
tags:
- uni00000048
- uni00000057
- text
- uni00000044
- uni00000056
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of distinguishing human-written
  creative fiction from LLM-generated text, a task where humans perform poorly but
  machine learning models achieve high accuracy. Using a dataset of 16,136 text samples
  from detective novels, various ML classifiers were tested with unigram features.
---

# Interpretable Text Classification Applied to the Detection of LLM-generated Creative Writing

## Quick Facts
- arXiv ID: 2601.07368
- Source URL: https://arxiv.org/abs/2601.07368
- Reference count: 7
- Primary result: Linear classifier achieves 98% accuracy distinguishing human detective fiction from LLM-generated text, while humans perform near chance level

## Executive Summary
This study demonstrates that machine learning models can reliably distinguish human-written creative fiction from LLM-generated text, achieving 98% accuracy versus human chance-level performance. Using a dataset of 16,136 text samples from detective novels, various ML classifiers were tested with unigram features. Analysis revealed that LLMs use a broader variety of synonyms and introduce stylistic changes undetectable by humans but identifiable by ML models. Key factors include temporal drift, Americanisms, foreign language usage, and colloquialisms. The findings suggest robust classification resistant to manipulation, aiding publishers in identifying AI-generated content.

## Method Summary
The study uses a binary classification approach where 8,068 excerpts from 12 classic detective novels (Agatha Christie, Dorothy L. Sayers) are compared against 8,068 rewritten versions generated by GPT-4.1 (temperature 0.7). Texts are chunked into 100-word segments, tokenized using a custom inclusive method that preserves punctuation and case, then converted to bag-of-words unigram features. A linear classifier with L2 regularization is trained using 70/15/15 train/validation/test splits. The interpretable model allows feature-level analysis by examining classifier weights, revealing that LLMs introduce synonym variety and stylistic anachronisms that skew probability distributions in detectable ways.

## Key Results
- Linear classifier achieves 98% test accuracy on binary classification task
- Human evaluators perform at near-chance level (49.9% accuracy) when distinguishing human vs. LLM text
- LLM-generated text shows 0.29 bits higher Shannon entropy and ~22% greater vocabulary variation than human text
- Ablation study shows accuracy drops to ~88% when top 100 features are removed, indicating vulnerability to adversarial manipulation

## Why This Works (Mechanism)

### Mechanism 1: Lexical Diversity via Synonym Substitution
LLM-generated text exhibits measurably higher lexical diversity than human-written text, detectable via unigram distribution analysis. LLMs introduce synonym variety during rewriting that skews word probability distributions away from human norms. A linear classifier learns weighted associations: certain words (e.g., "said," "very," "good") receive negative weights (human), while their rarer synonyms ("remarked," "exquisite") receive positive weights (LLM). The cumulative signal across thousands of unigrams creates separable decision boundaries. Core assumption: Rewriting prompts explicitly instructing structural variation amplify synonym substitution patterns. Break condition: If adversarial rewriting constrains vocabulary to match a specific author's historical word frequency profile, the entropy differential collapses and accuracy drops.

### Mechanism 2: Temporal and Dialectal Mismatches
Temporal and dialectal mismatches between training data eras and LLM training corpora create detectable stylistic anachronisms. Human corpus (1920s-1930s British detective fiction) contains period-specific usages ("to-day," "to-morrow," British spellings). LLMs, trained predominantly on contemporary American-weighted web text, default to modern patterns ("exited" as verb, "-ize" spellings) even when instructed to preserve style. These mismatches appear as high-weight features in the linear model. Core assumption: The effect generalizes beyond this specific corpus—any genre with distinctive temporal or regional markers will expose similar drift. Break condition: If LLM is few-shot prompted with period-appropriate exemplars or fine-tuned on genre-specific historical corpora, temporal signal attenuates.

### Mechanism 3: Statistical Regularities vs. Semantic Coherence
Human perception relies on semantic coherence; ML classifiers exploit statistical regularities invisible to readers. Human evaluators compare texts holistically, searching for semantic or syntactic anomalies. LLM outputs are semantically coherent, so humans perform at chance (49.9% in this study). ML models aggregate weak signals: a single "remarked" instead of "said" provides small positive evidence; combined with dozens of similar substitutions, posterior confidence exceeds threshold. Core assumption: The signal remains robust across LLM families and generation methods. Break condition: If adversarial post-processing nullifies the top 100 features, accuracy drops to ~88%—still above chance, but the margin narrows significantly.

## Foundational Learning

- **Concept: Bag-of-words unigram features**
  - **Why needed here:** The entire detection method rests on counting individual tokens and their relative frequencies across classes. Without understanding that word identity (not order) drives classification, the interpretability analysis is opaque.
  - **Quick check question:** Given two sentences—"The detective arrived" and "Arrived, the detective did"—do they produce identical unigram feature vectors? (Yes, if punctuation is normalized.)

- **Concept: Linear classifier weights as feature importance**
  - **Why needed here:** The paper's central contribution is interpretability through weight inspection. You must understand that positive weights push toward Class 1 (LLM), negative toward Class 0 (human), and magnitude indicates influence.
  - **Quick check question:** If "said" has weight -0.8 and "remarked" has weight +0.6, which single-word feature provides stronger classification signal? ("said"—larger absolute magnitude)

- **Concept: Shannon entropy for vocabulary diversity**
  - **Why needed here:** Section 5.2 uses entropy to quantify the synonym-variety hypothesis. Higher entropy = more uniform word distribution = more diverse vocabulary.
  - **Quick check question:** If a text uses only 10 unique words evenly distributed vs. 100 unique words with one word appearing 90% of the time, which has higher entropy? (The first—more uniform distribution yields higher entropy)

## Architecture Onboarding

- **Component map:** Data preparation (Project Gutenberg texts → chunking → LLM rewriting) -> Tokenization (custom inclusive method) -> Feature extraction (unigram bag-of-words) -> Classifier (linear model with L2 regularization) -> Evaluation (70/15/15 train/validation/test split)

- **Critical path:** Tokenization consistency is paramount—Section 3.3 shows different tokenizers yield similar accuracy, but interpretation requires exact feature-to-word mapping. Feature selection criteria (f_max ≥ 100, R ≥ 0.75) determine which unigrams enter manual annotation; adjust thresholds based on corpus size. Weight magnitude threshold (0.1 in ablation study) identifies "important" features for robustness testing.

- **Design tradeoffs:** Unigrams vs. n-grams: Paper finds unigrams sufficient; higher-order features add minimal accuracy but reduce interpretability. Linear vs. neural: 98% accuracy achievable with linear; neural gains don't justify opacity. Rewriting prompt style: Current prompt explicitly requests "structurally different" output—may inflate detectability compared to continuation or outline-based generation.

- **Failure signatures:** Accuracy drops sharply when test distribution shifts (different author era, genre, or LLM family not in training). False positives spike for human text with modern vocabulary or American spelling. Adversarial paraphrasing that systematically replaces high-weight synonyms (e.g., "remarked" → "said") degrades performance incrementally.

- **First 3 experiments:**
  1. **Baseline replication:** Train linear classifier on DET12 split; verify ~98% test accuracy; extract top 50 positive/negative weight features for manual inspection
  2. **Robustness probe:** Ablate top 100 high-magnitude features (set weights to 0); measure accuracy degradation curve (expected: ~88% floor)
  3. **Distribution shift test:** Generate new LLM samples using outline-based prompts (no rewriting); evaluate whether accuracy holds without explicit synonym-introducing instructions

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the high classification accuracy persist when LLMs generate text from scratch (e.g., outlines or continuations) rather than rewriting existing excerpts?
- **Basis in paper:** The authors note a limitation where the "rewriting" methodology may have artificially increased variation, stating, "further work is required to... establish the effects of different generation methods overall."
- **Why unresolved:** The current study only evaluated text generated via a specific "rewrite this excerpt" prompt, leaving the performance on generative tasks (writing from outlines) unconfirmed.
- **What evidence would resolve it:** Replicating the classification experiment using a dataset of LLM-generated novels based on plot summaries rather than rewritten paragraphs.

### Open Question 2
- **Question:** Are the identified "constellation" of features (temporal drift, Americanisms, synonym variety) consistent across different creative genres and LLM architectures?
- **Basis in paper:** The conclusion explicitly lists "analysing text produced by different LLMs, in various genres" as a necessary line of future investigation.
- **Why unresolved:** The current findings are based exclusively on GPT-4.1 and the specific sub-genre of classic detective fiction (Christie/Sayers).
- **What evidence would resolve it:** Testing the linear classifier on creative fiction from other genres (e.g., sci-fi, romance) and text generated by competing LLMs.

### Open Question 3
- **Question:** Is the classifier robust against adversarial attacks where malicious actors intentionally suppress or obfuscate the specific weighted features identified by the interpretable model?
- **Basis in paper:** The paper hypothesizes that the "constellation of features" makes the model robust against manipulation, but acknowledges the "paradox of interpretability"—that explaining the decision process could theoretically aid bad actors.
- **Why unresolved:** While the authors argue that manipulating hundreds of features is "daunting," they did not empirically test if the classifier can be fooled by prompts specifically designed to avoid the detected features (e.g., explicitly prompting for British English to counter "Americanisms").
- **What evidence would resolve it:** An adversarial study where the LLM is prompted to mimic the statistical profile of human-written text (e.g., "use common words," "avoid formal grammar") to see if classification accuracy drops significantly.

## Limitations

- The study's core claim of robust LLM detection rests on a narrow experimental base using only GPT-4.1 with a specific rewriting prompt at fixed temperature
- The temporal and dialectal mismatch mechanism depends heavily on the 1920s-1930s British detective fiction corpus—a highly specific stylistic domain
- The interpretability analysis relies on manual annotation of 2,029 high-weight features, which may miss subtle or emerging stylistic patterns not present in this specific corpus

## Confidence

- **High Confidence:** The empirical finding that humans perform at chance level (49.9%) while linear classifiers achieve ~98% accuracy on this specific dataset
- **Medium Confidence:** The claim that LLM-generated text exhibits measurably higher lexical diversity detectable via unigram distribution analysis
- **Medium Confidence:** The temporal/dialectal mismatch mechanism, though evidence is compelling for this corpus, requires validation across different historical periods and genres
- **Low Confidence:** The claim that the detection method is "robust to evasion" given ablation study shows vulnerability to feature removal

## Next Checks

1. **Distribution Shift Test:** Generate new LLM samples using outline-based prompts (no rewriting) and evaluate whether accuracy holds without explicit synonym-introducing instructions

2. **Cross-Domain Generalization:** Apply the trained classifier to LLM-generated text from different historical periods, genres, and dialect regions to measure accuracy degradation

3. **Adversarial Robustness Benchmark:** Systematically replace high-weight features in LLM-generated text and measure the relationship between feature removal and accuracy degradation