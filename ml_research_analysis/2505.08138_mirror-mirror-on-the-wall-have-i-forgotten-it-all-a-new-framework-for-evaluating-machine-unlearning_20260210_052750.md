---
ver: rpa2
title: Mirror Mirror on the Wall, Have I Forgotten it All? A New Framework for Evaluating
  Machine Unlearning
arxiv_id: '2505.08138'
source_url: https://arxiv.org/abs/2505.08138
tags:
- unlearning
- computational
- learning
- adversary
- forget
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a formal definition of computational unlearning,
  which requires that an adversary cannot distinguish between a model produced by
  an unlearning method and a model trained from scratch without the data to be forgotten.
  The authors propose two distinguisher algorithms based on membership inference attack
  scores and Kullback-Leibler divergence, and demonstrate that several representative
  unlearning methods from the literature fail to achieve computational unlearning
  on ResNet-18 models trained on CIFAR-10.
---

# Mirror Mirror on the Wall, Have I Forgotten it All? A New Framework for Evaluating Machine Unlearning

## Quick Facts
- arXiv ID: 2505.08138
- Source URL: https://arxiv.org/abs/2505.08138
- Reference count: 40
- Key outcome: Proposed framework shows heuristic and certified unlearning methods fail computational unlearning on ResNet-18/CIFAR-10, with adversary success rates of 60-100%.

## Executive Summary
This paper introduces a formal definition of computational unlearning that requires an adversary to be unable to distinguish between a model produced by an unlearning method and a model trained from scratch without the data to be forgotten. The authors propose two distinguisher algorithms based on membership inference attack scores and Kullback-Leibler divergence, and demonstrate that several representative unlearning methods from the literature fail to achieve computational unlearning on ResNet-18 models trained on CIFAR-10. The paper also shows that deterministic unlearning methods cannot achieve computational unlearning, and that using differential privacy to achieve black-box computational unlearning leads to utility collapse.

## Method Summary
The framework defines computational unlearning through a security game where an adversary attempts to distinguish between an unlearned model and a control model (retrained from scratch without the forget set). The authors propose two distinguisher algorithms: MIAScore based on membership inference attack scores and KLDScore based on Kullback-Leibler divergence between model outputs. They evaluate several representative unlearning methods (SSD, Amnesiac, Bad Teacher, CDU) on ResNet-18 trained on CIFAR-10, showing these methods fail to achieve computational unlearning with adversary success rates ranging from 60% to 100%. The framework also proves theoretical impossibility results for deterministic unlearning methods and shows that DP-based approaches lead to extreme utility collapse.

## Key Results
- Heuristic unlearning methods (SSD, Amnesiac, Bad Teacher, CDU) fail computational unlearning with adversary success rates of 60-100%
- Deterministic unlearning algorithms cannot provably achieve computational unlearning for entropic learning schemes
- DP-based approaches can satisfy computational unlearning but at the cost of extreme utility collapse
- The framework provides a rigorous way to evaluate unlearning methods beyond heuristic assessments

## Why This Works (Mechanism)

### Mechanism 1: Distributional Distinguishability via Scoring Functions
Current heuristic and certified unlearning methods leave statistical artifacts that allow an adversary to differentiate an "unlearned" model from a "retrained" model with high probability. The adversary utilizes two scoring functions, MIAScore and KLDScore. Unlearned models tend to over-minimize membership scores or diverge too little from the original model compared to the control model which is retrained from scratch. By computing the divergence between the candidate and original model on the forget set, the adversary identifies the unlearned model. A properly retrained model produces a probability distribution and MIA profile that is significantly distinct from the original model, whereas approximate unlearning methods fail to bridge this gap perfectly.

### Mechanism 2: The Entropy Mismatch of Deterministic Unlearning
Deterministic unlearning algorithms cannot provably achieve computational unlearning for learning schemes that rely on stochasticity. In an entropic learning scheme, the distribution of possible retrained models has high entropy (many possible weight initializations and update orders). A deterministic unlearning algorithm maps a specific original model to a single fixed output. An adversary can simply run the deterministic unlearning algorithm on the original model; if the provided model matches the result, it is unlearned. If it differs, it is retrained. This holds unless the unlearning algorithm introduces sufficient independent randomness to mask the deterministic path.

### Mechanism 3: Differential Privacy Utility Collapse
Using Differential Privacy to guarantee black-box computational unlearning requires privacy parameters so strict that the resulting model loses all meaningful utility. To satisfy computational unlearning via DP, the privacy loss ε must be negligible relative to the security parameter λ. If the privacy loss is negligible, the model's output distribution becomes indistinguishable from a model trained on no data (random initialization). Thus, while secure, the model is useless. This holds unless the definition of utility is relaxed or the query bound is strictly limited to a constant.

## Foundational Learning

- **Stochastic Gradient Descent (SGD) & Random Initialization**: The paper's theoretical impossibility proofs rely on the "entropic" nature of standard deep learning. You must understand that training a neural network is a stochastic process (random init + shuffled data order) producing a distribution of models, not just one, which creates the "entropy" required for the proofs. Quick check: If you train a ResNet-18 on CIFAR-10 twice with the same data but different random seeds, will you get identical weight tensors?

- **Membership Inference Attacks (MIA)**: The paper co-opts MIA—a technique usually used to violate privacy—as a tool to audit unlearning. You need to understand that MIA scores quantify how "familiar" a model is with a specific data point. Quick check: If a model has "forgotten" an image, should the MIA score for that image be high (indicating confidence it was in training) or low (indicating it looks like unseen data)?

- **Differential Privacy (DP)**: The paper contrasts its definition with DP. You need to understand that DP bounds the "privacy loss" (how much a single data point changes the output distribution). Quick check: In the context of DP, does a lower ε value imply stronger privacy (more noise) or weaker privacy (less noise)?

## Architecture Onboarding

- **Component map:** Challenger (C) -> generates models -> provides to Adversary (A) -> applies distinguishers -> outputs guess
- **Critical path:** The "Security Game" (Definitions 7 & 8). 1. Train M_o. 2. Adversary selects D_f. 3. Challenger generates M_u (via unlearn) and M_c (via retrain). 4. Challenger shuffles and sends models. 5. Adversary guesses the ordering.
- **Design tradeoffs:** Heuristic vs. Computational: Heuristic methods (SSD, Amnesiac) are fast but empirically fail the security game. Computational unlearning (via DP) passes the security game but destroys utility. There is currently no "sweet spot" in the proposed framework. White-box vs. Black-box: White-box access (seeing weights) allows stronger distinguishers (re-running the deterministic algorithm). Black-box requires statistical attacks (KL divergence).
- **Failure signatures:** Over-minimization: If M_u MIA scores are lower than M_c MIA scores, the adversary detects "unnatural" forgetting. Determinism: If unlearn(M_o) always returns the exact same weights, it fails white-box computational unlearning.
- **First 3 experiments:** 1. Sanity Check (Random Guess): Verify that an adversary cannot distinguish between two independently retrained models (M_c1 vs M_c2). Success rate should be ~50%. 2. KLDScore Distinguisher: Implement KLDScore on ResNet-18/CIFAR-10. Compare the score distribution of M_u (e.g., from SSD) vs. M_c. Confirm distinguishability > 60%. 3. Deterministic Attack: Attempt to "unlearn" a deterministic method (like Certified Removal) locally on M_o and directly compare the resulting weights to the candidate model provided by the challenger. Check for exact match.

## Open Questions the Paper Calls Out

None

## Limitations

- The experimental evaluation is limited to a single architecture (ResNet-18) and dataset (CIFAR-10), constraining generalizability
- The proposed distinguishers rely on heuristic scoring functions whose performance may vary significantly across different model architectures and data distributions
- The theoretical impossibility results for deterministic methods assume access to the unlearning algorithm description, which may not always be realistic in practical scenarios

## Confidence

- **High Confidence:** The theoretical impossibility of deterministic unlearning for entropic algorithms (Theorem 18) - follows from straightforward information-theoretic arguments
- **Medium Confidence:** The experimental results showing heuristic methods fail computational unlearning - methodology is sound but single-architecture evaluation limits generalizability
- **Medium Confidence:** The claim that DP-based approaches lead to utility collapse - theoretical analysis is convincing but empirical validation is limited

## Next Checks

1. **Architecture Generalization Test:** Evaluate the proposed distinguishers on additional architectures (e.g., Vision Transformers, smaller CNNs) and datasets (e.g., CIFAR-100, SVHN) to assess robustness across model families and data distributions.

2. **Practical DP Evaluation:** Implement and empirically evaluate DP-based unlearning methods with varying privacy budgets on real-world tasks to quantify the actual utility-privacy tradeoff curve, moving beyond the theoretical analysis.

3. **Real-World Forgetting Scenario:** Test the framework in a practical unlearning scenario where the forget set consists of data from specific users or time periods, rather than random samples, to assess performance in realistic privacy compliance contexts.