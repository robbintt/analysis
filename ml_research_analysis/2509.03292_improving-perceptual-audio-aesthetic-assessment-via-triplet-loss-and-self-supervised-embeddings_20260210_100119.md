---
ver: rpa2
title: Improving Perceptual Audio Aesthetic Assessment via Triplet Loss and Self-Supervised
  Embeddings
arxiv_id: '2509.03292'
source_url: https://arxiv.org/abs/2509.03292
tags:
- audio
- speech
- loss
- perceptual
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of multi-axis perceptual quality
  prediction for generative audio (TTS, TTA, TTM) when training and evaluation domains
  differ. To bridge the domain gap between natural and synthetic audio, the authors
  propose AESA-Net, which uses BEATs as a self-supervised feature extractor and incorporates
  triplet loss with buffer-based sampling to structure the embedding space by perceptual
  similarity.
---

# Improving Perceptual Audio Aesthetic Assessment via Triplet Loss and Self-Supervised Embeddings

## Quick Facts
- arXiv ID: 2509.03292
- Source URL: https://arxiv.org/abs/2509.03292
- Reference count: 22
- Achieves strong generalization to unseen synthetic audio with SRCC up to 0.896 and KTAU up to 0.737 across domains

## Executive Summary
This paper addresses the challenge of multi-axis perceptual quality prediction for generative audio when training and evaluation domains differ. The authors propose AESA-Net, which uses BEATs as a self-supervised feature extractor and incorporates triplet loss with buffer-based sampling to structure the embedding space by perceptual similarity. The model predicts four Audiobox Aesthetic Scores: Production Quality, Production Complexity, Content Enjoyment, and Content Usefulness. Experiments show that the model achieves strong generalization to unseen synthetic data, with high Spearman and Kendall rank correlations across domains. The triplet loss mechanism notably improves ordinal alignment over the baseline, demonstrating robust perceptual prediction without synthetic training data.

## Method Summary
The authors propose AESA-Net, a model that predicts four perceptual quality dimensions for synthetic audio using self-supervised BEATs embeddings as input features. The model employs triplet loss with a buffer-based sampling strategy to learn an embedding space where perceptually similar audio clips are closer together. This approach allows the model to generalize to unseen synthetic audio domains without requiring training data from those specific domains. The system predicts Audiobox Aesthetic Scores across four axes: Production Quality, Production Complexity, Content Enjoyment, and Content Usefulness.

## Key Results
- Achieves SRCC up to 0.896 and KTAU up to 0.737 on Audiobox test sets
- Strong generalization to unseen synthetic data without requiring synthetic training data
- Triplet loss mechanism improves ordinal alignment over baseline models
- High correlation scores across four perceptual dimensions: Production Quality, Production Complexity, Content Enjoyment, and Content Usefulness

## Why This Works (Mechanism)
The success of AESA-Net stems from its ability to bridge the domain gap between natural and synthetic audio through structured embedding spaces. By using BEATs as a self-supervised feature extractor, the model captures rich audio representations that are transferable across domains. The triplet loss with buffer-based sampling ensures that perceptually similar audio clips are mapped close together in the embedding space, creating a robust representation that generalizes well to unseen synthetic audio. This approach allows the model to predict perceptual quality scores accurately even when trained only on natural audio data.

## Foundational Learning

**BEATs (BEhavior-Aware Tracking of sound sources)**
- Why needed: Provides self-supervised audio representations that capture temporal and spectral patterns
- Quick check: Can BEATs embeddings distinguish between different sound sources and acoustic environments?

**Triplet Loss**
- Why needed: Enforces relative similarity relationships in embedding space for perceptual grouping
- Quick check: Does the loss function effectively pull together similar samples while pushing apart dissimilar ones?

**Buffer-based Sampling**
- Why needed: Ensures diverse and representative triplet mining during training
- Quick check: Does the buffer maintain sufficient diversity across the training dataset?

## Architecture Onboarding

**Component Map**
Audio Clip -> BEATs Encoder -> Triplet Loss Layer -> Embedding Space -> Quality Score Predictors -> 4 Audiobox Aesthetic Scores

**Critical Path**
Audio input → BEATs feature extraction → Triplet loss embedding learning → Multi-task regression for four quality scores

**Design Tradeoffs**
- Using frozen BEATs features vs. fine-tuning: Frozen features provide stability but may miss task-specific nuances
- Buffer size in triplet sampling: Larger buffers provide better diversity but increase memory requirements
- Number of quality dimensions: Four axes capture key aspects but may miss other perceptual factors

**Failure Signatures**
- Poor generalization to new synthetic systems suggests domain-specific artifacts in BEATs embeddings
- Degraded performance on extreme quality outliers indicates buffer sampling limitations
- Inconsistent rankings across quality dimensions suggest misalignment in embedding space structure

**First 3 Experiments**
1. Test cross-system generalization on non-Meta TTS/TTM outputs
2. Ablation study comparing standard vs. buffer-based triplet loss
3. Evaluate fine-tuned BEATs embeddings vs. frozen features

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Domain shift to entirely new synthetic generation systems remains untested
- Triplet loss buffer strategy may introduce sampling bias affecting generalization to quality outliers
- Self-supervised BEATs embeddings may not capture all relevant perceptual dimensions for future audio synthesis paradigms
- Focus on four specific Audiobox aesthetics axes leaves questions about applicability to other quality dimensions

## Confidence

**Major Claims Confidence Assessment:**
- **High Confidence**: Strong performance on Audiobox test sets (SRCC up to 0.896, KTAU up to 0.737) with clear experimental support
- **Medium Confidence**: Claims of generalization to unseen synthetic data supported but limited to Audiobox variants
- **Medium Confidence**: Effectiveness of BEATs embeddings demonstrated, but alternative representations not explored

## Next Checks
1. Test AESA-Net on TTS/TTM outputs from non-Meta generation systems (e.g., ElevenLabs, Google's models) to verify cross-platform generalization beyond the Audiobox family.

2. Conduct ablation studies removing the triplet loss buffer mechanism to quantify its specific contribution versus standard triplet loss, particularly examining performance on quality outliers and edge cases.

3. Evaluate whether fine-tuning BEATs embeddings on a small amount of paired synthetic-natural audio improves performance over using frozen BEATs features, determining if task-specific adaptation provides benefits.