---
ver: rpa2
title: 'Personalize Before Retrieve: LLM-based Personalized Query Expansion for User-Centric
  Retrieval'
arxiv_id: '2510.08935'
source_url: https://arxiv.org/abs/2510.08935
tags:
- user
- query
- retrieval
- arxiv
- expansion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a framework for improving retrieval-augmented
  generation (RAG) systems by personalizing query expansion before retrieval. Existing
  methods expand queries uniformly, ignoring user-specific semantics such as expression
  style and corpus structure, leading to retrieval mismatches.
---

# Personalize Before Retrieve: LLM-based Personalized Query Expansion for User-Centric Retrieval

## Quick Facts
- arXiv ID: 2510.08935
- Source URL: https://arxiv.org/abs/2510.08935
- Reference count: 39
- Introduces framework for personalizing query expansion in RAG systems using user history

## Executive Summary
This paper addresses a fundamental limitation in retrieval-augmented generation systems where uniform query expansion fails to account for individual user semantics, expression styles, and corpus structures. The authors propose a two-stage personalized query expansion framework that first generates stylistically aligned pseudo-feedback based on user history, then constructs semantic graphs over user corpora to identify anchor points. By fusing these personalized representations with dynamic weighting, the system achieves up to 10% improvements in recall and NDCG metrics compared to standard retrieval methods.

## Method Summary
The proposed framework consists of two main components working in tandem. P-PRF generates pseudo-feedback utterances and reasoning paths conditioned on user history to capture stylistic preferences, while P-Anchor builds a semantic graph over the user's corpus to identify representative anchor points. These components are fused using dynamic weighting based on query characteristics to produce personalized query representations. The system operates before retrieval, ensuring that the expanded queries better match user-specific semantics and corpus structures.

## Key Results
- Up to 10% improvements in recall metrics on PersonaBench and LongMemEval datasets
- Up to 10% improvements in NDCG scores compared to baseline retrieval methods
- Ablation studies confirm both stylistic (P-PRF) and structural (P-Anchor) personalization are essential for performance gains

## Why This Works (Mechanism)
The framework succeeds by addressing the semantic mismatch between generic query expansion and user-specific retrieval needs. Standard RAG systems expand queries uniformly without considering individual expression patterns or corpus organization, leading to irrelevant retrievals. By conditioning pseudo-feedback generation on user history, the system captures personalized language patterns and reasoning styles. The semantic graph construction identifies structurally important documents within the user's corpus, ensuring anchor points reflect the user's actual information landscape. Dynamic weighting allows the system to adaptively balance stylistic and structural cues based on query context.

## Foundational Learning

**Pseudo-feedback generation**: Creating synthetic relevant documents or utterances to expand queries. Needed because it enriches the query with contextually relevant terms that reflect user preferences. Quick check: Verify generated pseudo-feedback maintains topical coherence with user history.

**Semantic graph construction**: Building graph representations where nodes represent documents and edges capture semantic relationships. Needed to identify structural patterns and representative anchor points in user corpora. Quick check: Ensure graph connectivity reflects actual semantic relationships between documents.

**Dynamic weighting mechanisms**: Adaptive combination of multiple representation sources based on query characteristics. Needed to balance stylistic and structural personalization signals effectively. Quick check: Validate weight assignments improve retrieval performance across diverse query types.

## Architecture Onboarding

**Component map**: User History -> P-PRF -> Pseudo-feedback utterances/reasoning paths; User Corpus -> P-Anchor -> Semantic graph + anchor points; P-PRF output + P-Anchor output -> Dynamic weighting -> Personalized query representation -> Retrieval

**Critical path**: User history → P-PRF → pseudo-feedback → fusion → retrieval. The system critically depends on accurately capturing user stylistic preferences through P-PRF, as this drives the personalized expansion that distinguishes the approach from generic methods.

**Design tradeoffs**: The framework trades computational overhead for improved retrieval relevance. Generating pseudo-feedback and constructing semantic graphs adds latency, but the dynamic weighting mechanism helps optimize resource allocation by adjusting the contribution of each component based on query characteristics.

**Failure signatures**: Poor retrieval performance when user history is sparse or unrepresentative, semantic graph construction fails to capture meaningful document relationships, or dynamic weighting incorrectly prioritizes one personalization dimension over another for a given query type.

**Three first experiments**:
1. Baseline retrieval without personalization vs. with only P-PRF component
2. Baseline retrieval vs. with only P-Anchor component  
3. Full system with both components vs. individual components to validate dynamic weighting effectiveness

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Evaluation limited to only two benchmark datasets (PersonaBench and LongMemEval), potentially restricting generalizability
- Dynamic weighting mechanism lacks detailed analysis of criteria for weight assignment based on query characteristics
- Computational overhead of pseudo-feedback generation and semantic graph construction not quantified
- No assessment of potential biases introduced by conditioning on user history that could create echo chambers

## Confidence

**High Confidence**: Core methodology of separating stylistic and structural personalization into P-PRF and P-Anchor components is technically sound and well-defined. Performance improvements (up to 10% in recall and NDCG) are supported by experimental results.

**Medium Confidence**: Ablation study results are compelling but based on limited dataset coverage. The claim about addressing retrieval mismatches is supported but could benefit from more diverse failure case analysis.

**Low Confidence**: Practical scalability and computational efficiency claims lack quantitative backing. Dynamic weighting mechanism's effectiveness depends on underspecified criteria.

## Next Checks

1. **Computational Cost Analysis**: Measure and report inference time and resource requirements for both P-PRF and P-Anchor components across different user history lengths and corpus sizes to establish practical deployment feasibility.

2. **Dataset Generalization Study**: Evaluate the framework on at least two additional RAG benchmark datasets from different domains (e.g., technical documentation, conversational AI) to test robustness beyond current evaluation scope.

3. **Bias and Diversity Assessment**: Conduct experiments measuring retrieval diversity and potential personalization bias by comparing distributions of retrieved items with and without personalization components, ensuring the system doesn't overly narrow the retrieval space.