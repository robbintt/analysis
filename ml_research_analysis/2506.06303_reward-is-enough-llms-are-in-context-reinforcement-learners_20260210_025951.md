---
ver: rpa2
title: 'Reward Is Enough: LLMs Are In-Context Reinforcement Learners'
arxiv_id: '2506.06303'
source_url: https://arxiv.org/abs/2506.06303
tags:
- icrl
- reward
- learning
- task
- only
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that large language models (LLMs) exhibit
  reinforcement learning (RL)-like behavior during inference time through a simple
  multi-round prompting framework called ICRL prompting. The method guides LLMs to
  improve responses iteratively by providing scalar reward feedback for each attempt,
  without using textual gradients, sampling heuristics, or additional engineered modules.
---

# Reward Is Enough: LLMs Are In-Context Reinforcement Learners

## Quick Facts
- **arXiv ID:** 2506.06303
- **Source URL:** https://arxiv.org/abs/2506.06303
- **Authors:** Kefan Song; Amir Moeini; Peng Wang; Lei Gong; Rohan Chandra; Shangtong Zhang; Yanjun Qi
- **Reference count:** 40
- **Primary result:** Demonstrates LLMs exhibit RL-like behavior during inference through multi-round prompting with scalar reward feedback

## Executive Summary
This paper presents a novel approach called ICRL (In-Context Reinforcement Learning) prompting that enables large language models to improve their responses through iterative refinement using scalar reward feedback. The method operates during inference time without any model fine-tuning, treating the LLM's context window as a dynamic experience replay buffer. By providing rewards for each attempt and guiding the model through exploration and exploitation phases, ICRL consistently outperforms strong baselines across diverse benchmarks including mathematical reasoning, creative writing, and text-based game environments.

## Method Summary
The ICRL prompting framework implements a multi-round interaction where the LLM generates responses, receives scalar rewards, and uses this feedback to improve subsequent attempts. The algorithm maintains an experience buffer storing previous responses and their associated rewards, which is concatenated into the prompt for each new round. Two modes are explored: ICRL Preset (alternating exploration and exploitation instructions) and ICRL Autonomous (LLM decides when to explore vs exploit). The method works with both external reward functions (like SymPy for math verification) and self-generated rewards from the LLM acting as its own judge, demonstrating genuine learning from external feedback rather than mere parametric search.

## Key Results
- Achieves 90% success rate on Game of 24 compared to 49% for Best-of-N
- Demonstrates 59.48% win rate in creative writing vs 44.6% for Reflexion
- Scores 88% return on ScienceWorld benchmark compared to 83% for Self-Refine
- Shows consistent improvement even when rewards are generated by the same LLM
- Works across various open-source models including Qwen3-32B and Phi-4

## Why This Works (Mechanism)

### Mechanism 1: In-Context Policy Optimization via Experience Buffering
The LLM optimizes task-specific policies during inference by conditioning on state-action-reward tuples, using the context window as a dynamic experience replay buffer. The pre-trained model possesses emergent capability to recognize and maximize reward patterns from in-context data without weight updates.

### Mechanism 2: Reward-Driven Exploration vs. Parametric Search
ICRL enables genuine learning from external feedback rather than retrieving memorized solutions. The scalar reward signal provides orthogonal information to the model's prior beliefs, allowing Bayesian-like updating in context and uncovering solutions not present in training data.

### Mechanism 3: Balancing Exploration and Exploitation via Meta-Instructions
The exploration vs. exploitation instructions function as a high-level policy switch, allowing the LLM to oscillate between generating novel candidates and refining high-reward ones. The model semantically interprets constraints while simultaneously optimizing for numerical rewards.

## Foundational Learning

- **Concept: Markov Decision Process (MDP)**
  - **Why needed here:** The paper frames LLM generation as an MDP (State: tokens, Action: next token, Reward: scalar score). Understanding this is required to map the algorithm to standard RL terminology.
  - **Quick check question:** How does the paper define the "State" $S_t$ and "Action" $A_t$ in the context of text generation? (Answer: State is all generated tokens; Action is the next token).

- **Concept: Sparse vs. Dense Rewards**
  - **Why needed here:** Different benchmarks use different reward structures. Understanding this distinction is critical for debugging why a model might struggle to learn in specific setups.
  - **Quick check question:** In the Game of 24 experiment, is the reward dense or sparse, and how does the paper handle it? (Answer: They simulate dense rewards by scoring intermediate steps).

- **Concept: The "Reward is Enough" Hypothesis**
  - **Why needed here:** This theoretical stance underpins the entire methodologyâ€”that maximizing a scalar signal alone is sufficient to drive complex behavior, removing the need for explicit human demonstrations or textual feedback.
  - **Quick check question:** Why does the paper explicitly exclude "textual gradients" or "verbal feedback"? (Answer: To isolate the LLM's ability to learn from the scalar reward alone).

## Architecture Onboarding

- **Component map:** Policy LLM ($\pi_\theta$) -> Reward Function ($r$) -> Experience Buffer ($B$) -> Prompt Constructor
- **Critical path:** Initialize Buffer $B$ -> Construct $S_0$ (Task + Buffer + Instruction) -> LLM generates response $A$ -> Reward function computes scalar $R$ -> Push $(A, R)$ to $B$ -> Repeat for $K$ episodes
- **Design tradeoffs:**
  - External vs. Self-Generated Rewards: External rewards are more reliable but harder to scale; self-generated rewards scale easily but risk hallucination
  - Preset vs. Autonomous: Preset offers controlled oscillation; Autonomous relies on model's internal decision-making
  - Buffer Size: Longer context allows more learning episodes but increases inference cost
- **Failure signatures:**
  - Context Saturation: Performance drops if context window fills with low-reward noise
  - Reward Hacking: Model learns to trick the judge rather than solve the task
  - Collapse in Self-Refine: Performance degrades due to hallucinated feedback accumulation
- **First 3 experiments:**
  1. Implement Game of 24 task with CoT prompting and SymPy verification
  2. Run ablation on buffer size (3-episode vs full context) on creative writing task
  3. Compare LLM-as-judge rewards against ground-truth rewards on subset of data

## Open Questions the Paper Calls Out

1. **Training-time interventions:** How can training-time interventions enhance the in-context RL capability of LLMs? The paper demonstrates ICRL emerges in pretrained models but doesn't explore whether explicit training objectives could strengthen this capability.

2. **Performance ceiling comparison:** Is the performance ceiling lower when using self-evaluation rewards versus external feedback? The paper hypothesizes this but doesn't systematically test the claim through controlled comparison.

3. **Underlying mechanism:** Does ICRL implement genuine RL algorithms or rely on pattern matching from pretraining data? The paper uses behavioral evidence but hasn't proven whether the model implements specific RL computations.

4. **Scalability limits:** How does ICRL performance scale with task horizon and action space complexity? Tested tasks may not reveal whether ICRL degrades with substantially longer episodes or larger action spaces.

## Limitations
- Heavy dependence on LLM-as-judge for most experiments raises concerns about reward signal reliability
- Context window saturation suggests fundamental scalability limits for longer-horizon tasks
- The claim that LLMs are genuinely "learning" versus performing sophisticated search remains debatable

## Confidence
- **High Confidence:** Empirical demonstration that ICRL improves performance over strong baselines across multiple benchmarks
- **Medium Confidence:** Claim that LLMs perform genuine reinforcement learning during inference (behavioral evidence supports but underlying mechanism unclear)
- **Low Confidence:** Scalability claims for longer-horizon tasks (only tested up to 4-step reasoning)

## Next Checks
1. Replicate Game of 24 experiment with SymPy as sole reward function across all 100 problems to confirm learning isn't just pattern matching
2. Implement extended context version of Creative Writing (50+ episodes) to test scalability limits suggested by "Short Context" ablation
3. Design controlled experiment with manipulated reward signals (random rewards, syntactic-only rewards) to determine whether model genuinely optimizes for task completion or exploits reward patterns