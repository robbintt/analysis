---
ver: rpa2
title: 'BRIDGE: Building Representations In Domain Guided Program Verification'
arxiv_id: '2511.21104'
source_url: https://arxiv.org/abs/2511.21104
tags:
- reasoning
- code
- verification
- functional
- lean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents BRIDGE, a structured prompting framework for
  verified program synthesis that decomposes verification into three interconnected
  domains: Code (executable implementations), Specifications (formal intent statements),
  and Proofs (correctness arguments). The key innovation is using domain-specific
  reasoning strategies as intermediate representations to improve semantic consistency
  across these domains.'
---

# BRIDGE: Building Representations In Domain Guided Program Verification

## Quick Facts
- arXiv ID: 2511.21104
- Source URL: https://arxiv.org/abs/2511.21104
- Reference count: 40
- Primary result: BRIDGE achieves 1.5× improvement in Lean4 code correctness and 2× reduction in inference compute through structured domain-specific reasoning

## Executive Summary
BRIDGE introduces a structured prompting framework for verified program synthesis that decomposes verification into three interconnected domains: Code, Specifications, and Proofs. The key innovation is using domain-specific reasoning strategies as intermediate representations to improve semantic consistency across these domains. By organizing verification tasks around these three domains with specialized reasoning strategies, BRIDGE demonstrates substantial improvements in formal verification accuracy while reducing computational costs. The framework shows that structured reasoning strategies matter more than language familiarity for verification success, with functional paradigms consistently outperforming imperative ones.

## Method Summary
BRIDGE employs a three-domain decomposition approach where verification tasks are broken down into Code (executable implementations), Specifications (formal intent statements), and Proofs (correctness arguments). Each domain uses specialized reasoning strategies - functional for proofs, logical for specifications, and implementation-focused for code. The framework uses structured prompts that guide models through domain-specific reasoning before generating outputs, creating semantic consistency across domains. BRIDGE leverages this structured approach to achieve better verification outcomes while reducing computational overhead compared to direct generation approaches.

## Key Results
- Functional reasoning improves Lean4 code correctness by 1.5× (pass@5) and reduces inference-time compute by 2×
- Specification-driven prompting boosts Python coding pass rates by up to 17.5%
- Structured reasoning strategies matter more than language familiarity for verification success
- Functional paradigms consistently outperform imperative approaches in verification tasks

## Why This Works (Mechanism)
BRIDGE works by decomposing complex verification tasks into manageable domains with specialized reasoning strategies. This decomposition reduces cognitive load on language models and allows them to focus on domain-specific patterns rather than attempting holistic verification. The framework's intermediate representations (specifications and proofs) act as semantic bridges between code and verification, ensuring consistency and reducing errors. By leveraging functional programming's mathematical foundations and structured proof techniques, BRIDGE aligns with formal verification's inherent logical structure, making the verification process more tractable for AI systems.

## Foundational Learning
1. **Domain-specific reasoning** - Why needed: Different verification aspects require different logical approaches; Quick check: Can the model distinguish when to apply functional vs. logical reasoning?
2. **Intermediate representations** - Why needed: Direct code-to-proof generation is error-prone; Quick check: Does the specification accurately capture code intent before proof generation?
3. **Semantic consistency** - Why needed: Ensures specifications and proofs align with code; Quick check: Are cross-domain errors reduced compared to direct generation?
4. **Functional programming paradigms** - Why needed: Provides mathematical foundations for verification; Quick check: Does functional reasoning improve proof generation quality?
5. **Structured prompting** - Why needed: Guides models through complex verification workflows; Quick check: Are pass@5 rates improved with structured vs. unstructured prompts?
6. **Proof technique specialization** - Why needed: Different proof strategies suit different verification challenges; Quick check: Does technique selection impact verification success rates?

## Architecture Onboarding

**Component Map:** Code -> Specifications -> Proofs -> Verification

**Critical Path:** Domain decomposition → Specialized reasoning strategy application → Intermediate representation generation → Cross-domain consistency checking → Final verification output

**Design Tradeoffs:** Structured reasoning vs. flexibility, computational overhead vs. accuracy gains, domain specialization vs. generalization, intermediate representations vs. direct generation

**Failure Signatures:** Inconsistent specifications, failed proof generation, semantic drift between domains, computational inefficiency, poor technique selection for verification type

**First Experiments:**
1. Compare pass@5 rates for structured vs. unstructured prompting across 10 verification tasks
2. Measure inference-time compute reduction when using intermediate representations
3. Evaluate cross-domain consistency by checking specification-proof alignment on 50 code samples

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses on pass@5 metrics which may not capture practical utility in real-world development
- Results based on limited set of 16 proof techniques, potentially limiting generalizability
- Claims about structured strategies vs. language familiarity require validation across diverse verification domains
- Training approaches via expert iteration or RLVR are speculative without empirical validation

## Confidence

High confidence:
- BRIDGE's improvements in Lean4 code correctness (1.5× pass@5) and reduced inference compute (2×)
- Specification-driven prompting improving Python coding pass rates by up to 17.5%

Medium confidence:
- Structured reasoning strategies being more important than language familiarity for verification success

Low confidence:
- BRIDGE establishing foundation for training models via expert iteration or RLVR to internalize reasoning strategies

## Next Checks

1. Test BRIDGE's performance on verification tasks involving concurrent systems, probabilistic programs, and other domains not covered in current evaluation

2. Conduct user studies with professional developers to evaluate practical utility of BRIDGE-generated specifications and proofs in real-world software development workflows

3. Implement and evaluate proposed expert iteration and RLVR training approaches to verify models can internalize structured reasoning strategies demonstrated in BRIDGE