---
ver: rpa2
title: Vectors from Larger Language Models Predict Human Reading Time and fMRI Data
  More Poorly when Dimensionality Expansion is Controlled
arxiv_id: '2505.12196'
source_url: https://arxiv.org/abs/2505.12196
tags:
- data
- fmri
- larger
- human
- elements
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the relationship between language model
  size and predictive power for human sentence processing data. Previous research
  suggested larger models better predict reading times and brain imaging data, but
  this may be confounded by increased dimensionality in model vectors.
---

# Vectors from Larger Language Models Predict Human Reading Time and fMRI Data More Poorly when Dimensionality Expansion is Controlled

## Quick Facts
- arXiv ID: 2505.12196
- Source URL: https://arxiv.org/abs/2505.12196
- Authors: Yi-Chien Lin; Hongao Zhu; William Schuler
- Reference count: 9
- Primary result: Larger LLMs show inverse scaling in predicting human sentence processing data after controlling for dimensionality expansion

## Executive Summary
This study challenges the prevailing assumption that larger language models better predict human sentence processing data. Through systematic experiments across multiple reading time and fMRI datasets, the authors demonstrate that when controlling for dimensionality expansion using residualization against untrained models, larger models actually perform worse at predicting human responses. The research reveals that apparent positive scaling in previous studies was largely driven by increased degrees of freedom from higher-dimensional vectors rather than improved linguistic representations. These findings suggest that larger models may be increasingly misaligned with human cognitive processes for sentence comprehension.

## Method Summary
The study evaluated five human datasets (Natural Stories SPR, Dundee ET, Provo ET, Natural Stories fMRI, Pereira fMRI) using vector representations from four LLM families (GPT-2, GPT-Neo, OPT, Pythia) across various sizes. Token vectors were extracted from the final layer, with subword averaging for word-level predictions and HRF convolution for fMRI data. Data was partitioned into fit (50%), exploratory (25%), and held-out (25%) sets. The key innovation was residualization: trained model predictions were regressed against residuals from untrained model predictions to isolate training-derived signal from dimensionality effects. Linear regression was used to predict human responses from vector elements, with Pearson correlation as the evaluation metric.

## Key Results
- Larger trained models showed positive scaling in predicting human data when using raw vectors (replicating prior findings)
- Untrained model vectors alone showed significant positive scaling with parameter count, demonstrating the dimensionality confound
- After residualization against untrained models, inverse scaling emerged: larger models performed significantly worse at predicting human data
- The inverse scaling effect was statistically significant across most datasets (Natural Stories SPR p<0.005, Dundee ET p<0.001, Provo ET p<0.005, Natural Stories fMRI p<0.05)

## Why This Works (Mechanism)

### Mechanism 1: Dimensionality Expansion Confound in Vector-Based Regression
Larger LLMs appear to better predict human data not due to improved representations, but because higher-dimensional vectors provide more regression degrees of freedom. The final layer embedding size increases with model scale, giving linear regression more opportunities to find strong predictors even with random inputs. Residualization against untrained counterparts isolates training-derived signal from pure dimensionality effects.

### Mechanism 2: Inverse Scaling After Dimensionality Control
When dimensionality is controlled via residualization, larger trained models contribute less predictive power beyond their untrained baselines. Regressing fully-trained Pythia vectors onto residuals from untrained Pythia predictions shows significant negative correlation with parameter count across most datasets, suggesting larger models learn representations increasingly misaligned with human processing.

### Mechanism 3: Degrees of Freedom from Untrained Models Alone
Untrained LLM vectors, despite lacking learned representations, still show positive scaling with model size in predicting human data due purely to dimensionality. Even random high-dimensional projections can approximate kernel methods, enabling fit to structured signals. This demonstrates that dimensionality alone can drive apparent predictive power without any linguistic learning.

## Foundational Learning
- **Linear Regression Degrees of Freedom**: Understanding how predictor count inflates R² even without true signal is essential to interpret the dimensionality confound. Quick check: If you double the number of random predictors in a regression, what happens to held-out R² on average?
- **Residualization**: The core experimental control subtracts untrained model predictions from human data before fitting trained models. Quick check: What does residualizing Y against X isolate in the resulting target variable?
- **Inverse Scaling**: The central result contradicts the "quality-power" hypothesis. Quick check: In inverse scaling, does model performance on the target metric improve or degrade as model size increases?

## Architecture Onboarding
- **Component map**: Human response datasets (SPR, ET, fMRI) → LLM final layer vectors (trained/untrained) → Linear regression → Pearson correlation predictions vs. human responses
- **Critical path**: Preprocess data → extract vectors → fit regression on fit partition → predict held-out → compute correlation → (optional) residualize trained against untrained → re-fit and re-evaluate
- **Design tradeoffs**: Vectors capture richer representations but introduce dimensionality confounds; residualization controls for degrees of freedom but assumes linear relationship between trained and untrained contributions
- **Failure signatures**: Flat or negative slopes in predictive power vs. parameter count after residualization; high variance in small datasets; data leakage if LLM training includes corpus stimuli
- **First 3 experiments**: 1) Replicate positive scaling with trained model vectors on new reading time corpus; 2) Test untrained vs. trained vector predictive power for single Pythia variant; 3) Apply PCA dimensionality reduction to trained vectors and re-run regressions

## Open Questions the Paper Calls Out
The paper explicitly states in Limitations that findings may not generalize to languages beyond English, as all evaluated LLMs and datasets were English-only. This raises the open question of whether inverse scaling persists across typologically diverse languages with different syntactic properties.

## Limitations
- Results are limited to English-language LLMs and datasets, raising questions about cross-linguistic generalizability
- The residualization method assumes untrained models provide a clean baseline for dimensionality effects, which may not fully capture all structured noise
- Inverse scaling patterns vary across modalities (SPR, ET, fMRI), suggesting modality-dependent relationships between model size and human alignment

## Confidence
- **High Confidence**: The dimensionality expansion confound exists and can inflate predictive power of larger models in vector-based regression
- **Medium Confidence**: After controlling for dimensionality, larger trained models show inverse scaling in predictive power
- **Medium Confidence**: The findings suggest larger LLMs may be more misaligned with human sentence processing

## Next Checks
1. Apply PCA to project all trained model vectors to a common dimensionality (e.g., 768) and repeat regressions to compare scaling patterns with residualization results
2. Systematically analyze untrained model vectors to identify sources of predictive power and test whether removing specific components eliminates positive scaling
3. Apply the full residualization analysis to a held-out reading time corpus not used in the original study to test generalizability beyond Natural Stories dataset