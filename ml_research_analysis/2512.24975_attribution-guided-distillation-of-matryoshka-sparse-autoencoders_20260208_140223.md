---
ver: rpa2
title: Attribution-Guided Distillation of Matryoshka Sparse Autoencoders
arxiv_id: '2512.24975'
source_url: https://arxiv.org/abs/2512.24975
tags:
- core
- latents
- sparse
- matryoshka
- non-core
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Distilled Matryoshka Sparse Autoencoders (DMSAEs) address the\
  \ problem of redundant and unstable features in sparse autoencoders by introducing\
  \ an iterative distillation pipeline that identifies a compact core of consistently\
  \ useful latent features. The method trains Matryoshka SAEs with a shared core,\
  \ uses gradient\xD7activation attribution to measure each feature's contribution\
  \ to next-token loss, and iteratively selects the smallest subset explaining a fixed\
  \ fraction of the attribution."
---

# Attribution-Guided Distillation of Matryoshka Sparse Autoencoders

## Quick Facts
- arXiv ID: 2512.24975
- Source URL: https://arxiv.org/abs/2512.24975
- Authors: Cristina P. Martin-Linares; Jonathan P. Ling
- Reference count: 34
- Primary result: Iterative distillation identifies a compact core of 197 features that improve SAEBench metrics when used to train new SAEs

## Executive Summary
This paper introduces Distilled Matryoshka Sparse Autoencoders (DMSAEs), an iterative pipeline that distills a compact core of consistently useful latent features from Matryoshka SAEs. The method uses gradient×activation attribution to measure each feature's contribution to next-token loss, then iteratively selects the smallest subset explaining a fixed fraction of the attribution. By freezing only core encoder weight vectors across cycles while reinitializing decoder and non-core latents, DMSAEs identify feature detection directions that transfer across sparsity levels. On Gemma-2-2B layer 12 activations, seven cycles of distillation yielded a distilled core of 197 features that improved several SAEBench metrics compared to Matryoshka SAE baselines.

## Method Summary
DMSAEs address redundant and unstable features in sparse autoencoders through an iterative distillation pipeline. Starting from a SAEBench Matryoshka SAE checkpoint, the method computes gradient×activation attribution scores for each latent's contribution to next-token loss, then selects the smallest core achieving 90% coverage using 99th percentile quantile aggregation. Only core encoder weight vectors are transferred across cycles, while decoder and non-core latents are reinitialized. The core remains dense during distillation to avoid sparsity competition confounding attribution scores. After seven cycles of training (500M tokens each, 65k width), the final distilled core is the intersection of cores from the last two cycles. This core is then used to train new DMSAEs at various sparsity levels, improving SAEBench metrics compared to Matryoshka SAE baselines.

## Key Results
- Seven cycles of distillation (500M tokens, 65k width) yielded a distilled core of 197 features
- Distilled core maintained L0,core during training while random core drove L0,core to zero
- Using distilled core to train new SAEs improved several SAEBench metrics compared to Matryoshka SAE baselines
- Core stability varied with sparsity: intermediate values (k=160-320) showed good carryover, while extremes (k=20-80, 640) showed little or failed

## Why This Works (Mechanism)

### Mechanism 1: Attribution-guided core selection
Gradient×activation attribution identifies features that consistently contribute to model behavior across training restarts. For each latent j, compute $GxA_{u,j} = |a_{u,j} \cdot s_{u,j}|$ where $a_{u,j}$ is the activation and $s_{u,j}$ is the projection of the next-token loss gradient onto the decoder direction. Aggregate via high quantile (q=0.99) to handle heavy-tailed distributions, then select the smallest subset reaching coverage τ=0.9. Features with high attribution to next-token loss are behaviorally important and will remain useful across retraining.

### Mechanism 2: Selective parameter transfer
Freezing only encoder weight vectors stabilizes feature detection directions while allowing activation thresholds and reconstruction contributions to adapt. Transfer $\{w_{enc,j}\}_{j \in C^{(t)}}$ and freeze rows 0:c in next cycle. Reinitialize all encoder biases, decoder weights/biases, and non-core parameters. The encoder direction primarily determines what a feature detects; bias and decoder control how strongly it fires and contributes.

### Mechanism 3: Dense core during distillation
Dense core during distillation prevents sparsity competition from masking useful features. Apply BatchTopK only to non-core latents (Eq. 3), keeping core activations unrestricted during core discovery. This ensures attribution scores reflect true utility rather than sparsity-induced suppression. Features worth preserving should activate when relevant without competing for a budget.

## Foundational Learning

- **Concept: Matryoshka Representation Learning** - Why needed: DMSAEs build on MSAEs' nested prefix structure; understanding that early latents must reconstruct independently is prerequisite. Quick check: Can you explain why Matryoshka training reduces feature absorption compared to standard SAEs?

- **Concept: Gradient×Activation Attribution** - Why needed: Core selection relies on computing $\nabla_x L_{NT} \cdot \hat{w}_{dec,j}$ per latent; understanding gradient flow through SAE reconstruction is essential. Quick check: Given SAE reconstruction $\hat{x} = W_{dec} f(x) + b_{dec}$, how would you compute the gradient of next-token loss w.r.t. latent j?

- **Concept: BatchTopK Sparsity** - Why needed: DMSAEs use BatchTopK on non-core latents; understanding how top-k differs from L1 regularization informs why dense core is viable. Quick check: Why does BatchTopK produce different sparsity patterns than L1 penalty, and how does this affect feature competition?

## Architecture Onboarding

- **Component map:** Cycle 0 (SAEBench checkpoint) → attribution scoring → $C^{(0)}$ selection → cycle 1 training with frozen core encoder → re-score → iterate T=7 cycles → extract $C^* = C^{(7)} \cap C^{(6)}$

- **Critical path:** Cycle 0 (SAEBench checkpoint) → attribution scoring → $C^{(0)}$ selection → cycle 1 training with frozen core encoder → re-score → iterate T=7 cycles → extract $C^* = C^{(7)} \cap C^{(6)}$

- **Design tradeoffs:** τ=0.9 coverage vs. core size: higher τ includes more features but reduces compactness (Fig. 5); dense vs. sparse core: dense enables cleaner attribution signal; sparse enables direct sparsity-matched comparison (Appendix E); cycle count: more cycles increase compute but may improve core stability

- **Failure signatures:** L0,core → 0 during training: core not useful (compare random vs. distilled in Fig. 3); no cross-cycle carryover: sparsity k too low or too high (Fig. 6); reconstruction collapse: k too high (k=640 failed in experiments)

- **First 3 experiments:** 1) Reproduce Fig. 3: Train two DMSAEs (distilled core vs. random core) and plot L0,core over training to validate that distillation identifies genuinely useful directions; 2) Sweep attribution threshold τ ∈ {0.5, 0.7, 0.9, 1.0} and measure reconstruction loss vs. core size to find operating point; 3) Run 4 distillation cycles across k ∈ {160, 320} and compute cross-cycle feature overlap to verify core stability before committing to full T=7 run

## Open Questions the Paper Calls Out

- **Open Question 1:** Can latent ablation outperform gradient × activation as an attribution signal for core selection? The authors note that gradient × activation is a fast first-order approximation but may be suboptimal, while latent ablation offers a more direct measure of importance at higher computational cost.

- **Open Question 2:** Can the computational cost of identifying a stable core be reduced without iterating through multiple training cycles? The current pipeline requires seven full training cycles (500M tokens each) to distill the core, which is resource-intensive.

- **Open Question 3:** Why does the dense-core DMSAE degrade AutoInterp performance at low sparsities? Figure 4 shows a sharp drop in AutoInterp scores for dense-core DMSAEs at lower $k$ values, even while other metrics like Absorption improve.

## Limitations

- The auxiliary loss coefficient α, learning rate, batch size, and exact training schedule per cycle are not provided, making it difficult to replicate the training dynamics
- The exact implementation of BatchTopK sparsity and the calibration procedure for computing gradient×activation attributions are unclear
- The dense core approach during distillation may allow some low-quality features to persist without sparsity pressure, potentially reducing the quality of the distilled core

## Confidence

**High confidence:** The core claim that gradient×activation attribution can identify useful features is well-supported by the ablation showing random cores fail (Fig. 3) and the transfer to improved SAEBench metrics. The distillation pipeline structure and iterative selection mechanism are clearly specified.

**Medium confidence:** The claim that freezing only encoder weights preserves feature semantics is supported by the ablation but has limitations noted in the paper. The dense core approach for attribution scoring is justified theoretically but lacks direct empirical validation in the paper.

**Low confidence:** The long-term stability of distilled cores across extreme sparsity levels (k=20-80 vs k=640) is not well-established, with the paper only testing intermediate values. The exact attribution computation method for next-token loss gradients is underspecified.

## Next Checks

1. **Ablation on dense vs sparse core during distillation:** Run DMSAE with both dense and sparse core settings (keeping BatchTopK on all latents) to quantify the attribution signal quality difference and validate the dense-core assumption.

2. **Cross-cycle stability sweep:** Test the core selection process across k ∈ {80, 160, 320, 640} for multiple training seeds to measure feature overlap statistics and identify the optimal sparsity range for stable distillation.

3. **Attribution threshold sensitivity:** Systematically vary τ ∈ {0.5, 0.7, 0.9, 1.0} and measure both reconstruction loss and core size to find the Pareto-optimal operating point for downstream SAE training.