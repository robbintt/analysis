---
ver: rpa2
title: 'Reasoning and the Trusting Behavior of DeepSeek and GPT: An Experiment Revealing
  Hidden Fault Lines in Large Language Models'
arxiv_id: '2502.12825'
source_url: https://arxiv.org/abs/2502.12825
tags:
- receiver
- they
- send
- amount
- return
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a new methodology to assess trusting behavior
  in LLMs using a classic game-theoretic trust game. It systematically varies sender
  objectives (helpful, profit-maximizing, risk-seeking), reasoning strategies (direct,
  zero-shot chain-of-thought, self-consistency), and receiver behavior (0%, 50%, 100%
  returns) across multiple LLM versions.
---

# Reasoning and the Trusting Behavior of DeepSeek and GPT: An Experiment Revealing Hidden Fault Lines in Large Language Models

## Quick Facts
- arXiv ID: 2502.12825
- Source URL: https://arxiv.org/abs/2502.12825
- Authors: Rubing Li; João Sedoc; Arun Sundararajan
- Reference count: 39
- Primary result: DeepSeek models consistently outperform OpenAI models in complex trust game scenarios

## Executive Summary
This paper introduces a game-theoretic trust game framework to evaluate trusting behavior in large language models (LLMs). The authors systematically test five LLM versions (GPT-4o-mini, o1-mini, o3-mini, DeepSeek-R1, DeepSeek-R1-Distill-Llama-70b) across three sender objectives (helpful, profit-maximizing, risk-seeking), three reasoning strategies (direct, CoT, self-consistency), and three receiver behaviors (0%, 50%, 100% returns). The results reveal stark performance differences: DeepSeek models consistently outperform OpenAI models in complex settings, demonstrating sophisticated trusting behaviors rooted in forward planning and theory-of-mind, while OpenAI's o1-mini and o3-mini models exhibit collapsed trusting behavior when reconciling profit-maximizing and risk-seeking objectives with future trust returns.

## Method Summary
The experiment uses a 10-round repeated trust game where the LLM acts as a "sender" with $10 endowment, deciding how much to send to a deterministic receiver that returns a fixed percentage of the tripled amount. Each round provides the current round number, same-receiver information, and historical average returns. The study tests a 3×3×3 treatment grid (objectives × receiver behaviors × reasoning strategies) across five LLM versions. Metrics include final profit as fraction of theoretical maximum ($100 for 0% return, $150 for 50%, $300 for 100%) and amount sent per round. Context is reset each round with only cumulative statistics carried forward.

## Key Results
- DeepSeek models outperform OpenAI models in complex trust game scenarios with varying receiver behaviors and sender objectives
- OpenAI's o1-mini and o3-mini models exhibit collapsed trusting behavior when reconciling profit-maximizing and risk-seeking objectives with future trust returns
- DeepSeek's models display more sophisticated and profitable trusting behavior, showing early signs of theory-of-mind through reasoning traces

## Why This Works (Mechanism)
The methodology reveals hidden behavioral fault lines in LLMs by placing them in economic decision-making contexts that require balancing immediate rewards against future trust-building. By varying sender objectives and receiver behaviors, the experiment creates scenarios where models must demonstrate sophisticated reasoning about long-term consequences. The contrast between DeepSeek and OpenAI models' performance suggests fundamental differences in how these systems learn to balance short-term objectives with longer-term strategic thinking.

## Foundational Learning
- **Trust game mechanics**: Understanding the economic framework where players must balance immediate gains against future trust-building
  - *Why needed*: Forms the basis for evaluating trusting behavior in LLMs
  - *Quick check*: Verify understanding of how sender decisions and receiver returns create incentives for trust-building

- **Reasoning strategy implementation**: Different methods for prompting models to think through decisions (direct, CoT, self-consistency)
  - *Why needed*: Tests whether explicit reasoning prompts improve economic decision-making
  - *Quick check*: Confirm models actually use provided reasoning strategies rather than default behavior

- **Objective reconciliation**: How models balance conflicting goals (profit vs. risk vs. helpfulness)
  - *Why needed*: Reveals whether models can integrate multiple objectives in economic contexts
  - *Quick check*: Compare performance across different objective settings

- **Theory-of-mind assessment**: Evaluating whether models reason about other agents' mental states
  - *Why needed*: Indicates sophistication beyond simple pattern matching
  - *Quick check*: Analyze reasoning traces for evidence of forward planning

## Architecture Onboarding
- **Component map**: Game engine → Prompt generator → LLM API → Response parser → Metric calculator
- **Critical path**: Prompt construction with game state → LLM generation → Amount extraction → Round progression
- **Design tradeoffs**: Resetting context each round (simpler implementation) vs. maintaining full history (more realistic but complex)
- **Failure signatures**: Models ignoring reasoning prompts, inconsistent amount parsing, context leakage across rounds
- **First experiments**: 1) Baseline single-round game with simple receiver; 2) Multi-round game with fixed receiver behavior; 3) Cross-model comparison on identical prompts

## Open Questions the Paper Calls Out
1. **Generalizability to other economic games**: Do the observed differences in trusting behavior between DeepSeek and OpenAI models generalize to other economic games and real-world commercial deployment contexts? [explicit] The authors acknowledge difficulty in extrapolating from trust games to more general settings.

2. **Architectural explanations**: What specific architectural or training differences between DeepSeek and OpenAI's reasoning models explain DeepSeek's superior ability to reconcile short-term objectives with longer-term trust-building? [inferred] The paper documents behavioral differences but doesn't identify underlying causes.

3. **Prompt sensitivity**: How robust are the observed trusting behavior patterns across variations in prompt structure, language, temperature settings, and cultural contexts? [explicit] The authors acknowledge using default settings and limited prompts in English only.

4. **Theory-of-mind validity**: Does the "theory-of-mind" capability suggested in DeepSeek's reasoning traces represent genuine reasoning about other agents' mental states or surface-level pattern matching from training data? [inferred] The paper provides qualitative interpretation but no controlled tests.

## Limitations
- Unspecified number of experimental iterations per treatment condition limits statistical power assessment
- Exact prompt formulations for sender objectives not provided in main text
- API temperature settings described as "default" without specific values
- Experiments conducted only in English with limited prompt variations

## Confidence
- **High Confidence**: DeepSeek models outperform OpenAI models in complex trust game scenarios
- **Medium Confidence**: DeepSeek models demonstrate "sophisticated trusting behaviors rooted in forward planning and theory-of-mind" based on performance patterns
- **Low Confidence**: Claim about "collapsed trusting behavior" in OpenAI's o1-mini and o3-mini models lacks full mechanistic explanation

## Next Checks
1. **Parameter Sensitivity Analysis**: Run the experiment with varying temperature settings (0.0, 0.7, 1.0) for each model to determine how stochasticity affects trusting behavior and whether observed performance gaps persist.

2. **Prompt Structure Validation**: Test whether claimed differences between direct, zero-shot chain-of-thought, and self-consistency conditions are actually being implemented as intended, particularly for models with built-in reasoning capabilities.

3. **Extended Time Horizon**: Extend the trust game from 10 rounds to 20-30 rounds to assess whether observed performance differences persist over longer time horizons and whether any learning or adaptation occurs.