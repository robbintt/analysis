---
ver: rpa2
title: 'EXPERT: An Explainable Image Captioning Evaluation Metric with Structured
  Explanations'
arxiv_id: '2506.24016'
source_url: https://arxiv.org/abs/2506.24016
tags:
- evaluation
- score
- image
- human
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EXPERT, a reference-free image captioning
  evaluation metric that provides structured, criterion-specific explanations for
  scores based on fluency, relevance, and descriptiveness. The authors construct two
  large-scale explanation datasets (Polaris-exp and Nebula-exp) by extending existing
  human judgment datasets, and train a vision-language model using a two-stage evaluation
  template.
---

# EXPERT: An Explainable Image Captioning Evaluation Metric with Structured Explanations

## Quick Facts
- arXiv ID: 2506.24016
- Source URL: https://arxiv.org/abs/2506.24016
- Reference count: 33
- Introduces EXPERT, a reference-free image captioning evaluation metric with structured, criterion-specific explanations

## Executive Summary
This paper introduces EXPERT, a novel reference-free image captioning evaluation metric that provides structured, criterion-specific explanations for scores based on fluency, relevance, and descriptiveness. The authors construct two large-scale explanation datasets (Polaris-exp and Nebula-exp) by extending existing human judgment datasets, and train a vision-language model using a two-stage evaluation template. EXPERT achieves state-of-the-art correlation with human judgments across benchmark datasets while providing significantly higher-quality explanations than existing metrics, as validated through comprehensive human evaluation. The metric demonstrates that structured explanations based on standardized criteria, when combined with supervised fine-tuning on high-quality explanations, substantially improve both scoring accuracy and explanation quality compared to previous approaches.

## Method Summary
The EXPERT metric operates by evaluating image captions against three standardized criteria: fluency, relevance, and descriptiveness. The approach involves constructing two large-scale explanation datasets by extending existing human judgment datasets (Polaris and Nebula) with detailed explanations for each criterion. A vision-language model is trained using a two-stage evaluation template where the model first generates structured explanations and then produces corresponding scores. The metric is reference-free, meaning it can evaluate captions without requiring ground truth references, making it more practical for real-world applications. The two-stage training approach involves supervised fine-tuning on the explanation datasets, followed by alignment with human judgment scores from established benchmarks.

## Key Results
- EXPERT achieves state-of-the-art correlation with human judgments across multiple benchmark datasets
- The metric provides significantly higher-quality explanations than existing metrics according to human evaluation studies
- Structured explanations based on standardized criteria substantially improve both scoring accuracy and explanation quality

## Why This Works (Mechanism)
EXPERT works by leveraging structured explanations grounded in three explicit evaluation criteria (fluency, relevance, descriptiveness) that align with how humans naturally assess image captions. The two-stage training approach first teaches the model to generate detailed, criterion-specific explanations, then fine-tunes it to produce scores that correlate with human judgments. By using high-quality human-annotated explanation datasets (Polaris-exp and Nebula-exp), the model learns to mimic expert evaluation patterns rather than relying on heuristic-based scoring. The reference-free design eliminates the need for ground truth captions, making the evaluation more practical while maintaining high correlation with human judgments. The structured explanation format forces the model to provide interpretable reasoning for each score, which not only improves transparency but also enables targeted improvements in captioning systems.

## Foundational Learning
- **Vision-language models**: Multimodal models that process both visual and textual information; needed to understand the relationship between images and captions for evaluation tasks
- **Supervised fine-tuning**: Training a pre-trained model on task-specific labeled data; needed to adapt general-purpose models to the specific task of image captioning evaluation
- **Reference-free evaluation**: Scoring captions without comparing to ground truth references; needed for practical deployment where ground truth captions may not be available
- **Human judgment datasets**: Collections of human ratings and annotations; needed to train models that align with human evaluation standards
- **Two-stage training**: Sequential training process with distinct phases; needed to first learn explanation generation, then score alignment
- **Structured explanations**: Organized, criterion-specific reasoning; needed to provide interpretable and actionable feedback for caption improvement

## Architecture Onboarding

Component map:
Vision-Language Model -> Two-Stage Training Pipeline -> EXPERT Evaluation Module -> Structured Explanations + Scores

Critical path:
Image + Caption Input -> Feature Extraction -> Explanation Generation (Fluency/Relevance/Descriptiveness) -> Score Calculation -> Final Evaluation Output

Design tradeoffs:
- Reference-free vs reference-based: EXPERT sacrifices some correlation potential for practical deployability without ground truth captions
- Structured vs free-form explanations: Structured explanations provide consistency and comparability but may miss nuanced aspects of caption quality
- Two-stage training vs single-stage: Two-stage training improves explanation quality but requires more computational resources and data

Failure signatures:
- Over-reliance on specific visual features leading to biased relevance scores
- Generic explanations that don't address image-specific content
- Score discrepancies between training datasets and real-world applications
- Inability to handle domain-specific terminology or abstract concepts

First experiments:
1. Evaluate EXPERT on a held-out portion of the Polaris-exp dataset to measure explanation quality consistency
2. Compare correlation scores with human judgments on the Flickr-8K dataset against existing metrics
3. Conduct ablation studies removing each evaluation criterion to quantify their individual contributions

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on existing human judgment datasets which may not fully represent all image captioning scenarios
- Explanation quality assessment depends on human raters who may have subjective interpretations
- Two-stage training approach requires substantial computational resources for fine-tuning large vision-language models
- Performance may degrade when applied to languages or domains not represented in the current datasets

## Confidence
High for the technical methodology and experimental results. Medium for generalizability across diverse captioning tasks and languages not represented in the current datasets. Medium for long-term robustness as the field evolves.

## Next Checks
1. Evaluate EXPERT on out-of-domain image captioning datasets (e.g., medical imaging, technical diagrams) to assess generalizability beyond natural images
2. Conduct ablation studies removing each criterion (fluency, relevance, descriptiveness) to quantify their individual contributions to both scoring accuracy and explanation quality
3. Test the metric's sensitivity to adversarial captions designed to exploit specific criteria, examining whether the structured explanations remain meaningful under manipulation attempts