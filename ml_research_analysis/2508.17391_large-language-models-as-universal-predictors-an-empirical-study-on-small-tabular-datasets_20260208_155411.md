---
ver: rpa2
title: Large Language Models as Universal Predictors? An Empirical Study on Small
  Tabular Datasets
arxiv_id: '2508.17391'
source_url: https://arxiv.org/abs/2508.17391
tags:
- llms
- data
- regression
- tabular
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) can
  serve as universal predictors on small tabular datasets without fine-tuning. The
  authors evaluate GPT-5, GPT-4o, GPT-o3, Gemini-2.5-Flash, and DeepSeek-R1 using
  in-context learning across classification, regression, and clustering tasks on datasets
  with up to 500 samples.
---

# Large Language Models as Universal Predictors? An Empirical Study on Small Tabular Datasets

## Quick Facts
- arXiv ID: 2508.17391
- Source URL: https://arxiv.org/abs/2508.17391
- Authors: Nikolaos Pavlidis; Vasilis Perifanis; Symeon Symeonidis; Pavlos S. Efraimidis
- Reference count: 32
- Primary result: LLMs achieve strong classification performance (over 96% accuracy on clean datasets) but fail at regression (negative R² scores) and clustering tasks

## Executive Summary
This paper investigates whether large language models (LLMs) can serve as universal predictors on small tabular datasets without fine-tuning. The authors evaluate GPT-5, GPT-4o, GPT-o3, Gemini-2.5-Flash, and DeepSeek-R1 using in-context learning across classification, regression, and clustering tasks on datasets with up to 500 samples. Results show that LLMs achieve strong classification performance, with some models reaching over 96% accuracy on clean datasets like Iris. However, regression performance is notably poor, with many models producing negative R² scores, and clustering results are similarly limited. The study demonstrates that LLMs can function as zero-training baselines for classification tasks but highlights significant limitations for regression and clustering, suggesting their role as practical exploratory tools rather than universal predictors across all structured data tasks.

## Method Summary
The study evaluates LLMs as zero-training predictors on 9 small tabular datasets (max 500 samples) across classification, regression, and clustering tasks. Data undergoes Z-score normalization, then serialization into tabular text formats (CSV/JSON) for in-context learning prompts. The evaluation compares LLMs against traditional ML baselines (Logistic Regression, Random Forest, LightGBM, CatBoost) and Transformer-based models (TabPFN, TabICL) using standard metrics: accuracy/F1 for classification, R²/MSE/MAE for regression, and silhouette/Davies-Bouldin/Calinski-Harabasz for clustering. Ablation studies test format sensitivity and data fraction effects.

## Key Results
- Classification: LLMs achieve strong performance, with some models exceeding 96% accuracy on clean datasets like Iris
- Regression: Consistently poor performance with many models producing negative R² scores, indicating worse performance than a mean baseline
- Clustering: Limited effectiveness with suboptimal metric scores across all evaluated models
- Format Sensitivity: Performance varies significantly with serialization format (e.g., DeepSeek shows 72%→83% improvement switching from CSV to JSON on Lupus)

## Why This Works (Mechanism)

### Mechanism 1: Discrete Label Mapping via In-Context Learning
- **Claim:** LLMs serve as zero-training classifiers by mapping serialized tabular features to discrete token probabilities, functioning effectively when label spaces are small and distinct.
- **Mechanism:** The model retrieves relevant statistical patterns from pre-training and few-shot examples to approximate a decision boundary, predicting the most likely next token (the class label) given the serialized feature context.
- **Core assumption:** The model's pre-training covers sufficient statistical reasoning or data pattern recognition to emulate a mapping function without gradient updates.
- **Evidence anchors:**
  - [abstract] "LLMs can perform predictive tasks over structured inputs without explicit fine-tuning... with clear strengths in classification."
  - [section 4.1] "In discrete label prediction tasks, they can reach accuracy comparable to purpose-built models."
  - [corpus] *Universal Embeddings of Tabular Data* supports the premise that tabular data can be embedded into representations suitable for downstream tasks.
- **Break condition:** Performance degrades significantly if the number of classes exceeds the effective context window or if class labels are semantically ambiguous.

### Mechanism 2: Precision Loss in Continuous Autoregression
- **Claim:** LLMs fail at regression because the autoregressive generation of continuous numerical values introduces compounding precision errors and scaling instability.
- **Mechanism:** Unlike classification (selecting from $N$ tokens), regression requires generating a precise sequence of digit tokens. The model lacks an objective function tied to numerical error (e.g., MSE), leading to outputs that may be statistically plausible as text but numerically distant from the target.
- **Core assumption:** The failure is due to the fundamental mismatch between the discrete nature of token generation and the requirements of continuous function approximation.
- **Evidence anchors:**
  - [abstract] "Regression performance is notably poor... likely because regression demands outputs in a large (often infinite) space."
  - [section 4.2] "Continuous value prediction demands numerical precision that is difficult to maintain through autoregressive token generation."
  - [corpus] *Performance Prediction for Large Systems via Text-to-Text Regression* explores similar boundaries in system metrics.
- **Break condition:** The mechanism breaks down entirely (negative $R^2$) when the target variable requires high-precision float representation or wide range scaling.

### Mechanism 3: Prompt Sensitivity and Data Serialization
- **Claim:** Predictive performance is highly contingent on the serialization format (e.g., JSON vs. CSV) and the volume of in-context examples, rather than robust internal reasoning.
- **Mechanism:** The model's attention mechanism differentially weights structural tokens (brackets, commas) versus numerical values. Formats that align with pre-training distribution (e.g., JSON) may trigger better pattern matching than less structured formats.
- **Core assumption:** The LLM is not genuinely "learning" the data distribution but pattern-matching against the specific structure of the prompt.
- **Evidence anchors:**
  - [abstract] "...influence of context size and prompt structure on approximation quality..."
  - [section 4.4] "Deepseek demonstrates a clearer sensitivity to format, with JSON yielding noticeably better results."
  - [corpus] Evidence is weak or missing in the provided corpus regarding specific serialization format effects on LLMs; neighbors focus on embeddings rather than prompt engineering.
- **Break condition:** Performance fluctuates unpredictably when switching serialization formats or exceeding optimal context lengths (overloading attention).

## Foundational Learning

- **Concept: In-Context Learning (ICL)**
  - **Why needed here:** This is the core capability allowing the LLM to act as a "universal predictor" without weight updates. You must understand that the model infers the task purely from the prompt examples.
  - **Quick check question:** If I change the order of the few-shot examples in the prompt, should the model's theoretical "knowledge" change? (Answer: No, but the attention-based prediction often *does*, indicating sensitivity).

- **Concept: Data Serialization (Tabular to Text)**
  - **Why needed here:** The paper highlights that LLMs do not natively ingest dataframes; the method of converting rows to text (CSV vs. JSON) acts as a feature engineering step.
  - **Quick check question:** Does converting a row `[1.5, "A", 90]` to `Col1: 1.5, Col2: A, Col3: 90` change the semantic meaning for an LLM compared to `1.5, A, 90`? (Answer: Semantically no, but statistically yes for the attention mechanism).

- **Concept: Autoregressive Generation vs. Numerical Optimization**
  - **Why needed here:** To understand the failure mode in regression, one must distinguish between generating the next likely *word* (LLM behavior) and minimizing a numerical *error function* (Classical ML behavior).
  - **Quick check question:** Why is predicting "9.812" harder for an LLM than predicting "Yes"? (Answer: "Yes" is a single token from a fixed vocabulary; "9.812" is a specific sequence of tokens requiring precise numerical correlation).

## Architecture Onboarding

- **Component map:** Input Layer (Z-score normalized data) -> Serialization Module (CSV/JSON converter) -> Prompt Engine (template + few-shot sampler) -> Inference Core (LLM API) -> Post-Processor (regex/semantic matcher)
- **Critical path:** The **Serialization Module** is the most fragile component. As shown in Section 4.4, changing format from CSV to JSON shifts accuracy from 72% to 83% on Lupus for DeepSeek.
- **Design tradeoffs:**
  - **Universal Access vs. Reliability:** Using LLMs requires zero infrastructure (high access) but offers negative $R^2$ on regression (low reliability).
  - **Context Window vs. Data Depth:** Adding more examples (up to 100%) does not always improve performance (e.g., Lupus ablation), challenging the "more data is better" assumption.
- **Failure signatures:**
  - **Regression Collapse:** Negative $R^2$ scores indicating the model is worse than a mean baseline (observed in GPT-o3).
  - **Context Saturation:** "–" results in Table 3 indicate prompts exceeding token limits on datasets like Diabetes.
- **First 3 experiments:**
  1. **Format Sensitivity Test:** Run a binary classification task (e.g., Bankrupt) using identical data serialized as CSV, JSON, and Markdown to establish a baseline for prompt stability.
  2. **Regression Viability Check:** Attempt to predict a bounded continuous variable. If the model outputs values outside the known min/max range, discard LLM for this regression task.
  3. **Data Fraction Ablation:** Test classification accuracy at 25%, 50%, and 100% data availability to verify if the specific dataset benefits from in-context scaling or suffers from noise (as seen in Lupus).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can LLMs be extended or modified to achieve reliable performance on regression and clustering tasks for structured data?
- **Basis in paper:** [explicit] The conclusion explicitly states that "Further work is needed to better understand their limitations in regression and clustering and to explore ways to extend LLMs for more reliable performance."
- **Why unresolved:** Current ICL implementations fail to approximate continuous functions (resulting in negative R² scores) or capture data geometry for clustering, limiting LLMs to classification tasks.
- **What evidence would resolve it:** New prompting strategies or architectural adaptations that enable LLMs to consistently achieve R² scores competitive with TabPFN or gradient-boosted trees on regression benchmarks.

### Open Question 2
- **Question:** Why does increasing the number of in-context examples sometimes lead to performance degradation in classification tasks?
- **Basis in paper:** [inferred] The ablation study reveals non-monotonic behavior where F1-scores dropped for Lupus and Bankrupt datasets as training fractions increased from 0.5 to 0.75.
- **Why unresolved:** The authors suggest "overfitting to some patterns" or increased heterogeneity, but the specific mechanism causing this "negative scaling" with more data is not empirically validated.
- **What evidence would resolve it:** A detailed analysis of attention patterns correlating performance drops with the introduction of noisy or outlier samples in the context window.

### Open Question 3
- **Question:** Is the poor regression performance a fundamental constraint of autoregressive token generation or a failure of the underlying function approximation capability?
- **Basis in paper:** [inferred] The authors attribute regression failure to the difficulty of producing accurate continuous outputs via autoregressive generation, distinct from the discrete nature of classification.
- **Why unresolved:** It is unclear if the model fails to internalize the continuous function mapping (reasoning error) or if it simply cannot express the precise floating-point values required (expressiveness error).
- **What evidence would resolve it:** Experiments using constrained decoding or symbolic regression tools integrated into the LLM pipeline to see if structural reasoning improves while generation constraints remain.

## Limitations
- **Model accessibility:** Cited models (GPT-5, GPT-o3) were not available at time of writing, raising questions about replicability
- **Prompt template opacity:** Critical prompt engineering details remain unspecified, representing the primary source of performance variance
- **Context length constraints:** Negative results ("–") in regression clustering tables indicate token overflow, limiting applicability to larger datasets
- **Serialization sensitivity:** Performance differences between CSV and JSON formats suggest results may not generalize across data representation choices

## Confidence
- **Classification performance claims (High):** Strong empirical support with multiple datasets showing >96% accuracy on clean data
- **Regression failure claims (High):** Consistent negative R² scores across models provide robust evidence
- **Clustering limitations (Medium):** Fewer experimental results and metric dependencies reduce confidence
- **Format sensitivity findings (Medium):** Limited ablation scope (primarily DeepSeek) constrains generalizability

## Next Checks
1. **Serialization format benchmark:** Systematically test classification performance across CSV, JSON, and Markdown formats on all nine datasets to establish prompt engineering best practices
2. **Regression viability filter:** Implement pre-screening criteria based on target variable bounds; discard LLM regression attempts when predicted values fall outside min/max ranges
3. **Context scaling analysis:** Conduct controlled experiments varying few-shot example counts (25%, 50%, 75%, 100%) to identify optimal data fraction and detect potential overfitting patterns observed in Lupus dataset