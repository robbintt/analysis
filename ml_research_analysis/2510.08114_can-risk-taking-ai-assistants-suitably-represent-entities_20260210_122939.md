---
ver: rpa2
title: Can Risk-taking AI-Assistants suitably represent entities
arxiv_id: '2510.08114'
source_url: https://arxiv.org/abs/2510.08114
tags:
- risk
- human
- aversion
- decision
- behavior
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the manipulability of risk aversion (MoRA)
  in large language models (LLMs) to determine their ability to replicate human risk
  preferences across diverse contexts. Using the Holt and Laury (2002) risk preference
  tasks, we assess how well LLMs respond to manipulations in risk attitudes, such
  as gender identity, geographic location, and uncertainty.
---

# Can Risk-taking AI-Assistants suitably represent entities

## Quick Facts
- **arXiv ID:** 2510.08114
- **Source URL:** https://arxiv.org/abs/2510.08114
- **Reference count:** 0
- **Primary result:** Risk aversion manipulability in LLMs varies significantly across models and contexts, with reasoning-enhanced models showing better human alignment.

## Executive Summary
This study evaluates the Manipulability of Risk Aversion (MoRA) in large language models using Holt and Laury (2002) risk preference tasks. The research assesses how well LLMs respond to manipulations in risk attitudes across 12 models and 10 contextual prompts, measuring both MoRA and alignment with human risk preferences (DHRA). Results show that while some models like DeepSeek Reasoner and Gemini-2.0-flash-lite demonstrate strong alignment with human behaviors, others exhibit notable deviations, highlighting the need for refining bio-centric measures of manipulability to improve AI decision-making systems.

## Method Summary
The study uses the Holt-Laury risk preference task with 12 LLMs, presenting each with 10 lottery decisions per trial across 35 trials per context. Ten different contextual prompts (gender, geography, crisis, legal roles, manipulation) are applied to assess MoRA, calculated as the distance between risk-avoiding and risk-loving choice vectors. DHRA measures Euclidean distance to human benchmark values from Holt-Laury (2002). Models include DeepSeek, Google, Meta, OpenAI, and xAI systems, with switch points recorded and rationales collected post-choice.

## Key Results
- DeepSeek Reasoner and Gemini-2.0-flash-lite show strongest alignment with human risk behaviors across contexts
- Gender identity prompts induce differential risk aversion in some LLMs, with female prompts increasing risk aversion
- Models with reasoning capabilities demonstrate better human alignment than non-reasoning counterparts

## Why This Works (Mechanism)

### Mechanism 1: Prompt-Based Persona Injection
- Claim: Contextual prompts shift LLM risk preferences by biasing selection between safer/riskier lottery options
- Mechanism: LLMs encode persona traits in contextual embeddings; prompts bias the selection between safer/riskier lottery options
- Core assumption: The model's internal state is sufficiently malleable via natural language instructions
- Evidence anchors: Abstract shows assessment of LLM response to manipulations in risk attitudes; section defines MoRA as shift in risk aversion between prompts
- Break condition: If prompt modifications yield no measurable change in HL task switch points, MoRA ≈ 0

### Mechanism 2: Reasoning Model Sensitivity
- Claim: Reasoning-enhanced LLMs better align with human risk preferences
- Mechanism: Extended inference enables more nuanced evaluation of expected payoffs, leading to behavior closer to human benchmarks
- Core assumption: Extended reasoning improves alignment with human-like risk assessment patterns
- Evidence anchors: Abstract notes DeepSeek Reasoner and Gemini-2.0-flash-lite demonstrate strong human alignment; section describes DeepSeek-reasoner improvements
- Break condition: If reasoning models show high DHRA or negative MoRA, the mechanism fails

### Mechanism 3: Gender-Contextual Embedding Bias
- Claim: Gender-prompted contexts induce differential risk aversion (female prompts → higher risk aversion)
- Mechanism: Training data encodes human gender-risk stereotypes; these are activated by gender-primed prompts
- Core assumption: Training data correlations transfer to context-activated behaviors without explicit finetuning
- Evidence anchors: Section notes LMs like Gemini-2.0-flash-lite and DeepSeek Reasoner showed higher risk aversion with female identities
- Break condition: If female-prompted risk aversion ≤ male-prompted risk aversion across trials, the mechanism is absent

## Foundational Learning

Concept: Holt-Laury Risk Aversion Task
- Why needed here: Core protocol for measuring risk preferences via lottery choice switch points
- Quick check question: "What does a switch point <5 vs >5 indicate in the HL task?"

Concept: Euclidean Distance to Human Risk Aversion (DHRA)
- Why needed here: Metric for quantifying alignment between LLM and human risk profiles
- Quick check question: "How is DHRA computed from HL task vectors?"

Concept: Manipulability of Risk Aversion (MoRA)
- Why needed here: Key KPI for assessing how steerable an LLM's risk behavior is
- Quick check question: "What does MoRA = 0 imply about an LLM assistant's suitability?"

## Architecture Onboarding

Component map: Prompt System (persona/context) -> LLM Core (reasoning/embeddings) -> HL Task Interface -> Metrics Calculator (MoRA, DHRA)

Critical path: 1) Configure prompt context (identity, geography, crisis); 2) Run HL 10-decision task; 3) Extract switch point; 4) Compute MoRA/DHRA against human benchmarks

Design tradeoffs:
- Persona specificity vs. generalization risk (overfitting to prompt frames)
- Reasoning depth vs. latency/throughput constraints

Failure signatures:
- Negative MoRA (model moves opposite to intended direction)
- High DHRA (>threshold) with minimal MoRA

First 3 experiments:
1. Baseline MoRA/DHRA across 3 LLMs (DeepSeek Reasoner, Gemini-2.0-flash-lite, GPT-5) with neutral prompts
2. Gender-prompt contrast (male vs. female) to measure gender-bias delta in risk aversion
3. Crisis-context prompt to test sensitivity to uncertainty framing

## Open Questions the Paper Calls Out

Open Question 1
- Question: How can MoRA metric be refined to handle directional errors where models misinterpret manipulations?
- Basis in paper: Authors note current distance metrics fail when choice distributions overlap or produce negative differences
- Why unresolved: Current Euclidean distance calculation loses nuance when models like DeepSeek-chat misinterpret risk-avoiding prompts
- What evidence would resolve it: Development and validation of a signed or asymmetric metric that penalizes inverse responses

Open Question 2
- Question: To what extent do socio-cultural and linguistic framing mechanisms drive discrepancies in LLM risk preferences vs human baselines?
- Basis in paper: Paper cites need to investigate role of demographic factors and references prior work on linguistic framing effects
- Why unresolved: Study establishes models align with gender-specific attitudes but underlying drivers for cultural biases remain unidentified
- What evidence would resolve it: Cross-cultural study isolating linguistic prompts from cultural persona injections

Open Question 3
- Question: Can "bio-centric" measures of manipulability bridge gap between AI simulation and biological/cognitive determinants of human risk-taking?
- Basis in paper: Abstract and conclusion explicitly call for refining bio-centric measures because LLMs lack evolutionary foundations
- Why unresolved: Current metrics assess alignment with outcomes but fail to capture whether AI mimics biological/cognitive constraints
- What evidence would resolve it: Framework correlating AI "persona" stability with simulated biological constraints

## Limitations

- Temperature parameter for LLM responses is specified as "fixed" but exact value not provided, affecting reproducibility
- Human benchmark values from Holt-Laury (2002) required for DHRA calculations are referenced but not explicitly provided
- Interpretation depends on assumption that LLMs can meaningfully represent human risk preferences through context manipulation

## Confidence

- High confidence: Observed manipulability differences between LLMs and general pattern of some models better aligning with human risk preferences
- Medium confidence: Mechanism linking reasoning capabilities to improved human alignment requires further validation
- Medium confidence: Gender-contextual embedding bias findings need stronger corpus support and may reflect training data artifacts

## Next Checks

1. Replicate gender-prompt experiment across all 12 LLMs with standardized temperature settings to verify consistency of observed gender-bias patterns
2. Conduct ablation studies removing reasoning traces from DeepSeek Reasoner to isolate contribution of reasoning depth to human alignment
3. Compare MoRA and DHRA results using alternative lottery choice protocols beyond Holt-Laury to test generalizability of manipulability measures