---
ver: rpa2
title: 'A Hybrid Protocol for Large-Scale Semantic Dataset Generation in Low-Resource
  Languages: The Turkish Semantic Relations Corpus'
arxiv_id: '2601.13253'
source_url: https://arxiv.org/abs/2601.13253
tags:
- semantic
- turkish
- dataset
- terms
- synonym
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a hybrid protocol for generating large-scale
  semantic relationship datasets in low-resource languages, demonstrated through a
  Turkish semantic relations corpus. The method combines FastText embeddings with
  agglomerative clustering to identify semantic clusters, Gemini 2.5-Flash for automated
  semantic relationship classification, and integration with curated dictionary sources.
---

# A Hybrid Protocol for Large-Scale Semantic Dataset Generation in Low-Resource Languages: The Turkish Semantic Relations Corpus

## Quick Facts
- **arXiv ID:** 2601.13253
- **Source URL:** https://arxiv.org/abs/2601.13253
- **Reference count:** 15
- **Primary result:** 843,000 unique Turkish semantic pairs across synonyms, antonyms, and co-hyponyms generated at $65 cost

## Executive Summary
This paper introduces a hybrid protocol for generating large-scale semantic relationship datasets in low-resource languages, demonstrated through a Turkish semantic relations corpus. The method combines FastText embeddings with agglomerative clustering to identify semantic clusters, Gemini 2.5-Flash for automated semantic relationship classification, and integration with curated dictionary sources. The resulting dataset comprises 843,000 unique Turkish semantic pairs across three relationship types (synonyms, antonyms, co-hyponyms), representing a 10x scale increase over existing resources. The dataset was validated through two downstream tasks: an embedding model achieving 90% top-1 retrieval accuracy for synonym pairs and a classification model attaining 90% F1-macro. The hybrid approach couples large-scale automation with explicit quality control through dictionary validation, yielding high-quality semantic data at minimal cost ($65). The protocol is designed to be generalizable to other low-resource languages with minimal adaptation, addressing critical data scarcity challenges in Turkish NLP and similar language contexts.

## Method Summary
The protocol employs a three-phase pipeline: (1) FastText embeddings with Agglomerative Clustering (cosine distance, threshold 0.4) groups 77K expert-curated Turkish terms into 13K semantic clusters; (2) Gemini 2.5-Flash processes each cluster with structured prompts to classify and generate semantic relationships in JSON format; (3) An external Turkish synonym dictionary (20K entries, filtered to 16K high-reliability pairs) provides validation anchors. The final dataset contains 843K unique pairs (72% co-hyponyms, 18% synonyms, 10% antonyms). Downstream embedding and classification models achieve 90% accuracy on validation tasks using turkish-e5-large architecture with weighted loss functions to handle class imbalance.

## Key Results
- 843,000 unique Turkish semantic pairs generated across three relationship types
- Embedding model achieves 90% top-1 retrieval accuracy for synonym pairs
- Classification model attains 90% F1-macro across all three semantic relationship types
- Protocol costs only $65 while producing data 10x larger than existing Turkish semantic resources

## Why This Works (Mechanism)

### Mechanism 1
FastText embeddings with agglomerative clustering provide semantic scaffolding that improves LLM relationship classification accuracy. FastText's subword modeling captures morphological variants (critical for agglutinative Turkish), while agglomerative clustering with a lenient distance threshold (0.4) groups terms by broad thematic relevance rather than strict similarity. This ensures synonyms, antonyms (which share distributional contexts), and co-hyponyms co-occur in clusters, giving the LLM sufficient context to distinguish relationship types.

### Mechanism 2
LLM-based classification with structured prompting can induce high-quality semantic relationships at scale without manual annotation. Gemini 2.5-Flash receives each semantic cluster along with explicit categorization rules and outputs structured JSON. The model leverages both cluster context and its pretrained multilingual knowledge to classify and augment relationships.

### Mechanism 3
Dictionary integration with strict filtering provides high-precision validation anchors that ground the synthetic dataset. An external Turkish synonym dictionary (20,000 entries) is filtered to retain only headwords with ≤2 synonym candidates, yielding 16,000 high-reliability pairs. These serve as ground-truth anchors within the predominantly synthetic dataset (98.1% LLM-generated).

## Foundational Learning

- **Distributional Semantics Limitation (Antonym-Synonym Confusion):** Antonyms and synonyms receive similar vector representations because they appear in near-identical contexts. This is the core problem the hybrid protocol addresses.
  - *Quick check:* Why can't embedding distance alone distinguish "hot" from "cold" vs. "hot" from "warm"?

- **Agglomerative Clustering with Cosine Distance:** This is the primary grouping mechanism in Phase I. Understanding distance thresholds and their effect on cluster granularity is essential for adaptation.
  - *Quick check:* What happens to cluster composition if the distance threshold is set too low (e.g., 0.2) vs. too high (e.g., 0.6)?

- **Class Imbalance in Semantic Relations:** The dataset has 72% co-hyponyms, 18% synonyms, 10% antonyms. This natural imbalance requires weighted loss functions during classifier training.
  - *Quick check:* Why would a classifier trained without class weighting overpredict co-hyponyms?

## Architecture Onboarding

- **Component map:** [Term Lexicon (77K)] + [NER Augmentation] → [FastText Embeddings (300-dim)] → [Agglomerative Clustering (threshold=0.4)] → [13K Semantic Clusters] → [Gemini 2.5-Flash API] → [827K Synthetic Pairs] → [Dictionary Integration (16K pairs)] → [Final Dataset: 843K pairs] → [Downstream: Embedding Model (E5) | Classification Model (Turkish-E5-Large)]

- **Critical path:** Phase I clustering quality directly determines LLM enrichment quality. Poor clusters → insufficient context → classification errors at scale.

- **Design tradeoffs:**
  - Lenient clustering threshold (0.4): Increases recall (synonyms co-clustered) but includes noise (antonyms in same cluster). Trade-off resolved by letting LLM distinguish relationship types.
  - Strict dictionary filtering (≤2 synonyms): High precision (16K from 20K entries) but sacrifices coverage. Trade-off favors reliability over scale for validation anchors.
  - Co-hyponyms as hard negatives: Paper reports this degraded embedding model performance—excluded from training.

- **Failure signatures:**
  - High synonym false positives in downstream classifier → prompt too permissive, or antonym-synonym distinction insufficiently enforced.
  - Low retrieval accuracy on domain-specific terms → FastText vocabulary coverage gap; consider domain-adapted embeddings.
  - Cluster sizes too large (>50 terms) → LLM context overload; reduce distance threshold or pre-split clusters.

- **First 3 experiments:**
  1. Baseline reproduction: Replicate clustering on a 1K term subset, manually verify cluster coherence and LLM classification accuracy on sampled clusters.
  2. Threshold sensitivity: Test distance thresholds [0.3, 0.4, 0.5] and measure impact on cluster size distribution and downstream F1.
  3. Dictionary ablation: Train classification model with and without dictionary-derived pairs to quantify their contribution to validation metrics.

## Open Questions the Paper Calls Out

### Open Question 1
Does the hybrid protocol maintain effectiveness when applied to low-resource languages with significantly different morphological structures or weaker LLM support than Turkish? The authors claim the protocol is "generalizable to other low-resource languages" but validate it exclusively on Turkish using Gemini 2.5-Flash. Turkish is agglutinative and has relatively strong support in multilingual LLMs; the pipeline's reliance on "LLM enrichment" may fail for languages where the LLM lacks sufficient semantic competence.

### Open Question 2
To what extent does the dataset's heavy legal-domain bias impede performance on casual or conversational Turkish NLP tasks? The dataset is primarily grounded in legal domain vocabulary. Models trained on this data may underperform on casual or conversational Turkish. The validation tasks were performed on data splits derived from the generated corpus itself, rather than on external general-domain benchmarks.

### Open Question 3
How do LLM-specific hallucinations or biases in the 98% synthetic data portion impact the reliability of downstream models? Approximately 98% of the data is synthetically generated via LLM and LLM-specific biases may propagate to downstream models. While the authors validate the dataset with high F1-scores, this measures internal consistency rather than factual correctness or freedom from LLM "confabulations" regarding semantic relations.

### Open Question 4
Why does the inclusion of co-hyponyms as hard negatives degrade embedding model performance, and does this indicate noise in the co-hyponym labels? Including co-hyponyms as hard negatives "made the performance of the model worse." Standard contrastive learning theory suggests hard negatives improve representation quality; the failure suggests the "co-hyponym" label may be too broad or noisy for this specific architecture.

## Limitations

- The dataset is primarily grounded in legal domain vocabulary, potentially limiting performance on casual or conversational Turkish NLP tasks
- Approximately 98% of the data is synthetically generated via LLM, with potential propagation of LLM-specific biases to downstream models
- The protocol's effectiveness depends on availability of quality FastText embeddings, curated dictionaries, and LLM performance for the target language

## Confidence

- **High Confidence:** The dataset scale (843K pairs) and cost efficiency ($65) are verifiable facts from the reported methodology.
- **Medium Confidence:** The 90% F1-macro and 90% top-1 accuracy metrics, as these depend on turkish-e5-large availability and may not generalize across languages.
- **Low Confidence:** The generalizability claim to "any low-resource language with minimal adaptation" lacks empirical validation beyond the Turkish case study.

## Next Checks

1. **Cluster Quality Analysis:** Before LLM enrichment, manually evaluate 50 randomly sampled clusters for semantic coherence and relationship type distribution.

2. **Cross-Lingual Adaptation:** Apply the protocol to a different low-resource language (e.g., Finnish or Vietnamese) with available FastText embeddings and dictionary resources, measuring cluster quality and downstream task performance.

3. **Threshold Sensitivity Study:** Systematically test distance thresholds [0.3, 0.4, 0.5, 0.6] and measure impact on cluster size distribution, relationship classification accuracy, and downstream task performance.