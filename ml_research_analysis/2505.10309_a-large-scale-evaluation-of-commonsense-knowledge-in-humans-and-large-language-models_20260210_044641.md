---
ver: rpa2
title: A large-scale evaluation of commonsense knowledge in humans and large language
  models
arxiv_id: '2505.10309'
source_url: https://arxiv.org/abs/2505.10309
tags:
- statement
- human
- commonsensicality
- humans
- people
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new framework for evaluating commonsense
  knowledge in large language models (LLMs) by measuring their agreement with empirically
  observed human heterogeneity, rather than relying on ground-truth labels. The key
  idea is to treat both humans and LLMs as survey respondents who rate statements
  for agreement, then measure how well each model's ratings align with the majority
  human opinion.
---

# A large-scale evaluation of commonsense knowledge in humans and large language models

## Quick Facts
- arXiv ID: 2505.10309
- Source URL: https://arxiv.org/abs/2505.10309
- Reference count: 40
- Primary result: Most LLMs score below human median in commonsense knowledge when evaluated by agreement with human heterogeneity

## Executive Summary
This paper introduces a novel framework for evaluating commonsense knowledge in large language models by measuring their agreement with empirically observed human heterogeneity, rather than relying on ground-truth labels. The key insight is treating both humans and LLMs as survey respondents who rate statements for agreement, then measuring how well each model's ratings align with the majority human opinion. The framework distinguishes between two evaluation settings: individual-level commonsensicality (treating each LLM as an independent survey respondent) and statement-level commonsensicality (using each LLM to simulate a hypothetical population of "silicon samples").

The main results show that most LLMs score below the human median in individual commonsensicality, with only about one-third ranking above it. Surprisingly, smaller open-weight models like Mistral-7B and Flan-T5-XXL are more competitive than larger proprietary models. When used as population simulators, LLM-generated populations correlate modestly (up to r = 0.43) with human commonsense distributions, well below human internal reliability (r = 0.60). Some models like Gemini Pro 1.0 show systematic differences in what they consider commonsensical compared to humans, preferring figures of speech over literal statements.

## Method Summary
The evaluation framework uses 4,407 statements from seven sources (ConceptNet, ATOMIC, aphorisms, news media, campaign emails, and situational/categorical responses) rated by 2,046 human participants on Amazon Mechanical Turk. Each LLM answers two questions per statement: (a) "Do you agree?" and (b) "Do you think most people would agree?" These responses are used to compute consensus (agreement with human majority) and awareness (correct prediction of majority) scores. Individual commonsensicality is the geometric mean of these two scores. For population simulation, LLM output probabilities are interpreted as the response distribution of a hypothetical population of "silicon samples," and statement-level commonsensicality is computed from this simulated population's consensus and awareness.

## Key Results
- Most LLMs score below the human median in individual commonsensicality, with only about one-third ranking above it
- Smaller open-weight models like Mistral-7B and Flan-T5-XXL outperform larger proprietary models in commonsensicality
- When used as population simulators, LLM-generated populations correlate modestly (up to r = 0.43) with human commonsense distributions
- Some models show systematic differences in what they consider commonsensical compared to humans

## Why This Works (Mechanism)

### Mechanism 1: Individual-level commonsensicality via dual-component scoring
- Claim: LLMs can be evaluated as independent survey respondents by measuring both their agreement with human majority opinion (consensus) and their ability to predict that majority (awareness).
- Mechanism: Each LLM answers two questions per statement: (a) "Do you agree?" and (b) "Do you think most people would agree?" The geometric mean of consensus and awareness scores yields commonsensicality, enabling direct comparison with humans.
- Core assumption: Common sense requires both holding beliefs that coincide with a community AND knowing that others share those beliefs.
- Break condition: If LLMs systematically give identical answers to questions (a) and (b) (observed ~90% of the time), the awareness signal provides limited discriminative power.

### Mechanism 2: Silicon sample population simulation via output probability interpretation
- Claim: LLM output probabilities can be interpreted as the response distribution of a hypothetical population of "silicon samples."
- Mechanism: The probability an LLM outputs "yes" to a question is treated as the frequency with which sampled agents from a simulated population would respond "yes." Statement-level commonsensicality is computed from this population's consensus and awareness.
- Core assumption: LLM internal confidence is calibrated to empirical frequency of human endorsement—this is described as empirical rather than theoretically substantiated.
- Break condition: If LLMs are overconfident (probabilities near 0 or 1), the simulated population lacks heterogeneity, collapsing scores to extremes. This was observed for Claude 3 Opus and Gemini 1.0 Pro.

### Mechanism 3: Heterogeneity-aware benchmarking without ground truth
- Claim: Evaluating commonsense knowledge requires abandoning ground-truth labels in favor of measuring correspondence with observed human belief distributions.
- Mechanism: Rather than scoring models on accuracy against single correct answers, the framework computes the correlation between statement commonsensicality scores in human vs. silicon populations (r up to .43 vs. human split-half reliability of r = .60).
- Core assumption: What constitutes "common sense" is inherently pluralistic and culturally contingent—no universally held beliefs exist.
- Break condition: If the reference human population is not representative of the deployment context, the evaluation may mischaracterize model alignment with actual users.

## Foundational Learning

- Concept: **Consensus vs. Awareness in social knowledge**
  - Why needed here: The framework distinguishes between "knowing what your community believes" (consensus) and "knowing that others know" (awareness). This sociological distinction underpins the commonsensicality metric.
  - Quick check question: If a model disagrees with the human majority on a statement but correctly predicts what the majority believes, which score is affected—consensus or awareness?

- Concept: **Probabilistic interpretation of LLM outputs**
  - Why needed here: The population simulation mechanism relies on treating token probabilities as meaningful confidence estimates representing population response frequencies.
  - Quick check question: What happens to the simulated population's heterogeneity if an LLM outputs probabilities near 0 or 1 for most statements?

- Concept: **Cultural/pluralistic grounding of common sense**
  - Why needed here: The paper explicitly rejects the "one-truth myth" and frames common sense as socially constructed knowledge that varies across communities.
  - Quick check question: Why would a model that scores 90% on a traditional commonsense benchmark still rank below the human median in this framework?

## Architecture Onboarding

- Component map:
  - Statement corpus -> Human rating collection -> LLM prompting module -> Individual-level scorer -> Population-level scorer -> Correlation analyzer

- Critical path:
  1. Load human rating matrices A (agreement) and B (belief-about-others) from existing dataset
  2. For each LLM, prompt with questions (a) and (b) for all 4,407 statements
  3. Extract "yes"/"no" token probabilities, rescale to sum to 1
  4. For individual-level: binarize via argmax, compute accuracy against human majority ratings
  5. For population-level: use probabilities directly as population frequencies, compute statement commonsensicality
  6. Correlate LLM scores with human scores; compare to human split-half reliability baseline

- Design tradeoffs:
  - Temperature: Paper uses 1.0 (default) to evaluate models "as-is"; tuning could improve calibration but would not reflect default behavior
  - Prompting strategy: Separate chat sessions for (a) and (b) to avoid cross-contamination; explicit "Start with yes/no" instruction to simplify token extraction
  - Reference population: Uses MTurk sample (English-speaking, US-based); results may not generalize to other populations

- Failure signatures:
  - Overconfident models (probabilities at extremes) → collapsed heterogeneity in silicon populations, reduced correlation with humans
  - Models that always give same answer to (a) and (b) → reduced discriminative power of awareness score
  - Negative or near-zero correlation with human statement scores → systematic misalignment (e.g., LLaMA-3-8B r = -0.06)

- First 3 experiments:
  1. Replicate the individual-level analysis for your target model(s): compute consensus, awareness, and commonsensicality scores; compare against human median and top performers (Mixtral-8x22B at 82.3%, Flan-T5-XXL at 80.4%)
  2. Run population-level simulation: extract token probabilities for all statements, compute statement commonsensicality scores, correlate with human scores; target r > 0.40 to match best open-weight models
  3. Analyze statement-type sensitivity: compute commonsensicality separately for each epistemological dichotomy (fact/opinion, physical/social, literal/figurative); identify where your model over- or under-performs relative to humans

## Open Questions the Paper Calls Out

- What systematic characteristics of a group and its social processes determine the content of its shared commonsense knowledge?
- How does an LLM's commonsensicality change when evaluated against different reference populations (e.g., American college students vs. MTurk workers)?
- To what extent do instruction fine-tuning and human alignment training cause LLMs to sacrifice distributional representativeness (diversity of simulated opinions) in favor of confident singular outputs?
- How do LLMs perform on commonsense evaluation when the statement corpus includes domains currently absent from the study, such as theory of mind inferences?

## Limitations

- The framework relies on correlation with human opinion rather than ground-truth accuracy, potentially measuring agreement with shared biases rather than true commonsense understanding
- The MTurk reference population (English-speaking, US-based) may not represent global diversity in commonsense knowledge
- The awareness signal provides limited discriminative power since LLMs consistently give identical answers to questions (a) and (b) in ~90% of cases

## Confidence

- **High confidence**: Individual-level commonsensicality results showing most LLMs scoring below human median; smaller open-weight models (Mistral-7B, Flan-T5-XXL) outperforming larger proprietary models; population simulation mechanism and correlation metrics are well-specified
- **Medium confidence**: The framework's validity as a measure of commonsense knowledge rather than agreement with specific human samples; the claim that no ground truth exists for commonsense reasoning
- **Low confidence**: The practical significance of differences between models (e.g., Mixtral-8x22B at 82.3% vs. Claude 3 Opus at 77.7%) given the substantial gap from human split-half reliability (r = 0.60)

## Next Checks

1. Test the framework with a more diverse reference population to assess sensitivity to cultural/linguistic differences in commonsense beliefs
2. Validate whether high correlation with human opinion actually predicts useful commonsense reasoning in downstream applications, or merely reflects shared biases
3. Examine whether the framework can distinguish between models that share human biases versus those that demonstrate genuine reasoning ability that may diverge from common human beliefs