---
ver: rpa2
title: Improving Learning to Optimize Using Parameter Symmetries
arxiv_id: '2504.15399'
source_url: https://arxiv.org/abs/2504.15399
tags:
- learning
- neural
- teleportation
- symmetry
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes a learning-to-optimize (L2O) algorithm that
  incorporates parameter space symmetries to enhance optimization efficiency. The
  method augments L2O with learned symmetry transformations (teleportation) that move
  parameters across symmetry orbits before applying updates.
---

# Improving Learning to Optimize Using Parameter Symmetries
## Quick Facts
- arXiv ID: 2504.15399
- Source URL: https://arxiv.org/abs/2504.15399
- Reference count: 22
- Primary result: Analyzes L2O with parameter space symmetries, showing theoretical convergence benefits but mixed empirical performance

## Executive Summary
This paper presents a learning-to-optimize (L2O) algorithm that incorporates parameter space symmetries through learned teleportation transformations. The method augments standard L2O by moving parameters across symmetry orbits before applying updates, theoretically resembling Newton's method under certain conditions. While the approach shows promise for exploiting structural properties of optimization landscapes, empirical results reveal that vanilla L2O often outperforms the symmetry-augmented version, with benefits appearing only on specific problem distributions.

## Method Summary
The proposed method enhances L2O by learning symmetry transformations that map parameters to different points on their symmetry orbits before applying optimization updates. During training, the L2O model learns both the base optimization strategy and the optimal teleportation function. The theoretical analysis demonstrates that this approach locally approximates Newton's method, providing convergence advantages when the symmetry transformations are well-learned. The authors also prove conditions under which L2O can learn optimal teleportation strategies during meta-training.

## Key Results
- Theoretical analysis shows symmetry-augmented L2O locally resembles Newton's method under smoothness and invertibility conditions
- Empirical benchmark on low-dimensional test functions shows mixed results, with vanilla L2O often outperforming the teleportation-augmented version
- Adding learned momentum to the teleportation algorithm improves meta-optimizer performance on neural network training tasks

## Why This Works (Mechanism)
The method works by exploiting parameter space symmetries that exist in many optimization problems. When parameters can be transformed without changing the objective function value, the L2O model can learn to strategically teleport parameters to more favorable positions on their symmetry orbits before applying standard updates. This effectively increases the local curvature information available to the optimizer, mimicking second-order methods like Newton's method. The learned teleportation function acts as a preprocessing step that transforms the parameter space geometry to make optimization more efficient.

## Foundational Learning
- **Learning to optimize (L2O)**: Meta-learning approach where a model learns optimization strategies from data
  - Why needed: Provides foundation for understanding how neural networks can learn optimization algorithms
  - Quick check: Can the L2O model learn basic gradient descent without symmetry transformations?

- **Parameter space symmetries**: Transformations that leave the objective function invariant
  - Why needed: Core concept that enables the teleportation approach
  - Quick check: Are the test functions truly symmetric under the proposed transformations?

- **Teleportation**: Mapping parameters to different points on symmetry orbits
  - Why needed: The key mechanism for incorporating symmetry knowledge into L2O
  - Quick check: Does the learned teleportation function actually move parameters along symmetry orbits?

## Architecture Onboarding
**Component Map**: Input parameters -> Symmetry transformation network -> Teleportation function -> Base L2O optimizer -> Updated parameters

**Critical Path**: The symmetry transformation network must learn invertible mappings that preserve objective function values while improving optimization efficiency. The learned teleportation function needs to be differentiable for end-to-end training.

**Design Tradeoffs**: Using more complex symmetry transformation architectures increases representational power but may lead to overfitting on small datasets. Simpler transformations are more likely to learn true symmetries but may miss beneficial non-symmetry transformations.

**Failure Signatures**: Poor performance when the symmetry transformation network fails to learn invertible mappings, or when the learned transformations move parameters away from optimal solutions rather than toward them.

**First Experiments**:
1. Test learned teleportation on 1D quadratic functions with known symmetries
2. Evaluate sensitivity to symmetry transformation network architecture (number of layers, activation functions)
3. Compare against baseline L2O with random teleportation versus learned teleportation

## Open Questions the Paper Calls Out
The paper does not explicitly identify open questions in the provided information, but several natural extensions arise from the mixed empirical results. These include investigating conditions under which symmetry-based teleportation provides clear benefits, exploring the relationship between symmetry discovery and optimization efficiency, and determining whether learned teleportation can generalize across different optimization problems.

## Limitations
- Theoretical analysis requires strong assumptions about smoothness and invertibility that may not hold in practice
- Empirical evaluation limited to low-dimensional test functions and simple neural network architectures
- Mixed empirical results show no clear practical advantage over vanilla L2O in most cases
- Does not establish clear criteria for when symmetry-based teleportation is beneficial versus detrimental
- The method's scalability to high-dimensional problems with complex symmetry structures remains unexplored

## Confidence
- Theoretical convergence analysis: Medium - Conditional on specific assumptions about symmetry transformations
- Empirical results on test functions: Medium - Show mixed performance with no clear superiority
- Neural network training experiments: Medium - Limited scope reduces generalizability

## Next Checks
1. Test the teleportation-augmented L2O on high-dimensional optimization problems with known symmetries to verify if theoretical benefits translate to practical improvements
2. Investigate sensitivity to different symmetry transformation architectures and training procedures to identify when the method is most effective
3. Compare against other state-of-the-art L2O methods that incorporate different forms of prior knowledge or structural assumptions about the optimization landscape