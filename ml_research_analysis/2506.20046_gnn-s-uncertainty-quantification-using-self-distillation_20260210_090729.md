---
ver: rpa2
title: GNN's Uncertainty Quantification using Self-Distillation
arxiv_id: '2506.20046'
source_url: https://arxiv.org/abs/2506.20046
tags:
- uncertainty
- ensemble
- teacher
- self-distillation
- classifier
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of quantifying predictive uncertainty
  in Graph Neural Networks (GNNs) for healthcare applications, where trustworthy predictions
  are critical. Existing methods like Bayesian inference and ensemble approaches are
  computationally expensive or lack precision in capturing model diversity.
---

# GNN's Uncertainty Quantification using Self-Distillation

## Quick Facts
- **arXiv ID:** 2506.20046
- **Source URL:** https://arxiv.org/abs/2506.20046
- **Reference count:** 34
- **Primary result:** A self-distillation-based approach for quantifying predictive uncertainty in GNNs, reducing training/inference time compared to ensembles while achieving comparable performance.

## Executive Summary
This paper introduces a novel method for uncertainty quantification (UQ) in Graph Neural Networks (GNNs) using self-distillation. The approach trains multiple classifier heads within a single GNN architecture, with the deepest classifier serving as the "teacher" and shallower ones as "students." By computing a weighted Jensen-Shannon Divergence between student and teacher predictions, the method generates an enhanced uncertainty metric. Experiments on MIMIC-IV and Enzymes datasets demonstrate performance comparable to MC Dropout and ensemble methods, while significantly reducing computational overhead. The method also shows promise in distinguishing in-distribution from out-of-distribution data.

## Method Summary
The core innovation is self-distillation within a GNN architecture. Multiple classifier heads are attached after each hidden layer of the GNN, with the deepest head serving as the teacher. During training, a composite loss function combines cross-entropy, KL divergence (for distillation), and L2 feature penalties. The final 20 epochs disable distillation for the teacher to aid convergence. At inference, predictions from all classifier heads are collected, and a weighted Jensen-Shannon Divergence between student and teacher predictions generates the final uncertainty score. This approach avoids the computational expense of training multiple separate models while maintaining diversity through the multiple heads.

## Key Results
- Achieves F1-scores and ROC-AUC comparable to MC Dropout and ensemble methods on MIMIC-IV and Enzymes datasets
- Reduces training and inference time by avoiding separate model training required by ensemble approaches
- Effectively distinguishes in-distribution from out-of-distribution data using the enhanced uncertainty metric

## Why This Works (Mechanism)
The method works by creating diversity within a single model through multiple classifier heads at different depths. The teacher (deepest classifier) provides a strong prediction target, while students (shallower classifiers) learn to approximate it through distillation. The weighted Jensen-Shannon Divergence captures disagreement between students and the teacher, with weights based on classifier depth. This disagreement serves as a proxy for epistemic uncertainty—the model's uncertainty about its predictions. By aggregating these weighted divergences, the method produces a single uncertainty score that reflects both the depth of the classifier and its confidence relative to the teacher.

## Foundational Learning

- **Concept: Knowledge Distillation & Self-Distillation**
  - Why needed here: This is the core architectural choice. Understanding that a large "teacher" model's knowledge can be transferred to a smaller "student" model via soft labels (KL divergence) is essential. Self-distillation applies this *within* a single model, using deeper layers as teachers for shallower ones.
  - Quick check question: Explain the role of the KL divergence term in the loss function and why α is set to zero for the final (teacher) layer.

- **Concept: Uncertainty in GNNs: Aleatoric vs. Epistemic**
  - Why needed here: The paper targets predictive uncertainty. Knowing that *aleatoric* is data noise and *epistemic* is model uncertainty (which ensembles and this method aim to capture) is crucial for interpreting the results and the motivation.
  - Quick check question: How does this method's disagreement metric aim to capture epistemic uncertainty?

- **Concept: Divergence Measures for Probability Distributions**
  - Why needed here: The proposed uncertainty metric is built on a divergence measure. Understanding why KL divergence is unbounded and asymmetric, and why Jensen-Shannon Divergence (JSD) is a preferred alternative for a bounded, symmetric score is critical for understanding the final formula.
  - Quick check question: Why does the paper choose Jensen-Shannon Divergence over the standard KL divergence for its final uncertainty metric?

## Architecture Onboarding

- **Component map:** GNN -> Classifier Head 1 -> Classifier Head 2 -> ... -> Classifier Head N (Teacher)
- **Critical path:** 1) **Model Definition:** Add classifier heads after each GNN layer. 2) **Training Loop:** Implement the composite loss function, correctly setting α and λ for teacher vs. student layers. 3) **Inference:** For each sample, get predictions from all heads. 4) **Uncertainty Calculation:** For each student head, check if its predicted class differs from the teacher's. If so, apply the weight function. Compute JSD between student and teacher predictions. Sum weighted JSDs for the final score.

- **Design tradeoffs:**
    - **Performance vs. Efficiency:** The method trades the superior uncertainty of a full ensemble for the efficiency of a single model's forward pass.
    - **Simplicity vs. Precision:** The linear weight function is simpler but may be less expressive than the nonlinear option.
    - **Alpha (α) Parameter:** Higher α means stronger distillation (students mimic the teacher more). Too high may reduce beneficial diversity for OOD detection; too low may mean students don't learn effectively.

- **Failure signatures:**
    - **Teacher Collapse:** If the teacher classifier is underperforming, all students will learn to mimic its mistakes, providing useless uncertainty estimates.
    - **Student-Teacher Agreement on OOD:** If the OOD data is too similar to ID data, the teacher and all students may confidently agree on a single (incorrect) class, yielding a low uncertainty score, which is a failure of OOD detection.
    - **Over-regularization:** Strong feature penalty (λ) could suppress useful variations in the hidden layers, hurting overall model performance.

- **First 3 experiments:**
  1. **Baseline Implementation:** Implement the self-distillation GNN on a simple dataset (like Enzymes). Reproduce the F1-score and compare training time against a standard GNN and a full ensemble to validate the core efficiency claim.
  2. **Metric Ablation:** Replace the weighted JSD metric in the paper with the standard disagreement metric from ensemble literature. Evaluate on the same OOD data to see if the proposed metric's "precision" advantage is empirically visible (e.g., does it better separate ID/OOD entropy plots?).
  3. **OOD Detection Test:** Train the model on the MIMIC-IV dataset with the "Admitted to ICU" class excluded (making it OOD). At test time, introduce the ICU samples and measure the separation in the uncertainty score distribution between ID and OOD samples.

## Open Questions the Paper Calls Out

- **Open Question 1**
  - Question: How does the choice between linear and non-linear weight functions impact the precision of uncertainty quantification across different domains?
  - Basis: [explicit] Section 3.3 states that while a non-linear alternative is introduced, "understanding the impact requires further empirical investigation."
  - Why unresolved: The paper utilizes a linear weight function by default and acknowledges that the non-linear option's behavior is not yet verified.
  - What evidence would resolve it: A comparative ablation study on multiple datasets measuring calibration error and OOD detection recall for both weight functions.

- **Open Question 2**
  - Question: Can uncertainty quantification performance be improved by using alternative teacher selection methods, such as ensemble or transitive distillation, rather than the deepest classifier?
  - Basis: [explicit] Section 5 lists exploring "alternative teacher selection methods" as a specific future direction.
  - Why unresolved: The current implementation strictly designates the deepest classifier as the teacher, leaving other architectural configurations unexplored.
  - What evidence would resolve it: Experimental results comparing the current deepest-layer teacher against ensemble or dense teacher setups using the proposed uncertainty metric.

- **Open Question 3**
  - Question: What is the sensitivity of the self-distillation process to the imitation (α) and trade-off (λ) parameters?
  - Basis: [explicit] Section 4.1 notes that limited tuning was performed and "a more in-depth ablation study is necessary."
  - Why unresolved: The specific values used (0.6 and 0.04) were derived from limited tuning, and their generalizability to other GNN architectures is unconfirmed.
  - What evidence would resolve it: A sensitivity analysis tracking convergence speed and uncertainty precision across a wide range of values for α and λ.

## Limitations
- The MLP architecture for classifier heads and total training epochs are unspecified, affecting reproducibility
- The claimed "precision" improvement over standard disagreement metrics is demonstrated only qualitatively
- Limited hyperparameter ablation for α and λ, which control critical aspects of the distillation process
- No formal OOD detection metric (e.g., AUROC) is reported, despite OOD detection being a key claim

## Confidence

- **High confidence:** Efficiency claims are well-supported by the inherent design advantage of training a single model versus multiple separate models
- **Medium confidence:** Uncertainty quantification performance is comparable to baselines but lacks rigorous OOD detection validation
- **Medium confidence:** The enhanced uncertainty metric's "precision" advantage needs quantitative validation against standard metrics

## Next Checks

1. **Ablation Study:** Implement and evaluate the method using both linear and nonlinear weight functions for classifier depth. Quantify the impact on OOD detection performance.
2. **Hyperparameter Sensitivity:** Systematically vary α (distillation strength) and λ (feature penalty) to determine their optimal ranges and the resulting trade-off between performance and uncertainty quality.
3. **Baseline Comparison:** Replace the proposed Jensen-Shannon Divergence metric with the standard ensemble disagreement metric on the same OOD test set. Measure if the proposed metric's claimed "precision" advantage is empirically significant.