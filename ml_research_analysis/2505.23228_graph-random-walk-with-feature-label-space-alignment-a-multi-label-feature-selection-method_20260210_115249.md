---
ver: rpa2
title: 'Graph Random Walk with Feature-Label Space Alignment: A Multi-Label Feature
  Selection Method'
arxiv_id: '2505.23228'
source_url: https://arxiv.org/abs/2505.23228
tags:
- feature
- selection
- features
- labels
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes GRW-SCMF, a multi-label feature selection method
  that addresses the challenge of capturing complex nonlinear associations between
  features and labels in high-dimensional data. The method constructs a random walk
  graph integrating feature-feature, label-label, and feature-label relationships
  to capture both direct and implicit indirect associations.
---

# Graph Random Walk with Feature-Label Space Alignment: A Multi-Label Feature Selection Method

## Quick Facts
- arXiv ID: 2505.23228
- Source URL: https://arxiv.org/abs/2505.23228
- Reference count: 16
- One-line primary result: Combines random walk graph with structured correlation matrix factorization and manifold alignment to capture nonlinear feature-label associations, outperforming 7 baselines across 7 datasets on multiple evaluation metrics.

## Executive Summary
This paper introduces GRW-SCMF, a multi-label feature selection method that addresses the challenge of capturing complex nonlinear associations between features and labels in high-dimensional data. The method constructs a random walk graph integrating feature-feature, label-label, and feature-label relationships to capture both direct and implicit indirect associations. It then aligns the variable spaces using low-dimensional representation coefficients while preserving the manifold structure between the original high-dimensional data and the low-dimensional representation space. Experiments on seven benchmark datasets demonstrate the method's superiority, achieving significant improvements in classification performance across multiple evaluation metrics including Micro-F1, Macro-F1, Hamming Loss, and Zero-One Loss when compared to seven state-of-the-art multi-label feature selection methods.

## Method Summary
GRW-SCMF addresses multi-label feature selection by constructing a composite graph that integrates feature-feature, label-label, and feature-label relationships, then performing random walks to capture both direct and implicit indirect associations. The method uses non-negative matrix factorization to learn low-dimensional representations of features and labels, incorporating a manifold alignment constraint to preserve the geometric structure between the original high-dimensional data and the low-dimensional representation space. The optimization alternates between updating the latent matrices V, Q, and B while minimizing reconstruction error, aligning the feature and label spaces, and ensuring consistency with the random walk-derived association matrix. Features are ranked based on the row norms of the product Q^T B.

## Key Results
- Achieved significant improvements in Micro-F1 and Macro-F1 scores across seven benchmark datasets compared to seven state-of-the-art multi-label feature selection methods
- Demonstrated effectiveness in reducing Hamming Loss and Zero-One Loss when using the top 20% selected features for classification
- Ablation studies showed that both the random walk component and manifold alignment constraint contribute to performance improvements
- Convergence analysis confirmed stable optimization with appropriate hyperparameter settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A random walk over a feature-label composite graph can capture both direct and implicit indirect associations between features and labels, which linear decomposition methods may miss.
- Mechanism: A composite graph G=(V, E) is constructed where V includes feature nodes V_f and label nodes V_l, and E includes feature-feature, label-label, and feature-label edges. A random walk traverses this graph, and for any feature-label pair (f, l) encountered, their association weight RW(f, l) is updated. This weight is incremented by decay_factor^{d(f,l)} · MI(f, l), where d(f,l) is the distance (steps) in the walk sequence. This process propagates relevance through intermediate nodes (other features and labels), thereby modeling high-order, indirect relationships. The final RW matrix, scaled to [0, 1], represents these captured associations.
- Core assumption: The relationships between features and labels in high-dimensional multi-label data include significant indirect dependencies that are not merely linear and can be discovered by propagation through a graph structure.
- Evidence anchors:
  - [abstract]: "First, we design a random walk graph that integrates feature-feature, label-label, and feature-label relationships to accurately capture nonlinear and implicit indirect associations..."
  - [section]: Page 3, "Random Walk Graph" and "Random Walk Method" sections describe the graph construction and walk rules. Page 3, "Feature-Label Relationship Update Rule" section defines the RW matrix update. "Implicit indirect relationships refer to the potential high-order associations... transmitted through intermediate nodes... This process constructs a feature-label association matrix that represents high-order associations..."
  - [corpus]: This specific mechanism of a feature-label composite graph for random walk is not directly corroborated by the provided corpus neighbor abstracts, which focus on attention networks, binary hashing, or federated learning for multi-label feature selection.
- Break condition: The mechanism would likely fail if the graph structure is extremely sparse (few edges to propagate information) or excessively dense (random walks become undifferentiated noise), or if the decay factor is not tuned correctly, causing either too much emphasis on distant, weak associations or insufficient propagation.

### Mechanism 2
- Claim: Enforcing a constraint that aligns the learned low-dimensional representations of features and labels in a shared latent space improves feature selection by preserving manifold structure.
- Mechanism: The objective function (Eq. 13) includes an alignment term δ||XQ^T - YB^T||^2_F. Here, XQ^T represents the low-dimensional mapping of features, and YB^T represents the low-dimensional mapping of labels, both projected into the shared semantic space defined by latent matrix V. Minimizing this term forces similar samples in the feature space and label space to be mapped to nearby locations in the shared low-dimensional space. This ensures structural consistency between the two domains and that the learned relationships are faithful to the original data's manifold.
- Core assumption: High-dimensional feature and label data lie on low-dimensional manifolds, and a meaningful feature selection requires a shared latent space that preserves the geometric relationships (manifold structure) of the original data.
- Evidence anchors:
  - [abstract]: "Second, we align the variable spaces by leveraging low-dimensional representation coefficients, while preserving the manifold structure between the original high-dimensional multi-label data and the low-dimensional representation space."
  - [section]: Page 4, the objective function formulation discussion. "This term can be regarded as an alignment constraint, aiming to ensure the consistency between the feature space and the label space in the low-dimensional representation."
  - [corpus]: The paper "Reconsidering Feature Structure Information and Latent Space Alignment in Partial Multi-label Feature Selection" (arXiv:2503.10115) explicitly mentions "Latent Space Alignment," providing strong corroborating evidence that this is a recognized and relevant mechanism in the domain.
- Break condition: The assumption may break if the data does not adhere to a simple low-dimensional manifold, or if the dimensionality k of the latent space V is chosen too small to capture the necessary structure, leading to a loss of critical information during the alignment.

### Mechanism 3
- Claim: Combining the explicit association matrix from random walks with the learned association matrix from low-rank decomposition yields a more robust feature-label representation.
- Mechanism: The objective function (Eq. 10) includes a term γ||Rw - Q^T B||^2_F. Rw is the association matrix from the random walk process (capturing implicit/nonlinear relations), while Q^T B is the feature-label association matrix reconstructed from the low-dimensional latent space (capturing linear/direct relations). This term acts as a regularizer, forcing the low-rank decomposition to account for the complex, high-order associations discovered by the random walk. It blends the strengths of both nonlinear graph-based discovery and linear factorization.
- Core assumption: A superior feature-label representation can be achieved by synergistically combining the complementary information from graph-based nonlinear exploration and matrix-factorization-based linear embedding.
- Evidence anchors:
  - [abstract]: "...while optimizing the latent representations of associations between features and labels after low-rank decomposition."
  - [section]: Page 4, Eq. 10 and its explanation. "By combining the explicit modeling capability of linear decomposition with the implicit relationship mining power of random walks, this term significantly improves the model's ability to represent high-order nonlinear relationships..."
  - [corpus]: No direct evidence in the corpus for this specific combination of a random walk matrix with matrix factorization for multi-label feature selection. Related work like "Multi-label feature selection based on binary hashing learning and dynamic graph constraints" (arXiv:2503.13874) combines learning and graph constraints, but the specific mechanisms differ.
- Break condition: The mechanism's success relies on the quality of the Rw matrix. If the random walk parameters (e.g., decay factor, walk length) are poorly chosen, the Rw matrix will be noisy or uninformative. Forcing the learned representation to match this flawed matrix would then degrade performance.

## Foundational Learning

- Concept: **Non-negative Matrix Factorization (NMF)**
  - Why needed here: The method uses NMF to decompose the feature matrix X and label matrix Y into low-dimensional latent spaces (V, Q, B). Understanding NMF is essential to grasp how the model learns compact representations and how the alignment and random walk constraints are applied within this framework.
  - Quick check question: What is the primary difference between NMF and Principal Component Analysis (PCA) regarding the values in the resulting matrices?

- Concept: **Random Walk on Graphs**
  - Why needed here: A core contribution of the paper is the random walk strategy on a composite feature-label graph. Understanding transition probabilities, the role of a decay factor, and how a walk propagates information across edges is crucial for implementing and tuning the RWMI algorithm.
  - Quick check question: In a random walk, what does the "transition probability" from one node to another represent, and how is it typically calculated in a simple unweighted graph?

- Concept: **Manifold Learning**
  - Why needed here: The method's second key contribution is aligning the feature and label spaces while preserving their "manifold structure." This concept is foundational for understanding the alignment term in the objective function and why it's assumed to improve feature selection.
  - Quick check question: What is the core assumption of manifold learning regarding the structure of high-dimensional data?

## Architecture Onboarding

- Component map:
  1. **Graph Constructor**: Builds the composite graph G with nodes for features and labels, and edges for feature-feature, label-label, and feature-label relationships using Gaussian kernel and Mutual Information
  2. **RWMI Engine**: Performs random walks on the graph and generates the feature-label association matrix Rw by accumulating weighted mutual information based on walk distances
  3. **Matrix Factorization Core**: Performs non-negative matrix factorization on X and Y into latent matrices V, Q, B
  4. **Optimization Engine**: Solves the objective function (Eq. 13) by alternately updating V, Q, B to minimize reconstruction error, align spaces, and match the Rw matrix

- Critical path:
  1. Compute adjacency matrices A_features, A_labels, and mutual information matrix MI
  2. Run the RWMI algorithm (Algorithm 1) to generate the final Rw matrix
  3. Initialize matrices V, Q, B for factorization
  4. Iteratively update V, Q, B using the update rules in Eq. 20-22 until convergence
  5. Rank features based on the row norms of the product Q^T B

- Design tradeoffs:
  - **Random Walk Parameters**: There's a tradeoff between the computational cost of longer/more numerous walks and the quality of the captured indirect associations. The parameters n_walks, walk_length, jump_prob, and decay_factor must be tuned
  - **Latent Dimensionality (k)**: Choosing a smaller k speeds up computation but may fail to capture complex label correlations, while a larger k increases the risk of overfitting and computational cost
  - **Regularization Weights (α, β, γ, δ, ε)**: The balance between reconstruction accuracy, alignment, random walk consistency, and sparsity is controlled by these hyperparameters. The paper uses Bayesian optimization to find them

- Failure signatures:
  - **Non-convergence**: The objective function is non-convex. If initial learning rates or regularization weights are poorly set, the optimization may not converge. Check the convergence plots (like Fig. 5)
  - **Poor Rw Quality**: If the Rw matrix is uninformative (e.g., all zeros due to incorrect walk parameters), the alignment term will not provide useful guidance. Verify the distribution of values in Rw
  - **Noisy Feature Ranking**: If the final feature rankings from ||(Q^T B)_i||_2 are unstable across runs, it may indicate an issue with the optimization or hyperparameter settings

- First 3 experiments:
  1. **Ablation on RW Component**: Run the model with and without the random walk term (||Rw − Q^T B||^2_F) to quantify its contribution to performance (Micro-F1, Macro-F1) as shown in Table 4
  2. **Convergence Test**: Train the model on a dataset (e.g., Arts) and plot the objective function value over iterations to verify it converges smoothly, similar to the analysis in Figure 5
  3. **Parameter Sensitivity Analysis**: Vary the five key hyperparameters (α, β, γ, δ, ε) one at a time while keeping others fixed and plot the resulting Micro-F1 scores to understand their sensitivity, as done in Figure 4

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does GRW-SCMF compare to deep learning-based multi-label feature selection methods, such as Graph Neural Networks (GNNs) or autoencoders, in capturing complex nonlinear associations?
- Basis in paper: [inferred] The paper explicitly critiques linear decomposition for failing to capture nonlinear associations and claims to solve this using random walks. However, the experiments restrict comparisons to traditional linear, statistical, and matrix factorization methods, omitting deep learning baselines that are standard for modeling nonlinearity.
- Why unresolved: It is unclear if the proposed random walk strategy offers a superior or complementary mechanism for capturing non-linearity compared to the hierarchical feature learning inherent in deep neural networks.
- What evidence would resolve it: Comparative experiments on the benchmark datasets against state-of-the-art deep learning-based multi-label feature selection methods.

### Open Question 2
- Question: Can the computational efficiency of GRW-SCMF be maintained when applied to large-scale datasets with high dimensionality and massive sample sizes?
- Basis in paper: [inferred] The methodology involves constructing a composite graph using Gaussian kernels and mutual information, followed by iterative random walks and alternating optimization updates. These steps typically scale poorly (often O(n^2) or worse) with sample size and feature dimension.
- Why unresolved: The paper provides convergence curves but lacks a formal time complexity analysis or experiments on large-scale industrial datasets, leaving its scalability unverified.
- What evidence would resolve it: A theoretical complexity analysis and experimental validation on datasets with significantly larger feature sets and sample counts than those used in Table 1.

### Open Question 3
- Question: Is it possible to develop an adaptive parameter selection mechanism to reduce the model's dependency on computationally expensive Bayesian optimization?
- Basis in paper: [inferred] The "Parameter Selection" section notes that the method requires tuning 5 regularization parameters and 4 random walk parameters (e.g., walk length, jump probability) via grid search or Bayesian optimization.
- Why unresolved: Relying on external optimization for 9 hyperparameters limits the method's practical applicability and stability, as optimal parameters likely vary significantly across different data domains.
- What evidence would resolve it: A study demonstrating robust performance using adaptive parameter settings or a reduced set of sensitive hyperparameters across the seven benchmark datasets.

## Limitations

- The method requires tuning 9 hyperparameters (5 regularization weights and 4 random walk parameters) via computationally expensive Bayesian optimization
- The non-convex optimization landscape introduces sensitivity to initialization and may converge to suboptimal solutions
- The quality of the RWMI matrix depends heavily on the accuracy of mutual information estimates, which can be noisy for high-dimensional continuous features
- The scalability to large-scale datasets with high dimensionality and massive sample sizes remains unverified

## Confidence

- **High Confidence**: The experimental results showing superior performance over seven baselines across seven datasets are well-documented and reproducible
- **Medium Confidence**: The theoretical formulation combining random walk, NMF, and manifold alignment is sound, but the effectiveness depends heavily on hyperparameter tuning
- **Medium Confidence**: The claim that capturing implicit indirect associations improves feature selection is plausible given the mechanism, but the corpus provides limited direct evidence for this specific approach

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically vary the five key regularization weights (α, β, γ, δ, ε) across a grid of values on a validation set to identify stable regions and quantify sensitivity, as suggested by Figure 4

2. **RWMI Matrix Quality Assessment**: Before running the full optimization, inspect the distribution and sparsity patterns of the Rw matrix for different walk parameters (n_walks, walk_length, decay_factor) on a sample dataset to ensure it captures meaningful associations

3. **Ablation Study Replication**: Implement and run the full GRW-SCMF pipeline alongside ablated versions (without RW term, without alignment term) on at least two datasets (e.g., Yeast and Emotions) to verify the claimed performance improvements in Table 4