---
ver: rpa2
title: Iterative Training of Physics-Informed Neural Networks with Fourier-enhanced
  Features
arxiv_id: '2510.19399'
source_url: https://arxiv.org/abs/2510.19399
tags:
- solution
- neural
- equation
- problem
- basis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces IFeF-PINN, a novel iterative training algorithm\
  \ for physics-informed neural networks that addresses spectral bias\u2014the tendency\
  \ of PINNs to struggle with high-frequency components. The method enriches the latent\
  \ space using Random Fourier Features (RFF) through a two-stage process: generating\
  \ a nominal basis with hidden layers and performing linear regression on RFF-extended\
  \ basis functions."
---

# Iterative Training of Physics-Informed Neural Networks with Fourier-enhanced Features

## Quick Facts
- arXiv ID: 2510.19399
- Source URL: https://arxiv.org/abs/2510.19399
- Reference count: 40
- Primary result: IFeF-PINN achieves relative L2 errors as low as 3.5×10⁻⁵ on Helmholtz equations and 4.3×10⁻⁵ on convection equations by mitigating spectral bias through Random Fourier Feature-enhanced latent spaces

## Executive Summary
This paper introduces IFeF-PINN, an iterative training algorithm for physics-informed neural networks that addresses the fundamental challenge of spectral bias—the tendency of standard PINNs to struggle with high-frequency components. The method enriches the latent space using Random Fourier Features (RFF) through a two-stage process: generating a nominal basis with hidden layers and performing linear regression on RFF-extended basis functions. For linear PDEs, this approach guarantees convexity and convergence, while empirically improving approximation of high-frequency PDEs. Extensive numerical experiments on benchmark problems demonstrate superior performance over state-of-the-art methods.

## Method Summary
IFeF-PINN uses a bi-level optimization framework where the upper level learns a feature extractor h_ω and the lower level solves a convex regression problem for coefficients θ. The method starts with a warm-start (typically a trained vanilla PINN) to obtain an initial feature basis, then extends this basis using Random Fourier Features γ_D(h_ω). For linear PDEs, the lower-level problem is solved via a closed-form Tikhonov-regularized inverse, while the upper level is updated using hypergradient descent computed via implicit function theorem. The algorithm iterates between these two levels until convergence, with each iteration refining the latent space to better capture high-frequency components.

## Key Results
- Achieves relative L2 errors as low as 3.5×10⁻⁵ on Helmholtz equations and 4.3×10⁻⁵ on convection equations
- Successfully captures high-frequency and multi-scale dynamics that standard PINNs fail to resolve
- Demonstrates superior performance over state-of-the-art methods including Gabor-Enhanced PINNs and S-DGM on benchmark problems
- Spectrum analysis shows the method can fit high-frequency signals even without iterative training, whereas vanilla PINNs fail

## Why This Works (Mechanism)

### Mechanism 1: RFF-Enhanced Latent Space
Injecting Random Fourier Features into the latent space mitigates spectral bias by explicitly encoding high-frequency components that standard networks struggle to learn. The RFF map γ_D(h_ω) transforms the latent features into a higher-dimensional space where high-frequency modes are linearly separable, effectively upgrading the implicit kernel to a stationary RBF-like kernel. This works when the matrix B_D is sampled with appropriate variance σ aligned with the true solution's frequency.

### Mechanism 2: Decoupled Convex Optimization
Decoupling basis learning from coefficient fitting allows for a convex regression step (for linear PDEs), guaranteeing a global optimum for the readout layer. The loss function becomes quadratic with respect to θ, enabling solution via a single linear system solve rather than iterative gradient descent. This requires the PDE operators to be linear and sufficient sampling density to satisfy the rank condition N_u + N_f ≥ 2D.

### Mechanism 3: Iterative Basis Refinement
Iteratively updating the feature extractor h_ω based on the optimal readout θ* ensures the latent space adapts to residuals. The algorithm alternates between solving for θ and taking a gradient step for ω using hypergradients computed via the Implicit Function Theorem. This refinement continues until the basis functions are optimized to minimize the PDE residual that the current Fourier basis cannot yet explain.

## Foundational Learning

- **Concept: Spectral Bias (F-principle)**
  - Why needed: This is the core failure mode IFeF-PINN is designed to fix. Without understanding that standard neural networks prioritize low frequencies, the motivation for RFF extension is lost.
  - Quick check: Why does a standard MLP fail to fit a high-frequency sine wave (sin(100x)) even when it has enough neurons to theoretically represent it?

- **Concept: Bi-level Optimization**
  - Why needed: The training algorithm is not standard end-to-end backpropagation. It relies on a hierarchy where one optimization problem is nested inside another.
  - Quick check: In IFeF-PINN, which parameters are updated via a convex solve, and which are updated via gradient descent?

- **Concept: Random Fourier Features (RFF)**
  - Why needed: The "Fourier-enhanced" component of the name comes from this technique. Understanding that RFF approximates a stationary kernel (like an RBF kernel) explains why it unlocks high-frequency learning.
  - Quick check: What is the role of the matrix B_D in the architecture, and is it learned during training?

## Architecture Onboarding

- **Component map:** Input x → Hidden trunk h_ω → RFF block γ_D(h_ω) → Output head θ → Prediction
- **Critical path:** Algorithm 1: Initialize ω via Vanilla PINN pre-training → Loop: Compute basis ψ_D = γ_D(h_ω) → Lower Update: Solve QP for θ* → Upper Update: Compute hypergradient and step ω → Return final θ, ω
- **Design tradeoffs:** Sampling vs. Rank (need N_u + N_f ≥ 2D collocation points), Linearity vs. Convexity (global optimum only for linear PDEs), Feature dimension vs. computational overhead
- **Failure signatures:** Aliased Solutions (occurs if N_u + N_f < 2D), Non-convergence (occurs in nonlinear PDEs if updates are too aggressive)
- **First 3 experiments:**
  1. Low-freq 1D Convection: Implement warm-start + 1-step lower solve to verify RFF extension works (target error ≈ 10⁻⁵)
  2. High-freq Helmholtz: Test full iterative loop on linear high-frequency PDE (a=100) to validate convex solver's ability to capture frequencies
  3. Rank Condition Ablation: Run high-freq problem with D=800 features but reduce collocation points to N < 1600 to observe aliasing failure mode

## Open Questions the Paper Calls Out

### Open Question 1: Nonlinear PDE Convergence
How can the bi-level optimization framework be modified to guarantee global optimality or convergence for nonlinear PDEs where the lower-level problem is nonconvex? The current convergence guarantees rely on μ-strong convexity, which only holds for linear PDEs. A modified loss landscape or optimization algorithm that restores convexity for specific nonlinear operators could resolve this.

### Open Question 2: Sampling Robustness
How can the method's sensitivity to collocation resampling be mitigated to improve robustness? The current approach depends strongly on minima found on a fixed dataset, and changing collocation points disrupts the stability of the learned basis. A theoretical analysis of sampling stability or adaptive resampling scheme could help.

### Open Question 3: Optimal σ Selection
What is the optimal strategy for selecting the Random Fourier Feature standard deviation σ relative to the frequency content of the target PDE solution? While σ=1 was used for all experiments, the authors note that σ should theoretically align with the target function's frequency. A sensitivity analysis correlating σ with spectral bias on multi-scale problems could provide guidance.

## Limitations

- Theoretical guarantees (convergence, global optimality) strictly apply only to linear PDEs; nonlinear cases rely on empirical validation
- Hyperparameter sensitivity requires careful tuning of RFF dimension D, frequency scale σ, and regularization γ
- Sampling requirements (N_u + N_f ≥ 2D) may be prohibitive for high-dimensional problems where D grows exponentially
- Computational overhead increases training time compared to standard PINNs due to linear system solves and hypergradient computation

## Confidence

- **High confidence**: Linear PDE convergence guarantees, convex lower-level optimization for linear problems, basic RFF mechanism for spectral bias mitigation
- **Medium confidence**: Empirical performance on nonlinear PDEs, generalization to multi-scale dynamics, comparison with state-of-the-art methods
- **Low confidence**: Claims about superiority over all alternative spectral bias mitigation techniques (limited ablation studies)

## Next Checks

1. **Linear PDE stress test**: Apply IFeF-PINN to a challenging linear high-frequency problem (e.g., 2D Helmholtz with a=200) and verify whether the theoretical convergence rate matches empirical performance

2. **Nonlinear PDE convergence analysis**: Track the sequence {ω_i} for Burgers' equation to empirically assess whether iterates approach a stationary point despite the lack of strong convexity

3. **Feature dimension sensitivity**: Systematically vary D from 200 to 2000 on Helmholtz problems to quantify the tradeoff between approximation quality and sampling requirements, explicitly testing the rank condition boundary