---
ver: rpa2
title: Transferable Adversarial Attacks on Black-Box Vision-Language Models
arxiv_id: '2505.01050'
source_url: https://arxiv.org/abs/2505.01050
tags:
- image
- arxiv
- attack
- adversarial
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive analysis of adversarial attacks
  on Vision-Language Models (VLLMs), demonstrating that targeted adversarial examples
  can effectively transfer to widely-used proprietary models like GPT-4o, Claude,
  and Gemini. The authors develop a method to craft perturbations that induce specific
  attacker-chosen misinterpretations of visual information, such as misclassifying
  hazardous content as safe or overlooking sensitive material.
---

# Transferable Adversarial Attacks on Black-Box Vision-Language Models

## Quick Facts
- **arXiv ID**: 2505.01050
- **Source URL**: https://arxiv.org/abs/2505.01050
- **Reference count**: 24
- **Primary result**: Adversarial examples crafted using open-source models can successfully attack proprietary VLLMs like GPT-4o, Claude, and Gemini with >90% targeted attack success rates.

## Executive Summary
This paper demonstrates that targeted adversarial attacks can transfer from open-source surrogate models to black-box proprietary Vision-Language Models (VLLMs). The authors develop a method using an ensemble of diverse surrogate models, visual contrastive loss, and data augmentation to craft perturbations that induce specific misinterpretations of visual content. These perturbations achieve high success rates across multiple state-of-the-art VLLMs including GPT-4o, Claude, and Gemini, with attacks exceeding 90% success for targeted object misclassification. The work reveals a critical vulnerability in current VLLM deployments and calls for urgent development of robust defenses.

## Method Summary
The attack uses an ensemble of open-source surrogate models (8 CLIP variants, 4 VLLMs, and 2 visual-only models) to craft adversarial perturbations through visual contrastive loss optimization. The method employs data augmentation during optimization to enhance robustness against unknown preprocessing, and applies perturbation moving average and regularization techniques (DropPath, PatchDrop) to improve transferability. Perturbations are bounded by ℓ∞ constraints and evaluated on proprietary VLLMs using prompt-based tasks including object recognition, visual question answering, and image captioning. The attack achieves targeted success by making VLLMs misclassify specific visual content according to attacker-defined categories.

## Key Results
- Targeted adversarial examples transfer successfully to GPT-4o, Claude, and Gemini with >90% attack success rates
- Universal perturbations can consistently induce misinterpretations across multiple proprietary VLLMs
- Visual contrastive loss with N=50 positive/negative examples and K=10 top-K selection outperforms image-text similarity approaches
- Data augmentation and regularization techniques significantly improve cross-model transferability
- Attack effectiveness varies by task, with object recognition being most vulnerable and text recognition most robust

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Optimizing perturbations across an ensemble of diverse surrogate models improves transferability to black-box VLLMs.
- **Mechanism**: Aggregating gradients from multiple models prevents overfitting to any single model's idiosyncratic vulnerabilities, yielding perturbations that exploit shared weaknesses across the visual encoder landscape.
- **Core assumption**: Proprietary VLLMs share similar visual representations with open-source CLIP-based encoders (most VLLMs initialize from CLIP).
- **Evidence anchors**:
  - [abstract]: "The attack leverages an ensemble of open-source models... to enhance transferability"
  - [Section 3.2, Table 3]: 8 CLIP models achieve 94.4% ASR on GPT-4o vs 85.1% with 3 CLIP models
  - [corpus]: Related work (FMR~0.55-0.62) confirms transferability as established phenomenon in vision domain
- **Break condition**: If black-box VLLM uses a fundamentally different visual encoder architecture with incompatible feature spaces.

### Mechanism 2
- **Claim**: Visual contrastive loss using image-image similarity outperforms image-text similarity for cross-model transfer.
- **Mechanism**: By relying only on visual encoder alignment (not both visual and textual), the perturbation needs to match fewer embedding space characteristics. Multiple positive/negative examples provide robust semantic boundaries.
- **Core assumption**: Visual embedding spaces are more aligned across models than the joint image-text spaces.
- **Evidence anchors**:
  - [Section 3.3, Eq. 7]: Loss defined using Ev (visual encoder only), not text encoder
  - [Table 5]: Visual examples (N=50, K=10) achieve 94.4% ASR vs textual examples at 82.0% on GPT-4o
  - [corpus]: Limited direct evidence on visual-only vs image-text loss comparison
- **Break condition**: If target model's visual encoder produces embeddings on entirely different scale or semantics.

### Mechanism 3
- **Claim**: Data augmentation during optimization produces perturbations robust to unknown preprocessing pipelines in black-box models.
- **Mechanism**: Random crop/pad/resize, Gaussian noise, and JPEG compression force the optimizer toward flat minima where perturbations remain effective under transformations the black-box may apply.
- **Core assumption**: Black-box VLLMs apply standard preprocessing operations (resize, crop, possibly JPEG compression).
- **Evidence anchors**:
  - [Section 3.4, Algorithm 1]: Complete augmentation pipeline with probability thresholds
  - [Table 5]: Removing DropPath drops Claude 3.5 ASR from 58.7% to 42.4%
  - [corpus]: General transfer attack literature supports augmentation as effective technique
- **Break condition**: If target model applies preprocessing not covered by augmentation (e.g., aggressive denoising, different color normalization).

## Foundational Learning

- **Concept: Transfer-based adversarial attacks**
  - Why needed here: The entire method depends on crafting attacks on accessible models that transfer to inaccessible ones.
  - Quick check question: Can you explain why perturbations optimized on one model might fool another model it was never trained on?

- **Concept: CLIP vision-language embedding spaces**
  - Why needed here: Surrogate models are predominantly CLIP-based; understanding how images map to embeddings is essential.
  - Quick check question: What does cosine similarity between image and text embeddings represent in CLIP?

- **Concept: ℓ∞-norm bounded perturbations**
  - Why needed here: Attacks constrain ∥δ∥∞ ≤ ε to keep perturbations imperceptible.
  - Quick check question: Why is ℓ∞ constraint more restrictive for visual imperceptibility than ℓ2?

## Architecture Onboarding

- **Component map**:
  Surrogate ensemble (8 CLIP variants, 4 open VLLMs, 2 visual-only, 2 adversarially trained) -> Visual contrastive loss optimization -> Data augmentation pipeline -> Perturbation moving average -> Regularization (DropPath, PatchDrop) -> Black-box VLLM victim models

- **Critical path**:
  1. Collect positive/negative example images for target category
  2. Initialize perturbation δ = 0
  3. For each optimization step: apply augmentation → forward through surrogate ensemble → compute visual contrastive loss → aggregate gradients → update δ with projected gradient descent
  4. Apply perturbation moving average for final output
  5. Apply perturbed image to black-box VLLM with text prompt

- **Design tradeoffs**:
  - More surrogate models → better transfer but higher compute cost
  - Larger ε → higher ASR but more visible artifacts
  - Visual-only loss → better transfer but requires collecting example images (not just text labels)
  - Augmentation → robustness to preprocessing but slower optimization convergence

- **Failure signatures**:
  - Low ASR on Claude despite high ASR on GPT-4o: likely different visual training; increase augmentation intensity
  - Perturbation visible but ineffective: may be overfitting to surrogates; increase regularization (DropPath, PatchDrop)
  - Text recognition fails more than object recognition: requires fine-grained localization, may need higher ε

- **First 3 experiments**:
  1. **Baseline transfer test**: Use 3 CLIP surrogates with standard image-text loss (Eq. 3) on ImageNet captioning task; measure ASR on GPT-4o and Claude to establish baseline.
  2. **Ablation on surrogate diversity**: Compare 3 CLIP vs 8 CLIP vs 8 CLIP + VLLMs; expect plateau after sufficient CLIP coverage (Table 3 pattern).
  3. **Loss function comparison**: Textual (N=1) vs textual (N=50) vs visual (N=50, K=10); validate that visual examples with top-K selection yields highest transfer (Table 5 pattern).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What specific defense mechanisms or training interventions can effectively mitigate the transferability of adversarial perturbations to proprietary VLLMs without significantly impairing model utility?
- **Basis in paper**: [explicit] The conclusion explicitly states an "urgent need for robust defense mechanisms" to ensure safe deployment, noting that the paper focuses on exposing vulnerabilities rather than solving them.
- **Why unresolved**: The paper characterizes the attack surface and success rates but does not propose or evaluate potential defenses (e.g., adversarial training, purification) against the proposed transferable attacks.
- **What evidence would resolve it**: A study evaluating the attack success rates on VLLMs fine-tuned with specific defensive strategies (e.g., contrastive learning, noise injection) or protected by input preprocessing pipelines.

### Open Question 2
- **Question**: What specific architectural or data characteristics enable Claude 3.5 Sonnet to exhibit significantly higher robustness compared to GPT-4o and Gemini?
- **Basis in paper**: [inferred] Table 2 and Section 5.1 show Claude 3.5 Sonnet has a much lower Attack Success Rate (ASR) (e.g., 15.1% vs 83.9% for GPT-4o at $\epsilon=8/255$). The authors hypothesize this is due to visual training diverging from public models but do not isolate the cause.
- **Why unresolved**: The proprietary nature of the models prevents the authors from determining if the robustness stems from the visual encoder architecture, the training data distribution, or internal safety filtering.
- **What evidence would resolve it**: Ablation studies on open-source models that vary visual encoders and training datasets to replicate the robustness profile observed in Claude, or white-box analysis of the specific failure modes of the attack on Claude.

### Open Question 3
- **Question**: Does increasing the number of surrogate models yield diminishing returns or a plateau in attack transferability, and does this scale linearly with computational cost?
- **Basis in paper**: [inferred] Section 3.2 notes that while increasing surrogate models helps, the number "cannot be scaled easily due to computational constraints," leaving the theoretical upper bound of this method unexplored.
- **Why unresolved**: The authors were restricted by computational resources, limiting the ensemble size and leaving open the question of whether saturation occurs where more models provide negligible benefit.
- **What evidence would resolve it**: Large-scale experiments plotting ASR against the number of surrogate models ($n$) to identify if a performance ceiling exists relative to the optimization cost.

## Limitations
- Attack effectiveness varies significantly across different VLLM architectures, with Claude showing notably higher robustness than GPT-4o and Gemini
- The method requires collecting positive/negative example images, making it more resource-intensive than text-label-only approaches
- Limited evaluation on Gemini 1.5 Pro suggests potential blind spots in cross-model transferability analysis

## Confidence

- **High Confidence**: The effectiveness of ensemble-based transfer attacks on VLLMs, supported by extensive empirical results across multiple models and tasks
- **Medium Confidence**: The universality of perturbations across different VLLM architectures, given the observed but not thoroughly explained variation in attack success rates
- **Medium Confidence**: The superiority of visual contrastive loss over image-text loss, based on ablation studies but with limited comparative analysis

## Next Checks

1. **Architecture Diversity Test**: Evaluate attack transferability on VLLMs using non-CLIP visual encoders (e.g., DINOv2-based models) to validate the core assumption about shared visual representations across proprietary models.

2. **Cross-Domain Robustness**: Test the same perturbations on VLLMs specialized for different tasks (medical imaging, satellite imagery) to assess whether the discovered vulnerabilities are truly universal or domain-specific.

3. **Defense Implementation**: Implement and evaluate two candidate defenses: (a) adversarial training on the discovered universal perturbations, and (b) input transformation techniques like randomized smoothing or feature squeezing to measure potential mitigation effectiveness.