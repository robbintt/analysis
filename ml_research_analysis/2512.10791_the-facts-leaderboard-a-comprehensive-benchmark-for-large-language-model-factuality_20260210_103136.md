---
ver: rpa2
title: 'The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model
  Factuality'
arxiv_id: '2512.10791'
source_url: https://arxiv.org/abs/2512.10791
tags:
- facts
- benchmark
- factuality
- response
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The FACTS Leaderboard introduces a comprehensive benchmark suite
  that evaluates large language models'' factuality across four distinct dimensions:
  multimodal (image-based questions), parametric (closed-book factoid questions),
  search (information-seeking with external tools), and grounding (long-form responses
  based on provided documents). Each sub-leaderboard uses automated judge models to
  score responses, with the final suite score being an average across all four components.'
---

# The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality

## Quick Facts
- arXiv ID: 2512.10791
- Source URL: https://arxiv.org/abs/2512.10791
- Reference count: 6
- Top model achieves 68.8% accuracy, indicating significant room for improvement

## Executive Summary
The FACTS Leaderboard introduces a comprehensive benchmark suite that evaluates large language models' factuality across four distinct dimensions: multimodal (image-based questions), parametric (closed-book factoid questions), search (information-seeking with external tools), and grounding (long-form responses based on provided documents). Each sub-leaderboard uses automated judge models to score responses, with the final suite score being an average across all four components. The benchmark suite includes both public and private test sets to allow external participation while preventing overfitting. The evaluation reveals that the top-performing model, Gemini 3 Pro, achieves only 68.8% accuracy, indicating significant room for improvement.

## Method Summary
The FACTS Leaderboard evaluates factuality through four sub-benchmarks: Multimodal (711 image-based questions with rubric-based coverage and contradiction checks), Parametric (2,104 adversarially filtered factoid questions graded by automated judges), Search (1,884 questions across four subsets using Brave Search API), and Grounding v2 (long-form responses to documents evaluated by dual judges). The final FACTS Score averages the four subset accuracies. Automated judges (primarily Gemini 2.5 Pro/Flash) score responses based on coverage of essential facts and absence of contradictions. Public splits are available on Kaggle while private splits prevent overfitting.

## Key Results
- Gemini 3 Pro achieves the highest overall score of 68.8% accuracy across all four sub-leaderboards
- Models show varying strengths: some excel at coverage while others prioritize precision and avoiding contradictions
- The top-performing model leaves considerable headroom for improvement, indicating the benchmark remains challenging
- Model-specific trade-offs emerge: Gemini models are recall-oriented while GPT models are precision-oriented

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing factuality into four distinct sub-benchmarks yields diagnostic signals that a single aggregate metric would obscure.
- Mechanism: Each sub-leaderboard isolates a specific knowledge source and capability (visual grounding with world knowledge, closed-book parametric recall, search-augmented retrieval, and document-grounded generation). The final FACTS Score averages these, but the per-subset accuracy reveals model-specific trade-offs.
- Core assumption: These four dimensions are sufficiently distinct and collectively representative of real-world factuality demands.
- Evidence anchors:
  - [abstract] "The suite provides a holistic measure of factuality by aggregating the performance of models on four distinct sub-leaderboards"
  - [Page 2] "Models that excel at summarizing a provided document may struggle to answer factoid questions from memory, and vice-versa."
  - [corpus] Related work (FACTS Grounding v1, FACTORY) focuses on single dimensions; this suite explicitly targets multi-dimensional coverage.
- Break condition: If a new deployment scenario relies heavily on a capability not covered (e.g., video understanding, rapid knowledge updates), the aggregate score may mislead.

### Mechanism 2
- Claim: Rubric-based evaluation with automated judges enables scalable, reproducible factuality assessment.
- Mechanism: Human annotators create rubrics listing essential and non-essential facts. Automated judges (e.g., Gemini 2.5 Pro) then verify coverage (essential facts present) and no-contradiction (no claims conflict with rubric, image, or common knowledge). Accuracy requires both.
- Core assumption: The automated judge's verdicts correlate sufficiently with human judgments.
- Evidence anchors:
  - [Page 3] "The evaluation produces two primary scores — a Coverage score and a No-Contradiction score — determined by an automated judge"
  - [Page 5] "For Coverage... Spearman's rank correlation of 0.64 with human judgments... For No-Contradiction... macro F1 score of 78.2."
  - [corpus] VeriFact and other decompose-verify pipelines similarly rely on claim-level verification but often struggle with inter-sentence dependencies.
- Break condition: If judge models exhibit systematic bias (e.g., self-preference), scores may not generalize. The paper mitigates this by using multiple judges for Grounding v2.

### Mechanism 3
- Claim: Adversarial sampling using open-weight models prevents benchmark saturation and ensures continued challenge.
- Mechanism: For FACTS Parametric, questions are filtered to retain only those that five strong open-weight models answer incorrectly in closed-book mode. This decouples adversarial filtering from the proprietary API models being evaluated.
- Core assumption: Open-weight model failures are predictive of frontier model difficulty.
- Evidence anchors:
  - [Page 8] "To identify the most challenging questions among this verified set, we collect responses from five strong open-weight models... we retain only the questions that none of these open-weight models answered correctly."
  - [Page 16] "the top performing model has an average accuracy of only 69%, leaving considerable headroom for future progress."
  - [corpus] SimpleQA Verified and FACTORY similarly use filtering to ensure difficulty, but FACTS explicitly uses adversarial sampling with diverse model families.
- Break condition: If open-weight models improve rapidly or if frontier models develop qualitatively different reasoning, the adversarial filter may become less predictive.

## Foundational Learning

- **Parametric vs. grounded factuality**
  - Why needed here: The benchmark fundamentally distinguishes knowledge retrieved from internal weights (Parametric) vs. knowledge grounded in external context (Grounding, Multimodal, Search).
  - Quick check question: Can you explain why a model might excel at FACTS Grounding but struggle with FACTS Parametric?

- **Automated judge validation**
  - Why needed here: All four sub-leaderboards rely on LLM-based judges; understanding their validation (correlation with human judgments, F1 scores) is critical for interpreting results.
  - Quick check question: What does a macro F1 of 78.2 for No-Contradiction imply about judge reliability?

- **Public/private test splits**
  - Why needed here: The leaderboard uses private splits to guard integrity while allowing external participation on public splits.
  - Quick check question: Why might a model perform differently on public vs. private splits, and what would overfitting to public data look like?

## Architecture Onboarding

- Component map:
  - **FACTS Multimodal**: ~1,500 image-based questions → rubric creation → Coverage + No-Contradiction verdicts via autorater → Accuracy = both conditions met
  - **FACTS Parametric**: 2,104 factoid QA pairs → adversarially filtered → automated grader (correct/incorrect/not-attempted/unknown) → Accuracy + F1 + Hedging rate
  - **FACTS Search**: 1,884 questions (4 subsets: Hard Tail, Wiki Two-Hop, Wiki Multi-Doc, KG Hops) → Brave Search API → auto-rater verdict → Accuracy + F1
  - **FACTS Grounding v2**: Long-form prompts with context documents → dual judge evaluation (Gemini 2.5 Flash + GPT-5) → eligibility check → adjusted factuality score
  - **Aggregation**: FACTS Score = average of four subset accuracies

- Critical path:
  1. Understand the evaluation protocol for the sub-leaderboard most relevant to your use case.
  2. Review judge validation metrics (Table 4, Table 10) to assess evaluation reliability.
  3. Examine per-model trade-offs (e.g., Table 3: Gemini models are recall-oriented, GPT models are precision-oriented) to select models aligned with your risk tolerance.

- Design tradeoffs:
  - **Single judge vs. ensemble**: FACTS Parametric uses Gemini 2.5 Pro alone for simplicity; Grounding v2 uses dual judges to reduce bias.
  - **Coverage vs. precision**: Models optimize differently; Claude shows high hedging (low attempted-accuracy trade-off), Gemini shows high coverage with more contradictions.
  - **Hardness vs. realism**: Adversarial filtering ensures difficulty but may over-index on edge cases not representative of typical user queries.

- Failure signatures:
  - **High accuracy, low coverage**: Model is hedging or giving overly brief responses (check hedging rate in Parametric, eligibility in Grounding).
  - **High coverage, low accuracy**: Model is hallucinating additional claims (check No-Contradiction score in Multimodal).
  - **Search count anomalies**: Excessive searches may indicate inefficient tool use; very few may indicate over-reliance on parametric knowledge.

- First 3 experiments:
  1. Run your model on the public split of the sub-leaderboard most relevant to your deployment (e.g., Grounding for RAG systems) and compare to Table 1 baselines.
  2. Ablate your system with search disabled on FACTS Search to quantify the contribution of tool use.
  3. Submit responses to the Kaggle leaderboard to obtain private split evaluation and compare public vs. private performance to detect overfitting.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does entity frequency in training data correlate with factual recall accuracy in the FACTS Parametric benchmark?
- Basis in paper: [explicit] The authors state: "it would be interesting to check if this is reflected in FACTS Parametric" regarding Kandpal et al.'s finding that infrequent entities are harder to learn.
- Why unresolved: The benchmark reports overall accuracy but does not analyze performance as a function of entity frequency or popularity in training corpora.
- What evidence would resolve it: Correlation analysis between FACTS Parametric accuracy and entity frequency metrics from Wikipedia page views or training data occurrence counts.

### Open Question 2
- Question: Can notions of "tailness" be defined and measured for search-based factuality, and do they predict which facts are harder for models to find via search tools?
- Basis in paper: [explicit] The authors state: "it would be interesting to study notions of 'tailness' for FACTS Search, where some facts might be harder to search for than others."
- Why unresolved: Unlike parametric knowledge where entity frequency is well-studied, search difficulty depends on retrieval quality, result ranking, and information scattered across sources.
- What evidence would resolve it: Define search tailness metrics (e.g., search result position, number of sources, query complexity) and correlate with FACTS Search accuracy across question categories.

### Open Question 3
- Question: What factors determine question hardness across the FACTS sub-benchmarks, and are there unified predictors of factual difficulty?
- Basis in paper: [explicit] The authors note: "it will be useful to obtain more fine-grained analysis, studying what affects the hardness of questions."
- Why unresolved: The paper reports aggregate accuracy but does not analyze which linguistic, semantic, or structural properties make questions harder for models.
- What evidence would resolve it: Regression analysis relating question features (complexity, ambiguity, multi-hop requirements, entity types) to per-question accuracy across models.

## Limitations

- Judge model reliability remains uncertain despite validation metrics, as automated judges may not fully capture human-like factuality judgments
- The four sub-benchmarks may not capture all real-world factuality challenges, potentially missing emerging scenarios like video understanding
- Adversarial filtering effectiveness may diminish as both open-weight and frontier models rapidly improve

## Confidence

- **High Confidence**: The fundamental decomposition of factuality into four distinct sub-leaderboards is well-supported by both theoretical reasoning and empirical evidence showing model-specific trade-offs across dimensions.
- **Medium Confidence**: The automated judge validation metrics suggest reasonable reliability, but the extent to which these judges capture human-like factuality judgments remains uncertain.
- **Low Confidence**: Long-term effectiveness of the adversarial sampling approach and the benchmark's ability to remain challenging as model capabilities advance.

## Next Checks

1. **Judge Reliability Stress Test**: Run a small-scale human evaluation (10-20 samples) comparing automated judge verdicts against human judgments for each sub-leaderboard. Calculate updated correlation metrics to validate the reported reliability scores and identify systematic biases.

2. **Cross-Benchmark Consistency Analysis**: Evaluate the same set of frontier models across all four sub-leaderboards and analyze whether performance patterns are consistent or reveal unexpected dependencies between dimensions. This would test the assumption that these four aspects are truly independent.

3. **Benchmark Evolution Tracking**: Implement a rolling evaluation schedule where the same models are periodically re-evaluated on the FACTS suite. Track how performance changes over time and whether the adversarial filtering maintains its effectiveness in keeping the parametric benchmark challenging.