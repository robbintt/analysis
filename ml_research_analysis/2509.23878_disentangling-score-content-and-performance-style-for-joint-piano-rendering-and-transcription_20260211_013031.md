---
ver: rpa2
title: Disentangling Score Content and Performance Style for Joint Piano Rendering
  and Transcription
arxiv_id: '2509.23878'
source_url: https://arxiv.org/abs/2509.23878
tags:
- style
- performance
- score
- music
- expressive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a unified framework for expressive piano
  performance rendering (EPR) and automatic piano transcription (APT) by disentangling
  score content and performance style representations. The approach uses a transformer-based
  sequence-to-sequence architecture trained with both paired and unpaired data, eliminating
  the need for fine-grained alignment.
---

# Disentangling Score Content and Performance Style for Joint Piano Rendering and Transcription

## Quick Facts
- **arXiv ID:** 2509.23878
- **Source URL:** https://arxiv.org/abs/2509.23878
- **Reference count:** 40
- **Primary result:** Unified framework for EPR and APT with disentangled score content and performance style representations, achieving competitive benchmark performance

## Executive Summary
This paper introduces a unified framework that jointly addresses Expressive Performance Rendering (EPR) and Automatic Piano Transcription (APT) by disentangling score content (pitch, rhythm) from performance style (dynamics, tempo) representations. The approach uses a transformer-based sequence-to-sequence architecture trained with both paired and unpaired data, eliminating the need for fine-grained alignment. A diffusion-based performance style recommendation module generates style embeddings from score content, enabling automated and stylistically appropriate rendering. Experiments show the model achieves competitive performance on EPR and APT benchmarks, with effective content-style disentanglement and reliable style transfer capabilities.

## Method Summary
The method employs a transformer-based sequence-to-sequence architecture with five components: separate encoders for score content, performance content, and performance style, plus corresponding decoders for each task. Score and performance data are tokenized into discrete sequences (pitch, IOI, velocity, etc.). The model is trained jointly on four subtasks: EPR, APT, and masked reconstruction for both scores and performances, using only sequence-aligned data without fine-grained note-level alignment. A diffusion-based Performance Style Recommendation (PSR) module is trained separately to generate style vectors from score content embeddings. The framework uses a 5-layer transformer (8 heads, d=512) trained on 967 performances from the ASAP dataset, with unpaired data from MuseScore and YouTube transcriptions.

## Key Results
- Achieves F1-scores of 0.945 (Ep), 0.913 (Emiss), and 0.953 (Eextra) on the ASAP APT benchmark
- Outperforms baseline EPR models on alignment/insertion/missing rates and subjective human-likeness ratings
- Demonstrates effective disentanglement with style→composer accuracy of 0.876 and content→performer accuracy of 0.108
- PSR module generates era-appropriate styles with significant clustering by historical period (Baroque vs. Romantic)

## Why This Works (Mechanism)

### Mechanism 1: Disentanglement via Architectural Constraints and Regularization
The architecture enforces separation by assigning distinct encoders: content encoders ($f_{c,X}, f_{c,Y}$) output note-level sequences, while the style encoder ($f_{s,Y}$) outputs a single global vector via a `[CLS]` token. A Kullback-Leibler (KL) divergence penalty further regularizes the style space to prevent it from encoding content-specific information. The core assumption is that performance style is primarily a global attribute (e.g., "heavy" or "relaxing") that can be summarized by a single vector, while content is note-dependent. Evidence includes the explicit architectural description in section 3.3 and discussion of KL penalty $L_{KL}$. If the style embedding $z_s$ is required to predict the pitch of the next note, or if the content embedding $z_c$ is insufficient to recover the score in APT without the style vector, the disentanglement would fail.

### Mechanism 2: Sequence-to-Sequence (Seq2Seq) Duality for Weak Supervision
The model treats both symbolic scores and performance MIDI as token sequences, using a standard transformer architecture with masked reconstruction objectives (similar to BERT). By formulating EPR and APT as inverse Seq2Seq tasks, the model learns robust cross-domain representations without requiring explicit alignment labels. The tasks are trained jointly: EPR ($\hat{y} = g_Y(z_x \oplus z_s)$) and APT ($\hat{x} = g_X(z_y)$). The core assumption is that the transformer's attention mechanism is sufficient to resolve temporal discrepancies between the score and expressive performance without explicit alignment supervision. Evidence includes the definition of EPR and APT losses in section 3.2 and validation of self-supervised pre-training in the corpus. If the model fails to converge on unpaired data, or if generated performances drift temporally from the input score, the Seq2Seq duality would break down.

### Mechanism 3: Diffusion-Based Style Recommendation (PSR)
The PSR module is an independent diffusion network trained to denoise a style vector $z_s$ conditioned on a global score embedding $e_g$. It models the distribution $p(z_s|e_g)$, effectively learning "how a pianist would play this score" based on its musical era or composer characteristics. The core assumption is that there exists a learnable correlation between musical content (e.g., genre, composer traits) and appropriate expressive styles (the "Horowitz factor"). Evidence includes the detailed DDPM training objective $L_{PSR}$ in section 3.4 and visualization of PSR-generated styles clustering by historical era. If the PSR generates generic, low-variance style vectors for all inputs (posterior collapse), resulting in "robotic" outputs indistinguishable from the input score MIDI, the mechanism would fail.

## Foundational Learning

- **Concept: Disentangled Representation Learning (DRL)**
  - **Why needed here:** The entire framework relies on the premise that $z_c$ (content) and $z_s$ (style) can be separated. Without understanding DRL, the mutual supervision between EPR and APT is opaque.
  - **Quick check question:** Can you explain why minimizing mutual information between $z_c$ and $z_s$ might help in transferring the style of a *Chopin* piece to a *Bach* piece?

- **Concept: Transformers for Symbolic Music (Seq2Seq)**
  - **Why needed here:** The model discards alignment constraints in favor of attention mechanisms.
  - **Quick check question:** How does Rotary Positional Encoding (RoPE) or relative attention help a transformer handle the temporal "stretching" found in expressive performance compared to a rigid score?

- **Concept: Denoising Diffusion Probabilistic Models (DDPM)**
  - **Why needed here:** The PSR module uses DDPM to generate the style embedding.
  - **Quick check question:** Why is a diffusion model suitable for generating a "style vector" compared to a simple deterministic Multi-Layer Perceptron (MLP)?

## Architecture Onboarding

- **Component map:** Score MIDI -> ScoreEncoder ($f_{c,X}$) -> Content Vector ($z_x$) -> ScoreDecoder ($g_X$) -> Transcribed Score; Performance MIDI -> PerfEncoder ($f_{c,Y}$) -> Content Vector ($z_y$) -> PerfDecoder ($g_Y$) -> Rendered Performance; Performance MIDI -> StyleEncoder ($f_{s,Y}$) -> Style Vector ($z_s$) -> PerfDecoder ($g_Y$); Score MIDI -> ScoreEncoder ($f_{c,X}$) -> Score Embedding ($e_g$) -> PSR Module -> Style Vector ($\hat{z}_s$) -> PerfDecoder ($g_Y$)

- **Critical path:** Input Processing: Convert Score/Performance to discrete tokens (pitch, IOI, velocity) -> Joint Training: Forward pass through encoders -> Latent Space ($z_x, z_y, z_s$) -> Decoders -> Loss ($L_{total}$) -> PSR Inference: Score -> Content Encoder -> Diffusion Denoiser -> Style Vector ($\hat{z}_s$)

- **Design tradeoffs:** Global vs. Local Style: The model uses a *global* style vector (one vector per piece) rather than note-level style features. This simplifies disentanglement but may miss fine-grained local nuances. Alignment-Free: Removes the brittleness of alignment tools (e.g., handling trills) but requires the model to learn strict timing alignment implicitly via attention.

- **Failure signatures:** Style Collapse: Generated performances have zero velocity variance (flat dynamics). Content Leakage: Style embeddings cluster by pitch/rhythm rather than expressive character. PSR Hallucination: The diffusion module generates styles incompatible with the score (e.g., playing *Bach* with excessive rubato), indicating the score embedding $e_g$ failed to capture genre constraints.

- **First 3 experiments:** 1. Sanity Check (Overfitting): Train on a single paired (Score, Performance) pair. The model should perfectly reconstruct the performance (L_rec, L_EPR $\approx 0$). 2. Disentanglement Ablation: Remove the KL divergence term ($\lambda_{KL} = 0$). Check if the style classifier accuracy drops (indicating the latent space became entangled). 3. PSR Validation: Generate style vectors for a Baroque piece and a Romantic piece using the PSR. Visualize them (t-SNE) to ensure they form distinct clusters rather than a single Gaussian blob.

## Open Questions the Paper Calls Out
- Can the unified framework effectively generalize to popular music, which exhibits greater stylistic diversity and looser structural constraints than the classical piano repertoire used in this study? [explicit] The Conclusion states, "As future work, we aim to extend this framework to popular music, which presents greater stylistic diversity and practical relevance than classical music."
- Does the use of a single global style vector limit the model's ability to capture time-varying expressive nuances (e.g., mood shifts or rubato variations within a single piece)? [inferred] Section 3.2 defines the style representation $z_s$ as a single latent vector summarizing the "high-level artistic character of a performance."
- To what extent does the noise introduced by the synthetic "unpaired performance dataset" (created via an upstream APT model) degrade the quality of the learned representations? [inferred] Section 4.1 notes that the unpaired performance dataset was "transcribing the audio... using a state-of-the-art audio-to-MIDI transcription model."

## Limitations
- The assumption that performance style can be adequately represented by a single global vector may oversimplify the complexity of expressive performance, potentially missing important local variations in dynamics and articulation
- The framework's reliance on unpaired data and self-supervised learning creates uncertainty about how well the model generalizes to truly unseen styles or eras beyond the ATEPP evaluation
- The diffusion-based PSR module's effectiveness depends heavily on the quality of the score embedding $e_g$, which is not fully characterized in terms of what musical features it captures

## Confidence
- **High confidence:** The architectural feasibility of disentangling content and style through separate encoders and the KL regularization approach
- **Medium confidence:** The effectiveness of the sequence-to-sequence duality for handling unpaired data
- **Medium confidence:** The PSR module's ability to generate stylistically appropriate performances

## Next Checks
1. **Statistical significance test:** Conduct a paired t-test on subjective evaluation scores (human-likeness, style appropriateness) between the proposed model and baselines to confirm the reported improvements are not due to chance
2. **Style transfer robustness test:** Systematically transfer styles between compositions of different eras (e.g., Baroque style to Romantic pieces and vice versa) and measure the degradation in both style appropriateness and content preservation using quantitative metrics
3. **PSR diversity validation:** Generate multiple style vectors for the same score using the PSR module and compute the variance in resulting performance dynamics (velocity) to verify the model produces meaningfully diverse outputs rather than collapsing to a single style representation