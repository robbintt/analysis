---
ver: rpa2
title: Offline Multi-Task Multi-Objective Data-Driven Evolutionary Algorithm with
  Language Surrogate Model and Implicit Q-Learning
arxiv_id: '2512.15149'
source_url: https://arxiv.org/abs/2512.15149
tags:
- optimization
- objective
- surrogate
- q-metasur
- evolutionary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Q-MetaSur, a large language model-based meta-surrogate
  for offline multi-task multi-objective optimization (MTMOO). The key innovation
  is a unified textual representation that encodes task metadata, decision variables,
  and multi-objective fitness values, enabling a single LLM to jointly model multiple
  tasks and objectives without modifying the architecture.
---

# Offline Multi-Task Multi-Objective Data-Driven Evolutionary Algorithm with Language Surrogate Model and Implicit Q-Learning

## Quick Facts
- **arXiv ID:** 2512.15149
- **Source URL:** https://arxiv.org/abs/2512.15149
- **Reference count:** 40
- **One-line primary result:** Q-MetaSur is a unified LLM-based meta-surrogate that jointly models multiple MTMOO tasks and objectives, achieving superior objective approximation and Pareto front quality.

## Executive Summary
This paper introduces Q-MetaSur, a large language model-based meta-surrogate for offline multi-task multi-objective optimization (MTMOO). The key innovation is a unified textual representation that encodes task metadata, decision variables, and multi-objective fitness values, enabling a single LLM to jointly model multiple tasks and objectives without modifying the architecture. A two-stage training strategy combines supervised fine-tuning with offline reinforcement learning using Conservative Q-Learning to enhance generalization and numerical accuracy. Experimental results on CEC2019 benchmarks show Q-MetaSur outperforms representative surrogate baselines in objective approximation accuracy and, when integrated with MTMOO algorithms, achieves improved Pareto front quality and optimization convergence. The approach demonstrates strong emergent generalization to unseen tasks and dimensions.

## Method Summary
Q-MetaSur uses a pre-trained T5 encoder-decoder to function as a unified meta-surrogate for MTMOO. Decision vectors and objectives are encoded into a unified textual format (Scientific Notation Encoding, SNE) that includes task metadata. The model is trained in two stages: (1) Supervised Fine-Tuning (SFT) with Priority-Weighted Cross-Entropy to learn the "numerical language," and (2) Offline Reinforcement Learning (ILQL+CQL) with data augmentation to directly optimize for the numerical error metric. The RL stage generates noisy labels and uses a reward function based on normalized RMSE to train Q and V heads for value estimation. Inference uses Advantage-Guided Decoding to bias token selection toward higher-value sequences.

## Key Results
- Q-MetaSur achieves superior surrogate accuracy (sMAE, RÂ²) compared to traditional single-task and multi-task baselines on CEC2019 benchmarks.
- When integrated with MTMOO algorithms, Q-MetaSur produces Pareto fronts with better IGD and MSS values.
- The model demonstrates strong zero-shot generalization, performing well on unseen tasks and dimensions without retraining.
- Ablation studies confirm the critical role of Conservative Q-Learning in stabilizing RL training and preventing overestimation.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A single Large Language Model (LLM) can serve as a unified surrogate for multiple optimization tasks and objectives with heterogeneous dimensions.
- **Mechanism:** The model uses a unified textual representation called Scientific Notation Encoding (SNE). Task metadata (e.g., function name, dimensionality) and decision vectors are serialized into a token sequence, conditioning the LLM to decode the corresponding objective values. This transforms regression into a sequence-to-sequence problem.
- **Core assumption:** The structural priors of pre-trained LLMs (learned from text) can effectively transfer to numerical reasoning and function approximation without explicit architectural modifications for dimensionality.
- **Evidence anchors:**
  - [abstract] Mentions "unified textual representation that encodes task metadata, decision variables, and multi-objective fitness values."
  - [section 3.2] Details the SNE process and templated metadata.
  - [corpus] "Large Language Model as Meta-Surrogate..." confirms this as a proof-of-principle extension.
- **Break condition:** If numerical precision requires strict floating-point error bounds beyond the token granularity (e.g., extremely small mantissa differences), the tokenization resolution may fail to capture necessary variance.

### Mechanism 2
- **Claim:** Reinforcement Learning (RL) fine-tuning improves numerical accuracy over standard supervised learning by directly optimizing for the error metric.
- **Mechanism:** A two-stage training process. First, Supervised Fine-Tuning (SFT) establishes a "numerical language." Second, Offline RL (specifically Implicit Q-Learning) optimizes a sequence-level reward based on normalized RMSE. This bridges the gap between token-level cross-entropy (proxy loss) and the actual numerical error (target metric).
- **Core assumption:** The reward function accurately captures the optimization utility, and the "perturb-and-score" data augmentation (generating noisy labels) sufficiently simulates the environment dynamics for offline RL.
- **Evidence anchors:**
  - [abstract] Highlights "synergy of supervised tuning and RL fine-tuning."
  - [section 3.4] Describes the perturb-and-score augmentation and reward definition ($R(\hat{y}; y)$).
  - [corpus] "Offline Model-Based Optimization: Comprehensive Review" provides context on the difficulty of offline objective learning.
- **Break condition:** If the reward function is misspecified (e.g., fails to penalize sign errors heavily enough), the model may learn to generate syntactically valid but numerically nonsensical sequences.

### Mechanism 3
- **Claim:** Conservative Q-Learning (CQL) regularizes the surrogate to prevent overestimation of values for out-of-distribution (OOD) tokens.
- **Mechanism:** By integrating CQL into the fine-tuning stage, the algorithm penalizes Q-values for actions (tokens) that deviate significantly from the offline dataset distribution. This stabilizes training in the absence of online environment interaction.
- **Core assumption:** The offline dataset covers the high-probability regions of the solution space sufficiently well that "conservatism" (staying close to data) correlates with better generalization rather than stagnation.
- **Evidence anchors:**
  - [section 3.4.3] Explicitly formulates the CQL loss term ($L_{CQL}$) to mitigate OOD shift.
  - [section 4.4.1] Ablation studies show that removing CQL causes catastrophic failure (R2 collapse) on difficult instances like Inst4.
- **Break condition:** If the optimization landscape requires exploration far outside the support of the offline dataset, excessive conservatism might prevent the surrogate from modeling novel, high-performing regions.

## Foundational Learning

- **Concept: Sequence-to-Sequence (Seq2Seq) Modeling**
  - **Why needed here:** The core innovation is framing optimization regression as text generation. You must understand how encoder-decoder architectures (like T5) map an input sequence (problem definition + parameters) to an output sequence (objective values).
  - **Quick check question:** Can you explain how a Transformer decoder generates tokens autoregressively conditioned on an encoder's output?

- **Concept: Offline Reinforcement Learning (Offline RL)**
  - **Why needed here:** The paper relies on Offline RL (specifically ILQL and CQL) to fine-tune the model. Unlike standard RL, this assumes no interaction with the "environment" (true objective function) during training, which creates specific challenges regarding distributional shift.
  - **Quick check question:** Why does standard Q-learning tend to fail in offline settings, and how does Conservative Q-Learning (CQL) theoretically address this?

- **Concept: Surrogate Modeling in Evolutionary Algorithms**
  - **Why needed here:** The LLM replaces traditional surrogates (like Gaussian Processes). You need to grasp the original problem: expensive function evaluations necessitate a cheap approximation model to guide the evolutionary search.
  - **Quick check question:** What is the trade-off between training a single multi-task surrogate versus training $N$ independent surrogates for $N$ tasks?

## Architecture Onboarding

- **Component map:** Metadata + Decision Vector -> SNE Tokenizer -> T5 Encoder-Decoder -> SNE Detokenizer -> Objective Values
- **Critical path:**
  1. **SFT Stage:** Train LLM to predict exact $y$ from $x$ using Priority-Weighted Cross Entropy (PWCE).
  2. **Augmentation:** Create "bad" examples (noisy $y$) and assign low rewards.
  3. **RL Stage:** Train Q/V heads using ILQL + CQL on the mix of real and augmented data to learn the value of accurate sequences.
  4. **Inference:** Use Advantage-Guided Decoding (Q - V) to bias token selection toward higher value.

- **Design tradeoffs:**
  - **SNE Precision vs. Sequence Length:** Increasing mantissa digits ($n_{digit}$) improves precision but increases sequence length, potentially degrading context handling and speed.
  - **Generalization vs. Accuracy:** A single meta-surrogate (one model for all tasks) trades off the precision of per-task models for cross-task generalization and zero-shot capabilities.

- **Failure signatures:**
  - **Catastrophic R2 Drop:** Indicates failure to regularize OOD tokens (CQL missing or weight too low).
  - **Order-of-Magnitude Errors:** Suggests SNE tokenization failure or "exposure bias" where early token mistakes cascade (mitigated by the RL stage).

- **First 3 experiments:**
  1. **SNE Validity:** Verify the tokenizer can correctly round-trip a float $\to$ text $\to$ float without precision loss for your specific value ranges.
  2. **Ablation on CQL:** Reproduce the "w/o-CQL" experiment on a single instance to observe the stability difference; this is the most sensitive failure point.
  3. **Zero-Shot Transfer:** Train on tasks $1...k$ and test on task $k+1$ to verify if the model is actually learning generalizable "optimization priors" vs. memorizing specific functions.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the Q-MetaSur framework be extended to provide well-calibrated uncertainty intervals or confidence bounds alongside objective predictions?
- **Basis in paper:** [explicit] Section 6 lists "uncertainty enhancement" and "distribution calibration" as future work, noting the need for "coverage-guaranteed intervals."
- **Why unresolved:** The current model functions as a deterministic point predictor; it lacks the inherent variance estimation capabilities found in Gaussian Process surrogates.
- **What evidence would resolve it:** Successful integration of conformal prediction or evidential deep learning that provides theoretical coverage guarantees on the predicted Pareto fronts.

### Open Question 2
- **Question:** How can active calibration with ultra-low-cost ground-truth evaluations be integrated to transition the model from zero-shot to few-shot refinement?
- **Basis in paper:** [explicit] Section 6 proposes "Adaptive calibration with few ground truths" to address sensitivity near distribution boundaries.
- **Why unresolved:** The current study focuses strictly on an offline setting where no additional true evaluations are permitted during optimization.
- **What evidence would resolve it:** An active learning pipeline where injecting $k$ new evaluations (where $k \ll$ training size) significantly reduces IGD compared to the static offline baseline.

### Open Question 3
- **Question:** Can Mixture-of-Experts (MoE) or adapter-based architectures effectively mitigate negative transfer in extremely heterogeneous MTMOO landscapes?
- **Basis in paper:** [explicit] Section 6 suggests "Scalable expertization" to handle "extreme heterogeneity" and prevent negative transfer.
- **Why unresolved:** A single unified LLM backbone may struggle to model task correlations that are weak or conflicting without conditional routing mechanisms.
- **What evidence would resolve it:** Demonstrated performance stability and reduced error when scaling to problem suites containing highly dissimilar tasks (e.g., combinatorial vs. continuous) compared to the dense baseline.

## Limitations

- **Synthetic Benchmark Focus:** The evaluation is limited to synthetic CEC2019 benchmarks, with no validation on real-world optimization problems.
- **Computational Overhead:** The paper does not provide runtime or resource usage comparisons to substantiate claims of efficiency versus training multiple per-task surrogates.
- **Offline Assumption:** The "offline" setting restricts applicability to scenarios with sufficient historical data, limiting exploration in novel optimization landscapes.

## Confidence

- **High Confidence:** The core mechanism of encoding optimization problems as text sequences and using LLMs as meta-surrogates is well-supported by the experiments. The SNE format and two-stage training (SFT + RL) are clearly described and validated.
- **Medium Confidence:** The claim of "strong emergent generalization to unseen tasks and dimensions" is supported within the CEC2019 benchmark suite but lacks validation on real-world problems or significantly different problem structures.
- **Low Confidence:** The assertion that the approach is "computationally efficient" compared to training multiple per-task surrogates is not substantiated by runtime or resource usage comparisons in the paper.

## Next Checks

1. **Real-World Scalability:** Apply Q-MetaSur to a real-world multi-objective optimization problem (e.g., neural architecture search or drug discovery) and compare its performance and resource usage against traditional surrogate methods.
2. **Robustness to Data Scarcity:** Evaluate the model's performance when trained on limited or imbalanced data to test the practical limits of the "offline" assumption.
3. **Generalization Beyond CEC2019:** Test the model on optimization problems with fundamentally different structures (e.g., discrete variables, non-continuous objectives) to assess the breadth of its learned "optimization priors."