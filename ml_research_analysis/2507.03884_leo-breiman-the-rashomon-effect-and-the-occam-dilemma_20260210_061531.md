---
ver: rpa2
title: Leo Breiman, the Rashomon Effect, and the Occam Dilemma
arxiv_id: '2507.03884'
source_url: https://arxiv.org/abs/2507.03884
tags:
- breiman
- data
- rashomon
- have
- effect
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper revisits Leo Breiman's "Two Cultures" paper from 2001,
  contrasting data models (modeling data generation) with algorithmic models (machine
  learning). The author argues that after 25 years of advances in computing, several
  of Breiman's key claims no longer hold.
---

# Leo Breiman, the Rashomon Effect, and the Occam Dilemma

## Quick Facts
- arXiv ID: 2507.03884
- Source URL: https://arxiv.org/abs/2507.03884
- Reference count: 5
- One-line primary result: The accuracy-interpretability tradeoff (Occam Dilemma) is nullified by the Rashomon Effect, enabling interpretable models to match black-box accuracy for noisy tabular problems.

## Executive Summary
This paper revisits Leo Breiman's "Two Cultures" paper from 2001, arguing that advances in computing over the past 25 years have invalidated several of Breiman's key claims. The central thesis is that the "Occam Dilemma" - the supposed tradeoff between model accuracy and simplicity - no longer holds due to the "Rashomon Effect," which describes the existence of many equally good models for a given dataset. Through this lens, the paper demonstrates that interpretable machine learning methods like sparse decision trees and generalized additive models can achieve accuracy comparable to black-box models for most noisy problems while maintaining interpretability.

The paper also addresses causality, arguing that it can be investigated through model-free methods rather than relying on single interpretable models. The modern perspective shows that the goals of both cultures - causality, simplicity, and accuracy - can be achieved through different approaches than what Breiman considered. The Rashomon Effect serves as a key theoretical tool in proving the nullification of the Occam Dilemma, showing that simpler models exist within sets of approximately-equally-good models when noise is present.

## Method Summary
The paper demonstrates the nullification of the Occam Dilemma through theoretical arguments and empirical observations. The core approach involves enumerating or approximating the "Rashomon set" - all models within ε of minimum loss - for interpretable function classes like sparse decision trees and generalized additive models. By showing that simpler models exist within these sets, the paper argues that the accuracy-simplicity tradeoff does not hold for noisy tabular problems. The method relies on modern algorithms like GOSDT (Globally-Optimal Sparse Decision Trees) and sparse GAM implementations that can find accurate, interpretable models within seconds or minutes. For causality, the paper proposes model-free approaches like Rashomon Importance Distribution (RID) and Almost Matching Exactly (AME) that avoid committing to single models while investigating variable importance and treatment effects.

## Key Results
- The Occam Dilemma (accuracy-simplicity tradeoff) is nullified by the Rashomon Effect for noisy tabular problems.
- Interpretable models like sparse decision trees and GAMs can achieve accuracy comparable to black-box models when noise is present.
- Causality can be investigated through model-free methods (RID, AME) rather than relying on single interpretable models.
- The tradeoff does not hold for most noisy problems, though it may still exist for non-noisy problems like computer vision and NLP.

## Why This Works (Mechanism)

### Mechanism 1: Rashomon Set Theory Dissolves the Accuracy-Interpretability Tradeoff
- Claim: When a large set of approximately-equally-good models exists, simpler interpretable models are likely present within that set.
- Mechanism: Noise in outcomes increases loss variance → forces simpler hypothesis spaces → larger fraction of models appear equally good → Rashomon set expands. Within this set, nested function classes ensure simpler models exist inside balls of complex models.
- Core assumption: The problem has noisy outcomes (y is random given x); simpler function classes can approximate complex ones sufficiently well.
- Evidence anchors:
  - [abstract] "Interestingly, the Rashomon Effect is a key tool in proving the nullification of the Occam Dilemma."
  - [section 2.2] "Thus, we generally would not simultaneously have a large Rashomon Effect and an accuracy/simplicity tradeoff; they are not compatible."
  - [corpus] "Why are there many equally good models? An Anatomy of the Rashomon Effect" explores causes of the effect, categorizing into statistical, optimization, and data structure sources.
- Break condition: Deterministic/non-noisy problems (e.g., computer vision, NLP) where labels are fixed given inputs—Rashomon Effect may not manifest to the same extent.

### Mechanism 2: Expanded Simplicity Definition Captures the "Sweet Spot"
- Claim: Breiman's observed tradeoff arose from equating "simple" with linear models and unoptimized CART trees, missing the expressive middle ground of GAMs and globally-optimized decision trees.
- Mechanism: GAMs allow nonlinear relationships per feature while remaining human-readable. Modern optimal sparse decision tree algorithms (OSDT, GOSDT) solve global objectives rather than relying on greedy heuristics, finding accurate small trees in seconds/minutes.
- Core assumption: The data-generating process has structure amenable to additive or tree-based decomposition; computational budget exists for global optimization.
- Evidence anchors:
  - [abstract] "...by 'simple,' he appears to consider only linear models or unoptimized decision trees."
  - [section 3.2] "CART does not actually optimize for any global objective; it instead uses splitting and pruning heuristics... Nowadays, we can find accurate, sparse, trees within seconds or minutes."
  - [corpus] "Many Ways to be Right: Rashomon Sets for Concept-Based Neural Networks" extends Rashomon analysis to concept-based architectures, suggesting generalizability beyond tabular data.
- Break condition: Problems requiring highly non-additive interactions across many features simultaneously (e.g., pixel-level image patterns) may not admit compact interpretable representations.

### Mechanism 3: Model-Free Causal Inference via Rashomon Ensembles
- Claim: Causality can be investigated without committing to a single model by aggregating variable importance across many good models (RID) or using case-based matching (AME).
- Mechanism: RID computes variable importance distributions over bootstrapped Rashomon sets, estimating a probability distribution for each variable's true importance. AME matches nearly-identical observations differing only on treatment variable v, revealing counterfactual outcomes without parametric assumptions.
- Core assumption: No unmeasured confounding for AME; sufficient overlap in covariate distributions; Rashomon set adequately sampled for RID.
- Evidence anchors:
  - [abstract] "causality can be investigated through model-free methods rather than relying on single interpretable models."
  - [section 5] "My lab's latest approach to this problem is the Rashomon Importance Distribution (RID)... If we want to estimate the effect that a chosen variable v has on the outcome... I prefer Almost Matching Exactly (AME) approaches."
  - [corpus] "Doctor Rashomon and the UNIVERSE of Madness" examines variable importance under unobserved confounding, noting dependence on which variables are included—relevant to RID's limitations.
- Break condition: Severe unobserved confounding; insufficient data density for reliable matching; high-dimensional treatments.

## Foundational Learning

- Concept: **Rashomon Effect**
  - Why needed here: Central to the paper's thesis that multiplicity of good models enables finding simpler ones. Without understanding this, the nullification argument for Occam's Dilemma is opaque.
  - Quick check question: Given a dataset, how would you empirically test whether a Rashomon set exists (i.e., multiple structurally-different models within 1% accuracy)?

- Concept: **Generalized Additive Models (GAMs)**
  - Why needed here: Represents Rudin's "expanded simplicity" definition—more expressive than linear models but still interpretable through per-feature shape plots.
  - Quick check question: Write the functional form of a GAM and explain why it can capture nonlinear age effects that a linear model cannot.

- Concept: **Global vs. Greedy Optimization in Decision Trees**
  - Why needed here: Explains why CART (greedy) underperforms relative to modern optimal sparse tree methods—relevant to practical implementation choices.
  - Quick check question: What objective does CART optimize at each split versus what a globally-optimal tree solver optimizes?

## Architecture Onboarding

- Component map:
  Rashomon Set Enumerator -> Simplicity Filter -> Variable Importance Aggregator (RID) -> Causal Matching Engine (AME)

- Critical path:
  1. Validate problem type: noisy vs. non-noisy (check if Rashomon Effect is likely via pilot experiments).
  2. Enumerate or approximate Rashomon set for interpretable function class (trees, sparse GLMs, GAMs).
  3. Select simplest model(s) meeting accuracy threshold.
  4. If causality needed, apply RID for variable importance or AME for individual treatment effects.

- Design tradeoffs:
  - Rashomon set enumeration vs. sampling: Full enumeration guarantees finding simplest model but is computationally expensive; sampling is faster but may miss optimal points.
  - Simplicity definition strictness: Stricter constraints (depth ≤3 vs. ≤5) may break the "no tradeoff" finding if the Rashomon set doesn't extend that far.
  - Model-free vs. model-based causality: AME/RID avoid single-model assumptions but require more data and strong overlap/confounding assumptions.

- Failure signatures:
  - All models in Rashomon set are complex → Occam Dilemma actually holds; consider expanding function class or accepting lower accuracy.
  - RID importance distributions extremely wide across bootstraps → unstable variable importance; likely insufficient data or high noise.
  - AME returns no matches or imbalanced groups → insufficient overlap; consider dimensionality reduction or different matching metric.

- First 3 experiments:
  1. **Rashomon detection**: Train multiple algorithms (logistic regression, random forest, gradient boosting, sparse GAM) on your dataset with cross-validation. If performance variance across structurally-different models is <2%, Rashomon Effect present.
  2. **Simplicity sweep**: Within an interpretable class (e.g., sparse decision trees), vary sparsity constraint and plot accuracy. If curve plateaus before reaching maximum complexity, tradeoff does not exist for your problem.
  3. **RID baseline**: Implement basic RID by training 100 sparse models on bootstrap samples, computing per-feature importance. Compare distribution width to single-model importance—wider distributions indicate sensitivity to model selection.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the nullification of the accuracy/interpretability tradeoff apply to non-noisy problems like computer vision?
- Basis in paper: [explicit] Section 2.3 explicitly contrasts noisy problems with non-noisy problems (vision/NLP), noting that complex neural networks currently dominate the latter, implying the author's theory regarding simple models may not hold there.
- Why unresolved: The Rashomon Set Theory links the lack of tradeoff to noise; non-noisy problems may possess smaller Rashomon sets that exclude interpretable models.
- What evidence would resolve it: Rigorous benchmarks showing optimized interpretable models achieving parity with deep learning on standard non-noisy datasets (e.g., ImageNet).

### Open Question 2
- Question: Under what theoretical conditions does the "simpler class approximation" assumption fail?
- Basis in paper: [inferred] The "Rashomon Set Theory" in Section 2.2 relies on the assumption that functions from a simpler class can represent those from a complex class "sufficiently well."
- Why unresolved: If the true data generating process is sufficiently complex, the Rashomon set may not contain any interpretable approximations, reinstating the Occam Dilemma.
- What evidence would resolve it: Theoretical characterization of data distributions where the Rashomon set is large but contains no simple, interpretable functions.

### Open Question 3
- Question: How can we rigorously quantify the "size" of the Rashomon set to predict the existence of interpretable models?
- Basis in paper: [inferred] Section 2.2 argues that a "large" Rashomon Effect suppresses the accuracy/simplicity tradeoff, but the paper provides no formal metric or threshold for "large."
- Why unresolved: Without a method to measure the volume of the Rashomon set relative to the function space, it is difficult to predict a priori if an interpretable model exists for a new dataset.
- What evidence would resolve it: A defined metric for Rashomon set volume that correlates with the empirical discoverability of sparse, accurate models.

## Limitations
- The paper's claims about the nullification of the Occam Dilemma depend critically on the existence and size of the Rashomon set, which varies substantially across problem domains.
- The generalizability to non-tabular data (images, text) remains unclear, as these domains often have deterministic labels given inputs, potentially invalidating the core mechanism.
- The definition of "interpretability" used here (sparse trees, GAMs) may not align with practitioner needs in all domains.

## Confidence
- High confidence: The theoretical argument that noise increases Rashomon set size and enables simpler models within it.
- Medium confidence: The practical claim that modern interpretable methods (GOSDT, GAMs) achieve black-box accuracy across most noisy tabular problems—depends heavily on dataset selection and implementation details.
- Low confidence: The extension to causality via model-free methods (RID, AME), which requires strong assumptions about unmeasured confounding and data density.

## Next Checks
1. **Domain applicability test**: Systematically evaluate Rashomon Effect size across diverse problem types (noisy tabular, deterministic vision, NLP) to identify boundary conditions where the tradeoff reemerges.
2. **Interpretability definition stress test**: Define quantitative interpretability metrics (e.g., human comprehension time, feature interaction count) and verify that models within the Rashomon set maintain these properties while matching black-box accuracy.
3. **RID stability benchmark**: Using synthetic data with known variable importance and controlled confounding, test whether RID distributions accurately recover ground truth and remain stable across bootstrap samples.