---
ver: rpa2
title: Can Third-parties Read Our Emotions?
arxiv_id: '2504.18673'
source_url: https://arxiv.org/abs/2504.18673
tags:
- annotators
- emotion
- third-party
- annotations
- first-party
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study directly compares third-party annotations (human and
  LLM-based) with first-party (author-provided) emotion labels, finding that both
  types of third-party annotations poorly capture authors' intended emotions. LLMs
  consistently outperform human annotators across most emotions, but still achieve
  only low to fair alignment scores (Cohen's kappa 0-0.45; F1 scores 0.2-0.6).
---

# Can Third-parties Read Our Emotions?

## Quick Facts
- **arXiv ID**: 2504.18673
- **Source URL**: https://arxiv.org/abs/2504.18673
- **Reference count**: 40
- **Primary result**: Third-party annotations (human and LLM) poorly capture authors' intended emotions

## Executive Summary
This study challenges the assumption that third-party annotators can reliably infer emotions from text. Using a 28-category emotion taxonomy and 729 social media posts from 123 participants, researchers compared first-party author labels against both human and LLM-based third-party annotations. The results show consistently low alignment scores across all third-party approaches, with LLMs outperforming humans but still achieving only fair agreement. Notably, demographic alignment between annotators and authors significantly improves human annotation performance, while including first-party demographic information in LLM prompts provides only marginal benefits.

## Method Summary
The research employed a multi-label emotion classification task comparing third-party annotations (both human and LLM) against first-party author labels. The dataset consisted of 729 social media posts from 123 participants across various demographics. Six human annotators evaluated each post (three in-group sharing all demographics with the author, three out-group differing on two or more traits), while five different LLMs were prompted with identical instructions. Performance was measured using Cohen's kappa, macro F1 scores, precision, and recall, with statistical significance tested through Wilcoxon signed-rank tests and mixed linear models.

## Key Results
- Third-party annotations poorly capture authors' intended emotions (Cohen's kappa 0-0.45; F1 scores 0.2-0.6)
- LLMs consistently outperform human annotators but still achieve only low to fair alignment scores
- Human annotators sharing demographic traits with authors show significantly better performance (median F1 0.29 vs 0.00 for out-group)

## Why This Works (Mechanism)
The study reveals that emotion inference from text is fundamentally challenging due to both the lack of explicit linguistic cues and subjective interpretation differences. The mechanism behind third-party annotation failure stems from the private nature of emotional states and the contextual dependencies of emotional expression. While LLMs leverage pattern recognition from training data to make more consistent predictions, they still struggle with nuanced emotions and the implicit nature of many emotional expressions in social media posts.

## Foundational Learning
- **Emotion taxonomy design**: Understanding the 28-category GoEmotions framework is crucial for interpreting annotation results and comparing across studies. Quick check: Can you map each emotion category to its definition?
- **Demographic alignment effects**: The significant performance difference between in-group and out-group annotators demonstrates how shared cultural and personal contexts influence emotion interpretation. Quick check: What demographic combinations showed the strongest alignment?
- **LLM annotation capabilities**: Comparing different models (GPT-4 Turbo, GPT-4o, Gemini 1.5 Pro/Flash, Claude 3.5 Sonnet) reveals variation in emotion recognition performance. Quick check: Which LLM achieved the highest average F1 score?

## Architecture Onboarding

**Component map**: First-party authors -> Posts (with text/images) -> Third-party annotators (humans/LLMs) -> Emotion labels -> Performance metrics (kappa, F1) -> Statistical analysis

**Critical path**: Data collection → Annotator assignment (in-group/out-group) → Emotion annotation → Aggregation (majority voting) → Performance evaluation → Statistical comparison

**Design tradeoffs**: The study chose a relatively small sample size (729 posts) for deeper demographic analysis over broader coverage, prioritizing detailed demographic stratification over scale. This enabled robust statistical comparisons of demographic alignment effects but may limit generalizability.

**Failure signatures**: Low inter-rater agreement causing high tie rates in majority voting, LLM over-labeling tendencies, and inconsistent recognition of nuanced emotions like relief or realization indicate the fundamental difficulty of emotion inference from text.

**First experiments**:
1. Replicate the demographic alignment test with a smaller subset of posts to verify the in-group/out-group performance gap
2. Run a single LLM with and without demographic prompting on a sample of 50 posts to measure the marginal benefit
3. Compare annotation consistency across the five LLMs on a common set of posts to identify performance patterns

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample size (729 posts from 123 participants) may limit generalizability
- Specific demographic composition (40% age 18-29, 51% Asian) may not represent broader populations
- Truncated LLM prompts in materials limit full reproducibility
- Does not account for cultural nuances beyond three racial categories examined

## Confidence
- **Core finding (third-party annotations poorly capture emotions)**: High confidence
- **LLM vs human performance comparison**: Medium confidence
- **Demographic alignment effect**: Medium confidence
- **Marginal benefit of demographic prompting**: Medium confidence

## Next Checks
1. Replicate the study with a larger, more diverse sample spanning additional racial/ethnic categories and cultural backgrounds
2. Conduct ablation studies varying LLM prompt structure (temperature, maximum tokens, few-shot examples)
3. Test the impact of including additional contextual information beyond demographics on annotation accuracy