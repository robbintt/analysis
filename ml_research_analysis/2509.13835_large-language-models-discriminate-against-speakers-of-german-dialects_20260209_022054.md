---
ver: rpa2
title: Large Language Models Discriminate Against Speakers of German Dialects
arxiv_id: '2509.13835'
source_url: https://arxiv.org/abs/2509.13835
tags:
- dialect
- bias
- task
- german
- standard
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates whether large language models (LLMs) exhibit\
  \ dialect-based stereotypes toward German dialect speakers, focusing on traits like\
  \ \u201Cuneducated\u201D and \u201Ccareless.\u201D Using association and decision\
  \ tasks, we test both explicit (dialect naming) and implicit (dialect usage) bias.\
  \ Our results show that all evaluated LLMs consistently exhibit significant bias\
  \ against dialect speakers, linking them to negative traits and lower-status occupations."
---

# Large Language Models Discriminate Against Speakers of German Dialects

## Quick Facts
- arXiv ID: 2509.13835
- Source URL: https://arxiv.org/abs/2509.13835
- Reference count: 40
- Key result: All evaluated LLMs show significant bias against German dialect speakers, with explicitly labeled dialects triggering stronger bias than implicit cues.

## Executive Summary
This study investigates whether large language models exhibit dialect-based stereotypes toward German dialect speakers, focusing on traits like “uneducated” and “careless.” Using association and decision tasks, we test both explicit (dialect naming) and implicit (dialect usage) bias. Our results show that all evaluated LLMs consistently exhibit significant bias against dialect speakers, linking them to negative traits and lower-status occupations. Notably, explicitly labeling linguistic demographics amplifies bias more than implicit cues. Larger LLMs within the same family show stronger biases, suggesting increased nuanced discrimination. These findings reveal that LLMs continue to display explicit discriminatory behavior toward German dialect speakers.

## Method Summary
The study uses a parallel corpus (WikiDIR) of 350 Wikipedia articles across 7 German dialects, translated to standard German via GPT-4o with human verification. Two tasks assess bias: an association task (IAT-inspired) measuring adjective assignments, and a decision task evaluating occupational recommendations. Bias scores are calculated on a [-1, 1] scale, with statistical tests (t-tests, Cohen’s d) comparing dialect naming vs. usage bias. Prompts control for writer order and adjective positioning to avoid positional bias.

## Key Results
- All LLMs exhibit significant dialect naming and dialect usage bias across association and decision tasks.
- Explicitly labeling dialects amplifies bias more than implicit dialect usage (70-88% of cases).
- Larger models within families show stronger dialect biases, suggesting scaling amplifies nuanced discrimination.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicitly labeling a speaker’s linguistic demographic (dialect naming) amplifies model bias more strongly than implicit cues (dialect usage).
- Mechanism: Explicit labels may activate stronger associations between the labeled group and stereotypical traits stored in the model’s representations, whereas implicit cues require the model to first infer the group before activating associated traits.
- Core assumption: LLMs internalize and can retrieve societal stereotypes present in their training data when cued appropriately.
- Evidence anchors:
  - [abstract] "explicitly labeling linguistic demographics—German dialect speakers—amplifies bias more than implicit cues like dialect usage."
  - [Section 5.1] "70% cases showing higher dialect naming bias than dialect usage bias" in association task; "88% cases show greater dialect naming bias" in decision task.
  - [corpus] No direct corpus evidence for this causal mechanism; claim is based on controlled experimental comparisons within the paper.
- Break condition: If future work shows that explicit labels trigger a different cognitive process (e.g., increased caution or refusal) rather than amplified stereotype activation, this mechanism would need revision.

### Mechanism 2
- Claim: Dialect usage alone (implicit linguistic features) is sufficient to trigger significant negative bias, even without explicit dialect labels.
- Mechanism: Non-standard linguistic patterns (lexical, morphological, syntactic) serve as cues that the model has learned to associate with lower social status traits during pretraining, leading to discriminatory outputs.
- Core assumption: The model’s training data contains texts where dialectal language co-occurs with negative stereotypical contexts or is underrepresented in high-status domains.
- Evidence anchors:
  - [abstract] "all evaluated LLMs exhibit significant dialect naming and dialect usage bias against German dialect speakers, reflected in negative adjective associations."
  - [Section 5.1] Significant bias scores for dialect usage bias across nearly all traits and models.
  - [Section 6.2] Dialect text elicits stronger bias than synthetically noisy text, indicating bias is not merely a response to low-perplexity/noisy input.
- Break condition: If bias from dialect usage were shown to disappear when controlling for topic or domain (e.g., dialect text is always about rural topics), the mechanism might be confounded by content rather than linguistic form.

### Mechanism 3
- Claim: Larger LLMs within the same model family exhibit stronger dialect biases, suggesting scaling amplifies nuanced discriminatory capabilities.
- Mechanism: Improved language understanding and world knowledge in larger models may enhance their ability to recognize and leverage subtle associations between linguistic varieties and social stereotypes.
- Core assumption: Larger models capture more complex statistical regularities, including those linking dialect to social traits, and are less constrained by safety fine-tuning on this specific issue.
- Evidence anchors:
  - [Section 5.1] "Larger LLMs within the same family show stronger dialect naming and dialect usage biases for both the association and the decision task" with specific percentages (e.g., 90% higher dialect usage bias in association task).
  - [corpus] Related work (Hofmann et al., 2024; Bai et al., 2024) shows similar scaling effects for other bias dimensions, providing indirect support.
- Break condition: If larger models were found to have been trained on significantly more stereotyped dialect data, the effect could be data-driven rather than an emergent scaling property.

## Foundational Learning
- Concept: **Perceptual Dialectology**
  - Why needed here: The paper’s traits (uneducated, careless, rural, etc.) are drawn from sociolinguistic literature on how humans perceive dialects. Understanding this helps interpret why LLMs exhibit these specific biases.
  - Quick check question: Why might the trait "friendly" be the only positive stereotype in human perception, and why do LLMs potentially reverse it?
- Concept: **Implicit Association Test (IAT) adapted for LLMs**
  - Why needed here: The association task uses an IAT-inspired method to measure the strength of trait associations. Grasping its logic (measuring relative association strengths) is key to understanding the bias scores.
  - Quick check question: In the association task, what does a bias score of +1 mean, and what does 0 mean?
- Concept: **Decision Tasks as Proxies for Real-World Harm**
  - Why needed here: The decision task moves beyond measuring associations to assessing biased actions (e.g., job recommendations). This links abstract bias to potential downstream harms.
  - Quick check question: Why is the decision task critical for demonstrating the potential real-world impact of dialect bias in LLMs?

## Architecture Onboarding
- Component map: WikiDIR dataset -> dialect sampling -> GPT-4o translation -> manual verification -> paired dialect/standard texts -> prompting framework -> association/decision task -> bias scoring engine -> analysis tools
- Critical path:
  1. Build/obtain a parallel corpus in the target language varieties.
  2. Design prompts for both association and decision tasks, carefully controlling for positional bias.
  3. Run experiments across multiple model families and sizes.
  4. Calculate bias scores and perform statistical comparisons.
  5. Conduct robustness checks (e.g., noisy text baseline, topic variation).
- Design tradeoffs:
  - **Forced-choice vs. neutral option**: The main study uses forced-choice to reveal underlying bias; adding a neutral option (Section B.3) shows weaker but still significant bias. Choose based on whether you want to measure maximal latent bias or more realistic behavior.
  - **Prompt language**: Instructions in English, content in German/dialect. This improves instruction-following but may introduce cross-lingual artifacts.
- Failure signatures:
  - **High instruction rejection rates**: Smaller models may fail to follow the response format, leading to discarded samples. Monitor rejection rates.
  - **Noisy text confound**: If bias scores for dialect text are not significantly higher than for synthetically noisy text, the result may be explained by perceived text quality rather than dialect stereotypes. Always include this control.
  - **Positional bias**: If results flip systematically with Writer A/B order swapping, the design is flawed. The paper randomizes order to mitigate this.
- First 3 experiments:
  1. **Replication on a different language**: Apply the same framework to another language with dialectal variation (e.g., Italian, Arabic) to test cross-linguistic generalizability. Use the corpus neighbors (e.g., INDIC DIALECT) for guidance.
  2. **Mitigation probing**: Test whether prompt-based interventions (e.g., instructing the model to avoid stereotypes) reduce bias scores in the decision task.
  3. **Fine-grained dialect analysis**: Instead of aggregating across dialects, analyze each dialect separately with larger sample sizes to see if bias strength correlates with specific linguistic distance from standard German or with socioeconomic stereotypes of the regions.

## Open Questions the Paper Calls Out
None

## Limitations
- Dialect Generalization: Findings may not generalize to other languages or dialect families without cross-linguistic validation.
- Causal Attribution: Cannot definitively attribute bias to linguistic stereotypes versus domain/topic associations.
- Scaling Interpretation: Unclear if larger model bias reflects emergent capability or more stereotyped training data.

## Confidence
- **High Confidence**: LLMs exhibit significant dialect-based bias in both explicit and implicit conditions; bias is robust across model families and tasks.
- **Medium Confidence**: Dialect naming amplifies bias more than dialect usage; larger models within families show stronger bias.
- **Low Confidence**: The precise mechanism linking dialect form to stereotype activation; whether bias is linguistic or topical in origin.

## Next Checks
1. **Cross-linguistic Replication**: Apply the same experimental framework to another language with dialectal variation (e.g., Italian, Arabic) to test generalizability.
2. **Domain Control**: Re-run the dialect usage task with matched topics across dialect and standard texts to isolate linguistic form effects from content confounds.
3. **Bias Mitigation Testing**: Test whether prompt interventions (e.g., "avoid stereotypes") reduce dialect bias scores in the decision task, providing initial evidence for potential mitigation strategies.