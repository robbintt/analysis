---
ver: rpa2
title: 'ConceptFlow: Hierarchical and Fine-grained Concept-Based Explanation for Convolutional
  Neural Networks'
arxiv_id: '2509.18147'
source_url: https://arxiv.org/abs/2509.18147
tags:
- concept
- concepts
- conceptual
- pathways
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces ConceptFlow, a framework for interpreting\
  \ CNNs by tracing how high-level concepts evolve across layers. It combines filter-wise\
  \ concept attentions\u2014associating each filter with relevant semantic concepts\u2014\
  and conceptual pathways\u2014quantifying how concepts transform between filters\
  \ via a concept transition matrix."
---

# ConceptFlow: Hierarchical and Fine-grained Concept-Based Explanation for Convolutional Neural Networks

## Quick Facts
- arXiv ID: 2509.18147
- Source URL: https://arxiv.org/abs/2509.18147
- Authors: Xinyu Mu; Hui Dou; Furao Shen; Jian Zhao
- Reference count: 25
- Key outcome: Framework traces how high-level concepts evolve across CNN layers, showing that conceptual pathways are functionally critical for model reasoning.

## Executive Summary
ConceptFlow provides a novel framework for interpreting convolutional neural networks by mapping how semantic concepts evolve across layers. It introduces filter-wise concept attentions that associate each filter with relevant high-level concepts, and conceptual pathways that quantify how concepts transform between filters. Experiments demonstrate that ConceptFlow reveals semantically meaningful transitions aligned with human reasoning and that removing these conceptual pathways sharply degrades model performance, highlighting their importance for model reasoning.

## Method Summary
ConceptFlow combines learning images per filter, cross-attention between patch and concept embeddings, and Spearman correlation analysis to extract conceptual pathways. The method generates filter-specific learning images, enhances inputs with these images, computes concept attention via cross-attention, and identifies conceptual flows through correlation thresholding. The framework culminates in a concept transition matrix that enables Markov-style analysis of concept propagation through the network.

## Key Results
- Demonstrates semantically meaningful concept transitions aligned with human reasoning on CMNIST and CAWA datasets
- Shows that removing conceptual pathways causes disproportionate accuracy degradation compared to random pruning
- Validates that hierarchical concept evolution (e.g., strokes → shapes → digits) can be traced through network layers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Overlaying filter-specific learning images enhances concept representations, enabling accurate filter-to-concept associations.
- Mechanism: The Neural Network Scanner generates a learning image per filter. Blending this image with the original input (ratio=1) amplifies the feature patterns the filter has learned. A cross-attention module then computes attention scores between the modified input's patch embeddings and a trainable concept embedding matrix, producing a concept attention vector for that filter.
- Core assumption: The features captured in a filter's learning image correspond to the semantic concepts that activate that filter.
- Evidence anchors:
  - [section] Section 3.2 describes computing concept attention by feeding the learning-image-enhanced input through the backbone and cross-attention module.
  - [abstract] Mentions "concept attentions, which associate each filter with relevant high-level concepts."
  - [corpus] SL-CBM and related CBM papers establish concept bottleneck paradigms but lack filter-level granularity.
- Break condition: If learning images fail to capture discriminative patterns or if the concept set lacks relevant concepts, attention scores become uninformative.

### Mechanism 2
- Claim: Strong monotonic relationships between concept attention distributions across adjacent-layer filters indicate conceptual flow.
- Mechanism: For each filter, concept attention vectors are aggregated across samples into a concept attention matrix. Spearman's rank correlation is computed between all concept pairs across adjacent filters. High absolute correlation (above threshold τ) suggests that when concept i is activated in an earlier filter, concept j tends to be activated in a later filter, revealing a conceptual pathway.
- Core assumption: Concept propagation manifests as correlated attention distributions across samples; if concept A causally enables concept B, their activations will co-vary monotonically.
- Evidence anchors:
  - [section] Section 3.3 defines the Spearman matrix P and thresholding to extract conceptual flows.
  - [abstract] States that conceptual pathways are "derived from a concept transition matrix that quantifies how concepts propagate and transform between filters."
  - [corpus] Corpus shows interest in mechanistic interpretability (e.g., structured knowledge graphs for concept tracing), but Spearman-based pathway extraction is novel here.
- Break condition: If correlations are spurious or driven by dataset artifacts, extracted pathways may not reflect true model reasoning. Importance sampling and top-k selection mitigate but do not eliminate this risk.

### Mechanism 3
- Claim: Pruning connections identified as conceptual pathways disproportionately degrades model performance, indicating these pathways are functionally critical.
- Mechanism: Conceptual pathway pruning zeroes out specific weight matrix entries corresponding to extracted inter-filter conceptual flows. As pruning aggressiveness increases (lower τ), performance drops sharply compared to random or magnitude-based pruning.
- Core assumption: If conceptual pathways genuinely encode reasoning steps, their disruption should impair decision-making more than disrupting arbitrary connections.
- Evidence anchors:
  - [abstract] Highlights that "removing conceptual pathways sharply degrades model performance."
  - [section] Section 4.3 and Figure 10 show accuracy collapse (to ~50% on CMNIST, below random on CAWA) when conceptual pathways are pruned.
  - [corpus] Corpus lacks comparable pathway-pruning validation; most concept-based methods focus on attribution rather than pathway importance.
- Break condition: If pathways capture correlations without causal roles in model computation, pruning effects would be indistinguishable from random pruning. Evidence suggests differentiation, but causal mechanism is inferred, not proven.

## Foundational Learning

- Concept: Cross-attention mechanism
  - Why needed here: ConceptFlow uses cross-attention between visual patch embeddings and concept embeddings to compute relevance scores. Understanding query-key-value operations is essential for interpreting how filter representations map to concepts.
  - Quick check question: Can you explain how the softmax of scaled dot-products between queries and keys produces attention weights?

- Concept: Spearman rank correlation
  - Why needed here: Pathway extraction relies on Spearman correlation to detect monotonic relationships in concept attention distributions. Unlike Pearson, Spearman captures non-linear monotonic trends.
  - Quick check question: If two variables have a perfect Spearman correlation of 1, what does that imply about their rank ordering?

- Concept: Transition matrices and Markov properties
  - Why needed here: The concept transition matrix formalizes inter-filter concept propagation as a local heterogeneous Markov process. Understanding row normalization and multi-step transitions clarifies how ConceptFlow quantifies cumulative conceptual flow.
  - Quick check question: Given a row-stochastic transition matrix, what does each entry represent, and how do you compute a 2-step transition probability?

## Architecture Onboarding

- Component map: NNS → learning image blending → backbone feature extraction → cross-attention → concept attention matrix → Spearman correlation → threshold τ → conceptual pathways → transition matrix

- Critical path: Neural Network Scanner generates learning images per filter, which are blended with inputs and processed through the backbone CNN. The cross-attention module computes concept attention vectors, which are aggregated and correlated across filters to extract conceptual pathways via thresholding.

- Design tradeoffs:
  - **Threshold τ**: Higher τ yields fewer but more confident pathways; lower τ captures more flows but risks spurious correlations.
  - **Top-k selection**: Using only top-k activated concepts per filter reduces noise but may miss subtle conceptual interactions.
  - **Concept set design**: Requires hierarchical, domain-specific concept annotations; quality directly affects interpretability fidelity.

- Failure signatures:
  - **Flat attention distributions**: If concept attention vectors are near-uniform, filter-to-concept mapping has failed (check learning image quality or concept set relevance).
  - **Pathway sparsity or overload**: Too few pathways suggest τ is too high or concept set is misaligned; too many suggest τ is too low.
  - **Pruning insensitivity**: If conceptual pathway pruning behaves like random pruning, extracted pathways may not reflect functional reasoning (revisit correlation assumptions).

- First 3 experiments:
  1. **Validate filter-wise concept attention sensitivity**: Mask or enhance specific input regions and verify that concept attention changes appropriately (e.g., Figure 3, 4).
  2. **Visualize hierarchical pathway evolution**: Extract and cluster concept transition matrices for a target class; confirm semantic progression (e.g., strokes → shapes → digits for CMNIST).
  3. **Ablation via pathway pruning**: Compare conceptual pathway pruning against L1-norm and non-conceptual pruning; confirm disproportionate accuracy degradation (e.g., Figure 10).

## Open Questions the Paper Calls Out
None

## Limitations
- The assumption that filter-specific learning images reliably capture semantically relevant features is not empirically validated.
- Spearman correlation-based pathway extraction may capture spurious statistical relationships rather than genuine causal reasoning.
- The pruning experiments show disproportionate performance degradation, but this could potentially be explained by pathway connections having higher-magnitude weights rather than being functionally important for reasoning.

## Confidence
- **High**: The framework's architectural design (learning images + cross-attention + correlation analysis) is internally consistent and technically sound.
- **Medium**: The semantic interpretability of extracted pathways is plausible but not definitively proven; human validation studies would strengthen claims.
- **Low**: The causal importance of conceptual pathways for model reasoning remains inferential; correlation-based pruning effects could have alternative explanations.

## Next Checks
1. Conduct ablation studies with permuted concept labels to test whether pathway importance persists when semantic meaning is destroyed.
2. Compare Spearman-based pathway extraction against alternative methods (mutual information, Granger causality) to assess robustness of extracted flows.
3. Perform human evaluation studies where annotators trace concept evolution across layers and rate agreement with ConceptFlow's extracted pathways.