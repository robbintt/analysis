---
ver: rpa2
title: 'SlideAgent: Hierarchical Agentic Framework for Multi-Page Visual Document
  Understanding'
arxiv_id: '2510.26615'
source_url: https://arxiv.org/abs/2510.26615
tags:
- slide
- slideagent
- page
- visual
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "SlideAgent introduces a hierarchical, multi-agent framework for\
  \ fine-grained reasoning over multi-page visual documents such as slide decks. It\
  \ decomposes document understanding into three levels\u2014global, page, and element\u2014\
  each handled by specialized agents that generate query-agnostic knowledge."
---

# SlideAgent: Hierarchical Agentic Framework for Multi-Page Visual Document Understanding

## Quick Facts
- arXiv ID: 2510.26615
- Source URL: https://arxiv.org/abs/2510.26615
- Authors: Yiqiao Jin; Rachneet Kaur; Zhen Zeng; Sumitra Ganesh; Srijan Kumar
- Reference count: 40
- Primary result: SlideAgent improves over both proprietary (+7.9 overall, +8.3 numeric) and open-source models (+9.8 overall, +11.7 numeric) on multi-page visual document understanding

## Executive Summary
SlideAgent introduces a hierarchical, multi-agent framework for fine-grained reasoning over multi-page visual documents such as slide decks. It decomposes document understanding into three levels—global, page, and element—each handled by specialized agents that generate query-agnostic knowledge. During inference, agents are selectively activated and retrieve query-specific content for synthesis. Experiments on SlideVQA, TechSlides, and FinSlides show SlideAgent improves over both proprietary (+7.9 overall, +8.3 numeric) and open-source models (+9.8 overall, +11.7 numeric), excelling in multi-hop and visual reasoning. It also enhances page-level retrieval performance, especially with text-based retrievers. The ablation study confirms the critical role of page and element-level reasoning, while global guidance adds thematic coherence.

## Method Summary
SlideAgent uses a two-stage framework: knowledge construction and inference. During construction, three specialized agents process slide decks hierarchically—global agent analyzes first 3 pages for themes, page agents sequentially process each page with cross-page context, and element agents decompose pages into discrete components with semantic roles. This generates query-agnostic knowledge (Kg, Kp, Ke) stored hierarchically. During inference, a query classifier activates relevant agents, subqueries are generated for retrieval, and an answer synthesizer combines agent outputs when they disagree. The system uses structured knowledge generation, selective agent activation, and fuzzy matching for synthesis.

## Key Results
- Improves over proprietary models (+7.9 overall accuracy, +8.3 numeric accuracy)
- Improves over open-source models (+9.8 overall accuracy, +11.7 numeric accuracy)
- Excels in multi-hop reasoning (+9.8 improvement) and visual reasoning (+8.5 improvement)
- Enhances text-based retriever performance (+6.4 MRR for SFR, +4.9 MRR for BM25)

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Decomposition
- Claim: Decomposing document understanding into global, page, and element levels improves fine-grained reasoning by reducing cognitive load
- Mechanism: Global agent samples first 3 pages for themes, page agents process sequentially with cross-page context, element agents decompose pages into components with bounding boxes and semantic roles
- Core assumption: Document meaning distributes predictably across structural hierarchies, each level contributing distinct reasoning signals
- Evidence: Hierarchical decomposition works in similar frameworks (MHier-RAG), ablation shows degradation without page/element agents

### Mechanism 2: Query-Agnostic Knowledge Construction
- Claim: Structured query-agnostic knowledge improves retrieval over raw OCR or page embeddings
- Mechanism: Agents generate structured descriptions during construction, subqueries target key entities during inference, enabling richer semantic signals
- Core assumption: Structured textual descriptions capture more retrieval-relevant semantics than raw visual embeddings
- Evidence: Text-based retrievers show +6.4 MRR improvement with SlideAgent artifacts; structured understanding enhances RAG retrieval

### Mechanism 3: Selective Agent Activation
- Claim: Query classification reduces noise and computational cost while maintaining answer quality
- Mechanism: Query types (global, fact-based, multi-hop, layout/visual) map to specific agent combinations; unknown queries activate all agents
- Core assumption: Query intent can be reliably classified, different query types benefit from different reasoning perspectives
- Evidence: Multi-hop reasoning shows +9.8 improvement, suggesting specialization helps complex queries most

## Foundational Learning

- Concept: Hierarchical document representation
  - Why needed: SlideAgent relies on understanding how document structure maps to reasoning granularity
  - Quick check: Given a slide deck about Q3 earnings, what knowledge belongs at global vs. page vs. element level?

- Concept: Retrieval-augmented generation with structured knowledge
  - Why needed: System pre-indexes query-agnostic knowledge and retrieves query-specific content
  - Quick check: Why does generating subqueries from original query improve retrieval over using raw query directly?

- Concept: Agent orchestration and answer synthesis
  - Why needed: Multiple agents may generate conflicting answers requiring synthesis
  - Quick check: If page agent says "revenue is $5M" and element agent says "revenue is $5.2M," how should synthesis handle this disagreement?

## Architecture Onboarding

- Component map:
  - Global Agent → processes first 3 pages → generates Kg (summary, objectives, structure)
  - Page Agent → processes pages sequentially → generates Kp conditioned on Kg and previous Kp
  - Element Agent → uses Docling/EasyOCR → generates Ke with bounding boxes, types, semantic roles
  - Agent Orchestrator → classifies query type → activates relevant agents
  - Retriever (SFR default) → uses subqueries + knowledge artifacts → returns top-k pages/elements
  - Answer Synthesizer → combines hg, hp, he → produces final answer when agents disagree

- Critical path:
  1. Knowledge construction: Global → Page (sequential) → Element (parallel per page)
  2. Inference: Query classification → Subquery generation → Retrieval → Agent reasoning → Synthesis
  3. Bottleneck: Page agent must process pages sequentially due to context dependency

- Design tradeoffs:
  - Sequential page processing preserves coherence but limits parallelization
  - Text-based retrievers are efficient but may miss purely visual patterns
  - Query-agnostic knowledge enables reuse but requires upfront computation
  - Three-level hierarchy adds complexity vs. end-to-end models

- Failure signatures:
  - Element fragmentation from OCR—boxes too small/granular
  - Subquery generation noise leading to irrelevant retrievals
  - Agent disagreement when synthesis cannot resolve conflicting answers
  - Classification errors routing queries to wrong agent combinations

- First 3 experiments:
  1. Replicate ablation on SlideVQA subset: Remove page agent, measure degradation on multi-hop questions
  2. Test retrieval-only improvement: Apply subquery generation + page knowledge to BM25 on TechSlides
  3. Stress test element parsing: Run element agent on slides with complex layouts, inspect bounding box accuracy

## Open Questions the Paper Calls Out

### Open Question 1: Element Relations
- Question: Does explicitly modeling inter-element relations using graph structures improve multi-hop reasoning?
- Basis: Appendix E notes extending to model relations between elements offers promising direction
- Why unresolved: Current framework treats elements independently without capturing structural relationships
- What evidence would resolve: Ablation study comparing independent element treatment against graph-based approach on multi-hop reasoning

### Open Question 2: Domain-Specific Retrievers
- Question: How much do domain-specific multimodal retrievers improve performance vs. text-based retrievers?
- Basis: Section 6 suggests exploring multimodal retrievers or domain-specific strategies for slide decks
- Why unresolved: Paper focused on text-based retrieval; multimodal retrievers were mentioned but not benchmarked
- What evidence would resolve: Benchmarking SlideAgent using specialized multimodal retrievers on TechSlides and FinSlides

### Open Question 3: Element Parsing Reliability
- Question: How does noise from external element parsing pipeline impact downstream reasoning reliability?
- Basis: Appendix E notes external tools may struggle with visually complex or low-contrast designs
- Why unresolved: Evaluation datasets may not systematically cover edge cases where layout parsing fails
- What evidence would resolve: Error analysis correlating layout parsing confidence scores with final answer accuracy

## Limitations
- Exact prompt templates for page agent and query classifier remain unspecified
- Top-k retrieval values used in experiments are not documented
- Subquery generation mechanism implementation details are unclear
- Performance gains over proprietary models should be contextualized against model size differences

## Confidence
- **High confidence**: Hierarchical decomposition mechanism reduces cognitive load for fine-grained reasoning
- **Medium confidence**: Retrieval improvement claims for text-based retrievers, though implementation details are unclear
- **Low confidence**: Selective agent activation mechanism's independent contribution to performance

## Next Checks
1. Implement ablation variant activating all agents for all query types to isolate selective activation contribution
2. Apply only subquery generation and structured knowledge construction to baseline retriever on TechSlides
3. Run element agent on slides with challenging layouts and manually evaluate parsing quality