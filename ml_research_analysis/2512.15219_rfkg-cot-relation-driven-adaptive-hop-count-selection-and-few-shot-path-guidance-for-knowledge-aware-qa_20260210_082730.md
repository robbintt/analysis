---
ver: rpa2
title: 'RFKG-CoT: Relation-Driven Adaptive Hop-count Selection and Few-Shot Path Guidance
  for Knowledge-Aware QA'
arxiv_id: '2512.15219'
source_url: https://arxiv.org/abs/2512.15219
tags:
- reasoning
- paths
- path
- knowledge
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'RFKG-CoT improves knowledge-intensive QA by addressing two limitations
  in KG-CoT: rigid hop-count selection and underutilization of reasoning paths. It
  introduces a relation-driven adaptive hop-count selector that dynamically adjusts
  reasoning steps based on KG relations (e.g., 1-hop for direct relations, 2-hop for
  indirect chains), formalized via a relation mask.'
---

# RFKG-CoT: Relation-Driven Adaptive Hop-count Selection and Few-Shot Path Guidance for Knowledge-Aware QA

## Quick Facts
- arXiv ID: 2512.15219
- Source URL: https://arxiv.org/abs/2512.15219
- Reference count: 9
- RFKG-CoT improves KGQA accuracy by up to 14.7 percentage points on WebQSP benchmark

## Executive Summary
RFKG-CoT addresses critical limitations in KG-CoT by introducing two complementary innovations for knowledge-intensive question answering. The approach dynamically adjusts reasoning hop counts based on KG relation types through a relation-driven selector, replacing KG-CoT's rigid hop-count selection. Additionally, it incorporates few-shot in-context learning path guidance with Chain-of-Thought prompting to teach LLMs how to interpret reasoning paths effectively. Experimental results demonstrate significant accuracy improvements across four KGQA benchmarks, with the two components working synergistically to transform KG evidence into more faithful answers.

## Method Summary
RFKG-CoT builds upon KG-CoT's foundation but addresses two key limitations through innovative solutions. First, it replaces the rigid hop-count selection with a relation-driven adaptive mechanism that analyzes KG relations to determine optimal reasoning steps - typically 1-hop for direct relations and 2-hop for indirect chains, formalized via a relation mask. Second, it introduces few-shot in-context learning path guidance with CoT prompting to teach LLMs how to interpret reasoning paths rather than relying on fixed templates. These components work together to improve the faithfulness and accuracy of knowledge-grounded answers in KGQA tasks.

## Key Results
- Achieves up to 14.7 percentage point accuracy improvement on WebQSP benchmark using Llama2-7B
- Demonstrates consistent performance gains across four KGQA benchmarks
- Ablation studies confirm that relation-driven hop-count selector and path guidance components are complementary

## Why This Works (Mechanism)
The method works by addressing fundamental limitations in how KG-CoT handles knowledge graph traversal and path interpretation. The relation-driven adaptive hop-count selector enables more nuanced reasoning by analyzing the semantic properties of KG relations rather than applying fixed step counts. This allows the system to appropriately handle both direct and indirect relationship chains. The few-shot path guidance component teaches LLMs to interpret reasoning paths through in-context examples, improving their ability to extract meaningful answers from complex KG structures. Together, these innovations enable more faithful transformation of KG evidence into accurate answers.

## Foundational Learning
- Knowledge Graph Relations: Why needed - to understand semantic connections between entities; Quick check - verify relation types and their cardinalities
- Chain-of-Thought Prompting: Why needed - to guide LLM reasoning through complex multi-step problems; Quick check - test with simple arithmetic examples
- Hop-count Selection: Why needed - determines depth of graph traversal for answer extraction; Quick check - compare fixed vs adaptive step counts on sample queries
- In-context Learning: Why needed - enables few-shot teaching without model fine-tuning; Quick check - measure performance with varying numbers of examples
- Relation Masking: Why needed - formalizes dynamic hop-count decisions based on relation properties; Quick check - validate mask generation against known relation patterns
- Path Interpretation: Why needed - converts reasoning paths into final answers; Quick check - test path-to-answer conversion on simple chains

## Architecture Onboarding

Component Map: Input Query -> Relation Analyzer -> Hop-count Selector -> KG Traversal -> Path Generator -> Few-shot Prompt Builder -> LLM -> Output Answer

Critical Path: The most timing-sensitive sequence is Relation Analyzer -> Hop-count Selector -> KG Traversal, as delays here bottleneck the entire pipeline before LLM processing begins.

Design Tradeoffs:
- Adaptive vs Fixed Hop-count: Adaptive provides better accuracy but adds computational overhead for relation analysis
- Few-shot vs Zero-shot Path Guidance: Few-shot improves performance but requires high-quality path examples
- Relation Mask Granularity: More granular masks provide better adaptation but increase complexity

Failure Signatures:
- Over-short paths: Missing answers due to insufficient hop counts
- Over-long paths: Noisy answers from excessive traversal
- Poor relation mask quality: Incorrect hop-count decisions leading to failed reasoning
- Inadequate path examples: LLMs fail to learn proper path interpretation

First 3 Experiments:
1. Test relation-driven hop-count selector on a simple KG with known relation patterns
2. Validate few-shot path guidance with controlled path examples and simple queries
3. Measure accuracy degradation when removing either component to confirm complementarity

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Computational overhead from relation-driven hop-count selector not thoroughly analyzed
- Effectiveness depends heavily on KG relation quality and completeness
- Few-shot path guidance scalability limited by availability of high-quality annotated paths
- No runtime or token efficiency comparisons with baseline approaches provided

## Confidence

- High confidence: Core architectural contributions are technically sound and well-motivated
- Medium confidence: Reported accuracy improvements are valid but generalization remains uncertain
- Low confidence: Claims about component complementarity lack quantitative support beyond same-dataset ablations

## Next Checks
1. Conduct runtime and token efficiency analysis comparing RFKG-CoT against KG-CoT and other baselines across all benchmark datasets
2. Test relation-driven hop-count selector on KGs with varying relation completeness to assess robustness
3. Evaluate few-shot path guidance performance when path examples are synthetically degraded or reduced in number