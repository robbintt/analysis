---
ver: rpa2
title: A Comparative Review of RNA Language Models
arxiv_id: '2505.09087'
source_url: https://arxiv.org/abs/2505.09087
tags:
- structure
- prediction
- secondary
- sequences
- protein
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study provides a systematic comparison of 13 RNA language
  models, 3 DNA models, and 1 protein model on zero-shot tasks of RNA secondary structure
  prediction and functional classification. Models were grouped into three categories:
  (I) pretrained on diverse ncRNA types, (II) specific-purpose RNAs, and (III) unified
  models for RNA/DNA/proteins.'
---

# A Comparative Review of RNA Language Models

## Quick Facts
- arXiv ID: 2505.09087
- Source URL: https://arxiv.org/abs/2505.09087
- Authors: He Wang; Yikun Zhang; Jie Chen; Jian Zhan; Yaoqi Zhou
- Reference count: 0
- Models trained on diverse ncRNA types generally outperformed DNA models in functional classification, with RNA-FM achieving the best overlap ratio (0.070).

## Executive Summary
This systematic comparison evaluates 13 RNA language models, 3 DNA models, and 1 protein model on zero-shot tasks of RNA secondary structure prediction and functional classification. Models were categorized into three groups: (I) pretrained on diverse ncRNA types, (II) specific-purpose RNAs, and (III) unified models for RNA/DNA/proteins. The study reveals that Class I models generally outperform DNA models in functional classification, while RNA-MSM (MSA-based) achieves the best secondary structure prediction despite training on only ~4000 families. Notably, models excelling in structure prediction (e.g., RNA-km, MP-RNA) perform poorly in classification, indicating unbalanced training.

## Method Summary
The paper evaluates models on two zero-shot tasks: secondary structure prediction via attention map extraction and classification via sequence embedding similarity. For structure prediction, attention maps from each head-layer are extracted, symmetrized, corrected with APC, and thresholded to obtain binary contact maps. The optimal head-layer and threshold are selected on validation set VL1 and evaluated on test set TS. For classification, embeddings are reduced to 128D via FFT, and overlap ratio between homologous and non-homologous similarity distributions is computed. The evaluation uses datasets including Rfam families, ArchiveII, and VL1/TS sets with PDB structures.

## Key Results
- Class I models outperformed DNA models in functional classification, with RNA-FM achieving the best overlap ratio (0.070)
- RNA-MSM (MSA-based) performed best in secondary structure prediction despite training on only ~4000 families, outperforming AIDO.RNA (1.6B parameters)
- Models excelling in secondary structure (RNA-km, MP-RNA) performed poorly in classification, indicating unbalanced training
- Protein models (ESM2) achieved higher and more consistent F1 scores than RNA models, likely due to greater sequence conservation in proteins

## Why This Works (Mechanism)

### Mechanism 1
MSA-based evolutionary information substantially improves secondary structure prediction over single-sequence models, even with smaller training data. Homologous sequence alignments expose correlated mutations that constrain base-pairing relationships. The MSA-Transformer architecture directly encodes these co-evolutionary signals into attention maps, bypassing the need to infer pairing rules from single sequences alone. Core assumption: RNA-MSM's success generalizes beyond the ~4000 Rfam families it was trained on; sufficient homologs exist for target RNAs. Break condition: Target RNAs lack detectable homologs in sequence databases → MSA generation fails → mechanism degrades to single-sequence performance.

### Mechanism 2
Structure-enhanced pre-training objectives create embedding representations that trade off functional discrimination ability. Models like RNA-km and MP-RNA bias attention toward local base-pairing patterns (k-mers, explicit structure annotations). This improves secondary structure F1 but compresses embeddings around structural motifs rather than functional family signatures, reducing separability in homology classification. Core assumption: The observed performance trade-off reflects representational interference, not simply dataset bias or insufficient evaluation. Break condition: Multi-task pre-training with explicit regularization across structure and function objectives could mitigate the trade-off.

### Mechanism 3
Diverse ncRNA pre-training corpora provide stronger functional representations than DNA-only or domain-specific RNA datasets. Class I models (RNA-FM, etc.) trained on RNAcentral's broad ncRNA distribution learn family-conserved sequence patterns across tRNA, rRNA, miRNA, and other classes. This diversity yields embeddings that better separate homologous from non-homologous sequences compared to DNA LMs or narrowly scoped RNA models. Core assumption: RNAcentral's ~23M sequences provide sufficient coverage of functional RNA families; rRNA/tRNA dominance (61%/22.5%) does not catastrophically bias representations. Break condition: Target RNAs belong to families poorly represented in RNAcentral (e.g., metagenomic RNAs) → embeddings lack discriminative power.

## Foundational Learning

- Concept: **Masked Language Modeling (MLM) for nucleotide sequences**
  - Why needed here: All compared RNA LMs use BERT-style pre-training where 15% of tokens are masked/modified. Understanding this is essential for interpreting what embeddings capture.
  - Quick check question: Can you explain why bidirectional masking helps RNA structure prediction compared to autoregressive (GPT-style) approaches?

- Concept: **Attention map interpretation for secondary structure**
  - Why needed here: The paper extracts secondary structure predictions directly from attention maps without fine-tuning. This requires understanding how self-attention heads can encode base-pairing relationships.
  - Quick check question: Given an attention matrix from layer L, head H, how would you convert it to a binary contact map?

- Concept: **Zero-shot evaluation methodology**
  - Why needed here: The paper's conclusions rest on zero-shot (no fine-tuning) comparisons. This isolates what pre-training alone teaches models, independent of downstream task optimization.
  - Quick check question: Why might a model with lower zero-shot performance outperform a higher one after fine-tuning?

## Architecture Onboarding

- Component map: Input tokenization (single-base/k-mer/BPE/MSA) -> Stacked Transformer layers (6-33) -> Attention heads (12-40 per layer) -> Embedding dimension (128-2560) -> Max sequence length (512-131,072)

- Critical path: 1) Choose tokenization strategy (k-mer captures local dependencies; MSA requires homolog generation) 2) Select pre-training corpus (RNAcentral for general; specialized for purpose-specific) 3) Define pre-training objective (MLM alone vs. structure-enhanced variants) 4) Extract representations: attention maps for structure, embeddings for classification

- Design tradeoffs: Model size vs. compute (RiNALMo shows 28% F1 improvement scaling from 148M to 651M parameters); Structure vs. function (RNA-km/MP-RNA optimize structure at embedding cost; RNA-FM balances both); MSA requirement (RNA-MSM needs homologs; single-sequence models work de novo but with lower F1)

- Failure signatures: Near-random F1 scores on secondary structure → likely DNA or Class II model applied to ncRNA; High variance across individual RNAs → single-sequence RNA LM on challenging families; OR ≈ 0.5+ on classification → embeddings lack family-level discrimination

- First 3 experiments: 1) Reproduce zero-shot structure pipeline: Take RNA-FM, extract attention from optimal head-layer, apply APC correction and thresholding, evaluate F1 on TS dataset 2) Test structure-function trade-off hypothesis: Compare RNA-km vs. RNA-FM embeddings on held-out Rfam family; visualize t-SNE of homolog vs. non-homolog separability 3) Validate MSA advantage boundary: Run RNA-MSM on RNAs with <10 homologs vs. >100 homologs; quantify F1 degradation curve

## Open Questions the Paper Calls Out

### Open Question 1
How can RNA language models be trained to achieve balanced performance between secondary structure prediction and functional classification? The authors state "models doing well on secondary structure prediction often perform worse in function classification or vice versa, suggesting that more balanced unsupervised training is needed" and that models like RNA-km and MP-RNA performed well on structure but poorly on classification.

### Open Question 2
Will pre-training on substantially larger RNA databases (e.g., MARS with 1B sequences) improve RNA language model performance? The authors note "it is not certain if a large RNA database (MARS) will also help because UNI-RNA is not yet openly available."

### Open Question 3
Can RNA language models match protein language model performance with larger model sizes alone? The paper shows protein ESM2 achieves consistent F1 scores (~0.67) largely independent of model size, while RNA models like AIDO.RNA improved 32.2% from 648M to 1.6B parameters, suggesting RNA models may need different scaling laws or approaches.

## Limitations

- The RNAcentral corpus composition (61% rRNA, 22.5% tRNA) creates potential representation bias that wasn't systematically evaluated
- Zero-shot evaluation framework doesn't account for potential performance gains from task-specific fine-tuning
- Attention-based secondary structure prediction pipeline relies on unstated implementation details (APC correction formulas, symmetrization methods, threshold optimization)

## Confidence

- High confidence: MSA-based models outperform single-sequence models on secondary structure prediction (Mechanism 1) - aligns with established evolutionary biology principles about co-evolutionary constraints
- Medium confidence: Structure-function trade-off hypothesis (Mechanism 2) - empirical pattern observed but causal mechanism remains speculative
- Low confidence: Class I models' diverse pre-training corpora provide superior functional representations (Mechanism 3) - RNAcentral composition bias wasn't explicitly addressed

## Next Checks

1. **Corpus bias validation**: Test Class I models on RNAs from underrepresented families in RNAcentral (e.g., long non-coding RNAs, metagenomic RNAs) to quantify performance degradation and validate the diversity advantage claim.

2. **Fine-tuning gap analysis**: Select top-performing zero-shot models and evaluate their performance after minimal fine-tuning on secondary structure and classification tasks to establish the gap between zero-shot and optimal performance.

3. **Representation disentanglement**: Conduct representational analysis (e.g., centered kernel alignment, probing classifiers) on RNA-km and RNA-FM to directly measure the trade-off between structural and functional information in their embeddings.