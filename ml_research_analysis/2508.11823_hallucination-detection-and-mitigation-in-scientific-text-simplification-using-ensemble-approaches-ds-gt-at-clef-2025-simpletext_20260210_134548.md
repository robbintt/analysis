---
ver: rpa2
title: 'Hallucination Detection and Mitigation in Scientific Text Simplification using
  Ensemble Approaches: DS@GT at CLEF 2025 SimpleText'
arxiv_id: '2508.11823'
source_url: https://arxiv.org/abs/2508.11823
tags:
- text
- source
- input
- simplification
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents an ensemble-based approach for detecting and\
  \ mitigating hallucinations in scientific text simplification, addressing CLEF 2025\
  \ SimpleText Task 2. The authors combine four complementary methods\u2014a fine-tuned\
  \ BERT classifier, semantic similarity scoring, NLI entailment modeling, and LLM-based\
  \ reasoning\u2014into a unified framework for spurious text detection and information\
  \ distortion classification."
---

# Hallucination Detection and Mitigation in Scientific Text Simplification using Ensemble Approaches: DS@GT at CLEF 2025 SimpleText

## Quick Facts
- arXiv ID: 2508.11823
- Source URL: https://arxiv.org/abs/2508.11823
- Reference count: 16
- Primary result: Ensemble model achieves up to 0.95 F1 score in spurious text detection and 0.763 F1 in distortion classification for scientific text simplification

## Executive Summary
This paper presents an ensemble-based approach for detecting and mitigating hallucinations in scientific text simplification, addressing CLEF 2025 SimpleText Task 2. The authors combine four complementary methods—a fine-tuned BERT classifier, semantic similarity scoring, NLI entailment modeling, and LLM-based reasoning—into a unified framework for spurious text detection and information distortion classification. For grounded generation (Task 2.3), they employ an LLM-based post-editing system that revises simplified text using the original source as reference. Experimental results show strong performance across all tasks, with the ensemble model achieving up to 0.95 F1 score in spurious text detection and 0.763 F1 in distortion classification. Grounded systems demonstrated higher semantic fidelity (BLEU scores up to 18.27) compared to baseline simplifications, though with slightly lower SARI scores reflecting a trade-off between faithfulness and simplification aggressiveness.

## Method Summary
The approach combines four detection methods: a fine-tuned BERT binary classifier for lexical patterns, cosine similarity using multi-qa-MiniLM-L6-cos-v1 embeddings for semantic alignment, NLI entailment via facebook/bart-large-mnli for logical consistency, and LLaMA-3.3-70B-Versatile as a reasoning judge. These signals are aggregated by a three-layer neural network meta-classifier. Source documents are chunked at 100 words with 50-word overlap to preserve context. For grounded generation (Task 2.3), an LLM-based post-editing system revises simplified text by strictly adhering to the source document using structured prompts.

## Key Results
- Ensemble model achieves up to 0.95 F1 score in spurious text detection across both sourced and post-hoc settings
- Information distortion classification reaches 0.763 F1 score using the ensemble approach
- Grounded generation systems show higher semantic fidelity (BLEU up to 18.27) but lower simplification effectiveness (SARI down to 33.41) compared to baseline
- ROC AUC drops from 0.68 to 0.64 in post-hoc setting, indicating reduced ranking confidence when source text is unavailable

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Aggregating heterogeneous detection signals via a meta-classifier enhances robustness and balances precision-recall trade-offs better than individual detectors.
- **Mechanism:** The system fuses four distinct signals: lexical patterns (BERT classifier), semantic vector alignment (Cosine Similarity), logical consistency (NLI model), and reasoning heuristics (LLM). A three-layer neural network learns to weigh these conflicting probabilities.
- **Core assumption:** The errors made by individual detectors are uncorrelated, allowing the meta-classifier to smooth out noise.
- **Evidence anchors:**
  - [abstract]: "...diverse signals are combined using meta-classifiers to enhance the robustness of spurious and distortion detection."
  - [section]: Page 6, Table 1 shows the ensemble balancing the high precision of the LLM (0.94) with the high recall of the BERT classifier (0.98).
  - [corpus]: Neighbors like *Theoretical Foundations and Mitigation of Hallucination in Large Language Models* suggest that hybrid approaches are theorized to mitigate specific failure modes.
- **Break condition:** If the input features from different models become highly correlated, the ensemble adds no new information and may overfit to training noise.

### Mechanism 2
- **Claim:** Overlapping document chunking preserves semantic context for long scientific abstracts, enabling effective comparison against limited-context transformer models.
- **Mechanism:** Source abstracts are split into 100-word chunks with a 50-word overlap. Similarity and NLI scores are computed against these chunks individually, and the maximum score is retained.
- **Core assumption:** The semantic relationship between the input text and the source is localized enough to be captured in 100-word windows.
- **Evidence anchors:**
  - [section]: Page 3 states: "Each source abstract is segmented into overlapping passages... 100 words per chunk with a 50-word overlap. This ensures coverage and contextual continuity..."
  - [section]: Page 3 (Figure 1 description) confirms the max-pooling strategy for NLI and similarity scores across chunks.
- **Break condition:** If a critical logical entailment requires context spanning >100 words, this chunking mechanism fails to link the relevant tokens.

### Mechanism 3
- **Claim:** LLM-based post-editing enforces factual grounding (faithfulness) but reduces simplification aggression (readability).
- **Mechanism:** An LLM acts as a "corrective agent," prompted to revise a simplified text by strictly adhering to the source document. It removes hallucinations but tends to retain complex phrasing from the source.
- **Core assumption:** Semantic fidelity is strictly more important than lexical simplicity in the target use case.
- **Evidence anchors:**
  - [section]: Page 8, Table 4 shows grounded models having higher BLEU (up to 18.27) but lower SARI (e.g., 33.41 vs 42.98 baseline), indicating less deletion/simplification.
  - [section]: Page 8 concludes: "This reflects a fundamental trade-off: grounded models... may be less effective in performing bold rewrites... thus lowering their SARI scores."
- **Break condition:** If the source text is extremely dense, the LLM post-editor may default to copying the source verbatim, failing the "simplification" objective entirely.

## Foundational Learning

- **Concept: Natural Language Inference (NLI)**
  - **Why needed here:** NLI models are used to output probability scores for *entailment* vs. *contradiction*, acting as a logic-check against hallucinations.
  - **Quick check question:** Can you explain why an NLI model might output a low entailment score even for a factually correct simplification? (Hint: Over-generalization).

- **Concept: Meta-classification (Stacking)**
  - **Why needed here:** The paper trains a small neural network to weigh inputs rather than using simple voting. You must understand how to concatenate probability outputs into feature vectors for a secondary classifier.
  - **Quick check question:** If the BERT classifier fails on a specific domain of scientific text, how should the Meta-classifier theoretically behave if the LLM signal is still accurate?

- **Concept: Evaluation Metrics for Simplification (SARI vs. BLEU)**
  - **Why needed here:** The paper relies on the observation that SARI rewards deletion (simplicity) while BLEU rewards n-gram overlap (fidelity). Understanding this tension is required to interpret the results of Task 2.3.
  - **Quick check question:** Why does a "perfect" copy of the source text achieve a high BLEU score but potentially a low SARI score?

## Architecture Onboarding

- **Component map:** Input Processor (Chunker + Dense Passage Retriever) -> Detector Suite (BERT Classifier, Sentence-BERT, BART-MNLI, LLaMA-3.3) -> Aggregator (3-Layer Neural Network Meta-classifier) -> Mitigator (LLaMA-3.3 Post-Editor)
- **Critical path:** The **Meta-classifier** is the bottleneck. If the training data for the ensemble is sparse, the aggregator will likely just mimic the strongest individual model (the BERT classifier), negating the value of the complex LLM/NLI pipeline.
- **Design tradeoffs:**
  - **Sourced vs. Post-hoc:** The system loses ROC AUC (0.68 -> 0.64) in post-hoc mode. The tradeoff is computational cost vs. source availability.
  - **Fidelity vs. Simplicity:** The grounded generation model sacrifices SARI (simplicity) for BLEU (faithfulness). This architecture is unsuitable for applications requiring extreme simplification for low-literacy audiences.
- **Failure signatures:**
  - **High Precision, Low Recall:** LLM-as-judge is conservative; if the system misses too many hallucinations, check if the LLM prompt is too strict.
  - **Verbatim Copying:** If the grounded generation output is too complex, the post-editing prompt may be lacking negative constraints (e.g., "Do not copy complex clauses").
- **First 3 experiments:**
  1. **Ablation Study:** Remove the LLM-as-judge from the ensemble inputs and measure the drop in F1 score to quantify the specific value of the expensive LLM call.
  2. **Chunk Sensitivity Analysis:** Test 50-word vs. 200-word chunks on the NLI model to see if context truncation is the cause of the low NLI recall observed in Table 1.
  3. **Prompt Optimization for SARI:** Adjust the Task 2.3 prompt to explicitly reward "simple language" and measure if the SARI score recovers without a catastrophic drop in BLEU.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the trade-off between semantic faithfulness and simplification aggressiveness be optimized to prevent grounded models from becoming too complex?
- **Basis in paper:** The Conclusion states that future work must "explore techniques to optimize this trade-off, aiming to preserve simplicity without sacrificing factual correctness."
- **Why unresolved:** The results for Task 2.3 show that while grounded generation improves faithfulness (higher BLEU), it lowers SARI scores because the model retains more original wording and performs fewer bold rewrites.
- **What evidence would resolve it:** A new generation strategy or loss function that maintains high factual consistency while achieving SARI scores comparable to non-grounded baseline models.

### Open Question 2
- **Question:** What specific factors contribute to the degradation of ranking confidence (ROC AUC) in post-hoc hallucination detection when source text is unavailable?
- **Basis in paper:** The Conclusion notes a "notable drop in ROC AUC" (0.68 to 0.64) and suggests "additional investigation is needed to better understand this discrepancy."
- **Why unresolved:** While the F1 score remained stable (0.95) in the post-hoc setting, the model's ability to rank predictions by confidence was significantly diminished, a nuance not explained by the current feature ablation.
- **What evidence would resolve it:** An analysis of the confidence score distributions from the individual ensemble components (BERT, LLM) in the post-hoc setting versus the sourced setting to identify the source of the uncertainty.

### Open Question 3
- **Question:** Does the computational complexity of the ensemble meta-classifier yield statistically significant improvements over the standalone fine-tuned BERT classifier?
- **Basis in paper:** Tables 1 and 2 show the BERT classifier alone matches the ensemble's F1 score (0.95) and accuracy in both sourced and post-hoc settings, suggesting the ensemble may be redundant.
- **Why unresolved:** The paper presents the ensemble as the primary contribution, yet the simpler BERT baseline appears equally effective for the binary spurious detection task.
- **What evidence would resolve it:** Statistical significance testing (e.g., McNemar's test) on the error rates of the BERT classifier versus the ensemble to determine if the addition of NLI and LLM signals provides a marginal but significant correction.

## Limitations
- The chunking strategy (100 words with 50 overlap) is heuristic without validation that this window size optimally captures entailment relationships in scientific abstracts.
- The fine-tuning procedure for the BERT classifier is underspecified, lacking details on training data source, class balance, or hyperparameters.
- The computational efficiency claims are absent; the LLaMA-3.3-70B inference costs are not discussed relative to simpler alternatives.

## Confidence
- **High Confidence (8/10):** The ensemble model's superior performance metrics (F1 scores up to 0.95 for detection, 0.763 for distortion classification) are well-supported by the reported results in Tables 1 and 2.
- **Medium Confidence (6/10):** The claim that overlapping chunking preserves semantic context is theoretically sound but lacks empirical validation comparing different chunk sizes or non-overlapping approaches.
- **Low Confidence (4/10):** The generalizability of results beyond the CLEF 2025 SimpleText dataset is untested, as is performance on non-scientific domains.

## Next Checks
1. **Ablation Study of Detection Components:** Remove each of the four detection signals from the ensemble and measure the change in F1 scores to quantify whether the expensive LLM-based judge provides unique value beyond the BERT classifier and NLI model.
2. **Chunk Size Sensitivity Analysis:** Systematically vary the chunking window (e.g., 50, 100, 200 words with corresponding overlaps) and measure the impact on NLI recall and overall detection performance to validate whether the chosen 100/50 configuration is optimal.
3. **Cross-Domain Generalization Test:** Apply the trained ensemble model to a different scientific domain (e.g., biomedical literature or engineering abstracts) not represented in the CLEF dataset and measure performance degradation to assess robustness beyond the specific corpus used.