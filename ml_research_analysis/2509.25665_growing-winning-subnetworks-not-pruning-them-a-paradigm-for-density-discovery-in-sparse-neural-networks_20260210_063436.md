---
ver: rpa2
title: 'Growing Winning Subnetworks, Not Pruning Them: A Paradigm for Density Discovery
  in Sparse Neural Networks'
arxiv_id: '2509.25665'
source_url: https://arxiv.org/abs/2509.25665
tags:
- density
- training
- pwmpr
- growth
- sparse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes PWMPR, a method that grows sparse neural networks
  from an initial seed rather than pruning dense ones. The approach uses path weight
  magnitude products to guide connection growth and logistic-based stopping criteria
  to automatically determine optimal density.
---

# Growing Winning Subnetworks, Not Pruning Them: A Paradigm for Density Discovery in Sparse Neural Networks

## Quick Facts
- arXiv ID: 2509.25665
- Source URL: https://arxiv.org/abs/2509.25665
- Reference count: 38
- The paper proposes PWMPR, a method that grows sparse neural networks from an initial seed rather than pruning dense ones.

## Executive Summary
PWMPR introduces a novel paradigm for discovering optimal sparse network densities by growing connections rather than pruning them. Starting from a highly sparse seed network (2-5% density), PWMPR iteratively trains and adds connections guided by path weight magnitude products (PWMP). The method automatically determines the optimal density through logistic curve fitting to accuracy-density curves. Evaluated on CIFAR, TinyImageNet, and ImageNet with ResNet and Vision Transformer architectures, PWMPR achieves performance close to iterative magnitude pruning-derived lottery tickets but at substantially lower training cost (1.5× dense training vs 3-4× for IMP-C), though at higher density.

## Method Summary
PWMPR is a growth-based sparse network training method that begins with a highly sparse initial network (typically 2-5% density) created via PHEW initialization. The algorithm alternates between rough training epochs and connection growth phases. During growth, PWMPR computes path weight magnitude products (PWMP) by performing forward and backward passes with absolute weights, using the resulting scores to probabilistically add new connections. The growth continues until a logistic curve fit to the accuracy-density relationship reaches 95% of its asymptotic value, automatically determining the optimal density. This approach contrasts with pruning-based methods that start dense and remove connections iteratively.

## Key Results
- PWMPR achieves accuracy within 0.5-1.0% of IMP-C (Iterative Magnitude Pruning) across CIFAR-10/100, TinyImageNet, and ImageNet
- Training cost is substantially lower: 1.5× dense training FLOPs versus 3-4× for IMP-C
- PWMPR automatically discovers near-optimal densities through logistic curve fitting
- The method maintains better connectivity than pruning-based approaches at low densities

## Why This Works (Mechanism)
PWMPR works by growing networks from sparse seeds using path weight magnitude products to guide connection addition. The PWMP score captures the potential importance of connections by considering the product of incoming and outgoing weight magnitudes through all paths. This growth-based approach avoids the dense pretraining required by pruning methods, leading to lower computational cost. The logistic stopping criterion automatically identifies when further growth yields diminishing returns, eliminating the need for manual density selection.

## Foundational Learning
- **PHEW initialization**: Progressive Hybrid Expansion and Weighting for creating sparse networks without isolated nodes; needed for reliable training at extremely low initial densities; quick check: verify τ-core ratio > 0.8 before training
- **Path Weight Magnitude Products (PWMP)**: Forward/backward pass-based saliency scoring using |θ| values; needed to identify promising connections for growth; quick check: ensure sum of PWMP scores correlates with final weight magnitudes
- **Logistic curve fitting**: P(ρ) = P₀ + A(1 - exp(-βρ)) to predict accuracy asymptote; needed for automatic stopping; quick check: fit should explain >90% variance in early observations
- **τ-core decomposition**: Measures k-robustness of sparse networks; needed to assess connectivity quality; quick check: τ-core ratio should decrease smoothly with density for PWMPR
- **Rough training**: Short training epochs (10 per iteration) sufficient for growth guidance; needed to minimize training cost; quick check: validation loss should plateau within 3 epochs
- **Edge sampling proportional to PWMP**: Stochastic connection addition based on computed scores; needed to avoid deterministic bottlenecks; quick check: diversity of added edges across seeds should be high

## Architecture Onboarding

### Component Map
PHEW init -> Rough training (10 epochs) -> PWMP computation (forward/backward passes) -> Edge sampling & addition (γ=25%) -> Logistic curve fit -> Stopping check

### Critical Path
Initial sparse network → PWMP-guided growth → Accuracy-density curve → Logistic fit → Stopping criterion

### Design Tradeoffs
Growth-only (PWMPR) vs. prune-only (IMP): PWMPR trades ability to reach extreme sparsity for lower training cost and better early-stage connectivity. The growth factor γ=25% balances exploration of new connections with computational efficiency.

### Failure Signatures
- Isolated nodes at low density preventing gradient flow
- Deterministic PWMP scoring creating bottlenecks in specific network regions
- Logistic fit converging to local minima, causing early stopping
- Q/K magnitude decoupling in attention layers leading to poor growth decisions

### Exactly 3 First Experiments
1. Implement PHEW initialization at ρ_init=5% on CIFAR-10 ResNet-32, verify connectivity and τ-core ratio
2. Compute PWMP scores on a simple 2-layer network with known weights, validate scoring mechanism
3. Run PWMPR training loop for 3 iterations on CIFAR-10 ResNet-32, plot accuracy-density curve and fit logistic

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can hybrid grow-prune algorithms combine the benefits of constructive (growth-based) and destructive (pruning-based) updates to achieve both low training cost and extreme sparsity?
- Basis in paper: [explicit] The authors state: "growth-only methods cannot reach the extreme sparsity levels of pruning, since low-importance connections are never explicitly removed" and identify "hybrid grow-prune methods to combine the benefits of constructive and destructive updates" as a natural extension.
- Why unresolved: PWMPR is purely growth-based; it never removes connections, so it cannot discover extremely sparse subnetworks (matching IMP-C at ~15-20% density) that require explicit pruning of low-importance weights.
- What evidence would resolve it: A hybrid method that interleave PWMPR-guided growth with periodic pruning, demonstrating IMP-comparable accuracy at sub-30% densities while maintaining ~1.5× training cost.

### Open Question 2
- Question: What attention-specific growth rules can effectively guide connection growth in query-key matrices, where weight magnitudes decouple from functional importance due to softmax normalization?
- Basis in paper: [explicit] The authors defer "attention-specific growth rules informed by synaptic diversity or head-level importance" to future work, noting PWMP "does not naturally extend to query-key matrices in attention, where magnitudes decouple from functional importance."
- Why unresolved: The PWMP score relies on weight magnitude products, but softmax normalization in self-attention makes attention scores largely insensitive to absolute Q/K weight magnitudes, invalidating the PWMP heuristic for these parameters.
- What evidence would resolve it: Alternative saliency metrics (e.g., gradient-based, diversity-based, or head-importance measures) that predict which Q/K connections should be grown, validated by improved performance on transformer architectures.

### Open Question 3
- Question: Can more sophisticated learning-curve extrapolation techniques improve stopping point detection compared to the simple logistic-fit heuristic?
- Basis in paper: [explicit] The authors note: "our stopping rule, based on logistic-fit extrapolation, is a simple heuristic compared to more sophisticated learning-curve extrapolation techniques."
- Why unresolved: The logistic-fit rule only uses sparse observations from early training and may not capture complex accuracy-density relationships, potentially stopping too early or late.
- What evidence would resolve it: Comparing PWMPR with Bayesian learning-curve extrapolation or prior-data fitted networks (as cited in Adriaensen et al., 2023) showing tighter stopping decisions and reduced variance in discovered densities.

### Open Question 4
- Question: Does growth-based density discovery transfer effectively to large-scale NLP or speech domains with transformer architectures?
- Basis in paper: [explicit] The authors state: "our experiments are limited to vision benchmarks with relatively modest-scale transformers; validation in large-scale NLP or speech domains remains an important direction for future work."
- Why unresolved: PWMPR has only been evaluated on image classification (CIFAR, TinyImageNet, ImageNet) with scaled-down ViTs; transfer to language modeling or speech involves different architectural patterns and scale requirements.
- What evidence would resolve it: Experiments applying PWMPR to standard NLP benchmarks (e.g., language modeling on WikiText, GLUE tasks) or speech tasks, showing comparable efficiency gains relative to dense training.

## Limitations
- Cannot reach extreme sparsity levels achievable by pruning-based methods due to lack of explicit connection removal
- PWMP scoring mechanism does not naturally extend to attention layers where weight magnitudes decouple from functional importance
- Limited evaluation scope (vision tasks only) with modest-scale transformers, lacking validation in large-scale NLP or speech domains

## Confidence

**High Confidence**: The core PWMPR growth algorithm with PWMP scoring and logistic stopping criterion is well-defined and reproducible. The 1.5× vs 3-4× training cost comparison to IMP-C is clearly supported.

**Medium Confidence**: Claims about matching IMP-C accuracy while being more efficient are reasonably supported, though some variance exists. The logistic curve fitting for density discovery works in practice but has implementation ambiguity.

**Low Confidence**: Claims about superiority at very low densities (2-5%) are less well-supported due to limited comparison with state-of-the-art pruning at these extremes. Generalization claims to new tasks based on previously discovered masks need more validation.

## Next Checks
1. **Connectivity Validation**: Verify PHEW initialization maintains full network connectivity at ρ_init=2-5% by computing τ-core ratio before training
2. **PWMP Scoring Implementation**: Implement and validate PWMP computation on small ResNet-18 with known weights, comparing against analytical expectations
3. **Logistic Stopping Criterion**: Test logistic density prediction on CIFAR-10 ResNet-32 training curves from multiple seeds, verifying 95% asymptote criterion identifies optimal densities