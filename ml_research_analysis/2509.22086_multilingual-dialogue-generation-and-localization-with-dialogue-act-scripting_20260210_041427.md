---
ver: rpa2
title: Multilingual Dialogue Generation and Localization with Dialogue Act Scripting
arxiv_id: '2509.22086'
source_url: https://arxiv.org/abs/2509.22086
tags:
- dialogue
- dialogues
- cultural
- human
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Dialogue Act Script (DAS), a framework for
  generating culturally appropriate multilingual dialogues by abstracting conversations
  into structured intent representations rather than direct translation. DAS encodes
  utterances into dialogue acts with semantic parameters, localizes cultural references
  (e.g., replacing "Cuervo Gold margaritas" with "Negroni" for Italian), and decodes
  them into fluent target-language dialogue.
---

# Multilingual Dialogue Generation and Localization with Dialogue Act Scripting

## Quick Facts
- arXiv ID: 2509.22086
- Source URL: https://arxiv.org/abs/2509.22086
- Authors: Justin Vasselli; Eunike Andriani Kardinata; Yusuke Sakai; Taro Watanabe
- Reference count: 29
- Key outcome: DAS framework generates culturally appropriate multilingual dialogues, outperforming both machine and human translations on cultural relevance, coherence, and situational appropriateness (p < 0.001)

## Executive Summary
This paper introduces Dialogue Act Script (DAS), a framework for generating culturally appropriate multilingual dialogues by abstracting conversations into structured intent representations rather than direct translation. DAS encodes utterances into dialogue acts with semantic parameters, localizes cultural references, and decodes them into fluent target-language dialogue. Human evaluations across Italian, German, and Chinese show DAS-generated dialogues consistently outperform both machine-translated and human-translated versions on cultural relevance, coherence, and situational appropriateness. The modular three-step pipeline (encode-localize-decode) significantly outperforms single-prompt localization, demonstrating that separating intent abstraction from surface realization enables more natural, culturally adaptive generation.

## Method Summary
The DAS framework uses a three-step pipeline with GPT-4o: (1) Encoding converts raw text into structured dialogue acts with semantic slots; (2) Localization modifies cultural entities within the structured script; (3) Decoding generates fluent target-language dialogue from the localized script. The system uses a 15-function dialogue act taxonomy and evaluates on the DailyDialog dataset with Italian, German, and Chinese translations. Human pairwise evaluation assesses fluency, coherence, cultural relevance, and situational appropriateness against machine and human translation baselines.

## Key Results
- DAS achieved 96.2% win rate over human translations on cultural relevance for Italian
- Modular pipeline outperformed single-prompt localization by ~60 percentage points on fluency metrics
- Cohen's Kappa of 0.75-0.77 for GPT-human agreement on encoding accuracy
- High accuracy for concrete slots (City Name: 1.00) but lower for cultural entities (Music Artist: 0.65)

## Why This Works (Mechanism)

### Mechanism 1
Structured intent abstraction acts as a "noise filter" for translation artifacts and anglocentric biases. By converting natural language into formalized Dialogue Act Script, the system discards surface-level phrasing and regenerates text grounded in intent rather than syntactic structure.

### Mechanism 2
Slot-based localization injects cultural context more effectively than end-to-end prompting. The intermediate DAS representation serves as a "cultural API," forcing hard alignment with target culture norms before generation.

### Mechanism 3
Modular pipeline decomposition outperforms monolithic prompting by reducing "cognitive load" on the model. Separating encoding, localization, and decoding forces the model to solve distinct problems sequentially.

## Foundational Learning

- **Concept**: **Dialogue Acts (Speech Act Theory)**
  - **Why needed here**: DAS relies on classifying utterances by function (inform, inquire, commit) rather than content.
  - **Quick check question**: If a user says "It's cold in here," is the Dialogue Act `inform(weather)` or `seek_action(adjust_thermostat)`?

- **Concept**: **Slot Filling / Semantic Role Labeling**
  - **Why needed here**: The "script" in DAS is composed of acts + slots (subject, object, topic).
  - **Quick check question**: In "Book a flight to Paris," what is the `action` slot and what is the `destination` slot?

- **Concept**: **Translationese vs. Localization**
  - **Why needed here**: The paper explicitly optimizes against "translationese" (unnatural text resulting from literal translation).
  - **Quick check question**: Translating "It's raining cats and dogs" literally into Chinese creates a fluency error or a cultural relevance error?

## Architecture Onboarding

- **Component map**: Encoder (LLM) -> Localizer (LLM/Rule-based) -> Decoder (LLM)
- **Critical path**: **The Encoder**. If initial dialogue act classification or slot extraction is inaccurate, subsequent steps propagate these errors.
- **Design tradeoffs**: Modularity vs. Latency (3 LLM calls vs. single prompt); Consistency vs. Diversity (temperature settings vary by step)
- **Failure signatures**: "Textbook" Tone (single-prompt usage); Hallucinated Entities (model lack of domain knowledge); Intent Drift (information loss during encoding)
- **First 3 experiments**: (1) Validate Encoder against human annotations; (2) Test slot localization by manually swapping entities; (3) Compare pipeline vs. single-prompt baseline on cultural specificity

## Open Questions the Paper Calls Out

- **Generalization to specialized domains**: Does DAS work for task-oriented dialogues in customer support, healthcare, or legal contexts?
- **Performance on low-resource languages**: How does DAS perform in morphologically complex or low-resource languages?
- **Alternative localization methods**: Can rule-based substitutions or retrieval systems replace LLM-based localization?
- **Granular cultural appropriateness**: Are individual slot substitutions culturally appropriate according to native speaker judgments?

## Limitations

- Heavy reliance on GPT-4o introduces model-specific biases and computational costs
- Slot substitution accuracy varies significantly by entity type (0.65 for Music Artist vs. 1.00 for City Name)
- Human evaluation limited to two annotators per language with unclear cultural expertise requirements
- Performance on languages with different linguistic structures remains unproven

## Confidence

- **High Confidence**: Modular pipeline architecture's superiority over single-prompt approaches
- **Medium Confidence**: Generalizability of 15-function taxonomy across domains
- **Medium Confidence**: Claim that DAS mitigates translationese artifacts
- **Low Confidence**: Performance on languages with significantly different linguistic structures

## Next Checks

1. **Cross-Domain Stress Test**: Run DAS on specialized dialogue corpus (medical/legal) to evaluate domain-specific intent coverage
2. **Bias Audit Protocol**: Systematically evaluate cultural representation and stereotype propagation across multiple target cultures
3. **Minimal Viable Alternative**: Replace GPT-4o encoding with rule-based classifier to assess modular benefits without expensive models