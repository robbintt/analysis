---
ver: rpa2
title: 'Beyond Scaling Law: A Data-Efficient Distillation Framework for Reasoning'
arxiv_id: '2508.09883'
source_url: https://arxiv.org/abs/2508.09883
tags:
- reasoning
- corpus
- teacher
- zhang
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a data-efficient distillation framework (DED)
  to enhance reasoning capabilities in large language models (LLMs) without relying
  on extensive scaling. The method focuses on optimizing teacher model selection,
  carefully curating a smaller training corpus, and encouraging diverse reasoning
  trajectories.
---

# Beyond Scaling Law: A Data-Efficient Distillation Framework for Reasoning

## Quick Facts
- arXiv ID: 2508.09883
- Source URL: https://arxiv.org/abs/2508.09883
- Reference count: 8
- Primary result: Data-efficient distillation framework achieves state-of-the-art reasoning performance using only 0.8k examples

## Executive Summary
This paper challenges the prevailing scaling law paradigm in reasoning model development by introducing a data-efficient distillation framework (DED) that achieves state-of-the-art performance with minimal training data. The framework optimizes teacher model selection through empirical smoke testing, curates a carefully filtered corpus focused on difficult examples, and enhances diversity of reasoning trajectories. Experiments demonstrate that DED achieves 79.2% accuracy on AIME 2024 with only 0.8k examples, surpassing models trained on 1.8k-6.5k examples, while maintaining strong out-of-domain generalization.

## Method Summary
The framework consists of three core components: teacher model selection via smoke testing to identify models with optimal "teaching ability," corpus compression through difficulty-based filtering and diversity enhancement of reasoning trajectories, and structured knowledge distillation that emphasizes low-entropy reasoning paths. The approach specifically addresses the trade-off between in-domain reasoning improvement and out-of-domain performance degradation that occurs with large-scale training.

## Key Results
- Achieves 79.2% accuracy on AIME 2024 using only 0.8k examples (vs. 6.5k for comparable models)
- Outperforms DeepSeek-R1 (59.6%) on AIME 2025 with 77.5% accuracy
- Maintains strong performance across multiple benchmarks including MATH-500 (89.8%) and LiveCodeBench

## Why This Works (Mechanism)

### Mechanism 1: Optimal Teacher Selection via Smoke Testing
The framework demonstrates that teaching ability is distinct from raw model strength, requiring empirical smoke tests rather than benchmark scores alone. By running quick distillation experiments with candidate teachers, the optimal teacher is identified based on student performance rather than absolute reasoning capability.

### Mechanism 2: Quality-Driven Corpus Compression
A smaller, curated corpus of difficult examples outperforms larger datasets when augmented with diverse reasoning paths. The approach focuses on examples where the student model struggles, then enhances these with multiple reasoning trajectories to improve learning efficiency while preserving out-of-domain performance.

### Mechanism 3: Low-Entropy Reasoning Trajectory Distillation
Lower token entropy in teacher outputs correlates with better student learning outcomes. The framework shows that structured, predictable reasoning trajectories with reduced variability lead to more stable convergence and stronger generalization compared to high-entropy outputs.

## Foundational Learning

- **Knowledge Distillation (KD)**: Understanding the core idea of training a smaller student model to mimic the behavior of a larger teacher model is essential, as DED is a specialized form of KD. Quick check: What is the core difference between training a model from scratch on raw data vs. training it via knowledge distillation?

- **Chain-of-Thought (CoT) Reasoning**: The paper distills reasoning capabilities, not just answers, requiring understanding of how to teach the step-by-step reasoning process. Quick check: Why is distilling the step-by-step reasoning process (CoT) more valuable for improving reasoning skills than distilling just the final answer?

- **Out-of-Domain (OOD) Generalization & Catastrophic Forgetting**: Understanding how domain-specific fine-tuning can degrade general capabilities is crucial for appreciating the corpus compression approach. Quick check: What are two common negative side-effects of fine-tuning a pre-trained model on a very large, single-domain dataset?

## Architecture Onboarding

- **Component map**: Teacher Model Candidates -> Smoke Test Engine -> Evaluator -> Corpus Filter -> Diversity Sampler -> Final SFT Pipeline
- **Critical path**: 1. Assemble teacher candidates. -> 2. Run smoke tests, select best teacher. -> 3. Generate M trajectories per question. -> 4. Filter corpus for correctness and hardness. -> 5. Apply diversity sampling. -> 6. Train final student model.
- **Design tradeoffs**: Smoke Test Cost vs. Teacher Selection Quality, Corpus Size vs. OOD Performance, Diversity vs. Coherence
- **Failure signatures**: No improvement over baseline (student saturated, poor seed corpus, or insufficient smoke test), Degraded OOD performance (corpus too large or not targeted enough), Student model collapses (diversity sampling selected incoherent trajectories)
- **First 3 experiments**: 1. Teacher Validation: Run smoke test on new task using 2-3 different teachers to verify "teaching ability != model strength," 2. Data Ablation: Train three students (large/unfiltered, hard-only, and hard + diverse) to measure performance/OOD trade-off, 3. Entropy Analysis: Measure token entropy of generated corpus and compare to known effective teacher (like QwQ-32B) as quality check

## Open Questions the Paper Calls Out
None

## Limitations
- Smoke testing introduces a chicken-and-egg problem where the selection process itself requires choosing hyperparameters and a small corpus
- Corpus compression assumes "hard" examples identified for one student remain challenging for other students or under different evaluation conditions
- Token entropy correlation with teaching effectiveness needs broader validation across diverse reasoning domains

## Confidence

- **High confidence**: The general framework structure (teacher selection → corpus curation → diversity enhancement) is well-motivated and experimental results show clear improvements
- **Medium confidence**: The specific mechanism linking token entropy to distillation effectiveness requires broader validation
- **Medium confidence**: The optimal corpus size of ~0.8k examples may be task-specific and not generalize to all reasoning domains

## Next Checks

1. **Teacher Selection Robustness Test**: Run smoke tests using different evaluation metrics (beyond accuracy) like reasoning depth or solution novelty to verify if optimal teacher selection changes, testing robustness of "teaching ability" concept.

2. **Cross-Domain Corpus Transferability**: Apply same corpus curation pipeline (hard example selection + diversity enhancement) to different reasoning domain (e.g., logical reasoning instead of math) and measure whether 0.8k optimal size holds or if domain characteristics require different parameters.

3. **Token Entropy Correlation Study**: Systematically vary diversity sampling threshold while measuring both final student performance and token entropy of resulting corpus to establish whether lower entropy consistently correlates with better student outcomes across multiple runs and domains.