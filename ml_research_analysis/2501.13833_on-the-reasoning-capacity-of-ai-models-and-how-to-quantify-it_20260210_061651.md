---
ver: rpa2
title: On the Reasoning Capacity of AI Models and How to Quantify It
arxiv_id: '2501.13833'
source_url: https://arxiv.org/abs/2501.13833
tags:
- reasoning
- strategy
- positional
- accuracy
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a phenomenological framework for analyzing
  AI model reasoning capabilities beyond traditional accuracy metrics. The authors
  develop two complementary models: a Probabilistic Mixture Model (PMM) that decomposes
  responses into reasoning, memorization, and guessing components, and an Information-Theoretic
  Consistency (ITC) analysis that quantifies the relationship between model confidence
  and strategy selection.'
---

# On the Reasoning Capacity of AI Models and How to Quantify It

## Quick Facts
- arXiv ID: 2501.13833
- Source URL: https://arxiv.org/abs/2501.13833
- Authors: Santosh Kumar Radha; Oktay Goktas
- Reference count: 0
- This paper introduces a phenomenological framework for analyzing AI model reasoning capabilities beyond traditional accuracy metrics.

## Executive Summary
This paper presents a novel phenomenological framework for quantifying AI reasoning capabilities by systematically analyzing cognitive strategies through controlled perturbations. The authors develop two complementary models - a Probabilistic Mixture Model (PMM) that decomposes responses into reasoning, memorization, and guessing components, and an Information-Theoretic Consistency (ITC) analysis that quantifies the relationship between model confidence and strategy selection. Through experiments on positional bias in multiple-choice reasoning tasks using GPT-4o-mini on GPQA, the study demonstrates that current models exhibit significant limitations in genuine logical deduction, with apparent success often relying on sophisticated combinations of memorization and pattern matching rather than true reasoning.

## Method Summary
The methodology employs systematic positional perturbations to reveal fundamental aspects of model decision-making. The framework uses a two-pronged approach: the Probabilistic Mixture Model decomposes accuracy into distinct cognitive strategies (reasoning, memorization, guessing) based on position-specific performance differences, while the Information-Theoretic Consistency analysis quantifies prediction uncertainty through entropy-accuracy relationships. The authors systematically vary answer positions across multiple trials to distinguish position-invariant reasoning from position-dependent memorization, enabling quantitative assessment of reasoning capacity beyond traditional accuracy metrics.

## Key Results
- Current models exhibit significant limitations in genuine logical deduction, with apparent success often relying on sophisticated combinations of memorization and pattern matching
- The analysis reveals distinct cognitive strategy distributions across different question types, enabling identification of reasoning versus heuristic-based responses
- The framework establishes quantitative criteria for distinguishing between reasoning and heuristic-based responses through systematic deviations from optimal probability distributions

## Why This Works (Mechanism)

### Mechanism 1: Positional Perturbation as a Strategy Discriminator
- Claim: Varying answer position systematically distinguishes position-invariant reasoning from position-dependent memorization.
- Mechanism: An order parameter θ ∈ [0, 1] controls answer randomization; models relying on memorization show accuracy drops when positional cues become unreliable, while true reasoning exhibits position-invariant performance.
- Core assumption: Reasoning produces consistent accuracy across positions; memorization produces position-dependent accuracy.
- Evidence anchors:
  - [abstract] "Using positional bias in multiple-choice reasoning tasks as a case study, we demonstrate how systematic perturbations can reveal fundamental aspects of model decision-making."
  - [section IVA] Position D exhibits markedly lower selection probability (~0.1) as incorrect choice versus other positions (~0.2-0.25), revealing systematic positional bias.
  - [corpus] Weak direct corpus support; neighbor papers focus on compositional reasoning evaluation but not positional perturbation methods.
- Break condition: If models develop position-invariant memorization strategies (e.g., content-based associations), the positional probe loses discriminative power.

### Mechanism 2: Accuracy Differential Isolates Memorization Contribution
- Claim: The difference between accuracy at memorized versus non-memorized positions quantitatively isolates memorization probability.
- Mechanism: PM(q) = ΔA(q)/(1 - PO) (Eq. 17), where ΔA(q) is the accuracy differential and PO is the random guessing baseline (0.25 for 4-option MC). This directly yields reasoning PR(q) and guessing PG(q) via constraint satisfaction.
- Core assumption: Strategies combine linearly; reasoning is perfect when applied (PR = 1); guessing is uniform random.
- Evidence anchors:
  - [section IIIB] "This key equation isolates the memorization probability: PM(q) = ΔA(q)/(1-PO)"
  - [section IVB] Model validation shows strong predictive accuracy for medium-to-high accuracy questions (α > 0.4), with systematic deviations in low-accuracy regime indicating higher-order effects.
  - [corpus] No direct corpus validation of this specific decomposition approach.
- Break condition: If reasoning is imperfect (PR < 1) or guessing is non-uniform, the linear decomposition underestimates complexity.

### Mechanism 3: Entropy-Accuracy Frontier Detects Miscalibration
- Claim: Empirical entropy-accuracy pairs systematically deviate from the theoretical calibration frontier, revealing overconfidence.
- Mechanism: The theoretical frontier H_ideal(A) (Eq. 30) represents optimal probability distribution given accuracy level. Points below frontier indicate overconfidence (under-dispersed probabilities); points above indicate underconfidence.
- Core assumption: An ideal calibrated model distributes probability mass optimally between correct and incorrect options.
- Evidence anchors:
  - [section IVC] "All observed questions fall below this frontier, indicating systematic under-dispersion of probability mass compared to ideal calibration."
  - [section IVC] Deviations of 0.2-0.5 bits in low-to-moderate accuracy regime (α < 0.7), narrowing at high accuracy.
  - [corpus] Weak support; neighbor papers do not address entropy-based calibration analysis.
- Break condition: If model outputs are not well-calibrated probabilities (e.g., softmax artifacts), entropy measures may not reflect true uncertainty.

## Foundational Learning

- **Concept: Shannon Entropy for Calibration Analysis**
  - Why needed here: Quantifies prediction uncertainty; enables comparison against theoretical optimal calibration frontier.
  - Quick check question: Given a 4-option MC task, what is the maximum entropy and what does it indicate about model confidence?

- **Concept: Mixture Model Decomposition**
  - Why needed here: Provides mathematical framework to separate cognitive strategies from aggregate accuracy metrics.
  - Quick check question: If accuracy at memorized position is 0.8 and at other positions is 0.45 (4 options), what is the memorization probability PM?

- **Concept: Phase Space Dynamics**
  - Why needed here: Enables visualization of strategy evolution under controlled perturbations; reveals attractors and phase transitions.
  - Quick check question: What does a stable attractor in strategy space indicate about model behavior under perturbation?

## Architecture Onboarding

- **Component map:**
  - PMM Module -> ITC Module -> Perturbation Engine

- **Critical path:**
  1. Define θ schedule (e.g., θ ∈ {0, 0.25, 0.5, 0.75, 1.0})
  2. For each question, generate N trials with correct answer at each position o ∈ {A, B, C, D}
  3. Compute α_om(q) and α_other(q) → derive PM(q), PR(q), PG(q)
  4. Extract model probability distribution → compute entropy H(q)
  5. Map (α, H) pairs against theoretical frontier

- **Design tradeoffs:**
  - Linear vs higher-order decomposition: Linear model accurate for α > 0.4; higher-order corrections needed for low-accuracy regime
  - Trial count vs cost: 100 trials per question used in paper; fewer trials increase variance in strategy estimates
  - Assumption of perfect reasoning (PR = 1): Simplifies math but may misattribute partial reasoning to guessing

- **Failure signatures:**
  - High deviation δα > 0.15 in low-accuracy regime indicates model mismatch
  - Points above calibration frontier suggest implementation error in entropy calculation
  - Uniform PM across all positions suggests positional perturbation not applied correctly

- **First 3 experiments:**
  1. **Baseline positional bias:** Run θ = 0 with correct answers fixed at each position; verify position D shows lower selection probability as distractor
  2. **Randomization sweep:** Vary θ ∈ [0, 1] with inclusive and exclusive protocols; confirm convergence to 0.25 accuracy at θ = 1
  3. **Strategy decomposition validation:** For questions with α > 0.4, compare α_expected vs α_observed; deviations < 0.1 validate linear decomposition

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the stable attractors observed in the strategy phase space represent fundamental computational constraints of transformer architectures, or are they artifacts of current training methodologies?
- Basis in paper: [explicit] The authors explicitly ask whether these attractors are architectural constraints or training artifacts in the context of the observed "sweet spots" in the trade-off between pattern matching and logical deduction.
- Why unresolved: The current phenomenological approach characterizes the existence and location of these attractors but does not isolate the underlying causal mechanism (architecture vs. training data/objectives).
- What evidence would resolve it: A comparative analysis of attractor formation across fundamentally different architectures (e.g., RNNs vs. Transformers) or identical architectures trained with different objective functions to see if the attractors persist or shift.

### Open Question 2
- Question: How does the application of reasoning-enhancing techniques, such as Chain-of-Thought (CoT) or Iteration-of-Thought (IoT), shift the model's position within the proposed strategy phase space?
- Basis in paper: [explicit] The paper notes that experiments employed direct answer elicitation and states that "exploring the impact of such reasoning-enhancing techniques remains an avenue for future investigation."
- Why unresolved: The study establishes a baseline for direct elicitation; it is currently unknown if these techniques fundamentally increase the reasoning probability ($P_R$) or merely stabilize the memorization-reasoning balance.
- What evidence would resolve it: Applying the Probabilistic Mixture Model (PMM) and Information-Theoretic Consistency (ITC) analysis to model outputs generated via CoT/IoT prompting and quantifying the resulting shift in strategy probabilities.

### Open Question 3
- Question: How can the Probabilistic Mixture Model be extended to capture higher-order correlations and non-ideal behaviors in the low-accuracy regime?
- Basis in paper: [inferred] The authors observe systematic deviations between the model and empirical data in the low-accuracy regime ($\alpha < 0.4$), suggesting the linear decomposition fails to capture "higher-order effects" like position-dependent reasoning or non-uniform guessing.
- Why unresolved: The current model assumes strategies combine linearly and that reasoning is perfect ($P_R=1$), which creates residuals when models exhibit partial understanding or complex heuristic interactions.
- What evidence would resolve it: Developing a non-linear mixture model that includes interaction terms between positional bias and strategy selection, verifying if it reduces residuals in the low-accuracy regime.

### Open Question 4
- Question: Can the phenomenological framework be generalized to quantify reasoning capabilities using systematic perturbations other than positional bias?
- Basis in paper: [explicit] The conclusion suggests that by "expanding this approach to examine other forms of systematic bias beyond positional effects," researchers can develop more sophisticated methods for understanding cognitive strategies.
- Why unresolved: The utility of the PMM and ITC frameworks has only been validated against positional perturbations; their robustness to other perturbation types remains unproven.
- What evidence would resolve it: Replicating the experiments using different perturbation strategies (e.g., syntactic restructuring, token swapping) to verify if similar phase transitions and strategy decompositions occur.

## Limitations

- The assumption of linear strategy combination breaks down in the low-accuracy regime (α < 0.4), where systematic deviations suggest higher-order effects or interactions between strategies
- Reliance on positional perturbation as a discriminator may be circumvented by sophisticated memorization strategies that become position-invariant through pattern matching
- The entropy-based calibration analysis assumes well-calibrated probability outputs, which may not hold for all model implementations or temperature settings

## Confidence

- **High Confidence:** The identification of positional bias effects and the general methodology of using controlled perturbations to distinguish strategies
- **Medium Confidence:** The linear decomposition of accuracy into PMM components (PM, PR, PG) for medium-to-high accuracy questions
- **Low Confidence:** The universal applicability of the entropy-accuracy frontier as a calibration metric across different model architectures and temperature settings

## Next Checks

1. **Higher-Order Decomposition Validation:** For questions with α < 0.4, implement a second-order correction to the PMM decomposition. Compare predictions using linear vs second-order models against empirical accuracy to quantify the importance of interaction terms between strategies.

2. **Position-Invariant Memorization Detection:** Design a new experiment where distractor answers are systematically replaced with semantically similar but position-matched alternatives. If positional bias persists despite semantic randomization, this would indicate the presence of position-invariant memorization strategies that the current framework cannot distinguish from reasoning.

3. **Cross-Temperature Calibration Analysis:** Repeat the entropy-accuracy frontier analysis across multiple temperature settings (e.g., T ∈ {0.1, 0.5, 1.0, 1.5}) to determine whether the calibration frontier remains consistent. Significant shifts in the frontier location would indicate that entropy measures are sensitive to temperature-induced probability distribution changes rather than true uncertainty calibration.