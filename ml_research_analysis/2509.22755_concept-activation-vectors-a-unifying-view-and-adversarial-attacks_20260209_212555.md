---
ver: rpa2
title: 'Concept activation vectors: a unifying view and adversarial attacks'
arxiv_id: '2509.22755'
source_url: https://arxiv.org/abs/2509.22755
tags:
- concept
- data
- vectors
- mean
- regression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work establishes a probabilistic framework for analyzing\
  \ Concept Activation Vectors (CAVs) by treating them as random vectors induced by\
  \ the distribution of (non-)concept inputs in latent space. The authors derive closed-form\
  \ expressions for the mean and covariance of different CAV variants\u2014PatternCAV,\
  \ FastCAV, and ridge regression-based CAVs\u2014revealing their theoretical equivalence\
  \ under certain conditions (balanced classes and large regularization)."
---

# Concept activation vectors: a unifying view and adversarial attacks

## Quick Facts
- arXiv ID: 2509.22755
- Source URL: https://arxiv.org/abs/2509.22755
- Reference count: 0
- One-line primary result: CAVs from different methods are theoretically equivalent under balanced classes and large regularization, but TCAV scores are vulnerable to adversarial manipulation.

## Executive Summary
This paper establishes a probabilistic framework for analyzing Concept Activation Vectors (CAVs) by treating them as random vectors induced by the distribution of concept and non-concept inputs in latent space. The authors derive closed-form expressions for the mean and covariance of different CAV variants—PatternCAV, FastCAV, and ridge regression-based CAVs—revealing their theoretical equivalence under certain conditions. They show that FastCAV and PatternCAV are equivalent classifiers with identical accuracy, and that FastCAV is a scaled version of PatternCAV. Additionally, the authors demonstrate that TCAV scores can be manipulated through adversarial attacks by modifying the CAV direction, highlighting vulnerabilities in concept-based explanations that depend on arbitrary non-concept distributions.

## Method Summary
The paper introduces a probabilistic framework treating CAVs as random vectors in latent space, enabling closed-form derivation of their statistical properties. Three CAV computation methods are analyzed: PatternCAV (mean difference between classes), FastCAV (mean difference from global mean), and ridge regression (regularized least squares). The framework derives theoretical expressions for classification accuracy predictions using Gaussian approximations of classifier scores, and demonstrates adversarial vulnerabilities in TCAV scores through sign manipulation attacks on non-concept distributions.

## Key Results
- PatternCAV, FastCAV, and ridge regression CAVs are theoretically equivalent under balanced classes and large regularization
- FastCAV and PatternCAV are equivalent classifiers with identical accuracy (FastCAV = 1/2 × PatternCAV for balanced classes)
- Theoretical accuracy predictions from Gaussian approximations match empirical results on synthetic GMMs and CIFAR-10
- TCAV scores are vulnerable to adversarial attacks through strategic selection of non-concept examples

## Why This Works (Mechanism)

### Mechanism 1: Probabilistic Treatment of CAVs as Random Vectors
- Claim: Treating CAVs as random vectors induced by (non-)concept input distributions enables closed-form derivation of their statistical properties.
- Mechanism: The non-linear transformation of input distributions through neural network layers produces latent activations that, under the q-exponential concentration assumption, yield predictable Gaussian-like distributions for CAV projections. This allows expressing mean and covariance of PatternCAV and FastCAV directly in terms of class-specific first and second-order statistics.
- Core assumption: Hidden-layer activations satisfy Assumption 1 (q-exponential concentration), which holds for Lipschitz transformations of concentrated distributions including neural network outputs.
- Evidence anchors:
  - [abstract] "Adopting a probabilistic perspective, the distribution of the (non-)concept inputs induces a distribution over the CAV, making it a random vector in the latent space."
  - [section 3.1, Proposition 1] Derives E[w_pat] = μ₂ - μ₁ and Cov(w_pat) = (1/n₁)Σ₁ + (1/n₂)Σ₂
  - [corpus] "On The Variability of Concept Activation Vectors" (FMR=0.589) supports the importance of understanding CAV variability, though does not provide the same probabilistic framework.
- Break condition: If latent activations violate concentration assumptions (e.g., heavy-tailed distributions without Lipschitz structure), Gaussian approximations for g(x) may fail, invalidating accuracy predictions from Theorem 1.

### Mechanism 2: Theoretical Equivalence Under Balanced Classes and Large Regularization
- Claim: PatternCAV, FastCAV, and ridge regression-based CAVs converge to equivalent classifiers when classes are balanced (n₁ = n₂) and regularization λ is sufficiently large.
- Mechanism: For balanced classes, FastCAV becomes a scaled version of PatternCAV (w_fast = (1/2)w_pat). With large λ = O(√n), ridge regression solution approximates w_ridge ≈ (1/λ)Xy/√n = (1/2)w_pat. Since classification accuracy depends on the direction rather than magnitude of w, all three methods yield identical accuracy.
- Core assumption: Balanced class sizes and regularization parameter λ scaling as O(√n) for ridge regression equivalence.
- Evidence anchors:
  - [abstract] "The authors derive closed-form expressions...revealing their theoretical equivalence under certain conditions (balanced classes and large regularization)."
  - [section 3.1, Remark 1] Shows w_ridge ≈ (1/2)w_pat for λ = √n
  - [corpus] "FastCAV" paper (arXiv:2505.17883) introduces FastCAV but does not establish the equivalence relationship; corpus evidence for equivalence is absent.
- Break condition: If classes are severely imbalanced (n₁ ≪ n₂ or n₁ ≫ n₂), the scaling relationship w_fast = n₁/(n₁+n₂)w_pat deviates from 1/2, and ridge regression with small λ may not approximate the mean-difference direction.

### Mechanism 3: TCAV Vulnerability via Non-Concept Distribution Manipulation
- Claim: TCAV scores can be adversarially manipulated by modifying the CAV direction through strategic selection of non-concept examples.
- Mechanism: Since CAVs are derived from concept vs. non-concept comparisons, the arbitrary choice of non-concept distribution directly affects the learned direction. The proposed attack optimizes sign assignments to push sensitivity scores S_{C,k,l}(x) away from target values, using a sigmoid-smoothed objective to minimize the count of samples with undesired signs.
- Core assumption: Adversary has access to modify or select non-concept examples used in CAV computation.
- Evidence anchors:
  - [abstract] "CAVs can strongly depend on the rather arbitrary non-concept distribution, a factor largely overlooked in prior work."
  - [section 3.2, Fig. 4] Demonstrates manipulation of TCAV scores across different sign configurations on CIFAR-10
  - [corpus] Corpus evidence on adversarial attacks against CAVs is weak; no directly comparable attack methodology found in neighbor papers.
- Break condition: If non-concept distribution is constrained or randomized systematically (as suggested in original TCAV work), single-attack success probability decreases, though the fundamental vulnerability remains.

## Foundational Learning

- Concept: **Concentration of Measure**
  - Why needed here: Assumption 1 relies on q-exponential concentration to justify Gaussian approximations for classification scores g(x) in high dimensions.
  - Quick check question: Given a d-dimensional isotropic Gaussian, what happens to the variance of 1-Lipschitz functions as d increases?

- Concept: **Ridge Regression in High Dimensions**
  - Why needed here: Theoretical equivalence between CAV methods uses the large-λ approximation where (XX^T/n + λI)^(-1) ≈ (1/λ)I.
  - Quick check question: When λ >> ||XX^T||/n, how does the ridge regression solution relate to the unregularized least squares solution?

- Concept: **Directional Derivatives and Sensitivity**
  - Why needed here: TCAV scores are defined via directional derivatives of class logits with respect to CAV directions (Equation 14).
  - Quick check question: If ∇h_{l,k} is orthogonal to v_C^l at a point, what is the sensitivity score S_{C,k,l}(x)?

## Architecture Onboarding

- Component map:
  Input Data (concept + non-concept examples) -> Neural Network f^l (layers 1 to L) -> Hidden-Layer Activations X ∈ R^{d×n} -> CAV Computation Module -> Classification Accuracy Predictor (Theorem 1) -> TCAV Score Computation

- Critical path:
  1. Collect balanced concept/non-concept examples (n₁ ≈ n₂)
  2. Extract activations at target layer l
  3. Compute class means μ₁, μ₂ and covariances Σ₁, Σ₂
  4. Select CAV method based on computational budget (FastCAV is O(nd), ridge is O(d²n + d³))
  5. Validate accuracy prediction using Theorem 1 before trusting TCAV scores

- Design tradeoffs:
  - FastCAV vs PatternCAV: Identical accuracy but FastCAV requires computing global mean; PatternCAV directly computes difference
  - Large λ ridge regression: More stable but may underfit if true separating hyperplane is not orthogonal to high-variance directions
  - Non-concept selection: Random samples provide baseline robustness; domain-specific non-concepts increase sensitivity but may introduce bias

- Failure signatures:
  - Predicted accuracy significantly exceeds empirical test accuracy → Concentration assumption violated or insufficient samples
  - TCAV scores near 0.5 across all concepts → CAV direction captures noise rather than meaningful concept; check if concept class is well-separated from non-concept in latent space
  - Large discrepancy between ridge and PatternCAV accuracies → λ too small or classes imbalanced

- First 3 experiments:
  1. Validate equivalence claim: On CIFAR-10 with ResNet-18, compute PatternCAV, FastCAV, and ridge CAVs at multiple layers with balanced classes; plot classification accuracies vs λ to confirm convergence at large λ.
  2. Test accuracy prediction: Compute theoretical ε from Proposition 1 + Theorem 1 on synthetic GMM data with known μ_ℓ, Σ_ℓ; compare to empirical test error across varying n and d ratios.
  3. Reproduce adversarial attack: Implement sigmoid-based objective from Section 3.2; measure TCAV score manipulation success rate under different non-concept sampling strategies (random vs optimized).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the robustness of Concept Activation Vectors (CAVs) be generally characterized beyond the specific adversarial attack presented?
- Basis in paper: [explicit] The conclusion states the authors are "leaving a general investigation of CAV's robustness for future work."
- Why unresolved: The paper demonstrates a specific vulnerability but does not provide a comprehensive theoretical or empirical analysis of CAV stability under various perturbations.
- What evidence would resolve it: A formal robustness framework or bounds on CAV variance under distributional shifts of the input data.

### Open Question 2
- Question: How does the arbitrary choice of the non-concept distribution systematically influence the resulting CAV and TCAV scores?
- Basis in paper: [explicit] The abstract notes the vulnerability underscores "the need for a more systematic study" of the dependence on arbitrary non-concept distributions.
- Why unresolved: The paper identifies that CAVs depend on these distributions but does not propose a method to standardize or correct for this dependency.
- What evidence would resolve it: A theoretical model quantifying the sensitivity of the CAV direction $\mathbf{v}_C^l$ to the statistics of the non-concept class.

### Open Question 3
- Question: Do the theoretical equivalences between FastCAV and PatternCAV hold when the $q$-exponential concentration assumption (Assumption 1) is violated?
- Basis in paper: [inferred] The derivations rely on Assumption 1 (data concentration) and Gaussian approximations for the classifier scores.
- Why unresolved: Real-world neural activations may not always satisfy Lipschitz continuity or concentration inequalities, potentially invalidating the predicted accuracy.
- What evidence would resolve it: Empirical evaluation of CAV accuracy predictions on heavy-tailed or non-concentrated activation distributions.

### Open Question 4
- Question: How does class imbalance between concept and non-concept examples affect the equivalence and relative performance of ridge-based CAVs versus FastCAV?
- Basis in paper: [inferred] The theoretical equivalence between FastCAV and PatternCAV is derived specifically for the balanced class case ($n_1=n_2$).
- Why unresolved: While claimed as a mild restriction, the exact degradation of FastCAV's accuracy relative to ridge regression in unbalanced settings is not quantified.
- What evidence would resolve it: A comparison of misclassification errors $\varepsilon$ for different CAV types under controlled class imbalance ratios.

## Limitations

- Theoretical equivalences rely on balanced class sizes and asymptotic assumptions that may not hold in practical settings
- Concentration assumption (Assumption 1) is critical for Gaussian approximations but lacks empirical validation across diverse network architectures and datasets
- Adversarial attack demonstration focuses on sign manipulation but does not explore robustness against constrained non-concept distributions or alternative CAV computation methods

## Confidence

- High Confidence: PatternCAV/FastCAV equivalence relationship (Eq. 11-12) and their identical classification accuracy
- Medium Confidence: Ridge regression equivalence under large λ (Theorem 1 predictions validated on synthetic data but not extensively tested on real networks)
- Low Confidence: General applicability of concentration assumption across different network architectures and the effectiveness of adversarial attacks against alternative TCAV implementations with constrained non-concept distributions

## Next Checks

1. **Empirical validation of concentration assumption**: Test Assumption 1 across multiple network architectures (CNNs, transformers) and datasets by measuring q-exponential concentration parameters and comparing predicted vs empirical g(x) distributions.

2. **Robustness of adversarial attacks**: Evaluate attack success rates when non-concept distributions are constrained (e.g., sampled from specific domains) or when using ensemble CAV methods that average across multiple non-concept sets.

3. **Imbalanced class experiments**: Systematically vary class imbalance ratios and regularization parameters to map the boundary conditions where theoretical equivalences break down, particularly for the ridge regression approximation.