---
ver: rpa2
title: 'DPOT: A DeepParticle method for Computation of Optimal Transport with convergence
  guarantee'
arxiv_id: '2506.23429'
source_url: https://arxiv.org/abs/2506.23429
tags:
- transport
- figure
- network
- training
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DPOT, a novel method for computing optimal
  transport (OT) maps between continuous distributions using unpaired samples. The
  approach combines the DeepParticle method with a min-min optimization framework,
  avoiding the instability of adversarial minimax formulations.
---

# DPOT: A DeepParticle method for Computation of Optimal Transport with convergence guarantee

## Quick Facts
- arXiv ID: 2506.23429
- Source URL: https://arxiv.org/abs/2506.23429
- Authors: Yingyuan Li; Aokun Wang; Zhongjian Wang
- Reference count: 40
- Primary result: Novel min-min optimization OT method with convergence guarantee

## Executive Summary
This paper introduces DPOT, a novel method for computing optimal transport (OT) maps between continuous distributions using unpaired samples. The approach combines the DeepParticle method with a min-min optimization framework, avoiding the instability of adversarial minimax formulations. The method learns non-entropic OT maps through a neural network, enabling direct, single-pass transport without iterative sampling.

Theoretical contributions include a weak convergence guarantee and a quantitative error bound between the learned map and the true OT map. Specifically, the L2-distance between the learned map and the optimal map is controlled by the duality gap, with empirical validation showing good correlation between optimality gap and relative error.

## Method Summary
DPOT formulates optimal transport as a min-min optimization problem where a neural network learns to map samples from a source distribution to a target distribution. The key innovation is replacing the traditional adversarial minimax formulation with a cooperative minimization objective that combines a transport cost term with a Wasserstein-2 distance term. The method uses unpaired samples and avoids entropy regularization, learning non-entropic OT maps that can be applied directly without iterative sampling. The training process alternates between updating the transport map and solving discrete OT problems to approximate the Wasserstein-2 distance to the target.

## Key Results
- Achieves stable training through min-min optimization, avoiding saddle-point instability of adversarial methods
- Provides theoretical convergence guarantee with quantitative error bound relating learned map accuracy to duality gap
- Demonstrates effectiveness across synthetic and real-world tasks including image color transfer and high-dimensional Bayesian inference
- Shows flexibility with different network architectures (MLP, ResNet, ICNN) and scalability to high dimensions
- Achieves significant speedups in high-dimensional Bayesian inference compared to traditional sampling methods

## Why This Works (Mechanism)

### Mechanism 1: Min-min Optimization Stability
- **Claim:** If the learning objective is formulated as a min-min problem rather than a minimax problem, the training process may avoid the saddle-point instability inherent in adversarial OT methods (like WGANs).
- **Mechanism:** The DPOT loss function $P(T) := \lambda(I_\mu(T))^{1/2} + W_2(T_\#\mu, \nu)$ requires minimizing both the transport cost and the Wasserstein-2 distance to the target. This contrasts with dual formulations that require maximizing a discriminator while minimizing a generator. By resolving the inner optimization (calculating $W_2$ via a discrete solver) and the outer optimization (updating the map $T$) as cooperative minimization steps, gradients become more stable.
- **Core assumption:** The discrete OT solver used to approximate the $W_2$ term provides a sufficiently smooth or stable signal for the outer neural network to follow.
- **Evidence anchors:**
  - [abstract] "The proposed method leads to a min-min optimization during training and does not impose any restriction on the network structure."
  - [section 3.1] "Compared to traditional adversarial minimax OT methods... (3.1) yields a stable min-min optimization problem... This avoids saddle-point instability."
  - [corpus] Neighbors like "Stochastic Optimization in Semi-Discrete Optimal Transport"

## Foundational Learning

### Concept 1: Optimal Transport (OT)
- **Why needed:** Provides the theoretical foundation for measuring distances between probability distributions and finding optimal maps between them.
- **Quick check:** Verify understanding of Wasserstein distance and Kantorovich duality as the basis for OT formulations.

### Concept 2: Min-max vs Min-min Optimization
- **Why needed:** Critical distinction that explains DPOT's stability advantage over adversarial methods.
- **Quick check:** Compare convergence properties and training dynamics of min-max vs min-min formulations in optimization problems.

### Concept 3: DeepParticle Method
- **Why needed:** The particle-based approach that enables learning OT maps from unpaired samples without density estimation.
- **Quick check:** Understand how particle representations approximate continuous distributions and enable gradient-based learning.

### Concept 4: Discrete Optimal Transport Solvers
- **Why needed:** The computational engine that approximates Wasserstein distances within the learning loop.
- **Quick check:** Review computational complexity and numerical properties of solvers like Sinkhorn or auction algorithms.

### Concept 5: Neural Network Architectures for OT
- **Why needed:** Different architectures (MLP, ResNet, ICNN) offer tradeoffs between expressiveness, stability, and computational efficiency.
- **Quick check:** Compare activation functions and architectural constraints that preserve desirable properties in OT learning.

## Architecture Onboarding

### Component Map
DPOT consists of: Neural Network (Transport Map) -> Discrete OT Solver -> Loss Function -> Gradients -> Network Update

### Critical Path
1. Sample source and target distributions
2. Apply neural network to source samples
3. Compute Wasserstein-2 distance using discrete OT solver
4. Calculate combined loss (transport cost + Wasserstein term)
5. Backpropagate gradients to update network parameters

### Design Tradeoffs
- **Discrete OT solver choice:** Affects computational cost and gradient quality
- **Network architecture:** Balances expressiveness with training stability
- **Regularization parameter λ:** Controls tradeoff between transport cost and target matching
- **Particle count:** Impacts approximation accuracy vs computational burden

### Failure Signatures
- **Training instability:** May indicate inappropriate solver choice or learning rate issues
- **Poor convergence:** Could suggest insufficient network capacity or suboptimal λ value
- **Mode collapse:** May occur if network overfits to dominant modes in target distribution

### First Experiments
1. Simple Gaussian-to-Gaussian mapping to verify basic functionality
2. Swiss roll to Gaussian to test handling of non-linear transformations
3. Image color transfer to validate practical utility on real data

## Open Questions the Paper Calls Out
None

## Limitations

- **Scalability with Problem Size:** The method relies on solving discrete OT problems to approximate $W_2(T_\#\mu, \nu)$, which has computational complexity that scales poorly with the number of particles.
- **Convergence Guarantee Scope:** The theoretical convergence guarantee applies to weak convergence of the pushforward measure, but the quantitative error bound requires careful interpretation.
- **Architecture Dependence:** While the method claims flexibility with network architectures, the empirical results heavily favor ResNets and ICNNs.

## Confidence

- **High Confidence:** The min-min formulation provides stability advantages over adversarial methods
- **Medium Confidence:** The quantitative error bound relating duality gap to map accuracy
- **Low Confidence:** Scalability claims to high-dimensional problems

## Next Checks

1. **Scalability Benchmark:** Systematically evaluate performance scaling with particle count (e.g., 1k, 10k, 100k particles) on a standardized OT problem to quantify computational bottlenecks.

2. **Architecture Robustness Test:** Compare convergence rates and final accuracy across different network architectures (MLP, CNN, Transformer) on the same OT tasks to validate architecture independence claims.

3. **Generalization Stress Test:** Evaluate map generalization to unseen regions of the target distribution by training on partial target samples and testing transport accuracy on held-out target regions.