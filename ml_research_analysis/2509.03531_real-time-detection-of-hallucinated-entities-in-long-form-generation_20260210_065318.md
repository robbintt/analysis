---
ver: rpa2
title: Real-Time Detection of Hallucinated Entities in Long-Form Generation
arxiv_id: '2509.03531'
source_url: https://arxiv.org/abs/2509.03531
tags:
- long-form
- hallucination
- probes
- detection
- probe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses hallucination detection in long-form language
  model generation, where fabricated entities can cause serious harm in high-stakes
  applications. The authors present a scalable approach for real-time identification
  of hallucinated tokens by training lightweight probes that detect fabricated entities
  as they are generated.
---

# Real-Time Detection of Hallucinated Entities in Long-Form Generation

## Quick Facts
- **arXiv ID:** 2509.03531
- **Source URL:** https://arxiv.org/abs/2509.03531
- **Reference count:** 40
- **Primary result:** Achieves AUC of 0.90 on Llama-3.3-70B vs 0.71 for semantic entropy baselines for real-time hallucination detection

## Executive Summary
This work addresses hallucination detection in long-form language model generation, where fabricated entities can cause serious harm in high-stakes applications. The authors present a scalable approach for real-time identification of hallucinated tokens by training lightweight probes that detect fabricated entities as they are generated. The method uses an annotation pipeline with web search to label entity spans as supported or fabricated, then trains linear and LoRA-based probes on hidden states to predict these labels at the token level. Across four model families, the probes achieve strong performance with effective generalization from long-form to short-form settings and across different models.

## Method Summary
The approach involves generating long-form completions from models like Llama and Gemma, then using Claude 4 Sonnet with web search to label entity spans as "Supported" or "Not Supported/Insufficient." Linear probes and LoRA adapters are trained on hidden states to classify tokens as hallucinated or real. The probe loss combines token-wise binary cross-entropy with span-max binary cross-entropy, weighted to focus on entity tokens. LoRA probes include KL regularization to minimize behavioral changes to the underlying model. The system enables real-time intervention by detecting hallucination risk and allowing systems to abstain from answering when necessary.

## Key Results
- Linear probes achieve AUC of 0.90 on Llama-3.3-70B, significantly outperforming semantic entropy baseline (0.71)
- Probes effectively transfer from long-form to short-form settings and generalize across different model families
- LoRA probes with KL regularization maintain generation quality while providing strong detection performance
- Real-time detection enables systems to abstain from answering when hallucination risk is detected

## Why This Works (Mechanism)
The approach works by leveraging web search-annotated data to create a strong signal for what constitutes a hallucinated entity. By training lightweight probes on hidden states at the token level, the system can identify fabricated entities as they are generated in real-time. The span-max strategy ensures that the entire entity span is flagged if any part of it is detected as hallucinated, while KL regularization preserves the model's generation capabilities.

## Foundational Learning
- **Entity-level hallucination detection**: Needed to identify fabricated facts in long-form generation where traditional metrics fail; quick check: probe performance on synthetic hallucination datasets
- **Hidden state probing**: Required to extract meaningful signals from model internals without fine-tuning the full model; quick check: compare probe performance vs full fine-tuning
- **Web search annotation**: Essential for creating ground truth labels distinguishing supported vs fabricated entities; quick check: manual validation of annotation pipeline accuracy
- **KL regularization for LoRA**: Critical to prevent probe training from degrading generation quality; quick check: monitor win rates and KL divergence during LoRA training
- **Span-max classification**: Necessary to ensure entire entity spans are flagged when hallucinations are detected; quick check: precision-recall trade-off at different span thresholds

## Architecture Onboarding
- **Component map:** Data generation -> Annotation pipeline -> Hidden state extraction -> Probe training -> Real-time detection
- **Critical path:** The annotation pipeline with web search is the bottleneck, as it requires expensive LLM calls and introduces label noise that constrains probe performance
- **Design tradeoffs:** Linear probes offer better detection but degrade generation quality, while LoRA probes balance detection with preservation of model behavior through KL regularization
- **Failure signatures:** Early plateau in probe performance indicates label noise issues; significant drop in win rates suggests model degradation; poor generalization indicates overfitting to specific domains
- **First experiments:** 1) Train probe on synthetic hallucination dataset to establish baseline performance, 2) Evaluate probe generalization to short-form generations, 3) Test LoRA probe with and without KL regularization on generation quality

## Open Questions the Paper Calls Out
- **Relational hallucinations:** Can detection scale to errors between real entities without explicit supervision? The authors note errors are often "relational rather than atomic" and suggest future work should expand beyond entity spans.
- **Generation-time interventions:** Can errors be corrected mid-generation rather than forcing abstention? Section 7 calls for "sophisticated generation-time interventions that preserve informativeness."
- **Annotation pipeline improvement:** How can the pipeline reduce ~20% missed recall and 15.8% false positive rate? The authors state annotation noise creates a performance ceiling that constrains training effectiveness.

## Limitations
- Automated annotation pipeline introduces significant label noise (15.8% false positive rate, 80.6% recall), potentially limiting probe effectiveness
- Performance degrades substantially on smaller models and requires fine-tuning to maintain generation quality
- Method focuses on entity-level hallucinations and cannot detect non-entity hallucinations like logical inconsistencies

## Confidence
- **High confidence:** Core methodology and experimental results showing AUC improvements over baselines are well-documented and reproducible
- **Medium confidence:** Generalization claims across model families are supported but based on limited model combinations
- **Low confidence:** Real-world impact assessment in high-stakes domains is primarily theoretical with limited empirical validation

## Next Checks
1. **Label quality audit:** Manually verify 100 randomly sampled training labels to quantify actual false positive rate and assess whether 15.8% estimate is accurate
2. **Cross-domain generalization test:** Evaluate probe performance on held-out domain (e.g., medical or legal text) not represented in LongFact/LongFact++
3. **Behavioral impact study:** Measure actual change in model win rates and output quality when LoRA probes are applied with and without KL regularization across 5+ diverse generation tasks