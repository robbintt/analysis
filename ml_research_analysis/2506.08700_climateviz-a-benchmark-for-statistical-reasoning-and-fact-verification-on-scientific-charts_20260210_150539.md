---
ver: rpa2
title: 'ClimateViz: A Benchmark for Statistical Reasoning and Fact Verification on
  Scientific Charts'
arxiv_id: '2506.08700'
source_url: https://arxiv.org/abs/2506.08700
tags:
- chart
- scientific
- claims
- reasoning
- charts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ClimateViz introduces the first large-scale benchmark for scientific
  fact-checking using real-world expert-curated charts. It contains 49,862 claims
  paired with 2,896 visualizations, each annotated as support, refute, or not enough
  information.
---

# ClimateViz: A Benchmark for Statistical Reasoning and Fact Verification on Scientific Charts

## Quick Facts
- arXiv ID: 2506.08700
- Source URL: https://arxiv.org/abs/2506.08700
- Reference count: 40
- Key outcome: Current MLLMs achieve only 76.2-77.8% accuracy on scientific chart fact-checking vs. 89.3-92.7% human performance

## Executive Summary
ClimateViz introduces the first large-scale benchmark for scientific fact-checking using real-world expert-curated charts. It contains 49,862 claims paired with 2,896 visualizations, each annotated as support, refute, or not enough information. Structured knowledge graph explanations capture trends, comparisons, and causal relations. Evaluations across state-of-the-art multimodal models show significant challenges: even the best models (e.g., Gemini 2.5, InternVL 2.5) reach only 76.2-77.8% accuracy, far below human performance (89.3-92.7%). Explanation-augmented outputs improve some closed-source models, but few-shot prompting offers limited benefit. Fine-tuned chart-specific models lag behind general-purpose MLLMs. Results indicate that statistical reasoning over charts remains difficult for current models, highlighting the need for advances in multimodal reasoning and structured explanation generation.

## Method Summary
ClimateViz benchmark comprises 49,862 claims paired with 2,896 expert-curated climate science charts. The task involves predicting whether a textual claim is supported, refuted, or has not enough information (NEI) relative to a chart image and caption. Two input settings are used: CT (Chart + Caption + Claim) and CTT (Chart + DePlot-extracted table + Caption + Claim). Two output modes: label-only and explanation-augmented (triplets + label). Models are evaluated in zero-shot and few-shot (6 examples) settings. Knowledge graph explanations are generated using GPT-4o with canonicalization via self-definition prompts. Performance metrics include accuracy, macro F1 for labels, and BLEU/METEOR/ROUGE-L/BERTScore for triplets.

## Key Results
- Best models (Gemini 2.5, InternVL 2.5) achieve only 76.2-77.8% accuracy, significantly below human performance of 89.3-92.7%
- Explanation-augmented outputs improve closed-source models (o3: 84.6% vs 59.3% label-only)
- CTT setting boosts open-source models (InternVL 2.5, Qwen 2.5-VL) to 77.8% accuracy under few-shot
- Few-shot prompting shows limited benefit, sometimes degrading performance
- Fine-tuned chart-specific models underperform general-purpose MLLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explanation-augmented outputs improve fact-checking accuracy for closed-source models on scientific charts.
- Mechanism: Generating intermediate structured reasoning triplets before the final label forces models to explicitly ground predictions in extracted facts, reducing hallucination and improving alignment with visual evidence.
- Core assumption: The model can reliably produce canonicalized triplets that accurately reflect chart content.
- Evidence anchors:
  - [abstract] "Explanation-augmented outputs significantly enhance performance in some closed-source models, notably o3 and Gemini 2.5."
  - [section] Table 5: o3 achieves 84.6% accuracy in explanation-augmented CT setting vs 59.3% label-only.
  - [corpus] Limited direct corpus support; TSVer paper addresses time-series reasoning but not explanation generation mechanisms.
- Break condition: If triplets contain factual errors or non-canonicalized relations, the reasoning chain corrupts the final label.

### Mechanism 2
- Claim: Adding structured tabular representations (chart-to-table conversion) boosts open-source model performance under few-shot prompting.
- Mechanism: DePlot extracts numerical data from charts into tables, providing an explicit structured scaffold that helps smaller models perform quantitative comparisons without relying solely on visual perception.
- Core assumption: DePlot conversions faithfully represent chart data; table extraction errors propagate.
- Evidence anchors:
  - [section] "CTT setting significantly boosts the performance of open-source models under few-shot prompting... Qwen 2.5 and InternVL both reach 77.8% accuracy."
  - [section] Appendix E: Manual evaluation shows DePlot is "Fully Accurate" for 58% of line/bar charts but struggles with maps and scatter plots.
  - [corpus] No corpus papers directly validate chart-to-table conversion as a fact-checking enhancement mechanism.
- Break condition: If chart has overlapping text, spatial encodings, or small fonts (common in maps/scatter plots), table extraction fails, degrading CTT performance.

### Mechanism 3
- Claim: Knowledge graph explanations enable interpretable multi-hop reasoning by capturing trends, comparisons, and causal relations.
- Mechanism: GPT-4o extracts (head, relation, tail) triplets from charts with metadata (time, units, uncertainty), canonicalized via self-definition prompts to standardize entity/relation names across examples.
- Core assumption: LLM-based extraction produces factually correct triplets; canonicalization preserves semantics.
- Evidence anchors:
  - [abstract] "Structured knowledge graph explanations that capture statistical patterns, temporal trends, spatial comparisons, and causal relations."
  - [section] Appendix D: Triplets include metadata like time_range, unit, uncertainty; canonicalization maps "was about" → "amount".
  - [corpus] GraphCheck paper proposes entity-relationship graphs for multi-hop reasoning but doesn't address chart-specific KGs.
- Break condition: Ambiguous captions or overloaded visual encodings produce incorrect or incomplete triplets, breaking the reasoning chain.

## Foundational Learning

- **Multimodal fact-checking**: Combining visual chart understanding with textual claim verification. Why needed: ClimateViz requires models to simultaneously parse visual elements (axes, legends, trends) and linguistically analyze claims for consistency. Quick check: Can you trace how a model would verify "CO2 levels rose 12% from 2010-2020" against a line graph showing actual values?

- **Statistical reasoning types**: Temporal comparison, value extraction, anomaly detection, trend detection, etc. Why needed: 79% of ClimateViz claims require 3-4 distinct reasoning types compositionally (Figure 2). Models must chain these operations. Quick check: What reasoning types are needed to verify "Greenland ice mass loss contributed ~14mm to sea level rise from 2000-2020"?

- **Canonicalization in knowledge graphs**: Standardizing entity/relation names. Why needed: The paper shows high BERTScore but low BLEU for triplet generation—models produce semantically correct but non-canonicalized outputs, hurting evaluation. Quick check: How would you map "led to" and "caused" to a canonical relation like "contributes_to"?

## Architecture Onboarding

- **Component map**: Chart + caption → (optional DePlot table) → MLLM → if explanation-augmented: generate triplets first, then label; if label-only: direct prediction
- **Critical path**: Chart + caption → (optional DePlot table) → MLLM → if explanation-augmented: generate triplets first, then label; if label-only: direct prediction
- **Design tradeoffs**:
  - CT vs CTT: CTT adds computational overhead (DePlot extraction) but significantly helps open-source models; CT is simpler but relies entirely on visual perception
  - Label-only vs explanation-augmented: Explanations improve closed-source accuracy but require canonicalization infrastructure; not all models benefit (GPT-4o degrades in few-shot FS for explanations—Table 6)
  - Zero-shot vs few-shot: Few-shot helps open-source models in CTT but hurts closed-source models in CT (Gemini 2.5 drops from 76.2% to 57.4%)
- **Failure signatures**:
  - Low BLEU but high BERTScore: Semantically plausible but non-canonicalized triplets
  - CTT underperforms CT: Likely DePlot extraction errors on complex chart types (maps, scatter plots with overlapping labels)
  - Few-shot degrades performance: Overloading context window or prompt mismatch with model training
- **First 3 experiments**:
  1. Baseline CT zero-shot on test set with label-only output; measure accuracy gap from human (89.3%).
  2. Compare CT vs CTT for a single open-source model (e.g., InternVL 2.5) under few-shot; expect ~16-point accuracy gain per Table 5.
  3. Evaluate explanation-augmented output for o3 on a 50-example subset; manually check if triplet errors correlate with label errors.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can advanced reasoning strategies like Chain-of-Thought (CoT) or Tree-of-Thought (ToT) prompting significantly improve fact-checking accuracy on scientific charts compared to standard few-shot learning?
- Basis in paper: [explicit] The authors explicitly state in the "Limitations" section that they did not explore advanced prompting strategies such as CoT or program-guided reasoning, which may further aid compositional tasks.
- Why unresolved: The paper only evaluates zero-shot and few-shot in-context learning, finding limited benefits from standard few-shot prompting.
- What evidence would resolve it: Experimental results applying CoT or ToT prompting to the ClimateViz benchmark showing significant accuracy gains over the reported few-shot baselines.

### Open Question 2
- Question: Do models trained or evaluated on ClimateViz generalize effectively to scientific fact-checking tasks in other domains with different visual conventions?
- Basis in paper: [explicit] The "Limitations" section notes that the dataset is domain-specific (climate) and that generalization to other scientific disciplines with different terminologies or visual formats remains untested.
- Why unresolved: The current benchmark is restricted to climate science charts, leaving performance on diverse scientific visualizations (e.g., genetic sequencing plots, chemical spectra) unknown.
- What evidence would resolve it: Cross-domain evaluation results where models fine-tuned on ClimateViz are tested on expert-curated chart datasets from fields like biomedicine or materials science.

### Open Question 3
- Question: To what extent do automatic metrics for explanation generation (e.g., BLEU, BERTScore) correlate with human judgments of factual soundness and logical validity?
- Basis in paper: [explicit] The "Limitations" section highlights that current automatic metrics may not fully capture factual soundness or semantic coherence, suggesting future work incorporate human evaluations.
- Why unresolved: The paper reports high BERTScores but low BLEU scores for triplets, indicating semantic plausibility but poor canonicalization, yet the true logical validity of these explanations remains unverified by humans.
- What evidence would resolve it: A human evaluation study comparing model-generated triplets against ground truth for logical entailment and factual accuracy, correlated with automatic metric scores.

## Limitations

- Explanation-augmented performance shows inconsistent benefits across model families, with GPT-4o degrading in few-shot settings
- DePlot table extraction reliability is limited, with only 58% "Fully Accurate" for line/bar charts and poor performance on maps/scatter plots
- Canonicalization schema may not fully capture semantic richness needed for complex scientific claims

## Confidence

- **High confidence**: The core finding that current multimodal models struggle with scientific chart fact-checking (76.2-77.8% accuracy vs 89.3-92.7% human performance). The dataset construction methodology and basic experimental setup are well-documented and reproducible.
- **Medium confidence**: The specific performance numbers for individual models across settings, particularly the explanation-augmented improvements. These depend on proprietary API behaviors and prompt engineering details not fully specified.
- **Low confidence**: The generalization of CTT setting benefits across chart types and the scalability of the knowledge graph explanation approach to more complex scientific domains beyond climate science.

## Next Checks

1. **Triplets-to-labels error analysis**: Select 100 test samples where models make incorrect predictions and manually examine whether errors stem from (a) incorrect triplet extraction, (b) incorrect canonicalization, or (c) correct triplets but wrong final label. This will isolate whether the bottleneck is in KG construction or reasoning.

2. **DePlot robustness testing**: Systematically evaluate DePlot extraction accuracy across all 2,896 charts in ClimateViz, not just the sampled manual evaluation. Compare CTT performance stratified by chart type (line/bar vs. maps/scatter) to quantify the contribution of table extraction quality to overall accuracy gains.

3. **Cross-domain generalization**: Apply the ClimateViz methodology to a small sample (50-100 examples) from a different scientific domain (e.g., biomedical charts from medical literature). Measure whether the same models achieve comparable performance and whether triplet extraction patterns transfer across domains.