---
ver: rpa2
title: Exploring the Reliability of Self-explanation and its Relationship with Classification
  in Language Model-driven Financial Analysis
arxiv_id: '2503.15985'
source_url: https://arxiv.org/abs/2503.15985
tags:
- language
- classification
- factuality
- self-explanations
- financial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the reliability of self-explanations generated
  by language models (LMs) in financial classification tasks, focusing on factuality
  and causality. Using a public financial dataset, the research demonstrates a statistically
  significant relationship between the accuracy of classifications and the factuality
  or causality of self-explanations.
---

# Exploring the Reliability of Self-explanation and its Relationship with Classification in Language Model-driven Financial Analysis

## Quick Facts
- arXiv ID: 2503.15985
- Source URL: https://arxiv.org/abs/2503.15985
- Reference count: 12
- One-line primary result: Self-explanation quality (factuality/causality) correlates with classification accuracy in zero-shot financial tasks, suggesting explanations can serve as confidence proxies.

## Executive Summary
This study investigates the reliability of self-explanations generated by language models in financial classification tasks, focusing on factuality and causality. Using the German Credit Dataset, the research demonstrates that factual inaccuracies and logical inconsistencies in explanations correlate with classification errors. Among evaluated models, Llama-3.2-3B showed fewer factuality issues while Gemma-2-2B exhibited fewer causality issues. The findings establish an empirical foundation for using self-explanations as proxies for classification confidence and optimizing financial classification through reasoning.

## Method Summary
The study employed zero-shot inference with instruction-tuned language models (Llama-3.2-3B, Phi-3.5-3.8B, Gemma-2-2B) on the German Credit Dataset. Models generated classifications with self-explanations, which were manually annotated for factuality (any false statement) and causality (logical inconsistency) issues. Chi-squared tests examined the statistical dependence between explanation quality and classification accuracy. The dataset was preprocessed with currency conversion, feature binarization, and imbalance handling, with experiments conducted on 100 cases (50 positive/50 negative).

## Key Results
- Factual inaccuracies and logical inconsistencies in self-explanations correlate with classification errors (Chi-squared p<0.05)
- Llama-3.2-3B exhibited fewer factuality issues while Gemma-2-2B showed fewer causality issues
- Factuality demonstrated a stronger correlation with classification accuracy than causality in most scenarios
- Zero-shot self-explanations can serve as proxies for classification confidence in financial tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-explanation quality (factuality and causality) can serve as a proxy signal for classification confidence in zero-shot financial tasks.
- Mechanism: When LMs generate factually incorrect or logically inconsistent explanations, these errors correlate with classification mistakes. The presence of hallucinated information or contradictory reasoning in self-explanations signals degraded internal representation quality, which manifests as incorrect predictions.
- Core assumption: The relationship between explanation quality and classification accuracy is directionally consistent—poor explanations indicate uncertain classifications rather than being independent phenomena.
- Evidence anchors: "Chi-squared tests reveal that factual inaccuracies and logical inconsistencies in explanations correlate with classification errors"; "The P value is lower than 0.05, suggesting statistically significant dependence between the occurrence of factuality and the accuracy of classification at a confidence level of 95%."
- Break condition: If factuality and causality issues occur at similar rates regardless of classification correctness, the proxy relationship dissolves.

### Mechanism 2
- Claim: Factuality serves as a stronger confidence proxy than causality for detecting unreliable classifications.
- Mechanism: Factual grounding errors (hallucinations) represent more fundamental failures in knowledge retrieval than logical inconsistency, which may reflect surface-level reasoning noise. Models that fabricate information have likely activated incorrect knowledge pathways that also corrupt the classification decision.
- Core assumption: Factual errors and logical inconsistencies are measurable along independent dimensions; factual errors propagate more directly to decision boundaries.
- Evidence anchors: "In most scenarios, while both factuality and causality exhibited statistically significant relationship with classification accuracy, factuality demonstrated a stronger correlation"; "Recent advancements in LMs have substantially mitigated the factuality issue."
- Break condition: If causality issues scale with model size while factuality remains stable, causality may become the dominant signal in larger models.

### Mechanism 3
- Claim: Zero-shot self-explanations reveal proprietary reasoning processes that correlate with output quality without requiring in-context examples.
- Mechanism: Instruction-tuned models generate explanations conditioned on their internal representations. When these representations are well-formed (accurate knowledge, coherent reasoning), both the explanation and classification reflect this quality. The explanation acts as an observable trace of otherwise latent reasoning.
- Core assumption: Self-explanations faithfully reflect—rather than post-hoc rationalize—the reasoning process used for classification.
- Evidence anchors: "We investigated zero-shot classification scenarios, where explanations, referred to as self-explanations, were generated by LMs without the aid of in-context examples"; "Our study built an empirical foundation for approximating classification confidence through self-explanations and for optimizing classification via proprietary reasoning."
- Break condition: If self-explanations are confabulations unconnected to actual decision processes, the correlation is spurious and cannot support confidence estimation.

## Foundational Learning

- Concept: **Chi-squared test for independence**
  - Why needed here: The paper's central claim rests on Chi-squared tests demonstrating non-independence between explanation quality and classification accuracy. Understanding p-values and null hypothesis rejection is essential to evaluate evidence strength.
  - Quick check question: If p = 0.06 for a Chi-squared test, what conclusion follows about the relationship between variables?

- Concept: **Zero-shot vs. few-shot prompting**
  - Why needed here: The study isolates zero-shot performance to evaluate self-explanations without in-context guidance, distinguishing their findings from prior work on explanation-enhanced few-shot learning.
  - Quick check question: Why might zero-shot self-explanations differ qualitatively from explanations generated with few-shot examples?

- Concept: **Hallucination and faithfulness in LLMs**
  - Why needed here: Factuality issues are a form of hallucination; causality issues relate to faithfulness. Distinguishing "what the model says" from "why the model decided" is critical for interpreting self-explanations as confidence proxies.
  - Quick check question: Can a self-explanation be faithful but non-factual? Give an example.

## Architecture Onboarding

- Component map: Processed German credit dataset -> Instruction-tuned LMs (Llama-3.2-3B, Phi-3.5-3.8B, Gemma-2-2B) -> Structured classification + natural language self-explanation -> Human annotation for factuality/causality -> Chi-squared test for independence

- Critical path: Data preprocessing (currency conversion, feature binarization, imbalance handling) -> Zero-shot prompt design requesting classification + explanation -> Annotation protocol for factuality and causality errors -> Statistical testing (Chi-squared) to establish relationship

- Design tradeoffs: Smaller models (2-4B parameters) enable local inference but may underrepresent capabilities of larger models; binary issue classification (any error = issue) loses granularity but simplifies annotation; single dataset limits generalizability but enables controlled analysis

- Failure signatures: High factuality issue prevalence (>0.30) with low classification accuracy suggests unreliable proxy relationship; causality issues >80% (as in Llama-3.2-3B) may indicate ceiling effects limiting proxy utility; non-significant Chi-squared p-values indicate explanation quality is independent of accuracy—proxy fails

- First 3 experiments: 1) Replicate Chi-squared analysis on held-out cases from the same dataset to validate correlation stability; 2) Test whether automated factuality detection (e.g., grounding against external knowledge) correlates with human annotations at similar strength; 3) Evaluate whether prompting strategies that explicitly request step-by-step reasoning reduce causality issues and improve classification accuracy

## Open Questions the Paper Calls Out

- Does the statistically significant relationship between self-explanation quality and classification accuracy generalize to financial tasks beyond credit risk classification? [explicit] The authors state in the Discussion that "experiments were limited to a single task" and that "future study should evaluate a broader range of tasks to verify the generalizability of our findings." The study relied exclusively on the German credit dataset, leaving uncertainty about whether the correlations between factuality/causality and accuracy hold in other financial contexts like sentiment analysis or stock movement prediction. Replication of the experimental pipeline on diverse financial NLP tasks (e.g., FinTextQA, earning call analysis) yielding similar Chi-squared statistics between explanation errors and classification errors would resolve this.

- Does the correlation between hallucination rates and classification accuracy persist in larger, state-of-the-art language models? [explicit] The paper notes that "due to computational constraints, we did not extend our experiments to LMs such as Llama-3.2-90B" and suggests future research should determine if "susceptibility to hallucination with larger model parameters also hold in financial applications." The study was restricted to smaller models (2B–4B parameters); it is unknown if emergent abilities or different error modes in larger models decouple explanation quality from classification performance. Experimental results from models with significantly larger parameter counts (e.g., 70B+), showing whether the prevalence of factuality issues continues to correlate strongly with classification errors, would resolve this.

- Can an automated detector for factuality and causality issues reliably serve as a proxy for classification confidence? [explicit] The authors explicitly schedule further study to "fine-tune an automated detector for accurately identifying factuality or causality issues within self-explanations and test its utility as a confidence proxy." The current findings relied on manual annotations of a subset of cases (top 100), whereas a practical confidence metric requires an automated, scalable system. The successful training of a classifier that identifies logical/factual errors in explanations and demonstrates a high correlation with the likelihood of a correct classification in unseen data would resolve this.

- To what extent can reasoning-enhancement strategies optimize financial classification performance when benchmarked against automatically identified explanation errors? [explicit] The authors list a scheduled further study to "benchmark existing reasoning-enhancement strategies... based on automatically identified errors in optimizing financial applications." While the paper establishes that explanations correlate with accuracy, it has not yet validated the reverse: that specifically targeting and correcting these explanation errors via techniques like Reflexion improves the final classification output. A comparative study showing that models utilizing error-based reasoning enhancements achieve higher accuracy (and lower weighted cost) compared to standard zero-shot baselines would resolve this.

## Limitations
- Findings hinge on subjective human annotation of factuality and causality issues with no established inter-rater reliability metrics reported
- Results derived from a single financial dataset and three small models (2-4B parameters), limiting generalizability
- Chi-squared tests demonstrate correlation but not causation between explanation quality and classification accuracy

## Confidence
- **High confidence**: The empirical finding that self-explanation quality correlates with classification accuracy in the German Credit dataset using the tested models and annotation protocol
- **Medium confidence**: The generalizability of self-explanations as reliable confidence proxies across financial domains and larger models
- **Medium confidence**: The comparative claim that factuality serves as a stronger proxy than causality for detecting unreliable classifications

## Next Checks
1. Replicate the Chi-squared analysis on an independent financial dataset (e.g., credit scoring or loan default prediction) to test domain generalizability
2. Implement automated factuality detection using knowledge-grounding techniques and compare correlation strength with human annotations
3. Test the proxy relationship with larger models (20B+ parameters) to determine if the factuality-causality dynamics scale predictably