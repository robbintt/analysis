---
ver: rpa2
title: FFE-Hallu:Hallucinations in Fixed Figurative Expressions:Benchmark of Idioms
  and Proverbs in the Persian Language
arxiv_id: '2601.20105'
source_url: https://arxiv.org/abs/2601.20105
tags:
- figurative
- persian
- task
- meaning
- idiom
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces FFE-Hallu, the first benchmark for evaluating
  figurative hallucination in large language models (LLMs), focusing on fixed figurative
  expressions (FFEs) such as idioms and proverbs in Persian. FFE-Hallu consists of
  600 curated instances across three tasks: (1) FFE generation from meaning, (2) detection
  of fabricated FFEs, and (3) FFE-to-FFE translation from English to Persian.'
---

# FFE-Hallu: Hallucinations in Fixed Figurative Expressions: Benchmark of Idioms and Proverbs in the Persian Language

## Quick Facts
- arXiv ID: 2601.20105
- Source URL: https://arxiv.org/abs/2601.20105
- Reference count: 28
- Key outcome: Introduces FFE-Hallu, the first benchmark for evaluating figurative hallucination in LLMs using 600 Persian FFEs across generation, detection, and translation tasks, finding GPT-4.1 best overall but most models struggle with cultural grounding.

## Executive Summary
This paper introduces FFE-Hallu, the first benchmark for evaluating figurative hallucination in large language models, focusing on fixed figurative expressions (FFEs) such as idioms and proverbs in Persian. The benchmark consists of 600 curated instances across three tasks: FFE generation from meaning, detection of fabricated FFEs, and FFE-to-FFE translation from English to Persian. Evaluating six state-of-the-art multilingual LLMs, the study finds that GPT-4.1 achieves the best overall performance with lower hallucination rates and higher accuracy in rejecting fabricated expressions. However, most models struggle to distinguish real FFEs from high-quality fabrications and frequently hallucinate during cross-lingual translation, highlighting substantial gaps in current LLMs' handling of figurative language and cultural grounding.

## Method Summary
The FFE-Hallu benchmark consists of 600 items: 200 authentic Persian FFEs with meanings, 200 fabricated FFEs across four adversarial categories, and 200 bilingual English-Persian idiom pairs. Six LLMs were evaluated across three tasks: (1) generating Persian FFEs from given meanings, (2) detecting fabricated FFEs using both "Is this fake?" and "Is this real?" prompts, and (3) translating English FFEs to Persian equivalents. Human annotators classified outputs into Correct, Incorrect, or Hallucinated, while LLM-as-judge evaluation with web search compared automatic vs. human judgments using Cohen's κ agreement.

## Key Results
- GPT-4.1 achieved the lowest hallucination rates (11.5% generation, 11.0% detection, 13.0% translation) and highest accuracy across all tasks
- Most models confused high-quality fabrications with real FFEs, particularly struggling with Semantic Inversion and Structural Mimicry categories
- Prompt sensitivity revealed instability: DeepSeek-V3 showed nearly 60% performance decline between "fake" and "real" detection prompts
- LLM-as-judge evaluation showed moderate reliability (κ range 0.26-0.60), with Gemini 2.5 Pro outperforming GPT-4.1 for high-hallucination generators

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Figurative hallucination constitutes a distinct failure mode from factual hallucination, requiring cultural-linguistic grounding rather than fact verification.
- Mechanism: FFEs are non-compositional and conventionally fixed; models generate plausible-sounding expressions by recombining surface patterns without possessing authentic cultural knowledge. The paper operationalizes this by constructing adversarial fabrications across four categories (Word Perturbation, Semantic Inversion, Structural Mimicry, Cultural Fabrication) that probe different failure sources.
- Core assumption: Models rely on statistical surface patterns rather than grounded cultural knowledge when processing figurative language.
- Evidence anchors:
  - [abstract] "We define figurative hallucination as the generation or endorsement of expressions that sound idiomatic and plausible but do not exist as authentic figurative expressions in the target language."
  - [Section 2.1] "Unlike factual hallucinations, such errors cannot be reliably identified through external fact checking and instead require cultural and linguistic grounding for accurate assessment."
  - [corpus] "Beyond Understanding" paper (arxiv 2510.23828) similarly emphasizes pragmatic gap in cultural processing of figurative language.

### Mechanism 2
- Claim: Prompt framing significantly affects hallucination detection reliability, revealing instability in models' figurative competence.
- Mechanism: Models process "Is this fake?" and "Is this real?" prompts asymmetrically. The paper measures cross-prompt agreement as a reliability indicator—GPT-4.1 achieved 86% agreement while DeepSeek-V3 showed only 38.5%, indicating the latter responds to question framing rather than authentic recognition.
- Core assumption: Genuine figurative competence should be invariant to prompt framing.
- Evidence anchors:
  - [Section 4.1, Task 2 results] "Performance is consistently lower under the Fake-detection prompt than under Real-confirmation... exposing instability in figurative hallucination detection."
  - [Section 4.1] "DeepSeek-V3 exhibit[s] a sharp performance decline—nearly 60%—between prompt types."

### Mechanism 3
- Claim: Automatic evaluation of figurative hallucination depends critically on hallucination-detection capability of the judge model.
- Mechanism: LLM-as-judge evaluation shows higher agreement with human annotations when judge models can reliably distinguish non-existent FFEs from rare but valid expressions. Gemini 2.5 Pro outperformed GPT-4.1 as judge (higher Cohen's κ), particularly on high-hallucination generators.
- Core assumption: Judge models with web-search access can approximate expert human cultural knowledge for verification.
- Evidence anchors:
  - [Section 5.1] "Hallucination detection emerges as the primary challenge for automatic judges. As hallucination rates increase, GPT-4.1 frequently misclassifies non-existent FFEs as merely incorrect real expressions."
  - [Section 5] "Gemini 2.5 Pro is more robust in these cases, maintaining higher κ values and better distinguishing hallucinated FFEs from rare but valid expressions."

## Foundational Learning

- Concept: Non-compositional meaning in idioms/proverbs
  - Why needed here: FFEs cannot be interpreted via literal word-by-word analysis; understanding this explains why models fail on both generation and translation tasks.
  - Quick check question: Can you explain why "kick the bucket" cannot be understood compositionally, and how this differs from a novel phrase like "kick the chair"?

- Concept: Hallucination taxonomy (intrinsic vs. extrinsic; factual vs. non-factual)
  - Why needed here: The paper positions figurative hallucination as extrinsic, non-factual—understanding this taxonomy clarifies why standard fact-verification approaches don't apply.
  - Quick check question: If a model produces a real idiom but with wrong meaning for the context, is this a hallucination per the paper's definition?

- Concept: Cultural grounding in language models
  - Why needed here: FFE competence requires not just linguistic fluency but cultural knowledge; this distinction underpins the benchmark's design rationale.
  - Quick check question: What makes detecting a culturally plausible but fabricated proverb harder than detecting a factually false statement?

## Architecture Onboarding

- Component map:
  - Task 1 (Generation): Input = figurative meaning → Output = Persian FFE; labels: Correct/Incorrect/Hallucinated
  - Task 2 (Detection): Input = fabricated FFE → Output = real/fake judgment; two prompt framings tested
  - Task 3 (Translation): Input = English FFE → Output = Persian equivalent FFE
  - Fabrication categories: Word Perturbation, Semantic Inversion, Structural Mimicry, Cultural Fabrication
  - Evaluation: Human annotation (Tasks 1, 3), automatic ground-truth (Task 2), LLM-as-judge validation

- Critical path:
  1. Understand the three-category labeling scheme (Correct/Incorrect/Hallucinated) and distinction between semantic errors vs. hallucinations
  2. Review the four fabrication construction strategies in Appendix B to understand adversarial design
  3. Note prompt-sensitivity testing protocol (Fake-detection vs. Real-confirmation) for Task 2

- Design tradeoffs:
  - Single-language focus (Persian) ensures annotation quality but limits generalizability
  - 600-item dataset size is modest but reflects curation difficulty for validated FFEs
  - LLM-as-judge offers scalability but imperfect alignment with human judgments (Cohen's κ range 0.26–0.60)

- Failure signatures:
  - High hallucination + low incorrect rate (e.g., Qwen3-235B): model lacks cultural knowledge entirely
  - Prompt-sensitivity gap >40% (e.g., DeepSeek-V3): model responds to framing, not genuine recognition
  - High agreement + low κ: annotators agree on obvious failures but disagree on subtle cases

- First 3 experiments:
  1. Replicate Task 2 using both prompt framings on your target model; calculate cross-prompt agreement as reliability indicator
  2. Test whether paraphrased meanings reduce hallucination (as observed in Task 1 results) to probe memorization vs. generalization
  3. Validate LLM-as-judge against human annotations on a sample to calibrate automatic evaluation reliability before scaling

## Open Questions the Paper Calls Out
- Can mitigation strategies—such as culturally-grounded fine-tuning or retrieval augmentation—substantially reduce figurative hallucination rates in LLMs?
- Do findings generalize to other languages, particularly low-resource or typologically diverse ones?
- Why are "Semantic Inversion" and "Structural/Syntactic Mimicry" fabrication categories consistently harder for models to detect than "Word Perturbation" and "Cultural/Historical Fabrication"?
- Can improved LLM-as-a-judge models or prompt designs achieve reliability comparable to expert human annotators for figurative hallucination detection?

## Limitations
- Benchmark restricted to Persian language, limiting generalizability to other linguistic contexts
- Dataset size (600 items) is modest relative to typical LLM evaluation scales
- LLM-as-judge evaluation shows substantial reliability gaps (Cohen's κ range 0.26-0.60) compared to human annotation

## Confidence
- **High confidence**: GPT-4.1's superior performance across all tasks is consistently demonstrated with clear statistical separation from other models. The prompt-framing effects in Task 2 are robust and reproducible across multiple models.
- **Medium confidence**: The cultural grounding hypothesis is well-supported by the data but relies on indirect inference from performance gaps rather than direct measurement of cultural knowledge. The fabrication categories effectively probe different failure modes but may not be exhaustive.
- **Low confidence**: The automatic evaluation reliability (LLM-as-judge) shows substantial variance and limited agreement with human annotations, particularly for high-hallucination generators. This suggests the current approach may not provide definitive judgments for subtle cases.

## Next Checks
1. **Cross-linguistic replication**: Apply the FFE-Hallu methodology to a Romance language (e.g., Spanish or Portuguese) to test whether observed hallucination patterns are language-specific or reflect fundamental LLM limitations with figurative expressions.

2. **Judge reliability calibration**: Conduct systematic ablation studies comparing human vs. LLM-as-judge performance across hallucination severity levels, measuring how judge reliability degrades as generator hallucination rates increase.

3. **Cultural knowledge probing**: Design targeted experiments distinguishing between surface pattern memorization and genuine cultural grounding by testing models on novel but culturally plausible FFE variants versus purely fabricated expressions.