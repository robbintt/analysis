---
ver: rpa2
title: 'Adding LLMs to the psycholinguistic norming toolbox: A practical guide to
  getting the most out of human ratings'
arxiv_id: '2509.14405'
source_url: https://arxiv.org/abs/2509.14405
tags:
- words
- estimates
- human
- data
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a methodology for using Large Language Models
  (LLMs) to estimate psycholinguistic word characteristics, addressing the challenge
  of obtaining human-based norms which are often costly and time-consuming. The authors
  propose leveraging LLMs to directly predict various word features, both through
  direct use of base models and fine-tuning approaches.
---

# Adding LLMs to the psycholinguistic norming toolbox: A practical guide to getting the most out of human ratings

## Quick Facts
- arXiv ID: 2509.14405
- Source URL: https://arxiv.org/abs/2509.14405
- Reference count: 16
- Primary result: LLM-based psycholinguistic norming achieves 0.8 Spearman correlation with human ratings, improving to 0.9 with fine-tuning

## Executive Summary
This paper presents a methodology for using Large Language Models (LLMs) to estimate psycholinguistic word characteristics, addressing the challenge of obtaining human-based norms which are often costly and time-consuming. The authors propose leveraging LLMs to directly predict various word features, both through direct use of base models and fine-tuning approaches. They emphasize validation against human "gold standard" norms and provide a software framework supporting commercial and open-weight models. A case study on estimating English word familiarity demonstrates that base models achieve a Spearman correlation of 0.8 with human ratings, which improves to 0.9 with fine-tuning. The work provides practical guidance, including prompt engineering and parameter configuration, while highlighting limitations like model inscrutability and the importance of careful validation.

## Method Summary
The methodology employs LLMs to generate psycholinguistic norm estimates through two approaches: direct inference from base models and fine-tuning on human-rated data. Base models extract token probability distributions (logprobs) to produce continuous estimates rather than single discrete outputs. Fine-tuning adapts pre-trained models to specific rating scales using a subset of human data. The approach is validated against established human "gold standard" norms, with performance measured using Spearman and Pearson correlations. The framework supports both commercial APIs and open-weight models, with a Python implementation for managing prompts, API calls, and parameter configurations. The case study focuses on English word familiarity ratings from established databases.

## Key Results
- Base LLM models achieve Spearman correlation of 0.8 with human familiarity ratings
- Fine-tuning increases correlation to 0.9 with human ratings
- The methodology successfully estimates word familiarity, concreteness, valence, and arousal
- Commercial models currently outperform open-weight alternatives

## Why This Works (Mechanism)

### Mechanism 1: Base LLM Token Probability Extraction for Norm Estimation
- Claim: Base LLMs can generate psycholinguistic norm estimates (e.g., familiarity, concreteness) that correlate with human ratings by leveraging token probability distributions.
- Mechanism: The LLM is prompted to output a rating (e.g., 1-7). Instead of taking the single highest-probability token, the model's log-probabilities (logprobs) for all possible rating tokens are extracted. A weighted average is calculated from these probabilities to produce a continuous estimate, providing higher granularity than a single integer output.
- Core assumption: The distributional statistics learned by the LLM during pre-training encode sufficient information about human lexical perception (e.g., frequency, context usage) to approximate subjective human judgments on psycholinguistic dimensions.
- Evidence anchors:
  - [abstract]: "Our approach covers both the direct use of base LLMs... Using base models, we achieved a Spearman correlation of 0.8 with human ratings."
  - [section 2.1 / 5]: Explains that models output a probability distribution over tokens (logprobs). Section 5 details calculating a weighted average: "In this way, we have calculated estimates of word familiarity, concreteness, valence, and arousal."
  - [corpus]: Weak/Indirect. Corpus neighbors discuss LLM alignment but do not detail this specific logprob-weighting mechanism.
- Break condition: The prompt asks for output requiring multiple tokens (e.g., "4.25") instead of a single token integer, making full log-probability tree extraction computationally infeasible (Section 5, A.3.1).

### Mechanism 2: Fine-Tuning for Alignment Improvement
- Claim: Fine-tuning an LLM on a subset of human-rated psycholinguistic data significantly improves the correlation of its estimates with human norms for unseen words.
- Mechanism: A base LLM is further trained (fine-tuned) on a dataset containing words and their corresponding human psycholinguistic ratings (e.g., ~1500-2000 words). The model's weights are adjusted via backpropagation to minimize the error between its predictions and the human targets, thereby adapting its output distribution for the specific rating task.
- Core assumption: The linguistic knowledge captured during massive pre-training is general and robust; fine-tuning primarily serves to align the model's output format and internal representation to the specific scale and criteria of the human judgment task without requiring extensive new data.
- Evidence anchors:
  - [abstract]: "...fine-tuning of models, an alternative that can yield substantial performance gains... Spearman correlation... increased to 0.9 when employing fine-tuned models."
  - [section 6 / A.4]: Section 6 states fine-tuning can yield substantial gains. Section A.4.3 shows GPT correlation improvements of ~0.10 and Llama improvements of ~0.22-0.30 after fine-tuning.
  - [corpus]: Weak. Corpus neighbors do not provide comparative data on fine-tuning effectiveness for this specific norming task.
- Break condition: The fine-tuning dataset is small, biased, or unrepresentative (e.g., only early-acquired words), causing the model to fail on items outside the training distribution (Section A.4.1).

### Mechanism 3: Validation-Driven Pipeline
- Claim: Reliable LLM-based psycholinguistic norms require a systematic validation pipeline that compares LLM outputs against a "gold standard" of human ratings.
- Mechanism: The process involves (1) collecting human ratings for a representative sample of words, (2) generating LLM estimates for the same words using controlled prompts and parameters (e.g., via API, temperature=0), and (3) computing correlations (Spearman/Pearson) between LLM and human ratings to quantify alignment before using the model for data augmentation.
- Core assumption: Human inter-annotator agreement on psycholinguistic ratings provides a meaningful and achievable upper bound for LLM performance; exceeding this suggests the model may be capturing a form of "wisdom of the crowd" (referenced in text).
- Evidence anchors:
  - [abstract]: "A major emphasis in the guide is the validation of LLM-generated data with human 'gold standard' norms."
  - [section 4]: States that inter-annotator correlation (~0.8-0.9) is a threshold for human-like performance and a basic check is comparing model output with human data.
  - [corpus]: Relevant. Corpus paper "Psycholinguistic Word Features: a New Approach for the Evaluation of LLMs Alignment with Humans" aligns with this validation focus.
- Break condition: No validation data exists for the target language or construct, or the validation set is contaminated (i.e., was part of the LLM's pre-training data), leading to unreliable or uninterpretable correlation scores.

## Foundational Learning

- **Concept**: LLM Log-Probabilities (Logprobs)
  - Why needed here: This is the core technical mechanism for extracting granular, continuous estimates from an LLM rather than a single discrete token. Understanding it is essential for implementing the base model approach described in Mechanism 1.
  - Quick check question: If an LLM assigns logprobs of -0.5 (token '4') and -1.0 (token '5'), which token has a higher raw probability and how would you calculate a weighted score?

- **Concept**: Fine-Tuning vs. Pre-Training
  - Why needed here: The paper proposes fine-tuning as the primary method for boosting performance. Knowing the difference is critical for choosing between using a base model or investing in a fine-tuning workflow.
  - Quick check question: Does fine-tuning primarily add new factual knowledge to an LLM, or does it adjust existing weights to specialize the model's output format and behavior for a specific task?

- **Concept**: Prompt Engineering and Context Contamination
  - Why needed here: Prompt design and API configuration (temperature, independent calls) directly impact the reliability and reproducibility of the estimates. Misunderstanding this leads to the "pitfalls" described in Section 7.
  - Quick check question: Why is it recommended to use an API with independent calls (no conversation history) and a temperature of 0 for generating stable psycholinguistic norms?

## Architecture Onboarding

- **Component map**: 
  - Validation Dataset (Human Gold Standard) -> LLM (Base or Fine-tuned) -> Prompting/Execution Layer -> Logprob Extraction & Weighting Module -> Evaluation Module

- **Critical path**:
  1. Acquire/create a human-rated validation dataset for the target psycholinguistic feature
  2. Develop prompts and configure API calls (e.g., temperature=0) for the chosen LLM
  3. Generate initial estimates using the base LLM and calculate correlations with the validation set
  4. (Optional/Recommended) Split data into training/test sets, fine-tune the LLM on the training set, and re-evaluate on the test set
  5. If correlations meet or exceed inter-annotator agreement benchmarks, deploy the model for data augmentation on new words

- **Design tradeoffs**:
  - Commercial (e.g., GPT-4o) vs. Open-Weight (e.g., Llama): Trade-off between higher performance/reproducibility risk (commercial) and lower performance/full control/reproducibility (open-weight). The paper notes commercial models currently outperform open-weight ones but carry risks of deprecation.
  - Base Model vs. Fine-Tuning: Trade-off between implementation simplicity (zero-shot prompting) and higher accuracy/alignment (fine-tuning). Fine-tuning requires data, compute, and more complex setup.

- **Failure signatures**:
  - Low Correlation (œÅ < 0.7-0.8): Indicates poor prompt design, unsuitable model, or that the construct is too abstract/unreliable even for humans
  - Collapsed Output Range: Model only outputs integers (e.g., 3, 4, 5) or ignores decimal instructions. Often solved by logprob weighting
  - Data Contamination: Suspiciously high performance on known public datasets but poor generalization to new words

- **First 3 experiments**:
  1. **Baseline Establishment**: Select a simple prompt and a base model (e.g., GPT-4o-mini). Generate estimates for words in an existing human-rated dataset (e.g., Glasgow norms) and calculate the Spearman correlation. This establishes a baseline.
  2. **Logprob Optimization**: Modify the prompting code to extract token log-probabilities for the rating scale (1-5 or 1-7). Calculate a weighted average score for each word and re-calculate correlation with the human data to measure improvement over the discrete token baseline.
  3. **Fine-Tuning Test**: Split the validation dataset (e.g., 1500 training, 1000 test). Fine-tune the base model on the training set. Generate estimates for the test set with the new fine-tuned model and compare its correlation to the base model's performance on the same test set.

## Open Questions the Paper Calls Out
None

## Limitations
- The methodology currently only validated for English word familiarity ratings, with uncertain generalizability to other psycholinguistic dimensions and languages
- Reliance on commercial APIs introduces risks of model deprecation, cost accumulation, and reproducibility challenges
- Fine-tuning requires computational resources and curated training data that may not be readily available for all researchers

## Confidence

**High Confidence**: The core methodology of using LLM log-probabilities for generating continuous psycholinguistic estimates is technically sound and well-demonstrated. The correlation results (0.8 base, 0.9 fine-tuned) are plausible given the task and validation approach, and the validation pipeline concept is methodologically rigorous.

**Medium Confidence**: The reported performance improvements from fine-tuning (~0.10-0.30 correlation gain) are reasonable but depend heavily on the quality and representativeness of the fine-tuning dataset. The generalization of these findings to other psycholinguistic constructs and languages remains uncertain without additional empirical validation.

**Low Confidence**: Claims about the broader applicability of this approach across all psycholinguistic dimensions and languages are speculative, as the current evidence is limited to English familiarity ratings from two specific datasets.

## Next Checks

1. **Cross-Dimension Validation**: Apply the same methodology to estimate concreteness, valence, and arousal ratings using the Glasgow norms dataset, then calculate correlations with human ratings for each dimension to assess generalizability beyond familiarity.

2. **Language Transfer Test**: Translate the optimal few-shot prompt and validation dataset into a second language (e.g., Spanish or German) with available human psycholinguistic norms, then replicate the base model and fine-tuning experiments to evaluate cross-linguistic performance.

3. **Data Contamination Analysis**: Select a subset of words from the Glasgow norms that were definitely not in the pre-training data (based on publication date and vocabulary analysis), then compare LLM performance on this clean subset versus the full dataset to quantify the impact of potential contamination on correlation scores.