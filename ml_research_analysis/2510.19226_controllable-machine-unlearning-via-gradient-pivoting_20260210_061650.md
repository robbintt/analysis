---
ver: rpa2
title: Controllable Machine Unlearning via Gradient Pivoting
arxiv_id: '2510.19226'
source_url: https://arxiv.org/abs/2510.19226
tags:
- unlearning
- methods
- class
- performance
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of machine unlearning, where
  the goal is to remove the influence of specific data from a trained model while
  maintaining model fidelity. The authors propose a novel approach, CUP (Controllable
  Unlearning by Piviting Gradient), which reframes machine unlearning as a multi-objective
  optimization (MOO) problem.
---

# Controllable Machine Unlearning via Gradient Pivoting

## Quick Facts
- arXiv ID: 2510.19226
- Source URL: https://arxiv.org/abs/2510.19226
- Reference count: 40
- Key outcome: CUP reframes unlearning as MOO, uses pivoting mechanism to navigate Pareto frontier with single hyperparameter γ, outperforming baselines on classification and generation tasks

## Executive Summary
This paper introduces CUP (Controllable Unlearning by Pivoting Gradient), a novel approach to machine unlearning that frames the problem as multi-objective optimization. The method employs a unique pivoting mechanism that allows precise navigation of the entire Pareto frontier between unlearning efficacy and model fidelity using a single hyperparameter called 'unlearning intensity'. CUP demonstrates superior performance compared to existing methods on various vision tasks including image classification (CIFAR-10, SVHN) and image generation (CIFAR-10 with DDPM).

## Method Summary
CUP reframes machine unlearning as a multi-objective optimization problem where the goal is to balance unlearning efficacy (forgetting specific data) against model fidelity (maintaining performance on remaining data). The method introduces a pivoting mechanism that computes two orthogonal gradient anchors: g_eff (orthogonal to the gradient of remaining data) and g_fid (orthogonal to the gradient of forgetting data). By interpolating between these anchors using a single hyperparameter γ, CUP enables fine-grained control over the trade-off between forgetting and fidelity. The approach is validated using a hypervolume indicator as a holistic metric to evaluate solution set quality and diversity.

## Key Results
- CUP outperforms existing methods (WS, GA, SalUn) on both classification and generation tasks
- Achieves superior trade-off between unlearning efficacy and model fidelity across the entire Pareto frontier
- Demonstrates effective class-wise forgetting on CIFAR-10, SVHN with ResNet-18, and image generation with DDPM

## Why This Works (Mechanism)
The pivoting mechanism works by creating two orthogonal gradient anchors that represent the extremes of the unlearning-fidelity trade-off. By interpolating between these anchors using γ, the method can precisely navigate the Pareto frontier. The orthogonality ensures that movements along the interpolated direction simultaneously optimize both objectives without interference. This geometric approach provides continuous control rather than discrete points, allowing fine-grained adjustment of the unlearning intensity.

## Foundational Learning
- **Multi-objective optimization (MOO)**: Framework for optimizing multiple competing objectives simultaneously; needed to formalize unlearning as balancing forgetting vs fidelity; quick check: verify Pareto optimality conditions
- **Gradient projection**: Mathematical operation to find orthogonal components of vectors; needed to compute the two gradient anchors; quick check: confirm dot products equal zero
- **Hypervolume indicator**: Metric for evaluating solution set quality in MOO; needed to holistically compare different unlearning methods; quick check: validate against known Pareto fronts
- **Orthogonal decomposition**: Technique for separating vector components; needed to construct the pivoting mechanism; quick check: verify norm preservation
- **Pareto frontier**: Set of optimal trade-off solutions; needed to understand the space of possible unlearning-fidelity balances; quick check: plot trade-off curves
- **Gradient normalization**: Scaling gradients to unit vectors; needed for stable interpolation in the pivoting mechanism; quick check: verify ||g_γ|| ≈ 1

## Architecture Onboarding
**Component map**: Pre-trained model -> Compute ∇L_f and ∇L_r -> Project to get g_eff and g_fid -> Interpolate via g_γ -> Update parameters -> Evaluate metrics

**Critical path**: The pivoting mechanism (g_γ interpolation) is the core innovation that distinguishes CUP from baselines. It must be implemented with precise numerical stability.

**Design tradeoffs**: Single hyperparameter γ vs. multiple hyperparameters in baselines; continuous control vs. discrete solutions; computational overhead of orthogonal projections vs. simpler gradient manipulation.

**Failure signatures**: Over-forgetting (RA drops sharply while UA saturates), numerical instability in projections (exploding gradients), poor hypervolume due to non-diverse solutions (flat Pareto front).

**First experiments**:
1. Verify orthogonal projection step: Compute g_eff and g_fid for a simple 2D case and confirm orthogonality
2. Test pivoting mechanism: Sweep γ from 0 to 1 and plot the resulting RA-UA trade-off curve
3. Baseline comparison: Implement WS method and compare hypervolume on CIFAR-10 forgetting task

## Open Questions the Paper Calls Out
None

## Limitations
- The pivoting mechanism relies on precise gradient normalization and orthogonal projections that are not fully detailed
- The relationship between γ values and actual trade-off points appears somewhat empirical rather than theoretically grounded
- Hypervolume comparisons may be affected by different normalization schemes across heterogeneous metrics

## Confidence
**High confidence**: CUP outperforms baseline methods on reported metrics across classification and generation tasks; experimental methodology is reproducible.

**Medium confidence**: The pivoting mechanism provides continuous control over the Pareto frontier; single hyperparameter γ claim needs more rigorous validation.

**Low confidence**: Hypervolume indicator comparisons across methods are fully fair given different normalization schemes and metric definitions.

## Next Checks
1. Reproduce the orthogonal projection step for g_eff and g_fid with controlled numerical precision to verify the pivoting mechanism works as claimed.
2. Generate the complete Pareto frontier for multiple forgetting scenarios to validate the continuity and coverage claims of the γ parameter.
3. Cross-validate the hypervolume calculations by implementing the exact normalization procedure described for each metric to ensure fair comparison across baselines.