---
ver: rpa2
title: Optimizing Retrieval Components for a Shared Backbone via Component-Wise Multi-Stage
  Training
arxiv_id: '2602.00805'
source_url: https://arxiv.org/abs/2602.00805
tags:
- retrieval
- legal
- training
- stage
- shared
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of optimizing shared retrieval
  backbones in production legal AI systems, where a single model serves multiple applications.
  The proposed solution employs a multi-stage training framework with progressively
  harder supervision, applied to both embedding-based retrievers and rerankers.
---

# Optimizing Retrieval Components for a Shared Backbone via Component-Wise Multi-Stage Training

## Quick Facts
- arXiv ID: 2602.00805
- Source URL: https://arxiv.org/abs/2602.00805
- Reference count: 24
- Key outcome: Mixed-stage training (Stage 3 embeddings + Stage 2 rerankers) improves MRR from 0.855 to 0.935 and nDCG from 0.615 to 0.680 on CSAID legal benchmark

## Executive Summary
This work addresses the challenge of optimizing shared retrieval backbones in production legal AI systems where a single model serves multiple applications. The proposed solution employs a multi-stage training framework with progressively harder supervision, applied to both embedding-based retrievers and rerankers. Component-wise evaluation reveals that embeddings benefit from later-stage training for improved recall, while rerankers exhibit stage-dependent trade-offs in ranking quality. A mixed-stage configuration combining Stage 3 embeddings with Stage 2 rerankers achieves end-to-end performance improvements with only 6.7% relative latency increase.

## Method Summary
The method employs a three-stage curriculum-style training approach for dense retrievers and rerankers. Stage 1 uses large-scale weak supervision for broad semantic alignment, Stage 2 refines on auto-mined hard samples where the model underperforms, and Stage 3 applies robustness calibration with dynamically refreshed challenging negatives. Both embeddings and rerankers undergo all three stages, but the optimal checkpoint differs: Stage 3 embeddings achieve highest recall while Stage 2 rerankers deliver best ranking quality on context-rich queries. The approach uses LoRA adapters stacked across stages, enabling component-wise optimization without retraining from scratch.

## Key Results
- Stage 3 embeddings achieve significantly higher recall than earlier stages at all retrieval budgets (K=20, 60, 100)
- Stage 2 rerankers outperform Stage 3 on CSAID's context-rich queries (MRR 0.710 vs 0.680) but Stage 3 generalizes better to STARD
- Mixed-stage configuration (Stage 3 embedding + Stage 2 reranker) achieves 0.935 MRR and 0.680 nDCG on CSAID
- Production deployment shows 54.6% user preference rate over baseline with only 6.7% latency overhead

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Progressively harder supervision across training stages improves embedding recall for shared retrieval backbones.
- Mechanism: Curriculum-style training moves from broad semantic alignment (Stage 1: large-scale weak supervision) → hard-sample refinement (Stage 2: auto-mined failure cases) → robustness calibration (Stage 3: dynamically refreshed challenging negatives). Each stage targets different retrieval objectives rather than monotonically optimizing a single metric.
- Core assumption: Hard samples from earlier stages meaningfully represent edge cases that improve discrimination, rather than just noise.
- Evidence anchors:
  - [abstract] "multi-stage optimization framework for dense retrievers and rerankers...different retrieval components exhibit stage-dependent trade-offs"
  - [section 2] "each stage emphasizes a different aspect of retrieval quality, with supervision progressing from broad semantic alignment to fine-grained relevance discrimination and robustness calibration"
  - [corpus] Related work on domain adaptation (arXiv:2601.13525) suggests compression/ adaptation improves transfer, but does not directly validate curriculum schedules for legal domains
- Break condition: If Stage 1 weak supervision is too noisy or domain-mismatched, later stages may amplify errors rather than refine representations.

### Mechanism 2
- Claim: Embeddings and rerankers reach optimal performance at different training stages, making mixed-stage configuration more effective than uniform checkpoint selection.
- Mechanism: Embeddings primarily affect candidate coverage (recall-oriented), which improves monotonically through Stage 3. Rerankers handle fine-grained ranking, which peaks at Stage 2 on context-rich queries (CSAID) but Stage 3 on standardized retrieval (STARD), revealing non-monotonic behavior and explicit trade-offs.
- Core assumption: The functional roles of retrieval components (coverage vs. ranking) can be decoupled and optimized independently via stage selection.
- Evidence anchors:
  - [section 3.2] "later training stages consistently achieve higher recall under the same retrieval budgets" for embeddings; "reranking quality does not improve monotonically across training stages"
  - [section 3.3] "mixed-stage configuration...combining the Stage 3 embedding model with the Stage 2 reranker" achieves best end-to-end performance
  - [corpus] Pre-training vs. fine-tuning studies (arXiv:2505.07166) examine knowledge acquisition in dense retrieval but do not address component-wise stage selection
- Break condition: If downstream applications have highly heterogeneous relevance criteria, a single mixed-stage configuration may not generalize across all use cases.

### Mechanism 3
- Claim: Improved embedding recall enables smaller retrieval budgets with equivalent coverage, reducing downstream reranking cost and latency.
- Mechanism: Later-stage embeddings achieve higher recall at fixed K, meaning the same recall target can be reached with fewer candidates. This allows latency-sensitive configurations to reduce candidates passed to rerankers without sacrificing coverage.
- Core assumption: Latency gains from reduced candidate retrieval and reranking outweigh the ~6.7% overhead from multi-stage LoRA adapter stacking.
- Evidence anchors:
  - [section 3.4] "later-stage embeddings achieve comparable recall with substantially smaller retrieval budgets, allowing the system to reduce the number of retrieved candidates"
  - [section 3.5] "average overhead of approximately 0.10 seconds per query (6.7% relative increase), mainly due to stacking multiple LoRA adapters"
  - [corpus] Evidence on latency-budget trade-offs in dense retrieval is limited in neighbor corpus; no direct validation of this specific mechanism
- Break condition: If application latency budgets are extremely tight (<100ms total), even 6.7% overhead may be unacceptable despite quality gains.

## Foundational Learning

- Concept: **Curriculum Learning**
  - Why needed here: The multi-stage training framework is explicitly inspired by curriculum learning—understanding how progressively harder training signals affect model behavior is essential for interpreting stage-dependent trade-offs.
  - Quick check question: Can you explain why training on easy examples before hard ones might change which local minimum a model converges to?

- Concept: **Dense Retrieval Architecture (Embedding + Reranking)**
  - Why needed here: The paper assumes a two-stage retrieval pipeline where embeddings handle candidate recall and rerankers handle fine-grained ordering. Understanding this separation is necessary to interpret why different stages optimize different components.
  - Quick check question: Why might a model optimized for recall (embeddings) behave differently from one optimized for ranking (rerankers)?

- Concept: **Shared-Backbone Deployment Constraints**
  - Why needed here: The entire motivation rests on industrial settings where one model serves multiple applications. Understanding coupling constraints (deployment, rollback, monitoring) explains why component-wise optimization matters.
  - Quick check question: What happens to all dependent applications if a shared retrieval backbone is rolled back due to a failure in one downstream task?

## Architecture Onboarding

- Component map:
  - Data Layer: Legal corpora (Case Law, Regulatory, Commentary)
  - Retrieval Layer: Embedding Models → Vector Index → Reranker Models
  - Application Layer: Regulatory Search, Legal Research & Reasoning, Long-form Text Analysis, Agent-based Orchestration
  - Interaction Layer: Multiple independent entrances (IME/API)

- Critical path:
  1. Query enters through Interaction Layer
  2. Embedding model encodes query → Vector Index retrieval (Top-K candidates)
  3. Reranker re-orders candidates → Return to Application Layer
  4. Multi-stage training pipeline (offline): Stage 1 (weak supervision) → Stage 2 (hard samples) → Stage 3 (challenging negatives)

- Design tradeoffs:
  - Embedding stage selection: Later stages = better recall but more training compute
  - Reranker stage selection: Stage 2 = better CSAID ranking; Stage 3 = better STARD generalization
  - Retrieval budget (K): Higher K = better coverage but more reranking latency
  - Mixed vs. uniform configuration: Mixed-stage optimizes component roles but increases model management complexity

- Failure signatures:
  - Recall plateau: If embedding recall doesn't improve across stages, check weak supervision quality and domain alignment
  - Reranker degradation on specific query types: May indicate stage mismatch—try earlier or later checkpoint
  - Latency spike beyond 6.7%: Check LoRA adapter stacking efficiency and vector index scaling
  - Preference rate near 50% in A/B tests: Mixed-stage config may not suit all query distributions

- First 3 experiments:
  1. Baseline component sweep: Evaluate Base, Stage 1, Stage 2, Stage 3 embeddings and rerankers independently on CSAID and STARD to reproduce stage-dependent trade-offs before mixing.
  2. Retrieval budget ablation: Plot recall@K curves for each embedding stage to identify minimum K achieving target recall (e.g., 0.80), then measure end-to-end latency impact.
  3. Mixed-stage validation: Combine Stage 3 embeddings with Stage 2 rerankers on held-out queries not in training/evaluation sets; verify MRR/nDCG improvements hold and latency overhead remains <10%.

## Open Questions the Paper Calls Out
- How to optimize latency overhead from stacking multiple LoRA adapters while preserving mixed-stage performance benefits
- Whether the mixed-stage configuration generalizes across different legal domains and languages beyond Chinese
- How to scale the approach for larger models and more diverse retrieval scenarios

## Limitations
- The paper lacks precise details on training data composition, hard negative mining algorithms, and exact LoRA adapter stacking procedures
- The 6.7% latency overhead is reported but not decomposed into embedding retrieval time vs. reranking time contributions
- The mixed-stage configuration assumes component roles are stable across query distributions, which may not hold for all legal applications

## Confidence
- **High Confidence**: The empirical observation that embeddings benefit from later-stage training for recall improvement is directly supported by quantitative Recall@K curves
- **Medium Confidence**: The claim that mixed-stage configuration outperforms uniform checkpoints is well-supported by end-to-end MRR/nDCG metrics but relies on assumptions about component decoupling
- **Low Confidence**: The mechanism explaining why Stage 2 rerankers perform better on CSAID while Stage 3 generalizes to STARD is plausible but not directly validated through ablation studies

## Next Checks
1. Component independence test: Freeze Stage 3 embeddings and train rerankers from Stage 1 through Stage 3 independently to verify if Stage 2 truly optimizes context-rich ranking vs. Stage 3 optimizing generalization
2. Cross-domain transfer: Apply the mixed-stage configuration (Stage 3 embedding + Stage 2 reranker) to a non-legal dense retrieval benchmark to test if the component-wise stage selection generalizes beyond legal domains
3. Latency breakdown analysis: Measure individual latency contributions of embedding retrieval, candidate scoring, and reranking at each stage to identify optimization opportunities beyond the reported 6.7% overhead