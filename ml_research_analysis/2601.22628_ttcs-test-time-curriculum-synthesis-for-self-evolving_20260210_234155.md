---
ver: rpa2
title: 'TTCS: Test-Time Curriculum Synthesis for Self-Evolving'
arxiv_id: '2601.22628'
source_url: https://arxiv.org/abs/2601.22628
tags:
- test
- training
- questions
- ttcs
- solver
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes TTCS, a co-evolving test-time training framework
  that couples a capability-aware question synthesizer with an online self-evolving
  solver. The synthesizer generates progressively challenging variants of test questions
  to create a structured curriculum, while the solver updates itself using self-consistency
  rewards from sampled responses.
---

# TTCS: Test-Time Curriculum Synthesis for Self-Evolving

## Quick Facts
- arXiv ID: 2601.22628
- Source URL: https://arxiv.org/abs/2601.22628
- Reference count: 40
- Primary result: Co-evolving test-time training framework achieving +24.19 average improvement on mathematical benchmarks

## Executive Summary
TTCS introduces a test-time curriculum synthesis framework where a capability-aware question synthesizer and an online self-evolving solver co-train via GRPO. The synthesizer generates progressively challenging variants of test questions, targeting the solver's capability frontier, while the solver updates itself using self-consistency rewards. This co-evolutionary approach significantly outperforms existing test-time methods, especially on difficult tasks like AIME24/25, by creating structured curricula that bridge the distribution gap between solver capability and test question difficulty.

## Method Summary
TTCS couples a question synthesizer and solver that iteratively co-evolve via GRPO. The synthesizer generates M=4 synthetic questions per test question, targeting capability frontier via variance-driven rewards. The solver samples K=10 responses per synthetic question to compute self-consistency scores and rewards. Both agents update via GRPO (synthesizer: 5 steps; solver: 15 steps) with online filtering (|s-0.5|≤δ=0.25) to ensure high-quality pseudo-labels. The framework uses capability-adaptive rewards, similarity penalties, and KL regularization to maintain curriculum quality and stability.

## Key Results
- Outperforms existing methods by +24.19 average points on mathematical benchmarks
- Shows strongest gains on difficult tasks (AIME24/25)
- Demonstrates effective out-of-domain transfer to general reasoning tasks
- Ablation studies confirm importance of synthesizer training, online filtering, and diversity penalties

## Why This Works (Mechanism)

### Mechanism 1: Variance-Driven Capability Frontier Targeting
The synthesizer generates questions where self-consistency score s(x')≈0.5, maximizing reward variance and gradient signal. The capability-adaptive reward R_cap = (4s(1-s))^γ peaks at this frontier, providing optimal learning signal rather than vanishing updates from near-deterministic outcomes.

### Mechanism 2: Co-Evolutionary Feedback Stabilization
Bidirectional feedback between solver and synthesizer creates stable self-improvement. Solver performance guides synthesizer to generate questions at the learnable frontier, while synthesized variants provide higher-quality pseudo-labels than raw test questions, preventing reward collapse.

### Mechanism 3: Curriculum Bridging Over Distribution Gap
Test questions often exceed solver capability, yielding unreliable consensus. Synthetic variants at controlled difficulty provide stable supervision, transforming intractable problems into learnable curricula while preserving essential reasoning structure.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**
  - Why needed: Core optimization algorithm for both agents; requires understanding advantage computation and KL regularization
  - Quick check: Given rewards [0,0,1,1], compute advantages using Equation 2

- **Self-Consistency Pseudo-Labeling**
  - Why needed: Generates unsupervised training signal from majority voting; critical to understand failure modes
  - Quick check: With answers [A,A,B,B,B,C,C,C,C,C], what's the pseudo-label and consistency score?

- **Curriculum Learning Principles**
  - Why needed: TTCS implements dynamic curriculum construction; understanding intermediate-difficulty optimization is essential
  - Quick check: Why does training exclusively on always-solved or always-failed problems produce weak gradients?

## Architecture Onboarding

- **Component map:** Base Model → fork → [Synthesizer π_ϕ] + [Solver π_θ] → Question Rollout → Response Sampling → Capability Eval ← Consensus & Reward → Synthesizer GRPO → Solver GRPO + Filtering → Loop T iterations → Final Solver

- **Critical path:**
  1. Synthesizer rollout (M=4 questions per test question)
  2. Solver evaluation (K=10 responses per synthetic question for quality scoring)
  3. Reward computation (R_cap − R_sim with validity check)
  4. Solver training with online filtering (|s(x)−0.5| ≤ δ=0.25)
  5. Iterative co-update via GRPO (synthesizer: 5 steps; solver: 15 steps)

- **Design tradeoffs:**
  - Rollout group size (M, G): Larger groups improve reward estimation but increase compute; paper uses M=4, G=8 (G=16 for hard benchmarks)
  - Filtering threshold (δ): Stricter filtering ensures cleaner data but may discard useful samples; δ=0.25 balances quality and coverage
  - Temperature (γ): Higher γ sharpens frontier focus but risks missing diverse learning opportunities; γ=1.2 used

- **Failure signatures:**
  - Model collapse: Synthesizer generates near-identical questions → check R_group clustering statistics
  - Reward hacking: Synthesizer exploits format without preserving reasoning structure → monitor cross-domain transfer for degradation
  - Training instability: Solver accuracy oscillates wildly → verify filtering is active, check KL coefficient β=0.01

- **First 3 experiments:**
  1. Sanity check: Run TTCS on MATH-500 subset (50 questions) with Qwen2.5-Math-1.5B; verify synthetic questions maintain s(x')∈[0.3,0.7] distribution and solver improves ≥5 points over 15 iterations
  2. Ablation replication: Disable online data filtering (set δ=1.0); expect performance drop similar to Table 3's −2.5 AMC points
  3. Generalization probe: Train on AIME24 only, evaluate on AMC23 and MATH-500; verify OOD transfer pattern matches Figure 3b

## Open Questions the Paper Calls Out
- Can the TTCS framework be effectively adapted for dynamic agentic applications? The authors plan to extend this framework to more useful and practical agentic applications.
- How does the method perform in domains where rewards are not easily verifiable via self-consistency? The method relies heavily on self-consistency rewards which presuppose a single correct answer, failing in creative writing or open-ended reasoning.
- Can the co-evolutionary loop be optimized to reduce the high computational overhead of iterative sampling? The framework requires multiple sampling stages per iteration, and efficiency compared to simpler inference-time scaling methods is unexplored.

## Limitations
- Critical GRPO hyperparameters (clipping parameter ε) are unspecified
- Full synthesizer prompt templates are truncated in Appendix D
- Specific similarity thresholds for reference-based penalties are not numerically defined
- Computational overhead from multiple sampling stages per iteration

## Confidence
- **High confidence**: Core co-evolutionary mechanism and substantial performance gains (+24.19 points) are well-supported
- **Medium confidence**: Curriculum bridging claims rely on assumptions about synthetic question quality preservation
- **Low confidence**: Exact implementation details for GRPO parameters, prompt templates, and similarity thresholds are missing

## Next Checks
1. **Implementation fidelity check**: Replicate synthesizer prompt template construction and GRPO parameter settings exactly; test whether synthetic questions maintain targeted self-consistency distribution (s(x') ≈ 0.5)
2. **Cross-model generalization test**: Apply TTCS to a different base model family (e.g., LLaMA or Mistral) on the same benchmarks to validate general applicability
3. **Curriculum diversity analysis**: Quantify diversity of synthetic questions over training iterations; measure cluster sizes in R_group and semantic drift from test questions