---
ver: rpa2
title: Exploring Generative Error Correction for Dysarthric Speech Recognition
arxiv_id: '2505.20163'
source_url: https://arxiv.org/abs/2505.20163
tags:
- speech
- dysarthric
- hypotheses
- recognition
- while
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of accurately transcribing dysarthric
  speech, which remains difficult for modern ASR systems due to irregular articulation,
  atypical prosody, and inconsistent speaking rates. The proposed two-stage framework
  combines a Whisper ASR model with a generative error correction (GER) stage using
  FlanT5.
---

# Exploring Generative Error Correction for Dysarthric Speech Recognition

## Quick Facts
- **arXiv ID**: 2505.20163
- **Source URL**: https://arxiv.org/abs/2505.20163
- **Reference count**: 0
- **Primary result**: Two-stage framework (ASR + GER) achieves 6.40% WER on development data and 12.89% WER on TEST-2 set for dysarthric speech transcription

## Executive Summary
This work addresses the challenge of accurately transcribing dysarthric speech by proposing a two-stage framework that combines a Whisper ASR model with a generative error correction (GER) stage using FlanT5. The approach first generates multiple transcription hypotheses from ASR, then uses a language model to select and refine the best transcription based on linguistic patterns. Experiments on the Speech Accessibility Project dataset demonstrate significant improvements over ASR alone, particularly for structured speech and digital assistant commands, though the method struggles with single-word recognition where it achieves 63.08% WER.

## Method Summary
The proposed framework consists of two stages: an ASR stage using Whisper (either zero-shot LARGE-V3 or fine-tuned LARGE-V2) to generate 20-best hypotheses, followed by a GER stage using FlanT5 with LoRA fine-tuning to select and refine the most accurate transcription from the N-best list. The diversity-based hypothesis selection algorithm retains the top-scoring hypothesis and iteratively selects additional hypotheses that maximize minimum edit distance to previously selected ones, capturing genuinely different interpretations. The system is trained on a combination of SAP, TORGO, and VoxPopuli datasets with data augmentation including noise injection, time stretching, and SpecAugment.

## Key Results
- Development set: 6.40% WER and 92.47 SemScore with fine-tuned ASR + GER
- TEST-2 set: 12.89% WER (significant improvement over ASR alone)
- Single-word recognition: 63.08% WER, indicating fundamental architectural limitations
- Digital Assistant Commands: GER improves WER from 16.65% to 14.00%
- Sentences from novels: GER improves WER from 5.97% to 4.88%

## Why This Works (Mechanism)

### Mechanism 1: Complementary Acoustic-Linguistic Processing
The framework separates acoustic pattern recognition (ASR) from linguistic error correction (LLM), allowing specialized components to handle distinct aspects of dysarthric speech. The ASR captures acoustic information including alternative interpretations through N-best hypotheses, while the LLM applies linguistic knowledge to identify coherent transcriptions by analyzing patterns across multiple hypotheses, correcting grammatical errors and resolving ambiguities. This works because correct transcriptions often appear in lower-ranked hypotheses when top-ranked ones are incorrect for dysarthric speech.

### Mechanism 2: Diversity-Based Hypothesis Selection
Instead of selecting top-N candidates, the framework uses a diversity algorithm that selects hypotheses representing genuinely different interpretations rather than minor variations. This approach identifies the top-scoring hypothesis and then iteratively selects additional hypotheses that maximize minimum edit distance to previously selected ones. This is effective because correct transcriptions may appear at different ranks and with different acoustic interpretations that aren't simply minor variations of incorrect top hypotheses.

### Mechanism 3: Domain Adaptation via Targeted Fine-tuning
Both ASR and GER models undergo targeted fine-tuning on dysarthric speech data. The ASR fine-tuning adapts acoustic representations using augmented data (noise injection, time stretching, SpecAugment), while the GER model learns to map error patterns to corrections through LoRA adaptation. This targeted approach improves performance because zero-shot models lack exposure to the specific acoustic and linguistic patterns of dysarthric speech.

## Foundational Learning

- **N-best Hypotheses in ASR**: Understanding beam search decoding and hypothesis ranking is essential for diagnosing why correct answers appear in lower-ranked positions. Quick check: Given an ASR model generating N-best hypotheses, what does high confidence on an incorrect transcription indicate about the acoustic-linguistic tradeoff?

- **Sequence-to-Sequence Error Correction**: The GER stage uses FlanT5 to map N-best lists to corrected transcriptions. Understanding encoder-decoder attention patterns enables better prompt design. Quick check: How would a seq2seq model handle conflicting information across multiple input hypotheses?

- **LoRA (Low-Rank Adaptation)**: Both ASR and GER use LoRA for efficient fine-tuning. Understanding rank, alpha, and target layer selection balances adaptation capacity with computational efficiency. Quick check: If LoRA introduces ~1% additional parameters with rank=16, alpha=32, what happens to model plasticity if you increase rank to 64?

## Architecture Onboarding

- **Component map**: Audio input → Whisper ASR (20-best hypotheses) → Diversity selection (5 hypotheses) → FlanT5 GER (corrected transcription) → Final output

- **Critical path**: 1) Audio input → ASR inference → 20-best generation 2) Hypothesis selection (diversity algorithm) → 5 candidates 3) Prompt construction → GER inference → corrected transcription 4) For fine-tuned systems: ASR adaptation must complete before GER training

- **Design tradeoffs**: Whisper V3 has better zero-shot but exhibits instability during fine-tuning (repetitive artifacts); V2 is more stable for adaptation. GER model size (3B vs. 11B) shows 11B provides better development performance but diminishing returns on test sets. N-best size shows most diversity captured in top 5; generating 20 provides marginal improvement.

- **Failure signatures**: Single-word recognition (63.08% WER) where model bias toward complete utterances causes single words to be misinterpreted as phrases ("football" → "What law?"). Repetitive pattern artifacts when fine-tuning Whisper LARGE-V3 indicate overfitting or inappropriate learning rate. Development-test gap (~6 percentage points) indicates overfitting or insufficient generalization.

- **First 3 experiments**: 1) Baseline ASR evaluation: Run Whisper LARGE-V3 zero-shot on SAP development set to establish WER baseline. 2) N-best analysis: Extract 20 hypotheses for sample utterances; manually inspect whether correct transcription appears and at what rank. 3) GER ablation: Train FlanT5-XL with N={1, 5, 20} hypotheses to quantify contribution of diverse input vs. single-best ASR output.

## Open Questions the Paper Calls Out

- **Single-word recognition**: How can specialized architectures or training methods be developed to improve isolated word recognition for dysarthric speech, given current models' strong bias toward generating complete utterances? This remains unresolved with 63.08% WER achieved.

- **Robust adaptation**: What robust adaptation techniques are required to better handle the variability of dysarthric speech patterns and minimize the generalization gap between development and test sets? Despite fine-tuning, a consistent performance gap remains between development and test sets.

- **Whisper V3 instability**: What are the underlying causes of instability when fine-tuning Whisper Large-V3 on dysarthric speech, and what strategies can mitigate the resulting repetitive pattern artifacts? The authors were forced to revert to Whisper Large-V2, leaving the instability unexplained.

## Limitations

- Single-word recognition failure at 63.08% WER indicates fundamental architectural limitations for isolated word processing
- Development-test performance gap (~6 percentage points) suggests overfitting to specific speakers or patterns
- Diversity selection algorithm effectiveness uncertain - doesn't quantify how often correct transcription appears in selected hypotheses
- Whisper LARGE-V3 fine-tuning instability prevents using superior zero-shot capabilities

## Confidence

- **High Confidence**: Two-stage framework design (ASR + GER) is well-supported by experimental evidence with development WER of 6.40% versus 7.17% for ASR alone
- **Medium Confidence**: Whisper LARGE-V2 vs. LARGE-V3 fine-tuning results show stable adaptation with V2 but instability with V3, though intermediate approaches weren't explored
- **Low Confidence**: Single-word recognition failure (63.08% WER) suggests fundamental architectural mismatch, with attribution to bias toward complete utterances but no exploration of modifications

## Next Checks

1. **Single-Word Architecture Evaluation**: Conduct controlled experiments isolating single-word recognition performance by modifying the ASR architecture to prioritize isolated word decoding over sentence completion. Measure whether specialized acoustic modeling for single words improves performance beyond the current 63.08% WER.

2. **Hypothesis Presence Analysis**: For utterances where GER fails, analyze the 20-best ASR hypotheses to determine: (a) whether the correct transcription appears at all, and (b) if present, at what rank. This quantifies the assumption that correct answers exist within N-best lists and identifies failure modes where the approach cannot succeed.

3. **Cross-Dataset Generalization Test**: Evaluate the fine-tuned system on an independent dysarthric speech dataset (e.g., TORGO or UASpeech) without further adaptation. This measures true generalization capability beyond the SAP development-test split and validates whether performance improvements transfer to unseen dysarthric speech patterns.