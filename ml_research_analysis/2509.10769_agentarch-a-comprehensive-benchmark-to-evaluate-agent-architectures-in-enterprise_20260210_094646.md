---
ver: rpa2
title: 'AgentArch: A Comprehensive Benchmark to Evaluate Agent Architectures in Enterprise'
arxiv_id: '2509.10769'
source_url: https://arxiv.org/abs/2509.10769
tags:
- agent
- complete
- tools
- tool
- react
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AgentArch benchmark evaluates 18 agentic architectures across 6
  large language models on enterprise workflows, systematically testing orchestration,
  prompting style, memory management, and thinking tool integration. Performance peaks
  at only 35.3% success on complex tasks and 70.8% on simpler tasks, revealing fundamental
  gaps in agentic reliability.
---

# AgentArch: A Comprehensive Benchmark to Evaluate Agent Architectures in Enterprise

## Quick Facts
- arXiv ID: 2509.10769
- Source URL: https://arxiv.org/abs/2509.10769
- Reference count: 15
- Primary result: AgentArch benchmark evaluates 18 agentic architectures across 6 large language models on enterprise workflows

## Executive Summary
AgentArch presents a comprehensive benchmark for evaluating agent architectures in enterprise settings, testing 18 different configurations across 6 large language models. The benchmark systematically examines orchestration methods, prompting styles, memory management, and thinking tool integration on both complex and simple enterprise workflows. Results reveal that even the best-performing configurations achieve only 35.3% success on complex tasks and 70.8% on simpler tasks, highlighting fundamental limitations in current agentic systems. The study provides empirically-grounded guidance for enterprise agent design, identifying strong model-specific architectural preferences and critical performance tradeoffs between different approaches.

## Method Summary
The benchmark evaluates 18 agentic architectures across 6 large language models on enterprise workflows, systematically testing orchestration, prompting style, memory management, and thinking tool integration. The evaluation framework measures performance across both complex and simple tasks, tracking success rates, hallucination occurrences, and latency impacts. The study employs a fixed evaluation protocol with predefined enterprise workflow scenarios, comparing function calling versus ReAct approaches, multi-agent versus single-agent configurations, and the impact of thinking tools on non-reasoning models.

## Key Results
- Function calling generally outperforms ReAct orchestration across most models, with LLaMA 3.3 70B being the exception
- Multi-agent systems achieve higher accuracy for final decisions despite lower overall scores
- Hallucinations occur exclusively under ReAct settings, particularly in multi-agent configurations
- Thinking tools improve non-reasoning models on simpler tasks but add significant latency

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its systematic approach to isolating architectural components and their interactions. By controlling for model variations and testing across both complex and simple workflows, the framework reveals how different architectural choices impact performance in specific contexts. The combination of quantitative metrics (success rates) with qualitative observations (hallucination patterns, latency) provides a comprehensive view of architectural strengths and weaknesses. The focus on enterprise workflows ensures relevance to real-world applications while the controlled testing environment enables reproducible comparisons between configurations.

## Foundational Learning

**Orchestration Methods (Why needed: Determines how agents execute tasks)**
- Function calling: Direct method invocation with explicit parameter specification
- ReAct: Reason and Act paradigm combining reasoning with tool use
- Quick check: Compare success rates between methods on identical tasks

**Memory Management (Why needed: Critical for maintaining context across interactions)**
- Short-term memory: Session-based context retention
- Long-term memory: Persistent knowledge storage and retrieval
- Quick check: Measure context retention across multi-step workflows

**Prompting Strategies (Why needed: Influences agent reasoning quality and output)**
- Zero-shot prompting: No examples provided
- Few-shot prompting: Example demonstrations included
- Quick check: Compare output quality and task completion rates

**Thinking Tools Integration (Why needed: Enables complex reasoning beyond model capabilities)**
- Chain-of-thought reasoning: Step-by-step logical progression
- Tool calling: External API or function execution
- Quick check: Measure impact on success rates for complex reasoning tasks

## Architecture Onboarding

**Component Map:**
Model Selection -> Orchestration Method -> Prompting Style -> Memory Management -> Tool Integration -> Performance Evaluation

**Critical Path:**
Orchestration Method -> Prompting Style -> Performance Evaluation (most direct impact on outcomes)

**Design Tradeoffs:**
- Function calling vs ReAct: Reliability vs flexibility
- Single vs multi-agent: Control vs distributed reasoning
- Thinking tools: Accuracy vs latency overhead

**Failure Signatures:**
- ReAct settings: Hallucinations in multi-agent configurations
- Multi-agent ReAct: Lower overall scores despite higher final decision accuracy
- Function calling: Model-specific performance variations

**3 First Experiments:**
1. Compare function calling vs ReAct on identical simple workflow tasks
2. Test single-agent vs multi-agent performance on complex decision-making
3. Measure latency impact of thinking tools on non-reasoning models

## Open Questions the Paper Calls Out
None

## Limitations
- Performance results depend heavily on the specific set of enterprise workflows tested
- Fixed evaluation framework may not capture emergent behaviors in dynamic environments
- Study focuses on a limited set of 6 LLMs and 18 architectures, potentially missing important interactions

## Confidence

**High confidence:**
- Relative performance rankings between orchestration methods (function calling vs ReAct)
- Consistent finding that function calling generally outperforms ReAct across most models

**Medium confidence:**
- Architectural recommendations for specific task complexities
- Hallucination findings (detection relied on predefined criteria)

## Next Checks
1. Test the benchmark on a broader set of enterprise workflows across different industries to assess generalizability
2. Evaluate additional LLM families to determine if observed patterns hold across the broader model landscape
3. Conduct longitudinal studies tracking agent performance over extended interaction periods to validate hallucination findings and assess long-term reliability