---
ver: rpa2
title: Investigating Data Pruning for Pretraining Biological Foundation Models at
  Scale
arxiv_id: '2512.12932'
source_url: https://arxiv.org/abs/2512.12932
tags:
- training
- data
- prediction
- influence
- pruning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates data pruning for pretraining biological
  foundation models to reduce computational costs while maintaining performance. The
  authors propose a post-hoc influence-guided data pruning framework that estimates
  sample importance without full training access.
---

# Investigating Data Pruning for Pretraining Biological Foundation Models at Scale

## Quick Facts
- arXiv ID: 2512.12932
- Source URL: https://arxiv.org/abs/2512.12932
- Reference count: 40
- This paper investigates data pruning for pretraining biological foundation models to reduce computational costs while maintaining performance.

## Executive Summary
This paper investigates data pruning for pretraining biological foundation models to reduce computational costs while maintaining performance. The authors propose a post-hoc influence-guided data pruning framework that estimates sample importance without full training access. They introduce a subset-based self-influence formulation and two selection strategies: Top-k Influence and Coverage-Centric Influence. The framework is evaluated on RNA-FM and ESM-C models using only 0.1% of training data (0.2M sequences vs. 23M or 2.78B). Results show consistent improvement over random selection baselines across multiple downstream tasks. For RNA structure prediction, the pruned model even outperforms the full-data model on contact map prediction. Models trained on selected coresets outperform random subsets 10x larger, demonstrating significant redundancy in biological sequence datasets. The work establishes a practical pathway for efficient, accessible biological AI research through data pruning.

## Method Summary
The paper proposes a post-hoc influence-guided data pruning framework for biological foundation model pretraining. The method first fine-tunes a pretrained model on a small random subset (0.2M sequences) to estimate sample influences. They introduce a subset-based self-influence formulation that approximates the influence of each training sample on the model's performance on the subset itself. Two selection strategies are proposed: Top-k Influence selects samples with highest influence scores, while Coverage-Centric Influence (CCI) combines influence with diversity by pruning hardest samples and stratifying the remainder. The pruned coreset is then used to pretrain a model from scratch. The framework is evaluated on RNA structure prediction and protein sequence tasks, comparing pruned models against random subsets and full-data baselines.

## Key Results
- Pruned models using 0.1% of training data (0.2M sequences) match or exceed full-data models on multiple downstream tasks
- RNA structure prediction model trained on pruned coreset outperforms full-data model on contact map prediction
- Coresets selected by influence-based methods outperform random subsets 10x larger, demonstrating significant dataset redundancy
- The proposed CCI selection strategy provides consistent improvements across different biological modalities and task types

## Why This Works (Mechanism)
The method works by leveraging the observation that biological sequence datasets contain significant redundancy, and that samples vary substantially in their contribution to model performance. The post-hoc influence estimation identifies the most informative samples without requiring full training access. The subset-based formulation makes this computationally feasible by approximating influence on a smaller random subset rather than the full dataset. The CCI strategy further improves upon pure influence selection by ensuring coverage of diverse sequence patterns, preventing the model from overfitting to similar sequences. This combination of importance and diversity in sample selection enables effective pretraining with dramatically reduced data.

## Foundational Learning
- **Influence Functions**: Estimate how much each training sample affects model predictions on a specific task. Why needed: To identify which samples are most valuable for pretraining. Quick check: Verify influence scores correlate with sample difficulty and downstream performance.
- **Diagonal Fisher Approximation**: Simplifies influence computation by approximating the Hessian with its diagonal. Why needed: Makes influence computation tractable at scale. Quick check: Compare with full Fisher for small subsets to verify approximation quality.
- **Stratified Sampling**: Divides data into strata and samples proportionally to ensure diversity. Why needed: Prevents selection of only high-influence but similar samples. Quick check: Verify stratum distributions match overall data distribution.
- **Self-Influence Formulation**: Computes influence of samples on model performance on the same subset. Why needed: Enables influence estimation without full training access. Quick check: Confirm that subset performance improves with selected samples.

## Architecture Onboarding
**Component Map**: Pretrained Model -> Subset Fine-tuning -> Influence Computation -> Coreset Selection -> Full Training
**Critical Path**: The subset fine-tuning step is critical as it must satisfy the assumption that the model is well-trained on the subset for accurate influence estimation. Failure here invalidates all downstream results.
**Design Tradeoffs**: Diagonal Fisher approximation vs. computational cost; pure influence vs. diversity in CCI; subset size vs. influence accuracy. The choices balance scalability with effectiveness.
**Failure Signatures**: Poor downstream performance indicates either inadequate subset fine-tuning, poor influence estimation, or selection strategy failure. Memory errors during influence computation suggest batch size issues.
**First Experiments**: 1) Verify subset fine-tuning converges on 0.2M sequences; 2) Compute influence scores on small subset and check distribution; 3) Test CCI selection with k=50 strata on validation set.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: What are the optimal coreset sizes for different biological modalities and tasks? The paper uses 0.1% (0.2M) but doesn't explore the full tradeoff curve between coreset size and performance.
- Question: How does the method scale to truly massive datasets (e.g., 100M+ sequences)? The current approach's computational requirements for influence estimation at full scale remain unclear.

## Limitations
- The computational requirements for influence computation at full dataset scale are not fully characterized, particularly regarding memory management and batch processing strategies.
- The assumption that post-hoc fine-tuning reliably satisfies the mathematical assumptions for accurate influence estimation is reasonable but not empirically validated across diverse dataset characteristics.
- The study focuses on RNA structure prediction and protein sequence tasks without exploring other biological domains, limiting generalizability claims.

## Confidence
**High confidence**: The core finding that data pruning can achieve performance comparable to full-data pretraining with orders-of-magnitude fewer sequences. The ablation showing random subsets 10x larger fail to match pruned coreset performance provides strong evidence of dataset redundancy.

**Medium confidence**: The practical applicability of the method given unspecified computational requirements for influence computation. The lack of details about batch processing and memory management creates uncertainty about real-world implementation feasibility.

**Medium confidence**: The generalizability across different biological modalities, as the study focuses specifically on RNA structure prediction and protein sequence tasks without exploring other biological domains.

## Next Checks
1. **Reproduce the influence computation pipeline** using a smaller dataset subset (e.g., 100K sequences) to verify the diagonal Fisher approximation approach and identify memory bottlenecks before scaling up.

2. **Implement the CCI selection algorithm** with the specified parameters (α=90, β=5, k=50) on a held-out validation set to confirm the stratified sampling produces the expected sequence diversity and influence score distribution.

3. **Benchmark downstream task performance** on the RNA-FM contact map prediction task, comparing the pruned model against both the random baseline and full-data model to verify the claim of superior performance on this specific task.