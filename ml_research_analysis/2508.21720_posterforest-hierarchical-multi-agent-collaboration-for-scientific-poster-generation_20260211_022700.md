---
ver: rpa2
title: 'PosterForest: Hierarchical Multi-Agent Collaboration for Scientific Poster
  Generation'
arxiv_id: '2508.21720'
source_url: https://arxiv.org/abs/2508.21720
tags:
- poster
- content
- layout
- arxiv
- tree
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a training-free framework for automated scientific
  poster generation that addresses key limitations in existing methods. The core innovation
  is a hierarchical intermediate representation called the "Poster Tree," which explicitly
  models the document's structure and visual-textual relationships at multiple levels
  (sections, subsections, paragraphs, and visual elements).
---

# PosterForest: Hierarchical Multi-Agent Collaboration for Scientific Poster Generation

## Quick Facts
- arXiv ID: 2508.21720
- Source URL: https://arxiv.org/abs/2508.21720
- Authors: Jiho Choi; Seojeong Park; Seongjong Song; Hyunjung Shim
- Reference count: 7
- One-line primary result: A training-free framework for automated scientific poster generation using hierarchical multi-agent collaboration that outperforms existing methods on the Paper2Poster benchmark.

## Executive Summary
This paper introduces PosterForest, a training-free framework for automated scientific poster generation that addresses key limitations in existing methods. The core innovation is a hierarchical intermediate representation called the "Poster Tree," which explicitly models document structure and visual-textual relationships at multiple levels. The framework employs a multi-agent collaboration strategy where specialized agents iteratively coordinate and provide feedback to jointly optimize logical consistency, content fidelity, and visual coherence. Extensive experiments on multiple academic domains demonstrate that PosterForest outperforms existing baselines in both qualitative and quantitative evaluations.

## Method Summary
PosterForest takes a scientific paper PDF as input and generates a poster through a hierarchical process. First, the paper is parsed into a Raw Document Tree, which is then refined into a Content Tree by a summarization agent. Simultaneously, a Layout Agent creates an initial spatial structure (Layout Tree). These are merged into a Poster Tree that binds content to spatial constraints. The system then enters an iterative refinement loop where specialized Content and Layout agents traverse the tree, exchanging feedback to optimize text density and spatial arrangement. The process continues until evaluation constraints are satisfied or a maximum iteration limit is reached, after which the final Poster Tree is rendered into the output poster.

## Key Results
- PosterForest outperforms existing methods on the Paper2Poster benchmark with higher MLLM-as-Judge scores across all evaluation criteria
- The generated posters achieve quality closest to expert-designed ground truth posters
- The framework delivers superior information preservation, structural clarity, and user preference compared to baseline approaches

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Intermediate Representation (Poster Tree)
Structuring the input document as a tree explicitly preserves semantic relationships and section hierarchy, reducing logical errors common in flat-sequence processing. The system parses the raw document into a tree structure ($T_{raw}$), refines it into a Content Tree ($T_{content}$), and initializes a Layout Tree ($T_{layout}$). Merging these creates the **Poster Tree** ($T_{poster}$), which binds specific content to spatial constraints before rendering. Assumes the input document has extractable structural markers that can be mapped to a tree topology via the parser agent.

### Mechanism 2: Decoupled Agent Specialization with Feedback
Separating content optimization from layout optimization, while forcing them to negotiate, resolves the "content-layout" trade-off better than sequential pipelines. A **Content Agent** ($A_{Content}$) analyzes text volume and fidelity, while a **Layout Agent** ($A_{Layout}$) analyzes spatial balance. They exchange opinions ($O_c, O_l$) over $K$ rounds. Assumes the LLM can accurately simulate "feedback" and understand spatial constraints during text-based negotiation without explicit visual grounding.

### Mechanism 3: Iterative Tree Refinement (Training-Free Optimization)
Iteratively traversing and updating the Poster Tree allows the system to approximate global optimization without requiring gradient-based training or regression models. The system performs a breadth-first traversal of the tree. At each node, agent decisions are committed. After a full traversal, an evaluation function checks for constraints like text overflow. Assumes that local node-level updates, when aggregated via tree traversal, converge toward a globally coherent poster layout.

## Foundational Learning

- **Tree Data Structures (specifically Hierarchical/Parent-Child relationships)**
  - Why needed here: The core innovation is the "Poster Tree." You cannot debug the system without understanding how nodes (sections) contain children (subsections/paragraphs) and how attributes (layout vs. content) are stored at different depths.
  - Quick check question: If a "Conclusion" section is removed from the Content Tree, does its corresponding layout node in the Layout Tree automatically get garbage collected, or does it leave an empty panel?

- **Multi-Agent Debate/Negotiation**
  - Why needed here: The engine relies on agents "exchanging proposals." Understanding prompt engineering for role-play (e.g., "You are a Layout Critic") is required to debug why an agent might be ignoring constraints.
  - Quick check question: In the collaboration stage, if Agent A proposes "reduce text by 20%" and Agent B says "reject," what is the default resolution strategy hardcoded in the system?

- **Breadth-First Search (BFS) Traversal**
  - Why needed here: The modification plan uses BFS to refine the tree. Understanding this helps predict performance bottlenecks (queue size) and ensures high-level layout changes happen before deep paragraph tweaks.
  - Quick check question: Why would BFS be preferred over DFS (Depth-First Search) in this context? (Hint: Does it allow stabilizing the outer layout structure before refining inner text content?)

## Architecture Onboarding

- **Component map:** Parser Agent -> Summarizer Agent -> Layout Agent (Init) -> Merger -> Refinement Loop -> Evaluator -> Renderer
- **Critical path:** The **Merge** operation (Eq. 4) and the **Node-level Collaboration** (Eq. 5-7). If the mapping between the Content Tree nodes and Layout Tree nodes is misaligned during the merge, the refinement loop will fail to find a valid solution.
- **Design tradeoffs:** Training-free vs. Regressor: The system is more flexible (no dataset training required) but slower due to iterative LLM calls per node. Modularity vs. Consistency: By separating Content and Layout agents, the system gains specialized reasoning but risks "coordination overhead" where agents cycle disagreements without progress.
- **Failure signatures:** Empty Panels: The Parser likely missed the section text, but the Layout Agent reserved space for it. Over-constrained Layout: Agents reduced text too much trying to fit a rigid layout tree, resulting in sparse posters. Dense Figure Clusters: The paper notes failure in parsing dense figure clusters, leading to missing visuals in the final output.
- **First 3 experiments:**
  1. **Sanity Check (Ablation):** Run the pipeline with *only* the Content Agent or *only* the Layout Agent active to reproduce the failure modes shown in Figure 5 (text overflow vs. layout imbalance).
  2. **Hyperparameter Sensitivity:** Vary the collaboration rounds ($K$) and max iterations ($T_{max}$) to determine if quality saturates quickly (confirming the paper's claim that $K=1$ or $T_{max}=2$ is sufficient).
  3. **Stress Test:** Input a document with intentionally ambiguous section headers to test the Parser's ability to construct a valid Poster Tree without manual intervention.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the asset parsing and reasoning mechanisms be enhanced to robustly handle documents with densely clustered or heavily interleaved visual elements without error propagation?
  - Basis in paper: The authors identify failure cases where the parser struggles with papers containing many figures clustered in short text spans, leading to missing or misaligned assets in the final poster.
  - Why unresolved: The current framework relies on external parsing tools which struggle with complex visual-spatial layouts, causing errors that propagate through the generation pipeline.
  - What evidence would resolve it: Demonstration of a parsing module that maintains high accuracy in asset extraction and matching specifically on datasets of papers with high visual density.

- **Open Question 2:** How can the relative importance and semantic richness of individual figures be modeled to optimize their scale and placement relative to the poster's context?
  - Basis in paper: The authors state that the current framework does not yet fully capture the perceived importance or semantic richness of figures, which human designers use to determine scale and position.
  - Why unresolved: Existing agents focus on spatial balance and fit rather than the nuanced semantic value of specific visual elements.
  - What evidence would resolve it: A mechanism that dynamically weights figures based on semantic contribution, validated through user studies confirming improved information hierarchy.

- **Open Question 3:** How can robust automated metrics be developed to comprehensively assess poster quality, overcoming the limitations of current MLLM-as-Judge evaluations?
  - Basis in paper: The paper highlights the "lack of robust quality metrics" as a limitation and calls for the development of advanced evaluation methodologies in future research.
  - Why unresolved: Current automated metrics fail to fully capture subjective human preferences or subtle design qualities, creating a gap between quantitative scores and actual utility.
  - What evidence would resolve it: A new evaluation protocol that shows a statistically significantly higher correlation with human expert rankings than existing automated metrics.

## Limitations
- Dependence on accurate hierarchical parsing from source PDFs, which may not hold for papers with non-standard formatting or dense visual layouts
- Computational overhead through iterative LLM calls that may not scale efficiently for longer documents
- Limited generalization to papers outside the Paper2Poster benchmark dataset

## Confidence
- **High Confidence**: The core mechanism of hierarchical tree representation and its advantage over flat-sequence processing is well-supported by the paper's empirical results and comparison with existing methods.
- **Medium Confidence**: The multi-agent collaboration framework shows promise, but the paper's ablation studies could benefit from more granular analysis of agent interactions and potential failure modes.
- **Medium Confidence**: The training-free optimization through iterative refinement is validated on the Paper2Poster benchmark, but generalization to papers outside this dataset remains untested.

## Next Checks
1. **Cross-Dataset Generalization**: Test PosterForest on papers from venues outside the Paper2Poster benchmark to assess robustness to diverse formatting styles and content structures.
2. **Computational Efficiency Analysis**: Measure the actual runtime and resource consumption of the iterative refinement process, particularly for longer papers, and compare against regressor-based approaches.
3. **Human Preference Validation**: Conduct a user study with domain experts to evaluate not just the automated metrics but also the aesthetic and communicative effectiveness of generated posters in real-world scenarios.