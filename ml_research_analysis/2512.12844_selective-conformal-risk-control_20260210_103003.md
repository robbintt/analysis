---
ver: rpa2
title: Selective Conformal Risk Control
arxiv_id: '2512.12844'
source_url: https://arxiv.org/abs/2512.12844
tags:
- prediction
- risk
- control
- conformal
- selective
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Selective Conformal Risk Control (SCRC),
  a framework that integrates conformal prediction with selective classification to
  address the practical inefficiency of large prediction sets in high-stakes applications.
  The method uses a two-stage approach: first selecting confident samples via a selection
  threshold, then constructing calibrated prediction sets for the selected subset
  using conformal risk control.'
---

# Selective Conformal Risk Control

## Quick Facts
- arXiv ID: 2512.12844
- Source URL: https://arxiv.org/abs/2512.12844
- Reference count: 39
- One-line primary result: Introduces SCRC framework integrating conformal prediction with selective classification to produce compact, risk-controlled prediction sets

## Executive Summary
Selective Conformal Risk Control (SCRC) addresses the inefficiency of large prediction sets in high-stakes applications by integrating conformal prediction with selective classification. The framework employs a two-stage approach: first selecting confident samples via a selection threshold, then constructing calibrated prediction sets for the selected subset using conformal risk control. This yields compact, reliable uncertainty quantification while maintaining target coverage and risk levels.

The method offers two algorithmic variants: SCRC-T preserves exchangeability for exact finite-sample guarantees, while SCRC-I provides PAC-style probabilistic guarantees with improved computational efficiency. Experiments on CIFAR-10 and Diabetic Retinopathy Detection datasets demonstrate significant reduction in prediction set sizes compared to standard conformal prediction, with both variants achieving nearly identical performance while maintaining the desired risk-control properties.

## Method Summary
SCRC uses a two-stage selective conformal approach where samples are first filtered by a selection threshold based on confidence scores, then conformal prediction is applied only to the selected subset. The framework introduces SCRC-T, which preserves exchangeability for exact finite-sample guarantees, and SCRC-I, a computationally efficient variant with PAC-style guarantees. The conformal risk control mechanism ensures that the final prediction sets maintain both coverage and risk thresholds simultaneously. The selection threshold determines which samples receive prediction sets, allowing control over the trade-off between coverage and set size efficiency.

## Key Results
- Both SCRC-T and SCRC-I achieve target coverage and risk levels while significantly reducing prediction set sizes compared to standard conformal prediction
- SCRC-I demonstrates computational advantages over SCRC-T while maintaining nearly identical performance metrics
- Experiments on CIFAR-10 and Diabetic Retinopathy Detection show the framework's effectiveness across different domains and risk levels

## Why This Works (Mechanism)
The framework works by decoupling the selection process from the conformal prediction step, allowing independent control over which samples receive prediction sets and how those sets are constructed. By selecting only high-confidence samples, the method reduces the burden on conformal prediction, enabling smaller prediction sets while maintaining coverage guarantees. The two-stage approach allows for explicit risk control through the selection threshold, providing a mechanism to trade off between coverage and efficiency that is not available in standard conformal prediction methods.

## Foundational Learning

**Conformal Prediction**: A framework for constructing prediction sets with guaranteed coverage. Needed to provide statistical guarantees on prediction sets. Quick check: Verify coverage on held-out calibration data.

**Selective Classification**: Techniques for filtering samples based on confidence. Needed to reduce the number of samples requiring prediction sets. Quick check: Evaluate selection accuracy and coverage trade-off.

**Exchangeability**: Statistical property required for finite-sample guarantees in conformal prediction. Needed for SCRC-T's theoretical guarantees. Quick check: Test distributional assumptions on calibration data.

**Risk Control**: Explicit management of prediction error rates. Needed to ensure practical utility in high-stakes applications. Quick check: Monitor empirical risk against target thresholds.

## Architecture Onboarding

**Component Map**: Input data -> Selection threshold -> Filter samples -> Conformal risk control -> Output prediction sets

**Critical Path**: The selection threshold determines which samples proceed to conformal prediction, directly controlling both efficiency and risk. The calibration set quality and size determine the reliability of the final prediction sets.

**Design Tradeoffs**: SCRC-T provides exact finite-sample guarantees but requires exchangeability preservation and more computation. SCRC-I offers computational efficiency with PAC-style guarantees but slightly reduced theoretical rigor.

**Failure Signatures**: Poor selection thresholds lead to either insufficient coverage or unnecessarily large prediction sets. Violation of exchangeability assumptions in SCRC-T can invalidate guarantees. Insufficient calibration data results in unreliable risk estimates.

**First Experiments**: 1) Verify coverage and risk control on held-out test data, 2) Compare prediction set sizes across different selection thresholds, 3) Evaluate computational efficiency difference between SCRC-T and SCRC-I variants

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes risk is known a priori, which may not hold in practice
- Potential brittleness of exchangeability assumption in SCRC-T when selection threshold interacts with data distribution
- Performance on out-of-distribution data and noisy labels is unclear

## Confidence

High confidence: Empirical effectiveness in reducing prediction set sizes while maintaining coverage and risk levels on tested datasets

Medium confidence: Theoretical guarantees of SCRC-T due to exchangeability assumption, practical applicability of SCRC-I due to PAC-style guarantees

Low confidence: Generalizability to other domains, robustness to data shifts and label noise

## Next Checks

1. Evaluate the framework on out-of-distribution data to assess robustness to distribution shifts
2. Conduct experiments with noisy labels to test sensitivity to label uncertainty
3. Compare computational efficiency of SCRC-I and SCRC-T on smaller datasets and simpler models to verify claimed advantages