---
ver: rpa2
title: 'Reward Evolution with Graph-of-Thoughts: A Bi-Level Language Model Framework
  for Reinforcement Learning'
arxiv_id: '2509.16136'
source_url: https://arxiv.org/abs/2509.16136
tags:
- reward
- learning
- llms
- tasks
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RE-GoT, a bi-level framework that enhances
  Large Language Models (LLMs) with graph-based reasoning and Visual Language Models
  (VLMs) for automated reward design in reinforcement learning. The framework decomposes
  tasks into text-attributed graphs to enable structured reasoning and uses VLMs to
  evaluate rollout videos without human intervention.
---

# Reward Evolution with Graph-of-Thoughts: A Bi-Level Language Model Framework for Reinforcement Learning

## Quick Facts
- arXiv ID: 2509.16136
- Source URL: https://arxiv.org/abs/2509.16136
- Reference count: 34
- Introduces RE-GoT, a bi-level framework that improves LLM-based reward design for RL using graph-based reasoning and VLM visual feedback, achieving up to 32.25% improvement in success rates.

## Executive Summary
This paper introduces RE-GoT, a bi-level framework that enhances Large Language Models (LLMs) with graph-based reasoning and Visual Language Models (VLMs) for automated reward design in reinforcement learning. The framework decomposes tasks into text-attributed graphs to enable structured reasoning and uses VLMs to evaluate rollout videos without human intervention. Experiments on 10 RoboGen and 4 ManiSkill2 tasks show RE-GoT improves average success rates by 32.25% on RoboGen and achieves 93.73% on ManiSkill2, surpassing existing LLM-based methods and even expert-designed rewards. Ablation studies confirm the effectiveness of graph-of-thoughts reasoning and in-context learning in improving reward function quality and stability.

## Method Summary
RE-GoT employs a bi-level optimization framework where the upper level uses a VLM (Gemini 1.5 Pro) to analyze rollout videos and provide textual feedback, while the lower level uses an LLM (GPT-4o) to generate and refine reward functions based on this feedback. The LLM constructs a Text-Attributed Graph (GoT) from task descriptions, generating reward code that is then evaluated through RL training. The process iterates up to 8 times, with the VLM analyzing videos and the LLM refining rewards based on the feedback. The framework uses Soft Actor-Critic (SAC) for most tasks and Proximal Policy Optimization (PPO) for specific ManiSkill2 tasks.

## Key Results
- RE-GoT improves average success rates by 32.25% on RoboGen tasks compared to existing LLM-based methods.
- Achieves 93.73% success rate on ManiSkill2 tasks, surpassing both expert-designed rewards and other LLM-based approaches.
- Ablation studies confirm the effectiveness of graph-of-thoughts reasoning and in-context learning in improving reward function quality and stability.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Structured graph-based decomposition improves the coherence and stability of reward functions for long-horizon tasks compared to linear prompting.
- **Mechanism:** By enforcing a Text-Attributed Graph ($G=(V, E, T_v, T_e)$) representation, the LLM must explicitly define states (nodes) and transitions (edges). This structural constraint forces the model to reason about task topology and dependencies before generating code, reducing the likelihood of omitting intermediate sub-goals.
- **Core assumption:** The LLM possesses sufficient world knowledge to accurately map natural language task descriptions to a valid graph topology without generating cyclic errors or disconnected components.
- **Evidence anchors:**
  - [abstract] "decomposes tasks into text-attributed graphs to enable structured reasoning"
  - [section IV.C.2] "LLMs can explore multiple reasoning paths simultaneously... mitigating hallucinations"
  - [corpus] *Adaptive Graph of Thoughts* (neighbor) supports the premise that graph structures unify reasoning better than linear chains for complex problems.
- **Break condition:** If the task is inherently non-decomposable or purely reactive (requiring continuous adjustment rather than discrete stages), the graph structure may introduce artificial rigidity, failing to capture nuance.

### Mechanism 2
- **Claim:** Replacing human feedback with VLM-based visual analysis creates a scalable, closed-loop optimization cycle for reward refinement.
- **Mechanism:** The framework treats the VLM as an unsupervised error signal generator. By analyzing rollout videos $D(\xi(\pi_{R_\theta}))$, the VLM outputs structured text (Video Description, Potential Problems). This text serves as the "gradient" direction for the LLM to update the reward code, minimizing the behavioral loss without requiring differentiable reward models.
- **Core assumption:** The VLM can reliably deduce physical causality and task failure modes from video frames alone, matching or exceeding human supervision quality for the specific tasks.
- **Evidence anchors:**
  - [abstract] "iteratively refines rewards using visual feedback from VLMs without human intervention"
  - [section IV.B] "The VLMs acts as an automated evaluator... providing structured, textual feedback"
  - [corpus] *Uncertainty-aware Reward Design Process* (neighbor) highlights general trends in automating reward shaping, though specific VLM-loop mechanisms vary.
- **Break condition:** If the VLM hallucinates events in the video or misinterprets the physics (e.g., attributing a failure to a collision when it was a grasping error), the reward update will be corrupted.

### Mechanism 3
- **Claim:** In-context learning (few-shotting) stabilizes the Graph-of-Thought generation process.
- **Mechanism:** Providing examples of task-to-graph mappings in the prompt restricts the output space of the LLM, grounding the abstract reasoning process in the specific syntax required by the framework (nodes, edges, attributes).
- **Core assumption:** The examples provided in the context are sufficiently representative of the target task structure to induce positive transfer.
- **Evidence anchors:**
  - [section IV.C.2] "we employ heuristic rules and in-context learning with a few examples to guide the LLM"
  - [section V.D.2] "few-shot enables to greatly improve the reasoning ability... [Fig 6]"
  - [corpus] *V2V-GoT* (neighbor) similarly utilizes multimodal models with structured reasoning, implicitly supporting the utility of robust prompting strategies.
- **Break condition:** If the provided examples conflict with the target task logic, or if the prompt exceeds the context window efficiency, performance may degrade due to distraction.

## Foundational Learning

- **Concept: Reinforcement Learning (RL) Reward Shaping**
  - **Why needed here:** The entire paper focuses on generating the scalar signal $R_\theta$ that guides the robot. Understanding the difference between sparse rewards (success only) and dense rewards (shaping progress) is required to grasp what the LLM is coding.
  - **Quick check question:** How does a dense reward function help an agent learn a multi-step task compared to a binary success signal?

- **Concept: Graph Theory (Nodes & Edges)**
  - **Why needed here:** The "Graph-of-Thoughts" approach relies on representing tasks as a graph $G$. You must understand that Nodes represent states (sub-goals) and Edges represent transitions (behaviors) to interpret the framework's core contribution.
  - **Quick check question:** In a "Store Item" task, does the action "grasp toy" belong to a node or an edge in this framework?

- **Concept: Bi-Level Optimization**
  - **Why needed here:** The RE-GoT system is explicitly bi-level. The "Upper Level" optimizes the reward based on visual feedback, while the "Lower Level" optimizes the robot's policy based on that reward.
  - **Quick check question:** Which level of the optimization loop requires running the RL training algorithm (e.g., SAC/PPO)?

## Architecture Onboarding

- **Component map:**
  1.  **Upper Level (Reflector):** VLM (Gemini 1.5 Pro) + Rollout Video → Textual Feedback.
  2.  **Lower Level (Generator):** LLM (GPT-4o) + Environment Abstraction + GoT → Reward Code ($R_\theta$).
  3.  **Executor:** RL Simulator (RoboGen/ManiSkill2) + Policy Optimizer (SAC/PPO) → Trained Agent & Rollout Video.
  4.  **Interface:** Python APIs (e.g., `get_position`, `get_velocity`) bridge the LLM code and the simulator.

- **Critical path:** The integrity of the **Text-Attributed Graph**. If the initial graph decomposition is flawed (e.g., missing a state node for "open door"), the generated reward code will fail to incentivize that step, causing the RL agent to stall, regardless of how many iterations run.

- **Design tradeoffs:**
  - **Automation vs. Stability:** Fully automating feedback with VLMs removes human bottlenecks but introduces noise if the VLM misinterprets video.
  - **Graph Rigidity vs. Flexibility:** Enforcing a fixed graph structure helps structure but may struggle if the optimal solution path changes dynamically during training.

- **Failure signatures:**
  - **VLM Drift:** The VLM repeatedly suggests changes that conflict with previous advice, causing the reward function to oscillate.
  - **Graph Disconnect:** The LLM generates code for nodes 1 and 3 but forgets the transition logic for node 2, resulting in a local minimum where the robot performs half the task.
  - **Reward Hacking:** The agent maximizes the generated reward (e.g., "move gripper close to object") without achieving the goal ("grasp object"), and the VLM fails to catch the nuance.

- **First 3 experiments:**
  1.  **Static Graph Validation:** Run the system *without* the RL loop. Input a text task, prompt the LLM for the GoT, and manually inspect the JSON graph structure to verify if all substeps are present and connected.
  2.  **VLM Sanity Check:** Feed a video of a known failure (e.g., robot knocking an object over) to the VLM with the analysis template. Verify it correctly identifies "Potential Problems" relevant to the physics.
  3.  **Single-Iteration Baseline:** Run the full pipeline on a simple task (e.g., PickCube) for exactly one iteration to measure the "zero-shot" quality of the GoT-generated reward before the evolution loop begins.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can RE-GoT be effectively deployed on physical robots without degradation in performance or safety due to the sim-to-real gap?
- Basis in paper: [explicit] The authors state, "Future work could explore the integration of more advanced LLMs and VLMs for deployment on real robots."
- Why unresolved: All experiments were conducted in simulation (RoboGen and ManiSkill2). Real-world visual feedback introduces noise, lighting variability, and latency that may disrupt the VLM's evaluation accuracy.
- What evidence would resolve it: Successful trials of RE-GoT on physical robotic arms performing the manipulation tasks, demonstrating comparable success rates and safety profiles to the simulation results.

### Open Question 2
- Question: Does fine-tuning the LLM on a domain-specific Graph-of-Thoughts dataset provide significant improvements over general-purpose prompting?
- Basis in paper: [explicit] The authors propose to "create an extensive GoT dataset of different tasks and fine-tune the LLMs to improve the performance."
- Why unresolved: The current framework relies on prompting off-the-shelf models (GPT-4o). It is unknown if specialized training on reward graphs enhances reasoning capability or reduces hallucinations more effectively than in-context learning.
- What evidence would resolve it: A comparative study between the baseline GPT-4o implementation and a fine-tuned variant, measuring success rates on novel, out-of-distribution tasks.

### Open Question 3
- Question: Can the visual feedback loop be stabilized to prevent performance collapse when VLMs misinterpret agent behavior?
- Basis in paper: [inferred] The authors note a performance drop in the `OpenCabinetDrawer` task, suspecting "the VLM fails to analyze the rollout videos correctly."
- Why unresolved: The framework currently lacks a verification mechanism to filter or correct erroneous feedback from the VLM, which can lead to incorrect reward refinement and policy degradation.
- What evidence would resolve it: The introduction of a confidence scoring mechanism or self-correction layer for VLM outputs, resulting in monotonic improvement curves in previously unstable tasks.

## Limitations
- The VLM feedback quality is critical but not quantitatively validated against human supervision, with performance dips suggesting potential instability in the VLM loop.
- The few-shot examples and exact prompt templates are not provided, making systematic reproduction challenging and introducing significant variability in Graph-of-Thoughts generation quality.
- The framework's reliance on VLMs for feedback introduces potential error propagation if the VLM misinterprets visual data, which isn't fully characterized.

## Confidence
- **High confidence:** The bi-level architecture is clearly defined, and the empirical gains over baselines are statistically meaningful (32.25% average improvement on RoboGen).
- **Medium confidence:** The graph-based reasoning mechanism shows promise, but the specific claim that it reduces hallucinations is inferred rather than directly measured.
- **Medium confidence:** VLM-based feedback as a scalable alternative to human supervision is demonstrated but lacks systematic error analysis or comparison to human-labeled video data.

## Next Checks
1. **VLM Error Analysis:** Manually inspect VLM-generated feedback on a subset of rollout videos to quantify hallucination rates and identify systematic failure patterns.
2. **Graph Quality Validation:** Run the LLM GoT generation step in isolation on all 14 tasks and measure the structural completeness (e.g., percentage of required subtasks captured in the graph) before any RL training.
3. **Stability Benchmarking:** Implement a running average success rate metric across iterations to formally quantify whether the observed performance fluctuations represent meaningful learning or reward function oscillation.