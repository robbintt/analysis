---
ver: rpa2
title: 'Beyond the Black Box: Identifiable Interpretation and Control in Generative
  Models via Causal Minimality'
arxiv_id: '2512.10720'
source_url: https://arxiv.org/abs/2512.10720
tags:
- concepts
- concept
- causal
- latent
- hierarchical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the interpretability challenge in deep generative\
  \ models like diffusion and autoregressive models, which often operate as opaque\
  \ black boxes. The authors propose a principled foundation for interpretable generative\
  \ models by leveraging the causal minimality principle\u2014favoring the simplest\
  \ causal explanation\u2014to endow latent representations with clear causal interpretation\
  \ and robust, component-wise identifiable control."
---

# Beyond the Black Box: Identifiable Interpretation and Control in Generative Models via Causal Minimality

## Quick Facts
- **arXiv ID**: 2512.10720
- **Source URL**: https://arxiv.org/abs/2512.10720
- **Reference count**: 40
- **Primary result**: Causal minimality and sparsity constraints enable interpretable concept extraction and fine-grained steering in Stable Diffusion and Flux models, achieving competitive unlearning performance while maintaining generation quality

## Executive Summary
This paper addresses the interpretability challenge in deep generative models by proposing a principled framework that leverages causal minimality to endow latent representations with clear causal interpretation and robust control. The authors introduce hierarchical selection models where higher-level concepts emerge from constrained compositions of lower-level variables, capturing complex data dependencies more naturally than traditional approaches. Under theoretically derived sparsity and compression constraints, learned representations become equivalent to true latent variables of the data-generating process. Applied to Stable Diffusion and Flux, the method successfully extracts hierarchical concept graphs and enables fine-grained steering for tasks like unlearning, achieving FID of 17.81 and CLIP score of 31.3 on SD 1.4 baseline.

## Method Summary
The method trains K-sparse autoencoders (SAEs) on model features at specific timesteps to extract sparse latent representations, then applies the PC algorithm to discover causal structure between features across hierarchical levels. For vision models, SAEs are trained on U-Net features at timesteps 899, 500, and 100; for language models, on embedding spaces. The sparsity constraint (K) enforces causal minimality, making representations identifiable with true latent variables. Steering is achieved by modifying SAE latents at specific timesteps using learned intervention vectors. For unlearning, concepts associated with target attributes (e.g., nudity) are identified and negatively steered during generation.

## Key Results
- Successfully extracted hierarchical concept graphs from Stable Diffusion and Flux models, revealing interpretable parent-child relationships between abstract and detailed concepts
- Achieved competitive unlearning performance while maintaining generation quality (FID: 17.81, CLIP: 31.3 for SD 1.4 baseline)
- Demonstrated fine-grained steering capability, allowing localized modifications to specific concepts without affecting others
- Showed that diffusion timesteps naturally align with hierarchical levels, with noisy timesteps encoding global concepts and clean timesteps encoding details

## Why This Works (Mechanism)

### Mechanism 1: Causal Minimality Enables Component-wise Identifiability
Imposing sparsity constraints on latent representations makes them provably equivalent to true latent variables of the data-generating process. Causal minimality (favoring simplest causal explanation) manifests as sparse connectivity in concept graphs. Under Condition 4.2-iv (Sparse Connectivity), each parent concept has a unique "fingerprint" via children with no shared parents, enabling disentanglement via subspace intersection operations. If the true concept graph is dense, sparsity constraints would incorrectly prune real connections, yielding incomplete identification.

### Mechanism 2: Selection-Based Hierarchy Captures Part-Whole Dependencies
Selection models (where higher-level concepts are effects of lower-level configurations) better represent natural data than traditional top-down hierarchical models. Higher-level variable Z^l is determined by selection function g mapping lower-level constituents Z^{l+1} to Z^l. Conditioning on Z^l induces dependencies among Z^{l+1} components—e.g., "bicycle" concept emerges only when wheels/frame/handlebars are correctly arranged. If concepts are truly independent given their parents, selection structure adds unnecessary complexity without benefit.

### Mechanism 3: Timestep-Concept Hierarchy Alignment in Diffusion
Diffusion timesteps naturally map to hierarchical concept levels—noisy timesteps encode global/abstract concepts; clean timesteps encode fine details. Progressive denoising mirrors hierarchical generation P(Z^{l+1}|Z^l). At t=899 (high noise), fine details are obscured, forcing representation to capture only coarse structure. At t=100 (low noise), details become discriminable. For distilled/accelerated models with few steps, granularity may be insufficient for meaningful hierarchy extraction.

## Foundational Learning

- **Causal Minimality Principle**: Why needed: Justifies why sparsity/compression constraints lead to identifiability. Quick check: Given two causal graphs that fit observed data equally well, which does minimality select?
- **Sparse Autoencoders (SAEs)**: Why needed: Practical instantiation of minimality constraints. K-sparse SAEs enforce that only K latent units activate per sample. Quick check: How does L1 regularization in standard SAEs differ from TopK activation sparsity?
- **Causal Discovery (PC Algorithm)**: Why needed: Used to infer causal structure across hierarchical levels from SAE feature activations. Quick check: What does the PC algorithm assume about latent confounders, and how might this affect selection model discovery?

## Architecture Onboarding

- **Component map**: Prompt → Image generation → Feature extraction at timesteps {899, 500, 100} → SAE encoding → Causal discovery → Steering vector computation → Modified denoising
- **Critical path**: Prompt → Image generation → Feature extraction at timesteps {899, 500, 100} → SAE encoding → Causal discovery → Steering vector computation → Modified denoising
- **Design tradeoffs**:
  - **K (sparsity)**: Low K yields cleaner graphs but may miss concepts; high K causes entanglement (Table 6: K=100 has 23.5% overlap vs. K=10's 8.9%)
  - **Timestep selection**: Must balance against model's sampling steps; Flux.1 uses only 4 steps, limiting granularity
  - **SAE latent dimension**: Larger enables more concepts but increases computation
- **Failure signatures**:
  - Dense/entangled graphs with many edges → sparsity too weak
  - Incomplete concept coverage (e.g., missing object parts) → sparsity too strong or wrong timesteps
  - Steering causes unintended changes → concept not truly disentangled; check parent/child relationships
- **First 3 experiments**:
  1. Reproduce SD1.4 hierarchy extraction: Train K=10 SAEs at timesteps 899/500/100 on LAION-COCO subset; verify "face → eyes/mouth" type structures emerge
  2. Sparsity ablation: Compare K∈{4, 10, 100} on held-out prompts; measure concept overlap (IoU) and coverage
  3. Steering validation: Apply positive/negative steering on identified concepts; verify changes are localized to concept's hierarchical level

## Open Questions the Paper Calls Out

### Open Question 1
Can modern vision-language models (VLMs) fully automate the semantic interpretation and labeling of identified latent concepts, thereby removing the need for human validation? [explicit] Appendix D notes that assigning human-understandable descriptions to identified concepts currently requires human validation, but speculates that "perhaps modern vision-language models have the potential to automate this process."

### Open Question