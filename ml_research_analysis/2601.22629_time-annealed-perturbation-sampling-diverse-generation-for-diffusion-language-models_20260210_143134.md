---
ver: rpa2
title: 'Time-Annealed Perturbation Sampling: Diverse Generation for Diffusion Language
  Models'
arxiv_id: '2601.22629'
source_url: https://arxiv.org/abs/2601.22629
tags:
- diversity
- generation
- minutes
- quality
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Time-Annealed Perturbation Sampling (TAPS) addresses the challenge
  of improving output diversity in diffusion language models (Diffusion-LMs) while
  maintaining generation quality. TAPS introduces time-dependent noise injection into
  the conditioning signal during inference, with stronger perturbations applied early
  in the denoising process to encourage semantic branching, followed by gradual attenuation
  to preserve fluency and instruction adherence.
---

# Time-Annealed Perturbation Sampling: Diverse Generation for Diffusion Language Models
## Quick Facts
- arXiv ID: 2601.22629
- Source URL: https://arxiv.org/abs/2601.22629
- Reference count: 40
- Primary result: TAPS improves semantic diversity by 7-10 percentage points while maintaining quality

## Executive Summary
Time-Annealed Perturbation Sampling (TAPS) introduces a novel approach to enhance output diversity in diffusion language models by incorporating time-dependent noise injection during the denoising process. The method applies stronger perturbations early in generation to encourage semantic branching, then gradually attenuates noise to preserve fluency and instruction adherence. Through a rescaling-and-mixing mechanism, TAPS maintains semantic fidelity while enabling exploration of diverse outputs.

## Method Summary
TAPS modifies the standard diffusion language model inference by introducing time-annealed noise perturbations to the conditioning signal. During the denoising process, TAPS injects higher levels of noise at earlier timesteps when the model has less information about the final output, encouraging exploration of diverse semantic directions. As denoising progresses and more context becomes available, the noise level is gradually reduced to maintain coherence and quality. The method employs a rescaling-and-mixing mechanism to ensure that the perturbed signals remain semantically faithful to the original conditioning while enabling diverse generation paths.

## Key Results
- TAPS achieves 7-10 percentage point improvements in semantic diversity metrics (Sent-BERT, EAD) compared to baselines
- On GSM8K reasoning tasks, TAPS improves majority-vote accuracy by 4-6 percentage points versus top-k sampling
- Consistent performance gains observed across creative writing, instruction following, and reasoning tasks using two different 8B parameter diffusion LM backbones

## Why This Works (Mechanism)
TAPS leverages the inherent uncertainty in early denoising timesteps by introducing controlled perturbations that encourage the model to explore multiple semantic paths. The time-annealed approach is crucial because early timesteps contain minimal information about the final output, making them ideal for diversity injection without compromising quality. By gradually reducing perturbations as more context becomes available, TAPS ensures that the final outputs remain coherent and faithful to the original instructions while benefiting from the semantic exploration enabled by early perturbations.

## Foundational Learning
- Diffusion Language Models: Sequence-to-sequence generative models that denoise text through iterative refinement steps
  - Why needed: Understanding TAPS requires grasping how diffusion models generate text through denoising
  - Quick check: Can you explain the forward and reverse processes in diffusion models?
- Semantic Diversity Metrics: Quantitative measures of output variation at the meaning level
  - Why needed: TAPS performance is evaluated using semantic diversity metrics rather than simple n-gram diversity
  - Quick check: What's the difference between lexical and semantic diversity?
- Annealing Schedules: Gradual reduction strategies for hyperparameters over time
  - Why needed: TAPS uses time-dependent noise reduction to balance diversity and quality
  - Quick check: How does temperature annealing work in traditional sampling methods?

## Architecture Onboarding
Component map: Input Text -> Time-Annealed Noise Module -> Diffusion LM Backbone -> Rescaling-and-Mixing -> Output Text

Critical path: The noise injection occurs at each denoising timestep, with perturbations being strongest at early timesteps (t â‰ˆ T) and gradually reduced as t approaches 0. The rescaling-and-mixing mechanism operates at each step to ensure semantic fidelity.

Design tradeoffs: TAPS trades computational overhead for improved diversity, requires additional hyperparameter tuning (annealing schedule, noise scaling), and may introduce some unpredictability in output generation due to the stochastic perturbations.

Failure signatures: Over-perturbation leading to incoherent outputs, under-perturbation resulting in insufficient diversity gains, or poor annealing schedule choices causing quality degradation.

First experiments: 1) Ablation study varying noise levels at different timesteps, 2) Comparison of different annealing schedule functions, 3) Analysis of diversity-quality tradeoff curves across multiple tasks.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses on semantic diversity metrics rather than practical diversity measures like novelty or coverage
- Introduces additional hyperparameters requiring tuning, potentially limiting practical applicability
- Experiments conducted on relatively small 8B parameter models, raising scalability questions

## Confidence
- High confidence in diversity improvement claims (supported by consistent metric improvements across multiple tasks and model backbones)
- Medium confidence in quality maintenance (rescaling-and-mixing mechanism described but implementation details limited)
- Medium confidence in reasoning task improvements (majority-vote setup may artificially inflate performance gains)

## Next Checks
1. Evaluate TAPS on larger model scales (30B+ parameters) to assess scalability and whether diversity gains persist at production scale
2. Conduct human evaluation studies to assess whether increased semantic diversity translates to more useful or creative outputs in practical applications
3. Compare TAPS against alternative diversity-promoting methods like temperature scaling, nucleus sampling, or contrastive decoding to establish relative effectiveness across different diversity objectives