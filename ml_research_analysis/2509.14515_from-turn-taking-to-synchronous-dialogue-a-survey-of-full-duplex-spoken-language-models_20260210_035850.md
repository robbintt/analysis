---
ver: rpa2
title: 'From Turn-Taking to Synchronous Dialogue: A Survey of Full-Duplex Spoken Language
  Models'
arxiv_id: '2509.14515'
source_url: https://arxiv.org/abs/2509.14515
tags:
- full-duplex
- language
- dialogue
- spoken
- fd-slms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey provides a comprehensive analysis of Full-Duplex Spoken
  Language Models (FD-SLMs), addressing the critical limitation of current spoken
  dialogue systems that operate in sequential listen-think-speak cycles. The authors
  establish a formal taxonomy distinguishing Engineered Synchronization (modular architectures
  with explicit state arbitration) from Learned Synchronization (end-to-end systems
  with intrinsic full-duplex capabilities), and unify fragmented evaluation approaches
  into a four-pillar framework: Temporal Dynamics, Behavioral Arbitration, Semantic
  Coherence, and Acoustic Performance.'
---

# From Turn-Taking to Synchronous Dialogue: A Survey of Full-Duplex Spoken Language Models

## Quick Facts
- **arXiv ID:** 2509.14515
- **Source URL:** https://arxiv.org/abs/2509.14515
- **Authors:** Yuxuan Chen; Haoyuan Yu
- **Reference count:** 0
- **Primary result:** Comprehensive analysis of Full-Duplex Spoken Language Models (FD-SLMs) and unified evaluation framework

## Executive Summary
This survey addresses the critical limitation of current spoken dialogue systems that operate in sequential listen-think-speak cycles, establishing a formal taxonomy and unified evaluation framework for Full-Duplex Spoken Language Models. The authors distinguish between Engineered Synchronization (modular architectures with explicit state arbitration) and Learned Synchronization (end-to-end systems with intrinsic full-duplex capabilities), unifying fragmented evaluation approaches into a four-pillar framework. Through comparative analysis of seven representative FD-SLMs, the survey reveals fundamental challenges including synchronous data scarcity, architectural divergence, and evaluation gaps, while providing a roadmap for advancing human-AI communication.

## Method Summary
The survey establishes a formal taxonomy distinguishing Engineered Synchronization from Learned Synchronization approaches to full-duplex dialogue. It unifies fragmented evaluation approaches into a four-pillar framework: Temporal Dynamics, Behavioral Arbitration, Semantic Coherence, and Acoustic Performance. The methodology involves comparative analysis of seven representative FD-SLMs, examining their architectures, capabilities, and performance across the four evaluation dimensions. The survey identifies key challenges including synchronous data scarcity, architectural divergence, and evaluation gaps, while proposing synthetic data generation and comprehensive behavioral evaluation as potential solutions.

## Key Results
- Acoustic quality approaches human levels (MOS ~4.85) while critical gaps persist in behavioral arbitration (interruption success rate: 54-86% vs. 94% human)
- Identified inverse latency-coherence trade-off constraining current systems, where optimizing for sub-200ms real-time processing often degrades semantic context maintenance
- Revealed fundamental challenges including synchronous data scarcity (<5% of available datasets contain genuine overlapping speech) and architectural divergence between evaluation standards

## Why This Works (Mechanism)
The survey's unified evaluation framework works by providing a systematic approach to assess full-duplex capabilities across multiple dimensions simultaneously. The four-pillar structure captures the essential aspects of synchronous dialogue: temporal dynamics ensure real-time responsiveness, behavioral arbitration measures appropriate turn-taking and interruption handling, semantic coherence evaluates understanding during overlapping speech, and acoustic performance ensures natural-sounding output. This comprehensive approach reveals trade-offs between dimensions that isolated evaluations would miss.

## Foundational Learning
- **Temporal Dynamics (FTO, SL, IRD):** Why needed - Measures real-time responsiveness in full-duplex dialogue; Quick check - Calculate Floor Transfer Offset between user and system responses
- **Behavioral Arbitration (ISR):** Why needed - Evaluates interruption handling and turn-taking appropriateness; Quick check - Measure interruption success rate against human baseline
- **Semantic Coherence (WER, QA):** Why needed - Ensures understanding during overlapping speech; Quick check - Compare semantic scores under duplex vs. simplex conditions
- **Acoustic Performance (MOS):** Why needed - Validates natural-sounding output during simultaneous speaking; Quick check - Compare synthesized vs. human audio quality scores
- **Synthetic Data Generation:** Why needed - Addresses scarcity of authentic overlapping speech datasets; Quick check - Train model on synthetic data and measure performance transfer
- **Real-time Safety Mechanisms:** Why needed - Ensures harm prevention without violating latency constraints; Quick check - Measure safety layer overhead impact on cognitive clock

## Architecture Onboarding

**Component Map:** User Audio -> Audio Encoder -> State Manager -> Text Encoder/Decoder -> TTS -> Agent Audio (Engineered); User Audio + Agent Audio -> Joint Encoder -> Decoder -> Agent Audio (Learned)

**Critical Path:** Audio capture → Real-time processing (≤200ms) → State arbitration → Response generation → Audio synthesis

**Design Tradeoffs:** Engineered approaches offer explicit control but require complex coordination; Learned approaches provide end-to-end optimization but struggle with data scarcity and interpretability

**Failure Signatures:** Duplex collapse (ignoring input while speaking), latency violation (>200ms processing), semantic degradation during overlap, inappropriate interruption handling

**First Experiments:** 1) Measure FTO and SL distributions on standard datasets, 2) Compare ISR across architectures on overlapping speech samples, 3) Evaluate semantic coherence under varying latency constraints

## Open Questions the Paper Calls Out

**Open Question 1:** Can the inverse relationship between response latency and semantic coherence be resolved, or is it a fundamental constraint of real-time cognitive processing? The survey identifies this trade-off but the precise functional relationship remains unexplored.

**Open Question 2:** What data synthesis or augmentation methods can effectively capture the complex prosodic entrainment and overlap dynamics missing from current TTS-generated training sets? Current synthetic data fails to reproduce subtle timing of natural interruptions.

**Open Question 3:** How can models be improved to close the gap in behavioral arbitration, specifically regarding the ability to distinguish semantic interruptions from backchannels? Systems currently struggle with real-time semantic arbitration.

**Open Question 4:** How can robust safety mechanisms be integrated into full-duplex streams without violating the strict latency requirements of simultaneous listening and speaking? Safety classifiers typically introduce delays conflicting with <200ms constraints.

## Limitations
- Fundamental data scarcity for synchronous dialogue constrains both training and evaluation
- Architectural divergence between Engineered and Learned Synchronization creates incompatible evaluation standards
- Reliance on proxy metrics for behavioral arbitration introduces measurement uncertainty
- Safety and privacy considerations lack comprehensive evaluation frameworks for real-world deployment

## Confidence
- **High Confidence:** The four-pillar evaluation framework provides logically coherent structure supported by established metrics
- **Medium Confidence:** Comparative analysis is methodologically sound though some implementation details are referenced rather than specified
- **Medium Confidence:** Identified inverse latency-coherence trade-off reflects observed patterns but underlying mechanisms require further investigation

## Next Checks
1. **Synthetic Data Generation Validation:** Create overlapping speech corpus using text-to-speech and dialogue simulation, then measure whether models trained on this data show improved performance on true full-duplex benchmarks compared to models trained only on turn-taking data.

2. **Architectural Convergence Experiment:** Implement hybrid model combining explicit state management with end-to-end learning, then measure whether this achieves better behavioral arbitration scores while maintaining acceptable latency.

3. **Safety Mechanism Evaluation:** Develop and test specific safety mechanisms for full-duplex systems (e.g., real-time content filtering during overlapping speech) and measure their impact on both safety violation rates and system performance metrics.