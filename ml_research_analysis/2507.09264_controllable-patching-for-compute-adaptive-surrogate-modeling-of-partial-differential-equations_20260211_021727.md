---
ver: rpa2
title: Controllable Patching for Compute-Adaptive Surrogate Modeling of Partial Differential
  Equations
arxiv_id: '2507.09264'
source_url: https://arxiv.org/abs/2507.09264
tags:
- patch
- size
- fixed
- these
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a method to enable dynamic patch size control\
  \ in transformer-based PDE surrogate models at inference time. The authors introduce\
  \ two lightweight, architecture-agnostic modules\u2014Convolutional Kernel Modulation\
  \ (CKM) and Convolutional Stride Modulation (CSM)\u2014that allow models to adjust\
  \ tokenization resolution without retraining."
---

# Controllable Patching for Compute-Adaptive Surrogate Modeling of Partial Differential Equations

## Quick Facts
- arXiv ID: 2507.09264
- Source URL: https://arxiv.org/abs/2507.09264
- Reference count: 40
- Key outcome: Dynamic patch size control at inference time without retraining, combined with cyclic rollout for artifact suppression

## Executive Summary
This paper introduces the first framework enabling dynamic patch size control in transformer-based PDE surrogate models at inference time. The authors propose two lightweight, architecture-agnostic modules—Convolutional Kernel Modulation (CKM) and Convolutional Stride Modulation (CSM)—that allow models to adjust tokenization resolution without retraining. These methods are combined with a cyclic patch-size rollout strategy that mitigates patch artifacts and improves long-term stability in autoregressive forecasting. Evaluated across multiple 2D and 3D PDE datasets, the approach achieves competitive or superior accuracy compared to fixed-patch baselines while enabling compute-adaptive deployment.

## Method Summary
The method enables dynamic patch size control through two lightweight modules: CKM resizes convolutional kernels via pseudoinverse interpolation to preserve feature alignment across patch sizes, while CSM varies convolution stride with learned padding for simpler implementation. Both modules are architecture-agnostic and allow selection of patch sizes {4, 8, 16} per forward pass without retraining. The framework includes a cyclic patch-size rollout strategy that alternates patch sizes during autoregressive forecasting to suppress harmonic spectral artifacts inherent to fixed-patch tokenization. Models are trained with random patch/stride sampling across all target sizes, using standard transformer processors (Vanilla ViT, Axial ViT, etc.) with two-stage hMLP encoder/decoder.

## Key Results
- Dynamic patch size control at inference time without accuracy loss or retraining
- Cyclic patch-size rollout suppresses harmonic spectral artifacts and improves long-term stability
- Competitive or superior accuracy compared to fixed-patch baselines across 2D and 3D PDE datasets
- First framework to enable inference-time patch-size tunability in patch-based PDE surrogates

## Why This Works (Mechanism)

### Mechanism 1: PI-Resize Kernel Projection
Dynamically resizing convolutional kernels via pseudoinverse interpolation preserves feature alignment across patch sizes without retraining. The CKM computes a projection matrix W = (B^T)†W_base, where B is a bicubic interpolation matrix and W_base is the trained base kernel. This minimizes the expected squared error between original and resized patch embeddings. At inference, any target patch size k ∈ {4, 8, 16} can be selected per forward pass.

### Mechanism 2: Stride Modulation with Learned Padding
Varying convolution stride while keeping kernel fixed enables flexible token count control with simpler implementation than kernel resizing. CSM samples stride s ∈ {4, 8, 16} per forward pass, applying learned boundary padding to avoid edge artifacts. The fixed kernel provides stable feature extraction while stride directly controls downsampling rate.

### Mechanism 3: Cyclic Patch Rollout for Artifact Suppression
Alternating patch sizes cyclically during autoregressive rollout suppresses harmonic spectral artifacts inherent to fixed-patch tokenization. At each timestep, the model cycles through k ∈ {4, 8, 16}. This diversifies tokenization grids, preventing accumulation of grid-aligned errors that manifest as spectral spikes at patch harmonics.

## Foundational Learning

- **Vision Transformer (ViT) Patch Tokenization**: Understanding how patches become tokens is prerequisite since both CKM and CSM modify the standard patch embedding layer. Quick check: Given a 256×256 input and patch size 16, how many tokens are produced? (Answer: 256)

- **Transposed Convolution for Upsampling**: The decoder reconstructs full-resolution fields from latent tokens using strided transposed convolutions synchronized with encoder. Quick check: What happens to output shape if stride increases from 4 to 8 in a transposed convolution? (Answer: Output spatial dimensions double)

- **Autoregressive Error Accumulation**: Rollout stability is a core claim; understanding how prediction errors compound over timesteps contextualizes artifact concerns. Quick check: Why might small tokenization artifacts become severe after 50 autoregressive steps? (Answer: Errors feed back as input, compounding multiplicatively)

## Architecture Onboarding

- **Component map**: Input Field (H×W×T×C) → [CKM/CSM Encoder] ← k or s sampled from {4,8,16} → Token Sequence (N tokens, N varies with k/s) → [Transformer Processor] ← Architecture-agnostic (Vanilla ViT, Axial ViT, etc.) → Latent Representation → [CKM/CSM Decoder] ← Same k/s as encoder → Prediction (H×W×1×C)

- **Critical path**: The k/s sampling decision must occur once per forward pass and propagate identically to encoder and decoder. Position embeddings must adapt if using absolute encodings.

- **Design tradeoffs**: CKM offers finer control via kernel resizing but requires pseudoinverse computation; CSM is simpler but padding learning adds complexity. Patch diversity during training is essential—must train on all target patch sizes. Cyclic schedules outperform random for rollout.

- **Failure signatures**: Checkerboard artifacts in rollout images indicate fixed patch used; switch to cyclic rollout. Accuracy collapse at specific patch size suggests that size was omitted during training. Edge artifacts indicate padding insufficient for stride.

- **First 3 experiments**: 1) Train fixed-patch (p=16) vanilla ViT on one dataset to establish baseline. 2) Replace with CKM, train with random k ∈ {4,8,16}, compare inference accuracy at each k against fixed-patch models. 3) Run 50-step autoregressive prediction with cyclic k schedule; compare spectral residuals and visual artifacts against fixed-patch baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does cyclic patch-size switching during rollout consistently outperform random switching schedules, and what are the theoretical mechanisms behind this improvement?
- Basis in paper: [explicit] "Finally, we compare cyclic vs. random patch switching during rollouts. Random schedules significantly hurt performance (App. Table G.4)..."
- Why unresolved: The paper reports the empirical finding but does not investigate or explain the underlying mechanism causing this difference.
- What evidence would resolve it: Systematic analysis comparing different scheduling strategies with theoretical justification, examining how cyclic patterns interact with learned representations.

### Open Question 2
- Question: Can CKM/CSM modules be successfully integrated into non-ViT patch-based architectures such as AFNO, DPOT, and Swin transformers while maintaining compute-adaptive benefits?
- Basis in paper: [explicit] "If compute resources were not a limiting factor, our study could be extended along several directions. For example, we could test our CKM/CSM modules for flexifying non-ViT but patch-based architectures, such as AFNO, or state-of-the-art neural operator models like DPOT..."
- Why unresolved: The authors explicitly mention this as a future direction constrained by compute resources, noting that preliminary AFNO experiments showed patch size sensitivity.
- What evidence would resolve it: Successful application of CKM/CSM to these architectures with benchmark comparisons showing maintained accuracy and compute-adaptive capabilities.

## Limitations
- Method's reliance on patch-based tokenization limits applicability to problems with fine-scale structures that do not align to grid patches
- Two untested scaling assumptions: PI-resize projections generalize beyond three trained sizes, and learned padding fully compensates for stride-induced receptive field changes
- Lack of testing whether random rollout schedules perform comparably to cyclic, which would challenge the core diversification mechanism

## Confidence

- **High confidence**: Cyclic patch rollout reduces spectral artifacts and improves long-term stability (supported by quantitative spectral analysis and visual inspection)
- **Medium confidence**: CKM and CSM enable accurate inference at untrained patch sizes (supported by ablation showing performance drop when sizes are omitted during training)
- **Low confidence**: No accuracy loss occurs from kernel resizing/projection (lack of direct comparison to individually trained fixed-patch models for each size)

## Next Checks

1. Test kernel resizing beyond {4,8,16} (e.g., p=12, p=20) to verify the PI-resize projection's extrapolation capability and determine practical limits
2. Compare cyclic rollout against multiple random schedules to definitively establish whether patch-size diversification is the artifact-suppression mechanism
3. Implement and evaluate both CKM and CSM on a problem with strong boundary effects (e.g., small domains with high stride) to test sufficiency of learned padding and identify conditions where one approach clearly outperforms the other