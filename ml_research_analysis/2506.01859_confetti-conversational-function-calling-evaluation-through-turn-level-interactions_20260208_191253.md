---
ver: rpa2
title: 'CONFETTI: Conversational Function-Calling Evaluation Through Turn-Level Interactions'
arxiv_id: '2506.01859'
source_url: https://arxiv.org/abs/2506.01859
tags:
- user
- function
- dialog
- turns
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CONFETTI is a conversational function-calling benchmark that evaluates
  large language models on complex multi-turn interactions with 109 human-authored
  conversations, 313 user turns, and 86 APIs. The benchmark uses turn-level off-policy
  evaluation to assess function-calling accuracy, response quality, and dialog acts
  across conversational complexities like follow-ups, goal correction, and ambiguous
  goals.
---

# CONFETTI: Conversational Function-Calling Evaluation Through Turn-Level Interactions

## Quick Facts
- **arXiv ID**: 2506.01859
- **Source URL**: https://arxiv.org/abs/2506.01859
- **Reference count**: 3
- **Primary result**: Nova Pro achieves 40.01% AST soft accuracy on CONFETTI's function-calling benchmark

## Executive Summary
CONFETTI is a benchmark for evaluating large language models on conversational function-calling tasks involving multi-turn interactions with APIs. The benchmark consists of 109 human-authored conversations, 313 user turns, and 86 APIs, designed to test function-calling accuracy, response quality, and dialog act classification across various conversational complexities. Using turn-level off-policy evaluation with gold-history context, the benchmark reveals that while models like Nova Pro can achieve reasonable accuracy on single function calls, performance degrades significantly with chained calls, more APIs, or longer conversations.

## Method Summary
CONFETTI evaluates function-calling through off-policy, turn-level assessment where models receive gold-history context and must produce either function calls or text responses. The primary metric is AST soft accuracy, comparing parsed function call trees with soft string matching for parameters. The benchmark includes 506 function-calling examples and 663 response quality examples across 12 conversational complexity types. Models are evaluated using temperature 0 inference, with results retrieved from gold truth rather than executed. The evaluation also incorporates dialog act classification and parameter hallucination detection using LLM judges.

## Key Results
- Nova Pro achieves 40.01% AST soft accuracy, outperforming Claude Sonnet v3.5 (35.46%) and Llama 3.1 405B (33.19%)
- Performance degrades with more APIs (1-9: 44.76% vs 20-29: 9.65%) and longer conversations (1-5 turns: 45.16% vs 11+ turns: 21.31%)
- Chained function-calling accuracy drops sharply for chains of length ≥3, with most models scoring below 22% for longer chains
- Models frequently over-trigger (calling functions when ground truth is inform/seek_info) or under-trigger (producing text when function call is required)

## Why This Works (Mechanism)
CONFETTI's turn-level off-policy evaluation methodology enables fine-grained analysis of function-calling behavior by isolating each turn with gold-history context. This approach eliminates simulator noise while capturing conversational complexities like goal correction and ambiguous goals. The AST soft matching with AlignScore allows robust comparison of function calls with parameter variations, while dialog act classification provides insight into over- and under-triggering behaviors. The benchmark's design specifically targets multi-turn scenarios where function-calling errors compound over conversation history.

## Foundational Learning

- **Concept: Off-Policy vs. On-Policy Evaluation**
  - **Why needed here:** CONFETTI uses off-policy, turn-level evaluation where the history is fixed to gold truth. This removes simulator noise but does not fully replicate live deployment where the model conditions on its own previous predictions.
  - **Quick check question:** "If a model scores 40% AST soft accuracy on CONFETTI, can you conclude it will achieve the same success rate in a live conversation? Why or why not?"

- **Concept: Abstract Syntax Tree (AST) Parsing**
  - **Why needed here:** The primary metric parses function calls into trees (function name, parameters, values) to enable structured comparison. Understanding this is required to interpret metric outputs.
  - **Quick check question:** "Given a predicted call `search_flights(dest='Naples, FL')` and gold call `search_flights(dest='Naples')`, how would AST soft scoring (with AlignScore) likely compare them versus exact string matching?"

- **Concept: Dialog Acts**
  - **Why needed here:** Response quality is evaluated by classifying agent turns into acts such as `seek_info`, `inform`, `reject`, and `function_calling`. This provides signal on over- or under-triggering of calls.
  - **Quick check question:** "If a model produces a text response asking for clarification instead of making a function call, which dialog act should be assigned, and how does this affect its function-calling benchmark score?"

## Architecture Onboarding

- **Component map:** Human-authored conversations (109) → split into turn-level instances (506 function-calling, 663 response quality) → evaluation harness with AST parsing → dialog act classification → parameter hallucination detection
- **Critical path:** Load conversation and API schemas → prompt model with gold history → parse function calls into AST → compute soft accuracy → classify dialog acts → detect parameter hallucinations
- **Design tradeoffs:** Off-policy evaluation enables reproducible comparisons but may not reflect live behavior; mocked API responses ensure consistency but reduce realism; LLM judges provide flexibility but introduce potential error
- **Failure signatures:** Over-triggering (calling functions when ground truth is `inform` or `seek_info`), under-triggering (producing text when function call is required), chained call collapse (accuracy drops for chains ≥3), parameter hallucination (values not grounded in conversation history)
- **First 3 experiments:** 1) Baseline run: evaluate model on function-calling benchmark, report AST soft accuracy overall and by slice; 2) Error analysis: categorize mismatches into over-triggering, under-triggering, wrong function, wrong parameters, hallucinated parameters; 3) Chain ablation: measure accuracy separately for chain lengths 1, 2, 3, 4+ to identify where cascading failures begin

## Open Questions the Paper Calls Out

- **Question:** Does the off-policy evaluation methodology artificially inflate performance for later turns compared to on-policy inference?
  - **Basis in paper:** [explicit] Section 7 states that using off-policy trajectories can induce "indirect in-context learning" from gold-truth history, potentially causing "artificial inflation in model performance for later turns."
  - **Why unresolved:** The paper isolates turn-level behavior for fine-grained analysis but acknowledges this static context differs from dynamic inference where the model relies on its own prior predictions.
  - **What evidence would resolve it:** A comparative experiment evaluating the same models in a dynamic, on-policy setting where the conversation history consists of the model's own generated turns rather than ground-truth text.

- **Question:** How does the lack of live API execution and stateful interaction impact the validity of the function-calling assessment?
  - **Basis in paper:** [explicit] Section 7 identifies the reliance on gold-truth lookups for function results as a limitation, noting it is a "deviation from inference-time behavior."
  - **Why unresolved:** The benchmark mocks responses to ensure consistency, but real-world failure cases often stem from handling unexpected API outputs, errors, or state changes, which are bypassed in this setup.
  - **What evidence would resolve it:** A study implementing the benchmark tasks in a stateful sandbox environment where models must process live execution results rather than retrieved text blocks.

- **Question:** To what extent do errors in the LLM-based judges (dialog act classifier and hallucination detector) obscure the true comparative performance of the evaluated models?
  - **Basis in paper:** [explicit] Section 7 notes that while the LLM judges show high recall, they "can still make mistakes which can obscure the true model performance."
  - **Why unresolved:** The evaluation of response quality and parameter validity relies entirely on these automated "LLM-as-a-judge" methods, introducing potential noise into the ranking of models like Nova Pro and Claude Sonnet.
  - **What evidence would resolve it:** A correlation analysis comparing the automated evaluation scores against human-annotated ground truth for dialog acts and hallucination rates across a significant sample of the dataset.

## Limitations
- Off-policy evaluation may not accurately reflect real-world performance where prediction errors compound over turns
- Reliance on LLM-based judges (Claude Haiku v3 and gpt-4o-mini) for dialog act classification and hallucination detection introduces potential judge error
- Function calls are evaluated against gold responses rather than executed, reducing ecological validity and missing execution errors
- Relatively small dataset (109 conversations, 313 user turns) may limit generalizability to all real-world scenarios

## Confidence

**High confidence:** The benchmark construction methodology (109 human-authored conversations with 86 APIs), the AST soft accuracy metric implementation, and the overall performance rankings of evaluated models (Nova Pro > Claude Sonnet v3.5 > Llama 3.1 405B) are well-documented and reproducible.

**Medium confidence:** The dialog act classification accuracy and parameter hallucination detection results depend on LLM judges whose reliability is difficult to independently verify. The specific threshold values and scoring methodology for these components are not fully detailed.

**Low confidence:** Generalization claims about model performance across all conversational complexities are limited by the relatively small dataset size. The benchmark may not capture edge cases or rare but important failure modes that occur in production deployments.

## Next Checks

1. **On-policy validation:** Implement an on-policy evaluation where the model conditions on its own previous predictions rather than gold history. Compare performance degradation between off-policy (benchmark) and on-policy results to quantify the impact of error compounding.

2. **LLM judge validation:** Create a small manually-labeled subset of turns for dialog acts and parameter hallucinations. Compare LLM judge classifications against human annotations to estimate error rates and calibrate confidence in the reported metrics.

3. **Execution simulation:** Implement a mocked API execution environment that simulates execution errors, latency, and partial failures. Rerun the benchmark to measure how execution failures affect chained function-calling accuracy and overall task completion rates.