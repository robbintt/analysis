---
ver: rpa2
title: 'FLAG-Trader: Fusion LLM-Agent with Gradient-based Reinforcement Learning for
  Financial Trading'
arxiv_id: '2502.11433'
source_url: https://arxiv.org/abs/2502.11433
tags:
- policy
- trading
- financial
- value
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FLAG-Trader integrates LLM-based language reasoning with RL policy
  optimization for financial trading. It uses a partially fine-tuned LLM as the policy
  network, where frozen layers retain pre-trained knowledge while trainable layers
  adapt to financial decision-making.
---

# FLAG-Trader: Fusion LLM-Agent with Gradient-based Reinforcement Learning for Financial Trading

## Quick Facts
- arXiv ID: 2502.11433
- Source URL: https://arxiv.org/abs/2502.11433
- Reference count: 40
- Primary result: FLAG-Trader achieves superior cumulative returns and Sharpe ratios versus both buy-and-hold and LLM-agentic baselines across multiple assets.

## Executive Summary
FLAG-Trader integrates LLM-based language reasoning with RL policy optimization for financial trading. It uses a partially fine-tuned LLM as the policy network, where frozen layers retain pre-trained knowledge while trainable layers adapt to financial decision-making. The model processes market data via structured text prompts and employs policy gradient methods like PPO to optimize trading performance. Experiments show FLAG-Trader consistently outperforms both buy-and-hold strategies and LLM-agentic baselines across multiple stocks, achieving superior cumulative returns and Sharpe ratios. Notably, a 135M-parameter open-source LLM model surpasses much larger proprietary models, demonstrating the effectiveness of RL fine-tuning in enhancing LLM-driven trading strategies.

## Method Summary
FLAG-Trader formulates single-asset trading as a finite-horizon POMDP with discrete actions {Buy, Hold, Sell}. Market states (prices, indicators, sentiment) and account balance are encoded as structured text prompts, processed by a partially fine-tuned LLM. The architecture separates parameters into frozen base layers (preserving general language reasoning) and trainable top layers (adapting to financial decisions). PPO optimizes the policy via Sharpe ratio-based rewards, with action masking ensuring valid trades. Training uses standard actor-critic objectives with entropy regularization, and deployment leverages vLLM for efficient inference.

## Key Results
- FLAG-Trader achieves 2.13x cumulative return versus buy-and-hold baseline on MSFT test period.
- The 135M-parameter SmolLM2-135M-Instruct LLM outperforms much larger proprietary models when RL fine-tuned.
- Across six assets, FLAG-Trader consistently delivers higher Sharpe ratios (1.68-2.49) compared to baselines (0.68-1.85).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Partial parameter fine-tuning allows domain adaptation without catastrophic forgetting of pre-trained reasoning.
- Mechanism: Partition LLM parameters into frozen layers θ_frozen (preserving general knowledge) and trainable top layers θ_train (adapting to financial decision-making). The policy network processes textual market states through both, outputting action probabilities via a policy head. Only θ_train receives gradient updates from trading rewards.
- Core assumption: The pre-trained LLM's general reasoning capabilities are primarily encoded in lower/earlier layers, while domain-specific adaptation can be localized to upper layers.
- Evidence anchors:
  - [abstract]: "a partially fine-tuned LLM acts as the policy network, leveraging pre-trained knowledge while adapting to the financial domain through parameter-efficient fine-tuning"
  - [section 4.2]: "This separation allows the model to retain general language understanding while adapting to financial decision-making with minimal computational overhead."
  - [corpus]: Limited direct corpus evidence for partial fine-tuning specifically in LLM-RL fusion; related work focuses on risk-aware rewards or dueling architectures.
- Break condition: If trainable layers are too shallow, domain adaptation may be insufficient; if too deep, pre-trained knowledge may degrade, reducing generalization.

### Mechanism 2
- Claim: Policy gradient optimization directly shapes LLM outputs toward trading objectives via reward-driven learning.
- Mechanism: The LLM policy network outputs action log-likelihoods. PPO computes clipped surrogate losses using advantage estimates derived from Sharpe-ratio-based rewards. Gradients flow through the policy head, value head, and trainable LLM layers, updating θ_P, θ_V, and θ_train to maximize expected discounted reward.
- Core assumption: Trading reward signals (Sharpe ratio differentials) provide sufficiently informative gradients for LLM policy refinement.
- Evidence anchors:
  - [abstract]: "Through policy gradient optimization driven by trading rewards, our framework... enhances LLM performance in trading"
  - [section 4.3]: "The parameters of the model are updated using stochastic gradient descent (SGD), leveraging the computed policy and value losses to drive optimization."
  - [corpus]: Trading-R1 and QTMRL similarly apply RL to financial tasks but differ in reward formulation; Trading-R1 emphasizes reasoning via RL.
- Break condition: If reward signals are sparse or noisy, gradients may fail to meaningfully shape policy; exploration may collapse.

### Mechanism 3
- Claim: Textual state encoding enables LLMs to process multimodal financial data without manual feature engineering.
- Mechanism: Numerical market states (prices, indicators, sentiment) are converted into structured text prompts lang(s_t). The LLM processes these through its transformer backbone, leveraging pre-trained language understanding to infer trading decisions. Action masking ensures only valid actions (e.g., not selling with zero holdings) receive non-zero probability.
- Core assumption: Structured text representation preserves sufficient signal from numerical market data for effective decision-making.
- Evidence anchors:
  - [section 4.1]: "lang(s) = 'Price: $p, Vol: v, RSI: r,...' This transformation enables the model to leverage the LLM's textual reasoning capabilities"
  - [section 4.2.1]: "action masking for invalid trades: π(a|s) = 0 if a ∉ A"
  - [corpus]: News-Aware Direct Reinforcement Trading incorporates news sentiment but uses pre-derived sentiment scores rather than raw text; limited corpus on direct numerical-to-text state encoding in LLM-RL.
- Break condition: If textual encoding introduces ambiguity or loses precision, the LLM may misinterpret market conditions; prompt design becomes critical failure point.

## Foundational Learning
- Concept: **Markov Decision Processes (MDPs)**
  - Why needed here: FLAG-Trader formulates trading as a finite-horizon POMDP with state, action, transition, and reward definitions.
  - Quick check question: Can you define the state space S and action space A for the trading MDP used in this paper?

- Concept: **Proximal Policy Optimization (PPO)**
  - Why needed here: The framework uses PPO for stable policy gradient updates with clipped surrogate objectives and value function losses.
  - Quick check question: What does the clipping coefficient ε do in the PPO surrogate objective L_P(θ_policy)?

- Concept: **Parameter-Efficient Fine-Tuning (PEFT)**
  - Why needed here: Only a subset of LLM parameters (trainable top layers) are updated, reducing computational cost while preserving pre-trained knowledge.
  - Quick check question: What are the tradeoffs between freezing more vs. fewer LLM layers in this architecture?

## Architecture Onboarding
- Component map:
  Environment -> State s_t (market observations + account balance) -> Prompt Encoder -> lang(s_t) (textual state description) -> LLM Backbone -> Frozen layers θ_frozen + Trainable layers θ_train -> Policy Head -> Action distribution π(a|s) over {Buy, Hold, Sell} -> Value Head -> State value estimate V(s) -> PPO Optimizer -> Updates θ_P, θ_V, θ_train via gradient descent on L_total -> Replay Buffer -> Stores experience tuples (s_t, a_t, r_t, s_{t+1})

- Critical path:
  1. State observation -> Prompt construction -> LLM forward pass -> Policy head output -> Action execution -> Reward computation -> Buffer storage
  2. Periodic PPO update: Sample mini-batch -> Compute advantage (GAE) -> Compute losses (policy, value, entropy) -> Backpropagate to θ_train, θ_P, θ_V

- Design tradeoffs:
  - Frozen vs. trainable layers: More frozen layers preserve general reasoning but may limit domain adaptation; more trainable layers increase adaptability but risk overfitting and computational cost.
  - Reward function: Sharpe ratio differential balances return and risk but may be noisy; alternative reward formulations (e.g., Treynor ratio, downside risk) could be explored per corpus neighbors.
  - Model scale: 135M-parameter LLM is computationally efficient but may lack reasoning depth of larger models; RL training compensates partially.

- Failure signatures:
  - Policy divergence: Rapidly changing outputs, unstable Sharpe ratios -> PPO clipping too loose or learning rate too high.
  - Invalid actions: Non-zero probability on disallowed actions -> Action masking not applied correctly.
  - Reward hacking: High returns but extreme drawdowns -> Reward function not sufficiently penalizing risk.
  - Catastrophic forgetting: Degraded reasoning on non-trading tasks -> Too many layers made trainable.

- First 3 experiments:
  1. **Baseline validation**: Run buy-and-hold and LLM-agentic baselines on a single asset (e.g., MSFT) using provided test period; verify reproduction of reported cumulative return and Sharpe ratio.
  2. **Ablation on trainable layers**: Vary the number of top layers set as trainable (e.g., 1, 2, 4 layers); measure impact on Sharpe ratio, volatility, and training stability.
  3. **Reward function sensitivity**: Replace Sharpe ratio differential reward with a risk-aware composite reward (e.g., incorporating downside risk per corpus neighbor "A Risk-Aware Reinforcement Learning Reward for Financial Trading"); compare cumulative return and maximum drawdown.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can FLAG-Trader be extended with explicit risk-sensitive constraints (e.g., VaR, CVaR) to reduce maximum drawdown while maintaining competitive returns?
- Basis in paper: [explicit] The Limitations section states: "real-world trading requires stringent risk management, and FLAG-Trader currently optimizes for financial returns without explicitly incorporating risk-sensitive constraints."
- Why unresolved: The current reward function only uses Sharpe ratio, which does not directly penalize tail risks or catastrophic losses, leading to high MDD on some assets.
- What evidence would resolve it: Experiments comparing FLAG-Trader variants with risk-constrained reward formulations (e.g., CVaR-penalized rewards) showing reduced MDD without significant return degradation.

### Open Question 2
- Question: Can continual learning or meta-learning techniques improve FLAG-Trader's adaptability to non-stationary market conditions over longer time horizons?
- Basis in paper: [explicit] The Limitations section notes: "financial markets exhibit high volatility and non-stationarity, posing challenges for long-term generalization. Future work should explore techniques such as continual learning or meta-learning."
- Why unresolved: The experiments cover only ~7-month test periods; regime shifts or structural breaks in markets may cause performance decay over longer horizons.
- What evidence would resolve it: Evaluations across multiple years with documented regime changes, comparing standard FLAG-Trader against continual/meta-learning variants on rolling-window performance metrics.

### Open Question 3
- Question: Does the RL fine-tuning advantage of FLAG-Trader generalize to larger LLM architectures, or is the benefit specific to small-scale models?
- Basis in paper: [inferred] Only SmolLM2-135M-Instruct is used as the backbone; the paper claims small models surpass large ones but does not test whether RL-fine-tuned larger models would perform even better.
- Why unresolved: It remains unclear whether the observed gains stem from the RL framework itself or from the combination of RL with small models specifically.
- What evidence would resolve it: Ablation experiments applying the same PPO-based fine-tuning to 7B and 70B parameter LLMs (e.g., Llama-3.1) on identical trading tasks.

### Open Question 4
- Question: Why does FLAG-Trader exhibit significantly higher volatility and maximum drawdown on some assets despite optimizing for Sharpe ratio?
- Basis in paper: [inferred] Table 1 shows FLAG-Trader on UVV has AV=67.76% and MDD=35.04% versus Buy & Hold AV=29.30%, MDD=15.41%, raising questions about whether Sharpe ratio alone adequately captures risk.
- Why unresolved: The Sharpe ratio formulation may not fully penalize extreme volatility or drawdowns in finite-sample trading episodes.
- What evidence would resolve it: Analysis of learned policy distributions and reward signals on high-volatility assets; experiments with alternative reward formulations directly penalizing volatility or drawdown.

## Limitations
- The partial fine-tuning approach assumes a specific mapping between layer depth and capability type that remains empirically unverified for financial reasoning tasks.
- The Sharpe ratio differential reward function may be unstable due to its dependence on moving averages over short windows, potentially leading to noisy gradients during training.
- The claim about 135M-parameter model superiority over larger models requires careful interpretation due to confounding factors like inference efficiency and fine-tuning stability.

## Confidence
- **High confidence**: The overall framework architecture (LLM + PPO + partial fine-tuning) is well-specified and technically sound. The experimental methodology (baseline comparisons, multiple assets, standard metrics) is rigorous.
- **Medium confidence**: The specific mechanisms (layer-wise parameter freezing, textual state encoding) are theoretically justified but would benefit from additional ablation studies to isolate their individual contributions.
- **Low confidence**: The claim about 135M-parameter model superiority over larger models requires independent verification, as factors beyond pure reasoning capability (inference latency, fine-tuning stability) could influence results.

## Next Checks
1. **Layer freezing ablation**: Systematically vary the number of frozen layers (0-100%) and measure the impact on cumulative return and Sharpe ratio to empirically validate the partial fine-tuning hypothesis.
2. **Reward function robustness**: Replace the Sharpe ratio differential with alternative formulations (e.g., risk-adjusted return with drawdown penalties) and assess whether performance gains persist across reward designs.
3. **Cross-model generalization**: Replicate the core trading performance using a different small LLM (e.g., Phi-2) to determine whether results generalize beyond the specific SmolLM2-135M-Instruct architecture.