---
ver: rpa2
title: 'S2LPP: Small-to-Large Prompt Prediction across LLMs'
arxiv_id: '2505.20097'
source_url: https://arxiv.org/abs/2505.20097
tags:
- prompt
- prompts
- llms
- across
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the consistency of prompt preferences across
  Large Language Models (LLMs) of varying sizes and proposes a method to leverage
  smaller models for efficient prompt selection for larger models. The authors demonstrate
  that LLMs within the same family consistently prefer the same prompts, even across
  different model sizes, through experiments on Question Answering and Natural Language
  Inference tasks.
---

# S2LPP: Small-to-Large Prompt Prediction across LLMs

## Quick Facts
- arXiv ID: 2505.20097
- Source URL: https://arxiv.org/abs/2505.20097
- Reference count: 30
- Small models can reliably select optimal prompts for larger models within same family, achieving comparable performance to oracle prompts with much lower computational cost.

## Executive Summary
This paper investigates whether prompt preferences are consistent across Large Language Models (LLMs) of varying sizes within the same family. The authors demonstrate that smaller models can reliably select optimal prompts for larger target models on QA and NLI tasks, with high Recovery Rates of Performance (RRoP). Based on this finding, they propose the Small-to-Large Prompt Prediction (S2LPP) method, which uses smaller models to automatically select prompt templates for larger target models. The approach achieves high performance comparable to using oracle prompts while significantly reducing computational costs.

## Method Summary
The authors investigate prompt preference consistency across LLMs and propose S2LPP for efficient prompt selection. The method involves generating 10 candidate prompts per relation using ChatGPT, then using smaller selection models (e.g., LLaMA-2-7B) to evaluate these prompts on a development set. The highest-performing prompt per relation is selected and applied to the larger target model (GPT-3.5) for final evaluation. No training is involved - the pipeline is inference-only. The approach is tested across fourteen LLMs and extended to RAG and Chain-of-Thought prompting for arithmetic reasoning.

## Key Results
- LLMs within the same family consistently prefer the same prompts across different model sizes (POPM > 0.9 within-family)
- S2LPP achieves RRoP values often exceeding 70% compared to oracle prompts
- The method significantly reduces computational costs, with prompt selection taking ~10 minutes per relation using 7B models
- S2LPP generalizes to broader NLP applications including RAG and CoT prompting

## Why This Works (Mechanism)
The method works because LLMs within the same family share similar pre-training distributions and architectural foundations, leading to consistent prompt preferences across scales. Smaller models can effectively rank prompts because the relative performance ordering of prompts remains stable when scaling up model size. This consistency allows efficient selection without requiring expensive inference on the target large model.

## Foundational Learning
- **Prompt consistency across scales**: Understanding that optimal prompts for smaller models often work well for larger models of the same family. Why needed: Forms the theoretical basis for S2LPP's efficiency gains.
- **POPM (Proportion of Optimal-Prompt Matches)**: Metric measuring how often small models select the same optimal prompt as the target model. Why needed: Quantifies the reliability of cross-scale prompt selection.
- **RRoP (Recovery Rate of Performance)**: Metric calculating the ratio of accuracy achieved by selected prompt versus oracle prompt. Why needed: Measures the practical effectiveness of S2LPP in real-world applications.

## Architecture Onboarding

### Component Map
ChatGPT prompt generator -> Small selection model (LLaMA-2-7B) -> Prompt evaluation on dev set -> Optimal prompt selection -> Target model (GPT-3.5) -> Final evaluation

### Critical Path
The critical path is the inference pipeline: small model evaluates candidate prompts on dev set → selects top-performing prompt → applies to target model for final inference. This path determines both accuracy and computational efficiency.

### Design Tradeoffs
- **Prompt diversity vs. computational cost**: 10 candidates per relation balances exploration with efficiency
- **Dev set size vs. selection accuracy**: 100 samples per dataset chosen to balance statistical reliability with computational constraints
- **Model family vs. cross-family consistency**: Within-family selection (LLaMA-2→LLaMA-2) yields higher POPM than cross-family selection

### Failure Signatures
- Low POPM or RRoP indicates poor prompt selection, often due to small dev set or high prompt diversity variance
- Cross-family consistency drops (POPM ~0.6) when selection and target models have different pre-training distributions
- Inconsistent results across different random seeds suggest sensitivity to prompt generation or sampling

### 3 First Experiments
1. Replicate within-family prompt consistency (e.g., LLaMA-2-7B→LLaMA-2-70B) to verify core mechanism
2. Test cross-family generalization (e.g., Mistral-7B→GPT-3.5) to measure POPM drop-off
3. Vary dev set sizes (10, 50, 100, 500 samples) to quantify the accuracy-cost tradeoff

## Open Questions the Paper Calls Out
- **Can smaller models be effectively utilized to generate prompt candidates for Question Answering tasks, rather than relying solely on large LLMs like ChatGPT?** The current approach still relies on powerful LLMs to generate candidates and further research is required to explore using smaller models for prompt generation.
- **Does the high performance of the S2LPP method generalize to a wider variety of open-source large target LLMs beyond GPT-3.5?** Due to limited computational resources, only GPT-3.5 was used as the target model, with plans to experiment with more open-sourced large target LLMs.
- **To what extent does the similarity in pre-training corpus distributions cause the observed prompt preference consistency across model families?** The authors conjecture this originates from pre-training but identify deeper investigations into the source of this consistency as important future work.

## Limitations
- Cross-family prompt consistency is less reliable (POPM drops to ~0.6) compared to within-family consistency
- The method still relies on large LLMs for prompt generation, not addressing the full computational cost
- Results depend heavily on prompt diversity and dataset size, with small dev sets (100 samples) potentially limiting reliability

## Confidence
- **High confidence**: Within-family prompt consistency (LLaMA-2, Mistral, Vicuna families); S2LPP's computational efficiency gains
- **Medium confidence**: Cross-family generalization (POPM ~0.6-0.9); extension to RAG and CoT tasks
- **Low confidence**: Performance in low-resource settings; robustness to prompt template engineering

## Next Checks
1. Replicate experiments with selection and target models from different families (e.g., Mistral-7B→GPT-3.5) and measure POPM variance across multiple seeds
2. Systematically vary dev set sizes (10, 50, 100, 500 samples) to quantify the tradeoff between selection accuracy and computational cost
3. Generate candidate prompts using alternative strategies (e.g., few-shot demonstrations, structured templates) and assess impact on POPM and RRoP