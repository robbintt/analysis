---
ver: rpa2
title: 'Sharing Knowledge without Sharing Data: Stitches can improve ensembles of
  disjointly trained models'
arxiv_id: '2512.17592'
source_url: https://arxiv.org/abs/2512.17592
tags:
- performance
- data
- network
- networks
- stitches
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work investigates asynchronous collaboration in deep learning,\
  \ where parties share trained models without sharing raw data or coordinating training.\
  \ It addresses limitations of federated learning, which requires synchronized, online\
  \ model updates, by proposing stitching\u2014a method to combine intermediate representations\
  \ from independently trained models."
---

# Sharing Knowledge without Sharing Data: Stitches can improve ensembles of disjointly trained models

## Quick Facts
- arXiv ID: 2512.17592
- Source URL: https://arxiv.org/abs/2512.17592
- Reference count: 40
- Key result: Stitched ensembles match or exceed federated learning performance while maintaining better generalization

## Executive Summary
This work introduces stitching as an asynchronous method for collaborative deep learning where parties share trained models without raw data or coordination. The approach uses learned transformation layers to align intermediate representations from independently trained models, enabling meaningful combination even with different architectures. A novel double-batched training method improves stitch robustness by accounting for downstream effects during training. Experiments on medical image segmentation datasets show stitched ensembles match or exceed performance of data-sharing methods while maintaining better generalization to out-of-distribution data.

## Method Summary
The method trains parent networks independently on local datasets using nnUNet, then exports them as DAGs and matches corresponding layers using a modified Hirschberg's algorithm based on positional progress. 1×1 convolution stitching layers transform activations between matched layers, with multiplexers selecting between original, stitched, or averaged feature maps. Double-batched training doubles the batch dimension—half maintaining original features as reference, half carrying stitched features through the network—with MSE loss computed at multiple points. This accounts for downstream activation functions and enables robust multi-stitch ensembles that improve generalization while recovering local performance.

## Key Results
- Stitched ensembles match or exceed performance of data-sharing methods (merging datasets, federated learning) while maintaining better generalization
- Double-batched training substantially improves performance compared to direct matching, especially with multiple stitches
- Stitching outperforms fine-tuning and basic ensembles, which suffer from performance degradation on out-of-distribution data

## Why This Works (Mechanism)

### Mechanism 1: Representation Translation via Stitching Layers
Stitching layers transform intermediate feature maps between networks' representation spaces using learned 1×1 convolutions. This corrects for symmetries (permutation, sign flips) that would otherwise make direct combination infeasible. The core assumption is that networks trained on related tasks develop partially aligned intermediate representations that are learnably transformable. If feature representations are fundamentally incompatible, stitching layers cannot learn meaningful transformations.

### Mechanism 2: Double-Batched Training for Stitch Robustness
Training stitches with awareness of downstream layers prevents performance degradation when multiple stitches are combined. Each sample is duplicated—one copy maintains original features as reference, the other carries stitched features through the network. Losses are computed at multiple points, allowing stitches to co-adapt to downstream nonlinearities. This approach is essential because direct MSE matching at the stitch point ignores how errors propagate through subsequent layers.

### Mechanism 3: Intermediate Feature Ensembling with Stitch Alignment
Averaging feature maps at intermediate layers (not just outputs) improves generalization while recovering local performance when combined with stitching to align representations. This corrects errors earlier in the network where they have semantic meaning, rather than just combining final predictions. The core assumption is that intermediate features encode transferable high-level concepts; aligning and averaging them provides benefits beyond output ensembling.

## Foundational Learning

- **Federated Learning (Synchronous)**
  - Why needed here: Stitching is positioned as an asynchronous alternative; understanding FL's requirements (weight averaging, same architecture, coordination rounds) clarifies what constraints stitching removes
  - Quick check: Can you explain why weight averaging fails when networks have different initializations or architectures?

- **Neural Network Feature Maps / Intermediate Representations**
  - Why needed here: Stitching operates on activations at intermediate layers; understanding that these encode hierarchical features (edges → shapes → objects) is essential
  - Quick check: What does a 1×1 convolution do to a feature map spatially and channel-wise?

- **Ensemble Methods and Variance Reduction**
  - Why needed here: The paper builds on ensembling; understanding why ensembles improve generalization helps contextualize why intermediate-feature ensembling could offer additional benefits
  - Quick check: Why does averaging predictions from multiple independently trained models typically outperform any single model?

## Architecture Onboarding

- **Component map:** Parent Networks A and B (independently trained nnUNet models) → Layer Matching (Hirschberg's algorithm variant) → Stitching layers (1×1 conv or linear) → Switch layers (multiplexers) → Combined graph with averaged feature maps

- **Critical path:**
  1. Train parent networks independently on local datasets using nnUNet
  2. Export networks as DAGs with layer-level annotations
  3. Run layer matching algorithm to identify candidate stitch pairs (same spatial scale, acyclic)
  4. Initialize stitching layers and insert switch points
  5. Train stitches using double-batched training with frozen parent weights
  6. Select stitch position based on validation performance (Rule 2: lowest mean rank over all folds recommended)
  7. Deploy with averaged feature maps at selected stitch point

- **Design tradeoffs:**
  - Stitch position: Early stitches have minimal effect; late stitches (indices 50-65, ~60-70% through network) perform best; final stitch consistently worst
  - Training method: Double-batched requires 2× batch memory and backprop through more layers, but enables multi-stitch robustness
  - Selection strategy: Greedy (Rule 1) risks overfitting to validation fold; fold-mean (Rule 2) more stable; overall-best-position (Rule 3) requires cross-dataset info unavailable in practice

- **Failure signatures:**
  - Catastrophic performance drop with multiple stitches: Indicates direct-matching training without downstream awareness
  - Degradation on own-data performance in basic ensemble: Weaker model's predictions skew ensemble; stitch alignment needed
  - No improvement from stitching: Possible mismatch in layer pairing or incompatible task definitions between networks
  - Out-of-distribution activations: Observed with task-loss training; mitigated by double-batched approach

- **First 3 experiments:**
  1. Validate double-batched training benefit: Train stitches on a single network pair using both methods; plot Dice vs. stitch count. Expect: direct-matching degrades with >20 stitches; double-batched maintains performance.
  2. Stitch position sweep: For a fixed network pair, evaluate all stitch indices on validation data. Identify the "sweet spot" (expected: 50-70% network depth). Verify positional trends generalize across folds.
  3. Own-data vs. cross-data tradeoff: Compare single-dataset model, basic ensemble, and stitched ensemble on both local and partner test sets. Quantify: (a) performance recovery on local data, (b) generalization gain on partner data. Expect: stitched ensemble approaches federated learning performance without data sharing.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the generalization benefits of stitched ensembles be preserved when compressed via knowledge distillation into a single efficient network?
- Basis in paper: [explicit] The Discussion section notes that stitched networks carry the computational cost of ensembles and states, "It may be worthwhile to, for example, distill the knowledge of the stitched network into a more compact format."
- Why unresolved: The authors demonstrate improved performance but acknowledge the overhead; they did not test distillation methods to mitigate this cost.
- What evidence would resolve it: Experiments showing that a distilled student model matches the out-of-distribution robustness of the stitch-ensemble teacher.

### Open Question 2
- Question: How does stitching perform when source models are trained on datasets with inconsistent labeling protocols?
- Basis in paper: [explicit] The Discussion assumes consistent label definitions but admits, "In practice, this may not always be the case, as different protocols may be used... causing discrepancies in ground truth."
- Why unresolved: The experiments utilized datasets with consistent labeling tasks (e.g., cervical cancer MRI), leaving the impact of semantic label noise or protocol shifts unstudied.
- What evidence would resolve it: Evaluation of stitch-ensembles on disjoint datasets where annotations follow different guidelines or class definitions.

### Open Question 3
- Question: Can a universal heuristic be established for selecting optimal stitching layers without requiring validation-based search?
- Basis in paper: [inferred] Section 5.2 analyzes positional trends but finds variability; the authors conclude that while Rule 3 offers a guideline, "the chosen stitch is a key driver... positional trends appear... [but] the last stitch consistently performs the worst."
- Why unresolved: The results indicate that while late-stage stitching is often better, the optimal index varies by dataset and architecture, currently requiring empirical selection.
- What evidence would resolve it: A theoretical or empirical rule linking layer depth/feature stability to stitch success that holds across the tested datasets (medical and polyp).

## Limitations
- Method's performance on non-medical datasets or non-segmentation tasks remains unverified
- Double-batched training requires 2× batch memory, potentially limiting scalability to very large models
- Layer matching algorithm has limited testing beyond nnUNet architectures

## Confidence
- **High Confidence**: Basic stitching improves ensemble generalization compared to standard averaging
- **Medium Confidence**: Double-batched training provides measurable improvements over direct matching; stitching performance approaches federated learning while maintaining better generalization
- **Low Confidence**: Method's effectiveness on non-medical datasets, very deep architectures (>100 layers), or tasks with fundamentally different feature spaces

## Next Checks
1. **Architecture Generalization Test**: Apply stitching to non-medical datasets (e.g., Cityscapes for segmentation, CIFAR for classification) to verify method robustness across domains and tasks
2. **Memory Efficiency Analysis**: Implement gradient checkpointing or mixed-precision training to quantify practical scalability limits of double-batched training on larger models
3. **Layer Matching Robustness**: Test the DAG layer matching algorithm on networks with significantly different architectures (e.g., ResNet vs. U-Net) to identify failure modes and required modifications