---
ver: rpa2
title: Characterizing Web Search in The Age of Generative AI
arxiv_id: '2510.11560'
source_url: https://arxiv.org/abs/2510.11560
tags:
- search
- organic
- generative
- engines
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares traditional Google search with four generative
  AI search engines (Google AI Overview, Gemini, GPT-4o Search, GPT-4o with Search
  Tool) across queries from four domains. The study finds that generative engines
  generally cover a wider range of sources than traditional search, with AI Overviews
  accessing domains not found in top-10 or top-100 organic results.
---

# Characterizing Web Search in The Age of Generative AI

## Quick Facts
- **arXiv ID:** 2510.11560
- **Source URL:** https://arxiv.org/abs/2510.11560
- **Reference count:** 40
- **Primary result:** Generative search engines access different source domains than traditional search and vary significantly in their reliance on external vs internal knowledge

## Executive Summary
This paper compares traditional Google search with four generative AI search engines across multiple query domains, finding that generative engines generally access a wider range of sources than traditional search. The study reveals significant variation among generative models in their reliance on external web pages versus internal knowledge, with GPT-Tool citing far fewer sources on average. While overall topical coverage is similar between generative and traditional search, each engine surfaces distinct sets of concepts, and traditional search often outperforms on ambiguous queries.

The research highlights the need for new evaluation methods that account for source diversity, conceptual coverage, and synthesis behavior in generative search systems. The findings suggest that generative search engines are not simply replacing traditional search but are creating new search paradigms with different strengths and weaknesses.

## Method Summary
The study compares traditional Google search (Organic) with four generative AI search engines (Google AI Overview, Gemini-2.5-Flash, GPT-4o-Search, GPT-4o with Search Tool) across six datasets: MS Marco, WildChat, AllSides, Regulatory Actions, Science Queries, and Products. Queries were processed through SERP API for Organic and native APIs for generative engines, with responses analyzed for source links, concept coverage using LLooM, and temporal stability. The analysis measured links per query, source popularity via Tranco ranking, overlap with Organic results, website category distribution, response length, and concept coverage through systematic LLooM processing.

## Key Results
- Generative engines access domains not found in traditional search top-10 or top-100 results, with AI Overviews showing particularly high source diversity
- GPT-Tool relies almost exclusively on internal knowledge, citing near 0% web sources across most datasets compared to 50-75% for other engines
- Traditional search often outperforms generative engines on ambiguous queries, suggesting different optimal use cases
- Each engine surfaces distinct sets of concepts despite similar overall topical coverage, highlighting complementary strengths

## Why This Works (Mechanism)
Generative search engines synthesize information from multiple sources rather than simply ranking results, enabling them to access diverse domains that may not rank highly in traditional search. The variation in external knowledge reliance reflects architectural differences - some models are designed to heavily integrate web search while others prioritize internal knowledge bases. This synthesis capability allows generative engines to provide comprehensive answers even when no single source is authoritative, though it also introduces variability in source attribution and potential for hallucination.

## Foundational Learning
- **Source diversity measurement**: Why needed - to understand if generative engines truly expand information access beyond traditional rankings; Quick check - compare domain overlap percentages between engines
- **LLooM concept extraction**: Why needed - to quantify topical coverage beyond simple keyword matching; Quick check - verify concept induction consistency across multiple runs
- **Temporal stability analysis**: Why needed - to assess reliability of generative search over time; Quick check - measure Jaccard similarity of concepts before/after query repetition

## Architecture Onboarding
**Component map**: Query Generator -> Search Engine API -> Response Parser -> Source Extractor -> LLooM Analyzer -> Metrics Calculator

**Critical path**: Query generation and API execution must complete before concept extraction and analysis can begin, as concept coverage depends on having all engine responses for comparison

**Design tradeoffs**: The study prioritizes breadth (multiple engines, domains) over depth (detailed per-query analysis), accepting that LLooM-based concept extraction may introduce measurement noise in exchange for scalable coverage comparison

**Failure signatures**: Low AIO trigger rates indicate queries where generative responses are deemed inappropriate; minimal web citations from GPT-Tool suggest over-reliance on internal knowledge; inconsistent concept extraction indicates LLooM parameter sensitivity

**First experiments**: 1) Test query filtering criteria on a small sample to optimize AIO trigger rates, 2) Compare concept extraction results using human annotation vs LLooM on 50 queries, 3) Measure response time differences between engines for identical queries

## Open Questions the Paper Calls Out
None

## Limitations
- AIO trigger rate of only 65% may bias generative engine comparisons toward queries where Google deemed generative responses appropriate
- LLooM concept extraction methodology introduces potential measurement error that could affect coverage analysis conclusions
- The study focuses on US/DE perspectives through GCP VMs, potentially missing regional variations in search behavior

## Confidence
- **Source diversity findings**: High - supported by multiple metrics across diverse datasets
- **GPT-Tool knowledge reliance**: High - extreme statistical difference clearly demonstrated
- **Topical coverage similarity**: Medium - depends heavily on LLooM concept extraction pipeline

## Next Checks
1. Replicate concept coverage analysis using human annotation on a subset of queries to verify LLooM results
2. Test temporal stability by repeating the same queries after 2-4 weeks to measure concept coverage changes
3. Conduct controlled experiment varying query ambiguity levels to systematically test traditional vs generative performance claims