---
ver: rpa2
title: Gamma Mixture Modeling for Cosine Similarity in Small Language Models
arxiv_id: '2510.05309'
source_url: https://arxiv.org/abs/2510.05309
tags:
- distribution
- gamma
- mixture
- similarity
- cosine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the distribution of cosine similarities
  between sentence embeddings in small language models, finding that they are often
  well-modeled by shifted and truncated gamma distributions or gamma mixtures. A heuristic
  hierarchical clustering model is proposed to explain this phenomenon, where cosine
  similarities arise from mixtures of gamma distributions due to topic-based embeddings.
---

# Gamma Mixture Modeling for Cosine Similarity in Small Language Models

## Quick Facts
- arXiv ID: 2510.05309
- Source URL: https://arxiv.org/abs/2510.05309
- Authors: Kevin Player
- Reference count: 24
- The paper demonstrates that cosine similarity distributions in small language models are well-modeled by shifted and truncated gamma distributions or gamma mixtures, with a proposed EM algorithm for fitting.

## Executive Summary
This paper investigates the distribution of cosine similarities between sentence embeddings in small language models, finding that they are often well-modeled by shifted and truncated gamma distributions or gamma mixtures. A heuristic hierarchical clustering model is proposed to explain this phenomenon, where cosine similarities arise from mixtures of gamma distributions due to topic-based embeddings. An expectation-maximization (EM) algorithm is introduced for fitting shifted gamma mixtures. Experiments on three small models across three datasets demonstrate that single gamma distributions provide good fits for most cases, with gamma mixtures offering better approximations when needed.

## Method Summary
The paper proposes modeling cosine similarity distributions using shifted, truncated gamma distributions or their mixtures. The method involves computing all pairwise cosine similarities between a query embedding and corpus embeddings, then fitting either a single shifted gamma or a mixture of shifted gammas using an expectation-maximization algorithm. A warm-start optimization technique is introduced that runs 95% of EM iterations on a small subset of data (1/20) before refining on the full dataset, achieving approximately 10x speedup. The hierarchical clustering model explains why gamma mixtures arise naturally when embeddings organize around latent topic hierarchies.

## Key Results
- Shifted gamma distributions provide superior fits to empirical cosine similarity distributions compared to normal, beta, and von Mises-Fisher alternatives
- Single gamma distributions often suffice, but gamma mixtures improve fit for more complex, multi-modal distributions
- The warm-start EM optimization achieves ~10x speedup without accuracy loss
- The method provides a practical tool for modeling similarity distributions with significantly reduced data requirements compared to permutation tests

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shifted, truncated gamma distributions better model cosine similarity in topical sentence embeddings than symmetric alternatives (normal, beta, von Mises-Fisher).
- Mechanism: Cosine similarities in domain-restricted corpora exhibit asymmetry with a positive mean and heavy right tail. A gamma distribution shifted by parameter c and truncated to [-1, 1] captures this skew, whereas vMF produces a heavy left tail inconsistent with empirical observations.
- Core assumption: The embedding corpus is topically coherent, producing distributions with positive mean (Section 2, "this restriction to S₀ significantly affects the shape of D_q").
- Evidence anchors:
  - [abstract]: "distributions are often well captured by a gamma distribution shifted and truncated to [-1,1]"
  - [section 2.1]: Equation (4) shows vMF produces g(t) ∝ (1-t²)^((d-3)/2) e^(κt), yielding heavy left tail; Figure 4 illustrates mismatch
  - [corpus]: No direct corpus support for gamma-over-vMF preference in NLP embeddings; weak external validation
- Break condition: When embeddings span unrelated topics (no topical coherence), distribution may symmetricze; gamma advantage diminishes.

### Mechanism 2
- Claim: Hierarchical topic clustering in embedding space naturally produces gamma-mixture distributions for cosine similarity.
- Mechanism: Algorithm 1 generates a binary tree where child nodes are perturbed from parents via y ← η·x + noise. Documents sharing recent common ancestors with the query yield high similarity; deeper splits yield decay. Level-wise contributions form overlapping distributions that blend into a right-skewed shape approximating gamma mixtures.
- Core assumption: Embeddings organize around latent topic hierarchies with correlation strength η controlling cluster tightness (Section 4).
- Evidence anchors:
  - [section 4.1]: "Each successive level contains half as many nodes... producing a geometric decay" visible in Figure 7
  - [section 4]: η = 0.95 yields single gamma fit (Figure 5); η = 0.995 yields two-gamma mixture (Figure 6)
  - [corpus]: Gopal & Yang [12] (cited in paper) uses vMF mixtures for clustering—indirect support for mixture structures in embedding spaces
- Break condition: If topic structure is flat or uniformly distributed (no hierarchy), mixture components collapse toward single gamma.

### Mechanism 3
- Claim: Warm-starting EM on a small subset before full-data refinement achieves ~10× speedup without accuracy loss.
- Mechanism: Run 95% of EM iterations on 1/20 of data to approximate parameter initialization; final 5% of iterations on full dataset refines estimates. Early iterations move parameters toward basin; late iterations polish.
- Core assumption: Subset is representative of full distribution; initial basin is reachable from subset-estimated parameters.
- Evidence anchors:
  - [section 6]: "95% of the iterations on 1/20 of the data... speed gain is an order of magnitude"
  - [section 6]: Benchmarks show competitive timing vs scipy.stats.gamma.fit
  - [corpus]: No corpus papers validate warm-start for gamma mixtures specifically; standard EM practice
- Break condition: If subset is biased (e.g., not i.i.d. sample), warm-start may converge to wrong local optimum.

## Foundational Learning

- Concept: Gamma distribution (shape α, rate λ, shift c)
  - Why needed here: Core parametric family; EM updates require digamma ψ(α) and trigamma ψ'(α) functions for α-estimation (Section 3.5)
  - Quick check question: Given α=13.3, λ=35.5, c=-0.28, what is the mode of the shifted gamma?

- Concept: Expectation-Maximization (EM) for mixture models
  - Why needed here: Algorithm fits τᵢ (weights), αᵢ, cᵢ, λᵢ via iterative E-step (γ-responsibilities) and M-step (parameter updates)
  - Quick check question: In equation (9), why must τᵢ sum to 1, and how is this enforced?

- Concept: Cosine similarity geometry on unit hypersphere
  - Why needed here: Embeddings are L₂-normalized; similarity is dot product in [−1,1]; vMF comparison (Section 2.1) assumes spherical constraint
  - Quick check question: Why does vMF's density g(t) ∝ (1-t²)^((d-3)/2)e^(κt) produce a left tail for typical κ, d values?

## Architecture Onboarding

- Component map: Corpus S₀ -> Embeddings E(d) via sentence transformer -> Cosine similarities D_q(S₀) -> Shifted gamma G(α,c,λ) OR mixture Σ τᵢG(αᵢ,cᵢ,λᵢ) -> EM fitting with E-step/M-step -> Validation metrics

- Critical path:
  1. Extract embeddings for corpus + query
  2. Compute all cosine similarities → empirical histogram
  3. Initialize mixture parameters (s components, s≥1)
  4. Run warm-start EM: 95% iterations on 5% data, 5% iterations on full data
  5. Validate fit (visual inspection, KS test, or likelihood)

- Design tradeoffs:
  - Single gamma vs mixture: Single is faster (116ms vs 236ms for 2-component); mixture better for multi-modal D_q (Figure 3)
  - Number of components s: Paper uses s=1–2; no automatic selection criterion stated (Assumption: BIC/AIC applicable)
  - Warm-start ratio: 1/20 data for 95% iterations; tune for speed/accuracy tradeoff

- Failure signatures:
  - Non-convergence: γ-responsibilities oscillate → check data for outliers or initialization
  - Poor fit on left tail: Beta or vMF may be superior (contradicts paper's claim; validate on your data)
  - cᵢ ≥ min(xₜ): Shifted gamma undefined for xₜ ≤ cᵢ → enforce constraint during M-step

- First 3 experiments:
  1. Reproduce Figure 2 on a held-out arXiv abstract: Fit single shifted gamma, report α, c, λ and KS-statistic
  2. Ablate warm-start: Compare convergence time and final log-likelihood with vs without warm-start on 100K samples
  3. Cross-model validation: Fit gamma mixtures to all-MiniLM-L6-v2 vs all-mpnet-base-v2 on Wikipedia subset; assess if α, c, λ distributions differ significantly across models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is a single-step nonlinear optimization of the joint likelihood function $Q'$ faster than the iterative Expectation Conditional Maximization (ECM) coordinate ascent?
- Basis in paper: [explicit] Section 3.7 states, "It is an open problem to see if nonlinear fitting $Q'$ in one step is faster than the ECM coordinate zig-zag."
- Why unresolved: The Hessian of the objective function is generally non-convex for the relevant data range, preventing the use of standard efficient convex solvers.
- What evidence would resolve it: A benchmark comparison showing convergence rates of a non-convex joint optimizer versus the current bisection-based coordinate ascent method.

### Open Question 2
- Question: What is the rigorous theoretical justification for why a shifted, truncated gamma distribution fits embedding cosine similarities better than von Mises-Fisher or beta distributions?
- Basis in paper: [explicit] In Section 2, regarding the single gamma fit, the text notes, "the precise theoretical justification remains open."
- Why unresolved: The fit is currently supported only by empirical observation and a heuristic hierarchical clustering argument, rather than a derivation from the geometric properties of the embedding space.
- What evidence would resolve it: A theoretical derivation linking the properties of transformer attention mechanisms or training dynamics to the gamma distribution parameters.

### Open Question 3
- Question: Does the gamma mixture modeling accuracy degrade when applied to Large Language Models (LLMs) with significantly higher embedding dimensions?
- Basis in paper: [inferred] The paper title and experiments restrict scope to "Small Language Models" (specifically 384, 768, and 1024 dimensions).
- Why unresolved: Geometric properties of high-dimensional spaces (the "curse of dimensionality") often alter distribution behaviors; it is untested if the gamma approximation holds for dimensions typical of modern LLMs (e.g., 4096+).
- What evidence would resolve it: Empirical fitting results of the proposed model on embeddings from large-scale models (e.g., Llama 3 or Mistral) across the same datasets.

## Limitations
- The gamma mixture modeling approach is tested only on small language models (384-1024 dimensions) and may not generalize to larger models
- The theoretical justification for why shifted gamma distributions fit better than alternatives remains open and is currently based on empirical observation
- The hierarchical clustering mechanism is heuristic and lacks rigorous mathematical proof connecting it to the observed gamma mixture distributions

## Confidence
- **High confidence**: The shifted gamma distribution provides superior fit to empirical cosine similarity distributions compared to normal, beta, and von Mises-Fisher alternatives on tested datasets (Section 2, Figures 3-4)
- **Medium confidence**: The hierarchical clustering mechanism naturally produces gamma mixture distributions as a mathematical consequence of the perturbation model (Section 4, Figure 7)
- **Medium confidence**: Warm-starting EM on 5% of data yields ~10x speedup without accuracy loss (Section 6, timing benchmarks)
- **Low confidence**: The specific parameter ranges (α≈13.3, λ≈35.5, c≈-0.28) generalize across models and domains without re-estimation

## Next Checks
1. **Cross-domain robustness test**: Apply the gamma mixture fitting procedure to non-topical similarity tasks (semantic similarity, paraphrase detection, information retrieval) across diverse domains (biomedical, legal, social media) to test assumption validity
2. **Mechanism validation**: Design a controlled experiment where synthetic embeddings are generated with known topic hierarchies (varying η, tree depth) and verify that the empirical cosine similarity distributions match the predicted gamma mixture parameters
3. **Alternative distribution comparison**: Systematically compare shifted gamma mixtures against beta distributions (constrained to [-1,1]) and truncated normal distributions using proper information criteria (BIC/AIC) rather than visual inspection alone