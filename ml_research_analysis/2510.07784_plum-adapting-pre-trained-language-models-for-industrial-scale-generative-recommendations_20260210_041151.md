---
ver: rpa2
title: 'PLUM: Adapting Pre-trained Language Models for Industrial-scale Generative
  Recommendations'
arxiv_id: '2510.07784'
source_url: https://arxiv.org/abs/2510.07784
tags:
- training
- retrieval
- arxiv
- generative
- recommendation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PLUM adapts pre-trained language models for industrial-scale generative
  recommendations by integrating item tokenization using Semantic IDs, continued pre-training
  on domain-specific data, and fine-tuning for retrieval tasks. The framework addresses
  the challenge of bridging the domain gap between LLMs and recommendation systems
  by enriching models with user behavior and item corpus through continued pre-training.
---

# PLUM: Adapting Pre-trained Language Models for Industrial-scale Generative Recommendations

## Quick Facts
- **arXiv ID:** 2510.07784
- **Source URL:** https://arxiv.org/abs/2510.07784
- **Reference count:** 40
- **Primary result:** PLUM achieves 2.6x larger effective vocabulary and superior sample efficiency using generative retrieval with Semantic IDs

## Executive Summary
PLUM presents a framework for adapting pre-trained language models to industrial-scale generative recommendation systems. The approach addresses the challenge of bridging the domain gap between large language models and recommendation systems by using Semantic IDs (SIDs) as discrete representations of items. Through continued pre-training on domain-specific user behavior and item corpus data, PLUM enables LLMs to directly generate recommendations in the form of SID sequences, achieving substantial improvements over traditional embedding-based retrieval methods.

## Method Summary
The PLUM framework consists of three main stages: (1) generating Semantic IDs using a Residual Quantization VAE (RQ-VAE) that converts item content into hierarchical token sequences, (2) continued pre-training the LLM on a 50/50 mixture of user behavior sequences and item metadata to align SIDs with language, and (3) supervised fine-tuning for the retrieval task using reward-weighted sampling. The method leverages multi-modal content embeddings, hierarchical codebook refinements, and co-occurrence contrastive regularization to improve representation quality. During inference, beam search generates top-K SID sequences which are mapped back to actual items.

## Key Results
- Achieves 2.6x larger effective vocabulary compared to production embedding tables
- Uses less than 0.55x the training FLOPs while achieving superior performance
- Scaling studies show continued performance improvements up to 900M activated parameters in MoE models
- Substantial gains in retrieval performance over heavily-optimized production baselines on large-scale video datasets

## Why This Works (Mechanism)
PLUM works by transforming the recommendation problem from a nearest-neighbor search in embedding space to a generative sequence modeling task. By using SIDs as discrete item representations, the model learns to directly generate relevant items as sequences of tokens rather than searching through a large embedding table. The continued pre-training stage is crucial for aligning the general-purpose LLM with domain-specific user behavior patterns and item content, enabling it to understand the semantic relationships between user contexts and recommended items. The hierarchical nature of SIDs allows for both efficient representation and fine-grained item differentiation.

## Foundational Learning
- **Residual Quantization (RQ-VAE)**
  - Why needed: Core technique for creating Semantic IDs by recursively quantizing embeddings with hierarchical codebooks
  - Quick check: Given initial embedding `[0.9, 0.1]` and first-level centroid `[1.0, 0.0]`, what is the residual vector passed to the second-level quantizer?

- **Autoregressive Sequence Modeling (Decoder-only Transformer)**
  - Why needed: PLUM model predicts next SID token based on previous tokens and user context
  - Quick check: Why is causal attention mask essential in decoder-only models? How does this differ from BERT-style encoder?

- **Transfer Learning / Fine-tuning**
  - Why needed: Framework transfers knowledge from pre-trained LLM through continued pre-training and task-specific fine-tuning
  - Quick check: What's the key difference between CPT stage and final SFT stage in terms of training objective and data used?

## Architecture Onboarding

- **Component map:** Item Tokenizer (RQ-VAE) -> Base LLM -> Continued Pre-training (CPT) -> Supervised Fine-tuning (SFT) -> Inference Server

- **Critical path:**
  1. Generate SIDs using trained RQ-VAE for complete item corpus
  2. Expand base LLM vocabulary with new SID tokens
  3. Build massive CPT dataset mixing user behavior and item metadata
  4. Fine-tune on retrieval task using labeled user context and clicked item SIDs

- **Design tradeoffs:**
  - SID Length vs. Granularity: Longer SIDs reduce collisions but slow inference and increase training difficulty
  - CPT Data Mixture: 50/50 split between user behavior and item metadata affects model properties
  - Model Size vs. Performance: Larger models improve performance but require more compute; optimal size depends on compute budget

- **Failure signatures:**
  - High Hallucination Rate: Model generates invalid SID sequences (likely insufficient grounding during CPT)
  - Low Effective Vocabulary: Model recommends only popular items (likely insufficient CPT on diverse behaviors)
  - Poor SID-to-Item Mapping: Many items map to same SID (likely RQ-VAE codebooks too small or SID length too short)

- **First 3 experiments:**
  1. Validate RQ-VAE/SIDs: Train RQ-VAE, generate SIDs, report SID uniqueness (>95% target) and inspect reconstructions
  2. CPT Ablation: Train small model in three configurations (random init + SFT, LLM init + SFT, LLM + CPT + SFT) and compare recall
  3. Zero-Shot/Few-Shot Probing: Test CPT checkpoint's ability to associate SIDs with semantic content using prompts like "The video <sid_A> is about cooking..."

## Open Questions the Paper Calls Out
None

## Limitations
- Proprietary components like specific multi-modal encoders and handcrafted reward function limit exact replication
- Performance improvements are primarily demonstrated within Google's infrastructure and may not transfer directly to other domains
- Scaling study relies on specific MoE model configurations that may be difficult to reproduce without equivalent pre-trained checkpoints

## Confidence

**High Confidence:** Core architectural framework (RQ-VAE for SID generation, continued pre-training, autoregressive generative retrieval) is well-specified and theoretically sound with clear scaling laws and sample efficiency comparisons.

**Medium Confidence:** Performance improvements over production systems and effectiveness of proposed enhancements are supported by experiments but depend on undisclosed implementation details. Transferability to non-video domains requires validation.

**Low Confidence:** Precise replication requires access to proprietary components including pre-trained LLM checkpoints, multi-modal encoders, and exact reward function formulation. Production deployment details and real-world performance beyond controlled experiments are not fully specified.

## Next Checks
1. **RQ-VAE Quality Validation:** Rigorously evaluate SID generation quality on your dataset - measure uniqueness percentage (target >95%), inspect reconstruction quality, and calculate collision rates before proceeding with LLM fine-tuning.

2. **CPT Ablation Study Reproduction:** Implement three-stage ablation (random initialization + SFT, LLM initialization + SFT, LLM + CPT + SFT) on a subset using small model and measure recall@10 and effective vocabulary size to quantify continued pre-training value for your specific domain.

3. **Zero-Shot Semantic Association Test:** Take CPT checkpoint and perform few-shot probing with prompts like "The video <sid_A> is about cooking..." to validate whether CPT successfully aligned SIDs with semantic content before investing in full SFT training.