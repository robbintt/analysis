---
ver: rpa2
title: Constrained Black-Box Attacks Against Multi-Agent Reinforcement Learning
arxiv_id: '2508.09275'
source_url: https://arxiv.org/abs/2508.09275
tags:
- attack
- agents
- attacks
- hadamard
- align
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies black-box test-time adversarial attacks on\
  \ deployed cooperative multi-agent reinforcement learning (c-MARL) systems, where\
  \ the adversary can only perturb agents' observations without accessing policy weights\
  \ or actions. The core insight is that misalignment of agents' perceptions undermines\
  \ coordination\u2014by making agents perceive the environment inconsistently, the\
  \ team's collaborative performance degrades."
---

# Constrained Black-Box Attacks Against Multi-Agent Reinforcement Learning

## Quick Facts
- arXiv ID: 2508.09275
- Source URL: https://arxiv.org/abs/2508.09275
- Reference count: 40
- Primary result: Novel black-box adversarial attacks on c-MARL via observation perturbation, achieving up to 30% IQM return reduction with as few as 1,000 samples.

## Executive Summary
This paper introduces a constrained black-box attack framework for cooperative multi-agent reinforcement learning (c-MARL), where an adversary can only perturb agents' observations at test time without accessing policy weights or actions. The key insight is that misaligned perceptions between agents undermine coordination, causing collaborative performance to degrade. Two attack strategies are proposed: Align attack, which learns perturbations by maximizing a trained observation-alignment network's loss, and Hadamard attack, which uses structured perturbations from partial Hadamard matrices for scenarios with no access. The targeted variant selects vulnerable agents using the alignment network. Evaluated across 22 tasks from three benchmarks (LBF, RW ARE, SMAC), the attacks achieve significant performance drops with far fewer samples than prior methods, even in highly cooperative and partially observable settings.

## Method Summary
The paper proposes two main black-box attack strategies for c-MARL: Align and Hadamard. Align attack learns observation perturbations by maximizing the loss of a trained observation-alignment network, which predicts the impact of misaligned perceptions on team performance. This attack requires 1,000 samples and a pretrained alignment network. Hadamard attack uses structured perturbations derived from partial Hadamard matrices, enabling attacks with no access or training data. A targeted variant identifies vulnerable agents using the alignment network and applies focused perturbations. The attacks are evaluated in fully and partially observable, highly cooperative MARL settings across three benchmarks, showing substantial performance degradation (e.g., 20-30% IQM return reduction) with minimal samples compared to prior methods.

## Key Results
- Align and Hadamard attacks achieve up to 30% IQM return reduction across 22 tasks from three benchmarks.
- Attacks require as few as 1,000 samples, versus millions for prior methods.
- Effectiveness demonstrated in both fully and partially observable, highly cooperative settings, even with large agent counts and high-dimensional observations.

## Why This Works (Mechanism)
The attacks exploit the fact that misaligned perceptions between agents disrupt their ability to coordinate effectively. By perturbing agents' observations, the adversary forces agents to perceive the environment inconsistently, undermining the collaborative policies that depend on shared or aligned information. The Align attack directly targets this misalignment by maximizing the loss of a trained observation-alignment network, while the Hadamard attack uses structured perturbations to induce similar effects without requiring access to the alignment network.

## Foundational Learning
- **Cooperative MARL**: Agents collaborate to maximize a shared reward; needed to understand the attack's impact on teamwork.
- **Observation perturbation**: Adversary modifies agent inputs without accessing policy internals; needed for constrained black-box setting.
- **Observation alignment**: Predicting impact of perception misalignment on team performance; needed to guide targeted attacks.
- **Partial Hadamard matrices**: Structured perturbations for efficient black-box attacks; needed for attack with no access.
- **IQM (Interquartile Mean)**: Robust performance metric; needed to summarize results across tasks.
- **Test-time attacks**: Adversary acts during deployment, not training; needed to frame the threat model.

## Architecture Onboarding
**Component Map**: Observation space → Perturbation generator → Attack (Align/Hadamard) → Alignment network (for targeted variant) → Performance degradation

**Critical Path**: Observation perturbation → Misaligned perceptions → Coordination breakdown → Performance drop

**Design Tradeoffs**: Align attack trades sample efficiency for need for alignment network; Hadamard attack sacrifices some effectiveness for no access requirement.

**Failure Signatures**: Ineffective attacks in sparse-reward or competitive MARL; reduced impact when observation dimensions not powers of two; alignment network predictions become unreliable under attack.

**First Experiments**:
1. Test attack effectiveness in sparse-reward or competitive MARL scenarios.
2. Evaluate performance when observation dimensions are not powers of two.
3. Assess robustness of alignment network under active attack.

## Open Questions the Paper Calls Out
None

## Limitations
- Attack effectiveness not tested in sparse-reward or competitive MARL settings.
- Partial Hadamard matrix construction assumes observation dimension is a power of two.
- Alignment network's predictions may not remain robust under active attack.

## Confidence
- Core empirical findings: High (substantial performance degradation, well-supported ablation studies)
- Scalability to large agent counts: Medium (experiments limited to modest numbers of agents)
- Generalizability to other MARL settings: Medium (results in cooperative settings with dense rewards)

## Next Checks
1. Test the attacks in sparse-reward or competitive MARL scenarios to assess generalizability.
2. Evaluate attack performance when observation dimensions are not powers of two, using adaptive perturbation strategies.
3. Assess the robustness of the observation-alignment network when under active attack, to ensure its predictions remain meaningful.