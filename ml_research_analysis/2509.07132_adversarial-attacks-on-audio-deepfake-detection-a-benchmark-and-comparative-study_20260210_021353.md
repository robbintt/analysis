---
ver: rpa2
title: 'Adversarial Attacks on Audio Deepfake Detection: A Benchmark and Comparative
  Study'
arxiv_id: '2509.07132'
source_url: https://arxiv.org/abs/2509.07132
tags:
- attacks
- methods
- audio
- adds
- deepfake
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study provides the first comprehensive, large-scale comparative\
  \ analysis of audio deepfake detection (ADD) methods under adversarial anti-forensic\
  \ (AF) attacks, spanning both raw audio and spectrogram-based approaches. The work\
  \ evaluates twelve state-of-the-art ADD models across five benchmark datasets against\
  \ twelve diverse AF attacks\u2014four statistical (pitch shifting, filtering, noise\
  \ addition, quantization) and eight optimization-based (FGSM, PGD, C&W, DeepFool)."
---

# Adversarial Attacks on Audio Deepfake Detection: A Benchmark and Comparative Study

## Quick Facts
- arXiv ID: 2509.07132
- Source URL: https://arxiv.org/abs/2509.07132
- Reference count: 40
- One-line primary result: First comprehensive benchmark comparing 12 ADD methods across 5 datasets under 12 AF attacks, showing significant performance degradation and partial resilience via adversarial training.

## Executive Summary
This study presents the first large-scale comparative analysis of audio deepfake detection (ADD) methods under adversarial anti-forensic (AF) attacks. Twelve state-of-the-art ADD models are evaluated across five benchmark datasets against twelve diverse AF attacks, including both statistical (pitch shifting, filtering, noise addition, quantization) and optimization-based methods (FGSM, PGD, C&W, DeepFool). The results demonstrate that both raw audio and spectrogram-based ADD approaches suffer substantial performance degradation under adversarial conditions, with raw models achieving 0.75 AUC/0.30 EER and spectrogram models 0.86 AUC/0.19 EER on clean data but dropping significantly under attack. While adversarial training provides some resilience, the study highlights the urgent need for more robust and generalizable ADD systems capable of countering evolving AF techniques.

## Method Summary
The authors conducted a systematic benchmark evaluation of twelve ADD models across five benchmark datasets (ASVspoof2017, ASVspoof2019, DeepMine, VOCD, UADFV) under twelve adversarial AF attacks. Four statistical attacks (pitch shifting, filtering, noise addition, quantization) and eight optimization-based attacks (FGSM, PGD, C&W, DeepFool variants) were applied to both raw audio and spectrogram-based inputs. Performance was measured using AUC and EER metrics, with comparisons between baseline, attacked, and adversarially trained models. The study focused on quantifying the vulnerability of different ADD architectures to various AF attack strategies while identifying potential mitigation approaches.

## Key Results
- Raw audio ADD models achieved 0.75 AUC/0.30 EER on clean data but showed significant degradation under AF attacks
- Spectrogram-based models performed better on clean data with 0.86 AUC/0.19 EER but also suffered substantial performance drops under adversarial conditions
- Adversarial training provided partial resilience but failed to restore baseline performance levels
- Both statistical and optimization-based AF attacks were effective across all ADD model categories
- The study establishes a comprehensive benchmark framework for evaluating ADD robustness against adversarial threats

## Why This Works (Mechanism)
The effectiveness of AF attacks on ADD models stems from exploiting vulnerabilities in both raw audio and spectrogram feature representations. Statistical attacks manipulate basic audio properties like pitch, frequency content, and signal-to-noise ratio, while optimization-based attacks leverage gradient information to craft adversarial examples that maximize detection errors. The study demonstrates that ADD models, particularly those relying on spectrogram features, are susceptible to imperceptible perturbations that significantly degrade their ability to distinguish between real and synthetic audio content.

## Foundational Learning
- Audio Deepfake Detection (ADD): Why needed - to identify synthetic or manipulated audio content in security-critical applications; Quick check - evaluate model performance on clean vs. attacked datasets
- Anti-Forensic (AF) Attacks: Why needed - to understand and counter methods that evade detection systems; Quick check - test model robustness against multiple attack types
- Adversarial Training: Why needed - to improve model resilience against adversarial examples; Quick check - compare baseline vs. adversarially trained model performance
- Spectrogram vs. Raw Audio Processing: Why needed - different input representations affect model vulnerability; Quick check - analyze performance differences across input types
- AUC and EER Metrics: Why needed - standard evaluation metrics for detection system performance; Quick check - verify metric calculations across experimental conditions

## Architecture Onboarding
Component Map: Raw Audio Models -> AF Attacks -> Performance Degradation; Spectrogram Models -> AF Attacks -> Performance Degradation
Critical Path: Model Training -> Attack Application -> Performance Evaluation -> Adversarial Training -> Resilience Assessment
Design Tradeoffs: Raw models offer computational efficiency but lower baseline accuracy; spectrogram models provide higher accuracy but increased computational cost and vulnerability to certain attacks
Failure Signatures: Sharp performance drops under optimization-based attacks; consistent degradation across multiple attack types; adversarial training provides limited improvement
First 3 Experiments: 1) Baseline performance evaluation on clean datasets; 2) Performance assessment under individual AF attack types; 3) Adversarial training effectiveness comparison

## Open Questions the Paper Calls Out
The paper identifies several critical research directions for future work, including the development of more robust ADD architectures that can generalize across diverse attack types, investigation of real-time defense mechanisms for practical deployment scenarios, exploration of adaptive attack strategies that combine multiple techniques, and assessment of model performance in imbalanced or noisy real-world conditions. Additionally, the study suggests examining the trade-offs between detection accuracy on clean data and adversarial robustness, as well as evaluating the effectiveness of ensemble methods and hybrid approaches for improved resilience.

## Limitations
- Results may not generalize to ADD models and AF attacks beyond those included in the benchmark
- Evaluation assumes static AF attack methods rather than adaptive or combined attack strategies
- Study does not address trade-offs between robustness and detection accuracy on clean data
- Limited exploration of real-world deployment challenges and risk quantification

## Confidence
High: Performance degradation under AF attacks is well-supported by experimental results and comparative analysis
Medium: Adversarial training resilience findings are supported but generalizability to other scenarios is uncertain
Low: Urgency claims about robust ADD systems are reasonable but lack specific real-world risk quantification

## Next Checks
1. Test ADD model robustness against adaptive AF attacks that combine multiple techniques or evolve dynamically
2. Extend benchmark to include additional ADD architectures (e.g., transformer-based) and AF attacks (e.g., physical-world perturbations)
3. Investigate trade-offs between adversarial robustness and clean-data detection accuracy in imbalanced or noisy datasets