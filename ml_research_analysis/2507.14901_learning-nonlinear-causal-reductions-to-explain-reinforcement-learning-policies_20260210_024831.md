---
ver: rpa2
title: Learning Nonlinear Causal Reductions to Explain Reinforcement Learning Policies
arxiv_id: '2507.14901'
source_url: https://arxiv.org/abs/2507.14901
tags:
- causal
- high-level
- linear
- learning
- press
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces nonlinear Targeted Causal Reduction (nTCR)
  to explain reinforcement learning (RL) policies by treating agent-environment interactions
  as a low-level causal model and learning a simplified high-level causal model that
  preserves interventional consistency. The method introduces random perturbations
  to policy actions during execution and observes their effects on cumulative rewards,
  learning interpretable nonlinear reduction functions that summarize the most influential
  factors determining policy success or failure.
---

# Learning Nonlinear Causal Reductions to Explain Reinforcement Learning Policies

## Quick Facts
- arXiv ID: 2507.14901
- Source URL: https://arxiv.org/abs/2507.14901
- Authors: Armin Kekić; Jan Schneider; Dieter Büchler; Bernhard Schölkopf; Michel Besserve
- Reference count: 40
- Primary result: Introduces nTCR to explain RL policies via interventional consistency between low-level and high-level causal models

## Executive Summary
This paper introduces nonlinear Targeted Causal Reduction (nTCR) to explain reinforcement learning (RL) policies by treating agent-environment interactions as a low-level causal model and learning a simplified high-level causal model that preserves interventional consistency. The method introduces random perturbations to policy actions during execution and observes their effects on cumulative rewards, learning interpretable nonlinear reduction functions that summarize the most influential factors determining policy success or failure. Theoretical analysis establishes uniqueness and existence guarantees for exact solutions under specific conditions. Applied to pendulum swing-up and robot table tennis tasks, nTCR uncovers actionable insights about policy behavior and identifies specific factors driving success or failure.

## Method Summary
nTCR treats RL episodes as low-level causal models and learns nonlinear reduction functions mapping high-dimensional state/action/reward variables to interpretable low-dimensional causal explanations. The method introduces shift interventions (random perturbations) to policy actions, observes their effects on cumulative rewards, and learns reduction maps minimizing interventional consistency loss between low-level and high-level distributions. The framework uses Gaussian kernel-based interpretable functions for both reduction and intervention maps, with normality regularization to prevent overfitting in high-noise regimes. The approach combines causal abstraction learning with reinforcement learning policy explanation, providing actionable insights about which factors drive policy success.

## Key Results
- Synthetic experiments demonstrate nTCR's ability to identify ground-truth solutions with near-zero consistency loss, validating theoretical guarantees
- Pendulum task reveals unexpected policy biases (clockwise vs. counterclockwise motion performance differences) and identifies actionable improvements
- Robot table tennis simulation uncovers specific ball trajectories and racket positions leading to successful returns versus failures, identifying edge-case challenges

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shift interventions on policy actions create causal signal for learning interpretable reductions
- Mechanism: Random perturbations δAt ~ N(0, σt) are added to policy actions before execution. These "shift interventions" modify structural equations without replacing them entirely, allowing observation of how action deviations propagate to cumulative reward Y = Σ Rt.
- Core assumption: The low-level RL system can be modeled as an additive noise SCM where interventions have well-defined effects on downstream variables.
- Evidence anchors:
  - [abstract]: "We introduce random perturbations to policy actions during execution and observe their effects on the cumulative reward"
  - [section 3]: "Each action At selected by the policy is perturbed by a small random shift δAt ~ N(0, σt) before being executed"
  - [corpus]: Weak direct evidence; related work (Causal Policy Learning in RL) addresses confounding but not intervention-based explanation.
- Break condition: If interventions are too small relative to exogenous noise, signal-to-noise ratio becomes insufficient (see App. F.1, high-noise regime).

### Mechanism 2
- Claim: Approximate interventional consistency enables learning meaningful high-level causal explanations
- Mechanism: Reduction maps (τ, ω) are learned to minimize divergence between: (1) pushforward of low-level interventional distributions τ#[P(i)L] and (2) corresponding high-level model distributions P(ω(i))H. This makes the diagram in Fig. 1b approximately commutative.
- Core assumption: A simplified two-node high-level model (Z → Y) can capture the essential causal patterns affecting the target.
- Evidence anchors:
  - [abstract]: "ensuring learned explanations reflect meaningful causal patterns"
  - [section 2.3, Eq. 2.5]: Lcons(τ, ω, γ) = Ei~PI[D(P̂(i)τ(Y,Z) || P(ω(i))H(Y,Z))]
  - [corpus]: Causal Abstraction Inference under Lossy Representations discusses similar consistency requirements for abstraction learning.
- Break condition: If the intervention prior PI doesn't cover enough of the intervention space, multiple non-equivalent reductions may achieve low consistency loss (see Conjecture D.2).

### Mechanism 3
- Claim: Normality regularization prevents overfitting and improves identifiability in high-noise regimes
- Mechanism: A Wasserstein-1 distance between the standardized high-level cause distribution and N(0,1) is added to the loss: Lnorm = Ei~PI[W1(P̂(i)τ,std(Z), N(0,1))]. This encourages unimodal, interpretable cause distributions.
- Core assumption: The "true" high-level cause should have simple distributional properties for interpretability.
- Evidence anchors:
  - [section 4.1]: "normality regularization serves as an important inductive bias that prevents overfitting"
  - [Fig. D]: Shows ηnorm=1 substantially improves identification in high-noise settings but provides minimal benefit in low-noise settings
  - [corpus]: No direct corpus evidence for this specific regularization approach.
- Break condition: If intervention variance is high relative to noise, regularization may be unnecessary and could slightly constrain expressivity.

## Foundational Learning

- Concept: **Structural Causal Models (SCMs)**
  - Why needed here: The entire framework represents RL episodes as SCMs with structural equations, interventions, and exogenous noise. Without this, "shift intervention" and "interventional consistency" have no formal meaning.
  - Quick check question: Can you explain why Xj := fj(Paj, Uj) + il is a shift intervention rather than a hard intervention?

- Concept: **Pushforward Distributions**
  - Why needed here: The consistency objective requires computing τ#[P(i)L]—how low-level distributions transform under the reduction map. This is the core mathematical operation being optimized.
  - Quick check question: If X ~ N(0,1) and τ(x) = 2x, what is τ#[P(X)]?

- Concept: **Interventional vs. Observational Queries**
  - Why needed here: The method learns from perturbed (interventional) data, not observational trajectories. Understanding do-calculus distinction is essential for interpreting what the reduction actually captures.
  - Quick check question: Why can't we learn the same reduction from unperturbed episodes alone?

## Architecture Onboarding

- Component map:
  Low-level RL episode (S0,A0+δA0,R1,...,ST) → τ1 (Gaussian kernel-based, learnable weights wj,t) → High-level cause Z (scalar) → α (linear causal coefficient) → Target Y (cumulative reward)
  
  Low-level intervention (δA0,...,δAT-1) → ω1 (parallel structure to τ1) → High-level intervention J

- Critical path:
  1. Collect Nint interventions × Nep episodes per intervention with shift perturbations
  2. Initialize τ1, ω1 with Gaussian kernels spanning observed value ranges
  3. Optimize Ltotal = Lcons + ηnorm·Lnorm via Adam with cosine annealing
  4. Interpret: examine learned wj,t weights to identify which (feature, timestep) pairs drive explanations

- Design tradeoffs:
  - Kernel count (128 for Pendulum, 32 for Table Tennis): Higher = better resolution but more GPU memory
  - Smoothing parameter c: Higher = smoother functions but may miss sharp transitions
  - ηnorm: Higher = more Gaussian causes but potentially underfitting; crucial in high-noise regimes
  - Intervention strength σ: Must be strong enough for signal but not so strong that policy behavior degrades (see Fig. A)

- Failure signatures:
  - Consistency loss plateaus high: Function class may be insufficient; try more kernels or check intervention coverage
  - High-level cause becomes multi-modal (check CDF plots): Increase ηnorm
  - ω-map shows no clear patterns: Intervention variance may be too low relative to exogenous noise
  - Validation loss diverges: Reduce learning rate or increase weight decay

- First 3 experiments:
  1. Synthetic validation: Generate data from SCM following Prop. 4.2 structure; confirm identification loss Lid(τ) converges to near-zero (validates implementation)
  2. Pendulum directional bias: Train nTCR on Policy A data; verify τ-map distinguishes clockwise vs. counterclockwise trajectories (validates interpretability on known task)
  3. Ablation on ηnorm: Run Table Tennis with ηnorm ∈ {0, 0.1, 1, 10}; observe high-level cause CDF and identification metrics (determines regularization needs for your domain)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the uniqueness and existence guarantees for exact nonlinear transformations be extended to broader classes of causal models beyond the specific additive noise structures in Propositions 4.1 and 4.2?
- Basis in paper: [explicit] "extending these guarantees to more general model classes should be addressed by future work"
- Why unresolved: The current theoretical results rely on specific structural assumptions (additive noise with non-vanishing Fourier transform, linear-Gaussian constraints) that may not hold for arbitrary nonlinear causal systems.
- What evidence would resolve it: Proofs establishing identifiability conditions for more general nonlinear model classes, or counterexamples showing where uniqueness fails.

### Open Question 2
- Question: How can nTCR be adapted for discrete action spaces, which are common in many RL settings?
- Basis in paper: [explicit] "there are no fundamental restrictions preventing the use of other interventions more adapted to discrete action spaces, which are common in many RL settings. Those could be implemented as perturbations of the continuous parameters controlling the policy function"
- Why unresolved: The current framework uses shift interventions naturally suited to continuous actions; the proposed approach of perturbing policy parameters is mentioned but not implemented or validated.
- What evidence would resolve it: An extended nTCR framework with discrete-appropriate interventions, demonstrated on standard discrete-action RL benchmarks.

### Open Question 3
- Question: How should nTCR be extended to learn multiple high-level causes while maintaining identifiability and interpretability?
- Basis in paper: [inferred] Appendix A.2 presents the multiple-cause formulation but states it "would require the introduction of extra regularization term in order to constrain the method to separate distinct causes"
- Why unresolved: Without additional constraints, multiple learned causes may capture redundant or overlapping information, compromising interpretability.
- What evidence would resolve it: A regularized extension validated on tasks with known multiple causal factors, showing that learned causes capture distinct aspects of policy behavior.

### Open Question 4
- Question: Is Conjecture D.2 (infinitely many solutions when intervention prior vanishes on an open subset) formally provable?
- Basis in paper: [explicit] "While we could not derive an equivalent result for nonlinear TCR, we conjecture instead a negative result for the loss L_cons of linear TCR"
- Why unresolved: The conjecture is supported by intuitive reasoning about Fourier domain constraints but lacks formal proof.
- What evidence would resolve it: A formal proof establishing conditions under which non-identifiability arises, or demonstration that the conjectured non-uniqueness does not hold.

## Limitations
- Synthetic validation focuses on exact solutions rather than robustness to model misspecification or distributional shifts, potentially limiting generalizability
- Effectiveness depends critically on appropriate intervention variance selection, with qualitative guidance but lacking systematic selection method
- Two-node high-level model may not capture complex multi-factor causal relationships in more complex RL domains

## Confidence

- **High confidence**: The theoretical framework connecting interventional consistency to causal abstraction learning is sound, with clear mathematical foundations in SCM theory and pushforward distributions.
- **Medium confidence**: Empirical demonstrations on Pendulum and Table Tennis show interpretable discoveries, but sample sizes and domain complexity are limited.
- **Low confidence**: The normality regularization's effectiveness appears highly dependent on intervention noise levels, with limited guidance on when this inductive bias helps versus hurts performance.

## Next Checks

1. **Robustness to model misspecification**: Apply nTCR to synthetic SCMs where the true causal structure violates additive noise assumptions (e.g., multiplicative interactions). Measure consistency loss and compare to ground truth to quantify sensitivity.

2. **Intervention variance sensitivity analysis**: Systematically vary σ across multiple orders of magnitude on Pendulum task. Plot consistency loss, identification metrics, and discovered explanations to identify optimal intervention strength ranges for different noise regimes.

3. **Scalability test on multi-factor domains**: Apply nTCR to a domain with multiple independent causal factors (e.g., a grid-world with distinct obstacle types affecting success differently). Verify whether the two-node abstraction can capture distinct causal mechanisms or requires architectural extensions.