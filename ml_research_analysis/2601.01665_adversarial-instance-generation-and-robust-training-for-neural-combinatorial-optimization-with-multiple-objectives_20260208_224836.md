---
ver: rpa2
title: Adversarial Instance Generation and Robust Training for Neural Combinatorial
  Optimization with Multiple Objectives
arxiv_id: '2601.01665'
source_url: https://arxiv.org/abs/2601.01665
tags:
- instances
- neural
- solvers
- preference
- hard
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of improving the robustness and
  out-of-distribution generalization of deep reinforcement learning (DRL) solvers
  for multi-objective combinatorial optimization problems (MOCOPs). The key contributions
  are a preference-based adversarial attack (PAA) that generates hard instances tailored
  to specific preferences, and a dynamic preference-augmented defense (DPD) that integrates
  hardness-aware preference selection into adversarial training to reduce overfitting
  and improve generalization.
---

# Adversarial Instance Generation and Robust Training for Neural Combinatorial Optimization with Multiple Objectives

## Quick Facts
- arXiv ID: 2601.01665
- Source URL: https://arxiv.org/abs/2601.01665
- Reference count: 36
- Primary result: Preference-based adversarial attack (PAA) generates hard instances that reduce solver hypervolume and optimality gaps; dynamic preference-augmented defense (DPD) improves robustness and out-of-distribution generalization.

## Executive Summary
This paper addresses the challenge of improving the robustness and generalization of deep reinforcement learning (DRL) solvers for multi-objective combinatorial optimization problems (MOCOPs). The authors propose a novel framework consisting of two components: a preference-based adversarial attack (PAA) that generates hard instances tailored to specific preferences, and a dynamic preference-augmented defense (DPD) that integrates hardness-aware preference selection into adversarial training. PAA exposes solver vulnerabilities by generating problem distributions that significantly reduce hypervolume values and increase optimality gaps. DPD enhances solver performance across various problems by reducing overfitting and improving out-of-distribution generalization, particularly for instances generated via Gaussian mixture models.

## Method Summary
The framework generates adversarially hard instances through preference-conditioned gradient ascent on the solver's reinforcement loss function, iteratively updating instance features to maximize the loss while projecting results back to feasible space. The defense component combines clean and adversarially generated instances in training, using a hardness-aware preference selection mechanism that identifies and focuses on the solver's weakest preference regions during optimization. The training process employs REINFORCE updates with baseline subtraction, operating on a mixture of clean and hard instances to improve generalization across the entire Pareto front.

## Key Results
- PAA successfully generates problem distributions that significantly reduce hypervolume (HV) values and increase optimality gaps compared to baseline methods
- DPD consistently enhances the performance of neural solvers across MOTSP, MOCVRP, and MOKP, achieving improvements in HV values and reducing optimality gaps
- DPD demonstrates superior out-of-distribution generalization, particularly for Gaussian test distributions with varying c_DIST parameters
- The framework maintains computational efficiency while providing robustness gains across multiple problem types

## Why This Works (Mechanism)

### Mechanism 1: Preference-Conditioned Gradient Ascent (PAA)
Optimizing instance perturbations against specific scalarization preferences effectively exposes solver blind spots by degrading performance on targeted subproblems, which in turn affects the overall Pareto front quality.

### Mechanism 2: Hardness-Aware Preference Selection (DPD)
Dynamically selecting training preferences based on observed solver weakness reduces overfitting to restricted preference regions by focusing optimization capacity on the weakest areas of the Pareto front.

### Mechanism 3: Adversarial Distribution Mixing
Training on a mixture of clean and adversarially generated instances creates a curriculum that improves out-of-distribution generalization by forcing the model to learn features robust to the perturbations introduced by PAA.

## Foundational Learning

- **Concept: Pareto Dominance and Scalarization**
  - Why needed: The paper relies on decomposing MOCOPs into single-objective subproblems using preference vectors via Weighted Sum or Tchebycheff methods
  - Quick check: Can you explain how a Tchebycheff scalarization function converts a multi-objective vector into a single scalar loss?

- **Concept: Policy Gradient (REINFORCE)**
  - Why needed: The underlying neural solvers are trained using RL (specifically REINFORCE with a baseline), and the attack method computes gradients based on the RL loss
  - Quick check: In Eq. (4), why is the baseline b(x) subtracted from the loss L(π|x) before multiplying by the log-probability?

- **Concept: Hypervolume (HV) Indicator**
  - Why needed: This is the primary metric for evaluating the quality of the Pareto front approximation (convergence and diversity)
  - Quick check: If a solver achieves a lower Hypervolume (HV) gap compared to a baseline, does that mean its performance is better or worse?

## Architecture Onboarding

- **Component map:**
  - PAA: Input Instance -> Gradient Calculator (w.r.t Loss ℓ) -> Iterative Perturbation (α · ∇) -> Projection (Π_N) -> Hard Instance x^(t+1)
  - DPD: Hard Instance Set -> Preference Sampler (λ) -> Perturbative Generator (λ') -> Hardness Evaluator (Tch Value) -> Softmax Selector -> REINFORCE Update

- **Critical path:** The generation of valid "hard" instances (PAA) is the prerequisite for the Defense training. If PAA fails to degrade the solver's performance (verified by HV drop), the DPD training will not differ significantly from standard training.

- **Design tradeoffs:**
  - Attack Intensity: Increasing gradient steps (t) or step size (α) creates harder instances but risks generating infeasible or unrealistic problem data
  - Preference Scope: Sampling more preferences yields better coverage but increases computational overhead for the attack generation phase

- **Failure signatures:**
  - Stagnant HV: If PAA does not reduce Hypervolume during the attack phase, check the projection function Π_N or step size α
  - Overfitting: If DPD improves performance on generated hard instances but fails on Gaussian/OOD test sets, the perturbation range (ε) for preferences may be too narrow

- **First 3 experiments:**
  1. Verify PAA Efficacy: Run PAA against a pre-trained solver on 50-node TSP instances and plot HV degradation vs. gradient iteration t
  2. Ablation on Preference Selection: Train two versions of DPD (with vs. without hardness-aware selection) and compare their convergence speed and final HV on Gaussian test distributions
  3. Generalization Test: Train a solver from scratch using DPD and compare its performance against a baseline solver on provided benchmarks (e.g., kroAB100)

## Open Questions the Paper Calls Out

### Open Question 1
Can the proposed framework be effectively adapted for dynamic real-world MOCOP instances where problem parameters change over time? The current study validates the framework solely on static benchmark problems, while the authors aim to extend their method to address dynamic real-world instances.

### Open Question 2
How does the integration of complex, domain-specific constraints affect the performance of the Preference-based Adversarial Attack and Defense methods? The experiments utilize standard problem formulations without complex, real-world restrictions such as time windows or loading constraints.

### Open Question 3
What specific mechanisms are required to maintain robustness and generalizability in strictly online environments? The current Dynamic Preference-augmented Defense relies on an adversarial training phase that assumes a semi-offline training regimen; continuous online adaptation is not addressed.

## Limitations
- The core mechanism for generating adversarially hard instances has not been extensively tested across different solver architectures or alternative preference scalarization methods
- Computational overhead of generating hard instances via PAA may be prohibitive for large-scale problems
- The paper lacks thorough exploration of the robustness of the preference-conditioned gradient attack approach

## Confidence

- **High confidence:** Experimental results demonstrating improved HV values and reduced optimality gaps for DPD across multiple MOCOP benchmarks
- **Medium confidence:** Theoretical framework for PAA's preference-conditioned gradient ascent, as the paper lacks extensive ablation studies on attack parameters
- **Medium confidence:** Hardness-aware preference selection mechanism in DPD, as the paper does not provide evidence of its superiority over simpler selection strategies

## Next Checks

1. **Ablation on attack parameters:** Systematically vary the number of gradient steps (t) and step size (α) in PAA to determine their impact on instance hardness and solver degradation

2. **Solver architecture invariance:** Test PAA against different neural solver architectures (e.g., transformer-based vs. pointer network-based) to assess the generality of the attack

3. **Alternative preference selection:** Implement and compare DPD with a simpler preference selection strategy (e.g., uniform sampling) to isolate the contribution of the hardness-aware mechanism