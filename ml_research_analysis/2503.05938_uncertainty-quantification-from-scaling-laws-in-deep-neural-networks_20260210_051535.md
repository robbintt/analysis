---
ver: rpa2
title: Uncertainty Quantification From Scaling Laws in Deep Neural Networks
arxiv_id: '2503.05938'
source_url: https://arxiv.org/abs/2503.05938
tags:
- scaling
- neural
- infinite-width
- networks
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work addresses uncertainty quantification (UQ) in deep neural
  networks by examining the scaling behavior of mean test loss and its variance as
  a function of training set size. The authors compute these quantities analytically
  in the infinite-width limit using neural tangent kernel (NTK) initialization, and
  compare with finite-width network experiments on three tasks: MNIST classification,
  CIFAR classification, and calorimeter energy regression.'
---

# Uncertainty Quantification From Scaling Laws in Deep Neural Networks

## Quick Facts
- arXiv ID: 2503.05938
- Source URL: https://arxiv.org/abs/2503.05938
- Authors: Ibrahim Elsharkawy; Yonatan Kahn; Benjamin Hooberman
- Reference count: 0
- Primary result: Shows coefficient of variation in neural network loss becomes independent of dataset size for large datasets, enabling scalable uncertainty quantification

## Executive Summary
This work addresses uncertainty quantification (UQ) in deep neural networks by examining the scaling behavior of mean test loss and its variance as a function of training set size. The authors compute these quantities analytically in the infinite-width limit using neural tangent kernel (NTK) initialization, and compare with finite-width network experiments on three tasks: MNIST classification, CIFAR classification, and calorimeter energy regression. They find that while the mean loss scales predictably with dataset size, the coefficient of variation (standard deviation divided by mean) becomes independent of dataset size for sufficiently large datasets. This scaling behavior is observed in both infinite and finite-width networks, suggesting that the coefficient of variation may be approximated by its infinite-width value.

## Method Summary
The authors employ a combination of analytical derivations and empirical validation to study uncertainty quantification in deep neural networks. They leverage the neural tangent kernel (NTK) framework to compute the mean test loss and its variance analytically in the infinite-width limit. The NTK initialization regime assumes networks with near-zero initialization, allowing for tractable analysis of the scaling behavior. Empirical validation is performed using finite-width networks trained on MNIST, CIFAR, and calorimeter energy regression tasks. The authors compute the coefficient of variation across varying dataset sizes and network widths to validate their theoretical predictions.

## Key Results
- Mean test loss scales predictably with dataset size, following expected inverse relationships
- Coefficient of variation (standard deviation/mean) becomes independent of dataset size for large datasets
- Scaling behavior observed in both infinite-width (analytical) and finite-width (empirical) networks
- Results show relative insensitivity to learning rate hyperparameters
- Three benchmark tasks (MNIST, CIFAR, calorimeter regression) demonstrate consistent scaling patterns

## Why This Works (Mechanism)
The scaling laws emerge from the interplay between network expressivity and dataset statistics in the NTK regime. As dataset size increases, the mean loss decreases predictably due to better coverage of the input space. However, the variance structure stabilizes because the NTK inverse elements scale in a way that their product with the loss variance becomes dataset-size independent. This occurs because the NTK matrix captures the effective dimensionality of the function space, and as more data points are added, the relative contribution of each to the overall uncertainty stabilizes.

## Foundational Learning
- **Neural Tangent Kernel (NTK)**: Framework for analyzing infinitely wide neural networks during training. Needed to derive analytical scaling laws; quick check: verify NTK matrix inversion properties.
- **Mean Squared Error (MSE) decomposition**: Breaking down prediction error into bias and variance components. Needed to understand uncertainty sources; quick check: confirm MSE scaling with dataset size.
- **Coefficient of Variation**: Ratio of standard deviation to mean, providing scale-invariant uncertainty measure. Needed to identify dataset-size independent behavior; quick check: verify CV stability across dataset sizes.

## Architecture Onboarding
- **Component map**: NTK initialization -> Analytical scaling derivation -> Empirical validation on benchmark tasks
- **Critical path**: The theoretical prediction that CV becomes dataset-size independent is the central claim requiring both analytical derivation and empirical verification
- **Design tradeoffs**: Infinite-width analytical tractability vs. finite-width practical relevance; NTK initialization vs. practical initialization schemes
- **Failure signatures**: If CV continues scaling with dataset size, the infinite-width approximation breaks down; if mean loss doesn't follow expected scaling, NTK assumptions may not hold
- **First experiments**: 1) Verify mean loss scaling with dataset size on CIFAR; 2) Check CV stability across different network widths on MNIST; 3) Test learning rate sensitivity on calorimeter regression task

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis restricted to NTK initialization regime, which may not generalize to practical initialization schemes
- Analytical derivations rely on near-zero initialization assumptions that may limit practical applicability
- Empirical validation covers only three standard benchmark tasks with limited architectural diversity
- Claim about finite-width effects becoming negligible for large datasets needs stronger empirical support across broader domains

## Confidence
- High confidence: The scaling law behavior for mean loss and its relationship to dataset size is well-established and mathematically rigorous within the NTK framework
- Medium confidence: The claim that coefficient of variation becomes dataset-size independent is supported empirically but lacks rigorous theoretical justification for finite-width networks
- Medium confidence: The insensitivity to learning rate is demonstrated empirically but requires more systematic hyperparameter studies

## Next Checks
1. Test the scaling laws across a broader range of initialization schemes beyond NTK to assess generalizability
2. Validate the coefficient of variation predictions on diverse architectures including modern CNN and transformer-based networks
3. Conduct systematic experiments varying width-depth tradeoffs to quantify when finite-width corrections become significant for UQ