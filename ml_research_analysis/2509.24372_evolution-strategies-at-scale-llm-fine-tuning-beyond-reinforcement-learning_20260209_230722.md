---
ver: rpa2
title: 'Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning'
arxiv_id: '2509.24372'
source_url: https://arxiv.org/abs/2509.24372
tags:
- fine-tuning
- uni00a0
- learning
- reward
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Evolution strategies (ES) were successfully scaled to fine-tune
  the full parameters of billion-parameter large language models (LLMs), demonstrating
  that ES can efficiently optimize in extremely high-dimensional spaces and outperform
  established reinforcement learning (RL) methods in multiple respects: sample efficiency,
  robustness to long-horizon rewards, stability across runs, and reduced susceptibility
  to reward hacking. In the Countdown task with sparse outcome-only rewards, ES achieved
  substantially better accuracy than RL across various model families and sizes.'
---

# Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning

## Quick Facts
- arXiv ID: 2509.24372
- Source URL: https://arxiv.org/abs/2509.24372
- Reference count: 40
- Primary result: Evolution strategies scaled to fine-tune billion-parameter LLMs, outperforming RL on sample efficiency, stability, and robustness to reward hacking

## Executive Summary
This paper demonstrates that evolution strategies (ES) can be effectively scaled to fine-tune the full parameters of large language models (LLMs), challenging the dominance of gradient-based reinforcement learning methods. The authors show that ES achieves superior performance on multiple tasks including sparse reward optimization, conciseness fine-tuning, and math reasoning, while exhibiting greater stability across runs and resistance to reward hacking. The work establishes ES as a viable backpropagation-free alternative for LLM post-training, particularly valuable for tasks where traditional RL struggles with sample efficiency or long-horizon credit assignment.

## Method Summary
The paper scales evolution strategies to full-parameter LLM fine-tuning by implementing a distributed ES framework that handles the computational challenges of billion-parameter optimization. The approach uses population-based search with gradient-free updates, where multiple perturbed model copies are evaluated in parallel, and parameter updates are computed from the performance distribution across the population. The method operates without backpropagation, relying instead on perturbation-based gradient estimation through finite differences across the parameter space. The framework was tested across various model families and sizes, demonstrating that ES can efficiently navigate extremely high-dimensional optimization landscapes while maintaining stability and avoiding common RL failure modes like reward hacking and KL divergence collapse.

## Key Results
- ES achieved substantially better accuracy than RL on the sparse reward Countdown task across multiple model families and sizes
- In conciseness fine-tuning, ES found superior Pareto frontiers without reward hacking or requiring KL divergence penalties
- ES matched or exceeded state-of-the-art RL baselines on math reasoning benchmarks and successfully solved challenging puzzle tasks (ARC-AGI, Sudoku) that base models failed on

## Why This Works (Mechanism)
Evolution strategies work by maintaining a population of perturbed model parameters and using their collective performance to estimate gradients through finite differences. This gradient-free approach avoids the vanishing/exploding gradient problems common in RL while providing inherent exploration through parameter perturbations. The population-based evaluation provides natural credit assignment across sequences, making it particularly effective for sparse reward tasks where traditional RL struggles with long-horizon credit assignment. The method's robustness to reward hacking stems from its global parameter optimization approach that considers the full reward landscape rather than optimizing for specific reward signals that can be exploited.

## Foundational Learning

1. **Evolution Strategies (ES)**: Population-based optimization method using parameter perturbations and performance distributions
   - Why needed: Provides gradient-free optimization suitable for extremely high-dimensional spaces
   - Quick check: Verify population size scales appropriately with parameter count

2. **Finite Difference Gradient Estimation**: Approximates gradients by evaluating performance differences across perturbed parameters
   - Why needed: Enables optimization without backpropagation in non-differentiable reward landscapes
   - Quick check: Confirm gradient estimates converge as population size increases

3. **Reward Hacking**: Models exploiting reward function loopholes rather than solving intended task
   - Why needed: Critical failure mode in RL that ES naturally avoids
   - Quick check: Monitor for degenerate solutions that maximize reward without task completion

4. **KL Divergence Regularization**: Constrains model updates to prevent drift from base model
   - Why needed: Prevents semantic drift in fine-tuning while maintaining task performance
   - Quick check: Measure semantic similarity between base and fine-tuned models

5. **Sparse Reward Optimization**: Learning from outcome-only feedback without intermediate guidance
   - Why needed: Common in real-world applications where dense rewards are unavailable
   - Quick check: Evaluate performance on tasks with varying reward sparsity levels

6. **Population-based Search**: Maintaining multiple model variants simultaneously for exploration
   - Why needed: Provides natural exploration and stability through ensemble averaging
   - Quick check: Compare single-model vs population-based performance curves

## Architecture Onboarding

**Component Map**: Data → ES Population Generator → Parallel Evaluator → Performance Aggregator → Parameter Updater → Model

**Critical Path**: Model initialization → Population perturbation → Parallel evaluation → Performance aggregation → Parameter update → Model checkpoint

**Design Tradeoffs**: ES trades computational efficiency (requiring multiple model evaluations per update) for gradient-free optimization and inherent exploration. The population-based approach provides stability but increases resource requirements compared to single-path optimization.

**Failure Signatures**: Degenerate reward maximization without task completion, parameter divergence without performance improvement, population collapse to suboptimal solutions, and excessive computational overhead from large population sizes.

**Three First Experiments**:
1. Compare single-model vs population-based performance on simple optimization tasks to verify population benefits
2. Measure gradient estimation accuracy with varying population sizes and perturbation magnitudes
3. Test convergence speed on dense reward tasks versus sparse reward tasks to characterize credit assignment properties

## Open Questions the Paper Calls Out
None provided in source material.

## Limitations
- Long-horizon scaling properties remain unproven, with sample efficiency degradation possible for tasks requiring hundreds of sequential decisions
- ES stability advantages lack statistical significance testing across independent runs
- Absence of KL divergence penalties introduces risk of arbitrary semantic drift from base models
- Performance comparisons may be influenced by implementation differences beyond the core optimization algorithm

## Confidence
- High confidence: ES can scale to full-parameter LLM fine-tuning in high-dimensional spaces
- Medium confidence: ES outperforms RL on sample efficiency and short-to-medium horizon tasks
- Low confidence: ES generalizes to long-horizon tasks without efficiency degradation
- Medium confidence: ES is inherently more robust to reward hacking than RL
- Low confidence: ES stability advantages are statistically significant across all tested conditions

## Next Checks
1. Benchmark ES fine-tuning on tasks with explicit long-horizon requirements (e.g., 50+ step reasoning chains) and measure sample efficiency degradation compared to RL baselines
2. Implement KL divergence monitoring in ES fine-tuning to quantify semantic drift from base models and establish drift thresholds for practical deployment
3. Conduct ablation studies varying ES hyperparameters (population size, mutation rates, selection pressure) to identify which factors contribute most to the reported stability advantages over RL