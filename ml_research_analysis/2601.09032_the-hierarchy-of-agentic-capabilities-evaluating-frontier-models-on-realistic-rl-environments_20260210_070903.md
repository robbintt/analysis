---
ver: rpa2
title: 'The Hierarchy of Agentic Capabilities: Evaluating Frontier Models on Realistic
  RL Environments'
arxiv_id: '2601.09032'
source_url: https://arxiv.org/abs/2601.09032
tags:
- tasks
- agents
- evaluation
- tool
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates frontier AI models on 150 realistic workplace
  tasks within an e-commerce RL environment, identifying a hierarchy of five agentic
  capabilities: tool use, planning, adaptability, groundedness, and common-sense reasoning.
  Even the best-performing models fail approximately 40% of tasks, with weaker models
  failing at basic tool use and planning, while stronger models primarily struggle
  with common-sense reasoning requiring contextual inference.'
---

# The Hierarchy of Agentic Capabilities: Evaluating Frontier Models on Realistic RL Environments

## Quick Facts
- arXiv ID: 2601.09032
- Source URL: https://arxiv.org/abs/2601.09032
- Reference count: 2
- Key outcome: Frontier AI models evaluated on 150 e-commerce tasks show a five-tier hierarchy of agentic capabilities, with even best models failing ~40% of tasks primarily due to common-sense reasoning gaps

## Executive Summary
This paper introduces a comprehensive evaluation framework for frontier AI models using a realistic e-commerce RL environment with 150 workplace tasks. The study identifies a hierarchical structure of agentic capabilities ranging from basic tool use to complex common-sense reasoning, revealing that current models struggle significantly even on realistic business scenarios. The research demonstrates that while stronger models excel at tool use and planning, they still fail approximately 40% of tasks, with common-sense reasoning representing the most challenging capability across all model tiers.

## Method Summary
The authors developed a task-centric RL environment featuring 150 diverse e-commerce tasks designed with domain expert input. They evaluated multiple frontier models across a five-tier hierarchy of agentic capabilities: tool use, planning, adaptability, groundedness, and common-sense reasoning. The evaluation methodology emphasized task diversity and realistic workplace scenarios, with detailed failure analysis to identify capability gaps. The environment design prioritized realistic workflows over synthetic benchmarks, allowing for nuanced assessment of model limitations in practical contexts.

## Key Results
- Frontier models fail approximately 40% of realistic e-commerce tasks, with weaker models struggling at basic tool use and planning while stronger models primarily fail at common-sense reasoning
- A clear hierarchy of agentic capabilities emerged: tool use → planning → adaptability → groundedness → common-sense reasoning
- Domain expert contributions significantly enhanced task diversity and environmental realism, producing more actionable insights than synthetic benchmark approaches

## Why This Works (Mechanism)
The hierarchical capability framework reveals that agent performance degrades progressively as task complexity increases, with each tier building upon lower-level competencies. Tool use failures indicate fundamental API invocation problems, while planning failures suggest inadequate task decomposition skills. Adaptability failures reveal context-switching limitations, groundedness failures point to state-tracking deficiencies, and common-sense reasoning failures expose the most fundamental gaps in human-like understanding. This tiered approach allows precise identification of where models break down rather than just measuring aggregate performance.

## Foundational Learning
- **RL environment design principles**: Task-centric approach with domain expert input creates more realistic evaluation scenarios than synthetic benchmarks
- **Agent capability hierarchy**: Understanding that capabilities build progressively (tool use → planning → adaptability → groundedness → common-sense reasoning) enables targeted model improvement
- **Failure mode analysis**: Detailed examination of task failures reveals specific capability gaps rather than generic performance metrics
- **Model tier differentiation**: Different model families exhibit distinct failure patterns across the capability hierarchy, informing selection for specific applications
- **Environmental realism**: E-commerce workflows provide practical context for evaluating agent performance on workplace tasks
- **Multi-step task evaluation**: Complex workflows better reveal agent limitations than single-step interactions

## Architecture Onboarding

**Component Map**: Task Environment -> Agent Models -> Capability Hierarchy -> Failure Analysis

**Critical Path**: Task generation → Model evaluation → Capability assessment → Failure categorization → Improvement recommendations

**Design Tradeoffs**: 
- Task diversity vs. environmental complexity: More varied tasks improve generalization but increase evaluation overhead
- Realistic workflows vs. controlled conditions: Real-world scenarios provide practical insights but introduce uncontrolled variables
- Granular failure analysis vs. aggregate metrics: Detailed failure categorization enables targeted improvements but requires more sophisticated analysis

**Failure Signatures**: 
- Basic tool use failures: Inability to invoke correct APIs or utilities
- Planning failures: Incorrect task decomposition or step ordering
- Adaptability failures: Inflexibility when conditions change mid-task
- Groundedness failures: Loss of context or misunderstanding of current state
- Common-sense reasoning failures: Inability to make contextually appropriate inferences

**First Experiments**:
1. Evaluate a single model across all 150 tasks to establish baseline performance and identify specific failure patterns
2. Compare model performance on tool use-only tasks versus common-sense reasoning tasks to validate the capability hierarchy
3. Test model adaptability by introducing controlled environmental changes mid-task and measuring response quality

## Open Questions the Paper Calls Out
- How can the identified capability hierarchy be leveraged to guide targeted improvements in frontier models?
- What architectural modifications would most effectively address common-sense reasoning failures at the highest capability tier?
- Can the task-centric evaluation framework be adapted for other domains beyond e-commerce while maintaining environmental realism?
- How do different model architectures (transformer variants, symbolic integration approaches) perform across the capability hierarchy?

## Limitations
- Evaluation confined to e-commerce scenarios may not generalize across industries
- 150 curated tasks may miss edge cases and rare but critical failure modes
- Focus on specific frontier models may introduce selection bias in capability assessment
- Common-sense reasoning evaluation relies on subjective interpretation of task success
- Environmental complexity may obscure fundamental capability differences between models

## Confidence
- **High confidence**: Task failure rates across model categories, basic tool use and planning deficiencies, task diversity methodology
- **Medium confidence**: The five-capability hierarchy framework, adaptability measurement, groundedness assessment
- **Medium confidence**: Common-sense reasoning failure patterns, domain expert contribution value, environmental design principles

## Next Checks
1. **Cross-domain validation**: Replicate the evaluation framework across 2-3 different industry verticals (e.g., healthcare, finance, manufacturing) to test the universality of the capability hierarchy and identify domain-specific failure modes.

2. **Long-horizon task assessment**: Extend task duration beyond current multi-step limits to evaluate agent performance on workflows requiring sustained context retention and strategic planning over extended time periods.

3. **Human-AI collaboration evaluation**: Design mixed-initiative scenarios where human oversight and agent autonomy interact, measuring how different capability levels affect team performance and identifying optimal division of labor between human experts and AI agents.