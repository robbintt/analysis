---
ver: rpa2
title: 'Outbidding and Outbluffing Elite Humans: Mastering Liar''s Poker via Self-Play
  and Reinforcement Learning'
arxiv_id: '2511.03724'
source_url: https://arxiv.org/abs/2511.03724
tags:
- player
- solly
- game
- poker
- players
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Solly, the first AI agent to achieve elite
  human-level play in reduced-format Liar's Poker, a multi-player imperfect-information
  game. The authors trained Solly using self-play with a model-free actor-critic deep
  reinforcement learning algorithm based on regularized Nash dynamics.
---

# Outbidding and Outbluffing Elite Humans: Mastering Liar's Poker via Self-Play and Reinforcement Learning

## Quick Facts
- arXiv ID: 2511.03724
- Source URL: https://arxiv.org/abs/2511.03724
- Authors: Richard Dewey; Janos Botyanszki; Ciamac C. Moallemi; Andrew T. Zheng
- Reference count: 12
- Solly is the first AI to achieve elite human-level play in reduced-format Liar's Poker

## Executive Summary
This paper introduces Solly, the first AI agent to achieve elite human-level performance in reduced-format Liar's Poker, a multi-player imperfect-information game. Trained through self-play using a model-free actor-critic deep reinforcement learning algorithm based on regularized Nash dynamics, Solly demonstrates that strong performance in complex strategic games can be achieved without extensive search algorithms or massive computational resources. The agent successfully outbids and outbluffs elite human players, winning 48-55% of hands in direct competition and outperforming large language models.

The research challenges the conventional wisdom that mastering imperfect-information games requires complex search mechanisms or specialized architectures. Instead, Solly's success with a relatively straightforward reinforcement learning approach suggests that self-play can effectively navigate the strategic complexities of bluffing and counter-bluffing in multi-agent settings. The work contributes to the broader understanding of how AI can handle games requiring both probabilistic reasoning and strategic deception.

## Method Summary
Solly was trained using a model-free actor-critic deep reinforcement learning algorithm based on regularized Nash dynamics, employing self-play as the primary training methodology. The agent learned through repeated interactions with copies of itself, gradually refining its bidding and bluffing strategies in the reduced-format Liar's Poker environment. This approach allowed Solly to develop sophisticated gameplay patterns without requiring explicit search algorithms or hand-crafted strategies, demonstrating that effective learning in imperfect-information games can emerge from simple self-play dynamics.

## Key Results
- Solly achieved elite human-level performance, winning over 50% of hands against elite human players
- The agent won 48-55% of hands against elite humans in various configurations, demonstrating consistent superiority
- Solly outperformed large language models, winning 60% of heads-up games against GPT-4.1 and 55% against OpenAI o3

## Why This Works (Mechanism)
Solly's success stems from the regularized Nash dynamics framework that guides the actor-critic learning process. This mechanism ensures stable learning by preventing extreme policy updates that could destabilize training. The self-play framework creates a curriculum where the agent faces increasingly sophisticated versions of itself, naturally discovering optimal bluffing frequencies and bidding strategies. The model-free approach avoids the computational complexity of search-based methods while still capturing the strategic depth required for high-level play.

## Foundational Learning
- **Regularized Nash Dynamics**: Provides stability in multi-agent learning by smoothing policy updates - needed to prevent catastrophic forgetting during self-play, quick check: verify KL divergence constraints are properly enforced
- **Actor-Critic Architecture**: Separates policy (actor) and value estimation (critic) for stable learning - needed to balance exploration and exploitation, quick check: monitor actor-critic loss ratio during training
- **Self-Play Curriculum**: Generates progressively challenging opponents through iterative improvement - needed to scale skill level without manual intervention, quick check: track win rate against past versions
- **Imperfect Information Handling**: Uses information sets to represent hidden state - needed to model the uncertainty in card holdings, quick check: validate information set abstraction quality
- **Multi-Agent Reinforcement Learning**: Coordinates multiple agents in competitive settings - needed to handle the strategic interdependence of players, quick check: analyze convergence of joint policies

## Architecture Onboarding

**Component Map:**
Environment -> State Encoder -> Actor Network -> Action Sampler -> Environment
                       -> Critic Network -> Value Estimate

**Critical Path:**
State observation → Encoder → Actor → Action → Environment feedback → Reward → Critic update → Actor update

**Design Tradeoffs:**
- Model-free vs model-based: Chosen for simplicity and scalability despite potentially suboptimal sample efficiency
- Regularized vs unregularized dynamics: Regularization provides stability at the cost of slower convergence
- Reduced format vs full game: Enables tractable training while preserving core strategic elements

**Failure Signatures:**
- Training collapse: Monitor for sudden drops in performance against past versions
- Exploitation vulnerability: Track win rates against fixed baseline policies
- Policy collapse: Watch for reduced action diversity indicating overfitting

**First 3 Experiments:**
1. Verify self-play training curve against fixed baseline policies
2. Test exploitability by introducing simple counter-strategies
3. Validate generalization by testing against rule variations

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation is limited to reduced-format Liar's Poker, limiting generalizability to full-scale versions
- Performance claims based on relatively limited human trials without comprehensive statistical analysis
- Novel strategy claims lack detailed qualitative analysis of the discovered bidding patterns

## Confidence
- High: Solly's ability to achieve positive equity and win rates above 50% in both simulated and human play
- Medium: Claims about novel strategy development and effective randomization
- Medium: Superiority over large language models, though the comparison methodology could be more rigorous

## Next Checks
1. Conduct systematic exploitability analysis by having expert human players attempt to develop counter-strategies specifically designed to exploit Solly's play patterns
2. Perform statistical power analysis on human trial results to establish confidence intervals and determine required sample sizes for robust performance claims
3. Test Solly's generalization by evaluating performance on full-format Liar's Poker and comparing strategy evolution across different game scales