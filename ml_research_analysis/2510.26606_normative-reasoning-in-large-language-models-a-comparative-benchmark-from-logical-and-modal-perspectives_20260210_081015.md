---
ver: rpa2
title: 'Normative Reasoning in Large Language Models: A Comparative Benchmark from
  Logical and Modal Perspectives'
arxiv_id: '2510.26606'
source_url: https://arxiv.org/abs/2510.26606
tags:
- reasoning
- normative
- epistemic
- valid
- deontic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the logical consistency of large language
  models in normative reasoning by comparing their performance on deontic (normative)
  and epistemic (knowledge-based) reasoning tasks. A new dataset was created to test
  reasoning patterns across both domains, incorporating cognitive factors that influence
  human reasoning.
---

# Normative Reasoning in Large Language Models: A Comparative Benchmark from Logical and Modal Perspectives

## Quick Facts
- arXiv ID: 2510.26606
- Source URL: https://arxiv.org/abs/2510.26606
- Reference count: 14
- Key outcome: LLMs generally follow valid reasoning patterns but exhibit notable inconsistencies in specific normative inferences, particularly those involving negation and the transition from obligation to permission.

## Executive Summary
This study evaluates the logical consistency of large language models in normative reasoning by comparing their performance on deontic (normative) and epistemic (knowledge-based) reasoning tasks. A new dataset was created to test reasoning patterns across both domains, incorporating cognitive factors that influence human reasoning. Results show that LLMs generally follow valid reasoning patterns but exhibit notable inconsistencies in specific normative inferences, particularly those involving negation and the transition from obligation to permission. Models also display human-like content effects, with performance varying based on whether conclusions align with common sense. While few-shot prompting improves accuracy, chain-of-thought reasoning does not consistently enhance robustness and sometimes introduces errors.

## Method Summary
The study created the NeuBAROCO dataset to evaluate LLMs on normative (deontic) and epistemic reasoning across two tasks: Deontic Logic (single-premise) and Syllogistic (multi-premise). The dataset includes 640 normative and 480 epistemic deontic logic problems, plus 480 normative and 480 epistemic syllogistic problems, with three content types (Congruent, Incongruent, Nonsense). Five models were tested (GPT-4o, GPT-4o-mini, Llama-3.1-8B-In, Llama-3.3-70B-In, Phi-4) using three prompt settings (Zero-Shot, Few-Shot with 1 exemplar per pattern, and Zero-Shot Chain-of-Thought). Models were evaluated on binary classification accuracy (entailment vs non-entailment) with temperature=0.0 and max tokens of 10 (standard) or 1024 (CoT).

## Key Results
- LLMs correctly follow valid reasoning patterns in normative reasoning but struggle with specific failures in negation handling and obligation-permission transitions
- Content effects significantly influence model performance, with lower accuracy on incongruent content compared to congruent content in Zero-Shot settings
- Few-shot prompting improves accuracy while chain-of-thought reasoning does not consistently enhance robustness and sometimes introduces errors
- Performance drops on Modus Tollens and Denying the Antecedent patterns, both involving negation, compared to affirmative patterns

## Why This Works (Mechanism)

### Mechanism 1: Linguistic Interpretation Over Logical Abstraction
LLMs process normative inputs via semantic association with natural language phrases rather than by instantiating abstract modal operators. When logical valid inferences are expressed with specific phrases, semantic mismatches cause rejection despite logical validity.

### Mechanism 2: Content-Effect Heuristics (Probability Override)
LLMs rely on "common sense" priors from training data, leading to errors when logical validity contradicts semantic likelihood. Models act as probabilistic approximators, suppressing logical inferences that contradict more probable semantic associations.

### Mechanism 3: Negation-Induced Instability
The presence of logical negation in syllogistic premises degrades model performance more severely than the complexity of the logical rule itself. Negation introduces cognitive load or tokenization noise that disrupts multi-hop inference chains.

## Foundational Learning

- **Concept:** Standard Deontic Logic (SDL) & The Deontic Square
  - **Why needed here:** The paper benchmarks models against SDL. Understanding that "Obligation" ($O$) implies "Permission" ($P$) in SDL is essential to diagnose why the "Mu-Mi" failure is a logical error.
  - **Quick check question:** If "It is obligatory to save the file," is "It is permitted to save the file" logically valid in SDL? (Answer: Yes).

- **Concept:** Syllogistic Patterns (MP, MT, AC, DA)
  - **Why needed here:** The paper uses these four patterns to test reasoning. Distinguishing valid forms (MP, MT) from invalid forms (AC, DA) is necessary to evaluate if the model is reasoning or guessing.
  - **Quick check question:** In "If P then Q; Not Q," what is the valid conclusion? (Answer: Not P / Modus Tollens).

- **Concept:** Content Effects (Cognitive Bias)
  - **Why needed here:** To distinguish between a model failing a logic test vs. a model "refusing" a socially weird conclusion.
  - **Quick check question:** Why might a model correctly infer "Socrates is mortal" but fail to infer "The rock is mortal" given the same logical premises? (Answer: Content/semantic priors override logic).

## Architecture Onboarding

- **Component map:** NeuBAROCO Dataset -> Model (LLM) -> Prompt Interface (Zero-Shot/Few-Shot/CoT) -> Evaluator (Exact string match)
- **Critical path:** 1. Select reasoning pattern (e.g., Mu-Mi) -> 2. Inject content type (e.g., Incongruent) -> 3. Model generates "entailment" or "non-entailment" -> 4. Compare against Ground Truth (SDL rules)
- **Design tradeoffs:** Chain-of-Thought increases explainability but introduces error propagation; Few-Shot improves pattern recognition but risks surface-level copying.
- **Failure signatures:** Semantic Refusal (rejects valid obligation→permission inferences due to phrase interpretation); Negation Blindness (accuracy drops on MT/DA patterns).
- **First 3 experiments:** 1. Sanity Check (Mu-Mi): Run "Must A → Can choose A" prompt to confirm linguistic interpretation failure mode; 2. Negation Stress Test: Compare MP vs MT accuracy using "Nonsense" content to isolate logical failure from content bias; 3. CoT Intervention: Run "Must A → Permitted A" with CoT and inspect intermediate outputs for flawed logic chains.

## Open Questions the Paper Calls Out

### Open Question 1
Can prompting strategies or training interventions be designed that consistently improve normative reasoning without introducing the errors observed with Chain-of-Thought approaches? The study tested Zero-Shot, Few-Shot, and CoT prompting but found none reliably improved normative reasoning. No alternative intervention was proposed or tested.

### Open Question 2
How can the observed logical inconsistency between accepting NotMi-NotMu (contrapositive) while rejecting Mu-Mi (obligation to permission inference) be resolved in LLMs? The paper documents the inconsistency and hypothesizes that linguistic expression is interpreted as optionality rather than permission, but does not propose methods to enforce logical equivalence across paraphrases.

### Open Question 3
Do LLMs perform adequately on normative reasoning in open-ended, real-world contexts involving contextual interpretation and pragmatic constraints? The study focused on controlled syllogistic and deontic logic tasks, excluding the contextual factors that characterize actual normative deliberation in legal, ethical, or social domains.

## Limitations
- Single evaluation run per model-configuration pair limits statistical power and makes it difficult to distinguish systematic failures from model stochasticity
- The NeuBAROCO dataset may not fully capture the complexity of real-world normative reasoning scenarios
- Focus on English-language normative expressions may not generalize to other languages or cultural contexts

## Confidence
- **High Confidence:** LLMs follow valid reasoning patterns but exhibit specific failures in negation handling and obligation-permission transitions
- **Medium Confidence:** Content effects significantly influence model performance, but exact magnitude and mechanisms require further investigation
- **Medium Confidence:** Few-shot prompting improves accuracy while chain-of-thought reasoning does not consistently enhance robustness

## Next Checks
1. **Statistical Validation:** Run multiple evaluation passes (n≥5) for each model-configuration pair to establish confidence intervals and determine whether observed differences are statistically significant.
2. **Cross-Cultural Validation:** Test the same normative reasoning patterns using normative expressions from different cultural contexts or languages to assess generalizability of the failure modes.
3. **Mechanistic Investigation:** Design controlled experiments to isolate whether obligation-permission transition failures are due to semantic interpretation or statistical priors by systematically varying the linguistic expressions used for normative concepts.