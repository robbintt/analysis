---
ver: rpa2
title: 'Efficient extraction of medication information from clinical notes: an evaluation
  in two languages'
arxiv_id: '2502.03257'
source_url: https://arxiv.org/abs/2502.03257
tags:
- clinical
- drug
- extraction
- french
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes a new transformer-based architecture for extracting
  medication information from clinical narratives. The method improves upon existing
  approaches by simultaneously classifying all relationships, significantly reducing
  computational cost while maintaining state-of-the-art accuracy.
---

# Efficient extraction of medication information from clinical notes: an evaluation in two languages

## Quick Facts
- **arXiv ID**: 2502.03257
- **Source URL**: https://arxiv.org/abs/2502.03257
- **Reference count**: 40
- **Primary result**: Transformer-based architecture achieves F1 scores of 0.82 (French) and 0.96 (English) for relation extraction while reducing training time by 10-23×

## Executive Summary
This study introduces a transformer-based architecture for extracting medication information from clinical narratives that simultaneously classifies all entity relationships in a single pass. The method significantly reduces computational cost while maintaining state-of-the-art accuracy on both French and English clinical corpora. The approach introduces a "frame" representation to capture complex drug-attribute relationships and temporal dependencies, improving precision for multi-frame prescriptions. Evaluated on the 2018 n2c2 corpus (English) and the Corp-HUS dataset (French), the architecture achieved competitive performance with substantially lower computational requirements.

## Method Summary
The architecture processes clinical text using a sliding window approach (300 characters) with pretrained clinical transformers (ClinicalBERT/BioBERT for English, CamemBERT-BIO for French). It fuses token embeddings with learnable label embeddings, applies multi-head self-attention, and classifies all pairwise relationships simultaneously using a shared dense layer. A masked cross-entropy loss restricts computation to labeled token pairs, achieving 10-23× speedup. The frame-based representation groups drug-attribute relations into temporal contexts, improving extraction of complex regimens. For end-to-end evaluation, the model is combined with the NLStruct library for named entity recognition.

## Key Results
- Achieved F1 scores of 0.82 and 0.96 for relation extraction on French and English corpora respectively
- End-to-end performance reached 0.69 and 0.82 on French and English corpora
- Reduced training time by a factor of 10-23 compared to pairwise classification baselines
- Frame-based representation improved F1 score from 0.807 to 0.821 on French corpus
- Multi-head attention with 4 heads provided optimal balance between performance and efficiency

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Processing all entity pair relations simultaneously via a single transformer pass reduces computational cost by 10-23× while maintaining accuracy comparable to state-of-the-art pairwise methods.
- **Mechanism**: Instead of feeding each candidate entity pair through the transformer independently (requiring O(n²) forward passes for n entities), the architecture generates contextual embeddings once for all tokens, then constructs pairwise representations by concatenating token embeddings with relative position embeddings. These pair representations pass through a shared dense layer and classifier in parallel.
- **Core assumption**: Relations can be determined from contextual token embeddings and relative positions without requiring full transformer re-computation per pair.
- **Evidence anchors**:
  - [abstract]: "reduce the computational cost by 10-23 times compared to existing transformer-based methods"
  - [section]: "Our model depicted Figure 3 is designed to classify all relationships simultaneously... token pairs are represented by concatenating their embeddings and incorporating relative position embeddings"
  - [corpus]: Weak — neighboring papers (e.g., "Scalable Medication Extraction" using LLMs) do not validate this simultaneous classification approach; no direct replication in corpus.
- **Break condition**: If entity density or document length exceeds sliding window capacity, long-range dependencies are missed (observed: recall drops for modifiers spanning multiple drugs beyond window boundaries).

### Mechanism 2
- **Claim**: Grouping drug-attribute relations into temporal "frames" improves precision for complex regimens, increasing F1 from 0.807 to 0.821 on the French corpus.
- **Mechanism**: A frame binds a specific drug instance with its associated attributes (dose, frequency, route, duration, temporal context). This enables the model to distinguish between different prescription periods for the same drug (e.g., "every 4 weeks July-October, then every 2 weeks until December").
- **Core assumption**: Therapy modifications (same drug, different frequencies/periods) occur frequently enough that explicit frame modeling improves extraction beyond treating all drug-attribute links independently.
- **Evidence anchors**:
  - [abstract]: "introduces a 'frame' representation to capture complex drug-attribute relationships and temporal dependencies"
  - [section]: "The frame-based representation proved particularly effective, improving the F1 score from 0.807 to 0.821 on the French corpus"
  - [corpus]: Weak — no neighboring papers explicitly validate frame-based schemas; this annotation approach appears novel to this work.
- **Break condition**: Limited evaluation data (only 3.9% multi-frame drugs in training, <5% in test) constrains ability to quantify benefits for truly complex multi-frame scenarios.

### Mechanism 3
- **Claim**: Masked cross-entropy loss restricted to labeled token pairs, combined with character-based sliding windows, achieves computational efficiency with controlled accuracy tradeoffs.
- **Mechanism**: Rather than computing loss over all O(n²) possible token pairs, a mask restricts gradient computation to pairs where tokens carry entity labels. Sliding windows (200-300 characters) segment documents while preserving local context.
- **Core assumption**: Most clinically relevant relations occur within short textual proximity; 300-character windows capture sufficient context for most medication-attribute associations.
- **Evidence anchors**:
  - [abstract]: Implied by computational gains, not explicitly detailed
  - [section]: "During training, we optimize computation by applying a mask to the loss function, restricting its application to labeled token pairs... Increasing the sliding window size from 200 characters to 300 enhanced the recall by three points"
  - [corpus]: No corpus validation found for masked-loss approach.
- **Break condition**: Modifiers applying to multiple drugs across window boundaries cause recall degradation (acknowledged in error analysis).

## Foundational Learning

- **Concept**: Transformer contextual embeddings (BERT-family)
  - Why needed here: The architecture uses pretrained clinical BERT variants (ClinicalBERT, BioBERT, CamemBERT-BIO) to generate domain-specific token representations.
  - Quick check question: Can you explain why BERT produces contextualized embeddings versus static word vectors?

- **Concept**: Multi-head self-attention
  - Why needed here: After fusing token and label embeddings, a multi-head attention layer (4 heads in optimal config) captures dependencies between tokens for relation inference.
  - Quick check question: What different patterns might separate attention heads learn in a relation extraction context?

- **Concept**: Relation extraction as structured prediction
  - Why needed here: Medication extraction requires predicting multiple interdependent relations; treating each independently ignores structural constraints (e.g., one drug, multiple attributes in a frame).
  - Quick check question: Why would predicting "drug-frequency" and "drug-route" relations independently fail to capture prescription coherence?

## Architecture Onboarding

- **Component map**: Text -> Tokenizer -> Transformer embeddings -> Label embedding concat -> Self-attention -> Pairwise concatenation + position -> Dense -> Softmax -> Masked loss

- **Critical path**: Text → Tokenizer → Transformer embeddings → Label embedding concat → Self-attention → Pairwise concatenation + position → Dense → Softmax → Masked loss

- **Design tradeoffs**:
  - Precision vs. Recall: Architecture favors precision (0.834 vs. 0.782 on French); increase window size to shift toward recall
  - Efficiency vs. Long-range capture: Smaller windows = faster but miss cross-sentence modifiers
  - Frame richness vs. Data availability: Complex annotation schema requires more multi-frame training examples than currently available

- **Failure signatures**:
  - Low recall on modifiers spanning multiple drugs (shared dates, frequencies)
  - Relations crossing sliding window boundaries dropped entirely
  - Near-zero F1 on rare relations (Discontinue, Contraindicated) due to insufficient examples
  - End-to-end pipeline bottleneck at NER stage (RE-only F1 0.82 → end-to-end F1 0.69 on French)

- **First 3 experiments**:
  1. **Baseline reproduction on n2c2**: Train with ClinicalBERT, window=300, no frames. Target RE-only F1 ≥ 0.95. If missed, verify tokenization and masked loss implementation.
  2. **Window size ablation**: Compare 200 vs. 300 char windows on French corpus. Expect ~3-point recall improvement. If not observed, check newline token handling in preprocessing.
  3. **Frame augmentation test**: Add same-frame relations between drug attributes, retrain on French corpus. Expect ~1.5-point F1 gain. If no improvement, verify multi-frame prevalence in dataset exceeds 5% threshold.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the frame-based representation significantly improve extraction performance in clinical domains with a high prevalence of complex, multi-frame prescriptions compared to standard annotation schemas?
- Basis in paper: [inferred] Page 11 notes that multi-frame prescriptions constituted less than 5% of the test set, restricting a "comprehensive evaluation of the frame-based approach," despite initial improvements.
- Why unresolved: The evaluation corpus (rheumatoid arthritis patients) lacked sufficient examples of complex regimen changes (e.g., oncology) to fully validate the frame concept's utility.
- What evidence would resolve it: Evaluation of the model on a corpus rich in therapy adjustments (e.g., chemotherapy logs) demonstrating statistically significant F1-score improvements over baselines.

### Open Question 2
- Question: How can the model's architecture be modified to improve recall for modifiers that apply to multiple drugs or span across the sliding window boundaries?
- Basis in paper: [explicit] Page 10 states the "lower recall... was primarily attributed to the handling of modifiers that apply to multiple drugs" and suggests "Future research could explore how to better capture long-range dependencies."
- Why unresolved: The current use of a sliding window (n characters) truncates context, causing the model to miss relationships when a modifier extends beyond the window or applies to several distinct drugs.
- What evidence would resolve it: An architecture update that maintains the 10-23x computational efficiency while achieving recall scores comparable to the baseline on dispersed entity relations.

### Open Question 3
- Question: What computational methods are required to accurately reconstruct longitudinal medication timelines from the extracted frame-based information?
- Basis in paper: [explicit] Page 11 states, "the development of reconstruction algorithms is still required to accurately assemble and represent these timelines" from longitudinal data.
- Why unresolved: The current study successfully extracts information from individual notes but does not address the integration of these snapshots into a coherent, temporal patient history.
- What evidence would resolve it: A post-processing algorithm capable of resolving temporal contradictions and ordering extracted frames into a valid sequence, validated against manual patient timelines.

## Limitations
- The French Corp-HUS dataset is private, preventing independent validation of French performance and frame-based architecture effectiveness
- Limited evaluation data for complex multi-frame prescriptions (<5% prevalence) constrains generalizability to truly complex regimens
- Masked cross-entropy loss implementation may not be directly available in standard transformer libraries, requiring custom implementation

## Confidence

- **High Confidence**: Transformer-based simultaneous relation classification reduces computational cost (10-23×) while maintaining competitive accuracy - this follows established principles of efficient transformer inference and is supported by explicit performance metrics.
- **Medium Confidence**: Frame-based representation improves precision for complex regimens - the F1 improvement is demonstrated, but limited evaluation data (3.9% multi-frame drugs) constrains ability to validate for complex scenarios.
- **Low Confidence**: Masked cross-entropy loss restricted to labeled pairs achieves optimal efficiency-accuracy tradeoff - while the mechanism is sound, no external validation or ablation studies confirm this approach outperforms alternatives.

## Next Checks

1. **Baseline reproducibility**: Independently reproduce RE-only F1 scores on n2c2 corpus (target ≥0.95) to verify architecture implementation and masked loss computation before evaluating efficiency claims.

2. **Window boundary analysis**: Systematically evaluate relation extraction performance on cross-window cases by comparing 200 vs 300 character windows on French corpus to quantify recall degradation for long-range dependencies.

3. **Frame complexity scaling**: Analyze the relationship between frame complexity (number of multi-frame drugs) and model performance to determine if the claimed 1.5 F1 point improvement scales with more complex medication regimens beyond the current 5% prevalence threshold.