---
ver: rpa2
title: 'SplitWise Regression: Stepwise Modeling with Adaptive Dummy Encoding'
arxiv_id: '2505.15423'
source_url: https://arxiv.org/abs/2505.15423
tags:
- splitwise
- stepwise
- regression
- both
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces SplitWise regression, a novel framework that
  enhances traditional stepwise regression by adaptively transforming numeric predictors
  into threshold-based binary features using shallow decision trees. This transformation
  is only applied when it improves model fit, as determined by AIC or BIC.
---

# SplitWise Regression: Stepwise Modeling with Adaptive Dummy Encoding

## Quick Facts
- arXiv ID: 2505.15423
- Source URL: https://arxiv.org/abs/2505.15423
- Reference count: 9
- The paper introduces SplitWise regression, a novel framework that enhances traditional stepwise regression by adaptively transforming numeric predictors into threshold-based binary features using shallow decision trees.

## Executive Summary
SplitWise regression is a novel statistical learning framework that extends stepwise regression by adaptively encoding numeric predictors as binary features based on data-driven thresholds identified via shallow decision trees. The method selectively applies dummy encoding only when it improves model fit, as determined by AIC or BIC. This approach maintains the interpretability of linear models while capturing nonlinear relationships. Implemented as an R package, SplitWise is evaluated on both synthetic and real-world datasets and consistently outperforms traditional stepwise and penalized regression techniques in terms of parsimony, generalizability, and adjusted R² metrics.

## Method Summary
SplitWise regression enhances traditional stepwise regression by introducing adaptive dummy encoding for numeric predictors. The method uses shallow decision trees to identify optimal thresholds for splitting continuous variables into binary features. These transformations are only applied when they improve model fit according to AIC or BIC criteria. The framework maintains the transparency of linear models while flexibly capturing nonlinear effects through selective feature engineering. The approach is implemented as an R package and evaluated across multiple datasets, demonstrating superior performance in producing parsimonious models with better generalization.

## Key Results
- SplitWise achieved the lowest RMSE and highest adjusted R² in most settings
- The method produces more parsimonious models than traditional stepwise regression
- Performance remains robust across different initialization strategies and dimensions

## Why This Works (Mechanism)
SplitWise works by combining the interpretability of stepwise regression with the flexibility of decision tree-based feature engineering. The key mechanism involves using shallow decision trees to identify optimal split points for continuous predictors, converting them into binary features only when this transformation improves model fit. This adaptive approach allows the model to capture nonlinear relationships without sacrificing the transparency of linear models. By using AIC/BIC to guide threshold selection, the method ensures that added complexity is justified by improved fit, maintaining parsimony while enhancing predictive power.

## Foundational Learning
- **Stepwise regression**: Sequential variable selection method for building parsimonious models; needed to understand the baseline framework being enhanced
- **AIC/BIC criteria**: Information-theoretic measures for model selection that balance fit and complexity; needed to determine when adaptive encoding improves the model
- **Decision tree splitting**: Algorithm for finding optimal thresholds to partition continuous variables; needed to identify where to split numeric predictors
- **Dummy encoding**: Process of converting categorical or thresholded variables into binary features; needed to represent split variables in the linear model
- **Model parsimony**: Principle of preferring simpler models when performance is comparable; needed to understand the goal of reducing unnecessary complexity
- **Adjusted R²**: Model fit metric that accounts for the number of predictors; needed to evaluate model performance while penalizing overfitting

## Architecture Onboarding

Component Map:
- Data preprocessing -> SplitWise algorithm -> AIC/BIC evaluation -> Model selection

Critical Path:
The core workflow involves: (1) initializing with standard stepwise regression, (2) evaluating each numeric predictor for potential splitting using shallow decision trees, (3) testing candidate binary encodings against AIC/BIC criteria, (4) incorporating beneficial splits into the model, and (5) continuing stepwise selection with the expanded feature set. The algorithm iterates until no further improvements are found.

Design Tradeoffs:
The primary tradeoff is between model complexity and flexibility. While adaptive encoding can capture nonlinear relationships, it increases the number of features and potential for overfitting. The AIC/BIC criteria help mitigate this by only accepting splits that improve the information criterion. Another tradeoff involves the choice of tree depth - shallow trees ensure interpretability but may miss complex patterns, while deeper trees could capture more structure but reduce transparency.

Failure Signatures:
The method may struggle with highly collinear predictors, where multiple variables capture similar information, potentially leading to unstable threshold selection. It may also underperform when the true relationships are highly complex and cannot be adequately captured by binary splits. The reliance on AIC/BIC could lead to suboptimal selections if these criteria are not well-suited to the specific dataset characteristics.

First Experiments:
1. Test on a synthetic dataset with known nonlinear relationships to verify the method can recover the underlying structure
2. Apply to a standard regression benchmark (like Boston Housing) to compare against established methods
3. Evaluate performance on a high-dimensional dataset to assess scalability and robustness to multicollinearity

## Open Questions the Paper Calls Out
None

## Limitations
- Limited empirical validation with narrow set of datasets and comparison methods
- Unclear scalability to very high-dimensional data or highly collinear predictors
- Sensitivity of threshold selection to choice of AIC/BIC criteria not thoroughly explored

## Confidence
- Core claims: Medium
- Theoretical framing is sound and stepwise framework is well established
- Empirical evidence is positive but not yet comprehensive
- Improvements in parsimony and generalizability are plausible but need further testing

## Next Checks
1. Test SplitWise regression on high-dimensional datasets (p >> n) to assess scalability and robustness to multicollinearity
2. Compare performance using alternative information criteria (e.g., cross-validation-based metrics) to evaluate sensitivity to AIC/BIC choice
3. Evaluate stability of the tree-based encoding by performing repeated subsampling and measuring variance in selected thresholds and model coefficients