---
ver: rpa2
title: Diffusion Policies with Offline and Inverse Reinforcement Learning for Promoting
  Physical Activity in Older Adults Using Wearable Sensors
arxiv_id: '2509.18433'
source_url: https://arxiv.org/abs/2509.18433
tags:
- activity
- policy
- learning
- reward
- physical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes KANDI, a novel offline inverse reinforcement
  learning framework combining Kolmogorov-Arnold Networks (KAN) and Diffusion Policies
  for promoting physical activity in older adults using wearable sensor data from
  a clinical trial. KANDI addresses challenges in defining reward functions and capturing
  temporal variability in healthcare settings by inferring rewards from expert behavior
  using KAN's flexible function approximation, and generating high-fidelity actions
  using diffusion models within an Actor-Critic framework.
---

# Diffusion Policies with Offline and Inverse Reinforcement Learning for Promoting Physical Activity in Older Adults Using Wearable Sensors

## Quick Facts
- **arXiv ID:** 2509.18433
- **Source URL:** https://arxiv.org/abs/2509.18433
- **Reference count:** 40
- **Primary result:** KANDI, an offline IRL framework using KAN and Diffusion Policies, learned optimal timing to increase physical activity in 134 older adults, outperforming state-of-the-art on D4RL benchmarks.

## Executive Summary
This paper proposes KANDI, a novel offline inverse reinforcement learning framework that combines Kolmogorov-Arnold Networks (KAN) and Diffusion Policies to promote physical activity in older adults using wearable sensor data. The method addresses key challenges in healthcare RL by inferring latent rewards from expert behavior using KAN's flexible function approximation and generating high-fidelity actions via diffusion models within an Actor-Critic framework. Evaluated on data from 134 older adults in a clinical trial, KANDI successfully learned time-dependent policies that increased standing behavior during daytime hours and demonstrated superior performance on standard RL benchmarks.

## Method Summary
KANDI uses KAN to infer latent rewards from expert states by learning B-spline transformations on connection edges, then employs a diffusion-based Actor network to generate actions conditioned on state while mitigating distribution shift. The framework includes time-varying parameters to capture circadian rhythms and uses a 30-minute look-back window for state context. The method was evaluated on wearable sensor data from older adults and validated on D4RL benchmark tasks, achieving competitive normalized scores across multiple domains.

## Key Results
- Successfully learned optimal timing and policies to increase physical activity levels in older adults
- Higher action probabilities during daytime hours (6am-10pm) compared to nighttime
- Outperformed state-of-the-art approaches on D4RL benchmark with normalized scores of 50.2 (halfcheetah-medium-v2), 111.8 (walker2d-medium-expert-v2), 112.0 (hopper-medium-expert-v2), 96.4 (kitchen-complete-v0), 74.3 (kitchen-mixed-v0), and 66.1 (antmaze-large-play-v0)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** KAN improves reward inference by replacing fixed node activations with learnable edge-based B-spline functions.
- **Mechanism:** KAN applies learnable univariate functions on connection edges rather than fixed activation functions on nodes, allowing smoother approximation of complex non-linear mappings between physical activity states and latent rewards.
- **Core assumption:** The "Rational" group (low fall-risk participants) represents optimal behavior trajectories from which generalized reward signals can be distilled.
- **Evidence anchors:** Abstract and Section III.A describe KAN's use of B-splines for flexible function approximation; related work supports need for flexible function approximation in dynamic systems.
- **Break condition:** If expert data is noisy or relationships are discontinuous, spline interpolation may overfit or smooth over critical threshold behaviors.

### Mechanism 2
- **Claim:** Diffusion policies mitigate distribution shift by generating actions via iterative denoising rather than direct regression.
- **Mechanism:** Diffusion models generate actions by reversing a noise process conditioned on state, forcing policies to stay close to the data manifold while allowing multi-modal action generation.
- **Core assumption:** The 100-step iterative denoising process is computationally feasible for intervention latency requirements.
- **Evidence anchors:** Section III.B describes diffusion policy generation; literature supports diffusion models for handling multi-modal behavior and distributional shifts.
- **Break condition:** Reduced diffusion steps degrade action quality; contradictory actions in dataset may cause convergence to unrealistic averages.

### Mechanism 3
- **Claim:** Explicitly modeling time-variability aligns intervention policy with human circadian rhythms.
- **Mechanism:** Time is treated as a continuous variable with time-varying parameter α_t (positive for day, negative for night) and 30-minute look-back window, allowing learning of distinct policies for different times of day.
- **Core assumption:** Optimal policy is highly time-dependent and recent history is sufficient context for predicting current activity suitability.
- **Evidence anchors:** Section III.A introduces time-varying parameter; Section IV.B shows reward patterns over 24 hours.
- **Break condition:** Highly irregular daily routines may cause global time-parameters to fail generalization across population.

## Foundational Learning

- **Concept: Offline Inverse Reinforcement Learning (IRL)**
  - **Why needed here:** Healthcare rewards (e.g., "reduced fall risk") are latent rather than explicit, requiring learning what experts were optimizing for before creating replicating policies.
  - **Quick check question:** Can you distinguish between imitating an action and inferring the intent (reward) behind that action?

- **Concept: Diffusion Models (Denoising Probabilistic Models)**
  - **Why needed here:** Standard RL policies output single "mean" actions, while diffusion models allow representing distributions of possible good actions critical in healthcare where multiple responses might be valid.
  - **Quick check question:** Explain how adding noise to data and then learning to remove it helps generate new, high-quality data samples.

- **Concept: Kolmogorov-Arnold Networks (KAN) vs. MLP**
  - **Why needed here:** KAN replaces fixed activation functions with learnable splines, key to fitting complex non-linear reward surfaces without requiring massive network depth.
  - **Quick check question:** How does placing learnable functions on edges (KAN) differ from placing them on nodes (MLP), and why does this aid interpretability?

## Architecture Onboarding

- **Component map:** Wearable sensor data + Expert Group Labels -> KAN Network (Expert States -> B-Spline Transformations -> Latent Reward Signal) -> Diffusion Actor Network (Generates action candidates) -> Critic Network (Evaluates candidates using reward) -> Time-dependent probability of "Standing" action

- **Critical path:** KAN-based reward inference is the linchpin; if inferred rewards don't correlate with actual activity levels, the Diffusion Policy will optimize for wrong behavior.

- **Design tradeoffs:**
  - **Spline Knots vs. Smoothness:** 10 knots balance capturing sharp reward changes versus overfitting to noise
  - **Diffusion Steps (100) vs. Inference Speed:** 100 steps ensure high-fidelity actions but introduce latency potentially prohibitive for real-time interventions

- **Failure signatures:**
  - **Flat Reward Curve:** Constant rewards regardless of time/activity suggests insufficient "Rational" group data or too high regularization
  - **Action Collapse:** "Stand" suggestions at 3 AM indicate critic failed to penalize nighttime actions, suggesting α_t misalignment

- **First 3 experiments:**
  1. **Reward Validation:** Visualize inferred reward function over 24 hours to verify daytime peaks and nighttime drops
  2. **Ablation on Architecture:** Replace KAN with standard MLP to isolate performance gain from Kolmogorov-Arnold architecture
  3. **Policy Alignment Check:** Run learned policy on test set and plot "Probability of Standing" vs. Time to compare PEER vs. Control groups

## Open Questions the Paper Calls Out
None

## Limitations
- Framework performance heavily depends on quality and representativeness of "expert" data; if Rational group doesn't represent optimal behavior, inferred rewards may be misaligned
- 30-minute look-back window may not capture longer-term fatigue or environmental factors affecting activity decisions
- 100 denoising steps introduce computational latency that may be prohibitive for real-time interventions

## Confidence

- **High confidence:** Theoretical advantages of KAN over MLPs for reward inference are well-established; diffusion policy framework for mitigating distribution shift has strong empirical support
- **Medium confidence:** Specific implementation choices (10 knots, 30-minute window, 100 steps) are justified but not extensively validated through ablation studies
- **Low confidence:** Claim that KANDI "successfully learned optimal timing" requires stronger evidence as paper primarily shows correlation rather than sustained behavior change

## Next Checks
1. **Temporal Generalization Test:** Apply learned policy to separate test set covering different seasons/time periods to verify time-dependent reward function generalizes beyond training distribution
2. **Reward Function Sensitivity Analysis:** Systematically vary number of B-spline knots (5, 10, 20) and regularization strength β to quantify impact on reward smoothness versus fidelity
3. **Intervention Efficacy Study:** Conduct randomized controlled trial deploying KANDI to half of new cohort while other half receives standard care, measuring actual physical activity levels over 3-6 months to establish causal impact