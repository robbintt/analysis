---
ver: rpa2
title: Accelerating Large Language Models through Partially Linear Feed-Forward Network
arxiv_id: '2501.10054'
source_url: https://arxiv.org/abs/2501.10054
tags:
- tardis
- linear
- activation
- compression
- range
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TARDIS, a novel approach for accelerating
  large language models (LLMs) by leveraging constant folding optimization on feed-forward
  networks (FFNs). The key insight is that activation functions in LLMs can be partially
  approximated as linear functions in frequently occurring input ranges, enabling
  parameter reduction through matrix multiplication reordering.
---

# Accelerating Large Language Models through Partially Linear Feed-Forward Network

## Quick Facts
- arXiv ID: 2501.10054
- Source URL: https://arxiv.org/abs/2501.10054
- Reference count: 40
- Primary result: TARDIS achieves 1.6× end-to-end inference speedup with 80% parameter reduction on FFN blocks while maintaining high accuracy

## Executive Summary
This paper introduces TARDIS, a novel approach for accelerating large language models (LLMs) by leveraging constant folding optimization on feed-forward networks (FFNs). The key insight is that activation functions in LLMs can be partially approximated as linear functions in frequently occurring input ranges, enabling parameter reduction through matrix multiplication reordering. TARDIS handles complex non-linear activations like GELU by using an adaptive thresholding mechanism to determine optimal linear approximation ranges for each neuron, and employs a predictor to identify and correct out-of-range inputs during inference. The method achieves up to 80% parameter reduction in FFN blocks while maintaining high accuracy - outperforming state-of-the-art pruning methods like Wanda and RIA by up to 65% in accuracy. In practical deployments, TARDIS delivers 1.6× end-to-end inference speedup on vLLM and 1.4× on HuggingFace with only a 10.9% accuracy trade-off for a 7B model.

## Method Summary
TARDIS compresses FFN blocks in LLMs by approximating non-linear activations (GELU/SiLU) with linear functions in frequently occurring input ranges, enabling constant folding to merge two weight matrices into one. The method profiles activation input distributions per neuron using calibration data, applies adaptive two-level thresholding to assign coverage thresholds based on approximation error, and performs greedy range search with KDE centroid initialization. Linear approximations are fit via least squares, then folded into a single matrix C with bias B. A quantized predictor identifies out-of-range inputs for correction during inference. The approach is evaluated on Falcon-7B, Falcon2-11B, BLOOMZ-7B1, GPT2-XL, and OPT-6.7B, achieving up to 80% compression while maintaining perplexity and accuracy within acceptable bounds.

## Key Results
- 80% parameter reduction in FFN blocks while maintaining high accuracy
- 1.6× end-to-end inference speedup on vLLM and 1.4× on HuggingFace
- Outperforms state-of-the-art pruning methods (Wanda, RIA) by up to 65% in accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Partial linear approximation of non-linear activation functions enables constant folding, reducing FFN parameters by up to 87.5% theoretically.
- Mechanism: When activation σ is approximated as linear (y = ax + b), the FFN computation σ(xW₁)W₂ reorders to x(aW₁W₂). The term aW₁W₂ contains only constants and can be pre-computed into a single matrix C, transforming two matrix multiplications into one.
- Core assumption: Activation function inputs in LLMs concentrate in narrow "hot" ranges where linear approximation has low error.
- Evidence anchors:
  - [abstract] "treating activation functions in LLMs as linear functions...enables parameter reduction"
  - [Section 3.1] "For typical LLM architectures like GPT2, BLOOM, and Falcon where h = 4d, this reduces the parameter count by up to 87.5% per FFN block"
  - [Section 4.1] "approximately 65% of activation inputs for each neuron are concentrated within just 20% of the total input range"
  - [corpus] Related work on activation sparsity (22007) supports the premise that activations have exploitable structure, but constant folding specifically for LLMs lacks direct corpus validation
- Break condition: If activation inputs become uniformly distributed across ranges, the linear approximation error accumulates beyond recoverable thresholds.

### Mechanism 2
- Claim: Adaptive thresholding assigns more conservative linear ranges to high-error neurons, preserving accuracy while maximizing coverage.
- Mechanism: Two-level optimization distributes a global threshold t across layers (minimizing ΣEᵢtᵢ) then neurons (minimizing ΣEᵢₙtᵢₙ), where E represents approximation error. Critical neurons receive tighter ranges; less important neurons get aggressive approximation.
- Core assumption: Error contribution at neuron and layer level predicts their importance to downstream task performance.
- Evidence anchors:
  - [abstract] "adaptive thresholding mechanism to determine optimal linear approximation ranges for each neuron"
  - [Section 4.1, Insight 2] "error of the first layer (10⁻⁷) is an order of magnitude larger than that of the second layer (10⁻⁸)"
  - [Section 5.1.1] "neuron importance within a layer varies significantly...error variations by nearly three orders of magnitude"
  - [corpus] Weak direct evidence; related pruning work (38871 Olica) uses similar importance-based approaches but different methodology
- Break condition: If error metrics don't correlate with actual task degradation, adaptive allocation provides no benefit over uniform thresholding.

### Mechanism 3
- Claim: Speculative approximation with selective result fixing preserves accuracy by correcting only neurons predicted to be outside their linear range.
- Mechanism: System optimistically computes xC + B assuming all inputs are in-range, then uses a lightweight quantized predictor to identify outlier neurons. For flagged neurons: (1) subtract incorrect linear result, (2) recompute with original activation function.
- Core assumption: The predictor's false negative rate is sufficiently low that most out-of-range inputs get corrected.
- Evidence anchors:
  - [abstract] "employs a predictor to identify and correct out-of-range inputs during inference"
  - [Section 5.4] "correction involves two steps. First, we remove the approximate results for the identified neurons. Then, we compute and add back their actual results"
  - [Section 7.6] "perplexity decreases as predictor size increases, with a maximum difference of 0.12" - suggesting predictor precision matters but is not catastrophic
  - [corpus] No direct corpus evidence for this specific predictor-fixing pattern in LLM acceleration
- Break condition: If out-of-range frequency exceeds prediction/recomputation overhead, the scheme becomes slower than baseline.

## Foundational Learning

- Concept: Constant folding in compiler optimization
  - Why needed here: This is the intellectual ancestor of TARDIS; understanding how compilers pre-compute constant expressions (e.g., `y = 3 * 4 + 2` → `y = 14`) provides the mental model for why merging W₁ and W₂ works.
  - Quick check question: Given FFN(x) = σ(xW₁)W₂ with linear σ(x) = 2x + 1, what single matrix C satisfies FFN(x) = xC + b?

- Concept: Kernel Density Estimation (KDE)
  - Why needed here: TARDIS uses KDE to find the centroid of activation input distributions for each neuron before expanding linear ranges.
  - Quick check question: If activation inputs cluster around 0.5 with high density but have sparse outliers at -10 and +15, where would KDE suggest centering a linear approximation range?

- Concept: Matrix associativity vs. floating-point associativity
  - Why needed here: TARDIS exploits associativity to reorder computations, but floating-point arithmetic is not perfectly associative. Understanding numerical stability tradeoffs is critical for debugging precision issues.
  - Quick check question: In float16, would (A × B) × C always equal A × (B × C)? What could go wrong?

## Architecture Onboarding

- Component map: Calibration data → Error statistics → Adaptive thresholding → Linear range search (KDE + greedy) → Folded matrix (C, B) + Quantized predictor (W₁') → Online: x → Speculative compute (xC + B) → Predictor check → Result fixing → Output

- Critical path: The range assignment algorithm (Section 5.1.1, Algorithm 1) dominates offline time at ~30 minutes/layer for Falcon-7B. Online latency is dominated by result fixing (~40% of FFN time) due to sparse matrix indexing overhead.

- Design tradeoffs:
  - Higher threshold t → more compression but more frequent out-of-range errors → more fixing overhead
  - Larger predictor → better accuracy but larger memory footprint (~4.8% of model size at 2-bit)
  - Multi-range approximation → better fit but exponential folded matrix growth (rʰ matrices for h neurons with r ranges each) - rejected for this reason

- Failure signatures:
  - Accuracy collapse to near-zero (e.g., Lambada 75% → 0%) indicates overly aggressive linear approximation without adaptive thresholding
  - Slower-than-baseline inference indicates result fixing triggered too frequently (predictor threshold too strict or input distribution shift from calibration)
  - Perplexity instability across calibration datasets suggests distribution mismatch

- First 3 experiments:
  1. Reproduce the linear approximation failure mode: Replace GELU with a global linear function in Falcon-7B, verify the 75% → 0% Lambada accuracy drop claimed in Section 3.2. This validates the problem exists before testing the solution.
  2. Validate the skewed distribution insight: Profile activation inputs on 8 random C4 samples for a single FFN layer, compute what fraction falls within the 20% central range. Compare to the ~65% concentration claimed in Section 4.1.
  3. Test the adaptive vs. uniform thresholding hypothesis: Implement constant folding with (a) uniform 85% threshold across all neurons, (b) adaptive thresholding per Section 5.1.1. Measure perplexity delta on WikiText-2 at 80% compression ratio.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can TARDIS be adapted to efficiently compress GLU-variant Feed-Forward Networks without incurring exponential parameter growth?
- Basis in paper: [explicit] The Discussion section states that applying the current method to GLU-variants (e.g., in LLaMA3, Qwen 2.5) causes the folded matrices to grow exponentially (e.g., 254× for LLaMA-2), undermining the compression goal.
- Why unresolved: The element-wise multiplication (gating) in GLU variants prevents the direct reordering of matrix multiplications required for constant folding without a combinatorial explosion in stored matrices.
- What evidence would resolve it: A modified folding technique or approximation strategy that successfully compresses a GLU-based model (like LLaMA) while maintaining a net parameter reduction.

### Open Question 2
- Question: Can the significant inference time overhead (40%) caused by the "result fixing" phase be reduced?
- Basis in paper: [explicit] Section 7.5 and Figure 14 reveal that "result fixing" dominates the execution time (40%) due to expensive sparse matrix indexing and loading operations in the CUDA kernel.
- Why unresolved: The current implementation relies on sparse operations to correct out-of-range predictions, which inherently introduces latency that offsets the gains from matrix folding.
- What evidence would resolve it: An optimized kernel or algorithmic change that lowers the fixing overhead percentage while maintaining the accuracy guarantees of the predictor.

### Open Question 3
- Question: Is it feasible to apply constant folding optimization to the Multi-Head Attention (MHA) blocks in LLMs?
- Basis in paper: [explicit] The Discussion section notes that the Multi-head attention block "encounters similar limitations" to GLU-variants regarding the difficulty of applying folding techniques.
- Why unresolved: The complex, non-linear interactions (specifically softmax and quadratic dependencies) in attention mechanisms do not map easily to the linear approximations used in the FFN blocks.
- What evidence would resolve it: A theoretical framework or approximation method that enables constant folding in attention layers without excessive memory or accuracy costs.

### Open Question 4
- Question: Can multi-range approximation strategies be made viable to improve accuracy without causing storage explosion?
- Basis in paper: [explicit] Section 5.1 explains that while approximating activations with multiple linear ranges would be more accurate, it is currently impractical because it leads to $r^h$ folded matrices.
- Why unresolved: The current design forces a "single-range strategy" to keep storage linear, potentially leaving accuracy on the table for complex activation patterns.
- What evidence would resolve it: A sparse or shared matrix scheme that allows multiple approximation ranges per neuron while keeping the memory footprint manageable.

## Limitations
- Calibration limited to 8 samples from single C4 shard may not capture distribution variability across domains
- Inference time overhead from result fixing phase (40%) offsets some computational gains
- Cannot be directly applied to GLU-variant architectures due to exponential parameter growth
- Floating-point precision handling not thoroughly explored across different precision formats

## Confidence

- **High confidence**: The fundamental mechanism of constant folding through linear approximation (Mechanism 1) is well-grounded in compiler optimization theory and the empirical observation that activation inputs concentrate in narrow ranges. The 87.5% theoretical parameter reduction calculation is mathematically sound.
- **Medium confidence**: The adaptive thresholding approach (Mechanism 2) is plausible given the reported error variance across layers and neurons, but the specific optimization formulation and its correlation with actual task performance lacks direct validation. The two-level optimization appears sound but may not generalize.
- **Low confidence**: The predictor-based result fixing mechanism (Mechanism 3) shows promise in maintaining accuracy, but the specific implementation details and threshold tuning are underspecified. The claim that this adds only ~40% overhead to FFN computation seems optimistic without knowing the exact false positive/negative rates.

## Next Checks

1. **Distribution stability validation**: Run TARDIS calibration on three different datasets (C4, OpenWebText, and a domain-specific corpus) for the same model. Measure how much the adaptive thresholds and linear ranges change across datasets, and quantify the resulting accuracy/perplexity variance. This will reveal whether the 8-sample calibration is sufficient or if dataset-specific calibration is required.

2. **Precision sensitivity analysis**: Implement TARDIS with bfloat16, float16, and float32 intermediates for the folded matrix computation. Measure the impact on perplexity and task accuracy for each precision setting, particularly focusing on whether the claimed 10.9% accuracy trade-off holds across precisions or if it's specific to the reported configuration.

3. **Predictor efficiency evaluation**: Systematically vary the predictor size (1-bit to 8-bit quantization) and measure the false positive rate, false negative rate, and resulting accuracy/speedup trade-offs. This will determine whether the 2-bit configuration represents an optimal balance or if there are better operating points for specific deployment scenarios.