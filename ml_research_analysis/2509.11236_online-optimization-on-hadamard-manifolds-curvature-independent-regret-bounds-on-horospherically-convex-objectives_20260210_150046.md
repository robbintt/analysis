---
ver: rpa2
title: 'Online Optimization on Hadamard Manifolds: Curvature Independent Regret Bounds
  on Horospherically Convex Objectives'
arxiv_id: '2509.11236'
source_url: https://arxiv.org/abs/2509.11236
tags:
- optimization
- riemannian
- online
- regret
- convexity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies online Riemannian optimization on Hadamard\
  \ manifolds, focusing on horospherical convexity (h-convexity) to address limitations\
  \ of geodesic convexity, where regret bounds depend on manifold curvature. The authors\
  \ analyze Riemannian Online Gradient Descent (ROGD) for h-convex and strongly h-convex\
  \ functions, establishing curvature-independent regret bounds of O(\u221AT) and\
  \ O(log T), respectively."
---

# Online Optimization on Hadamard Manifolds: Curvature Independent Regret Bounds on Horospherically Convex Objectives

## Quick Facts
- arXiv ID: 2509.11236
- Source URL: https://arxiv.org/abs/2509.11236
- Authors: Emre Sahinoglu; Shahin Shahrampour
- Reference count: 34
- Primary result: Curvature-independent regret bounds of O(√T) and O(log T) for Riemannian Online Gradient Descent on horospherically convex objectives

## Executive Summary
This paper studies online Riemannian optimization on Hadamard manifolds, focusing on horospherical convexity (h-convexity) as a geometric condition that overcomes limitations of geodesic convexity. The key insight is that h-convexity enables curvature-independent regret bounds for Riemannian Online Gradient Descent (ROGD), matching the performance of Euclidean online optimization. The authors establish O(√T) regret for h-convex functions and O(log T) regret for strongly h-convex functions without curvature-dependent terms. This is achieved through a careful analysis of the retraction operator and the properties of h-convexity on Hadamard manifolds. The results are validated through experiments on the manifold of symmetric positive definite (SPD) matrices with the affine-invariant metric, demonstrating the practical relevance of the theoretical framework.

## Method Summary
The authors analyze Riemannian Online Gradient Descent (ROGD) for online optimization on Hadamard manifolds with horospherical convexity (h-convexity). The algorithm maintains a sequence of iterates by performing gradient descent steps and projecting back to the manifold using a retraction operator. The key contribution is establishing regret bounds that depend only on the convexity parameter and not on the manifold's curvature. For h-convex functions, the regret bound is O(√T), and for strongly h-convex functions, it is O(log T). These bounds are achieved through a careful analysis of the geometry of h-convex functions and the properties of the retraction operator. The theoretical analysis is complemented by experiments on two specific manifolds and online optimization problems, demonstrating the practical applicability of the framework.

## Key Results
- Curvature-independent O(√T) regret bound for h-convex functions using Riemannian Online Gradient Descent
- Curvature-independent O(log T) regret bound for strongly h-convex functions
- Experimental validation on the manifold of SPD matrices with affine-invariant metric for online Tyler's M-estimation (h-convex) and Fréchet mean computation (strongly h-convex)
- Results demonstrate that h-convexity provides a powerful framework for extending online optimization theory to non-Euclidean settings with curvature-independent performance guarantees

## Why This Works (Mechanism)
The paper's mechanism for achieving curvature-independent regret bounds relies on the geometric properties of horospherical convexity (h-convexity) on Hadamard manifolds. Unlike geodesic convexity, which can introduce curvature-dependent terms in regret bounds, h-convexity allows for a more flexible definition of convexity that is better suited for non-Euclidean optimization. The key insight is that h-convexity enables a decomposition of the regret into components that can be bounded independently of the manifold's curvature. This is achieved through a careful analysis of the retraction operator and the properties of h-convex functions, which allows for the derivation of regret bounds that match the Euclidean setting. The curvature independence is crucial for ensuring that the optimization performance is not compromised by the geometric complexity of the underlying manifold.

## Foundational Learning
- **Hadamard Manifolds**: Complete, simply connected Riemannian manifolds with non-positive sectional curvature. Why needed: The analysis relies on the geometric properties of Hadamard manifolds, including the existence of unique geodesics and the behavior of the exponential map. Quick check: Verify that the manifold is complete and simply connected with non-positive sectional curvature.
- **Horospherical Convexity (h-convexity)**: A generalization of geodesic convexity that allows for a more flexible definition of convexity on manifolds. Why needed: h-convexity enables the derivation of curvature-independent regret bounds, which is the key contribution of the paper. Quick check: Verify that the objective function satisfies the h-convexity condition with respect to a given horosphere.
- **Retraction Operator**: A mapping from the tangent space to the manifold that approximates the exponential map. Why needed: The retraction operator is used to project the gradient descent step back to the manifold, ensuring that the iterates remain on the manifold. Quick check: Verify that the retraction operator satisfies the necessary properties, such as being a first-order approximation of the exponential map.
- **Riemannian Online Gradient Descent (ROGD)**: An online optimization algorithm that performs gradient descent on Riemannian manifolds. Why needed: ROGD is the primary algorithm analyzed in the paper, and its performance is evaluated in terms of regret bounds. Quick check: Verify that the algorithm correctly implements the gradient descent steps and retraction operator.
- **Regret Bounds**: A measure of the performance of an online optimization algorithm, defined as the difference between the cumulative loss of the algorithm and the cumulative loss of the best fixed decision in hindsight. Why needed: Regret bounds are the primary metric used to evaluate the performance of the proposed algorithm. Quick check: Verify that the regret bounds are correctly derived and match the theoretical predictions.
- **Strongly h-convex Functions**: A stronger version of h-convexity that implies a unique minimizer and faster convergence rates. Why needed: Strongly h-convex functions enable the derivation of O(log T) regret bounds, which is a significant improvement over the O(√T) bound for h-convex functions. Quick check: Verify that the objective function satisfies the strong h-convexity condition with respect to a given horosphere.

## Architecture Onboarding
- **Component Map**: Online Learner -> Gradient Computation -> Retraction -> Next Iterate
- **Critical Path**: The critical path involves computing the gradient of the loss function at the current iterate, performing a gradient descent step, and projecting the result back to the manifold using the retraction operator.
- **Design Tradeoffs**: The choice of retraction operator and the definition of h-convexity involve tradeoffs between computational complexity and the tightness of the regret bounds.
- **Failure Signatures**: Failure can occur if the retraction operator is not a good approximation of the exponential map, or if the objective function does not satisfy the h-convexity condition.
- **First Experiments**:
  1. Implement ROGD on a simple Hadamard manifold (e.g., the hyperbolic plane) with a known h-convex function.
  2. Experiment with different retraction operators and evaluate their impact on the regret bounds.
  3. Validate the curvature independence of the regret bounds by comparing the performance on manifolds with different curvatures.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can curvature-independent regret bounds be achieved for decentralized online Riemannian optimization under h-convexity?
- Basis in paper: [explicit] The conclusion states, "Future directions include extending our theoretical framework to decentralized... Riemannian optimization."
- Why unresolved: The current analysis focuses on a centralized learner, whereas decentralized settings introduce network constraints and local information sharing that may complicate the maintenance of curvature-independent guarantees.
- What evidence would resolve it: A theoretical derivation of regret bounds for a decentralized Riemannian OGD algorithm on h-convex functions that matches the centralized performance without curvature-dependent terms.

### Open Question 2
- Question: Do curvature-independent convergence guarantees hold for stochastic Riemannian optimization methods applied to h-convex objectives?
- Basis in paper: [explicit] The conclusion lists "stochastic Riemannian optimization" as a specific target for extending the theoretical framework.
- Why unresolved: The current paper analyzes the online adversarial setting with deterministic gradients at each step, whereas stochastic settings involve noise variance which requires different concentration arguments not explored here.
- What evidence would resolve it: A convergence rate proof for Riemannian Stochastic Gradient Descent (SGD) on h-convex functions that explicitly avoids curvature-dependent constants.

### Open Question 3
- Question: Can dynamic regret bounds be established for online h-convex optimization that are independent of curvature?
- Basis in paper: [inferred] The paper derives static regret bounds ($O(\sqrt{T})$ and $O(\log T)$), while the literature review notes that "dynamic regret guarantees" exist for g-convexity.
- Why unresolved: Extending the analysis from a fixed comparator (static regret) to a time-varying sequence of minimizers (dynamic regret) requires handling more complex geometric path integrals which might re-introduce curvature dependencies.
- What evidence would resolve it: A theoretical result establishing curvature-independent dynamic regret bounds for ROGD on h-convex objectives.

### Open Question 4
- Question: Can projection-free online algorithms be designed for h-convex objectives while retaining curvature independence?
- Basis in paper: [inferred] The proposed algorithm relies on a projection oracle (Assumption 3.1), but the literature review identifies "projection-free variants" as an active area for g-convexity.
- Why unresolved: The current proofs rely on the non-expansiveness of the projection operator; projection-free methods (e.g., Frank-Wolfe) utilize linear optimization oracles, and it is unclear if the h-convexity properties suffice to guarantee curvature independence without projection.
- What evidence would resolve it: A regret analysis for a Riemannian conditional gradient method on h-convex functions proving bounds independent of curvature.

## Limitations
- The analysis is limited to Hadamard manifolds, which may not cover all applications of Riemannian optimization.
- The strong h-convexity assumption for the O(log T) regret bound may be restrictive in practice.
- The experimental validation is limited to two specific manifolds and two online optimization problems.

## Confidence
- **O(√T) regret bound on h-convex functions**: High confidence, as this follows directly from established Riemannian optimization theory.
- **O(log T) regret bound on strongly h-convex functions**: Medium confidence, as this requires additional assumptions on the strong h-convexity parameter.
- **Experimental results**: Medium confidence, as the paper only presents results on two specific manifolds and two specific online optimization problems.

## Next Checks
1. Extend the analysis to non-Hadamard manifolds with non-positive sectional curvature.
2. Investigate the tightness of the O(√T) regret bound for h-convex functions.
3. Experimentally validate the regret bounds on a broader range of manifolds and online optimization problems.