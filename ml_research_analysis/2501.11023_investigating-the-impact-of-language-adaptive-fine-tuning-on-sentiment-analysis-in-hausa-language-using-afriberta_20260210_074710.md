---
ver: rpa2
title: Investigating the Impact of Language-Adaptive Fine-Tuning on Sentiment Analysis
  in Hausa Language Using AfriBERTa
arxiv_id: '2501.11023'
source_url: https://arxiv.org/abs/2501.11023
tags:
- hausa
- laft
- sentiment
- languages
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the impact of Language-Adaptive Fine-Tuning
  (LAFT) on sentiment analysis for Hausa, a low-resource African language. Using AfriBERTa,
  a multilingual language model pre-trained on African languages, the researchers
  first curated a diverse, unlabelled corpus of formal Hausa text across multiple
  domains.
---

# Investigating the Impact of Language-Adaptive Fine-Tuning on Sentiment Analysis in Hausa Language Using AfriBERTa

## Quick Facts
- arXiv ID: 2501.11023
- Source URL: https://arxiv.org/abs/2501.11023
- Reference count: 13
- Hausa sentiment analysis shows AfriBERTa outperforms non-Hausa pre-trained models, with modest LAFT improvements

## Executive Summary
This study investigates Language-Adaptive Fine-Tuning (LAFT) for Hausa sentiment analysis using AfriBERTa, a multilingual language model pre-trained on African languages. The researchers first curated a diverse, unlabelled corpus of formal Hausa text across multiple domains, then adapted AfriBERTa through LAFT before fine-tuning on the NaijaSenti sentiment dataset. Results showed that LAFT yielded only modest improvements over AfriBERTa's baseline performance, likely due to the formal nature of the fine-tuning corpus compared to the informal language in sentiment tasks. Nevertheless, AfriBERTa significantly outperformed models not pre-trained on Hausa, demonstrating the importance of using language-specific pre-trained models in low-resource contexts.

## Method Summary
The study employed a two-phase fine-tuning approach using AfriBERTa-small. First, LAFT was applied to an unlabeled Hausa corpus containing formal text from various sources (novels, news, literature) for 5 epochs. Second, the adapted model was fine-tuned on the NaijaSenti labeled dataset for 3 epochs. The evaluation used standard metrics (accuracy, F1, precision, recall) and compared performance against AfriBERTa without LAFT and other baseline models. The approach leveraged AdamW optimization with a learning rate of 1e-5 and batch size of 8.

## Key Results
- AfriBERTa-small achieved F1 score of 77% on Hausa sentiment classification
- LAFT provided only modest improvements over AfriBERTa baseline
- Formal LAFT corpus domain mismatch with informal sentiment data likely limited LAFT effectiveness
- AfriBERTa significantly outperformed models not pre-trained on Hausa

## Why This Works (Mechanism)

### Mechanism 1: Language-Adaptive Pre-Training Transfer
Pre-training on African languages enables better downstream task performance by creating language-specific representations that capture morphological and syntactic patterns of African languages, reducing the cold start problem in low-resource settings.

### Mechanism 2: Two-Phase Fine-Tuning (LAFT → Task-Specific)
Intermediate language-adaptive fine-tuning on unlabeled corpus improves downstream task performance by refining language representations before task learning through continued MLM training on domain-specific text.

### Mechanism 3: Domain Alignment Gap
LAFT improvements are constrained when adaptation data differs in register/formality from downstream task data, as formal Hausa lacks code-switching, slang, and informal expressions common in social media sentiment data.

## Foundational Learning

- Concept: **Masked Language Modeling (MLM) Pre-training**
  - Why needed here: LAFT extends MLM pre-training on new data. Understanding that MLM learns bidirectional context by predicting masked tokens explains why additional MLM exposure helps adaptation.
  - Quick check question: Can you explain why MLM pre-training creates representations useful for classification tasks it was never explicitly trained on?

- Concept: **Transfer Learning in Transformers**
  - Why needed here: The entire approach relies on transferring knowledge from pre-training → LAFT → sentiment classification. Understanding layer-wise transfer is essential.
  - Quick check question: Why might lower transformer layers transfer better across domains than higher layers?

- Concept: **Domain Shift / Distribution Mismatch**
  - Why needed here: The paper's central finding is that domain mismatch (formal vs. informal text) limits LAFT effectiveness. This is a fundamental NLP concern.
  - Quick check question: What signals in your data would suggest a domain shift problem is likely affecting performance?

## Architecture Onboarding

- Component map: Input Text → AfriBERTa Tokenizer (SentencePiece, 512 max tokens) → AfriBERTa Encoder (97M params, 4 layers, 6 heads, 768 hidden) → Classification Head (3-way: Positive/Negative/Neutral) → Softmax Output

- Critical path:
  1. Load pre-trained AfriBERTa-small from Hugging Face
  2. Phase 1 (LAFT): Continue MLM training on unlabeled Hausa corpus (5 epochs, LR=1e-5)
  3. Save adapted model
  4. Phase 2 (Task): Fine-tune on NaijaSenti labeled data (3 epochs, LR=1e-5)
  5. Evaluate on held-out test set

- Design tradeoffs:
  - AfriBERTa-small (97M params) vs. large (126M): Paper reports small achieved F1=0.77 with 398s runtime vs. large F1=0.79 with 875s—small chosen for resource efficiency
  - Learning rate reduction (2e-5 → 1e-5) traded faster convergence for stability and reduced overfitting
  - 70/10/20 split prioritizes test set size for reliable evaluation in low-data regime

- Failure signatures:
  - Validation loss rising after epoch 2-3 while training loss continues dropping → overfitting
  - Neutral class systematically misclassified as negative → class boundary confusion
  - Test performance unchanged after LAFT while training improves → domain mismatch between LAFT corpus and task data

- First 3 experiments:
  1. **Baseline replication**: Fine-tune AfriBERTa-small directly on NaijaSenti without LAFT; confirm F1 ≈ 0.75-0.77 on test set
  2. **Ablation on LAFT data source**: Create separate LAFT models using only novels, only news, or only scanned literature to isolate domain effects
  3. **Learning rate sweep**: Test LR ∈ {5e-6, 1e-5, 2e-5, 5e-5} for both LAFT and task phases to find optimal stability-efficiency tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
Would LAFT yield significantly greater improvements if the adaptation corpus consisted of informal, conversational Hausa text (e.g., social media data) rather than formal literary sources?

### Open Question 2
To what extent does the Kano dialect bias in the LAFT corpus limit model generalization to other Hausa dialects?

### Open Question 3
Would a custom Hausa-specific tokenizer trained on sentiment-relevant lexicons improve semantic preservation and classification performance compared to the generic SentencePiece tokenizer?

## Limitations

- Domain mismatch between formal LAFT corpus and informal sentiment data limits LAFT effectiveness
- Low-resource constraints: small LAFT corpus (~40K sentences) and limited pre-training data for Hausa
- Privacy restrictions prevented collection of sufficient social media data for domain-specific fine-tuning

## Confidence

**High Confidence:**
- AfriBERTa's strong baseline performance relative to non-Hausa pre-trained models
- The two-phase fine-tuning methodology is correctly implemented
- The modest LAFT improvements are accurately reported

**Medium Confidence:**
- The attribution of LAFT limitations to formal-informal domain mismatch
- The quantitative results and statistical significance of improvements
- The comparison of AfriBERTa-small versus large model variants

**Low Confidence:**
- Generalizability of findings to other low-resource African languages
- Optimal LAFT corpus composition for sentiment tasks
- The impact of alternative tokenization or preprocessing strategies

## Next Checks

1. **Domain Alignment Experiment**: Create separate LAFT models using only novels, only news, or only social media data to isolate the effect of corpus domain on sentiment classification performance.

2. **Multi-Domain Corpus Construction**: Collect and integrate informal Hausa text from social media platforms (with proper ethical approval) to create a balanced LAFT corpus spanning formal and informal registers.

3. **Cross-Lingual Transfer Analysis**: Test AfriBERTa's performance on sentiment analysis for closely related Chadic languages (e.g., Fulfulde, Kanuri) to assess the model's cross-linguistic transfer capabilities.