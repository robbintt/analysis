---
ver: rpa2
title: 'Plan-Then-Execute: An Empirical Study of User Trust and Team Performance When
  Using LLM Agents As A Daily Assistant'
arxiv_id: '2502.01390'
source_url: https://arxiv.org/abs/2502.01390
tags:
- user
- execution
- trust
- plan
- planning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates human-AI collaboration with LLM agents
  as daily assistants. The authors conducted a study (N=248) where participants collaborated
  with LLM agents on six everyday tasks using a plan-then-execute approach, with varying
  levels of user involvement in planning and execution stages.
---

# Plan-Then-Execute: An Empirical Study of User Trust and Team Performance When Using LLM Agents As A Daily Assistant

## Quick Facts
- arXiv ID: 2502.01390
- Source URL: https://arxiv.org/abs/2502.01390
- Reference count: 40
- Primary result: User involvement in LLM agent workflows improves task performance but doesn't consistently calibrate trust due to convincing but imperfect plans

## Executive Summary
This paper investigates human-AI collaboration using LLM agents as daily assistants through a plan-then-execute framework. The authors conducted a study with 248 participants who collaborated with LLM agents on six everyday tasks, varying the level of user involvement in planning and execution stages. The study reveals that while user involvement can improve task performance, it doesn't guarantee better trust calibration because users often trust plausible but imperfect plans generated by LLMs. The research highlights a critical tension between the benefits of human oversight and the cognitive costs of involvement.

## Method Summary
The study employed a 2x2 factorial design where participants collaborated with GPT-3.5-turbo agents on six daily tasks (finance, travel, etc.) in different conditions: automatic vs. user-involved planning and automatic vs. user-involved execution. Tasks were performed in a simulated environment using Flask APIs rather than real-world systems. Participants were recruited from Prolific and measured across multiple dimensions including task performance, cognitive load (NASA-TLX), and trust calibration. The agent used a hierarchical plan format where primary steps could contain multiple sub-actions, and users could edit plans or provide feedback during execution.

## Key Results
- User involvement in planning and execution stages doesn't consistently calibrate user trust - participants often trusted plausible but imperfect plans generated by LLMs
- User involvement improves task performance, particularly when fixing imperfect plans or correcting execution errors
- Tasks with higher quality plans led to better execution accuracy and higher user confidence
- User involvement increased cognitive load and reduced user confidence compared to fully automated execution

## Why This Works (Mechanism)

### Mechanism 1: Illusory Trust from Structured Outputs
- **Claim:** High-quality, structured LLM outputs can induce "illusory trust," causing users to approve plausible but imperfect plans
- **Mechanism:** The system generates plans with clear hierarchical formatting (1., 1.x) that creates surface-level plausibility, reducing critical scrutiny during planning
- **Core assumption:** Users rely on heuristics like formatting and logical flow to judge plan correctness rather than deep verification
- **Evidence anchors:** Participants often trusted plausible but imperfect plans; grammar errors in plans caused missing steps but were accepted
- **Break condition:** Obvious formatting errors or explicit contradictions trigger correction mode

### Mechanism 2: Execution Fallback for Agent Uncertainty
- **Claim:** User involvement in execution improves performance by serving as a fallback for agent uncertainty
- **Mechanism:** Users can reject invalid actions or wrong parameters during execution, correcting probabilistic failures in LLM action translation
- **Core assumption:** LLM agents frequently produce invalid or sub-optimal action predictions that humans can spot during step-by-step review
- **Evidence anchors:** User-involved execution showed significant contribution to task performance, especially in tasks with high invalid action rates
- **Break condition:** Low plan quality prevents recovery even with execution involvement

### Mechanism 3: Agency-Performance Trade-off
- **Claim:** Increased user agency reduces confidence and increases cognitive load
- **Mechanism:** Active monitoring and plan editing imposes mental demand and frustration, lowering subjective confidence in outcomes
- **Core assumption:** Debugging an LLM agent requires more effort than consuming automated results, leading users to prefer the easier automated path
- **Evidence anchors:** User involvement posed high cognitive load and decreased user confidence across multiple measures
- **Break condition:** Low-risk tasks with high-quality plans make involvement detrimental

## Foundational Learning

- **Concept: Trust Calibration vs. Reliance**
  - **Why needed here:** The paper distinguishes between trusting a system (subjective attitude) and calibrated trust (trusting correct outputs, distrusting incorrect ones)
  - **Quick check question:** If a user accepts 100% of an AI's suggestions, is their trust "calibrated"? (Answer: No, only if the AI is 100% correct)

- **Concept: The "LLM-Modulo" Framework**
  - **Why needed here:** This architecture relies on LLM as generator and human as validator, separating planning (drafting) from execution (validation)
  - **Quick check question:** In this framework, is the LLM the primary decision-maker or primary generator? (Answer: Generator; human is validator)

- **Concept: Simulation Environments for Agents**
  - **Why needed here:** The study runs tasks in simulated backend APIs rather than live web, controlling variables but introducing gaps between predictions and real-world consequences
  - **Quick check question:** Why simulate? (Answer: To control risk and reliably measure action sequence accuracy against ground truth)

## Architecture Onboarding

- **Component map:** Task Input -> Agent Core (GPT-3.5-turbo) -> Interface (Planning/Execution tools) -> Backend (Flask APIs) -> State (Plan Store â†’ Action Predictor)

- **Critical path:**
  1. User submits natural language request
  2. Agent generates hierarchical plan (1., 1.x); user must fix grammar errors if present
  3. Agent maps primary steps to action predictions (API calls)
  4. User reviews action prediction card and chooses "Proceed," "Feedback," or "Specify Action"

- **Design tradeoffs:**
  - AP-AE (Full Auto): Highest confidence, lowest load, but prone to hallucinated plans
  - UP-UE (Full Control): Best execution accuracy with attentive users, but highest cognitive load
  - Recommendation: Use iterative simulation - let agent simulate outcome first, intervene only if wrong

- **Failure signatures:**
  - "Grammar Errors" in Plans: Agent merges multiple actions into one primary step, causing silent action drops
  - Convincingly Wrong Plans: Structured but missing critical parameters (e.g., wrong route selection)
  - Silent Action Failures: Invalid action names rejected by backend

- **First 3 experiments:**
  1. **Plan Quality Validation:** Run 50 tasks through planning stage only; count grammar errors and test if users can spot them
  2. **Execution Accuracy Stress Test:** Feed high-quality plans to executor; measure invalid action rate to isolate planning vs. execution capabilities
  3. **Cognitive Load Measurement:** Compare time-on-task and error correction rates between Auto and User-Involved modes

## Open Questions the Paper Calls Out

- **Question 1:** What specific interventions are most effective for calibrating user trust when LLM-generated plans appear plausible but are factually incorrect?
  - **Basis:** The authors conclude that future work should explore effective interventions for user trust calibration with convincingly wrong LLM outcomes
  - **Why unresolved:** Study demonstrated trust miscalibration but didn't test specific interventions like uncertainty indicators or verification prompts
  - **What evidence would resolve it:** Empirical results from follow-up study comparing different trust-calibration interventions against baseline methods

- **Question 2:** How does performance and user trust of plan-then-execute LLM agents differ between simulation and real-world environments?
  - **Basis:** Authors acknowledge limitation regarding transferability to real-world environments with additional dependencies
  - **Why unresolved:** Study used simulated environment lacking unpredictability and integration failures of real systems
  - **What evidence would resolve it:** Replication using live external tools or high-fidelity prototypes simulating network errors

- **Question 3:** Does a flexible collaborative workflow allowing simultaneous intervention in planning and execution improve outcomes compared to rigid separation?
  - **Basis:** Authors suggest need for more flexible workflow where humans can fix planning and execution simultaneously
  - **Why unresolved:** Experimental design forced strict separation preventing observation of dynamic interaction effects
  - **What evidence would resolve it:** User studies testing interfaces permitting plan-editing during execution

## Limitations
- Simulation Gap: Tasks performed in simulated backend environment rather than real-world systems, potentially limiting generalizability
- Single LLM Architecture: Only GPT-3.5-turbo tested; results may not generalize to other LLMs
- Fixed Task Set: Six tasks selected from UltraTool represent narrow slice of daily assistant use cases

## Confidence
- **High Confidence:** Finding that user involvement increases cognitive load and reduces confidence is well-supported by multiple measures
- **Medium Confidence:** Trust miscalibration mechanism is strongly evidenced but relies on interpretation of why users accept plausible-but-wrong plans
- **Medium Confidence:** Execution accuracy improvements from user involvement are statistically significant but show task-dependent variability

## Next Checks
1. **Cross-LLM Validation:** Replicate study with GPT-4 and Claude to test whether trust miscalibration persists across different model architectures
2. **Real-World Deployment:** Test plan-then-execute workflow with actual web APIs (banking, travel booking) to validate simulation findings in production environments
3. **Longitudinal Trust Dynamics:** Conduct week-long study tracking how trust calibration evolves as users gain experience with system's error patterns