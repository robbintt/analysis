---
ver: rpa2
title: 'CLiViS: Unleashing Cognitive Map through Linguistic-Visual Synergy for Embodied
  Visual Reasoning'
arxiv_id: '2506.17629'
source_url: https://arxiv.org/abs/2506.17629
tags:
- reasoning
- video
- clivis
- cognitive
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CLiViS introduces a training-free framework for Embodied Visual
  Reasoning (EVR) that addresses the dual challenges of long-term spatiotemporal perception
  and complex compositional reasoning in egocentric video. It achieves this by orchestrating
  synergistic collaboration between Large Language Models (LLMs) and Vision-Language
  Models (VLMs), where LLMs decompose tasks into sub-instructions while VLMs perform
  open-vocabulary visual perception.
---

# CLiViS: Unleashing Cognitive Map through Linguistic-Visual Synergy for Embodied Visual Reasoning

## Quick Facts
- **arXiv ID**: 2506.17629
- **Source URL**: https://arxiv.org/abs/2506.17629
- **Reference count**: 40
- **Primary result**: Training-free framework for Embodied Visual Reasoning achieving 48.4% accuracy across three benchmarks

## Executive Summary
CLiViS addresses the dual challenges of long-term spatiotemporal perception and complex compositional reasoning in egocentric video through a training-free framework that orchestrates synergistic collaboration between Large Language Models (LLMs) and Vision-Language Models (VLMs). The framework leverages LLMs to decompose tasks into sub-instructions while VLMs perform open-vocabulary visual perception. Central to CLiViS is a dynamic Cognitive Map that evolves through linguistic-visual interactions, providing structured semantic representation of the embodied scene. This map, along with an evidence memory module, bridges fine-grained visual perception and high-level reasoning. Extensive experiments across three benchmarks demonstrate CLiViS's effectiveness, achieving state-of-the-art accuracy and showing particular strength in handling long-term visual dependencies.

## Method Summary
CLiViS introduces a training-free framework for Embodied Visual Reasoning (EVR) that addresses long-term spatiotemporal perception and complex compositional reasoning challenges. The framework orchestrates synergistic collaboration between LLMs and VLMs, where LLMs decompose tasks into sub-instructions while VLMs perform open-vocabulary visual perception. The core innovation is a dynamic Cognitive Map that evolves through linguistic-visual interactions, providing structured semantic representation of the embodied scene. This map, along with an evidence memory module, bridges fine-grained visual perception and high-level reasoning. The approach is model-agnostic and improves reasoning performance across different VLM backbones.

## Key Results
- Achieved state-of-the-art accuracy of 48.4% overall across three benchmarks (OpenEQA, EgoSchema, and EgoTempo)
- Demonstrated particular strength in handling long-term visual dependencies
- Showed model-agnostic improvements across different VLM backbones

## Why This Works (Mechanism)
The framework works by decomposing complex EVR tasks into manageable sub-instructions through LLM reasoning, while VLMs provide open-vocabulary visual perception to populate a dynamic Cognitive Map. This map evolves through continuous linguistic-visual interactions, creating structured semantic representations of the embodied scene. The evidence memory module stores and retrieves relevant visual and linguistic information, enabling the system to maintain context across long temporal sequences. This synergy between linguistic decomposition and visual perception allows the system to handle both the compositional complexity of reasoning tasks and the spatiotemporal nature of egocentric video.

## Foundational Learning

**Large Language Models (LLMs)** - Pre-trained models for natural language understanding and generation
*Why needed*: To decompose complex reasoning tasks into manageable sub-instructions
*Quick check*: Can the LLM generate coherent task decompositions for novel scenarios?

**Vision-Language Models (VLMs)** - Models that can process both visual and textual information
*Why needed*: To perform open-vocabulary visual perception and extract semantic information from egocentric video
*Quick check*: Can the VLM accurately identify and describe objects and relationships in diverse visual contexts?

**Cognitive Map** - Dynamic semantic representation of the embodied scene
*Why needed*: To bridge fine-grained visual perception and high-level reasoning by providing structured scene understanding
*Quick check*: Does the Cognitive Map maintain coherence and accuracy across long temporal sequences?

**Evidence Memory Module** - System for storing and retrieving relevant information
*Why needed*: To maintain context and support reasoning across long-term dependencies
*Quick check*: Can the module effectively retrieve relevant information when queried with complex reasoning tasks?

**Embodied Visual Reasoning (EVR)** - The task of answering questions based on egocentric video
*Why needed*: The target application domain requiring both visual perception and complex reasoning
*Quick check*: Can the system answer questions that require integrating information across multiple time steps?

## Architecture Onboarding

**Component map**: Input Video -> VLM Perception -> Cognitive Map -> LLM Reasoning -> Evidence Memory -> Output Answer

**Critical path**: Video frames are processed by VLMs to extract visual features, which populate the Cognitive Map. LLMs use this map along with evidence memory to decompose and solve reasoning tasks, producing final answers.

**Design tradeoffs**: The framework trades computational efficiency for reasoning capability by maintaining a dynamic Cognitive Map rather than processing frames independently. It also sacrifices task-specific optimization for model-agnostic applicability.

**Failure signatures**: Degradation in VLM performance will directly impact Cognitive Map quality. LLM reasoning failures will propagate through task decomposition. Memory limitations in evidence storage may cause context loss in very long sequences.

**First experiments to run**:
1. Test Cognitive Map accuracy on static scene understanding tasks before temporal reasoning
2. Evaluate evidence memory retrieval precision on short video clips
3. Compare reasoning performance with and without the Cognitive Map on compositional tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on zero-shot VLM capabilities without task-specific fine-tuning may limit robustness across diverse real-world scenarios
- Cognitive Map effectiveness depends heavily on VLM-generated spatial and semantic information quality, which may degrade in cluttered or visually ambiguous environments
- Evidence memory performance is bounded by contextual limits of underlying LLMs and VLMs, potentially constraining reasoning depth for highly complex tasks

## Confidence

**State-of-the-art claims**: Medium - The 48.4% accuracy is reported, but lacks comparison with fine-tuned baselines to isolate the contribution of linguistic-visual synergy.

**Model-agnostic improvements**: Medium - Claims require validation across a broader range of VLM backbones beyond those tested.

**Real-world applicability**: Low - Current evaluation focuses on controlled benchmarks, leaving uncertainty about performance in unstructured, open-world environments.

## Next Checks

1. Test CLiViS on real-world egocentric video datasets with dynamic agents and environmental changes to assess robustness beyond controlled benchmarks.

2. Compare performance against task-specific fine-tuned baselines to quantify the exact contribution of the linguistic-visual synergy framework.

3. Evaluate the system's ability to handle long-tail, rare objects and scenes not well-represented in the training data of the underlying VLMs.