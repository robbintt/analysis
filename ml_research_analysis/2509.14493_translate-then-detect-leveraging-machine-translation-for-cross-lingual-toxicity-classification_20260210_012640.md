---
ver: rpa2
title: 'Translate, then Detect: Leveraging Machine Translation for Cross-Lingual Toxicity
  Classification'
arxiv_id: '2509.14493'
source_url: https://arxiv.org/abs/2509.14493
tags:
- translation
- toxicity
- language
- languages
- translate-classify
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Cross-lingual toxicity detection is challenging due to limited
  training data for many languages. This work compares translation-based approaches
  (translate-test) against language-specific and multilingual classifiers.
---

# Translate, then Detect: Leveraging Machine Translation for Cross-Lingual Toxicity Classification

## Quick Facts
- arXiv ID: 2509.14493
- Source URL: https://arxiv.org/abs/2509.14493
- Authors: Samuel J. Bell; Eduardo Sánchez; David Dale; Pontus Stenetorp; Mikel Artetxe; Marta R. Costa-jussà
- Reference count: 30
- Translation-based methods outperform out-of-distribution classifiers in 81.3% of cases across 17 languages

## Executive Summary
Cross-lingual toxicity detection is challenging due to limited training data for many languages. This work compares translation-based approaches (translate-test) against language-specific and multilingual classifiers. Using 27 pipelines across 17 languages, it finds that translation-based methods outperform out-of-distribution classifiers in 81.3% of cases. Translation benefits increase with language resource level and translation quality. Traditional classifiers outperform LLM judges, especially for low-resource languages. MT-specific fine-tuning on LLMs reduces refusal rates but may lower accuracy for low-resource languages. Translation-based pipelines are recommended as a robust baseline for scalable multilingual toxicity detection.

## Method Summary
The study evaluates 27 pipelines combining 4 translation systems (NLLB, Llama 3.1, Gemma 3, GPT-4o) with 8 toxicity classifiers across 17 languages. Three regimes are tested: in-distribution classifiers (language-specific fine-tuning), out-of-distribution classifiers (multilingual models without target language fine-tuning), and translate-classify (translating to English then using English classifiers). The primary metric is AUC, with translation quality measured via COMETKiwi-DA-XL. MT-specific fine-tuning (MT-SFT) is tested on Llama 3.1 8B Instruct to reduce translation refusals.

## Key Results
- Translation-based approaches outperform OOD classifiers in 81.3% of language-dataset combinations
- Translation benefits scale log-linearly with resource level (FineWeb-2 document counts)
- Translation quality strongly correlates with downstream classification performance
- Traditional fine-tuned classifiers outperform LLM judges, especially for low-resource languages
- MT-SFT reduces refusal rates but can negatively impact accuracy for low-resource languages

## Why This Works (Mechanism)

### Mechanism 1: Translation Quality as Transfer Bottleneck
Better machine translation quality leads to better downstream toxicity classification accuracy because high-quality translations preserve semantic meaning and toxic signals, allowing English classifiers—trained on abundant English toxicity data—to make accurate predictions on translated inputs. The translation system acts as a bridge, and its fidelity determines how much toxic signal survives the transfer.

### Mechanism 2: Resource-Level Compounding Effect
Translation-based approaches show greater benefits for higher-resourced languages because high-resource languages have more MT training data → better translation quality → better classification. This creates a compounding advantage where well-resourced languages get both better MT and stronger downstream performance.

### Mechanism 3: Classifier-Judge Asymmetry in Multilingual Settings
Traditional fine-tuned classifiers outperform zero-shot LLM judges, especially for low-resource languages, because traditional classifiers are explicitly fine-tuned on toxicity data and output continuous scores enabling AUC evaluation. LLM judges rely on emergent multilingual capabilities that are unevenly distributed across languages.

## Foundational Learning

- **Translate-test paradigm**: This is the core method evaluated—translating inputs to English at inference time rather than training multilingual models. Why needed: It enables zero-shot cross-lingual classification without requiring target language toxicity data. Quick check: Can you distinguish translate-test (inference-time translation) from translate-train (training-time translation)?

- **AUC (Area Under ROC Curve)**: The paper evaluates all pipelines using AUC, which measures separability without requiring threshold selection. Why needed: AUC is preferred for toxicity detection where class imbalance is common. Quick check: Why would AUC be preferred over accuracy for toxicity detection where class imbalance is common?

- **In-distribution vs. Out-of-distribution (ID vs. OOD)**: The key comparison is translate-classify vs. OOD classifiers—the realistic scenario where no language-specific fine-tuned model exists. Why needed: For a new language with no labeled toxicity data, you would be in an OOD scenario. Quick check: For a new language with no labeled toxicity data, would you be in an ID or OOD scenario?

## Architecture Onboarding

- **Component map**: Source Text → MT System → English Translation → Classifier → Toxicity Score (or LLM Judge → Discrete Label)

- **Critical path**: Input → Translation to English → English toxicity classifier → continuous score. The translation step is the primary variability source.

- **Design tradeoffs**:
  - NMT vs. LLM translation: NMT (NLLB) may have better quality-coverage balance; LLMs may refuse toxic content but offer easier integration
  - Traditional classifier vs. LLM judge: Classifier gives continuous scores (better AUC); LLM judge is simpler to prompt but emits discrete labels
  - MT-SFT vs. standard instruction-tuned: MT-SFT reduces refusals but may harm low-resource accuracy

- **Failure signatures**:
  - High refusal rates (>5%) → LLM translator declining toxic content; switch to NMT or MT-SFT model
  - translate-classify underperforms OOD classifier → Translation quality likely poor; verify with COMETKiwi
  - Large accuracy drop for low-resource languages after MT-SFT → Revert to standard instruction-tuned model

- **First 3 experiments**:
  1. Measure refusal rate of your translation system on a sample of known-toxic content in your target language
  2. Compare translate-classify (using toxic-bert + your MT system) against distilbert-base-multilingual-cased-toxicity on your target language
  3. Estimate expected translation benefit by measuring translation quality (COMETKiwi-DA-XL) on a representative sample

## Open Questions the Paper Calls Out

- **Open Question 1**: Does fine-tuning classifiers specifically on machine-translated data improve toxicity detection compared to using off-the-shelf models? The study prioritized simulating a practitioner's setup using existing models rather than training new classifiers on translated data.

- **Open Question 2**: What specific mechanism causes MT-specific fine-tuning (MT-SFT) to degrade toxicity detection accuracy in low-resource languages? The paper identifies the trade-off but does not isolate whether the degradation is due to catastrophic forgetting, domain shift, or specific translation artifacts.

- **Open Question 3**: Can translation-based approaches be modified to mitigate the performance disparities between high-resource and low-resource languages? The work establishes the correlation between resources and performance gains but does not test specific interventions to flatten this curve.

## Limitations

- Dataset splits from 10 toxicity benchmarks are used but exact splits and random seeds are not specified, making exact reproduction difficult
- COMETKiwi-DA-XL model checkpoint and BOUQuET dataset access details are not provided for translation quality estimation
- Minos classifier implementation details for measuring translation refusals are missing

## Confidence

- **High confidence**: Translation-based approaches outperforming OOD classifiers (81.3% of cases) is well-supported with extensive empirical data across 17 languages
- **Medium confidence**: The mechanism that translation quality directly determines classification performance is supported but depends on the assumption that COMETKiwi-DA-XL captures all relevant translation quality aspects for toxicity detection
- **Medium confidence**: The resource-level compounding effect is observed but the relationship between FineWeb-2 document counts and actual MT training data quality is assumed rather than proven

## Next Checks

1. **Refusal rate baseline**: Measure translation refusal rates on a sample of known toxic content in your target language before committing to any pipeline. If >5%, switch to NMT or MT-SFT models.

2. **Translation quality verification**: Compute COMETKiwi-DA-XL scores on a representative sample of your target language to estimate expected performance gains before full evaluation.

3. **Classifier comparison sanity check**: Compare translate-classify (toxic-bert + your MT system) against distilbert-base-multilingual-cased-toxicity on your target language to establish baseline performance difference.