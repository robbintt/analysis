---
ver: rpa2
title: Geometric Uncertainty for Detecting and Correcting Hallucinations in LLMs
arxiv_id: '2509.13813'
source_url: https://arxiv.org/abs/2509.13813
tags:
- uncertainty
- local
- response
- hallucination
- geometric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose a geometric framework for detecting and correcting
  hallucinations in large language models using only black-box access. Their approach
  uses archetypal analysis to compute global uncertainty via convex hull volume and
  local uncertainty via three geometric suspicion metrics derived from the archetypes.
---

# Geometric Uncertainty for Detecting and Correcting Hallucinations in LLMs

## Quick Facts
- **arXiv ID:** 2509.13813
- **Source URL:** https://arxiv.org/abs/2509.13813
- **Reference count:** 40
- **Primary result:** Geometric Volume and Geometric Suspicion methods detect and reduce hallucinations across five benchmarks using only black-box model access.

## Executive Summary
The paper introduces a geometric framework for detecting and correcting hallucinations in large language models using only black-box access. The approach leverages archetypal analysis to compute global uncertainty via convex hull volume and local uncertainty through three geometric suspicion metrics. Experiments on five benchmarks demonstrate superior or comparable hallucination detection performance, especially on medical QA, while the Best-of-N selection method reduces hallucination rates across models and datasets. The method is memory-efficient, relying on small fixed archetypes, and robust across varying batch sizes and PCA dimensions.

## Method Summary
The method generates multiple responses per query at sampling temperature > 0, embeds them using a sentence encoder, and applies archetypal analysis to find extremal semantic boundary points. Global uncertainty is quantified as the log of the convex hull volume spanned by these archetypes, theoretically linked to entropy. Local uncertainty uses three geometric metrics—local density, distance from consensus, and archetype usage rarity—combined via Fisher's method to rank responses for Best-of-N selection. The approach requires no internal model access and is computationally efficient due to fixed-size archetype sets.

## Key Results
- Geometric Volume achieves competitive or superior AUROC for hallucination detection across all five benchmarks, particularly excelling on medical QA.
- Geometric Suspicion reduces hallucination rates via Best-of-N selection across multiple models and datasets, with ΔH scores ranging from 4.1% to 16.6%.
- Theoretical analysis proves convex hull volume provides an upper bound on differential entropy, validating the geometric approach.
- The method is robust to hyperparameter choices (K=16 archetypes, d'=15 PCA dimensions) and scales efficiently with sample size.

## Why This Works (Mechanism)

### Mechanism 1: Archetypal Analysis for Semantic Boundary Detection
Archetypes capture extremal semantic boundaries better than clustering centroids for uncertainty estimation. Archetypal analysis finds K archetypes constrained to lie on the convex hull of the data, representing semantic boundaries rather than central tendencies. This bi-convex constraint captures semantic diversity in sampled responses that correlates with model uncertainty about the correct answer.

### Mechanism 2: Convex Hull Volume as Entropy Upper Bound
The volume of the convex hull spanned by archetypes provides a theoretical upper bound on the differential entropy of the response distribution. For any continuous distribution supported on a simplex with volume V, differential entropy H(x) ≤ log V, with equality only under uniform distribution. Larger hull volumes permit more dispersed distributions, implying higher uncertainty.

### Mechanism 3: Geometric Suspicion via Multi-Metric Combination
Three complementary geometric metrics—local density, distance from consensus, and archetype usage rarity—jointly identify unreliable responses for Best-of-N selection. Local Density measures isolation (responses in sparse regions are suspicious). Distance from Consensus penalizes deviation from central tendency. Usage Rarity flags responses reconstructed from rarely-used archetypes. These are converted to p-values and combined via Fisher's method.

## Foundational Learning

- **Archetypal Analysis (Cutler & Breiman, 1994):** Foundation for extracting extremal boundary points that define the semantic simplex. Quick check: How do archetypes differ from k-means centroids in terms of where they lie relative to the data cloud?

- **Differential Entropy and Its Geometric Bounds:** Theoretical justification linking convex hull volume to uncertainty; understanding why log V bounds entropy. Quick check: For a uniform distribution over a 3D simplex with volume V=8, what is the differential entropy?

- **Black-box vs. White-box Uncertainty Quantification:** Positions the method in the UQ taxonomy; clarifies that only sampled responses are used, not internal states. Quick check: Why can't this method detect uncertainty from a single response without sampling?

## Architecture Onboarding

- **Component map:** Response Sampler -> Sentence Embedder -> L2 Normalization + PCA -> Archetypal Analysis -> Geometric Volume/Geometric Suspicion
- **Critical path:** Archetypal Analysis optimization (2000 steps per batch). If this fails or converges poorly, both global and local metrics degrade. Ensure K ≤ min(n, d′ + 1) to maintain simplex feasibility.
- **Design tradeoffs:** Sample size (n): Higher n improves both detection and correction but increases latency. Paper shows AUARC improves from 0.576 (n=5) to 0.585 (n=20). PCA dimensions (d′): Too low loses semantic structure; d′=2 performs poorly. Default d′=15 works well. Number of archetypes (K): Surprisingly robust; K=4 to K=16 show similar AUROC (~0.753). Use K=16 for better local metrics. Threshold tuning: Requires labeled validation set (10% split). Critical for F1 optimization.
- **Failure signatures:** "Confidently wrong" scenario: When the model is wrong and samples are nearly all hallucinations, geometric suspicion cannot recover a correct answer (see Qwen3-8b on TriviaQA, ∆H = -0.5%). Long-form responses: Single embedding may not capture semantic complexity for answers >1000 words. High hallucination rate batches (>75%): Component metrics invert; consider deferral instead of selection.
- **First 3 experiments:**
  1. Reproduce global uncertainty on TriviaQA: Sample n=20 responses at T=1.0, embed with gte-Qwen2, run AA with K=16 and d′=15. Compute Geometric Volume and AUROC against hallucination labels. Target: ~0.75 AUROC (Table 7c).
  2. Ablate sample size: Repeat experiment 1 with n ∈ {5, 10, 15, 20}. Plot AUARC vs. n. Expect monotonic improvement (Table 3).
  3. Local uncertainty Best-of-N selection: For questions where greedy decode hallucinates, use Geometric Suspicion to select the lowest-suspicion response from the batch. Compute ∆H (baseline hallucination rate minus post-selection rate). Target: positive ∆H on MedicalQA (Table 2).

## Open Questions the Paper Calls Out

- Can the geometric framework be extended to use multi-embedding representations for long-form or semantically complex responses beyond QA tasks? The authors state it "remains to be proven for complex datasets beyond QA tasks" and note the method "relies on the capability of a single embedding vector to sufficiently capture the semantic content of an answer."

- Can black-box uncertainty methods be developed to detect hallucinations in "confidently wrong" scenarios where sampled responses show no diversity? The authors explicitly note "to our knowledge no black-box methods have successfully solved this problem" and identify "confidently wrong" scenarios as a key limitation.

- What is the optimal relationship between the number of sampled responses (n), embedding dimensionality (d′), and number of archetypes (K) across different domains? The ablation studies show performance varies with these parameters, but the authors select K=16 and d′=15 empirically without theoretical justification for these choices.

## Limitations

- **Confidently wrong scenarios:** When the model is consistently wrong, geometric suspicion cannot recover correct answers as sampled responses lack diversity.
- **Long-form response limitation:** Single embeddings may not capture semantic complexity for answers exceeding 1000 words.
- **High hallucination rate failure:** When hallucination rates exceed ~75%, geometric suspicion metrics invert and become unreliable.

## Confidence

- **High Confidence:** The theoretical foundation linking convex hull volume to entropy (Theorem 1), the bi-convex optimization formulation for archetypal analysis, and the basic geometric intuition behind the three suspicion metrics.
- **Medium Confidence:** The empirical effectiveness across multiple benchmarks, the robustness to parameter choices (K, d', n), and the Best-of-N hallucination reduction rates.
- **Low-Medium Confidence:** The method's effectiveness in "confidently wrong" scenarios and with long-form responses, as well as potential domain-specific limitations not captured in the five benchmarks.

## Next Checks

1. **Boundary Case Analysis:** Systematically test Geometric Suspicion performance when hallucination rates exceed 75% to validate the claimed inversion effect and identify safe deferral thresholds.

2. **Embedding Sensitivity Study:** Compare performance using different sentence embedding models (beyond gte-Qwen2) and evaluate robustness to embedding quality variations across domains.

3. **Long-form Response Validation:** Extend the method to handle multi-sentence responses by testing hierarchical embedding strategies (e.g., embedding individual sentences then aggregating) and measuring impact on both global and local metrics.