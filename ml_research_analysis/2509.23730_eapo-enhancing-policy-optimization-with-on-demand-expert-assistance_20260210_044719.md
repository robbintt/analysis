---
ver: rpa2
title: 'EAPO: Enhancing Policy Optimization with On-Demand Expert Assistance'
arxiv_id: '2509.23730'
source_url: https://arxiv.org/abs/2509.23730
tags:
- expert
- reasoning
- policy
- training
- experts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Expert-Assisted Policy Optimization (EAPO),
  a novel reinforcement learning framework that enhances exploration by integrating
  on-demand expert assistance during training. Unlike prior methods where policies
  reason in isolation, EAPO allows the policy model to adaptively consult external
  experts at critical steps, yielding richer reward signals and more reliable reasoning
  trajectories.
---

# EAPO: Enhancing Policy Optimization with On-Demand Expert Assistance

## Quick Facts
- arXiv ID: 2509.23730
- Source URL: https://arxiv.org/abs/2509.23730
- Reference count: 20
- Introduces Expert-Assisted Policy Optimization (EAPO), a reinforcement learning framework that integrates on-demand expert assistance to enhance exploration and reasoning accuracy.

## Executive Summary
EAPO is a novel reinforcement learning framework designed to enhance policy optimization by allowing on-demand consultation with external expert models during training. Unlike prior approaches where policies reason in isolation, EAPO enables adaptive expert assistance at critical decision points, yielding richer reward signals and more reliable reasoning trajectories. The framework progressively encourages the policy to internalize expert knowledge, reducing reliance on consultation over time. Extensive experiments on mathematical reasoning benchmarks (AIME 2024, AIME 2025, AIMO 2025) demonstrate that EAPO consistently outperforms expert-assisted workflows, expert-distilled models, and self-exploratory RL baselines, achieving an average accuracy gain of 5 points and greater stability.

## Method Summary
EAPO integrates on-demand expert assistance into the reinforcement learning loop by allowing the policy model to adaptively consult external experts during training. This consultation is guided by a confidence-based mechanism, where the policy decides when expert input is needed. The framework uses a dual-reward structure: one reward for achieving correct final answers and another for consulting experts at critical steps. Over time, the policy is encouraged to internalize expert knowledge through a progressive reward shaping scheme, reducing reliance on external consultation. This approach enhances exploration and yields more reliable reasoning trajectories compared to self-exploratory or expert-distilled baselines.

## Key Results
- EAPO achieves an average accuracy gain of 5 points on mathematical reasoning benchmarks (AIME 2024, AIME 2025, AIMO 2025) compared to expert-assisted workflows, expert-distilled models, and self-exploratory RL baselines.
- The framework demonstrates greater stability and consistency across multiple training runs and competition-level math problems.
- EAPO outperforms all baselines in both accuracy and robustness, particularly in scenarios requiring complex reasoning steps.

## Why This Works (Mechanism)
EAPO enhances policy optimization by integrating on-demand expert assistance, which provides richer reward signals and more reliable reasoning trajectories. The adaptive consultation mechanism allows the policy to seek expert input at critical steps, improving exploration without compromising final reasoning quality. Progressive reward shaping encourages the policy to internalize expert knowledge over time, reducing dependency on external consultation. This approach addresses the limitations of self-exploratory RL, which often struggles with sparse rewards, and expert-distilled models, which may not generalize well to novel problems.

## Foundational Learning
- **Reinforcement Learning (RL)**: A framework for training agents to make sequences of decisions by maximizing cumulative rewards. Why needed: RL is the backbone of EAPO, enabling the policy to learn from interactions with the environment and expert feedback.
- **Expert Distillation**: The process of transferring knowledge from a larger, more capable model (expert) to a smaller, more efficient model (student). Why needed: Expert distillation provides a baseline for comparison and highlights the limitations of relying solely on pre-trained expert knowledge.
- **Reward Shaping**: The technique of modifying the reward function to guide the learning process more effectively. Why needed: Progressive reward shaping in EAPO encourages the policy to internalize expert knowledge, reducing reliance on consultation over time.
- **Adaptive Consultation**: A mechanism where the policy decides when to seek expert input based on confidence levels. Why needed: This allows EAPO to balance exploration and exploitation, improving reasoning accuracy without over-relying on external experts.

## Architecture Onboarding
- **Component Map**: Policy Model -> Adaptive Consultation Mechanism -> Expert Model -> Reward Shaping -> Internalization Loss -> Updated Policy
- **Critical Path**: The policy interacts with the environment, consults the expert when needed, receives dual rewards (answer correctness and consultation value), and updates its parameters through backpropagation.
- **Design Tradeoffs**: EAPO balances exploration (via expert consultation) with exploitation (via internalized knowledge). The adaptive consultation mechanism reduces computational overhead compared to constant expert involvement but may introduce latency in decision-making.
- **Failure Signatures**: Over-reliance on expert consultation could lead to poor generalization if the policy fails to internalize knowledge. Conversely, insufficient consultation may result in suboptimal reasoning trajectories.
- **First Experiments**: 1) Evaluate EAPO on a single math benchmark (e.g., AIME 2024) to establish baseline performance. 2) Conduct ablation studies to isolate the contribution of each EAPO component. 3) Test EAPO on a non-mathematical domain (e.g., code generation) to assess generalization.

## Open Questions the Paper Calls Out
None

## Limitations
- The reported performance improvements are based on a limited set of competition-level math benchmarks, which may not generalize to other reasoning domains or broader RL applications.
- The framework's reliance on external expert models raises concerns about scalability and cost in real-world deployments.
- There is limited discussion of potential overfitting to the specific expert model used or how EAPO would perform with different or less capable expert sources.

## Confidence
- **Accuracy Claims**: Medium
- **Generalizability**: Low
- **Scalability**: Low
- **Statistical Significance**: Low

## Next Checks
1. Conduct ablation studies to isolate the contribution of each EAPO component (consultation policy, reward shaping, internalization loss) to the overall performance gain.
2. Evaluate EAPO across multiple domains (e.g., code generation, logical reasoning) to assess generalization beyond math competition problems.
3. Perform statistical significance testing (e.g., paired t-tests) on accuracy differences between EAPO and baselines across multiple training runs.