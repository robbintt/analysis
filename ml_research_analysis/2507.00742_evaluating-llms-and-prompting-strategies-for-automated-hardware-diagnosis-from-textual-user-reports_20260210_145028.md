---
ver: rpa2
title: Evaluating LLMs and Prompting Strategies for Automated Hardware Diagnosis from
  Textual User-Reports
arxiv_id: '2507.00742'
source_url: https://arxiv.org/abs/2507.00742
tags:
- llms
- performance
- prompting
- hardware
- component
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluated 27 open-source LLMs (1B-72B parameters) and
  two proprietary models using four prompting strategies (Zero-Shot, Few-Shot, Chain-of-Thought,
  and CoT+Few-Shot) to classify textual user-reports of hardware faults. Models were
  tested on 853 user-generated reports covering eight hardware components.
---

# Evaluating LLMs and Prompting Strategies for Automated Hardware Diagnosis from Textual User-Reports

## Quick Facts
- arXiv ID: 2507.00742
- Source URL: https://arxiv.org/abs/2507.00742
- Reference count: 3
- Primary result: mistral-small-24b-instruct with Chain-of-Thought achieved highest F1-score of 0.76 for classifying hardware fault reports

## Executive Summary
This study systematically evaluates 27 open-source LLMs (1B-72B parameters) and two proprietary models using four prompting strategies to classify textual user-reports of hardware faults. Tested on 853 user-generated reports covering eight hardware components, the research identifies optimal model-size and prompting-strategy combinations for automated diagnostics. Results show that larger models with Chain-of-Thought prompting achieve the best performance, while knowledge distillation could enable efficient on-device diagnostics using compact models with NPUs.

## Method Summary
The research evaluated multiple LLMs using llama.cpp with Q4_K_M 4-bit quantization and 8,192 token context. Four prompting strategies were tested: Zero-Shot, Few-Shot (3 examples), Chain-of-Thought (step-by-step reasoning), and CoT+Few-Shot. Models generated predictions in dictionary format `{"component": "X"}` for 8 hardware categories. Performance was measured using F1-score with bootstrap resampling for confidence intervals across 98,948 total inferences.

## Key Results
- mistral-small-24b-instruct with Chain-of-Thought achieved highest F1-score of 0.76
- gemma-2-2b-it achieved 0.73 F1-score with only 2 billion parameters
- Smaller models (<7B) failed without Few-Shot prompting due to formatting issues, not reasoning limitations
- Performance plateaus around 30 billion parameters with diminishing returns beyond that point

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chain-of-Thought prompting improves diagnostic classification accuracy for larger models by structuring reasoning before prediction.
- Mechanism: CoT prompts instruct the model to "explain reasoning step-by-step" before outputting the final classification. This forces explicit intermediate reasoning about symptom-component relationships, reducing premature or hallucinated mappings.
- Core assumption: The model has sufficient capacity to maintain multi-step reasoning chains without losing coherence.
- Evidence anchors: Table 2 shows mistral-small-24b-instruct achieves 0.761 F1 with CoT vs. 0.742 with Zero-Shot; gpt-4o achieves 0.767 with CoT vs. 0.730 with Zero-Shot. Figure 4 shows non-parametric regression curves indicating CoT yields higher peak performance than Zero-Shot across model sizes.

### Mechanism 2
- Claim: Few-Shot prompting is critical for small models (< 7B parameters) primarily to enforce structured output formatting rather than improve reasoning.
- Mechanism: Small models often embed the correct answer in unstructured text but fail to produce the required dictionary format `{"component": "X"}`. Few-Shot examples demonstrate the exact output structure, enabling automated parsing.
- Core assumption: The task difficulty lies more in format compliance than semantic understanding for small models.
- Evidence anchors: Page 9 states very small models exhibit extremely low F1-Scores below 0.02 in Zero-Shot and CoT strategies due to failure to generate structured outputs. Table 2: qwen2.5-1.5b-instruct scores 0.000 (Zero-Shot), 0.673 (Few-Shot); yi-6b scores 0.013 (CoT), 0.702 (Few-Shot).

### Mechanism 3
- Claim: Model performance scales with parameter count but exhibits diminishing returns beyond ~30B parameters for this classification task.
- Mechanism: Larger models capture more nuanced symptom-component associations from pre-training, but the task complexity (8-class classification from short text) saturates, limiting marginal gains.
- Core assumption: The dataset distribution and class boundaries are learnable without task-specific fine-tuning.
- Evidence anchors: Page 8 reveals general performance improvement as model size increases, but growth levels off near 30 billion parameters. Figure 5 Pareto frontier identifies gemma-2-2b-it (0.73 F1, 2B params) and mistral-small-24b-instruct (0.76 F1, 24B params) as optimal trade-offs.

## Foundational Learning

- Concept: F1-Score as harmonic mean of precision and recall
  - Why needed here: Class imbalance in hardware fault reports (some components more common) makes accuracy misleading; F1 balances false positives and false negatives.
  - Quick check question: If a model predicts "Storage" for every input and 30% of reports are actually Storage-related, what is the F1-Score for the Storage class?

- Concept: Pareto frontier for multi-objective optimization
  - Why needed here: Selecting models requires balancing performance (F1) against resource constraints (VRAM, latency); Pareto-optimal models are those where no smaller model performs better.
  - Quick check question: If Model A has 10B params and 0.70 F1, Model B has 5B params and 0.69 F1, and Model C has 8B params and 0.68 F1, which are Pareto-optimal?

- Concept: Quantization (Q4_K_M) for memory-efficient inference
  - Why needed here: 4-bit quantization reduces VRAM by ~4x with minimal performance loss, enabling deployment on edge devices with NPUs.
  - Quick check question: A 7B parameter model in FP16 requires ~14GB VRAM. Estimate VRAM needed with 4-bit quantization.

## Architecture Onboarding

- Component map: User report -> Prompt template -> LLM engine -> Output parser -> Component classification
- Critical path:
  1. User submits free-text complaint
  2. Prompt constructed based on selected strategy (ZS/FS/CoT/CoT+FS)
  3. LLM generates response including reasoning (if CoT) and prediction
  4. Parser extracts component dictionary; rejects malformed outputs
  5. Downstream diagnostic pipeline receives component label

- Design tradeoffs:
  - CoT vs. Zero-Shot: CoT improves accuracy (~3-4% F1) but increases output tokens and latency
  - Few-Shot vs. Zero-Shot: Few-Shot essential for small models but adds prompt tokens (~500-1000)
  - Large vs. small models: 24B+ models peak at ~0.76 F1; 2B models achieve ~0.73 F1 with 6x less VRAM
  - On-device vs. cloud: Small quantized models (1-2B) fit in NPU memory; large models require GPU servers

- Failure signatures:
  - Zero-Shot with small models: F1 near 0.0 due to missing dictionary format in output
  - CoT without Few-Shot on small models: Model outputs reasoning but no structured prediction
  - Overly strict parsing: Rejects correct predictions embedded in explanatory text
  - Class confusion: "Memory" vs. "Motherboard" for BSOD reports; "Video Card" vs. "Motherboard" for display issues

- First 3 experiments:
  1. Baseline comparison: Run all four prompting strategies (ZS, FS, CoT, CoT+FS) on gemma-2-2b-it and mistral-small-24b-instruct using the FACTO dataset; measure F1, output token count, and latency.
  2. Format robustness test: Evaluate gemma-2-2b-it with Zero-Shot and Few-Shot; manually inspect 50 failure cases to classify errors (wrong label, missing format, ambiguous input).
  3. On-device feasibility: Deploy llama-3.2-1b-instruct with Q4_K_M quantization on a laptop with NPU (e.g., Snapdragon X Elite or Apple M4); measure VRAM usage and inference latency under Few-Shot prompting.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can knowledge distillation from larger models (e.g., mistral-small-24b) effectively transfer diagnostic capabilities to compact models for on-device hardware diagnosis?
- Basis in paper: The authors explicitly propose "developing knowledge distillation pipelines" as a primary direction to transfer expertise from larger to smaller models.
- Why unresolved: The study only evaluates pre-trained models using prompting strategies; no distillation or fine-tuning experiments were conducted.
- What evidence would resolve it: Benchmarks showing distilled 1-2B parameter models achieving F1-scores comparable to the 0.76 baseline of the 24B model.

### Open Question 2
- Question: How can high-quality labeled examples be generated from high-performing models to improve diagnostic generalization?
- Basis in paper: The authors identify "generating high-quality labeled examples from high-performing models" as a "key challenge" for improving generalization.
- Why unresolved: The paper highlights the need for better data but does not propose or validate a specific methodology for synthetic data generation.
- What evidence would resolve it: A validated pipeline where synthetic labels generated by LLMs successfully train smaller models without introducing hallucination or bias.

### Open Question 3
- Question: Do the observed prompting strategy trends generalize to datasets with a broader taxonomy of hardware components or multi-fault reports?
- Basis in paper: The paper notes the evaluation is limited to "eight hardware component categories" but suggests the results could apply to other failure types.
- Why unresolved: The FACTO dataset represents a restricted scope, leaving model performance on complex, overlapping, or unseen hardware failures unverified.
- What evidence would resolve it: Consistency of the CoT and Few-Shot performance hierarchy when applied to a dataset with a significantly larger set of component labels.

## Limitations

- Dataset size of 853 reports across 8 categories raises concerns about robust evaluation and potential overfitting to specific symptom descriptions
- FACTO dataset not publicly available, preventing independent validation of results
- Study focuses on English-language reports, limiting applicability to multilingual environments common in global technical support

## Confidence

- High Confidence: The finding that Few-Shot prompting is essential for small models (<7B parameters) due to formatting failures rather than reasoning limitations
- Medium Confidence: The claim that performance plateaus around 30 billion parameters for this diagnostic task
- Medium Confidence: The assertion that knowledge distillation could enable efficient on-device diagnostics

## Next Checks

1. Evaluate top-performing models (gemma-2-2b-it, mistral-small-24b-instruct) on an independently collected hardware diagnostic dataset to assess cross-dataset performance and robustness to symptom variation.

2. Deploy the Pareto-optimal small models (gemma-2-2b-it with Q4_K_M quantization) on actual edge hardware (Snapdragon X Elite NPU or Apple M4 Neural Engine) and measure real-world latency, VRAM usage, and thermal performance under continuous inference.

3. Systematically vary Few-Shot example selection methods (random vs. diversity-based vs. hard examples) and Chain-of-Thought template structure to quantify the contribution of each component to overall performance gains.