---
ver: rpa2
title: Retrieval-Augmented Purifier for Robust LLM-Empowered Recommendation
arxiv_id: '2504.02458'
source_url: https://arxiv.org/abs/2504.02458
tags:
- item
- user
- collaborative
- recommendation
- perturbations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RETURN, a framework designed to enhance the
  robustness of LLM-empowered recommender systems against adversarial attacks. The
  core idea is to leverage external collaborative knowledge from item-item co-occurrence
  graphs to identify and filter out malicious perturbations in user interaction histories.
---

# Retrieval-Augmented Purifier for Robust LLM-Empowered Recommendation

## Quick Facts
- arXiv ID: 2504.02458
- Source URL: https://arxiv.org/abs/2504.02458
- Authors: Liangbo Ning; Wenqi Fan; Qing Li
- Reference count: 40
- Primary result: Up to 48.8% improvement in defense performance (D-H@5) against adversarial attacks while maintaining recommendation accuracy

## Executive Summary
This paper introduces RETURN, a framework designed to enhance the robustness of LLM-empowered recommender systems against adversarial attacks. The core idea is to leverage external collaborative knowledge from item-item co-occurrence graphs to identify and filter out malicious perturbations in user interaction histories. Specifically, RETURN employs a retrieval-augmented perturbation positioning strategy to identify potential perturbations and a retrieval-augmented denoising strategy to cleanse them using deletion or replacement based on the external collaborative signals. Extensive experiments on three real-world datasets (ML1M, Taobao, LastFM) demonstrate that RETURN significantly improves the robustness of two representative LLM-based recommender systems (P5 and TALLRec), achieving up to 48.8% improvement in defense performance (D-H@5) and maintaining high recommendation accuracy even under strong adversarial attacks.

## Method Summary
RETURN operates by first constructing an item-item co-occurrence graph from historical user interaction data to capture collaborative relationships between items. When a user interaction sequence is received, the framework uses retrieval-augmented positioning to identify potential malicious perturbations by comparing the sequence against the external knowledge graph. Once perturbations are identified, a retrieval-augmented denoising module cleanses them through either deletion or replacement, guided by the collaborative signals from the external graph. This approach effectively purifies the input before it reaches the LLM-based recommender system, making it more robust to various attack types including direct attacks and indirect poisoning attacks.

## Key Results
- RETURN achieves up to 48.8% improvement in defense performance (D-H@5) across three real-world datasets
- The framework maintains high recommendation accuracy even under strong adversarial attacks
- Demonstrated effectiveness on two representative LLM-based recommender systems (P5 and TALLRec)
- Works in a plug-and-play manner without requiring modifications to the underlying LLM architecture

## Why This Works (Mechanism)
RETURN leverages external collaborative knowledge to provide a robustness signal that is orthogonal to the internal representations learned by LLM-based recommenders. By using item-item co-occurrence graphs, the framework can detect anomalies in user interaction sequences that deviate from normal collaborative patterns. The retrieval-augmented positioning identifies which interactions are most likely to be adversarial by measuring their consistency with the external knowledge, while the denoising module then either removes these suspicious interactions or replaces them with more plausible alternatives based on collaborative signals.

## Foundational Learning

**Item-Item Co-occurrence Graphs**: Networks where nodes represent items and edges represent how frequently items appear together in user interactions. Why needed: Provides external collaborative knowledge that captures item relationships beyond what any single user's history reveals. Quick check: Verify the graph construction captures meaningful item associations by checking if connected items have similar user demographics or contextual usage patterns.

**Retrieval-Augmented Positioning**: A mechanism that uses external knowledge to identify positions in a sequence that are likely to contain perturbations. Why needed: Enables precise localization of adversarial modifications without requiring full sequence reconstruction. Quick check: Measure precision and recall of perturbation detection on sequences with known injected attacks.

**Retrieval-Augmented Denoising**: A strategy that uses external knowledge to either delete or replace identified perturbations. Why needed: Provides context-aware cleansing that maintains sequence coherence while removing malicious content. Quick check: Evaluate recommendation performance when using only deletion versus only replacement strategies.

## Architecture Onboarding

**Component Map**: User Interaction Sequence -> Retrieval-Augmented Positioning -> Perturbation Identification -> Retrieval-Augmented Denoising -> Cleansed Sequence -> LLM-Based Recommender

**Critical Path**: The sequence purification pipeline is the critical path, as it directly affects the quality of recommendations. The external knowledge graph construction is a prerequisite that must be maintained separately but is not on the real-time critical path.

**Design Tradeoffs**: Deletion trades completeness for certainty (removing potentially useful but suspicious interactions), while replacement trades precision for coverage (maintaining sequence length but potentially introducing noise). The framework must balance these based on attack severity and data sparsity.

**Failure Signatures**: Degradation in recommendation quality when external knowledge becomes stale or when collaborative patterns shift due to legitimate changes in user behavior. Over-aggressive denoising may lead to loss of genuine user preferences, while under-aggressive denoising fails to protect against attacks.

**First 3 Experiments**:
1. Test positioning accuracy by injecting known perturbations at various positions and measuring detection rates
2. Compare deletion versus replacement strategies on sequences with different levels of contamination
3. Evaluate performance degradation when the external knowledge graph is intentionally corrupted with false co-occurrence information

## Open Questions the Paper Calls Out
None

## Limitations
- Performance may degrade when external collaborative knowledge is incomplete, noisy, or unavailable
- Computational overhead of retrieval-augmented mechanisms is not discussed, which could impact real-time recommendation systems
- Does not explore defenses against attacks specifically targeting the external knowledge sources themselves

## Confidence
- **High confidence**: Experimental methodology is sound with multiple datasets and attack types, with clear baseline comparisons supporting quantitative robustness improvements
- **Medium confidence**: Effectiveness of denoising strategy demonstrated but could benefit from deeper analysis of optimal strategy selection
- **Medium confidence**: Plug-and-play claims supported by experiments, but real-world deployment with varying data quality and computational constraints not explored

## Next Checks
1. Evaluate RETURN's performance when external collaborative knowledge is partially corrupted or incomplete to test robustness to knowledge source failures
2. Conduct ablation studies to quantify computational overhead introduced by retrieval-augmented mechanisms and assess impact on real-time recommendation performance
3. Design adaptive attacks specifically targeting the external knowledge retrieval component to test whether adversaries can bypass purification by poisoning collaborative knowledge sources