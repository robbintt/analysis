---
ver: rpa2
title: A Novel Hat-Shaped Device-Cloud Collaborative Inference Framework for Large
  Language Models
arxiv_id: '2503.18989'
source_url: https://arxiv.org/abs/2503.18989
tags:
- inference
- delay
- devices
- phase
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper proposes HAT, a device-cloud collaborative inference
  framework for large language models (LLMs) that combines U-shaped inference for
  privacy and speculative decoding for low latency. HAT partitions the LLM into three
  submodels: input and output submodels on the device, and the middle submodel in
  the cloud, along with a lightweight adapter network for speculative decoding.'
---

# A Novel Hat-Shaped Device-Cloud Collaborative Inference Framework for Large Language Models

## Quick Facts
- arXiv ID: 2503.18989
- Source URL: https://arxiv.org/abs/2503.18989
- Reference count: 40
- Key outcome: HAT reduces time-to-first-token (TTFT) by 41% to 54% and time-between-tokens (TBT) by 41% to 77% compared to baseline methods

## Executive Summary
HAT is a device-cloud collaborative inference framework for LLMs that combines U-shaped inference for privacy and speculative decoding for low latency. The framework partitions the LLM into three submodels: input and output submodels on the device, and the middle submodel in the cloud, along with a lightweight adapter network for speculative decoding. HAT introduces a prompt chunking mechanism that segments long prompts into smaller chunks, enabling parallel processing and transmission. The system dynamically determines optimal chunk sizes based on device and cloud state information.

## Method Summary
HAT partitions an LLM into three submodels: input/output layers on the device and the middle layers in the cloud. A lightweight adapter network is trained via knowledge distillation to enable speculative decoding. The framework introduces prompt chunking to reduce TTFT by overlapping communication and computation delays, and implements parallel drafting to utilize device idle time during cloud verification. The system uses MPI for hidden state transmission and Docker Swarm for orchestration.

## Key Results
- TTFT reduced by 41% to 54% compared to baseline methods
- TBT reduced by 41% to 77% compared to baseline methods
- Validated on testbed with 30 NVIDIA Jetson devices and 8 NVIDIA A6000 GPUs

## Why This Works (Mechanism)

### Mechanism 1: U-Shaped Speculative Decoding
- **Claim:** Integrating U-shaped inference with speculative decoding appears to reduce time-between-tokens (TBT) while maintaining data privacy by keeping raw tokens on-device.
- **Mechanism:** The LLM is partitioned into three submodels: input/output layers on the device and the computationally intensive middle layers in the cloud. A lightweight "adapter network" is trained via knowledge distillation to bridge the device-side layers into a functional Small Language Model (SLM). This SLM generates draft tokens locally, which are then verified in a single step by the cloud-based middle submodel using hidden states rather than raw tokens.
- **Core assumption:** The adapter network can sufficiently align the SLM's output distribution with the LLM's middle submodel to ensure a high draft acceptance rate.
- **Evidence anchors:**
  - [abstract] Mentions combining U-shaped inference and speculative decoding.
  - [Section 3.4] Describes the construction of the draft model $w_S$ using the adapter $\Lambda$ and knowledge distillation loss.
  - [corpus] General consistency found in "Collaboration of Large Language Models and Small Recommendation Models," but specific validation for the HAT adapter mechanism is absent in the provided corpus.
- **Break condition:** If the distribution divergence between the SLM (with adapter) and the LLM middle layers is too high, verification rejection rates increase, negating latency gains.

### Mechanism 2: Pipelined Prompt Chunking
- **Claim:** Segmenting long prompts into smaller chunks likely reduces Time-to-First-Token (TTFT) by overlapping communication and computation delays.
- **Mechanism:** Instead of transmitting the hidden states of a long prompt in one costly block, prompts are split into chunks. The system pipelines the process so that the transmission of chunk $N$ occurs simultaneously with the cloud computation of chunk $N-1$.
- **Core assumption:** The transmission delay for a chunk of size $X$ is roughly equivalent to or greater than the cloud computation delay for that chunk, allowing for effective overlap.
- **Evidence anchors:**
  - [abstract] Introduces the prompt chunking mechanism for parallel transmission.
  - [Section 3.3] Details Equation 3, which balances transmission time against computation time to determine optimal chunk size.
  - [Section 2.3] Figure 1d shows the trade-off between chunk size and delay reduction.
- **Break condition:** If the chunk size is too small, overhead dominates; if too large, the pipeline stalls waiting for computation, eroding TTFT benefits.

### Mechanism 3: Parallel Drafting
- **Claim:** Utilizing device idle time during cloud verification may further lower TBT by pre-generating subsequent draft candidates.
- **Mechanism:** During the cloud verification phase (which incurs network latency), the device does not remain idle. Instead, it speculatively generates candidate draft sequences for the *next* round using the top-k tokens from the previous step. If the cloud verification result matches one of these top-k candidates, the pre-generated sequence is utilized immediately.
- **Core assumption:** The probability of the verified token falling within the top-k candidates is sufficiently high to make the pre-computation useful.
- **Evidence anchors:**
  - [Section 3.5] Describes the parallel drafting module and the calculation of inference steps $\lambda_i$.
  - [Figure 5] Visualizes the timeline where drafting (D) occurs during verification (V).
  - [corpus] Weak support; neighbor papers like "EcoAgent" discuss device-cloud collaboration but do not specifically validate this parallel drafting optimization.
- **Break condition:** If network latency is near-zero or device compute is overwhelmed, the overhead of managing candidate sequences might exceed the latency savings.

## Foundational Learning

- **Concept: Knowledge Distillation**
  - **Why needed here:** This is the technique used to train the lightweight "Adapter Network" so that the on-device SLM mimics the behavior of the cloud LLM, enabling the speculative decoding loop.
  - **Quick check question:** How does the Smooth L1 loss function help align the hidden states between the teacher (LLM) and student (SLM)?

- **Concept: U-Shaped Split Inference**
  - **Why needed here:** This defines the privacy boundary of the architecture, determining which model layers reside on the device versus the cloud to prevent raw data transmission.
  - **Quick check question:** Why does transmitting "hidden states" (tensors) protect privacy better than transmitting "tokens," and what is the bandwidth trade-off?

- **Concept: Pipeline Parallelism**
  - **Why needed here:** Essential for understanding the Prompt Chunking mechanism, where the goal is to keep both the network link and the GPU busy simultaneously.
  - **Quick check question:** In the context of prompt chunking, what happens to the speedup if the transmission time is significantly faster than the computation time?

## Architecture Onboarding

- **Component map:** Input Submodel -> Adapter Network -> Output Submodel (on device) -> Middle Submodel (on cloud)
- **Critical path:**
  1.  **Prefill:** Request arrives → Prompt Chunking → Device processes chunks → Upload Hidden States (overlapped) → Cloud processes chunks → Download First Token.
  2.  **Decode:** Device SLM drafts tokens → Upload Hidden States → Cloud verifies (Device parallel drafts next step) → Download Verified Tokens.
- **Design tradeoffs:**
  - **Chunk Size:** Small chunks reduce per-step computation but increase transmission frequency/overhead; large chunks risk pipeline stalls.
  - **Adapter Complexity:** A larger adapter might mimic the LLM better but slows down on-device inference.
  - **Draft Length ($n$):** Longer drafts increase potential speedup but risk higher rejection rates and wasted compute.
- **Failure signatures:**
  - **Stalled Pipeline:** TTFT spikes despite chunking; likely caused by incorrect chunk size calculation (Equation 3) where computation time far exceeds transmission time.
  - **Low Acceptance Rate:** TBT remains high; indicates the Adapter Network is poorly distilled or distribution drift has occurred.
  - **Batch Interference:** High variance in TBT suggests the State Monitoring module is not effectively isolating prefill requests from decode requests.
- **First 3 experiments:**
  1.  **Latency Profiling:** Measure transmission vs. computation time for varying prompt lengths to validate the chunk size optimizer.
  2.  **Adapter Evaluation:** Run inference with the SLM alone vs. the full HAT framework to determine the acceptance rate of draft tokens.
  3.  **Ablation Study:** Disable Parallel Drafting and Prompt Chunking separately to quantify their individual contributions to TTFT and TBT reduction.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the HAT framework perform when applied to non-transformer or non-decoder-only LLM architectures?
- Basis in paper: [explicit] The authors state in the conclusion, "In the future, we plan to explore the application of HAT in more complex scenarios and verify its performance on different LLM architectures."
- Why unresolved: The experiments were restricted to the LLaMA-family (Vicuna-7B/13B), leaving the efficacy of the U-shaped split and adapter network on architectures like Mixture-of-Experts (MoE) or Encoder-Decoder models unknown.
- What evidence would resolve it: Evaluation results of HAT implemented on diverse architectures such as T5 or Mixtral, analyzing changes in TTFT/TBT and adapter alignment accuracy.

### Open Question 2
- Question: What is the vulnerability level of the transmitted hidden states to reconstruction attacks, and how would privacy-preserving noise affect the speculative decoding acceptance rate?
- Basis in paper: [inferred] While the paper claims privacy by keeping raw tokens on-device, it transmits hidden states directly. It contrasts itself with methods that "add noise to intermediate data" (Section 2.3), implying HAT does not, yet it does not analyze the potential for data leakage from these states.
- Why unresolved: The paper prioritizes latency and accuracy, assuming hidden states are secure without quantifying the privacy budget or testing against model inversion attacks.
- What evidence would resolve it: A formal privacy analysis quantifying the mutual information between raw input tokens and transmitted hidden states, or experiments showing the latency/accuracy trade-off when adding differential privacy noise.

### Open Question 3
- Question: Does the lightweight adapter network maintain high alignment and acceptance rates when user prompts differ significantly from the distillation training data?
- Basis in paper: [inferred] Section 3.4 notes the adapter is trained on ShareGPT, while Section 4.1 evaluates on SpecBench and CNN/DM. The performance relies on the adapter mimicking the cloud model, but the impact of domain shift on the speculative decoding "accept length" is not explicitly isolated.
- Why unresolved: Knowledge distillation often struggles with out-of-distribution data; a significant domain shift could lower the acceptance rate, negating the speedup benefits of speculative decoding.
- What evidence would resolve it: Ablation studies measuring draft acceptance rates specifically on datasets stylistically distinct from ShareGPT (e.g., code generation or highly specialized medical texts).

## Limitations
- The performance claims rely heavily on the adapter network's ability to maintain distribution alignment between the device-side SLM and cloud-side LLM middle layers.
- The prompt chunking mechanism assumes predictable transmission-computation overlap ratios, but lacks sensitivity analysis under variable network conditions.
- The evaluation uses relatively small Vicuna models (7B and 13B) on specialized hardware, with scaling behavior for larger models remaining unvalidated.

## Confidence

**High Confidence:**
- The core concept of U-shaped inference for privacy preservation follows well-established principles in split computing.
- The use of knowledge distillation for creating SLMs from LLMs is a proven technique with extensive prior validation.

**Medium Confidence:**
- The speculative decoding acceleration claims (41-77% TBT reduction) depend on achieving high draft acceptance rates, which the paper shows but doesn't fully characterize under varying workloads.
- The prompt chunking mechanism's effectiveness is theoretically sound but lacks comprehensive validation across different network conditions and prompt distributions.

**Low Confidence:**
- The parallel drafting optimization's contribution is difficult to assess due to limited discussion of its overhead and failure modes.
- The state monitoring and dynamic adjustment mechanisms lack detailed validation showing robustness to rapid state changes.

## Next Checks
1. **Adapter Network Robustness Test:** Conduct stress testing by generating prompts from diverse domains (code, technical writing, casual conversation) and measuring acceptance rates and distribution divergence between SLM and LLM outputs.
2. **Network Condition Sensitivity Analysis:** Systematically vary network bandwidth (e.g., 10-100 Mbps) and latency (1-100ms) while measuring TTFT and TBT to identify operational boundaries.
3. **Scaling Behavior Validation:** Test the framework with incrementally larger models (13B → 30B → 70B) on the same hardware to establish whether the 41-54% TTFT reduction scales proportionally.