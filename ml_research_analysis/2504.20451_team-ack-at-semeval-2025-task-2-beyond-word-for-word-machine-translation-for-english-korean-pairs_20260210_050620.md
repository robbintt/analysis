---
ver: rpa2
title: 'Team ACK at SemEval-2025 Task 2: Beyond Word-for-Word Machine Translation
  for English-Korean Pairs'
arxiv_id: '2504.20451'
source_url: https://arxiv.org/abs/2504.20451
tags:
- translation
- entity
- bleu
- wang
- comet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Team ACK evaluated 13 machine translation models, including large
  language models (LLMs) and traditional multilingual translation systems, on English-Korean
  translation of knowledge-intensive and entity-dense text. Automatic metrics (BLEU,
  COMET, M-ETA) and human evaluation by bilingual annotators were used.
---

# Team ACK at SemEval-2025 Task 2: Beyond Word-for-Word Machine Translation for English-Korean Pairs

## Quick Facts
- arXiv ID: 2504.20451
- Source URL: https://arxiv.org/abs/2504.20451
- Reference count: 25
- Team ACK evaluated 13 MT models (LLMs and traditional systems) on English-Korean entity-dense translation, finding LLMs generally outperform traditional MT but all models struggle with culturally-adapted entity translation.

## Executive Summary
Team ACK conducted a comprehensive evaluation of 13 machine translation models on English-Korean translation of knowledge-intensive, entity-dense text. The study combined automatic metrics (BLEU, COMET, M-ETA) with human evaluation by bilingual annotators to assess translation quality. LLMs demonstrated superior performance compared to traditional multilingual translation systems, but all models faced significant challenges with entity translation requiring cultural adaptation (transcreation vs. transliteration). The research revealed that entity translation quality varies systematically by entity type and popularity level, while automatic metrics often fail to capture these nuanced errors.

## Method Summary
The evaluation used the XC-Translate dataset (5,082 English-Korean pairs) with 13 models spanning proprietary LLMs (GPT-4o, o1, Claude 3.5, Gemini, Grok), open LLMs (Llama 3, DeepSeek R1), and traditional MT (mBART-50, NLLB-200). All models were evaluated using identical prompts. Automatic metrics included BLEU, COMET, and M-ETA (entity-level accuracy). Human evaluation was conducted on 650 samples (50 per model) by two bilingual annotators with 5+ years residence in both countries. The analysis stratified results by 14 entity types and 5 popularity tiers based on Wikipedia page views.

## Key Results
- LLMs outperformed traditional MT models across all automatic metrics and human evaluation
- Incorrect responses and entity name errors were the most common translation failures (308/650 and 266/650 samples respectively)
- Entity translation quality varied significantly by type (M-ETA scores: 0.08 for Book Series vs. 0.64-0.66 for Plant/Natural place)
- Entity popularity influenced entity-level accuracy (M-ETA: 0.45-0.60 for high vs. 0.25-0.36 for low popularity) but not surrounding sentence translation
- BLEU and COMET moderately correlated with human correctness (point-biserial 0.41, p=3.54e-28) but missed entity-level errors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Entity translation quality varies with entity type due to differing linguistic adaptation requirements.
- Mechanism: Entity types like "Book Series" more often permit literal/phonetic/word-for-word conversion, while types like "Plant" and "Natural place" require unique language-specific names, creating differential difficulty independent of training frequency.
- Core assumption: The linguistic properties characteristic of specific entity categories determine translation difficulty beyond mere popularity effects.
- Evidence anchors: [Section 5.1] "Entity type Book Series contains a higher concentration of names that could be simply literally, phonetically, or word-for-word translated, whereas entity type Plant and Natural place presents more challenges like requiring unique language-specific names." [Section 5.1] Table 4 shows M-ETA scores of 0.08 for Book Series vs. 0.64-0.66 for Plant/Natural place across models.

### Mechanism 2
- Claim: Entity popularity influences entity-level translation quality but not surrounding sentence translation.
- Mechanism: Frequently-viewed entities (per Wikipedia page views as training proxy) receive more accurate translations (higher M-ETA), but standard n-gram and semantic metrics (BLEU, COMET) remain stable because entities constitute small token proportions within full sentences.
- Core assumption: Wikipedia page view statistics serve as valid proxy for entity frequency in proprietary model training corpora.
- Evidence anchors: [Section 5.1] "M-ETA demonstrates a notable variation of 0.00224 across the popularity spectrum" while "traditional metrics like BLEU and COMET remain relatively stable." [Section 5.1] High popularity entities show M-ETA of 0.45-0.60 vs. 0.25-0.36 for low popularity across top models.

### Mechanism 3
- Claim: Automatic metrics moderately correlate with human judgment but miss fine-grained entity errors.
- Mechanism: BLEU and COMET capture overall semantic adequacy (point-biserial correlation 0.41 with human correctness judgments), but entity-level errors constitute small token fractions that dilute metric sensitivity; M-ETA partially addresses this by focusing exclusively on entity accuracy.
- Core assumption: Human binary correctness judgments reliably ground translation quality assessment.
- Evidence anchors: [Section 5.2] "BLEU and COMET demonstrate a moderately positive correlation with binary human evaluations... point-biserial correlation coefficient of 0.41 with a p-value of 3.54e-28." [Section 5.2] "M-ETA correctly identifies 88.7% samples containing entity translation errors according to human labels."

## Foundational Learning

- Concept: **Transcreation vs. Transliteration**
  - Why needed here: The paper's central problem—entity translation—requires distinguishing phonetic conversion (transliteration, e.g., "Rotten Tomatoes" → "로튼 토마토") from culturally-adapted translation (transcreation, e.g., preserving brand recognition).
  - Quick check question: For "McDonald's" in Korean, would you transliterate (맥도날드) or transcreate? What determines the choice?

- Concept: **Entity Type Taxonomy (Wikidata/NER)**
  - Why needed here: Performance analysis disaggregates by 14 entity types (Plant, Book Series, Person, etc.); understanding these categories is prerequisite to interpreting Table 4's cross-type variation.
  - Quick check question: Why might "Fictional Entity" and "Person" require different translation strategies?

- Concept: **BLEU/COMET/M-ETA Metric Families**
  - Why needed here: The paper's methodological contribution is combining three metric types with human evaluation; each captures different quality dimensions with different blind spots.
  - Quick check question: If a translation has perfect BLEU but incorrect entity names, what does this reveal about metric limitations?

## Architecture Onboarding

- Component map:
  XC-Translate dataset (5,082 pairs) -> 13 models (LLMs + traditional MT) -> Automatic metrics (BLEU, COMET, M-ETA) + Human annotation -> Error taxonomy analysis -> Stratified analysis by entity type/popularity

- Critical path:
  1. Dataset preparation → 2. Model inference with identical prompts → 3. Automatic metric computation → 4. Human annotation with error categorization → 5. Stratified analysis by entity type/popularity → 6. Metric-human correlation analysis

- Design tradeoffs:
  - Human evaluation scope (650/5,082 = 12.8% sampling) balances depth vs. cost; may miss rare error patterns
  - Annotator selection (5+ years residence in both countries) prioritizes cultural fluency over inter-annotator agreement measurement
  - Wikipedia page views as popularity proxy enables analysis but assumes training data correlation

- Failure signatures:
  - "Incorrect Response" errors (308/650) suggest instruction-following failures rather than translation failures—check prompt formatting
  - Claude 3.5 variants show anomalously low BLEU (0.16-0.20) despite reasonable COMET—investigate tokenization or output format issues
  - DeepSeek R1 and Llama 3 near-zero scores across metrics indicate fundamental capability gaps for this language pair

- First 3 experiments:
  1. **Entity-type stratified prompting**: Add entity type hints to prompts (e.g., "This text contains a Plant entity named X") to test whether explicit type awareness improves transcreation vs. transliteration decisions.
  2. **Popularity-controlled sampling**: Construct matched pairs of high/low popularity entities within the same type to isolate popularity effects from type effects.
  3. **Metric ensemble validation**: Train a simple classifier to predict human correctness from (BLEU, COMET, M-ETA) combinations to identify complementary metric strengths; compare against individual metric correlations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the error patterns and metric discrepancies identified in English-Korean translation generalize to morphologically rich or typologically distinct language families like Semitic (e.g., Arabic) or Altaic (e.g., Turkish) languages?
- Basis in paper: [explicit] The limitations section states that the paper lacks investigation into other language families and suggests "Future work should focus on related in-depth analysis on other languages and dialects... to develop a robust and generalizable understanding of errors."
- Why unresolved: The current study is restricted to English-Korean pairs, making it difficult to distinguish which failures are specific to the Korean language structure versus general LLM limitations in cultural adaptation.
- What evidence would resolve it: A replication of this study's evaluation protocol using the same 13 models on English-to-Arabic and English-to-Turkish entity-dense datasets.

### Open Question 2
- Question: Does the error taxonomy established in this study remain sufficient when applied to long-form or narrative text genres, or do new error categories emerge?
- Basis in paper: [explicit] The authors acknowledge that their "evaluation was restricted to a controlled set of question templates" and suggest that "a broader analysis of diverse text genres, such as long-form documents or narrative content, would likely reveal additional error categories."
- Why unresolved: The current dataset relies on short, knowledge-intensive questions, which may not expose translation failures that occur over longer contexts, such as coreference resolution or narrative consistency errors.
- What evidence would resolve it: A follow-up human evaluation using the proposed taxonomy on a dataset of long-form articles or stories containing culturally nuanced entities.

### Open Question 3
- Question: How can automatic evaluation metrics be modified to better penalize literal or phonetic translations of low-frequency entities that standard metrics like BLEU and COMET currently miss?
- Basis in paper: [inferred] The authors conclude that "standard metrics such as BLEU and COMET fail to capture these nuances" of entity popularity and type, noting the "necessity for more fine-grained evaluation metrics."
- Why unresolved: The paper demonstrates a mismatch where automatic scores remain stable despite significant drops in entity translation quality for specific popularity segments, indicating current metrics are insensitive to these specific errors.
- What evidence would resolve it: A new metric or weighting scheme that shows a strong correlation with human annotator scores specifically for low-popularity or culturally specific entities, outperforming current M-ETA results.

## Limitations
- Human evaluation covers only 12.8% of dataset (650/5,082 pairs), potentially missing rare error patterns
- Wikipedia page views used as popularity proxy without direct validation of correlation with training data frequency
- Error taxonomy relies on binary correctness judgments without inter-annotator agreement measurement
- Study restricted to English-Korean pair, limiting generalizability to other language families

## Confidence
- **High Confidence:** LLMs outperform traditional MT models (consistent automatic and human evaluation results)
- **Medium Confidence:** Entity type affects translation difficulty (supported by performance variation but potential frequency confounds)
- **Medium Confidence:** Popularity affects entity translation quality (M-ETA variation observed but proxy assumption unverified)

## Next Checks
1. **Training Data Correlation Analysis:** Validate the Wikipedia page view popularity proxy by analyzing whether training corpora (where available) show correlation between entity frequency and Wikipedia popularity metrics.
2. **Inter-Annotator Agreement Measurement:** Conduct reliability analysis on human annotations to quantify agreement levels and assess the stability of binary correctness judgments across annotators.
3. **Error Type Impact Analysis:** Perform controlled experiments varying entity prominence in source sentences (e.g., entity-only vs. entity-in-context) to isolate when transcreation vs. transliteration strategies are most critical for translation quality.