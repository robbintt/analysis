---
ver: rpa2
title: 'GenTREC: The First Test Collection Generated by Large Language Models for
  Evaluating Information Retrieval Systems'
arxiv_id: '2501.02408'
source_url: https://arxiv.org/abs/2501.02408
tags:
- documents
- collections
- gentrec
- test
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GenTREC, the first test collection generated
  entirely by a large language model (LLM), eliminating the need for manual relevance
  judgments in information retrieval (IR) evaluation. The method generates documents
  based on the assumption that LLM-generated content is inherently relevant to its
  prompt.
---

# GenTREC: The First Test Collection Generated by Large Language Models for Evaluating Information Retrieval Systems

## Quick Facts
- arXiv ID: 2501.02408
- Source URL: https://arxiv.org/abs/2501.02408
- Reference count: 40
- Key outcome: First LLM-generated test collection eliminating manual relevance judgments, producing system rankings similar to TREC collections at 1000× lower cost

## Executive Summary
This paper introduces GenTREC, the first test collection generated entirely by a large language model (LLM), eliminating the need for manual relevance judgments in information retrieval (IR) evaluation. The method generates documents based on the assumption that LLM-generated content is inherently relevant to its prompt. Using existing TREC topics, the approach creates relevant documents, generates subtopics to increase diversity, produces tricky non-relevant documents that share similarities with relevant ones, and adds random-topic documents to enlarge the collection. The resulting GenTREC contains 96,196 documents, 300 topics, and 18,964 relevance judgments at a cost of only $126.

Evaluation results show that GenTREC produces system rankings highly similar to traditional TREC collections for P@100 (reaching 1.0 and 0.95 Kendall's τ with TREC6 and Robust2004 respectively), MAP, and RPrec metrics. However, rankings differ significantly for P@10. The generated documents are found to be relevant in 83% of cases, with tricky non-relevant documents achieving 94% accuracy. GenTREC contains shorter documents with lower lexical diversity but requires higher education levels for comprehension compared to human-authored documents. The approach offers a promising low-cost alternative for IR evaluation, significantly reducing the burden of building and maintaining future evaluation resources.

## Method Summary
The GenTREC method generates an entire test collection using GPT-3.5 without manual relevance judgments. For each of 300 TREC topics, the system first generates 100 subtopics to increase diversity, then creates one "long text" document per subtopic. Tricky non-relevant documents are produced by masking named entities in topic descriptions and generating content from these perturbed prompts. Random-topic documents are added to reach the target collection size. Relevance is implicitly defined by generation: documents are relevant only to their generating prompt. The collection is evaluated by comparing system rankings against traditional TREC collections using Kendall's τ correlation, with results showing high agreement for P@100, MAP, and RPrec but significant divergence for P@10.

## Key Results
- GenTREC achieves Kendall's τ = 1.0 with TREC6 and 0.95 with Robust2004 for P@100, demonstrating high system ranking similarity
- Generated documents show 83% relevance accuracy with 94% accuracy for tricky non-relevant documents
- Collection contains 96,196 documents, 300 topics, and 18,964 relevance judgments at $126 cost
- P@10 shows only 0.2-0.25 Kendall's τ correlation, indicating fundamental limitations for shallow ranking evaluation

## Why This Works (Mechanism)

### Mechanism 1: Prompt-Bound Relevance by Construction
- Claim: Documents generated from a topic prompt are treated as inherently relevant to that topic, eliminating manual labeling.
- Mechanism: The system inverts the traditional pipeline—rather than judging existing documents, it generates documents whose relevance is defined by the generating prompt. A document is considered relevant only to its originating topic, while all other topic-document pairs are treated as non-relevant.
- Core assumption: LLM outputs faithfully address the semantic intent of the prompt with sufficient accuracy for evaluation purposes.
- Evidence anchors:
  - [abstract] "Our approach is based on the assumption that documents generated by an LLM are inherently relevant to the prompts used for their generation."
  - [Section 5.5] Manual validation found 83% average relevance accuracy for generated "relevant" documents, with high variance across topics (0% to 100%).
  - [corpus] Related work on LLM-judges (paper ID 16407) confirms LLM relevance judgments show alignment with humans but with documented biases.
- Break condition: Topics requiring specific historical incidents (e.g., "Africanized bee attacks") fail because LLMs generate related but non-specific content—average relevance drops to 37% for such topics.

### Mechanism 2: Subtopic-Based Diversity Injection
- Claim: Generating multiple subtopics per topic increases lexical and topical diversity of relevant documents.
- Mechanism: Rather than regenerating from identical prompts (which yields high overlap), the system first generates ~100 subtopics per topic, then produces one document per subtopic. This creates distinct prompts leading to less redundant documents.
- Core assumption: Subtopic decomposition captures meaningful aspects of an information need.
- Evidence anchors:
  - [Section 3.1] Using distinct prompts per subtopic reduces median textual overlap from 0.125 to 0.055 compared to memorization-based regeneration.
  - [Section 3.2] Specifying "long text" without document type further reduces maximum overlap to 0.113 vs. 0.202 for news article format.
  - [corpus] Weak/no direct corpus evidence on subtopic decomposition effectiveness.
- Break condition: Optimal subtopic count varies by topic—relevance degrades after ~35 documents for some topics, suggesting diminishing returns.

### Mechanism 3: Adversarial Hard Negatives via Masked Topic Perturbation
- Claim: Generating "tricky" non-relevant documents through entity masking creates harder retrieval challenges.
- Mechanism: Named entity recognition identifies keywords in topic descriptions, replaces them with `[MASK]` tokens, prompts LLM to fill variants, then generates documents from perturbed topics. These documents share vocabulary but fail the original information need.
- Core assumption: Masked topic variants produce superficially similar but semantically distinct content.
- Evidence anchors:
  - [Section 3.3] Describes 10 variants × 5 documents = 50 tricky non-relevant documents per topic.
  - [Section 5.5] 94% accuracy for tricky non-relevant documents being truly non-relevant.
  - [Section 6.2.2] Counter-intuitively, removing tricky non-relevant documents maintained or slightly improved rank correlations—suggesting imperfect differentiation.
  - [corpus] No direct corpus evidence on adversarial hard negative generation for IR evaluation.
- Break condition: Topic 437 showed only 64% non-relevance accuracy, indicating some masked variants accidentally satisfied the original need.

## Foundational Learning

- Concept: **Cranfield-style test collections**
  - Why needed here: GenTREC fundamentally reinterprets this paradigm—understanding the traditional components (documents, topics, qrels) is required to appreciate what's being replaced.
  - Quick check question: What three components define a Cranfield collection, and which does GenTREC generate vs. inherit?

- Concept: **Kendall's τ rank correlation**
  - Why needed here: The paper's central claim rests on whether GenTREC produces system rankings similar to TREC—measured by τ. Values ≥0.9 indicate high agreement; the paper reports 0.95 for P@100 but ~0.2 for P@10.
  - Quick check question: Why might high τ on P@100 but low τ on P@10 indicate the collection is "too easy"?

- Concept: **Pooling method and unjudged document assumptions**
  - Why needed here: GenTREC extends the "unjudged = non-relevant" assumption from pooling. Random-topic documents are assumed non-relevant to all topics by analogy.
  - Quick check question: In standard pooling, why are unjudged documents treated as non-relevant, and how does GenTREC replicate this logic?

## Architecture Onboarding

- Component map:
TREC Topics -> Subtopic Generator (GPT-3.5) -> 100 subtopics/topic -> Document Generator (GPT-3.5) -> ~63 relevant docs/topic -> Hard Negative Generator -> NER masking -> 50 tricky docs/topic -> Random Topic Generator -> ~60K filler documents -> GenTREC Collection (96,196 docs, 300 topics, 18,964 "judgments")

- Critical path: Subtopic quality -> document relevance accuracy -> system ranking validity. The 83% relevance accuracy (Section 5.5) is the ceiling for evaluation reliability.

- Design tradeoffs:
  - **Cost vs. scale**: $126 for 96K documents is ~1000× cheaper than manual qrels, but collection is 5.5× smaller than TREC Disks 4-5.
  - **Diversity vs. coherence**: Single LLM (GPT-3.5) produces lower lexical diversity (TTR: 0.39 vs. 0.53) but more consistent style.
  - **P@10 vs. deeper metrics**: High relevant-document ratio (0.197 vs. 0.01 in TREC) saturates P@10, making it less discriminative.

- Failure signatures:
  - **Historical incident topics**: Relevance drops to 37% (Section 5.5)—the system cannot generate specific past events.
  - **P@10 divergence**: τ = 0.2 indicates fundamental mismatch at shallow cutoffs (Section 6.2.1).
  - **Over-generated subtopics**: Diminishing returns after ~30-50 subtopics for some topics.

- First 3 experiments:
  1. **Validate relevance accuracy on your domain**: Manually sample 50-100 generated documents from your topics before trusting the collection. The paper's 83% may not generalize.
  2. **Test P@10 specifically**: If your system evaluation prioritizes top-10 results, this collection may not be suitable—run comparison against a small human-judged pilot.
  3. **Ablate tricky non-relevant documents**: Section 6.2.2 shows removal doesn't harm and may help correlations—test whether your retrieval system actually needs them for meaningful differentiation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the document generation process be modified to produce reliable system rankings for precision at 10 (P@10)?
- Basis in paper: [explicit] The authors state that GenTREC leads to "substantially different evaluations for P@10" compared to TREC collections, which "requires further investigation" (Section 6.2.1).
- Why unresolved: The generated relevant documents appear "easy" to retrieve, causing systems to score uniformly high (saturating the top 10), which limits the metric's ability to distinguish between system performance levels.
- What evidence would resolve it: Experiments using prompts designed to generate "harder" relevant documents, followed by a correlation analysis showing improved Kendall's τ scores for P@10 against traditional TREC benchmarks.

### Open Question 2
- Question: How can the generation method be improved to handle topics requiring specific references to past incidents or historical facts?
- Basis in paper: [explicit] Section 5.5 notes that for topics like "Africanized bee attacks," generated documents addressed general concepts but missed specific historical incidents, resulting in a 0% relevance rate.
- Why unresolved: The current approach systematically underperforms on topics where the information need is tied to specific, real-world historical records rather than general topical descriptions.
- What evidence would resolve it: Testing retrieval-augmented generation (RAG) or specific prompt engineering for incident-based topics, followed by human evaluation of the factual accuracy of the generated text.

### Open Question 3
- Question: Can LLMs be utilized to generate documents with graded relevance levels rather than just binary relevance?
- Basis in paper: [explicit] The authors identify the reliance on binary relevance judgments as a limitation and state that generating documents at "varying relevance levels" is a direction for future investigation (Section 7.3).
- Why unresolved: The current methodology assumes a document is either strictly relevant to its generating prompt or non-relevant, lacking the nuance of partial relevance found in real-world collections.
- What evidence would resolve it: Developing prompts that instruct the LLM to generate "partially relevant" or "highly relevant" documents and validating these grades against human annotations.

## Limitations

- 83% relevance accuracy for generated documents introduces measurable noise into evaluation
- Lower lexical diversity (TTR 0.39 vs 0.53) and shorter documents compared to human-authored collections
- P@10 shows only 0.2-0.25 Kendall's τ correlation, making shallow ranking evaluation unreliable
- Historical incident topics demonstrate particularly poor performance with relevance accuracy dropping to 37%

## Confidence

- **High Confidence**: Kendall's τ correlation for P@100 (1.0) and MAP/RPrec metrics (0.95) accurately reflect system ranking similarity between GenTREC and traditional collections
- **Medium Confidence**: The 83% relevance accuracy represents a generalizable upper bound for this approach, though performance varies significantly by topic domain
- **Low Confidence**: Whether GenTREC's cost-efficiency ($126 vs 100K+) justifies its evaluation limitations across diverse IR tasks

## Next Checks

1. Manually validate relevance accuracy on 50-100 documents from your specific domain before adopting GenTREC
2. Test system rankings specifically on P@10 to confirm whether shallow cutoff evaluation is reliable for your use case
3. Conduct ablation study removing tricky non-relevant documents to determine if they meaningfully differentiate retrieval systems in your evaluation scenario