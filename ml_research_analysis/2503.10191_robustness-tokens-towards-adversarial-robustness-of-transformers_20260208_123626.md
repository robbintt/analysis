---
ver: rpa2
title: 'Robustness Tokens: Towards Adversarial Robustness of Transformers'
arxiv_id: '2503.10191'
source_url: https://arxiv.org/abs/2503.10191
tags:
- tokens
- robustness
- adversarial
- attacks
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the vulnerability of large vision foundation
  models to adversarial attacks, especially when such models are fine-tuned for downstream
  tasks. The authors propose Robustness Tokens (ROB tokens), a novel method that adds
  a small number of secret, learnable tokens to the input sequence of Vision Transformer
  models.
---

# Robustness Tokens: Towards Adversarial Robustness of Transformers

## Quick Facts
- arXiv ID: 2503.10191
- Source URL: https://arxiv.org/abs/2503.10191
- Reference count: 40
- Key outcome: ROB tokens improve adversarial robustness for ViTs without modifying model parameters, achieving up to 30.5% classification accuracy under attack (vs 0.3% baseline).

## Executive Summary
This paper introduces Robustness Tokens (ROB tokens), a method to enhance adversarial robustness of Vision Transformers without modifying model parameters. ROB tokens are learnable vectors prepended to the input sequence that preserve clean feature representations while correcting adversarial perturbations. The approach shows significant improvements in robustness to white-box attacks across multiple models (DiNOv2, DEIT-III, OpenCLIP) and tasks (classification, segmentation) while maintaining or slightly improving clean accuracy. The tokens are kept secret, adding uncertainty for attackers.

## Method Summary
The method appends a small number of learnable ROB tokens (typically 10) to the input sequence of a frozen Vision Transformer. These tokens are optimized via dual objectives: Linv preserves clean feature representations while Ladv corrects adversarial perturbations to align with clean features. Training uses adversarial samples generated by PGD attacks on the frozen model, with optimization performed only on the token parameters. The approach is efficient, requiring minimal tokens and few training steps (~200 on 1600 images).

## Key Results
- Classification accuracy under attack improves from 0.3% to 30.5% for DiNOv2-S
- Feature cosine similarity improves from 0.01 to 0.93 with ROB tokens
- Segmentation mIoU increases from 0.9% to 22.8% on ADE20K
- Robustness generalizes across models, attack types, and downstream tasks
- Massive activations exploited by adversarial attacks are mitigated through ROB tokens

## Why This Works (Mechanism)

### Mechanism 1: Feature Space Correction via Sequence Extension
- Claim: Appending learned tokens redirects adversarial perturbations while preserving clean features through attention-based global influence.
- Evidence: [abstract] "preserve feature representations for clean inputs while correcting for adversarial perturbations"; [Section 3] dual-objective training with cosine similarity maximization; [corpus] representation vulnerability studies validate intervention point.

### Mechanism 2: Massive Activation Normalization
- Claim: ROB tokens regulate abnormally high activations exploited by adversarial attacks, preventing degradation of outputs.
- Evidence: [Section 4.8] "bridge the gap between massive activations obtained with clean samples and those obtained with adversarial samples"; [Figure 6] shows activation normalization across layers after ROB token introduction.

### Mechanism 3: Secret Token Injection as Defense Uncertainty
- Claim: Keeping ROB tokens private creates information asymmetry that degrades attack effectiveness even under white-box assumptions.
- Evidence: [abstract] "kept secret, adding a layer of uncertainty for malicious attackers"; [Section 1] emphasizes secrecy as a defense layer; [corpus] limited support for secrecy as defense mechanism.

## Foundational Learning

- **Self-Attention and Token Interactions in Transformers**
  - Why needed: Understanding how additional tokens influence all other tokens through attention is essential to grasp why appending a few tokens can globally modify feature representations.
  - Quick check: In a Vision Transformer, how does prepending a single learnable token affect the final representation of a patch token at position 196?

- **Adversarial Attacks and Transferability**
  - Why needed: The threat model relies on attacks crafted on public models transferring poorly to models augmented with secret tokens.
  - Quick check: Why do adversarial perturbations crafted on ResNet-50 often transfer to ViT-B/16, and how might changing the input token sequence disrupt this?

- **Cosine Similarity in Feature Space**
  - Why needed: Both training objectives and robustness metrics are built entirely on cosine similarity between feature vectors.
  - Quick check: If feature vectors for clean and adversarial images have cosine similarity of 0.01 without ROB tokens and 0.93 with them, what does this imply about the angular relationship and practical usability?

## Architecture Onboarding

- **Component map**: Frozen backbone (pre-trained ViT) -> ROB tokens (learnable vectors) -> Token concatenation (ROB + original sequence) -> Feature extractor (extracts class/patch tokens only) -> Dual-loss computer (Linv + Ladv) -> Adversarial generator (PGD attacks)

- **Critical path**: Initialize 10 ROB tokens randomly -> Sample batch and generate adversarial samples via 30-step PGD -> Forward pass with concatenated tokens -> Compute Linv + Ladv and backpropagate to ROB tokens only -> Repeat for ~200 steps

- **Design tradeoffs**: Token count (1-10 optimal, diminishing returns beyond); Attack strength during training (stronger attacks increase robustness but raise compute cost); Token position (prepended vs. appended); Training data scale (1600 images sufficient)

- **Failure signatures**: Linv decreasing during training (tokens degrading clean quality); Ladv plateauing below 0.8 (insufficient correction capacity); Massive activations remaining elevated in adversarial samples (incomplete mechanism engagement); Sharp performance drop on unseen attack types (overfitting)

- **First 3 experiments**: 1) Token count ablation (train with nâˆˆ{1,5,10,20,50} on DiNOv2-S); 2) Cross-attack generalization (train with PGD, evaluate against AutoAttack, FGSM, C&W); 3) Activation monitoring (log per-layer max absolute activations for clean, adversarial without/with ROB)

## Open Questions the Paper Calls Out
- To what extent can Robustness Tokens be inferred or reverse-engineered by an attacker? The authors acknowledge further research is needed to understand the extent to which tokens can be inferred by attackers.
- Are Robustness Tokens effective for transformer architectures in non-vision domains, such as Natural Language Processing? The paper notes applicability in other domains needs further study.
- Can an attacker compromise the defense by training their own set of Robustness Tokens to craft stronger attacks? The Discussion posits that an attacker might train such tokens to obtain more effective attacks.

## Limitations
- The security-through-obscurity approach relies on keeping ROB tokens secret, which may not withstand sophisticated query-based extraction attacks
- The massive activation normalization mechanism doesn't fully explain effectiveness on smaller models, suggesting incomplete understanding of the primary mechanism
- The paper focuses exclusively on Vision Transformers without extensive validation on other transformer architectures or domains

## Confidence
- High confidence: Core experimental results showing ROB tokens improve adversarial robustness while maintaining clean accuracy
- Medium confidence: Mechanism explanations, particularly massive activation normalization hypothesis
- Low confidence: Generalizability to all ViT variants, downstream tasks beyond classification/segmentation, and long-term security against sophisticated attackers

## Next Checks
1. Implement a black-box query-based approach to attempt reconstruction of ROB tokens from model outputs to validate the security-through-obscurity assumption
2. For small (DeiT-S) and large (DiNOv2-XL) ViTs, disable massive activation normalization and measure ROB token effectiveness to isolate the primary mechanism
3. Apply ROB tokens to a ViT fine-tuned for object detection or segmentation on Cityscapes to test practical generalizability beyond classification