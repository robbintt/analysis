---
ver: rpa2
title: Has My System Prompt Been Used? Large Language Model Prompt Membership Inference
arxiv_id: '2502.09974'
source_url: https://arxiv.org/abs/2502.09974
tags:
- prompt
- prompts
- system
- task
- detective
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces Prompt Detective, a training-free statistical\
  \ method for prompt membership inference that determines whether a proprietary system\
  \ prompt was reused in a third-party LLM-based service. The method compares response\
  \ distributions from two models\u2014one using the proprietary prompt and another\
  \ with an unknown prompt\u2014using BERT embeddings and a permutation test on mean\
  \ vector cosine similarity."
---

# Has My System Prompt Been Used? Large Language Model Prompt Membership Inference

## Quick Facts
- **arXiv ID:** 2502.09974
- **Source URL:** https://arxiv.org/abs/2502.09974
- **Authors:** Roman Levin; Valeriia Cherepanova; Abhimanyu Hans; Avi Schwarzschild; Tom Goldstein
- **Reference count:** 23
- **Primary result:** Training-free statistical method (Prompt Detective) detects prompt reuse with near-zero false positive rates and false negative rates close to significance level (0.05)

## Executive Summary
This paper introduces Prompt Detective, a training-free statistical method for prompt membership inference that determines whether a proprietary system prompt was reused in a third-party LLM-based service. The method compares response distributions from two models—one using the proprietary prompt and another with an unknown prompt—using BERT embeddings and a permutation test on mean vector cosine similarity. Across multiple models and two prompt datasets, Prompt Detective achieves near-zero false positive rates and false negative rates close to the significance level (0.05), demonstrating strong reliability. In challenging scenarios with highly similar prompts, performance improves with more generations per task prompt, with 50 generations enabling near-perfect separation.

## Method Summary
Prompt Detective generates task prompts for two models—one with a known system prompt and one with an unknown prompt—collecting multiple responses per task. Responses are embedded using BERT, mean vectors are computed, and cosine similarity is calculated. A permutation test with label shuffling establishes statistical significance, determining if the prompts are the same. The method requires no training, works across different model architectures, and scales with the number of generations and task prompts to improve detection accuracy for similar prompts.

## Key Results
- Across multiple models (Llama2, Llama3, Mistral, Mixtral, Claude Haiku, GPT-3.5) and two prompt datasets, achieves near-zero false positive rates and false negative rates close to 0.05
- In challenging scenarios with highly similar prompts, performance improves with more generations per task prompt, with 50 generations enabling near-perfect separation
- Method works effectively in black-box settings, confirming practical applicability for verifying prompt reuse with statistical significance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Distinct system prompts cause LLMs to occupy different "role trajectories," resulting in statistically distinct response distributions even when semantic content appears similar.
- **Mechanism:** The system prompt shifts the conditional probability distribution of the model's output tokens. By projecting responses into a high-dimensional embedding space (BERT), these distributional differences manifest as separate clusters, making the mean vectors of the clusters distinguishable via cosine similarity.
- **Core assumption:** The embedding model (BERT) captures stylistic or structural nuances induced by the system prompt, not just semantic content.
- **Evidence anchors:**
  - [abstract] "Our work reveals that even minor changes in system prompts manifest in distinct response distributions..."
  - [section 7] "...suggesting that large language models take distinct low-dimensional 'role trajectories'..."
  - [corpus] Corpus papers on Membership Inference (e.g., arXiv:2508.18665) discuss inferring attributes from model outputs, supporting the premise that model behavior leaks internal state, though they focus on training data rather than input context.
- **Break condition:** If the system prompt is generic (e.g., "You are a helpful assistant") or task prompts fail to elicit role-specific behavior, the distributions may overlap significantly.

### Mechanism 2
- **Claim:** A permutation test applied to mean embedding vectors provides a non-parametric significance test for distributional identity.
- **Mechanism:** The test constructs a null distribution by shuffling labels (prompt A vs. B) within task groups and recomputing cosine similarity. If the observed similarity is an outlier (low p-value) relative to the null distribution, the groups likely originate from different underlying prompts.
- **Core assumption:** The sample size (generations per prompt) is sufficient to approximate the true response distribution mean.
- **Evidence anchors:**
  - [abstract] "...comparing response distributions... using BERT embeddings and a permutation test..."
  - [section 3.2] "The permutation test is a non-parametric approach... assessing whether the observed difference... is significantly larger than what would be expected by chance."
  - [corpus] The corpus mentions "Statistical MIA" (arXiv:2602.01150), reinforcing that statistical tests are reliable tools for auditing model internals, though this paper applies them to prompt context.
- **Break condition:** Insufficient sample size (e.g., k=2 generations for highly similar prompts) leads to high variance, causing false negatives (failure to distinguish).

### Mechanism 3
- **Claim:** Task prompts act as probes that amplify the signal of the system prompt.
- **Mechanism:** Task prompts are chosen to force the model to reference its instructions (e.g., asking for persona opinions). This maximizes the divergence between models with different system prompts, whereas generic queries might yield universal answers.
- **Core assumption:** The user can select task prompts relevant to the proprietary prompt's domain.
- **Evidence anchors:**
  - [section 3.3] "We consider a task prompt a good probe... if it elicits responses that are directly influenced by and related to the system prompt."
  - [section 5.2] "Having particularly long generations... is not as useful, indicating that the optimal setup includes generating shorter responses to more task prompts..."
  - [corpus] No direct corpus evidence for "task probing"; related works generally assume static query sets.
- **Break condition:** If task prompts are irrelevant to the specific system prompt instructions (e.g., generic math questions for a "Dream Interpreter" bot), the signal-to-noise ratio drops.

## Foundational Learning

- **Concept: Permutation Testing (Non-parametric Statistics)**
  - **Why needed here:** This is the core statistical engine of Prompt Detective. Unlike t-tests, it does not assume the embedding data is Gaussian, which is crucial for high-dimensional text embeddings.
  - **Quick check question:** If you shuffle the labels of two datasets and compute a test statistic 1,000 times, what does the distribution of those statistics represent?

- **Concept: Cosine Similarity in Vector Space**
  - **Why needed here:** The method relies on measuring the "closeness" of the mean response vectors. Cosine similarity measures orientation rather than magnitude, which helps normalize for response length variations.
  - **Quick check question:** If two vectors point in the same direction but have different magnitudes (lengths), what is their cosine similarity?

- **Concept: LLM Generation Sampling (Temperature/Top-P)**
  - **Why needed here:** The method requires a *distribution* of responses. Understanding that LLMs are probabilistic samplers is necessary to grasp why we need k > 1 generations per prompt to estimate the mean response vector.
  - **Quick check question:** Why would setting temperature to 0 (deterministic) break a statistical test that relies on variance?

## Architecture Onboarding

- **Component map:** Target Service -> Reference Model -> Encoder -> Statistical Engine
- **Critical path:** Generate Queries -> Collect Generations (Target & Ref) -> Embed Text -> Calculate Mean Vectors -> Observed Similarity -> **Permutation Loop** (Shuffle & Recalculate) -> P-value Calculation
- **Design tradeoffs:**
  - **Accuracy vs. Cost:** Increasing generations ($k$) and task prompts ($n$) improves separation of similar prompts (hard examples) but linearly increases API costs and latency.
  - **Length vs. Count:** The paper suggests generating *shorter* responses to *more* task prompts is more token-efficient for detection than generating long responses to few prompts.
- **Failure signatures:**
  - **High P-value (>0.05) for distinct prompts:** Occurs when generation counts are too low for "Hard Examples" (highly similar prompts).
  - **High False Positive Rate:** Reported specifically for Claude Haiku in the paper (FPR 0.05-0.10), suggesting model-specific nuances in how strictly it adheres to system prompts.
- **First 3 experiments:**
  1. **Sanity Check (Identity):** Run Prompt Detective comparing a model against itself with the *same* prompt. Verify p-values center around 0.5 (uniform distribution).
  2. **Hard Example Scaling:** Fix a pair of highly similar prompts (Similarity Level 1). Run the test with $k=2, 10, 50$ generations to observe the drop in p-value (increase in confidence).
  3. **Black-Box Mismatch:** Configure the Reference Model as Llama2 while the Target is GPT-3.5. Verify if the method still detects prompt distinctness despite the architectural difference (Table 3 results).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can an adversary adversarially optimize a system prompt to maintain semantic utility while specifically minimizing the statistical distance of the output distribution to evade Prompt Detective?
- **Basis in paper:** [inferred] The paper demonstrates that minimally rephrased prompts (Similarity Level 1) yield high false positive rates (up to 0.65) with low generation counts, suggesting that semantic similarity correlates with statistical indistinguishability.
- **Why unresolved:** The "Hard Examples" tested were generated by an LLM for variety, not specifically optimized to defeat the permutation test statistic used by Prompt Detective.
- **What evidence would resolve it:** An experiment where a "null" prompt is iteratively refined against the loss function of the test statistic to see if evasion is computationally feasible.

### Open Question 2
- **Question:** How does domain-specific fine-tuning of the third-party model impact the distinguishability of system prompts?
- **Basis in paper:** [inferred] Section 6 (Black Box Setup) explicitly assumes the third-party model is one of the six standard off-the-shelf models (e.g., Llama 2 13B, GPT-3.5).
- **Why unresolved:** It is unclear if the distinct "role trajectories" observed in standard models persist when the underlying model weights have been significantly altered by domain-specific fine-tuning, which might overshadow the system prompt's influence.
- **What evidence would resolve it:** Evaluation of the method's false positive/negative rates when the target model $f_p$ is a fine-tuned variant of the reference model family.

### Open Question 3
- **Question:** Can the selection of task queries be automated to minimize the required number of generations?
- **Basis in paper:** [explicit] The authors state in Section 3.3 that "The selection of task prompts... is an important component" and currently rely on generating them with Claude 3 Sonnet.
- **Why unresolved:** While the paper shows scaling with the number of queries, it does not explore an adaptive strategy where task queries are selected based on real-time feedback to maximize distributional divergence.
- **What evidence would resolve it:** A comparative study evaluating an active learning approach for query selection versus the current method of using generic generated queries.

## Limitations
- **Temperature Sensitivity:** The paper does not specify generation hyperparameters (temperature, top_p, max tokens), yet these directly impact response variance and the method's statistical power.
- **Model-Specific Behavior:** Claude Haiku shows notably higher false positive rates (0.05-0.10) compared to other models, suggesting the method's effectiveness varies significantly across model architectures.
- **Task Prompt Dependency:** The method's success hinges on selecting appropriate task prompts that elicit responses influenced by the system prompt, which may fail for generic system prompts.

## Confidence
- **High Confidence:** The core mechanism (permutation test on BERT embeddings detecting distributional differences) is theoretically sound and empirically validated across multiple model families.
- **Medium Confidence:** The claim that k=50 generations enables "near-perfect separation" for hard examples is supported by experiments but requires verification across diverse prompt pairs.
- **Medium Confidence:** The black-box setting performance shows promise but may not generalize to all model pairs, particularly those with vastly different architectural characteristics.

## Next Checks
1. **Hyperparameter Sensitivity Test:** Systematically vary temperature (0.0, 0.7, 1.5) and top_p (0.9, 1.0) while measuring FPR/FNR on the Anthropic Library dataset to quantify how generation randomness affects detection accuracy.
2. **Cross-Architecture Transferability:** Test Prompt Detective when the reference model and target model have different base architectures (e.g., reference: Llama2, target: Claude Haiku) to measure whether the method maintains statistical significance across architectural boundaries.
3. **Minimal Prompt Detection:** Evaluate the method's performance on system prompts that contain only generic instructions (e.g., "You are a helpful assistant") versus highly specific persona instructions to determine the method's lower bound for prompt complexity.