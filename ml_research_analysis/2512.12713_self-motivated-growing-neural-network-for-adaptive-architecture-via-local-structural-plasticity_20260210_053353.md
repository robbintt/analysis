---
ver: rpa2
title: Self-Motivated Growing Neural Network for Adaptive Architecture via Local Structural
  Plasticity
arxiv_id: '2512.12713'
source_url: https://arxiv.org/abs/2512.12713
tags:
- growth
- learning
- structural
- smgrnn
- local
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Self-Motivated Growing Neural Network (SMGrNN) addresses the
  problem of fixed-capacity neural architectures in deep reinforcement learning by
  introducing a local Structural Plasticity Module (SPM) that dynamically adjusts
  network topology during training. The SPM monitors local neuron activity and edge-wise
  weight update statistics over short temporal windows, using these signals to trigger
  neuron insertion and pruning without relying on global error signals or task-specific
  growth schedules.
---

# Self-Motivated Growing Neural Network for Adaptive Architecture via Local Structural Plasticity

## Quick Facts
- arXiv ID: 2512.12713
- Source URL: https://arxiv.org/abs/2512.12713
- Reference count: 38
- Primary result: SMGrNN dynamically adjusts network topology during training using local structural plasticity, achieving task-appropriate capacity without global error signals or task-specific growth schedules.

## Executive Summary
The Self-Motivated Growing Neural Network (SMGrNN) introduces a local Structural Plasticity Module (SPM) that autonomously adjusts neural network architecture during training. The SPM monitors local neuron activity and edge-wise weight update statistics over short temporal windows, triggering neuron insertion and pruning based on local instability signals rather than global error gradients. This approach addresses the fundamental limitation of fixed-capacity neural architectures in deep reinforcement learning by enabling systematic, data-driven adaptation of network complexity to task demands.

Evaluated on control benchmarks through policy distillation, SMGrNN demonstrates comparable or superior performance to fixed-capacity multilayer perceptron baselines while using task-appropriate network sizes. The framework achieves this through three complementary mechanisms: edge-driven growth via gradient instability detection, pruning via local magnitude and stagnation signals, and random exploratory growth for structural diversity. The SPM's local and modular design enables future integration with Hebbian plasticity rules, supporting both artificial and spiking neural implementations driven by local learning rules.

## Method Summary
SMGrNN implements a dynamic graph neural network where architecture evolves during training through the Structural Plasticity Module (SPM). The method begins with a minimal graph containing only input and output nodes, then uses three mechanisms to adjust topology: edge-driven growth triggered by gradient instability on existing connections, random edge additions to explore structural diversity, and pruning of low-magnitude, stagnant edges along with orphan nodes. Training proceeds via policy distillation from a pre-trained expert MLP, with standard backpropagation updating weights while SPM independently modifies structure. The SPM maintains rolling statistics (mean and variance) over weight updates for each edge, using these local signals to decide when to grow or prune. This approach enables systematic capacity adaptation without task-specific schedules or global error signals.

## Key Results
- SMGrNN achieves similar or higher returns compared to fixed-capacity MLP baselines across CartPole-v1, Acrobot-v1, and LunarLander-v3 benchmarks
- Network sizes adapt systematically to task difficulty, with SMGrNN using orders of magnitude fewer parameters than growth-only variants
- Ablation studies show structural plasticity improves reward stability and reduces variance, particularly on harder tasks like LunarLander-v3
- The framework demonstrates low sensitivity to initial topology and maintains performance with minimal architectural assumptions

## Why This Works (Mechanism)

### Mechanism 1: Edge-Driven Growth via Gradient Instability Detection
Edges whose weight updates fluctuate around zero with high variance indicate representational insufficiency, and adding capacity at these locations improves learning stability. For each edge k, the SPM computes rolling mean μ_Δk and variance σ_Δk² over T=100 steps. When |μ_Δk| < 0.5·σ_Δk AND σ_Δk² > λ_edge·|μ_Δk|, a new relay node is inserted along the edge, creating a parallel two-step pathway while retaining the original connection. New weights are initialized small (|w_init| ≪ 1) to avoid disruption. Core assumption: Gradient instability reflects genuine representational uncertainty rather than optimization noise or inadequate learning rate.

### Mechanism 2: Pruning via Local Magnitude and Stagnation Signals
Removing low-magnitude edges with stalled updates and orphan nodes prevents unbounded expansion while maintaining performance. Edges enter removal candidate set W_t if |w_k| ≤ τ_w AND |μ_Δk| ≤ τ_Δ. A fraction η_prune=0.8 of candidates are randomly deleted each pruning step (every s=30 steps). Orphan nodes (zero in-degree OR zero out-degree) are removed along with incident edges. Core assumption: Small weights with near-zero updates contribute minimally to function; orphan nodes are computationally useless.

### Mechanism 3: Random Exploratory Growth for Structural Diversity
Occasional random edge additions prevent the architecture from being trapped in suboptimal local structures. Bernoulli trigger G_t ~ Bernoulli(p_rand=0.25) proposes m_t = max(⌊ρ_rand·N_t⌋, 1) new edges, sampled from candidate pool of non-existent node pairs. Edges are initialized with small weights. Core assumption: Random connectivity exploration compensates for the reactive-only nature of edge-driven growth.

## Foundational Learning

- **Concept: Rolling statistics (mean/variance over sliding windows)**
  - Why needed here: SPM decisions require smoothed estimates of gradient dynamics, not instantaneous values, to distinguish signal from noise.
  - Quick check question: Given a sequence [0.1, -0.15, 0.08, -0.12, 0.05], compute the sample mean and variance. How would a window of T=3 change these estimates as you slide forward?

- **Concept: Graph topology and directed acyclic/yclic graphs**
  - Why needed here: SMGrNN allows cycles and skip connections to emerge; understanding degree (in/out), paths, and connectivity is essential for implementing orphan removal and analyzing structure.
  - Quick check question: In a directed graph with nodes {A, B, C} and edges {A→B, B→C, A→C}, which nodes would be removed by the orphan rule (zero in-degree OR zero out-degree among hidden nodes)?

- **Concept: Policy distillation (student-teacher loss)**
  - Why needed here: SMGrNN is trained via distillation from a pre-trained expert MLP; the supervised loss L_t(θ) drives weight updates while SPM handles structure independently.
  - Quick check question: If expert outputs action a_E and student outputs â, write the MSE distillation loss. How does this differ from a reinforcement learning policy gradient objective?

## Architecture Onboarding

- **Component map:**
  - Graph G_t = (V_t, E_t): nodes (input, hidden, output) and directed weighted edges
  - History buffers: rolling T=100-step buffers per node (activations) and per edge (Δw updates)
  - SPM: three sub-modules—edge-driven growth (instability detector), random growth (Bernoulli sampler), pruning (threshold filters + orphan remover)
  - Weight optimizer: standard Adam/SGD on distillation loss, independent of SPM

- **Critical path:**
  1. Initialize graph with input/output nodes, zero hidden nodes, 0.8 edge density
  2. Forward pass → compute loss → backprop → update weights → record Δw to buffers
  3. Every step: SPM reads buffers, computes μ_Δk and σ_Δk² per edge
  4. If t mod s ≠ 0 (growth step): check instability criterion → insert relay nodes; then random growth with p_rand
  5. If t mod s = 0 (pruning step): identify W_t → delete η_prune fraction → remove orphan nodes
  6. Repeat until training episodes complete

- **Design tradeoffs:**
  - Window T: larger T smooths noise but delays structural response; T=100 worked across tasks but may be too slow for fast-changing environments
  - Pruning fraction η_prune=0.8: aggressive pruning keeps networks compact but risks over-pruning; ablation shows growth-only expands 50–100×
  - Instability threshold λ_edge: controls sensitivity of edge-driven growth; too sensitive → excessive growth; too strict → missed capacity needs

- **Failure signatures:**
  - Unbounded growth + poor performance → check pruning disabled or τ_w/τ_Δ too high
  - No growth despite underperformance → check λ_edge too strict or learning rate too low (low gradient variance)
  - High variance across seeds → random growth too aggressive or instability criterion noisy

- **First 3 experiments:**
  1. Reproduce CartPole ablation (growth enabled vs. static) with 5 seeds; verify hidden-node count converges to ~30 and late reward ~497 with low variance.
  2. Ablate random growth alone (set p_rand=0) to isolate edge-driven growth contribution; compare final network size and reward stability.
  3. Sensitivity test on LunarLander with T ∈ {50, 100, 200}; measure how quickly network size adapts and whether reward curves shift meaningfully.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can SMGrNN mitigate catastrophic forgetting in explicit continual learning settings with task sequences?
- Basis in paper: [explicit] The authors state: "the focus here is on within-task capacity adaptation, not on catastrophic forgetting across task sequences; explicit continual learning is left to future work."
- Why unresolved: The current evaluation only covers single-task policy distillation, not sequential task learning where retaining prior skills is critical.
- What evidence would resolve it: Evaluate SMGrNN on standard continual learning benchmarks (e.g., Permuted MNIST sequences, Atari task sequences) measuring both forward transfer and backward forgetting metrics.

### Open Question 2
- Question: Can the SPM be combined with