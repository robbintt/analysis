---
ver: rpa2
title: Efficient Zero-Shot Inpainting with Decoupled Diffusion Guidance
arxiv_id: '2512.18365'
source_url: https://arxiv.org/abs/2512.18365
tags:
- diffusion
- algorithm
- ding
- preprint
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new zero-shot inpainting method using diffusion
  models that eliminates the need for backpropagation through the denoiser network,
  addressing a key computational bottleneck in existing approaches. The method approximates
  posterior sampling transitions with a decoupled Gaussian mixture distribution that
  can be sampled exactly without vector-Jacobian products.
---

# Efficient Zero-Shot Inpainting with Decoupled Diffusion Guidance

## Quick Facts
- **arXiv ID:** 2512.18365
- **Source URL:** https://arxiv.org/abs/2512.18365
- **Reference count:** 40
- **Primary result:** A new zero-shot inpainting method that eliminates backpropagation through the denoiser, achieving superior performance compared to state-of-the-art methods and even outperforming a fine-tuned Stable Diffusion 3 model.

## Executive Summary
This paper introduces a novel zero-shot inpainting method using diffusion models that eliminates the computational bottleneck of backpropagation through the denoiser network. The key innovation is decoupling the likelihood guidance from the current state by evaluating the noise predictor at an independent proxy sample, which yields a Gaussian mixture distribution that can be sampled exactly without vector-Jacobian products. Experiments on multiple datasets demonstrate that this approach achieves state-of-the-art performance for zero-shot inpainting while being significantly faster and more memory-efficient than existing methods.

## Method Summary
The method reformulates inpainting as posterior sampling π0(x0|y) ∝ ℓ0(y|x0)p0(x0), where the prior p0 comes from a pretrained latent diffusion model and the likelihood ℓ0 is Gaussian on observed pixels. Instead of computing likelihood gradients through the denoiser (requiring backpropagation), DING evaluates the noise predictor at an independent proxy sample zs drawn from the pretrained transition. This decoupling yields a tractable Gaussian mixture posterior that can be sampled exactly without VJPs. The method operates in the latent space using a modified DDIM schedule ηt = σt(1-αt) and requires two denoiser evaluations per step (one for current state, one for proxy), though this can be reduced to one at the cost of performance.

## Key Results
- Achieves superior performance compared to state-of-the-art zero-shot methods across FFHQ, DIV2K, and PIE-Bench datasets
- Outperforms a fine-tuned Stable Diffusion 3 model for inpainting tasks
- Runs in 2.9s with 22GB memory versus 7-8s and 24GB for VJP-based baselines
- Shows particular effectiveness in low-NFE regimes (≤200 function evaluations)

## Why This Works (Mechanism)

### Mechanism 1: Decoupled Denoiser Evaluation
Standard zero-shot methods compute likelihood gradients by differentiating through the denoiser at the current state. DING breaks this dependency by evaluating the noise predictor at an independent proxy sample zs ~ pθs|t(·|xt), yielding a tractable mixture distribution that can be sampled exactly without vector-Jacobian products.

### Mechanism 2: Gaussian Conjugacy for Inpainting
For Gaussian inpainting likelihood on observed pixels, the decoupled structure yields closed-form Gaussian posterior transitions. The posterior factors into two independent Gaussians: masked pixels follow standard DDIM transition, while unmasked pixels follow a Gaussian combining DDIM mean and effective observation from proxy prediction.

### Mechanism 3: DDIM Schedule Optimization
The DDIM stochasticity schedule ηt = σt(1-αt) decays faster than alternatives while maintaining sufficient early-process stochasticity. This faster decay enables coherent completion with fewer steps, crucial for low-NFE performance.

## Foundational Learning

- **Bayesian Inverse Problems with Diffusion Priors**
  - Why needed: Reformulates inpainting as posterior sampling requiring understanding of how likelihood guidance modifies diffusion sampling
  - Quick check: Why does posterior sampling require modifying the reverse diffusion process rather than just running the prior diffusion?

- **DDIM Transitions and Stochasticity Control**
  - Why needed: DING builds on DDIM reverse transitions with controlled variance ηt, determining exploration-vs-fidelity balance
  - Quick check: What happens when ηt = 0 (deterministic DDIM) vs. ηt = σt (maximum stochasticity)?

- **Gaussian Conjugacy in Bayesian Inference**
  - Why needed: Tractable posterior sampling relies on Gaussian-Gaussian conjugacy for closed-form updates
  - Quick check: Given prior N(μ, σ²) and likelihood N(y|x, τ²), what is the posterior mean and variance?

## Architecture Onboarding

- **Component map:** Latent diffusion model → Proxy sampler (zs) → Decoupled denoiser evaluation → Gaussian posterior updater → Inpainted latents
- **Critical path:**
  1. Forward denoiser pass on current state xt → μ
  2. Sample proxy zs from DDIM transition
  3. Decoupled denoiser evaluation at zs → noise prediction
  4. Compute γ coefficient and apply separate updates to masked/unmasked regions
- **Design tradeoffs:**
  - Two NFEs/step vs. one: Required for decoupled evaluation; "Delayed DING" ablation shows reusing previous noise prediction degrades performance
  - Latent vs. pixel space: Operates in latent space for efficiency; requires mask downsampling via average pooling + thresholding
  - Stochasticity level: Default ηt = σt(1-αt) balances exploration/fidelity; alternatives tested in Table 7
- **Failure signatures:**
  - Prompt underspecification: Reconstructions lack coherence when text prompts are vague
  - High NFE saturation: Performance plateaus around 200 NFEs
  - Very small σy: Over-constrains to observations, preventing coherent completion
- **First 3 experiments:**
  1. Reproduce FFHQ inpainting (Table 2): Run DING with 50 NFEs on half-mask, verify FID < 10 and cPSNR > 31
  2. Ablate decoupling (Table 6): Compare DING vs. Delayed DING, confirm ~2 dB cPSNR gap from decoupling
  3. Profile efficiency (Table 1): Measure runtime and memory vs. VJP-based baselines, verify 2.9s/22GB vs. 7-8s/24GB

## Open Questions the Paper Calls Out

### Open Question 1
How can the decoupled guidance framework be extended to accommodate general forward operators beyond inpainting while preserving VJP-free efficiency in the latent domain? The authors note this remains challenging as lifting general observation operators to the compressed latent space without differentiating through the decoder is technically difficult.

### Open Question 2
Can alternative noise schedules or guidance schemes be designed to ensure the method scales gracefully with increased compute, avoiding the observed performance saturation at high NFEs? The current DDIM schedule is optimized for low-to-mid NFE regimes but leads to diminishing returns beyond 200 NFEs.

### Open Question 3
How can the framework be made robust to vague or under-specified textual prompts, particularly when masking large image regions? The current dependence on specific prompts creates a usability bottleneck for "zero-shot" editing where optimal prompts might not be known.

## Limitations
- Performance does not monotonically improve as the compute budget increases, saturating around 200 NFEs
- Current applicability is limited to inpainting because it is the only operator reliably lifted to the latent domain
- Quality of reconstructions is highly sensitive to the specificity of the textual prompt

## Confidence

- **High Confidence:** Mathematical derivation of decoupled Gaussian mixture distribution and closed-form sampling properties; experimental comparison against zero-shot baselines on standard metrics
- **Medium Confidence:** Superiority over fine-tuned models (depends on specific baseline); efficiency gains (based on reported values)
- **Low Confidence:** Generalization to non-Gaussian likelihoods and highly irregular masks; performance on domains outside faces and natural images

## Next Checks

1. Test DING with Poisson or Bernoulli likelihoods to verify if the decoupled approach still yields tractable posteriors or requires approximation

2. Conduct an ablation study varying the DDIM schedule across diverse inpainting tasks to quantify sensitivity to this hyperparameter

3. Apply DING to a structurally different domain (e.g., document image inpainting or medical image reconstruction) to assess whether advantages transfer beyond evaluated datasets