---
ver: rpa2
title: Do Large Language Models Truly Understand Cross-cultural Differences?
arxiv_id: '2512.07075'
source_url: https://arxiv.org/abs/2512.07075
tags:
- cultural
- cross-cultural
- injection
- across
- concept
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses limitations in evaluating cross-cultural understanding
  of LLMs, including lack of contextual scenarios, insufficient cross-cultural concept
  mapping, and limited deep cultural reasoning. It proposes SAGE, a scenario-based
  benchmark built through cross-cultural core concept alignment and generative task
  design.
---

# Do Large Language Models Truly Understand Cross-cultural Differences?

## Quick Facts
- arXiv ID: 2512.07075
- Source URL: https://arxiv.org/abs/2512.07075
- Reference count: 30
- Primary result: Current LLMs show critically deficient cross-cultural reasoning, with best zero-shot performance at 0.41 accuracy

## Executive Summary
This paper addresses critical limitations in evaluating cross-cultural understanding of LLMs, including lack of contextual scenarios, insufficient cross-cultural concept mapping, and limited deep cultural reasoning. It proposes SAGE, a scenario-based benchmark built through cross-cultural core concept alignment and generative task design. The benchmark includes 4530 items across 15 real-world scenarios and 9 cultural dimensions, grounded in cultural theory. Experiments show that current LLMs have critically deficient cross-cultural reasoning capabilities, with best zero-shot performance reaching only 0.41 accuracy. The benchmark reveals consistent performance hierarchies across cultural dimensions, with abstract domains like spirituality and metaphysics showing the largest gaps. Knowledge injection substantially improves performance, especially in abstract dimensions and Spanish-language tasks. The framework supports continuous expansion and cross-lingual transfer, with successful extension to Korean. The results highlight that while progress has been made, LLMs are still far from achieving nuanced cross-cultural understanding.

## Method Summary
The SAGE benchmark employs a three-stage pipeline: (1) defining 210 Cross-cultural Core Concepts (CCCs) across 9 dimensions and 3 layers using expert panel selection and cultural iceberg theory; (2) generating 15 real-world scenarios with 4 interaction types (Integration, Conflict, Communication, Judgment); and (3) producing 4,530 test items through LLM-assisted expansion with "Cultural Traps" like Universalist Fallacy and Analogy Misalignment. The evaluation uses three injection levels (Zero, Weak, Strong) to diagnose whether failures stem from missing knowledge or retrieval issues. Models are evaluated zero-shot with strict formatting requirements, and non-responses are scored incorrect. The benchmark spans Chinese and Spanish with extension to Korean, using accuracy and sensitivity metrics across cultural dimensions.

## Key Results
- Current LLMs show critically deficient cross-cultural reasoning, with best zero-shot performance at 0.41 accuracy
- Knowledge injection substantially improves performance, especially in abstract dimensions and Spanish-language tasks
- Abstract domains like spirituality and metaphysics show the largest performance gaps, revealing an epistemic distance hierarchy

## Why This Works (Mechanism)

### Mechanism 1: Scenario-Grounded Concept Activation
If cultural concepts are evaluated solely as factual statements, models may rely on surface correlations; embedding them in real-world scenarios forces the activation of reasoning over retrieval. The benchmark constructs items by mapping 210 "Cross-cultural Core Concepts" (CCCs) into 15 specific interaction scenarios (e.g., "Mourning Ritual Observation," "Business Negotiations"). This forces the model to interpret a concept like "Death" not as a definition, but through its behavioral implications (solemnity vs. celebration) within a specific social context. The mechanism assumes that "true" understanding requires the application of knowledge in situated contexts, consistent with the "Cultural Iceberg" theory where deep culture drives surface behaviors.

### Mechanism 2: Diagnostic Knowledge Injection
Performance gaps in cross-cultural tasks often stem from a failure to retrieve relevant cultural schemas rather than a complete absence of knowledge, which can be diagnosed via explicit prompt augmentation. The evaluation protocol tests three levels: Zero Injection (no context), Weak Injection (one-sentence definition), and Strong Injection (detailed cultural explanation). By observing the delta in accuracy (e.g., GPT-4o jumping from ~0.40 to ~0.91), the mechanism distinguishes between "knowledge missing" and "knowledge dormant." This assumes that if a model improves significantly under Strong Injection, the underlying parametric knowledge exists but is not contextually triggered.

### Mechanism 3: Epistemic Distance Hierarchy
Model performance degrades predictably based on the "epistemic distance" of the cultural dimension, with abstract domains (Spirituality, Metaphysics) failing before concrete ones (Society, Social Order). The paper organizes evaluation into 9 dimensions across 3 layers (Symbolic, Ritual, Value). The mechanism posits that "Society" concepts (governance, laws) are more standardized across training corpora, whereas "Metaphysics" concepts (cosmology, unseen beliefs) have high variance and fewer explicit textual anchors. This assumes that LLM training data better covers institutional/instrumental concepts than subjective/abstract ones.

## Foundational Learning

- **Concept: Cultural Iceberg / Onion Model**
  - Why needed here: The SAGE architecture is explicitly built on a 3-layer taxonomy (Symbolic, Behavioral, Value). Understanding that "Deep Culture" (values/cognition) is implicit and harder to operationalize than "Surface Culture" (symbols/rituals) is necessary to interpret the performance gaps in the results.
  - Quick check question: Can you explain why the "Society" dimension might yield higher accuracy than "Spirituality" based on the "Surface vs. Deep" culture distinction?

- **Concept: Cultural Vacancy & Non-Equivalence**
  - Why needed here: The benchmark refuses to force 1:1 mappings between languages (e.g., Chinese "Yuan" is not simply Spanish "Destino"). Evaluators must understand that some concepts lack direct translation to correctly assess the model's handling of "Full," "Partial," or "Vacant" correspondences.
  - Quick check question: In the concept mapping phase, how should the system handle a concept that is salient in Chinese but has no lexical counterpart in Spanish?

- **Concept: Knowledge Injection as a Probe**
  - Why needed here: This is a core evaluation method in the paper. One must distinguish between "Zero-shot" (testing application) and "Strong Injection" (testing raw knowledge capacity) to correctly diagnose model failures.
  - Quick check question: If a model scores 0.2 on Zero Injection but 0.9 on Strong Injection, what specific capability gap does this diagnose?

## Architecture Onboarding

- **Component map:** CCC Set (Input) -> Scenario Engine -> Question Generator -> Evaluation Protocol
- **Critical path:** The definition of the **CCC Set (Cross-cultural Core Concepts)** is the most critical step. The paper emphasizes that concepts must be grounded in "culture-internal" scholarship (conceptual history) rather than outsider assumptions. If the CCC seed set is biased or shallow, the scenario generation and injection diagnostics will amplify that error.
- **Design tradeoffs:**
  - Depth vs. Automation: The pipeline uses LLMs to generate questions but constrains them to human-curated concepts to prevent hallucination of cultural facts.
  - Specificity vs. Transfer: Focusing on specific real-world scenarios (e.g., "Art Exhibitions") increases validity but requires more effort to scale compared to generic QA pairs.
- **Failure signatures:**
  - Prototype Collapse: The model merges distinct cultural prototypes (e.g., assuming all "Dragons" are malicious or all are auspicious).
  - Universalist Fallacy: The model selects options implying a symbol has the same meaning globally (a common trap in the dataset).
  - Flat Injection Curve: Model performance fails to improve even with Strong Injection, indicating a fundamental lack of cultural knowledge rather than a retrieval issue.
- **First 3 experiments:**
  1. Baseline Profiling: Run the target model on the full SAGE set under **Zero Injection** to establish the "Cultural Dimension Hierarchy" (identifying if the model fails specifically at Metaphysics/Spirituality).
  2. Injection Sensitivity Test: Re-run on a subset (e.g., 500 items) across Weak and Strong injection levels. Calculate the "gain delta" to determine if failures are retrieval-based or knowledge-based.
  3. Cross-Lingual Transfer Check: Evaluate the model on the Korean extension (if available) or Spanish tasks to see if "Cultural Proximity" (shared Confucian values between CN/KO) correlates with higher accuracy, validating the epistemic distance theory.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can instruction tuning on the SAGE benchmark effectively transfer to improved zero-shot performance in unseen cross-cultural scenarios?
- Basis in paper: The authors state in the conclusion that "Future work can leverage SAGE to guide instruction tuning and cross-lingual alignment."
- Why unresolved: The current study evaluates models under zero-shot and prompt-based knowledge injection settings but does not measure the efficacy of fine-tuning on the dataset itself.
- What evidence would resolve it: Empirical results showing that models fine-tuned on the SAGE training set exhibit improved accuracy on held-out cross-cultural scenarios without requiring explicit concept definitions in the prompt.

### Open Question 2
- Question: How can internal model mechanisms be modified to autonomously trigger the deep cultural schemas that currently require explicit "strong injection" to activate?
- Basis in paper: The results show that while zero-shot performance is low, "knowledge injection substantially improves performance," indicating models possess the knowledge but fail to retrieve it during situated reasoning.
- Why unresolved: The paper diagnoses a gap between knowledge availability and application but does not propose architectural methods to bridge this retrieval gap without external prompt scaffolding.
- What evidence would resolve it: Demonstration of a model that achieves "strong injection" level performance in zero-shot settings through architectural modifications rather than prompt engineering.

### Open Question 3
- Question: Is the consistent performance hierarchy across cultural dimensions (where abstract domains like spirituality underperform concrete ones like social order) a data distribution issue or a fundamental limitation of current LLM reasoning?
- Basis in paper: The paper observes a consistent hierarchy where abstract domains show the largest gaps and are most sensitive to injection, suggesting an "epistemic distance" between universal knowledge and localized values.
- Why unresolved: The study identifies this systematic limitation across models but does not isolate whether the failure stems from training data sparsity or the inability of models to handle implicit, non-observable value structures.
- What evidence would resolve it: Ablation studies on models trained specifically on deep cultural values to see if the performance hierarchy flattens, or analysis showing whether attention mechanisms fail to link abstract concepts to situational contexts.

## Limitations
- Concept Selection Bias: The 210 CCCs were selected via expert panel, but the specific panel composition, selection criteria, and validation process are not fully specified, introducing potential cultural insider-outsider bias.
- Evaluation Rubric Granularity: While expert agreement (Fleiss' Îº=0.87) is reported for Chinese tasks, the scoring criteria for short-answer questions are not fully detailed, limiting reproducibility.
- Language Scope Constraints: The benchmark covers Chinese-Spanish with Korean extension mentioned but not detailed; generalization to other language pairs remains unproven.

## Confidence
- **High Confidence**: The identification of three key limitations in existing cross-cultural evaluation, the basic architecture of SAGE, and the finding that LLMs show critical deficiencies in cross-cultural reasoning.
- **Medium Confidence**: The epistemic distance hierarchy showing abstract dimensions having lower accuracy than concrete ones, and the effectiveness of knowledge injection in improving performance.
- **Low Confidence**: The specific numerical accuracy values (e.g., 0.41 best zero-shot performance) due to unknown prompt templates and scoring rubrics, and the cross-lingual transfer findings between Chinese and Korean.

## Next Checks
1. Replicate with Transparent Prompting: Obtain and test the exact prompt templates for all three injection levels across all question types to verify the reported accuracy scores are reproducible.
2. Cross-Cultural Expert Validation: Have independent cultural experts from each dimension review a random sample of 100 items to validate that the "Cultural Traps" (Universalist Fallacy, Analogy Misalignment) are correctly identified and scored.
3. Language Pair Generalization: Extend the evaluation to include at least one non-East Asian language pair (e.g., Arabic-English) to test whether the epistemic distance hierarchy holds across different cultural domains.