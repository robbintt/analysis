---
ver: rpa2
title: 'ER-REASON: A Benchmark Dataset for LLM-Based Clinical Reasoning in the Emergency
  Room'
arxiv_id: '2505.22919'
source_url: https://arxiv.org/abs/2505.22919
tags:
- clinical
- reasoning
- patient
- notes
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ER-Reason, a benchmark dataset for evaluating
  large language models (LLMs) in emergency room clinical reasoning. The dataset includes
  3,984 patients with 25,174 de-identified longitudinal clinical notes spanning discharge
  summaries, progress notes, history and physical exams, consults, echocardiography
  reports, imaging notes, and ER provider documentation.
---

# ER-REASON: A Benchmark Dataset for LLM-Based Clinical Reasoning in the Emergency Room

## Quick Facts
- **arXiv ID:** 2505.22919
- **Source URL:** https://arxiv.org/abs/2505.22919
- **Reference count:** 38
- **Primary result:** Introduces ER-Reason, a benchmark dataset for evaluating large language models (LLMs) in emergency room clinical reasoning, revealing gaps between LLM and clinician performance across five ER workflow stages.

## Executive Summary
This paper introduces ER-Reason, a benchmark dataset for evaluating large language models (LLMs) in emergency room clinical reasoning. The dataset includes 3,984 patients with 25,174 de-identified longitudinal clinical notes spanning discharge summaries, progress notes, history and physical exams, consults, echocardiography reports, imaging notes, and ER provider documentation. The benchmark evaluates LLMs across five key stages of the ER workflow: triage intake, EHR review, initial assessment, treatment selection, disposition planning, and final diagnosis. Each task is designed to reflect core clinical reasoning processes such as differential diagnosis via rule-out reasoning. The dataset also includes 72 physician-authored rationales that explain reasoning processes mimicking residency training. Evaluations of state-of-the-art LLMs reveal a gap between LLM-generated and clinician-authored clinical reasoning for ER decisions, with models showing particular weaknesses in synthesizing complex clinical information and accurately predicting patient disposition. The results highlight the need for future research to improve LLM clinical reasoning capabilities in realistic medical settings.

## Method Summary
The ER-Reason benchmark evaluates LLMs on five emergency room clinical reasoning tasks using 3,984 patients with 25,174 longitudinal clinical notes. Tasks include acuity assessment, case summarization, treatment planning, final diagnosis, and disposition. The benchmark uses zero-shot inference with models including LLaMA 3.2-3B Instruct, GPT-3.5, GPT-4o, and o3-mini. Evaluation employs task-specific metrics: accuracy for acuity and disposition, ROUGE-F1 for summarization, CUI recall via cTAKES for reasoning, and ICD-10/HCC match for diagnosis. Physician-authored rationales for 72 cases provide ground truth for reasoning evaluation.

## Key Results
- LLMs show highest agreement with physicians on medical decision factors but modest overall agreement across clinical reasoning dimensions.
- Models exhibit significant bias in acuity prediction, over-predicting "Urgent" cases and failing to identify "Less/Non-Urgent" cases.
- Models over-predict admission and under-predict discharge in disposition tasks, with o3-mini predicting only ~15% discharge vs. 37% actual.
- GPT-4o matched disease entities and tests but omitted patient symptoms entirely, indicating incomplete reasoning despite CUI overlap.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aligning evaluation tasks to distinct ER workflow stages captures interdependent clinical reasoning that isolated benchmarks miss.
- Mechanism: The benchmark decomposes ER care into five stages—triage intake, EHR review, initial assessment, treatment selection, and disposition—each with staged inputs and outputs. Coherence across stages (e.g., how initial assessment informs treatment choice) provides implicit evaluation of reasoning chains, not just final outputs.
- Core assumption: Real clinical decisions are sequentially dependent and context-aware; evaluating them in isolation reduces construct validity.
- Evidence anchors:
  - [abstract] "The benchmark includes evaluation tasks that span key stages of the ER workflow: triage intake, initial assessment, treatment selection, disposition planning, and final diagnosis—each structured to reflect core clinical reasoning processes."
  - [section 3.3] "Clinical reasoning is captured both implicitly, through the coherence of decisions across stages (e.g., how earlier assessments inform later choices), and explicitly, through documented rationales."
  - [corpus] Related work on SLMs for ED decision support (arXiv:2510.04032) similarly emphasizes multi-task evaluation but does not provide longitudinal notes or reasoning rationales.

### Mechanism 2
- Claim: Longitudinal, heterogeneous clinical notes force models to synthesize incomplete, temporally fragmented information, better reflecting real-world reasoning difficulty.
- Mechanism: Each patient encounter includes an average of seven notes across diverse types (discharge summaries, imaging, consults, ER provider notes) from prior visits. Models must identify relevant historical context for the current chief complaint, mirroring how clinicians prioritize information under time pressure.
- Core assumption: Clinical reasoning relies on extracting signal from noisy, unstructured records; curated vignettes artificially simplify this task.
- Evidence anchors:
  - [abstract] "ER-Reason includes data from 3,984 patients, encompassing 25,174 de-identified longitudinal clinical notes spanning discharge summaries, progress notes, history and physical exams, consults, echocardiography reports, imaging notes, and ER provider documentation."
  - [section 3.1] "Unlike standardized note formats such as SOAP... the diversity and unstructured nature of these notes present additional complexity for reasoning tasks."
  - [corpus] DENSE (arXiv:2507.14079) similarly models temporal progress notes but focuses on note generation rather than reasoning evaluation.

### Mechanism 3
- Claim: Semantic concept-level evaluation via UMLS CUI mapping captures clinical reasoning equivalence despite lexical variation.
- Mechanism: The cTAKES toolkit maps free-text model outputs to Concept Unique Identifiers (CUIs), enabling structured comparison with physician-authored rationales. This allows "Coronary Artery Disease" and "CAD" to be treated as equivalent, measuring whether models reason about the same clinical concepts rather than matching exact phrasing.
- Core assumption: Overlapping CUIs reflect overlapping clinical reasoning; missing CUIs indicate reasoning gaps.
- Evidence anchors:
  - [section 3.3.3] "For example, if a model predicts 'Coronary Artery Disease' and a physician writes 'CAD' in their differential, both are assigned to the same CUI (C1956346), maintaining semantic equivalence despite lexical variation."
  - [section 4, Task 3 results] "Evaluation of the baseline LLMs across the three dimensions of clinical reasoning show the highest agreement with physicians in the domain of medical decision factors. However, overall agreement rates remained modest."
  - [corpus] No corpus papers explicitly use CUI-based reasoning evaluation; most rely on ROUGE or accuracy metrics.

## Foundational Learning

- Concept: **Clinical reasoning as rule-out reasoning**
  - Why needed here: The benchmark explicitly evaluates whether models can enumerate plausible diagnoses and identify steps to confirm or exclude them—a core ER cognitive pattern.
  - Quick check question: Given chest pain, can you list three differential diagnoses and the tests needed to rule each out?

- Concept: **ER workflow stages and decision points**
  - Why needed here: Each benchmark task maps to a specific stage; understanding what information is available at each stage is essential for interpreting inputs and outputs.
  - Quick check question: At triage intake, what patient information is typically available versus what becomes available during initial assessment?

- Concept: **Construct validity in benchmark design**
  - Why needed here: The paper argues that licensing-exam benchmarks lack construct validity—they don't represent real clinical workflows. Understanding this concept helps contextualize why ER-Reason's design matters.
  - Quick check question: Why would high accuracy on multiple-choice diagnosis questions not guarantee strong performance on real ER decision-making?

## Architecture Onboarding

- Component map: Data layer (3,984 patients, 25,174 longitudinal notes) -> Task layer (5 ER workflow tasks) -> Annotation layer (72 physician rationales) -> Evaluation layer (task-specific metrics)

- Critical path:
  1. Load longitudinal notes for a patient encounter
  2. Filter/select notes relevant to current chief complaint (per task requirements)
  3. Construct prompt with patient context, chief complaint, and task-specific instructions (see Appendix prompts)
  4. Generate model output
  5. Extract CUIs via cTAKES for reasoning tasks; compare to physician-authored rationale CUIs
  6. Compute task-specific metrics

- Design tradeoffs:
  - **Sparse vs. full context**: Triage uses only chief complaint, vitals, demographics; treatment planning uses all notes. Tradeoff between realism and input length limits.
  - **Lexical vs. semantic evaluation**: ROUGE captures surface overlap for summarization; CUI recall captures semantic equivalence for reasoning. Neither alone is sufficient.
  - **Exact vs. category matching**: ICD-10 exact match is strict; HCC category match tolerates clinically minor variations. Paper reports both (e.g., o3-mini: 34.40% ICD-10 vs. 81.08% HCC).

- Failure signatures:
  - **Acuity compression**: Models default to mid-level acuity ("Urgent"), failing to identify extremes—o3-mini predicted 73.62% Urgent vs. 54.83% actual, with 0% Less/Non-Urgent predictions.
  - **Disposition bias**: Models over-predict admission and under-predict discharge—o3-mini predicted ~15% discharge vs. 37% actual.
  - **Missing symptom integration**: In Figure 3, GPT-4o matched disease entities and tests but omitted patient symptoms entirely, indicating incomplete reasoning even when CUI overlap appears strong.
  - **Hallucination in rule-out reasoning**: Table 5 shows slightly higher hallucination severity (1.9/5) for rule-out reasoning vs. medical factors (1.1/5), likely because models infer undocumented information to exclude diagnoses.

- First 3 experiments:
  1. **Establish baseline on Task 3 (treatment planning)** using a standard LLM with the provided prompts. Compute CUI recall for rule-out reasoning, medical factors, and treatment planning separately to identify which reasoning dimension is weakest.
  2. **Ablate note types**: Run Task 3 using only discharge summaries vs. all available notes. Measure impact on CUI recall to test whether longitudinal context improves reasoning or introduces noise.
  3. **Calibrate disposition predictions**: Evaluate whether threshold tuning on model confidence scores can reduce the admission overprediction bias observed in o3-mini. Compare raw accuracy vs. calibrated accuracy against physician dispositions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can incorporating hospital-level contextual factors (bed availability, staffing constraints, institutional protocols) improve LLM accuracy on triage prioritization and disposition decisions?
- Basis in paper: [explicit] The authors state: "a key limitation of ER-REASON is that it does not yet account for hospital-level contextual factors—such as bed availability, staffing constraints, and institutional protocols—which can significantly influence decisions such as triage prioritization and patient disposition."
- Why unresolved: The current benchmark excludes systemic constraints that shape real ER decisions; models may appear inaccurate when they lack context physicians actually use.
- What evidence would resolve it: Ablation studies comparing model performance with vs. without hospital-level context features on disposition and triage tasks.

### Open Question 2
- Question: What training or prompting strategies can correct the "regression to the mean" phenomenon where models over-predict mid-level acuity (Urgent) and under-predict both extremes?
- Basis in paper: [explicit] The paper notes o3-mini "fails to identify any Less Urgent or Non-Urgent cases, effectively compressing the acuity spectrum," and describes "a 'regression to the mean' phenomenon in its clinical assessments."
- Why unresolved: Current models lack calibration for the full acuity spectrum, potentially causing resource misallocation.
- What evidence would resolve it: Demonstration of methods (calibration, class-balanced training, specialized prompting) that yield balanced acuity predictions across all 5 ESI levels matching true distribution.

### Open Question 3
- Question: Can LLMs be trained to incorporate patient symptoms as anchors in diagnostic reasoning, given that GPT-4o failed to reference symptoms despite their importance in clinical reasoning?
- Basis in paper: [explicit] Figure 3 analysis states: "the model fails to incorporate any patient symptoms (green), which are critical anchors in clinical diagnostic reasoning."
- Why unresolved: The model omission suggests a fundamental gap in how LLMs weight and integrate symptom information during reasoning.
- What evidence would resolve it: Comparative evaluation showing improved CUI overlap and clinical utility when models are explicitly trained or prompted to anchor reasoning on presenting symptoms.

## Limitations

- The benchmark's clinical realism depends on whether the 3,984 patient encounters adequately represent the full spectrum of ER presentations, particularly rare but critical cases.
- Semantic evaluation via UMLS CUI mapping may underestimate reasoning quality if important clinical concepts lack CUI coverage or if models use clinically valid terminology outside the CUI dictionary.
- The gap between LLM and physician performance may partly reflect differences in evaluation criteria—physicians can recognize correct reasoning even with lexical variation that CUI matching might miss.

## Confidence

- **High Confidence**: The benchmark structure (5-stage workflow with longitudinal notes) is clearly specified and directly supported by the abstract and methods section. The task definitions and evaluation metrics are unambiguous.
- **Medium Confidence**: The claim that this represents the first ER-specific clinical reasoning benchmark is supported by the corpus analysis showing no direct predecessors, though some related work exists in ED decision support and temporal note modeling.
- **Low Confidence**: The interpretation that model weaknesses in disposition prediction reflect fundamental reasoning limitations rather than task-specific biases or training data artifacts requires further validation.

## Next Checks

1. **Test robustness to CUI coverage gaps**: Identify clinically important concepts missing from UMLS and measure how often these appear in physician rationales versus model outputs. This will determine if semantic evaluation underestimates reasoning quality.
2. **Evaluate case distribution**: Analyze the demographic and clinical characteristics of the 3,984 patients to verify representation across acuity levels, chief complaints, and outcomes. Compare against real ER population statistics.
3. **Conduct inter-rater reliability**: Have multiple physicians independently evaluate a subset of model outputs to determine if the CUI-based performance gaps align with human judgment of reasoning quality.