---
ver: rpa2
title: 'Rethinking Predictive Modeling for LLM Routing: When Simple kNN Beats Complex
  Learned Routers'
arxiv_id: '2505.12601'
source_url: https://arxiv.org/abs/2505.12601
tags:
- routing
- performance
- utility
- approaches
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the prevailing trend toward complex learned
  routing approaches for selecting the best large language model (LLM) for a given
  input. Instead, it demonstrates that simple k-Nearest Neighbors (kNN) methods, when
  properly implemented, can match or outperform sophisticated learned routers across
  diverse text and multi-modal routing tasks.
---

# Rethinking Predictive Modeling for LLM Routing: When Simple kNN Beats Complex Learned Routers

## Quick Facts
- arXiv ID: 2505.12601
- Source URL: https://arxiv.org/abs/2505.12601
- Reference count: 40
- Simple kNN methods can match or outperform complex learned routers for LLM selection across diverse tasks

## Executive Summary
This paper challenges the prevailing trend toward complex learned routing approaches for selecting the best large language model (LLM) for a given input. Instead, it demonstrates that simple k-Nearest Neighbors (kNN) methods, when properly implemented, can match or outperform sophisticated learned routers across diverse text and multi-modal routing tasks. The study introduces standardized routing benchmarks spanning instruction-following, question-answering, reasoning, and vision-language tasks, enabling systematic evaluation of routing approaches. Theoretical analysis shows that kNN routers require lower sample complexity than parametric methods when the embedding space exhibits strong locality properties, explaining their effectiveness. Across all evaluated scenarios, kNN consistently achieved competitive or superior performance compared to complex approaches like Graph Neural Networks, attention mechanisms, and matrix factorization-based routers, with simple linear models also showing strong results.

## Method Summary
The study evaluates routing methods across text and multi-modal benchmarks, using 70/10/20 train/val/test splits. kNN routers retrieve k nearest neighbors from a support set using cosine similarity on embeddings (BERT-base for text, VLM2Vec for multi-modal) and aggregate their recorded performance scores and costs to predict utility. The routing objective maximizes utility (performance - λ·cost), with evaluation using AUC of the Pareto front for utility prediction and average utility across three λ settings for model selection. The research compares kNN against parametric methods including MLPs, attention mechanisms, GNNs, and matrix factorization approaches.

## Key Results
- kNN routers match or exceed complex learned routers across all evaluated text and multi-modal routing tasks
- Simple linear models show surprisingly strong performance, suggesting complexity isn't always necessary
- Theoretical analysis proves kNN has lower sample complexity than parametric approaches under locality assumptions
- The study establishes the first standardized routing benchmarks with consistent splits and evaluation protocols

## Why This Works (Mechanism)

### Mechanism 1: Locality Property in Embedding Space
- Claim: kNN-based routers leverage the locality property of model performance in embedding space to make effective routing decisions.
- Mechanism: Semantically similar queries, as measured by embedding distance, tend to yield similar performance from the same models. kNN aggregates outcomes from the k-nearest neighbors in the training set to estimate performance and cost for a new query.
- Core assumption: The query embedding space exhibits δ-locality, where utility differences between nearby queries are bounded: d(x₁, x₂) < δ ⟹ |u(x₁, m) - u(x₂, m)| < ε(δ), with ε monotonically increasing and ε(0) = 0.
- Evidence anchors:
  - [abstract] "locality properties of model performance in embedding space enable simple non-parametric methods to achieve strong routing decisions"
  - [section 7] Theoretical analysis formalizes δ-locality and shows sample complexity advantage for kNN (Θ(C_{X,d}/δ^d · log(1/α))) vs. parametric (Ω(L/ε(δ)²)).
  - [corpus] Limited direct corpus support for locality in routing; related work on kNN for MoE routing ("Routing by Analogy") suggests neighborhood-based assignment helps, but doesn't quantify locality.
- Break condition: If embedding distance no longer correlates with model performance agreement (e.g., adversarial embeddings, domain shift where semantic similarity doesn't imply similar utility), locality degrades and kNN predictions become unreliable.

### Mechanism 2: Sample Complexity Advantage from Non-Parametric Estimation
- Claim: kNN routers achieve comparable performance to learned parametric routers with fewer training samples due to lower sample complexity requirements.
- Mechanism: kNN is a memory-based estimator that doesn't learn global function parameters. Under δ-locality, coverage of the embedding space with sufficient neighbors ensures bounded regret, with complexity scaling with intrinsic dimension d rather than model capacity.
- Core assumption: The embedding space has relatively low intrinsic dimension d, and the training distribution provides adequate coverage of query regions (no extreme sparse regions).
- Evidence anchors:
  - [abstract] "lower sample complexity than parametric approaches"
  - [section 7, Theorem 1] Formal proof: kNN requires Θ(C_{X,d}/δ^d · log(1/α)) samples for O(ε(δ)) regret; parametric with L Lipschitz layers requires Ω(L/ε(δ)²).
  - [corpus] No corpus papers directly compare sample complexity for routing; xRouter focuses on cost-aware RL but doesn't address sample efficiency relative to kNN.
- Break condition: If the intrinsic dimension d of the effective embedding space is high, or if the utility function varies sharply (large Lipschitz constant for true u(x,m)), the covering number explodes and kNN requires prohibitively many samples.

### Mechanism 3: Support Set Contextualization via Retrieval
- Claim: Using a support set of (query, model, score, cost) tuples provides contextual signal that improves routing accuracy, particularly for non-parametric methods.
- Mechanism: At inference, retrieve k nearest neighbors from the support set and aggregate their recorded outcomes (for utility prediction: weighted average; for selection: majority vote). This grounds predictions in actual observed performance on similar inputs.
- Core assumption: The support set is representative of the test distribution and contains reliable (non-noisy) performance labels; retrieval embedding space aligns with utility similarity.
- Evidence anchors:
  - [section 4] "Non-parametric methods like k-Nearest Neighbors (kNN) estimate s(x, m) and c(x, m) by retrieving similar inputs xᵢ from D_support and aggregating their recorded outcomes."
  - [section 6] Results show increasing k from 10 to 100 improves performance in most cases, indicating sufficient neighborhood context matters.
  - [corpus] "Routing by Analogy" also uses kNN to augment MoE expert assignment, supporting retrieval-based contextualization.
- Break condition: If support set labels are noisy (evaluation metric instability), or if distribution shift makes retrieved neighbors unrepresentative, aggregated estimates introduce systematic error.

## Foundational Learning

- Concept: k-Nearest Neighbors (kNN) Regression/Classification
  - Why needed here: Core algorithm used for routing; understanding distance metrics, aggregation strategies (mean vs. weighted), and sensitivity to k is essential.
  - Quick check question: For a query x, if k=5 nearest neighbors have scores [0.8, 0.6, 0.7, 0.9, 0.5] for model m, what's the kNN utility estimate?

- Concept: Sample Complexity and Covering Numbers
  - Why needed here: Theoretical justification for kNN's efficiency relies on covering number arguments; knowing how intrinsic dimension affects sample needs helps predict when simple methods suffice.
  - Quick check question: If intrinsic dimension d=3 and δ=0.1, roughly how does covering number scale with 1/δ?

- Concept: Utility-Based Routing (Performance - λ·Cost)
  - Why needed here: The routing objective combines quality and cost; understanding the Pareto front, λ sensitivity, and AUC evaluation is critical for implementation.
  - Quick check question: If model A has score 0.9 and cost $0.10, model B has score 0.7 and cost $0.01, which does λ=10 prefer?

## Architecture Onboarding

- Component map:
  - Embedding Encoder: BERT (text) / VLM2Vec (multi-modal) → query representation
  - Support Set Store: Indexed collection of (query embedding, model, score, cost) tuples
  - kNN Retrieval Module: Finds k nearest neighbors by cosine similarity
  - Aggregation Module: Computes utility estimates (weighted mean) or selection (majority vote)
  - Decision Layer: Selects model maximizing predicted utility

- Critical path:
  1. Encode query → embedding
  2. Retrieve k neighbors from support set
  3. Aggregate neighbor outcomes per candidate model
  4. Compute utility = estimated_score - λ·estimated_cost
  5. Return model with highest utility

- Design tradeoffs:
  - k value: Small k (e.g., 10) is more local but noisier; large k (e.g., 100) smooths estimates but may include less-relevant neighbors.
  - Embedding model: BERT is fast and adequate; SFR embeddings offer modest gains but at higher dimension/cost.
  - Support set size: Larger sets improve coverage but increase retrieval latency and memory.

- Failure signatures:
  - Performance degrades sharply on out-of-distribution queries (no nearby neighbors within δ).
  - High variance in predictions when support set is sparse in certain embedding regions.
  - Cost estimates inaccurate if token counts vary widely across neighbors.

- First 3 experiments:
  1. Baseline kNN validation: Implement kNN router with k∈{10,50,100}, BERT embeddings, on RouterBench split. Measure AUC vs. random/oracle.
  2. Locality sanity check: Plot embedding distance vs. model performance agreement for held-out pairs. Confirm negative correlation (as in Fig. 1).
  3. Sample efficiency comparison: Train kNN and MLP routers on subsets (10%, 30%, 50%, 100% of training data). Compare AUC curves to verify kNN's lower sample complexity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the effectiveness of simple kNN routers persist when trained on alternative signals, such as reward models or pairwise preferences, rather than direct evaluation metrics?
- Basis in paper: [explicit] The authors state that "investigating how simple routers perform with alternative training signals (reward models, preferences) remains an important direction" (Section 8).
- Why unresolved: The current study relies exclusively on direct evaluation metrics (e.g., accuracy, win-rates) derived from leaderboards to populate support sets.
- What evidence would resolve it: A comparative analysis of router performance when the utility scores in the support set are derived from proxy reward models or preference data rather than ground-truth evaluation results.

### Open Question 2
- Question: Can specialized embedding optimization enhance routing performance in domains where standard pre-trained embeddings exhibit weak locality properties?
- Basis in paper: [explicit] Section 8 highlights "Embedding Optimization" as a future area, suggesting that "specialized embedding learning could further enhance routing performance, particularly where locality properties are weaker."
- Why unresolved: The paper establishes that current embeddings show strong locality, but does not explore if custom embeddings could improve performance in difficult sub-domains or complex reasoning tasks.
- What evidence would resolve it: Experiments fine-tuning embedding models specifically for the routing task to determine if they increase the correlation between embedding distance and performance similarity (locality) compared to off-the-shelf models like BERT or VLM2Vec.

### Open Question 3
- Question: How can non-parametric kNN approaches be effectively extended to batch routing scenarios involving global computational constraints?
- Basis in paper: [explicit] The discussion identifies "Batch Routing" as a challenge, specifically "extending kNN approaches to batch routing with global computational constraints."
- Why unresolved: The current framework optimizes routing for individual queries in isolation ($m^* = \arg\max s(x,m)$) rather than optimizing a collection of queries under a single aggregate budget.
- What evidence would resolve it: The formulation of a kNN-based algorithm that selects models for a batch of inputs to maximize total utility while adhering to a global cost constraint, compared against parametric optimization methods.

## Limitations

- The study's conclusions rely heavily on the assumption that query embedding space exhibits strong locality properties, which may not hold in real-world scenarios with significant domain shifts
- The cost estimation relies on API pricing and assumed token counts, which may not reflect actual deployment scenarios
- The paper doesn't fully address how kNN performance degrades with noisy support set labels or when the embedding space has high intrinsic dimensionality

## Confidence

- **High Confidence**: kNN achieves competitive performance to complex learned routers on the evaluated benchmarks
- **Medium Confidence**: Sample complexity advantage of kNN over parametric methods
- **Medium Confidence**: Locality property as the primary mechanism

## Next Checks

1. **Distribution Shift Robustness**: Test kNN routers on out-of-distribution queries from different domains to measure performance degradation when locality assumptions fail.
2. **Intrinsic Dimension Analysis**: Empirically measure the intrinsic dimensionality of the effective embedding space across benchmarks to validate theoretical sample complexity predictions.
3. **Cost Estimation Validation**: Compare kNN's predicted costs against actual measured costs in a real deployment to assess the accuracy of the token-count-based cost model.