---
ver: rpa2
title: 'Game of Thought: Robust Information Seeking with Large Language Models Using
  Game Theory'
arxiv_id: '2602.01708'
source_url: https://arxiv.org/abs/2602.01708
tags:
- questions
- game
- item
- items
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of robust information-seeking with
  large language models (LLMs) under worst-case assumptions. The authors formalize
  the Strategic Language Search (SLS) problem as a two-player zero-sum extensive-form
  game, where an adversarial Item Chooser selects an item and a Questioner seeks it
  through Yes/No questions.
---

# Game of Thought: Robust Information Seeking with Large Language Models Using Game Theory

## Quick Facts
- arXiv ID: 2602.01708
- Source URL: https://arxiv.org/abs/2602.01708
- Reference count: 40
- Primary result: Game-theoretic framework improves worst-case performance in information-seeking tasks using LLMs

## Executive Summary
This paper addresses the challenge of robust information-seeking with large language models under worst-case assumptions by formalizing the Strategic Language Search (SLS) problem as a two-player zero-sum extensive-form game. The authors propose Game of Thought (GoT), which approximates Nash equilibrium strategies using depth-limited subgame search and heuristic evaluation. The framework demonstrates significant improvements in worst-case performance across multiple domains including 20 Questions, medical diagnosis, and troubleshooting, with up to 40% improvement in weighted variants where item severity is considered.

## Method Summary
The authors model information-seeking as a zero-sum game between an adversarial Item Chooser and a Questioner who seeks to identify the chosen item through Yes/No questions. GoT approximates optimal play by performing depth-limited subgame search combined with a heuristic evaluation function to estimate values of unexplored branches. The framework uses Monte Carlo Tree Search principles adapted for language-based questions, where the questioner aims to minimize worst-case query count while the adversary maximizes it. For weighted variants, item severities are incorporated into the evaluation function, allowing the system to prioritize finding more severe cases with fewer questions.

## Key Results
- GoT achieves worst-case interaction length of 10.2 questions vs. 11 for UoT on the Common dataset
- Weighted variants show up to 40% improvement over baseline methods
- Consistent improvements across three domains: 20 Questions, medical diagnosis, and troubleshooting
- Optimizing for worst-case scenarios proves more effective than average-case information gain approaches

## Why This Works (Mechanism)
The framework works by explicitly modeling the adversarial nature of information-seeking, where worst-case performance matters more than average-case efficiency. By treating the problem as a zero-sum game, GoT forces the questioner to prepare for the most difficult scenarios rather than optimizing for typical cases. The depth-limited search with heuristic evaluation allows tractable approximation of Nash equilibrium strategies without exhaustive computation, while the game-theoretic formulation ensures robustness against adversarial item selection.

## Foundational Learning

- Zero-sum extensive-form games: Needed to model adversarial item selection and question-asking interaction; Quick check: Verify game tree structure correctly represents question-answer sequences
- Nash equilibrium approximation: Required for finding robust strategies without exhaustive search; Quick check: Confirm approximation quality degrades gracefully with search depth
- Monte Carlo Tree Search adaptation: Essential for tractable exploration of question space; Quick check: Validate UCB1 exploration-exploitation balance in question selection
- Heuristic evaluation functions: Critical for estimating unexplored subgame values; Quick check: Test correlation between heuristic estimates and actual subgame outcomes
- Weighted game formulations: Necessary for incorporating item severity into decision-making; Quick check: Verify severity weights properly influence question prioritization

## Architecture Onboarding

Component map: Game tree -> MCTS search -> Heuristic evaluation -> Question selection
Critical path: Adversarial setup → Tree construction → Search with pruning → Heuristic fallback → Question output
Design tradeoffs: Depth-limited search vs. computational cost, heuristic accuracy vs. search completeness
Failure signatures: Poor heuristic estimates lead to suboptimal questions; shallow search misses important branches
First experiments: 1) Test with known optimal solutions to verify search correctness; 2) Compare different heuristic functions on same game tree; 3) Measure performance degradation as search depth decreases

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability concerns for item sets larger than tested ranges
- Absolute interaction lengths (e.g., 10+ questions) may be impractical for real deployment
- Limited validation on truly independent datasets beyond the studied domains
- Focus on binary questions restricts applicability to domains requiring richer query types

## Confidence
High: Game-theoretic formulation is well-established
Medium: Empirical results across multiple domains support claims
Low: Long-term scalability and real-world deployment feasibility remain unproven

## Next Checks
1. Test GoT on item sets with 10x more elements to evaluate scalability and computational requirements
2. Implement ablation studies removing the heuristic evaluation to quantify its contribution to performance
3. Validate the framework with multi-choice questions rather than binary to assess generalization to richer query spaces