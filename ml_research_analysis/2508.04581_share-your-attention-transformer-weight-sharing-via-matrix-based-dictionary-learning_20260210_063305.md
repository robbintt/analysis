---
ver: rpa2
title: 'Share Your Attention: Transformer Weight Sharing via Matrix-based Dictionary
  Learning'
arxiv_id: '2508.04581'
source_url: https://arxiv.org/abs/2508.04581
tags:
- masa
- attention
- matrix
- dictionary
- sharing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MASA, a matrix-based dictionary learning approach
  to reduce parameter redundancy in transformer attention layers. MASA decomposes
  query, key, value, and output projection matrices into shared atoms, enabling each
  layer to reconstruct its weights via linear combinations.
---

# Share Your Attention: Transformer Weight Sharing via Matrix-based Dictionary Learning

## Quick Facts
- arXiv ID: 2508.04581
- Source URL: https://arxiv.org/abs/2508.04581
- Reference count: 24
- Primary result: MASA achieves 66.7% parameter reduction in transformer attention with maintained performance

## Executive Summary
This paper introduces MASA (Matrix-based Attention Sharing Approach), a dictionary learning method for compressing transformer attention layers. MASA decomposes query, key, value, and output projection matrices into shared dictionary atoms, enabling each layer to reconstruct its weights via linear combinations. The method achieves 66.7% reduction in attention parameters while maintaining performance on language and vision tasks. Experiments show MASA outperforms existing compression methods including GQA, low-rank baselines, and sequential/Repeat-all-over sharing, with 34.43% average accuracy and 112.23 WikiText perplexity on a 110M-parameter model.

## Method Summary
MASA decomposes attention projection matrices (Q, K, V, O) into shared dictionary atoms D_s, where each layer's weight matrix W_l is reconstructed as W_l = Σ c_ls D_s using learned coefficients c_ls. The method uses separate dictionaries per projection type to preserve functional specialization, with coefficients predicted by a 3-layer MLP from block embeddings (discarded after training). Two compression variants exist: MASA-QKV (50% compression on Q,K,V only) and MASA-QKVO (66.7% on all four projections). For pretrained models, Matrix PCA initializes dictionaries, followed by layer grouping and local refinement.

## Key Results
- Achieves 66.7% parameter reduction in attention modules (S=4 for 12-layer model)
- 34.43% average accuracy across 6 reasoning tasks on 110M-parameter model
- 112.23 WikiText perplexity with 66.7% compression
- Outperforms GQA, low-rank baselines, and sequential/Repeat-all-over sharing methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dictionary decomposition exploits inter-block redundancy without rigid weight tying
- Mechanism: Each layer's projection matrix W_l = Σ c_ls D_s is reconstructed from shared atoms D_s via learned coefficients c_ls, allowing layers to share structure while maintaining functional specialization
- Core assumption: Transformer attention projections across layers lie in a shared low-dimensional subspace
- Evidence anchors: 66.7% parameter reduction with maintained performance; formal decomposition W ≈ DC with reconstruction formula
- Break condition: If layers require near-orthogonal subspaces, reconstruction error accumulates and degrades reasoning tasks

### Mechanism 2
- Claim: Separate dictionaries per projection type preserves functional specialization
- Mechanism: Q, K, V, O projections maintain independent dictionary pools rather than sharing atoms across projection types
- Core assumption: Query, key, value, and output transformations serve distinct computational roles with different subspace structures
- Evidence anchors: QKVO-Separate achieves 34.43% accuracy vs 32.86% for QKV-Common; independent dictionaries preserve specialized transformations
- Break condition: Forcing all projections to share one dictionary over-constrains representational capacity, particularly harming language modeling perplexity

### Mechanism 3
- Claim: Embedding-based coefficient parameterization stabilizes training dynamics
- Mechanism: Per-block embedding vectors pass through a 3-layer MLP to predict coefficients c_l, decoupling optimization from direct gradient updates and acting as implicit regularization
- Core assumption: Overparameterized coefficient generation smooths training without harming final expressivity
- Evidence anchors: MLP discards after training, retaining only final coefficient matrix C; overparameterization reduces gradient fluctuations
- Break condition: If MLP fails to converge to stable coefficients during training, inference performance may degrade

## Foundational Learning

- **Dictionary Learning / Sparse Coding**: Core mathematical framework—representing signals (weight matrices) as linear combinations of learned basis elements (atoms)
  - Quick check question: Can you explain why dictionary learning differs from PCA (hint: learned vs. fixed basis, sparsity constraints)?

- **Transformer Attention Projections (Q, K, V, O)**: Understanding what each projection computes is essential for interpreting why separate dictionaries outperform shared ones
  - Quick check question: Which projection transforms the attention output back to hidden dimension, and why might it be less compressible?

- **Low-Rank Matrix Approximation (SVD/Eigendecomposition)**: Matrix PCA for pretrained models requires computing eigenvectors of WW^T; understanding rank-error tradeoffs is critical
  - Quick check question: Given Eckart-Young-Mirsky theorem, what does the Frobenius norm of reconstruction error equal?

## Architecture Onboarding

- **Component map**:
  - Dictionary pools: S matrix atoms per projection type (D_Q, D_K, D_V, D_O), each D_s ∈ ℝ^(d×h)
  - Coefficient generator: Block embedding → 3-layer MLP → coefficient vector c_l ∈ ℝ^S (discarded after training)
  - Weight reconstruction: Ŵ_l = Σ c_ls D_s computed per layer at forward pass
  - Compression formula: r = 1 - S(L·d·h)/(L·d·h) ≈ 1 - S/L

- **Critical path**:
  1. Initialize dictionaries (random or from pretrained weights via Matrix PCA)
  2. For each forward pass: reconstruct Ŵ_l^Q, Ŵ_l^K, Ŵ_l^V, Ŵ_l^O from dictionaries + coefficients
  3. Standard attention computation proceeds with reconstructed weights
  4. Backpropagation updates both dictionaries and coefficient generator

- **Design tradeoffs**:
  - S (dictionary size) vs. compression: S=4 gives 66.7% compression; S=8 gives 33.3% but higher accuracy
  - QKVO (compress all) vs. QKV (preserve O): QKV generally outperforms at same S; O projection more sensitive
  - Training-from-scratch vs. pretrained adaptation: Pretrained requires Matrix PCA + grouping + local refinement

- **Failure signatures**:
  - Perplexity spikes on WikiText/LAMBADA: Check if O projection is over-compressed
  - Accuracy drops on reasoning benchmarks: May indicate S too small for model scale
  - Training instability: Verify coefficient MLP learning rate is decoupled from main optimizer
  - Pretrained model degradation: Check grouping strategy—first/last layers often need isolation

- **First 3 experiments**:
  1. **Baseline compression sweep**: Train Transformer-S with MASA-QKVO at S=2,4,6,8 on RefinedWeb 2.2B tokens; plot WikiText perplexity vs. S
  2. **Projection ablation**: Compare MASA-QKV vs. MASA-QKVO at S=4 on same model; expect QKV to show lower perplexity
  3. **Pretrained adaptation test**: Apply Matrix PCA + grouping (3 groups) to small pretrained model (110M params) with 20% compression; compare zero-shot accuracy before/after

## Open Questions the Paper Calls Out

- **Hierarchical clustering for layer grouping**: Can spectral or hierarchical clustering methods improve the layer grouping strategy for training-free adaptation compared to the current greedy approach?
- **Dictionary sparsity and orthogonality**: Does enforcing explicit sparsity constraints or orthogonality on learned dictionary atoms yield further parameter reduction without performance degradation?
- **FFN layer compression**: Can the MASA decomposition strategy be effectively applied to Feed-Forward Network layers to achieve holistic Transformer compression?

## Limitations
- Unknown MLP architecture details (hidden dimensions, activation functions) for coefficient prediction
- Coefficient MLP and embeddings are discarded post-training without ablation studies on retention
- Pretraining adaptation relies on heuristic grouping strategies without systematic evaluation of optimality

## Confidence

- **High Confidence**: Dictionary learning mechanism (W = DC decomposition) and parameter reduction math
- **Medium Confidence**: Claims about QKV vs QKVO performance differences and pretrained model adaptation results
- **Low Confidence**: Claims about training stability benefits from coefficient MLP parameterization

## Next Checks
1. **MLP Architecture Sensitivity**: Systematically vary MLP hidden dimensions and activation functions in MASA-QKVO; measure training stability and final accuracy
2. **Coefficient MLP Ablation**: Train MASA with direct coefficient learning vs. MLP-based prediction; compare training curves and final performance
3. **Grouping Strategy Analysis**: For pretrained adaptation, systematically evaluate different grouping approaches (k-means, layer-wise similarity, random) on a small pretrained model; measure impact on zero-shot transfer performance