---
ver: rpa2
title: 'Mano: Restriking Manifold Optimization for LLM Training'
arxiv_id: '2601.23000'
source_url: https://arxiv.org/abs/2601.23000
tags:
- manifold
- mano
- optimization
- muon
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Mano, a novel optimizer for training large
  language models (LLMs) that combines manifold optimization with momentum-based updates.
  Unlike AdamW, which uses diagonal curvature estimates, and Muon, which applies global
  spectral normalization but loses curvature information, Mano projects momentum onto
  the tangent space of model parameters and constrains updates on a rotational Oblique
  manifold.
---

# Mano: Restriking Manifold Optimization for LLM Training

## Quick Facts
- **arXiv ID:** 2601.23000
- **Source URL:** https://arxiv.org/abs/2601.23000
- **Reference count:** 40
- **One-line result:** Mano achieves up to 1.75× faster convergence and lower perplexity than AdamW/Muon on LLMs up to 1.7B parameters

## Executive Summary
This paper introduces Mano, a novel optimizer that combines manifold optimization with momentum-based updates for training large language models. Unlike AdamW's diagonal curvature estimates or Muon's global spectral normalization, Mano projects momentum onto the tangent space of parameters and constrains updates on a rotational Oblique manifold. Extensive experiments on LLaMA and Qwen3 models demonstrate that Mano consistently outperforms AdamW and Muon in test perplexity while consuming less memory and having significantly lower computational complexity.

## Method Summary
Mano implements a manifold-constrained optimization framework where parameter updates are projected onto tangent spaces of a rotational Oblique manifold. The algorithm alternates between column-wise and row-wise normalization, applies tangent space projection to preserve curvature information, and constrains only the update direction (not parameters themselves) to maintain model expressivity. Key hyperparameters include momentum μ=0.95, weight decay λ=0.1, and a 0.2√n_k rescaling factor for learning rate. The method achieves 1× memory usage (half of AdamW) and ~11mn FLOPs per layer, making it computationally efficient compared to Muon's quadratic complexity.

## Key Results
- Mano achieves up to 1.75× faster convergence in wall-clock time compared to Muon on LLaMA-1.3B
- Consistent perplexity improvements across tested models: 8.994 (Mano) vs 9.254 (Muon) on LLaMA-1.3B
- Memory efficient: 1× weight buffer (half of AdamW, same as Muon/SGD-M)
- Computational complexity: ~11mn FLOPs per layer (negligible vs forward/backward pass)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Projecting momentum onto the tangent space of parameters preserves curvature information while regularizing update directions.
- **Mechanism:** The tangent projection removes the component of momentum aligned with the parameter's normal direction, retaining gradient-derived curvature in the update.
- **Core assumption:** The gradient encodes useful curvature information that should not be discarded during normalization.
- **Evidence anchors:**
  - [abstract] "Muon applies global spectral normalization at the expense of losing curvature information"
  - [section 5.3] "Mano achieves efficient spectral regularization through manifold normalization that increases the relative magnitude of rare directions with a monotone transformation of singular values in the momentum"
- **Break condition:** If gradients are noise-dominated, tangent projection may amplify noise rather than signal.

### Mechanism 2
- **Claim:** Alternating column-wise and row-wise normalization (rotational Oblique manifold) captures complementary structural dependencies in parameter matrices.
- **Mechanism:** On odd steps, normalize columns; on even steps, normalize rows. This creates oscillating orientation that approximates doubly stochastic constraints without expensive Sinkhorn-Knopp convergence.
- **Core assumption:** Neither column nor row structure dominates exclusively in LLM parameter matrices.
- **Evidence anchors:**
  - [section 4.2] "under fixed column-wise normalization, Mano achieves comparable test perplexity on LLaMA-350M but performs significantly worse on LLaMA-1B"
  - [table 4] Static manifold degrades LLaMA-1.3B performance (9.254 vs 8.994 perplexity)
- **Break condition:** If a model's parameter structure strongly favors one dimension (columns or rows), rotation may introduce instability.

### Mechanism 3
- **Claim:** Constraining only the update direction (not parameters themselves) to the manifold preserves model expressivity while gaining geometric regularization.
- **Mechanism:** Traditional Riemannian SGD uses retractions to map parameters onto manifold surfaces, constraining the solution space. Mano keeps parameters in Euclidean space, applying manifold constraints only to momentum and updates.
- **Core assumption:** The learning trajectory benefits from geometric structure even when the solution doesn't lie on a manifold.
- **Evidence anchors:**
  - [section 4.1] "our update rule can be viewed as imposing a soft manifold constraint: it projects each update step onto the manifold surface defined by the parameters θ, while keeping the objective and solution unchanged in the Euclidean space"
  - [figure 7] RSGD-M fails to converge (loss stuck at 6-8) while Mano reaches loss ~2.5
- **Break condition:** If the true optimal solution does lie on a specific manifold, Mano's soft constraints may converge more slowly than hard-constrained methods.

## Foundational Learning

- **Concept:** Riemannian manifolds and tangent spaces
  - **Why needed here:** Mano projects momentum onto tangent spaces; understanding why this removes the "normal" component is essential for debugging.
  - **Quick check question:** If a vector v is projected onto the tangent space of a sphere at point p, what component of v is eliminated?

- **Concept:** Oblique manifold (unit-norm column matrices)
  - **Why needed here:** This is the geometric structure Mano uses; the normalization `A⊘∥A∥₂,d` maps back to this manifold.
  - **Quick check question:** For a 3×4 matrix on the Oblique manifold, how many independent constraints does this impose?

- **Concept:** Convergence of stochastic gradient with constrained updates
  - **Why needed here:** Theorem 1 provides theoretical grounding; understanding the `sin(φ) ≥ γ` assumption reveals when Mano might fail.
  - **Quick check question:** Why does the proof require the gradient to never be perfectly aligned with the weights?

## Architecture Onboarding

- **Component map:** Input θ_t → Momentum update → Rotating dimension select → Parameter normalization → Tangent projection → Update normalization → Weight update

- **Critical path:** The tangent projection (7 FLOPs per element) must correctly subtract the normal component. Incorrect broadcasting in `⊙` or `⟨·,·⟩_k` will cause divergence.

- **Design tradeoffs:**
  - Memory: 1× weight buffer (half of AdamW, same as Muon/SGD-M)
  - Compute: ~11mn FLOPs per layer (negligible vs forward/backward pass)
  - vs AdamW: No per-parameter adaptive rates; may underperform on sparse vocabulary layers
  - vs Muon: No MatMul; scales linearly with dimension rather than quadratically

- **Failure signatures:**
  - Loss plateauing early: Check learning rate scaling (should use 0.2√n_k factor)
  - Loss spikes: Momentum coefficient may be too high; try μ=0.9
  - Embedding/lm_head layers underperforming: These should use AdamW (per paper recommendations)
  - Static manifold behavior on larger models: Ensure rotation is implemented (k toggles)

- **First 3 experiments:**
  1. **Baseline sanity check:** Train LLaMA-130M on C4 for 1000 steps comparing Mano vs AdamW vs Muon with identical LR schedules. Verify Mano achieves lower perplexity by step 1000.
  2. **Rotation ablation:** Run same experiment with static column-wise normalization only. Confirm perplexity degrades for larger models (per Table 4 pattern).
  3. **Wall-clock profiling:** Measure per-step time for Mano normalization vs Muon's Newton-Schulz iteration on a 1B model. Target: Mano should be <15% of Muon's overhead (per Table 3, ~0.17ms vs 4.68ms for MLP).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the convergence analysis of Mano be extended to include momentum dynamics and the full rotational Oblique manifold scheme?
- **Basis in paper:** [explicit] "Extending the theory to cover momentum dynamics and broader optimization regimes remains an important future direction."
- **Why unresolved:** Theorem 1 only proves convergence for a simplified setting without momentum and with a static Oblique manifold, which does not match the actual Mano optimizer used in practice.
- **What evidence would resolve it:** A theoretical proof showing convergence guarantees for the complete Mano algorithm with momentum coefficient μ and rotational manifold switching.

### Open Question 2
- **Question:** Does Mano's performance advantage scale to larger LLMs (e.g., 7B, 70B, 100B+ parameters)?
- **Basis in paper:** [inferred] The empirical scope is "constrained by available computational resources" with largest tested models being Qwen3-1.7B and LLaMA-1.3B; ablation shows static manifold underperforms on larger models.
- **Why unresolved:** The paper's experiments are limited to models under 2B parameters, leaving the scaling behavior to production-scale LLMs unvalidated.
- **What evidence would resolve it:** Pretraining experiments on LLaMA-7B/70B or comparable large-scale models, demonstrating whether Mano maintains its perplexity and wall-clock advantages.

### Open Question 3
- **Question:** Why does Mano exhibit slower initial convergence but faster late-stage convergence compared to Muon and AdamW?
- **Basis in paper:** [explicit] "We are expecting to expand these over-trained experiments to bigger LLMs and further understand this intriguing loss descent pattern of Mano in the later convergence stage."
- **Why unresolved:** The authors observe this pattern empirically but lack a theoretical or mechanistic explanation for the phase-dependent convergence behavior.
- **What evidence would resolve it:** Analysis of gradient/loss landscape geometry across training phases, or theoretical characterization of Mano's escape-from-local-minima properties over time.

## Limitations
- **Experimental scope:** Limited to small LLMs (up to 1.7B parameters) and short training schedules (10K steps), raising questions about scaling to larger models
- **Theoretical gap:** Convergence guarantees assume specific gradient alignment conditions that may not hold in practical LLM training
- **Hyperparameter sensitivity:** Performance depends on specific choices of momentum (μ=0.95), weight decay (λ=0.1), and rotation frequency without comprehensive sensitivity analysis

## Confidence

**High confidence:** Mano's core algorithmic structure (tangent projection, alternating normalization, manifold-constrained updates) is clearly specified and demonstrably different from baselines. The memory and computational complexity claims are verifiable.

**Medium confidence:** The perplexity improvements over AdamW and Muon on tested architectures are robust within the experimental conditions, but generalization to larger models and longer training schedules remains uncertain.

**Low confidence:** The theoretical convergence claims in Theorem 1 have unclear practical implications for LLM training, and the mechanism explanations connecting geometric properties to performance gains are plausible but not rigorously validated.

## Next Checks

1. **Scaling validation:** Reproduce the core comparison (Mano vs AdamW vs Muon) on a 7B parameter model trained for 100K steps on a full dataset to test whether the 1.75× wall-clock speedup persists at scale.

2. **Rotation mechanism ablation:** Systematically test different rotation frequencies (every step, every 2 steps, every 4 steps) and static-only variants across model sizes to validate whether alternating normalization is genuinely beneficial.

3. **Failure mode characterization:** Intentionally misconfigure key parameters (learning rate, momentum, rotation frequency) and document the exact failure modes to reveal the optimizer's robustness envelope.