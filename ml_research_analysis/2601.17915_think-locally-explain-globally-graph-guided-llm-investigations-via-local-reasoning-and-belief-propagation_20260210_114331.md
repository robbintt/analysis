---
ver: rpa2
title: 'Think Locally, Explain Globally: Graph-Guided LLM Investigations via Local
  Reasoning and Belief Propagation'
arxiv_id: '2601.17915'
source_url: https://arxiv.org/abs/2601.17915
tags:
- evidence
- reasoning
- entity
- agent
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "EoG improves diagnostic accuracy and consistency over ReAct agents\
  \ by separating deterministic graph traversal and belief bookkeeping from local\
  \ abductive reasoning, enabling systematic exploration and belief revision through\
  \ semantic message passing. On ITBench SRE tasks, EoG achieves a 7\xD7 improvement\
  \ in Majority@k F1 score over ReAct baselines, with significantly narrower reliability\
  \ gaps between pass@k and majority@k."
---

# Think Locally, Explain Globally: Graph-Guided LLM Investigations via Local Reasoning and Belief Propagation

## Quick Facts
- **arXiv ID:** 2601.17915
- **Source URL:** https://arxiv.org/abs/2601.17915
- **Reference count:** 40
- **Primary result:** EoG achieves 7× improvement in Majority@k F1 score over ReAct baselines on ITBench SRE tasks, with significantly narrower reliability gaps between pass@k and majority@k.

## Executive Summary
This paper introduces Explain Globally (EoG), a graph-guided LLM investigation architecture that separates deterministic graph traversal and belief bookkeeping from local abductive reasoning. EoG addresses the challenge of non-monotonic reasoning in complex, graph-structured investigations where early conclusions must be revised as new evidence emerges. By using semantic belief propagation to broadcast belief changes across the dependency graph, EoG enables systematic exploration while maintaining consistent global explanations. The approach demonstrates a 7× improvement in diagnostic accuracy over ReAct agents on the ITBench SRE benchmark.

## Method Summary
EoG introduces a novel algorithm that combines deterministic graph traversal with local abductive reasoning and semantic belief propagation. The system uses a Deterministic Controller to manage exploration through an ActiveSet queue and Global Ledger, while LLM-based abductive policies classify nodes as origin, symptom, healthy, or defer. Context Contract MCP tools provide bounded local evidence and neighbor information, and belief changes trigger semantic message passing to update neighboring nodes' beliefs. This separation of concerns allows systematic exploration of complex dependency graphs while maintaining global consistency through belief revision.

## Key Results
- Achieves 7× improvement in Majority@k F1 score over ReAct baselines on ITBench SRE tasks
- Significantly narrows the reliability gap between pass@k and majority@k evaluation metrics
- Demonstrates effective handling of non-monotonic reasoning where early conclusions must be revised
- Successfully identifies minimal frontier of origin nodes explaining all observations in 35 IT scenarios

## Why This Works (Mechanism)
EoG works by separating deterministic graph traversal (ActiveSet queue management, Global Ledger tracking) from local abductive reasoning (LLM classification of node states). This separation prevents premature conclusions while enabling systematic exploration. Semantic belief propagation ensures that when a node's belief changes, neighboring nodes are notified and can revise their own beliefs, capturing the non-monotonic nature of real investigations. The damping heuristic and visit limits prevent belief oscillation while still allowing necessary revisions.

## Foundational Learning
- **ActiveSet Queue Management**: Required to systematically explore graph nodes while avoiding redundant queries; quick check: verify queue ordering follows breadth-first exploration with priority on high-certainty nodes
- **Semantic Belief Propagation**: Needed to revise beliefs across the graph when new evidence emerges; quick check: confirm belief change triggers neighbor notification and subsequent belief revision
- **Damping Heuristic**: Prevents infinite belief oscillation by forcing defer state after threshold flips; quick check: ensure node transitions to absorbing Defer state after k_thresh belief changes
- **Context Contract Design**: Provides bounded evidence to prevent token overflow while enabling local reasoning; quick check: validate returned evidence size stays within token limits
- **Global Ledger State Tracking**: Maintains consistent global view of node classifications and evidence; quick check: confirm ledger accurately reflects all belief changes and evidence sources
- **Token-Aware Pagination**: Enables processing of large evidence sets without exceeding context limits; quick check: verify evidence is chunked and processed in tournament-style relevance reduction

## Architecture Onboarding

### Component Map
Deterministic Controller -> Context Contract MCP -> Abductive Policy LLM -> Semantic Belief Propagation -> Global Ledger

### Critical Path
1. Controller initializes ActiveSet with candidate nodes
2. CxC returns local evidence and neighbor context
3. LLM classifies node state (Origin/Symptom/Healthy/Defer)
4. Belief changes trigger SBP message broadcast
5. Global Ledger updates maintain consistent state
6. Controller updates ActiveSet based on new beliefs

### Design Tradeoffs
- **Deterministic vs LLM Control**: Controller provides systematic exploration while LLM handles complex reasoning; tradeoff is reduced LLM autonomy for increased consistency
- **Bounded vs Complete Context**: CxC limits evidence per node to prevent token overflow; tradeoff is potential loss of relevant information for local reasoning
- **Belief Revision vs Stability**: SBP enables non-monotonic reasoning but risks oscillation; damping heuristic balances these needs

### Failure Signatures
- Belief oscillation: nodes repeatedly flip between states; diagnostic: track flip count per node
- Exploration incompleteness: ground truth entity never visited; diagnostic: monitor GT discovery funnel
- Context overflow: evidence exceeds token limits; diagnostic: monitor token counts during processing

### First Experiments
1. Implement Context Contract MCP tool with specified interface and test integration with Deterministic Controller using synthetic graph data
2. Reconstruct abductive policy prompt templates and validate functionality on a small test scenario
3. Run single ITBench scenario through complete EoG pipeline to verify ActiveSet management, SBP message passing, and belief classification interaction

## Open Questions the Paper Calls Out
None specified in the paper.

## Limitations
- Prompt templates and MCP tool implementations are described but not fully specified, requiring reconstruction for exact reproduction
- Performance metrics depend on access to specific proprietary LLM models that may not be publicly available
- The 35 ITBench scenarios represent a limited domain, though ablation design strengthens causal claims
- Claims about scalability to billion-parameter graphs remain theoretical without empirical validation

## Confidence

### Major Uncertainties and Limitations
- Prompt templates for abductive policy phases require reconstruction
- MCP tool implementations are specified at interface level but internal logic is missing
- Absolute performance metrics depend on specific LLM model access
- Limited domain representation in evaluation dataset

### Confidence Labels
- **High**: EoG architecture improves over ReAct by separating deterministic traversal from reasoning; systematic exploration via SBP is effective; damping heuristic prevents belief oscillation
- **Medium**: Absolute Majority@k F1 scores (7× improvement); RC Reasoning accuracy gains; Pass@k to Majority@k reliability gap reduction
- **Low**: Claims about scalability to billion-parameter graphs; performance on non-IT domains; relative performance on newer models not tested

## Next Checks
1. Implement the Context Contract MCP tool with the specified interface and test its integration with the Deterministic Controller using synthetic graph data
2. Reconstruct the abductive policy prompt templates based on the described phases (bootstrap, explore, finalize) and validate their functionality on a small test scenario
3. Run a single ITBench scenario through the complete EoG pipeline to verify the interaction between ActiveSet management, SBP message passing, and belief classification before scaling to full evaluation