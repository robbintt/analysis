---
ver: rpa2
title: 'NexusSum: Hierarchical LLM Agents for Long-Form Narrative Summarization'
arxiv_id: '2505.24575'
source_url: https://arxiv.org/abs/2505.24575
tags:
- nexus
- marianne
- elinor
- summarization
- narrative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NEXUS SUM, a hierarchical multi-agent LLM
  framework that transforms long-form narrative summarization by converting dialogue
  to descriptive prose and applying iterative compression. The method improves coherence,
  controls summary length, and preserves key narrative details across diverse storytelling
  domains.
---

# NexusSum: Hierarchical LLM Agents for Long-Form Narrative Summarization

## Quick Facts
- **arXiv ID:** 2505.24575
- **Source URL:** https://arxiv.org/abs/2505.24575
- **Reference count:** 40
- **Key outcome:** Hierarchical LLM framework that transforms dialogue to prose and applies iterative compression, achieving up to 30.0% BERTScore (F1) improvement over state-of-the-art methods

## Executive Summary
NEXUS SUM addresses long-form narrative summarization by introducing a three-stage hierarchical LLM agent framework. The method transforms dialogue-heavy narratives into descriptive prose, summarizes in scene-based chunks, and applies iterative compression to control output length while preserving key details. Evaluated on four diverse benchmarks (BookSum, MovieSum, MENSA, SummScreenFD), NEXUS SUM demonstrates significant improvements in coherence, length adherence, and content retention compared to existing approaches.

## Method Summary
The method employs three specialized LLM agents in sequence: Preprocessor (P) converts dialogue to third-person narrative prose and segments text into scene-based chunks; Narrative Summarizer (S) generates summaries from these chunks; Compressor (C) applies sentence-based iterative compression with dynamic lower-bound targeting until reaching specified word count limits. The entire pipeline uses Mistral-Large-Instruct-2407 (123B) with fixed hyperparameters (temperature=0.3, top-p=1.0) and operates without fine-tuning. Chunk sizes are empirically tuned per benchmark (δ=300 words for most, Maximum for SummScreenFD) with scene-based chunking fixed at 8 scenes per chunk.

## Key Results
- Achieves up to 30.0% improvement in BERTScore (F1) over state-of-the-art methods
- Length Adherence Rate (LAR) reaches 0.883-0.990 across targets vs. 0.245-0.605 for Zero-Shot
- Improves coherence through dialogue-to-description transformation (BERTScore +2.45 in ablation)
- Demonstrates robust performance across four diverse benchmarks spanning books, movies, and TV scripts

## Why This Works (Mechanism)

### Mechanism 1
Converting dialogue to descriptive prose improves downstream summarization coherence by reducing fragmentation in narrative inputs. The Preprocessor agent segments input into scene-based chunks and transforms each using LLM prompting that reframes dialogues into third-person narrative prose while preserving speaker intent and emotional tone. Core assumption: LLMs process unified prose more effectively than mixed dialogue/prose formats for abstractive summarization tasks. Evidence: Table 3 ablation shows P alone improves BERTScore from 54.81 to 57.26 (+2.45) over Zero-Shot baseline.

### Mechanism 2
Hierarchical chunk processing with concatenation preserves long-range dependencies better than single-pass summarization. Narrative Summarizer processes preprocessed text in scene-based chunks (8 scenes per chunk), summarizes each independently, then concatenates outputs. This maintains local context while scaling to arbitrary input lengths. Core assumption: Scene boundaries represent natural semantic units where context reset is acceptable without losing cross-scene dependencies. Evidence: Adding S to P improves BERTScore from 57.26 to 62.12 (+4.86) in Table 3.

### Mechanism 3
Iterative compression with dynamic lower-bound targeting enables precise length control while retaining key information. Compressor applies sentence-based chunking followed by hierarchical compression iterations. Each iteration refines the previous summary until target word count θ is reached or maximum iterations (10) exhausted. Chunk size δ controls compression granularity. Core assumption: Compression quality degrades gracefully with smaller chunks; larger chunks enable higher compression ratios but may lose detail. Evidence: Table 4 shows NEXUS SUM achieves LAR of 0.883-0.990 across targets vs. 0.245-0.605 for Zero-Shot.

## Foundational Learning

- **Concept: Chunk-and-concat pipeline architecture**
  - Why needed here: Understanding how to partition long inputs, process independently, and merge outputs is essential for debugging information loss at chunk boundaries.
  - Quick check question: Given a 50K-token input with 8K-token context window, what chunk size and overlap strategy would you use?

- **Concept: Prompt engineering for multi-agent orchestration**
  - Why needed here: Each agent (P, S, C) requires domain-specific prompting; performance depends on instruction design (see Appendix A for full prompts).
  - Quick check question: What prompt modifications would you make if summarizing technical documentation instead of narratives?

- **Concept: Compression ratio vs. quality tradeoff**
  - Why needed here: Chunk size δ directly controls compression behavior; understanding this relationship is critical for configuration (see Tables 13-15).
  - Quick check question: If BookSum achieves optimal BERTScore at δ=300 but MovieSum degrades below δ=1000, what explains the difference?

## Architecture Onboarding

- **Component map:** Preprocessor (P) -> Narrative Summarizer (S) -> Compressor (C)
- **Critical path:** Input → P (scene chunks, 8 scenes each) → S (scene chunks, 8 scenes each) → C (sentence chunks, size δ) → Output. Each stage must complete before the next begins.
- **Design tradeoffs:**
  - Smaller δ → better detail retention, lower compression ratio, more API calls
  - Higher θ → longer output, higher BERTScore, may exceed user requirements
  - Chunk size at P/S stage (fixed at 8 scenes) balances context retention vs. context window limits
- **Failure signatures:**
  - Output exceeds target length: θ set too high or δ too small; check compression iteration count
  - Missing plot elements: Check chunk boundaries at scene transitions; may need chunk overlap
  - Poor coherence on dialogue-heavy inputs: Verify P transformation is applied; check prompt adherence
  - Readability issues (per human eval in Section 5.5): Consider adding refinement agent for fluency
- **First 3 experiments:**
  1. Reproduce MENSA ablation (Table 3) to validate each component's contribution on a held-out sample.
  2. Sweep δ values (300, 500, 1000, 2000) on a single long document to observe compression ratio vs. BERTScore tradeoff curves.
  3. Compare Zero-Shot with explicit length instructions vs. NEXUS SUM on length adherence using LAR metric to validate iterative compression effectiveness.

## Open Questions the Paper Calls Out

### Open Question 1
Can a unified framework simultaneously optimize readability and factual content retention in narrative summarization, or is a tradeoff inherent? Current approach optimizes content retention through iterative compression but produces "denser summaries that may feel less natural" (p. 9). NEXUS SUMR improves readability (+1.5 points) but slightly reduces factuality. Resolution requires comparative study measuring both readability and factuality across multiple methods balancing these objectives.

### Open Question 2
What is the optimal strategy for automatically adapting chunk size (δ) parameters across narrative domains with varying length distributions? The paper manually tunes δ per benchmark but provides no automatic mechanism to adapt to new narrative types without empirical search. Resolution requires development and validation of a meta-learning or heuristic approach that predicts optimal δ from input narrative characteristics.

### Open Question 3
Does the dialogue-to-description transformation introduce systematic information loss or bias for specific narrative styles? The preprocessing stage converts all dialogue to third-person prose, but evaluation relies solely on BERTScore/ROUGE which capture semantic content but not stylistic fidelity or nuanced dialogue dynamics. Resolution requires fine-grained analysis comparing preserved vs. transformed dialogue elements across genres.

## Limitations
- Component independence validation is incomplete; interactions between P, S, and C agents are not systematically studied
- Compression configuration sensitivity shows empirical tuning but lacks theoretical justification for optimal δ values
- Selective benchmark reporting without statistical significance testing could influence claimed superiority

## Confidence
- **High Confidence:** Hierarchical multi-agent architecture design and three-stage processing pipeline (P→S→C) are clearly specified and reproducible
- **Medium Confidence:** Dialogue-to-prose transformation mechanism shows consistent performance gains in ablation studies but lacks direct corpus validation
- **Low Confidence:** Compression ratio-quality tradeoff relationships are empirically observed but not theoretically grounded

## Next Checks
1. Cross-domain chunk size sensitivity analysis: Systematically vary δ across BookSum, MovieSum, MENSA, and SummScreenFD to map compression ratio vs. BERTScore tradeoff curves
2. Dialogue-specific ablation study: Compare NEXUS SUM with and without dialogue-to-prose transformation on controlled dataset with quantified dialogue density
3. Factual consistency validation: Implement post-hoc factual consistency check using fine-tuned factuality classifier to measure iterative compression effects on factual alignment