---
ver: rpa2
title: Investigating Language and Retrieval Bias in Multilingual Previously Fact-Checked
  Claim Detection
arxiv_id: '2509.25138'
source_url: https://arxiv.org/abs/2509.25138
tags:
- task
- language
- bias
- retrieval
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates language and retrieval bias in multilingual
  Large Language Models (LLMs) applied to previously fact-checked claim detection
  (PFCD). Language bias manifests as systematic performance disparities across languages,
  with models favoring high-resource languages like English over low-resource counterparts.
---

# Investigating Language and Retrieval Bias in Multilingual Previously Fact-Checked Claim Detection

## Quick Facts
- arXiv ID: 2509.25138
- Source URL: https://arxiv.org/abs/2509.25138
- Reference count: 38
- Key outcome: This paper investigates language and retrieval bias in multilingual Large Language Models (LLMs) applied to previously fact-checked claim detection (PFCD).

## Executive Summary
This study examines how multilingual Large Language Models (LLMs) exhibit systematic biases when detecting previously fact-checked claims across different languages. Using the AMC-16K dataset with 16,000 claim-post pairs spanning 20 languages, the research identifies significant performance disparities between high-resource languages (particularly English) and low-resource languages. The study also reveals retrieval bias, where certain fact-checked claims are disproportionately selected regardless of relevance. Through systematic experimentation with six multilingual LLMs and various prompting strategies, the authors demonstrate that bias can be partially mitigated through few-shot demonstrations and LLM-based reranking approaches.

## Method Summary
The research employs the AMC-16K dataset containing 16,000 claim-post pairs across 20 languages to evaluate six multilingual LLMs (Qwen3, Llama3.1, Gemma3) under different prompting strategies including zero-shot, few-shot, and chain-of-thought approaches. Translated instructions are used to ensure consistent task framing across languages. The methodology involves systematic evaluation of language bias through cross-linguistic performance comparison and retrieval bias analysis by examining claim selection patterns. LLM-based reranking is proposed as a mitigation strategy, improving semantic alignment between posts and claims.

## Key Results
- Most models exhibit significant language bias, favoring high-resource languages like English over low-resource languages, particularly under zero-shot prompting
- Few-shot demonstrations and task descriptions can effectively mitigate language bias across evaluated models
- Retrieval bias is observed where certain claims with generic phrasing or high topical prevalence are disproportionately retrieved regardless of actual relevance

## Why This Works (Mechanism)
Language bias emerges from training data imbalances where high-resource languages have substantially more training examples, leading models to develop stronger representations for these languages. Retrieval bias occurs due to semantic similarity patterns in the embedding space, where certain claims share more generic features that match diverse posts. Few-shot prompting provides explicit examples that help models understand task requirements across languages, while LLM-based reranking improves semantic matching by considering contextual relationships beyond simple similarity scores.

## Foundational Learning
1. **Cross-lingual transfer learning**: Why needed - Enables models to apply knowledge from high-resource to low-resource languages; Quick check - Evaluate performance drop when training data for a language is removed
2. **Prompt engineering**: Why needed - Controls model behavior without fine-tuning; Quick check - Compare performance across zero-shot, few-shot, and chain-of-thought approaches
3. **Semantic embedding alignment**: Why needed - Ensures comparable representations across languages; Quick check - Measure embedding similarity consistency across language pairs
4. **Retrieval bias detection**: Why needed - Identifies systematic selection patterns in claim retrieval; Quick check - Analyze claim frequency distribution in retrieved results
5. **Multilingual fact-checking pipelines**: Why needed - Real-world application context for bias mitigation; Quick check - Test end-to-end system performance across language communities
6. **Evaluation metrics for bias**: Why needed - Quantifies performance disparities and retrieval patterns; Quick check - Calculate language-specific F1 scores and claim selection entropy

## Architecture Onboarding
**Component map**: AMC-16K dataset -> Multilingual LLMs (Qwen3, Llama3.1, Gemma3) -> Prompting strategies -> Evaluation metrics -> Bias analysis

**Critical path**: Dataset preparation → Model inference with prompting → Performance evaluation → Bias quantification → Mitigation strategy testing

**Design tradeoffs**: Zero-shot prompting offers scalability but suffers from higher bias, while few-shot approaches reduce bias but require additional examples and annotation effort. Chain-of-thought reasoning improves accuracy but increases computational cost.

**Failure signatures**: Language bias manifests as systematically lower F1 scores for low-resource languages, particularly under zero-shot prompting. Retrieval bias appears as skewed claim frequency distributions where certain claims dominate retrieval results regardless of query relevance.

**First experiments**: 
1. Baseline evaluation: Measure cross-lingual performance disparities across all six models using zero-shot prompting
2. Few-shot impact test: Compare language bias mitigation effectiveness across different numbers of demonstration examples
3. Retrieval bias analysis: Examine claim selection frequency distributions to identify disproportionately retrieved claims

## Open Questions the Paper Calls Out
None

## Limitations
- The AMC-16K dataset may not fully represent linguistic diversity across all language families, potentially limiting generalizability
- Results focus on specific LLM architectures and prompting strategies, which may not extend to other model families
- Long-term stability of bias mitigation strategies remains uncertain as prompt engineering effects can be sensitive to model updates

## Confidence
- **High confidence** in observed language bias patterns due to consistent replication across multiple models and evaluation metrics
- **Medium confidence** in generalizability of retrieval bias patterns, as bias may be influenced by specific dataset characteristics
- **Low confidence** in long-term stability of bias mitigation strategies due to sensitivity to model updates

## Next Checks
1. Cross-dataset validation: Replicate language and retrieval bias experiments using independent multilingual fact-checked claim datasets to verify pattern persistence
2. Model architecture ablation: Conduct controlled experiments comparing different multilingual LLM architectures to determine consistency of bias patterns
3. Real-world deployment monitoring: Implement longitudinal study tracking language and retrieval bias in actual multilingual fact-checking systems over time