---
ver: rpa2
title: 'SafeCoop: Unravelling Full Stack Safety in Agentic Collaborative Driving'
arxiv_id: '2510.18123'
source_url: https://arxiv.org/abs/2510.18123
tags:
- driving
- arxiv
- preprint
- agent
- collaborative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper systematically studies safety and security issues in\
  \ natural-language-based collaborative driving systems. It introduces a taxonomy\
  \ of four attack surfaces\u2014connection disruption, relay/replay interference,\
  \ content spoofing, and multi-connection forgery\u2014and proposes SafeCoop, an\
  \ agentic defense pipeline that integrates semantic firewall, language-perception\
  \ consistency checks, and multi-source consensus with agentic transformation functions\
  \ for spatial alignment."
---

# SafeCoop: Unravelling Full Stack Safety in Agentic Collaborative Driving

## Quick Facts
- arXiv ID: 2510.18123
- Source URL: https://arxiv.org/abs/2510.18123
- Authors: Xiangbo Gao; Tzu-Hsiang Lin; Ruojing Song; Yuheng Wu; Kuan-Ru Huang; Zicheng Jin; Fangzhou Lin; Shinan Liu; Zhengzhong Tu
- Reference count: 33
- Primary result: 69.15% driving score improvement under malicious attacks in CARLA simulation

## Executive Summary
This paper addresses safety and security challenges in natural-language-based collaborative driving systems by introducing SafeCoop, a multi-agent defense framework. The system targets four attack surfaces—connection disruption, relay/replay interference, content spoofing, and multi-connection forgery—using a layered defense architecture. SafeCoop integrates semantic firewall, language-perception consistency checks, and multi-source consensus with agentic transformation functions for spatial alignment. Evaluated in CARLA simulation across 32 critical scenarios, the framework achieves significant improvements in driving performance and malicious detection rates under adversarial conditions.

## Method Summary
SafeCoop implements a three-agent defense pipeline (Firewall, Language-Perception Consistency, Multi-Source Consensus) that processes multi-source messages from connected vehicles. Each agent applies distinct verification checks: semantic inspection of message content, grounding of language descriptions against ego perception via Agentic Transformation Function (ATF), and cross-vehicle consensus with temporal consistency analysis. Messages failing trust thresholds are excluded from downstream action modules. The framework uses MLLMs for reasoning across all defense components, with trust scores aggregated via weighted averaging and thresholded for binary decisions.

## Key Results
- 69.15% driving score improvement under malicious attacks compared to undefended systems
- Up to 67.32% F1 score for malicious detection in combined attack scenarios
- Ablation study confirms additive contributions: Firewall (49.29% DS) + LPC (+2.49%) + MSC (+0.84%)

## Why This Works (Mechanism)

### Mechanism 1: Layered Defense with Orthogonal Checks
The three-agent architecture targets distinct attack signatures—semantic manipulation, perception inconsistency, and multi-source anomalies. Each layer provides independent verification so attacks bypassing one may be caught by another. Trust scores aggregate via weighted average and thresholding. Break condition: coordinated attacks crafting messages simultaneously plausible across all dimensions may evade detection.

### Mechanism 2: Language-Perception Grounding via Agentic Transformation
The LPC agent uses ATF to transform spatial descriptions from sender to ego frame and verifies consistency with ego's sensors. ATF parses spatial language into coordinates, applies SE(3) transforms, and recomposes descriptions for verification. Break condition: ATF errors in parsing ambiguous terms or transformation failures due to perception occlusion can produce false results.

### Mechanism 3: Multi-Source Consensus for Sybil and Temporal Attack Detection
MSC combines global consensus (outlier detection), pairwise verification against ego observations, and temporal consistency across frames. Scores average across checks; high deviation indicates malicious behavior. Break condition: MCF attacks creating majority false consensus overwhelm global consensus, while MLLM temporal reasoning limits may miss subtle replay delays.

## Foundational Learning

- **SE(3) Transformations and Spatial Reference Frames**: ATF relies on SE(3) to transform spatial descriptions across vehicle coordinate frames. Quick check: Given sender pose (x=10, y=5, yaw=30°) and receiver pose (x=20, y=10, yaw=0°), can you compute the relative transformation and apply it to a point at (2, 0) in sender frame?

- **Multi-Modal Large Language Model (MLLM) Reasoning and Hallucination**: SafeCoop uses MLLMs for semantic verification and perception consistency. Quick check: Why might an MLLM struggle to detect a replay attack using 5-second-old messages? (Hint: MLLM temporal reasoning gaps)

- **Trust Score Aggregation and Thresholding**: Defense agents output scores [1,5], aggregated via weighted average and thresholded at τ=2.5. Quick check: If Firewall=4.0, LPC=2.0, MSC=3.0 with equal weights, is the agent flagged as malicious? How would τ=3.0 change false positive vs. false negative rates?

## Architecture Onboarding

- **Component map**: Input JSON messages + ego sensing data → Message Extractor → Defense Agents (Firewall → LPC → MSC) → Trust scores → Aggregator → Binary decision → Filtered messages
- **Critical path**: Receive message set → Run agents in parallel → Aggregate scores → Threshold at τ=2.5 → Filter malicious agents → Pass filtered messages to action module
- **Design tradeoffs**: Model size vs. latency (GPT-4.1: 3.10s latency, 67-74% F1 vs. GPT-4.1-mini: 0.73s, 14.48% F1), safety vs. efficiency (conservative aggregation increases false positives), ATF accuracy vs. ambiguity (confidence thresholds reduce false positives but may miss legitimate vague descriptions)
- **Failure signatures**: RI attacks yield low detection (33.43% F1) due to MLLM temporal reasoning limits, MCF paradoxically improves detection by extending reasoning tokens, LPC bottleneck with 0.85-3.09s latency due to multi-image input
- **First 3 experiments**:
  1. Reproduce baseline CS attack impact: Confirm ~46% DS drop (55.94% → 30.31%) without defense
  2. Ablate MSC defense: Measure F1 drop from 67.32% to ~55-62% under CS+MCF attacks
  3. Profile ATF accuracy: Evaluate 50 spatial descriptions for parsing error and transformation fidelity

## Open Questions the Paper Calls Out
None

## Limitations
- Real-time feasibility gap: All evaluated MLLMs exceed practical latency requirements (20-500ms) for safety-critical systems
- ATF ambiguity handling: Relies on heuristics for implicit spatial language, with limited validation under linguistic variation
- MCF consensus vulnerability: MSC assumes honest agents outnumber malicious ones, but coordinated MCF attacks can create majority false consensus

## Confidence
- **High confidence**: Layered defense architecture and ablation results showing additive contributions; attack taxonomy grounded in CARLA scenarios
- **Medium confidence**: ATF mechanism effectiveness and spatial alignment accuracy (limited empirical validation)
- **Low confidence**: Real-time deployment viability and MSC robustness against sophisticated MCF attacks

## Next Checks
1. **ATF accuracy benchmark**: Test 100 spatial descriptions varying in complexity; measure parsing error rates and transformation accuracy against ground truth coordinates
2. **MCF consensus stress test**: Simulate scenarios with 10%, 30%, 50%, 70% forged agents; measure MSC false positive/negative rates and identify consensus breakdown threshold
3. **Latency-optimized deployment**: Profile SafeCoop on edge hardware with quantized MLLM variants; measure detection performance degradation at sub-100ms latency budgets