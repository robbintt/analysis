---
ver: rpa2
title: Empirical Comparison of Membership Inference Attacks in Deep Transfer Learning
arxiv_id: '2510.05753'
source_url: https://arxiv.org/abs/2510.05753
tags:
- learning
- lira
- efficacy
- attacks
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares the performance of diverse membership inference
  attacks (MIAs) against models fine-tuned with transfer learning, addressing the
  gap in prior assessments that relied on a limited subset of attacks. The authors
  systematically evaluate score-based MIAs, including shadow-model-based (ML-Leaks,
  LiRA, Trajectory-MIA, RMIA) and shadow-model-free (LOSS, Attack-P, QMIA, IHA) methods,
  using consistent experimental setups across CIFAR-10, CIFAR-100, and PatchCamelyon
  datasets.
---

# Empirical Comparison of Membership Inference Attacks in Deep Transfer Learning

## Quick Facts
- arXiv ID: 2510.05753
- Source URL: https://arxiv.org/abs/2510.05753
- Authors: Yuxuan Bai; Gauri Pradhan; Marlon Tobaben; Antti Honkela
- Reference count: 40
- Primary result: LiRA achieves the most robust and superior performance across most scenarios, while white-box IHA shows exceptional efficacy in high-shot regimes on PatchCamelyon

## Executive Summary
This study systematically evaluates eight membership inference attacks against deep transfer learning models, addressing the gap in prior assessments that relied on limited attack subsets. The authors compare shadow-model-based attacks (ML-Leaks, LiRA, Trajectory-MIA, RMIA) and shadow-model-free attacks (LOSS, Attack-P, QMIA, IHA) across CIFAR-10, CIFAR-100, and PatchCamelyon datasets. They find that attack efficacy generally decreases with increasing training data following a power-law relationship, but identify the white-box Inverse Hessian Attack (IHA) as an exception that performs exceptionally well in high-data regimes on PatchCamelyon. The study also reveals that different fine-tuning parameterization strategies show minimal differences in attack efficacy for the strongest attacks, suggesting parameterization choices can be guided by utility rather than privacy concerns.

## Method Summary
The study evaluates eight membership inference attacks against pre-trained models (ViT-B/16, BiT-M-R50x1) fine-tuned on three datasets using three parameterization schemes. Hyperparameter optimization is performed using Optuna with 20 trials, and shadow models are trained with consistent hyperparameters to ensure fair comparison. The primary metric is TPR@FPR=0.001, with 10 experimental repeats and M=64 shadow models for LiRA and RMIA. The evaluation covers various training data volumes (shots) and includes both black-box score-based attacks and white-box attacks leveraging Hessian information.

## Key Results
- Attack efficacy generally decreases with increasing training data following a power-law relationship for most score-based MIAs
- LiRA demonstrates the most robust and superior performance across most experimental scenarios
- White-box Inverse Hessian Attack (IHA) shows superior performance in high-shot regimes on PatchCamelyon, contrary to the general trend
- Different fine-tuning parameterization schemes (Head-only, FiLM, ALL) show minimal differences in MIA efficacy for the strongest attacks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Increasing training data volume reduces the efficacy of most score-based MIAs in transfer learning
- **Mechanism:** As training examples increase, models overfit less to individual samples, reducing divergence between loss distributions for members versus non-members
- **Core assumption:** Standard threat model where privacy leakage is driven by overfitting on limited data
- **Evidence anchors:** Power-law relationship confirmed in Section 5.1; supported by Tobaben et al. (2024) and related dataset properties research
- **Break condition:** Fails for non-standard threat models or when fine-tuning data has significant distribution shift from pre-training data

### Mechanism 2
- **Claim:** White-box attacks utilizing Hessian information (IHA) outperform black-box attacks in high-data regimes on out-of-distribution datasets
- **Mechanism:** IHA leverages local similarity of model parameters and approximates the Hessian matrix, detecting membership based on precise influence of single data points on model parameters
- **Core assumption:** Full white-box access to model parameters and knowledge of almost entire training dataset
- **Evidence anchors:** IHA shows non-monotonic patterns and increased efficacy at larger shots on PatchCamelyon (Section 5.1)
- **Break condition:** Degrades if local similarity assumption doesn't hold or attacker lacks required white-box access and dataset knowledge

### Mechanism 3
- **Claim:** Choice of fine-tuning parameterization strategy does not significantly alter efficacy of strongest MIAs
- **Mechanism:** Strong attacks like LiRA detect membership leakage regardless of whether backbone is frozen or adapted, as leakage signal is primarily function of final model state relative to data distribution
- **Core assumption:** Attack methodology is robust enough to capture residual privacy leakage in any high-utility fine-tuning approach
- **Evidence anchors:** LiRA maintains consistent performance across Head-only, FiLM, and ALL schemes (Section 5.2)
- **Break condition:** Weaker attacks may show variance across parameterizations, breaking invariance observation

## Foundational Learning

- **Concept: Transfer Learning & Fine-tuning**
  - **Why needed here:** Paper evaluates MIAs in context of fine-tuning pre-trained models on downstream tasks, contrasting with training from scratch
  - **Quick check question:** How does freezing the backbone (Head-only) versus updating all weights (ALL) theoretically change the attack surface?

- **Concept: Membership Inference Attacks (MIAs)**
  - **Why needed here:** Core subject; understanding black-box vs white-box attacks essential to interpret LiRA vs IHA results
  - **Quick check question:** What is the difference between shadow-model-based attack (like LiRA) and shadow-model-free attack (like LOSS)?

- **Concept: The Power Law in Data Scaling**
  - **Why needed here:** Paper relies on power-law relationship to explain how attack efficacy scales with dataset size
  - **Quick check question:** According to paper, does increasing dataset size linearly decrease attack success?

## Architecture Onboarding

- **Component map:** Pre-trained backbone -> Fine-tuning parameterization -> Target model -> Shadow models -> Attack engine
- **Critical path:** Perform HPO with same hyperparameters for target/shadow models → Train M shadow models with IN/OUT splits → Run attack (LiRA uses M+1 models)
- **Design tradeoffs:**
  - Shadow Model Count (M): Increasing M improves LiRA stability but increases compute cost; M=64 is cost-efficient inflection point
  - Parameterization: Head-only is computationally cheaper but offers no privacy advantage over ALL/FiLM against strong attacks
- **Failure signatures:**
  - Inconsistent HPO: Different hyperparameters for shadow vs target models invalidates attack efficacy
  - Data Augmentation: Unlike training from scratch, augmentation during fine-tuning does not significantly boost MIA efficacy
  - Weak Attacks: Relying solely on simple attacks underestimates privacy risks compared to LiRA
- **First 3 experiments:**
  1. Reproduce Power Law: Fine-tune Head-only ViT on CIFAR-10 with varying shots and plot LiRA TPR@FPR=0.001
  2. Shadow Model Sensitivity: Run LiRA with M=4, 16, 64 to observe stabilization and confirm M≥64 recommendation
  3. Parameterization Check: Compare LiRA efficacy against Head-only vs ALL-parameter fine-tuned models on same data

## Open Questions the Paper Calls Out
None

## Limitations
- Power-law relationship assumes standard threat models and may not hold for attacks exploiting non-overfitting vulnerabilities or under significant data distribution shifts
- IHA's exceptional performance relies on strong assumptions (full parameter access, near-complete dataset knowledge) that violate typical MIA threat models
- Minimal differences across fine-tuning parameterizations apply specifically to strongest attacks (LiRA) and may not generalize to weaker attack methods

## Confidence
- **High:** Systematic evaluation methodology, comparison across multiple attacks/datasets, general observation that attack efficacy decreases with increased training data
- **Medium:** Specific power-law parameters, identification of IHA as outlier, claim of minimal privacy differences across parameterization schemes
- **Low:** Theoretical explanation for IHA's exceptional performance on PatchCamelyon due to weak corpus evidence for specific "Inverse Hessian" mechanism in transfer learning

## Next Checks
1. **Threat Model Validation:** Test IHA under progressively relaxed assumptions (partial parameter access, unknown dataset size) to quantify practical impact of strong assumptions
2. **Distribution Shift Analysis:** Evaluate attack efficacy when fine-tuning data exhibits significant distribution shift from pre-training data to test power-law relationship under domain adaptation
3. **Parameterization Stress Test:** Compare attack performance across Head-only, FiLM, and ALL schemes using range of attack strengths to determine if weaker attacks show significant variance for practical security auditing