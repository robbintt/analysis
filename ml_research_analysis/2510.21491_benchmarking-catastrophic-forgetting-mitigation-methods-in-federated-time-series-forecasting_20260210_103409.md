---
ver: rpa2
title: Benchmarking Catastrophic Forgetting Mitigation Methods in Federated Time Series
  Forecasting
arxiv_id: '2510.21491'
source_url: https://arxiv.org/abs/2510.21491
tags:
- learning
- data
- tasks
- forecasting
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study introduces the first benchmarking framework for evaluating\
  \ catastrophic forgetting (CF) mitigation methods in federated continual time series\
  \ forecasting. Using the Beijing Multi-site Air Quality dataset across 12 decentralized\
  \ clients, the authors systematically compare five core CL strategies\u2014Replay,\
  \ Elastic Weight Consolidation (EWC), Online EWC, Learning without Forgetting (LwF),\
  \ and Synaptic Intelligence (SI)\u2014integrated within a federated learning pipeline."
---

# Benchmarking Catastrophic Forgetting Mitigation Methods in Federated Time Series Forecasting

## Quick Facts
- arXiv ID: 2510.21491
- Source URL: https://arxiv.org/abs/2510.21491
- Authors: Khaled Hallak; Oudom Kem
- Reference count: 26
- Primary result: First benchmarking framework for CF mitigation in federated time series forecasting using Beijing Air Quality dataset across 12 clients

## Executive Summary
This study introduces the first comprehensive benchmarking framework for evaluating catastrophic forgetting mitigation methods in federated continual time series forecasting. The authors systematically compare five core continual learning strategies - Replay, Elastic Weight Consolidation (EWC), Online EWC, Learning without Forgetting (LwF), and Synaptic Intelligence (SI) - within a federated learning pipeline. Using the Beijing Multi-site Air Quality dataset across 12 decentralized clients, the framework simulates real-world non-i.i.d. time series environments through LSTM models, lagged sequence generation, and seasonal task partitioning.

The evaluation reveals that replay-based methods consistently achieve the best trade-off between forgetting mitigation and plasticity, though at higher computational cost. EWC and Online EWC provide strong stability but lower plasticity, while LwF and SI offer balanced performance. The study highlights the need for tailored continual learning strategies in federated time series forecasting and provides open-source tools for advancing research in this domain.

## Method Summary
The study employs a federated continual learning framework where clients train LSTM models on non-i.i.d. time series data from the Beijing Multi-site Air Quality dataset. The framework partitions time series data into seasonal tasks, simulating realistic continual learning scenarios where models must adapt to changing patterns while retaining historical knowledge. Five continual learning strategies are integrated into the federated pipeline: experience replay with memory buffers, regularization-based methods (EWC and Online EWC), distillation-based approaches (LwF), and parameter importance tracking (SI). The federated learning architecture uses FedAvg for model aggregation, with each client maintaining local buffers for replay-based methods. The framework evaluates performance using established metrics including average forgetting, plasticity, and average performance across tasks.

## Key Results
- Replay-based methods achieve the best trade-off between forgetting mitigation and plasticity across all metrics
- EWC and Online EWC provide strong stability but exhibit lower plasticity in adapting to new tasks
- LwF and SI methods offer balanced performance between stability and plasticity
- The framework demonstrates significant performance variation between mitigation strategies, with replay methods outperforming regularization and distillation approaches
- Computational cost analysis shows replay methods incur higher resource requirements compared to regularization-based alternatives

## Why This Works (Mechanism)
Assumption: The effectiveness of replay-based methods likely stems from their ability to directly retain and revisit historical data patterns, providing concrete examples for the model to maintain previously learned knowledge. Regularization methods work by constraining parameter updates to preserve important weights identified during earlier training phases. Distillation approaches help maintain knowledge by training on synthetic examples that capture the essence of previous tasks. The federated architecture enables distributed processing while the CL methods ensure knowledge retention across temporal shifts in the time series data.

## Foundational Learning

Catastrophic Forgetting (CF)
- Why needed: Understanding CF is essential as it represents the core problem being addressed - neural networks' tendency to rapidly forget previously learned information when trained on new data
- Quick check: Verify that the framework's evaluation metrics (average forgetting, plasticity) directly measure CF phenomena in the federated context

Continual Learning (CL)
- Why needed: CL strategies form the core mitigation methods being benchmarked, making understanding their mechanisms critical for interpreting results
- Quick check: Confirm that each CL method's theoretical approach (replay, regularization, distillation) aligns with its observed performance characteristics

Federated Learning (FL)
- Why needed: The federated architecture determines how CL methods are implemented and evaluated across decentralized clients
- Quick check: Validate that the FedAvg aggregation and client-server communication patterns are correctly implemented in the benchmarking framework

## Architecture Onboarding

Component Map:
Data Preprocessing -> Task Partitioning -> Federated CL Pipeline -> Evaluation Metrics

Critical Path:
1. Data preprocessing and lagged sequence generation
2. Seasonal task partitioning and client distribution
3. Federated training with CL method integration
4. Performance evaluation using forgetting, plasticity, and average performance metrics

Design Tradeoffs:
The framework balances between realistic federated scenarios (non-i.i.d. data, seasonal changes) and computational feasibility. Replay methods offer superior performance but higher resource costs, while regularization methods provide better efficiency at the expense of some plasticity. The choice of LSTM models trades architectural complexity for training efficiency and interpretability.

Failure Signatures:
- Poor performance across all methods suggests issues with data preprocessing or task partitioning
- Inconsistent results between clients may indicate non-i.i.d. data distribution problems
- High forgetting rates despite CL implementation could point to inadequate memory buffer sizes or regularization strength
- Computational bottlenecks in replay methods suggest memory buffer management issues

First Experiments:
1. Baseline evaluation without any CF mitigation to establish the severity of catastrophic forgetting
2. Single-client evaluation of each CL method to isolate method performance from federated effects
3. Sensitivity analysis of memory buffer sizes for replay methods to determine optimal resource allocation

## Open Questions the Paper Calls Out
None provided

## Limitations
- Single dataset focus (Beijing Multi-site Air Quality) may limit generalizability to other time series domains
- LSTM architecture choice may not represent performance of more advanced models like Transformers
- Seasonal task partitioning approach could introduce biases based on season definitions
- Qualitative rather than quantitative computational cost assessment for replay methods

## Confidence

High confidence: Systematic comparison methodology and established CL metrics implementation
Medium confidence: Claim about replay methods' superior trade-off performance
Low confidence: Need for tailored CL strategies in federated time series forecasting

## Next Checks

1. Evaluate the same CL methods on additional time series datasets with varying characteristics (different seasonality patterns, data volumes, and non-i.i.d. distributions) to assess generalizability.

2. Implement and test alternative model architectures (Transformers, Temporal Convolutional Networks) to determine if observed performance trends hold across different neural network designs.

3. Conduct quantitative analysis of computational costs for replay-based methods under varying resource constraints to provide concrete guidance for practical deployment in federated settings.