---
ver: rpa2
title: 'VERITAS: Verification and Explanation of Realness in Images for Transparency
  in AI Systems'
arxiv_id: '2507.05146'
source_url: https://arxiv.org/abs/2507.05146
tags:
- images
- image
- adversarial
- artifacts
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents VERITAS, a framework that detects AI-generated
  images and explains why they are classified as synthetic through artifact localization
  and semantic reasoning. The framework addresses the challenge of distinguishing
  real from AI-generated small (32x32) images, which is critical for combating misinformation
  and identity fraud in digital platforms.
---

# VERITAS: Verification and Explanation of Realness in Images for Transparency in AI Systems

## Quick Facts
- arXiv ID: 2507.05146
- Source URL: https://arxiv.org/abs/2507.05146
- Reference count: 35
- Detects and explains AI-generated images (32x32) through artifact localization and semantic reasoning

## Executive Summary
VERITAS addresses the critical challenge of distinguishing real from AI-generated small images, which is essential for combating misinformation and identity fraud in digital platforms. The framework combines super-resolution, attention-based artifact localization, CLIP-based artifact classification, and VLM-generated textual explanations to provide transparent and interpretable insights into the synthetic nature of images. VERITAS demonstrates superior explanation quality compared to baseline models, particularly for low-resolution images, by identifying and describing various artifacts such as misaligned body panels, anatomically incorrect structures, and frequency domain signatures.

## Method Summary
VERITAS employs a five-stage pipeline to detect AI-generated images and explain why they are classified as synthetic. The process begins with super-resolution of the input 32x32 image using DRCT, followed by GradCAM-based heatmap generation to identify regions of interest. The image is then divided into patches weighted by attention, and CLIP-based artifact scoring is performed using positive, negative, and neutral descriptors. Finally, MOLMO-7B-D generates textual explanations for high-scoring patches. The framework is evaluated on the CIFAKE dataset, a balanced collection of 120K real and synthetic 32x32 images generated by Stable Diffusion 1.4.

## Key Results
- VERITAS provides more detailed and interpretable explanations of artifacts compared to baseline models like MOLMO, Qwen2.5 VL, and Pixtral 12B
- The framework successfully identifies and describes various artifacts in synthetic images, including misaligned body panels, anatomically incorrect structures, and frequency domain signatures
- Experimental results demonstrate the effectiveness of VERITAS in detecting and explaining the synthetic nature of low-resolution images

## Why This Works (Mechanism)
VERITAS leverages a combination of super-resolution, attention-based localization, and semantic reasoning to detect and explain AI-generated images. The super-resolution step enhances the visual quality of low-resolution images, making it easier to identify artifacts. GradCAM-based heatmaps highlight regions of interest, which are then divided into patches and weighted by attention. CLIP-based artifact scoring uses pre-trained descriptors to classify the nature of the artifacts, and MOLMO-7B-D generates human-readable explanations for the identified artifacts.

## Foundational Learning
- **Super-resolution (DRCT)**: Enhances low-resolution images to improve artifact detection and explanation quality. *Quick check: Verify that super-resolved images retain essential details without introducing artifacts.*
- **GradCAM**: Generates attention maps to identify regions of interest in the image. *Quick check: Ensure that GradCAM heatmaps accurately highlight relevant areas for patch extraction.*
- **CLIP-based artifact scoring**: Uses pre-trained descriptors to classify artifacts based on semantic similarity. *Quick check: Validate that artifact descriptors are comprehensive and representative of common generative model artifacts.*
- **VLM-generated explanations (MOLMO)**: Provides human-readable explanations for identified artifacts. *Quick check: Assess the accuracy and fluency of MOLMO-generated explanations compared to ground truth.*

## Architecture Onboarding

### Component Map
CIFAKE Image -> DRCT Super-resolution -> GradCAM Heatmap -> Patch Division (Weighted) -> CLIP Artifact Scoring -> MOLMO Explanation

### Critical Path
The critical path involves super-resolution, GradCAM-based heatmap generation, patch-level analysis weighted by attention, CLIP-based artifact classification, and textual explanation generation via MOLMO.

### Design Tradeoffs
- Super-resolution vs. artifact preservation: DRCT upscaling may introduce smoothing artifacts that mask or mimic generative artifacts
- Heatmap granularity vs. computational efficiency: GradCAM on 32x32 images produces coarse attention maps, which may affect patch weighting accuracy
- VLM explanations vs. hallucination risk: MOLMO-7B-D explanations may suffer from hallucination, though future work on Woodpecker correction is proposed

### Failure Signatures
- DRCT upscaling introduces smoothing/ringing artifacts that mask or mimic generative artifacts
- GradCAM on 32x32 produces coarse attention maps, affecting patch weighting accuracy
- VLM hallucinations in artifact descriptions, undermining user trust in explanations

### First 3 Experiments to Try
1. Evaluate VERITAS on a broader dataset with images generated by multiple models (e.g., DALL-E, Midjourney) and varying resolutions to assess generalizability
2. Conduct a human evaluation study to compare the interpretability and accuracy of VERITAS explanations against baseline models like MOLMO and Qwen2.5 VL
3. Test the framework's robustness to adversarial perturbations or watermarked images to ensure reliable detection in real-world scenarios

## Open Questions the Paper Calls Out
1. Can domain generalization methods prevent overfitting to specific model artifacts and improve detection accuracy across diverse, unseen generative architectures?
2. Can certified robustness techniques effectively preserve classification stability and GradCAM localization integrity under adversarial perturbations?
3. Do hallucination correction techniques effectively filter factually unsupported descriptions from VLM-generated explanations without losing nuance?

## Limitations
- Reliance on the CIFAKE dataset, which may not fully represent real-world diversity in image content and generative model artifacts
- Untested performance on larger image resolutions beyond 32x32 pixels
- Lack of explicit artifact descriptor templates, potentially affecting reproducibility

## Confidence
- **Core claim (High)**: VERITAS's ability to detect and explain AI-generated images is supported by structured pipeline and experimental validation on a balanced dataset
- **Artifact descriptors generalizability (Medium)**: Lack of explicit descriptor templates and cross-dataset evaluation raises uncertainty
- **Explanation interpretability (Medium)**: VLM-based explanations are prone to hallucination, though future work on Woodpecker correction is proposed

## Next Checks
1. Evaluate VERITAS on a broader dataset with images generated by multiple models (e.g., DALL-E, Midjourney) and varying resolutions to assess generalizability
2. Conduct a human evaluation study to compare the interpretability and accuracy of VERITAS explanations against baseline models like MOLMO and Qwen2.5 VL
3. Test the framework's robustness to adversarial perturbations or watermarked images to ensure reliable detection in real-world scenarios