---
ver: rpa2
title: 'Evaluating Financial Sentiment Analysis with Annotators Instruction Assisted
  Prompting: Enhancing Contextual Interpretation and Stock Prediction Accuracy'
arxiv_id: '2505.07871'
source_url: https://arxiv.org/abs/2505.07871
tags:
- sentiment
- financial
- annotators
- stock
- aiap
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of subjective sentiment annotations\
  \ in financial sentiment analysis benchmarks by introducing AIAP, a prompt that\
  \ incorporates annotators\u2019 instructions into LLM evaluation. AIAP aligns machine\
  \ interpretations with human-defined sentiment classes, improving model performance\
  \ significantly (up to 9.08% accuracy gain) on a new WallStreetBets dataset."
---

# Evaluating Financial Sentiment Analysis with Annotators Instruction Assisted Prompting: Enhancing Contextual Interpretation and Stock Prediction Accuracy

## Quick Facts
- arXiv ID: 2505.07871
- Source URL: https://arxiv.org/abs/2505.07871
- Reference count: 40
- Key outcome: AIAP improves financial sentiment classification accuracy by up to 9.08% and enhances stock price prediction when used with CSBS scoring method

## Executive Summary
This paper addresses the challenge of subjective sentiment annotations in financial sentiment analysis benchmarks by introducing AIAP, a prompt that incorporates annotators' instructions into LLM evaluation. AIAP aligns machine interpretations with human-defined sentiment classes, improving model performance significantly on a new WallStreetBets dataset. The approach also introduces a confidence-score-based sentiment-indexing method (CSBS) that enhances stock price prediction when used as an additional feature. Results show consistent performance gains across multiple models (FinGPT, Llama3, GPT-4) and demonstrate the value of context-aware prompting in both sentiment classification and financial applications.

## Method Summary
The study constructs AIAP (Annotators' Instruction Assisted Prompting) by integrating annotator instructions—definitions, domain grounding (bullish/bearish/stable), and canonical examples—into LLM prompts for financial sentiment analysis. The methodology evaluates sentiment classification on the WSBS dataset (2,920 samples from r/WallStreetBets) using base prompts, AIAP, and traditional few-shot learning approaches. For stock prediction, the paper implements CSBS (Confidence-Score-Based Sentiment scoring), which weights sentiment contributions using classifier softmax probabilities and handles neutral-class polarity. The pipeline combines sentiment scores with price/volume features for regression using Linear Regression, SVR, and XGBoost models.

## Key Results
- AIAP achieves up to 9.08% accuracy improvement on WSBS All-Agree dataset compared to base prompts
- GPT-4 with AIAP reaches 80.91% accuracy on WSBS Full dataset
- CSBS reduces GME stock prediction RMSE from 3.567 to 2.801 compared to baseline
- Performance gains are consistent across FinGPT, Llama3, and GPT-4 models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Providing annotator instructions to LLMs aligns model predictions with human judgment by reducing ambiguity in task definition.
- **Mechanism**: AIAP injects three structured components—definitions, domain grounding (bullish/bearish/stable), and canonical examples—into the prompt. This leverages instruction-tuned LLMs' capacity to follow explicit guidelines during inference, reducing the "guesswork" required when facing underspecified sentiment classes.
- **Core assumption**: Instruction-tuned LLMs can interpret and apply task guidelines consistently across samples, analogous to human annotators given the same instructions.
- **Evidence anchors**:
  - [abstract]: "By integrating detailed task instructions originally intended for human annotators into the LLMs' prompt framework, AIAP aims to standardize the understanding of sentiment across both human and machine interpretations."
  - [section 4.1]: Figure 2 shows progressive accuracy gains as instruction components are added (Base → D → D+G → D+G+E), demonstrating that richer context yields better alignment.
  - [corpus]: Neighbor paper "Reasoning or Overthinking: Evaluating Large Language Models on Financial Sentiment Analysis" (FMR=0.58) similarly examines LLM alignment with human-labeled sentiment, suggesting context-aware prompting is an active area but not yet settled science.
- **Break condition**: If the annotator instructions are poorly defined, internally inconsistent, or too dataset-specific, LLMs may not generalize. The paper notes AIAP improved FinGPT on Financial PhraseBank by only 0.4% (86.0% → 86.4%), suggesting limited transfer across domains with mismatched annotation protocols.

### Mechanism 2
- **Claim**: Performance gains are not due to few-shot learning alone; structured definitional context provides additional signal beyond example demonstrations.
- **Mechanism**: The paper isolates the contribution of structured instruction (definition + grounding) by comparing AIAP against traditional few-shot prompting (1-shot, 2-shot, 3-shot). AIAP outperforms all few-shot variants, indicating that the *instructional structure*—not merely the presence of examples—drives improvement.
- **Core assumption**: LLMs can distinguish between task-defining semantics and mere pattern-matching from examples.
- **Evidence anchors**:
  - [section 4.2, Table 3]: On All-Agree dataset, base prompt achieves 64.74%, 3-shot achieves 62.82%, but AIAP achieves 72.37%. Few-shot prompting sometimes degrades performance, while AIAP consistently improves it.
  - [corpus]: Neighbor paper "Analogy-Driven Financial Chain-of-Thought" (FMR=0.53) explores reasoning-based prompting for FSA, suggesting prompting strategy design significantly impacts outcomes, though optimal strategies remain domain-dependent.
- **Break condition**: If examples in AIAP are over-specific or poorly matched to the target distribution, adding them can degrade performance. The paper notes minor drops in 2 of 6 cases on the Full dataset when adding examples (D+G → D+G+E).

### Mechanism 3
- **Claim**: Confidence-score-based sentiment scoring (CSBS) extracts more value from classifier outputs for downstream tasks by incorporating prediction certainty and handling neutral-class polarity.
- **Mechanism**: CSBS uses softmax probabilities from classifier predictions. For neutral predictions, it assigns directional contribution based on the relative strength of positive vs. negative logits. This produces a continuous sentiment signal rather than discrete labels, improving regression-based stock price prediction.
- **Core assumption**: Classifier confidence correlates with information content useful for downstream time-series prediction.
- **Evidence anchors**:
  - [section 5.2, Algorithm 1]: Pseudocode shows neutral samples contribute `sign(P[pos] - P[neg]) · P[neu]` to the daily score.
  - [section 5.4, Table 5]: BERT-CSBS reduces GME RMSE from 3.567 (baseline) to 3.139; FinGPT-AIAP reduces it to 2.801. Improvements are most pronounced for meme stocks (GME, AMC) with high WSB discussion volume.
  - [corpus]: No direct neighbor papers validate CSBS specifically; this is a novel contribution requiring independent replication.
- **Break condition**: CSBS assumes calibrated classifier probabilities. If the classifier is overconfident but wrong (e.g., systematically misclassifying sarcasm), the confidence-weighted signal may introduce noise rather than signal.

## Foundational Learning

- **Concept: In-Context Learning (ICL)**
  - **Why needed here**: AIAP relies entirely on ICL—no fine-tuning occurs. The LLM must adjust its behavior based solely on prompt content during inference.
  - **Quick check question**: Can you explain why adding instructions at inference time differs from fine-tuning on labeled data?

- **Concept: Annotation Subjectivity in Financial Sentiment**
  - **Why needed here**: The paper's motivation rests on the observation that existing FSA benchmarks (Financial PhraseBank, FiQA) exhibit high inter-annotator disagreement due to undefined sentiment classes.
  - **Quick check question**: Why might two annotators label "Revenue increased but guidance was cautious" differently without explicit instructions?

- **Concept: Encoder vs. Decoder Transformer Architectures**
  - **Why needed here**: The paper uses both encoder-based classifiers (BERT for CSBS) and decoder-based LLMs (FinGPT, Llama3, GPT-4 for AIAP). Each serves different roles in the pipeline.
  - **Quick check question**: Which architecture provides softmax probabilities over class labels natively, and which requires prompt-based extraction?

## Architecture Onboarding

- **Component map**:
  Raw WSB text → [Preprocessing/Filtering by ticker]
                → [Sentiment Classification Layer]
                    ├── LLM path: FinGPT/Llama3/GPT-4 + AIAP prompt → discrete label
                    └── Encoder path: BERT-base fine-tuned on WSBS → class probabilities
                → [Sentiment Aggregation Layer]
                    ├── QuantSS: (Pos - Neg) / Total
                    └── CSBS: confidence-weighted with neutral polarity handling
                → [Stock Prediction Layer]
                    └── Regression (Linear/SVR/XGBoost) with sentiment score as feature

- **Critical path**:
  1. Construct AIAP prompt: base instruction + definition + grounding + examples (Figure 1)
  2. Run inference on target texts with instruction-tuned LLM
  3. Extract predictions; if using encoder path, also extract softmax probabilities
  4. Aggregate to daily sentiment scores using CSBS (Algorithm 1)
  5. Append to price/volume features for regression

- **Design tradeoffs**:
  - **LLM vs. Encoder classifier**: LLMs with AIAP achieve higher accuracy (up to 80.91% with GPT-4) but require API access and provide only discrete labels. Encoders provide calibrated probabilities for CSBS but require fine-tuning data.
  - **QuantSS vs. CSBS**: QuantSS is simpler but ignores confidence. CSBS extracts more signal but assumes probability calibration.
  - **Dataset specificity**: AIAP instructions are tailored to WSB's informal language. Applying to formal financial news (e.g., FPB) yields minimal gains (+0.4%).

- **Failure signatures**:
  - Accuracy drops when identifier term ("news" vs "tweet" vs "input") is mismatched to data source (Table 2 shows 8-15 point spread)
  - CSBS provides no improvement on low-volume tickers (AAPL) or index ETFs (SPY) where sentiment signal is weaker
  - FinBERT underperforms BERT-base on WSB data due to domain mismatch (formal vs. slang-heavy)

- **First 3 experiments**:
  1. **Baseline validation**: Run base prompt vs. AIAP on WSBS All-Agree subset across all three identifier terms. Confirm ~5-9% accuracy delta matches paper.
  2. **Ablation on instruction components**: Replicate Figure 2—measure accuracy at Base → D → D+G → D+G+E. Verify progressive gains and identify where examples help vs. hurt.
  3. **CSBS vs. QuantSS on held-out ticker**: Train BERT-base on WSBS, generate sentiment scores for a ticker not in paper's top 5 (e.g., NVDA), compare regression RMSE with vs. without CSBS.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the specificity and semantic nature of examples within the Annotators' Instruction Assisted Prompt (AIAP) affect performance stability on noisy financial datasets?
- Basis in paper: [explicit] The authors observe performance drops in 2 out of 6 cases when moving from "Definition+Grounding" to adding "Examples" and explicitly state: "Future study can focus on the nature of the 'examples' provided in the prompt."
- Why unresolved: The paper identifies that examples can sometimes lead to performance degradation (potentially due to "over-specific nature"), but does not isolate which attributes of an example (e.g., length, lexical complexity) cause this negative impact.
- What evidence would resolve it: An ablation study systematically varying the syntactic structure and domain specificity of the examples within the prompt would determine the optimal example characteristics for financial sentiment tasks.

### Open Question 2
- Question: Can the AIAP methodology be generalized to formal financial texts, or is it inherently limited to informal social media contexts like WallStreetBets?
- Basis in paper: [inferred] In Section 4.3, the authors note that applying the WSB-specific AIAP to the formal Financial PhraseBank (FPB) dataset yielded only negligible improvements (0.4%), unlike the significant gains seen on WSB data.
- Why unresolved: The paper demonstrates that AIAP is highly effective for the colloquial, slang-heavy language of WSB, but the failure to transfer these gains to formal financial news suggests the prompting strategy may be domain-dependent rather than universal.
- What evidence would resolve it: Constructing an AIAP using instructions derived from formal financial analyst guidelines (rather than WSB annotator instructions) and testing it on datasets like FPB or FiQA would test the framework's generalizability.

### Open Question 3
- Question: Is the proposed Confidence-Score-Based Sentiment (CSBS) method robust enough to improve stock price prediction for stable, high-market-cap assets, or is it limited to high-volatility "meme stocks"?
- Basis in paper: [inferred] The experimental results in Section 5.4 show that CSBS significantly improved prediction accuracy for GME and AMC, but failed to provide consistent value for stable assets like SPY and AAPL.
- Why unresolved: The paper attributes this discrepancy to the unique nature of the GME short-squeeze event but leaves open the question of whether the CSBS method intrinsically fails to capture relevant signals in efficient or less volatile markets.
- What evidence would resolve it: Applying the CSBS method to a longer time horizon of stable stocks during periods of high market uncertainty would clarify if the method is structurally flawed for blue-chip assets or simply sensitive to noise.

## Limitations
- **Instruction specificity trade-off**: AIAP's effectiveness is tightly coupled to the quality and specificity of annotator instructions. The paper acknowledges minimal gains on Financial PhraseBank (+0.4%) when applying WSB-tailored instructions, suggesting poor cross-dataset generalization.
- **Downstream stock prediction**: While CSBS improves prediction for meme stocks (GME, AMC) with high WSB discussion volume, improvements vanish for low-volume tickers (AAPL) and index ETFs (SPY).
- **Model architecture dependency**: Performance gains vary significantly by LLM architecture—GPT-4 achieves 80.91% accuracy while FinGPT-SA achieves 71.64% on WSBS Full dataset.

## Confidence
- **High confidence**: AIAP's accuracy improvements on WSBS dataset (up to 9.08% gain) are well-supported by ablation studies showing progressive gains from adding instruction components.
- **Medium confidence**: The claim that structured instructions outperform few-shot learning is supported but requires context. While AIAP beats all few-shot variants on All-Agree subset, few-shot performance varies across datasets.
- **Low confidence**: The stock prediction improvements using CSBS are promising but not fully validated. The method shows strong results for GME (RMSE reduction from 3.567 to 2.801) but fails on SPY and AAPL.

## Next Checks
1. **Cross-dataset instruction transfer**: Apply AIAP to Financial PhraseBank using its original annotation guidelines rather than WSB instructions. Measure whether providing dataset-specific instructions yields similar accuracy gains as observed on WSBS.
2. **Confidence calibration analysis**: For CSBS, measure whether classifier probabilities are well-calibrated by computing Expected Calibration Error (ECE) on WSBS. Compare ECE for BERT-base vs. FinBERT to explain why BERT-base outperforms FinBERT despite domain pretraining.
3. **Temporal robustness test**: Extend stock prediction evaluation to include post-2022 market data and different market regimes (bull vs. bear). Test whether sentiment scores from 2020-2022 maintain predictive power during market volatility changes.