---
ver: rpa2
title: Investigating an Overfitting and Degeneration Phenomenon in Self-Supervised
  Multi-Pitch Estimation
arxiv_id: '2506.23371'
source_url: https://arxiv.org/abs/2506.23371
tags:
- data
- training
- supervised
- music
- self-supervised
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates a phenomenon in self-supervised multi-pitch
  estimation (MPE) where a model simultaneously overfits to supervised training data
  while degenerating on self-supervised data. The authors extend supervised MPE by
  incorporating self-supervised objectives based on pitch-invariant and pitch-equivariant
  properties, showing substantial improvements under closed training conditions.
---

# Investigating an Overfitting and Degeneration Phenomenon in Self-Supervised Multi-Pitch Estimation

## Quick Facts
- **arXiv ID**: 2506.23371
- **Source URL**: https://arxiv.org/abs/2506.23371
- **Reference count**: 0
- **Primary result**: Self-supervised MPE objectives improve supervised performance but cause model to output blank predictions on unlabeled data

## Executive Summary
This paper investigates a previously undocumented phenomenon in self-supervised multi-pitch estimation where models simultaneously overfit to supervised training data while degenerating to trivial (blank) predictions on self-supervised-only data. The authors extend supervised MPE by incorporating self-supervised objectives based on pitch-invariant and pitch-equivariant properties, showing substantial improvements under closed training conditions. However, when applying these objectives to broader data without supervision, the model exhibits this unexpected degeneration. The study demonstrates and investigates this issue, offering insights into the underlying problem and highlighting the need for new objective formulations that prevent trivial solutions while maintaining prediction flexibility.

## Method Summary
The method extends supervised multi-pitch estimation by incorporating self-supervised objectives based on pitch-invariant and pitch-equivariant properties. The framework uses Harmonic CQT (HCQT) spectrograms with 6 harmonic channels (H={0.5,1,2,3,4,5}) and trains a modified Timbre-Trap 2D autoencoder. Self-supervised losses include Liv-t (timbre-invariance via random equalization), Liv-p (percussion-invariance via superimposed percussion), and Lev-g (geometric-equivariance via pitch/time shift and stretch transformations). The model is trained on URMP dataset with additional self-supervision from NSynth, MusicNet, and FMA datasets. Evaluation uses F1-score with 0.5 threshold via mir_eval.

## Key Results
- Geometric-equivariance objectives (Lev-g) provide the most substantial improvement, yielding 4.2 point F1 gain on Bach10 dataset
- Joint training with self-supervised objectives improves supervised MPE performance under closed conditions
- Model simultaneously overfits to supervised data while degenerating to blank predictions on self-supervised-only data
- Energy-based objectives (Leg + Lspr) prevent collapse but introduce excessive false alarms, degrading overall quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint training with self-supervised pitch-equivariant objectives improves supervised MPE performance under closed conditions
- Mechanism: The geometric-equivariance loss (Lev-g) enforces that vertical/horizontal translations and time-stretching of HCQT spectrograms produce corresponding transformations in the predicted salience-gram. This leverages the shift-invariance of the HCQT representation to explicitly teach harmonic relationships, acting as data augmentation while encoding structural priors about pitch.
- Core assumption: The HCQT frequency bins align with pitch space such that vertical translations correspond to semitone shifts.
- Evidence anchors:
  - [Section 3.3] "The geometric-equivariance objective is most effective and yields a significant improvement across all datasets... likely due to the model capturing harmonic relationships explicitly by leveraging the shift-invariance exhibited by the HCQT representation."
  - [Table 1] Lspv + Lev-g achieves 90.0 F1 on Bach10 vs 85.8 for supervised-only—a 4.2 point gain.
  - [corpus] Related work PESTO confirms equivariance objectives work for monophonic pitch estimation, but corpus lacks direct polyphonic validation.
- Break condition: If input representation lacks log-frequency bin spacing, the equivariance mapping between translations and pitch shifts breaks down.

### Mechanism 2
- Claim: Pitch-invariant objectives provide modest regularization by teaching the model to ignore timbral and percussive variations
- Mechanism: Random equalization curves (tiv-t) and superimposed percussion (tiv-p) are applied to input spectrograms; the invariance loss penalizes changes to the predicted salience-gram. This prevents overfitting to spectral envelope patterns that don't indicate pitch.
- Core assumption: Equalization and percussion additions primarily affect timbre without altering fundamental frequencies.
- Evidence anchors:
  - [Section 3.3] "The timbre-invariance objective is least effective and has a mixed effect... It is possible that the property of timbre-invariance is already strongly indicated by the supervised objective."
  - [Section 3.3] "The percussion-invariance objective is moderately beneficial, even for some datasets without percussion."
  - [corpus] No corpus papers validate timbre-invariance for polyphonic MPE specifically.
- Break condition: If equalization curves are too extreme, they may mask harmonic structure, creating conflicting signals.

### Mechanism 3
- Claim: Self-supervision on additional unlabeled data causes degeneration to trivial (blank) predictions on that data while preserving supervised performance
- Mechanism: Without ground-truth labels to anchor non-zero predictions, the invariance/equivariance objectives can be minimized by outputting uniformly inactive salience-grams. The model partitions: supervised data maintains correct predictions via Lspv; self-supervised-only data collapses. Energy-based objectives (Leg) prevent collapse but introduce excessive false alarms, degrading overall quality.
- Core assumption: The trivial solution (all zeros) is a local minimum for invariance/equivariance losses when no supervised signal counteracts it.
- Evidence anchors:
  - [Abstract] "model simultaneously overfits to the supervised data while degenerating on data used for self-supervision only"
  - [Section 4.1] "the underlying issue is too strong of a pull towards the trivial solution for the non-supervised data, irrespective of its distribution"
  - [Figure 3] "there is a noticeable relationship between the amount of samples for self-supervision only and the severity of degeneration"
  - [corpus] No corpus papers document this specific degeneration phenomenon in polyphonic settings.
- Break condition: If a mechanism enforces content existence without introducing excessive false alarms, the trivial solution path is blocked.

## Foundational Learning

- Concept: **Harmonic CQT (HCQT) spectrograms**
  - Why needed here: The paper's entire framework depends on HCQT's harmonic channels indexed across frequency, enabling both the equivariance property and convolutional processing.
  - Quick check question: Can you explain why a 6-channel HCQT with harmonics H={0.5,1,2,3,4,5} provides stronger pitch inductive bias than a standard spectrogram?

- Concept: **Invariance vs. equivariance in self-supervised learning**
  - Why needed here: The two self-supervised objective classes have opposite expectations—invariance expects predictions to stay constant; equivariance expects predictions to transform in lockstep.
  - Quick check question: If you pitch-shift an audio signal up 3 semitones, should a pitch estimation model's output be invariant or equivariant to this transformation?

- Concept: **Trivial solutions in self-supervised learning**
  - Why needed here: The core failure mode is collapse to blank predictions—understanding why models exploit shortcuts is essential for debugging.
  - Quick check question: Why might a model output all zeros to minimize an invariance loss, and what countermeasure does the paper attempt?

## Architecture Onboarding

- Component map: HCQT extraction -> Encoder (4 dilated conv blocks) -> Latent (conv + layer norm) -> Decoder (4 transposed conv blocks) -> Salience-gram -> Peak-picking (0.5 threshold) -> F0 estimates
- Critical path: HCQT extraction → encoder → latent → decoder → salience-gram → peak-picking → threshold at 0.5 → F0 estimates
- Design tradeoffs:
  - Energy-based objectives (Leg + Lspr) prevent collapse but introduce false alarms and reduce flexibility
  - More self-supervised samples accelerates degeneration; fewer samples limits regularization benefit
  - Fine-tuning from pretrained weights doesn't prevent degeneration—the pull toward trivial solutions persists
- Failure signatures:
  - Model outputs blank salience-grams on data without supervised labels (check by visualizing Ŷ for held-out samples)
  - F1-score remains stable on supervised validation set but collapses to ~0 on evaluation datasets
  - Gradual decrease in prediction amplitude over training (recall drops while precision may stay high)
- First 3 experiments:
  1. **Reproduce closed-loop improvement**: Train Lspv alone vs. Ltotal (Lspv + Liv-t + Liv-p + Lev-g) on URMP. Verify Table 1 F1 gains appear (expect ~3-4 point improvement on out-of-distribution sets like GuitarSet).
  2. **Isolate degeneration trigger**: Add 16 unlabeled MusicNet samples with self-supervision only. Monitor F1 on both URMP validation and MusicNet test. Confirm collapse occurs only on MusicNet after ~2000 batches.
  3. **Test energy-based mitigation**: Repeat experiment 2 with Leg + Lspr on self-supervised samples. Check whether blank predictions are prevented but overall F1 degrades compared to reference.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What self-supervised objectives or training mechanisms can prevent trivial solution degeneration while maintaining flexibility for accurate multi-pitch estimation?
- Basis in paper: [explicit] Authors state "A solution for the polyphonic setting may require some sort of objective that enforces the existence of content in the predictions, to protect against the trivial solution. The energy-based objectives are one such protection, but they evidently remove too much flexibility and lead to worse predictions."
- Why unresolved: Energy-based objectives successfully prevent collapse but introduce excessive constraint, degrading overall performance. No alternative formulation is proposed or tested.
- What evidence would resolve it: Development of new objectives achieving comparable performance to supervised baseline while preventing degeneration on self-supervised data.

### Open Question 2
- Question: Would substantially larger and more diverse supervised training datasets eliminate or reduce the degeneration phenomenon?
- Basis in paper: [explicit] Authors note "It is unclear whether these interactions would persist if larger and more diverse data were used for supervision" after observing experiments with only 1-2 hours of supervised URMP data.
- Why unresolved: All experiments use limited supervised data; the interaction between dataset scale and degeneration severity remains unexplored.
- What evidence would resolve it: Replication experiments using significantly larger supervised datasets (10+ hours) to observe whether degeneration magnitude decreases or disappears.

### Open Question 3
- Question: What training dynamics predict or coincide with the onset of degeneration, enabling early detection or intervention?
- Basis in paper: [inferred] Figures 1-4 show progressive degeneration over training, but no analysis of gradient magnitudes, loss landscapes, or feature distributions explains when or why degeneration initiates.
- Why unresolved: The paper documents the phenomenon but lacks mechanistic analysis of training dynamics that could reveal predictive signals.
- What evidence would resolve it: Detailed training dynamics analysis including gradient norms, per-objective loss trajectories, and representation similarity metrics across training iterations.

## Limitations
- The exact architectural hyperparameters remain unspecified, making precise reproduction challenging
- The paper lacks ablation studies to isolate which self-supervised objectives most contribute to degeneration
- Energy-based mitigation (Leg + Lspr) introduces excessive false alarms without fully resolving the underlying issue

## Confidence

- **High confidence**: The existence of simultaneous overfitting and degeneration on self-supervised data; the effectiveness of geometric-equivariance objectives under closed training; the failure of energy-based objectives to prevent collapse without degrading performance
- **Medium confidence**: The mechanism explanation (trivial solution as local minimum); the relative effectiveness of different self-supervised objectives; the claim that more self-supervised samples accelerate degeneration
- **Low confidence**: The exact architectural parameters needed for faithful reproduction; the universality of this phenomenon across different model architectures

## Next Checks

1. **Ablation study**: Train models with individual self-supervised objectives (Lspv+Lev-g, Lspv+Liv-t, Lspv+Liv-p) to identify which most strongly contributes to degeneration
2. **Cross-architecture validation**: Implement the same self-supervised framework on a different MPE architecture (e.g., Transformer-based) to test if degeneration persists
3. **Extended mitigation testing**: Evaluate alternative non-trivial solutions such as entropy regularization or contrastive learning approaches to prevent blank predictions while maintaining accuracy