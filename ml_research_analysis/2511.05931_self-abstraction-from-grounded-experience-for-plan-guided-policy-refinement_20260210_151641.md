---
ver: rpa2
title: Self-Abstraction from Grounded Experience for Plan-Guided Policy Refinement
arxiv_id: '2511.05931'
source_url: https://arxiv.org/abs/2511.05931
tags:
- plan
- agent
- sage
- agents
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes SAGE, a framework that enables LLM-based agents
  to learn and improve from their own task executions through self-abstraction. The
  method operates in three stages: initial exploration of the task, inducing a concise
  plan abstraction from the trajectory that captures key steps and dependencies, and
  plan-augmented execution where the agent re-attempts the task with the induced plan
  as contextual guidance.'
---

# Self-Abstraction from Grounded Experience for Plan-Guided Policy Refinement

## Quick Facts
- arXiv ID: 2511.05931
- Source URL: https://arxiv.org/abs/2511.05931
- Reference count: 40
- Primary result: 7.2% relative improvement over baseline on SWE-Bench Verified bug-fixing tasks

## Executive Summary
This paper proposes SAGE, a framework that enables LLM-based agents to learn and improve from their own task executions through self-abstraction. The method operates in three stages: initial exploration of the task, inducing a concise plan abstraction from the trajectory that captures key steps and dependencies, and plan-augmented execution where the agent re-attempts the task with the induced plan as contextual guidance. SAGE was evaluated on SWE-Bench Verified for bug-fixing tasks, demonstrating consistent performance gains across different LLM backbones and agent architectures.

## Method Summary
SAGE improves LLM-based software agents through a three-stage pipeline: (1) Exploration - run an agent to produce a trajectory of actions and observations, (2) Plan Abstraction - use an LLM to analyze the trajectory and generate a structured plan with Analysis, Feedback, and Suggested Plan sections, and (3) Plan-augmented Execution - re-run the same agent with the generated plan injected into its context. The framework achieves improvement without updating model weights, instead using the plan as contextual guidance during inference. The method was tested with mini-swe-agent framework on SWE-Bench Verified, using bash-only interfaces with 250-step limits and $3 cost caps per instance.

## Key Results
- 7.2% relative improvement over Mini-SWE-Agent baseline with GPT-5 (high)
- 73.2-74% Pass@1 resolve rates when combined with Mini-SWE-Agent and OpenHands CodeAct frameworks
- Cross-model plan induction (different LLM for planning vs. execution) reduces self-assessment bias
- Induced plan elements correlate with successful repairs in fail→pass transitions

## Why This Works (Mechanism)

### Mechanism 1: Trajectory-to-Plan Compression
The initial trajectory, even if unsuccessful, contains recoverable task structure that, when abstracted, improves subsequent attempts. The exploration agent produces trajectory τ containing (state, action, reward) sequences. A plan abstraction agent P_ϕ then induces a concise plan ψ that distills key steps, dependencies, and constraints. This compressed representation retains task-relevant signal while discarding noise, making it tractable for context-conditioned re-execution. If initial trajectories are too short, too noisy, or fail to engage relevant environment states, the induced plan will lack grounding and provide misleading guidance.

### Mechanism 2: Plan-Guided Policy Conditioning
Conditioning a policy on an induced plan improves action selection without modifying model weights. The plan-augmented agent A⁺_θ follows a revised policy π⁺_θ(·|s_t, ψ) that incorporates the plan as additional context. This is formalized as an option in the semi-MDP framework, where the plan defines an intra-option policy µ_o that guides temporal abstraction over action sequences. If the plan is too abstract, too specific, or misaligned with the agent's action vocabulary, conditioning may introduce confusion rather than guidance.

### Mechanism 3: Cross-Model Plan Induction Reduces Self-Bias
Using a different LLM for plan abstraction than for execution reduces self-assessment bias and improves plan quality. LLMs exhibit self-preference when judging their own outputs. By assigning plan induction to P_ϕ with different parameters/backbone than execution agent A_θ, SAGE obtains more objective trajectory critiques. Table 2 shows Claude Sonnet 4.5 with GPT-5 (high) planning achieves 73.2% vs. 72.4% self-planned. If the plan-induction model is significantly weaker than the execution model or has incompatible reasoning patterns, cross-model plans may degrade performance.

## Foundational Learning

- **Markov Decision Processes (MDPs)**: The paper formalizes agent-environment interaction as an MDP with states (repository content, test status), actions (tool calls, edits), and transitions. Understanding this formulation is necessary to interpret the trajectory τ and policy π notation. Quick check: Can you explain why a trajectory is represented as (s₁, a₁, r₁, ..., s_t, a_t, r_t) and what each component represents in a bug-fixing task?

- **Options Framework in RL**: SAGE connects plan-augmented execution to the options framework (initiation set I_o, intra-option policy µ_o, termination β_o). The induced plan functions as a temporal abstraction over primitive actions. Quick check: How does viewing a plan as an "option" differ from viewing it as simply additional context in the prompt?

- **Test-Time Adaptation vs. Weight Updates**: SAGE improves agents at inference time without gradient-based learning. This contrasts with fine-tuning approaches and requires understanding in-context learning limitations. Quick check: What constraints does test-time adaptation impose on how much improvement is achievable, compared to methods that update model parameters?

## Architecture Onboarding

- **Component map**: Exploration Agent -> Plan Abstraction Agent -> Plan-Augmented Execution Agent
- **Critical path**: 1) Exploration rollout → capture trajectory with thoughts, actions, observations 2) Format trajectory for plan agent (elide long outputs, normalize action format) 3) Plan induction → parse Analysis, Feedback, New Plan from response 4) Inject plan into execution prompt under `<previous_attempt>` tags 5) Re-execute with 250-step limit, $3 cost cap per instance
- **Design tradeoffs**: Same vs. different LLM for planning (simpler vs. reduced bias); trajectory verbosity (more signal vs. context limits); plan granularity (generalizes better vs. actionable specificity)
- **Failure signatures**: Plan ignored (agent proceeds original strategy); Plan overfitting (copies failed trajectory's mistakes); Context overflow (plan exceeds budget); Self-bias loop (plan agent rates own trajectory too positively)
- **First 3 experiments**: 1) Ablation on plan components (Analysis, Feedback, Induced Plan separately) 2) Trajectory length sensitivity (truncate at 25%, 50%, 75% of full length) 3) Cross-framework transfer (apply plan format to different agent framework like AutoCodeRover)

## Open Questions the Paper Calls Out

1. How does SAGE performance scale with additional episodes beyond the current two-episode design (exploration + plan-augmented execution)? The paper mentions this as future work, noting the connection to PSRL which typically updates posteriors after each episode.

2. What is the optimal strategy for selecting plan-abstraction models relative to execution models to mitigate self-assessment bias? While cross-model planning shows benefits, the optimal pairing strategy (stronger model for planning vs. execution) remains unclear.

3. Does SAGE generalize to software engineering tasks beyond bug-fixing, such as repository-level code generation, refactoring, or feature implementation? Current evaluation is restricted to SWE-Bench Verified bug-fixing tasks.

## Limitations

- The paper doesn't test whether similar gains could be achieved through simpler prompt engineering without formal plan induction
- The theoretical connection to options framework lacks empirical validation
- Plan component importance isn't fully characterized through ablation analysis

## Confidence

- **High**: Cross-model planning reduces self-bias; empirical results are robust across different LLM backbones
- **Medium**: Trajectory-to-plan abstraction improves performance; mechanism is plausible but incomplete ablation analysis
- **Low**: Plan-as-option framework meaningfully improves grounding; theoretical contribution lacks empirical validation

## Next Checks

1. **Plan Component Ablation**: Run SAGE with only Analysis, only Feedback, and only Induced Plan sections enabled separately to quantify each component's contribution to performance gains.

2. **Minimum Viable Trajectory Length**: Systematically truncate exploration trajectories at 25%, 50%, and 75% of full length before plan induction to identify the shortest trajectory that still yields meaningful plan quality.

3. **Cross-Framework Plan Transfer**: Apply SAGE's plan format to a different agent framework (e.g., AutoCodeRover or another SWE-Bench agent) with minimal reformatting to test whether plan structure is framework-agnostic.