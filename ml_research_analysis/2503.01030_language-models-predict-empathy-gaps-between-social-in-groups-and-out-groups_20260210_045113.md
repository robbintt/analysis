---
ver: rpa2
title: Language Models Predict Empathy Gaps Between Social In-groups and Out-groups
arxiv_id: '2503.01030'
source_url: https://arxiv.org/abs/2503.01030
tags:
- social
- prompt
- emotion
- group
- person
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether large language models (LLMs) replicate
  human-like empathy gaps between social in-groups and out-groups. The authors formulate
  an emotion intensity prediction task where an LLM, assigned a perceiver social identity,
  predicts the intensity of emotion felt by an experiencer with a given social identity.
---

# Language Models Predict Empathy Gaps Between Social In-groups and Out-groups

## Quick Facts
- arXiv ID: 2503.01030
- Source URL: https://arxiv.org/abs/2503.01030
- Reference count: 33
- Key outcome: LLMs consistently assign higher emotion intensity scores to in-group members than out-group members across race, nationality, and religion.

## Executive Summary
This study investigates whether large language models (LLMs) replicate human-like empathy gaps between social in-groups and out-groups. The authors formulate an emotion intensity prediction task where an LLM, assigned a perceiver social identity, predicts the intensity of emotion felt by an experiencer with a given social identity. By manipulating perceiver and experiencer identities across race/ethnicity, nationality, and religion, the study measures how predicted emotion intensities differ between in-group and out-group settings. Results show that LLMs consistently assign higher emotion intensity scores to in-group members than out-group members across all three social grouping categories, with Llama-3.1-8B exhibiting the strongest intergroup bias.

## Method Summary
The study uses emotion intensity prediction tasks with LLMs assigned social identities (perceivers) who rate narratives from other social identities (experiencers). Using the crowd-enVENT corpus (6,050 events), researchers manipulate perceiver-experiencer pairings across race/ethnicity (18 variations), nationality (21 countries), and religion (5 religions). They test different prompt variations (persona strength, scale granularity, narrative perspective) and measure the empathy gap score (δ), which quantifies the difference between in-group and out-group emotion intensity predictions. Results are analyzed through matrix-based bias analysis with z-score normalization and permutation testing for statistical significance.

## Key Results
- LLMs consistently predict higher emotion intensities for in-group members than out-group members across all social categories (race, nationality, religion).
- Prompt variations affect the magnitude of bias but not its direction, with stricter persona prompts amplifying the effect.
- Specific group pairings (e.g., Palestine-Israel, Ukraine-Russia) show lower empathy scores, reflecting learned historical conflicts from training data.

## Why This Works (Mechanism)

### Mechanism 1
LLMs, when assigned a social group persona ("perceiver"), predict higher emotion intensity scores for individuals ("experiencers") who share the same social group identity (in-group) compared to those with different identities (out-group). The model's training data contains vast amounts of human-generated text reflecting intergroup psychology, including "ingroup favoritism" and "intergroup empathy bias." A persona prompt activates latent representations associated with that identity's typical perspectives, biasing the prediction task.

### Mechanism 2
The magnitude of intergroup empathy bias in LLM predictions is sensitive to how strongly the persona is enforced in the prompt, but the direction of bias (in-group > out-group) remains consistent across variations. Prompt engineering that explicitly instructs the model to "strictly" follow the persona or be "critical" about characteristics strengthens the association between the persona identity and biased behavioral patterns in the model's latent space.

### Mechanism 3
Specific group pairings (e.g., Palestine-Israel, Ukraine-Russia) exhibit lower empathy scores compared to other out-group pairings, reflecting learned historical and geopolitical conflicts from the training corpus. Training data contains disproportionate text describing conflict and hostility between specific groups (e.g., news reports, political commentary), creating stronger negative associations in the model's embedding space that activate during prediction.

## Foundational Learning

- **Concept: Intergroup Empathy Bias (Social Psychology)**
  - **Why needed here:** The entire paper operationalizes this psychological concept, which posits humans empathize more with in-group than out-group members. Understanding it is required to interpret LLM behavior as replicating human bias.
  - **Quick check question:** A person reads a sad story about someone from their own country vs. a rival country. According to intergroup empathy bias, whose sadness would they rate as more intense?

- **Concept: Persona Prompting in LLMs**
  - **Why needed here:** The methodology depends on assigning identity to the LLM via prompts. The "perceiver" is a model-adopted persona. Without understanding how persona prompts condition outputs, the experimental setup is unclear.
  - **Quick check question:** What is the function of the system prompt "You are a white person" in this experiment's context?

- **Concept: Matrix-based Bias Analysis (Perceiver × Experiencer)**
  - **Why needed here:** Results are analyzed via matrix `M` where rows are perceivers and columns are experiencers. Diagonal (in-group) vs. off-diagonal (out-group) comparisons yield the "empathy gap score" (`δ`). Understanding this framework is crucial for grasping measurement methodology.
  - **Quick check question:** In the analysis matrix, where would you find the emotion intensity predicted by a "Christian perceiver" for a "Muslim experiencer"?

## Architecture Onboarding

- **Component map:**
  - Prompt Generator (`mk_prompt`) -> LLM Inference Engine -> Data Corpus (`crowd-enVENT`) -> Analysis Pipeline

- **Critical path:**
  1. Select perceiver-experiencer identity pair from defined social groups
  2. Generate prompt with persona instruction + experiencer narrative
  3. Run LLM inference at temperature 0 to predict intensity score
  4. Aggregate scores across 6,050 events → compute `M` matrix
  5. Calculate `δ` and run structured permutation test (10k permutations) for statistical significance

- **Design tradeoffs:**
  - Scale granularity: 0-100 offers finer distinctions than 0-10 but increases variance
  - Persona enforcement strength: Stricter persona prompts increase bias magnitude but may trigger higher refusal rates
  - Narrative perspective: First-person vs. third-person rewrites affect variance

- **Failure signatures:**
  - High refusal rates (43-54% for strict personas with Llama-3.1-8B)
  - Inconsistent unspecified baseline (Llama-3.1-8B shows higher intensity for unspecified identities)
  - Scale compression with 0-10 scale reducing nuanced differences

- **First 3 experiments:**
  1. Replicate baseline bias: Run default prompt (P0, S0, T0) on Llama-3.1-8B for nationality category; compute δ to verify z-score gap (~2.40)
  2. Test prompt sensitivity: Compare δ across persona prompt variations (P0 vs. P2) on same event subset to confirm magnitude change
  3. Probe historical conflict pair: Focus on Palestine perceiver + Israel experiencer subset vs. neutral nationality pair; verify lower average intensity scores

## Open Questions the Paper Calls Out

- **Question:** Where do LLMs acquire intergroup empathy bias—from training data, model architecture, or post-training procedures?
  - **Basis:** "We need to study where they learn the intergroup bias so we can intervene the downstream decision-making tasks" (Page 9)
  - **Why unresolved:** The paper measures bias magnitude across models but does not investigate its developmental origins or acquisition mechanisms.

- **Question:** How does intergroup empathy bias manifest when individuals hold multiple, intersecting social identities (e.g., both racial and national group membership)?
  - **Basis:** "Groups involving multiple categories have also not been studied. It is common for a person to identify with both racial and national groups" (Page 9, Limitations)
  - **Why unresolved:** The study assigns only single-category identities to perceiver and experiencer personas, leaving intersectional dynamics unexplored.

- **Question:** Does a group's relative social power or status systematically influence the magnitude or direction of empathy bias toward out-groups?
  - **Basis:** The paper asks "if a group's relative social power plays a role" after documenting asymmetric patterns without explanation (Page 7)
  - **Why unresolved:** Group-level asymmetries are documented but remain correlational; causal mechanisms involving power dynamics are untested.

## Limitations
- High refusal rates (43-54%) for Llama-3.1-8B with strict persona prompts may bias results by excluding certain identity perspectives.
- The analysis relies on z-score normalization within matrices, making absolute intensity comparisons across categories problematic.
- Only crowd-enVENT corpus (Western-centric) was used, limiting generalizability to non-Western emotional contexts.

## Confidence
- **High confidence**: LLMs consistently predict higher emotion intensities for in-group versus out-group members across all tested social categories.
- **Medium confidence**: The claim that specific historical conflicts produce lower empathy scores is supported but requires stronger causal evidence.
- **Medium confidence**: The mechanism that persona prompts activate latent intergroup bias representations is plausible but relies on indirect evidence.

## Next Checks
1. **Refusal rate analysis**: Systematically compare emotion intensity predictions from Llama-3.1-8B using default prompts (P0) versus strict prompts (P2/P3) while controlling for refusal rates.
2. **Cross-cultural validation**: Test the same methodology with LLMs trained on non-Western datasets to determine whether observed intergroup empathy gaps are universal or culturally specific.
3. **Temporal stability assessment**: Replicate the analysis using sequential snapshots of model weights to determine whether empathy gap magnitudes change as models are updated.