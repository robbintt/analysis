---
ver: rpa2
title: Addressing Personalized Bias for Unbiased Learning to Rank
arxiv_id: '2508.20798'
source_url: https://arxiv.org/abs/2508.20798
tags:
- user
- users
- user-aware
- estimator
- examination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses personalized bias in unbiased learning to
  rank (ULTR) by recognizing that existing methods assume "average" users, ignoring
  individual differences in search and browsing behaviors. The authors introduce a
  user-aware causal framework that models how user-specific query preferences and
  examination propensities create backdoor paths affecting click data.
---

# Addressing Personalized Bias for Unbiased Learning to Rank

## Quick Facts
- arXiv ID: 2508.20798
- Source URL: https://arxiv.org/abs/2508.20798
- Authors: Zechun Niu; Lang Mei; Liu Yang; Ziyuan Zhao; Qiang Yan; Jiaxin Mao; Ji-Rong Wen
- Reference count: 40
- Primary result: User-aware causal framework addresses personalized bias in ULTR by modeling user-specific query preferences and examination propensities, outperforming existing methods

## Executive Summary
This paper identifies personalized bias in unbiased learning to rank (ULTR) systems, where existing methods assume "average" users and fail to account for individual differences in search and browsing behaviors. The authors introduce a user-aware causal framework that models how user-specific query preferences and examination propensities create backdoor paths in the causal graph, leading to biased click data. They propose a novel user-aware inverse-propensity-score estimator that aggregates user-weighted examination probabilities per query rather than per session, theoretically proving it is unbiased and has lower variance than straightforward user-specific methods.

## Method Summary
The method involves three key components: (1) estimating user distributions P(u|q) by counting user frequencies per query in logs, (2) estimating personalized examination propensities P(e=1|k,u) via personalized Regression-EM or clustering algorithms, and (3) computing propensity as the weighted sum Σ_u P(e=1|k,u)P(u|q) for each query-document pair. The ranking model is then trained using an IPS-weighted loss function. The approach was validated on Yahoo! LETOR, Baidu-ULTR, and real-world datasets with 1 million simulated sessions across 10 user clusters with varying examination parameters.

## Key Results
- User-aware estimator significantly outperforms existing ULTR methods and straightforward user-specific estimators in mitigating personalized bias
- Theoretical proof demonstrates the user-aware estimator is unbiased with lower variance than straightforward methods
- Extensive experiments show consistent improvements across nDCG@1/3/5/10 and ERR@1/3/5/10 metrics on both semi-synthetic and real-world datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Standard ULTR methods fail when user behavior affects both query selection and document examination because this creates an unblocked "backdoor path" in the causal graph.
- **Mechanism**: The user variable u creates a spurious correlation path: e ← u → q → x → r → c. Standard IPS-PBM ignores u, failing to block this path and resulting in "personalized bias."
- **Core assumption**: Users exhibit heterogeneous behaviors in both the queries they issue and how they scan search results.
- **Break condition**: If users have personalized browsing behaviors but do not have personalized query behaviors, the backdoor path is broken.

### Mechanism 2
- **Claim**: Aggregating examination propensities at the query level reduces estimator variance without introducing bias.
- **Mechanism**: A straightforward approach conditions propensity on specific users (P(e|k, u)), causing high variance when users have low examination probabilities. The user-aware estimator aggregates propensities based on user distribution per query (Σ P(e|k,u)P(u|q)), smoothing out extreme weights.
- **Core assumption**: The distribution of users issuing a specific query can be accurately estimated from logged behavior data.
- **Break condition**: If training data is too sparse to reliably estimate user distributions per query.

### Mechanism 3
- **Claim**: Personalized examination propensities can be estimated offline by clustering users based on browsing behaviors.
- **Mechanism**: The authors use "personalized Regression-EM" to estimate user-specific examination parameters by identifying clusters of users with similar scanning patterns.
- **Core assumption**: User browsing behavior is consistent enough to be modeled by clusters rather than being purely random.
- **Break condition**: If user behavior changes drastically per session or is inconsistent.

## Foundational Learning

- **Concept: Inverse Propensity Scoring (IPS)**
  - **Why needed here**: This is the fundamental mathematical tool used to "de-bias" training data by weighting clicked documents by the inverse of examination probability.
  - **Quick check question**: If a document at rank 5 is clicked with examination probability 0.1, what weight does IPS assign? (Answer: 10)

- **Concept: The Backdoor Criterion (Causal Inference)**
  - **Why needed here**: The paper frames bias as a causal graph problem where backdoor paths create spurious correlations.
  - **Quick check question**: In the causal graph u → e and u → q, why can't we simply ignore u when estimating the causal effect of relevance r on click c?

- **Concept: Variance-Bias Tradeoff**
  - **Why needed here**: The paper argues against straightforward user-specific estimators not because they're biased, but because they have high variance.
  - **Quick check question**: Why might an IPS estimator with very large weights (e.g., 1/0.001) lead to unstable model training?

## Architecture Onboarding

- **Component map**: User Profiler -> Personalized Propensity Estimator -> Loss Estimator -> Ranking Model
- **Critical path**:
  1. Offline: Estimate user distributions (P(u|q)) and personalized examination probabilities (P(e|k,u))
  2. Training: Compute user-aware propensities for each query-document pair in logs
  3. Optimization: Train ranker using weighted loss function
- **Design tradeoffs**:
  - Granularity vs. Data Sparsity: Modeling every user individually provides accuracy but suffers from extreme data sparsity
  - Complexity: User-aware method adds preprocessing overhead but requires no inference changes
- **Failure signatures**:
  - Exploding Gradients: NaN losses due to massive IPS weights from users with low examination propensity
  - Stagnant Metrics: Standard IPS-PBM plateaus lower than expected on heterogeneous user bases
- **First 3 experiments**:
  1. Compare Standard IPS vs. User-aware IPS while varying user behavior heterogeneity
  2. Track IPS weight distributions to verify user-aware estimator produces tighter distributions
  3. Test different numbers of user clusters (5 vs. 20) to measure impact on nDCG

## Open Questions the Paper Calls Out
- **Open Question 1**: How can the causal framework be extended to handle scenarios where users have personalized relevance judgments rather than shared relevance?
- **Open Question 2**: Is the user-aware estimator robust when users follow heterogeneous click models rather than a single model with personalized parameters?
- **Open Question 3**: How does the method perform in cold-start scenarios with severe data sparsity?

## Limitations
- Theoretical proofs rely on assumptions about user-query distributions that may not hold in practice
- Personalized Regression-EM implementation details are not fully specified, raising reproducibility concerns
- Performance on rare queries with limited user interaction data remains untested

## Confidence
- **High Confidence**: Identification of personalized bias as a real problem and basic framework formulation
- **Medium Confidence**: Mathematical proof of unbiasedness and variance reduction claims
- **Medium Confidence**: Experimental results showing performance improvements

## Next Checks
1. Apply user-aware estimator to production search log with diverse user demographics to measure real-world performance gains
2. Compare variance of IPS weights across standard IPS-PBM, straightforward user-specific, and user-aware estimators on real click data
3. Test estimator performance on rare queries with limited user interaction data to assess cold-start robustness