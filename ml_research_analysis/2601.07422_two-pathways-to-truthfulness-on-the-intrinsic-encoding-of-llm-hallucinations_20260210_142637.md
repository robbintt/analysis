---
ver: rpa2
title: 'Two Pathways to Truthfulness: On the Intrinsic Encoding of LLM Hallucinations'
arxiv_id: '2601.07422'
source_url: https://arxiv.org/abs/2601.07422
tags:
- a-anchored
- q-anchored
- triviaqa
- popqa
- hotpotqa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies two distinct pathways through which large
  language models encode truthfulness signals: a Question-Anchored pathway that depends
  on question-answer information flow and an Answer-Anchored pathway that derives
  self-contained evidence from the generated answer. The authors validate these mechanisms
  through attention knockout and token patching experiments across 12 diverse models
  and 4 datasets.'
---

# Two Pathways to Truthfulness: On the Intrinsic Encoding of LLM Hallucinations

## Quick Facts
- arXiv ID: 2601.07422
- Source URL: https://arxiv.org/abs/2601.07422
- Reference count: 40
- Primary result: Identifies Q-Anchored and A-Anchored truthfulness encoding pathways in LLMs, achieving up to 10% AUC gains in hallucination detection.

## Executive Summary
This paper identifies two distinct pathways through which large language models encode truthfulness signals: a Question-Anchored pathway that depends on question-answer information flow and an Answer-Anchored pathway that derives self-contained evidence from the generated answer. The authors validate these mechanisms through attention knockout and token patching experiments across 12 diverse models and 4 datasets. They discover that Q-Anchored encoding predominates for well-established facts within the model's knowledge boundary, while A-Anchored encoding is favored for long-tail cases. Internal representations can also distinguish which mechanism is being employed, suggesting intrinsic self-awareness. Based on these findings, the authors propose two pathway-aware detection methods—Mixture-of-Probes and Pathway Reweighting—that leverage these complementary mechanisms to improve hallucination detection.

## Method Summary
The authors develop a framework for analyzing LLM truthfulness encoding through attention knockout interventions and linear probing on hidden states. They extract exact question and answer tokens using GPT-4o, then systematically block attention from question tokens to answer positions to identify pathway dependence. Two pathway-aware detection methods are proposed: Mixture-of-Probes (MoP) which routes to specialized expert probes based on a self-awareness gate, and Pathway Reweighting (PR) which rescales attention weights during inference. Experiments span 12 LLMs including Llama-3.1, Mistral-7B, and Qwen3 models across 4 QA datasets with 2,000 train/test samples each.

## Key Results
- Q-Anchored signals rely on question-derived cues while A-Anchored signals are robust to their removal
- Pathway classification accuracy ranges from 75-93% AUC across models and datasets
- MoP achieves up to 5% AUC improvement over baseline probe detection
- PR provides plug-and-play enhancement without retraining the underlying LLM
- Q-Anchored encoding predominates for well-established facts while A-Anchored is favored for long-tail cases

## Why This Works (Mechanism)

### Mechanism 1: Question-Anchored Truthfulness Encoding
Truthfulness signals for well-established facts emerge through attention-mediated information flow from exact question tokens to answer token representations. The model retrieves stored knowledge about the subject-property pair during forward passes, with attention heads carrying semantic dependencies from question tokens to answer positions. When this pathway is blocked via attention knockout, probe predictions shift substantially, indicating the truthfulness signal relies on this question-to-answer information transfer. This mechanism breaks when the model lacks stored knowledge of the entity or when blocking attention from exact question tokens produces <5% prediction change.

### Mechanism 2: Answer-Anchored Truthfulness Encoding
Hallucination detection can operate using only internal patterns within the generated answer, independent of question context. The model encodes self-consistency or plausibility signals within its output representation—detecting linguistic anomalies, logical gaps, or confidence mismatches. When questions are entirely removed, A-Anchored probes maintain stable predictions, confirming self-contained evidence extraction. This mechanism breaks when answers are extremely short (<3 tokens) or lack sufficient internal structure for consistency analysis.

### Mechanism 3: Intrinsic Pathway Self-Awareness
LLM hidden states encode discriminative information about which encoding pathway (Q-Anchored or A-Anchored) is active, enabling pathway-aware intervention. A linear probe trained on intermediate representations can predict the pathway type with high accuracy (75-93% AUC). This gating signal enables Mixture-of-Probes routing or Pathway Reweighting attention scaling. This mechanism breaks when representation extraction uses suboptimal layers or when probe training data has pathway label noise.

## Foundational Learning

- **Attention knockout intervention**: Core technique for isolating Q-Anchored vs A-Anchored contributions by selectively blocking information flow from exact question tokens. Quick check: If you set attention weights $A_l(i, E_Q) = 0$ for all layers $l \leq k$ and positions $i > E_Q$, what happens to the representation at position $i$?

- **Linear probing on hidden states**: All detection methods (baseline probe, expert probes, self-awareness gate) are linear classifiers on intermediate activations. Quick check: Why might a linear probe suffice for extracting truthfulness from a 4096-dim hidden state?

- **Semantic frame theory / exact tokens**: Identifying core frame elements (exact subject, property, answer tokens) is critical—interventions on non-exact tokens produce negligible effects. Quick check: In "What is the capital of South Carolina?", which tokens are core frame elements vs. peripheral context?

## Architecture Onboarding

- **Component map**: GPT-4o token extraction -> Hidden state extraction from attention/MLP outputs -> Self-awareness probe (binary pathway classification) -> Expert probes (Q-Anchored and A-Anchored) -> MoP gating combination OR PR attention rescaling -> Hallucination probability output

- **Critical path**: 1) Parse QA pair → identify exact tokens (subject, property, answer) 2) Forward pass → extract $h_{l^*}(x)$ at best-performing layer $l^*$ 3) Self-awareness probe → compute $\pi(h_{l^*})$ 4a) (MoP) Combine expert probe outputs via gating 4b) (PR) Rescale attention $A_l(i, E_Q)$ using $s(h_{l^*}) = \pi_Q \alpha_l^Q - (1-\pi_Q) \alpha_l^A$ 5) Output hallucination probability

- **Design tradeoffs**: MoP provides higher accuracy gains (+3-5% AUC) but requires multiple trained probes and has no inference-time modification; PR is plug-and-play and lightweight (~2 params/layer/head) but modifies attention at detection time only; layer selection varies by model scale and architecture.

- **Failure signatures**: MoP-RandomGate ≈ MoP indicates gating network not learning (check self-awareness probe training); both pathways show similar ΔP under knockout suggests exact token extraction failing or token choice incorrect; PR degrades performance indicates attention rescaling disrupting non-target information flows (reduce α magnitude).

- **First 3 experiments**: 1) Replicate attention knockout: Compare ΔP when blocking exact question tokens vs. random question tokens (expect bimodal separation: Q-Anchored high, A-Anchored near zero) 2) Train self-awareness probe: Verify pathway classification AUC > 75% on validation split before using for gating 3) Ablate MoP: Compare full MoP vs. MoP-RandomGate vs. MoP-VanillaExperts (confirm both specialization and gating contribute independently)

## Open Questions the Paper Calls Out

- **Black-box adaptation**: Can pathway-aware detection methods be adapted for strictly black-box settings where internal representations are inaccessible? The proposed MoP and PR methods fundamentally depend on hidden state access for gating and reweighting; no approximation strategy is explored. Demonstrating that proxy signals (e.g., output logits, consistency across samples, or API-level confidence scores) can approximate pathway identification with comparable detection performance would resolve this.

- **Pretraining vs fine-tuning emergence**: How do Q-Anchored and A-Anchored pathways emerge during pretraining versus instruction-tuning? The paper observes consistent pathway patterns across base, instruction-tuned, and reasoning models, but doesn't investigate whether these mechanisms arise from pretraining or are shaped by subsequent fine-tuning stages. Layer-wise pathway analysis at checkpoints throughout pretraining and fine-tuning, tracking when the bimodal saliency distribution first appears, would provide evidence.

- **Generalization to non-QA tasks**: Do the two pathways generalize to non-QA tasks such as summarization, translation, or code generation? All experiments use question-answering datasets. The "question-to-answer" framing may not transfer directly to tasks without explicit question-answer structure. Evaluating attention knockout and token patching on hallucination detection benchmarks for summarization or code generation, assessing whether analogous pathways exist, would resolve this.

## Limitations

- The pathway detection mechanism relies heavily on exact token extraction via GPT-4o, but the paper doesn't validate whether this extraction is consistently accurate across all 12 models and 4 datasets.

- The attention knockout intervention assumes that blocking exact question tokens reveals pathway dependence, but this doesn't account for models that may encode semantic rather than exact token-level dependencies.

- The claim that Q-Anchored encoding is inherently tied to well-established facts while A-Anchored handles long-tail cases is based on correlation rather than causal manipulation of the model's knowledge boundary.

## Confidence

- **High Confidence**: The existence of two distinguishable encoding pathways (Q-Anchored vs A-Anchored) and the effectiveness of attention knockout as an intervention technique.
- **Medium Confidence**: The claim that pathway selection is systematically encoded in hidden states (self-awareness probe performance varies considerably).
- **Low Confidence**: The assertion that Q-Anchored encoding is inherently tied to well-established facts while A-Anchored handles long-tail cases—this requires systematic manipulation of model knowledge boundaries.

## Next Checks

1. **Token Extraction Validation**: Manually verify exact token extraction accuracy on 100 random samples across multiple models to confirm the preprocessing assumption isn't introducing artifacts.

2. **Knowledge Boundary Manipulation**: Systematically test pathway dominance by varying question difficulty from well-known facts to obscure queries, measuring actual knowledge boundary effects rather than dataset correlations.

3. **Alternative Dependency Encoding**: Replace exact token knockout with semantic similarity-based attention blocking to test whether the Q-Anchored mechanism depends on exact tokens versus semantic content.