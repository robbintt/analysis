---
ver: rpa2
title: 'Think in Games: Learning to Reason in Games via Reinforcement Learning with
  Large Language Models'
arxiv_id: '2508.21365'
source_url: https://arxiv.org/abs/2508.21365
tags:
- game
- language
- learning
- games
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of enabling large language models
  (LLMs) to develop procedural knowledge for interactive tasks by bridging the gap
  between declarative knowledge and procedural understanding. The proposed Think in
  Games (TiG) framework reformulates reinforcement learning (RL) as a language modeling
  task, allowing LLMs to generate language-guided policies that are refined through
  direct interaction with game environments.
---

# Think in Games: Learning to Reason in Games via Reinforcement Learning with Large Language Models

## Quick Facts
- arXiv ID: 2508.21365
- Source URL: https://arxiv.org/abs/2508.21365
- Reference count: 40
- Primary result: TiG framework achieves competitive performance with significantly lower data and computational demands compared to conventional RL methods

## Executive Summary
This paper introduces the Think in Games (TiG) framework, which addresses the challenge of enabling large language models to develop procedural knowledge for interactive tasks. By reformulating reinforcement learning as a language modeling task, TiG allows LLMs to generate language-guided policies that are refined through direct interaction with game environments. The framework successfully bridges the gap between declarative knowledge and procedural understanding while maintaining LLMs' reasoning and explanatory abilities.

The proposed approach is validated in the Honor of Kings game environment, where it demonstrates superior efficiency and interpretability compared to conventional RL methods. Notably, smaller models like Qwen-3-14B trained with TiG achieve 90.91% accuracy, outperforming larger models such as Deepseek-R1 (86.67%). The framework also provides step-by-step natural language explanations for its decisions, enhancing transparency in complex interactive tasks.

## Method Summary
The Think in Games framework reformulates reinforcement learning as a language modeling task, allowing LLMs to generate language-guided policies that are refined through direct interaction with game environments. The approach combines the reasoning capabilities of LLMs with experiential learning from environment interactions. The framework generates natural language explanations for decisions while learning procedural knowledge through gameplay, creating a bridge between declarative and procedural understanding.

## Key Results
- TiG achieves competitive performance with significantly lower data and computational demands compared to conventional RL methods
- Smaller models like Qwen-3-14B trained with TiG achieve 90.91% accuracy, outperforming larger models such as Deepseek-R1 (86.67%)
- The framework provides step-by-step natural language explanations for decisions, enhancing transparency and interpretability

## Why This Works (Mechanism)
TiG works by reformulating reinforcement learning as a language modeling task, allowing LLMs to leverage their existing reasoning and explanation capabilities while learning through direct environmental interaction. The framework treats policy generation as a language generation problem, where the LLM produces language-guided actions that are then refined through experience. This approach maintains the interpretability of LLMs while grounding them in procedural learning through gameplay.

## Foundational Learning
- **Reinforcement Learning basics**: Understanding reward maximization and policy optimization is essential for grasping how TiG reframes RL as language modeling. Quick check: Verify understanding of Q-learning and policy gradients.
- **Language modeling fundamentals**: Knowledge of transformer architectures and sequence generation is needed to understand how LLMs generate language-guided policies. Quick check: Review attention mechanisms and next-token prediction.
- **Procedural vs declarative knowledge**: Distinguishing between knowing what versus knowing how is crucial for understanding TiG's bridging approach. Quick check: Compare examples of declarative rules versus procedural skills.
- **Game environment interaction**: Understanding how agents perceive and act within game environments is necessary for grasping the experiential learning component. Quick check: Examine state-action-reward cycles in simple games.

## Architecture Onboarding

**Component Map:**
LLM -> Language Policy Generator -> Game Environment -> Reward Signal -> LLM Update

**Critical Path:**
The critical path involves the LLM generating language-guided actions, executing them in the game environment, receiving rewards, and updating its policy through language modeling. This loop enables continuous refinement of both procedural knowledge and explanatory capabilities.

**Design Tradeoffs:**
- Language-guided policies vs direct action generation: Maintains interpretability but may introduce latency
- Integration of reasoning and experiential learning: Balances declarative and procedural knowledge acquisition
- Model size efficiency: Smaller models can outperform larger ones with appropriate training framework

**Failure Signatures:**
- Degraded performance when environment complexity exceeds language modeling capacity
- Loss of interpretability if policy generation becomes too procedural
- Potential mismatch between generated explanations and actual decision-making process

**First Experiments to Run:**
1. Baseline comparison: Evaluate TiG against standard RL methods on simple grid-world environments
2. Explanation quality test: Assess the coherence and accuracy of generated explanations across different game scenarios
3. Scalability assessment: Test performance degradation as environment complexity increases

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation confined to single game environment (Honor of Kings), limiting generalizability across diverse interactive domains
- Potential scalability challenges when applying to more complex environments with higher-dimensional state spaces
- Limited evaluation of reasoning and explanatory capabilities outside the specific game context

## Confidence
- **High Confidence**: The core technical contribution of reformulating RL as a language modeling task is well-articulated with sufficient implementation details and supported comparative performance metrics.
- **Medium Confidence**: Claims about smaller models outperforming larger models are based on specific experimental conditions and may not generalize across different tasks or evaluation metrics.
- **Low Confidence**: Assertions about retaining reasoning and explanatory abilities while grounding in experiential learning lack comprehensive evaluation of real-world application transferability.

## Next Checks
1. Cross-Domain Generalization: Evaluate TiG's performance across multiple game environments and non-game interactive tasks (e.g., robotics control, dialogue systems) to assess generalizability and identify domain-specific limitations.

2. Scalability Analysis: Test TiG with increasingly complex environments featuring higher-dimensional state spaces, longer time horizons, and more intricate reward structures to determine scalability boundaries and identify bottlenecks.

3. Long-Term Reasoning Evaluation: Conduct experiments measuring the framework's ability to maintain coherent reasoning and policy consistency over extended interaction sequences, particularly in scenarios requiring strategic planning and adaptation to changing environmental conditions.