---
ver: rpa2
title: 'GeoPQA: Bridging the Visual Perception Gap in MLLMs for Geometric Reasoning'
arxiv_id: '2509.17437'
source_url: https://arxiv.org/abs/2509.17437
tags:
- reasoning
- training
- perception
- geometric
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper identifies a fundamental perceptual bottleneck in multimodal
  large language models (MLLMs) that limits their performance on vision-intensive
  reasoning tasks like geometric problem solving. To address this, the authors introduce
  a two-stage reinforcement learning framework: first training the model on perception-focused
  question-answering tasks to enhance visual understanding of geometric concepts,
  then training on reasoning tasks to develop complex multi-step logical deductions.'
---

# GeoPQA: Bridging the Visual Perception Gap in MLLMs for Geometric Reasoning

## Quick Facts
- **arXiv ID:** 2509.17437
- **Source URL:** https://arxiv.org/abs/2509.17437
- **Reference count:** 17
- **Primary result:** 9.7% improvement in geometric reasoning and 9.1% in geometric problem solving using a two-stage RL framework

## Executive Summary
Multimodal Large Language Models (MLLMs) struggle with vision-intensive reasoning tasks like geometric problem solving due to perceptual limitations in understanding visual geometric concepts. GeoPQA addresses this by implementing a two-stage reinforcement learning framework that first trains models on perception-focused tasks to enhance visual understanding of geometry, then trains on reasoning tasks to develop complex logical deductions. When applied to Qwen2.5-VL-3B-Instruct, this approach achieves state-of-the-art results among open-source models and demonstrates generalization to other vision-intensive domains such as figure and textbook understanding.

## Method Summary
GeoPQA introduces a novel two-stage reinforcement learning approach that tackles the perceptual bottleneck in MLLMs. The first stage focuses on perception-oriented question-answering tasks, training the model to better understand and interpret geometric visual concepts. The second stage transitions to reasoning-oriented tasks, developing the model's ability to perform multi-step logical deductions required for complex geometric problem solving. This sequential training strategy systematically builds both visual perception and reasoning capabilities, addressing the fundamental limitation that direct reasoning training alone cannot overcome in vision-intensive tasks.

## Key Results
- 9.7% improvement in geometric reasoning accuracy compared to direct reasoning training
- 9.1% improvement in geometric problem solving performance
- Achieves state-of-the-art results among open-source MLLM models
- Demonstrates successful generalization to other vision-intensive domains like figure and textbook understanding

## Why This Works (Mechanism)
The effectiveness of GeoPQA stems from addressing a fundamental perceptual bottleneck in MLLMs. By first training on perception-focused tasks, the model develops stronger visual understanding of geometric concepts before attempting complex reasoning. This staged approach allows the model to build robust feature representations for geometric elements (shapes, angles, relationships) before applying logical deduction processes. The sequential training ensures that visual perception capabilities are sufficiently developed to support downstream reasoning tasks, creating a foundation that direct reasoning training alone cannot establish.

## Foundational Learning
- **Geometric Visual Concepts**: Understanding shapes, angles, spatial relationships, and geometric properties is essential for interpreting visual geometry problems. Quick check: Can the model identify and describe geometric elements in complex diagrams?
- **Reinforcement Learning for Vision Tasks**: RL provides a framework for optimizing model behavior through reward-based learning, particularly effective for sequential decision-making in perception and reasoning. Quick check: Does the reward structure appropriately balance perception accuracy and reasoning quality?
- **Multimodal Fusion**: Combining visual and textual representations effectively is critical for geometric reasoning that requires both image understanding and logical deduction. Quick check: Are visual features properly integrated with language representations in the model architecture?
- **Geometric Problem-Solving Strategies**: Understanding how humans approach geometric problems provides insight into the reasoning patterns that should be modeled. Quick check: Does the model's reasoning trace follow logical geometric proof structures?
- **Vision-Language Pre-training**: The base MLLM's pre-training influences its ability to handle geometric tasks, affecting both perception and reasoning capabilities. Quick check: How does the model's performance vary with different pre-training strategies?

## Architecture Onboarding

**Component Map**: Visual Input -> Perception Stage (RL Training) -> Geometric Feature Extraction -> Reasoning Stage (RL Training) -> Answer Generation

**Critical Path**: The perception stage must successfully complete before the reasoning stage can be effective. Visual input processing and geometric feature extraction are bottlenecks that determine the quality of subsequent reasoning performance.

**Design Tradeoffs**: The staged approach trades computational efficiency for performance gains, requiring twice the training time but achieving significantly better results. The framework balances between perception accuracy and reasoning capability, with hyperparameters controlling the relative emphasis of each stage.

**Failure Signatures**: Poor performance indicates either inadequate visual feature extraction (perception stage failure) or inability to chain logical deductions (reasoning stage failure). Early stopping in the perception stage leads to fundamental limitations in handling complex geometric concepts.

**First Experiments**: 1) Ablation study comparing single-stage versus two-stage training to quantify the contribution of each component. 2) Transfer learning experiments testing performance on different geometric problem types. 3) Robustness testing across varying levels of geometric complexity and visual noise.

## Open Questions the Paper Calls Out
None

## Limitations
- Results are demonstrated only on Qwen2.5-VL-3B-Instruct, limiting generalizability to other MLLM architectures or model sizes
- The relative importance of perception versus reasoning stages and potential diminishing returns from sequential training remain unclear
- Evaluation metrics and dataset details are not fully specified, making it difficult to assess robustness across different geometric reasoning tasks

## Confidence
- Perception bottlenecks are a fundamental limitation for MLLMs in vision-intensive tasks: **High**
- Two-stage RL approach outperforms direct reasoning training: **Medium**
- Generalization to other vision-intensive domains: **Low**

## Next Checks
1. Test the framework across multiple MLLM architectures (e.g., LLaVA, InternVL, Gemini) to establish generalizability beyond Qwen2.5-VL-3B-Instruct
2. Conduct detailed ablation studies isolating the perception and reasoning stages to quantify their individual contributions and identify optimal training ratios
3. Evaluate performance on diverse geometric reasoning datasets with varying complexity levels to determine whether improvements hold for both elementary and advanced mathematical problems