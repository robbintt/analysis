---
ver: rpa2
title: Agentic AI framework for End-to-End Medical Data Inference
arxiv_id: '2507.18115'
source_url: https://arxiv.org/abs/2507.18115
tags:
- data
- agent
- preprocessing
- agents
- clinical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an Agentic AI framework that automates the
  entire clinical data pipeline from ingestion to inference, addressing the high cost
  and labor intensity of deploying machine learning in healthcare. The system uses
  modular, task-specific agents to handle structured and unstructured data, enabling
  automatic feature selection, model selection, and preprocessing without manual intervention.
---

# Agentic AI framework for End-to-End Medical Data Inference

## Quick Facts
- arXiv ID: 2507.18115
- Source URL: https://arxiv.org/abs/2507.18115
- Reference count: 40
- Key outcome: Automates clinical data pipeline from ingestion to inference using modular agents, reducing expert intervention and enabling scalable AI deployment in healthcare.

## Executive Summary
This paper introduces an Agentic AI framework that automates the entire clinical data pipeline from ingestion to inference, addressing the high cost and labor intensity of deploying machine learning in healthcare. The system uses modular, task-specific agents to handle structured and unstructured data, enabling automatic feature selection, model selection, and preprocessing without manual intervention. Evaluated on publicly available datasets from geriatrics, palliative care, and colonoscopy imaging, the framework demonstrates its ability to autonomously process diverse clinical data types. Key agents include the Ingestion Identifier Agent for file-type detection, the Data Anonymizer Agent for privacy compliance, the Feature Extraction Agent for semantic feature identification, and the Model-Data Matcher Agent for aligning data with appropriate AI models. The framework reduces the need for repeated expert intervention, offering a scalable and cost-efficient pathway for operationalizing AI in clinical environments.

## Method Summary
The framework uses seven specialized agents orchestrated by Google's Agent Development Kit to automate clinical ML pipelines. Structured data processing employs SapBERT embeddings to semantically match user-provided column headers against a curated model database, selecting appropriate machine learning models based on cosine similarity thresholds. Unstructured image data is processed through a two-stage MedGemma vision-language model approach that first identifies imaging modality then classifies disease category. The system handles both tabular and image data, with preprocessing recommendations based on metadata analysis and inference using domain-specific models like Perpetual Booster for tabular data and DETR for images. The architecture emphasizes modularity with explicit data contracts between agents, though it currently relies on cloud services for key functions and uses rule-based rather than learned preprocessing strategies.

## Key Results
- Successfully automates clinical data pipeline from ingestion to inference using modular task-specific agents
- Demonstrates semantic embedding-based matching for tabular data and vision-language inference for medical images
- Reduces need for repeated expert intervention in clinical ML deployment
- Handles diverse data types including structured tabular data from geriatrics and palliative care, plus colonoscopy imaging

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic embedding-based matching enables automated alignment between heterogeneous clinical data schemas and pretrained model requirements.
- Mechanism: Tabular column headers are encoded into 768-dimensional vectors using SapBERT (biomedical-pretrained). Cosine similarity compares user headers against model-required headers. A greedy matching scheme assigns each user column to at most one model header, with eligibility thresholded at similarity > 0.6. If all required headers match, the model becomes a candidate; an LLM agent then selects the best fit from candidates.
- Core assumption: Clinical column names share sufficient semantic overlap with biomedical literature terminology for SapBERT embeddings to capture meaningful similarity; non-standard naming degrades performance.
- Evidence anchors:
  - [abstract] "Feature Extraction Agent identifies features using an embedding-based approach for tabular data, extracting all column names"
  - [section IV, Model-Data Matcher Agent] "each header is first transformed into a fixed-length embedding vector (size=768) using the SapBERT model... similarity score exceeding a threshold (empirically set to 0.6)"
  - [corpus] No direct corpus validation for SapBERT-based model matching; AutoML-Med (arXiv:2508.02625) addresses tabular automation but uses different approaches
- Break condition: User feature names are ambiguous, non-standard, or semantically distant from known clinical terms → matching fails or selects suboptimal models (acknowledged in Section V Limitations).

### Mechanism 2
- Claim: Multi-stage vision-language inference enables modality-aware routing for unstructured medical images without manual labeling.
- Mechanism: A random anonymized image passes through MedGemma in stages: (1) classify imaging modality by matching visual features against a curated modality set; (2) given detected modality and model database captions, classify disease category. Confidence thresholds gate progression; low confidence triggers re-sampling or termination. Outputs (modality, disease-type) index into the model repository.
- Core assumption: MedGemma's medical vision-language pretraining transfers sufficiently to novel clinical imaging distributions; a single random image sample represents dataset-level characteristics.
- Evidence anchors:
  - [abstract] "multi-stage MedGemma-based approach for image data, which gives us the modality and disease name"
  - [section IV, Feature Extraction Agent] "agent first classifies the imaging 'Modality'... Following modality recognition, we further prompt MedGemma to classify the most likely 'Disease Category'"
  - [corpus] MedSAM-Agent (arXiv:2602.03320) and mAIstro (arXiv:2505.03785) demonstrate MLLM-based orchestration for medical imaging but use different architectures
- Break condition: Images fall outside MedGemma's training distribution (e.g., novel imaging protocols, rare pathologies) → modality or disease misclassification propagates to wrong model selection.

### Mechanism 3
- Claim: Modular agent specialization with explicit handoff protocols reduces integration complexity compared to monolithic AutoML pipelines.
- Mechanism: Seven specialized agents operate sequentially with structured data contracts: Ingestion Identifier outputs MIME type → Anonymizer outputs redacted data → Feature Extractor outputs "headers" (column names or modality/disease) → Matcher outputs selected model + filtered dataset → Preprocessing Recommender outputs transformation specs → Implementor applies transformations → Inference Agent runs model with explainability (SHAP/LIME/attention maps). Google's Agent Development Kit orchestrates workflow.
- Core assumption: Clinical data pipelines decompose cleanly into ordered stages with stable intermediate representations; edge cases requiring backtracking or parallel processing are rare.
- Evidence anchors:
  - [abstract] "system of modular, task-specific agents... automates the entire clinical data pipeline, from ingestion to inference"
  - [section IV, Agent Pipeline] "The complete workflow is orchestrated using Google's Agent Development Kit (ADK)"
  - [corpus] mAIstro (arXiv:2505.03785) uses similar multi-agent orchestration for radiomics; CP-Env (arXiv:2512.10206) evaluates LLMs on clinical pathways with staged environments
- Break condition: Complex cases requiring iterative refinement (e.g., failed anonymization requiring re-ingestion, preprocessing-recommender conflicts) → no documented feedback loops for agent-level recovery.

## Foundational Learning

- **Embedding-based Semantic Matching**
  - Why needed here: Core to how the framework aligns user data with models without explicit schema mapping. Understanding that SapBERT encodes biomedical semantics into vector space explains both the matching capability and its fragility with non-standard terminology.
  - Quick check question: If a dataset uses "BP_systolic" instead of "blood pressure systolic," would you expect higher or lower cosine similarity to a model expecting the latter? (Answer: Lower—semantic distance increases, risking threshold failure.)

- **Vision-Language Models for Medical Imaging**
  - Why needed here: MedGemma's role in inferring modality and disease from pixels requires understanding how multimodal models bridge visual features and textual descriptors. This is distinct from pure classification models.
  - Quick check question: Why does the framework use a two-stage prompt (modality → disease) rather than direct disease classification? (Answer: Hierarchical conditioning reduces search space and improves accuracy by constraining disease candidates to modality-relevant options.)

- **Agent Orchestration Patterns**
  - Why needed here: The framework's modularity depends on understanding how autonomous agents communicate, maintain state, and hand off data. Without this, the architecture appears as seven disconnected components rather than a coordinated system.
  - Quick check question: What happens if the Feature Extraction Agent outputs headers that no model in the database matches? (Answer: The Model-Data Matcher finds no eligible candidates; the paper does not specify fallback behavior—this is a documented limitation.)

## Architecture Onboarding

- **Component map:**
  User Upload → Ingestion Identifier (Magika MIME detection) → Data Anonymizer (Google Cloud DLP API) → Feature Extractor (SapBERT embeddings for tabular / MedGemma VLM for images) → Model-Data Matcher (cosine similarity + LLM selection) → Preprocessing Recommender (rule-based metadata analysis) → Preprocessing Implementor (applies transformations) → Model Inference (GBM for small tabular / DETR for images + SHAP/LIME/attention)

- **Critical path:** Feature Extraction → Model-Data Matching. If headers are incorrectly extracted or matched, all downstream steps (preprocessing, inference) operate on misaligned data. Validation here is essential before production deployment.

- **Design tradeoffs:**
  - **Cloud dependency vs. data sovereignty:** Google Cloud DLP and ADK provide robust infrastructure but violate data-locality requirements in some jurisdictions. The paper notes this as an unresolved limitation.
  - **Rule-based vs. learned preprocessing:** Preprocessing Recommender uses heuristics (e.g., "binary = exactly 2 unique values") rather than learning from outcomes. Faster to implement; less adaptive to edge cases.
  - **Single-sample image inference vs. dataset-level characterization:** MedGemma processes one random anonymized image to infer modality/disease for the entire dataset. Efficient but risks unrepresentative sampling.

- **Failure signatures:**
  - Model selection returns no candidates: Check if user headers have <0.6 cosine similarity to all model headers—likely non-standard naming.
  - Image modality misclassification: Verify image falls within MedGemma's supported modalities; check if confidence threshold is too permissive.
  - Preprocessing crashes on large datasets (>50 MB): System disables user-specified preprocessing and auto-selects—ensure default pipeline handles edge cases.

- **First 3 experiments:**
  1. **Header synonym robustness test:** Submit identical clinical datasets with varying column name conventions (e.g., "age" vs. "patient_age_years") to measure matching success rate and identify threshold sensitivity.
  2. **MedGemma modality boundary probe:** Feed images from modalities not explicitly listed in the model database (e.g., dermoscopy, OCT) to characterize out-of-distribution behavior and failure modes.
  3. **End-to-end latency profiling:** Instrument each agent to measure time spent in ingestion, anonymization, feature extraction, matching, preprocessing, and inference. Identify bottlenecks for scaling decisions (cloud DLP API latency is a likely candidate).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the Preprocessing Recommender Agent be augmented to learn from historical outcomes rather than relying on static, rule-based heuristics?
- Basis in paper: [explicit] Section V notes the "rule-based design does not incorporate learning from historical outcomes... Future iterations could benefit from feedback-aware mechanisms."
- Why unresolved: The current agent uses static metadata inference and fixed heuristics to classify column types and recommend steps, limiting adaptability to diverse datasets.
- What evidence would resolve it: Demonstration of an adaptive preprocessing loop where model performance metrics on validation data automatically refine the recommendation strategy for subsequent data batches.

### Open Question 2
- Question: How can the semantic matching process be made robust to non-standard or ambiguous feature names that fail the current embedding-based similarity thresholds?
- Basis in paper: [explicit] Section V states the method "breaks down when user features are non-standard, ambiguous, or semantically distant from known clinical terms."
- Why unresolved: The SapBERT embedding approach relies on cosine similarity, which is sensitive to terminology gaps and lacks a fallback for low-confidence matches.
- What evidence would resolve it: Development of a hybrid matching approach (e.g., incorporating reasoning or ontology-based lookup) that increases model selection success rates on datasets with noisy headers.

### Open Question 3
- Question: What quantitative benchmarks and human-centered metrics are required to validate the safety and reliability of end-to-end agentic frameworks in clinical settings?
- Basis in paper: [explicit] Section VI highlights that the framework "lacks formal evaluation standards" and requires "structured testing" involving domain experts.
- Why unresolved: The paper demonstrates functionality on public datasets but does not quantify safety, reliability, or clinical trust.
- What evidence would resolve it: A comprehensive evaluation framework including adversarial testing, edge-case failure analysis, and clinician usability studies showing statistically significant trust scores.

## Limitations

- Semantic matching fails when clinical column names use non-standard or ambiguous terminology, with no fallback mechanism for low-confidence matches
- Single random image sample may not represent dataset-level characteristics for heterogeneous imaging collections, risking modality/disease misclassification
- Cloud dependencies (Google Cloud DLP, ADK) introduce cost and potential data sovereignty conflicts not addressed in current implementation

## Confidence

- **High confidence:** Modular agent architecture and handoff protocols (clearly specified in Section IV with concrete implementation details)
- **Medium confidence:** SapBERT-based semantic matching mechanism (theoretically sound but unvalidated on clinical column name variations)
- **Low confidence:** MedGemma's ability to generalize to novel imaging protocols and represent dataset-level characteristics from single samples

## Next Checks

1. **Header synonym robustness test:** Systematically evaluate matching success rates across clinical datasets with varying column naming conventions (standard vs. non-standard terminology) to quantify the 0.6 threshold's practical limitations
2. **MedGemma out-of-distribution characterization:** Test the image analysis agent on imaging modalities and disease categories outside the stated model database to measure failure rates and confidence calibration
3. **End-to-end performance benchmarking:** Measure end-to-end pipeline latency and success rates on diverse clinical datasets, with particular attention to cloud API dependencies and their impact on scalability