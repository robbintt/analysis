---
ver: rpa2
title: 'LANCE: Low Rank Activation Compression for Efficient On-Device Continual Learning'
arxiv_id: '2509.21617'
source_url: https://arxiv.org/abs/2509.21617
tags:
- learning
- lance
- memory
- continual
- activation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes LANCE, a low-rank activation compression framework
  for efficient on-device learning. LANCE addresses the high memory cost of storing
  intermediate activations during backpropagation by performing a one-shot higher-order
  singular value decomposition (HOSVD) to obtain a reusable low-rank subspace for
  activation projection.
---

# LANCE: Low Rank Activation Compression for Efficient On-Device Continual Learning

## Quick Facts
- arXiv ID: 2509.21617
- Source URL: https://arxiv.org/abs/2509.21617
- Authors: Marco Paul E. Apolinario; Kaushik Roy
- Reference count: 27
- Primary result: Proposes LANCE, achieving up to 250× activation memory reduction while maintaining full backpropagation accuracy for on-device continual learning

## Executive Summary
This paper introduces LANCE, a low-rank activation compression framework designed to enable efficient on-device continual learning by addressing the prohibitive memory cost of storing intermediate activations during backpropagation. The key innovation is a one-shot higher-order singular value decomposition (HOSVD) that creates reusable low-rank subspaces for activation projection, eliminating the need for repeated decompositions during training. This approach allows models to be fine-tuned and continually adapted across tasks while using orthogonal subspaces to prevent interference, all at a fraction of the memory cost compared to traditional methods.

## Method Summary
LANCE addresses the memory bottleneck in on-device continual learning by performing a single HOSVD decomposition to obtain a fixed low-rank subspace for activation projection. Instead of storing full high-dimensional activation tensors during backpropagation, the method projects activations onto this precomputed subspace, dramatically reducing memory requirements. The framework enables efficient fine-tuning by allocating different tasks to orthogonal subspaces, preventing catastrophic forgetting without requiring storage of large task-specific matrices. This one-shot decomposition approach eliminates the computational overhead of repeated SVD operations during training while maintaining comparable accuracy to full backpropagation.

## Key Results
- Achieves up to 250× reduction in activation storage requirements compared to standard backpropagation
- Maintains accuracy comparable to full backpropagation across various datasets and model architectures
- Demonstrates competitive performance with orthogonal gradient projection methods for continual learning at significantly lower memory cost

## Why This Works (Mechanism)
LANCE leverages the observation that intermediate activation tensors in neural networks often exhibit low-rank structure, meaning most of their information can be captured in a much lower-dimensional subspace. By performing HOSVD once to identify these subspaces, the method creates a compressed representation that preserves essential gradient information while drastically reducing memory footprint. The orthogonal subspace allocation strategy ensures that updates from different tasks do not interfere with each other, enabling effective continual learning without the need for task-specific weight matrices.

## Foundational Learning
- **Higher-Order Singular Value Decomposition (HOSVD)**: Needed to decompose multi-dimensional activation tensors into their principal components; quick check: verify decomposition captures >95% variance with low rank
- **Orthogonal Subspace Theory**: Required to understand how task-specific learning can occur without interference; quick check: confirm subspace orthogonality through inner product calculations
- **Backpropagation Memory Analysis**: Essential to quantify activation storage requirements; quick check: profile memory usage during standard training vs LANCE
- **Continual Learning Dynamics**: Important for understanding catastrophic forgetting and interference; quick check: measure forgetting rates across task sequences
- **Rank Selection Criteria**: Critical for balancing compression ratio and accuracy; quick check: sweep rank values and plot accuracy vs memory trade-off
- **Tensor Compression Techniques**: Relevant for understanding alternative compression approaches; quick check: compare LANCE with other tensor decomposition methods

## Architecture Onboarding

**Component Map**: Input -> Forward Pass -> Activation Projection (HOSVD) -> Backward Pass (in subspace) -> Weight Update -> Orthogonal Subspace Allocation

**Critical Path**: The forward pass followed by activation projection and backward pass through the low-rank subspace represents the critical computational path, with HOSVD decomposition occurring once during initialization.

**Design Tradeoffs**: The method trades a one-time computational cost for HOSVD decomposition against ongoing memory savings. Higher compression (lower rank) reduces memory further but may degrade accuracy. Orthogonal subspace allocation prevents interference but requires careful rank selection to ensure sufficient capacity for all tasks.

**Failure Signatures**: Accuracy degradation when rank is too low to capture essential gradient information; increased training time if HOSVD decomposition is not properly optimized; potential interference between tasks if subspaces are not sufficiently orthogonal.

**First Experiments**:
1. Verify memory reduction by profiling activation storage with varying rank values on a small CNN
2. Test accuracy preservation by comparing full backpropagation vs LANCE on a single task with different rank settings
3. Evaluate catastrophic forgetting by training on sequential tasks and measuring performance on previous tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Actual computational overhead during HOSVD decomposition phase not explicitly quantified in terms of wall-clock time or energy consumption
- Claims of memory reduction represent upper bounds with variability across different model architectures and datasets not fully characterized
- Performance on longer task sequences (>3-4 tasks) not evaluated, raising questions about long-term stability and catastrophic forgetting

## Confidence
- Memory reduction claims: Medium confidence (primarily measured during single-task training)
- Accuracy preservation: Medium confidence (comparison with full backpropagation limited to specific scenarios)
- Continual learning effectiveness: Low confidence (limited task sequence evaluation and lack of comparison with state-of-the-art methods)

## Next Checks
1. Perform comprehensive ablation studies varying the rank selection strategy and its impact on both memory savings and accuracy across diverse model architectures (CNNs, Transformers, MLPs)
2. Evaluate the method's performance on longer task sequences (more than 3-4 tasks) to assess potential catastrophic forgetting and convergence stability
3. Measure actual on-device inference latency and energy consumption with LANCE-enabled models compared to both full fine-tuning and other compression-based approaches