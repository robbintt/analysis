---
ver: rpa2
title: 'HEATACO: Heatmap-Guided Ant Colony Decoding for Large-Scale Travelling Salesman
  Problems'
arxiv_id: '2601.19041'
source_url: https://arxiv.org/abs/2601.19041
tags:
- heatmap
- decoding
- edges
- heataco
- local
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "HEATACO introduces a heatmap-guided Max\u2013Min Ant System (MMAS)\
  \ decoder for large-scale Travelling Salesman Problems. Treating the neural heatmap\
  \ as a soft prior over edges, HEATACO biases MMAS sampling toward high-confidence\
  \ candidates while pheromone updates provide lightweight, instance-specific feedback\
  \ to resolve global degree/subtour conflicts."
---

# HEATACO: Heatmap-Guided Ant Colony Decoding for Large-Scale Travelling Salesman Problems

## Quick Facts
- **arXiv ID:** 2601.19041
- **Source URL:** https://arxiv.org/abs/2601.19041
- **Reference count:** 40
- **Primary result:** HEATACO+2opt achieves gaps as low as 0.11%/0.23%/1.15% on TSP500/1K/10K under seconds-to-minutes CPU decoding.

## Executive Summary
HEATACO introduces a heatmap-guided Max–Min Ant System (MMAS) decoder for large-scale Travelling Salesman Problems. Treating the neural heatmap as a soft prior over edges, HEATACO biases MMAS sampling toward high-confidence candidates while pheromone updates provide lightweight, instance-specific feedback to resolve global degree/subtour conflicts. The decoder scales efficiently via sparse candidate lists and supports optional 2-opt/3-opt local improvement. Across TSP500/1K/10K, using four pretrained heatmaps, HEATACO+2opt achieves gaps as low as 0.11%/0.23%/1.15% under seconds-to-minutes CPU decoding, outperforming greedy merging and published MCTS-guided decoders. Analysis shows gains depend on heatmap reliability: under distribution shift, miscalibration and low-confidence collapse bound improvements, highlighting the importance of heatmap calibration for further progress.

## Method Summary
HEATACO decodes neural heatmaps into feasible TSP tours using heatmap-guided Max–Min Ant System (MMAS). The method filters heatmaps with a threshold (ε_h=10^-4), builds sparse per-node candidate lists (k=20), and modifies MMAS transition probabilities to include the heatmap as a multiplicative prior. Standard MMAS components (pheromone evaporation, elite-tour reinforcement) provide instance-specific feedback. Optional 2-opt or 3-opt local improvement operates on the candidate graph. The decoder is evaluated on TSP500/1K/10K benchmarks using four pretrained heatmaps, measuring tour length gaps and wall-clock time.

## Key Results
- HEATACO+2opt achieves ≤1.15% gaps on large-scale synthetic TSPs under seconds-to-minutes CPU decoding.
- HEATACO outperforms greedy edge merging and published MCTS-guided decoders on TSP500/1K/10K.
- Performance degrades under distribution shift due to heatmap miscalibration and low-confidence collapse.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Injecting the neural heatmap as a multiplicative prior in MMAS transition probabilities accelerates convergence by biasing sampling toward structurally plausible edges.
- **Mechanism:** The transition probability p_{i→j} ∝ (τ_{ij})^α (η_{ij})^β (H̃_{ij})^γ combines pheromone, distance heuristic, and heatmap confidence. The exponent γ controls how strongly ants follow the heatmap ranking. High-confidence edges receive more visits, enabling faster pheromone learning on promising regions while preserving exploration through the pheromone and heuristic terms.
- **Core assumption:** The heatmap preserves a meaningful edge ranking after filtering—specifically, that benchmark-tour edges concentrate in a narrow mid/high-confidence band (Assumption: this pattern generalizes beyond the evaluated distributions).
- **Evidence anchors:**
  - [abstract] "transition policy is softly biased by the heatmap while pheromone updates provide lightweight, instance-specific feedback"
  - [Section 3.2] Eq. (9) and Figure 1 show the multiplicative prior integration
  - [corpus] NeuFACO similarly combines neural guidance with ACO, suggesting the prior-injection pattern is compatible across instantiations
- **Break condition:** Under severe miscalibration (e.g., DIMES on TSPLIB pr1002/pr2392, where Edges/N balloons to 116–493), the ranking becomes uninformative and heatmap guidance degrades toward heatmap-free performance (Table 6, Appendix E).

### Mechanism 2
- **Claim:** Pheromone updates provide lightweight instance-specific feedback that corrects globally inconsistent local heatmap choices.
- **Mechanism:** Standard MMAS pheromone update evaporates all edges and reinforces an elite tour. Edges that repeatedly appear in short tours accumulate pheromone, while spuriously confident edges are progressively suppressed relative to edges that participate in shorter, globally consistent tours.
- **Core assumption:** Iterated sampling with solution-quality feedback can resolve degree/subtour conflicts that greedy commitment cannot repair—specifically, that the pheromone matrix captures global coordination signals across iterations (Assumption: this holds within practical iteration budgets).
- **Evidence anchors:**
  - [Section 3.2] "pheromone updates provide instance-specific feedback to resolve global inconsistencies"
  - [Table 2] HEATACO (γ>0) outperforms vanilla MMAS (γ=0) without local search, confirming heatmap prior provides actionable structure
  - [corpus] Weak direct corpus evidence on pheromone-as-correction; related work on Neural Focused ACO suggests neural-pheromone synergy but does not isolate this mechanism
- **Break condition:** When the heatmap candidate set misses too many optimal-tour edges (>3% miss rate at fixed threshold), pheromone feedback cannot recover absent edges. On TSP10K, DIMES misses ~2.5% of optimal edges (Table 4), contributing to larger gaps.

### Mechanism 3
- **Claim:** Sparse candidate lists derived from filtered heatmaps enable O(Nk) decoding while preserving high optimal-tour recall.
- **Mechanism:** Light thresholding (ϵ_h = 10^-4) reduces dense heatmaps to O(N) candidate edges. Per-node lists (k=20) cap neighborhood size. Nearest-neighbor fallback ensures connectivity when heatmap-derived candidates are insufficient. Decoding and local improvement operate only on these lists.
- **Core assumption:** A fixed global threshold and per-node cap preserve >96% optimal-tour recall across scales and predictors (Assumption: this holds for in-distribution instances but can fail under distribution shift).
- **Evidence anchors:**
  - [Section 3.3] "both tour construction and local improvement operate on sparse candidate lists"
  - [Table 4] At ϵ_h = 10^-4, heatmap methods achieve 3.6–11.4 Edges/N with 96–100% coverage across TSP500/1K/10K
  - [corpus] No direct corpus comparison on sparse-list construction; Edge-wise Topological Divergence work uses MST-based guidance rather than heatmap sparsification
- **Break condition:** Under distribution shift, candidate graphs can densify catastrophically. On TSPLIB pr1002, DIMES yields 116 Edges/N; on pr2392, 493 Edges/N (Table 6), inflating the effective branching factor and degrading both sampling and local improvement.

## Foundational Learning

- **Concept:** Max–Min Ant System (MMAS)
  - **Why needed here:** HEATACO builds directly on MMAS; understanding pheromone bounds [τ_min, τ_max], evaporation, and elite-tour reinforcement is prerequisite to modifying the transition rule.
  - **Quick check question:** If γ=0 in Eq. (9), what does HEATACO reduce to?

- **Concept:** TSP feasibility constraints (degree-2, single Hamiltonian cycle)
  - **Why needed here:** The paper frames decoding as constrained sampling under these constraints; greedy merge fails because it only enforces local feasibility (degree ≤2, avoid premature subtours) without global coordination.
  - **Quick check question:** Why can't greedy edge-merging repair a single wrong early commitment on TSP10K?

- **Concept:** Heatmap calibration and confidence distribution
  - **Why needed here:** The paper's failure analysis hinges on confidence-mass concentration and low-confidence collapse; interpreting Figures 3 and 8 requires understanding how predictors allocate probability mass.
  - **Quick check question:** If most candidate edges fall in [0.01, 0.05] confidence but optimal-tour edges concentrate in [0.4, 0.5], what does this imply for decoder design?

## Architecture Onboarding

- **Component map:** Heatmap preprocessing → sparse candidates → biased MMAS sampling → optional 2-opt/3-opt → output tour
- **Critical path:** Heatmap → sparse candidates → biased MMAS sampling → optional 2-opt/3-opt → output tour. The heatmap exponent γ and local improvement choice dominate the quality–time tradeoff.
- **Design tradeoffs:**
  - Larger γ accelerates convergence but can over-trust miscalibrated edges (Table 2, Figure 10: on TSP10K with 2-opt, DIMES degrades from 1.15% at γ=0.1 to 6.73% at γ=1.0)
  - 2-opt is fast and sufficient for minute-level budgets; 3-opt provides ~1–2 order-of-magnitude gap reduction at higher cost (Table 3)
  - Sparse candidate lists enable scalability but can exclude recoverable edges under strict filtering
- **Failure signatures:**
  - **Candidate densification:** Edges/N >> 20 on diagnostics indicates miscalibration; reduce γ or increase exploration
  - **High miss rate (>3%):** Optimal-tour edges absent from candidate set; loosen threshold or add NN fallback
  - **Stagnation:** Gap plateaus early in convergence curves; pheromone may be clamped at bounds—check τ_min/τ_max settings
- **First 3 experiments:**
  1. **Baseline sanity check:** Run HEATACO (γ=0) vs. HEATACO (γ=1.0) on TSP500 with a fixed heatmap (e.g., DIFUSCO). Verify that heatmap guidance improves time-to-quality (Figure 2).
  2. **γ sensitivity sweep:** On TSP1K, sweep γ ∈ {0.1, 0.5, 1.0, 2.0} with and without 2-opt. Confirm that larger γ helps without local search but can hurt with local search (Section 6.2, Figure 10).
  3. **Distribution-shift diagnostic:** Run candidate-set diagnostics (Edges/N, Coverage, Miss) on a TSPLIB instance (e.g., pr1002). If Edges/N explodes, verify that smaller γ and stronger local search partially recover performance (Table 5 vs. Table 6).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can heatmap predictors be trained or recalibrated to mitigate the "low-confidence collapse" phenomenon observed under distribution shift, thereby preventing the bounding of decoding improvements?
- **Basis in paper:** [explicit] The analysis in Section 5.2 identifies "low-confidence collapse" on TSPLIB instances, and the Conclusion states that "improving heatmap calibration and out-of-distribution generalisation is a direct and promising path."
- **Why unresolved:** HEATACO is a modular decoder that treats the heatmap predictor as a fixed black box; the paper analyzes the failure mode but does not propose modifications to the neural predictor's architecture or loss function.
- **Evidence:** Developing a training objective that penalizes dense low-confidence regions on OOD data, resulting in HEATACO maintaining low gaps on TSPLIB without relying heavily on distance heuristics.

### Open Question 2
- **Question:** Can the proposed label-free entropy-targeted heuristic for selecting the heatmap exponent $\gamma$ guarantee robust performance across diverse structural distributions without requiring manual validation sets?
- **Basis in paper:** [explicit] Appendix G.2 proposes a "label-free heuristic to choose $\gamma$ by targeting the entropy" but frames it as a practical recommendation rather than a proven universal solution.
- **Why unresolved:** The heuristic relies on the assumption that heatmap confidence distributions have consistent entropy profiles across instances, which may not hold for future predictors or drastically different problem geometries.
- **Evidence:** A comprehensive ablation showing that the entropy-targeted $\gamma$ consistently matches or outperforms grid-search $\gamma$ on clustered or fractal datasets where uniformity assumptions fail.

### Open Question 3
- **Question:** Does the soft prior integration mechanism of HEATACO generalize effectively to constrained routing problems (e.g., CVRP) where feasibility constraints differ from the degree-2 subtour constraints of TSP?
- **Basis in paper:** [inferred] The method is strictly evaluated on TSP. While the background mentions VRPs, the MMAS transition rules and feasibility checks (Section 3) are specifically engineered for the Hamiltonian cycle structure of TSP.
- **Why unresolved:** VRP constraints (capacity, time windows) complicate the tour construction process, potentially making the pheromone feedback loop less effective at resolving the specific conflicts induced by these new constraints.
- **Evidence:** Adapting the HEATACO decoder to CVRP benchmarks and demonstrating that heatmap guidance improves time-to-quality relative to classical heuristics despite the more complex feasibility checks.

## Limitations
- The mechanism relying on heatmap ranking preservation is only validated on synthetic uniform TSP distributions; performance under structured or real-world benchmarks (TSPLIB) degrades sharply when miscalibration causes dense candidate graphs (e.g., 493 Edges/N on pr2392).
- Pheromone feedback as a correction mechanism lacks direct empirical isolation—related ACO variants show neural-pheromone synergy but do not quantify standalone pheromone repair of heatmap errors.
- Sparse-list recall >96% assumes in-distribution uniformity; distribution shift can cause >3% optimal-edge misses, beyond which pheromone cannot recover.

## Confidence
- **High:** HEATACO with 2-opt achieves ≤1.15% gaps on large-scale synthetic TSPs; heatmap prior accelerates convergence vs vanilla MMAS; dense candidates under miscalibration hurt performance.
- **Medium:** Sparse candidates preserve optimal-edge coverage across scales; local search recovers from low-confidence heatmap collapse; MMAS hyperparameters are standard and transferable.
- **Low:** Pheromone feedback quantitatively corrects heatmap errors; candidate sparsification generalizes to structured benchmarks; hyperparameter transferability across predictors.

## Next Checks
1. Run HEATACO on structured TSPLIB instances (e.g., pr1002, pr2392) and log Edges/N and confidence-mass distribution to quantify miscalibration impact.
2. Implement an ablation: heatmap-guided MMAS vs pheromone-only MMAS with identical hyperparameters, measuring gap reduction attributable to pheromone feedback.
3. Test candidate-list sensitivity: vary ϵ_h and k on synthetic TSPs, measure optimal-edge miss rate and final gap to identify recall thresholds for reliable decoding.