---
ver: rpa2
title: 'The Curious Case of Factuality Finetuning: Models'' Internal Beliefs Can Improve
  Factuality'
arxiv_id: '2507.08371'
source_url: https://arxiv.org/abs/2507.08371
tags:
- data
- training
- factuality
- gold
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies how the factuality of finetuning data affects\
  \ language model hallucination rates. The authors find that training on model-generated\
  \ data filtered by internal knowledge probes\u2014EPISTEMIC TRAINING\u2014reduces\
  \ hallucinations more than training on factual gold data, even when the generated\
  \ data contains unsupported claims."
---

# The Curious Case of Factuality Finetuning: Models' Internal Beliefs Can Improve Factuality

## Quick Facts
- **arXiv ID:** 2507.08371
- **Source URL:** https://arxiv.org/abs/2507.08371
- **Reference count:** 40
- **Primary result:** Training on model-generated data filtered by internal knowledge probes (EPISTEMIC TRAINING) reduces hallucinations more than training on factual gold data, even when the generated data contains unsupported claims.

## Executive Summary
This paper explores how the factuality of finetuning data affects language model hallucination rates. The authors find that training on model-generated data filtered by internal knowledge probes reduces hallucinations more effectively than training on factual gold data, even when the generated data contains unsupported claims. Filtering with internal knowledge probes often outperforms filtering with entailment to external documents. Factuality gains transfer across domains, though in-domain data still performs best. Across three datasets and three model families, EPISTEMIC TRAINING improves factuality by 12-30 percentage points while maintaining reasonable detail.

## Method Summary
The method, called EPISTEMIC TRAINING, involves generating synthetic documents from entity prompts, atomizing them into atomic claims, filtering claims using either entailment to external documents or internal knowledge probes, merging filtered atoms back into training documents, and fine-tuning the target model with LoRA. The internal knowledge probes are trained to predict whether the model "believes" a claim is factual based on mid-layer hidden states, using entailment-based silver labels on held-out entities. The approach outperforms both standard finetuning on gold data and finetuning on unfiltered model-generated data.

## Key Results
- Training on model-generated data filtered by internal probes (f_int) achieves 12-30 percentage point improvement in factuality compared to no finetuning
- EPISTEMIC TRAINING outperforms training on factual gold data despite the filtered data being only 63-85% factual (vs 100% for gold)
- Internal probe filtering (f_int) outperforms external entailment filtering (f_ext) across all three datasets and model families
- Factuality gains transfer across domains, though in-domain data still performs best

## Why This Works (Mechanism)

### Mechanism 1: Familiarity-Alignment Reduces Hallucination Pressure
Training on claims the model already "believes" (per internal probes) causes less hallucination than training on factual-but-unfamiliar gold data. Gold documents contain entities and claims with low pretraining frequency. When fine-tuned on unfamiliar facts, models learn a generation policy that fabricates for similar entities. Filtering by internal probes preferentially retains high-frequency knowledge, reducing pressure to hallucinate on rare entities.

### Mechanism 2: Internal Probe Filters Confidence, Not Ground-Truth Factuality
Internal probes select claims the model can consistently express, which indirectly improves factuality even when some retained claims lack external grounding. Logistic-regression probes on mid-layer hidden states predict whether the model "knows" a claim. Training on probe-positive claims reinforces generation within the model's stable knowledge manifold, reducing drift into hallucinated content.

### Mechanism 3: Length/Detail Control via Aggressive Filtering
Part of the factuality gain comes from generating fewer, simpler claims—f_int filters more aggressively than f_ext. f_int marks more atoms unsupported than f_ext. Shorter, less detailed generations have fewer opportunities for error. The paper controls for length during training but test-time generations remain shorter.

## Foundational Learning

- **Atomic claim decomposition (FActScore-style)**: Needed because the pipeline requires granular units to filter and recombine. Without atomization, filtering operates at sentence/document level with poor precision. Quick check: Can you explain why "Washington was born in Virginia" and "Washington was the first US president" should be atomized separately for factuality filtering?

- **Linear probes on hidden states**: Needed because f_int depends on training classifiers on model representations to predict claim support. Understanding probe training (layer selection, binary classification, silver labels) is prerequisite to reproducing the method. Quick check: Why might mid-layer representations (vs. final layer) better encode whether a model "knows" a claim?

- **Precision–recall trade-off in factuality**: Needed because the paper explicitly notes factuality gains coincide with reduced detail and increased abstention. Practitioners must decide acceptable operating points. Quick check: A model that always refuses to answer has 100% factuality but 0 utility. How would you quantify an acceptable factuality–detail trade-off for a specific deployment?

## Architecture Onboarding

- **Component map**: Generator (e.g., Mistral-7B-Instruct) -> Atomizer (Mistral-7B-Instruct with few-shot prompt) -> External filter f_ext (MiniCheck/Flan-T5-large) -> Internal filter f_int (logistic-regression probe on hidden states) -> Merger (Llama-3-8B-Instruct) -> Fine-tuning loop (LoRA)

- **Critical path**:
  1. Collect entity set E_train and gold documents D_gold
  2. Generate D_gen (10 samples/entity, temperature 0.7)
  3. Atomize all samples
  4. Train f_int probe on held-out entities using f_ext silver labels
  5. Filter atoms with chosen strategy (f_int or f_ext)
  6. Apply length control (select top-p atoms per sample)
  7. Merge atoms into training documents
  8. Fine-tune target model with LoRA

- **Design tradeoffs**:
  - f_int vs. f_ext: f_int yields higher factuality but lower training-data factuality; requires probe training data (~150-250 entities). f_ext needs no probe but underperforms
  - D_gen vs. D_gold: Generated data outperforms but requires sampling/compute. Gold data is cleaner but includes unfamiliar claims
  - Length control aggressiveness: Stricter p improves factuality, reduces detail

- **Failure signatures**:
  - Probe predicts all unsupported: Seen with Llama-2 on Bios. Likely probe overfitting or insufficient training data
  - High abstention rate: f_int can produce 30-50% abstentions. Indicates over-aggressive filtering or domain mismatch
  - Merging introduces hallucinations: Validated at 5/20 errors in qualitative check

- **First 3 experiments**:
  1. Reproduce f_int probe training: On a held-out entity split, train linear probes at layers 10, 15, 20. Report F1 on validation claims. Confirm layer-15 peak matches Figure 2
  2. Ablate filtering strategies: On a single domain (Bios), compare f_int vs. f_ext vs. random filtering. Measure factuality, detail, abstention. Confirm f_int > f_ext > random
  3. Cross-domain transfer test: Train with EPISTEMIC TRAINING on Bios, evaluate on Medical and Plots. Report factuality delta vs. no-finetuning baseline. Confirm transfer but reduced gain vs. in-domain

## Open Questions the Paper Calls Out

### Open Question 1
Can EPISTEMIC TRAINING be adapted to teach language models *new* factual information they do not currently possess, rather than just reinforcing existing knowledge? The current method relies on filtering data based on internal knowledge probes (what the model already "believes"). If the model does not know a fact, the probe will likely classify it as unsupported, filtering it out and preventing the model from learning that new information during finetuning.

### Open Question 2
Does training on self-generated data filtered by internal beliefs propagate or amplify existing societal biases (e.g., regarding gender or race) in model outputs? The study focuses on aggregate factuality scores but does not conduct a demographic or fairness audit. If a model's internal beliefs reflect historical biases, filtering based on those beliefs may codify those biases into the finetuned model's behavior.

### Open Question 3
How can the precision-recall trade-off be better managed to maintain high factuality without significantly reducing the detail (length) of model generations? The current filtering mechanism improves factuality primarily by removing atoms that cannot be verified, effectively pruning the output. It is unclear if this method can be modified to encourage the generation of *more* verified detail rather than just subtracting unverified content.

### Open Question 4
How can the evaluation and filtering pipeline be improved to handle "unverifiable" claims that are factually correct but not entailed by the limited gold documents? The current methodology relies on a single gold document for entailment, creating a false negative rate where the model is penalized for knowing things not contained in that specific document.

## Limitations
- The method's effectiveness depends on the probe's ability to generalize across domains and models, which may not hold for all entity distributions or model architectures
- Some factuality gains may stem from generating fewer, simpler claims rather than improved factual accuracy, raising questions about utility for tasks requiring high detail
- Cross-domain transfer results are promising but based on a single source domain (Bios), limiting generalizability

## Confidence
- **High Confidence:** The core experimental design and results for EPISTEMIC TRAINING are well-supported by ablation studies and quantitative metrics across three datasets and model families
- **Medium Confidence:** The interpretation that probe-filtered training data works because it reinforces "familiar" knowledge is plausible but not definitively proven
- **Low Confidence:** The cross-domain transfer results are promising but based on a single source domain (Bios)

## Next Checks
1. **Probe Robustness Across Domains:** Train internal probes on multiple source domains (e.g., Bios, Plots, Medical) and evaluate their performance on a held-out target domain (e.g., Scientific abstracts). Measure probe accuracy, abstention rates, and downstream factuality gains to assess domain transfer.

2. **Mechanistic Ablation (Length vs. Factuality):** Conduct a controlled experiment where generated claims are length-matched across filtering strategies (f_int, f_ext, random). Compare factuality improvements to isolate the contribution of detail reduction from improved factual accuracy.

3. **Probe Data Efficiency:** Systematically vary the size of the held-out entity set used to train internal probes (e.g., 50, 150, 250, 500 entities). Measure probe accuracy, abstention rates, and downstream factuality gains to determine the minimum viable training set size and identify diminishing returns.