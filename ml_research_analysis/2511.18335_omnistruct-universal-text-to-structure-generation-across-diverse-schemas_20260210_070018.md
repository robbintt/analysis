---
ver: rpa2
title: 'OmniStruct: Universal Text-to-Structure Generation across Diverse Schemas'
arxiv_id: '2511.18335'
source_url: https://arxiv.org/abs/2511.18335
tags:
- tasks
- task
- answer
- json
- text-to-structure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OmniStruct, a comprehensive benchmark for
  evaluating large language models' ability to generate structured outputs across
  diverse tasks and schemas. The authors identify existing datasets from information
  extraction, table generation, and function calling tasks, adapting them into a unified
  schema-following JSON generation format.
---

# OmniStruct: Universal Text-to-Structure Generation across Diverse Schemas

## Quick Facts
- arXiv ID: 2511.18335
- Source URL: https://arxiv.org/abs/2511.18335
- Authors: James Y. Huang; Wenxuan Zhou; Nan Xu; Fei Wang; Qin Liu; Sheng Zhang; Hoifung Poon; Muhao Chen
- Reference count: 28
- Primary result: OmniStruct benchmark unifies diverse structured output tasks; models fine-tuned on synthetic data match GPT-4o performance, with distilled 8B model achieving F1 up to 76.9 on NER and 84.0 on function calling

## Executive Summary
OmniStruct introduces a comprehensive benchmark for evaluating large language models' ability to generate structured outputs across diverse schemas and tasks. The authors unify existing datasets from information extraction, table generation, and function calling into a common JSON format, creating a standardized evaluation framework. They develop a synthetic data generation pipeline using GPT-4o to enable fine-tuning of smaller models without supervised data. Their experiments demonstrate that models trained on this synthetic data can achieve performance comparable to GPT-4o on the benchmark tasks, with an 8B distilled model reaching competitive F1 scores across multiple task types.

## Method Summary
The paper addresses the challenge of structured output generation by first unifying diverse datasets into a common schema-following JSON format. The authors adapt existing information extraction datasets (like CoNLL-2003 and MIT Movie), table generation datasets (like WebNLG and LogicNLG), and function calling datasets (like MultiWOZ and PLEX) into this unified format. They then develop a synthetic data generation pipeline using GPT-4o, which generates examples through a chain-of-thought process with few-shot demonstrations. This pipeline enables the creation of training data for smaller models without requiring human annotations. The approach involves generating synthetic examples, filtering low-quality outputs, and fine-tuning base models on this synthetic data to achieve strong performance on the benchmark tasks.

## Key Results
- OmniStruct benchmark unifies 28 datasets across 6 task types and 5 domains
- Models fine-tuned on synthetic data achieve F1 scores up to 76.9 on NER tasks and 84.0 on function calling tasks
- Distilled 8B model trained on synthetic data rivals GPT-4o's performance on benchmark tasks
- Synthetic data generation pipeline enables effective training without supervised data

## Why This Works (Mechanism)
The approach works by leveraging GPT-4o's strong reasoning capabilities to generate high-quality synthetic training data for smaller models. The unified schema-following format ensures consistent evaluation across diverse task types, while the chain-of-thought generation process helps produce more accurate and structured outputs. By fine-tuning base models on this synthetic data, the approach transfers GPT-4o's knowledge to more efficient models that can be deployed in production settings.

## Foundational Learning
- **Schema-following generation**: Converting diverse structured output tasks into a unified JSON format enables standardized evaluation and training across different domains. Why needed: Existing benchmarks use inconsistent formats, making cross-task comparisons difficult. Quick check: Verify that all benchmark datasets can be converted to the unified schema without loss of information.
- **Synthetic data generation**: Using GPT-4o to generate training examples eliminates the need for expensive human annotations while maintaining quality through careful filtering. Why needed: Supervised data for structured output tasks is expensive to obtain and limited in coverage. Quick check: Compare synthetic examples against ground truth to measure generation quality.
- **Chain-of-thought prompting**: Generating intermediate reasoning steps before final structured output improves accuracy and consistency. Why needed: Direct generation often produces errors that compound, while step-by-step reasoning provides better control. Quick check: Measure performance difference between direct generation and chain-of-thought approaches.

## Architecture Onboarding

**Component map:** Base model -> Synthetic data generator (GPT-4o) -> Training pipeline -> Fine-tuned model -> Evaluation on benchmark

**Critical path:** The pipeline's success depends on the quality of synthetic data generation, as this directly determines the fine-tuned model's performance. The chain-of-thought generation process and filtering mechanisms are critical for ensuring high-quality training data.

**Design tradeoffs:** The approach trades computational cost of synthetic data generation for reduced annotation costs and broader task coverage. Using GPT-4o for generation enables high-quality data but requires significant compute resources during the data synthesis phase.

**Failure signatures:** Poor performance may indicate issues with synthetic data quality, inadequate filtering of low-quality examples, or misalignment between the unified schema and task requirements. Contamination from evaluation data in training can also lead to inflated performance metrics.

**First experiments:** 1) Generate a small batch of synthetic examples and manually evaluate quality; 2) Fine-tune a base model on synthetic data and evaluate on a held-out validation set; 3) Compare performance across different synthetic data generation parameters (few-shot examples, chain-of-thought steps).

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on GPT-4o for both data synthesis and evaluation raises contamination concerns, with up to 6.8% dataset contamination identified
- Benchmark coverage may not fully represent real-world structured output diversity, particularly in specialized domains
- The synthetic data generation pipeline requires significant computational resources for GPT-4o inference

## Confidence
- **High confidence**: The technical methodology for creating unified schema-following format and experimental results demonstrating performance gains from fine-tuning are well-supported and reproducible
- **Medium confidence**: The synthetic data generation pipeline's effectiveness and benchmark's representativeness across diverse schemas, as these depend on quality and diversity of GPT-4o outputs
- **Low confidence**: Claims about benchmark's completeness and absence of data contamination effects, given acknowledged contamination issues and limited scope of evaluated task types

## Next Checks
1. Conduct thorough audit of benchmark's task coverage by systematically evaluating models on additional schema types not included in current OmniStruct benchmark, particularly in specialized domains like biomedical or legal text processing
2. Implement and test alternative evaluation methodologies less susceptible to potential data contamination, such as using smaller, uncontaminated models for evaluation or developing contamination-resistant evaluation metrics
3. Perform ablation studies to quantify impact of synthetic data generation parameters (e.g., chain-of-thought steps, few-shot examples) on final model performance, to optimize data synthesis pipeline and understand its limitations