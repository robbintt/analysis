---
ver: rpa2
title: Study of Training Dynamics for Memory-Constrained Fine-Tuning
arxiv_id: '2510.19675'
source_url: https://arxiv.org/abs/2510.19675
tags:
- gradient
- memory
- layers
- random
- channel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes TraDy, a memory-efficient transfer learning
  approach for fine-tuning deep neural networks under strict resource constraints.
  The method exploits heavy-tailed stochastic gradient distributions and architecture-dependent
  layer importance to dynamically select channels for update while maintaining memory
  budgets.
---

# Study of Training Dynamics for Memory-Constrained Fine-Tuning

## Quick Facts
- arXiv ID: 2510.19675
- Source URL: https://arxiv.org/abs/2510.19675
- Authors: Aël Quélennec; Nour Hezbri; Pavlo Mozharovskyi; Van-Tam Nguyen; Enzo Tartaglione
- Reference count: 40
- Primary result: Achieves up to 99% activation sparsity, 95% weight derivative sparsity, and 97% reduction in FLOPs while maintaining competitive performance under strict memory constraints

## Executive Summary
TraDy introduces a memory-efficient transfer learning approach that exploits heavy-tailed stochastic gradient distributions and architecture-dependent layer importance to dynamically select channels for update during fine-tuning. The method achieves state-of-the-art performance across various downstream tasks while maintaining strict memory budgets through stochastic channel resampling within pre-selected layers between epochs. By approximating full gradient updates with significantly reduced memory consumption, TraDy enables fine-tuning of large models on resource-constrained devices without sacrificing accuracy.

## Method Summary
TraDy leverages the observation that stochastic gradients exhibit heavy-tailed distributions and that different layers contribute unequally to model performance. The method dynamically selects channels for gradient computation based on importance metrics, performing stochastic resampling between epochs to approximate full fine-tuning. This approach maintains memory budgets by computing gradients only for selected channels while achieving competitive accuracy through strategic channel selection and resampling. The framework integrates seamlessly with existing fine-tuning pipelines and provides tunable sparsity levels to balance memory constraints against performance requirements.

## Key Results
- Achieves up to 99% activation sparsity and 95% weight derivative sparsity
- Reduces FLOPs for weight derivative computation by up to 97%
- Maintains competitive performance across multiple downstream tasks and architectures
- Successfully operates under strict memory constraints while preserving fine-tuning effectiveness

## Why This Works (Mechanism)
TraDy exploits the inherent structure of stochastic gradients and layer importance in deep networks. Heavy-tailed gradient distributions indicate that only a subset of parameters significantly influences learning dynamics, while architecture-dependent layer importance reveals which components contribute most to downstream task performance. By selectively updating channels based on these properties and resampling between epochs, the method approximates full gradient updates with substantially reduced computational requirements. This stochastic approximation maintains convergence properties while dramatically reducing memory footprint through sparsity in both activations and gradient computations.

## Foundational Learning
- Heavy-tailed stochastic gradient distributions: Critical for understanding which parameters drive learning dynamics; quick check: visualize gradient histogram across layers
- Architecture-dependent layer importance: Determines which components most influence task performance; quick check: perform layer ablation studies on validation data
- Stochastic approximation theory: Provides theoretical foundation for approximating full gradients with partial updates; quick check: verify convergence bounds for selected channel subsets
- Memory-accuracy tradeoff optimization: Balances computational constraints against performance requirements; quick check: sweep sparsity levels and measure accuracy degradation
- Channel-wise parameter importance: Enables fine-grained selection of parameters for update; quick check: analyze correlation between channel importance scores and gradient magnitudes

## Architecture Onboarding
Component map: Input -> Channel Selection Module -> Dynamic Resampling -> Sparse Gradient Computation -> Model Update -> Output
Critical path: Data flows through channel selection and dynamic resampling before gradient computation, with sparse updates propagating through the model architecture
Design tradeoffs: Memory efficiency versus approximation accuracy, channel selection frequency versus convergence stability, computational overhead of importance scoring
Failure signatures: Degraded performance when channel selection misses critical parameters, convergence issues with insufficient resampling frequency, memory overflows when importance metrics are poorly calibrated
First experiments: 1) Validate channel importance scoring on small network with known parameter sensitivity, 2) Test dynamic resampling frequency impact on convergence rate, 3) Measure memory savings versus accuracy trade-off across different sparsity levels

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on specific architectural characteristics may limit cross-architecture generalization
- Channel selection criteria are empirically derived without theoretical convergence guarantees
- Hyperparameter sensitivity around resampling frequency and magnitude lacks comprehensive analysis
- Limited evaluation on architectures with fundamentally different designs (e.g., transformers)

## Confidence
High: Memory efficiency improvements and sparsity metrics are directly measurable and consistently demonstrated
Medium: Performance claims relative to baselines depend on implementation details and hyperparameter tuning
Low: Generalizability across diverse architectures and tasks requires further validation

## Next Checks
1) Test TraDy on transformer-based architectures (BERT, ViT) to verify cross-architecture applicability
2) Conduct ablation studies on channel selection frequency and criteria to understand hyperparameter sensitivity
3) Evaluate performance on tasks with significantly different data distributions and label complexities than the evaluation suite