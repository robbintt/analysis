---
ver: rpa2
title: Evaluating Large Language Models for Fair and Reliable Organ Allocation
arxiv_id: '2504.03716'
source_url: https://arxiv.org/abs/2504.03716
tags:
- candidate
- candidates
- donor
- allocation
- fairness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates the use of large language models (LLMs) for
  fair and reliable kidney organ allocation. The authors assess medical viability
  and demographic fairness by testing three LLMs (Claude, GPT-4o, and Gemini 2.0 Pro)
  on two tasks: choosing one candidate and ranking all candidates from a waiting list.'
---

# Evaluating Large Language Models for Fair and Reliable Organ Allocation

## Quick Facts
- **arXiv ID**: 2504.03716
- **Source URL**: https://arxiv.org/abs/2504.03716
- **Reference count**: 40
- **Primary result**: LLMs show task-dependent demographic preferences in kidney allocation, with fairness metrics revealing systematic biases despite exposure-based metrics suggesting equity

## Executive Summary
This paper evaluates large language models (LLMs) for fair and reliable kidney organ allocation, testing three models (Claude 3.5 Sonnet, GPT-4o, Gemini 2.0 Pro) on medical viability classification and demographic fairness across selection and ranking tasks. The study reveals that LLMs can infer medical compatibility but performance varies significantly by model and prompting strategy. Critically, demographic preferences are highly task-dependent, showing inverted trends between selection and ranking tasks. While exposure-based metrics suggest equitable outcomes, probability-based metrics uncover systematic preferential sorting. The findings demonstrate that current LLMs can introduce inequalities in real-world allocation scenarios, highlighting the need for rigorous fairness evaluation and human oversight before deployment.

## Method Summary
The study uses OPTN dataset filtered to medically viable candidates, testing three LLMs across two tasks: Choose-One (selecting single candidate) and Rank-All (ranking all candidates). Three prompting strategies are employed: out-of-box, zero-shot (criteria only), and few-shot (criteria plus examples). Medical viability is assessed using ground-truth criteria (blood type compatibility, height/weight differences). Fairness metrics include proportional parity for selection tasks, rND (probability-based) and exposure (attention-based) for ranking tasks. Statistical analysis uses linear mixed models for rND and Welch's t-tests/Dunnett's tests for exposure comparisons across demographic groups.

## Key Results
- LLMs achieve high medical viability classification accuracy (F1 scores 79.6-85.4%) but performance varies by model and prompting strategy
- Demographic preferences invert between tasks: groups favored in Choose-One may be disadvantaged at Rank 1 in Rank-All
- Exposure-based metrics suggest equitable outcomes while probability-based rND metrics uncover systematic preferential sorting at specific rank cutoffs
- Gemini 2.0 Pro shows particularly unstable behavior, reversing demographic preferences between task formulations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task formulation (selection vs. ranking) causally determines which demographic groups receive priority, with preferences potentially inverting between formulations
- Mechanism: LLMs employ different implicit decision policies when asked to select one candidate versus producing a complete ordered list, engaging different reasoning patterns that can favor different groups at the top position
- Core assumption: The inversion reflects genuine policy differences rather than noise; models have stable but task-contingent preferences
- Evidence anchors: Abstract finding that demographic preferences are "highly task-dependent, showing inverted trends"; Section 6.2 showing Gemini reversing behavior between tasks

### Mechanism 2
- Claim: Probability-based metrics (rND) detect preferential sorting that exposure-based metrics obscure, because they evaluate distributional deviations at discrete rank cutoffs rather than aggregate visibility
- Mechanism: Exposure metrics compute average visibility using logarithmic discounting across all positions, which can balance over-representation at some ranks with under-representation at others. rND penalizes deviations from proportional parity at each specified cutoff independently
- Core assumption: The logarithmic weighting in exposure metrics appropriately models how ranking position affects allocation probability in organ transplantation contexts
- Evidence anchors: Abstract statement that "while exposure-based metrics suggest equitable outcomes, probability-based metrics uncover systematic preferential sorting"; Section 6.1 showing female candidates' over-representation at Top-3/Top-5/Top-10 despite slight under-representation at Rank 1

### Mechanism 3
- Claim: Prompting strategy causally affects medical viability classification accuracy, but effects are model-dependent and no single strategy is optimal across models
- Mechanism: Different models have different prior knowledge and reasoning patterns. Explicit criteria and examples can guide reasoning but may also introduce noise if the model's internal representations conflict with provided exemplars
- Core assumption: The "ground truth" viability criteria correctly capture medical suitability
- Evidence anchors: Abstract noting "performance varies by model and prompting strategy"; Section 4, Table 1 showing GPT's degradation from 85.4% F1 (zero-shot) to 62.7% F1 (few-shot)

## Foundational Learning

- Concept: **Proportional parity** (selection fairness)
  - Why needed here: Choose-One task evaluation; measures whether selection rates match demographic representation in the candidate pool
  - Quick check question: If 30% of candidates are Black and the model selects 40% Black candidates across trials, is proportional parity achieved?

- Concept: **Normalized Discounted Difference (rND)** (probability-based ranking fairness)
  - Why needed here: Rank-All task evaluation; quantifies distributional deviations from proportional representation at specified cutoffs with higher-weighted top positions
  - Quick check question: If Asian candidates are 8% of the pool but 20% of Top-3 selections, how does rND's logarithmic weighting affect the penalty compared to uniform weighting?

- Concept: **Exposure in rankings** (attention-based fairness)
  - Why needed here: Captures aggregate visibility using position bias (v_j = 1/logâ‚‚(1+j)); measures whether demographic groups receive comparable expected attention
  - Quick check question: If a group is over-represented at Rank 2-5 but under-represented at Rank 1 and 6-15, can their average exposure still equal the baseline?

## Architecture Onboarding

- Component map: OPTN dataset -> Filter to viable candidates -> 15-candidate pools -> LLM prompts (Choose-One/Rank-All) -> Fairness metrics (proportional parity/rND/exposure) -> Statistical analysis
- Critical path: 1) Define viability criteria and filter candidates, 2) Run repeated trials with shuffled ordering, 3) Compute selection probabilities or ranking distributions, 4) Apply fairness metrics with statistical tests, 5) Compare demographic deviations across tasks and models
- Design tradeoffs: Context-length constraints excluded some clinical features; static allocation ignores temporal dynamics; ground truth absence requires proportional parity assumption
- Failure signatures: Metric divergence (exposure fair but rND shows clustering), task inversion (group favored in selection disadvantaged in ranking), prompting degradation (performance drops with examples), mid-ranking instability (high uncertainty for ranks 6-10)
- First 3 experiments: 1) Baseline replication: Run both tasks, verify inversions occur, 2) Rank-wise analysis: Plot deviations at each position to identify clustering, 3) Prompt sensitivity test: Compare strategies on viability classification, measure precision/recall tradeoffs

## Open Questions the Paper Calls Out

- **Question**: To what extent are observed demographic disparities caused by correlations with clinical variables versus inherent model bias, as revealed by counterfactual analysis?
  - Basis in paper: [explicit] The Conclusion states that "counterfactual analyses that systematically vary demographic attributes while holding clinical profiles constant may provide further insight"
  - Why unresolved: The study used real-world data where demographic attributes naturally correlate with clinical profiles
  - What evidence would resolve it: Results from experiments where demographic attributes are swapped between candidates with identical clinical profiles

- **Question**: How do LLM fairness and performance metrics change when evaluated in dynamic allocation environments that evolve over time?
  - Basis in paper: [explicit] The Limitations section notes that "real-world organ allocation unfolds dynamically over time, with candidate statuses, urgency, and availability evolving continuously"
  - Why unresolved: The experimental design relied on static snapshots rather than temporal complexity
  - What evidence would resolve it: Evaluation results from a longitudinal simulation where candidate health status and waitlist composition change dynamically

- **Question**: Can new fairness metrics be developed to accurately reflect the "winner-takes-all" consequences of organ allocation?
  - Basis in paper: [explicit] The Conclusion advises prioritizing "rank-aware evaluations that reflect the 'winner-takes-all' dynamics inherent in allocation settings"
  - Why unresolved: Standard exposure-based metrics often obscured biases that probability-based metrics caught
  - What evidence would resolve it: Validation of a novel metric that correlates more strongly with actual allocation rates in single-winner scenarios

## Limitations

- Static allocation scenarios fail to capture the dynamic nature of real organ allocation where patient status changes rapidly
- Simplified ground-truth viability criteria may not fully represent clinical practice, affecting model performance
- Task-dependent preference inversions may reflect task-specific reasoning patterns rather than stable model biases

## Confidence

- **High confidence**: Task formulation effects on demographic preferences (strong empirical evidence with consistent inversions)
- **Medium confidence**: Metric divergence findings (rND vs. exposure), though theoretically sound and supported by results
- **Medium confidence**: Prompting strategy effects, particularly for GPT-4o (results show clear degradation but sample size per model is limited)

## Next Checks

1. **Temporal validation**: Replicate analysis using time-series OPTN data with dynamic status changes to assess how LLMs handle evolving allocation scenarios

2. **Clinical criterion expansion**: Test model performance with additional clinical features (diabetes onset, transplant history) that were excluded due to context constraints but may be clinically relevant

3. **Cross-task consistency**: Conduct controlled experiments varying only task framing (selection vs. ranking) while holding all other variables constant to isolate framing effects on demographic outcomes