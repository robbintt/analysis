---
ver: rpa2
title: 'Kimi-Dev: Agentless Training as Skill Prior for SWE-Agents'
arxiv_id: '2509.23045'
source_url: https://arxiv.org/abs/2509.23045
tags:
- test
- file
- data
- path
- parameter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel perspective on training large language
  models (LLMs) for software engineering (SWE) tasks, arguing that agentless and agentic
  paradigms are complementary rather than competing. The authors introduce Kimi-Dev,
  an open-source SWE LLM trained through an agentless recipe that induces transferable
  skill priors including bug localization, code editing, and self-reflection.
---

# Kimi-Dev: Agentless Training as Skill Prior for SWE-Agents

## Quick Facts
- arXiv ID: 2509.23045
- Source URL: https://arxiv.org/abs/2509.23045
- Reference count: 40
- Kimi-Dev achieves 60.4% accuracy on SWE-bench Verified using workflow-based methods

## Executive Summary
This paper proposes a novel perspective on training large language models (LLMs) for software engineering (SWE) tasks, arguing that agentless and agentic paradigms are complementary rather than competing. The authors introduce Kimi-Dev, an open-source SWE LLM trained through an agentless recipe that induces transferable skill priors including bug localization, code editing, and self-reflection. This approach achieves 60.4% accuracy on SWE-bench Verified, the best among workflow-based methods. Crucially, the induced skills enable efficient adaptation to agentic frameworks: after minimal fine-tuning on 5,000 publicly-available trajectories, Kimi-Dev powers SWE-agents to 48.6% pass@1, matching Claude 3.5 Sonnet's performance. The results demonstrate that structured skill priors from agentless training can bridge workflow and agentic frameworks, enabling more effective and generalizable coding agents.

## Method Summary
Kimi-Dev is trained through a staged pipeline that first builds transferable skill priors via agentless training, then enables efficient adaptation to agentic frameworks. The approach uses mid-training on diverse data including natural diffs and PR commits, followed by cold-start fine-tuning to activate reasoning capabilities, and reinforcement learning with verifiable rewards on code-editing tasks. The resulting model powers both agentless inference through a BugFixer and TestWriter self-play mechanism, and agentic adaptation with minimal trajectory fine-tuning.

## Key Results
- Kimi-Dev achieves 60.4% accuracy on SWE-bench Verified, the highest among workflow-based methods
- After fine-tuning on 5,000 trajectories, Kimi-Dev powers SWE-agents to 48.6% pass@1, matching Claude 3.5 Sonnet
- RL prior requires only 2²³ tokens to match Base prior's performance at 1.5×2²⁸ tokens, demonstrating sample efficiency
- Self-play with generated tests scales performance from 48.0% to 60.4%, surpassing majority voting baselines

## Why This Works (Mechanism)

### Mechanism 1: Skill Prior Induction from Structured Workflow Training
Agentless training induces transferable skill priors—localization, code editing, and self-reflection—that enable efficient adaptation to multi-turn agentic frameworks. A staged training pipeline (mid-training → cold-start → RL) exposes the model to structured single-turn subproblems with verifiable rewards, building atomic capabilities that later compose into agentic behavior with minimal additional supervision. Core assumption: Skills learned in decomposed, single-turn settings transfer to end-to-end, multi-turn interactions without requiring full-trajectory training from scratch.

### Mechanism 2: Execution-Based Self-Play as Scalable Verification
Coordinated BugFixer and TestWriter outputs, validated via sandboxed execution, provide a scalable test-time verification signal that improves patch selection beyond majority voting. Generate N patches and M tests independently; filter invalid tests; execute each patch against each test; score patches by fail-to-pass and pass-to-pass ratios; select highest-scoring patch. Core assumption: Generated tests that fail pre-fix and pass post-fix provide a reliable proxy for issue resolution correctness.

### Mechanism 3: Outcome-Only RL with Curriculum Prompt Selection
Reinforcement learning using only binary execution outcomes, combined with adaptive difficulty curriculum, improves code-edit capability without process-based supervision. Initialize from cold-started model; sample multiple rollouts per prompt; normalize returns using average rewards as baseline; discard prompts with pass@16=0 initially; re-introduce harder prompts as success rates exceed thresholds; reinforce positive examples in later stages. Core assumption: Sparse outcome rewards suffice when the model already possesses sufficient prior knowledge to explore productively.

## Foundational Learning

- Concept: **Reinforcement Learning with Verifiable Rewards (RLVR)**
  - Why needed here: RL training relies on executable test outcomes (pass/fail) rather than learned reward models; understanding this is essential to see why outcome-only rewards work for code but not all tasks.
  - Quick check question: Why can SWE-bench provide verifiable rewards but open-ended conversation tasks cannot?

- Concept: **Skill Transfer / Prior-Based Learning**
  - Why needed here: The core hypothesis is that agentless training builds priors; you must distinguish between "training from scratch" vs. "adapting a strong prior" to evaluate efficiency claims.
  - Quick check question: What evidence in Figure 5 shows the RL prior is more sample-efficient than the Base prior for SWE-Agent adaptation?

- Concept: **Agentless vs. Agentic Paradigms**
  - Why needed here: The paper reframes these as complementary; you need to map the tradeoffs (modularity vs. flexibility, single-turn vs. multi-turn) to understand when each applies.
  - Quick check question: Why does the agentless framework struggle with the `sympy-sympy-20590` case (Appendix G.2) while the SWE-Agent with RL prior succeeds?

## Architecture Onboarding

- Component map: [Mid-Training Data] → [Cold-Start SFT] → [RL on Code-Edit Stage] → [Kimi-Dev Model] → [Agentless Inference: BugFixer + TestWriter self-play] or [SWE-Agent Adaptation: SFT on 5k trajectories]

- Critical path: Mid-training quality → cold-start reasoning activation → RL stability → self-play scaling at test time. Errors in mid-training data curation (e.g., contamination, low-quality diffs) propagate through all stages.

- Design tradeoffs:
  - **Agentless RL vs. End-to-End Agentic RL**: Agentless RL is more stable (single-turn, verifiable rewards) but limited exploration; agentic RL has higher ceiling but suffers from sparse rewards and trajectory instability.
  - **Self-Play vs. Ground-Truth Tests**: Self-play with generated tests is scalable but allows false positives; using ground-truth tests is more reliable but not available at inference time.
  - **Mid-Training Data Mix**: Upsampling synthetic reasoning/agentic data (4×) trades diversity for pattern reinforcement; filtering rules prune to ~50B diff tokens + ~20B commit tokens.

- Failure signatures:
  - **RL collapse**: Pass rate drops, response length stops growing (monitor Figure 3 patterns).
  - **TestWriter false positives**: Tests pass but don't actually verify the issue (inspect Appendix E.2 examples; check test coverage against ground-truth).
  - **SWE-Agent loops**: Infinite action patterns when prior is too weak; mitigate with stronger cold-start.
  - **Localization errors**: Agentless framework localizes to wrong files; agentic exploration may correct this.

- First 3 experiments:
  1. **Ablate mid-training scale**: Train mid-training variants at 50B, 100B, 150B tokens; evaluate BugFixer pass@1 to confirm scaling relationship.
  2. **Validate self-play scaling**: Run inference with 1×1, 10×10, 40×40 patch-test pairs on a held-out SWE-bench subset; confirm self-play > majority voting and identify TestWriter failure modes.
  3. **Prior efficiency test**: Fine-tune Base, MT, SFT, and RL priors on 100, 500, 2000, 5000 SWE-Agent trajectories; plot pass@1 curves to quantify sample-efficiency gains from agentless pretraining.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What advanced agentic Reinforcement Learning (RL) techniques can further improve SWE-agent performance beyond the current basic policy gradient approach?
- Basis in paper: [explicit] "We leave the exploration of more advanced agentic RL techniques for further improvement as future work."
- Why unresolved: The authors utilized a simple REINFORCE-based policy gradient method and suggest that more sophisticated algorithms could better leverage the skill priors.
- What evidence would resolve it: Experiments showing that implementing advanced RL algorithms (e.g., PPO with specific KL constraints, exploration bonuses, or hierarchical RL) yields higher pass@k rates on SWE-bench compared to the current recipe.

### Open Question 2
- Question: How can training recipes be optimized to enable stronger out-of-distribution and task-agnostic generalization for coding agents?
- Basis in paper: [explicit] "The exploration for recipes that enable stronger out-of-distribution and task-agnostic generalization is left for future work."
- Why unresolved: While the model transfers skills to SWE-bench Verified, performance gaps remain on SWE-bench-live and SWE-bench Multilingual, indicating the priors are not fully task-agnostic.
- What evidence would resolve it: A training recipe where a single model maintains consistent performance across diverse benchmarks without requiring separate fine-tuning for each domain.

### Open Question 3
- Question: How can the TestWriter skill prior be evaluated in isolation within agentic frameworks to better understand its contribution to issue resolution?
- Basis in paper: [explicit] "A more fine-grained evaluation that isolates the TestWriter skill prior is left for future work."
- Why unresolved: The current analysis conflates TestWriter skills with reflection skills, as performance gains are measured broadly from intermediate steps to completion.
- What evidence would resolve it: A new evaluation protocol or benchmark metric that directly measures the quality and coverage of generated tests independent of the subsequent bug-fixing success.

## Limitations

- Transfer hypothesis vulnerability: Agentic tasks requiring novel exploration strategies or tool-use patterns absent from agentless training may not benefit from skill priors
- Self-play verification reliability: Generated tests may contain false positives that impact both agentless and agentic performance
- Exploration dynamics uncertainty: RL training assumes sufficient prior knowledge for productive exploration, but collapse conditions are not fully characterized

## Confidence

**High Confidence**: The skill prior induction mechanism is well-supported by quantitative comparisons showing the RL prior requires significantly fewer tokens to match Base prior performance. The execution-based self-play scaling demonstrates consistent improvement from 48.0% to 60.4% pass@1 with increasing patch-test pairs.

**Medium Confidence**: The outcome-only RL training shows promising pass rate improvements during training, but long-term stability and exploration efficiency require further validation. Transfer efficiency claims rely on relative improvements that, while substantial, need replication across diverse task distributions.

**Low Confidence**: Generalization claims beyond SWE-bench to other software engineering domains or tool environments remain largely untested. The impact of TestWriter false positives on agentic adaptation quality is not fully characterized.

## Next Checks

1. **Cross-domain generalization test**: Evaluate Kimi-Dev with RL prior on software engineering tasks from repositories outside the SWE-bench training distribution, including tasks requiring novel API usage or different programming languages not well-represented in the original training corpus.

2. **False positive impact analysis**: Systematically measure the prevalence of TestWriter-generated false positive tests and quantify their impact on both agentless performance and downstream agentic adaptation quality. Compare results when using ground-truth tests versus self-generated tests.

3. **Exploration dynamics characterization**: Instrument the RL training stage to monitor exploration patterns, including diversity of code edits generated, frequency of novel solution discovery, and convergence behavior across multiple random seeds. Identify conditions under which exploration collapses and test mitigation strategies.