---
ver: rpa2
title: 'VicSim: Enhancing Victim Simulation with Emotional and Linguistic Fidelity'
arxiv_id: '2501.03139'
source_url: https://arxiv.org/abs/2501.03139
tags:
- user
- emotional
- human
- training
- vicsim
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VicSim, a novel victim simulator model for
  scenario-based training in public safety sectors. VicSim addresses key challenges
  in simulating realistic victim interactions, including informational faithfulness,
  emotional dynamics, and language style fidelity.
---

# VicSim: Enhancing Victim Simulation with Emotional and Linguistic Fidelity

## Quick Facts
- arXiv ID: 2501.03139
- Source URL: https://arxiv.org/abs/2501.03139
- Authors: Yerong Li; Yiren Liu; Yun Huang
- Reference count: 40
- Key outcome: VicSim outperforms GPT-4 in human-likeness, emotional fidelity, and linguistic realism for victim simulation in public safety training

## Executive Summary
This paper introduces VicSim, a novel victim simulator model for scenario-based training in public safety sectors. VicSim addresses key challenges in simulating realistic victim interactions, including informational faithfulness, emotional dynamics, and language style fidelity. The model employs a GAN-based training workflow, combining a Flan-T5-based discriminator with a Llama-2 chat-based generator, enhanced by key-information-based prompting. Evaluations demonstrate that VicSim outperforms GPT-4 in terms of human-likeness, achieving higher emotional and linguistic fidelity in simulated victim responses. The model successfully captures nuanced emotional expressions and grammar styles, providing a valuable tool for dispatcher training and enhancing preparedness for real-world incident handling.

## Method Summary
VicSim uses GAN-based training with Llama-2 7B chat as generator and FLAN-T5 as discriminator. The discriminator is fine-tuned on GoEmotions for emotion classification and a grammar error dataset for grammar classification. CoreNLP extracts keywords from scenarios to reduce hallucination. The prompt structure includes system message, scenario, dialogue history, and extracted keywords. Training uses standard GAN losses: L_G = -E[log D(G(p))], L_D = -E[log D(x)] - E[log(1 - D(G(p)))].

## Key Results
- VicSim outperforms GPT-4 in human-likeness, emotional fidelity, and linguistic realism
- Achieves higher Recall (22.69 vs 13.12) and F1 score (18.66 vs 14.59) for informational faithfulness compared to GPT-4
- Successfully mimics human emotional trajectories, with negative feelings peaking in the first 60% of dialogue
- Generates more human-like grammatical imperfections, particularly punctuation errors, compared to standard LLMs

## Why This Works (Mechanism)

### Mechanism 1: Adversarial Alignment of Linguistic "Imperfections"
The system trains a Flan-T5 discriminator to recognize "grammar and emotional cues" as indicators of synthetic content. During GAN-based training, the Llama-2 generator receives gradients that reward the reproduction of these cues to fool the discriminator. This mechanism assumes that grammatical "errors" (specifically missing punctuation) are a primary heuristic humans use to identify real victims, and that standard LLMs are overly grammatically correct.

### Mechanism 2: Hallucination Suppression via Entity Anchoring
By extracting keywords via CoreNLP and explicitly listing them in the prompt, the model is provided with a "ground truth" checklist. This directs the attention mechanism to attend to specific tokens rather than drawing on generic parametric knowledge. The assumption is that hallucination in this context is largely an information-retrieval problem rather than a reasoning problem, solvable by highlighting local context.

### Mechanism 3: Emotional Trajectory Modeling via Adversarial Feedback
The discriminator is fine-tuned on emotional datasets (GoEmotions). It penalizes the generator if the emotional flow deviates from the distribution seen in real victim data (e.g., early negative peaks). This mechanism assumes that real victims follow a predictable emotional arc which standard LLMs fail to replicate naturally without explicit guidance.

## Foundational Learning

- **Concept: Generative Adversarial Networks (GANs) for Text**
  - Why needed here: Understanding how the paper navigates the difficulty of backpropagation from discriminator to generator in discrete text is crucial for debugging the training loop.
  - Quick check question: How does the discriminator signal to the Llama-2 generator that a sentence is "too perfect"?

- **Concept: Hallucination vs. Factual Consistency**
  - Why needed here: The paper frames hallucination as a primary failure mode of GPT-4 in this context. Understanding that hallucinations often stem from the model "guessing" plausible details when prompts are vague is key to understanding why keyword augmentation works.
  - Quick check question: Why would an LLM invent a description of a "small black bag" when asked about a suspect?

- **Concept: Named Entity Recognition (NER)**
  - Why needed here: The architecture relies on CoreNLP to extract keywords to anchor the generation.
  - Quick check question: What happens to the "Informational Faithfulness" metric if the NER system fails to extract a critical proper noun (e.g., a location) from the source text?

## Architecture Onboarding

- **Component map:** CoreNLP (Entity Extraction) -> Llama-2 7B Chat (VicSim) -> Flan-T5 (Discriminator) -> Prompt Constructor
- **Critical path:**
  1. Ingest Scenario: Raw text description of incident
  2. Enrich: CoreNLP extracts entities (Person, Location, Time)
  3. Prompt: Construct context window with history + extracted keywords
  4. Generate: Llama-2 produces a candidate response
  5. Discriminate: Flan-T5 scores the candidate on "realness" (emotional/grammar alignment)
  6. Update: Generator weights adjust to maximize "realness" score

- **Design tradeoffs:**
  - Stability vs. Fidelity: The GAN approach improves human-likeness (punctuation errors) but is unstable to train compared to standard supervised fine-tuning (SFT)
  - Recall vs. Precision: The keyword augmentation increases Recall significantly, but the paper implies this may not fully solve Precision without the adversarial component

- **Failure signatures:**
  - "The panicky liar": When the scenario lacks detail, the model compensates with excessive negative emotion
  - "The robot with bad grammar": If the discriminator over-indexes on punctuation, the model may produce incoherent text just to satisfy the "missing period" heuristic

- **First 3 experiments:**
  1. Ablation on Keyword Augmentation: Run VicSim with NER keywords disabled. Measure the drop in Informational Faithfulness (Recall/F1) to isolate the impact of the hallucination suppression mechanism.
  2. Discriminator Feature Importance: Feed the discriminator sentences with only punctuation errors vs. only emotional misalignment. Determine which feature the discriminator relies on more heavily.
  3. Human Eval Stress Test: Provide scenarios specifically designed to be information-sparse. Ask human raters to evaluate if VicSim's "emotional overcompensation" (hallucination-induced emotion) feels realistic or uncanny.

## Open Questions the Paper Calls Out
None

## Limitations
- Training hyperparameters (learning rates, epochs, batch sizes) are unspecified, making faithful reproduction impossible
- The keyword-based approach may only partially solve hallucination, as evidenced by the "panicky liar" failure mode where emotional overcompensation occurs when scenarios lack detail
- The training data scope is limited to a single text-based reporting system from 2018-2019, raising questions about performance on different incident types or reporting formats

## Confidence
- **High Confidence:** The core GAN-based training approach with Llama-2 generator and FLAN-T5 discriminator is well-specified and the evaluation methodology is sound and reproducible
- **Medium Confidence:** The specific mechanisms by which adversarial training improves emotional trajectory modeling and linguistic imperfections are supported by results but lack detailed ablation studies to confirm causality
- **Low Confidence:** The hallucination suppression mechanism via keyword anchoring may not generalize beyond the specific dataset and CoreNLP extraction pipeline used, and the paper acknowledges this limitation with the emotional overcompensation failure mode

## Next Checks
1. **Ablation on Keyword Augmentation:** Run VicSim with NER keywords disabled to measure the drop in Informational Faithfulness (Recall/F1) metrics, isolating the impact of the hallucination suppression mechanism and testing generalizability beyond the specific CoreNLP pipeline.

2. **Discriminator Feature Importance Analysis:** Systematically test the discriminator's reliance on different error types by feeding it sentences with only punctuation errors vs. only emotional misalignment vs. only semantic incoherence, determining whether the generator is learning the intended multifaceted "human-likeness" or overfitting to punctuation.

3. **Information-Sparsity Stress Test:** Create scenarios specifically designed to be information-sparse (minimal details about suspects, locations, or events) and conduct human evaluation to assess whether VicSim's emotional overcompensation (hallucination-induced emotion) remains realistic or becomes uncanny, validating the paper's own noted failure mode.