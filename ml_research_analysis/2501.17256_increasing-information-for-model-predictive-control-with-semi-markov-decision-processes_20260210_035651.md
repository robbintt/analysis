---
ver: rpa2
title: Increasing Information for Model Predictive Control with Semi-Markov Decision
  Processes
arxiv_id: '2501.17256'
source_url: https://arxiv.org/abs/2501.17256
tags:
- control
- learning
- information
- system
- process
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of sample complexity in Learning-Based
  Model Predictive Control (LB-MPC) by introducing temporal abstraction through Semi-Markov
  Decision Processes (SMDP). The core idea is to improve information gathering during
  exploration by allowing temporally extended actions, where the system can remain
  under a fixed control for variable durations rather than updating at every time
  step.
---

# Increasing Information for Model Predictive Control with Semi-Markov Decision Processes

## Quick Facts
- arXiv ID: 2501.17256
- Source URL: https://arxiv.org/abs/2501.17256
- Reference count: 13
- Primary result: SMDP-based exploration with variable inter-decision times achieves higher Expected Information Gain and improved sample complexity compared to standard MDP approaches in LB-MPC

## Executive Summary
This paper addresses the sample complexity challenge in Learning-Based Model Predictive Control (LB-MPC) by introducing temporal abstraction through Semi-Markov Decision Processes (SMDP). The core innovation extends the Trajectory Information Planning (TIP) algorithm by allowing the agent to select both control actions and their duration (inter-decision times), rather than updating at every time step. This temporal abstraction increases the information gathered per sample by enabling transitions to more distant states while reducing temporal autocorrelation in observations. Experiments on Inverted Pendulum and Lorenz attractor systems demonstrate that SMDP-based exploration reaches optimal control performance in fewer iterations, particularly when inter-decision times are moderate (2-4 steps).

## Method Summary
The method extends TIP by incorporating inter-decision times into the control policy, creating SM-TIP. Instead of updating control at every time step, the agent selects a control action u and duration t ∈ {1, ..., t_max} to apply the action before the next decision. The Expected Information Gain (EIG) criterion is modified to account for these extended actions, using Monte Carlo estimation with bootstrapped future states from a Gaussian Process (GP) transition model. The algorithm iteratively selects (u, t) pairs maximizing EIG, executes them for t steps, collects the resulting transition, and updates the GP model. The control policy is periodically evaluated using an iCEM-based MPC controller. The approach is tested with t_max ∈ {1, 2, 4, 8} on two benchmark systems.

## Key Results
- SM-TIP achieves higher EIG values than standard TIP (t_max = 1), especially in early sampling iterations
- Sample complexity is improved: SM-TIP reaches optimal control performance in fewer iterations
- Best performance occurs with moderate inter-decision times (t_max = 2-4); t_max = 8 degrades due to bootstrapping error
- The method effectively decorrelates collected states, increasing effective information content per sample

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Temporal abstraction via variable inter-decision times increases state-space reachability during trajectory-constrained exploration, improving information density per sample
- Mechanism: By extending control space from U to U × T, the agent can hold control constant over multiple time steps, enabling transitions to more distant states unreachable under single-step MDP constraints. The expanded dataset support Supp(D_TIP) ⊆ Supp(D_SM-TIP) permits sampling from trajectory subsequences
- Core assumption: System dynamics are sufficiently smooth for predictable trajectories over short-to-moderate horizons (2-4 steps)
- Evidence anchors: Abstract claim of increased information for fixed sampling budget; Section 3.5 proof of dataset support inclusion; corpus evidence weak on SMDP for information-theoretic exploration
- Break condition: Performance degrades when t_max ≥ 8 due to accumulated bootstrapping error

### Mechanism 2
- Claim: Extended actions reduce temporal autocorrelation in sequential observations, yielding less redundant data per sampling iteration
- Mechanism: Consecutive states in trajectory-constrained settings are highly correlated. By selecting inter-decision times τ > 1, collected states become more decorrelated, increasing effective information content. EIG criterion implicitly captures this by favoring state-control pairs that maximize entropy reduction
- Core assumption: Underlying system exhibits multi-scale temporal dynamics where information content varies with time resolution
- Evidence anchors: Section 1 Figure 1 shows persistent autocorrelation; Section 5 demonstrates larger EIG for SM-TIP until one-fourth sampling budget; no direct corpus evidence
- Break condition: If system dynamics are too fast relative to t_max, critical state transitions may be skipped, losing information

### Mechanism 3
- Claim: Modified EIG criterion (EIG_SM-TIP) correctly quantifies information gain for temporally-extended actions by conditioning on inter-decision time as decision variable
- Mechanism: EIG formulation extends original TIP criterion by including inter-decision time in conditioning: H[X_{κ_n+t+1} | D_n, X_{κ_n} = x, U_{κ_n:κ_n+t} = u, κ_n] minus expected posterior entropy given optimal trajectory estimate. Allows acquisition function to jointly optimize (u, t) pairs
- Core assumption: Monte Carlo estimation with m samples provides sufficient approximation of EIG integral; GP posterior entropy is tractable via closed-form Gaussian expressions
- Evidence anchors: Section 3.5 Equation 10 formal definition; Section 4 describes Monte Carlo estimation with bootstrapped future states; corpus paper on information-theoretic exploration but not SMDP extension
- Break condition: Monte Carlo estimation variance or GP bootstrapping bias can mislead EIG estimates, particularly when model is poorly calibrated early in training

## Foundational Learning

- **Gaussian Process Regression for Dynamics Modeling**
  - Why needed here: GP provides uncertainty quantification required for EIG computation; posterior entropy is analytically tractable
  - Quick check question: Given dataset D = {(x_i, u_i, x'_i)}, can you compute GP posterior mean and covariance at query point (x*, u*)?

- **Expected Information Gain (EIG) / Mutual Information**
  - Why needed here: EIG serves as acquisition function guiding exploration; understanding as negative conditional mutual information clarifies why it promotes informative sampling
  - Quick check question: Explain why maximizing EIG is equivalent to minimizing conditional mutual information I[H*_T ; X_{n+1} | D_n, X_n, U_n]

- **Semi-Markov Decision Processes (SMDP)**
  - Why needed here: SMDPs generalize MDPs by allowing random inter-decision times; understanding transition probability P_SMDP(dx' | (x, (u, t))) is essential for implementation
  - Quick check question: In SMDP with inter-decision time τ = 3 and constant control u, how many underlying MDP transitions occur before next decision epoch?

## Architecture Onboarding

- **Component map**: True System -> GP Transition Model -> EIG Estimator -> Action Selector -> MPC Controller -> Dataset Buffer -> GP Transition Model (feedback)

- **Critical path**: 1) Initialize GP with prior (mean μ, kernel Σ) 2) At decision epoch κ_n: sample candidate (u, t) pairs uniformly 3) For each candidate, compute EIG_SM-TIP via Monte Carlo (bootstrapping future states with GP) 4) Execute best (u*, t*) for t* steps, collect final transition (x_{κ_n}, u*, x_{κ_n+t*}) 5) Update GP with new observation 6) Every 2 iterations: evaluate MPC policy on true system, log objective J

- **Design tradeoffs**: 
  - t_max selection: Larger values increase state-space coverage but amplify bootstrapping error. Paper finds t_max ∈ {2, 4} optimal; t_max = 8 fails
  - Monte Carlo samples (m): More samples improve EIG estimation accuracy but increase compute cost
  - GP kernel choice: Not specified; RBF/SE kernels common for smooth dynamics. Poor kernel selection degrades uncertainty estimates
  - Assumption: Non-causal EIG requires looking ahead; implementation uses GP prediction which introduces bias

- **Failure signatures**:
  - EIG consistently high but control performance flat: GP overconfident, uncertainty estimates unreliable
  - Inter-decision times always at t_max: EIG not properly discriminating; check Monte Carlo sampling
  - Performance degrades with more samples: Model overfitting or EIG guiding to uninformative regions
  - t_max = 8 underperforms t_max = 1: Bootstrapping error dominates; reduce t_max or improve GP model

- **First 3 experiments**:
  1. Baseline comparison: Run TIP (t_max = 1) vs. SM-TIP (t_max = 2, 4) on Inverted Pendulum with n_max = 200. Track EIG evolution and final objective J. Verify Figure 4(b) pattern: SM-TIP converges faster
  2. Ablation on t_max: Systematically vary t_max ∈ {1, 2, 4, 6, 8} on Lorenz system. Plot EIG curves and identify crossover point where larger t_max hurts performance due to bootstrapping error
  3. Inter-decision time analysis: Log distribution of selected τ values during training. Confirm τ > 1 is frequently chosen but not always at t_max, indicating EIG is meaningfully optimizing duration

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness critically depends on GP bootstrapping reliability for multi-step predictions; no theoretical bounds on approximation error provided
- Method assumes sufficient system regularity for temporal abstraction to be beneficial—highly chaotic or discontinuous dynamics may violate this assumption
- Performance range appears narrow and system-specific (optimal t_max ∈ {2, 4}); scalability to high-dimensional systems unverified

## Confidence

- **High confidence**: Core theoretical framework extending EIG to SMDPs is sound. Mathematical derivation of EIG_SM-TIP from TIP is rigorous and proof of dataset support inclusion is correct.
- **Medium confidence**: Empirical results showing improved sample complexity with moderate t_max values are convincing. Inverted Pendulum and Lorenz attractor experiments demonstrate expected pattern, though lacking statistical significance tests.
- **Low confidence**: Claim that EIG_SM-TIP provides superior exploration in general settings. While experiments support this for tested systems, performance on high-dimensional or highly nonlinear systems remains unverified. Paper doesn't address computational overhead or scalability concerns.

## Next Checks

1. **Model uncertainty validation**: Systematically measure GP predictive uncertainty at τ-step ahead predictions during training. If variance grows excessively for larger τ, this would confirm bootstrapping error limitation observed when t_max = 8 underperforms.

2. **Statistical significance testing**: Apply paired t-tests or bootstrap confidence intervals to control objective J across 10 random seeds for each method. This would verify whether performance differences between SM-TIP and TIP are statistically significant rather than random variation.

3. **Computational complexity analysis**: Measure and report wall-clock time per iteration for different t_max values. Given EIG_SM-TIP requires Monte Carlo estimation over larger action space (U × T), understanding computational trade-off between sample complexity gains and per-iteration cost is essential for practical deployment.