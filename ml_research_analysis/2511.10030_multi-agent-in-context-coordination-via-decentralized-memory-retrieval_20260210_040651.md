---
ver: rpa2
title: Multi-agent In-context Coordination via Decentralized Memory Retrieval
arxiv_id: '2511.10030'
source_url: https://arxiv.org/abs/2511.10030
tags:
- learning
- trajectories
- tasks
- multi-agent
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MAICC tackles rapid adaptation to unseen cooperative multi-agent
  tasks in decentralized settings where agents have limited observability and no individual
  rewards. It introduces a centralized embedding model (CEM) trained on team-level
  trajectories, with decentralized embedding models (DEMs) distilling CEM knowledge
  to capture individual-level information.
---

# Multi-agent In-context Coordination via Decentralized Memory Retrieval

## Quick Facts
- arXiv ID: 2511.10030
- Source URL: https://arxiv.org/abs/2511.10030
- Reference count: 14
- Multi-agent in-context RL method achieves faster adaptation and higher returns than ICRL and multi-task MARL baselines on LBF and SMAC benchmarks

## Executive Summary
MAICC introduces a decentralized memory retrieval approach for multi-agent in-context reinforcement learning, enabling rapid adaptation to unseen cooperative tasks under partial observability. The method trains a centralized embedding model (CEM) on team-level trajectories, then distills this knowledge to decentralized embedding models (DEMs) that capture individual-level information. During testing, agents retrieve relevant trajectories from a hybrid memory (offline data plus online experiences) weighted by exponential time decay, using a hybrid utility score that balances team and predicted individual returns for effective credit assignment.

## Method Summary
MAICC addresses decentralized cooperative multi-agent tasks where agents have limited observability and no individual rewards. The method employs a two-phase training approach: first training a centralized embedding model (CEM) on team-level trajectories with three prediction heads (actions, rewards, next observations), then distilling this knowledge to decentralized embedding models (DEMs) via KL divergence. During testing, agents retrieve relevant trajectories from a hybrid memory combining offline data and online experiences, weighted by exponential time decay. A hybrid utility score balances team and predicted individual returns for effective credit assignment. The method demonstrates superior performance on Level-Based Foraging and SMAC benchmarks compared to existing ICRL and multi-task MARL baselines.

## Key Results
- MAICC achieves faster adaptation and higher returns than existing ICRL and multi-task MARL baselines on Level-Based Foraging and SMAC benchmarks
- Ablation studies show performance drops of ~20-30% when using only offline or only online memory components
- Visualizations confirm effective trajectory embeddings with same-task trajectories clustering together in t-SNE space

## Why This Works (Mechanism)
MAICC enables rapid adaptation by combining centralized training of team-level embeddings with decentralized execution using distilled knowledge. The exponential time decay heuristic balances exploration of new experiences with exploitation of prior knowledge, while the hybrid utility score provides effective credit assignment in the absence of individual rewards. The retrieval-based approach allows agents to leverage relevant past experiences for decision-making in novel situations.

## Foundational Learning
- **Dec-POMDP**: Decentralized Partially Observable Markov Decision Process - why needed: formal framework for multi-agent coordination under partial observability; quick check: verify task formulation matches Dec-POMDP assumptions
- **Memory-based RL**: Using past experiences for decision-making - why needed: enables in-context learning and adaptation; quick check: confirm memory construction and retrieval mechanisms function correctly
- **Knowledge Distillation**: Transferring knowledge from larger to smaller models - why needed: enables decentralized execution while preserving centralized training benefits; quick check: verify KL divergence between CEM and DEM outputs
- **Multi-agent Coordination**: Joint decision-making under partial observability - why needed: core challenge being addressed; quick check: verify team performance exceeds individual agent performance
- **Exponential Time Decay**: Heuristic for weighting recent vs. past experiences - why needed: balances exploration and exploitation in memory retrieval; quick check: verify retrieval quality varies with decay parameter

## Architecture Onboarding

**Component Map**: Offline Data -> CEM -> DEMs -> Decision Model -> Agent Actions; Online Buffer -> Hybrid Memory -> Retrieval -> Decision Model

**Critical Path**: Offline data collection -> CEM training -> DEM distillation -> Online adaptation -> Hybrid memory retrieval -> Decision making

**Design Tradeoffs**: Centralized training enables better global coordination understanding but requires distillation to enable decentralized execution; exponential time decay is simple but may not capture uncertainty in data quality.

**Failure Signatures**: 
- Degraded retrieval quality when RTG token is included (trajectories fragment by return values)
- Poor adaptation when memory composition is imbalanced (β=0 or β=1)
- "Lazy agent" behavior when individual return prediction is inaccurate

**3 First Experiments**:
1. Verify CEM training converges and produces meaningful trajectory embeddings by visualizing t-SNE of same-task trajectories
2. Test DEM distillation quality by comparing CEM vs DEM retrieval performance on held-out validation tasks
3. Validate hybrid utility score by ablating α parameter and measuring impact on credit assignment quality

## Open Questions the Paper Calls Out
1. Can incorporating uncertainty-based metrics into the memory construction mechanism improve performance and generalization compared to the current exponential time decay heuristic?
2. To what extent does the information bottleneck between the Centralized Embedding Model (CEM) and Decentralized Embedding Models (DEMs) limit coordination in tasks requiring extensive global state inference?
3. How robust is the hybrid utility score when the predicted individual return (R̃) is systematically biased due to deceptive local observations?

## Limitations
- Training duration and convergence criteria unspecified, making reproduction difficult
- Exact tokenization scheme for observations and actions not fully specified
- Intra-team visibility causal mask implementation details incomplete

## Confidence
- **High confidence**: Central claims about MAICC's ability to adapt to unseen cooperative tasks via decentralized memory retrieval, supported by quantitative results across multiple benchmarks
- **Medium confidence**: Practical applicability due to significant implementation gaps in hyperparameter specification and architectural details
- **Low confidence**: Long-term generalization beyond tested benchmarks and robustness to highly deceptive local observations

## Next Checks
1. Implement the GPT-2-based embedding models with 64-dim tokens, 8 layers, 8 heads, and verify the causal mask implementation for intra-team visibility
2. Systematically vary the online memory weight β from 0 to 1 in increments of 0.25 to confirm the reported 20-30% performance drop when using only offline or only online memory
3. Remove the RTG token from the embedding model and compare retrieval quality using t-SNE visualizations to verify the claim that including RTG prevents trajectory fragmentation