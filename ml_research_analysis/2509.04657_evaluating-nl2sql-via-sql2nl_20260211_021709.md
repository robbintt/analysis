---
ver: rpa2
title: Evaluating NL2SQL via SQL2NL
arxiv_id: '2509.04657'
source_url: https://arxiv.org/abs/2509.04657
tags:
- queries
- paraphrased
- schema
- original
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel framework to evaluate the robustness
  of Natural Language to SQL (NL2SQL) models to linguistic variation. It uses SQL-to-NL
  (SQL2NL) to generate semantically equivalent but lexically diverse paraphrased queries,
  isolating the effect of linguistic variation from schema alignment errors.
---

# Evaluating NL2SQL via SQL2NL

## Quick Facts
- arXiv ID: 2509.04657
- Source URL: https://arxiv.org/abs/2509.04657
- Reference count: 40
- State-of-the-art NL2SQL models show 10-20% accuracy drops on paraphrased queries

## Executive Summary
This paper introduces a novel evaluation framework for NL2SQL models using SQL2NL to generate semantically equivalent but lexically diverse paraphrases of natural language queries. The approach isolates linguistic variation from schema alignment errors, revealing that current state-of-the-art models are significantly more brittle than standard benchmarks suggest. The study demonstrates that models like LLaMa3.3-70B and LLaMa3.1-8B experience substantial accuracy drops (10.23% and nearly 20% respectively) when evaluated on paraphrased queries, with smaller models being disproportionately affected. The authors propose Pass@K as a more robust evaluation metric and demonstrate that SQL2NL can be used both for evaluation and to improve model robustness through fine-tuning.

## Method Summary
The paper proposes using SQL-to-Natural-Language (SQL2NL) generation to create paraphrased versions of NL2SQL queries, isolating the effect of linguistic variation from schema alignment errors. The framework evaluates model robustness by comparing performance on original queries versus paraphrased versions generated by SQL2NL models. The study uses execution accuracy as the primary metric and introduces Pass@K as an alternative evaluation approach. The methodology involves generating paraphrases across different query complexities, datasets, and domains to comprehensively assess model robustness. The authors also demonstrate that the framework can be used for training and fine-tuning NL2SQL models to improve their robustness to linguistic variation.

## Key Results
- LLaMa3.3-70B shows 10.23% drop in execution accuracy on paraphrased Spider queries
- LLaMa3.1-8B suffers nearly 20% accuracy drop on paraphrased queries
- GPT-4o mini is disproportionately affected by paraphrasing, showing significant robustness degradation
- Pass@K metric proposed as more robust alternative to traditional evaluation metrics

## Why This Works (Mechanism)
The SQL2NL framework works by systematically introducing linguistic variation while preserving semantic meaning and schema alignment. By generating paraphrases from SQL rather than natural language, the approach ensures that semantic equivalence is maintained while varying surface-level linguistic features. This isolates the effect of linguistic variation from other sources of error, allowing for targeted evaluation of model robustness to paraphrasing. The method reveals that current NL2SQL models are brittle to surface-level linguistic changes despite maintaining semantic correctness, suggesting that models rely heavily on pattern matching rather than true semantic understanding.

## Foundational Learning

**SQL-to-NL generation**: Converting structured SQL queries into natural language paraphrases; needed to create controlled linguistic variations while preserving semantic meaning; quick check: verify generated paraphrases maintain semantic equivalence through execution testing.

**Execution accuracy metric**: Measuring whether generated SQL queries execute correctly against target databases; needed to provide objective evaluation of semantic correctness; quick check: validate accuracy against ground truth SQL execution results.

**Pass@K evaluation**: Computing accuracy as the fraction of queries where at least one of K generated candidates executes correctly; needed to account for multiple valid paraphrases and improve evaluation robustness; quick check: test sensitivity to different K values across model families.

## Architecture Onboarding

**Component map**: SQL query -> SQL2NL generator -> Paraphrased NL queries -> NL2SQL model -> Generated SQL -> Execution engine -> Accuracy metrics

**Critical path**: SQL2NL generation → NL2SQL model inference → SQL execution → Accuracy computation

**Design tradeoffs**: The framework trades off between comprehensive evaluation coverage and computational cost, as generating multiple paraphrases and testing across K candidates increases evaluation time but provides more robust assessment of model capabilities.

**Failure signatures**: Major accuracy drops on paraphrased queries indicate brittleness to linguistic variation; smaller models showing disproportionate degradation suggest vulnerability to paraphrasing complexity; inconsistent Pass@K performance reveals sensitivity to candidate diversity.

**3 first experiments**:
1. Compare execution accuracy of NL2SQL models on original vs. paraphrased queries using Spider dataset
2. Test Pass@K metric sensitivity by varying K values from 1 to 10 across different model families
3. Fine-tune NL2SQL models using SQL2NL-generated paraphrases and measure robustness improvement

## Open Questions the Paper Calls Out
- Generalizability of SQL2NL-generated paraphrases across different database schemas and domains
- Semantic equivalence verification for complex SQL operations beyond basic joins and aggregations
- Hyperparameter sensitivity of Pass@K metric across diverse model architectures and query complexities
- Extension of framework to real-world schemas with domain-specific terminology and nuanced linguistic variations

## Limitations
- SQL2NL framework limited to basic SQL operations, potentially missing nuanced linguistic variations in complex queries
- Evaluation relies on execution accuracy which may not capture semantic equivalence when syntactically valid but semantically different SQL is produced
- Pass@K metric introduces hyperparameter sensitivity requiring further investigation across diverse model architectures
- Study focuses on Spider and GeoQuery datasets, limiting generalizability to more complex real-world schemas

## Confidence

**High confidence**: State-of-the-art NL2SQL models show substantial accuracy drops (10-20%) on paraphrased queries, as empirically demonstrated across multiple model families and datasets.

**Medium confidence**: SQL2NL can improve model robustness through fine-tuning, based on preliminary evidence, though extensive ablation studies are needed for stronger validation.

**Low confidence**: Pass@K universally outperforms traditional metrics across all model types and datasets, as analysis is limited to specific model subset and requires broader testing.

## Next Checks

1. Test SQL2NL robustness across diverse database schemas beyond Spider and GeoQuery, including domain-specific databases with specialized terminology

2. Conduct human evaluation studies to verify semantic equivalence between original and paraphrased queries, particularly for complex SQL operations

3. Perform ablation studies on Pass@K hyperparameter sensitivity to determine optimal K values across different model families and query complexities