---
ver: rpa2
title: 'SwiftSolve: A Self-Iterative, Complexity-Aware Multi-Agent Framework for Competitive
  Programming'
arxiv_id: '2510.22626'
source_url: https://arxiv.org/abs/2510.22626
tags:
- code
- task
- complexity
- agent
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "SwiftSolve is a multi-agent system for competitive programming\
  \ that integrates algorithmic planning, empirical profiling, and complexity-guided\
  \ repair. It uses specialized agents\u2014Planner, Static Pruner, Coder, Profiler,\
  \ and Complexity Analyst\u2014to generate correct and efficient code under strict\
  \ time/memory constraints."
---

# SwiftSolve: A Self-Iterative, Complexity-Aware Multi-Agent Framework for Competitive Programming

## Quick Facts
- **arXiv ID:** 2510.22626
- **Source URL:** https://arxiv.org/abs/2510.22626
- **Reference count:** 22
- **Key outcome:** SwiftSolve achieves 61.54% pass@1 and 80.77% solved@≤3 on 26 competitive programming problems, outperforming single-agent baselines at ~2× runtime overhead.

## Executive Summary
SwiftSolve is a multi-agent system for competitive programming that integrates algorithmic planning, empirical profiling, and complexity-guided repair. It uses specialized agents—Planner, Static Pruner, Coder, Profiler, and Complexity Analyst—to generate correct and efficient code under strict time/memory constraints. The Static Pruner filters high-risk plans before code generation, while the Profiler empirically measures runtime and memory growth, and the Complexity Analyst fits log-log growth curves to classify complexity and dispatch targeted patches.

Evaluated on 26 problems (16 BigO Bench, 10 Codeforces Div. 2) with 78 task–seed runs, SwiftSolve achieves 61.54% pass@1 and 80.77% solved@≤3 with marginal latency increase (12.4 s avg). Aggregate run-level success is 73.08%. Failures are predominantly resource-bound, confirming efficiency as the main barrier. Compared to a Claude Opus 4 single-agent baseline, SwiftSolve improves run-level success (73.1% vs. 52.6%) at ~2× runtime overhead (12.4 s vs. 6.8 s). Efficiency metrics and complexity-fit accuracy demonstrate that profiling and replanning reduce inefficiency while preserving correctness.

## Method Summary
SwiftSolve uses a five-agent pipeline: Planner (Claude 4 Opus) generates algorithm sketches from problem statements; Static Pruner applies deterministic rules to filter high-risk plans; Coder (GPT-4.1) converts sketches to ISO C++17; Profiler compiles and executes code in a POSIX sandbox with fixed input schedule; Complexity Analyst fits log-log runtime curves to classify complexity and dispatch targeted patches. The system supports up to 3 replan attempts per task, stopping when improvement gain falls below threshold. Profiling uses wall-clock time and RSS measurements on input sizes [0, 1, 10^3, 5×10^3, 10^4, 5×10^4, 10^5].

## Key Results
- SwiftSolve achieves 61.54% pass@1 and 80.77% solved@≤3 on 26 competitive programming problems
- System improves run-level success from 52.6% to 73.1% compared to single-agent baseline
- Resource-bound failures dominate (26.9%), confirming efficiency as primary challenge
- Average latency is 12.4 seconds, ~2× single-agent baseline runtime

## Why This Works (Mechanism)

### Mechanism 1: Static Risk Filtering
- **Claim:** Deterministic static analysis reduces wasted compute by rejecting algorithmically risky plans before expensive code generation.
- **Mechanism:** Static Pruner applies heuristic rules (e.g., rejecting "while-loop multiplicity > 2 at n ≥ 10^5") to Planner's text output, acting as low-cost gatekeeper.
- **Core assumption:** Pruner's regex and keyword-based heuristics accurately correlate with resource violations in constraint regime.
- **Evidence anchors:** Abstract states "deterministic Static Pruner filters high-risk plans"; Section 3.2.2 describes rule-based rejection before LLM generation.
- **Corpus anchor:** Corpus papers discuss profiling and multi-agent systems generally but don't validate static pruning rules for competitive programming.
- **Break condition:** Over-aggressive heuristics harm success rates if they reject valid but complex-looking algorithms.

### Mechanism 2: Empirical Complexity Inference via Log-Log Fitting
- **Claim:** Fitting regression lines to empirical runtime data in log-log space estimates algorithmic complexity and detects efficiency bugs.
- **Mechanism:** Profiler executes code on fixed input schedule; Complexity Analyst calculates slope $s$ and $R^2$, mapping to complexity classes and flagging inefficiencies.
- **Core assumption:** Generated input schedule exercises worst-case complexity and wall-clock time scales predictably in sandbox.
- **Evidence anchors:** Abstract mentions "Profiler compiles and executes candidates... Complexity Analyst fits log-log growth"; Section 3.2.5 defines efficiency thresholds.
- **Corpus anchor:** "PRAGMA: A Profiling-Reasoned Multi-Agent Framework" supports profiling-guided reasoning for optimization.
- **Break condition:** Fails if task-agnostic generator doesn't trigger worst-case behavior, leading to "false efficient" classification.

### Mechanism 3: Targeted Patch Dispatch
- **Claim:** Routing feedback to Coder (minor optimization) or Planner (algorithmic overhaul) based on complexity gap improves convergence speed.
- **Mechanism:** Complexity Analyst compares fitted complexity class against problem constraints, dispatching "algorithmic overhaul" or "minor fixes" accordingly.
- **Core assumption:** Analyst can synthesize correct natural-language patch interpretable by receiving agent.
- **Evidence anchors:** Abstract mentions "dispatch targeted patches to either Planner or Coder"; Section 3.2.5 describes patch routing logic.
- **Corpus anchor:** "Distributed Approach to Haskell..." suggests specialized agents improve tasks, but efficacy of complexity-guided routing is limited in corpus.
- **Break condition:** Downstream agent misunderstands patch or complexity analysis was false positive, causing non-productive iteration loop.

## Foundational Learning

- **Concept: Log-Log Regression for Power Laws**
  - **Why needed here:** Core novelty relies on fitting $T(n) \approx C \cdot n^s$; understanding straight line in log-log space indicates power law relationship is essential.
  - **Quick check question:** If slope $s \approx 2.0$ but $R^2$ is very low (< 0.5), what does that imply about $O(n^2)$ classification reliability? (Answer: Data is noisy or not clean power law; fit is unreliable).

- **Concept: Competitive Programming Constraints (TLE/MLE)**
  - **Why needed here:** Domain requires code to pass strict resource limits (e.g., 2s, 256MB); system explicitly optimizes for these thresholds.
  - **Quick check question:** Why is passing all unit tests insufficient for this system? (Answer: Code can be logically correct but too slow/large for contest constraints).

- **Concept: POSIX Sandboxing (`rlimit`, `/usr/bin/time`)**
  - **Why needed here:** Profiler relies on accurate measurement of wall time and RSS; understanding OS-level tools helps explain measurement noise or failures.
  - **Quick check question:** Why might wall-clock time be noisy in shared sandbox vs CPU cycles? (Answer: System load, context switching, and I/O blocking affect wall time independently of algorithm efficiency).

## Architecture Onboarding

- **Component map:** Planner (LLM) -> Natural Language $\to$ Algorithm Sketch & Constraints $\to$ Static Pruner (Deterministic) -> Sketch $\to$ (Pass/Fail) $\to$ Coder (LLM) -> Sketch $\to$ ISO C++17 $\to$ Profiler (Sandbox) -> C++ Binary $\to$ Runtime/Memory Vectors $\to$ Complexity Analyst (Regression/LLM) -> Vectors $\to$ Complexity Class + Patch $\to$ Controller orchestrates loop.

- **Critical path:** The Profile $\to$ Fit $\to$ Verdict sequence is critical; heaviest compute is sandbox execution and LLM calls, while regression fit is trivial but logically critical.

- **Design tradeoffs:** Latency vs. Success ($\sim 2\times$ runtime overhead for higher success); Task-Agnostic Inputs simplify pipeline but risk "false efficient" verdicts on algorithms requiring specific data shapes.

- **Failure signatures:** Static Prune Failed (Planner suggests recursion/loops exceeding thresholds); Ambiguous Fit ($R^2 < 0.7$ or uncertain slope ranges); Resource Exhaustion (Profiler reports $+\infty$ for runtime/memory).

- **First 3 experiments:** 1) Input Generator Ablation: Swap task-agnostic generator for task-specific adversarial generator on Quicksort to verify complexity detection accuracy improvement. 2) Pruner Sensitivity: Relax Static Pruner rules (e.g., allow while count = 3) to measure trade-off between rejected plans and false negatives. 3) Gain Threshold Tuning: Vary diminishing-returns threshold to see if more iterations resolve marginal TLEs or waste time.

## Open Questions the Paper Calls Out

- **Open Question 1:** How much does each agent component contribute to performance gains? The paper reports aggregate performance but doesn't isolate individual component contributions beyond noting pruner filters some plans. A systematic ablation study measuring success rate when disabling each agent would resolve this.

- **Open Question 2:** Can slope-based complexity classification be replaced with more robust non-parametric methods, particularly for small input sizes? Current classification uses fixed slope thresholds with LLM fallback; instability at small n may cause misclassification affecting repair decisions. Comparative evaluation of alternative classifiers on BigO(Bench) would resolve this.

- **Open Question 3:** Would task-specific adversarial input generators improve profiling signal quality compared to current task-agnostic generator? The default generator may miss worst-case execution paths, potentially accepting solutions that would TLE on adversarial inputs. Evaluation with pluggable adversarial generators would measure TLE detection rate and solution robustness.

- **Open Question 4:** Do per-task adaptive iteration budgets outperform fixed two-replan cap? The fixed cap may under-invest in hard problems while wasting iterations on easy ones. Experiments with confidence-based early stopping would report success vs. latency trade-offs.

## Limitations
- Prompt sensitivity: System relies heavily on prompt engineering for Planner, Coder, and Complexity Analyst; exact system prompts are referenced but not fully disclosed.
- Sandbox fidelity: Profiler uses POSIX sandboxing with `rlimit` and `/usr/bin/time`; real-world measurement noise or resource contention could affect complexity fit accuracy.
- Task-agnostic input generator: Default logarithmic input schedule is simple and may not trigger worst-case behavior for all algorithms, potentially leading to "false efficient" classifications.

## Confidence
- **High:** Overall multi-agent architecture and integration of profiling + complexity analysis are well-specified and reproducible.
- **Medium:** Empirical results (61.54% pass@1, 80.77% solved@≤3) are likely accurate for stated 26-task benchmark, but exact problem IDs and full prompt text needed for faithful replication.
- **Low:** Specific efficacy of Static Pruner rules and precise patch routing logic depend on unstated heuristic thresholds and prompt details.

## Next Checks
1. **Input Generator Ablation:** Replace task-agnostic logarithmic input schedule with task-specific adversarial generator for one algorithm (e.g., Quicksort) to verify if complexity detection accuracy improves.
2. **Pruner Sensitivity Analysis:** Relax Static Pruner's deterministic rules (e.g., allow up to 3 while-loops) to quantify trade-off between rejecting risky plans and potentially rejecting valid solutions.
3. **Gain Threshold Tuning:** Vary diminishing-returns threshold (`delta`) to determine if allowing more than 3 iterations resolves marginal TLE cases or simply increases latency without improving success rates.