---
ver: rpa2
title: Context Selection and Rewriting for Video-based Educational Question Generation
arxiv_id: '2504.19406'
source_url: https://arxiv.org/abs/2504.19406
tags:
- context
- question
- questions
- answer
- lecture
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating educational questions
  from real-world lecture videos, which contain noisy and lengthy transcripts. The
  authors introduce a new dataset, AIRC, featuring authentic classroom lectures and
  educator-created multiple-choice questions, highlighting the limitations of existing
  datasets based on idealized texts.
---

# Context Selection and Rewriting for Video-based Educational Question Generation

## Quick Facts
- arXiv ID: 2504.19406
- Source URL: https://arxiv.org/abs/2504.19406
- Reference count: 40
- Key outcome: Introduces COSER framework for generating educational questions from noisy lecture videos, achieving up to 41.09 NLI score with multimodal integration and rewriting.

## Executive Summary
This paper addresses the challenge of generating educational questions from real-world lecture videos, which contain noisy and lengthy transcripts. The authors introduce a new dataset, AIRC, featuring authentic classroom lectures and educator-created multiple-choice questions, highlighting the limitations of existing datasets based on idealized texts. To tackle the context selection and rewriting problem, they propose COSER, a framework that uses large language models to dynamically select relevant transcript segments and keyframes based on answer relevance and temporal proximity, then rewrites them into concise, answer-containing knowledge statements. Experiments with three different LLMs show that COSER consistently outperforms baselines, achieving up to 41.09 NLI score with multimodal integration and rewriting, and demonstrates the importance of explicit answer incorporation and selective context extraction for improving question quality and educational alignment.

## Method Summary
The COSER framework addresses video-based educational question generation by first processing lecture videos into transcripts and keyframes with descriptions. It then uses LLM-based context selection via Chain-of-Thought prompting to identify relevant segments based on answer relevance and temporal proximity. Selected contexts are rewritten into atomic, answer-containing knowledge statements to improve logical connection with the target answer. Finally, a question generator LLM produces multiple-choice questions using the rewritten context. The framework integrates both transcript and visual information from keyframes, comparing various selection and combination strategies across three different LLMs.

## Key Results
- COSER achieves up to 41.09 NLI score with multimodal integration and rewriting
- Context selection improves question quality over naive approaches (all context or random segments)
- Chain-of-Thought prompting outperforms direct extraction for context selection
- Context rewriting improves NLI scores in 24 out of 30 experimental settings and consistently boosts RQUGE (answerability) score

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic context selection via Chain-of-Thought (CoT) prompting yields more relevant contexts than fixed windows.
- Mechanism: A prompt guides an LLM to first list all potentially relevant segments, then refine its selection on a sentence-by-sentence basis. This process explicitly considers answer relevance and temporal proximity, filtering out irrelevant content that distracts generation models.
- Core assumption: The base LLM has sufficient reasoning capabilities to identify relevant segments without task-specific fine-tuning.
- Evidence anchors:
  - [abstract] “...our framework selects contexts from both lecture transcripts and video keyframes based on answer relevance and temporal proximity.”
  - [Page 7] CoT consistently improves NLI scores over both the “All” and “Rule-Best” baselines.
  - [corpus] Direct corpus evidence for CoT in EQG is weak; a related paper (PreQRAG) supports the broader value of query preprocessing in RAG pipelines.
- Break condition: If the lecture is highly unstructured or the target answer concept is ambiguously expressed, the selection may fail to identify the necessary context.

### Mechanism 2
- Claim: Context rewriting into atomic, answer-containing knowledge statements improves question-answer alignment.
- Mechanism: The selected raw context (which may be noisy) is rewritten into a set of concise statements. This process explicitly includes the answer span word-for-word, resolves ambiguous references, and organizes information at multiple levels of granularity.
- Core assumption: The rewriting LLM preserves factual accuracy and does not introduce hallucinations or alter key technical terms.
- Evidence anchors:
  - [abstract] “...rewrite the context into concise, answer-containing knowledge statements, to enhance the logical connection between the contexts and the desired answer.”
  - [Page 7-8] Rewriting improves NLI scores in 24 out of 30 experimental settings and consistently boosts the RQUGE (answerability) score.
  - [corpus] Direct evidence in corpus for this specific EQG rewriting step is limited; related work in query rewriting for RAG supports the general principle of improving retrieval/generation via rewriting.
- Break condition: If the rewriting step omits crucial constraints or incorrectly paraphrases technical terminology, the resulting questions may be misaligned or factually incorrect.

### Mechanism 3
- Claim: Integrating multi-modal information from transcripts and keyframes enhances question quality beyond single-modality inputs.
- Mechanism: Visual content from slides (captured as keyframes) provides structured, high-level concepts that complement the more detailed but noisy spoken transcript. The framework selects context from each modality and rewrites them together.
- Core assumption: The process of extracting keyframes and generating their textual descriptions sufficiently captures the semantic content of the visual slides.
- Evidence anchors:
  - [abstract] “...integrate the contexts selected from both modalities...”
  - [Page 8] The `CombineMM` strategy achieves the highest NLI scores (e.g., 41.09 on DL-Intro).
  - [corpus] Related work on Textbook Question Answering (Beyond Retrieval) confirms the value of multimodal context ranking for educational tasks.
- Break condition: If the visual content is sparse, the keyframe extraction algorithm fails, or the slide descriptions are inaccurate, the benefits of multimodal integration may be lost.

## Foundational Learning

- **Concept: Natural Language Inference (NLI)**
  - Why needed here: The paper introduces NLI score as a more reliable evaluation metric than traditional measures like BLEU/ROUGE, as it captures semantic entailment between the generated and reference questions.
  - Quick check question: What specific relationship does an NLI model evaluate between a candidate question and a reference question?

- **Concept: Chain-of-Thought (CoT) Prompting**
  - Why needed here: This is the core technique for zero-shot context selection, allowing an LLM to perform a reasoning process to identify relevant text segments.
  - Quick check question: How does the defined reasoning process for CoT in this paper differ from a simple direct instruction for extraction?

- **Concept: Multimodal Context Alignment**
  - Why needed here: The task is timestamp-aware, requiring the system to link spoken words with the visual information (slides) displayed at that moment.
  - Quick check question: The paper links transcript segments and keyframes based on what shared information?

## Architecture Onboarding

- **Component map:** Video input -> ASR transcription and keyframe detection -> COSER Framework (Context Selection via CoT prompting -> Context Rewriting -> Question Generation) -> MCQ output

- **Critical path:** The pipeline depends on the quality of the input data processing (ASR, keyframe detection) and the effectiveness of the **Context Selection** step. If selection fails to retrieve relevant information, rewriting and generation cannot recover the missing content.

- **Design tradeoffs:**
  - **Zero-shot vs. Fine-tuned Selection:** The paper chooses zero-shot LLM-based selection to avoid the cost of annotated training data, trading off potential accuracy gains from a supervised model.
  - **Rule-based vs. Dynamic Selection:** Simple fixed-length windows are computationally cheaper but less effective than dynamic LLM-based selection.
  - **Evaluation Metric:** Relying on NLI score prioritizes semantic equivalence, which may not fully capture pedagogical value (e.g., difficulty, cognitive level).

- **Failure signatures:**
  - **Context Drift:** Generated questions include irrelevant concepts from broad context windows.
  - **Answer Mismatch:** Questions are relevant to the topic but cannot be answered using the provided target answer span.
  - **Visual Hallucination:** Keyframe descriptions introduce incorrect information, leading to flawed questions.

- **First 3 experiments:**
  1. **Baseline Comparison:** Run the Question Generator with three context settings: (a) entire transcript, (b) fixed-size window around timestamp, and (c) random segments. Compare NLI and RQUGE scores to establish the impact of context quality.
  2. **Ablation on Selection:** Implement both “Direct” and “Chain-of-Thought” selection prompts. Compare the relevance and conciseness of the extracted contexts (manually or via automated metrics) before feeding them to the generator.
  3. **Rewriting Impact:** Run the full COSER pipeline, but skip the rewriting step. Compare the NLI and RQUGE scores of the final generated questions to those from the full pipeline to quantify the value of context reformulation.

## Open Questions the Paper Calls Out
None

## Limitations
- Context selection quality depends critically on the base LLM's ability to understand educational domain content without fine-tuning
- The rewriting step's accuracy is not independently verified, risking factual errors when reformulating technical content
- The dataset size (186 questions across three courses) limits generalizability

## Confidence

- **High confidence**: The empirical finding that context selection improves question quality over naive approaches (all context or random segments). This is directly supported by NLI score improvements across multiple experimental conditions.
- **Medium confidence**: The superiority of Chain-of-Thought prompting over direct extraction. While results show improvement, the mechanism relies on implicit reasoning capabilities of the base LLM that may vary across implementations.
- **Medium confidence**: The benefit of context rewriting. The improvement in RQUGE scores supports this claim, but the lack of independent verification of factual accuracy in rewritten contexts reduces confidence.

## Next Checks

1. **Human Expert Evaluation**: Conduct a blind review where educational experts rate question quality, relevance, and answerability for questions generated with and without the COSER pipeline. This would validate whether NLI score improvements translate to actual pedagogical value.

2. **Cross-Domain Robustness Test**: Apply the COSER framework to lecture videos from different STEM disciplines (e.g., chemistry, physics) and compare context selection accuracy and question quality to establish whether the zero-shot approach generalizes beyond the tested computer science courses.

3. **Error Analysis of Rewriting Step**: Manually examine a sample of rewritten contexts to identify instances where the LLM introduced inaccuracies, omitted critical technical terms, or altered the meaning of the original content. Quantify the frequency and severity of such errors to assess the reliability of the rewriting component.