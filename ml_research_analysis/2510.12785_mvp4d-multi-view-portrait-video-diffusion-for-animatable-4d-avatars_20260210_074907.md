---
ver: rpa2
title: 'MVP4D: Multi-View Portrait Video Diffusion for Animatable 4D Avatars'
arxiv_id: '2510.12785'
source_url: https://arxiv.org/abs/2510.12785
tags:
- video
- multi-view
- reference
- image
- frames
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MVP4D introduces a morphable multi-view video diffusion model (MMVDM)
  that generates 360-degree animatable portrait videos from a single reference image.
  The model synthesizes up to 392 frames across multiple viewpoints simultaneously
  using a state-of-the-art video diffusion transformer.
---

# MVP4D: Multi-View Portrait Video Diffusion for Animatable 4D Avatars

## Quick Facts
- arXiv ID: 2510.12785
- Source URL: https://arxiv.org/abs/2510.12785
- Authors: Felix Taubner; Ruihang Zhang; Mathieu Tuli; Sherwin Bahmani; David B. Lindell
- Reference count: 40
- Primary result: Achieves 23.39 PSNR on Nersemble self-reenactment, outperforming CAP4D-MMDM (22.17) and Human4DiT (21.47)

## Executive Summary
MVP4D introduces a morphable multi-view video diffusion model (MMVDM) that generates 360-degree animatable portrait videos from a single reference image. The model synthesizes up to 392 frames across multiple viewpoints simultaneously using a state-of-the-art video diffusion transformer. To overcome data limitations, a multi-modal training curriculum combines monocular, multi-view image, and multi-view video datasets. MVP4D significantly improves temporal and 3D consistency compared to previous methods, achieving higher PSNR (23.39 vs 22.17), lower LPIPS (0.294 vs 0.280), and better JOD (5.88 vs 5.30) on Nersemble self-reenactment. The generated videos can be distilled into real-time 4D avatars with superior realism and expression fidelity.

## Method Summary
MVP4D builds on CogVideoX-2B by modifying its transformer to jointly generate multi-view videos. The model uses FLAME-based 3DMM conditioning (pose, expression, view rays, masks, sub-frame motion) and introduces a multi-modal training curriculum combining dynamic monocular video, static multi-view images, and limited multi-view video data. A novel multi-view classifier-free guidance prevents token confusion across views. The system generates videos in four progressive modes (key views → fill views → extend frames → extend all) and reconstructs 4D avatars using FLAME-attached 3D Gaussian splatting with U-Net predicted frame-dependent deformations.

## Key Results
- PSNR: 23.39 (MVP4D) vs 22.17 (CAP4D-MMDM) vs 21.47 (Human4DiT) on Nersemble self-reenactment
- LPIPS: 0.294 (MVP4D) vs 0.280 (CAP4D-MMDM) vs 0.321 (Human4DiT)
- JOD: 5.88 (MVP4D) vs 5.30 (CAP4D-MMDM) vs 5.10 (Human4DiT)
- 4D avatar rendering: 60 FPS at 512×512 resolution

## Why This Works (Mechanism)

### Mechanism 1
Jointly generating multi-view videos in a single diffusion sampling run improves temporal and 3D consistency over sequential multi-view generation. The diffusion transformer attends simultaneously across all tokens spanning spatial, temporal, and viewpoint dimensions, allowing cross-view and cross-frame information sharing during denoising. This reduces the flickering artifacts seen in prior image-diffusion-based approaches that generate views independently.

### Mechanism 2
A multi-modal training curriculum combining dynamic monocular video with static multi-view images enables learning of both temporal coherence and view consistency without requiring large-scale multi-view video datasets. The model learns temporal dynamics from VFHQ (monocular), multi-view geometry from Ava-256/RenderMe-360 (static 360° images), and alignment between these modalities from limited dynamic multi-view data (Nersemble, RenderMe-360 video sequences).

### Mechanism 3
Per-view unconditional predictions in classifier-free guidance (multi-view CFG) prevents token confusion across views, improving generation quality. Standard CFG produces identical unconditional latents for all views (same positional encodings, no view-specific info), causing the transformer to confuse cross-view token relationships. Running independent unconditional forward passes per view provides distinct guidance signals.

## Foundational Learning

- **Video Diffusion Transformers (e.g., CogVideoX)**: Why needed: MVP4D builds directly on CogVideoX-2B, inheriting its spatio-temporal autoencoder and transformer architecture. Quick check: How many latent frames does a 49-frame video produce after CogVideoX encoding?

- **3D Morphable Models (FLAME)**: Why needed: The entire control signal pipeline (pose, expression, deformation maps) is derived from FLAME parameters. The 4D reconstruction attaches Gaussians to FLAME mesh triangles. Quick check: What is the difference between FLAME pose parameters and expression parameters, and which controls blendshapes?

- **3D Gaussian Splatting with Mesh Embedding**: Why needed: The final 4D avatar uses Gaussians attached to FLAME triangles, with per-frame UV-space deformations predicted by a U-Net. Understanding rasterization, blending, and deformation is required for the reconstruction stage. Quick check: Why attach Gaussians to mesh triangles rather than optimizing them freely in 3D space?

## Architecture Onboarding

- **Component map**: Reference image → 3DMM estimation (FlowFace) → FLAME params → rasterized maps (pose, expression, view rays, masks, sub-frame motion) → conditioning maps → concatenation with noisy latents → diffusion transformer (30-layer, joint attention across V×F×H×W tokens) → multi-view video output → 3DGS fitting (FLAME mesh + attached Gaussians + U-Net deformations + velocity regularization) → real-time 4D avatar

- **Critical path**: Reference image → 3DMM estimation (FlowFace) → driving sequence → 3DMM animation params → conditioning maps → concatenation with noisy latents → diffusion sampling (100 steps key views, 50 steps others) → ~6.5-10.5 hours → 3DGS fitting (1.5 hours) → real-time rendering

- **Design tradeoffs**: High spatiotemporal compression (16× spatial, ~4× temporal) enables long sequences but loses high-frequency temporal details. Joint generation vs. memory: V=8 views × F=49 frames requires 35GB VRAM; iterative generation extends this but introduces quality drift. Multi-view CFG quality vs. compute: Per-view unconditional passes increase inference cost.

- **Failure signatures**: Flickering/lack of 3D consistency indicates attention not effectively sharing cross-view info or CFG misconfigured. Temporal drift over long sequences indicates iterated generation quality degradation. Artifacts at extreme viewpoints indicate training data has limited 360° multi-view video coverage.

- **First 3 experiments**: 1) Reproduce ablation on CFG modes (Table 7): Train or load checkpoint, sample with no-CFG, conventional-CFG, and multi-view-CFG. Compare JOD and visual quality on 2-3 Nersemble sequences. 2) Validate training curriculum benefit: Compare Stage 2 only vs. full curriculum (Table S13 settings A vs. C) at reduced resolution (192×192) for ~1k iterations. 3) Test generalization beyond training views: Generate with V=8, 12, 16 views and measure 3D consistency (RE@LG metric) to characterize extrapolation limits.

## Open Questions the Paper Calls Out

### Open Question 1
Can the spatiotemporal auto-encoder be improved to prevent the loss of fine-grained temporal details, such as rapid mouth movements or appearing/disappearing teeth? The authors state, "due to the auto-encoder's high spatiotemporal compression, our method struggles to capture fine-grained temporal details, such as rapidly appearing and disappearing teeth."

### Open Question 2
Can the iterative generation strategy for long video sequences be modified to prevent the accumulation of errors and quality degradation over time? The paper notes that "when generating long sequences using the first-frame conditioning strategy (generation mode 3), quality degrades with each iteration, limiting the practical sequence length."

### Open Question 3
Can the model be extended to disentangle lighting and reflectance properties to support extreme lighting conditions and relighting? The authors list this as a limitation and future direction: "The method cannot accurately model extreme lighting conditions... integrating physically based lighting and reflectance models could enable more accurate control over avatar illumination."

### Open Question 4
Can the multi-hour inference pipeline be distilled into a feed-forward model to enable real-time 4D avatar generation? The authors suggest that "building on recent methods that directly generate 3D representations in a feed-forward manner [e.g., Bolt3D] may enable more efficient 4D avatar generation."

## Limitations

- Extreme viewpoints: The model struggles with extreme viewpoints (e.g., top/bottom views) where training data coverage is limited
- Fine-grained temporal details: High spatiotemporal compression of the auto-encoder causes loss of rapid motion details like teeth appearing/disappearing
- Long sequence quality: Iterative generation for long videos causes quality degradation over time

## Confidence

- **High confidence**: Empirical improvements in PSNR, LPIPS, and JOD over CAP4D and Human4DiT are robust and directly measured
- **Medium confidence**: The mechanism explaining why multi-view CFG improves quality is logically sound and supported by ablation (Table 7), but the exact cause of token confusion in conventional CFG is inferred rather than directly measured
- **Low confidence**: The claim that MVP4D generalizes to 360° views is overstated; the training data has limited multi-view video coverage at extreme angles, and quality degrades outside the learned view range

## Next Checks

1. **Validate CFG mechanism in isolation**: Generate multi-view outputs with and without multi-view CFG on 2-3 sequences. Measure JOD and visually inspect for cross-view token confusion (e.g., mismatched facial features across views) to confirm the stated mechanism.

2. **Test curriculum component contributions**: Train two ablations—one with only Stage 2 (multi-view + video) and one with full curriculum—on a small subset (e.g., 1k iterations at 192×192). Compare PSNR/LPIPS/JOD to isolate the benefit of progressive resolution/view expansion.

3. **Characterize view extrapolation limits**: Generate with V=8, 12, 16 views and measure 3D consistency (RE@LG) and visual quality. Identify the maximum V before quality collapse to quantify the model's true 360° generalization capability.