---
ver: rpa2
title: Convergence for Discrete Parameter Update Schemes
arxiv_id: '2512.04051'
source_url: https://arxiv.org/abs/2512.04051
tags:
- update
- discrete
- convergence
- training
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a convergence theory for discrete parameter
  update schemes in optimization. The authors propose a framework where updates are
  discrete by design rather than obtained by discretizing continuous updates.
---

# Convergence for Discrete Parameter Update Schemes

## Quick Facts
- arXiv ID: 2512.04051
- Source URL: https://arxiv.org/abs/2512.04051
- Reference count: 40
- Discrete update schemes can converge under Lipschitz-smooth objectives without quantization of continuous values

## Executive Summary
This paper introduces a framework for discrete parameter updates in optimization, where updates are inherently discrete rather than obtained by quantizing continuous updates. The authors establish convergence guarantees for a general class of discrete schemes under assumptions including Lipschitz-continuous gradients and bounds on update moments. They introduce a specific Zero-Inflated Multinomial (ZIM) update that samples from a zero-inflated multinomial distribution proportional to the gradient, proving it satisfies their convergence conditions with a tighter bound (proportional to √(dL)) compared to prior discrete methods.

## Method Summary
The ZIM update maps gradients directly to integer-valued updates without quantization by sampling from a zero-inflated multinomial distribution. Given gradient ∇F(w_k), it computes probability vector q = (|∇F| + c) / (‖∇F‖₁ + cd) and samples x ~ ZIMultinomial(n, r, q), then applies update ḡ = x ⊙ sign(∇F). The framework requires Lipschitz-continuous gradients and specific moment bounds on the discrete updates. Empirical evaluation on MNIST uses 10 epochs with 10 runs averaged, showing convergence with a 0.5-1% accuracy penalty compared to standard SGD.

## Key Results
- ZIM update satisfies convergence conditions with μ = n√(dL + cd), yielding error floor LM/μ
- Convergence bound scales as √(dL) compared to dL for prior discrete methods like BOLD
- Empirical validation on MNIST shows convergence but with 0.5-1% accuracy penalty vs SGD
- Theoretical framework establishes general conditions for discrete update schemes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Discrete-by-design updates can converge under Lipschitz-smooth objectives without requiring quantization of continuous values.
- Mechanism: The update function ḡ maps directly from R^d to Z^d (integer-valued outputs), eliminating the quantization step entirely. Convergence follows from bounding expected descent using Lipschitz continuity.
- Core assumption: Gradient function ∇F is Lipschitz continuous with constant L (Assumption 2).
- Evidence anchors:
  - [abstract] "We introduce an alternative approach where the update rule itself is discrete, avoiding the quantisation of continuous updates by design."
  - [section] Definition 1: w_{k+1} ← w_k − ḡ(w_k, ξ_k) where ḡ: R^d → S^d ⊆ Z^d
  - [corpus] Limited direct corpus support; related work (BOLD) addresses boolean architectures but with architecture-specific assumptions.
- Break condition: Objective lacks Lipschitz-continuous gradients; unbounded gradient norms violate moment assumptions.

### Mechanism 2
- Claim: Convergence is guaranteed when the discrete update satisfies specific moment bounds linking expected update direction to gradient norm.
- Mechanism: Two conditions must hold: (1) Expected update has positive alignment with gradient: ∇F(w_k)^T E[ḡ] ≥ α_k μ‖∇F(w_k)‖²₂; (2) Second moment bounded: E[‖ḡ‖²₂] ≤ α_k M + α²_k M_G ‖∇F(w_k)‖²₂. These enable telescoping-sum proof yielding error floor LM/μ.
- Core assumption: Assumption 3(b) — existence of scalars μ > 0, M ≥ 0, M_G > 0 satisfying moment bounds.
- Evidence anchors:
  - [section] Proposition 4: E[(1/K)Σ‖∇F(w_k)‖²₂] → LM/μ as K→∞
  - [section] Proof uses Proposition 9, which combines Assumption 3 bounds with Lipschitz descent inequality
  - [corpus] Assumption: Related work on quantized training (Li & De Sa 2019) shows similar error floors from discretization noise.
- Break condition: Update variance too high relative to gradient magnitude; μ becomes negative or approaches zero.

### Mechanism 3
- Claim: The ZIM update achieves a convergence bound proportional to √(dL), tighter than prior discrete methods with dL scaling.
- Mechanism: Zero-inflated multinomial samples integer counts with probability proportional to |∇F| + c (Laplace smoothing). The zero-inflation rate r acts as learning rate. Cauchy-Schwarz plus Lipschitz bound gives ‖∇F‖₁ ≤ √(d)·L, yielding μ = n√(dL + cd).
- Core assumption: Lipschitz bound on gradient enables norm inequality; smoothing constant c ensures valid probabilities at zero gradient.
- Evidence anchors:
  - [section] Definition 6: q = |∇F(w_k)| + c / (‖∇F(w_k)‖₁ + cd)
  - [section] Proposition 12: μ = n√(dL + cd) derived from Lipschitz + Cauchy-Schwarz
  - [corpus] No direct corpus comparison; paper cites BOLD bound dL as baseline.
- Break condition: Lipschitz constant L grows faster than dimension d; smoothing c insufficient to prevent division issues.

## Foundational Learning

- Concept: **Lipschitz continuity**
  - Why needed here: Core assumption for convergence proof; bounds how fast gradients can change, enabling descent guarantees.
  - Quick check question: Can you state the inequality ‖∇F(w) − ∇F(w̄)‖₂ ≤ L‖w − w̄‖₂ and explain why it implies the descent bound in Equation 2.1?

- Concept: **Stochastic gradient descent convergence theory**
  - Why needed here: Proof structure follows Bottou et al. (2018) framework; requires understanding expected descent, telescoping sums, and error floors.
  - Quick check question: Why does the error floor LM/μ not depend on learning rate α, and what does this imply about discrete vs. continuous optimization?

- Concept: **Multinomial distribution moments**
  - Why needed here: ZIM update requires computing E[x] and E[‖x‖²₂] for zero-inflated multinomial to verify moment bounds.
  - Quick check question: Given E[x_i] = nrq_i for standard multinomial, how does zero-inflation (adding p₀ = 1−r) change the first moment?

## Architecture Onboarding

- Component map:
  - Gradient computer -> Probability normalizer -> ZIMultinomial sampler -> Sign-applier -> Parameter updater

- Critical path:
  1. Compute gradient ∇F(w_k) — requires floating point (or gradient quantization, not addressed here)
  2. Normalize to probabilities q — requires ‖·‖₁ computation
  3. Sample from ZIMultinomial — discrete sampling operation
  4. Apply sign and subtract — purely integer operations

- Design tradeoffs:
  - **Accuracy vs. efficiency**: Paper reports 0.5–1% accuracy penalty on MNIST vs. SGD (Figure 1)
  - **Bound tightness vs. simplicity**: ZIM bound √(dL) improves on BOLD's dL but requires gradient magnitude normalization
  - **Learning rate encoding**: r serves dual role as zero-inflation probability and effective learning rate; constrains r ∈ [0,1]

- Failure signatures:
  - Convergence stalls at error floor LM/μ if μ too small (high dimension or large L)
  - Division instability if ‖∇F‖₁ → 0 without smoothing constant c
  - Assumption: High variance in sparse gradients may violate moment bounds

- First 3 experiments:
  1. **Sanity check**: Implement ZIM update on small MNIST MLP; verify convergence curve matches Figure 1 shape and confirm ~0.5–1% accuracy gap vs. SGD baseline.
  2. **Hyperparameter sweep**: Vary n (trial count), r (learning rate/zero-inflation), c (smoothing); plot convergence rate and final accuracy to validate bounds scale as predicted.
  3. **Ablation on dimension**: Test on models with varying parameter counts d; verify error floor scales approximately with √(d) rather than d by comparing convergence residuals.

## Open Questions the Paper Calls Out

- Can inherently discrete update schemes achieve computational or memory efficiency gains over quantisation approaches for non-real-valued systems such as Boolean circuits and binarised neural networks?
  - Basis in paper: [explicit] The authors state "A more systematic exploration of such schemes is left for future work; in this paper, we focus on laying the mathematical foundations" and mention their motivation is "to identify models where directly discrete updates offer greater efficiency than quantisation."
  - Why unresolved: The paper focuses on theoretical foundations and a proof-of-concept empirical evaluation; no experiments on inherently discrete architectures were conducted.
  - What evidence would resolve it: Empirical comparisons on Boolean circuit training or binarised neural networks showing wall-clock time, memory footprint, or energy consumption improvements.

- Can the 0.5–1% accuracy gap between ZIM and standard SGD be reduced while maintaining the theoretical guarantees?
  - Basis in paper: [explicit] The authors note "our method pays a 0.5%−1% accuracy penalty (reflecting the 'noise floor' discussed in Section 2)" and state "significant improvements be made by modifying the architectures and update schemes used."
  - Why unresolved: The error floor term LM/µ is fundamental to the current analysis; whether different discrete schemes or architectural co-design can reduce this remains unexplored.
  - What evidence would resolve it: A modified discrete update rule or architecture achieving accuracy within 0.1% of SGD on MNIST with convergence proof.

- How does the ZIM update scale to larger datasets and deeper architectures beyond MNIST with simple convolutional and ResNet models?
  - Basis in paper: [inferred] Empirical evaluation is limited to MNIST; no experiments on ImageNet, CIFAR-10, or modern architectures like Transformers are presented despite the motivation being "models with hundreds of billions of parameters."
  - Why unresolved: The practical viability for large-scale training remains untested.
  - What evidence would resolve it: Convergence and accuracy results on ImageNet with ResNet-50 or larger models, with comparison to quantised training baselines.

- Can alternative discrete update schemes be constructed that improve upon the √(dL) convergence bound dependency?
  - Basis in paper: [inferred] The paper compares ZIM's √(dL) bound favorably to BOLD's dL bound, but the bound is "tighter when the Lipschitz constant L grows slowly relative to model dimension d"—leaving open whether better dependencies exist.
  - Why unresolved: Only the ZIM scheme is analyzed; the general framework permits other discrete updates.
  - What evidence would resolve it: A discrete update scheme with a proven bound of o(√(dL)) or with better empirical convergence rates.

## Limitations

- Theoretical framework relies on strong assumptions about Lipschitz continuity and moment bounds that may not hold for all deep learning objectives
- Empirical validation limited to single dataset (MNIST) and simple architectures, making generalization to modern large-scale models uncertain
- Reported 0.5-1% accuracy penalty, while modest, represents nontrivial performance cost that may compound in more challenging tasks

## Confidence

- **High confidence**: Lipschitz continuity assumption and its role in convergence proofs (well-established mathematical framework)
- **Medium confidence**: The ZIM update satisfies the moment bounds and achieves the claimed √(dL) convergence rate (theoretical derivation appears sound but empirical validation is limited)
- **Low confidence**: The 0.5-1% accuracy penalty generalizes to other datasets and architectures (based on single experiment with MNIST)

## Next Checks

1. Test ZIM updates on CIFAR-10 with modern architectures (ResNet-18/34) to verify accuracy penalty scales as expected and doesn't increase substantially
2. Implement the convergence theory on synthetic objectives with known Lipschitz constants to empirically verify the error floor LM/μ appears as predicted
3. Compare ZIM against other discrete training methods (BOLD, rounding-based quantization) on identical tasks to isolate the contribution of discrete-by-design updates