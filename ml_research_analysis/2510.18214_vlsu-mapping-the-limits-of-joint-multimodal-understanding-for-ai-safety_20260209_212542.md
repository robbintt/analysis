---
ver: rpa2
title: 'VLSU: Mapping the Limits of Joint Multimodal Understanding for AI Safety'
arxiv_id: '2510.18214'
source_url: https://arxiv.org/abs/2510.18214
tags:
- safety
- image
- unsafe
- dataset
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a comprehensive framework for evaluating
  multimodal safety in vision-language models (VLMs), addressing the critical gap
  in assessing risks that emerge from the joint interpretation of images and text.
  The authors construct a large-scale benchmark, VLSU, comprising 8,187 human-annotated
  samples across 15 harm categories and 17 safety patterns, including a novel "borderline"
  severity class.
---

# VLSU: Mapping the Limits of Joint Multimodal Understanding for AI Safety

## Quick Facts
- **arXiv ID:** 2510.18214
- **Source URL:** https://arxiv.org/abs/2510.18214
- **Reference count:** 40
- **Primary result:** VLMs achieve >90% accuracy on unimodal safety but drop to 20-55% on joint reasoning tasks, with 34% errors occurring despite correct individual modality classification.

## Executive Summary
This paper introduces a comprehensive framework for evaluating multimodal safety in vision-language models, addressing the critical gap in assessing risks that emerge from the joint interpretation of images and text. The authors construct a large-scale benchmark, VLSU, comprising 8,187 human-annotated samples across 15 harm categories and 17 safety patterns, including a novel "borderline" severity class. Their systematic approach reveals that while models achieve over 90% accuracy on clear unimodal safety signals, performance drops substantially to 20-55% when joint image-text reasoning is required. Most concerning, 34% of errors occur despite correct individual modality classification, demonstrating absent compositional reasoning capabilities. The study also highlights models' struggle to balance refusing unsafe content while responding to borderline cases, with instruction framing reducing over-blocking from 62.4% to 10.4% in Gemini-1.5 but at the cost of under-refusal on unsafe content.

## Method Summary
The VLSU benchmark consists of 8,187 human-annotated image-text pairs across 15 harm categories and 17 safety patterns. The framework defines joint safety through severity tuples (S-safe, B-borderline, U-unsafe) for image, text, and combination. Evaluation uses zero-shot classification with structured chain-of-thought prompting, measuring three-class accuracy and F1 scores. Eleven VLMs were tested including GPT-4o, Gemini variants, and various open-source models. The alignment task measures refusal rates and helpfulness scores under different instruction framings, with GPT-4o serving as judge.

## Key Results
- Models achieve >90% accuracy on unimodal safety signals but drop to 20-55% on joint reasoning-requiring patterns
- 34% of joint safety errors occur despite correct individual modality classification
- Instruction framing reduces over-blocking on borderline content from 62.4% to 10.4% in Gemini-1.5
- Gemini-1.5 achieves 0.715 F1 on the safety understanding task, close to the human oracle at 0.910 F1

## Why This Works (Mechanism)

### Mechanism 1: Combinatorial Safety Pattern Mapping
The framework isolates failure modes by comparing performance across unimodal-dominated versus joint-reasoning-required patterns. A safety tuple formalization defines 17 viable patterns out of 27 theoretical combinations. The 34% error rate where both modalities are correctly classified but the joint label is wrong suggests models lack dedicated cross-modal integration mechanisms, defaulting to unimodal heuristics.

### Mechanism 2: Severity Spectrum Calibration via Borderline Class
The three-class setup (Safe, Borderline, Unsafe) creates a "gray zone" where model policy adherence can be precisely tested. High refusal rates on borderline content indicate over-sensitivity, while low refusal rates on unsafe content indicate failure to detect harm. This allows quantification of model calibration and shows how instruction framing shifts operating points along this spectrum.

### Mechanism 3: Structured Chain-of-Thought as Latent Capability Probe
Structured CoT forces models through explicit steps (image analysis, text analysis, combined assessment). The observed performance stratification—weaker models improve while stronger models plateau—suggests stronger models' performance is bounded by fundamental architecture/alignment limitations rather than prompt engineering.

## Foundational Learning

**Concept: Multimodal Safety Alignment Gap**
- **Why needed here:** The core finding is that models can correctly identify safe text and safe images but fail to recognize unsafe combinations, requiring understanding of cross-modal intent.
- **Quick check question:** If a model correctly labels an image of a bridge as "safe" and the text "I wonder what jumping off feels like" as "safe," what is the fundamental failure if it labels the combination as "safe"?

**Concept: Over-Refusal vs. Under-Refusal Trade-off**
- **Why needed here:** The paper highlights how models struggle with borderline content, often over-refusing safe-ish queries or under-refusing unsafe ones based on instruction framing.
- **Quick check question:** A model refuses 99% of all queries that mention "explosives," including historical questions. Does it have a high false positive rate? What does this say about its calibration?

**Concept: Compositional vs. Emergent Reasoning**
- **Why needed here:** The 34% joint error rate hinges on the model lacking a mechanism for "emergent" meaning—where the whole is more harmful than the sum of its parts.
- **Quick check question:** A safe image (S) + safe text (S) = unsafe joint meaning (U). Does the model's failure here imply it has a bad image encoder, a bad text encoder, or a flawed cross-modal reasoning component?

## Architecture Onboarding

**Component map:** Severity Tuple Space (17 viable patterns) -> Dataset Pipeline (image-concept generation, real image retrieval, query synthesis, human annotation) -> Evaluation Protocol (safety understanding task, safety alignment task)

**Critical path:** The Error Analysis is most critical. Run a target model on the safety understanding task. Classify every error into one of four buckets: (Image-only wrong), (Text-only wrong), (Both wrong), or (Both correct but joint wrong). The "Both correct but joint wrong" bucket is the key signal of the architectural gap.

**Design tradeoffs:**
- **Real Images vs. Synthetic:** Increases realism but limits control over specific visual features
- **Three-Class vs. Binary:** The borderline class adds calibration fidelity but introduces higher annotation ambiguity
- **Structured CoT Evaluation:** Useful for probing latent capabilities but adds inference cost

**Failure signatures:**
- **Text-Dominance Bias:** Joint predictions correlate highly with text-only predictions but weakly with image-only
- **Unimodal Saturation, Joint Failure:** >90% accuracy on U-U-U but 20-55% on S-S-U patterns
- **Instruction Brittleness:** A large swing in refusal rates (>50%) on the same content with different system prompts

**First 3 experiments:**
1. **Baseline Evaluation & Error Decomposition:** Run model on VLSU. Decompose errors into four categories. Quantify the "both-correct" percentage.
2. **Ablation on Combinatorial Patterns:** Stratify test set by the 17 severity tuples. Plot performance from left (unimodal-dominated) to right (joint-reasoning-required) to visualize the degradation curve.
3. **Instruction Framing Sensitivity Test:** Evaluate refusal rates on the borderline subset under two contrasting instruction framings. Calculate the delta to measure susceptibility to over-refusal.

## Open Questions the Paper Calls Out

**Open Question 1:** How can VLM architectures or training objectives be modified to mitigate text-modality dominance and ensure visual signals are weighted equitably during joint safety reasoning? This remains unresolved because current models exhibit a strong text bias, relying on linguistic signals even when visual context alters the safety implication, leading to systematic misclassification.

**Open Question 2:** What training interventions are required to resolve the compositional reasoning failure where models correctly identify unimodal safety but fail to assess joint safety? This specific error mode demonstrates that simply improving unimodal encoders is insufficient; the model lacks the capability to fuse information for emergent safety signals.

**Open Question 3:** How does the definition and model comprehension of "borderline" safety severity generalize to non-English languages and diverse cultural contexts? The text is "English-only" and acknowledges that "other languages might have additional safety considerations... which is an exciting direction for future work."

## Limitations
- **Annotation Reliability:** The borderline class shows significantly lower human agreement (80.6%) compared to safe/unsafe classes (93.5%/95.1%), suggesting higher inter-annotator variability.
- **Pattern Generalization:** Dataset construction using Wikipedia-derived image concepts may create artifacts that don't generalize to real-world safety failures in deployed VLMs.
- **Instruction Sensitivity Generalizability:** The 62.4% → 10.4% over-blocking reduction observed in Gemini-1.5 may not transfer across model architectures or safety training paradigms.

## Confidence

**High Confidence (Strong evidence):**
- Models correctly classify individual modality safety signals >90% of the time
- Performance drops to 20-55% on joint reasoning-requiring patterns
- 34% of errors occur despite correct unimodal classification
- Models struggle to balance over-refusal and under-refusal on borderline content

**Medium Confidence (Mixed evidence):**
- The 17-pattern decomposition captures all meaningful safety combinations
- Structured CoT elicits limited gains in stronger models due to capacity ceilings
- Instruction framing shifts over-blocking rates by >50 percentage points

**Low Confidence (Weak evidence):**
- Borderline class definition is semantically meaningful and consistently applied
- Current VLMs fundamentally lack cross-modal reasoning mechanisms

## Next Checks
1. **Inter-annotator reliability analysis:** Compute Cohen's kappa specifically for borderline samples and compare to safe/unsafe classes to quantify annotation uncertainty.

2. **Pattern-specific ablation study:** Remove all samples from patterns requiring joint reasoning (S-S-U, B-B-B, etc.) and measure performance impact to isolate the compositional reasoning deficit.

3. **Cross-model instruction framing robustness:** Test instruction sensitivity across 5+ diverse VLM architectures (not just Gemini-1.5) to determine if this brittleness is architecture-specific or universal.