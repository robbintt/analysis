---
ver: rpa2
title: Are manual annotations necessary for statutory interpretations retrieval?
arxiv_id: '2506.13965'
source_url: https://arxiv.org/abs/2506.13965
tags:
- sentences
- legal
- ndcg
- annotation
- concept
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether manual annotations are necessary
  for statutory interpretations retrieval. The authors conduct three experiments:
  (1) determining the optimal number of manual annotations per legal concept, (2)
  assessing if pre-sorting sentences before annotation improves results, and (3) evaluating
  if LLM-based automatic annotation can replace manual annotation.'
---

# Are manual annotations necessary for statutory interpretations retrieval?

## Quick Facts
- arXiv ID: 2506.13965
- Source URL: https://arxiv.org/abs/2506.13965
- Reference count: 29
- Manual annotation may not be necessary for statutory interpretations retrieval, as LLM-based automatic annotation achieves comparable or better results than manual annotation for NDCG@10 and NDCG@100 metrics.

## Executive Summary
This paper investigates whether manual annotations are necessary for statutory interpretations retrieval, where the task is to retrieve sentences from court rulings that provide interpretations of legal concepts, ranked by explanatory value. The authors conduct three experiments: (1) determining the optimal number of manual annotations per legal concept, (2) assessing if pre-sorting sentences before annotation improves results, and (3) evaluating if LLM-based automatic annotation can replace manual annotation. Using a dataset of 42 legal concepts with over 27,000 sentences, they fine-tune DeBERTa models and test Qwen-72B-Instruct LLM for automatic annotation. Results show that annotating up to 1,000 examples per concept yields near-optimal performance, pre-sorting provides minimal benefit, and LLM-based annotation achieves comparable or better results than manual annotation for NDCG@10 and NDCG@100 metrics, suggesting that manual annotation may not be necessary for this task.

## Method Summary
The study uses a dataset of 42 legal concepts with 27,000+ sentences from the Caselaw Access Project. The dataset includes "text", "concept", and after annotation "value" fields with 4 categories (high/certain/potential/no value → 3/2/1/0). A 6-fold split is used: 4 for cross-validation, 2 for final testing, with each sentence annotated by 2 law students. DeBERTa v.3 models (base: 184M, large: 435M params) are fine-tuned with specific hyperparameters (LR=2e-05, epochs=5, batch=8, warmup=0, max_length=768, FP16=true, 4 labels). Input format concatenates concept, sentence, and provision. NDCG@10 and NDCG@100 are primary metrics, with secondary accuracy and weighted F1 for classification. A voting scheme averages predictions across 4 CV models.

## Key Results
- Annotating up to 1,000 examples per concept yields near-optimal performance
- Pre-sorting sentences before annotation provides minimal benefit
- LLM-based automatic annotation achieves comparable or better results than manual annotation for NDCG@10 and NDCG@100 metrics

## Why This Works (Mechanism)
The study demonstrates that LLM-based annotation can effectively replace manual annotation for statutory interpretations retrieval by leveraging the model's ability to understand context and assign appropriate relevance scores to legal sentences. The DeBERTa models are fine-tuned on progressively larger subsets of annotated data, showing that even with limited annotations (around 1000 per concept), performance approaches optimal levels. The LLM-based annotation directly converts annotation guidelines into prompts, allowing for scalable and consistent labeling without human intervention.

## Foundational Learning
- Legal concept interpretation retrieval: Understanding how to extract and rank sentences that explain legal concepts from court rulings. Why needed: This is the core task being automated. Quick check: Verify dataset contains labeled sentences with legal concepts and relevance scores.
- NDCG@10/100 metrics: Normalized Discounted Cumulative Gain at different cut-off points. Why needed: Primary evaluation metric for ranking quality. Quick check: Ensure implementation matches the formula with rel(s_i) values of 3/2/1/0 and proper normalization.
- Fine-tuning DeBERTa: Process of adapting pre-trained transformer models to specific legal text classification tasks. Why needed: Core modeling approach for comparison. Quick check: Verify training setup with specified hyperparameters and 4-fold cross-validation.
- LLM prompt engineering for annotation: Converting annotation guidelines into effective prompts for automatic labeling. Why needed: Alternative to manual annotation. Quick check: Confirm prompt structure matches annotation guidelines and produces consistent labels.

## Architecture Onboarding

**Component map:** Dataset (42 concepts, 27k sentences) -> Pre-processing (concatenate concept+sentence+provision) -> Model training (DeBERTa v.3 fine-tuning) -> Evaluation (NDCG@10/100, F1) -> LLM annotation (Qwen-72B-Instruct prompting)

**Critical path:** Data preparation → Fine-tuning → Evaluation → LLM annotation comparison

**Design tradeoffs:** Base vs large DeBERTa models (parameter efficiency vs performance), manual vs LLM annotation (cost vs accuracy), 4-fold CV vs simpler validation schemes (robustness vs complexity)

**Failure signatures:** Low NDCG with base model on limited data indicates insufficient training examples; high variance across seeds suggests data split imbalance; LLM annotation producing inconsistent labels indicates prompt quality issues

**First experiments:** 1) Fine-tune base DeBERTa on 500 examples per concept and evaluate NDCG@10; 2) Test LLM annotation on a small subset with different prompt variations; 3) Compare pre-sorting vs random sampling on annotation efficiency

## Open Questions the Paper Calls Out

**Open Question 1:** Can a fine-tuned LLM outperform a prompt-based LLM in statutory interpretation retrieval, and what is the minimum number of annotated examples required to do so? The paper only evaluated fine-tuning smaller DeBERTa models and prompt-based inference on Qwen-72B; it did not experiment with fine-tuning the LLM itself on the dataset.

**Open Question 2:** Can automated prompt optimization frameworks (e.g., DSPy) improve annotation quality beyond the manually designed prompts used in this study? The authors relied on direct conversions of annotation guidelines and "improved" manual definitions for their prompts, without testing automated optimization loops.

**Open Question 3:** How do other open-source LLMs compare to Qwen-72B-Instruct in balancing annotation performance against inference cost? The study restricted its LLM evaluation to a single model architecture (Qwen), leaving the generalizability of the "manual annotation is unnecessary" conclusion unverified across other state-of-the-art models.

## Limitations
- Dataset covers only 42 legal concepts, which may not generalize to full diversity of legal domains
- LLM-based annotation experiments rely on Qwen-72B-Instruct without systematic comparison to other models or smaller parameter sizes
- Assumes availability of provisions alongside concepts and sentences without addressing extraction/validation methods

## Confidence
**High confidence:** Claims about optimal annotation quantity (k=1000 yielding near-optimal results) and pre-sorting ineffectiveness are well-supported by experimental design and statistical analysis.

**Medium confidence:** LLM-based annotation results showing comparable performance to manual annotation are promising but depend heavily on the specific LLM used and quality of conversion from annotation guidelines.

**Medium confidence:** Overall conclusion that manual annotation is unnecessary assumes availability of suitable LLM resources, which may not be accessible to all practitioners.

## Next Checks
1. Verify dataset accessibility by attempting to obtain the dataset from the cited PhD thesis or through author correspondence, ensuring JSON structure matches specifications.
2. Implement and test the full pipeline (fine-tuning DeBERTa v.3 with specified hyperparameters, 4-fold CV, NDCG@10/100 computation) on a subset of the data to confirm reproducibility of reported results.
3. Conduct sensitivity analysis on the voting scheme and provision integration methods to understand their impact on final retrieval performance.