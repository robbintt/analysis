---
ver: rpa2
title: High-Power Training Data Identification with Provable Statistical Guarantees
arxiv_id: '2510.09717'
source_url: https://arxiv.org/abs/2510.09717
tags:
- test
- data
- training
- power
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of identifying training data within
  large-scale models like LLMs and VLMs, which is crucial for copyright litigation,
  privacy auditing, and fair evaluation. The key challenge is controlling the false
  discovery rate (FDR) while maintaining high statistical power.
---

# High-Power Training Data Identification with Provable Statistical Guarantees

## Quick Facts
- arXiv ID: 2510.09717
- Source URL: https://arxiv.org/abs/2510.09717
- Reference count: 40
- Primary result: Achieves strict FDR control while significantly improving power over baseline methods

## Executive Summary
This paper addresses the critical problem of identifying training data within large-scale models like LLMs and VLMs, with applications in copyright litigation, privacy auditing, and fair evaluation. The key challenge is controlling the false discovery rate (FDR) while maintaining high statistical power. The authors propose PTDI, a novel method that achieves strict FDR control by combining conformal p-values with a Benjamini-Hochberg procedure, using a data-dependent p-value scaling technique based on a conservative estimator for data usage proportion. Empirical results across diverse models and datasets demonstrate that PTDI consistently controls the realized FDR below the target level while achieving substantially higher power than competing approaches.

## Method Summary
The authors propose Provable Training Data Identification (PTDI), which combines conformal p-values with the Benjamini-Hochberg procedure to achieve strict FDR control. The key innovation is a data-dependent p-value scaling technique that uses a conservative estimator for the data usage proportion. This scaling significantly improves statistical power without violating FDR guarantees. The method is model-agnostic and works in both white-box and black-box settings, making it broadly applicable across different model architectures and use cases.

## Key Results
- PTDI controls realized FDR below target (4.94% vs 13.11% for competitor on WikiMIA at 5% target FDR)
- Power improvement from 0.44 to 0.75 on WikiMIA with GPT-NeoX-20B at 0.5% FDR target
- Consistent performance across diverse models (LLMs and VLMs) and datasets
- Model-agnostic approach applicable in both white-box and black-box settings

## Why This Works (Mechanism)
PTDI works by combining conformal prediction methods with FDR control procedures. Conformal p-values provide valid statistical inference that is robust to model uncertainty, while the Benjamini-Hochberg procedure ensures strict FDR control. The key innovation is the data-dependent p-value scaling based on a conservative estimator of data usage proportion. This scaling adjusts the p-values to account for the actual distribution of data usage patterns, which improves statistical power without compromising the FDR guarantees. The method leverages the fact that data usage proportions in training are often concentrated in certain regions, allowing for more aggressive identification while maintaining statistical rigor.

## Foundational Learning
- **Conformal prediction**: Provides distribution-free uncertainty quantification for statistical inference; needed because model behavior is complex and non-parametric; quick check: valid p-values under exchangeability
- **Benjamini-Hochberg procedure**: Controls FDR in multiple hypothesis testing; needed because we test many potential training examples simultaneously; quick check: realized FDR ≤ target FDR
- **False Discovery Rate (FDR)**: Measures proportion of false positives among rejected hypotheses; needed as the primary error metric for data identification; quick check: FDR ≤ target level
- **Conservative estimation**: Provides upper bounds on unknown quantities; needed to ensure FDR control while improving power; quick check: estimator upper bounds true value with high probability
- **Data usage proportion**: Fraction of training data actually used by the model; needed to calibrate statistical tests appropriately; quick check: correlation with model performance on identified data

## Architecture Onboarding

**Component Map:**
Data instances -> Conformal p-value generation -> Conservative proportion estimation -> P-value scaling -> Benjamini-Hochberg procedure -> Identified training data

**Critical Path:**
The critical path flows from data instance evaluation through conformal p-value generation, then through the conservative proportion estimation, followed by p-value scaling, and finally the Benjamini-Hochberg procedure to output identified training data. Each component must function correctly for the overall FDR guarantee to hold.

**Design Tradeoffs:**
- Conservative estimation vs power: More conservative estimates provide stronger FDR guarantees but reduce power
- Black-box vs white-box access: Black-box setting is more general but may have lower power than white-box approaches
- Computational cost vs accuracy: More extensive p-value computation improves accuracy but increases runtime
- Target FDR level vs discovery rate: Lower target FDR provides more reliable results but identifies fewer training instances

**Failure Signatures:**
- Inflated realized FDR indicates violation of exchangeability assumptions or incorrect conservative estimation
- Low power suggests overly conservative proportion estimation or inappropriate p-value scaling
- Computational bottlenecks during conformal p-value generation may indicate scalability issues
- Inconsistent results across model types suggest limitations in model-agnostic assumptions

**First 3 Experiments to Run:**
1. Verify FDR control on a synthetic dataset with known ground truth to confirm theoretical guarantees
2. Compare power vs baseline methods across different target FDR levels to establish performance envelope
3. Test black-box performance on a simple model to validate model-agnostic capabilities

## Open Questions the Paper Calls Out
None

## Limitations
- Performance in black-box settings may not generalize to all model architectures or data modalities
- Conservative estimator may limit power gains when true usage proportion differs substantially from estimate
- Computational overhead for very large models (100B+ parameters) remains unclear
- Limited testing on non-text data modalities (images, audio, video)

## Confidence
- **High confidence**: FDR control guarantees and their theoretical validity
- **High confidence**: Relative performance improvements over baseline methods
- **Medium confidence**: Generalizability to black-box settings and other model types
- **Medium confidence**: Scalability to extremely large-scale models (100B+ parameters)

## Next Checks
1. Test PTDI's performance on multimodal models with non-text data (images, audio, video) to assess cross-modal applicability
2. Evaluate computational efficiency and runtime scaling when applied to models with 100B+ parameters
3. Validate the method's performance when the true data usage proportion differs significantly from the conservative estimator's assumptions