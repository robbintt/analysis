---
ver: rpa2
title: Practical Policy Distillation for Reinforcement Learning in Radio Access Networks
arxiv_id: '2511.06563'
source_url: https://arxiv.org/abs/2511.06563
tags:
- teacher
- distillation
- student
- policy
- generalization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper investigates how to deploy AI-driven link adaptation\
  \ in radio access networks (RANs) under tight computational and memory constraints.\
  \ It applies policy distillation\u2014transferring knowledge from large, scenario-agnostic\
  \ or multi-scenario teacher models to smaller student models\u2014to compress model\
  \ size while preserving generalization performance."
---

# Practical Policy Distillation for Reinforcement Learning in Radio Access Networks

## Quick Facts
- arXiv ID: 2511.06563
- Source URL: https://arxiv.org/abs/2511.06563
- Reference count: 19
- Primary result: Distilling AI link adaptation models achieves 30× compression with <1% throughput loss and ±7% BLER deviation in 5G RANs.

## Executive Summary
This paper demonstrates that policy distillation effectively compresses large reinforcement learning models for link adaptation in radio access networks, enabling deployment on legacy hardware with strict computational and memory constraints. By transferring knowledge from large, scenario-agnostic or multi-scenario teacher models to smaller student models, the approach maintains generalization across diverse network conditions while reducing model size by approximately 30× (from ~105k to 3.5k parameters). Experiments in a 5G-compliant simulator show that distilled students retain teacher performance with minimal degradation, whereas direct training of small models from scratch fails significantly. The work addresses a critical deployment challenge for AI-driven RAN functions.

## Method Summary
The method employs policy distillation to compress large DQN-based teacher models into smaller student models for link adaptation in 5G RANs. The teacher model (7-layer MLP with 128 units, ~105k parameters) is trained via distributed reinforcement learning with domain randomization across 5000 network simulations. A distillation dataset is generated by evaluating the trained teacher on 1000 randomized simulations (~20 million state-Q-value pairs). Student models of varying sizes (4×64, 4×32, 3×32 MLPs) are trained via offline distillation using KL divergence loss against the teacher's Q-value distributions. Two strategies are evaluated: single-policy distillation from one teacher, and multi-policy distillation combining multiple scenario-specific teachers into one student. Performance is measured on held-out MIMO, mMIMO, and SCSU benchmark scenarios.

## Key Results
- Distilling to a 3.5k parameter student model reduces size by ~30× while maintaining <1% throughput loss and ±7% BLER deviation vs teacher
- Direct training of small models from scratch fails catastrophically, showing 10-25% throughput degradation and action collapse onto limited MCS indices
- Multi-policy distillation achieves reasonable performance with specialized teachers (2.8% throughput deficit) at lower training cost than single generalized teacher
- Domain randomization during distillation dataset generation preserves generalization across heterogeneous RAN environments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Policy distillation transfers decision-making knowledge from large teacher models to compact student models while preserving generalization capability.
- Mechanism: The student model learns to mimic the teacher's Q-value distributions via KL divergence loss minimization. Specifically, the distillation dataset contains state-Q-value pairs from the teacher, and the student is trained to produce matching softmax distributions over actions. Lower temperature values (τ) sharpen the distribution, accentuating action preferences and improving policy fidelity.
- Core assumption: The teacher's Q-value distributions encode transferable decision logic that can be approximated by a smaller function class without catastrophic information loss.
- Evidence anchors:
  - [abstract] "distilled student models (down to 3.5 k parameters, ~30× smaller) retain the teacher's performance"
  - [section IV-A] "We employ the Kullback-Leibler (KL) divergence loss... softmax transformation is applied to both the teacher's Q-values scaled by a temperature parameter τ > 0"
  - [corpus] Rusu et al. "Policy distillation" (cited but foundational work, not directly validating RAN-specific claims)
- Break condition: If the student architecture is too small to represent the teacher's decision boundaries, or if distillation dataset lacks diversity, performance degradation accelerates non-linearly.

### Mechanism 2
- Claim: Domain randomization during distillation dataset generation preserves the teacher's generalization across heterogeneous RAN environments.
- Mechanism: States for distillation are sampled from randomized network simulations varying antenna configurations, cell radii, bandwidths, transmit powers, UE counts, speeds, and indoor/outdoor ratios. This forces the student to learn a mapping that works across the full configuration space rather than overfitting to specific conditions.
- Core assumption: The diversity in the distillation dataset approximates the diversity encountered in real deployments, and the student can interpolate across this space.
- Evidence anchors:
  - [abstract] "achieved throughput losses under 1% and BLER deviations within ±7% across MIMO, mMIMO, and SCSU benchmark scenarios"
  - [section V-C] "we further apply domain randomization to generate the distillation dataset DT by testing the teacher against 1000 randomized network simulations, resulting in approximately 20 million samples"
  - [corpus] Related work on generalization in RL for RANs (paper ID 88301) discusses similar challenges but corpus validation of this specific mechanism is limited
- Break condition: If real-world deployment conditions fall outside the randomized parameter ranges, or if critical state variables are omitted from randomization, generalization fails.

### Mechanism 3
- Claim: Multi-policy distillation consolidates specialized expertise from multiple scenario-specific teachers into a single generalist student.
- Mechanism: N independently trained teachers, each specialized for different network contexts, produce separate distillation datasets. These are shuffled and aggregated. The student learns from the unified dataset, effectively learning to emulate context-appropriate behavior without explicit context switching logic.
- Core assumption: The aggregated Q-value distributions from diverse teachers form a coherent supervisory signal that doesn't produce conflicting action preferences for similar states.
- Evidence anchors:
  - [section IV-C] "Since the student learns from the combined behavior of multiple specialized teachers, the resulting policy generalizes effectively across diverse RAN environments"
  - [section V-D] "throughput deficit of the smallest student model never exceeds −2.8%... BLER changes remain modest"
  - [corpus] Weak direct validation; corpus includes distillation papers but none specifically validate multi-teacher aggregation for RAN
- Break condition: If teachers produce contradictory Q-value distributions for similar states (e.g., different optimal MCS for similar channel conditions due to unobserved context), student learning becomes unstable.

## Foundational Learning

- Concept: **Markov Decision Process (MDP) formulation for RL**
  - Why needed here: Link adaptation is framed as an episodic MDP where states include CSI, HARQ feedback, and semi-static network info; actions are MCS indices; rewards are spectral efficiency with retransmission penalties. Understanding this formulation is prerequisite to interpreting the distillation setup.
  - Quick check question: Can you explain why the reward function penalizes retransmissions proportionally to transmission attempt number n?

- Concept: **Q-learning and Deep Q-Networks (DQN)**
  - Why needed here: The teacher policy is a trained DQN that outputs Q-values for each MCS action. Distillation transfers these Q-value distributions, not just final actions. Understanding what Q-values represent is essential.
  - Quick check question: What does a Q-value q(s, a) represent in this context, and why distill Q-values rather than just the argmax action?

- Concept: **Knowledge distillation fundamentals**
  - Why needed here: The paper assumes familiarity with teacher-student paradigms. KL divergence measures distribution similarity; temperature τ controls distribution sharpness. Without this background, the loss function design is opaque.
  - Quick check question: Why does lowering temperature τ sharpen the softmax distribution, and why would this improve distillation fidelity?

## Architecture Onboarding

- Component map:
  - **Teacher model**: 7-layer MLP (128 units/layer), ~105k parameters, trained via distributed RL with domain randomization
  - **Student models**: 4×64 (~15k), 4×32 (~5k), or 3×32 (~3.5k) MLP variants
  - **Distillation dataset**: ~20M state-Q-value pairs from teacher evaluation on 1000 randomized simulations
  - **Training infrastructure**: Distributed actors interacting with 5G-compliant simulator; single learner aggregating experience

- Critical path:
  1. Train teacher using distributed RL with domain randomization (5000 simulations, ~16 actors)
  2. Generate distillation dataset by evaluating trained teacher on randomized scenarios
  3. Train student via KL divergence minimization against teacher Q-values
  4. Evaluate on held-out benchmark scenarios (MIMO, mMIMO, SCSU)

- Design tradeoffs:
  - **Model size vs. performance**: 3×32 student loses ~2.8% throughput vs. <1% for 4×64; choose based on hardware constraints
  - **Single vs. multi-policy**: Single-policy preserves teacher performance better (~1% throughput loss) but requires expensive generalized teacher training; multi-policy enables cheaper specialized teachers with modest additional degradation
  - **Distillation dataset size**: Paper uses ~20M samples; smaller datasets may under-represent state space

- Failure signatures:
  - **Direct small-model training**: 3×32 model trained from scratch shows 10-25% throughput degradation and collapses onto limited MCS indices (peaks at 10, 16; rarely selects >19)
  - **Insufficient distillation diversity**: If distillation dataset lacks representative scenarios, student fails on benchmark environments
  - **Temperature too high**: Over-smoothed Q-value distributions provide weak supervision signal

- First 3 experiments:
  1. **Baseline reproduction**: Train the 7×128 teacher on a simplified single-scenario simulation; verify distillation to 4×64 student matches teacher performance in that scenario (sanity check of distillation pipeline)
  2. **Domain randomization ablation**: Generate distillation datasets with progressively fewer randomized parameters; measure correlation between dataset diversity and benchmark generalization gap
  3. **Student capacity sweep**: Distill to architectures between 4×64 and 3×32; plot throughput loss vs. parameter count to identify knee point for target hardware constraints

## Open Questions the Paper Calls Out
- Can online policy distillation achieve comparable or superior generalization performance to the offline approach in dynamic RAN environments?
- Do the distillation efficiency and generalization capabilities hold when deploying the student models on actual legacy baseband hardware (e.g., FPGAs or DSPs) rather than in a simulator?
- Is the proposed distillation framework effective for other L1/L2 RAN functions with different action spaces, such as channel estimation or resource scheduling?

## Limitations
- The paper lacks explicit details on key hyperparameters for both DQN teacher training and the distillation process
- The effectiveness of the proposed multi-policy distillation strategy is demonstrated only through a single multi-scenario experiment without ablation studies
- The paper does not address potential distribution shifts between the randomized distillation dataset and real-world deployment conditions

## Confidence
- **High Confidence**: Claims about policy distillation enabling significant model compression (~30× reduction from ~105k to 3.5k parameters) while maintaining performance (<1% throughput loss, ±7% BLER deviation) are directly supported by experimental results across multiple benchmark scenarios.
- **Medium Confidence**: The superiority of distillation over direct training of small models is well-demonstrated, but the exact magnitude of performance gaps depends on unreported hyperparameters and implementation details.
- **Low Confidence**: Claims about the specific effectiveness of the multi-policy distillation approach lack comparative validation against alternative knowledge aggregation strategies, and the mechanism by which domain randomization preserves generalization is only qualitatively justified.

## Next Checks
1. **Hyperparameter Sensitivity**: Systematically vary teacher training parameters (learning rate, γ, exploration schedule) and distillation parameters (τ, learning rate, batch size) to identify which most significantly impact final student performance and robustness.
2. **Real-World Deployment Test**: Deploy the distilled student model on actual legacy RAN hardware with live traffic to measure computational overhead, inference latency, and performance under realistic channel conditions not perfectly captured by the simulator.
3. **Multi-Policy Ablation Study**: Compare the proposed multi-policy distillation against alternative aggregation methods (weighted averaging of teacher outputs, context-based teacher selection) using the same teacher models to isolate the contribution of the aggregation strategy itself.