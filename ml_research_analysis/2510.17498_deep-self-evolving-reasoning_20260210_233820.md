---
ver: rpa2
title: Deep Self-Evolving Reasoning
arxiv_id: '2510.17498'
source_url: https://arxiv.org/abs/2510.17498
tags:
- reasoning
- self-evolving
- dser
- solution
- aime
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Deep Self-Evolving Reasoning (DSER) addresses the challenge of
  extending reasoning limits in open-weight, smaller-scale models on extremely hard
  problems. DSER introduces a probabilistic paradigm that treats iterative verification
  and refinement as a Markov chain, where convergence to a correct solution is guaranteed
  as long as the probability of improvement marginally exceeds that of degradation.
---

# Deep Self-Evolving Reasoning

## Quick Facts
- arXiv ID: 2510.17498
- Source URL: https://arxiv.org/abs/2510.17498
- Authors: Zihan Liu; Shun Zheng; Xumeng Wen; Yang Wang; Jiang Bian; Mao Yang
- Reference count: 40
- Primary result: 8B model solves 5/9 previously unsolvable AIME problems, boosting accuracy by 6.5% (AIME 2024) and 9.0% (AIME 2025)

## Executive Summary
Deep Self-Evolving Reasoning (DSER) introduces a probabilistic framework that treats iterative verification and refinement as a Markov chain, enabling smaller open-weight models to solve extremely hard problems through test-time scaling. By running multiple long-horizon, self-evolving processes in parallel and using majority voting, DSER amplifies small positive tendencies in the model's self-improvement capability, allowing convergence to correct solutions even when individual improvement probabilities are marginal. Empirically, DSER applied to DeepSeek-R1-0528-Qwen3-8B achieves state-of-the-art performance on AIME 2024-2025 benchmarks, solving problems that were previously unsolvable for this model class.

## Method Summary
DSER implements a verification-refinement loop where solutions iteratively improve through self-verification and refinement steps. For each problem, 64 parallel processes run independently, with each process performing up to 80 iterations of: (1) verifying the current solution, (2) refining based on verification feedback. The final answers from the last 10 iterations of each process (640 total solutions) are aggregated via majority voting. Unlike verification-dependent approaches that terminate based on verification signals, DSER continues iteration regardless of intermediate verification outcomes, treating the process as a Markov chain where convergence to correct solutions is guaranteed when improvement probability exceeds degradation probability. Each iteration uses up to 64K tokens for reasoning calls, with a total budget of approximately 800M tokens per problem.

## Key Results
- DSER solves 5 out of 9 previously unsolvable AIME problems
- Overall accuracy improves by 6.5% on AIME 2024 and 9.0% on AIME 2025
- 8B model surpasses single-turn accuracy of its 600B-parameter teacher through majority voting
- Verification-dependent approaches solved only 2/9 hard problems vs DSER's 5/9

## Why This Works (Mechanism)

### Mechanism 1
Iterative verification-refinement can converge to correct solutions even with weak individual step accuracy through Markov chain convergence. Solution correctness is modeled as a two-state Markov chain (Correct/Incorrect) where the stationary distribution π_C = p_IC/(p_IC + p_CI) dominates correct solutions when improvement probability p_IC marginally exceeds degradation probability p_CI. Convergence rate is governed by |λ₂| = |1 - p_IC - p_CI|. Core assumption: transition probabilities remain stationary across iterations for a given problem. Evidence: [abstract] guarantees convergence when p_IC > p_CI; [section 3.2-3.3] formalize transition matrix derivation; [corpus] AlphaApollo orchestrates verification tools but lacks Markov convergence guarantees. Break condition: if p_CI ≥ p_IC consistently, stationary distribution favors incorrect solutions; very small p_IC leads to impractically slow convergence.

### Mechanism 2
Parallel independent processes amplify small positive tendencies into reliable majority-vote correctness. Correct solutions converge to the same ground-truth answer while incorrect solutions diverge heterogeneously. Even when π_C < 0.5, majority voting across K parallel DSER runs can yield correct answers because incorrect answers scatter across different wrong values. Core assumption: incorrect solutions produce uncorrelated/diverse wrong answers rather than systematically converging to the same incorrect value. Evidence: [abstract] states parallel processes amplify small positive tendencies; [section 3.3] explains majority voting can succeed when π_C < 0.5; [corpus] Learning to Refine focuses on Best-of-N selection rather than Markov-guided iteration. Break condition: if incorrect answers systematically cluster around a common wrong value, majority voting fails regardless of iteration count.

### Mechanism 3
Verification-agnostic iteration outperforms verification-dependent early-exit strategies for hard problems. DSER marginalizes over verification outcomes, continuing iteration regardless of intermediate self-verification signals. This avoids premature termination and false-positive acceptance that plague verification-dependent approaches like Huang & Yang's framework, which creates absorbing states in the Markov chain. Core assumption: the model's self-verification accuracy is unreliable for problems beyond its baseline capacity. Evidence: [section 3.4] states verification-induced absorbing states hinder deep self-evolution; [section 4, Figure 6] shows verification-dependent approach solved only 2/9 hard problems vs DSER's 5/9; [corpus] no direct comparison of verification-agnostic vs verification-dependent iteration. Break condition: if verification accuracy is actually high for a problem class, verification-dependent exit can save computation; DSER's agnostic approach then wastes resources on unnecessary iterations.

## Foundational Learning

- **Markov chain stationary distributions**: Why needed: DSER's theoretical guarantee rests on proving convergence to a stationary distribution where correct solutions dominate. Quick check: Given a 2-state Markov chain with P(C→I)=0.3 and P(I→C)=0.4, what is the long-run probability of being in state C? (Answer: 0.4/(0.4+0.3) ≈ 0.57)

- **Test-time scaling via compute amortization**: Why needed: DSER trades test-time computation (80+ iterations × 64K tokens × 64 parallel runs) for extended reasoning capability. Quick check: If each iteration costs ~156K tokens (64K solution + 64K verify + ~32K refine), what's the total token budget for 80 iterations across 64 parallel runs? (~800M tokens)

- **Self-verification brittleness in smaller models**: Why needed: DSER is specifically designed for open-weight models with weak verification; understanding this limitation motivates the verification-agnostic approach. Quick check: Why does a high false-positive verification rate (claiming "correct" when wrong) specifically harm verification-dependent frameworks? (False acceptance creates absorbing states that cement wrong answers)

## Architecture Onboarding

- **Component map**: Question q → Initial Solve → s^(0) → [Self-Evolving Loop: iterations 1..N] → [Aggregation] → Majority vote over final 10 iterations across K=64 parallel runs

- **Critical path**: The self-evolving loop (Verify → Refine) dominates runtime. Each iteration requires 3 LLM calls (optional solve + verify + refine) with up to 64K tokens each. Convergence may require 20-80 iterations for hard problems.

- **Design tradeoffs**:
  - Iteration depth vs. parallel width: More iterations help slow-converging problems; more parallel runs help when π_C is low but majority-vote signal exists
  - Token budget per call: 64K enables long-form CoT but increases latency/cost; reducing this may truncate reasoning chains
  - Verification prompt complexity: Simple prompts (as used here) maintain generality; structured rubrics may help some domains but introduce brittleness

- **Failure signatures**:
  - Stagnation: Avg@K plateaus without Cons@K improving → p_IC ≈ 0, problem is genuinely beyond model's reachable capability
  - Oscillation: Solutions alternate between 2-3 wrong answers → systematic error in problem understanding, unlikely to self-correct
  - Degradation dominance: Accuracy decreases with iterations → p_CI > p_IC, model corrupts correct solutions faster than it improves incorrect ones

- **First 3 experiments**:
  1. Baseline calibration: Run single-turn inference with K=128 samples on target problem set to establish Pass@1 and identify "unsolvable" problems (those where majority voting fails). This determines which problems warrant DSER.
  2. Transition probability estimation: On held-out validation set, manually annotate correctness of solutions before/after one verify-refine cycle to estimate p_IC and p_CI. This predicts whether DSER will help and how many iterations are needed (|λ₂| ≈ 1 - p_IC - p_CI).
  3. Ablation on iteration depth: Fix K=32 parallel runs, vary iterations from {10, 20, 40, 80}. Plot Avg@K and Cons@K curves to identify point of diminishing returns for your problem class before committing to expensive long-horizon runs.

## Open Questions the Paper Calls Out

### Open Question 1
Can new reinforcement learning objectives be designed to explicitly optimize improvement probability (p_IC) and minimize degradation probability (p_CI), rather than relying on implicit incentives from verifiable rewards? Basis: [explicit] Section 3.3 states: "in addition to purely optimizing self-verification or self-correction capabilities... we could develop new optimization objectives to improve p_IC and decrease p_CI explicitly." Why unresolved: Current RL methods like GRPO optimize for final-answer accuracy but do not directly target the transition probabilities governing self-evolution quality. What evidence would resolve it: A training paradigm that demonstrably shifts p_IC > p_CI for hard problems, with measurable improvements in DSER convergence rates.

### Open Question 2
Does integrating DSER into GRPO's exploration phase improve discovery of high-quality reasoning traces for extremely difficult problems? Basis: [explicit] The conclusion states: "applying DSER to the exploration phase of reinforcement learning, such as in GRPO, could help discover high-quality reasoning traces for the most challenging problems." Why unresolved: DSER has only been evaluated at inference time; its utility during training exploration remains untested. What evidence would resolve it: Empirical comparison of GRPO with DSER-enhanced exploration versus standard GRPO on problem sets where baseline exploration fails.

### Open Question 3
Can learnable verification modules or sophisticated search algorithms enhance DSER's efficiency and success rate compared to simple prompt-based verification? Basis: [explicit] The conclusion states: "integrating more sophisticated search algorithms or learnable verification modules could enhance its efficiency and success rate." Why unresolved: DSER currently relies on concise prompts; the potential gains from external or trained verifiers are unknown. What evidence would resolve it: Ablation studies comparing prompt-based DSER against variants with trained verifiers or beam/search strategies on the same benchmark.

### Open Question 4
What training paradigms can produce models with intrinsic self-verification, constructive feedback generation, and degradation avoidance capabilities? Basis: [explicit] Page 3 states: "A key direction for future research is therefore to develop models that are capable of problem-solving, self-verification, providing constructive feedback, increasing correction likelihood, avoiding potential degradation, etc." Why unresolved: Current models exhibit weak, unstable verification and refinement, limiting DSER's stationary distribution quality. What evidence would resolve it: Training methods yielding models where p_IC significantly exceeds p_CI on held-out hard problems, with higher stationary correct-solution ratios.

## Limitations

- Theoretical foundation assumes stationary transition probabilities across iterations, but empirical evidence suggests these may drift as solutions evolve
- Parallel amplification mechanism critically depends on incorrect solutions producing uncorrelated wrong answers, a property not rigorously validated
- Extraordinary claim of surpassing a 600B-parameter teacher through majority voting requires careful scrutiny and validation

## Confidence

- **High confidence**: The Markov chain convergence framework and its basic mathematical properties are well-established. The empirical demonstration that DSER improves accuracy on AIME problems from 2/9 to 5/9 for previously unsolvable problems is reproducible given the described methodology.
- **Medium confidence**: The mechanism by which parallel independent processes amplify small positive tendencies relies on assumptions about solution diversity that are plausible but not proven. The practical effectiveness of verification-agnostic iteration over verification-dependent approaches shows strong empirical support but may not generalize to all problem domains.
- **Low confidence**: The claim of surpassing a 600B-parameter teacher through majority voting requires careful validation, as this would represent a fundamental shift in understanding the relationship between model capacity and test-time computation. The general applicability beyond mathematical reasoning problems remains untested.

## Next Checks

1. **Transition probability stability analysis**: For a representative subset of problems, manually annotate correctness of solutions at multiple iteration points to empirically verify whether transition probabilities (p_IC, p_CI) remain stationary across the self-evolving process. This directly tests the Markov chain assumption.

2. **Incorrect solution diversity validation**: Collect and cluster the final incorrect answers from parallel DSER runs on problems where majority voting succeeds despite low π_C. Analyze whether incorrect answers are truly diverse and uncorrelated, or if they systematically converge to common wrong values that would undermine the majority voting mechanism.

3. **Teacher accuracy gap quantification**: Systematically compare DSER's performance against the 600B-parameter teacher's single-turn accuracy across all problem types (not just previously unsolvable ones). This requires obtaining or simulating the teacher's performance to validate whether the claimed "surpassing" effect holds broadly or is specific to certain problem characteristics.