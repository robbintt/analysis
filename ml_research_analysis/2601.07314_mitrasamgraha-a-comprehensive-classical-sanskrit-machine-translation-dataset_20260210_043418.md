---
ver: rpa2
title: 'Mitrasamgraha: A Comprehensive Classical Sanskrit Machine Translation Dataset'
arxiv_id: '2601.07314'
source_url: https://arxiv.org/abs/2601.07314
tags:
- sanskrit
- dataset
- translation
- classical
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Mitrasamgraha, the largest publicly available
  Sanskrit-English machine translation dataset, containing 391,548 bitext pairs covering
  six literary domains and three millennia of Sanskrit. The dataset was created using
  BertAlign for sentence alignment, followed by manual inspection and post-correction.
---

# Mitrasamgraha: A Comprehensive Classical Sanskrit Machine Translation Dataset

## Quick Facts
- **arXiv ID:** 2601.07314
- **Source URL:** https://arxiv.org/abs/2601.07314
- **Reference count:** 25
- **Primary result:** Largest publicly available Sanskrit-English machine translation dataset with 391,548 bitext pairs across six literary domains and three millennia

## Executive Summary
This paper introduces Mitrasamgraha, the largest publicly available Sanskrit-English parallel corpus containing 391,548 bitext pairs. The dataset was constructed using BertAlign for sentence alignment followed by manual inspection and post-correction, covering six literary domains spanning three millennia of Sanskrit literature. The authors evaluate multiple automatic MT metrics and benchmark both commercial and open models, demonstrating that fine-tuning NLLB and Gemma models significantly improves translation quality. Despite these advances, challenges remain in handling complex compounds, philosophical concepts, and multi-layered metaphors. The dataset and models are publicly released to support Sanskrit NLP research.

## Method Summary
The Mitrasamgraha dataset was created through a systematic process involving sentence alignment using BertAlign, followed by comprehensive manual inspection and post-correction. The corpus encompasses six literary domains including philosophical, religious, and narrative texts spanning three millennia of Sanskrit literature. The authors evaluated translation quality using both traditional metrics (BLEU, chrF) and neural metrics (neural BLEURT, GEMBA/GEMBA*), finding stronger correlation between the latter and expert human judgments. Benchmark experiments included fine-tuning both commercial and open-source models, with particular attention to the performance improvements achieved through domain-specific adaptation on the Mitrasamgraha corpus.

## Key Results
- Mitrasamgraha contains 391,548 Sanskrit-English bitext pairs, making it the largest publicly available Sanskrit-English MT dataset
- Neural BLEURT and GEMBA/GEMBA* metrics show stronger correlation with expert human judgments than BLEU and chrF
- Fine-tuning NLLB and Gemma models on Mitrasamgraha significantly improves translation quality across multiple domains
- Translation challenges persist for complex compounds, philosophical concepts, and multi-layered metaphors

## Why This Works (Mechanism)
The effectiveness of Mitrasamgraha stems from its comprehensive coverage of Sanskrit's literary diversity across time periods and genres, combined with rigorous alignment and quality control procedures. By leveraging BertAlign for initial sentence alignment and following with manual inspection, the dataset achieves high alignment accuracy while maintaining the semantic richness of classical Sanskrit texts. The neural metrics (BLEURT, GEMBA) capture semantic nuances better than traditional n-gram based metrics, particularly important for a morphologically rich and semantically complex language like Sanskrit. The systematic fine-tuning approach demonstrates that domain-specific adaptation on comprehensive parallel data significantly improves translation quality, even for challenging linguistic phenomena like compounds and philosophical terminology.

## Foundational Learning
- **Sentence alignment using BertAlign**: Essential for creating accurate parallel corpora from non-aligned text sources; quick check: verify alignment quality on a sample of 100 sentence pairs
- **Manual inspection protocols**: Critical for ensuring translation quality and consistency; quick check: establish inter-annotator agreement metrics
- **Automatic MT evaluation metrics**: Understanding when traditional (BLEU, chrF) versus neural (BLEURT, GEMBA) metrics are appropriate; quick check: compare metric correlations with human judgments
- **Domain adaptation in MT**: The process of fine-tuning models on specific domain data improves performance; quick check: measure performance gains across different literary genres
- **Sanskrit morphology and compounding**: Understanding how complex morphological structures affect translation; quick check: analyze error patterns in compound translation
- **Literary domain classification**: Recognizing how different text types (philosophical, religious, narrative) present unique translation challenges; quick check: verify genre distribution in training and test sets

## Architecture Onboarding
- **Component map**: Source texts -> BertAlign alignment -> Manual inspection -> Quality control -> Dataset split -> Model fine-tuning -> Evaluation (BLEU/chrF/BLEURT/GEMBA) -> Expert validation
- **Critical path**: Text alignment and manual inspection constitute the most critical path, as dataset quality directly determines downstream model performance
- **Design tradeoffs**: Larger dataset size versus annotation quality tradeoff; comprehensive genre coverage versus focused domain expertise; traditional versus neural evaluation metrics
- **Failure signatures**: Poor alignment quality manifests as semantic drift in translations; insufficient manual inspection leads to noisy parallel data; inappropriate metric choice fails to capture semantic quality
- **First experiments**: 1) Train baseline NLLB model without fine-tuning, 2) Fine-tune NLLB on Mitrasamgraha and measure improvement, 3) Compare automatic metrics correlation with expert judgments

## Open Questions the Paper Calls Out
- None specified in the provided materials

## Limitations
- Representativeness concerns regarding sampling strategy and potential biases across Sanskrit's three millennia of literature
- Limited documentation of manual inspection quality control procedures and inter-annotator agreement metrics
- Observations about translation challenges lack systematic error analysis across diverse text genres
- Generalizability of translation quality improvements to unseen text types remains uncertain

## Confidence
- **High**: Mitrasamgraha represents the largest publicly available Sanskrit-English parallel corpus, supported by clear quantitative comparisons
- **Medium**: Neural BLEURT and GEMBA/GEMBA* metrics provide superior correlation with expert judgments, though based on specific evaluation conditions
- **Low**: Generalizability of observed translation challenges to all Sanskrit NLP applications, given limited benchmark testing scope

## Next Checks
1. Conduct inter-annotator agreement studies on a random sample of 1,000 sentence pairs to establish quality control metrics for the manual inspection process
2. Perform genre-balanced testing on held-out data from each of the six literary domains to verify model performance consistency across different text types
3. Compare translation quality on philosophical versus narrative texts using both automatic metrics and targeted human evaluation to quantify the impact of domain-specific challenges