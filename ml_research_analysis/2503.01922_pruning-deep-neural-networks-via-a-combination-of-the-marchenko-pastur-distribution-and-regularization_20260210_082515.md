---
ver: rpa2
title: Pruning Deep Neural Networks via a Combination of the Marchenko-Pastur Distribution
  and Regularization
arxiv_id: '2503.01922'
source_url: https://arxiv.org/abs/2503.01922
tags:
- pruning
- matrix
- singular
- weight
- values
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel pruning method for deep neural networks
  based on Random Matrix Theory (RMT), specifically using the Marchenko-Pastur distribution
  to identify and remove noise-like weights in neural network layers. The approach
  is applied to Vision Transformers (ViTs), achieving a 30-50% reduction in parameters
  with less than 1% accuracy loss on ImageNet.
---

# Pruning Deep Neural Networks via a Combination of the Marchenko-Pastur Distribution and Regularization

## Quick Facts
- arXiv ID: 2503.01922
- Source URL: https://arxiv.org/abs/2503.01922
- Reference count: 40
- Primary result: 30-50% parameter reduction in ViTs with <1% accuracy loss on ImageNet

## Executive Summary
This paper introduces a novel pruning method for deep neural networks based on Random Matrix Theory, specifically using the Marchenko-Pastur distribution to identify and remove noise-like weights in neural network layers. The approach is applied to Vision Transformers (ViTs), achieving significant parameter reduction with minimal accuracy loss. The method combines singular vector pruning, direct coefficient pruning, and regularization techniques to effectively prune ViT models while maintaining performance.

## Method Summary
The pruning method leverages the Marchenko-Pastur distribution to analyze the spectrum of weight matrices and identify noise-like components that can be safely removed. The approach involves three main components: singular vector pruning which targets the dominant singular vectors, direct coefficient pruning which removes individual weights below a threshold, and regularization which stabilizes the pruning process. The method is specifically applied to Vision Transformer architectures, analyzing both attention and MLP layers separately to optimize pruning effectiveness.

## Key Results
- Achieves 30-50% reduction in parameters for ViT models
- Maintains less than 1% accuracy loss on ImageNet
- Demonstrates state-of-the-art pruning results for ViT architectures
- Provides theoretical proofs for weight matrix randomness reduction during training

## Why This Works (Mechanism)
The method works by exploiting the statistical properties of weight matrices through Random Matrix Theory. The Marchenko-Pastur distribution identifies which eigenvalues correspond to noise versus signal in the weight matrices. By removing components associated with noise eigenvalues, the method can safely reduce model size without impacting performance. The regularization component helps stabilize this process by preventing over-pruning of important weights.

## Foundational Learning
- **Random Matrix Theory**: Why needed - provides mathematical framework for analyzing weight matrix spectra; Quick check - verify eigenvalue distribution follows Marchenko-Pastur law
- **Marchenko-Pastur distribution**: Why needed - identifies noise versus signal in weight matrices; Quick check - confirm noise eigenvalues fall within theoretical bounds
- **Singular value decomposition**: Why needed - enables targeted pruning of specific matrix components; Quick check - validate SVD decomposition accuracy
- **Vision Transformer architecture**: Why needed - target model for pruning application; Quick check - understand attention and MLP layer structures
- **Regularization techniques**: Why needed - stabilizes pruning process and prevents over-pruning; Quick check - verify gradient flow during training
- **Pruning criteria**: Why needed - determines which weights are most important; Quick check - validate pruning doesn't remove critical weights

## Architecture Onboarding

**Component Map**: Input -> Feature Extraction -> Classification Head
- Feature Extraction: Multi-head Self-Attention (MSA) -> MLP blocks
- Each block contains LayerNorm, MSA, MLP with skip connections
- Final classification head with global average pooling

**Critical Path**: Input tokens -> MSA layers -> MLP layers -> Classification head
- Attention mechanism processes token relationships
- MLP layers perform feature transformation
- Skip connections maintain gradient flow

**Design Tradeoffs**: 
- Layer-wise independence assumption vs. inter-layer dependencies
- Computational efficiency vs. pruning effectiveness
- Theoretical rigor vs. practical implementation complexity

**Failure Signatures**:
- Excessive accuracy drop (>1%) indicates over-pruning
- Training instability suggests improper regularization
- Suboptimal compression ratio indicates conservative pruning

**First Experiments**:
1. Apply method to base ViT model and verify parameter reduction
2. Test different regularization parameter values (λ, ρ)
3. Compare results against baseline pruning methods

## Open Questions the Paper Calls Out
None specified in the provided information.

## Limitations
- Effectiveness on architectures with complex connectivity patterns (skip connections, attention) remains uncertain
- Theoretical assumptions about weight matrix distributions may not generalize to all model variants
- Limited sensitivity analysis for critical hyperparameters (λ and ρ)

## Confidence
- **High confidence**: Empirical pruning results on ViT models (30-50% reduction with <1% accuracy loss)
- **Medium confidence**: Theoretical framework connecting Marchenko-Pastur distribution to weight importance
- **Low confidence**: Generalization to non-ViT architectures and different connectivity patterns

## Next Checks
1. Test the method on ResNet and ConvNeXt architectures to verify cross-architecture applicability
2. Conduct ablation studies varying λ and ρ parameters to establish robustness bounds
3. Apply the pruning approach to larger ViT variants (e.g., ViT-L/16) and evaluate scaling behavior