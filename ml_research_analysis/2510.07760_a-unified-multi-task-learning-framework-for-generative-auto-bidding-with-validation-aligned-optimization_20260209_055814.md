---
ver: rpa2
title: A Unified Multi-Task Learning Framework for Generative Auto-Bidding with Validation-Aligned
  Optimization
arxiv_id: '2510.07760'
source_url: https://arxiv.org/abs/2510.07760
tags:
- learning
- data
- task
- tasks
- auto-bidding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses data scarcity in generative auto-bidding by
  proposing Validation-Aligned Multi-task Optimization (VAMO), a framework that adaptively
  reweights cross-task data contributions based on alignment with validation gradients.
  This approach mitigates distribution shift bias and improves generalization in data-scarce
  tasks.
---

# A Unified Multi-Task Learning Framework for Generative Auto-Bidding with Validation-Aligned Optimization

## Quick Facts
- arXiv ID: 2510.07760
- Source URL: https://arxiv.org/abs/2510.07760
- Reference count: 40
- Primary result: Validation-Aligned Multi-task Optimization (VAMO) improves data-scarce generative auto-bidding by up to 46.8% relative performance over state-of-the-art baselines.

## Executive Summary
This paper addresses data scarcity in generative auto-bidding by proposing Validation-Aligned Multi-task Optimization (VAMO), a framework that adaptively reweights cross-task data contributions based on alignment with validation gradients. This approach mitigates distribution shift bias and improves generalization in data-scarce tasks. Theoretical analysis shows that VAMO tightens generalization bounds compared to naive data sharing. The method is integrated into a unified multi-task generative auto-bidding architecture with a shared backbone and FiLM module. Extensive experiments on real-world advertising datasets demonstrate that VAMO significantly outperforms state-of-the-art baselines, achieving up to 46.8% relative improvement in multi-task performance. Ablation studies confirm the effectiveness of validation-based weighting and FiLM structure.

## Method Summary
The method combines a shared Transformer backbone with FiLM conditioning to handle task-specific constraints in generative auto-bidding. VAMO introduces a validation-aligned optimization strategy that computes inner products between validation gradients and cross-task training gradients to determine adaptive task weights. These weights are entropy-regularized to maintain diversity while preventing gradient collapse. The architecture processes trajectory data with requirement conditioning vectors, and the FiLM module applies affine transformations to hidden states based on these requirements. Training involves relabeling source task trajectories for the target task and updating model parameters using the weighted combination of gradients, where weights are computed every 5 steps based on gradient alignment with validation signals.

## Key Results
- VAMO achieves up to 46.8% relative improvement in multi-task performance compared to state-of-the-art baselines
- Validation-based weighting outperforms both naive data sharing and training-gradient-only alignment
- FiLM module provides additional performance boost, with ablation showing -40.27% to -47.35% improvement when included
- Temperature hyperparameter λ significantly impacts minority task performance, with moderate values showing optimal results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weighting source tasks by alignment between their training gradients and a target-task validation gradient improves generalization on data-scarce tasks compared to data-proportional weighting.
- Mechanism: At each training step, compute the inner product $m_{ik} = \langle g_k, g_{ik} \rangle$ between the validation gradient $g_k$ (from held-out target data) and each cross-task training gradient $g_{ik}$ (from source task $i$ relabeled for target $k$). Use these alignment scores to compute softmax weights $w^*_{ik} = \text{softmax}(m_{ik}/\lambda)$. Update parameters with the weighted gradient combination. This downweights sources whose gradients oppose target generalization and upweights helpful sources.
- Core assumption: The validation gradient is a sufficiently unbiased estimator of the true target gradient direction, and alignment correlates with beneficial transfer despite distribution shift.
- Evidence anchors:
  - [abstract] "VAMO, which adaptively assigns task weights based on the alignment between per-task training gradients and a held-out validation gradient, thereby steering updates toward validation improvement and better matching deployment objectives."
  - [section 4.2] Equation (5): "The resulting change in validation loss is then approximated by: $\Delta R^{val}_k \approx -\eta \sum_{i=1}^K w_{ik} m_{ik}, m_{ik} \triangleq \langle g_k, g_{ik} \rangle$."
  - [corpus] Weak direct corpus support for this specific gradient-alignment mechanism; related work focuses on value-based signals in offline RL rather than generative settings.
- Break condition: If validation set is too small or non-representative, gradient estimates become noisy, causing unstable or misleading weights.

### Mechanism 2
- Claim: Entropy regularization on task weights prevents collapse to a single dominant source and preserves beneficial diversity from multiple auxiliary tasks.
- Mechanism: Add $-\lambda \sum_i w_{ik} \ln w_{ik}$ to the alignment maximization objective. The closed-form solution $w^*_{ik} = \exp(m_{ik}/\lambda) / \sum_j \exp(m_{jk}/\lambda)$ interpolates between hard assignment ($\lambda \to 0$) and uniform weighting ($\lambda \to \infty$).
- Core assumption: Multiple partially-aligned sources collectively provide more robust signal than any single source, even the highest-aligned one.
- Evidence anchors:
  - [section 4.2] "However, maximizing $\sum_{i=1}^K w_{ik}m_{ik}$ tends to lead to a sparse assignment, favoring only the task with the highest $m_{ik}$... To promote a more balanced integration of cross-task knowledge, we propose an entropy-regularized objective."
  - [section 5.3, Figure 5] Ablation on temperature $\lambda$ shows extreme values (0.01 or 1.0) degrade performance; moderate $\lambda$ achieves optimal balance.
  - [corpus] No direct corpus comparison; entropy-regularized MTL weighting is not discussed in neighbor papers.
- Break condition: If $\lambda$ is misconfigured (too low → overfit to one source; too high → reverts to naive uniform weighting), benefits diminish.

### Mechanism 3
- Claim: A shared Transformer backbone with FiLM (Feature-wise Linear Modulation) conditioning enables the model to capture task-agnostic trajectory structure while adapting feature representations to task-specific constraints.
- Mechanism: Input tokens concatenate trajectory requirements, state $s_t$, and advertiser features. The FiLM module applies affine transformations $\gamma(y) \cdot \text{Hidden} + \beta(y)$ conditioned on requirement embedding $y$, modulating intermediate features before the next Transformer block.
- Core assumption: Task-specific constraint information is cleanly separable in the conditioning vector and can be linearly injected into hidden representations.
- Evidence anchors:
  - [section 4.3] "To enhance the model's sensitivity to these requirements, we incorporate a Feature-wise Linear Modulation (FiLM) mechanism... It applies a linear transformation to the hidden features, conditioned on the requirements provided in the input tokens."
  - [section 5.3, Table 3] Ablation shows VAO without FiLM achieves $\Delta m\% = -40.27$; with FiLM, $\Delta m\% = -47.35$, confirming FiLM provides an additional boost.
  - [corpus] Neighbor papers on generative auto-bidding (e.g., AIGB, Decision Transformer variants) do not explicitly use FiLM; this is an architecture contribution specific to this work.
- Break condition: If requirement embeddings are poorly designed or contain conflicting signals (e.g., masked irrelevant constraints leak information), FiLM may introduce noise rather than useful modulation.

## Foundational Learning

- Concept: **Gradient alignment as a proxy for transfer utility**
  - Why needed here: VAO relies on interpreting inner products between gradients as indicators of whether a source task's update direction helps or harms target generalization.
  - Quick check question: Given two gradient vectors $g_1$ and $g_2$, what does a negative inner product $\langle g_1, g_2 \rangle < 0$ imply about their directional relationship?

- Concept: **Distribution shift in multi-task / transfer learning**
  - Why needed here: The theoretical motivation (Theorem 4.2) shows naive data sharing introduces bias proportional to distributional divergence; understanding this is essential to appreciate why reweighting helps.
  - Quick check question: If source distribution $P_i$ differs from target $P_k$, why might training on $P_i$ data with $P_k$'s loss function still not improve performance on $P_k$?

- Concept: **Rademacher complexity and generalization bounds**
  - Why needed here: The paper's theoretical justification uses Rademacher complexity to bound generalization error; readers should recognize this as a standard capacity measure.
  - Quick check question: What does higher Rademacher complexity imply about a hypothesis class's tendency to overfit?

## Architecture Onboarding

- Component map:
  - Data relabeling layer -> Shared Causal Transformer backbone -> FiLM conditioning module -> Gradient alignment compute -> Weight optimizer

- Critical path:
  1. Relabel all source datasets for target task $k$
  2. Forward/backward pass to compute $g_k$ (validation) and all $g_{ik}$ (training)
  3. Compute alignment scores $m_{ik} = \langle g_k, g_{ik} \rangle$
  4. Update weights $w^*_{ik} = \text{softmax}(m_{ik}/\lambda)$
  5. Apply parameter update: $\theta_{t+1} = \theta_t - \eta \sum_i w^*_{ik} g_{ik}$

- Design tradeoffs:
  - Validation set size: Larger sets give more stable gradient estimates but reduce training data; paper uses small held-out sets (Table 4 shows 20–400 validation trajectories per task)
  - Weight update frequency: Paper updates weights every 5 steps to reduce overhead; less frequent updates may miss dynamic alignment changes
  - Temperature $\lambda$: Low values emphasize only the best-aligned source; high values dilute benefits by including misaligned sources

- Failure signatures:
  - Weight collapse: All $w^*_{ik}$ concentrate on one source → check if $\lambda$ is too small or validation gradient is noisy
  - Negative transfer on minority tasks: Data-scarce task performance degrades → inspect if dominant source tasks have high $m_{ik}$ but large distribution shift
  - High variance across seeds: Standard error spikes → validation set may be too small or gradient estimation is unstable

- First 3 experiments:
  1. Baseline comparison (single-task vs. naive sharing vs. VAO) on a data-scarce task: Replicate Table 1 setup to confirm VAO improves over both no-sharing and naive-sharing baselines
  2. Ablation on validation signal: Compare VAO (validation gradient) vs. VAO-Train (training gradient) as in Figure 4 to verify the held-out signal is critical
  3. Temperature sweep: Run VAO with $\lambda \in \{0.01, 0.1, 0.5, 1.0\}$ on the minority task to identify the regime where alignment and diversity balance optimally

## Open Questions the Paper Calls Out

- What is the empirical lower bound for the validation set size ($|D_{val}|$) required to ensure stable gradient alignment without degrading target task performance?
- Can the temperature hyperparameter $\lambda$ be adaptively scheduled or learned during training to accommodate the differing sensitivity of minority vs. majority tasks?
- How does VAMO scale in computational efficiency and optimization stability when applied to a significantly larger number of tasks ($K > 50$)?

## Limitations

- Theoretical generalization bounds rely on simplifying assumptions (bounded gradients, convexity) that may not hold in practice
- Method requires a held-out validation set for each target task, which may be challenging in extremely data-scarce scenarios
- Performance claims are based on a single proprietary dataset (AuctionNet), limiting generalizability

## Confidence

- Confidence in main empirical claims: Medium - results from single proprietary dataset with limited public reproducibility
- Confidence in theoretical contributions: Low-Medium - relies on simplifying assumptions in theoretical analysis
- Confidence in ablation conclusions: High - structured experiments with clear comparative results

## Next Checks

1. **Cross-dataset generalization**: Replicate the VAO gains on a publicly available advertising or recommendation dataset to test robustness beyond AuctionNet
2. **Gradient alignment stability**: Measure the variance of validation gradient estimates under different validation set sizes to quantify sensitivity to data scarcity
3. **FiLM modularity test**: Replace FiLM with alternative conditioning mechanisms (e.g., hypernetworks or prompt tuning) to isolate whether the benefit comes from FiLM itself or the act of conditioning