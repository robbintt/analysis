---
ver: rpa2
title: Causality-aligned Prompt Learning via Diffusion-based Counterfactual Generation
arxiv_id: '2507.19882'
source_url: https://arxiv.org/abs/2507.19882
tags:
- counterfactual
- learning
- prompt
- causal
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of achieving causally invariant
  prompt learning in vision-language models. Existing methods struggle to capture
  robust features that generalize effectively across categories due to inadequate
  theoretical foundations.
---

# Causality-aligned Prompt Learning via Diffusion-based Counterfactual Generation

## Quick Facts
- arXiv ID: 2507.19882
- Source URL: https://arxiv.org/abs/2507.19882
- Reference count: 40
- Primary result: DiCap improves accuracy by 17.6% on seen classes and 3.87% on unseen classes compared to CLIP baseline on ImageNet.

## Executive Summary
This paper addresses the challenge of achieving causally invariant prompt learning in vision-language models. Existing methods struggle to capture robust features that generalize effectively across categories due to inadequate theoretical foundations. The proposed DiCap framework uses a diffusion-based counterfactual generation method to iteratively sample gradients from the marginal and conditional distributions of the causal model. This guides the generation of counterfactuals that satisfy the minimal sufficiency criterion, ensuring the identifiability of counterfactual outcomes while imposing strict bounds on estimation errors. A contrastive learning framework then leverages these counterfactuals to refine prompt embeddings aligned with causal features.

## Method Summary
The DiCap framework generates counterfactual images using diffusion models guided by anti-causal predictor gradients, then employs contrastive learning to train prompts that ignore spurious correlations. The method involves three key phases: (1) Abduction - using diffusion inversion to extract exogenous noise variables, (2) Action - generating counterfactuals through gradient-guided denoising where the causal label is changed, and (3) Estimation - using contrastive learning with counterfactuals as "hard negatives" to refine prompt embeddings. The framework trains a Counterfactual Prompt Constructor while freezing the VLM (CLIP) encoders and diffusion model, optimizing a dual loss that combines standard contrastive loss with a counterfactual contrastive loss.

## Key Results
- On ImageNet, DiCap improves accuracy by 17.6% on seen classes and 3.87% on unseen classes compared to CLIP baseline.
- The method demonstrates excellent performance across image classification, image-text retrieval, and visual question answering tasks.
- Particularly strong advantages are observed in unseen categories, demonstrating improved generalization.

## Why This Works (Mechanism)

### Mechanism 1
Diffusion-based inversion isolates exogenous noise variables ($u_x$) which represent the image's identity, allowing for controlled causal intervention. The model uses a forward diffusion process to map a factual image back to noise. By treating this noise as a proxy for exogenous variables, the model preserves non-causal features ($n$) while the "Action" phase replaces the causal factor ($y$) via classifier guidance during denoising.

### Mechanism 2
Gradient guidance from an anti-causal predictor enforces "Minimal Sufficiency" in counterfactuals. Instead of random editing, the denoising process is guided by $\nabla_x \log p_\phi(y_{cf}|x)$ (gradients from a classifier predicting the counterfactual label). This forces the generated image to change just enough to be classified as $y_{cf}$ while minimizing the distance to the original image.

### Mechanism 3
Contrastive learning with "hard" counterfactuals strips away spurious correlations from prompt embeddings. The prompt is trained to maximize similarity with the factual image while minimizing it with the generated counterfactual. Since the counterfactual shares the same non-causal background ($n$) but differs in causal label ($y$), the prompt must ignore $n$ to distinguish them.

## Foundational Learning

- **Concept:** Structural Causal Models (SCM) & Counterfactuals
  - **Why needed here:** The paper frames image generation as $x = f(y, n, u_x)$. Understanding the difference between *observation*, *intervention* ($do(y_{cf})$), and *counterfactual* (seeing $x$, imagining $x_{cf}$) is required to interpret Section 3.2-3.4.
  - **Quick check question:** Can you explain why counterfactual reasoning requires "Abduction" (retrieving $u_x$) while simple intervention does not?

- **Concept:** Denoising Diffusion Implicit Models (DDIM)
  - **Why needed here:** The method relies on inverting images to latent noise and deterministic sampling. Without understanding the ODE formulation of DDIM, the "Abduction" step appears as magic.
  - **Quick check question:** How does the DDIM inversion differ from standard DDPM noise addition, and why is inversion necessary for counterfactuals?

- **Concept:** Contrastive Learning (InfoNCE)
  - **Why needed here:** The objective function relies on distance metrics in embedding space. Understanding "hard negatives" is crucial for Section 3.5.
  - **Quick check question:** Why is a "hard negative" (semantically similar but distinct) more effective for representation learning than a random negative?

## Architecture Onboarding

- **Component map:** Input Image -> Diffusion Inversion (get noise/noisy latents) -> Anti-causal Gradient (compute $\nabla_x p(y_{cf}|x)$) -> Guided Denoising (generate $x_{cf}$) -> Contrastive Loss (Update $w$).
- **Critical path:** Input Image $\to$ **Diffusion Inversion** (get noise/noisy latents) $\to$ **Anti-causal Gradient** (compute $\nabla_x \log p(y_{cf}|x)$) $\to$ **Guided Denoising** (generate $x_{cf}$) $\to$ **Contrastive Loss** (Update $w$).
- **Design tradeoffs:** Scale $s$ (Eq. 2) controls counterfactual strength. Paper finds $5 < s < 20$ optimal. "Similarity" sampling outperforms "Random". Shorter prompts (length 4) generalize better to unseen classes than longer ones.
- **Failure signatures:** High CLD Score indicates counterfactuals are either too similar to factuals or distorted. If performance on "Unseen" classes drops below CLIP baseline, the model has likely overfitted to spurious correlations in the training set.
- **First 3 experiments:** 1) Hyperparameter Sweep ($s$): Run ablation on the scale parameter $s$ on a validation set to find the "sweet spot" where CLD is minimized. 2) Visual QC of Counterfactuals: Generate $x_{cf}$ for a batch of images. Verify that the object changes but the background remains stable. 3) Baseline Comparison (0-shot vs. DiCap): Reproduce Table 1 on a single dataset to verify the 3-4% lift on unseen classes before scaling up.

## Open Questions the Paper Calls Out
1. How can the DiCap framework be effectively extended to multi-modal prompt learning where both visual and textual branches are fine-tuned simultaneously?
2. Does the reliance on a pre-trained anti-causal predictor for gradient guidance propagate existing spurious correlations into the counterfactual generation process?
3. Do the theoretical guarantees of identifiability and error bounds hold in practical high-dimensional settings where the invertibility of the structural function cannot be perfectly enforced?
4. Can the sampling strategy for counterfactual labels be automated or adapted for fine-grained classification where the "second closest" class may share causal features with the source class?

## Limitations
- The effectiveness of diffusion inversion for extracting exogenous noise variables is predicated on the forward process fully destroying semantic information - a claim not empirically verified.
- The assumption that anti-causal predictor gradients align with true causal features rather than spurious correlations needs more rigorous testing.
- The contrastive learning framework assumes counterfactuals generated through this process are "hard negatives" - this quality depends heavily on the success of the underlying counterfactual generation.

## Confidence
- **High confidence:** The overall experimental results showing performance improvements across multiple benchmarks, and the basic viability of using diffusion-based counterfactuals in a contrastive learning framework.
- **Medium confidence:** The theoretical framing using causal inference concepts (SCM, minimal sufficiency criterion), as the connection between diffusion inversion and SCM variable extraction is plausible but not rigorously proven.
- **Low confidence:** The specific claim that the proposed method achieves "causally invariant" prompt learning - this requires more extensive testing on datasets specifically designed to test causal invariance versus spurious correlation capture.

## Next Checks
1. Systematically evaluate counterfactual images for preservation of non-causal features (backgrounds, textures) while changing causal features (objects) across multiple datasets with varying levels of confounding.
2. Conduct a more extensive ablation study on the guidance scale parameter $s$ across different dataset types to identify failure modes and confirm the narrow valid range claimed.
3. Design experiments specifically targeting causal invariance by testing on datasets where spurious correlations are known and controlled (e.g., colored MNIST variants), comparing against methods that explicitly target causal feature extraction.