---
ver: rpa2
title: 'Mitigating the Curse of Detail: Scaling Arguments for Feature Learning and
  Sample Complexity'
arxiv_id: '2512.04165'
source_url: https://arxiv.org/abs/2512.04165
tags:
- learning
- feature
- layer
- kernel
- scaling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of predicting when deep neural
  networks exhibit feature learning and determining the sample complexity required
  for learning in the "rich regime" where the network width is finite and feature
  learning is possible. The authors develop a theoretical framework that combines
  Large Deviation Theory (LDT) with variational approximations to predict the data
  and width scales at which various feature learning patterns emerge.
---

# Mitigating the Curse of Detail: Scaling Arguments for Feature Learning and Sample Complexity

## Quick Facts
- **arXiv ID**: 2512.04165
- **Source URL**: https://arxiv.org/abs/2512.04165
- **Reference count**: 40
- **Primary result**: Successfully reproduces known results for two-layer networks and predicts novel scaling laws for complex architectures, including P* ∝ d for three-layer non-linear networks and P* ∝ √Ld³ for softmax attention heads

## Executive Summary
This paper addresses the fundamental question of predicting when deep neural networks exhibit feature learning and determining the sample complexity required for learning in the "rich regime" where network width is finite. The authors develop a theoretical framework that combines Large Deviation Theory with variational approximations to predict data and width scales at which various feature learning patterns emerge. The approach provides a tractable alternative to exact theories of feature learning while maintaining first-principles predictive power, successfully extending theoretical understanding beyond analytically tractable toy models.

## Method Summary
The method combines posterior bounds via Large Deviation Theory with variational approximations over discrete feature learning patterns. Starting from a Bayesian neural network framework, the authors derive sample complexity bounds based on prior alignment probabilities, then approximate these intractable bounds using a variational approach that compares different feature learning patterns (GP, GFL, Specialization). Layer-wise propagation rules determine how patterns affect downstream kernel spectra, enabling predictions for complex architectures.

## Key Results
1. Successfully reproduces known results for two-layer networks, including sample complexity scaling exponents (e.g., P* ∝ d for two-layer FCNs on cubic targets)
2. Novel predictions for complex architectures, including three-layer non-linear networks where they predict P* ∝ d scaling and pattern transitions
3. Predictions for softmax attention heads with P* ∝ √Ld³ scaling, validated through extensive experiments

## Why This Works (Mechanism)

### Mechanism 1: Posterior-Alignment Bound via Large Deviation Theory
- **Claim:** Sample complexity scales with the negative log-probability of achieving strong alignment in the prior.
- **Mechanism:** The bound `log(Pr_π[A_f ≥ α]) < Pk/(2κ) + log(Pr_p0[A_f ≥ α])` connects posterior success to prior rarity. Strong alignment (A_f ≈ 1) is a rare event under the prior; the data term must overcome this improbability. Rearranging gives `P* ∝ −log Pr_p0[A_f ≥ α]`.
- **Core assumption:** Networks trained via SGLD approximate Bayesian posteriors; the bound tightens when overfitting is small (k/κ ~ O(1)).
- **Evidence anchors:**
  - [abstract]: "upper-bounding the posterior probability of strong alignment...then estimating this bound"
  - [section 3, Eq. 4-5]: Explicit derivation of the bound connecting P* to prior alignment probability
  - [corpus]: Weak direct evidence—neighbor papers discuss scaling laws but not this specific LDT-bound mechanism
- **Break condition:** When κ → 0 (vanishing ridge) or under mean-field scaling, overfitting trivializes the bound, making it vacuous.

### Mechanism 2: Variational Approximation over Feature Learning Patterns
- **Claim:** The intractable LDT bound can be approximated by minimizing a variational energy Ẽ_q over a discrete set of feature learning patterns.
- **Mechanism:** Rather than computing `E(α) = −log p_Af(α)` exactly, the Feynman-Bogoliubov inequality yields `E(α) ≈ min_q Ẽ_q(α)` where Ẽ_q decomposes as layer-wise costs Δ_{l,i} plus an alignment term a_y. Three candidate patterns compete: GP (lazy), GFL (covariance amplification), Specialization (mean shift).
- **Core assumption:** Pre-activation kernels fluctuate weakly; can replace fluctuating kernels with expectations under q. Log-partition terms are subleading relative to Ẽ_q.
- **Evidence anchors:**
  - [abstract]: "variational approximation over simplified 'feature learning patterns'"
  - [section 4, Eq. 10]: Variational energy formula with Δ_{l,i} and a_y terms
  - [section 5.1]: Explicit definitions of GP, GFL, and Specialization patterns
  - [corpus]: Moderate support—neighbor papers on feature learning regimes share conceptual overlap
- **Break condition:** When multiple features interact (superposition) or kernel fluctuations are not weak, the decoupled Gaussian ansatz fails.

### Mechanism 3: Layer-wise Feature Propagation Rules
- **Claim:** Features propagate through layers following predictable scaling rules that determine downstream kernel spectra.
- **Mechanism:** Claim (i): M specializing neurons create a spectral spike with RKHS norm scaling as O(N_l/M). Claim (ii): An eigenvalue amplified by factor D in layer l creates O(D^m) amplification for the m-th power of that feature in layer l+1. Claim (iii): Lazy layers preserve relative spectral scales.
- **Core assumption:** High-dimensional settings where bulk eigenvalues decay rapidly; specialized features occupy a negligible subspace.
- **Evidence anchors:**
  - [section 5.2]: Three explicit propagation claims with Sherman-Morrison justification
  - [section 6.1]: Application to three-layer networks showing how patterns compete
  - [corpus]: Weak—neighbor papers touch on compositionality but not these specific propagation rules
- **Break condition:** When width is small (N_l ~ rank of feature space) or spectra are flat, the spike approximation breaks down.

## Foundational Learning

- **Concept: Bayesian Neural Networks and Posteriors**
  - Why needed here: The entire framework assumes network outputs after SGLD training follow a Bayesian posterior `π(f|y,P)` with Gaussian weight priors.
  - Quick check question: Can you explain why `p_0(f) = ∫ dΘ p(Θ) δ[f − f_Θ]` defines a prior over functions?

- **Concept: Large Deviation Theory (Chernoff Bound)**
  - Why needed here: The LDT bound `E(α) = −log inf_{t>0} e^{−tα} E[e^{tA_f}]` converts tail probabilities into energy minimization.
  - Quick check question: Why does the saddle-point approximation convert Pr[A_f ≥ α] to a density p_Af(α)?

- **Concept: RKHS Norms and Kernel Operators**
  - Why needed here: The alignment term `a_y = ⟨y, K_{L−1}, y⟩^{-1}` measures target difficulty; propagation rules relate spectral properties to learnability.
  - Quick check question: If a feature Φ has RKHS norm R_K, what does R_K → ∞ imply about learning difficulty?

## Architecture Onboarding

- **Component map:** Posterior bound (Sec 3) → LDT energy (Sec 3, App B) → Variational approximation (Sec 4) → Feature patterns (Sec 5.1) → Propagation rules (Sec 5.2) → Concrete predictions (Sec 6)

- **Critical path:** Start with a target y and architecture → enumerate candidate patterns → compute Ẽ_q for each using propagation rules → select minimum → extract P* scaling. Table 1 shows this for three-layer networks.

- **Design tradeoffs:**
  - GP pattern: Zero Δ cost but poor a_y (∝ d^m for degree-m targets) → P* ∝ d^m
  - Specialization: Linear Δ cost (Md) but improved a_y (∝ N/M) → P* ∝ √(Nd)
  - GFL: Sublinear scaling but requires D > 1 amplification → P* ∝ (Nd)^{m/(m+1)}
  - Exact LDT (App B) is more accurate but O(d^N) complexity vs. O(1) for heuristics

- **Failure signatures:**
  - Vanishing ridge (κ → 0): Bound becomes vacuous; need effective ridge treatment
  - Multi-feature superposition: Decoupled patterns don't capture interactions
  - Early-time dynamics: Bayesian equilibrium may not reflect SGD at finite time

- **First 3 experiments:**
  1. **Two-layer erf network on He_3 target** (Sec E.1.1): Verify P* ∝ d scaling and count specializing neurons (∝ √(N/d)). Compare Fig 2 predictions vs. experiment.
  2. **Three-layer network varying N_1** (Sec 6.1): Confirm pattern transition from Sp.-Mag. to GP-Sp. as N_1 increases. Track first/second layer alignment with linear vs. cubic features (Fig 3c).
  3. **Softmax attention on cubic target** (Sec E.3): Test P* ∝ √(L d^3) scaling. Plot alignment vs. P/(√(L d^3)) across context lengths (Fig 3b).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the heuristic feature propagation claims regarding spectral spikes and amplified features be rigorously proven for general architectures?
- **Basis in paper:** [explicit] Section 5.2 states the claims are "heuristic principles or rationalizations" and that "proving them in general or augmenting for different architectures is left for future work."
- **Why unresolved:** The authors currently rely on empirical justification and approximations (e.g., Sherman-Morrison, Taylor expansions) rather than formal derivations to predict how kernels evolve across layers.
- **What evidence would resolve it:** A formal mathematical proof showing that neuron specialization inevitably creates spectral spikes with the predicted scaling in deep non-linear networks, or a counter-example limiting the scope of the claims.

### Open Question 2
- **Question:** Can the variational framework be extended to model the dynamics of learning, specifically predicting feature emergence in the early stages of training?
- **Basis in paper:** [explicit] Section 7 lists "extend[ing] our heuristic to dynamics of learning" as an avenue for improvement, noting that "Bayesian convergence times can be slow."
- **Why unresolved:** The current theory acts as an equilibrium analysis (proxy for fully trained networks) and does not capture the trajectory of gradient descent or the time-scales of "grokking" transitions.
- **What evidence would resolve it:** Deriving time-dependent scaling laws for the energy $\tilde{E}$ that align with empirical training curves at early time steps, rather than just at convergence.

### Open Question 3
- **Question:** Does the variational approach hold for multi-feature interaction effects, particularly in the context of superposition?
- **Basis in paper:** [explicit] Section 7 identifies "addressing multi-feature interaction effects as those appearing in the context of superposition" as a current limitation.
- **Why unresolved:** The proposed method compares distinct feature learning patterns (like specialization) individually; it is unclear if the variational bound remains tight when multiple features are learned simultaneously in a distributed manner.
- **What evidence would resolve it:** Applying the heuristic to settings with polysemantic neurons and verifying if the sample complexity scaling laws ($P^*$) still match the theoretical predictions.

## Limitations
- The framework assumes SGLD-trained networks approximate Bayesian posteriors at equilibrium, which may not hold for finite-time SGD dynamics
- The variational approximation breaks down for multi-feature interaction effects and superposition scenarios
- Propagation rule assumptions require high-dimensional settings with rapid bulk eigenvalue decay

## Confidence
- **High confidence**: Two-layer network predictions (P* ∝ d for cubic targets) are well-validated experimentally
- **Medium confidence**: Three-layer network predictions show good experimental agreement but rely on more approximations
- **Low confidence**: Attention head predictions (P* ∝ √Ld³) are more speculative with limited experimental validation

## Next Checks
1. **Early-time dynamics validation**: Compare Bayesian equilibrium predictions with finite-time SGD training curves for two-layer networks on cubic targets
2. **Multi-feature interaction test**: Design experiments with superposition targets to test whether the decoupled pattern approximation correctly predicts competition between features
3. **Width scaling validation**: Systematically vary hidden widths in three-layer networks to map out the complete phase diagram of pattern transitions