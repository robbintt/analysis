---
ver: rpa2
title: Terminal Velocity Matching
arxiv_id: '2511.19797'
source_url: https://arxiv.org/abs/2511.19797
tags:
- training
- arxiv
- velocity
- matching
- flow
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Terminal Velocity Matching (TVM) addresses the challenge of training
  high-fidelity one- and few-step generative models from scratch. The method generalizes
  flow matching by matching terminal velocities at the end of trajectories rather
  than initial velocities, providing theoretical guarantees through an upper bound
  on the 2-Wasserstein distance.
---

# Terminal Velocity Matching

## Quick Facts
- arXiv ID: 2511.19797
- Source URL: https://arxiv.org/abs/2511.19797
- Reference count: 40
- Terminal Velocity Matching (TVM) achieves state-of-the-art 3.29 FID with single-step generation and 1.99 FID with 4 function evaluations on ImageNet-256×256

## Executive Summary
Terminal Velocity Matching (TVM) introduces a novel approach to training high-fidelity one- and few-step generative models from scratch by matching terminal velocities at trajectory endpoints rather than initial velocities. The method provides theoretical guarantees through an upper bound on the 2-Wasserstein distance while introducing minimal architectural changes to Diffusion Transformers to ensure Lipschitz continuity. TVM achieves state-of-the-art performance with 3.29 FID for single-step generation and 1.99 FID with 4 function evaluations on ImageNet-256×256, outperforming previous one-step methods and matching diffusion baselines with just 4 steps.

## Method Summary
TVM generalizes flow matching by modeling the transition between any two diffusion timesteps and regularizing behavior at terminal time rather than initial time. The method parameterizes the displacement as f_θ(x_t, t, s, c, w) = (s-t)·w·F_θ(x_t, t, s, c, w) and computes terminal velocity through Jacobian-Vector Products. To handle Diffusion Transformers' lack of Lipschitz continuity, TVM introduces RMSNorm-based QK-normalization and time embedding normalization. An efficient Flash Attention kernel supporting Jacobian-Vector Product backward passes enables practical implementation. The method naturally interpolates between one-step and multi-step sampling without retraining and requires no training curriculum or loss modifications.

## Key Results
- Achieves 3.29 FID with single-step generation on ImageNet-256×256
- Matches diffusion baselines with 1.99 FID using only 4 function evaluations
- Outperforms previous one-step methods (3.93 FID) by significant margin
- Demonstrates stable training without curriculum or loss modifications

## Why This Works (Mechanism)

### Mechanism 1
Matching terminal velocities at trajectory endpoints upper-bounds distribution divergence when the model is Lipschitz continuous. Rather than regressing instantaneous velocity at time t, TVM enforces that the model's velocity field matches the ground-truth velocity at the arrival point along the trajectory. This provides a theoretical guarantee through an upper bound on the 2-Wasserstein distance via triangle inequality.

### Mechanism 2
RMSNorm-based QK normalization and normalized time-embedding modulation enforce sufficient Lipschitz control for stable training. Standard DiT uses LayerNorm and AdaLN with unbounded scale/shift parameters. TVM replaces LayerNorm with parameter-free RMSNorm (provably Lipschitz with constant ≤ 2/ε) and applies RMSNorm to modulation parameters, achieving partial but sufficient Lipschitz continuity.

### Mechanism 3
A fused Flash Attention kernel with JVP backward pass enables efficient gradient computation through terminal velocity derivatives. The kernel fuses JVP with forward attention, avoids storing N² attention matrices, and supports full backward gradients through JVP, preventing memory explosion during training.

## Foundational Learning

- **Ordinary Differential Equations and Flow Integration**: TVM models generative trajectories as ODEs; understanding flow maps ψ(x_t, t, s) = x_t + ∫_t^s u(x_r, r) dr is essential. Quick check: Can you explain why matching d/ds f at s=t reduces to standard Flow Matching?

- **Lipschitz Continuity and Normalization**: The theoretical guarantees require Lipschitz velocity fields; you must understand why LayerNorm breaks this. Quick check: What is the Lipschitz constant of RMSNorm(x) = x / RMS(x) with ε = 10⁻⁵?

- **Jacobian-Vector Products vs. Vector-Jacobian Products**: TVM computes ∂_s F_θ via JVP for the terminal velocity term; understanding forward-mode AD is critical for kernel design. Quick check: Why does JVP scale better than VJP for computing derivatives w.r.t. a single input dimension?

## Architecture Onboarding

- **Component map**: Input (x_t, t, s, c, w) → time embeddings → DiT blocks (with QK-RMSNorm, normalized AdaLN modulation) → F_θ output → scaled displacement f_θ = (s-t)w F_θ

- **Critical path**: 
  1. Verify RMSNorm is applied to ALL Q/K projections and ALL AdaLN modulation parameters (6 per layer)
  2. Ensure EMA rate γ ≈ 0.99 for target network; eval EMA at 0.9999
  3. Use gap* time sampling: bias t toward 1, s toward 0, with independent s' for FM loss

- **Design tradeoffs**:
  - Scaled parameterization f_θ = (s-t)w F_θ helps at high CFG but may hurt at low CFG
  - Higher CFG (w=2.5) improves 1-NFE but degrades 2-NFE vs. w=2.0 (capacity trade-off)
  - Random CFG training is stable but underperforms constant CFG

- **Failure signatures**:
  - Activation norm spikes → check RMSNorm on modulation parameters
  - OOM during backward → verify custom Flash Attention JVP kernel is used
  - Diverging gradient norm under random CFG → reduce β₂ to 0.95

- **First 3 experiments**:
  1. Train on 2D toy data with Lipschitz modifications disabled; observe instability vs. enabled
  2. Compare vanilla SDPA JVP vs. custom kernel on (H=24, S=8192); measure memory and latency
  3. Compare gap vs. trunc vs. clamp time sampling schemes on ImageNet-256 (200K steps, 1-NFE FID)

## Open Questions the Paper Calls Out

- **Open Question 1**: Can a single model simultaneously achieve optimal performance across multiple NFE settings, or is the trade-off between 1-NFE and multi-NFE quality fundamental? The authors observe that network capacity appears limited in fitting all NFEs well, suggesting a fundamental trade-off.

- **Open Question 2**: How can random CFG sampling during training be improved to avoid performance degradation at certain guidance scales? Random sampling of CFG is not optimal as some CFG scales experience degradation in FID during training.

- **Open Question 3**: Does the theoretical 2-Wasserstein bound remain meaningful under the partial Lipschitz control achieved by the proposed architectural modifications? The semi-Lipschitz control is empirically sufficient but not theoretically complete, creating a gap between theory and practice.

## Limitations

- The custom Flash Attention JVP kernel is central to TVM's efficiency but lacks open-source reference implementation, making memory overhead prohibitive for large models without it
- Lipschitz guarantees depend on architectural constraints that are not enforced end-to-end; LayerNorm violations in residual branches could still cause instability
- Time-sampling bias (gap*) improves single-step FID but may overfit to few-step regimes at the expense of multi-step consistency

## Confidence

- **High**: TVM's theoretical upper bound on 2-Wasserstein distance under Lipschitz conditions (Theorem 1); empirical superiority over single-step baselines (3.29 vs 3.93 FID); architectural modifications' effect on stability (Figure 4)
- **Medium**: Gap* time sampling's contribution to 1-NFE performance; CFG weight scaling's impact on low-NFE vs high-NFE trade-off; JVP kernel speedup claims without public implementation
- **Low**: Generalization of Lipschitz analysis to arbitrary DiT variants; exact conditions under which random CFG training converges; scaling laws for larger models/datasets

## Next Checks

1. Implement and benchmark the custom Flash Attention JVP kernel with backward pass support on DiT-XL/2 with 16K tokens; measure memory vs. vanilla SDPA and validate correctness of combined gradients
2. Train a controlled ablation: DiT-XL/2 with standard LayerNorm vs. RMSNorm-only (all QK and AdaLN layers) on ImageNet-256; compare activation norms and FID trajectories over 200K steps
3. Evaluate TVM with clamped vs. gap* time sampling on ImageNet-512×512; measure 1-NFE FID, 4-NFE FID, and multi-step consistency (x0→xt→x0 cycle)