---
ver: rpa2
title: Alibaba International E-commerce Product Search Competition DILAB Team Technical
  Report
arxiv_id: '2510.18499'
source_url: https://arxiv.org/abs/2510.18499
tags:
- data
- multilingual
- search
- tagging
- e-commerce
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents the DILAB team\u2019s multilingual e-commerce\
  \ search system developed for the Alibaba International E-commerce Product Search\
  \ Competition, achieving 5th place with an overall F1 score of 0.8819. The approach\
  \ addresses the challenge of cross-lingual query\u2013category and query\u2013item\
  \ relevance prediction by combining lightweight preprocessing, task-specific tagging,\
  \ and large language model fine-tuning."
---

# Alibaba International E-commerce Product Search Competition DILAB Team Technical Report

## Quick Facts
- arXiv ID: 2510.18499
- Source URL: https://arxiv.org/abs/2510.18499
- Reference count: 14
- Primary result: Achieved 5th place with overall F1 score of 0.8819 in Alibaba International E-commerce Product Search Competition

## Executive Summary
This paper presents the DILAB team's multilingual e-commerce search system developed for the Alibaba International E-commerce Product Search Competition. The approach addresses cross-lingual query–category and query–item relevance prediction by combining lightweight preprocessing, task-specific tagging, and large language model fine-tuning. Key strategies include Quality Refinement to clean noisy data, language tagging, hierarchical category tagging for QC, semantic item tagging with description generation for QI, and the use of In-Context Learning and Chain-of-Thought reasoning to enhance multilingual reasoning. Experimental results show significant improvements over baselines, with task-specific components yielding consistent gains across both QC and QI tasks.

## Method Summary
The system employs a four-stage pipeline: (1) Quality Refinement via eCeLLM-M LoRA fine-tuning and self-evaluation filtering using TF-IDF/Jaccard similarity to remove mislabeled data, (2) Structured tagging including Language Tagging, Hierarchical Category Tagging for QC, and Semantic Item Tagging plus Description Generation for QI, (3) In-Context Learning and Chain-of-Thought reasoning prompt construction, and (4) Qwen2.5-14B-Instruct LoRA fine-tuning for 2 epochs. The approach handles 13 languages with 6-7 in training and 13 in test, achieving strong cross-lingual generalization to previously unseen languages.

## Key Results
- Achieved 5th place in competition with overall F1 score of 0.8819 (QC: 0.8861, QI: 0.8778)
- Quality Refinement alone yields 0.8718 (QC) and 0.8701 (QI), establishing foundational gains
- Full strategy (QR+LT+HCT+SIT+DG) achieves highest scores: 0.8778 for QI
- Robust performance on unseen languages (ar, de, it, pl, id, vn) in test set

## Why This Works (Mechanism)

### Mechanism 1: Quality Refinement via Self-Evaluation Filtering
- Claim: Removing mislabeled or noisy training samples improves downstream model performance more than using all available data.
- Mechanism: A two-stage process where (1) an LLM fine-tuned on raw data re-predicts training labels, and (2) samples showing disagreement between original labels, model predictions, and TF-IDF/Jaccard similarity scores are flagged and removed.
- Core assumption: Disagreement between lexical similarity signals, original labels, and model predictions correlates with annotation noise rather than legitimate hard cases.
- Evidence anchors: [Section 2.1] "This refinement pipeline ensures that mislabeled or noisy data are filtered out before downstream training." [Table 4a/b] QR alone yields 0.8718 (QC) and 0.8701 (QI); ablation shows QR provides foundational gains across both tasks.

### Mechanism 2: Structured Tagging Provides Explicit Reasoning Scaffolds
- Claim: Injecting explicit linguistic and structural tags into prompts improves the model's ability to discriminate relevant from irrelevant pairs.
- Mechanism: Four complementary tagging strategies—Language Tagging (LT), Hierarchical Category Tagging (HCT), Semantic Item Tagging (SIT), and Description Generation (DG)—enrich input representations with explicit cues that reduce ambiguity.
- Core assumption: The model can leverage these structured tags during fine-tuning to learn more precise relevance boundaries, rather than treating them as noise.
- Evidence anchors: [Section 2.2] "Prior studies in e-commerce have demonstrated that explicit tagging of hierarchical categories and item attributes improves fine-grained relevance modeling." [Table 4a] HCT provides the largest single-task gain in QC (+0.0118 over LT alone).

### Mechanism 3: In-Context Learning and Chain-of-Thought Enhance Cross-Lingual Generalization
- Claim: Providing multilingual ICL examples and explicit CoT reasoning during fine-tuning improves generalization to unseen languages.
- Mechanism: ICL samples positive and negative examples across languages; CoT provides step-by-step reasoning cues that define relevance criteria explicitly in the prompt.
- Core assumption: The model transfers reasoning patterns learned from high-resource languages (en, es, fr) to low-resource or unseen languages (ar, de, it, pl, id, vn in test only).
- Evidence anchors: [Section 2.3] "ICL provided multilingual positive and negative examples to guide contextual understanding, while CoT offered explicit step-by-step reasoning cues." [Section 4] "These approaches enabled the model to remain robust when evaluated on test datasets containing previously unseen languages."

## Foundational Learning

- **LoRA (Low-Rank Adaptation)**
  - Why needed here: The paper fine-tunes a 14B-parameter model under a 15B constraint; full fine-tuning would be prohibitively expensive and may exceed parameter budget if storing full copies.
  - Quick check question: Can you explain why LoRA adds trainable low-rank matrices instead of updating all weights, and what rank setting balances capacity vs. efficiency?

- **TF-IDF and Jaccard Similarity**
  - Why needed here: These lexical measures serve as guards in Quality Refinement, providing language-agnostic similarity signals to cross-check LLM predictions.
  - Quick check question: For a query "red dress" and item "crimson evening gown," would you expect TF-IDF cosine similarity to be high or low? What does this imply for noise detection?

- **Cross-Lingual Transfer in LLMs**
  - Why needed here: The system must predict relevance for languages absent from training data; understanding how multilingual LLMs generalize is essential for debugging zero-shot failures.
  - Quick check question: If a model trained on English and Spanish queries fails on Arabic, what diagnostic steps would isolate whether the issue is tokenization, representation, or prompt design?

## Architecture Onboarding

- **Component map:**
  Raw Data → [Quality Refinement] → Refined Data → [Tagging: LT + HCT/SIT + DG] → [ICL/CoT Prompt Construction] → [LoRA Fine-Tuning on Qwen2.5-14B] → [Inference → submit_QC.txt / submit_QI.txt]

- **Critical path:** Quality Refinement → Tagging → Prompt Construction → Fine-tuning. Errors in QR propagate through all downstream stages; tag quality directly affects prompt effectiveness.

- **Design tradeoffs:**
  - QR aggressiveness: Higher filtering reduces noise but may discard legitimate hard negatives. The paper uses language-specific thresholds tuned on validation sets.
  - Tagging complexity: SIT+DG enriches QI inputs but adds inference overhead and dependency on auxiliary LLM (Meta-Llama-3-8B-Instruct for DG).
  - ICL example count: More examples improve guidance but increase prompt length; the paper does not specify exact counts, requiring empirical tuning.

- **Failure signatures:**
  - Low F1 on specific languages: Check whether language appears only in test (zero-shot) or if QR removed too many samples for that language.
  - High false positive rate: Inspect HCT/SIT tag quality; malformed tags can confuse rather than clarify.
  - Inconsistent predictions across runs: LoRA initialization or ICL example sampling may introduce variance; set seeds and fix ICL examples for reproducibility.

- **First 3 experiments:**
  1. **Reproduce baseline:** Run `bash script/train.sh 2 Qwen2.5-14B QC` and `bash script/predict.sh Qwen2.5-14B QC` without QR or tagging to confirm baseline F1 (~0.8684 for QC per Table 3).
  2. **Ablate QR:** Run full pipeline with QR disabled; compare F1 delta against Table 4 to validate reported +0.0036 (QC) and +0.0077 (QI) gains from full strategy.
  3. **Language-level analysis:** Stratify predictions by language code; identify which unseen languages (ar, de, it, pl, id, vn) show lowest per-language F1 and inspect failure patterns in corresponding query-item pairs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the multi-stage pipeline (QR, tagging, ICL/CoT, fine-tuning) be compressed or streamlined to reduce computational cost while preserving the observed performance gains?
- Basis in paper: [explicit] "future work will focus on further streamlining the fine-tuning process to reduce computational cost and exploring lightweight adaptation techniques to enable scalable real-world deployment."
- Why unresolved: The current system achieves strong results but requires substantial compute; the paper does not quantify the inference latency or training cost overhead introduced by each component (QR, tagging, ICL/CoT).
- What evidence would resolve it: Ablation studies that measure time and resource consumption per component, plus Pareto frontier analysis trading F1 against latency/cost.

### Open Question 2
- Question: To what extent do the task-specific tagging strategies (HCT, SIT, DG) transfer to different LLM architectures beyond Qwen2.5-14B-Instruct?
- Basis in paper: [inferred] The paper experiments with multiple backbone models (Qwen2.5-14B, Meta-LLaMA-3-8B, eCeLLM) but reports final results only with Qwen2.5-14B after all strategies are applied, leaving cross-architecture transferability unquantified.
- Why unresolved: Without architecture-level ablations, it is unclear whether the gains are specific to Qwen or broadly applicable.
- What evidence would resolve it: Full ablation tables (with and without HCT/SIT/DG) for Meta-LLaMA-3-8B and eCeLLM on the same test sets.

### Open Question 3
- Question: Does the self-evaluation Quality Refinement process introduce systematic bias against semantically correct but lexically dissimilar query–item pairs, particularly for morphologically rich or low-resource languages?
- Basis in paper: [inferred] QR combines model predictions with TF-IDF/Jaccard similarity and language-specific thresholds; however, lexical similarity can penalize paraphrases, synonyms, or languages with high morphological variability.
- Why unresolved: The paper reports overall F1 improvements but does not analyze per-language or per-error-type breakdown after QR to detect potential over-pruning.
- What evidence would resolve it: Fine-grained analysis of false negatives removed by QR across languages, with human annotation of semantic correctness on a sampled subset.

### Open Question 4
- Question: How robust is the system to domain shift beyond the competition datasets, such as new product categories or distributional drift in real-world search traffic?
- Basis in paper: [inferred] The paper claims robustness to "unseen languages and domains," but evaluation is limited to the competition test sets, which may not capture temporal drift or category expansion in production.
- Why unresolved: No out-of-distribution experiments on external e-commerce corpora or longitudinal data are presented.
- What evidence would resolve it: Evaluation on a held-out domain or temporal slice (e.g., new product categories released after the training period) with F1 and calibration metrics.

## Limitations

- **Hyperparameter transparency**: Critical implementation details remain unspecified including LoRA rank/alpha parameters, exact training hyperparameters (batch size, learning rate, warmup steps), and language-specific QR threshold values.
- **Cross-lingual generalization guarantees**: While robustness to unseen languages is asserted, no per-language F1 breakdown or ablation of ICL/CoT contributions is provided to quantify zero-shot effectiveness.
- **Computational cost**: The four-stage pipeline achieves strong results but requires substantial compute resources; the paper does not quantify inference latency or training cost overhead introduced by each component.

## Confidence

- **High confidence** in Quality Refinement mechanism: QR consistently improves both QC and QI tasks across ablation experiments (Table 4), with independent work confirming label noise as a critical barrier in multilingual e-commerce search.
- **Medium confidence** in tagging effectiveness: HCT shows largest single gain in QC (+0.0118), and full tagging combination achieves highest QI score (0.8778), but direct comparison with other competition teams' approaches is limited.
- **Low confidence** in ICL/CoT cross-lingual generalization claims: While the paper asserts robustness on unseen languages (ar, de, it, pl, id, vn), no per-language F1 breakdown or ablation of ICL/CoT contributions is provided. Related literature on multilingual e-commerce relevance does not isolate these components' effectiveness.

## Next Checks

1. **Reproduce baseline without QR or tagging**: Run the pipeline with only raw data and standard fine-tuning to confirm baseline F1 scores (~0.8684 for QC per Table 3), establishing the foundational performance before systematic improvements.

2. **Stratify predictions by language**: Generate per-language F1 scores for all 13 competition languages, particularly focusing on the 6 unseen languages (ar, de, it, pl, id, vn). This will validate whether reported robustness to zero-shot languages holds under scrutiny.

3. **Ablate ICL and CoT components**: Create experimental variants that disable ICL examples and CoT reasoning separately, then compare multilingual generalization performance against the full pipeline to isolate these components' contributions to cross-lingual transfer.