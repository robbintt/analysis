---
ver: rpa2
title: Hierarchical Imitation Learning of Team Behavior from Heterogeneous Demonstrations
arxiv_id: '2502.17618'
source_url: https://arxiv.org/abs/2502.17618
tags:
- learning
- agent
- team
- behavior
- imitation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DTIL addresses the challenge of learning multimodal team behaviors
  from heterogeneous multi-agent demonstrations, particularly in partially observable
  environments. The core method employs a hierarchical policy structure where each
  agent has high-level subtask selection and low-level action policies, trained using
  a factored distribution-matching approach.
---

# Hierarchical Imitation Learning of Team Behavior from Heterogeneous Demonstrations

## Quick Facts
- **arXiv ID**: 2502.17618
- **Source URL**: https://arxiv.org/abs/2502.17618
- **Reference count**: 40
- **Key outcome**: DTIL outperforms baselines in learning multimodal team behaviors from heterogeneous multi-agent demonstrations

## Executive Summary
DTIL (Diverse Team Imitation Learning) addresses the challenge of learning multimodal team behaviors from heterogeneous multi-agent demonstrations in partially observable environments. The method extends single-agent hierarchical imitation learning to multi-agent settings using a factored distribution-matching approach with high-level subtask selection and low-level action policies for each agent. Experiments across six domains demonstrate that DTIL achieves expert-level rewards and accurately models multimodal behavior with 75-78% subtask inference accuracy, outperforming baselines like MA-GAIL and MA-OptionGAIL.

## Method Summary
DTIL employs a hierarchical policy structure where each agent maintains separate high-level subtask selection and low-level action policies. The method uses a factored distribution-matching approach that decomposes the joint policy into individual agent policies, allowing for more stable training and better capture of hierarchical decision-making. The architecture enables learning from heterogeneous demonstrations by inferring subtask labels through a semi-supervised approach, showing improved performance even with only 20% labeled subtasks. This hierarchical structure is theoretically grounded as an extension of single-agent hierarchical imitation learning to multi-agent settings.

## Key Results
- DTIL outperforms MA-GAIL and MA-OptionGAIL baselines in task performance, achieving expert-level rewards
- DTIL achieves 75-78% subtask inference accuracy, demonstrating superior modeling of multimodal behaviors
- The method shows improved performance with semi-supervision, maintaining effectiveness with just 20% labeled subtasks

## Why This Works (Mechanism)
DTIL's hierarchical structure allows it to capture the multi-level decision-making process inherent in team behaviors. The high-level subtask selection policy determines which sub-task to execute, while the low-level action policy handles specific actions within that sub-task. This decomposition enables the method to learn complex, multimodal behaviors from heterogeneous demonstrations by identifying common sub-task patterns across different demonstrations. The factored distribution-matching approach ensures that the learned policies remain consistent with the demonstration data while allowing for individual agent specialization.

## Foundational Learning
- **Hierarchical Imitation Learning**: Required for capturing multi-level decision-making in team behaviors; quick check: verify single-agent extensions work before multi-agent scaling
- **Distribution Matching**: Essential for aligning learned policies with demonstration distributions; quick check: monitor KL divergence between learned and expert distributions
- **Factored Policy Representation**: Needed to decompose joint policies into individual agent policies for scalability; quick check: verify policy decomposition maintains joint behavior fidelity
- **Semi-supervised Learning**: Critical for handling limited sub-task annotations; quick check: test performance with varying annotation percentages
- **Partially Observable MDPs**: Necessary for modeling real-world scenarios where full state information is unavailable; quick check: evaluate performance under different observation noise levels
- **Multi-agent Coordination**: Fundamental for learning team behaviors from heterogeneous demonstrations; quick check: measure inter-agent coordination metrics during execution

## Architecture Onboarding

**Component Map**: Expert Demonstrations -> Subtask Labeler -> High-Level Subtask Selection -> Low-Level Action Policies -> Joint Policy

**Critical Path**: Subtask Labeler → High-Level Selection → Low-Level Actions → Joint Policy Execution

**Design Tradeoffs**: The hierarchical structure adds complexity but enables better modeling of multimodal behaviors; factored representation improves scalability but requires careful coordination between agent policies

**Failure Signatures**: Poor subtask inference accuracy indicates issues with the labeler; degraded joint performance suggests coordination problems between agent policies; reward gaps from expert performance indicate distribution matching failures

**First Experiments**:
1. Evaluate single-agent hierarchical IL performance on benchmark tasks before extending to multi-agent
2. Test subtask inference accuracy with varying levels of supervision (0%, 10%, 20%, 50%, 100%)
3. Compare DTIL performance against non-hierarchical baselines on simple multi-agent coordination tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Requires sub-task annotations during training, though shows some robustness to semi-supervised settings with 20% labeled data
- Inherits general limitations of imitation learning approaches, including potential distribution shift issues
- Hierarchical structure adds complexity and may face scalability challenges with larger team sizes

## Confidence

**High confidence**: Experimental results showing DTIL outperforming baselines (MA-GAIL, MA-OptionGAIL) in both reward achievement and subtask inference accuracy

**Medium confidence**: Claims about DTIL's superior modeling of multimodal behaviors supported by subtask inference accuracy (75-78%)

**Medium confidence**: Theoretical extension of single-agent hierarchical IL to multi-agent settings is sound but practical implications need further validation

## Next Checks

1. Evaluate DTIL's performance and stability with team sizes beyond those tested (3-4 agents) to assess scalability

2. Conduct experiments with varying levels of sub-task annotation availability (0%, 10%, 50%, 100%) to quantify trade-offs between supervision and performance

3. Test DTIL in environments with significant distribution shift from demonstration data to evaluate robustness and identify failure modes