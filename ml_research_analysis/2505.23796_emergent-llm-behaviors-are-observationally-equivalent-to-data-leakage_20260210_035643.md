---
ver: rpa2
title: Emergent LLM behaviors are observationally equivalent to data leakage
arxiv_id: '2505.23796'
source_url: https://arxiv.org/abs/2505.23796
tags:
- game
- they
- data
- player
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper argues that emergent conventions observed in LLM-based\
  \ coordination games are not spontaneous but rather reflect data leakage\u2014models\
  \ reproducing knowledge from their training data. The authors tested multiple LLMs\
  \ with a coordination game prompt and found that most models correctly identified\
  \ the game structure, optimal post-success strategies, and likely global convergence\
  \ outcomes."
---

# Emergent LLM behaviors are observationally equivalent to data leakage

## Quick Facts
- arXiv ID: 2505.23796
- Source URL: https://arxiv.org/abs/2505.23796
- Authors: Christopher Barrie; Petter Törnberg
- Reference count: 9
- Primary result: Emergent conventions in LLM coordination games are data leakage from training, not genuine emergence

## Executive Summary
This paper argues that emergent conventions observed in LLM-based coordination games are not spontaneous but rather reflect data leakage—models reproducing knowledge from their training data. The authors tested multiple LLMs with a coordination game prompt and found that most models correctly identified the game structure, optimal post-success strategies, and likely global convergence outcomes. They demonstrate that LLMs can map the game scenario to existing research on coordination and convention formation, suggesting observed behaviors are observational equivalents of memorization rather than genuine emergence. The study concludes that data leakage poses a fundamental challenge for generative ABM research, as off-the-shelf LLMs inevitably reflect their training data.

## Method Summary
The study tested 14 different LLMs (including llama3.2:3b, llama3:70b-instruct, deepseek-r1:8b, mistral:7b, gemma3:4b, and various Claude and GPT models) by prompting them with a coordination game setup featuring specific payoffs (+100 for matches, -50 for mismatches) and asking three questions: whether they recognized the game type, what the optimal post-success strategy was, and how they predicted the game would converge globally. Each model was queried 10 times, and responses were annotated using gpt-4.1 with manual verification. The authors then computed the percentage of runs where each model correctly identified each dimension and reproduced Figure 1 showing these results.

## Key Results
- Most LLMs correctly identified the coordination game structure from payoff descriptions
- Models consistently predicted optimal post-success strategies aligned with game-theoretic solutions
- LLMs frequently predicted convergence to unique global equilibrium patterns
- The study demonstrates that observed emergent behaviors are observationally equivalent to memorization of training corpus knowledge

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs correctly identify coordination game structures from payoff descriptions, suggesting pre-existing knowledge rather than emergent reasoning.
- Mechan: When prompted with game payoff structures, models retrieve related concepts from training corpus (game theory literature, coordination game research) and generate outputs consistent with that knowledge.
- Core assumption: The scientific literature on coordination games and convention formation is present in pre-training data.
- Evidence anchors: [abstract] "the models simply reproduce conventions they already encountered during pre-training"; [section] Figure 1 shows most models correctly identified coordination game structure, optimal post-success moves, and convergence patterns.

### Mechanism 2
- Claim: Successful coordination after initial match is mechanically enforced by code, not agent reasoning.
- Mechan: The inventory pruning rule is hard-coded in simulation code—after a successful match, the lexicon is reduced to the matched word, statistically biasing future successes regardless of LLM behavior.
- Core assumption: Readers may attribute lexicon reduction to agent strategy rather than external code.
- Evidence anchors: [section] "The inventory pruning rule is hard-coded into their simulation code... As such, it is by mechanics statistically more likely to get a success on this word/convention in subsequent pairings".

### Mechanism 3
- Claim: Emergent and memorized behaviors are observationally equivalent in LLM outputs.
- Mechan: Both genuinely emergent conventions and reproduced training knowledge produce identical observable outcomes (convergence to shared convention), making them impossible to distinguish without internal inspection.
- Core assumption: There exists no external behavioral signature that differentiates emergence from recall.
- Evidence anchors: [abstract] "observed behaviors are indistinguishable from memorization of the training corpus"; [section] "what Ashery et al. claim... are observationally equivalent to a series of LLM agents mapping the payoff description to its pretraining knowledge".

## Foundational Learning

- Concept: **Coordination Games / Naming Games**
  - Why needed here: The studied experiment uses a classic game-theoretic setup where players must match actions to receive rewards; understanding this base case is essential to evaluate whether LLM behavior is emergent or recalled.
  - Quick check question: Can you explain why repeated pairwise matching interactions lead to global convention convergence?

- Concept: **Data Contamination in ML Evaluation**
  - Why needed here: The paper's core argument depends on understanding how training data exposure invalidates test results; this is a known problem in NLP but has distinct implications for generative simulations.
  - Quick check question: Why does training on academic papers create specific contamination risks for social science simulations using LLMs?

- Concept: **Observational Equivalence**
  - Why needed here: The philosophical core of the paper is that two distinct causal mechanisms (emergence vs. memorization) produce identical observations; distinguishing them requires different methods.
  - Quick check question: If two processes produce identical outputs, what additional evidence would be required to distinguish them?

## Architecture Onboarding

- Component map:
  - Prompt template -> Game engine (external code) -> LLM agent -> Evaluation layer

- Critical path:
  1. Define game structure → 2. Prompt LLM with game description → 3. Model retrieves relevant training knowledge → 4. Generates strategy consistent with known game-theoretic solutions → 5. External code enforces lexicon pruning → 6. Convergence observed

- Design tradeoffs:
  - Using novel games avoids contamination but requires validating that LLMs model human behavior correctly in unfamiliar scenarios
  - Obfuscating game structure (unicode tokens) may fail if models recognize payoff patterns
  - Interpretability probing (sparse autoencoders) could detect memorization but remains untested in this context

- Failure signatures:
  - Models correctly predict global convergence before any simulation runs
  - Models identify game as "coordination game" or "convention formation" when asked directly
  - Rapid convergence regardless of prompt formatting variations

- First 3 experiments:
  1. Direct probing: Ask models "Does this setup remind you of any existing model or theory?" to assess prior knowledge activation
  2. Perplexity measurement: Compare perplexity scores on game descriptions vs. novel scenarios to detect familiarity
  3. Control comparison: Test models on genuinely novel games with no literature precedent to establish baseline behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can interpretability probing or perplexity measurements successfully distinguish genuine emergent behavior from data memorization in generative ABMs?
- Basis in paper: [explicit] The authors note, "Whether or not the application of these techniques to generative ABM settings would help overcome some of the problems we describe above is an open question."
- Why unresolved: These techniques have been proposed for standard LLM evaluation but remain untested in the specific context of complex, multi-agent social simulations.
- What evidence would resolve it: Empirical studies showing that these metrics correlate with independent ground-truth measures of "novelty" in agent behavior.

### Open Question 2
- Question: Is it methodologically feasible to design social simulations that are both theoretically relevant and completely absent from existing LLM training corpora?
- Basis in paper: [inferred] The authors suggest "inventing" a novel game as a solution but highlight the difficulty that this "requires us to come up with some novel model of human behavior that is of research interest."
- Why unresolved: There is a high bar for creating game-theoretic setups that are scientifically useful yet fundamentally distinct from the vast amount of internet text describing human behavior and game theory.
- What evidence would resolve it: A demonstration of a generative ABM using a validated "novel" game structure where LLMs exhibit behaviors not explainable by existing social science literature.

### Open Question 3
- Question: What experimental benchmarks could differentiate "spontaneous" self-organization in LLMs from the mere retrieval of known game-theoretic equilibria?
- Basis in paper: [explicit] The authors ask, "So what would we need to see to believe that the LLMs were, in fact, self-organizing in ways similar to human populations?"
- Why unresolved: The paper establishes that current behaviors are "observationally equivalent" to data leakage, meaning surface-level analysis cannot distinguish the source of the behavior.
- What evidence would resolve it: Identifying specific behavioral signatures—such as error patterns or learning trajectories during novel constraints—that diverge from the optimal strategies found in training data.

## Limitations

- Sampling parameters (temperature, top_p) for LLM queries were not specified, potentially affecting response variability
- Annotation reliability lacks inter-rater reliability metrics despite manual verification of gpt-4.1 annotations
- Direct evidence of coordination game literature presence in training corpora is weak and unquantified

## Confidence

- High Confidence: Mechanical enforcement of lexicon pruning in game engine (Mechanism 2) is well-supported by direct code citation
- Medium Confidence: Observational equivalence argument (Mechanism 3) is logically sound but philosophically contestable
- Low Confidence: Specific assertion about coordination game literature in pre-training data (Mechanism 1) lacks direct corpus analysis evidence

## Next Checks

1. Conduct systematic search of actual training corpora to quantify presence of coordination game literature and convention formation research
2. Measure and compare perplexity scores when models process coordination game prompts versus genuinely novel game scenarios
3. Apply sparse autoencoders or mechanistic interpretability techniques to detect whether model activations correspond to memorized training patterns versus novel reasoning pathways