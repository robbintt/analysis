---
ver: rpa2
title: Exploring the Effectiveness of Multi-stage Fine-tuning for Cross-encoder Re-rankers
arxiv_id: '2503.22672'
source_url: https://arxiv.org/abs/2503.22672
tags:
- learning
- u1d45e
- ectiveness
- contrastive
- u1d458
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effectiveness of multi-stage fine-tuning
  for cross-encoder re-rankers in information retrieval. The authors compare single-stage
  fine-tuning using contrastive learning or knowledge distillation with multi-stage
  approaches that combine both techniques.
---

# Exploring the Effectiveness of Multi-stage Fine-tuning for Cross-encoder Re-rankers

## Quick Facts
- arXiv ID: 2503.22672
- Source URL: https://arxiv.org/abs/2503.22672
- Reference count: 31
- Primary result: Single-stage contrastive learning outperforms knowledge distillation for cross-encoder fine-tuning; multi-stage fine-tuning yields no significant improvements.

## Executive Summary
This paper investigates whether multi-stage fine-tuning improves cross-encoder re-rankers compared to single-stage approaches. The authors compare fine-tuning with contrastive learning alone, knowledge distillation alone, and sequential combinations of both methods. Experiments on passage re-ranking benchmarks show that contrastive learning consistently outperforms distillation, but adding a second fine-tuning stage provides no statistically significant benefit. The findings suggest that single-stage contrastive learning is sufficient for effective cross-encoder re-ranking.

## Method Summary
The study evaluates cross-encoder fine-tuning using ELECTRA and RoBERTa models on MS MARCO passage data. Single-stage methods include contrastive learning with hard negatives (LCE loss) and knowledge distillation from RankGPT-3.5 (RankNet loss). Multi-stage approaches apply both methods sequentially in different orders. Training uses 99%/1% train/validation splits, with evaluation on DEV SMALL, TREC DL 19, DL 20, and DL HARD. The study employs two-tailed paired t-tests (p=0.01) for significance testing and uses ColBERTv2 and BM25 as retrievers for hard negative sampling.

## Key Results
- Cross-encoders fine-tuned with contrastive learning outperform those fine-tuned with knowledge distillation on most benchmarks.
- Multi-stage fine-tuning (combining contrastive learning and distillation) yields no statistically significant improvements over single-stage methods.
- Single-stage contrastive learning is sufficient to achieve effective cross-encoder re-rankers.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Single-stage contrastive learning with hard negatives produces more effective cross-encoder re-rankers than knowledge distillation from an LLM teacher.
- Mechanism: The model is trained with a contrastive loss (LCE) that forces it to score a relevant document higher than a set of "hard" non-relevant documents. This direct, relative ranking signal from human-labeled data is more effective than learning to mimic the soft labels from a teacher model (RankGPT-3.5 in this study).
- Core assumption: The "hard" negatives sampled from the top-$k$ list of a retriever (ColBERTv2) are reliably non-relevant and provide a meaningful contrast to the positive document.
- Evidence anchors:
  - [abstract] "Our experiments show that cross-encoders fine-tuned with contrastive learning outperform those fine-tuned with knowledge distillation on most benchmarks."
  - [section 1] "To enhance robustness and effectiveness, contrastive learning losses often incorporate hard negatives... The Localized Contrastive-Estimation (LCE) loss... is an effective variant."
  - [corpus] Corpus evidence is weak or missing for direct corroboration; related papers discuss different optimizers for cross-encoders, not this specific LCE vs. distillation comparison.
- Break condition: The advantage of contrastive learning would likely disappear if the hard negatives were of poor quality (e.g., random documents) or if the retriever used for sampling was ineffective.

### Mechanism 2
- Claim: Sequential multi-stage fine-tuning, applying both contrastive learning and distillation, does not yield statistically significant improvements over the best single-stage approach.
- Mechanism: The model's parameters, after fine-tuning with one strong objective, appear to reach a performance plateau. Adding a second fine-tuning stage with a different objective does not provide enough new, non-redundant information to overcome this plateau and further optimize the model's ranking behavior.
- Core assumption: The training datasets and hyperparameters (learning rate, number of steps) for each stage were sufficiently tuned to allow the second stage to make a meaningful impact.
- Evidence anchors:
  - [abstract] "...applying an additional fine-tuning stage using the other approach does not lead to statistically significant improvements over single-stage methods."
  - [section 5] "...further reﬁning ﬁne-tuned models with a second stage yields no additional beneﬁt. Our ﬁndings suggest that single-stage ﬁne-tuning is suﬃcient..."
  - [corpus] Corpus evidence is weak or missing; no directly comparable experiments on multi-stage vs. single-stage fine-tuning for cross-encoders were found in neighbor papers.
- Break condition: This finding may not hold if a much larger or higher-quality dataset is used for the second stage, or if a different combination of loss functions is applied.

### Mechanism 3
- Claim: The effectiveness of a student cross-encoder is fundamentally limited by the quality and nature of its teacher's ranking signal.
- Mechanism: Knowledge distillation trains a student model to mimic a teacher. If the teacher's ranking signal is suboptimal or misaligned with downstream evaluation tasks, the student will inherit these limitations. The paper suggests RankGPT-3.5's signal was less effective for this specific task than the contrastive signal from human labels.
- Core assumption: The superior performance of the contrastive learning approach is not merely due to the amount of data but to the quality and nature of the training signal itself.
- Evidence anchors:
  - [section 2] "...the eﬀectiveness of the student ranker is closely tied to the quality of the teacher, as its training heavily relies on teacher ranks."
  - [section 1] The paper frames distillation as an alternative, but results show it underperforms compared to contrastive learning in this setup.
  - [corpus] Corpus evidence is weak or missing for this specific claim about teacher quality vs. contrastive data.
- Break condition: This would be invalidated if a different, more powerful teacher model provided a superior signal that the student could successfully distill, outperforming the contrastive learning baseline.

## Foundational Learning

- Concept: Cross-Encoder Architecture
  - Why needed here: The paper's subject is the fine-tuning of cross-encoders. Understanding that they jointly encode a query and document with full cross-attention is foundational to why they are effective but computationally expensive re-rankers.
  - Quick check question: How does a cross-encoder's method of processing a query-document pair differ from a bi-encoder's, and what is the trade-off?

- Concept: Contrastive Learning Objectives
  - Why needed here: The core of the paper's most effective method is contrastive learning. Grasping that the model learns by contrasting a positive example against negative examples is essential.
  - Quick check question: In the context of this paper, what makes a negative sample a "hard negative," and why is it important?

- Concept: Knowledge Distillation
  - Why needed here: The paper systematically evaluates distillation as a fine-tuning strategy. Understanding that a smaller "student" model learns from the outputs of a larger "teacher" model is critical.
  - Quick check question: In the RankNet distillation loss used in this paper, what kind of labels does the student model learn from—hard binary labels or soft ranking signals?

## Architecture Onboarding

- Component map: Pre-trained Cross-Encoder (ELECTRA/RoBERTa) -> Fine-Tuning Stage(s) with Loss Function (Contrastive LCE or Distillation RankNet) -> Training Data (queries, relevant documents, hard negatives or teacher rankings)

- Critical path: The choice of fine-tuning objective is most critical for effectiveness. Contrastive learning with high-quality hard negatives from a good retriever produces the strongest results.

- Design tradeoffs: The main tradeoff explored is between training complexity and effectiveness. Multi-stage fine-tuning increases complexity but offers no significant effectiveness gain over simpler single-stage contrastive learning.

- Failure signatures: Using a teacher model that provides a weaker or less suitable signal than what can be derived from labeled data with contrastive learning. Using non-"hard" negatives for contrastive learning, which provides a weak training signal.

- First 3 experiments:
  1. Reproduce Single-Stage Baselines: Train a cross-encoder using only contrastive learning (LCE) and another using only knowledge distillation (RankNet). Evaluate both on a standard benchmark (e.g., TREC DL) to confirm that contrastive learning yields a more effective model.
  2. Ablate Multi-Stage Combinations: Take the better-performing single-stage model and fine-tune it further with the other method (e.g., Contrastive → Distillation). Compare its final performance to the single-stage baseline to test for improvements.
  3. Test Sequential Order: Perform the multi-stage fine-tuning in both possible orders (Contrastive → Distillation and Distillation → Contrastive) to determine if the sequence has a statistically significant impact on the final result.

## Open Questions the Paper Calls Out

- Question: Would utilizing alternative loss functions for contrastive learning or distillation yield statistically significant improvements in a multi-stage fine-tuning setup?
- Basis in paper: [explicit] The conclusion states future work could "explore other contrastive learning and knowledge distillation losses" beyond the ones tested.
- Why unresolved: The study was restricted to the Localized Contrastive-Estimation (LCE) loss for contrastive learning and the RankNet loss for distillation.
- What evidence would resolve it: Experiments repeating the multi-stage protocols using alternative losses (e.g., Approx. Discounted Rank MSE or standard Binary Cross-Entropy) showing significant gains over single-stage baselines.

- Question: Do the observed limitations of multi-stage fine-tuning apply to other neural re-ranker families, such as sequence-to-sequence or list-wise models?
- Basis in paper: [explicit] The authors explicitly suggest investigating "other... families of neural re-rankers" in future work to validate generalizability.
- Why unresolved: The experiments were conducted exclusively on point-wise cross-encoders (ELECTRA and RoBERTa).
- What evidence would resolve it: Applying the sequential fine-tuning strategies to models like monoT5 or list-wise architectures and observing if multi-stage training outperforms single-stage methods.

- Question: Can different hyperparameter configurations or training dataset compositions unlock performance gains for multi-stage fine-tuning?
- Basis in paper: [explicit] The conclusion lists "other training datasets, configurations" as necessary avenues for future research.
- Why unresolved: The study relied on specific datasets (Schlatt et al. and Sun et al.) and fixed training hyperparameters (e.g., learning rates, number of negatives), which may have limited the effectiveness of the second fine-tuning stage.
- What evidence would resolve it: Ablation studies varying the volume of training data, the number of hard negatives, or learning rate schedules that result in multi-stage models significantly outperforming single-stage models.

## Limitations
- The study focuses on cross-encoders only, limiting generalizability to other re-ranker architectures.
- The distillation results are specific to RankGPT-3.5 as the teacher model, which may not represent the full potential of knowledge distillation.
- The lack of multi-stage improvements may be dataset-specific, as the study uses a fixed set of benchmarks and training data.

## Confidence

- High Confidence: The claim that contrastive learning outperforms distillation for cross-encoder fine-tuning on the tested benchmarks. The experimental design is clear, and the results are consistent.
- Medium Confidence: The claim that multi-stage fine-tuning provides no statistically significant benefit. This is well-supported for the specific combinations tested but may not generalize to all possible loss function pairings or training regimes.
- Low Confidence: Any broad generalization about the fundamental limitations of distillation or the universal superiority of contrastive learning, as these depend heavily on the specific teacher model and dataset quality.

## Next Checks
1. Test with Alternative Teachers: Repeat the distillation experiments using a different, potentially more powerful teacher model (e.g., GPT-4 or a high-performing cross-encoder) to determine if the observed inferiority of distillation is specific to RankGPT-3.5.
2. Scale the Second Stage: For multi-stage experiments, increase the size of the dataset or the number of training steps in the second stage to test if a more extensive fine-tuning phase can overcome the performance plateau observed in the current study.
3. Explore Different Loss Combinations: Investigate multi-stage approaches that combine contrastive learning with other loss functions (e.g., a margin-based ranking loss or a listwise loss) to determine if a different combination yields improvements over single-stage methods.