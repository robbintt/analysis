---
ver: rpa2
title: Using Multi-modal Large Language Model to Boost Fireworks Algorithm's Ability
  in Settling Challenging Optimization Tasks
arxiv_id: '2511.03137'
source_url: https://arxiv.org/abs/2511.03137
tags:
- mllm
- optimization
- design
- information
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a framework leveraging multimodal large language
  models (MLLMs) to enhance the Fireworks Algorithm (FWA) for solving complex optimization
  problems. The framework introduces the concept of Critical Part (CP), which allows
  for the flexible design of both global and local FWAs by incorporating visual information
  from the optimization process.
---

# Using Multi-modal Large Language Model to Boost Fireworks Algorithm's Ability in Settling Challenging Optimization Tasks

## Quick Facts
- arXiv ID: 2511.03137
- Source URL: https://arxiv.org/abs/2511.03137
- Reference count: 31
- Primary result: MLLM-driven framework with visual optimization information significantly improves Fireworks Algorithm performance on TSP and EDA benchmarks

## Executive Summary
This paper presents a framework leveraging multimodal large language models (MLLMs) to enhance the Fireworks Algorithm (FWA) for solving complex optimization problems. The framework introduces the concept of Critical Part (CP), which allows for the flexible design of both global and local FWAs by incorporating visual information from the optimization process. This approach significantly broadens the applicability of FWAs to high-dimensional problems like the Traveling Salesman Problem (TSP) and Electronic Design Automation (EDA).

Experiments on TSP instances demonstrate that the proposed framework outperforms leading TSP heuristics and algorithms, achieving or surpassing state-of-the-art results on numerous benchmarks. Notably, some paths obtained with visual information are better than those given by TSPLIB in the floating-point sense. For EDA tasks, the framework delivers state-of-the-art performance on six out of eight benchmarks using extremely low computational resources.

## Method Summary
The framework evolves FWA operators through an iterative process using Doubao-1.5-pro-vision MLLM. It employs a Critical Part (CP) abstraction to handle both global FWA evolution for encodable problems (TSP) and local component evolution for high-dimensional problems (EDA step-size functions). The process uses mutation and crossover operators to generate candidate algorithms, evaluates them on problem instances, and selects top performers through greedy selection. Visual optimization information (route visualizations, heatmaps, density maps) is incorporated alongside textual code and performance metrics to guide the evolution process.

## Key Results
- Framework outperforms leading TSP heuristics across diverse TSPLib instances, achieving or surpassing state-of-the-art results
- EDA benchmarks show state-of-the-art performance on six out of eight instances with extremely low computational resources
- Visual information improves results for high-dimensional complex problems but degrades performance on simpler TSP instances (eil51, st70)
- Analysis reveals that visual information helps MLLM capture unique geometric features in TSP but fails to discriminate in EDA due to similar pixel-level topology

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Visual optimization information enables MLLM to generate more heterogeneous, task-customized algorithm components when problem instances exhibit visually distinguishable characteristics.
- Mechanism: The MLLM processes visual representations (path configurations, crossing heatmaps, density maps) alongside textual code/performance data. When visual features differ meaningfully across instances, the model generates structurally distinct operators. When visual features are similar (e.g., high-dimensional EDA layouts appearing alike), the model defaults to more homogeneous solutions.
- Core assumption: MLLM can extract actionable optimization cues from visual representations that correlate with effective algorithmic modifications.
- Evidence anchors:
  - [abstract]: "The analysis of MLLM design behavior reveals that visual information does not always guarantee improved performance but is valuable for high-dimensional complex problems."
  - [section IV.A]: "The average cross-instance similarity with visual information is 0.8848±0.03, significantly exceeding that without visual information (0.8325±0.06) with p-value <0.05...visual information helps MLLM to capture the unique geometric features."
  - [section IV.B]: "In EDA task, the calculated p-values no longer support the statistically significant conclusion...the layout diagrams of different instances may show similar topology at the pixel level."
  - [corpus]: Related work on multi-modal attention for routing problems (GAMA) suggests visual/graph features can inform optimization, but evidence for MLLM-specific visual reasoning in algorithm design remains preliminary.
- Break condition: If visual representations lack discriminative power across problem instances, or if MLLM cannot reliably map visual patterns to code modifications, the customization benefit degrades to text-only baseline.

### Mechanism 2
- Claim: The Critical Part (CP) abstraction extends FWA applicability by decoupling the optimization target from monolithic global optimization to selective component evolution.
- Mechanism: CP classifies problems into (1) those amenable to global FWA encoding (e.g., TSP with discrete permutations) and (2) high-dimensional problems requiring local operator enhancement within existing frameworks (e.g., EDA step-size control in DreamPlace). This allows FWA principles to scale from hundreds to millions of dimensions.
- Core assumption: The bottleneck for high-dimensional problems lies in specific subcomponents (e.g., step-size heuristics) rather than requiring full optimizer redesign.
- Evidence anchors:
  - [abstract]: "This paper presents a framework...introduces the concept of Critical Part (CP), which allows for the flexible design of both global and local FWAs."
  - [section III]: "To extend the task-solving capabilities of LLM-FWA, we introduce the Critical Part (CP)...which expands LLM driven FWA's optimization target space by classifying the problem."
  - [section IV.B]: EDA results show "six out of eight instances have generated better solutions than the SOTA" by evolving only the step-size function.
  - [corpus]: No direct corpus evidence on CP-like decomposition strategies for LLM-assisted optimization; this appears novel.
- Break condition: If problem performance bottleneck cannot be isolated to a modular component, or if component interface constraints prevent effective evolution, CP-based local evolution fails.

### Mechanism 3
- Claim: Evolutionary code generation with performance feedback (textual + visual) produces algorithms competitive with or exceeding domain-specific heuristics.
- Mechanism: Population of 5 algorithm variants undergoes mutation (modifying single operator) and crossover (combining operators from parents). Each iteration feeds current code, performance metrics, and visualizations to MLLM, which generates candidates evaluated against problem instances. Greedy selection maintains top performers.
- Core assumption: MLLM can produce syntactically correct, functionally improved code variants when provided with performance feedback and visual context.
- Evidence anchors:
  - [section IV.A, Table I]: "Our algorithm consistently outperforms all leading TSP heuristics/algorithms across diverse TSPLib instances."
  - [section IV.B, Table II]: EDA benchmarks show "all runtime metrics are normalized against the Dreamplace framework's execution time...our methodology demonstrates compelling performance."
  - [corpus]: Evolutionary heuristic design (EOH, FunSearch) provides precedent for LLM-driven algorithm discovery, but multi-modal feedback is less studied.
- Break condition: If MLLM generates syntactically invalid code at high rates, or if performance feedback fails to correlate with meaningful algorithmic improvements, the evolutionary loop stalls.

## Foundational Learning

- Concept: Swarm Intelligence Optimization (specifically Fireworks Algorithm)
  - Why needed here: FWA's operator-level design space (explosion, mutation, selection) provides structured targets for LLM modification. Understanding these operators is prerequisite to interpreting generated code.
  - Quick check question: Can you explain how FWA's explosion operator generates candidate solutions and how amplitude affects exploration vs. exploitation?

- Concept: Large Language Model Code Generation Capabilities and Limitations
  - Why needed here: The framework relies on MLLM producing executable, logically coherent algorithm variants. Understanding where LLMs struggle (long-range dependencies, complex constraints) informs prompt design and failure diagnosis.
  - Quick check question: What are common failure modes when LLMs generate optimization code, and how might visual context mitigate or exacerbate these?

- Concept: Multi-modal Representation Alignment
  - Why needed here: The framework assumes visual features (paths, heatmaps, layouts) can inform algorithmic design decisions. Understanding how vision-language models ground visual patterns in code-relevant concepts is critical for interpreting results.
  - Quick check question: How might an MLLM map a "path crossing" visualization to a specific mutation operator modification?

## Architecture Onboarding

- Component map:
  - MLLM (Doubao-1.5-pro-vision) -> Prompt Pattern -> Problem Evaluator -> Critical Part (CP) -> Population Manager

- Critical path:
  1. Classify problem dimension/encodability → select CP type (global FWA vs. local idea evolution)
  2. Initialize population with baseline algorithm (e.g., return base_step for EDA)
  3. For each iteration: generate mutation + crossover candidates via MLLM
  4. Evaluate candidates on problem instance
  5. Aggregate results (code + metrics + visualizations)
  6. Greedy selection updates population
  7. Repeat until convergence or max iterations

- Design tradeoffs:
  - Visual information: May improve results for high-dimensional/complex problems but degrades performance on simpler instances (eil51, st70). Not universally beneficial.
  - Population size (5) vs. exploration: Small population enables rapid iteration but may converge prematurely. Paper uses 5; sensitivity not analyzed.
  - Iteration count (200): Sufficient for TSP/EDA benchmarks but may require tuning for other domains.
  - MLLM choice: Results depend on Doubao-1.5-pro-vision; other models may produce different code characteristics.

- Failure signatures:
  - High code similarity across instances: Suggests visual information not discriminative enough (observed in EDA) or MLLM not leveraging visual cues.
  - Syntactic errors in generated code: Check prompt clarity, input/output format constraints.
  - Performance plateau: Population may have converged; consider increasing diversity (larger pool, different crossover strategy).
  - Runtime explosion: Generated code may call evaluation function excessively (noted in EDA results); add computational budget constraints to prompt.

- First 3 experiments:
  1. Reproduce TSP benchmark (eil51): Run framework with and without visual information. Verify that text-only matches or exceeds visual-enabled on this simpler instance, confirming the non-monotonic visual benefit finding.
  2. Test CP boundary: Apply global FWA to a problem at the dimension threshold (e.g., 1000-5000 variables). Compare against local CP approach to validate problem classification heuristic.
  3. Visual ablation on EDA high-dimensional instance (bigblue4): Compare (a) full visual, (b) no visual, (c) single visualization type (layout only, no heatmap/density). Identify which visual cues contribute most to step-size design improvements.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is optimizer-based design or heuristic-based design generally more effective for LLM-driven optimization tasks?
- Basis in paper: [explicit] The paper states, "The comparative effectiveness of these two paradigms remains an open question" regarding using LLMs to evolve optimizers (like FWA) versus discovering heuristic rules.
- Why unresolved: The authors focus solely on the FWA-based approach and do not conduct a direct comparison against pure heuristic discovery methods on the same tasks.
- What evidence would resolve it: A controlled study applying both paradigms to identical benchmark sets (e.g., TSP) to compare convergence speed and solution quality.

### Open Question 2
- Question: Under what specific conditions does visual modality information enhance versus hinder algorithmic design performance?
- Basis in paper: [explicit] The analysis reveals that visual information "does not always guarantee improved performance" and contradicts the intuition that more information is better (e.g., performance dropped on TSP instances eil51 and st70).
- Why unresolved: While the paper observes that visual cues help in high-dimensional complex problems but hurt in others, the precise mechanism or theoretical boundary for this trade-off is not defined.
- What evidence would resolve it: A theoretical framework identifying "operational optimization cues" versus "superficial visual similarity" that predicts performance changes before running the evolution.

### Open Question 3
- Question: Can the "Critical Part" (CP) classification step be automated to remove the reliance on human experience?
- Basis in paper: [inferred] The method description notes that researchers "need to take the initiative to classify the problem, which mainly relies on personal experience" to decide between Global FWA or Local Idea Evolution.
- Why unresolved: This step represents a manual "human-in-the-loop" limitation in a framework that is otherwise presented as an automated design paradigm.
- What evidence would resolve it: The development of a meta-classifier or adaptive mechanism that autonomously selects the correct CP strategy (global vs. local) based on problem dimensionality and constraints.

## Limitations

- Visual information benefit is non-monotonic and problem-dependent, working well for high-dimensional EDA but degrading performance on simpler TSP instances
- Framework's performance advantage over domain-specific heuristics (Gurobi, Concorde) shown but computational overhead of evolutionary process not quantified
- CP abstraction's boundary between global and local evolution is heuristic and may not hold for all problem classes
- Results depend on specific Doubao-1.5-pro-vision MLLM; generalizability to other vision-language models uncertain

## Confidence

- High Confidence: Experimental results showing competitive TSP performance and EDA improvements are well-supported by tables and statistical analysis. The mechanism of evolving algorithm components via evolutionary prompts is clearly demonstrated.
- Medium Confidence: Claims about visual information's value for high-dimensional problems are supported but the analysis of why visual features fail to discriminate in EDA (similar pixel-level topology) is speculative. The CP abstraction's generality beyond tested domains is asserted but not rigorously validated.
- Low Confidence: Assertion that this approach "significantly broadens" FWA applicability is strong and would require testing on a much wider range of optimization problems to substantiate.

## Next Checks

1. Cross-Model Validation: Reproduce the TSP eil51 experiment using a different MLLM (e.g., GPT-4V or Claude-3-Vision) to assess whether the observed non-monotonic visual benefit is model-dependent or a general phenomenon.

2. CP Boundary Stress Test: Systematically vary problem dimensionality (e.g., test at 1000, 5000, 10000 variables) for a representative optimization task to empirically validate the CP classification heuristic and identify where the transition from global to local evolution occurs.

3. Visual Ablation with Controlled Features: For the bigblue4 EDA instance, conduct a controlled ablation study isolating individual visual components (layout only, heatmap only, density only) to pinpoint which visual cues most strongly influence step-size design improvements and quantify their individual contributions.