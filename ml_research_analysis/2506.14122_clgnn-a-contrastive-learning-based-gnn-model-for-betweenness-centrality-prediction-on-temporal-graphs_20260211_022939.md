---
ver: rpa2
title: 'CLGNN: A Contrastive Learning-based GNN Model for Betweenness Centrality Prediction
  on Temporal Graphs'
arxiv_id: '2506.14122'
source_url: https://arxiv.org/abs/2506.14122
tags:
- temporal
- contrastive
- node
- clgnn
- path
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CLGNN introduces a contrastive learning-based GNN architecture
  to predict Temporal Betweenness Centrality (TBC) in highly imbalanced graphs. It
  addresses the challenge of skewed TBC distributions by combining a dual aggregation
  mechanism with path-time encodings and a stability-driven contrastive module (KContrastNet)
  that separates low-, medium-, and high-centrality nodes in embedding space.
---

# CLGNN: A Contrastive Learning-based GNN Model for Betweenness Centrality Prediction on Temporal Graphs

## Quick Facts
- arXiv ID: 2506.14122
- Source URL: https://arxiv.org/abs/2506.14122
- Reference count: 40
- Primary result: Up to 663.7× speedup over exact methods for Temporal Betweenness Centrality prediction

## Executive Summary
CLGNN introduces a contrastive learning-based GNN architecture to predict Temporal Betweenness Centrality (TBC) in highly imbalanced graphs. It addresses the challenge of skewed TBC distributions by combining a dual aggregation mechanism with path-time encodings and a stability-driven contrastive module (KContrastNet) that separates low-, medium-, and high-centrality nodes in embedding space. Experiments across 12 datasets show CLGNN achieves up to 663.7× speedup over exact methods, with 31.4× lower MAE and 16.7× higher Spearman correlation than static GNNs, and 5.7× lower MAE and 3.9× higher Spearman correlation than state-of-the-art temporal GNNs.

## Method Summary
CLGNN predicts Temporal Betweenness Centrality on temporal graphs using a dual-aggregation GNN architecture enhanced with contrastive learning. The model processes temporal graphs through a path-time dual-aware mechanism that filters invalid temporal paths via path-count encoding, then applies a weighted combination of mean and attention-based message aggregation. A stability-based clustering module (KContrastNet) identifies optimal cluster counts and generates contrastive pairs within clusters to handle the extreme class imbalance where most nodes have zero centrality. The model jointly optimizes a contrastive loss for node separation and a regression loss for TBC value prediction.

## Key Results
- Achieves up to 663.7× speedup over exact ETBC algorithms
- Reduces MAE by 31.4× compared to static GNNs
- Improves Spearman correlation by 16.7× over static GNNs and 3.9× over temporal GNNs

## Why This Works (Mechanism)

### Mechanism 1
Filtering invalid temporal paths via path-count encoding improves signal-to-noise ratio in message passing. The model calculates $P(u, v, t)$ (valid path count) for each edge, masks edges with $P=0$, and concatenates the log-transformed count with time encoding. This ensures the GNN propagates information primarily along viable temporal trajectories, assuming valid temporal paths are the primary structural determinant of high TBC scores.

### Mechanism 2
Stability-based clustering separates high- and low-centrality nodes in embedding space, preventing regression collapse. KContrastNet uses bootstrap resampling to find optimal cluster count $\hat{k}$ that minimizes instability, then samples positive/negative pairs within these clusters based on TBC distance. The re-weighted InfoNCE loss pushes dissimilar nodes apart, assuming node embeddings naturally cluster by TBC magnitude but require explicit separation to counter class imbalance.

### Mechanism 3
Dual aggregation (Mean + Attention) stabilizes learning on highly imbalanced temporal neighbors. The model fuses node-level mean aggregation (ensuring stability and equal neighbor contribution) with edge-level multi-head attention (capturing fine-grained temporal relevance) using weight $\lambda$. This hybridization is assumed necessary because a single aggregation strategy is brittle - mean aggregation provides a baseline robust to noise while attention captures critical temporal transitions.

## Foundational Learning

- **Concept: Temporal Betweenness Centrality (TBC)**
  - Why needed: TBC is the target variable; unlike static BC, it relies on "optimal temporal paths" respecting time ordering
  - Quick check: Can you explain why a node with high static degree might have zero TBC in a temporal network?

- **Concept: Contrastive Learning (InfoNCE)**
  - Why needed: Used to solve extreme class imbalance by pushing apart dissimilar nodes
  - Quick check: In this architecture, are negative pairs drawn from different clusters or the same cluster, and why?

- **Concept: Continuous Time Encoding**
  - Why needed: Essential for the "Path-Time dual-aware mechanism" to map time to vectors rather than discrete snapshots
  - Quick check: How does the model handle time encoding for edges with large time gaps $\delta$ compared to small gaps?

## Architecture Onboarding

- **Component map:** Input Graph -> Path-Time Encoder -> Dual Aggregator -> KContrastNet (Clustering -> Contrastive Loss) + ValueNet (MLP -> Regression Loss)

- **Critical path:** Pre-processing (Path Counting) -> Message Passing -> Embedding Generation -> **Stability Clustering** (Determines $\hat{k}$) -> Contrastive Pair Selection -> Joint Optimization ($L_{total}$)

- **Design tradeoffs:**
  - **Path Definitions:** "Shortest" vs. "Earliest Arrival" vs. "Latest Departure" - evidence suggests "Shortest" is generally more stable for GNNs
  - **Loss Balance ($\alpha$):** Balancing $L_{contrast}$ and $L_{regress}$ - tuning suggests $\alpha \approx 0.2$ is optimal
  - **Aggregation ($\lambda$):** Balancing Mean vs. Attention - tuning suggests $\lambda \approx 0.4$ improves ranking

- **Failure signatures:**
  - **Degenerate Prediction:** Model predicts all zeros (MAE is low, but Spearman is 0) - fix: increase contrastive loss weight $\alpha$ or check cluster validity
  - **High Variance:** High std dev in results - fix: increase Mean aggregation weight $\lambda$ or reduce learning rate
  - **Runtime Bottleneck:** Exact TBC computation during ground-truth generation

- **First 3 experiments:**
  1. **Sanity Check (Static vs. Temporal):** Run CLGNN against a static GNN (e.g., GCN) on `ia-reality-call` to verify temporal encoding captures TBC dynamics
  2. **Ablation (Contrastive):** Remove KContrastNet (set $\alpha=0$) and observe spike in MAE for high-value nodes to confirm handling of imbalance
  3. **Hyperparameter Sensitivity:** Vary $\lambda$ from 0.0 to 1.0 on `haggle` to visualize stability vs. accuracy trade-off

## Open Questions the Paper Calls Out

- **Open Question 1:** How sensitive is CLGNN to key architectural hyperparameters (time encoding basis dimension, cluster count in KContrastNet, attention head size)? The authors did not perform extensive hyperparameter search for these parameters due to computational limits.

- **Open Question 2:** Can advanced temporal graph augmentation techniques be developed to improve CLGNN's generalization? Standard random augmentations (node drop, edge perturbation) proved ineffective as they distort topology and temporal dependencies.

- **Open Question 3:** Can the CLGNN architecture be effectively extended to predict other temporal centrality measures (closeness, degree centrality)? The current model is specialized for TBC (path-based) and it's unproven whether the dual aggregation and contrastive modules suit other centrality definitions.

- **Open Question 4:** How can the model be adapted to maintain performance under restrictive temporal path definitions (latest departure constraints)? Performance degrades significantly when moving from simple "shortest path" to complex "Shortest + Latest Departure + Earliest Arrival" definitions.

## Limitations

- Data preparation bottleneck: Generating training data for new datasets still requires running ETBC (exact temporal betweenness centrality), creating a chicken-and-egg problem for real-world deployment.

- Clustering stability assumptions: The KContrastNet module assumes TBC values naturally cluster into distinct groups, but this may fail on datasets with continuous or uniform TBC distributions.

- Temporal path definitions: The paper doesn't explore edge cases like networks with very sparse temporal connectivity or networks where the optimal $\delta$ constraint varies significantly across nodes.

## Confidence

- **Speedup claims (663.7×):** High confidence - based on direct comparison with ETBC algorithm on same hardware
- **MAE improvement claims (31.4× over static GNNs):** Medium confidence - strong empirical results across 12 datasets, but assumes static GNNs can be reasonably applied to temporal graphs
- **Contrastive learning effectiveness:** Medium confidence - ablation studies show improvement, but specific contribution of stability-based clustering is difficult to isolate

## Next Checks

1. **Cluster validity test:** Run KContrastNet on a synthetic temporal graph with uniformly distributed TBC values to verify stability-based clustering degrades gracefully rather than producing misleading clusters.

2. **Ground truth generation cost analysis:** Measure actual time required to compute ETBC for a medium-sized temporal graph (10,000 nodes, 100,000 temporal edges) to quantify true deployment barrier for new datasets.

3. **Extreme imbalance robustness:** Create a temporal graph where 99.9% of nodes have zero TBC and measure whether CLGNN maintains performance advantage over static methods, particularly examining contrastive module's behavior in this regime.