---
ver: rpa2
title: 'FIRESPARQL: A LLM-based Framework for SPARQL Query Generation over Scholarly
  Knowledge Graphs'
arxiv_id: '2508.10467'
source_url: https://arxiv.org/abs/2508.10467
tags:
- sparql
- queries
- query
- knowledge
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces FIRESPARQL, a modular framework for generating\
  \ SPARQL queries over Scholarly Knowledge Graphs (SKGs) using fine-tuned Large Language\
  \ Models (LLMs). The authors identify two main error types in LLM-generated queries\u2014\
  structural inconsistencies and semantic inaccuracies\u2014and address them through\
  \ fine-tuning, optional Retrieval-Augmented Generation (RAG), and a SPARQL correction\
  \ layer."
---

# FIRESPARQL: A LLM-based Framework for SPARQL Query Generation over Scholarly Knowledge Graphs

## Quick Facts
- **arXiv ID**: 2508.10467
- **Source URL**: https://arxiv.org/abs/2508.10467
- **Reference count**: 25
- **Primary result**: Fine-tuning LLaMA3-8B-Instruct with LoRA achieves 0.90 ROUGE-L query accuracy and 0.85 RelaxedEM result accuracy on SciQA benchmark.

## Executive Summary
FIRESPARQL is a modular framework that generates SPARQL queries over Scholarly Knowledge Graphs using fine-tuned LLMs. The authors address two main error types—structural inconsistencies and semantic inaccuracies—through LoRA-based fine-tuning, optional RAG, and a post-generation SPARQL correction layer. Experiments on SciQA show fine-tuning outperforms prompting baselines, with larger models and more epochs yielding better results. RAG integration can degrade performance due to noisy context. The framework is open-sourced with the best model available on Hugging Face.

## Method Summary
FIRESPARQL fine-tunes LLaMA-3-8B-Instruct using LoRA on NLQ-SPARQL pairs from the SciQA benchmark. Training data is deduplicated and schema-aligned. The framework optionally integrates RAG for context retrieval and applies a lightweight SPARQL correction layer post-generation to fix syntactic errors. Queries are executed via QLever, and results are evaluated using BLEU-4, ROUGE-1/2/L, and RelaxedEM metrics. The best configuration uses 15 epochs of fine-tuning with LoRA.

## Key Results
- Fine-tuning achieves 0.90 ROUGE-L query accuracy and 0.85 RelaxedEM result accuracy using LLaMA3-8B-Instruct with 15 epochs.
- Larger models (8B) outperform smaller ones (3B) after fine-tuning.
- RAG integration degrades performance due to noisy or misaligned retrieved context.
- SPARQL correction layer increases execution success rate by fixing syntax errors.

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning with LoRA on NLQ-SPARQL pairs reduces structural inconsistencies in generated queries. LoRA freezes pre-trained weights and injects trainable low-rank decomposition matrices, allowing the model to implicitly capture the ontology and structural patterns of the target SKG without full parameter updates. The training data encodes valid query templates and schema relationships. Assumes the training distribution sufficiently covers the structural patterns needed at inference time.

### Mechanism 2
RAG integration can degrade performance due to noisy or misaligned retrieved context. Retrieved candidate properties/entities may be irrelevant or incorrect for the specific query. Without a context relevance checker, the LLM incorporates these noisy signals, overriding knowledge already encoded through fine-tuning. Assumes fine-tuned models have already internalized domain knowledge.

### Mechanism 3
Post-generation SPARQL correction layer improves execution success by fixing syntactic errors. A lightweight LLM-based corrector refines the initially generated query—removing extraneous text, fixing punctuation, and addressing syntax issues. Assumes syntax errors are local and correctable without semantic restructuring.

## Foundational Learning

- **SPARQL query structure**: Understanding SELECT, WHERE, triple patterns, GROUP BY, subqueries is essential to diagnose failure modes and evaluate generated queries. *Quick check*: Given a SPARQL query using MAX(?value), what additional clause is required for valid syntax?
- **Low-Rank Adaptation (LoRA)**: Core mechanism for parameter-efficient fine-tuning; informs training budget and infrastructure decisions. *Quick check*: What does LoRA freeze, and what does it train?
- **Knowledge Graph schema and ontology**: Errors are categorized as structural (schema misunderstanding) vs. semantic (entity/property mismatch). *Quick check*: In a triple `<subject> <predicate> <object>`, which component is typically the property in ORKG queries?

## Architecture Onboarding

- **Component map**: Fine-tuned LLM -> SPARQL correction layer -> QLever SPARQL endpoint -> Evaluation pipeline
- **Critical path**: Prepare NLQ-SPARQL training pairs -> Fine-tune LLM with LoRA -> At inference: generate SPARQL -> Apply correction layer -> Execute on QLever
- **Design tradeoffs**: Fine-tuning vs. few-shot (fine-tuning achieves higher accuracy but requires labeled data); Model size (8B outperforms 3B after fine-tuning); RAG inclusion (adds compute overhead and can degrade results)
- **Failure signatures**: "Variable ?X is selected but not aggregated" → Aggregation syntax error; "mismatched input 'SELECT' expecting '}'" → Malformed subquery; Empty results with valid syntax → Semantic inaccuracy
- **First 3 experiments**: 1) Reproduce fine-tuning baseline: LLaMA-3-8B-Instruct, 15 epochs, SciQA train split; 2) Ablate SPARQL correction layer: Measure execution success rate with/without correction on 100 held-out queries; 3) Controlled RAG test: Manually curate high-precision context for 50 queries; compare RelaxedEM vs. no-RAG baseline

## Open Questions the Paper Calls Out

1. **Generalizability to other SKGs**: The framework was fine-tuned exclusively on ORKG-specific schema and data, which may have resulted in overfitting. Evaluating on other benchmarks like LC-QuAD or QALD would assess generalizability.
2. **RAG with topological subgraphs**: Current RAG degrades performance due to noisy context. Using topological subgraphs or query templates instead of simple property lists could improve retrieval quality.
3. **Synthetic training data**: Reliance on supervised NLQ-SPARQL pairs is a limitation. Exploring synthetic data or weak supervision from ontology could reduce data annotation costs.

## Limitations

- Critical LoRA hyperparameters (rank, alpha, learning rate, batch size) are unspecified, limiting exact reproduction.
- RAG performance is highly context-dependent and may not generalize to knowledge graphs with different schema complexity.
- Evaluation relies solely on the SciQA benchmark, limiting external validity.
- SPARQL correction layer cannot recover from semantic errors, only syntactic ones.

## Confidence

- **High confidence**: Fine-tuning with LoRA improves SPARQL generation accuracy over zero-shot and one-shot baselines; larger models (8B) outperform smaller ones after fine-tuning.
- **Medium confidence**: RAG degrades performance due to noisy context; results are sensitive to retrieval quality and may not generalize.
- **Medium confidence**: SPARQL correction layer increases execution success by fixing syntactic errors, but cannot address semantic inaccuracies.

## Next Checks

1. **Hyperparameter sensitivity**: Reproduce fine-tuning with a grid search over LoRA rank and learning rate; assess impact on ROUGE-L and RelaxedEM.
2. **RAG quality control**: Implement a context relevance filter before RAG integration; measure whether high-precision retrieval can recover or exceed non-RAG performance.
3. **Schema generalization**: Evaluate FIRESPARQL on a different scholarly knowledge graph (e.g., MAG or DBpedia) to test robustness to ontological shifts.