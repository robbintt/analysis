---
ver: rpa2
title: Mitigating Algorithmic Bias in Multiclass CNN Classifications Using Causal
  Modeling
arxiv_id: '2501.07885'
source_url: https://arxiv.org/abs/2501.07885
tags:
- bias
- causal
- fairness
- emotion
- gender
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study applies causal modeling to detect and mitigate algorithmic
  bias in a multiclass CNN for emotion classification. The authors used the FairFace
  dataset with emotion labels from a pre-trained DeepFace model, training a custom
  CNN (2.78M parameters) that achieved 58.3% accuracy on test data.
---

# Mitigating Algorithmic Bias in Multiclass CNN Classifications Using Causal Modeling

## Quick Facts
- arXiv ID: 2501.07885
- Source URL: https://arxiv.org/abs/2501.07885
- Reference count: 23
- Primary result: Causal post-processing reduces gender bias in multiclass emotion classification while improving accuracy

## Executive Summary
This study addresses algorithmic bias in multiclass CNN emotion classification by applying causal modeling techniques to adjust predicted class probabilities post-hoc. Using the FairFace dataset with emotion labels from a pre-trained DeepFace model, the authors train a custom CNN that achieves 58.3% accuracy on test data. The analysis reveals significant gender bias: females are more often classified as "happy" or "sad," while males are more often classified as "neutral." The authors apply one-vs-all (OvA) causal modeling to estimate and remove gender effects from predicted probabilities, successfully reducing gender disparities across all emotion classes with negligible impact on overall accuracy (improving from 60.4% to 60.8% on the CM test set).

## Method Summary
The authors apply causal modeling post-hoc to a trained multiclass CNN for emotion classification. After training the CNN on FairFace images with DeepFace-generated emotion labels, they use OvA calibration to convert softmax probabilities into per-class binary probability estimates. Four linear regression models are trained (one per emotion class group) to quantify the direct effect of gender on predicted probabilities via path coefficients. These coefficients are then subtracted from the predicted probabilities at inference time to produce debiased estimates, with the final prediction being the class with the highest adjusted probability.

## Key Results
- Custom CNN achieves 58.3% accuracy on test data (7 emotion classes)
- Gender bias detected: females more likely classified as "happy/sad," males as "neutral"
- Debiasing reduces gender disparities across all emotion classes
- Overall accuracy improves from 60.4% to 60.8% after causal adjustment
- Path coefficients show significant gender effects (p-values small) for all classes

## Why This Works (Mechanism)

### Mechanism 1
Post-hoc causal adjustment can reduce gender disparities in multiclass CNN predictions without retraining the model. The One-vs-All (OvA) technique decomposes the multiclass problem into independent binary causal models, each estimating the direct effect of gender on predicted probabilities for one emotion class. Linear regression yields path coefficients (β) quantifying this bias, which are then subtracted from predicted probabilities to produce debiased estimates.

### Mechanism 2
Aggregating debiased probabilities via maximum selection preserves classification accuracy while improving fairness. After applying class-specific debiasing equations, the final prediction is the class with the highest adjusted probability. This preserves the decision boundary structure while shifting it to account for removed gender effects.

### Mechanism 3
Detecting bias via statistically significant path coefficients provides interpretable evidence of algorithmic unfairness. Linear regression on each OvA model tests whether the coefficient for gender (β_a,ŷ) is significantly non-zero, indicating systematic differences in predicted probabilities across genders.

## Foundational Learning

- **Causal Path Models**: Used to formalize how gender causally influences predictions, separating direct bias from legitimate correlations. Quick check: In a path diagram, what does an arrow from gender (a) to predicted probability (ŷ) represent versus an arrow from true label (y) to ŷ?
- **One-vs-All (OvA) Multiclass Decomposition**: Extends binary causal bias mitigation to 7 emotion classes by training 4 independent causal models (after grouping). Quick check: Why did the authors group "fear," "angry," "surprise," and "disgust" into a single "others" class before applying OvA?
- **Demographic Parity vs. Worldviews (WAE vs. WYSIWYG)**: The paper positions its approach within fairness frameworks, noting that demographic parity can lead to unintended consequences under the WYSIWYG worldview. Quick check: Why might demographic parity be criticized as "reverse discrimination" from a WYSIWYG perspective?

## Architecture Onboarding

- **Component map**: CNN backbone (4 conv blocks: 32→64→128→256 filters, batch norm, max-pooling → Flatten → Dense(256) + Dropout → Dense(128) + Dropout → Dense(7 softmax output)) → OvA calibration layer → Causal adjustment module (4 linear regression models)
- **Critical path**: Train CNN on FairFace + DeepFace emotion labels (86,744 samples) → Generate raw probabilities on held-out test set (5,477 samples) → Split test set into CM training (80%) and CM test (20%) → Apply OvA calibration; train 4 causal models via linear regression → At inference: adjust probabilities using learned β coefficients → Select class with maximum debiased probability
- **Design tradeoffs**: Post-processing vs. in-processing (avoids retraining but requires protected attributes at inference); class grouping (consolidates 4 small classes for sufficient data per causal model but loses emotion-specific bias signals); linear assumption (simple to implement but may miss non-linear gender effects)
- **Failure signatures**: Gender gap widens after debiasing (monitor TPR differences per class per gender); overall accuracy drops >2% (paper shows 60.4% → 60.8%, negligible); non-significant β coefficients (unlikely with n=4,381 but check if data is imbalanced)
- **First 3 experiments**: Ablation on class grouping (test keeping all 7 classes); Non-linear causal models (add interaction terms); Cross-dataset validation (apply to AffectNet)

## Open Questions the Paper Calls Out

- **Open Question 1**: Under what theoretical conditions does causal post-processing enhance fairness while simultaneously improving accuracy? The authors state a future goal to "develop a stronger theoretical foundation for the conditions under which post-processing can enhance fairness while improving accuracy."
- **Open Question 2**: Can causal modeling be adapted to address biases that originate from the training data distribution rather than solely the model outputs? The authors list as a future direction: "We will also explore the use of causal modeling to address biases that originate from the training data."
- **Open Question 3**: How does the validity of bias mitigation change when using human-annotated ground truth instead of synthetic labels from a pre-trained model? The methodology relies on emotion labels generated by the DeepFace model rather than human annotation.

## Limitations

- The study uses synthetic emotion labels from a pre-trained model rather than human annotation, potentially propagating existing biases
- The linear causal assumption may oversimplify complex relationships between gender and facial expression patterns
- Consolidation of four emotion classes into "others" limits the granularity of bias analysis and may mask class-specific disparities

## Confidence

- **High confidence**: CNN architecture achieves competitive accuracy (58.3% test, 60.8% CM test); statistical significance of gender path coefficients is well-established with large sample sizes
- **Medium confidence**: Post-hoc debiasing mechanism effectively reduces gender gaps across emotion classes; generalizability to other datasets requires further validation
- **Low confidence**: Specific linear regression coefficients and their stability across different train-test splits (no reported confidence intervals or cross-validation)

## Next Checks

1. **Ablation on class grouping**: Test whether keeping all 7 classes (no grouping) changes bias mitigation effectiveness and accuracy
2. **Non-linear causal models**: Add interaction terms (gender × predicted probability) to test if linear assumption holds
3. **Cross-dataset validation**: Apply the trained causal adjustment module to a different emotion dataset (e.g., AffectNet) to assess generalizability of β coefficients