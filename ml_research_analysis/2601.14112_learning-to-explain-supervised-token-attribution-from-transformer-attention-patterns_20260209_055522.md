---
ver: rpa2
title: 'Learning to Explain: Supervised Token Attribution from Transformer Attention
  Patterns'
arxiv_id: '2601.14112'
source_url: https://arxiv.org/abs/2601.14112
tags:
- expnet
- attention
- methods
- across
- sst-2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ExpNet learns to generate token-level explanations for transformer
  models by mapping attention patterns to human-aligned importance scores through
  supervised learning on rationale annotations. The method outperforms thirteen baseline
  explainers across three NLP tasks (SST-2, CoLA, HateXplain) with F1-score improvements
  of 13.9-31.4% over the best baseline in each domain, demonstrating robust cross-task
  generalization when trained on two tasks and evaluated on the third.
---

# Learning to Explain: Supervised Token Attribution from Transformer Attention Patterns

## Quick Facts
- arXiv ID: 2601.14112
- Source URL: https://arxiv.org/abs/2601.14112
- Authors: George Mihaila
- Reference count: 26
- ExpNet achieves 13.9-31.4% F1-score improvements over baselines in cross-task explanation generation

## Executive Summary
ExpNet introduces a supervised learning approach that transforms transformer attention patterns into human-aligned token importance scores. By training a lightweight neural network to map attention features to human-annotated rationales, ExpNet outperforms thirteen baseline explainers across sentiment, grammaticality, and hate speech tasks. The method demonstrates robust cross-task generalization when trained on two tasks and evaluated on the third, achieving near-attention-based computational efficiency while delivering superior explanation quality compared to both attention-based and model-agnostic approaches.

## Method Summary
ExpNet learns to generate token-level explanations by treating attention pattern interpretation as a binary classification problem. For each token, it extracts a 24-dimensional feature vector consisting of attention weights from 12 heads in both directions (CLS→token and token→CLS) from BERT's final layer. These features are fed into a two-layer MLP that outputs a probability indicating token importance. The network is trained on human-annotated rationales using focal loss to handle class imbalance, and inference applies a 0.5 threshold with fallback to the highest-scoring token if none exceed the threshold.

## Key Results
- ExpNet outperforms thirteen baseline explainers across SST-2, CoLA, and HateXplain with F1-score improvements of 13.9-31.4%
- Cross-task generalization succeeds when training on two tasks and evaluating on the third
- Computational efficiency reaches 13.889 examples/second, achieving 5-70× speedup over LIME/SHAP
- Bidirectional attention features prove essential, with task-to-token alone nearly matching full performance on SST-2

## Why This Works (Mechanism)

### Mechanism 1: Supervised Attention-to-Importance Mapping
A lightweight neural network learns to transform raw attention patterns into human-aligned token importance scores by discovering optimal feature combinations automatically rather than relying on predetermined rules. The network learns which attention configurations correlate with human-judged importance through supervised training on rationale annotations.

### Mechanism 2: Bidirectional Attention Feature Preservation
Preserving both attention directions (CLS→token and token→CLS) from individual heads enables learning of asymmetric importance signals. This allows the network to capture both task-to-token and token-to-task relationships that may carry distinct importance signals.

### Mechanism 3: Cross-Task Generalization via Linguistic Pattern Learning
Training on rationale annotations from multiple tasks enables zero-shot explanation on held-out tasks because attention heads capture task-agnostic linguistic phenomena. The leave-one-task-out protocol demonstrates that ExpNet learns generalizable explanation patterns rather than task-specific heuristics.

## Foundational Learning

- **Self-attention mechanism in transformers**: Understanding what attention weights represent is prerequisite to interpreting the method's logic. *Quick check: Can you explain why attention weight from token A to token B differs from B to A, and what each direction might signify?*
- **Focal loss for class imbalance**: Most tokens are unimportant, so standard cross-entropy would bias toward predicting "not important." *Quick check: Given focal loss L = -α(1-pt)^γ log(pt), how does increasing γ affect learning on easy vs. hard examples?*
- **Token-level vs. word-level alignment**: BERT tokenizes into subwords while human annotations are word-level; misalignment can introduce label noise. *Quick check: If "unhappy" tokenizes to ["un", "##happy"] and only "un" receives high attention, how should the word-level importance be computed?*

## Architecture Onboarding

- **Component map**: Input Text → BERT-base (frozen) → Layer 12 Attention Heads → 24-dim feature vector per token → ExpNet MLP → Binary importance prediction per token
- **Critical path**: 1) Fine-tune BERT classifier on target task, 2) Extract attention from final layer only, 3) Use CLS token as aggregation point for both directions, 4) Apply 0.5 threshold with fallback to highest-scoring token
- **Design tradeoffs**: Attention-only vs. gradient methods (5-70× speedup vs. potential signal loss), single layer vs. multi-layer aggregation (complexity reduction vs. possible information loss), training on correct predictions only (reduced noise vs. learned "successful reasoning" patterns only)
- **Failure signatures**: All tokens receiving similar importance scores (check focal loss settings), poor cross-task transfer (examine task diversity), empty explanations at inference (check threshold fallback), high variance across runs (small training data)
- **First 3 experiments**: 1) Train and test ExpNet on HateXplain to establish upper bound, 2) Compare full 24-dim features vs. directional ablations, 3) Measure throughput vs. LIME/SHAP to validate efficiency claims

## Open Questions the Paper Calls Out
The paper explicitly identifies extending the framework to decoder-only transformers (e.g., GPT-style LLMs) as future work to enable efficient, interpretable attributions in modern large language models.

## Limitations
- Trained exclusively on correct predictions, limiting reliability for explaining model errors
- Operates at token level without capturing phrase-level semantics for multi-word expressions
- Relies on human rationales that may not align with actual model reasoning patterns

## Confidence

**High confidence**: Superior performance vs. attention aggregation baselines, computational efficiency claims, basic supervised learning methodology

**Medium confidence**: Cross-task generalization results, bidirectional attention feature necessity, performance vs. model-agnostic methods

**Low confidence**: Absolute F1-score values, generalizability to other architectures, performance on diverse tasks

## Next Checks

1. **Architecture generalization test**: Evaluate ExpNet on a fourth, distinctly different task (e.g., medical text classification) to assess cross-task transfer beyond the original three tasks

2. **Attention layer sensitivity analysis**: Train ExpNet using attention patterns from different BERT layers (1-12) to determine if layer-12 is optimal or if multi-layer aggregation would improve performance

3. **Model-agnostic comparison benchmark**: Implement and compare against gradient-based methods (Integrated Gradients, LIME, SHAP) on identical hardware to verify the claimed 5-70× computational efficiency advantage while controlling for implementation differences