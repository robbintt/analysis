---
ver: rpa2
title: 'HausaNLP at SemEval-2025 Task 3: Towards a Fine-Grained Model-Aware Hallucination
  Detection'
arxiv_id: '2503.19650'
source_url: https://arxiv.org/abs/2503.19650
tags:
- task
- hallucination
- hallucinations
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of detecting hallucinations and
  overgeneration errors in LLM-generated text across multiple languages. The authors
  developed a model-aware approach using natural language inference and fine-tuned
  a ModernBERT model on a synthetic dataset of 400 samples.
---

# HausaNLP at SemEval-2025 Task 3: Towards a Fine-Grained Model-Aware Hallucination Detection

## Quick Facts
- arXiv ID: 2503.19650
- Source URL: https://arxiv.org/abs/2503.19650
- Reference count: 8
- Primary result: Achieved IoU score of 0.032 and correlation score of 0.422 on English hallucination detection task

## Executive Summary
This paper addresses the challenge of detecting hallucinations and overgeneration errors in LLM-generated text across multiple languages. The authors developed a model-aware approach using natural language inference and fine-tuned a ModernBERT model on a synthetic dataset of 400 samples. Their system achieved moderate positive correlation between model confidence and hallucination presence, though precise span localization remains challenging. These results demonstrate the inherent complexity of hallucination detection and provide a foundation for future improvements in ensuring the reliability of LLM outputs.

## Method Summary
The authors developed a model-aware hallucination detection system that analyzes token-level logits and hidden layer activations from LLM outputs. They fine-tuned a ModernBERT model using a synthetic dataset of 400 samples generated by ChatGPT, following Mu-SHROOM annotation guidelines. The approach leverages model-internal signals (hidden layer activations and logits) to compute character-level hallucination probabilities. Training techniques included cosine learning rate scheduling, mixed precision (FP16/FP32), and gradient clipping. The model outputs both hard labels and soft confidence scores for each token, enabling dual evaluation metrics (IoU for span accuracy and correlation for confidence calibration).

## Key Results
- IoU score of 0.032 indicates significant challenges in precise hallucination span localization
- Correlation score of 0.422 shows moderate positive relationship between model confidence and actual hallucination presence
- Precision of 0.49 and recall of 0.54 suggest slight false positive bias in hallucination predictions
- Ranked 42nd on IoU metric but 24th on correlation metric, indicating better performance in confidence estimation than boundary detection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic training data generated by LLMs can substitute for scarce labeled hallucination data in fine-tuning detection models.
- Mechanism: ChatGPT-generated samples adhering to annotation guidelines expose the model to diverse hallucination patterns, enabling generalization despite limited real training data.
- Core assumption: Synthetic hallucinations produced by one LLM share structural and semantic characteristics with hallucinations from other LLMs.
- Evidence anchors:
  - [abstract]: "fine-tuned a ModernBERT model using a synthetic dataset of 400 samples"
  - [section 4.1]: "Borra et al. (2024) have shown the success of using synthetic data in finetuning models for hallucination detection in LLMs"
  - [corpus]: Weak direct evidence—corpus papers use varied approaches (RAG pipelines, weak labeling, uncertainty-based methods) with limited synthetic data focus.
- Break condition: If synthetic hallucinations systematically differ in distribution from target LLM hallucinations, fine-tuning will not transfer.

### Mechanism 2
- Claim: Model-internal signals (hidden layer activations and logits) carry information about hallucination presence that can be extracted via probing.
- Mechanism: The model-aware approach analyzes token-level logits provided with LLM outputs, computing character-level hallucination probabilities rather than relying solely on surface text features.
- Core assumption: Hallucination correlates with measurable uncertainty patterns in the generating model's internal states.
- Evidence anchors:
  - [abstract]: "moderately positive correlation between model confidence scores and actual presence of hallucinations" (r = 0.422)
  - [section 3]: "Using vector values of hidden layers for hallucination detection was proposed in a method called Statement Accuracy Prediction, based on Language Model Activations (SAPLMA)"
  - [corpus]: keepitsimple paper (arXiv:2505.17485) specifically targets "LLM-Uncertainty based Approach" for this task, supporting the uncertainty-hallucination link.
- Break condition: If hallucinations arise from confident but wrong parametric knowledge, uncertainty-based detection will systematically miss these cases.

### Mechanism 3
- Claim: Long-context encoder models improve fine-grained span detection by maintaining broader contextual information during token classification.
- Mechanism: ModernBERT's extended context window allows the model to consider surrounding text when classifying each token, which is necessary because hallucination boundaries are context-dependent.
- Core assumption: Hallucination span boundaries can be determined from local and global context within the output text.
- Evidence anchors:
  - [section 3]: "ModernBERT... offers significant improvements in handling long-context inputs, computational efficiency, and robustness in token classification tasks"
  - [section 3]: "Hallucinations often manifest subtly, relying on context, making pinpointing their exact boundaries formidable"
  - [corpus]: UCSC paper (arXiv:2505.03030) emphasizes context optimization for detection, supporting context-dependence.
- Break condition: If hallucination detection requires external knowledge verification (not just context), this approach will hit a ceiling.

## Foundational Learning

- Concept: **Token Classification / Span Labeling**
  - Why needed here: The task requires identifying specific character spans constituting hallucinations, not just binary detection at document level.
  - Quick check question: Can you explain how BIO tagging or span-based labeling differs from sequence classification?

- Concept: **Intersection over Union (IoU) for Sequence Evaluation**
  - Why needed here: IoU measures overlap between predicted and ground-truth hallucination spans; understanding why IoU of 0.032 is low requires grasping this metric.
  - Quick check question: Given a predicted span [5:15] and ground truth [8:20], what is the IoU?

- Concept: **Model-Aware vs. Model-Agnostic Detection**
  - Why needed here: This submission uses model-aware detection (accessing logits/tokens), distinguishing it from approaches that only analyze final text.
  - Quick check question: What additional information does a model-aware approach have access to compared to a model-agnostic one?

## Architecture Onboarding

- Component map:
  - LLM output (text, tokens, logits) -> HuggingFace AutoTokenizer -> ModernBERT backbone -> Token classifier -> Hard labels + soft confidence scores

- Critical path:
  1. Synthetic data generation -> 2. Tokenization with span-aware labels -> 3. ModernBERT fine-tuning -> 4. Dual output generation (hard/soft labels) -> 5. IoU and correlation evaluation

- Design tradeoffs:
  - Single-language (English) focus vs. multilingual capability—traded breadth for depth due to time constraints
  - Synthetic data (400 samples) vs. real annotations—traded authenticity for availability
  - IoU (0.032) sacrificed for correlation (0.422)—model better at confidence calibration than precise boundary detection

- Failure signatures:
  - Low IoU (0.032) with moderate correlation (0.422): Model detects hallucination presence but struggles with exact boundaries
  - Precision 0.49 / Recall 0.54: Slight false positive bias; model over-predicts hallucination spans
  - Ranking 42nd on IoU vs. 24th on correlation: System architecture favors confidence estimation over localization

- First 3 experiments:
  1. Ablate synthetic data size: Test whether 400 samples is sufficient by training on 200, 400, 800 synthetic samples and measuring IoU/correlation curves.
  2. Replace ModernBERT with a standard BERT baseline: Isolate the contribution of long-context handling to the 0.422 correlation.
  3. Ensemble with uncertainty-based method from corpus (e.g., keepitsimple approach): Test whether combining model-aware logits with uncertainty features improves IoU beyond 0.032.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can multilingual hallucination detection models achieve performance comparable to monolingual approaches, and how do cross-lingual transfer patterns manifest in span-level detection?
- Basis in paper: [explicit] The authors state they "used English language only and this is due to time constraint," despite the shared task involving 14 languages, leaving the multilingual aspect unexplored.
- Why unresolved: The team focused resources on English only, and cross-lingual hallucination detection introduces complexities in tokenization, syntax, and semantic boundary alignment.
- What evidence would resolve it: Comparative experiments across multiple languages with consistent evaluation metrics (IoU and correlation scores), plus analysis of language-specific error patterns.

### Open Question 2
- Question: How does synthetic training data scale and quality impact hallucination detection performance, particularly for fine-grained span detection?
- Basis in paper: [inferred] The authors generated only 400 synthetic samples using ChatGPT, and achieved modest IoU of 0.032. The relationship between dataset characteristics and boundary detection accuracy remains unclear.
- Why unresolved: No ablation studies on synthetic data size or quality were conducted, and the paper does not analyze potential distributional gaps between synthetic and real hallucination patterns.
- What evidence would resolve it: Systematic experiments varying synthetic dataset size, analysis of annotation quality in synthetic data, and comparison with human-annotated datasets.

### Open Question 3
- Question: What model architectures or training paradigms can improve exact boundary detection for hallucinated spans beyond what token classification approaches achieve?
- Basis in paper: [inferred] The low IoU score of 0.032 versus the moderate correlation of 0.422 suggests the model can detect hallucination presence better than their exact boundaries.
- Why unresolved: The paper frames hallucinations as "subtle" and "context-dependent," but does not explore architectural innovations specifically targeting span boundary precision.
- What evidence would resolve it: Comparative studies of span prediction architectures (e.g., span-based transformers, sequence labeling with CRF layers) against token classification baselines on the same task.

### Open Question 4
- Question: How effectively can hidden layer activation probing (SAPLMA-style approaches) supplement or replace token-level confidence for hallucination detection?
- Basis in paper: [explicit] The authors reference SAPLMA as "a probing technique that utilises a feedforward neural network trained on activation values of the hidden layers of LLM" but do not implement or compare against it.
- Why unresolved: The paper mentions this approach in the model-aware methodology section but relies on ModernBERT fine-tuning instead, leaving internal state analysis unexplored.
- What evidence would resolve it: Direct comparison of activation-based probing versus fine-tuning approaches on identical test sets, with analysis of complementary strengths.

## Limitations
- Low IoU score (0.032) indicates fundamental limitations in precise hallucination span localization, despite moderate correlation (0.422) for presence detection
- Synthetic dataset of only 400 samples may not capture sufficient hallucination diversity for robust generalization
- Single-language focus (English) limits applicability to the multilingual nature of the shared task
- Uncertainty-based approach may systematically miss confident-but-wrong hallucinations (parametric errors)

## Confidence
- **Medium Confidence**: The moderate positive correlation (0.422) between model confidence and hallucination presence is empirically supported by the reported results. The correlation is statistically present but weak enough that we should be cautious about overclaiming the approach's effectiveness.
- **Low Confidence**: The IoU score of 0.032 combined with the moderate correlation suggests a fundamental architectural or training data limitation. The model detects "something is wrong" but cannot pinpoint where. This disconnect between detection and localization is concerning and not fully explained.
- **Medium Confidence**: The claim that synthetic data can substitute for scarce real annotations is supported by the paper's results, but the limited scale (400 samples) and lack of comparison to real-labeled data means we cannot assess the ceiling of this approach.

## Next Checks
1. **Ablation on synthetic data quantity**: Train identical models on 200, 400, and 800 synthetic samples to establish the relationship between training data volume and both correlation (0.422 target) and IoU (0.032 target). This will reveal whether the current 400-sample dataset is simply too small.

2. **External knowledge verification baseline**: Implement a simple RAG-based hallucination detection approach (retrieving relevant facts and checking consistency) and compare its IoU and correlation scores against the model-aware approach. This will test whether context-based methods can outperform uncertainty-based ones for span localization.

3. **Uncertainty-hallucination correlation analysis**: Using the validation set, compute per-token uncertainty scores (entropy of logit distributions) and correlate them with ground truth hallucination labels. This will directly test whether confident hallucinations (parametric errors) are systematically missed, as suggested by the moderate correlation score.