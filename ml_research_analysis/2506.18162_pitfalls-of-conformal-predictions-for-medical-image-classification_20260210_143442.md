---
ver: rpa2
title: Pitfalls of Conformal Predictions for Medical Image Classification
arxiv_id: '2506.18162'
source_url: https://arxiv.org/abs/2506.18162
tags:
- predictions
- conformal
- coverage
- classification
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Conformal predictions offer provable calibration guarantees but
  face significant limitations in medical image classification tasks. The authors
  demonstrate through dermatology and histopathology experiments that CP fails under
  distributional shifts in inputs and labels, and cannot ensure coverage for individual
  classes or patient subgroups.
---

# Pitfalls of Conformal Predictions for Medical Image Classification

## Quick Facts
- **arXiv ID**: 2506.18162
- **Source URL**: https://arxiv.org/abs/2506.18162
- **Reference count**: 27
- **Primary result**: CP fails under distributional shifts and is unreliable in few-class medical settings without strict data conditions.

## Executive Summary
Conformal predictions offer provable calibration guarantees but face significant limitations in medical image classification tasks. The authors demonstrate through dermatology and histopathology experiments that CP fails under distributional shifts in inputs and labels, and cannot ensure coverage for individual classes or patient subgroups. CP also cannot guarantee higher accuracy for selective classification—using only singleton prediction sets—and lacks practical value in binary or few-class medical tasks where prediction sets often become uninformative. While CP provides marginal coverage guarantees under exchangeability assumptions, violations due to domain or label distribution shifts break these guarantees unless recalibration is performed with new data. In summary, CP is unreliable for uncertainty estimation in safety-critical medical domains unless strict data conditions are met, and alternative frameworks should be considered for selective classification and few-class settings.

## Method Summary
The authors evaluate conformal prediction limitations using two medical image datasets: HAM10K (7-class skin lesions) and CAMELYON17 (binary histopathology). They train 5× ResNet-34 models per dataset using Adaptive Prediction Sets (APS) conformal algorithm. Experiments test coverage under marginal, class-conditional, and set-size conditions, with domain shift evaluated by training on CAMELYON17 centers 1,3,5 and testing on centers 2,4. Label distribution shifts are simulated by resampling test labels. Coverage curves and selective accuracy are measured across multiple calibration samplings.

## Key Results
- CP fails under domain shift: coverage drops significantly when test data comes from different clinical centers
- CP provides no conditional coverage guarantees: minority classes (e.g., melanoma) show severe under-coverage
- CP has limited practical value in binary settings: prediction sets often become uninformative by containing all classes
- CP cannot guarantee higher accuracy for selective classification: using only singleton prediction sets does not improve accuracy

## Why This Works (Mechanism)
None provided in source material.

## Foundational Learning
- **Conformal Prediction**: A framework providing statistical guarantees on prediction set coverage under exchangeability assumptions. Needed to understand the theoretical basis and limitations of CP in medical settings.
- **Distributional Shift**: When test data differs from training data in input features or label distributions. Quick check: Compare training vs. test set statistics (e.g., center distributions, label frequencies).
- **Exchangeability Assumption**: The requirement that data points are identically distributed and order-independent. Violation occurs with domain/label shifts, breaking CP coverage guarantees.
- **Selective Classification**: Using only predictions with high confidence (singleton sets) to improve accuracy. CP does not guarantee accuracy improvement here.
- **Marginal vs. Conditional Coverage**: Marginal coverage is an average guarantee; conditional coverage requires per-subgroup guarantees. CP only provides marginal guarantees.

## Architecture Onboarding
- **Component Map**: ResNet-34 models -> APS conformal algorithm -> Calibration sets -> Coverage evaluation
- **Critical Path**: Model training → Conformal score computation → Prediction set generation → Coverage measurement
- **Design Tradeoffs**: Strict data conditions for coverage vs. practical utility; resource allocation between calibration and training in data-scarce settings.
- **Failure Signatures**: Coverage violation under domain shift, under-coverage for minority classes, uninformative prediction sets in binary tasks.
- **First Experiments**: 1) Measure marginal coverage on ID data; 2) Test coverage under domain shift; 3) Evaluate per-class coverage for minority classes.

## Open Questions the Paper Calls Out
### Open Question 1
**Question**: Should scarce data from protected subgroups be prioritized for calibration to ensure conditional coverage or for training to improve the base model?
**Basis in paper**: [explicit] The authors note that addressing fairness requires separate calibration datasets for every attribute combination, raising a debate about whether this data would be better utilized to improve the classifier instead.
**Why unresolved**: It involves a resource allocation trade-off between achieving statistical fairness (subgroup coverage) and maximizing overall predictive performance in data-scarce medical settings.
**What evidence would resolve it**: Empirical analysis comparing the marginal gains in subgroup coverage versus overall accuracy when allocating fixed datasets to calibration versus training.

### Open Question 2
**Question**: How can conformal prediction be adapted to remain informative in binary or few-class medical classification tasks?
**Basis in paper**: [inferred] The paper demonstrates that CP has "limited practical value" in binary settings because prediction sets often become uninformative (e.g., containing both classes), yet offers no solution for this structural limitation.
**Why unresolved**: The low resolution of set-based uncertainty in small label spaces often forces a choice between uninformative sets (containing all classes) and high error rates.
**What evidence would resolve it**: A modified CP algorithm that maintains valid coverage while significantly reducing the proportion of uninformative, full-class prediction sets in binary medical tasks.

### Open Question 3
**Question**: Can coverage guarantees be preserved under label distribution shifts without requiring expensive recalibration on new data?
**Basis in paper**: [inferred] The authors show that CP fails under label shift and requires recalibration, but identify the detection of such shifts as a pitfall that is "harder to control and observe" than input shifts.
**Why unresolved**: Standard CP relies on the exchangeability assumption, and label distribution shifts (common in varying clinical populations) break this link in ways current methods cannot dynamically adjust to.
**What evidence would resolve it**: A method that robustly estimates unknown test-time label prevalence to adjust prediction sets dynamically, preserving coverage without new labeled data.

## Limitations
- CP requires strict exchangeability assumptions that are often violated in medical practice due to domain and label shifts
- CP provides only marginal coverage guarantees, failing to ensure reliability for individual classes or subgroups
- CP has limited practical value in binary or few-class medical tasks where prediction sets become uninformative

## Confidence
- **Experimental Reproducibility**: High - Clear datasets, methods, and metrics specified
- **Generalizability**: Medium - Results based on two datasets and one CP method (APS)
- **Theoretical Claims**: High - Well-established properties of CP under exchangeability

## Next Checks
1. **Hyperparameter Verification**: Reconstruct training details from [5] (or contact authors) to ensure exact matching of ResNet-34 training configurations
2. **Label Shift Resampling**: Clarify and document the exact resampling ratios and class mappings used in the label shift experiments
3. **Alternative CP Methods**: Repeat key experiments (domain shift, label shift) using other CP methods (e.g., split conformal, jackknife+) to assess if failures are method-specific