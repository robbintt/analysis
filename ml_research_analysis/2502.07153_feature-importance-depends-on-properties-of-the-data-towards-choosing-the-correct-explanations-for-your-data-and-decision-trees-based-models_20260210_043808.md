---
ver: rpa2
title: 'Feature Importance Depends on Properties of the Data: Towards Choosing the
  Correct Explanations for Your Data and Decision Trees based Models'
arxiv_id: '2502.07153'
source_url: https://arxiv.org/abs/2502.07153
tags:
- feature
- importance
- methods
- datasets
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the reliability of local explanation methods
  for decision tree-based models, focusing on feature importance estimates. The authors
  synthesize datasets with varying properties (noise, correlation, class imbalance)
  and compare six explainability methods (LIME, LSurro, Kshap, Sshap, Tshap, TI) against
  ground truth importance values.
---

# Feature Importance Depends on Properties of the Data: Towards Choosing the Correct Explanations for Your Data and Decision Trees based Models

## Quick Facts
- **arXiv ID**: 2502.07153
- **Source URL**: https://arxiv.org/abs/2502.07153
- **Reference count**: 40
- **Primary result**: Feature importance attribution methods show varying reliability across different data properties, with SHAP explainers being consistent among themselves but misaligned with ground truth, while LIME and LSurro are more robust to noise but overestimate unimportant features

## Executive Summary
This paper investigates the reliability of local explanation methods for decision tree-based models, focusing on how data properties affect feature importance estimates. The authors conduct systematic experiments using synthetic datasets with controlled noise, correlation, and class imbalance, comparing six explainability methods (LIME, LSurro, Kshap, Sshap, Tshap, TI) against ground truth importance values. The study reveals that feature importance attribution is significantly influenced by data characteristics, model type, and explanation method assumptions, leading to specific recommendations for method selection based on dataset properties.

## Method Summary
The authors synthesize datasets with varying properties (noise, correlation, class imbalance) and compare six explainability methods: LIME, LSurro, Kshap, Sshap, Tshap, and TI. They evaluate these methods using metrics including mean consistency, mean stability, and compactness. The study uses both synthetic and real-world datasets (Heart Diagnosis, Cervical Cancer, Adult Income, German Credit Risk) to assess feature importance attribution reliability. Ground truth importance values are established through controlled synthetic data generation, allowing direct comparison of explanation methods against known feature importance patterns.

## Key Results
- SHAP explainers (Kshap, Sshap, Tshap) produce highly consistent explanations among themselves but show poor alignment with ground truth importance
- LSurro and LIME are less affected by noise and feature correlation but tend to overestimate unimportant features
- Tree Interpreter (TI) is particularly sensitive to noise due to its feature and noise contribution decomposition
- Across datasets, SHAP explainers and TI share top-10 important features, while LSurro explains 100% of model output with only 5 features in some cases
- Feature importance attribution is significantly affected by data properties, model type, and explanation method assumptions

## Why This Works (Mechanism)
The mechanism underlying this work is the systematic investigation of how data properties (noise, correlation, class imbalance) affect the reliability of different feature importance attribution methods. By controlling these properties in synthetic datasets and comparing against ground truth, the authors establish that explanation method performance varies significantly based on data characteristics. The consistency among SHAP variants and their misalignment with ground truth suggests these methods may capture similar but potentially incorrect feature importance patterns. LIME and LSurro's robustness to noise but overestimation of unimportant features indicates their explanations may be more stable but less accurate. The sensitivity of TI to noise demonstrates how method-specific assumptions about feature contributions can amplify noise effects.

## Foundational Learning
The foundational learning aspect involves establishing ground truth feature importance values through controlled synthetic data generation, enabling systematic comparison of explanation methods. This approach reveals that explanation method reliability depends on both data properties and method-specific assumptions about feature contributions. The study demonstrates that while some methods (SHAP variants) show high internal consistency, this consistency does not guarantee alignment with true feature importance. The findings suggest that explanation method selection should be guided by data characteristics rather than assumed universal superiority, providing a framework for choosing appropriate explainability methods based on specific dataset properties.

## Architecture Onboarding
This paper focuses on decision tree-based models and their explainability methods, but does not explicitly detail architectural considerations for implementation. The experiments involve various tree ensemble configurations (Random Forest, XGBoost, AdaBoost) and different explanation methods that interact with these architectures. Key architectural considerations would include how explanation methods interface with tree-based models, the computational complexity of different explainability approaches, and how model architecture choices (depth, number of trees, boosting parameters) affect explanation reliability. The study suggests that architectural choices impact explanation quality, particularly regarding noise sensitivity and correlation handling.

## Open Questions the Paper Calls Out
The paper raises several open questions regarding the generalizability of its findings beyond tree-based models, the temporal stability of feature importance attributions, and the need for evaluation on larger, more complex real-world datasets. It questions whether the observed patterns of explanation method reliability would hold for neural networks or other model types, and how explanation methods perform when dataset properties change over time. The study also suggests investigating the trade-off between explanation consistency and accuracy, and developing principled approaches for selecting explanation methods based on dataset characteristics rather than treating all methods as universally applicable.

## Limitations
- Synthetic dataset approach may not fully capture real-world data complexity
- Focus on decision tree ensembles limits generalizability to other model types
- Evaluates feature importance at instance level rather than considering global importance patterns
- Ground truth establishment through synthetic data generation may not reflect true feature importance in complex real-world scenarios
- Limited exploration of how model hyperparameters affect explanation reliability
- Does not address computational efficiency differences between explanation methods
- Limited temporal analysis of feature importance stability across training iterations

## Confidence
- **High confidence**: Comparative analysis of different explanation methods and their sensitivity to noise and correlation
- **Medium confidence**: Generalizability of findings to non-tree-based models and more complex real-world scenarios
- **Medium confidence**: Ranking of explanation methods based on chosen metrics (consistency, stability, compactness)

## Next Checks
1. Validate findings across diverse model architectures (neural networks, random forests with different parameters) to assess method robustness beyond tree-based models
2. Conduct experiments on larger, more complex real-world datasets with known feature importance patterns to test method reliability in practical scenarios
3. Investigate temporal stability of feature importance attributions across different training epochs and data subsets to assess method consistency over time
4. Explore the impact of model hyperparameter choices (tree depth, number of estimators, learning rate) on explanation method reliability
5. Develop and validate principled approaches for selecting explanation methods based on dataset characteristics
6. Compare computational efficiency and scalability of different explanation methods for large-scale applications