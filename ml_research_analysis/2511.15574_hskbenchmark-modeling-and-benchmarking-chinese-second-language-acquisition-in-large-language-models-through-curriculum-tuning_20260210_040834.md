---
ver: rpa2
title: 'HSKBenchmark: Modeling and Benchmarking Chinese Second Language Acquisition
  in Large Language Models through Curriculum Tuning'
arxiv_id: '2511.15574'
source_url: https://arxiv.org/abs/2511.15574
tags:
- language
- chinese
- llms
- grammar
- writing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HSKBenchmark, the first benchmark for staged
  modeling and writing assessment of LLMs in Chinese second language acquisition.
  It covers HSK levels 3-6 and includes 6.76M tokens of authentic textbooks, 16K synthetic
  instruction samples, 30 test topics, and a linguistically-grounded evaluation system.
---

# HSKBenchmark: Modeling and Benchmarking Chinese Second Language Acquisition in Large Language Models through Curriculum Tuning

## Quick Facts
- arXiv ID: 2511.15574
- Source URL: https://arxiv.org/abs/2511.15574
- Reference count: 40
- Primary result: First benchmark for staged modeling of Chinese second language acquisition in LLMs, achieving writing performance on par with advanced human learners

## Executive Summary
HSKBenchmark introduces a comprehensive framework for modeling Chinese second language acquisition in large language models through curriculum tuning. The benchmark covers HSK levels 3-6 with 6.76M tokens from authentic textbooks, 16K synthetic instruction samples, and 30 test topics. A linguistically-grounded evaluation system is developed alongside HSKAgent, an automated evaluator fine-tuned on 10K learner compositions. The curriculum-tuning framework trains models progressively from beginner to advanced levels, simulating human acquisition trajectories. Experimental results demonstrate that fine-tuned LLMs achieve writing performance comparable to advanced human learners and exhibit human-like acquisition characteristics.

## Method Summary
The study develops a staged benchmark covering HSK levels 3-6, incorporating authentic textbook data, synthetic instruction samples, and test topics. The curriculum-tuning framework implements progressive training that mirrors human language acquisition patterns. HSKAgent is created as an automated evaluator through fine-tuning on 10K authentic learner compositions. The evaluation system is linguistically-grounded and designed for dynamic writing assessment. Models are trained and evaluated at each stage to track progression and compare performance against human learners.

## Key Results
- Fine-tuned LLMs achieve writing performance on par with advanced human learners
- Curriculum-tuning successfully simulates human acquisition trajectories
- Benchmark effectively supports Chinese SLA modeling with reliable dynamic writing assessment
- Models exhibit human-like acquisition characteristics through progressive training

## Why This Works (Mechanism)
The effectiveness stems from the staged curriculum-tuning approach that mirrors natural language acquisition patterns. By progressively training models from beginner to advanced levels using authentic learning materials, the framework captures the incremental nature of language learning. The linguistically-grounded evaluation system ensures assessments align with established SLA principles, while the large-scale training data provides sufficient diversity for robust learning. The automated evaluator (HSKAgent) enables consistent, scalable assessment that would be impractical with human raters alone.

## Foundational Learning
- HSK proficiency levels (3-6): Why needed - provides standardized framework for measuring language progression; Quick check - verify alignment with official HSK scoring criteria
- Curriculum-based language acquisition: Why needed - ensures learning follows pedagogically sound progression; Quick check - validate progression matches documented SLA research
- Automated essay scoring: Why needed - enables scalable, consistent evaluation across large datasets; Quick check - test inter-rater reliability between HSKAgent and human evaluators

## Architecture Onboarding

Component map:
HSKBenchmark (authentic textbooks + synthetic instructions + test topics) -> Curriculum-tuning framework -> HSKAgent (automated evaluator) -> Performance assessment

Critical path:
Training progression: Level 3 → Level 4 → Level 5 → Level 6, with evaluation checkpoints at each stage to measure acquisition progress

Design tradeoffs:
- Authentic vs. synthetic data balance: Prioritizing authentic textbooks for quality while using synthetic instructions for quantity
- Progressive vs. parallel training: Choosing staged curriculum to better simulate human learning patterns
- Automated vs. human evaluation: Leveraging HSKAgent for scalability while acknowledging potential bias from training data

Failure signatures:
- Model plateaus at intermediate levels without progressing to advanced proficiency
- Evaluation inconsistencies between HSKAgent and human raters
- Overfitting to synthetic instruction patterns rather than authentic language use

3 first experiments:
1. Baseline evaluation: Test untrained LLM performance on HSK 3-6 writing tasks
2. Stage-by-stage assessment: Measure performance gains after each curriculum level
3. Human comparison: Compare top-performing fine-tuned models against advanced human learners

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark covers only HSK levels 3-6, omitting beginner levels (1-2)
- Synthetic instruction samples (16K) and test topics (30) may lack real-world diversity
- Focus on writing assessment neglects speaking, listening, and cultural competence aspects
- Curriculum-tuning may not perfectly replicate natural human acquisition patterns

## Confidence

| Claim Cluster | Confidence Level |
|---------------|------------------|
| Benchmark construction and comprehensiveness | High |
| Curriculum-tuning effectiveness | Medium |
| Writing performance parity with advanced learners | Medium |
| Generalizability for Chinese SLA modeling | Low |

## Next Checks
1. Cross-validation with independent learner corpora: Test benchmark reliability using authentic learner compositions from multiple sources and institutions
2. Longitudinal acquisition tracking: Implement time-series evaluation to better understand progression patterns
3. Multi-skill assessment expansion: Extend framework to include speaking and listening components for comprehensive SLA evaluation