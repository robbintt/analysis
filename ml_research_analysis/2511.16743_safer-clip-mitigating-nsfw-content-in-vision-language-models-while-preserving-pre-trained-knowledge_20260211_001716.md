---
ver: rpa2
title: 'SafeR-CLIP: Mitigating NSFW Content in Vision-Language Models While Preserving
  Pre-Trained Knowledge'
arxiv_id: '2511.16743'
source_url: https://arxiv.org/abs/2511.16743
tags:
- safe
- unsafe
- safety
- image
- nsfw
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of reducing unsafe content in
  vision-language models like CLIP without degrading their generalization performance.
  The authors propose SafeR-CLIP, a fine-tuning framework that redirects unsafe concepts
  to their semantically closest safe alternatives instead of enforcing rigid mappings.
---

# SafeR-CLIP: Mitigating NSFW Content in Vision-Language Models While Preserving Pre-Trained Knowledge

## Quick Facts
- arXiv ID: 2511.16743
- Source URL: https://arxiv.org/abs/2511.16743
- Authors: Adeel Yousaf; Joseph Fioresi; James Beetham; Amrit Singh Bedi; Mubarak Shah
- Reference count: 28
- One-line primary result: SafeR-CLIP recovers up to 8.0% in zero-shot accuracy over prior methods while maintaining strong safety, improving unsafe-to-safe redirection by up to 44.1%.

## Executive Summary
SafeR-CLIP addresses the challenge of reducing unsafe content in vision-language models like CLIP without degrading their generalization performance. The authors propose a fine-tuning framework that redirects unsafe concepts to their semantically closest safe alternatives instead of enforcing rigid mappings. This proximity-aware realignment is achieved using two novel losses: relative cross-modal redirection and proximity-based alignment. To support robust evaluation, the authors also introduce NSFWCaps, a new benchmark with 1,000 highly-aligned safe-unsafe pairs. SafeR-CLIP recovers up to 8.0% in zero-shot accuracy over prior methods while maintaining strong safety.

## Method Summary
SafeR-CLIP is a fine-tuning framework that redirects unsafe embeddings toward semantically closest safe alternatives while preserving pretrained knowledge. The method uses CLIP ViT-L/14 with LoRA adapters (rank=16) and employs two novel losses: relative cross-modal redirection (which corrects false negatives in contrastive learning) and proximity-based alignment (which selects the best-matching safe target for each unsafe input). Training uses a progressive schedule starting with easy pairs and gradually including harder ones. The method is evaluated on ViSU dataset and a new NSFWCaps benchmark, measuring both zero-shot accuracy and unsafe-to-safe redirection performance.

## Key Results
- Recovers up to 8.0% zero-shot accuracy over prior methods while maintaining strong safety
- Improves unsafe-to-safe redirection by up to 44.1% in retrieval tasks
- Significantly reduces unsafe generations in both text-to-image and image-to-text tasks
- Achieves 60.2% average zero-shot accuracy, recovering from Safe-CLIP's 52.2% but still below original CLIP's 74.3%

## Why This Works (Mechanism)

### Mechanism 1: Proximity-Based Alignment Reduces Representation Drift
Aligning unsafe concepts to their semantically closest safe alternatives preserves pretrained knowledge better than rigid mappings. By computing cosine similarity between unsafe and safe captions in the frozen CLIP embedding space, the method selects the best-matching safe target (top-1). The loss then pulls the unsafe embedding toward this proximal target, minimizing the distance unsafe representations must travel and reducing disruption to global embedding geometry. Core assumption: frozen CLIP embedding space contains meaningful semantic structure where semantically similar concepts are proximal.

### Mechanism 2: Relative Cross-Modal Redirection Corrects False Negatives
Using the original unsafe embedding as the sole hard negative prevents the model from treating other valid safe concepts as negatives. Instead of standard InfoNCE loss that pushes the query away from all in-batch negatives (which may include valid safe examples), this loss maximizes the margin between (unsafe image, unsafe caption) and (unsafe image, safe caption) pairs. The frozen unsafe embedding serves as the negative anchor. Core assumption: in-batch negatives in standard contrastive learning often include semantically similar safe examples, creating false negatives that degrade generalization.

### Mechanism 3: Progressive Training Stabilizes Safety-Utility Trade-off
Introducing unsafe-safe pairs in order of increasing difficulty (easy to hard) stabilizes learning and preserves generalization. Pairs are categorized by cosine similarity between safe and unsafe captions. Training starts with high-similarity (easy) pairs and gradually includes lower-similarity (hard) pairs. This curriculum reduces abrupt shifts in the embedding space. Core assumption: high-similarity pairs are easier to align and cause smaller representation shifts; low-similarity pairs require larger shifts and can destabilize training if introduced too early.

## Foundational Learning

- Concept: Contrastive Learning and InfoNCE Loss
  - Why needed here: The core SafeR-CLIP mechanism redesigns the contrastive objective. Understanding how InfoNCE works (pulling positive pairs together, pushing negative pairs apart) is essential to grasp why relative redirection and proximity alignment are novel.
  - Quick check question: In a standard InfoNCE loss, what is the effect of treating a semantically similar but unlabeled example as a negative?

- Concept: CLIP Embedding Space Geometry
  - Why needed here: The method relies on the geometry of the pretrained CLIP space to define "proximal" safe alternatives and to measure representational shift.
  - Quick check question: Why is cosine similarity a suitable metric for measuring semantic proximity in the CLIP embedding space?

- Concept: Safety-Utility Trade-off in Fine-Tuning
  - Why needed here: The paper's central problem is the degradation of generalization (utility) when fine-tuning for safety. Understanding this tension is key to evaluating the proposed solutions.
  - Quick check question: Why might forcing an unsafe concept to align with a single, fixed safe target harm the model's ability to retrieve other semantically relevant safe content?

## Architecture Onboarding

- Component map: Frozen Reference Encoders (T0, V0) -> Trainable Encoders (T, V) with LoRA -> Proximity-Based Pair Selector (offline) -> Loss Functions (Relative Cross-Modal Redirection + Proximity-Based Uni-Modal Redirection + Preservation Losses) -> Progressive Training Scheduler
- Critical path: The most critical path for understanding performance is the loss calculation: L_total = L_redir (prox-based) + L_pres. The L_redir term is where the novel mechanisms (relative redirection + proximity alignment) act. Incorrect implementation of the relative margin (Eq. 3, 4, 9, 10) or the target selection is the most likely source of failure.
- Design tradeoffs:
  - Proximity vs. Safety: Selecting the closest safe target minimizes drift but may not be the safest target if proximal concepts are still marginally unsafe.
  - Curriculum Rigidity: The 1-epoch easy, 1-epoch medium, remainder hard schedule is a heuristic. More flexible or data-dependent curricula might yield better results.
  - LoRA Rank: The paper uses r=16. A higher rank might allow more effective redirection but increases the risk of drift.
- Failure signatures:
  - Safety Failure (Low Redirection): High NSFW scores in generation or retrieval. Check: Is the relative margin loss correctly implemented? Is the target selection returning the intended safe captions?
  - Utility Failure (Low Zero-Shot Accuracy): Large drop in accuracy on 11-dataset benchmark. Check: Is the preservation loss active? Is the progressive training schedule being followed? Are the LoRA weights drifting too far (check L2 distance)?
  - No Improvement over Baseline: Results similar to Safe-CLIP. Check: Are the losses using the proximity-based targets (ẑt_i, ẑv_i) or the original noisy pairs?
- First 3 experiments:
  1. Loss Ablation Reproduction: Re-run the ablation in Table 5. Train models with only relative redirection, then add proximity alignment, then add progressive training. Verify the reported gains (especially the 8% zero-shot recovery).
  2. Proximity Target Inspection: For a sample of unsafe captions, manually inspect the selected top-1 safe target. Quantify the semantic overlap and ensure it is higher than in the original ViSU pairs. This validates the core assumption of the frozen CLIP space's quality.
  3. Difficulty Distribution Analysis: Plot the cosine similarity distribution of the training pairs used in easy, medium, and hard phases. Verify that the "hard" examples are indeed lower-similarity and thus represent a more challenging alignment task.

## Open Questions the Paper Calls Out

- Can asymmetric encoder adaptation strategies (e.g., applying different fine-tuning objectives or intensities to the vision vs. text encoders) further improve the safety-utility trade-off compared to the symmetric approach? Basis: The conclusion states "Future work may explore asymmetric encoder adaptation and broader real-world deployment of safety-tuned models."
- Is it theoretically possible to close the remaining zero-shot accuracy gap (approx. 14%) between safety-aligned models and the original pretrained CLIP, or is this a fundamental limit of intervention? Basis: Table 6 shows SafeR-CLIP achieves 60.2% average accuracy, recovering significantly from Safe-CLIP (52.2%) but still falling short of the original CLIP (74.3%).
- How robust is the proximity-based alignment mechanism against adversarial attacks specifically designed to manipulate the semantic distance between unsafe inputs and their safe proxies? Basis: The method relies on selecting the "semantically closest safe alternative" (Section: Proximity-Based Alignment). This assumes the frozen reference encoder's similarity search is reliable and cannot be gamed.

## Limitations
- Limited ablation of loss components: While reporting 8.0% zero-shot accuracy recovery, individual contributions of each component to the safety-utility trade-off remain unclear.
- Proximity-based alignment assumption: The method assumes frozen CLIP embedding space contains meaningful semantic structure where semantically similar concepts are proximal, but this assumption is not empirically validated within the paper.
- NSFWCaps benchmark novelty: The new benchmark introduces 1,000 highly-aligned safe-unsafe pairs, but the paper does not provide detailed methodology for creating these pairs.

## Confidence

High confidence: The core claim that SafeR-CLIP improves unsafe-to-safe redirection by up to 44.1% while maintaining or improving zero-shot accuracy is well-supported by experimental results.

Medium confidence: The mechanism explanations (proximity-based alignment, relative cross-modal redirection, progressive training) are logically sound and internally consistent, but rely on assumptions about CLIP embedding space geometry that are not fully validated.

Low confidence: The NSFWCaps benchmark creation methodology and the exact formulation of preservation losses are insufficiently detailed to independently assess their validity.

## Next Checks

1. Loss component ablation validation: Independently implement and train models with each novel component (relative cross-modal redirection, proximity-based alignment, progressive training) in isolation. Verify that each component contributes the reported incremental improvements to zero-shot accuracy and unsafe-to-safe redirection metrics.

2. Proximity target quality inspection: For a stratified sample of unsafe captions across difficulty categories, manually inspect and rate the semantic similarity between each unsafe caption and its selected top-1 safe target. Quantify whether the selected targets are genuinely semantically closest and whether they represent safe alternatives.

3. Embedding space geometry validation: Analyze the CLIP embedding space structure by computing and visualizing the distribution of cosine similarities between safe-safe, unsafe-unsafe, and safe-unsafe pairs. Verify that semantically similar concepts are indeed proximal and that safe and unsafe concepts occupy meaningfully separated regions of the embedding space.