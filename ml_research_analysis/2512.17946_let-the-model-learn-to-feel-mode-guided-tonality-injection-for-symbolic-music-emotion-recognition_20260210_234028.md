---
ver: rpa2
title: 'Let the Model Learn to Feel: Mode-Guided Tonality Injection for Symbolic Music
  Emotion Recognition'
arxiv_id: '2512.17946'
source_url: https://arxiv.org/abs/2512.17946
tags:
- music
- mode
- symbolic
- midibert
- emotion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of capturing the intrinsic reasoning
  behind human emotional responses in symbolic music emotion recognition (SMER) by
  investigating the limitations of large-scale pre-trained models like MIDIBERT in
  encoding mode-emotion associations. To address this gap, the authors propose a Mode-Guided
  Enhancement (MoGE) strategy combined with a Mode-guided Feature-wise Linear Modulation
  Injection (MoFi) framework.
---

# Let the Model Learn to Feel: Mode-Guided Tonality Injection for Symbolic Music Emotion Recognition

## Quick Facts
- arXiv ID: 2512.17946
- Source URL: https://arxiv.org/abs/2512.17946
- Reference count: 13
- Key outcome: Mode-guided injection improves SMER accuracy by 11.8-13.8% over baseline MIDIBERT-Piano

## Executive Summary
This paper addresses the challenge of capturing the intrinsic reasoning behind human emotional responses in symbolic music emotion recognition (SMER) by investigating the limitations of large-scale pre-trained models like MIDIBERT in encoding mode-emotion associations. To address this gap, the authors propose a Mode-Guided Enhancement (MoGE) strategy combined with a Mode-guided Feature-wise Linear Modulation Injection (MoFi) framework. MoFi injects explicit mode knowledge—extracted using the Krumhansl–Kessler algorithm—into the least emotionally relevant layer of MIDIBERT via FiLM, enabling fine-grained modulation of feature representations. Experiments on EMOPIA and VGMIDI datasets show significant performance gains, with accuracy improvements of 11.8% (from 0.634 to 0.752) and 13.8% (from 0.473 to 0.591) over the baseline MIDIBERT-Piano model, respectively, validating the effectiveness of mode-guided modeling in enhancing SMER.

## Method Summary
The paper proposes a Mode-guided Feature-wise Linear Modulation (MoFi) framework to enhance MIDIBERT's emotional reasoning capabilities. The method extracts mode (major/minor) information from MIDI files using the Krumhansl–Kessler algorithm, then injects this explicit tonality knowledge into MIDIBERT's first layer via FiLM conditioning. The FiLM mechanism applies learned scaling (γ) and shifting (β) parameters to modulate feature representations based on mode. The model is fine-tuned on EMOPIA and VGMIDI datasets for 4-class emotion classification (Happy/Sad/Calm/Angry) using Russell's 4Q taxonomy. Training uses early stopping with batch sizes of 16 and 8 for the respective datasets, completing in under 30 minutes on a single RTX 3090.

## Key Results
- MoFi achieves 75.2% accuracy on EMOPIA dataset, representing an 11.8% improvement over baseline MIDIBERT-Piano (63.4%)
- MoFi achieves 59.1% accuracy on VGMIDI dataset, representing a 13.8% improvement over baseline MIDIBERT-Piano (47.3%)
- The mode-guided injection strategy demonstrates consistent performance gains across different music genres and dataset characteristics

## Why This Works (Mechanism)
The paper identifies that large-scale pre-trained models like MIDIBERT may not adequately encode the intrinsic mode-emotion associations that humans intuitively understand. By explicitly injecting mode knowledge through FiLM conditioning, the model gains direct access to tonality information that is strongly correlated with emotional perception. The injection at the first layer allows the mode information to influence subsequent feature transformations throughout the network. This targeted enhancement addresses a specific representational gap in the pre-trained model, enabling more accurate emotional reasoning aligned with human musical understanding.

## Foundational Learning
- Krumhansl-Kessler algorithm: Used to extract mode (major/minor) labels from MIDI files by analyzing pitch distributions. Needed because mode is a strong predictor of emotional perception in music.
- FiLM (Feature-wise Linear Modulation): A conditioning mechanism that applies learned scaling and shifting to feature maps based on auxiliary information. Required to inject mode knowledge into MIDIBERT's representations.
- Compound Word tokenization: MIDI is converted to Bar, Position, Pitch, and Duration events for MIDIBERT processing. Essential for representing symbolic music in transformer-friendly format.
- Russell's 4Q model: A 2D emotion space with valence and arousal dimensions. Used to categorize emotions into Happy/Sad/Calm/Angry for the classification task.
- Early stopping: Training halts when validation performance doesn't improve for 3 consecutive epochs. Prevents overfitting on limited training data.
- Quick check: Verify mode extraction by comparing music21's K-K outputs against known mode labels on a small test set.

## Architecture Onboarding

**Component Map**: MIDI -> CP Tokenization -> Embedding Layer -> FiLM Injection -> First Transformer Layer -> Subsequent Layers -> Classification Head

**Critical Path**: Mode Extraction (music21 K-K) -> FiLM Conditioning (MLP) -> MIDIBERT Fine-tuning (cross-entropy loss)

**Design Tradeoffs**: Injecting mode at the first layer maximizes influence on downstream representations but may disrupt pretrained weights if not carefully initialized. The FiLM mechanism provides flexible conditioning without requiring architectural modifications.

**Failure Signatures**: Mode extraction errors (incorrect major/minor labels), FiLM initialization causing representation collapse, or training instability compared to baseline.

**First Experiments**: 1) Verify Krumhansl-Kessler mode extraction accuracy on test pieces with known modes, 2) Monitor training loss curves comparing baseline vs MoFi for stability, 3) Test different FiLM conditioning architectures (single linear layer vs small MLP).

## Open Questions the Paper Calls Out
None

## Limitations
- The paper does not specify the exact architecture of the FiLM conditioning network (layers, activation functions, hidden dimensions).
- Training hyperparameters including optimizer type, learning rate, weight decay, and learning rate schedule are not provided.
- Implementation details for Compound Word tokenization (vocabulary size, special tokens) remain unspecified.

## Confidence
- Method description: Medium (clear on core mechanism but lacks architectural details)
- Reproducibility: Medium (sufficient for baseline and MoFi implementation but missing key hyperparameters)
- Claims verification: Medium (performance improvements are significant but depend on unspecified implementation choices)

## Next Checks
1. Verify Krumhansl-Kessler mode extraction accuracy by comparing outputs against known mode labels on a small test set.
2. Monitor training stability and compare loss curves between baseline and MoFi models to detect potential representation disruption.
3. Test multiple FiLM conditioning architectures (single linear layer vs. small MLP) to verify robustness of the reported improvements.