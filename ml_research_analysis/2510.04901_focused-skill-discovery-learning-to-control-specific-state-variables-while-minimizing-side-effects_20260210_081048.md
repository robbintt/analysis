---
ver: rpa2
title: 'Focused Skill Discovery: Learning to Control Specific State Variables while
  Minimizing Side Effects'
arxiv_id: '2510.04901'
source_url: https://arxiv.org/abs/2510.04901
tags:
- skill
- skills
- focused
- discovery
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a general method for modifying existing skill\
  \ discovery algorithms to learn focused skills\u2014skills that target and control\
  \ specific state variables. The authors introduce a focused skill reward that combines\
  \ two terms: one encouraging control of target variables and another penalizing\
  \ side effects on non-target variables."
---

# Focused Skill Discovery: Learning to Control Specific State Variables while Minimizing Side Effects

## Quick Facts
- **arXiv ID:** 2510.04901
- **Source URL:** https://arxiv.org/abs/2510.04901
- **Authors:** Jonathan Colaço Carr; Qinyi Sun; Cameron Allen
- **Reference count:** 9
- **Primary result:** Focused skills reach 3× more states during exploration and improve downstream task performance by minimizing side effects

## Executive Summary
This paper introduces focused skill discovery, a method that modifies existing skill discovery algorithms to learn skills that control specific state variables while minimizing changes to non-target variables. The approach adds a side effects penalty to the skill reward function, encouraging skills to affect only their designated target variables. The method is applied to three baseline algorithms (VIC, DIAYN, and LSD) and demonstrates significant improvements in exploration efficiency and downstream task performance, particularly in environments where minimizing side effects is crucial.

## Method Summary
The method modifies existing skill discovery algorithms by adding a side effects penalty to the skill reward function. For each skill targeting a subset of state variables, the focused reward combines the baseline skill reward with a penalty term that increases when non-target state variables change during skill execution. This is implemented by training separate discriminators for each state variable and using a weighted norm to measure changes to non-target variables. The approach is applied to VIC, DIAYN, and LSD algorithms, creating focused variants of each that learn to manipulate specific state variables while preserving others.

## Key Results
- Focused skills reach 3.67× more states during exploration compared to unfocused baselines
- In downstream tasks, focused skills enable agents to solve problems that baseline skills cannot, particularly in environments requiring minimal environmental impact
- The method demonstrates automatic avoidance of unwanted side effects when goals are underspecified, outperforming the DUSDi approach in environments with entangled state variables

## Why This Works (Mechanism)

### Mechanism 1: Side Effects Penalty
- Claim: Explicitly penalizing side effects in the skill reward forces skills to become controllable with respect to specific state variables
- Mechanism: The method modifies a baseline skill discovery algorithm's reward by adding a side effects penalty term (e.g., a weighted norm) to the objective function
- Core assumption: The state space is factored, allowing clear separation of target and non-target variables
- Break condition: State variables are too entangled to be independently controlled or penalized

### Mechanism 2: Modular Skill Library
- Claim: Focused skills create a modular library of behaviors, dramatically improving exploration efficiency
- Mechanism: By training skills that independently affect single state variables, the method creates composable primitives that enable agents to reach more unique states
- Core assumption: Downstream tasks can be solved by a sequence of independent state changes
- Break condition: The environment requires highly coupled actions where no useful skill affects only one variable

### Mechanism 3: Reward Misspecification Robustness
- Claim: Pre-training with a side effects penalty provides robustness against reward misspecification in downstream tasks
- Mechanism: Focused skills, having learned to avoid unnecessary changes, will naturally seek a path to the goal that preserves the state of non-rewarded variables
- Core assumption: The true task objective involves minimizing changes to the environment outside of explicitly rewarded goals
- Break condition: The true objective is not captured by the combination of the proxy reward and a "minimize change" prior

## Foundational Learning

- **Concept: Factored Markov Decision Process (MDP)**
  - Why needed here: The entire method relies on the state S being a product of independent variables S₁ x ... x S_N
  - Quick check question: Can you decompose your environment's state observation into distinct, meaningful variables?

- **Concept: Mutual Information (MI) Skill Discovery (e.g., VIC, DIAYN)**
  - Why needed here: The paper applies its method to these algorithms that make skills "distinguishable" by the states they reach
  - Quick check question: Do you understand why maximizing I(Z; S) encourages a skill to visit a unique set of states?

- **Concept: Options Framework**
  - Why needed here: The paper defines skills using the options framework (policy π, initiation set I, termination condition β)
  - Quick check question: What are the three components of an option/skill in the standard framework?

## Architecture Onboarding

- **Component map:** State variables -> Discriminators (one per variable) -> Focused Reward Calculator -> Skill Policies (one per skill) -> Side Effects Penalty Function

- **Critical path:**
  1. Identify State Factors: Manually or automatically decompose the state S into N variables
  2. Initialize Components: Create N discriminators/representation functions and a set of skill policies
  3. Training Loop: For each episode, sample a skill, execute it, and compute the r_focused using the new reward formula

- **Design tradeoffs:**
  - Penalty Strength (λ): Too high, skills become overly conservative; too low, they won't be focused
  - Assumption of Independence: Method assumes variables can be controlled independently
  - Scalability: A discriminator per variable can become expensive in high-dimensional state spaces

- **Failure signatures:**
  - Skills fail to minimize side effects (λ too low): Agent tracks excessive mud in MudWorld
  - Poor exploration if skill assignment doesn't cover all state variables: State coverage AUC similar to baseline
  - Performance degrades in downstream task: "Do not change other variables" prior is incorrect for true task

- **First 3 experiments:**
  1. Ablation on Penalty Strength: Run algorithm on FourRooms with λ = 0, λ = optimal, and λ = very_high
  2. Entanglement Test: Create toy environment where two state variables are perfectly correlated
  3. Downstream Transfer: Train both baseline and focused skills on reward-free task, then fine-tune on downstream task with underspecified reward

## Open Questions the Paper Calls Out

- **Open Question 1:** Can focused skill discovery scale effectively to larger environments and sets of continuous skills?
  - Basis: Authors state "it would be interesting to scale these experiments up to larger environments"
  - Why unresolved: Experiments limited to tabular gridworlds
  - Evidence needed: Successful application in high-dimensional environments using continuous skill latent spaces

- **Open Question 2:** How can the approach be extended to learn or utilize state abstractions rather than relying on pre-defined factored state representations?
  - Basis: Authors hope to extend to "other kinds of state abstractions"
  - Why unresolved: Current method depends on known factorization of state space
  - Evidence needed: Algorithm that learns state factors jointly with focused skills

- **Open Question 3:** Can theoretical guarantees be established to prove that focused skill discovery improves agent capabilities while mitigating reward hacking?
  - Basis: Authors suggest "interesting theoretical territory to explore"
  - Why unresolved: Paper provides empirical evidence but lacks formal theoretical proof
  - Evidence needed: Theoretical framework providing bounds on side effects or reward hacking probability

## Limitations

- The method assumes factored state spaces, restricting applicability to environments where state variables can be cleanly separated
- Performance in highly entangled state spaces remains untested and may be problematic
- The choice of penalty function and its weight (λ) appears critical but is not thoroughly explored across different environments

## Confidence

- **High confidence:** Claims about improved exploration efficiency (3× state coverage) and downstream task performance in tested environments
- **Medium confidence:** Claims about automatic side effect avoidance in underspecified reward settings
- **Low confidence:** Claims about superiority over DUSDi, given limited comparative analysis

## Next Checks

1. Test focused skills in environments with progressively more entangled state variables to identify breaking points
2. Conduct systematic ablation studies on penalty weight λ across different environments to establish robustness
3. Compare focused skills against DUSDi on identical tasks with fair hyperparameter tuning to validate relative performance claims