---
ver: rpa2
title: 'SwiftEval: Developing a Language-Specific Benchmark for LLM-generated Code
  Evaluation'
arxiv_id: '2505.24324'
source_url: https://arxiv.org/abs/2505.24324
tags:
- code
- language
- benchmark
- swift
- programming
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SwiftEval, the first language-specific benchmark
  for evaluating LLM-generated code in Swift. The authors identify significant issues
  with existing multilingual benchmarks (HumanEval-XL, MultiPL-E) when applied to
  Swift, including critical translation errors and lack of support for Swift-specific
  features.
---

# SwiftEval: Developing a Language-Specific Benchmark for LLM-generated Code Evaluation

## Quick Facts
- arXiv ID: 2505.24324
- Source URL: https://arxiv.org/abs/2505.24324
- Reference count: 32
- Primary result: Hand-crafted Swift-specific benchmark reveals significant performance gaps in LLM code generation compared to translated benchmarks

## Executive Summary
This paper introduces SwiftEval, the first language-specific benchmark for evaluating LLM-generated code in Swift. The authors identify significant issues with existing multilingual benchmarks when applied to Swift, including critical translation errors and lack of support for Swift-specific features. Unlike existing approaches that translate Python benchmarks using LLMs, SwiftEval consists of 28 hand-crafted problems designed to leverage Swift's unique features like static typing, protocols, and optionals. The benchmark evaluates 44 popular code LLMs and demonstrates that even small, language-tailored benchmarks can provide more insightful results than large, general-purpose ones.

## Method Summary
The authors created SwiftEval by hand-crafting 28 programming problems specifically designed for Swift, covering features like static typing, protocols, generics, and optionals. Each problem includes a natural language query, code context, entrypoint, and 3-5 unit tests. Models generate 20 completions per problem using temperature=0.2 and top-p=0.95, with pass@1 scores calculated after compilation and unit test execution. The benchmark is evaluated on 44 popular code LLMs using OpenAI API or HuggingFace Transformers, with code compiled using Swift 5.10 on macOS 14.6.1.

## Key Results
- Significant performance drops for models on Swift-specific problems, particularly for smaller models
- Stronger correlation between model size and performance on SwiftEval (0.50) compared to HumanEval (0.30)
- 86% of evaluations failed due to compilation errors, while only 14% failed due to unit test failures
- GPT-4o achieved approximately 88-90% performance, establishing a ceiling for the benchmark

## Why This Works (Mechanism)

### Mechanism 1: Targeted Feature Stress Testing
- **Claim:** If a benchmark explicitly requires language-unique features (e.g., Swift's optionals or static typing), it creates a performance bottleneck for models trained primarily on dynamic languages like Python.
- **Mechanism:** General multilingual benchmarks often use automated translation that defaults to generic programming patterns. By hand-crafting problems that mandate specific constructs—such as Swift's `protocols` or `generics`—the evaluation forces the model to retrieve low-frequency or specialized syntactic knowledge, rather than relying on generalized coding heuristics.
- **Core assumption:** The model has seen some Swift code, but its dominance of Python-centric patterns creates a specific weakness in Swift-unique idioms.
- **Evidence anchors:** [Section III] The authors designed SwiftEval to consider "unique Swift features like static typing, protocols, generics... [which] are not covered by any other benchmark." [Section IV] "Results show significant LLM scores drop for problems requiring language-specific features, most noticeable in the models of smaller sizes."

### Mechanism 2: Contamination Avoidance via Novelty
- **Claim:** Introducing unseen, hand-crafted problems reduces the probability of inflated scores caused by memorization of public benchmarks.
- **Mechanism:** Popular benchmarks like HumanEval are widely present in training datasets (contamination). SwiftEval uses novel prompts and distinct problem structures, forcing the model to synthesize solutions rather than potentially retrieving memorized answers.
- **Core assumption:** Standard benchmarks are sufficiently contaminated to artificially flatten performance differences between large and small models.
- **Evidence anchors:** [Section II] Cites concerns that "pre-training sets... overlap with HumanEval by 8–18%," potentially compromising evaluation due to overfitting. [Section IV] Shows HumanEval has a weak correlation (0.30) between model size and score, whereas SwiftEval has a stronger correlation (0.50), suggesting better differentiation.

### Mechanism 3: Semantic-Translation Alignment
- **Claim:** Hand-crafting problems ensures the "intent" of the problem aligns with the "semantics" of the target language, preventing errors introduced by literal translation.
- **Mechanism:** Automated translation often maps Python concepts 1:1 to Swift, resulting in logical errors (e.g., rounding behaviors) or impossible structures (e.g., dynamic tuples). Hand-crafted benchmarks align the unit test expectations with the language's native standard libraries (e.g., Swift's specific rounding rules).
- **Core assumption:** The drop in scores on other benchmarks is due to flawed evaluation design (translation errors) rather than purely model incompetence.
- **Evidence anchors:** [Section II] Identifies critical issues in other benchmarks, such as test cases assuming "Python's bankers rounding" (rounding to even) which fails in Swift. [Section II] Notes tasks that are "completely unsolvable in Swift because tuples in Swift have fixed sizes," unlike Python.

## Foundational Learning

- **Concept:** Pass@k Metric
  - **Why needed here:** This is the core evaluation statistic used to estimate functional correctness. You must understand that `Pass@1` (generating one correct solution) is harder and more indicative of capability than `Pass@20` (generating 20 and hoping one works).
  - **Quick check question:** If a model generates 20 samples and solves a problem once, how does that impact the Pass@1 estimate compared to a model that solves it 10 times?

- **Concept:** Static vs. Dynamic Typing
  - **Why needed here:** The paper argues that translated benchmarks fail because they ignore Swift's static type system. Understanding that Swift requires compile-time type checks (unlike Python's runtime checks) explains why models fail on "AnyHashable" or generic constraints.
  - **Quick check question:** Why would a Python-to-Swift translation fail if the input type is defined simply as "any object"?

- **Concept:** Language-Specific Idioms (Optionals/Generics)
  - **Why needed here:** Swift relies heavily on `Optionals` (handling null safety) and `Generics`. The paper highlights these as "unique features." Knowing these are distinct from Python's `None` or dynamic typing is required to understand why the benchmark is difficult.
  - **Quick check question:** How does Swift's handling of `nil` (Optionals) differ from Python's `None`, and how might this confuse a model trained mostly on Python?

## Architecture Onboarding

- **Component map:** Problem Set -> Generation Harness -> Compilation (Critical Filter) -> Unit Test Execution -> Score Calculation
- **Critical path:** Prompt Construction -> Model Inference -> **Compilation (Critical Filter)** -> Unit Test Execution -> Score Calculation.
- **Design tradeoffs:** Quantity vs. Quality: The authors chose 28 hand-crafted problems over 164+ translated ones. This increases validity but reduces the granularity of the score (higher variance per problem).
- **Failure signatures:**
  - Compilation Error (86% of failures): The model generates syntax that is valid Python or generic code but invalid Swift (e.g., wrong tuple sizes, type mismatch).
  - Logic/Rounding Errors: The model implements logic correctly but assumes Python behavior (e.g., Banker's rounding) in a Swift context.
- **First 3 experiments:**
  1. **Sanity Check (GPT-4o):** Run the top-tier model (GPT-4o) on SwiftEval to establish a ceiling (approx. 88-90%). If scores are significantly lower, the execution environment is likely misconfigured.
  2. **Translation vs. Hand-crafted Ablation:** Take a specific problem involving "rounding" or "tuples." Run it on both MultiPL-E (translated) and SwiftEval to confirm the "Translation Error" hypothesis.
  3. **Size Correlation Test:** Run a small model (e.g., 1.5B params) vs. a medium model (7B params). Confirm the hypothesis that smaller models show a "Critical Drop" (>30%) on Swift-specific features compared to HumanEval.

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the security of LLM-generated Swift code compare across models, and what vulnerability patterns emerge in language-specific code generation? The authors explicitly state they do not assess security and recommend running test executions in a sandboxed environment.
- **Open Question 2:** To what extent do SwiftEval results generalize as the benchmark scales beyond 28 problems? The authors acknowledge that results may vary slightly when more problems are added.
- **Open Question 3:** Which specific Swift features (protocols, generics, optionals, closures) cause the largest performance drops in smaller models, and what training data gaps explain these weaknesses? The authors plan to create subcategories to understand model strengths and weaknesses.

## Limitations
- Benchmark currently contains only 28 problems, significantly fewer than established benchmarks like HumanEval (164 problems)
- Security aspects of generated code were not assessed, requiring sandboxed execution for safety
- Results may vary slightly when more problems are added to the benchmark

## Confidence
- **High Confidence:** The identification of translation errors in existing benchmarks (Section II) and the compilation failure rate (86%) are empirically verifiable facts about current evaluation practices.
- **Medium Confidence:** The claim that hand-crafted benchmarks better measure model competence relies on the untested assumption that translation errors are the primary source of poor performance on existing benchmarks.
- **Medium Confidence:** The correlation between model size and SwiftEval performance (0.50 vs 0.30) suggests better measurement validity, but this could also reflect sample size effects or problem selection bias.

## Next Checks
1. **Contamination Audit:** Track the publication of new Swift code generation models over 12 months and monitor if SwiftEval scores increase without corresponding architectural improvements, indicating benchmark contamination.
2. **Translation Error Isolation:** Create a controlled experiment where identical logic problems are implemented both as translated benchmarks and hand-crafted Swift versions. Measure the delta in performance to isolate translation error impact.
3. **Cross-Language Transfer Test:** Evaluate whether models that score well on SwiftEval demonstrate improved performance on other Swift-specific tasks (e.g., SwiftUI development) compared to models with similar HumanEval scores but poor SwiftEval performance.