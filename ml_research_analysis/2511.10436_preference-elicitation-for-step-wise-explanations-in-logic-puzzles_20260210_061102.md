---
ver: rpa2
title: Preference Elicitation for Step-Wise Explanations in Logic Puzzles
arxiv_id: '2511.10436'
source_url: https://arxiv.org/abs/2511.10436
tags:
- facts
- explanation
- constraints
- explanations
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MACHOP, a novel interactive preference elicitation
  method for generating step-wise explanations in logic puzzles like Sudoku and Logic-Grid
  problems. The core challenge is to learn user preferences over multiple sub-objectives
  (e.g., number of constraints, facts) that define explanation quality.
---

# Preference Elicitation for Step-Wise Explanations in Logic Puzzles

## Quick Facts
- arXiv ID: 2511.10436
- Source URL: https://arxiv.org/abs/2511.10436
- Reference count: 24
- Key outcome: MACHOP reduces relative regret by up to 80% vs baseline and achieves 70.7% user preference alignment after 30 queries

## Executive Summary
This paper addresses the challenge of learning user preferences for step-wise explanations in logic puzzles like Sudoku and Logic-Grid problems. The core innovation is MACHOP, an interactive preference elicitation method that learns to rank multiple explanation steps based on user-defined quality criteria across several sub-objectives (e.g., number of constraints, facts). By combining non-domination constraints, dynamic normalization, and UCB-based diversification, MACHOP significantly outperforms baseline methods in both simulated and real-user evaluations, demonstrating better alignment with human preferences while maintaining interactive query generation times.

## Method Summary
MACHOP extends Constructive Preference Elicitation by introducing three key components: (1) a non-domination constraint that prevents trivial comparisons by ensuring meaningful trade-offs between explanation candidates, (2) local normalization that stabilizes learning by rescaling features based on recent query pairs rather than cumulative bounds, and (3) an UCB-based weighting scheme that guides diversification toward both important and unexplored objectives. The method operates in an online loop where it generates pairs of explanation steps, collects user preferences, and updates a linear weight vector to predict future preferences. The framework is evaluated on Sudoku and Logic-Grid puzzles using both simulated oracles and real users.

## Key Results
- Up to 80% reduction in relative regret compared to Choice Perceptron baseline
- 70.7% of real-user explanations preferred over baseline after 30 queries
- Consistent performance across both Sudoku and Logic-Grid problem domains
- Favorable runtime-quality trade-off enabling real-time learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic normalization stabilizes learning when sub-objectives vary in scale.
- Mechanism: MACHOP uses local normalization that updates feature bounds based on the most recent query pair, avoiding overestimation that can excessively shrink feature values and ensuring balanced gradient magnitudes during perceptron updates.
- Core assumption: Scale mismatches between sub-objectives are the primary source of learning instability.
- Evidence anchors:
  - [abstract] "because the explanation quality is measured using multiple sub-objectives that can vary a lot in scale, we propose two dynamic normalization techniques to rescale these features and stabilize the learning process."
  - [section] Page 5, "Approximate Nadir Point Normalisation... This approach overestimates the upper bound, which may result in low-scale normalized... Local Normalization... performs best on average."
- Break condition: If feature distributions shift drastically over time, local normalization could become unstable.

### Mechanism 2
- Claim: Non-domination constraints prevent trivial comparisons and improve learning signal.
- Mechanism: By enforcing a disjunctive constraint ensuring at least one objective improves, MACHOP forces genuinely trade-off queries that better reveal user priorities rather than predictable dominance cases.
- Core assumption: Users can reliably express preferences when meaningful trade-offs exist.
- Evidence anchors:
  - [abstract] "query generation strategy that combines non-domination constraints with Upper Confidence Bound (UCB) diversification."
  - [section] Page 6, "adding the disjunctive constraints yields a significantly lower regret in 7 out of 8 setups."
- Break condition: If the Pareto front is sparse, enforcing non-domination may produce very similar alternatives.

### Mechanism 3
- Claim: UCB-based diversification accelerates convergence by balancing exploitation and exploration of objectives.
- Mechanism: MACHOP uses UCB weights that prioritize objectives that are either estimated important (high q_i) or under-explored (low N_i), guiding diversification toward objectives most likely to improve the model.
- Core assumption: Importance of sub-objectives can be approximated by how often improvements correlate with user selection.
- Evidence anchors:
  - [abstract] "query generation strategy that combines non-domination constraints with Upper Confidence Bound (UCB) diversification."
  - [section] Page 7, "UCB weights consistently reduce regret by about 40% for both problems."
- Break condition: If user preferences are highly non-stationary, UCB's historical counts may lag.

## Foundational Learning

### Concept: Minimal Unsatisfiable Subsets (MUS)
- Why needed here: Explanation steps are derived from MUSs; understanding that an MUS is a minimal set of constraints causing unsatisfiability is key to seeing how explanations are constructed.
- Quick check question: Can you explain why removing any single constraint from an MUS makes the set satisfiable?

### Concept: Pareto domination in multi-objective optimization
- Why needed here: The non-domination constraint relies on the concept that one solution dominates another if it is no worse in any objective; recognizing non-dominated pairs is essential.
- Quick check question: Given two candidates with features [3, 10] and [5, 8] (both to minimize), which dominates which, or are they non-dominated?

### Concept: Online vs offline preference elicitation
- Why needed here: The paper contrasts online fact selection (computationally expensive, adaptive) with offline sequences (precomputed, faster); understanding this trade-off helps in deployment.
- Quick check question: What is the main risk of using an offline precomputed sequence for facts in a new problem instance?

## Architecture Onboarding

### Component map:
Instance Selector -> Feature Extractor -> Normalizer -> Query Generator -> User Interaction -> Weight Updater -> Explanation Generator

### Critical path:
Query generation (via OCUS solver) is the bottleneck; offline fact selection reduces this. Weight update is fast but sensitive to normalization and learning rate.

### Design tradeoffs:
- Online fact selection: Better quality, longer wait time
- Offline SES: Faster, reasonable quality
- Random offline: Fast but risk of complex explanations
- Local vs cumulative normalization: Local best for stable distributions; cumulative for drift

### Failure signatures:
- High regret after many queries: Likely over-exploitation (reduce learning rate, check UCB)
- Query generation timeout: Switch to offline SES
- Users often indifferent: Normalize or check feature scaling; may need more diverse features

### First 3 experiments:
1. Baseline replication: Run Choice Perceptron on Sudoku with default normalization, measure regret
2. Ablation on normalization: Compare local, cumulative, and no normalization with non-domination on
3. MACHOP vs baseline: Full MACHOP (local norm + non-domination + UCB) vs Choice Perceptron, track regret over 30 queries

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the preference elicitation framework be extended to learn non-linear utility functions without compromising the tractability of the underlying constraint solver?
- Basis in paper: [explicit] The authors state in the Conclusion that "Future work can explore learning preferences as a non-linear utility function," noting that linear models are currently used for solver support.
- Why unresolved: While non-linear functions may capture complex preferences better, they are computationally expensive to optimize and often unsupported by standard constraint solvers.
- What evidence would resolve it: A modification of MACHOP that successfully learns a non-linear preference model while maintaining feasible query generation times and low relative regret.

### Open Question 2
- Question: Does actively selecting the next problem instance for query generation, rather than selecting randomly, significantly accelerate the learning process?
- Basis in paper: [explicit] The Conclusion suggests that "learning could be further sped up by actively choosing which instance to generate a query for next."
- Why unresolved: The current methodology relies on random instance selection or pre-determined offline sequences, leaving the potential efficiency gains from active instance selection unexplored.
- What evidence would resolve it: Experiments demonstrating that an active instance selection strategy reduces the number of queries required to reach a specific threshold of user preference alignment.

### Open Question 3
- Question: Can "no preference" feedback from users be utilized as an explicit signal to improve the convergence speed of the preference learning algorithm?
- Basis in paper: [explicit] The Conclusion proposes that "using queries where users express no preference could accelerate learning too."
- Why unresolved: While the simulation model accounts for indifference, the current learning algorithm does not explicitly leverage "no preference" labels to refine the weight vector or select subsequent queries.
- What evidence would resolve it: A comparative study showing that an update rule incorporating indifference labels results in faster convergence or higher final accuracy compared to the current pairwise comparison approach.

### Open Question 4
- Question: How does MACHOP perform when applied to larger-scale, industrial combinatorial problems compared to the logic puzzles used in current evaluations?
- Basis in paper: [explicit] The Conclusion indicates that "the real-user evaluation paves the way for applying MACHOP to more practical and larger-scale scenarios, such as explanations for industrial problems."
- Why unresolved: The method has only been validated on Sudoku and Logic-Grid puzzles with a limited user pool; scalability to complex domains like nurse rostering or production scheduling remains untested.
- What evidence would resolve it: Results from experiments applying MACHOP to industrial benchmarks, maintaining the demonstrated reduction in relative regret and user waiting times.

## Limitations

- Confidence in 80% regret reduction is medium because comparison is only against baseline Choice Perceptron, not more recent methods
- Real-user evaluation based on small sample (30 queries Ã— 5 users) provides suggestive but not definitive evidence
- Performance in highly non-stationary preference environments is untested

## Confidence

- **High confidence**: The normalization mechanism (local vs cumulative) and its impact on learning stability, supported by ablation experiments
- **Medium confidence**: The non-domination constraint's contribution to learning quality (7/8 setups improved), as the mechanism is sound but effect size varies
- **Medium confidence**: The UCB-based diversification's 40% regret reduction claim, as the evidence is strong but interpretation is novel

## Next Checks

1. **External benchmark validation**: Test MACHOP against state-of-the-art multi-objective preference elicitation methods on both artificial and real user datasets

2. **Robustness to noise and user heterogeneity**: Conduct experiments with varying noise levels, different user simulation models, and heterogeneous user groups

3. **Feature sensitivity analysis**: Systematically vary sub-objective features and retrain MACHOP to determine which features are most critical for performance