---
ver: rpa2
title: Defense Against Model Stealing Based on Account-Aware Distribution Discrepancy
arxiv_id: '2503.12497'
source_url: https://arxiv.org/abs/2503.12497
tags:
- queries
- uni00000003
- malicious
- uni00000013
- benign
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of protecting image classification
  models from model stealing attacks by proposing a novel defense mechanism called
  D-ADD. The core idea is to use an Account-aware Distribution Discrepancy (ADD) detector
  that measures the deviation of query distributions from training distributions in
  the feature space using class-wise statistics.
---

# Defense Against Model Stealing Based on Account-Aware Distribution Discrepancy

## Quick Facts
- arXiv ID: 2503.12497
- Source URL: https://arxiv.org/abs/2503.12497
- Reference count: 31
- Primary result: D-ADD reduces clone model accuracy to near-random levels (< 0.1%) while preserving target model utility (< 0.1% accuracy drop).

## Executive Summary
This paper proposes D-ADD, a training-free defense mechanism against model stealing attacks that leverages account-aware distribution discrepancy detection. The core idea is to monitor the statistical deviation of query distributions from training distributions using class-wise statistics in the feature space. By combining this detector with random-based prediction poisoning, D-ADD effectively protects image classification models while maintaining high utility for benign users. The method demonstrates strong performance across multiple datasets and attack types, achieving high detection rates with minimal impact on legitimate users.

## Method Summary
D-ADD operates by maintaining a sliding window of recent query features for each account and calculating a Malicious Score based on the weighted sum of squared Fréchet Distances between window statistics and reference statistics per class. When the score exceeds a threshold, the system returns a random label instead of the true prediction. The defense is training-free, requiring only the computation of class-wise multivariate normal statistics from the training data offline. During operation, the detector continuously evaluates incoming queries and dynamically adjusts its response to protect against model stealing attempts while preserving accuracy for legitimate users.

## Key Results
- Achieves clone model accuracy dropping to near-random levels (< 0.1%) across all attack types
- Maintains target model accuracy drop below 0.1% for benign users
- Demonstrates high detection performance with AUROC > 0.96 and FPR@TPR95 < 0.05 across multiple datasets

## Why This Works (Mechanism)
The defense exploits the fundamental difference between legitimate user query distributions and those generated by model stealing attacks. Legitimate users typically query the model with diverse, naturally distributed inputs that match the training data statistics. In contrast, model stealing attacks often use surrogate data or synthetic queries that create detectable deviations in the feature space distribution. By tracking these deviations through class-wise multivariate normal statistics and applying prediction poisoning when anomalies are detected, D-ADD creates a strong barrier against unauthorized model extraction while remaining transparent to legitimate users.

## Foundational Learning
- **Feature Space Distribution Analysis**: Understanding how query distributions differ between legitimate and malicious users in the model's latent space. Why needed: Forms the basis for detecting anomalous query patterns. Quick check: Verify feature distributions of benign vs. attack queries using visualization tools.
- **Multivariate Normal Statistics**: Computing and comparing mean vectors and covariance matrices for class distributions. Why needed: Enables quantitative measurement of distribution deviation. Quick check: Validate covariance matrix calculations with synthetic data.
- **Fréchet Distance Calculation**: Measuring the similarity between two multivariate normal distributions. Why needed: Provides the mathematical foundation for the distribution discrepancy metric. Quick check: Test Fréchet distance implementation on known distributions.
- **Sliding Window Aggregation**: Maintaining recent query statistics per account to detect temporal patterns. Why needed: Enables account-aware detection without storing complete query histories. Quick check: Verify window update logic with timestamp-based queries.
- **Threshold Calibration**: Determining optimal detection thresholds based on utility constraints. Why needed: Balances protection effectiveness with benign user experience. Quick check: Sweep threshold values and measure accuracy drop vs. detection performance.

## Architecture Onboarding

**Component Map**: Training Data -> Reference Statistics -> ADD Detector -> Query Handler -> Target Model

**Critical Path**: Query → Feature Extraction → MS Calculation → Threshold Check → (Random Label | True Label) → Model Output

**Design Tradeoffs**: 
- Larger sliding windows improve detection reliability but reduce robustness to adaptive attacks
- Higher detection thresholds preserve utility but may allow more attacks to succeed
- Account-aware detection requires more memory but provides better discrimination than global detection

**Failure Signatures**: 
- High false positive rate indicates threshold set too low or initialization issues
- Failed covariance matrix calculations suggest numerical instability with small sample sizes
- Degraded target model performance indicates over-aggressive detection settings

**First Experiments**:
1. Verify ADD detector detects synthetic attack queries while accepting benign queries on held-out test set
2. Test prediction poisoning mechanism by confirming random label generation when MS exceeds threshold
3. Measure clone model accuracy when D-ADD is enabled versus disabled during attack simulation

## Open Questions the Paper Calls Out
- **Sybil Attack Adaptation**: How can D-ADD handle attacks where malicious queries are distributed across multiple fake accounts? The current account-aware approach may fail when adversaries split query streams across many accounts, preventing statistical evidence accumulation within individual windows.
- **Integration with Other Defenses**: Can D-ADD be effectively combined with complementary defensive strategies like model watermarking or adversarial training? The paper evaluates it as a standalone module without analyzing potential synergies or conflicts with other defense types.
- **Dynamic Window Sizing**: Is there an adaptive method for selecting optimal sliding window sizes to balance detection reliability with robustness against evasion? The paper uses fixed window sizes but does not provide a mechanism for automatic adjustment based on operational environment or threat model.

## Limitations
- Effectiveness depends on accurate estimation of class-wise multivariate normal distributions, which can be unstable with small sample sizes
- Threshold selection procedure is described but not precisely quantified, leaving ambiguity about exact operational settings
- Defense's effectiveness against adaptive adversaries who mimic benign query patterns is not explored
- Does not address privacy concerns of storing per-account feature windows

## Confidence
- **High**: Core mechanism of using account-aware distribution discrepancy with Fréchet distance is technically sound and well-explained
- **Medium**: "Training-free" claim is accurate for detector but end-to-end defense still requires target model training
- **Medium**: Generalization across multiple datasets suggests robustness, but limited to specific model architectures

## Next Checks
1. Reproduce ADD detector's detection performance on held-out benign test set to verify AUROC > 0.96 and low FPR when threshold tuned for utility
2. Implement complete end-to-end defense against KnockoffNets using surrogate data, verifying clone accuracy drops to near-random while target accuracy drop remains < 0.1%
3. Test covariance matrix calculation stability with edge cases (small windows, underrepresented classes) and apply regularization to prevent numerical failures