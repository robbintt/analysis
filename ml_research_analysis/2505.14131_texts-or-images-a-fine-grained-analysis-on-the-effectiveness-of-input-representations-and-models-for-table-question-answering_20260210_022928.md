---
ver: rpa2
title: Texts or Images? A Fine-grained Analysis on the Effectiveness of Input Representations
  and Models for Table Question Answering
arxiv_id: '2505.14131'
source_url: https://arxiv.org/abs/2505.14131
tags:
- table
- question
- tables
- reasoning
- small
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether table images or text representations
  are more effective for table question answering (TQA) when using large language
  models (LLMs) versus multimodal large language models (MLLMs). The authors create
  a controlled benchmark from six datasets, categorizing instances by question complexity
  (retrieval vs.
---

# Texts or Images? A Fine-grained Analysis on the Effectiveness of Input Representations and Models for Table Question Answering

## Quick Facts
- arXiv ID: 2505.14131
- Source URL: https://arxiv.org/abs/2505.14131
- Reference count: 25
- Large models perform better with table images, small models vary by setting; FRES achieves 10% EM improvement with 66% efficiency gain

## Executive Summary
This paper investigates whether text or image representations are more effective for table question answering (TQA) when using large language models (LLMs) versus multimodal large language models (MLLMs). The authors create a controlled benchmark from six datasets, categorizing instances by question complexity (retrieval vs. reasoning) and table size (small vs. big). They evaluate seven open-weight model pairs and find that model size significantly affects the best representation choice: large models perform better with table images, while small models' performance varies by setting. The study reveals that for small tables with reasoning questions, combining both text and image representations works best with MLLMs, while text alone suffices for retrieval questions. Based on these findings, the authors propose FRES, a feature-based table representation selection method that dynamically chooses the optimal representation. FRES achieves a 10% average exact match improvement over baseline approaches while reducing computational efficiency by up to 66% through fewer input tokens.

## Method Summary
The authors create a controlled benchmark by extracting subsets from six existing TQA datasets, categorizing instances by question complexity (retrieval vs. reasoning) and table size (small vs. big). They evaluate seven pairs of open-weight LLMs and MLLMs with similar architectures but different sizes. The models are tested on four input representations: text-only, image-only, combined text+image, and a proposed feature-based selection method (FRES). Performance is measured using exact match (EM) accuracy, while computational efficiency is assessed through input token count and inference time. The FRES method uses lightweight classifiers to predict the optimal representation for each instance based on features extracted from the question and table.

## Key Results
- Model size significantly influences optimal representation choice: large models perform better with table images, while small models vary by setting
- For small tables with reasoning questions, combining both text and image representations works best with MLLMs
- FRES achieves 10% average exact match improvement over baseline approaches while reducing computational efficiency by up to 66%

## Why This Works (Mechanism)
The effectiveness of different input representations depends on the trade-off between model capacity and information density. Large models have sufficient capacity to process rich visual information from table images, capturing layout, formatting, and visual cues that text alone may miss. Small models, with limited capacity, benefit more from compact text representations that focus on essential content without overwhelming their processing capabilities. For reasoning questions requiring cross-column comparisons or multi-step inference, the visual context in table images provides additional cues that facilitate complex reasoning, particularly for small tables where the visual layout is easier to parse.

## Foundational Learning

**Table Question Answering (TQA)**: The task of answering natural language questions using information from structured tables. Why needed: Forms the core problem domain being investigated. Quick check: Can you explain the difference between retrieval and reasoning questions in TQA?

**Multimodal Large Language Models (MLLMs)**: AI models capable of processing both text and visual inputs. Why needed: Central to understanding the performance differences between text and image representations. Quick check: What distinguishes MLLMs from traditional LLMs in the context of TQA?

**Input Representations**: Different ways of encoding table information (text, image, or combined) for model consumption. Why needed: The fundamental variable being studied for effectiveness. Quick check: How do text-only and image-only representations differ in what information they capture from tables?

## Architecture Onboarding

**Component Map**: Question -> Table Representation Selector (FRES) -> Optimal Representation (Text/Image/Combined) -> MLLM/LLM -> Answer

**Critical Path**: The representation selection and processing pipeline is critical, as incorrect representation choice can lead to significant performance degradation. The MLLM's ability to process multimodal inputs determines the upper bound of performance.

**Design Tradeoffs**: The study balances representation richness against model capacity and computational efficiency. Text representations are computationally cheaper but may miss visual context, while images capture more information but require more processing power and may exceed context limits.

**Failure Signatures**: Poor performance typically occurs when using image representations with small models on complex reasoning tasks, or when using text representations for tasks requiring visual layout understanding. Mismatched representation choices lead to the most significant failures.

**First Experiments**:
1. Test baseline models with all four input representations on the controlled benchmark to establish performance baselines
2. Implement and evaluate the FRES representation selector on held-out test data
3. Conduct ablation studies varying table size and question complexity independently to isolate their effects

## Open Questions the Paper Calls Out
None

## Limitations
- The controlled benchmark may not fully capture real-world table diversity, particularly complex formatting and nested structures
- Findings are based on open-weight models only, limiting generalizability to proprietary systems
- FRES performance assumes perfect classification accuracy in representation selection

## Confidence

**High Confidence**: Model size influences optimal representation choice; MLLMs benefit from combining representations for small tables with reasoning questions

**Medium Confidence**: The 10% exact match improvement claim for FRES, dependent on benchmark representativeness

**Low Confidence**: Computational efficiency claims, as token reduction doesn't directly translate to wall-clock time improvements

## Next Checks

1. Evaluate FRES on diverse real-world tables with varying formats and noise levels to test robustness
2. Conduct ablation studies isolating impact of table size versus question complexity on representation effectiveness
3. Measure actual inference latency and memory usage differences between representation approaches across different hardware configurations