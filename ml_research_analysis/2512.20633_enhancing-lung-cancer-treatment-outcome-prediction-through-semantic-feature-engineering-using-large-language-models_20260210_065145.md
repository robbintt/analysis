---
ver: rpa2
title: Enhancing Lung Cancer Treatment Outcome Prediction through Semantic Feature
  Engineering Using Large Language Models
arxiv_id: '2512.20633'
source_url: https://arxiv.org/abs/2512.20633
tags:
- data
- clinical
- cancer
- were
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of predicting lung cancer treatment
  outcomes using sparse, heterogeneous clinical data. The authors introduce a framework
  using Large Language Models (LLMs) as Goal-oriented Knowledge Curators (GKC) to
  convert laboratory, genomic, and medication data into high-fidelity, task-aligned
  features.
---

# Enhancing Lung Cancer Treatment Outcome Prediction through Semantic Feature Engineering Using Large Language Models

## Quick Facts
- arXiv ID: 2512.20633
- Source URL: https://arxiv.org/abs/2512.20633
- Reference count: 0
- Primary result: LLM-based semantic summarization (GKC) achieved AUC-ROC 0.803 vs. 0.675 for end-to-end transformer

## Executive Summary
This study addresses the challenge of predicting lung cancer treatment outcomes using sparse, heterogeneous clinical data. The authors introduce a framework using Large Language Models (LLMs) as Goal-oriented Knowledge Curators (GKC) to convert laboratory, genomic, and medication data into high-fidelity, task-aligned features. GKC operates as an offline preprocessing step, integrating seamlessly into hospital informatics pipelines. Using a lung cancer cohort (N=184), GKC achieved a mean AUC-ROC of 0.803 (95% CI: 0.799-0.807), significantly outperforming expert-engineered features (AUC-ROC 0.619), contextual text embeddings (AUC-ROC 0.678), and an end-to-end transformer baseline (AUC-ROC 0.675). An ablation study confirmed the complementary value of combining all three modalities.

## Method Summary
The framework extracts three clinical modalities (laboratory, genomic, medication) from EHR data and enriches them with biomedical knowledge base entries. A general-purpose LLM (Gemini 2.0 Flash) generates structured JSON summaries per modality using persona-based prompts. These summaries are embedded using a classification-optimized embedder and classified by a lightweight model (XGBoost). The approach operates as an offline preprocessing step, replacing the need for data-hungry end-to-end transformers in low-resource settings.

## Key Results
- GKC achieved mean AUC-ROC of 0.803 (95% CI: 0.799-0.807), significantly outperforming expert-engineered features (AUC-ROC 0.619)
- Outperformed contextual text embeddings (AUC-ROC 0.678) and end-to-end transformer baseline (AUC-ROC 0.675)
- Ablation study confirmed complementary value of combining all three modalities (Lab+Gene+Med)

## Why This Works (Mechanism)

### Mechanism 1
LLM-as-knowledge-curator improves prediction by reducing noise and aligning semantics with task goals through deterministic summarization. A prompted LLM performs offline summarization of each modality into structured JSON summaries, filtering noise and consolidating redundant information. These summaries are embedded and classified by a simple model, requiring fewer training examples than end-to-end approaches.

### Mechanism 2
Explicit, task-aligned summarization outperforms generic self-attention over long contexts in low-resource clinical settings. The GKC framework replaces the need for a large, data-hungry end-to-end model to learn implicit summarization. A single deterministic summarization pass by a general-purpose LLM distills task-relevant information, making it easier for simpler classifiers to learn from with fewer training examples.

### Mechanism 3
Balanced, synergistic integration of complementary modalities (Lab, Gene, Med) is more robust than relying on any single dominant feature set. The GKC framework produces separate, high-fidelity summaries for each modality, which are then embedded and concatenated. This preserves modality-specific insights while allowing the downstream classifier to learn cross-modal interactions.

## Foundational Learning

### Concept: Goal-Oriented Prompting / Persona-Based Role-Playing
**Why needed here:** The paper uses LLMs as simulated domain experts to extract task-specific, prognostic implications from raw data. Understanding how to design such prompts is critical to replicating the GKC framework.
**Quick check question:** How would you design a prompt to get an LLM to summarize a patient's lab results, not by listing values, but by inferring their "hematologic stability" and "inflammatory status"?

### Concept: Embeddings vs. Explicit Summarization
**Why needed here:** The paper demonstrates a clear performance hierarchy between different feature engineering strategies. Understanding the difference between directly embedding raw text vs. embedding a semantically processed summary is key to grasping why the proposed method works better.
**Quick check question:** In a single sentence, explain why a classification-optimized embedding of a *summary* might contain a stronger predictive signal than an embedding of the *source text* that summary was derived from.

### Concept: Ablation Studies for Multimodal Models
**Why needed here:** The paper uses an ablation study to prove the synergistic value of its three data modalities. This is a standard and crucial technique for validating that each component of a complex model contributes meaningfully.
**Quick check question:** Your multimodal model (Audio + Video) gets 90% accuracy. What two experiments must you run to determine if both modalities are truly helping?

## Architecture Onboarding

### Component map:
Data Preprocessing -> Knowledge Base Integration -> GKC Summarizer -> Feature Embedder -> Predictive Classifier -> Interpretability Layer

### Critical path:
The system's success depends on the quality of the prompt design and the LLM's ability to follow it deterministically. A poor prompt leads to noisy summaries, degrading the entire pipeline.

### Design tradeoffs:
GKC vs. End-to-End: Trades potential for higher performance with massive fine-tuning data for better performance with limited data and higher interpretability. General vs. Domain-Specific LLM: Found general-purpose LLM outperformed biomedical-specific one, suggesting strong instruction-following ability is more important than pure domain knowledge. Interpretability vs. Performance: Using tree-based XGBoost provides SHAP interpretability.

### Failure signatures:
Hallucination: LLM invents facts not in source text. Mitigation: Strict "from-context" prompting, deterministic decoding. Data Sparsity/Imbalance: Small cohort size. Mitigation: Use tree-based models, stratified cross-validation. Embedding Saturation: If summaries are too long. Mitigation: GKC summaries designed to be concise.

### First 3 experiments:
1. Baseline Replication: Implement CTE model (embed raw, knowledge-base-enriched text) and GKC model (embed LLM summaries). Compare AUC-ROC on held-out test set.
2. Ablation Study: Train three GKC models, each using only one modality (Lab, Gene, or Med), and a fourth using all three. Compare performance.
3. Classifier Sensitivity Analysis: Swap XGBoost classifier for others (Random Forest, Logistic Regression) on GKC features to confirm classifier choice is not primary driver.

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** Does the GKC framework generalize to other cancer types and multi-institutional cohorts with different data distributions and clinical practices?
**Basis in paper:** Future directions include validation across cancers and institutions; external validation on larger, multi-institutional cohorts is essential.
**Why unresolved:** Single-institution cohort of N=184 lung cancer patients with specific data collection protocols.
**What evidence would resolve it:** Replication of GKC performance (AUC-ROC â‰¥0.80) on independent cohorts from different institutions across at least 2-3 other cancer types.

### Open Question 2
**Question:** Can the GKC framework maintain its performance advantage when using fully open-source LLMs instead of proprietary APIs?
**Basis in paper:** Authors note reliance on proprietary APIs as a limitation and state "further work with open-source models will be critical to ensure long-term reproducibility and accessibility."
**Why unresolved:** Only proprietary models were tested; no open-source alternatives were evaluated.
**What evidence would resolve it:** Benchmarking GKC with open-source models showing comparable AUC-ROC within 0.02-0.03 of proprietary baseline.

### Open Question 3
**Question:** Does integrating unstructured clinical narratives and imaging data provide complementary predictive signal beyond current modalities?
**Basis in paper:** Inputs were restricted to structured and semi-structured data; unstructured clinical narratives and imaging remain untapped.
**Why unresolved:** Current ablation showed tri-modal synergy, but clinical notes and imaging were excluded despite containing staging and response information.
**What evidence would resolve it:** Adding imaging and narrative modalities to GKC and demonstrating statistically significant improvement (p<0.05) over current 0.803 AUC-ROC baseline.

## Limitations
- Reliance on single, relatively small clinical dataset (N=184) may limit generalizability
- Framework depends on general-purpose LLMs for knowledge curation, introducing uncertainty about performance consistency
- Deterministic summarization approach may not capture subtle clinical nuances important for treatment decisions
- Validation through ablation studies doesn't address potential missing modalities or alternative feature engineering approaches

## Confidence

### Major Uncertainties and Limitations
- **High Confidence:** Core mechanism of LLM-as-knowledge-curator outperforming end-to-end transformers in low-data regimes; ablation study results showing synergistic gains from multi-modality integration; primary finding of significantly higher AUC-ROC (0.803) vs. baselines
- **Medium Confidence:** Specific superiority of general-purpose LLMs over biomedical-specific ones; exact contribution of each modality to overall performance
- **Low Confidence:** Framework's performance on different cancer types or treatment settings; optimal prompt design for LLM summarization across different clinical contexts

## Next Checks

1. **External Validation:** Apply GKC framework to an independent lung cancer cohort from a different institution to assess generalizability and test for overfitting
2. **Clinical Impact Assessment:** Conduct physician validation study where oncologists evaluate interpretability and clinical utility of GKC-generated summaries vs. traditional features
3. **Prompt Robustness Testing:** Systematically vary LLM prompts and temperature settings to quantify sensitivity to prompt engineering choices and establish reproducibility across different LLM versions