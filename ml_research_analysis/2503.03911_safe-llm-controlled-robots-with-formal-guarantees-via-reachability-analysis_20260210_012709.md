---
ver: rpa2
title: Safe LLM-Controlled Robots with Formal Guarantees via Reachability Analysis
arxiv_id: '2503.03911'
source_url: https://arxiv.org/abs/2503.03911
tags:
- safety
- plan
- robot
- safe
- reachability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a safety framework for LLM-controlled robots
  using data-driven reachability analysis. The method provides formal safety guarantees
  by overapproximating reachable sets from historical trajectory data, eliminating
  the need for precise analytical models.
---

# Safe LLM-Controlled Robots with Formal Guarantees via Reachability Analysis

## Quick Facts
- arXiv ID: 2503.03911
- Source URL: https://arxiv.org/abs/2503.03911
- Reference count: 31
- Introduces safety framework for LLM-controlled robots using data-driven reachability analysis

## Executive Summary
This paper presents a safety framework for Large Language Model (LLM)-controlled robots that provides formal safety guarantees through reachability analysis. The method constructs reachable sets from historical trajectory data to overapproximate possible robot behaviors, then uses these sets to verify and adjust LLM-generated plans for obstacle avoidance. The approach eliminates the need for precise analytical models while ensuring collision-free navigation in complex environments. The framework is validated on TurtleBot3 and JetRacer platforms, demonstrating safe navigation with formal guarantees over multiple time steps.

## Method Summary
The proposed framework integrates LLM-based planning with data-driven reachability analysis to ensure safe robot navigation. Historical trajectory data is used to construct reachable sets that overapproximate the robot's possible future states over a planning horizon. When an LLM generates a navigation plan, the system computes reachable sets for each time step and checks for potential collisions with obstacles. If violations are detected, the plan is adjusted using gradient-based optimization to maintain safety while preserving the LLM's original intent. The method operates in real-time, providing formal safety guarantees without requiring precise analytical models of the robot's dynamics.

## Key Results
- Achieves safe navigation in obstacle-rich environments with collision-free trajectories
- Provides formal safety guarantees over multiple time steps, outperforming existing one-step safety filters
- Demonstrates execution times of 0.04-0.22 seconds and frequencies of 4.5-25 Hz across different scenarios
- Validated on TurtleBot3 and JetRacer platforms in controlled environments

## Why This Works (Mechanism)
The framework leverages the complementary strengths of LLMs and reachability analysis. LLMs excel at high-level planning and reasoning but lack formal safety guarantees. Reachability analysis provides mathematical guarantees about system behavior but typically requires precise models. By using historical trajectory data to construct reachable sets, the method bypasses the need for analytical models while maintaining formal safety properties. The gradient-based plan adjustment preserves the LLM's original intent while ensuring safety, creating a robust system that combines symbolic reasoning with data-driven safety verification.

## Foundational Learning
- **Reachability Analysis**: Mathematical technique to compute all possible states a system can reach over time - needed to provide formal safety guarantees, check by verifying computed sets contain actual trajectories
- **Data-driven Modeling**: Using historical data to approximate system behavior instead of analytical models - needed to avoid complex modeling requirements, check by comparing approximation quality against ground truth
- **Gradient-based Optimization**: Mathematical method for adjusting plans while preserving objectives - needed to modify LLM plans for safety, check by measuring objective preservation after adjustment
- **Formal Verification**: Mathematical proof of system properties - needed to ensure guaranteed safety, check by validating proof conditions hold for all scenarios
- **LLM Planning Integration**: Combining symbolic AI reasoning with geometric safety constraints - needed to leverage LLM capabilities while ensuring safety, check by measuring plan quality metrics

## Architecture Onboarding

**Component Map:**
LLM Planner -> Reachability Analyzer -> Safety Verifier -> Plan Adjuster -> Robot Controller

**Critical Path:**
LLM generates plan → Reachability sets computed → Safety verification performed → Collision detection → Plan adjustment via gradient optimization → Command execution

**Design Tradeoffs:**
- Data-driven vs analytical modeling: Trade precision for ease of implementation
- Overapproximation vs exact computation: Trade computational efficiency for guaranteed safety
- Real-time performance vs planning horizon: Trade execution speed for longer-term safety guarantees
- LLM autonomy vs safety constraints: Trade plan flexibility for guaranteed collision avoidance

**Failure Signatures:**
- Degraded performance with insufficient or unrepresentative training data
- Increased execution time in dense obstacle environments
- Plan adjustments that significantly deviate from LLM intent
- Failure to guarantee safety in novel scenarios outside training distribution

**First 3 Experiments:**
1. Test basic reachability computation on simple linear trajectories
2. Validate safety guarantees in obstacle-free environments
3. Measure plan adjustment quality when LLM generates unsafe plans

## Open Questions the Paper Calls Out
None

## Limitations
- Performance heavily dependent on quality and representativeness of historical trajectory data
- Computational overhead may limit real-time applicability in complex environments
- Safety guarantees are probabilistic rather than deterministic due to overapproximation
- Evaluation limited to specific hardware platforms and static environments

## Confidence
- Formal safety guarantees: Medium (dependent on data quality and approximation assumptions)
- Computational performance metrics: High (specific measurements provided)
- Comparative advantage over existing methods: Medium (evaluation criteria could be more rigorous)

## Next Checks
1. Test the framework in dynamic environments with moving obstacles to evaluate robustness beyond static scenarios
2. Conduct ablation studies comparing performance with varying amounts and quality of historical trajectory data
3. Implement the system on different robot platforms with varying dynamics to assess generalizability across morphologies