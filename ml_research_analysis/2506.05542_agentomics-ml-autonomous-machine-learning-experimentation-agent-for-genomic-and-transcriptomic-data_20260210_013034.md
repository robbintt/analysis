---
ver: rpa2
title: 'Agentomics-ML: Autonomous Machine Learning Experimentation Agent for Genomic
  and Transcriptomic Data'
arxiv_id: '2506.05542'
source_url: https://arxiv.org/abs/2506.05542
tags:
- datasets
- data
- agentomics-ml
- code
- zero
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Agentomics-ML addresses the challenge of automating machine learning\
  \ experimentation for heterogeneous -omics datasets, where traditional AutoML and\
  \ zero-shot LLM approaches struggle with generalization and success rates. The system\
  \ employs a fully autonomous agentic framework that iteratively executes a predefined\
  \ ML experimentation pipeline\u2014encompassing data exploration, representation\
  \ selection, model architecture design, training, and inference script generation\u2014\
  through bash-based file system interactions."
---

# Agentomics-ML: Autonomous Machine Learning Experimentation Agent for Genomic and Transcriptomic Data

## Quick Facts
- arXiv ID: 2506.05542
- Source URL: https://arxiv.org/abs/2506.05542
- Reference count: 16
- Autonomous agent achieves 93.33% success rate in producing working ML code for genomic/transcriptomic datasets, outperforming state-of-the-art systems and zero-shot LLMs.

## Executive Summary
Agentomics-ML is an autonomous machine learning experimentation system designed for genomic and transcriptomic sequence classification tasks. The system employs an agentic framework where a large language model acts as the "brain" to navigate a predefined ML experimentation pipeline through bash-based file system interactions. A key innovation is the reflection step, which converts training and validation metrics into verbal feedback that guides subsequent iterations, enabling error correction and progressive model refinement. Evaluated on six benchmark datasets, Agentomics-ML achieves significantly higher success rates than traditional AutoML and zero-shot LLM approaches while narrowing the performance gap with expert-built models.

## Method Summary
The system uses a pre-defined pipeline of steps (data exploration → representation selection → architecture design → script generation → training) executed by an LLM agent interacting via bash tools. The agent writes and executes Python files, with Pydantic AI framework validating output syntax/format. A reflection step takes train/validation metrics to generate verbal feedback for subsequent iterations. The system runs in isolated Docker containers using gpt-4.1 as the backbone LLM, achieving <$2 average cost per run. Best model selection uses validation metrics with strict test set abstraction during development.

## Key Results
- Achieves 93.33% success rate in producing working ML code across six benchmark datasets
- Significantly outperforms state-of-the-art agentic systems and zero-shot LLMs on complex datasets like AGO2_CLASH_Hejret
- Attains state-of-the-art automated performance on one dataset and narrows gap with expert-built models on others
- Robust generalization demonstrated even for atypical sequence-interaction tasks with variable-length dual RNA sequences

## Why This Works (Mechanism)
The system's success stems from its iterative agentic approach combined with structured reflection. Rather than relying on single-shot code generation, the agent progressively refines its models through multiple iterations. The reflection mechanism is crucial—it transforms scalar metrics (accuracy, loss) into actionable verbal feedback that the agent uses to modify its approach. This creates a feedback loop where the agent can recognize and correct issues like overfitting or suboptimal architectures. The bash-based interaction model ensures reproducible, executable code generation while the validation layer prevents syntactic errors from propagating.

## Foundational Learning
- **Concept: Agentic Framework for Code Generation**
  - **Why needed here:** Agentomics-ML is not just a model but a system that uses an LLM as its core "brain" to write code, execute it via bash commands, and manage files. Understanding how agents use tools (like bash and python execution) to interact with an environment is fundamental.
  - **Quick check question:** Can you explain the difference between an LLM generating code in a zero-shot prompt versus an agentic system using a bash tool to write and run that same code?

- **Concept: The Machine Learning Experimentation Lifecycle**
  - **Why needed here:** The system is built around a predefined set of steps that mirror what a human ML practitioner does. Knowing this lifecycle—data exploration, preprocessing, model selection, training, and evaluation—is essential to understand what the agent is trying to accomplish at each stage.
  - **Quick check question:** Name the key sequential steps the Agentomics-ML agent must complete, from reading the data to producing a final model.

- **Concept: Scalar Feedback & Verbal Reflection**
  - **Why needed here:** A core innovation is the reflection step. You must understand how the system takes non-verbal, scalar signals (like accuracy or loss), feeds them to an LLM, and has the LLM generate a verbal critique to guide its next attempt.
  - **Quick check question:** How does the Agentomics-ML system convert a training metric like a high training accuracy but low validation accuracy into a signal for improvement?

## Architecture Onboarding
- **Component map:** Core Agent (LLM) -> Tool Layer (bash, python writer/executor) -> Validation Module (Pydantic) -> Memory/Context Store (feedback storage) -> Iteration Controller (main loop)

- **Critical path:**
  1. Iteration controller starts pipeline for given dataset
  2. Agent completes data exploration using bash/python tools to produce summary
  3. Agent proceeds through representation and architecture steps, generating files
  4. Each file generation triggers Validation Module; failed validation forces retry
  5. Agent generates and executes training scripts
  6. Upon pipeline completion, Reflection Step invokes using training logs metrics
  7. Reflection generates verbal feedback (e.g., "model is overfitting, try adding dropout")
  8. Feedback stored and passed to agent in next iteration
  9. After all iterations, system selects model checkpoint with best validation metric

- **Design tradeoffs:**
  - Structure vs. Flexibility: Fixed step-based pipeline (less flexibility) versus open planning-based agent (higher failure chance). Structured approach argued more reliable for this domain.
  - Cost vs. Performance: Each iteration costs money (under $2 per run mentioned). More iterations allow more refinement but increase cost and time. Reflection mechanism adds computational overhead.

- **Failure signatures:**
  - Infinite Retry Loop: Agent repeatedly fails validation check for single step, unable to produce syntactically correct code
  - Non-Improving Iterations: Reflection step fails to provide useful feedback, agent performance plateaus or degrades
  - Catastrophic Code Errors: Generated training scripts consistently crash during execution due to library incompatibilities or logical errors not caught by syntax validation

- **First 3 experiments:**
  1. Ablation Study on Feedback: Run Agentomics-ML on drosophila_enhancers_stark dataset with reflection step disabled (1 iteration, no feedback) versus enabled (3 iterations with feedback). Compare final test set accuracy.
  2. Validation Robustness Test: Intentionally remove programmatic validation step. Run agent and measure "success rate" (percentage of runs producing executable code without crashing). Expect significant drop.
  3. Cross-Domain Generalization: Apply Agentomics-ML to new, unseen transcriptomic dataset from different source (not from paper's benchmarks). Evaluate whether agent can successfully navigate predefined steps on novel problem and where workflow breaks.

## Open Questions the Paper Calls Out
- Can extending the number of reflection iterations or adjusting the reflection mechanism architecture yield consistent and statistically significant performance improvements across all benchmark datasets? The authors note it's possible that extending iterations beyond tested levels or adjusting the reflection mechanism could push benefits further, but current t-test results show significance on only 2 of 6 datasets.
- How well does Agentomics-ML generalize to computational biology tasks beyond nucleotide sequence classification, such as protein structure prediction, expression-correlation tasks, or multi-omics integration? The limitations section explicitly states current evaluation covers only sequence classification while computational biology tasks are much more varied.
- Can an independent benchmark using never-before-published -omics datasets confirm Agentomics-ML's generalization claims without potential pre-training data contamination? The authors call for third-party evaluation with never-before-published datasets, noting closed-source LLM pre-training corpora are undisclosed and all evaluations were conducted by method's developers.

## Limitations
- Evaluation focuses on six benchmark datasets, leaving exact generalization capabilities to completely novel problem domains untested
- Reflection mechanism's effectiveness depends heavily on quality of verbal feedback generated by LLM, which could degrade with more complex or unfamiliar data patterns
- Computational cost per iteration (under $2) is mentioned but not broken down by step, making it difficult to assess where optimization efforts would be most effective

## Confidence
- **High Confidence:** Agentic framework successfully produces working ML code in 93.33% of cases, outperforming zero-shot LLMs and state-of-the-art agentic systems. Directly supported by experimental results.
- **Medium Confidence:** Reflection step meaningfully improves model performance across iterations. Demonstrated through comparative experiments, though exact mechanism could be more precisely characterized.
- **Medium Confidence:** System generalizes well to atypical sequence-interaction tasks. Strong performance on AGO2_CLASH_Hejret is compelling but represents limited sample of complex genomic tasks.

## Next Checks
1. **Ablation Study on Reflection:** Run Agentomics-ML on drosophila_enhancers_stark dataset with reflection disabled (1 iteration) versus enabled (3 iterations), measuring final test accuracy to quantify reflection mechanism's contribution.
2. **Cross-Domain Transfer:** Apply Agentomics-ML to new, unseen transcriptomic dataset from different source than benchmarks, evaluating whether agent successfully completes all pipeline steps and identifying where workflow breaks.
3. **Failure Mode Analysis:** Systematically induce validation failures by modifying validation schemas, then observe whether agent enters infinite retry loops or successfully adapts code generation approach.