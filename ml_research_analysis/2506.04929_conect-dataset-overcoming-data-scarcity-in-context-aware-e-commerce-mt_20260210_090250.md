---
ver: rpa2
title: 'ConECT Dataset: Overcoming Data Scarcity in Context-Aware E-Commerce MT'
arxiv_id: '2506.04929'
source_url: https://arxiv.org/abs/2506.04929
tags:
- translation
- context
- image
- machine
- product
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces ConECT, a new Czech-to-Polish e-commerce
  product translation dataset consisting of 11,400 sentence pairs with images and
  product metadata. The authors evaluate three approaches for context-aware machine
  translation: (1) fine-tuning a vision-language model (PaliGemma) with images, (2)
  incorporating category paths as special instruction tokens in text-to-text models,
  and (3) adding synthetic image descriptions as prefixes.'
---

# ConECT Dataset: Overcoming Data Scarcity in Context-Aware E-Commerce MT

## Quick Facts
- **arXiv ID:** 2506.04929
- **Source URL:** https://arxiv.org/abs/2506.04929
- **Reference count:** 19
- **Primary result:** Introduced ConECT dataset (11,400 Czech-Polish e-commerce sentence pairs with images) and evaluated three context-aware MT approaches, finding that visual context via VLM and category path prefixes improve translation quality.

## Executive Summary
This paper addresses the challenge of context-aware machine translation in e-commerce, where short product descriptions often suffer from lexical ambiguity. The authors introduce the ConECT dataset containing 11,400 Czech-Polish sentence pairs with product images and metadata, then systematically evaluate three approaches for incorporating context into translation models. Their findings reveal that visual context through vision-language models improves translation quality, while structured category paths as textual prefixes are particularly effective for product names. However, synthetic image descriptions as prefixes degrade performance, highlighting the complexity of context integration.

## Method Summary
The study evaluates three context-aware approaches on Czech-to-Polish e-commerce translation. First, a vision-language model (PaliGemma-3b) is fine-tuned using LoRA to process product images alongside text, testing whether visual features disambiguate translation. Second, a text-to-text Transformer is fine-tuned with hierarchical category paths prepended as special instruction tokens. Third, synthetic image descriptions generated by paligemma-3b-mix-224 are prefixed to source text. The baseline is a Transformer trained on 53M general parallel sentences plus 230K e-commerce sentences. Models are evaluated on product names, offer titles, and descriptions using chrF++ and COMET metrics.

## Key Results
- Visual context in VLM improves translation: PaliGemma with real images achieved chrF 83.48 vs 82.49 with black images on product names
- Category path prefixes enhance text-to-text performance: Achieved chrF 85.51 and COMET 0.9385 on product names
- Synthetic image descriptions degrade quality: chrF collapsed to 48.26 and COMET to 0.7243 on product descriptions

## Why This Works (Mechanism)

### Mechanism 1
Visual context from product images improves translation quality for short e-commerce texts when integrated through a vision-language model. PaliGemma processes images through a visual encoder while handling text translation, with LoRA fine-tuning adapting the model to leverage visual features for lexical disambiguation. The core assumption is that product images contain disambiguating visual information correlating with correct word sense in translation. Evidence shows measurable improvement when visual context is available versus black images. Break condition occurs when textual context is already sufficient or when processing longer descriptive text where visual grounding becomes less discriminative.

### Mechanism 2
Structured product category metadata as textual prefixes improves translation quality in text-to-text NMT models. Hierarchical category paths (e.g., "Sports » Bicycles » Tires") are prepended using special boundary tokens, with the Transformer learning to condition translation decisions on this structured context during fine-tuning. The core assumption is that category paths provide semantic constraints that reduce ambiguity without introducing significant noise. Evidence shows the category context model outperformed no-context variants on COMET across all content types. Break condition occurs if category metadata is noisy, incomplete, or misaligned with the actual product.

### Mechanism 3
Prefixing source text with synthetic image descriptions degrades translation quality, particularly for longer texts. Image descriptions generated by paligemma-3b-mix-224 are prepended using special tokens. The cascade introduces compounding errors as the description model may hallucinate or omit relevant details, and the translation model then conditions on this unreliable context. The core assumption that synthetic descriptions would accurately capture relevant visual features does not hold in practice. Evidence shows dramatic COMET drop on product descriptions. This mechanism already fails under tested conditions, amplifying noise rather than signal.

## Foundational Learning

- **Transformer Cross-Attention**: Why needed here - The baseline NMT model uses a Transformer (big) architecture; understanding how the decoder attends to encoder representations is essential for grasping how contextual prefixes influence output generation. Quick check question: How does prepending a category path to the source sequence change what the decoder can attend to during translation?

- **Parameter-Efficient Fine-Tuning (LoRA)**: Why needed here - The VLM experiments use LoRA (rank=8) rather than full fine-tuning; this affects how visual-linguistic alignment is learned and what capacity the model has for adaptation. Quick check question: What constraints does low-rank adaptation impose on what the model can learn about image-text relationships?

- **Multimodal Fusion in Vision-Language Models**: Why needed here - PaliGemma combines a SigLIP visual encoder with a Gemma language model; the mechanism by which visual features influence text generation determines whether images actually help translation. Quick check question: In a VLM architecture, where do image features enter the text generation pipeline—encoder, decoder, or both?

## Architecture Onboarding

- **Component map**: Parallel Czech-Polish sentences (230K aligned pairs for fine-tuning, 53M for baseline) -> product images (JPEG, resized to 224×224) -> hierarchical category paths -> PaliGemma-3b (SigLIP encoder + Gemma LLM) with LoRA fine-tuning -> text-to-text Transformer (big) baseline with category prefixes OR description prefixes -> evaluation using chrF++ and COMET

- **Critical path**: 1) Prepare aligned parallel data with image paths and category metadata 2) Train baseline Transformer on 53M general-domain + e-commerce sentence pairs 3) Fine-tune three variants: VLM (4 epochs, lr=1e-4), category-prefix NMT, description-prefix NMT 4) Evaluate on three content types: product names, offer titles, descriptions

- **Design tradeoffs**: VLM vs. text-only - VLM achieves competitive quality on product names (chrF 83.48) but requires 3B parameters and image availability at inference; Transformer baseline with category prefixes (chrF 85.51) is lighter and faster. Category vs. description prefixes - Category paths are human-curated structured data; image descriptions are synthetic and introduce hallucination risk—the paper shows the latter fails. Training data scale - Description-prefix model used less training data (only image-captioned subset), which may confound the comparison.

- **Failure signatures**: VLM with black images - If real and black-image conditions perform identically, visual features are not being used—check image preprocessing and encoder loading. Description prefix collapse - Dramatic COMET drop (0.94 → 0.72 on descriptions) suggests the model over-relies on noisy synthetic text—inspect generated descriptions for hallucinations. Metric disagreement - chrF increases while COMET decreases (VLM on descriptions) may indicate fluency improved but semantic adequacy degraded.

- **First 3 experiments**: 1) **Baseline reproduction**: Train the Transformer (big) model on OPUS + internal e-commerce data; verify you achieve chrF ~77-78 on the ConECT test set before any fine-tuning. 2) **Category prefix ablation**: Fine-tune with category paths vs. without; confirm the COMET improvement on product names is reproducible (>0.005 gain). 3) **VLM image ablation**: Fine-tune PaliGemma with real images vs. black images; isolate the visual contribution by keeping all other hyperparameters identical (lr=1e-4, batch=16, 4 epochs).

## Open Questions the Paper Calls Out

1. How can the integration of synthetic image descriptions be refined to avoid the performance degradation observed when using simple text prefixes? The authors conclude that their negative results using image description prefixes highlight that this "straightforward approach requires further refinement." The paper demonstrates that prefixing with descriptions impairs metrics but does not test alternative conditioning architectures. Experiments showing that alternative integration methods (e.g., cross-attention or soft prompts) yield positive gains over the baseline without the observed quality loss would resolve this.

2. Can a dynamic gating mechanism be developed to determine when visual context is beneficial versus when it introduces noise or hallucinations? The limitations section notes that "text alone is sufficient" in many cases and that "incorporating extra context can sometimes reduce translation quality" via hallucinations. The current models process provided context uniformly without distinguishing between instances where the text is ambiguous (requiring context) and instances where it is self-contained. A model architecture that selectively utilizes visual inputs, resulting in fewer hallucinations compared to the standard VLM approach while maintaining disambiguation performance would resolve this.

3. Is it possible to transfer the context-aware capabilities of large Vision-Language Models (VLMs) to smaller, resource-efficient NMT models? The authors highlight that VLM experiments "require significantly more computational resources than text-to-text NMT models." The study compares distinct model classes but does not explore techniques, such as knowledge distillation, to bridge the gap between VLM performance and NMT efficiency. A study successfully distilling a fine-tuned PaliGemma model into a compact Transformer that retains the visual grounding capability with significantly lower inference costs would resolve this.

## Limitations

- Single Czech-Polish e-commerce domain with 11,400 sentence pairs limits generalizability to other language pairs or domains
- Internal training data (53M general sentences, 230K e-commerce sentences, 7M back-translated examples) is not publicly available, making exact baseline reproduction impossible
- Synthetic image description failure mode demonstrated only in one cascade configuration, leaving open whether alternative description generation approaches might succeed

## Confidence

- **High Confidence**: Category path prefixes improve translation quality on product names and titles—supported by clear metric improvements (chrF 85.51 vs 82.49, COMET 0.9385 vs 0.9268) and straightforward mechanism
- **Medium Confidence**: Visual context aids translation in the VLM—while improvements are measurable (chrF +0.99, COMET +0.0042 on product names), the small absolute gains and single-model architecture make it uncertain whether this generalizes to other VLMs or domains
- **Low Confidence**: Synthetic image descriptions degrade quality—the severe collapse (chrF 48.26 on descriptions) could reflect data scarcity (training only on image-captioned subset) rather than inherent context failure

## Next Checks

1. **Controlled description generation ablation**: Generate synthetic descriptions using alternative models (e.g., GPT-4, BLIP-2) and test whether quality degradation persists across different generation approaches

2. **Cross-domain category effectiveness**: Apply the category prefix method to a non-e-commerce parallel corpus (e.g., medical or legal texts) to test whether structured metadata benefits generalize beyond product domains

3. **VLM visual contribution isolation**: Train a PaliGemma variant where visual features are explicitly disabled in the decoder attention mechanism to confirm that observed improvements derive from cross-modal integration rather than domain adaptation alone