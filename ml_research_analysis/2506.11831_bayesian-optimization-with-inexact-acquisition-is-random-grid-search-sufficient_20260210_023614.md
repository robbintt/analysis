---
ver: rpa2
title: 'Bayesian Optimization with Inexact Acquisition: Is Random Grid Search Sufficient?'
arxiv_id: '2506.11831'
source_url: https://arxiv.org/abs/2506.11831
tags:
- acquisition
- function
- optimization
- grid
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of inexact acquisition function
  maximization in Bayesian optimization (BO). The authors formalize a measure of worst-case
  accumulated inaccuracy in acquisition solutions and establish regret bounds for
  both GP-UCB and GP-TS under such inexactness.
---

# Bayesian Optimization with Inexact Acquisition: Is Random Grid Search Sufficient?

## Quick Facts
- **arXiv ID:** 2506.11831
- **Source URL:** https://arxiv.org/abs/2506.11831
- **Authors:** Hwanwoo Kim; Chong Liu; Yuxin Chen
- **Reference count:** 40
- **One-line primary result:** Random grid search is theoretically justified as a valid acquisition solver in Bayesian optimization, achieving sublinear regret with only linear grid growth.

## Executive Summary
This paper addresses the challenge of inexact acquisition function maximization in Bayesian optimization (BO). The authors formalize a measure of worst-case accumulated inaccuracy in acquisition solutions and establish regret bounds for both GP-UCB and GP-TS under such inexactness. They show that sublinear cumulative regret is achievable even when acquisition maximization is solved approximately, provided the accumulated inaccuracy grows slowly. A key contribution is the theoretical justification of random grid search as a valid and computationally efficient acquisition function solver, relaxing prior theoretical requirements from exponential to linear grid growth. Experiments on synthetic functions and a real-world AutoML task confirm that random grid search achieves competitive regret while offering significant computational savings compared to gradient-based solvers.

## Method Summary
The authors formalize Bayesian optimization with inexact acquisition maximization, where the inner loop of selecting the next query point is solved approximately via random grid search. They establish theoretical regret bounds (Theorem 4.1 and 4.2) showing that sublinear cumulative regret is achievable if the accumulated inaccuracy from the inexact solver grows sublinearly. Random grid search is implemented by sampling a grid of size O(t) at iteration t and selecting the argmax acquisition value. The method is tested on six synthetic benchmark functions (Branin, Rastrigin, Hartmann3/4/6, Levy) and a real-world AutoML task with gradient boosting hyperparameters, comparing against L-BFGS-B, Nelder-Mead, and conjugate gradient solvers.

## Key Results
- Sublinear cumulative regret is achievable with random grid search under inexact acquisition maximization, provided grid size grows linearly with iterations.
- Random grid search achieves competitive cumulative regret compared to sophisticated gradient-based solvers while offering significant computational savings (2-3x faster per iteration).
- The theoretical requirement for grid growth is relaxed from exponential (prior work) to linear, making random grid search both practical and theoretically sound.

## Why This Works (Mechanism)
The paper bridges the practical-implementational gap in BO by showing convergence guarantees without exact acquisition maximization. The key mechanism is controlling accumulated inaccuracy through linear grid growth. By formalizing worst-case inaccuracy and proving it grows slowly enough under linear scaling, the authors demonstrate that random grid search maintains the convergence properties of exact solvers. This works because the acquisition function's structure and the GP's uncertainty quantification allow approximate maximization to still provide sufficient exploration-exploitation balance.

## Foundational Learning
- **Reproducing Kernel Hilbert Space (RKHS):** Why needed here: Theoretical regret bounds are derived under the assumption that the objective function resides in an RKHS with a bounded norm. Understanding this is critical for knowing when the theorems apply.
    - Quick check question: Can you explain why the assumption $\|f\|_{\mathcal{H}_k} \le B$ is fundamental to the GP-UCB regret analysis?
- **Acquisition Function Maximization (Inner vs. Outer Loop):** Why needed here: The paper's central problem is the practical difficulty of the "inner loop"—maximizing the acquisition function. Grasping this distinction is key to understanding the contribution.
    - Quick check question: In BO, what is the difference between the goal of the outer optimization (finding $x^*$) and the goal of the inner optimization at each iteration $t$?
- **Sublinear Regret and No-Regret Algorithms:** Why needed here: The primary theoretical result is proving that the regret remains sublinear. This is the standard criterion for a BO algorithm's convergence and efficiency.
    - Quick check question: What does a sublinear cumulative regret ($R_T/T \to 0$ as $T \to \infty$) imply about an algorithm's long-term performance?

## Architecture Onboarding
- **Component Map:** Outer Loop (BO iterations) -> Inner Loop (Acquisition Solver) -> Objective Evaluation
- **Critical Path:** The path to theoretical and practical success relies on the growth of the random grid. The implementation must ensure the grid size $|\mathcal{X}_t|$ scales linearly with the iteration number $t$ (e.g., $100t$ in experiments). This is the key design decision that the paper proves is sufficient to control accumulated inaccuracy and maintain convergence guarantees.
- **Design Tradeoffs:** Theoretical Simplicity vs. Practical Performance: Random grid search is far simpler to implement and analyze but is less sample-efficient per iteration than a well-tuned multi-start L-BFGS-B. Computation vs. Convergence: The core tradeoff is computational cost. The paper shows you can trade a significant amount of inner-loop computation for a theoretically grounded, simpler solver with only a minimal impact on cumulative regret.
- **Failure Signatures:** Non-sublinear Regret: If the cumulative regret curve does not flatten (i.e., $R_T/T$ does not approach zero), it indicates the accumulated inaccuracy from the inexact solver is too high. Saturating Regret: If regret improvement stalls, the grid size $|\mathcal{X}_t|$ may not be growing fast enough to find better points in the search space. The paper's theoretical conditions on grid growth are violated.
- **First 3 Experiments:** Regret Comparison on Synthetic Functions: Reproduce the experiment from Figure 1. Implement GP-UCB with both Random Grid Search and L-BFGS-B on a 3D benchmark (e.g., Rastrigin or Hartmann). Plot cumulative regret vs. iterations to confirm the random grid method tracks the sophisticated optimizer. Computational Time Profiling: Reproduce the experiment from Figure 2. Measure and compare the wall-clock time per iteration for both solvers across different dimensions to quantify the computational savings. Varying Grid Size Ablation: Adapt the experiment from Appendix B.2.1. Compare the performance of a linearly growing grid ($|\mathcal{X}_t| = \Theta(t)$) against a fixed-size grid to empirically validate the paper's theoretical claim on the necessity of grid growth for convergence.

## Open Questions the Paper Calls Out
- **Open Question 1:** Can adaptive inexact optimization strategies, where computational effort is dynamically adjusted based on accumulated inaccuracy, improve upon fixed linear grid growth?
    - Basis in paper: [explicit] "Our framework opens avenues for studying adaptive inexact optimization strategies, where the computational effort allocated to acquisition function maximization is dynamically adjusted based on accumulated inaccuracy."
    - Why unresolved: The paper only analyzes a simple linear grid growth schedule; adaptive allocation remains unexplored.
    - What evidence would resolve it: Theoretical regret bounds for an adaptive scheme and empirical comparison showing improved regret-computation tradeoffs.
- **Open Question 2:** How does inexact acquisition optimization interact with approximate GP posterior inference methods such as sparse or variational GPs?
    - Basis in paper: [inferred] The paper assumes exact posterior inference (Table 1 notes this distinction) but real-world BO often uses sparse approximations (Vakili et al. citations).
    - Why unresolved: The analysis isolates inexact acquisition from approximate inference; their combined effect is unknown.
    - What evidence would resolve it: Cumulative regret bounds incorporating both inaccuracy sources, plus experiments with sparse GPs.
- **Open Question 3:** What is the optimal growth rate for random grid size beyond linear, and how does it depend on dimensionality and kernel properties?
    - Basis in paper: [inferred] Remark 10 notes superlinear growth reduces the inaccuracy factor, but no optimality analysis is provided.
    - Why unresolved: The paper establishes sufficiency of linear growth but does not characterize optimal scaling.
    - What evidence would resolve it: Lower bound analysis or empirical study across grid growth rates and dimensions.

## Limitations
- Theoretical assumptions (bounded RKHS norm, specific kernel parameters) are not fully specified for the experimental benchmarks, making it difficult to verify that all conditions for the theorems are met.
- The computational savings are measured in wall-clock time, which can be implementation-dependent and may not generalize across all hardware or software environments.
- The paper does not explore the impact of different grid sampling strategies (e.g., Sobol vs. uniform) on convergence or regret.

## Confidence
- **High**: Theoretical regret bounds for GP-UCB and GP-TS under inexact acquisition maximization.
- **Medium**: Empirical demonstration that random grid search achieves competitive regret and computational efficiency compared to gradient-based solvers.

## Next Checks
1. **Reproduce Regret Curves**: Implement GP-UCB with random grid search and L-BFGS-B on Branin and Hartmann6; plot cumulative regret vs. iterations to confirm theoretical predictions.
2. **Ablation on Grid Growth**: Compare linear vs. fixed grid sizes (e.g., |Xt| = 100t vs. |Xt| = 1000) to empirically validate the necessity of grid growth for sublinear regret.
3. **Kernel Sensitivity Analysis**: Test the impact of different Matérn kernel hyperparameters (ν) and length scales on regret and runtime to understand robustness to kernel choice.