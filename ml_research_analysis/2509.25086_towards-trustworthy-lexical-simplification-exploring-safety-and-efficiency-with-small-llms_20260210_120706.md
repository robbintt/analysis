---
ver: rpa2
title: 'Towards Trustworthy Lexical Simplification: Exploring Safety and Efficiency
  with Small LLMs'
arxiv_id: '2509.25086'
source_url: https://arxiv.org/abs/2509.25086
tags:
- shot
- llms
- fine-tuned
- harmful
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated efficient and safe lexical simplification
  (LS) using small language models (LLMs), motivated by privacy and resource constraints
  in real-world applications. The proposed approach explored in-context learning and
  knowledge distillation to fine-tune small LLMs (1B-1.5B parameters) for LS across
  five languages, evaluating both automatic metrics and manual harmfulness assessments.
---

# Towards Trustworthy Lexical Simplification: Exploring Safety and Efficiency with Small LLMs

## Quick Facts
- **arXiv ID:** 2509.25086
- **Source URL:** https://arxiv.org/abs/2509.25086
- **Reference count:** 30
- **Primary result:** Small LLMs (1B-1.5B parameters) achieve significant efficiency gains while maintaining quality in lexical simplification, with output log-probability serving as an effective signal for detecting harmful simplifications.

## Executive Summary
This study investigates efficient and safe lexical simplification (LS) using small language models (LLMs), motivated by privacy and resource constraints in real-world applications. The proposed approach explores in-context learning and knowledge distillation to fine-tune small LLMs (1B-1.5B parameters) for LS across five languages, evaluating both automatic metrics and manual harmfulness assessments. Results show that fine-tuning improves performance on automatic metrics but increases harmful simplifications, highlighting a safety trade-off. Crucially, the model's output log-probability serves as an effective signal for detecting harmful simplifications, enabling a filtering strategy that suppresses harmful outputs while preserving beneficial ones. Small LLMs achieve significant inference speedups compared to large models, demonstrating practical efficiency gains. This work establishes a benchmark for efficient and safe LS, advancing deployable, trustworthy tools for inclusive information access.

## Method Summary
The study employs two approaches: in-context learning (ICL) with 5-shot examples and knowledge distillation via QLoRA fine-tuning. Student models include Qwen 2.5 1.5B and Llama 3.2 1B (base versions), trained on synthesized data from Wikipedia using a 9B teacher model (Gemma 2). QLoRA settings use 4-bit quantization with LoRA on Query/Key projections. The MultiLS dataset for five languages is filtered and split (90 instances for development, rest for testing). Evaluation combines automatic metrics (Accuracy@1@top1, Potential@1) with manual harmfulness annotations across four error categories. A filtering strategy based on output log-probability scores identifies harmful simplifications, with latency measured on CPU using llama.cpp.

## Key Results
- Fine-tuned small LLMs achieve 8-10x speedup compared to 9B teacher model on CPU inference
- Output log-probability effectively filters harmful simplifications (AUC and Beneficial Rate at 10% Harmful metrics)
- Automatic metrics improve with fine-tuning but manual harmfulness assessments reveal increased harmful outputs, indicating a safety-performance trade-off
- Morphological complexity impacts safety signal reliability, with German and Japanese showing higher grammar error rates

## Why This Works (Mechanism)
The approach leverages knowledge distillation to transfer LS capabilities from a large teacher model to smaller, more efficient student models. By fine-tuning on high-confidence synthesized data, small models learn task-specific patterns while maintaining compact parameter counts. The critical innovation is using output log-probability as a proxy for harmfulness detection—beneficial simplifications tend to have higher confidence scores than harmful ones, enabling a threshold-based filtering mechanism that preserves quality while reducing computational overhead.

## Foundational Learning

**QLoRA fine-tuning** - Why needed: Enables efficient adaptation of large pre-trained models to specific tasks using low-rank adapters and quantization, reducing memory requirements while maintaining performance. Quick check: Verify LoRA training convergence by monitoring validation loss across epochs.

**Knowledge distillation** - Why needed: Transfers knowledge from a large, capable teacher model to smaller student models, enabling deployment efficiency without sacrificing task quality. Quick check: Compare student model outputs against teacher on held-out validation data to ensure knowledge transfer fidelity.

**Log-probability filtering** - Why needed: Provides a computationally cheap, model-native mechanism for detecting harmful simplifications without requiring external classifiers or human intervention. Quick check: Plot log-probability distributions for beneficial vs. harmful outputs to verify separation quality.

**Morphological complexity handling** - Why needed: Different languages have varying grammatical structures that affect simplification quality and harmfulness detection reliability. Quick check: Analyze error type distributions across languages to identify patterns in harmfulness categories.

## Architecture Onboarding

**Component Map:** Wikipedia sentences -> Teacher model (Gemma 2 9B) -> Synthesized training data -> Student model (Qwen 2.5 1.5B/Llama 3.2 1B) -> QLoRA fine-tuning -> Inference pipeline -> Log-probability filtering

**Critical Path:** Input sentence → Target word identification → Model generation → Log-probability scoring → Filtering decision → Output simplification

**Design Tradeoffs:** Fine-tuning improves automatic metrics but increases harmful outputs; log-probability filtering mitigates safety risks but may reject beneficial simplifications; smaller models offer speed but reduced robustness to grammatical errors.

**Failure Signatures:** High grammar error rates in morphologically complex languages that evade log-prob filtering; performance degradation after quantization; poor-quality synthesized training data from teacher model hallucinations.

**Three First Experiments:**
1. Run inference with 5-shot ICL on test set, compute ACC/POT metrics
2. Apply log-probability filtering to ICL outputs, calculate AUC and B_H0.1
3. Fine-tune student model on synthesized data, evaluate against dev set to select checkpoint

## Open Questions the Paper Calls Out

**Open Question 1:** Can contrastive learning with grammatical errors as negative examples improve small LLMs' ability to use output probability as a signal for filtering grammatically incorrect simplifications in morphologically complex languages? Unresolved because grammar errors in German and Japanese received high probability scores, making them difficult to distinguish from beneficial alternatives using the current threshold-based filtering approach.

**Open Question 2:** How does context window reduction affect the safety and quality of real-time lexical simplification on resource-constrained mobile devices? Unresolved because current inference time (~1.2 seconds for 30 tokens) may be too slow for real-time mobile use, but truncating context may degrade model understanding of target word semantics, potentially increasing harmful outputs.

**Open Question 3:** Can LLM-as-a-judge approaches effectively automate harmfulness detection in lexical simplification while maintaining consistency with human annotations? Unresolved because manual evaluation is costly and not scalable, but no automated harmfulness detection method has been validated against the paper's annotation scheme across the five languages studied.

**Open Question 4:** How robust is the log-probability filtering strategy across different quantization schemes for small LLMs? Unresolved because the study used 4-bit QLoRA for fine-tuning and GGUF for deployment, but the relationship between quantization level and probability-based safety signal quality remains unexplored.

## Limitations
- The filtering strategy using log-probability scores may be less reliable for morphologically complex languages where grammatically incorrect outputs can still have reasonable probabilities
- Manual harmfulness annotation depends on subjective judgments from a limited number of annotators per language, potentially introducing inter-annotator variability
- The study evaluates only two small LLM architectures, limiting generalizability to other small model families or parameter ranges
- Exact prompt configurations and synthesized training data filtering thresholds are unspecified, affecting reproducibility

## Confidence
**High Confidence:** Efficiency improvements (latency reductions) and the general observation that fine-tuning improves automatic metrics while increasing harmful outputs are well-supported by the experimental design and results.

**Medium Confidence:** The claim that small LLMs can achieve "trustworthy" lexical simplification is qualified by the observed trade-off between performance gains and safety risks. The filtering approach shows promise but its practical effectiveness depends on specific application tolerances.

**Low Confidence:** The assertion that this work "establishes a benchmark for efficient and safe LS" is somewhat premature given the limited model and language coverage, and the absence of comparison with other safety-aware approaches.

## Next Checks
1. **Cross-Lingual Log-Probability Stability Test:** Evaluate the log-probability harmfulness detection across additional languages, particularly those with rich morphology or non-Latin scripts, to assess whether the filtering threshold generalizes or requires language-specific calibration.

2. **Annotation Reliability Assessment:** Conduct inter-annotator agreement analysis on the harmfulness annotations to quantify the subjectivity in the manual evaluation and determine whether the reported filtering effectiveness is robust to annotator variability.

3. **Real-World Deployment Simulation:** Implement the filtering strategy in a simulated deployment scenario where harmful outputs are rejected and replaced with teacher model generations, measuring the practical latency impact and evaluating whether the rejection rate remains manageable in production settings.