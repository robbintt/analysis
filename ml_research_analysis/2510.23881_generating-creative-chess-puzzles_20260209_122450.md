---
ver: rpa2
title: Generating Creative Chess Puzzles
arxiv_id: '2510.23881'
source_url: https://arxiv.org/abs/2510.23881
tags:
- chess
- puzzles
- lichess
- creative
- analyse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating creative, aesthetic,
  and counter-intuitive chess puzzles using Generative AI and reinforcement learning.
  The authors formalize creativity metrics such as counter-intuitiveness, aesthetics,
  and novelty, and develop novel reward functions based on chess engine search statistics.
---

# Generating Creative Chess Puzzles

## Quick Facts
- arXiv ID: 2510.23881
- Source URL: https://arxiv.org/abs/2510.23881
- Reference count: 40
- Key outcome: RL approach achieves 10x increase in counter-intuitive puzzles (0.22% → 2.5%), surpassing Lichess baseline (2.1%) and best Lichess-trained model (0.4%)

## Executive Summary
This paper presents a novel approach to generating creative chess puzzles using generative AI and reinforcement learning. The authors formalize creativity metrics such as counter-intuitiveness, aesthetics, and novelty, and develop novel reward functions based on chess engine search statistics. By training generative models on the Lichess puzzle dataset and refining them with reinforcement learning that incorporates diversity filters, they achieve significant improvements in generating counter-intuitive puzzles. Human expert evaluation confirms that the generated puzzles are more creative and enjoyable than standard Lichess puzzles, approaching the quality of classic chess compositions.

## Method Summary
The method uses a 200M parameter decoder-only Transformer trained on the Lichess puzzle dataset (4.4M positions). The model is fine-tuned using Critic-free PPO reinforcement learning with a custom reward function based on Stockfish search statistics. The reward function prioritizes counter-intuitiveness by measuring the gap between shallow and deep engine evaluations, specifically rewarding positions where the correct move is invisible at low depths but discoverable at high depths. Diversity filters based on Levenshtein distance and sequence entropy prevent reward hacking and maintain puzzle quality. The approach significantly improves counter-intuitive puzzle generation while maintaining legal, realistic chess positions.

## Key Results
- RL approach increases counter-intuitive puzzle generation from 0.22% to 2.5% (10x improvement)
- Generated puzzles surpass Lichess baseline (2.1%) and best Lichess-trained model (0.4%)
- Human expert evaluation confirms generated puzzles are more creative, enjoyable, and counter-intuitive than standard Lichess puzzles
- Final curated booklet praised by world-renowned experts for creativity

## Why This Works (Mechanism)

### Mechanism 1: Search-Discrepancy Reward Shaping
Optimizing for the gap between shallow and deep engine search appears to be a viable proxy for generating counter-intuitive positions that humans find creative. The reward function calculates a "counter-intuitiveness score" as a weighted linear combination of search statistics, specifically prioritizing the "critical depth" (the depth at which Stockfish first identifies the final solution). By rewarding positions where the correct move is invisible at low depths but discoverable at high depths, the model learns to generate puzzles that defy "intuitive" (shallow) evaluation. This mechanism assumes that engine search depth correlates with human intuition.

### Mechanism 2: Novelty Filtering via Entropy Masking
Hard filtering of replay buffer samples based on Levenshtein distance and sequence entropy prevents the policy from collapsing into a single high-reward mode. Instead of relying solely on an entropy bonus in the loss function, the system uses a filter. Generated positions are compared against a replay buffer; if they are too similar to existing positions or have low sequence-level entropy, they receive a zero reward regardless of their puzzle quality. This assumes that "diversity" can be effectively captured by syntactic distance and model uncertainty.

### Mechanism 3: Supervised Pre-training as a Feasibility Anchor
Initializing the policy with a model pre-trained on legal puzzle data is required to ground the RL agent in the distribution of legal, realistic chess positions. The "Zero-RL" ablation shows that a randomly initialized transformer cannot generate valid FEN strings representing legal boards. Pre-training on the Lichess dataset constrains the policy to a manifold of realistic positions before the RL objective pushes it toward high-reward "creative" outliers. This assumes the distribution of "standard" puzzles contains the necessary latent structure to generate "creative" puzzles.

## Foundational Learning

- **Search Statistics (Depth vs. Evaluation):** Why needed - The core innovation is using search depth rather than just evaluation score to define creativity. Quick check - If Stockfish evaluates a position as +5.0 immediately at Depth 1, does it have a high "critical depth"?

- **Sequence-Level Entropy (in Autoregressive Models):** Why needed - The paper uses sequence entropy as a proxy for "novelty." Low entropy indicates the model is in a "safe," repetitive mode; high entropy suggests it is exploring unknown territory. Quick check - Does a low sequence entropy in a decoder-only transformer imply the model is generating a very common sequence or a highly novel one?

- **KL Divergence Constraints in RLHF:** Why needed - The method uses KL penalties to prevent the RL agent from drifting too far from the pre-trained model (becoming unrealistic). This is a standard RLHF technique applied here to "Realism." Quick check - If the KL divergence between the RL policy and the pre-trained policy increases rapidly, what is happening to the "realism" of the generated samples?

## Architecture Onboarding

- **Component map:** Generator (200M Parameter Decoder-only Transformer) -> Verifier (Stockfish 17 + AlphaZero) -> Novelty Buffer -> Trainer (Critic-free PPO)

- **Critical path:** Transformer samples a FEN string → Verifier checks Legality (reward -2 if fail) → Verifier runs Search to find uniqueness and counter-intuitiveness → Novelty Buffer checks board and PV similarity, sequence entropy → If passed, Reward = +1, Update Replay Buffer and optimize PPO objective

- **Design tradeoffs:** Uniqueness vs. Counter-intuitiveness - Optimizing for high uniqueness explicitly may suppress the complex positions needed for counter-intuitiveness. Filtering vs. Exploration - Hard filtering ensures diversity but wastes computation on rejected samples, while soft regularization is computationally efficient but led to OOD samples.

- **Failure signatures:** Piece Inflation - Model generates boards with 3 Queens or 5 Knights. Fix: Add piece count penalty. Reward Hacking - Model generates the exact same high-reward puzzle repeatedly. Fix: Enable novelty filter. Search Gap Failure - Generated puzzles are "tricks" that only fool the engine because of horizon effects. Fix: Tune the "Critical Point" weight lower.

- **First 3 experiments:** Golden Set Validation - Run the provided reward function on the "Golden Set" to reproduce the Average Precision scores reported in Table 1. Entropy Collapse Ablation - Train a vanilla PPO agent with high entropy bonus but without the diversity filter to observe the "collapse to one OOD example" phenomenon. Search Metric Ablation - Train two small agents: one using only "Uniqueness" as reward, and one using only "Counter-intuitiveness" to verify the different puzzle types they generate.

## Open Questions the Paper Calls Out

### Open Question 1
Can the "counter-intuitiveness" reward formulation, based on contrasting shallow and deep search evaluations, be generalized to non-game domains like automated theorem proving or LLM reasoning? The authors suggest their methodology components are "broadly generalizable" and could be applied "beyond the domain of chess," specifically citing "automated theorem proving" or "prompting 'deeper thinking' in large language models." This remains unresolved as the current study validates the framework exclusively within the domain of chess.

### Open Question 2
Does incorporating human-mimicking engines (e.g., Maia) into the reward function improve the alignment between generated puzzles and human perceptions of creativity and surprise? The authors identify a gap where current rewards rely on Stockfish/AlphaZero, suggesting future work could incorporate engines like Leela or Maia which are "designed to mimic human play more closely." This remains unresolved as current metrics proxy intuition using standard engine search depths.

### Open Question 3
How can computational models capture the "centrality" or "significance" of an aesthetic theme rather than just its mere presence to avoid generating uninteresting false positives? The authors highlight the "difficulty in encoding nuanced human intuition" regarding what makes a theme's presence "significant" or "creative," noting that defining a theme's centrality remains "elusive." This remains unresolved as current theme detectors flag patterns that exist but lack strategic importance.

## Limitations

- Evaluation of "creativity" and "aesthetic quality" remains inherently subjective, relying heavily on expert judgment rather than objective metrics
- The methodology assumes that engine search depth correlates with human intuition, but this relationship may not hold across all chess skill levels or puzzle types
- The diversity filtering mechanism may inadvertently block valid creative outputs that happen to be syntactically similar to existing puzzles

## Confidence

- **High Confidence:** Technical implementation of the RL framework with diversity filters is well-documented and reproducible
- **Medium Confidence:** Correlation between engine search statistics and human-perceived creativity requires further validation across broader expert populations
- **Medium Confidence:** Claim that generated puzzles "approach the quality of classic chess compositions" lacks systematic comparison with a larger corpus of traditional compositions

## Next Checks

1. **Expert Validation Expansion:** Conduct a larger-scale evaluation with 20+ chess experts across different skill levels, using a blind test design comparing AI-generated puzzles against both Lichess puzzles and classic compositions to validate the creativity assessment methodology.

2. **Cross-Domain Transferability Test:** Apply the search-discrepancy reward mechanism to a different game domain (e.g., Go or Shogi) where reliable search algorithms exist, to test whether the counter-intuitiveness metric generalizes beyond chess.

3. **Long-term Diversity Analysis:** Track the entropy and Levenshtein distance distributions of generated puzzles over extended training periods (50k+ steps) to verify that the diversity filtering mechanism maintains novelty without converging to a local optimum.