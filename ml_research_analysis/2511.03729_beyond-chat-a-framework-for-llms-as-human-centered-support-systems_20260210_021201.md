---
ver: rpa2
title: 'Beyond Chat: a Framework for LLMs as Human-Centered Support Systems'
arxiv_id: '2511.03729'
source_url: https://arxiv.org/abs/2511.03729
tags:
- systems
- llms
- human
- support
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a role-based framework for human-centered
  LLM support systems, categorizing them into four roles: companions, coaches, mediators,
  and curators. It compares real-world deployments across domains, identifies cross-cutting
  design principles (transparency, personalization, guardrails, memory with privacy,
  empathy-reliability balance), and outlines evaluation metrics extending beyond accuracy
  to trust, engagement, and longitudinal outcomes.'
---

# Beyond Chat: a Framework for LLMs as Human-Centered Support Systems

## Quick Facts
- arXiv ID: 2511.03729
- Source URL: https://arxiv.org/abs/2511.03729
- Authors: Zhiyin Zhou
- Reference count: 17
- Proposes a role-based framework for human-centered LLM support systems, categorizing them into four roles: companions, coaches, mediators, and curators

## Executive Summary
This paper proposes a role-based framework for human-centered LLM support systems, categorizing them into four roles: companions, coaches, mediators, and curators. It compares real-world deployments across domains, identifies cross-cutting design principles (transparency, personalization, guardrails, memory with privacy, empathy-reliability balance), and outlines evaluation metrics extending beyond accuracy to trust, engagement, and longitudinal outcomes. The paper analyzes risks including over-reliance, hallucination, bias, privacy exposure, and unequal access, and proposes future directions spanning unified evaluation, hybrid human-AI models, memory architectures, cross-domain benchmarking, and governance. The goal is to support responsible integration of LLMs in sensitive settings where people need accompaniment and guidance, not only answers.

## Method Summary
The paper develops a conceptual framework through literature review and analysis of existing LLM deployments across four domains. It identifies four distinct roles (companion, coach, mediator, curator) based on relational orientations toward users and derives corresponding design requirements and evaluation metrics. The framework synthesizes principles from human-computer interaction, psychotherapy, and AI ethics literature. No empirical validation studies are reported; the work is primarily theoretical and prescriptive.

## Key Results
- Proposes a four-role taxonomy (companion, coach, mediator, curator) for LLM support systems
- Identifies cross-cutting design principles: transparency, personalization, guardrails, memory with privacy, empathy-reliability balance
- Defines evaluation metrics extending beyond accuracy to include trust, engagement, longitudinal outcomes, and dependency patterns
- Outlines major risks: over-reliance, hallucination, bias, privacy exposure, and unequal access

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Role-based framing enables context-appropriate design and evaluation of LLM support systems.
- Mechanism: By categorizing LLM functions into companion, coach, mediator, and curator roles, designers can align system behavior, guardrails, and success metrics with the specific relational demands of each context (emotional presence vs. skill-building vs. access vs. information filtering).
- Core assumption: Users experience LLMs relationally rather than transactionally, and different relational modes require different failure modes and success criteria.
- Evidence anchors:
  - [abstract] "proposes a role-based framework for human-centered LLM support systems, categorizing them into four roles: companions, coaches, mediators, and curators"
  - [Section 3.2] "Each role reflects a distinct orientation toward the human user and comes with specific design requirements"
  - [corpus] Related work on human-centered evaluation frameworks (e.g., "Toward a Human-Centered Evaluation Framework for Trustworthy LLM-Powered GUI Agents") similarly argues for context-specific risk and metric alignment, though no direct validation of the four-role schema exists.
- Break condition: If users do not perceive LLMs in relational terms, or if design requirements across roles prove indistinguishable in practice, the framework collapses into generic chatbot design.

### Mechanism 2
- Claim: Longitudinal engagement and trust metrics predict system effectiveness better than accuracy alone in support contexts.
- Mechanism: Human-centered support requires sustained interaction; therefore, metrics like engagement duration, skill progression, emotional well-being, and dependency patterns capture whether the system scaffolds growth rather than providing one-time correct answers.
- Core assumption: Accuracy is necessary but insufficient; user outcomes depend on trust, continuity, and appropriate calibration of empathy vs. reliability.
- Evidence anchors:
  - [abstract] "evaluation metrics extending beyond accuracy to trust, engagement, and longitudinal outcomes"
  - [Section 5.1] "A companion system may provide factually accurate information yet fail to convey empathy, leaving the user unsatisfied"
  - [Section 5.3] Lists engagement, progress, emotional well-being, and dependency metrics as longitudinal measures
  - [corpus] "Large Language Model Psychometrics" review similarly calls for human-centered evaluation beyond static benchmarks, but does not validate specific longitudinal metrics for support roles.
- Break condition: If longitudinal metrics correlate poorly with actual user outcomes, or if short-term accuracy proves more predictive of benefit in certain domains, the expanded evaluation framework adds complexity without value.

### Mechanism 3
- Claim: Hybrid human-AI architectures with explicit handoff mechanisms can preserve scalability while maintaining accountability.
- Mechanism: LLMs handle routine, scalable support (drafting, practice, initial interpretation), while human experts review high-stakes outputs or intervene when uncertainty, risk, or complexity thresholds are crossed.
- Core assumption: Clear handoff protocols exist, and humans remain available and accountable for escalated cases; AI does not independently operate in domains where errors cause significant harm.
- Evidence anchors:
  - [abstract] "hybrid human-AI models" listed as a future direction
  - [Section 7.2] "an AI mediator might draft legal documents that are then reviewed by a human lawyer, or an AI coach might provide daily practice exercises while a teacher oversees long-term progress"
  - [corpus] Related papers on privacy-aware agents and trustworthy GUI agents emphasize human oversight, but empirical evidence on optimal handoff design remains sparse.
- Break condition: If handoff protocols are ambiguous, if human reviewers become bottlenecks or rubber-stamp AI outputs, or if liability for errors remains unresolved, hybrid systems may either fail to scale or fail to protect users.

## Foundational Learning

- Concept: **Zone of Proximal Development (ZPD)**
  - Why needed here: The paper explicitly references Vygotsky's ZPD to explain how LLMs function as scaffolding agents that help learners progress beyond independent capacity. Understanding scaffolding is essential for designing coach and companion roles.
  - Quick check question: Can you explain how an LLM acting as a coach would provide support within a learner's ZPD without overshooting into frustration or undershooting into boredom?

- Concept: **Therapeutic Alliance**
  - Why needed here: Companion systems depend on trust, empathy, and continuity, which the paper links to psychotherapy's concept of the therapeutic alliance. This frames why relational qualities matter more than factual accuracy in emotional support contexts.
  - Quick check question: What design features would strengthen vs. weaken a perceived therapeutic alliance between a user and an AI companion?

- Concept: **Computers Are Social Actors (CASA) Paradigm**
  - Why needed here: The paper cites Reeves and Nass to explain why users anthropomorphize conversational systems. This underpins why people form emotional attachments to AI companions and why over-reliance risks emerge.
  - Quick check question: If users treat LLMs as social actors, what safeguards should be implemented to prevent harmful emotional dependency?

## Architecture Onboarding

- Component map:
  - Role Layer -> Design Principles Layer -> Evaluation Layer -> Risk Mitigation Layer -> Handoff Layer

- Critical path:
  1. Define the primary role(s) the system will fulfill.
  2. Map role-specific design requirements (e.g., companion → emotional sensitivity + continuity; curator → citation accuracy + verifiability).
  3. Implement memory with user-controlled deletion and encryption.
  4. Build guardrails: uncertainty displays, source citations, professional resource redirects.
  5. Instrument longitudinal metrics from day one (engagement patterns, skill assessments, well-being surveys, dependency indicators).
  6. Design human handoff protocols with clear escalation triggers.

- Design tradeoffs:
  - Empathy vs. accuracy: Overly encouraging feedback may obscure errors; strict correction may undermine trust.
  - Personalization vs. privacy: Rich user profiles improve adaptation but increase data exposure risk.
  - Accessibility vs. nuance: Simplifying complex information (mediator, curator roles) may lose critical details.
  - Memory continuity vs. right-to-forget: Long-term context improves support but conflicts with deletion expectations.

- Failure signatures:
  - Users report emotional distress or dependency (companion over-reliance).
  - Hallucinated citations or fabricated advice in high-stakes outputs (curator/mediator failure).
  - Engagement drops after initial novelty period (longitudinal value not realized).
  - Bias complaints from underrepresented user groups (personalization amplifying inequity).
  - Privacy breach or unauthorized data retention (memory architecture failure).

- First 3 experiments:
  1. **Role alignment test**: Deploy the same LLM with role-specific system prompts (companion vs. coach vs. curator) to separate user cohorts; measure whether interaction patterns and user satisfaction diverge as predicted by the framework.
  2. **Memory-privacy tradeoff study**: Implement selective memory retention with user-controlled deletion; compare longitudinal engagement and trust scores against a no-memory baseline.
  3. **Handoff threshold calibration**: In a hybrid mediator prototype (e.g., legal document drafting), vary uncertainty thresholds for human escalation; measure error rates, user outcomes, and reviewer workload to identify viable operating points.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can systems that simultaneously fulfill multiple roles (e.g., coach and curator, companion and mediator) systematically resolve design conflicts between empathy and accuracy, or between simplification and comprehensiveness?
- Basis in paper: [explicit] Section 3.5 states: "Future work should investigate systematic methods for role integration, including adaptive switching mechanisms, multi-role evaluation benchmarks, and guidelines for mitigating conflicts when roles converge."
- Why unresolved: The framework identifies that roles overlap in practice but provides no mechanism for resolving trade-offs when design principles conflict.
- What evidence would resolve it: Empirical studies comparing single-role vs. multi-role systems, and prototypes demonstrating adaptive switching with measurable conflict resolution.

### Open Question 2
- Question: What are valid, reliable longitudinal metrics for detecting unhealthy over-reliance in companion and coaching systems before it causes harm?
- Basis in paper: [explicit] Section 6.1 calls for "evaluation [that] should monitor not only engagement levels but also patterns that suggest unhealthy dependency," and Section 5.3 lists "dependency metrics" as needed but undefined.
- Why unresolved: Current engagement metrics (frequency, duration) conflate healthy use with dependency; no validated instruments exist for AI-specific over-reliance.
- What evidence would resolve it: Development and validation of dependency scales through longitudinal user studies with clinical outcomes.

### Open Question 3
- Question: How should responsibility and liability be allocated between LLM systems and human professionals in hybrid human–AI support models?
- Basis in paper: [explicit] Section 7.2 notes: "Research is needed to determine optimal divisions of labor between human and AI agents, as well as strategies for seamless handoff when complex or high-risk situations arise."
- Why unresolved: Legal and ethical frameworks for shared accountability in domains like legal mediation and mental health coaching remain undefined.
- What evidence would resolve it: Comparative deployment studies with different liability models, and policy analysis across jurisdictions.

## Limitations

- The framework remains largely theoretical without empirical validation across real-world deployments
- Proposed longitudinal metrics lack validated measurement instruments
- Hybrid human-AI architectures face unresolved liability questions with no clear legal frameworks
- No empirical evidence demonstrates that users consistently experience LLMs through the proposed four distinct relational modes

## Confidence

- **High confidence**: The observation that accuracy alone inadequately captures LLM support system effectiveness; this aligns with established HCI findings that user experience depends on trust, emotional resonance, and sustained engagement.
- **Medium confidence**: The four-role framework provides useful design thinking scaffolding; while conceptually coherent and consistent with CASA and therapeutic alliance literature, empirical validation of distinct role-specific requirements remains pending.
- **Low confidence**: Specific evaluation metrics and handoff protocols will generalize across domains; these claims require domain-specific validation and may vary significantly based on regulatory context, user population, and task complexity.

## Next Checks

1. Deploy the same LLM backend with role-specific system prompts (companion vs. coach vs. curator) to separate user cohorts; measure interaction patterns, satisfaction scores, and longitudinal outcomes to test whether the framework predicts meaningful behavioral differences.

2. Implement selective memory retention with user-controlled deletion in companion/coach systems; compare longitudinal engagement and trust metrics against a no-memory baseline while monitoring privacy complaint rates.

3. In a hybrid mediator prototype (e.g., legal document drafting), vary uncertainty thresholds for human escalation; measure error rates, user outcomes, reviewer workload, and time-to-completion to identify viable operating points that balance safety and scalability.