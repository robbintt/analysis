---
ver: rpa2
title: 'Open-sci-ref-0.01: open and reproducible reference baselines for language
  model and dataset comparison'
arxiv_id: '2509.09009'
source_url: https://arxiv.org/abs/2509.09009
tags:
- training
- tokens
- reference
- datasets
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces open-sci-ref-0.01, a family of dense transformer
  models trained as research baselines across model scales (0.13B to 1.7B parameters)
  and token scales (up to 1T tokens) on 8 recent open reference datasets. The authors
  establish reference points through standardized benchmark evaluations that enable
  researchers to assess the sanity and quality of alternative training approaches
  across scales and datasets.
---

# Open-sci-ref-0.01: open and reproducible reference baselines for language model and dataset comparison

## Quick Facts
- arXiv ID: 2509.09009
- Source URL: https://arxiv.org/abs/2509.09009
- Reference count: 31
- Primary result: Established reference baselines show Nemotron-CC-HQ outperforms other datasets for pretraining, with English-only pretraining achieving superior multilingual performance compared to explicitly multilingual training

## Executive Summary
This paper introduces open-sci-ref-0.01, a family of dense transformer models trained as research baselines across model scales (0.13B to 1.7B parameters) and token scales (up to 1T tokens) on 8 recent open reference datasets. The authors establish reference points through standardized benchmark evaluations that enable researchers to assess the sanity and quality of alternative training approaches across scales and datasets. Their training runs use a modern, reproducible pipeline with QK-Norm and constant learning rate with linear cooldown schedule. The established reference baselines allow training procedures to be compared through their scaling trends on a common compute axis.

## Method Summary
The authors train Llama-style dense transformers (0.13B-1.7B parameters) on 8 reference datasets using Megatron-LM with QK-Norm normalization, SwiGLU activation, and tied embeddings. Training uses a WSD (Warmup-Stable-Decay) schedule with constant learning rate and 20% linear cooldown. Models are evaluated on 11 downstream tasks via lm-eval-harness. The pipeline enables comparison of training procedures and datasets across scales by establishing performance baselines on a common compute axis.

## Key Results
- Training on Nemotron-CC-HQ consistently outperforms other reference datasets, followed by DCLM-baseline and FineWeb-Edu
- Performance differences become more pronounced at larger scales
- Reference models match or outperform HuggingFace baselines despite using less compute
- English-only pretraining on high-quality datasets achieves superior multilingual performance compared to explicitly multilingual pretraining

## Why This Works (Mechanism)

### Mechanism 1: WSD Learning Rate Schedule
Constant learning rate with linear cooldown matches or outperforms cosine decay while enabling training resumption and compute savings across token budgets. The schedule maintains peak learning rate during the stable phase, allowing the model to converge to a good basin, then applies linear decay only in the final 20% to refine the solution.

### Mechanism 2: QK-Norm in Attention
Normalizing query and key vectors prevents attention logit explosion during training, enabling higher learning rates and more stable gradient flow. This is particularly important for deep models and high learning rates where unnormalized Q/K magnitudes would cause instability.

### Mechanism 3: English-Dominant Multilingual Transfer
High-quality English instruction-like data develops robust reasoning capabilities that transfer across languages via shared semantic structure. This allows English-only pretraining to achieve superior multilingual performance compared to explicitly multilingual pretraining, particularly on benchmarks like MMLU.

## Foundational Learning

- **WSD (Warmup-Stable-Decay) Learning Rate Schedule**: The entire training pipeline depends on this schedule; understanding why constant LR + cooldown works is essential to reproducing results. *Quick check: Can you explain why WSD enables training resumption more easily than cosine decay?*

- **QK-Norm in Attention**: Architecture modification that differs from standard Llama; impacts training stability and hyperparameter choices. *Quick check: What problem does normalizing query and key vectors solve in transformer attention?*

- **Compute-Optimal Scaling**: The paper aligns comparisons on a "common compute axis" (FLOPs); understanding compute-optimal frontiers is necessary to interpret scaling trends. *Quick check: Why does the paper warn against interpreting steep scaling slopes from suboptimal small-scale models?*

## Architecture Onboarding

- **Component map**: Tokenize with GPT-NeoX-20B tokenizer (vocab 50,304) -> Train with Megatron-LM using WSD schedule (constant LR + 20% cooldown) -> Evaluate via lm-eval-harness on 11 downstream tasks

- **Critical path**: 
  1. Tokenize with GPT-NeoX-20B tokenizer (vocab 50,304)
  2. Train with Megatron-LM using WSD schedule (constant LR + 20% cooldown)
  3. Evaluate via lm-eval-harness on 11 downstream tasks

- **Design tradeoffs**: 
  - Adding biases improved performance in ablation at 1.3B scale but increases parameter count
  - WSD schedule sacrifices potential peak performance at fixed token budget for flexibility across budgets
  - Tied embeddings reduce parameters but may limit expressivity at larger scales

- **Failure signatures**: 
  - Random-chance MMLU (~0.25): Likely indicates dataset lacks instruction-like samples
  - Unstable training with high LR: QK-Norm may not be enabled correctly
  - Suboptimal performance at 1T tokens: Warmup may be insufficient (paper found 25K iterations necessary)

- **First 3 experiments**:
  1. Reproduce 1.3B model on C4 at 50B tokens using hyperparameters from Table 3 to validate pipeline
  2. Ablate QK-Norm vs. no QK-Norm at 0.4B scale to observe stability differences
  3. Compare Nemotron-CC-HQ vs. FineWeb-Edu at 300B tokens on 0.4B model to confirm dataset ranking

## Open Questions the Paper Calls Out

### Open Question 1: Multilingual Data Contamination
Is the surprising multilingual capability of English-dominant models trained on Nemotron-CC-HQ caused by the inclusion of non-English data slipping through filtering tolerance thresholds? The authors hypothesize this could come from the fact that the tolerance threshold used when filtering for English text lets further substantial data in another languages to slip into Nemotron dataset.

### Open Question 2: Scaling Law Derivation
Can full scaling laws derived from these reference baselines enable robust performance predictions at larger scales (e.g., >1.7B parameters) where simple trend extrapolation fails? The authors state that establishing comparisons valid across broad scales "remains an open challenge and will require the derivation of full scaling laws."

### Open Question 3: Permissively Licensed Datasets
How do models trained exclusively on permissively licensed datasets (e.g., CommonPile, MixtureVitae) compare in quality and efficiency to the current reference baselines that contain non-permissive content? The authors explicitly list studying permissively licensed datasets as future work.

## Limitations

- Dataset quality heterogeneity: The ranking could shift if different evaluation tasks or metrics are used
- Context length constraints: Scaling trends and performance differences may not hold at longer context lengths
- Reproducibility dependencies: Successful replication requires specific versions of Megatron-LM, lm-eval-harness, and tokenizer implementations

## Confidence

- **High Confidence**: Dataset ranking (Nemotron-CC-HQ > DCLM-baseline > FineWeb-Edu)
- **Medium Confidence**: WSD schedule superiority
- **Medium Confidence**: QK-Norm benefits
- **Low Confidence**: English-only multilingual superiority

## Next Checks

1. **Schedule Ablation**: Train identical 1.3B models on C4 using both WSD and cosine decay schedules with the same 300B token budget. Measure final performance, training stability, and verify training resumption.

2. **Dataset Quality Analysis**: Analyze the token-level composition of Nemotron-CC-HQ versus DCLM-baseline and FineWeb-Edu to identify specific quality signals that correlate with downstream performance differences.

3. **Multilingual Generalization Test**: Evaluate the 1.7B English-only models on additional multilingual benchmarks beyond MMLU, including cross-lingual question answering, named entity recognition, and translation quality assessments.