---
ver: rpa2
title: Dissecting the Impact of Mobile DVFS Governors on LLM Inference Performance
  and Energy Efficiency
arxiv_id: '2507.02135'
source_url: https://arxiv.org/abs/2507.02135
tags:
- frequency
- energy
- inference
- governor
- frequencies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the performance and energy inefficiency
  caused by independent DVFS governors for CPU, GPU, and memory when running LLM inference
  on mobile devices. The authors measure that default governors can increase latency
  by up to 40.4% or energy by up to 16.6% compared to optimal frequency combinations.
---

# Dissecting the Impact of Mobile DVFS Governors on LLM Inference Performance and Energy Efficiency

## Quick Facts
- **arXiv ID**: 2507.02135
- **Source URL**: https://arxiv.org/abs/2507.02135
- **Reference count**: 40
- **Primary result**: Default independent DVFS governors increase latency by up to 40.4% and energy by up to 16.6% compared to optimal frequency combinations; FUSE governor reduces TTFT by 7.0-16.9% and TPOT by 25.4-36.8% while maintaining equal energy-per-token.

## Executive Summary
This work investigates how independent CPU, GPU, and memory DVFS governors on mobile devices degrade LLM inference performance and energy efficiency. Through controlled experiments, the authors demonstrate that governors select overly low frequencies due to low utilization measurements and antagonize each other in a "downward spiral," where CPU governor lowering frequency delays GPU task dispatch, triggering GPU governor to further lower frequency. To address this, they propose FUSE, a unified energy-aware governor that performs offline profiling to select optimal CPU/GPU frequencies per model and sequence length, achieving significant latency reductions while maintaining energy efficiency.

## Method Summary
The authors conduct experiments on Google Pixel 7/Pro with Google Tensor G2, running six decoder models (TinyLlama-1.1B through Llama-2-7B) in 4-bit quantization using llama.cpp with OpenCL support. They implement frequency pinning by writing to governor min/max interfaces, measure power with Monsoon power monitor (0.2ms sampling), and evaluate FUSE through a two-step offline search algorithm that scans GPU frequencies first, then fine-tunes CPU frequency. FUSE categorizes prefill lengths into five ranges and profiles one decode length plus five representative prefill lengths per model, requiring 14.5-30.8 inferences per model to build its lookup table.

## Key Results
- Default governors increase end-to-end latency by up to 40.4% and energy by up to 16.6% compared to optimal pinned frequencies
- FUSE reduces time-to-first-token (TTFT) by 7.0-16.9% and time-per-output-token (TPOT) by 25.4-36.8% while maintaining equal energy-per-token
- The "downward spiral" effect causes GPU frequency to cascade from 848 MHz to 151 MHz while CPU drops from 1277 MHz to 500 MHz in under 700 ms
- Offline profiling reduces search space from 2808 combinations to 14-31 inferences per model (374× reduction)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Independent CPU and GPU governors drive each other to progressively lower frequencies, degrading both performance and energy efficiency.
- Mechanism: When CPU utilization appears low, the CPU governor lowers frequency to meet utilization targets. This slows the OpenCL runtime on CPU, delaying GPU task dispatch, which reduces GPU utilization. The GPU governor responds by lowering GPU frequency, which extends wait times between tasks and further reduces CPU utilization—perpetuating a downward spiral.
- Core assumption: Governors optimize for individual component utilization targets without cross-component dependency awareness.
- Evidence anchors:
  - [abstract]: "GPU and CPU governors antagonize each other in a 'downward spiral,' further lowering frequencies and degrading efficiency."
  - [section 5.4]: Fig. 10 shows GPU frequency cascading from 848 MHz to 151 MHz while CPU drops from 1277 MHz to 500 MHz in under 700 ms.
  - [corpus]: No direct corpus evidence for this specific antagonistic mechanism in LLM inference contexts.
- Break condition: If governors coordinated decisions or if utilization metrics accounted for dependency-induced stalls rather than treating all idle time uniformly.

### Mechanism 2
- Claim: Default governors select overly low frequencies because LLM inference exhibits low measured utilization, which governors misinterpret.
- Mechanism: During decode, GPU utilization remains ~70% even at high CPU frequency (below vendor thresholds), prompting downward scaling. CPU utilization ranges 17–25%, causing EAS to select bimodal low frequencies (e.g., alternating 1426 MHz and 500–851 MHz). Governors cannot distinguish "idle because not needed" from "idle because blocked by upstream dependency."
- Core assumption: Utilization-based DVFS logic treats all low-utilization periods as opportunities to save power, regardless of cause.
- Evidence anchors:
  - [abstract]: "each governor tends to select overly low frequencies due to low utilization."
  - [section 5.1]: GPU governor operates TinyLlama at effective 424.4 MHz (optimal 848 MHz); EAS chooses 1130.8 MHz (optimal 2252 MHz).
  - [corpus]: Related DVFS-aware DNN inference work (arXiv:2502.06295, 2509.17970) confirms frequency scaling impacts DNN inference latency and energy but does not address the utilization interpretation problem for dependent pipelines.
- Break condition: If utilization metrics incorporated dependency-aware stall accounting or if workloads had higher intrinsic utilization.

### Mechanism 3
- Claim: Offline profiling can identify near-optimal CPU/GPU frequency combinations per model and sequence length, enabling runtime pinning that outperforms adaptive governors.
- Mechanism: FUSE performs a two-step search—first scanning GPU frequencies from highest downward until energy budget is met, then fine-tuning CPU frequency—reducing search from 2808 combinations to ~14–31 inferences per model. At runtime, it pins components to pre-computed frequencies based on prefill length category.
- Core assumption: Optimal frequencies are primarily determined by model architecture and sequence length, not specific input content.
- Evidence anchors:
  - [abstract]: "FUSE...performs offline profiling to select optimal CPU/GPU frequencies per model and sequence length."
  - [section 6.1–6.2]: Two-step search achieves 374× reduction; evaluation shows 7.0–16.9% TTFT reduction and 25.4–36.8% TPOT reduction with equal energy-per-token.
  - [corpus]: Metadata-Guided Adaptable Frequency Scaling (arXiv:2509.22707) explores profiling-based DVFS across heterogeneous apps, supporting the viability of offline profiling approaches.
- Break condition: If optimal frequencies varied significantly with input content, thermal conditions, or background workload interference.

## Foundational Learning

- Concept: **DVFS (Dynamic Voltage and Frequency Scaling)**
  - Why needed here: Central to the paper's problem—understanding how frequency choices trade off power draw versus execution time.
  - Quick check question: Why does lowering frequency not always reduce total energy for a fixed workload?

- Concept: **LLM Inference Stages (Prefill vs Decode)**
  - Why needed here: Paper shows governors behave differently across stages due to differing compute/memory characteristics.
  - Quick check question: Why would decode show lower GPU utilization than prefill even for the same model?

- Concept: **Energy Efficiency = Power × Runtime / Load**
  - Why needed here: The optimization objective—both power and runtime contribute to energy-per-token.
  - Quick check question: If frequency increases 20% and power increases 30% but runtime decreases 25%, does energy-per-token improve?

## Architecture Onboarding

- Component map:
  - CPU Governor (EAS) -> Frequency-invariant load estimation -> Per-cluster lookup table -> Lowest satisfying frequency
  - GPU Governor (Quickstep) -> Utilization thresholds per frequency level -> Scale up/down based on measured utilization
  - Memory Governor (MIF/interactive) -> Bus utilization -> Formula-based target frequency every 20 ms
  - OpenCL Runtime (CPU) -> Manages command queue, feeds kernels to shallow GPU hardware queue
  - FUSE -> Offline profiler -> Runtime lookup table -> Frequency pinning via governor min/max interfaces

- Critical path:
  1. Inference request arrives → FUSE determines prefill length bucket
  2. FUSE writes optimal frequencies to governor min/max interfaces (pin mode)
  3. Prefill stage executes at pinned frequencies → TTFT measured
  4. Transition to decode → FUSE may switch to decode-optimized frequencies
  5. Decode completes → TPOT and energy-per-token measured → governors released

- Design tradeoffs:
  - Offline profiling time (18–88 min/model) vs. runtime simplicity and consistency
  - Static frequency pinning vs. adaptivity to thermal throttling or concurrent workloads
  - Separate prefill/decode configurations vs. unified setting

- Failure signatures:
  - Effective GPU frequency clustering at 350–450 MHz during decode
  - CPU frequency showing bimodal distribution under EAS (e.g., 1426 + 851 MHz)
  - End-to-end latency 40%+ higher than optimal pinned configuration at same energy

- First 3 experiments:
  1. **Baseline characterization**: Run inference with default governors across models; record TTFT, TPOT, energy-per-token, and effective frequencies.
  2. **Exhaustive frequency sweep**: Pin all CPU/GPU/memory combinations; plot latency vs. energy Pareto frontier to identify optimality gap.
  3. **Isolation ablation**: Pin two components while letting the third governor run free; measure each governor's standalone suboptimality and cross-component effects.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided text.

## Limitations
- The antagonistic interaction between CPU and GPU governors is observed on a single device (Google Pixel 7/Pro) and may not generalize across different SoCs or governor implementations
- FUSE requires rooted devices for frequency control and power measurement, limiting practical deployment
- The assumption that optimal frequencies depend only on model and sequence length (not input content) is reasonable but unverified across diverse workloads

## Confidence

- **High Confidence**: The observation that default governors produce suboptimal frequencies, leading to measurable latency and energy inefficiencies (40.4% latency increase, 16.6% energy increase)
- **Medium Confidence**: The specific "downward spiral" mechanism between CPU and GPU governors, though well-supported by measurements, could vary with different governor implementations
- **Medium Confidence**: FUSE's performance improvements (7.0-16.9% TTFT reduction, 25.4-36.8% TPOT reduction) are well-measured but rely on offline profiling that may not adapt to dynamic conditions

## Next Checks
1. Test FUSE's generalizability by implementing it on a different mobile platform (e.g., Snapdragon-based device) to verify the downward spiral mechanism and performance improvements persist
2. Evaluate FUSE's robustness under concurrent workloads and thermal constraints by running background processes during inference and measuring degradation
3. Validate the content-independence assumption by profiling multiple diverse inputs per model/sequence length combination to measure performance variance