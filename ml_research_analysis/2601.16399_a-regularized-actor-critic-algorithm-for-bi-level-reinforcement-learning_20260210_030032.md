---
ver: rpa2
title: A Regularized Actor-Critic Algorithm for Bi-Level Reinforcement Learning
arxiv_id: '2601.16399'
source_url: https://arxiv.org/abs/2601.16399
tags:
- lemma
- algorithm
- bi-level
- where
- inequality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a single-loop, first-order actor-critic algorithm
  for bi-level reinforcement learning, where the upper-level variable parameterizes
  the reward of a lower-level MDP. The key innovation is introducing an attenuating
  entropy regularization to the lower-level objective, enabling asymptotically unbiased
  hyper-gradient estimation without requiring exact solution of the unregularized
  RL problem.
---

# A Regularized Actor-Critic Algorithm for Bi-Level Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2601.16399
- **Source URL:** https://arxiv.org/abs/2601.16399
- **Reference count:** 40
- **Primary result:** Achieves O(ε⁻¹⁰) sample complexity for bi-level RL with asymptotically unbiased hyper-gradient estimation via decaying entropy regularization.

## Executive Summary
This paper addresses bi-level reinforcement learning where an upper-level variable parameterizes the reward function of a lower-level MDP, and the objective depends on the optimal induced policy. The key innovation is introducing an attenuating entropy regularization to the lower-level objective, which enables asymptotically unbiased hyper-gradient estimation without requiring exact solution of the unregularized RL problem. The authors propose a single-loop actor-critic algorithm with five time scales that achieves convergence to a stationary point of the original bi-level RL objective. Experiments on GridWorld goal placement and language model fine-tuning demonstrate faster convergence and better performance than comparable baselines.

## Method Summary
The algorithm solves min_x Φ(x) = f(x, π*(x)) where x parameterizes the reward function r^x of a lower-level MDP and π*(x) is the optimal policy under r^x. A penalty-based reformulation with entropy regularization τ_k H(π) is used, where τ_k decays as τ₀/(k+1)^(1/20). The single-loop actor-critic maintains two policy iterates (θ_k for optimal policy, θ^L_k for penalty-augmented) and value estimates. Five time scales are employed: ζ_k ∝ (k+1)^(-9/10) for upper-level updates, α_k ∝ (k+1)^(-1/2) for policy gradient, β_k ∝ (k+1)^(-1/2) for value update, w_k ∝ (k+1)^(-3/20) for policy tracking, and τ_k ∝ (k+1)^(-1/20) for regularization decay. This enables asymptotically unbiased hyper-gradient estimation while maintaining convergence guarantees.

## Key Results
- Achieves O(ε⁻¹⁰) sample complexity for convergence to stationary point of unregularized bi-level objective
- Introduces asymptotically unbiased hyper-gradient estimator via decaying entropy regularization
- Demonstrates faster convergence and better performance than Partial SGD, Finite-Difference Approximation, and nested-loop variants
- Supports constant regularization variant with O(ε⁻³) sample complexity for faster convergence to regularized objective

## Why This Works (Mechanism)
The entropy regularization creates a time-varying optimization landscape that enables first-order hyper-gradient estimation without requiring exact solution of the unregularized RL problem. As τ_k → 0, the regularized policy converges to the optimal policy of the original problem, allowing the hyper-gradient to become unbiased asymptotically. The five time scales ensure proper tracking of the time-varying landscape while maintaining convergence guarantees. The algorithm's single-loop structure avoids the computational overhead of nested-loop methods while achieving similar theoretical guarantees.

## Foundational Learning
- **Bi-level optimization:** Optimization where the objective depends on the solution of another optimization problem. Needed to formulate the relationship between upper-level parameter x and lower-level optimal policy π*(x). Quick check: Verify understanding by identifying the upper and lower level problems in the GridWorld example.
- **Entropy regularization in RL:** Adding τH(π) to the RL objective to encourage exploration and create smooth optimization landscapes. Needed to enable tractable gradient estimation without exact policy optimization. Quick check: Confirm that τ = 0 recovers the original unregularized problem.
- **Hyper-gradient estimation:** Computing gradients of upper-level objective with respect to parameters that affect the lower-level problem. Needed to enable gradient-based optimization of the upper-level variable. Quick check: Verify the hyper-gradient estimator in Eq. (14) matches the theoretical derivation.
- **Time-scale separation in stochastic approximation:** Using different step sizes for different components to ensure proper convergence behavior. Needed to handle the coupling between upper-level updates and lower-level policy optimization. Quick check: Monitor that ζ_k ≪ α_k ≪ β_k ≪ w_k ≪ τ_k throughout training.
- **Softmax policy parameterization:** Using π(a|s) ∝ exp(θ_a,s) for tabular policies. Needed for tractable gradient computation and entropy regularization. Quick check: Verify policy gradients are correctly computed using the log-derivative trick.

## Architecture Onboarding

**Component Map:** Upper-level parameter x -> Reward function r^x -> Lower-level MDP -> Optimal policy π*(x) -> Upper-level objective Φ(x) -> Hyper-gradient estimation

**Critical Path:** x update → r^x computation → MDP interaction → Policy/value updates → Hyper-gradient accumulation → x update

**Design Tradeoffs:** Single-loop vs nested-loop (computational efficiency vs simplicity), decaying vs constant regularization (solution quality vs convergence speed), multiple time scales vs single time scale (convergence guarantees vs implementation simplicity)

**Failure Signatures:** Divergence when time-scale constraints violated, convergence to suboptimal regularized solution when τ_k decays too fast, poor performance when entropy regularization is insufficient for exploration

**3 First Experiments:**
1. GridWorld goal placement with known optimal solution to verify convergence behavior
2. Fixed-τ variant vs decaying-τ variant to demonstrate trade-off between convergence speed and solution quality
3. Comparison with Partial SGD baseline to validate hyper-gradient estimation accuracy

## Open Questions the Paper Calls Out

**Open Question 1:** Can variance reduction or momentum techniques be incorporated to improve the O(ε⁻¹⁰) convergence rate for unregularized bi-level RL? The current theoretical guarantees are derived for standard stochastic approximation without acceleration mechanisms. Evidence would be a theoretical proof demonstrating improved sample complexity (e.g., O(ε⁻⁵)) with momentum or variance reduction.

**Open Question 2:** Can the algorithm and convergence analysis be extended to settings where the transition kernel depends on the upper-level decision variable? The current analysis assumes P is independent of x, which simplifies the hyper-gradient expression significantly. Evidence would be a derivation of first-order hyper-gradient estimator accounting for ∇_x P(s'|s,a) with convergence proof.

**Open Question 3:** Can the sample complexity for the unregularized objective be improved to match the O(ε⁻³) rate achieved for the regularized problem? The substantial gap (O(ε⁻¹⁰) vs O(ε⁻³)) arises from tracking the time-varying landscape as regularization decays. Evidence would be a novel analysis achieving O(ε⁻³) or better for the unregularized problem, or a lower bound proving current rate is tight.

## Limitations
- Requires careful tuning of five decaying step-size parameters with complex constraint relationships
- GridWorld experiments limited to goal placement task with known optimal solution
- Performance relative to state-of-the-art bi-level RL methods unclear due to limited baseline comparisons
- Language model experiment lacks sufficient implementation details for independent verification

## Confidence
**High confidence:** Theoretical convergence guarantees, algorithmic formulation, and fundamental insight about entropy regularization enabling unbiased hyper-gradient estimation.
**Medium confidence:** Empirical performance on GridWorld task and language model fine-tuning, though results are promising but lack full reproducibility details.
**Low confidence:** Practical implementation details for language model experiment, including network architecture specifications and reward gradient computation methods.

## Next Checks
1. Implement GridWorld experiment with explicit monitoring of five step-size sequences to verify they satisfy equation (52) constraints throughout training.
2. Run GridWorld experiment with both constant regularization (τ_k = τ₀) and decaying regularization to empirically demonstrate convergence speed vs solution quality trade-off.
3. Implement GridWorld experiment with additional baselines including recent bi-level RL methods like those from [35] or [18] to provide stronger empirical validation of proposed method's advantages.