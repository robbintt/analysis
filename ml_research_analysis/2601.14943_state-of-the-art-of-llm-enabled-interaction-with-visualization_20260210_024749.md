---
ver: rpa2
title: State of the Art of LLM-Enabled Interaction with Visualization
arxiv_id: '2601.14943'
source_url: https://arxiv.org/abs/2601.14943
tags:
- visualization
- data
- interaction
- language
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey systematically examines how Large Language Models (LLMs)
  are enabling interaction with data visualization, focusing on visio-verbal interaction.
  By analyzing 48 papers through a PRISMA-guided review, the authors identify key
  tasks (data retrieval, transformation, visual encoding, sense-making, navigation),
  interaction modalities (text, speech, spatial, GUI), and visual representations
  (charts, spatial, networks, images).
---

# State of the Art of LLM-Enabled Interaction with Visualization

## Quick Facts
- arXiv ID: 2601.14943
- Source URL: https://arxiv.org/abs/2601.14943
- Reference count: 40
- Key outcome: Systematic survey of 48 papers analyzing LLM-enabled visio-verbal interaction tasks, modalities, and representations, identifying key research opportunities and evaluation challenges

## Executive Summary
This survey systematically examines how Large Language Models (LLMs) enable interaction with data visualization through visio-verbal interfaces. By analyzing 48 papers using PRISMA guidelines, the authors identify five core interaction tasks (data retrieval, transformation, visual encoding, sense-making, navigation), multiple interaction modalities (text, speech, spatial, GUI), and various visual representations (charts, networks, images). The survey reveals that LLMs are widely used for visualization recommendation and data transformation via natural language, with multimodal capabilities showing promise. However, evaluation remains a significant challenge, as few papers employ standardized benchmarks or address trust and correctness issues. The authors highlight opportunities in multimodal interaction, personalized interfaces, and external data integration while emphasizing the need for robust evaluation frameworks.

## Method Summary
The authors conducted a systematic review of 48 papers on LLM-enabled visualization interaction, guided by PRISMA principles. Papers were identified through database searches and screened for relevance to visio-verbal interaction. The analysis focused on categorizing interaction tasks, modalities, visual representations, and evaluation approaches. The survey synthesized findings across papers to identify patterns, challenges, and research opportunities, organizing results around the visualization pipeline and LLM integration mechanisms.

## Key Results
- Natural language is the dominant interaction modality, particularly for visualization recommendation and data transformation tasks
- Few papers employ standardized evaluation frameworks, creating challenges for comparing system effectiveness
- RAG and knowledge graph integration show promise for grounding LLM responses but face context window limitations
- Multimodal interaction combining natural language with direct manipulation yields more effective results than single modalities alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can translate natural language queries into executable visualization specifications through code generation or declarative grammars
- Mechanism: The LLM receives user intent in natural language, conditions on dataset schema/context, and outputs either imperative code (Python/D3.js) or declarative specifications (Vega-Lite). Chain-of-thought prompting decomposes complex queries into sequential transformation steps
- Core assumption: Users can articulate their visualization intent verbally, and the LLM has sufficient context about available data and chart libraries
- Evidence anchors: [abstract] "LLMs can interpret user queries expressed in natural language, generate relevant responses, and, in some cases, manipulate and update visualizations based on the interaction"; [section 5.1.2] "The first approach, adopted by a majority of papers, consists of generating code that has access to the dataset and produces a visualization using a specific framework"
- Break condition: Ambiguous queries without context; domain-specific visualizations lacking training examples in the model; hallucinated code that fails to execute

### Mechanism 2
- Claim: Combining natural language with direct manipulation (GUI widgets, selections) yields more effective interaction than either modality alone
- Mechanism: Natural language handles high-level, abstract intent; direct manipulation provides precision for fine-grained operations (e.g., selecting exact data points, adjusting parameters). The system passes selection context to the LLM prompt for deictic references ("zoom into this selection")
- Core assumption: Users will naturally switch modalities based on task granularity; systems can maintain context across modal switches
- Evidence anchors: [section 5.1.3] "users tend to use natural language in the exploration phase, when they were uncertain of the exact requirements, but used direct interaction otherwise"; [section 5.1.5] "InterChat provides a selection context to the LLM prompt to facilitate deictic interaction such as 'Zoom into this and move to that place,' where 'this' and 'that' correspond to previous mouse selections"
- Break condition: Context not properly propagated between modalities; users forced into a single modality for all tasks

### Mechanism 3
- Claim: Retrieval-augmented generation (RAG) and knowledge graph integration ground LLM responses in domain-specific data, reducing hallucinations
- Mechanism: External knowledge bases or knowledge graphs store factual/semantic information. User queries trigger similarity search or graph traversal; retrieved context augments the LLM prompt before response generation. Long-term memory (semantic, procedural, episodic) enables persistent learning across sessions
- Core assumption: Domain knowledge can be structured for retrieval; retrieved context fits within the LLM's context window without overwhelming the prompt
- Evidence anchors: [abstract] "external data integration" identified as a research opportunity; [section 5.1.1] "KnowNet... identifies three forms of knowledge: a procedural knowledge from human reasoning, knowledge learned by the LLM, and knowledge stored in the knowledge graph"; [section 6.6] "Systems using retrieval from knowledge graphs rely on semantic memory—they store and retrieve factual information"
- Break condition: Retrieved context is noisy, irrelevant, or exceeds context window; knowledge graph becomes stale; retrieval latency degrades interaction responsiveness

## Foundational Learning

- Concept: **Visualization Pipeline Stages** (data retrieval → transformation → visual encoding → navigation → sense-making)
  - Why needed here: The paper's task taxonomy maps LLM roles to specific pipeline stages; understanding this decomposition is prerequisite to system design
  - Quick check question: Can you identify which pipeline stage handles converting a natural language filter ("show only sales above $10K") into a data operation?

- Concept: **Prompt Engineering Strategies** (few-shot, chain-of-thought, zero-shot)
  - Why needed here: The survey documents that 9 papers use few-shot and 5 use chain-of-thought; these are standard techniques for conditioning LLMs on visualization tasks
  - Quick check question: What is the difference between zero-shot and few-shot prompting, and when would chain-of-thought be preferred?

- Concept: **Multimodal LLM Capabilities** (vision-language models, speech-to-speech models)
  - Why needed here: Section 6.4 documents VLMs for visual reading and iterative refinement; the survey notes LLMs have "poor sense or spatiality, temporality, and relationships, making them essentially 'blind' to the visualization"
  - Quick check question: Why might a VLM struggle with spatial reasoning in a 3D molecular visualization compared to reading a 2D bar chart?

## Architecture Onboarding

- Component map:
  LLM Engine -> Prompt Constructor -> Data Interface -> Code/Spec Executor -> Multimodal Input Handler -> Memory Module

- Critical path:
  1. User issues query (text/speech/spatial)
  2. System retrieves relevant context (RAG, KG, conversation history)
  3. Prompt Constructor builds conditioned prompt
  4. LLM generates response (code, spec, explanation)
  5. Executor runs code/renders visualization
  6. (Optional) VLM validates output visually; loop for refinement

- Design tradeoffs:
  - Cloud vs. Local LLM: Cloud offers state-of-the-art performance but introduces latency, cost, and data privacy concerns; local models offer privacy but may underperform
  - Code vs. Declarative Spec: Code generation (Python/D3) is flexible but prone to execution errors; declarative specs (Vega-Lite) are constrained but more reliable
  - Single vs. Multi-Agent: Single agent is simpler; multi-agent (Mixture of Agents) isolates responsibilities (e.g., manager, explorer, pilot agents in VOICE) but adds orchestration complexity
  - Context Window Management: Including full dataset schema improves accuracy but may exceed limits; pruning strategies risk omitting relevant context

- Failure signatures:
  - Hallucinated data/columns: LLM references fields not in schema—indicates insufficient grounding in dataset metadata
  - Code execution failures: Generated code throws exceptions—indicates need for scaffolding/validation pass
  - Ambiguity mishandling: System executes single interpretation without disambiguation—indicates missing clarification loop
  - Context loss across turns: Follow-up queries lose reference to prior selections—indicates broken dialogue state management
  - Stale memory: RAG returns outdated information—indicates knowledge base needs refresh

- First 3 experiments:
  1. **Baseline NL-to-Viz**: Implement a minimal text-to-visualization pipeline using GPT-4 with a small tabular dataset. Measure success rate on simple chart generation (bar, line, scatter) vs. custom requests requiring color/size specification
  2. **Modality Ablation**: Compare text-only, text+GUI-widgets, and text+speech input for a navigation task (zooming/filtering). Measure task completion time, error rate, and user preference
  3. **RAG vs. No-RAG**: For a domain-specific dataset (e.g., climate data per VIST5), compare LLM responses with and without retrieval augmentation from a knowledge base. Measure factual accuracy and hallucination rate

## Open Questions the Paper Calls Out

The paper identifies several key research opportunities: multimodal interaction capabilities beyond current implementations, personalized interfaces that adapt to user preferences and expertise, external data integration through RAG and knowledge graphs, and robust evaluation frameworks with standardized benchmarks. The authors also highlight the need for addressing trust and correctness concerns, particularly for critical applications where visualization errors could have serious consequences.

## Limitations

- The reliance on self-reported evaluation metrics from surveyed papers introduces potential bias, as many systems lack standardized benchmarks or independent validation
- The focus on English-language publications may miss relevant work from non-English research communities
- The rapid evolution of LLM capabilities means that findings based on models available during the review period may quickly become outdated
- The interaction modality analysis could be biased toward modalities explicitly discussed in the surveyed literature rather than emerging approaches

## Confidence

**High Confidence**: The identification of core interaction tasks (data retrieval, transformation, visual encoding, sense-making, navigation) and the dominance of natural language for visualization recommendation are well-supported across the surveyed literature. The observation that few papers employ standardized evaluation frameworks is consistently documented.

**Medium Confidence**: The proposed mechanisms for modality fusion (text + GUI) and RAG integration are theoretically sound based on cited examples, but direct comparative evidence is limited. The effectiveness of chain-of-thought prompting for complex visualization tasks is inferred from general LLM literature rather than visualization-specific studies.

**Low Confidence**: Claims about multimodal interaction opportunities and personalized interfaces are primarily identified as future research directions rather than empirically validated approaches. The survey identifies these as opportunities but provides limited evidence of their current feasibility or effectiveness.

## Next Checks

1. **Comparative Modality Study**: Implement a controlled experiment comparing text-only, text+GUI, and text+speech interaction for identical visualization tasks across multiple domains (tabular data, network data, time series). Measure task completion time, error rates, and user preference to validate the modality fusion hypothesis.

2. **RAG Effectiveness Benchmark**: Create a standardized benchmark with domain-specific knowledge bases (e.g., medical terminology, climate science) and measure hallucination rates and factual accuracy for LLM responses with and without retrieval augmentation across different knowledge graph structures and retrieval strategies.

3. **Cross-Model Generalization Test**: Test the same visualization task pipeline (e.g., natural language to Vega-Lite specification) across multiple LLM models (GPT-4, Claude, Llama, Gemini) with identical prompts and datasets to assess the consistency and model-dependence of the proposed mechanisms.