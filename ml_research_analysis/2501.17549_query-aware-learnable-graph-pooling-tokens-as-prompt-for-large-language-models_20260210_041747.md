---
ver: rpa2
title: Query-Aware Learnable Graph Pooling Tokens as Prompt for Large Language Models
arxiv_id: '2501.17549'
source_url: https://arxiv.org/abs/2501.17549
tags:
- graph
- information
- fusion
- query
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently representing
  complex graph-structured data for large language models (LLMs) in textual-attributed
  graph question answering (QA) tasks. The core method, Learnable Graph Pooling Tokens
  (LGPT), introduces learnable parameters that act as tokens for LLMs, balancing fine-grained
  node information and global graph information.
---

# Query-Aware Learnable Graph Pooling Tokens as Prompt for Large Language Models

## Quick Facts
- arXiv ID: 2501.17549
- Source URL: https://arxiv.org/abs/2501.17549
- Reference count: 9
- Primary result: Achieves 4.13% accuracy improvement on GraphQA benchmark using frozen LLM with learnable graph pooling tokens

## Executive Summary
This paper addresses the challenge of efficiently representing complex graph-structured data for large language models (LLMs) in textual-attributed graph question answering (QA) tasks. The core method, Learnable Graph Pooling Tokens (LGPT), introduces learnable parameters that act as tokens for LLMs, balancing fine-grained node information and global graph information. This approach overcomes the scalability issues of node-level projection and the information loss of graph-level projection. Additionally, an Early Query Fusion technique is proposed to integrate query context before constructing graph representations, leading to more effective embeddings. The method achieves a 4.13% performance improvement on the GraphQA benchmark without training the LLM, demonstrating significant gains in handling complex textual-attributed graph data.

## Method Summary
The method introduces Learnable Graph Pooling Tokens (LGPT) as a way to represent graphs for LLMs by using n learnable parameters that act as soft cluster centers connected to all graph nodes. These tokens undergo message passing through a Graph Neural Network to aggregate information from the graph structure. Early Query Fusion integrates query context by adding the query as a virtual node connected to all graph nodes before encoding. The graph is textualized using a discrete prompt template, then embeddings from LGPT, the textual prompt, and the query are concatenated and fed to a frozen LLM (LLaMA-2-7B). Only the graph encoder parameters (GNNs, LGPT, projection MLP) are trained using AdamW optimization.

## Key Results
- Achieves 4.13% performance improvement on GraphQA benchmark over mean pooling baseline
- Early Query Fusion improves performance by an average of 2.88% across tasks
- Optimal LGPT token count is 8; 32 tokens lead to overfitting and performance degradation
- LoRA fine-tuning adds 1.82% improvement by updating LLM parameters with additional learnable parameters

## Why This Works (Mechanism)

### Mechanism 1: Multi-Token Graph Pooling Reduces Information Loss
Distributing graph information across n learnable tokens (vs. single vector) preserves more structural information while remaining computationally tractable. n learnable parameters are fully connected to all graph nodes, with message passing propagating node information to each LGPT token. Each token specializes via soft clustering, capturing different graph aspects. Final n embeddings are projected to LLM embedding space.

### Mechanism 2: Early Query Fusion Improves Relevance Filtering
Integrating query context before graph encoding produces more task-relevant embeddings than post-hoc combination. Query is encoded as a virtual node connected to all graph nodes, with GNN_query propagating query-relevant signals through the graph structure. Subsequent GNN_graph encodes the query-conditioned graph, producing node embeddings that already reflect query relevance.

### Mechanism 3: Soft Clustering + Attention Yields Expressive Pooling
LGPT's effectiveness stems from combining hierarchical soft clustering with attention-based weighting in a single mechanism. Each LGPT token acts as a soft cluster center with nodes contributing to multiple tokens. Graph Transformer attention mechanism weights node contributions, allowing tokens to capture overlapping graph substructures with learned importance weighting.

## Foundational Learning

- **Concept: Message Passing in Graph Neural Networks**
  - **Why needed here:** LGPT and Early Query Fusion both rely on understanding how information propagates through graph structure via neighbor aggregation.
  - **Quick check question:** Given a 3-node path A→B→C, after 2 layers of message passing, what information does node A's representation contain?

- **Concept: Continuous/Soft Prompting**
  - **Why needed here:** LGPT produces n continuous embeddings that condition the LLM—not discrete text tokens. Understanding the difference is essential for grasping why this works.
  - **Quick check question:** How does a learnable continuous prompt embedding differ from a text token embedding, and what advantages might it offer?

- **Concept: Graph Pooling/Readout Functions**
  - **Why needed here:** LGPT is fundamentally a pooling innovation. Without understanding mean/sum pooling, hierarchical pooling, and attention pooling, you can't evaluate what's novel here.
  - **Quick check question:** If you compress a 500-node graph with 10 edge types to a single 768-dim vector via mean pooling, what specific information is most likely to be lost?

## Architecture Onboarding

### Component Map:
TextEnc -> GNN_query -> GNN_graph -> GNN_pool -> LGPT -> Proj -> Concat[LGPT, textual_prompt, query] -> LLM -> Answer

### Critical Path:
Query → TextEnc → Virtual Query Node → GNN_query (over S + n_q) → Query-aware graph S_q → GNN_graph → Graph nodes → GNN_pool (with LGPT) → n LGPT embeddings → Proj → Concat[LGPT, textual_prompt, query] → LLM → Answer

### Design Tradeoffs:
- **n (LGPT count)**: Paper finds n=8 optimal. n=1 = graph-level pooling (information loss). n=32 = overfitting, redundancy. Trade-off: expressiveness vs. trainability.
- **Early vs Late Fusion**: Early fusion (proposed) showed +2.88% avg improvement. Late fusion (cross-attention after encoding) degraded performance in mean pooling case. Trade-off: architectural complexity vs. effectiveness.
- **Frozen LLM vs LoRA**: Frozen LLM is cheap but limited. LoRA (+1.82% params) improves performance significantly but requires more compute and data.

### Failure Signatures:
- **n=32 underperforms n=8**: Overfitting/token redundancy. Reduce n, add dropout, or increase training data.
- **Late fusion degrades vs. no fusion**: Cross-attention introduces noise when embeddings are already task-specific. Switch to early fusion architecture.
- **Performance doesn't improve over mean pooling baseline**: Check LGPT initialization, learning rate, and whether GNN_pool is actually training (inspect gradients).
- **LLM ignores graph embeddings**: Verify Proj layer output dimension matches LLM embedding dim. Check that concatenated sequence order matches expected format.

### First 3 Experiments:
1. **Token scaling test**: Compare n ∈ {1, 4, 8, 16, 32} on SceneGraphs. Expect peak at n=8, drop at 32. Validates information bottleneck hypothesis.
2. **Fusion ablation**: Run (Early only) vs. (Late only) vs. (Both) vs. (Neither) with mean pooling and LGPT. Expect Early > Neither > Late for both pooling methods.
3. **Pooling method comparison**: Implement DiffPool and SAGPool as readout (single vector) vs. LGPT (n=8). Expect LGPT to outperform due to multi-token representation entering LLM.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the number of Learnable Graph Pooling Tokens ($n$) be determined dynamically based on graph complexity rather than set as a fixed hyperparameter?
- Basis in paper: Section 4.4 investigates performance based on $n$ (1, 8, 32) and concludes that 8 strikes the best balance. However, the analysis suggests that "too many learnable parameters can result in overfitting," implying the optimal $n$ likely varies depending on the information density of the specific graph instance.
- Why unresolved: The paper establishes a static value ($n=8$) for the benchmark but does not propose or test a mechanism to adaptively adjust the number of tokens for graphs of varying sizes or complexity levels.
- What evidence would resolve it: Experiments implementing a heuristic or gating mechanism where $n$ scales with node count or graph diameter, compared against the fixed-token baseline.

### Open Question 2
- Question: Why does the Late Fusion method benefit from a larger number of LGPT tokens (up to 32), while the Early Fusion method experiences performance degradation beyond 8 tokens?
- Basis in paper: Section 4.4 notes that for Late Fusion, "performance improves as the number of LGPT increases," whereas for Early Fusion, "performance did not improve further when increasing the number of LGPTs to 32." The paper observes this trend but leaves the underlying mechanism for this divergence unexplained.
- Why unresolved: The authors hypothesize that greater tokens allow richer information exchange in Late Fusion but do not explain why this capacity is detrimental or redundant in the Early Fusion context.
- What evidence would resolve it: An analysis of the entropy or variance of the token embeddings in both fusion scenarios to determine if Early Fusion suffers from representation collapse or overfitting when $n$ is high.

### Open Question 3
- Question: Does the LGPT projection mechanism generalize to Large Language Models with significantly different embedding spaces or attention mechanisms (e.g., decoder-only vs. encoder-decoder)?
- Basis in paper: The methodology is implemented exclusively on LLaMA-2-7b using a specific projection layer (MLP) to align GNN outputs with the LLM embedding space.
- Why unresolved: The paper demonstrates success on a specific model family, but the transferability of the "Graph Pooling Tokens" concept to other architectures (e.g., GPT-4, Mistral, or Flan-T5) without retraining the projection layer from scratch remains unverified.
- What evidence would resolve it: Cross-domain evaluation where the trained GNN encoder and LGPT parameters are tested on a different frozen LLM backbone to assess the robustness of the projection alignment.

### Open Question 4
- Question: To what extent does the reliance on a discrete textualization template limit the "continuous" learning capacity of the LGPT?
- Basis in paper: Section 3.2.1 describes the input as a concatenation of discrete prompt embeddings ($E_{text}$) derived from a template and continuous LGPT embeddings ($E_S$). The Introduction explicitly notes that "performance varies significantly depending on how the graph is textualized," yet the LGPT method still depends on this potentially sub-optimal textualization.
- Why unresolved: It is unclear if the LGPT is merely refining the discrete text or if it could entirely replace the textualized graph input, removing the "unknown optimal text encoding" bottleneck.
- What evidence would resolve it: Ablation studies removing the discrete graph description ($E_{text}$) and relying solely on the continuous LGPT tokens to convey graph structure.

## Limitations

- **Data dependency**: Method's performance relies heavily on GraphQA benchmark data and existing G-Retriever sub-graph retrieval components, limiting generalizability to datasets without pre-existing graph retrieval infrastructure.
- **Architecture complexity vs. gains**: The 4.13% improvement comes from combining three innovations (LGPT, Early Query Fusion, LoRA) without ablation studies isolating LGPT's specific contribution.
- **Scaling behavior**: While n=8 works optimally on GraphQA, the paper doesn't explore how LGPT scales to much larger graphs (thousands of nodes) or different graph types, lacking quantitative validation on diverse graph sizes.

## Confidence

**High confidence**: The architectural design is coherent and well-specified. The combination of soft clustering (DiffPool-like) with attention-based weighting (SAGPool-like) is theoretically sound, and the 8-token scaling result is directly validated with quantitative evidence.

**Medium confidence**: The Early Query Fusion mechanism shows consistent improvements (+2.88% average), but the comparison is limited to GraphQA benchmark tasks. The superiority over post-hoc query integration isn't demonstrated across diverse graph types or question-answering scenarios.

**Low confidence**: The claim that LGPT "overcomes scalability issues of node-level projection and information loss of graph-level projection" is largely theoretical. The paper doesn't provide runtime complexity analysis or memory usage comparisons across different pooling strategies on graphs of varying sizes.

## Next Checks

1. **Ablation study isolation**: Implement and test LGPT pooling alone (without Early Query Fusion or LoRA fine-tuning) on GraphQA to quantify the specific contribution of learnable pooling tokens to the 4.13% improvement.

2. **Graph size scalability test**: Evaluate LGPT performance and computational efficiency on graphs ranging from 50 to 5000 nodes, measuring both accuracy degradation and memory/time complexity relative to mean pooling baselines.

3. **Cross-domain generalization**: Apply the method to a non-GraphQA dataset (e.g., molecular property prediction or social network analysis) to test whether the learnable pooling token approach generalizes beyond question-answering tasks with textual attributes.