---
ver: rpa2
title: A Text-Image Fusion Method with Data Augmentation Capabilities for Referring
  Medical Image Segmentation
arxiv_id: '2510.12482'
source_url: https://arxiv.org/abs/2510.12482
tags:
- image
- text
- segmentation
- data
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an early fusion method for referring medical
  image segmentation that preserves spatial alignment between text and image during
  data augmentation. The method introduces a lightweight generator to project text
  embeddings into visual space, bridging the semantic gap.
---

# A Text-Image Fusion Method with Data Augmentation Capabilities for Referring Medical Image Segmentation

## Quick Facts
- arXiv ID: 2510.12482
- Source URL: https://arxiv.org/abs/2510.12482
- Reference count: 0
- Improved Dice scores and mIoU values exceeding state-of-the-art methods

## Executive Summary
This paper addresses the challenge of referring medical image segmentation, where text descriptions guide the segmentation of medical images. The key innovation is an early fusion strategy that preserves spatial alignment between text and image during data augmentation, overcoming a fundamental limitation of existing methods. The approach introduces a lightweight generator that projects text embeddings into visual space, creating a pseudo-image that can be concatenated with the original image. Experiments on three medical datasets (lung infection, polyp, and skin lesion segmentation) demonstrate significant performance improvements across four segmentation frameworks.

## Method Summary
The method employs CXR-BERT to extract 768-dimensional text embeddings, which are then projected through a fully connected layer to 784 dimensions and reshaped into a 28×28×1 pseudo-image. This pseudo-image passes through three stacked ConvTranspose2d layers (kernel size 4, strides 2, channels 16→8→C) with Sigmoid activation to generate a spatial representation. The pseudo-image is concatenated with the original image along channel dimensions to create a 2C-channel input. Kornia is used for data augmentation, and training employs AdamW optimizer with CosineAnnealingLR scheduler over 100 epochs. The loss function combines Dice loss with an L1 reconstruction loss (weighted by 0.1) that guides the pseudo-image toward the real image ROI.

## Key Results
- Improved Dice scores and mIoU values exceeding state-of-the-art methods
- Performance improvements: 3.62% Dice score and 4.01% mIoU for UNet compared to baseline
- Strong generalization across three diverse medical imaging tasks (lung infection, polyp, skin lesion)
- Computationally efficient approach requiring minimal changes to existing segmentation frameworks

## Why This Works (Mechanism)
The early fusion approach maintains spatial alignment between text and image by transforming text embeddings into a pseudo-image that undergoes the same geometric augmentations as the original image. This ensures that rotation, flipping, and scaling operations preserve the correspondence between textual descriptions and visual features. The lightweight generator bridges the semantic gap by learning to project abstract text concepts into spatial visual representations. The ROI-guided L1 loss provides explicit supervision for the generator, encouraging it to highlight relevant regions described in the text.

## Foundational Learning
- **Text embedding extraction** - Why needed: Transforms free-form text into dense numerical representations; Quick check: Verify CXR-BERT outputs 768-dim embeddings consistently
- **Spatial pseudo-image generation** - Why needed: Converts abstract text into spatial format compatible with image processing; Quick check: Visualize pseudo-images to confirm ROI localization
- **Channel-wise concatenation** - Why needed: Merges textual and visual information at input level for early fusion; Quick check: Confirm input shape is H×W×2C
- **ROI-guided reconstruction loss** - Why needed: Provides supervision signal for generator to learn meaningful text-to-image mapping; Quick check: Monitor L1 loss convergence during training
- **Geometric data augmentation** - Why needed: Increases dataset diversity while maintaining text-image alignment; Quick check: Verify augmentation applied after concatenation

## Architecture Onboarding
- **Component map**: Text description → CXR-BERT → FC layer → Reshape → ConvTranspose2d layers → Pseudo-image → Concatenation → Segmentation network
- **Critical path**: The lightweight generator (FC + ConvTranspose2d layers) that transforms text embeddings into pseudo-images, as it directly impacts the quality of information fusion
- **Design tradeoffs**: Early fusion doubles input channels (2C) which may affect pre-trained backbone performance vs. maintaining spatial alignment during augmentation
- **Failure signatures**: Noisy or uninformative pseudo-images indicate generator training issues; performance degradation suggests channel mismatch or augmentation misalignment
- **First experiments**: 1) Train generator independently to visualize pseudo-image quality; 2) Test augmentation invariance with controlled transformations; 3) Replicate ablation study on one dataset

## Open Questions the Paper Calls Out
- How does the proposed early fusion strategy maintain text-image alignment when applying non-spatial data augmentations, such as color transformations or CutMix?
- Does doubling the input channel count to accommodate the pseudo-image degrade the transferability of pre-trained visual encoders?
- Does the L1 reconstruction loss used for the generator impose a "visualness" constraint that limits the utility of abstract medical text descriptions?

## Limitations
- The CXR-BERT text encoder implementation lacks specific details about tokenization and checkpoint selection
- Data augmentation hyperparameters (rotation degrees, translation ranges) are not fully specified
- The claim of strong generalization is based on only three medical imaging tasks without testing on held-out domains
- No statistical validation provided for reported performance improvements

## Confidence
- High confidence: Core architectural contribution and motivation are clearly articulated
- Medium confidence: Quantitative results showing performance improvements are verifiable given implementation details
- Low confidence: Claims about computational efficiency and generalization to unseen medical domains due to limited experimental validation

## Next Checks
1. Implement and visualize the pseudo-image generation - Run the lightweight generator independently with sample text embeddings to verify it produces spatially coherent pseudo-images that capture ROI information before training the full model.
2. Test augmentation invariance - Create a controlled experiment where the same image-text pair undergoes different augmentations and verify that the segmentation network produces consistent outputs when the pseudo-image maintains spatial alignment.
3. Ablation study replication - Replicate the key ablation experiments (without ROI-guided loss, without early fusion) on at least one dataset to verify the claimed 3.62% Dice improvement is reproducible with the described implementation.