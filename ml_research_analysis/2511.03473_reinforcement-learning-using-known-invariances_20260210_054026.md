---
ver: rpa2
title: Reinforcement Learning Using known Invariances
arxiv_id: '2511.03473'
source_url: https://arxiv.org/abs/2511.03473
tags:
- learning
- kernel
- invariant
- reinforcement
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a symmetry-aware variant of kernel-based optimistic
  least-squares value iteration (LSVI) for reinforcement learning. The key idea is
  to incorporate known group symmetries into the algorithm via invariant kernels,
  which encode invariance in both rewards and transition dynamics.
---

# Reinforcement Learning Using known Invariances

## Quick Facts
- **arXiv ID:** 2511.03473
- **Source URL:** https://arxiv.org/abs/2511.03473
- **Reference count:** 40
- **Primary result:** Symmetry-aware kernel-based LSVI achieves better sample efficiency and performance than standard methods on symmetric MDPs.

## Executive Summary
This paper introduces a symmetry-aware variant of kernel-based optimistic least-squares value iteration (LSVI) for reinforcement learning. The key idea is to incorporate known group symmetries into the algorithm via invariant kernels, which encode invariance in both rewards and transition dynamics. The authors theoretically analyze the sample complexity gains from symmetry, establishing new bounds on maximum information gain and covering numbers for invariant reproducing kernel Hilbert spaces (RKHSs). Empirically, they validate their approach on a synthetic MDP, a symmetrized Frozen Lake environment, and a 2D placement problem. Results show that symmetry-aware RL achieves significantly better performance and sample efficiency compared to standard kernel-based methods.

## Method Summary
The paper proposes Kernel-based Optimistic Value Iteration (KOVI), an LSVI algorithm that uses invariant kernels to encode group symmetries in the MDP. The invariant kernel is constructed by averaging a base kernel over all group transformations, which restricts the RKHS to G-invariant functions. This reduces the effective state-action space to orbit representatives, leading to sample complexity improvements. The algorithm uses kernel ridge regression for function approximation and UCB bonuses for exploration. Theoretical analysis establishes new bounds on maximum information gain and covering numbers for invariant RKHSs, showing regret scaling with 1/|G|. Experiments validate the approach on three environments, demonstrating significant performance gains over standard kernel-based methods.

## Key Results
- Invariant kernels reduce the effective state-action space to orbit representatives, eliminating redundant learning.
- Sample complexity improves with the group size |G|, with regret bounds scaling as O(T^{1/p}|G|^{-1}).
- On Frozen Lake, the invariant kernel consistently outperforms the standard RBF kernel, with faster convergence and higher returns.
- The theoretical and empirical findings demonstrate that incorporating structural priors like symmetry into kernel-based RL algorithms can lead to substantial improvements in sample efficiency.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Invariant kernels constrain function approximation to symmetric hypothesis classes, eliminating redundant learning across equivalent states.
- Mechanism: The kernel averages over group orbits via \(k_G(z, z') = \frac{1}{|G|}\sum_{g \in G} k(g(z), z')\), restricting the RKHS to G-invariant functions. This reduces the effective state-action space to orbit representatives.
- Core assumption: The true reward function and transition dynamics are exactly invariant under the group action (Assumption 1).
- Evidence anchors:
  - [abstract]: "leverages invariant kernels to encode invariance in both rewards and transition dynamics"
  - [Section 3.3]: "The RKHS H_{k_G} induced by k_G consists of G-invariant functions"
  - [corpus]: Weak direct corpus support; related work (Brown et al., 2024) confirms invariant kernel construction for Bayesian optimization, but RL-specific extension is novel here.
- Break condition: If rewards or dynamics violate group invariance (e.g., asymmetric obstacles), the invariant kernel induces approximation error, and theoretical guarantees no longer apply.

### Mechanism 2
- Claim: Sample complexity improvements stem from reduced covering number and maximum information gain in the invariant RKHS.
- Mechanism: The eigendecay of the invariant kernel accelerates (eigenvalues scale as \(O((m|G|)^{-p})\)), leading to tighter bounds on \(\Gamma_{k_G}(T)\) and covering numbers \(N(\epsilon)\). Regret scales with \(1/|G|\) (Corollary 1).
- Core assumption: Mercer eigenvalues of the invariant kernel satisfy the decay profile in Assumption 2.
- Evidence anchors:
  - [Section 4.1]: "Γ_{k_G}(T) = O(T^{1/p}|G|^{-1})"
  - [Section 4.2, Theorem 1]: Explicit dependence of covering number on |G|
  - [corpus]: Tangentially supported by Tahmasebi & Jegelka (2023) on sample complexity gains from invariances in kernel regression, but RL-specific regret bounds are this paper's contribution.
- Break condition: If the kernel does not admit a Mercer expansion with the assumed decay (e.g., pathological kernels), the bounds may not hold.

### Mechanism 3
- Claim: Optimistic LSVI with invariant kernels focuses exploration uncertainty on orbit representatives, improving data efficiency.
- Mechanism: The UCB bonus \(\beta \cdot \sigma_t(z)\) is computed via kernel ridge regression with the invariant kernel. Since the posterior variance is constant on orbits, exploration does not waste samples on symmetric equivalents.
- Core assumption: The exploration bonus parameter \(\beta\) is set appropriately (theoretically \(\beta = B_T\), empirically tuned).
- Evidence anchors:
  - [Section 3.5]: "bQ^t_h is the kernel-based predictor... the term bQ^t_h + βb^t_h provides an optimistic estimate"
  - [Section 5, Limitations]: "\(\beta\) is empirically identified as the main contributor [to stability]"
  - [corpus]: Standard UCB exploration in kernelized RL is established (Yang et al., 2020a); invariance-aware uncertainty reduction is the novel extension.
- Break condition: If \(\beta\) is mis-specified (too low leads to under-exploration, too high causes instability), regret guarantees degrade.

## Foundational Learning

- Concept: **Reproducing Kernel Hilbert Spaces (RKHS)**
  - Why needed here: The function class for value and transition models is an RKHS; understanding the reproducing property and kernel-based prediction is essential.
  - Quick check question: Given a kernel \(k\), can you explain why \(f(z) = \langle f, k(\cdot, z) \rangle_{\mathcal{H}_k}\) holds for any \(f\) in the RKHS?

- Concept: **Group Actions and Invariance**
  - Why needed here: Symmetry is formalized via group actions on state/action spaces; invariance of rewards/dynamics under \(G\) is the core structural prior.
  - Quick check question: For a rotation group \(C_4\) acting on 2D positions, what does it mean for a reward function \(r(s, a)\) to be invariant?

- Concept: **Optimistic Exploration (UCB) in RL**
  - Why needed here: The LSVI algorithm uses optimism in the face of uncertainty; understanding how UCB bonuses drive exploration is critical.
  - Quick check question: In tabular Q-learning with UCB, why does adding \(\sqrt{\log t / N(s,a)}\) to the Q-estimate encourage exploration?

## Architecture Onboarding

- Component map: Symmetry identifier -> Invariant kernel constructor -> KOVI core -> Experience buffer
- Critical path:
  1. Verify MDP is \(G\)-invariant (rewards and dynamics satisfy Eq. 4-5).
  2. Construct invariant kernel \(k_G\); validate on held-out symmetric functions.
  3. Run KOVI: backward value iteration (steps \(H\) to 1) with UCB bonuses, forward trajectory rollouts.
  4. Monitor regret; compare against non-invariant kernel baseline.

- Design tradeoffs:
  - **Sample efficiency vs. compute**: Kernel inversion costs \(O(t^3)\) per step; sparse GP approximations trade accuracy for scalability.
  - **Exact vs. approximate symmetry**: Theoretical guarantees require exact invariance; real environments may only be approximately symmetric.
  - **Hyperparameter sensitivity**: \(\beta\), lengthscale, and regularization \(\lambda\) require tuning; theoretical \(\beta\) values are often impractically large.

- Failure signatures:
  - Regret does not decrease faster than baseline -> check if invariance actually holds in environment.
  - Instability in Q-estimates -> \(\beta\) may be too large; try reducing and adding regularization.
  - Slow training beyond ~2000 episodes -> kernel matrix inversion is prohibitive; switch to sparse/inducing-point methods.

- First 3 experiments:
  1. **Synthetic validation**: Generate an MDP with known \(G\)-invariant rewards/transitions; confirm regret scales with \(1/|G|\) vs. baseline.
  2. **Ablation on \(|G|\)**: Fix environment, vary group size (e.g., \(C_2, C_4, C_8\)); plot regret vs. \(|G|\) to validate theoretical scaling.
  3. **Approximate symmetry test**: Introduce small asymmetry in rewards; measure degradation in sample efficiency vs. exact case.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the theoretical framework be extended to handle environments with partial or approximate symmetries?
- Basis in paper: [explicit] The authors state, "it would be valuable... to extend our theoretical results to settings with partial or approximate symmetries."
- Why unresolved: The current analysis relies on exact group actions (\(G \subset O(d)\)), which limits applicability where dynamics are only approximately symmetric.
- What evidence would resolve it: Derivation of regret bounds that degrade gracefully with symmetry violations, or empirical validation on domains with stochastic symmetries.

### Open Question 2
- Question: How does the symmetry-aware LSVI algorithm perform on complex, high-dimensional input domains?
- Basis in paper: [explicit] The conclusion proposes future work to "evaluate our symmetry-aware LSVI algorithm on more complex or high-dimensional input domains."
- Why unresolved: Empirical validation was restricted to low-dimensional tasks (Frozen Lake, 2D placement), leaving high-dimensional scalability unverified.
- What evidence would resolve it: Successful application and maintained sample efficiency gains on high-dimensional benchmarks like robotic manipulation or vision-based RL.

### Open Question 3
- Question: Can the exploration parameter \(\beta\) be adaptively set in terms of the group size \(|G|\)?
- Basis in paper: [explicit] The limitations section notes that "more research needs to be done for adaptively setting \(\beta\) in terms of T (or |G|)."
- Why unresolved: The authors fixed \(\beta\) as a hyperparameter in experiments to avoid suboptimal results, deviating from the theoretical \(B_T\) scaling.
- What evidence would resolve it: An adaptive rule for \(\beta\) that adheres to theoretical constraints while improving convergence speed over fixed settings.

## Limitations

- **Approximate Symmetry:** The theoretical guarantees assume exact group invariance, but real-world environments often exhibit only approximate symmetries. The impact of symmetry violations on performance is not characterized.
- **Scalability:** Kernel matrix inversion scales cubically with trajectory count, limiting applicability beyond small episodic tasks. Sparse GP approximations are suggested but not empirically validated.
- **Hyperparameter Sensitivity:** The exploration bonus \(\beta\) is critical for stability but lacks a principled tuning procedure. Empirical tuning may not generalize across environments.

## Confidence

- **High Confidence:** Theoretical regret bounds under exact symmetry, kernel construction via orbit averaging, and the core mechanism of reduced covering numbers are well-supported.
- **Medium Confidence:** Empirical validation is limited to three small-scale environments. Generalization to larger, more complex tasks remains unproven.
- **Low Confidence:** The paper does not address approximate symmetry, scalability to high-dimensional state spaces, or principled hyperparameter tuning. These gaps limit practical deployment.

## Next Checks

1. **Approximate Symmetry Test:** Introduce controlled symmetry violations (e.g., asymmetric obstacles in Frozen Lake) and measure degradation in sample efficiency. Compare against the exact symmetry case.
2. **Scalability Analysis:** Implement a sparse GP variant (e.g., inducing points) and benchmark against exact kernel methods on a larger MDP. Measure computational time and regret scaling.
3. **Hyperparameter Sensitivity Study:** Systematically vary \(\beta\), lengthscale, and regularization across environments. Identify stable tuning ranges and assess robustness to misspecification.