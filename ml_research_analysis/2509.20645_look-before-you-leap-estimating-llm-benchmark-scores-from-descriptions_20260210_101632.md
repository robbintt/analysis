---
ver: rpa2
title: 'Look Before you Leap: Estimating LLM Benchmark Scores from Descriptions'
arxiv_id: '2509.20645'
source_url: https://arxiv.org/abs/2509.20645
tags:
- evaluation
- arxiv
- language
- performance
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of predicting large language model
  (LLM) performance on new tasks without running any experiments. The authors propose
  a novel "text-only performance forecasting" method, where an LLM predicts performance
  scores based solely on a textual description of the task and experimental setup.
---

# Look Before you Leap: Estimating LLM Benchmark Scores from Descriptions

## Quick Facts
- arXiv ID: 2509.20645
- Source URL: https://arxiv.org/abs/2509.20645
- Reference count: 40
- Primary result: GPT-5 predicts LLM benchmark scores from task descriptions with MAE as low as 9.9 on high-confidence predictions

## Executive Summary
This paper introduces text-only performance forecasting, a method for predicting LLM benchmark scores without running experiments. The authors build PRECOG, a dataset of 2,290 description-score pairs from 1,519 papers, and demonstrate that reasoning models like GPT-5 can achieve MAE of 14.6 on a leakage-controlled test set. The approach works by recognizing task-difficulty signals in descriptions and can be further improved with retrieval-augmented grounding to similar benchmarks.

## Method Summary
The method extracts anonymized, schema-aligned task descriptions from arXiv papers using LLMs, then predicts performance scores through zero-shot inference with reasoning models. GPT-5 or Qwen3-32B analyze redacted descriptions (containing task type, data source, evaluation protocol, difficulty, prompting, and other factors) to estimate normalized scores (0-100). The system optionally retrieves related papers via arXiv API to ground predictions, and models self-assess confidence through verbalized categorical outputs.

## Key Results
- GPT-5 achieves MAE of 14.6 on leakage-controlled test set (2025 papers)
- High-confidence predictions reduce MAE to 9.9 while covering 30% of cases
- Streaming (post-cutoff) predictions maintain comparable accuracy with MAE of 15.4
- Search-augmented prediction improves GPT-5 but not Qwen3 performance

## Why This Works (Mechanism)

### Mechanism 1: Description-to-Performance Mapping via Learned Task Priors
- **Claim:** Reasoning LLMs can estimate performance on novel tasks by recognizing regularities in task descriptions that correlate with difficulty.
- **Evidence:** "reasoning models achieve moderate prediction performance with well calibrated uncertainty, reaching mean absolute error as low as 9.9 at high-confidence thresholds"
- **Break condition:** Fails when task descriptions omit critical difficulty signals or when novel evaluation protocols lack historical precedent

### Mechanism 2: Retrieval-Augmented Grounding to Related Benchmarks
- **Claim:** Search-augmented prediction improves accuracy by grounding forecasts in empirically observed results from similar tasks.
- **Evidence:** "search improves GPT-5's performance prediction but not Qwen3's... GPT-5 issues more—and more variable—search calls per example"
- **Break condition:** Retrieval provides marginal gains when descriptions already encode key signals

### Mechanism 3: Confidence-Weighted Prediction Selection
- **Claim:** Verbalized confidence correlates with prediction error, enabling selective deployment on high-certainty cases.
- **Evidence:** "error dropping to 9.9 for high-confidence predictions" and "As the confidence threshold increases, coverage declines and MAE tends to decrease"
- **Break condition:** Calibration degrades on distribution shift or for tasks where the model's confidence is systematically miscalibrated

## Foundational Learning

- **Concept: Knowledge Cutoff and Data Leakage**
  - **Why needed here:** The paper's credibility hinges on showing predictions aren't memorized from training data; the 2025 test split is explicitly post-cutoff.
  - **Quick check question:** If a model was trained on papers through 2024, can you evaluate it on results published in 2025 without leakage concerns?

- **Concept: Regression from Text Descriptions**
  - **Why needed here:** Unlike classification, this task requires mapping free-form text to a continuous 0-100 score, not a discrete label.
  - **Quick check question:** Why is MAE more appropriate here than accuracy or F1?

- **Concept: Reasoning Models vs. Standard LLMs**
  - **Why needed here:** The paper shows explicit reasoning (CoT-style) improves prediction; ablations show non-reasoning variants perform worse.
  - **Quick check question:** What distinguishes a "reasoning model" from a standard LLM in terms of inference behavior?

## Architecture Onboarding

- **Component map:** arXiv scraper -> result/dataset paper retrieval -> description extraction -> schema-aligned descriptions -> reasoning LLM prediction -> optional retrieval -> confidence assessment -> MAE/Pearson evaluation
- **Critical path:** Source paper retrieval (result + dataset papers) -> Description extraction -> Prediction inference -> Optional retrieval -> Confidence calibration
- **Design tradeoffs:** Retrieval adds cost (+3 calls/instance for GPT-5) but modest accuracy gain; GPT-5 benefits, Qwen3 doesn't
- **Failure signatures:** Predictions cluster near dataset mean (r ≈ 0) -> model failing to exploit task features; High confidence + high error -> miscalibration on specific task types; Retrieval returns source paper -> leakage risk
- **First 3 experiments:**
  1. Baseline sanity check: Run test-set mean predictor; verify no LLM beats it significantly without task descriptions
  2. Reasoning ablation: Compare GPT-5 (reasoning mode) vs. GPT-5 (minimal reasoning) on 2025 split; expect ~1-2 MAE degradation
  3. Confidence calibration curve: Plot MAE vs. coverage across confidence thresholds; verify monotonic decrease

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the forecasting approach maintain accuracy when predicting performance for target models (e.g., small open-source models) that lack the extensive public evaluation history of GPT-4/4o?
- **Open Question 2:** Can more sophisticated retrieval strategies or hybrid text-metadata representations overcome the current finding that literature search provides minimal gains over reasoning alone?
- **Open Question 3:** Can the predictor be calibrated to reduce the significantly higher error rates observed for metrics like Recall and Precision compared to Accuracy or BLEU?

## Limitations

- The evaluation relies on automated retrieval and extraction pipelines whose quality directly impacts model predictions, with unknown selection bias from discarded papers
- The normalization of heterogeneous metrics to a 0-100 scale may introduce distortion, particularly for metrics like BLEU or ROUGE where absolute scales differ significantly across domains
- The paper doesn't address whether the model learns genuine task-difficulty signals versus exploiting spurious correlations in task descriptions

## Confidence

- **High confidence:** Reasoning models can predict performance from task descriptions given sufficient training data and the observation that confidence thresholds correlate with prediction accuracy
- **Medium confidence:** Retrieval-augmented prediction consistently improves accuracy across models, as results show GPT-5 benefits while Qwen3 does not
- **Low confidence:** The generalizability of findings to tasks outside the evaluated domains (text-based QA, reasoning, generation) and the robustness of confidence calibration on truly novel tasks

## Next Checks

1. **Leakage verification audit:** Manually sample 20 test-set predictions where the model made high-confidence forecasts, verify that retrieved documents don't contain the target paper's results or explicit performance indicators
2. **Cross-domain stress test:** Apply the same methodology to predict LLM performance on non-text domains (vision, robotics) where task descriptions encode different difficulty signals
3. **Confidence calibration stress test:** Systematically vary the complexity and novelty of task descriptions to identify where the model's confidence becomes miscalibrated, particularly for post-knowledge-cutoff predictions