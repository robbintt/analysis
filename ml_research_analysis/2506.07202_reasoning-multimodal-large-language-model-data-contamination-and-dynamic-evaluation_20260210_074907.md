---
ver: rpa2
title: 'Reasoning Multimodal Large Language Model: Data Contamination and Dynamic
  Evaluation'
arxiv_id: '2506.07202'
source_url: https://arxiv.org/abs/2506.07202
tags:
- task
- performance
- evaluation
- input
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a novel dynamic evaluation framework for\
  \ Multimodal Large Language Models (MLLMs) that addresses the growing concern of\
  \ data contamination in benchmarks. Unlike conventional approaches that perturb\
  \ inputs, this method perturbs the task itself\u2014using the same visual input,\
  \ models are evaluated across multiple related tasks such as visual question answering,\
  \ captioning, question generation, and answer verification."
---

# Reasoning Multimodal Large Language Model: Data Contamination and Dynamic Evaluation

## Quick Facts
- arXiv ID: 2506.07202
- Source URL: https://arxiv.org/abs/2506.07202
- Authors: Ming Liu; Wensheng Zhang
- Reference count: 40
- Primary result: Novel dynamic evaluation framework detects data contamination by evaluating models across four related tasks (QA, captioning, question generation, answer verification) on the same visual input

## Executive Summary
This paper introduces a dynamic evaluation framework that addresses data contamination in Multimodal Large Language Models by perturbing the task itself rather than the input. The method evaluates models on four related tasks (VQA, captioning, question generation, answer verification) using the same visual input, revealing whether high performance stems from genuine understanding or superficial task-specific cues. Experiments with 12 image and 11 video state-of-the-art MLLMs on MME, RealWorldQA, and CVRR-ES datasets demonstrate that models contaminated with test data show sharp performance drops on tasks other than the one they were fine-tuned on, while robust models maintain balanced performance across all tasks.

## Method Summary
The framework evaluates MLLMs on four tasks (T0: VQA, T1: Captioning, T2: Question Generation, T3: Answer Verification) using the same visual input. Discriminative tasks are scored via accuracy, while generative tasks use a reasoning judge model (VL-Rethinker). Performance metrics include average score, worst-task risk, standard deviation, and range across tasks. Contamination is simulated by PEFT fine-tuning on test QA data only, then comparing task performance profiles. The approach assumes genuine visual understanding transfers across task formulations while task-specific overfitting does not.

## Key Results
- Task perturbation effectively exposes contamination: PEFT on test QA data spikes T0 to 96.21% while degrading T2 from 70.78% to 63.40%
- Models heavily fine-tuned on test data show sharp performance drops on other tasks, while robust models maintain balanced performance
- InternVL v2 shows high task-space sharpness (Rng=28.60%) vs. GPT-o4 mini (Rng=16.97%) despite similar QA capabilities
- Performance variance across tasks quantifies "task-space sharpness," analogous to loss landscape curvature

## Why This Works (Mechanism)

### Mechanism 1: Task Perturbation Exposes Memorization
Task perturbation exposes memorization that survives input perturbations by forcing models to demonstrate understanding across task formulations. A model fine-tuned on leaked QA pairs optimizes parameters for that specific task-objective, not for the underlying visual representation. When asked to caption or generate questions from the same image, the memorized Q→A mapping provides no shortcut—performance drops reveal shallow understanding.

### Mechanism 2: Performance Variance Quantifies Task-Space Sharpness
Performance variance across tasks quantifies "task-space sharpness," analogous to loss landscape curvature. Contaminated models occupy sharp minima in task space—small deviations (task changes) cause large error increases. Generalizable models occupy flatter regions. The paper formalizes this via inter-task distance metrics (max pairwise score gap, standard deviation).

### Mechanism 3: Simulated Contamination Validates Detection
Simulated contamination via PEFT on test data reproduces contamination signatures. Fine-tuning Qwen2.5-VL on RealWorldQA test QA data spikes T0 to 96.21% while degrading T2 from 70.78% to 63.40%. This controlled experiment validates that task perturbation detects contamination.

## Foundational Learning

- **Concept: Flat vs. Sharp Minima**
  - Why needed here: The paper's central analogy—task-space sharpness—requires understanding why flat optima generalize better than sharp ones.
  - Quick check question: If a model achieves zero training loss in a narrow basin, what happens to test loss under small distribution shifts?

- **Concept: Data Contamination in Benchmark Evaluation**
  - Why needed here: Motivates the entire framework; without understanding contamination (test examples in training), the need for dynamic evaluation is unclear.
  - Quick check question: Why might a model score 95% on a benchmark yet fail on slightly rephrased questions?

- **Concept: Multi-Task Learning and Transfer**
  - Why needed here: The framework assumes tasks like captioning and QA share underlying visual representations; understanding task relationships is critical.
  - Quick check question: If two tasks have near-zero correlation in performance, what does that imply about shared representations?

## Architecture Onboarding

- **Component map**: Visual input X + task-specific prompt → 4 task outputs (T0-T3) → Judge model (for T1/T2) → 4 scores → Aggregated metrics (Avg, Range, SD, W.Risk)

- **Critical path**: For each visual input, run all 4 tasks with consistent prompts → Score discriminative tasks via accuracy; generative tasks via judge model → Compute ability vector and sharpness metrics → Compare across models; high Rng indicates brittleness

- **Design tradeoffs**: Judge model selection biases generative task scores; the paper uses VL-Rethinker but acknowledges subjectivity risk. Four tasks are not exhaustive; extending to explanation/dialogue tasks could reveal additional dimensions. Manual task definition limits scalability.

- **Failure signatures**: Model scores high on T0 but near-random on T1-T3 → likely QA-contaminated. Consistently low scores across all tasks → model capacity issue, not contamination. Judge scores conflict with human judgment → judge bias or rubric misalignment.

- **First 3 experiments**: 1) Replicate Table 2 PEFT contamination on a held-out benchmark to validate detection sensitivity. 2) Add a fifth task (e.g., visual explanation) to test if ability vector dimensionality improves discrimination. 3) Cross-validate judge model scores against human ratings on a 100-sample caption subset to calibrate trust.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the simultaneous application of input perturbations (e.g., visual distractors) and task perturbations affect the robustness and generalization profile of MLLMs? [explicit] Appendix A states, "Our framework currently evaluates input and task perturbations separately. Future work could combine these dimensions."

- **Open Question 2**: To what extent does Reinforcement Learning (RL) fine-tuning mitigate or mask "inherent contamination" from base models that may have already been exposed to benchmark data? [explicit] Appendix A notes that reasoning MLLMs derived from potentially contaminated base models present a challenge where "inherent contamination" might persist.

- **Open Question 3**: Does enforcing self-consistency, by requiring a model to answer its own generated questions, provide a more robust measure of visual understanding than using a judge model? [explicit] Appendix A highlights that the current protocol for question generation does not require the model to "subsequently answer its own proposed question."

## Limitations

- Task Distinctiveness Assumption: The framework assumes tasks T0-T3 require genuinely different reasoning skills, but this isn't empirically validated.
- Judge Model Reliability: Subjective task scoring relies entirely on VL-Rethinker's judgments without human validation data.
- Contamination Proxy Validity: PEFT on test data may not faithfully reproduce real contamination effects across diverse contamination types.

## Confidence

- **High Confidence**: The detection mechanism (task perturbation exposing contamination) works in controlled PEFT experiments; the conceptual framework is internally consistent.
- **Medium Confidence**: Real contamination cases are detected as effectively as simulated ones; the four-task framework captures sufficient behavioral diversity.
- **Low Confidence**: Judge model scores reliably reflect true task quality; task perturbation is the optimal dynamic evaluation approach versus alternatives.

## Next Checks

1. **Human Validation Study**: Have 3 independent annotators rate 100 random samples across all 4 tasks. Compute inter-annotator agreement (Krippendorff's alpha) and compare judge model scores against human consensus to establish scoring reliability.

2. **Task Correlation Analysis**: For each model, compute pairwise Pearson correlation coefficients between T0-T3 scores across the entire benchmark. Models with contamination should show near-zero correlations between T0 and other tasks; generalizable models should show moderate positive correlations.

3. **Extended Task Framework**: Add a fifth task (visual explanation generation) requiring multi-step reasoning. Re-run the contamination simulation experiment and verify whether the five-task ability vector provides better discrimination between contaminated and robust models than the original four-task framework.