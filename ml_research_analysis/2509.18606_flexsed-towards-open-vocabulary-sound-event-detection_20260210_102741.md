---
ver: rpa2
title: 'FlexSED: Towards Open-Vocabulary Sound Event Detection'
arxiv_id: '2509.18606'
source_url: https://arxiv.org/abs/2509.18606
tags:
- sound
- audio
- classes
- flexsed
- event
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FlexSED addresses the challenge of open-vocabulary sound event
  detection by integrating a pretrained audio SSL model with a CLAP text encoder and
  introducing an encoder-decoder architecture with adaptive fusion for efficient prompt-audio
  integration. To mitigate missing label issues, it employs LLM-guided negative query
  filtering during training.
---

# FlexSED: Towards Open-Vocabulary Sound Event Detection

## Quick Facts
- arXiv ID: 2509.18606
- Source URL: https://arxiv.org/abs/2509.18606
- Reference count: 32
- PSDS1A: 0.4484, PSDS1T: 0.5863 on AudioSet-Strong

## Executive Summary
FlexSED introduces an open-vocabulary sound event detection system that leverages pretrained audio SSL models combined with a CLAP text encoder. The system addresses the challenge of detecting arbitrary sound events without requiring class-specific training through an encoder-decoder architecture with adaptive fusion for prompt-audio integration. To handle the pervasive issue of missing annotations in real-world datasets, the authors employ LLM-guided negative query filtering during training, improving model robustness.

## Method Summary
FlexSED integrates a pretrained audio SSL model with a CLAP text encoder using an encoder-decoder architecture that incorporates adaptive fusion mechanisms for efficient prompt-audio integration. The system addresses missing label issues through LLM-guided negative query filtering during training, where large language models help identify and filter problematic queries. This approach enables the model to achieve superior performance on open-vocabulary detection tasks compared to traditional SED models, with particular strength in zero-shot and few-shot scenarios.

## Key Results
- Achieves PSDS1A of 0.4484 and PSDS1T of 0.5863 on AudioSet-Strong, outperforming vanilla SED models
- Demonstrates strong zero-shot temporal localization capabilities, retaining up to 88% of full performance
- Shows impressive few-shot learning, reaching near full performance with as few as 20-shot fine-tuning

## Why This Works (Mechanism)
FlexSED works by combining the complementary strengths of audio and text representations through adaptive fusion, allowing the model to generalize beyond predefined sound event classes. The LLM-guided negative query filtering addresses the fundamental challenge of incomplete annotations in real-world datasets, which is particularly problematic for open-vocabulary settings where any sound could theoretically appear. The encoder-decoder architecture with adaptive fusion enables more nuanced integration of text prompts with audio features compared to simple concatenation approaches.

## Foundational Learning
- **Contrastive Language-Audio Pretraining (CLAP)**: Why needed - enables semantic understanding of audio through text embeddings; Quick check - verify text-audio alignment quality on held-out pairs
- **Large Language Model (LLM) filtering**: Why needed - addresses missing label problem in training data; Quick check - measure precision/recall of LLM query filtering
- **Encoder-decoder architecture**: Why needed - enables sequence-to-sequence mapping for temporal localization; Quick check - validate output sequence quality on synthetic data
- **Adaptive fusion mechanisms**: Why needed - allows dynamic weighting of audio and text features; Quick check - analyze fusion weight distributions across different sound types
- **Zero-shot learning**: Why needed - enables detection of unseen sound events; Quick check - test on completely novel sound classes
- **Few-shot adaptation**: Why needed - balances between generalization and task-specific optimization; Quick check - measure performance gain with increasing shot count

## Architecture Onboarding

**Component map**: Pretrained Audio SSL Model -> CLAP Text Encoder -> Adaptive Fusion Layer -> Encoder-Decoder Network -> Output Classifier

**Critical path**: Audio input → Audio SSL encoder → Adaptive fusion with text prompts → Decoder → Temporal predictions

**Design tradeoffs**: The adaptive fusion mechanism adds computational overhead but enables more nuanced integration compared to simple concatenation; LLM filtering adds training complexity but addresses critical missing label issues; the encoder-decoder architecture supports temporal localization but requires more parameters than frame-level classification

**Failure signatures**: Performance degradation on acoustically similar but semantically distinct sounds; sensitivity to prompt quality and specificity; potential overfitting to dataset biases if LLM filtering is too aggressive

**First experiments**:
1. Test zero-shot performance on a held-out subset of AudioSet with verified complete annotations
2. Compare adaptive fusion against simple concatenation baseline while measuring computational overhead
3. Evaluate robustness across varying signal-to-noise ratios and recording environments

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Performance improvements over vanilla SED baselines, while statistically significant, show relatively modest gains (PSDS1A of 0.4484 vs typical SED baselines in 0.35-0.40 range)
- The 20-shot fine-tuning results lack comprehensive ablation studies to isolate which components contribute most to this capability
- The adaptive fusion mechanism's computational overhead compared to simpler approaches has limited analysis

## Confidence
- Zero-shot temporal localization results: High
- Full-dataset performance metrics: Medium (due to label completeness uncertainty)
- LLM-guided negative query filtering approach: Low

## Next Checks
1. Conduct controlled experiments on datasets with verified complete annotations (like FSD50K or manually verified subsets) to isolate the impact of missing labels on FlexSED's performance
2. Perform detailed ablation studies comparing the adaptive fusion mechanism against simpler text-audio integration approaches while measuring both accuracy and computational overhead
3. Test the model's robustness across diverse acoustic conditions by evaluating on out-of-domain datasets with varying signal-to-noise ratios and recording environments