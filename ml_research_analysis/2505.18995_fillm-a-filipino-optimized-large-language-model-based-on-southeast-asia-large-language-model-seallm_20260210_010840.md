---
ver: rpa2
title: FiLLM -- A Filipino-optimized Large Language Model based on Southeast Asia
  Large Language Model (SEALLM)
arxiv_id: '2505.18995'
source_url: https://arxiv.org/abs/2505.18995
tags:
- fillm
- language
- filipino
- performance
- calamancy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed FiLLM, a Filipino-optimized large language
  model based on SeaLLM-7B 2.5 using LoRA fine-tuning for improved memory efficiency.
  FiLLM was evaluated on NLP tasks including Named Entity Recognition, Part-of-Speech
  Tagging, Dependency Parsing, and Text Summarization using Filipino datasets.
---

# FiLLM -- A Filipino-optimized Large Language Model based on Southeast Asia Large Language Model (SEALLM)

## Quick Facts
- arXiv ID: 2505.18995
- Source URL: https://arxiv.org/abs/2505.18995
- Reference count: 0
- FiLLM achieved strong performance in POS Tagging (86% precision, 84% recall) and NER (86% precision, 93% recall) but struggled with Dependency Parsing (71% precision and recall)

## Executive Summary
This study developed FiLLM, a Filipino-optimized large language model based on SeaLLM-7B 2.5 using LoRA fine-tuning for improved memory efficiency. FiLLM was evaluated on NLP tasks including Named Entity Recognition, Part-of-Speech Tagging, Dependency Parsing, and Text Summarization using Filipino datasets. Results showed FiLLM achieved strong performance in POS Tagging and NER, but struggled with Dependency Parsing. Statistical analysis revealed FiLLM's performance was significantly lower than CalamanCy across all tasks, with FiLLM scoring 83.67 vs CalamanCy's 94.67 in F1-score comparisons. The findings highlight FiLLM's potential for Filipino NLP applications while indicating areas for improvement, particularly in syntactic parsing and overall model architecture.

## Method Summary
FiLLM leverages Low-Rank Adaptation (LoRA) fine-tuning on the SeaLLM-7B 2.5 model to optimize memory efficiency while maintaining task-specific performance. The approach involves freezing the pre-trained weights of SeaLLM-7B 2.5 and introducing trainable low-rank matrices (LoRA A and LoRA B) to capture task-specific Filipino language patterns. The model was evaluated on multiple NLP tasks using Filipino datasets including NER, POS tagging, Dependency Parsing, and Text Summarization, with results compared against the baseline CalamanCy model using paired t-tests.

## Key Results
- FiLLM achieved 86% precision and 84% recall in POS Tagging
- FiLLM achieved 86% precision and 93% recall in NER
- FiLLM scored 83.67 F1-score compared to CalamanCy's 94.67 F1-score across all tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LoRA fine-tuning enables memory-efficient adaptation while preserving foundational multilingual knowledge from the base model.
- Mechanism: The architecture keeps pre-trained weights (W) frozen and introduces trainable low-rank matrices (LoRA A: D×R and LoRA B: R×D). These matrices capture task-specific Filipino language patterns without modifying the 7B base parameters, reducing memory requirements significantly.
- Core assumption: Low-rank decomposition can sufficiently capture Filipino linguistic adaptations without full-rank updates.
- Evidence anchors:
  - [abstract] "FiLLM leverages Low-Rank Adaptation (LoRA) fine-tuning to optimize memory efficiency while maintaining task-specific performance"
  - [section 2.3] "The LoRA approach allows this fine-tuning without disrupting the pre-trained parameters, merging the adjustments with the original model's knowledge"
  - [corpus] Related work (FilBench, Batayan) confirms LLMs struggle with Filipino without specialized adaptation, supporting the need for language-specific fine-tuning approaches
- Break condition: If Filipino linguistic patterns require high-rank representations that cannot be decomposed into low-rank matrices, performance ceiling will be hit (observed in dependency parsing at 73% F1).

### Mechanism 2
- Claim: SeaLLM-7B 2.5 as a Southeast Asian-optimized base provides superior transfer learning for Filipino compared to English-centric models.
- Mechanism: The base model already contains representations for regional languages with similar morphological and syntactic properties to Filipino. LoRA adaptation leverages this prior knowledge rather than learning Filipino patterns from scratch.
- Core assumption: Southeast Asian language families share transferable linguistic features that benefit Filipino NLP tasks.
- Evidence anchors:
  - [abstract] "Built upon the SeaLLM-7B 2.5 model"
  - [section 2.1] "The SeaLLM-7B 2.5 model serves as the base model, which was fine-tuned using LoRA to adapt to Filipino linguistic characteristics"
  - [corpus] SeaLLMs paper (citation 7) establishes SeaLLM as specifically designed for Southeast Asian languages; OpenSeal and Compass-v3 papers confirm regional base models improve low-resource language performance
- Break condition: If target language (Filipino) has unique grammatical structures not represented in base training distribution, transfer will be limited.

### Mechanism 3
- Claim: Token-level classification tasks (NER, POS) transfer more effectively than structural parsing tasks through LoRA adaptation.
- Mechanism: NER and POS tagging rely on local context and surface-level patterns that can be captured by low-rank updates to attention and output layers. Dependency parsing requires global syntactic understanding and precise structural predictions that may need full-model fine-tuning or task-specific heads.
- Core assumption: LoRA rank (R) is sufficient for local pattern matching but insufficient for global syntactic reasoning.
- Evidence anchors:
  - [section 3] "For NER, the model achieved... F1-score of 0.89" while "For Dependency Parsing, the model showed... F1-score of 0.73"
  - [section 4] "Dependency Parsing remains a challenge, with precision and recall both at 71%, suggesting the need for improvements in handling Filipino syntax"
  - [corpus] Batayan benchmark paper evaluates structural vs. surface-level Filipino tasks, showing consistent gaps in syntactic understanding across LLMs
- Break condition: Structural tasks requiring multi-token reasoning beyond local context windows will underperform with low-rank adaptation alone.

## Foundational Learning

- Concept: Low-Rank Adaptation (LoRA)
  - Why needed here: Core technique used in the paper; understanding rank (R), matrix decomposition (A×B), and merging strategies is essential to interpret results and reproduce the approach.
  - Quick check question: If LoRA rank R=8 and input dimension D=4096, how many trainable parameters does one LoRA module add compared to full fine-tuning of a 4096×4096 weight matrix?

- Concept: Transformer Fine-tuning Paradigms
  - Why needed here: The paper compares LoRA fine-tuning against CalamanCy's approach; understanding full fine-tuning vs. parameter-efficient methods explains the performance gap.
  - Quick check question: Why might freezing base model weights limit adaptation to syntactic patterns that require modifying lower-layer representations?

- Concept: NLP Task Taxonomy
  - Why needed here: The paper evaluates fundamentally different tasks (sequence labeling vs. structured prediction vs. generation); each has different architectural requirements.
  - Quick check question: Why would Named Entity Recognition (span classification) transfer differently from Dependency Parsing (tree structure prediction) under the same fine-tuning approach?

## Architecture Onboarding

- Component map:
  SeaLLM-7B 2.5 (Frozen Base) → [Input: Filipino Text Datasets] → Tokenizer → Frozen Weights (W) → [Merge: W + (A × B) × scaling] → Task-Specific Outputs

- Critical path:
  1. Load SeaLLM-7B 2.5 weights and freeze all parameters
  2. Initialize LoRA matrices (A with random init, B with zeros for stable training start)
  3. Select LoRA rank (R) — not specified in paper, but typical values are 8-64
  4. Apply LoRA to target modules (attention projections, possibly MLP layers)
  5. Train on Filipino datasets with 80-20 train-test split
  6. Merge LoRA weights with base for inference

- Design tradeoffs:
  - **Rank selection**: Higher R captures more Filipino patterns but reduces memory efficiency (core benefit of LoRA). Paper doesn't report R value.
  - **Target modules**: Applying LoRA to more layers improves adaptation but increases parameters. Paper doesn't specify which modules received LoRA.
  - **Base model choice**: SeaLLM-7B vs. larger models (13B, 70B) — paper uses 7B for efficiency, but CalamanCy's superior performance suggests architecture or scale matters.

- Failure signatures:
  - **Dependency parsing collapse (73% F1)**: May indicate LoRA rank too low for structural prediction, or task requires dedicated parsing head rather than sequence generation.
  - **Statistical gap vs. CalamanCy (83.67 vs. 94.67)**: Suggests LoRA adaptation ceiling is below purpose-built pipeline architectures for Filipino.
  - **High variance in FiLLM results (85.31) vs. CalamanCy (16.34)**: Indicates training instability or insufficient data coverage.

- First 3 experiments:
  1. **Ablation on LoRA rank**: Test R ∈ {8, 16, 32, 64} on dependency parsing specifically to identify if structural tasks require higher rank. Hypothesis: Performance gap will narrow at higher ranks but memory efficiency degrades.
  2. **Base model comparison**: Replicate LoRA fine-tuning on other Southeast Asian base models (OpenSeal, Compass-v3) to isolate whether SeaLLM-7B 2.5 is optimal for Filipino. Corpus neighbors suggest multiple regional LLMs now exist.
  3. **Task-specific head addition**: Add a dedicated biaffine parsing head for dependency parsing (following CalamanCy-style architecture) while keeping LoRA for other tasks. Hypothesis: Structural prediction benefits from task-specific architectural priors beyond fine-tuning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural or data augmentation strategies can improve FiLLM's performance in Dependency Parsing?
- Basis in paper: [explicit] The authors note that "Dependency Parsing remains a challenge" (73% F1) and explicitly recommend "fine-tuning dependency structures and incorporating more training data."
- Why unresolved: The current LoRA-based adaptation failed to capture complex syntactic relationships effectively compared to the baseline CalamanCy (97% F1).
- Evidence: A modified model iteration achieving a Dependency Parsing F1-score comparable to the baseline (above 90%).

### Open Question 2
- Question: Can hyperparameter tuning alone close the statistically significant performance gap between FiLLM and CalamanCy?
- Basis in paper: [explicit] The study identifies a significant difference (p=0.03) in performance and proposes "hyperparameter tuning... to close this performance gap."
- Why unresolved: It is unclear if the gap (83.67 vs 94.67 mean scores) is due to trainable parameters or the fundamental limitations of the base model.
- Evidence: Statistical results showing no significant difference between FiLLM and CalamanCy following optimization.

### Open Question 3
- Question: Does the Low-Rank Adaptation (LoRA) technique inherently limit the model's ability to process complex linguistic structures compared to full fine-tuning?
- Basis in paper: [inferred] The paper prioritized memory efficiency via LoRA, but the model underperformed on complex tasks, suggesting a potential efficiency-accuracy trade-off.
- Why unresolved: The paper does not ablate the fine-tuning method to determine if LoRA constraints caused the lower syntactic performance.
- Evidence: A comparative study evaluating the same base model using full fine-tuning versus LoRA on Filipino NLP tasks.

## Limitations
- Missing hyperparameters (rank R, learning rate, training epochs) prevent exact replication and limit interpretation of performance results
- Statistical power is limited with only 25 related papers and zero citations, reducing confidence in generalizability claims
- Task formulation ambiguity for structured prediction tasks may explain significant performance gap versus CalamanCy

## Confidence
**High Confidence**
- FiLLM achieves state-of-the-art results among Filipino-optimized LLMs for POS tagging (86% precision) and NER (86% precision, 93% recall)
- LoRA fine-tuning successfully reduces memory requirements compared to full fine-tuning while maintaining task-specific performance
- Dependency parsing performance (73% F1) is significantly worse than token-level classification tasks, confirming structural prediction challenges

**Medium Confidence**
- FiLLM's overall performance is statistically significantly lower than CalamanCy across all tasks (t-test results)
- SeaLLM-7B 2.5 provides superior transfer learning for Filipino compared to English-centric models
- Token-level classification tasks transfer more effectively than structural parsing through LoRA adaptation

**Low Confidence**
- FiLLM represents a "significant advancement" in Filipino NLP (overstated given CalamanCy performance gap)
- The approach is "memory-efficient" without specifying actual memory savings versus alternatives
- Results generalize to real-world Filipino NLP applications without further validation

## Next Checks
1. **LoRA Rank Sensitivity Analysis**: Vary rank R ∈ {8, 16, 32, 64} specifically for dependency parsing to determine if the 73% F1 performance ceiling is rank-constrained. Measure memory consumption at each rank to quantify the efficiency trade-off and compare performance curves against CalamanCy's architecture.

2. **Base Model Ablation Study**: Replicate the exact LoRA fine-tuning procedure on alternative Southeast Asian base models (OpenSeal, Compass-v3) using identical hyperparameters. Test whether FiLLM's performance is specifically tied to SeaLLM-7B 2.5 or generalizes to other regional LLMs, including CalamanCy-style pipeline architecture as an additional comparison point.

3. **Task-Specific Head Architecture**: Implement dedicated biaffine parsing head for dependency parsing while maintaining LoRA for other tasks. Compare performance against the current sequence-to-sequence approach to isolate whether structural tasks require task-specific architectural priors, evaluating whether this hybrid approach can match or exceed CalamanCy's performance on syntactic tasks while preserving LoRA's efficiency benefits.