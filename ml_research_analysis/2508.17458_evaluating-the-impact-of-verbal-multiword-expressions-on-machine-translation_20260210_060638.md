---
ver: rpa2
title: Evaluating the Impact of Verbal Multiword Expressions on Machine Translation
arxiv_id: '2508.17458'
source_url: https://arxiv.org/abs/2508.17458
tags:
- translation
- vmwe
- sentence
- machine
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a comprehensive study on how verbal multiword\
  \ expressions (VMWEs) affect machine translation quality across eight state-of-the-art\
  \ MT systems and seven language pairs. The study evaluates three VMWE types\u2014\
  verbal idioms, verb-particle constructions, and light verb constructions\u2014using\
  \ both established VMWE datasets and WMT data, with reference-free quality estimation\
  \ and human direct assessment scores."
---

# Evaluating the Impact of Verbal Multiword Expressions on Machine Translation

## Quick Facts
- **arXiv ID**: 2508.17458
- **Source URL**: https://arxiv.org/abs/2508.17458
- **Reference count**: 40
- **Primary result**: VMWEs consistently degrade MT quality, with degradation severity correlating to non-compositionality; LLM-based paraphrasing improves translation of highly non-compositional VMWEs

## Executive Summary
This paper presents a comprehensive study on how verbal multiword expressions (VMWEs) affect machine translation quality across eight state-of-the-art MT systems and seven language pairs. The study evaluates three VMWE types—verbal idioms, verb-particle constructions, and light verb constructions—using both established VMWE datasets and WMT data, with reference-free quality estimation and human direct assessment scores. Results show VMWEs consistently degrade translation quality, with the degree of degradation correlating with non-compositionality: verbal idioms most affected, followed by verb-particle constructions, and light verb constructions least affected. To address this issue, the authors propose an LLM-based paraphrasing approach that replaces VMWEs with their literal equivalents prior to translation. Experimental results demonstrate significant quality improvements for verbal idioms and verb-particle constructions, validating the approach's effectiveness.

## Method Summary
The study employs a two-stage VMWE extraction pipeline combining rule-based heuristics with LLM validation, then evaluates MT quality using reference-free metrics and human assessment. VMWE sentences are identified through BLEU-4 matching for idioms, dependency parsing for VPCs, and exact matching for LVCs, followed by GPT-4o validation using PARSEME guidelines. Translation quality is assessed using MetricX-24-QE and xCOMET-QE metrics, with human direct assessment scores for WMT data. The LLM-based paraphrasing intervention uses Llama-3.3-70B to replace VMWEs with literal equivalents before translation, with quality improvements measured against direct translation baselines.

## Key Results
- VMWEs consistently degrade MT translation quality across all tested language pairs and MT systems
- Translation degradation severity correlates with non-compositionality: verbal idioms show largest drops, followed by verb-particle constructions, while light verb constructions are least affected
- LLM-based paraphrasing significantly improves translation quality for verbal idioms and verb-particle constructions, with minimal benefit for light verb constructions
- Different QE metrics (MetricX-24 vs xCOMET) show varying sensitivity to VMWE translation quality, particularly for light verb constructions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Translation quality degradation correlates with non-compositionality level of verbal multiword expressions.
- Mechanism: Non-compositional expressions (where meaning cannot be derived from constituent words) create semantic ambiguity that MT models resolve poorly via literal translation. Semi-compositional expressions (LVCs) allow partial inference from components, reducing degradation.
- Core assumption: MT models rely heavily on compositional semantic inference from training data; when this fails, they default to word-by-word translation rather than idiomatic equivalents.
- Evidence anchors:
  - [abstract] "degree of degradation correlates with non-compositionality, with verbal idioms experiencing the largest performance drop followed by verb-particle constructions, while light verb constructions are least affected due to their semi-compositional nature"
  - [Section 4.5] "LVCs are generally more compositional than other types of VMWEs—their meaning can often be inferred from their individual components... the noun carries most of the semantic weight, while the verb functions as grammatical support"
  - [corpus] Limited direct corpus evidence on this specific mechanism; related work (Manakhimova et al., 2023) examines idiom translation challenges but mechanism remains hypothesized
- Break condition: If degradation persists equally across all VMWE categories regardless of compositionality, this mechanism is invalidated.

### Mechanism 2
- Claim: LLM-based paraphrasing improves MT quality for highly non-compositional VMWEs by replacing idiomatic expressions with literal equivalents before translation.
- Mechanism: LLMs with strong semantic understanding identify VMWEs and generate compositional paraphrases that preserve meaning while eliminating the non-compositional structure that confuses MT systems. The paraphrased sentence can then be translated more reliably.
- Core assumption: The LLM correctly identifies the idiomatic meaning and generates a paraphrase that preserves semantics while being more compositionally transparent to downstream MT systems.
- Evidence anchors:
  - [abstract] "LLM-based paraphrasing approach that replaces VMWEs with their literal counterparts before translation... significantly improves translation quality for verbal idioms and verb-particle constructions"
  - [Section 4.3, Table 2] δpara consistently positive across VID and VPC categories; VIDs show largest gains (e.g., Opus en-cs: +4.24 improvement for VID vs +0.12 for LVC)
  - [corpus] Related work on MWE-aware NMT (Zaninello & Birch, 2020) supports preprocessing approaches but doesn't isolate this exact mechanism
- Break condition: If paraphrasing degrades quality or shows no improvement for non-compositional expressions, the mechanism fails. Note: LVC paraphrasing showed minimal/no benefit, consistent with mechanism.

### Mechanism 3
- Claim: Two-stage VMWE extraction (rule-based candidate generation + LLM disambiguation) enables accurate identification of VMWE sentences in MT evaluation datasets.
- Mechanism: Rule-based heuristics (BLEU matching for idioms, dependency parsing for VPCs, exact matching for LVCs) cast a wide net for candidates; LLM with PARSEME guidelines performs fine-grained semantic classification to filter true VMWEs from false positives.
- Core assumption: The LLM generalizes well from PARSEME guidelines to novel contexts, and the rule-based heuristics capture most true VMWEs with acceptable false positive rates.
- Evidence anchors:
  - [Section 3.3] "We adopt the best-performing model—GPT-4o—to validate and extract true VMWE sentences from the WMT datasets"
  - [Table 1, Section 4.2] GPT-4o achieves 80.5% accuracy on VID identification, 82.5% on VPC, 80.0% on LVC
  - [corpus] PARSEME shared task (Savary et al., 2017, 2023) provides standardized VMWE identification methodology; corpus evidence on LLM extraction accuracy is preliminary
- Break condition: If downstream analysis shows systematic extraction errors (e.g., consistently missing a VMWE subtype), the extraction pipeline may be biased.

## Foundational Learning

- **Concept: Compositionality in linguistics**
  - Why needed here: The entire analysis hinges on understanding why non-compositional expressions (where meaning ≠ sum of parts) challenge MT systems differently than compositional ones.
  - Quick check question: Would "kick the bucket" translated word-by-word preserve its idiomatic meaning "to die"? Why does "take a walk" cause less degradation?

- **Concept: Reference-free quality estimation (QE) for MT**
  - Why needed here: The paper uses MetricX-24-QE and xCOMET-QE to assess translation quality without human references. Understanding these metrics is essential for interpreting results.
  - Quick check question: Why would reference-free QE be preferable when evaluating VMWE translation specifically? What limitations might it have?

- **Concept: Verbal multiword expression taxonomy (VID/VPC/LVC)**
  - Why needed here: Each VMWE type has distinct linguistic properties that predict translation difficulty. Recognizing these categories enables targeted mitigation strategies.
  - Quick check question: Classify these expressions: "spill the beans" / "look up (information)" / "give a speech" — which is most non-compositional?

## Architecture Onboarding

- **Component map:**
Source Text → [VMWE Detection: Rule-based + LLM] → [Paraphrasing LLM] → [MT System] → Target Text
                                              ↓
                                    [QE Evaluator] → Quality Score

- **Critical path:**
1. VMWE extraction accuracy gates downstream analysis validity
2. Paraphrase semantic fidelity determines whether improvement is real or artifact
3. QE metric alignment with human judgment determines result interpretability

- **Design tradeoffs:**
- GPT-4o for extraction: Higher accuracy but proprietary/cost vs. open models (Llama, Phi) with lower LVC F1
- Paraphrasing preprocessing: Adds latency but improves VID/VPC; minimal benefit for LVC may not justify cost
- QE metrics: MetricX-24 vs xCOMET show different sensitivity patterns—consider both for robustness

- **Failure signatures:**
- High invalid translation rates (e.g., GemmaX2 en-cs: 75%+ errors, LLaMAX en-tr: 99%+ errors) — some LLM-based MT systems unreliable for certain language pairs
- Negative δmix values for LVC paraphrasing (Table 2) — paraphrasing doesn't help and may hurt for semi-compositional expressions
- MetricX-24/xCOMET disagreement on language rankings (Figure 13) — evaluation metric choice affects conclusions

- **First 3 experiments:**
1. Replicate VMWE extraction on held-out corpus; measure precision/recall against gold annotations to validate GPT-4o performance claims
2. Run paraphrasing intervention on new language pairs (beyond 7 tested) to test generalization; expect larger gains for morphologically distant languages
3. Compare direct human evaluation of paraphrased vs. original translations for a sample of VID sentences to validate QE metric alignment with perceived quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can alternative pre-processing or decoding strategies be developed to improve the translation of Light Verb Constructions (LVCs), where literal paraphrasing fails?
- Basis in paper: [explicit] Section 4.5 states that the LLM-based paraphrasing approach does not improve LVC translation scores because the paraphrases amount to minor lexical substitutions (e.g., "gave a speech" to "delivered a speech") rather than resolving structural ambiguity.
- Why unresolved: The paper demonstrates that current substitution methods work for non-compositional VMWEs (idioms) but fail for semi-compositional structures where the verb’s semantic weight is minimal but syntactically significant.
- What evidence would resolve it: Experiments showing that a strategy targeting the predicative noun-verb relationship in LVCs yields statistically significant translation quality improvements comparable to those seen in verbal idioms.

### Open Question 2
- Question: What modifications are required to reduce the high rate of invalid outputs (hallucinations or wrong language) in open-weight LLM-based Machine Translation systems?
- Basis in paper: [explicit] Section 4.1 and Appendix B report that LLM-based models like GemmaX2 and LlaMAX frequently produce invalid translations—such as outputting the wrong language or repetitive text—with error rates exceeding 75% for specific language pairs like English-to-Czech.
- Why unresolved: The study evaluates the *impact* of VMWEs on these systems but does not investigate the architectural or training deficiencies causing these models to fail basic instruction-following or language-identification tasks during translation.
- What evidence would resolve it: A comparative study showing that specific fine-tuning or constrained decoding techniques significantly lower the invalid translation rate for the identified problematic language pairs.

### Open Question 3
- Question: Does the correlation between non-compositionality and translation quality degradation hold when translating from non-English source languages?
- Basis in paper: [inferred] Section 5 discusses cross-linguistic variability in VMWEs, noting that different languages use unique syntactic patterns for these expressions; however, the study is strictly limited to English as the source language.
- Why unresolved: It is undetermined if the performance drop observed in Verbal Idioms and Verb-Particle Constructions is an artifact of English-specific linguistic structures or a universal property of translating non-compositional phrases.
- What evidence would resolve it: A replication of the extraction and evaluation pipeline on datasets where morphologically rich or structurally distinct languages (e.g., German, Turkish) serve as the source.

## Limitations

- VMWE extraction relies on proprietary GPT-4o with limited public validation beyond reported 80%+ accuracy
- Evaluation depends on reference-free QE metrics that may not perfectly align with human judgment, particularly for idiomatic content
- High failure rates for some LLM-based MT systems (GemmaX2, LLaMAX3) raise questions about method robustness
- Paraphrasing intervention shows minimal/no benefit for LVCs, suggesting the approach may not generalize across all VMWE types

## Confidence

- **High confidence**: VMWEs consistently degrade translation quality; non-compositionality correlates with degradation severity; paraphrasing improves VID/VPC translation
- **Medium confidence**: Exact magnitude of improvement depends on QE metric choice; LVC results suggest intervention limitations
- **Low confidence**: Generalization to language pairs beyond the seven tested; long-term effectiveness of paraphrasing approach

## Next Checks

1. Conduct direct human evaluation of paraphrased vs. original translations for a stratified sample of VID sentences to validate QE metric alignment
2. Replicate VMWE extraction pipeline on a held-out corpus with gold-standard annotations to independently verify GPT-4o extraction accuracy claims
3. Test paraphrasing intervention on morphologically distant language pairs not included in the original study to assess generalization limits