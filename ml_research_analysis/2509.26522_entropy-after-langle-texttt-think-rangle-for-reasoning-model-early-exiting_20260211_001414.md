---
ver: rpa2
title: Entropy After $\langle \texttt{/Think} \rangle$ for reasoning model early exiting
arxiv_id: '2509.26522'
source_url: https://arxiv.org/abs/2509.26522
tags:
- reasoning
- early
- answer
- think
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses overthinking in reasoning models, where models
  continue lengthy reasoning chains even after producing correct answers. The authors
  propose Entropy After </Think (EAT), a lightweight signal that monitors the entropy
  of the next token after a special stop thinking token.
---

# Entropy After $\langle \texttt{/Think} \rangle$ for reasoning model early exiting

## Quick Facts
- arXiv ID: 2509.26522
- Source URL: https://arxiv.org/abs/2509.26522
- Authors: Xi Wang; James McInerney; Lequn Wang; Nathan Kallus
- Reference count: 40
- Key outcome: Reduces token usage by 13-21% on MATH500 and AIME2025 without sacrificing accuracy

## Executive Summary
This paper addresses overthinking in reasoning models, where models continue lengthy reasoning chains even after producing correct answers. The authors propose Entropy After `</think>` (EAT), a lightweight signal that monitors the entropy of the next token after a special stop thinking token. EAT decreases and stabilizes when reasoning performance plateaus, enabling early stopping. Using an exponential moving average of EAT variance, the method adaptively exits reasoning when uncertainty stabilizes. On MATH500 and AIME2025, EAT reduces token usage by 13-21% without sacrificing accuracy. The approach also works in black-box settings using proxy models to compute EAT, making it applicable to commercial APIs.

## Method Summary
The method computes entropy over the next-token distribution after appending a stop-thinking token (`</think>`) to the reasoning context. An exponential moving average (EMA) tracks the variance of this entropy over time. When the variance drops below a threshold δ after sufficient warmup iterations (4/α), the algorithm exits reasoning early. The final answer is generated with a prefix like "The final answer:". For black-box models, a smaller proxy model computes EAT based on the reasoning text generated by the main model.

## Key Results
- EAT reduces token usage by 13-21% on MATH500 and AIME2025 while maintaining Pass@1 accuracy
- The method works with proxy models in black-box settings, achieving 6-9% token reduction when a 1.5B proxy monitors a 70B reasoning model
- EAT correlates with reasoning convergence: entropy decreases and stabilizes as Pass@1 plateaus

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Entropy over the single token after `</think>` correlates with reasoning convergence.
- **Mechanism**: Appending the stop-thinking token forces the model to produce an answer distribution; its entropy measures remaining uncertainty conditioned on current reasoning. As reasoning accumulates, uncertainty decreases and stabilizes, tracking Pass@1 saturation.
- **Core assumption**: The model's next-token distribution after `</think>` reflects its answer confidence, and reduced entropy signals higher solution certainty.
- **Evidence anchors**:
  - [abstract]: "EAT decreases and stabilizes when Pass@1 plateaus."
  - [Section 4.1, Eq. 6]: EAT captures information gain: IG = H(f(Q, ensemble, `</think>`, \n; θ)) – EAT.
  - [Fig. 1]: EAT trajectory drops and flattens exactly where Pass@1 (Avg@128) saturates.
  - [corpus]: Related work "Think Just Enough" similarly uses entropy as confidence signal, showing 25-50% savings.
- **Break condition**: If EAT does not stabilize (high variance persists), the problem may be unsolvable or Pass@1 may be decreasing; do not exit early (see GPQA failure cases, Appendix G.3).

### Mechanism 2
- **Claim**: Thresholding EMA variance of EAT provides a practical stopping rule.
- **Mechanism**: A running EMA of EAT variance (V̂) tracks local stability. When V̂ < δ, EAT has stabilized, indicating reasoning gain has plateaued. Smaller δ trades more tokens for stricter stability; larger δ exits earlier but risks under-reasoning.
- **Core assumption**: Stabilized entropy variance implies answer convergence; this holds for problems where the model can reach a confident answer.
- **Evidence anchors**:
  - [Section 4.2, Eq. 7]: EMA update M̂ = (1-α)M̂ + α·EAT, V̂ = (1-α)V̂ + α(EAT-M̂)².
  - [Algorithm 1]: Exit when (n ≥ 4/α and V̂n < δ) or `</think>` is generated.
  - [Fig. 3]: Visual shows V̂ declining below threshold aligns with Pass@1 saturation.
  - [corpus]: "Think or Not?" similarly proposes information-theoretic stopping but requires rollouts; EAT is cheaper.
- **Break condition**: If V̂ never drops below δ within token budget, the method defaults to max tokens; indicates problem difficulty or model limitation.

### Mechanism 3
- **Claim**: EAT computed by a proxy model can early-exit a black-box reasoning model.
- **Mechanism**: A small proxy (e.g., 1.5B) observes the reasoning text from the black-box model and computes EAT. The proxy's entropy trajectory mirrors the main model's convergence, enabling early exit without accessing the main model's logits.
- **Core assumption**: Proxy and main model answer distributions correlate sufficiently for stabilization detection.
- **Evidence anchors**:
  - [Section 4.2]: "EAT is computed with proxy models" in black-box settings.
  - [Fig. 4]: 1.5B proxy achieves 6-9% token reduction on 70B model (Llama-70B).
  - [Section 5.3]: "EAT remains effective when the model computing EAT differs from the reasoning model."
  - [corpus]: "Thought Manipulation" explores external thought efficiency but relies on model internals; EAT is API-compatible.
- **Break condition**: If proxy and main model distributions diverge significantly, proxy EAT may misjudge stabilization; validate proxy alignment per deployment.

## Foundational Learning

- **Concept: Shannon entropy over discrete distributions**
  - Why needed: EAT is defined as H(p) = -Σ p_i log p_i over the model's next-token distribution (Eq. 2); understanding entropy as uncertainty is essential.
  - Quick check: Given probabilities [0.8, 0.2], compute entropy. (Answer: ≈0.5 nats)

- **Concept: Exponential moving average (EMA) for time-series smoothing**
  - Why needed: The stopping rule uses EMA variance to track EAT stability (Eq. 7); α controls the effective window size (≈1/α steps).
  - Quick check: If α=0.2 and current EAT=0.5, previous EMA=0.4, what's the new EMA? (Answer: 0.42)

- **Concept: Chain-of-thought reasoning and test-time scaling**
  - Why needed: Reasoning models generate structured Q, begin-think, R, end-think, A (Eq. 4); longer reasoning can improve Pass@1 but causes overthinking.
  - Quick check: What token signals reasoning end? (Answer: `</think>`)

## Architecture Onboarding

- **Component map**: Reasoning Generator -> EAT Evaluator -> EMA Tracker -> Stopping Decider
- **Critical path**:
  1. Generate reasoning line (GenerateNewLine)
  2. Compute EAT via single forward pass (overhead ≈1 extra token generation)
  3. Update EMA mean and variance
  4. Check exit condition; if met, generate final answer (GenerateTillEoS)
- **Design tradeoffs**:
  - **Threshold δ**: Lower δ → more tokens, higher accuracy certainty; higher δ → more savings, risk of early exit
  - **EMA timescale α**: Larger α (0.2-0.4) responds faster to stabilization; smaller α (0.01) may delay exit
  - **Prefix string**: Older models benefit from "The final answer: " prefix for EAT informativeness (Appendix B)
  - **Proxy vs. same-model EAT**: Proxy enables black-box use but requires alignment validation
- **Failure signatures**:
  - **Unsolvable problems**: EAT never stabilizes, V̂ stays high → uses full token budget (Fig. 11)
  - **Decreasing Pass@1**: Model overcorrects; EAT may stabilize after optimal exit point (Fig. 12)
  - **Misaligned proxy**: Proxy EAT stabilizes early/late relative to main model → accuracy drop or wasted tokens
- **First 3 experiments**:
  1. Reproduce EAT-Pass@1 correlation: Plot EAT trajectory vs. Pass@1 (Avg@128) for 10 MATH500 questions; verify EAT drops and stabilizes at plateau (Fig. 1 style)
  2. Ablate α and δ: Sweep α ∈ {0.1, 0.2, 0.4} and δ ∈ {small, medium, large}; plot Agg. Pass@1 vs. total tokens to find optimal tradeoff (Fig. 10 style)
  3. Test proxy transfer: Use 1.5B model to compute EAT for 8B reasoning model on 50 questions; measure token savings and accuracy drop vs. same-model EAT (Fig. 4 style)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the method be extended to recognize and abandon reasoning that will never stabilize?
- **Basis in paper**: [explicit] The authors state, "A promising extension is to learn to recognize and abandon reasoning that will never stabilize, e.g., considering exiting when EAT or its variance remains high for many steps."
- **Why unresolved**: The current implementation relies on low variance to trigger an exit. If the model fails to solve a problem, the variance remains high, and the algorithm simply consumes the maximum token budget (T) without successfully terminating early.
- **What evidence would resolve it**: A modified stopping rule that identifies persistent high entropy/variance to trigger a "give up" state, thereby saving tokens on unsolvable problems compared to the current max-token fallback.

### Open Question 2
- **Question**: How can the early exiting logic be adapted to handle instances where reasoning accuracy (Pass@1) decreases as the chain of thought lengthens?
- **Basis in paper**: [inferred] Appendix G.3 identifies a failure mode where "Pass@1 often declines as reasoning continues," but the paper excludes these instances from evaluation. The current method assumes monotonic improvement or stabilization, failing to exit before the model "unlearns" the correct answer.
- **Why unresolved**: The EAT signal stabilizes when the model is confident, but confidence does not always correlate with correctness if the model drifts into a wrong reasoning path after initially being correct.
- **What evidence would resolve it**: A stopping mechanism that tracks answer consistency rather than just entropy, allowing it to exit at a peak performance point even if entropy has not yet reached the global threshold.

### Open Question 3
- **Question**: Does optimizing for per-instance adaptive thresholds outperform the single global variance threshold (δ) used in the study?
- **Basis in paper**: [explicit] The authors note, "We here treated problems as coming from a common distribution, analyzing early exit per instance with a single global threshold... Future work could relax these assumptions by adapting thresholds for each problem."
- **Why unresolved**: A global threshold is efficient but likely suboptimal; it may waste compute on easy problems that could exit earlier with a looser threshold, or truncate hard problems that need a stricter threshold to reach a solution.
- **What evidence would resolve it**: Experiments comparing the Area Under the Curve (AUC) of token usage vs. accuracy between the global δ baseline and a method using dynamic, per-question thresholds based on difficulty features.

## Limitations

- EAT effectiveness appears domain-dependent, with limited validation on complex reasoning tasks like GPQA-Diamond
- The EMA variance threshold δ requires careful tuning, with no systematic guidance provided for different model scales or task types
- While proxy-based EAT enables black-box applications, the assumption that proxy model entropy trajectories reliably mirror the main model's convergence needs broader validation

## Confidence

- **High confidence**: The core mechanism of using entropy after `</think>` as a convergence signal is well-supported by empirical results on MATH500 and AIME2025
- **Medium confidence**: The effectiveness of proxy-based EAT for black-box models is supported by limited experiments (1.5B proxy for 70B model) on specific datasets
- **Medium confidence**: The claim of 13-21% token reduction without accuracy loss is supported for the tested datasets, but may not generalize to all reasoning tasks or model architectures

## Next Checks

1. **Cross-dataset validation**: Test EAT on diverse reasoning datasets including scientific reasoning, code generation, and multi-step mathematical problems to assess generalizability beyond MATH500 and AIME2025.

2. **Threshold sensitivity analysis**: Conduct systematic ablation studies across a wider range of δ values and EMA parameters (α) to establish guidelines for threshold selection across different model scales and task complexities.

3. **Proxy alignment validation**: Evaluate proxy model effectiveness across multiple proxy sizes (1B, 3B, 8B) and reasoning model scales (7B, 14B, 70B) to quantify the relationship between proxy capacity and alignment accuracy, and establish minimum proxy requirements for reliable black-box early exiting.