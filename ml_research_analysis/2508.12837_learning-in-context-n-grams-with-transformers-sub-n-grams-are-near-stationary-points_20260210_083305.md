---
ver: rpa2
title: 'Learning In-context n-grams with Transformers: Sub-n-grams Are Near-stationary
  Points'
arxiv_id: '2508.12837'
source_url: https://arxiv.org/abs/2508.12837
tags:
- layer
- learning
- token
- attention
- in-context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies the loss landscape of transformers trained\
  \ on in-context next-token prediction, specifically focusing on learning in-context\
  \ n-gram language models. The authors provide a sufficient condition for parameter\
  \ configurations to be stationary points of the cross-entropy loss, and construct\
  \ parameter configurations for a simplified transformer model that represent k-gram\
  \ estimators for k \u2264 n."
---

# Learning In-context n-grams with Transformers: Sub-n-grams Are Near-stationary Points

## Quick Facts
- arXiv ID: 2508.12837
- Source URL: https://arxiv.org/abs/2508.12837
- Reference count: 40
- Primary result: Proves sub-n-gram estimators are near-stationary points of transformer loss landscapes, explaining stage-wise learning dynamics

## Executive Summary
This paper establishes a theoretical framework explaining why transformers exhibit stage-wise learning dynamics when trained on in-context next-token prediction tasks. The authors prove that parameter configurations representing simpler k-gram estimators (where k < n) are near-stationary points of the population cross-entropy loss for n-gram language models. By constructing explicit parameter configurations for a simplified transformer architecture that implement k-gram estimators, they show that the gradient vanishes asymptotically at these solutions. This theoretical result provides a mechanistic explanation for widely observed phenomena including optimization plateaus and emergent phase transitions during training.

## Method Summary
The authors analyze a simplified two-layer disentangled attention-only transformer trained on data generated from an n-gram language model. They construct explicit parameter configurations θ*_k that implement k-gram estimators for k ≤ n by using the first layer to retrieve specific history tokens and the second layer to match these histories against previous positions. The key theoretical result bounds the gradient norm at these configurations by the statistical error of the k-gram estimator, establishing stationarity in the limit of infinite sequence length and parameter norm. Empirical validation uses a synthetic trigram language model with vocabulary size 5 and sequence length 32, training a simplified transformer with fixed value matrices and concatenated head outputs.

## Key Results
- Proves sub-n-gram estimators (k < n) are near-stationary points of the population loss for n-gram models
- Constructs explicit parameter configurations θ*_k implementing k-gram estimators in simplified transformers
- Shows gradient norm vanishes at these solutions as T, c → ∞
- Demonstrates stage-wise learning dynamics in experiments align with theoretical predictions

## Why This Works (Mechanism)

### Mechanism 1: Sub-n-gram Stationarity
Parameter configurations representing simpler k-gram estimators are near-stationary points because the gradient depends only on the error against the conditional probability of the suffix history, not the full history. If the model correctly estimates this simpler conditional probability, the gradient norm vanishes asymptotically. This requires T, c → ∞.

### Mechanism 2: Two-Layer History Aggregation
A two-layer attention architecture implements n-gram estimators by separating history retrieval (Layer 1 attends to specific relative positions) from matching (Layer 2 compares constructed histories). Layer 1 heads attend to positions -1, -2, etc. to construct history vectors, while Layer 2 uniformly attends to positions where histories match, averaging subsequent tokens.

### Mechanism 3: Softmax Gradient Sparsity
The deactivation of heads irrelevant to the current k-gram solution is reinforced by softmax derivative self-bounding. At k-gram solutions, attention scores for non-matching histories are near zero, and because softmax derivatives are bounded by output values, gradients for deactivated components vanish, stabilizing plateaus.

## Foundational Learning

**Concept: Markov Chains (n-gram models)**
Why needed: The paper assumes the data generating process is an n-gram model where p(x_t|x_<t) = p(x_t|x_t-n+1, ..., x_t-1). Understanding this Markov property is essential to grasp what sub-n-gram estimators approximate.
Quick check: If a process is a trigram (n=3), what history does a bigram (k=2) estimator condition on? Answer: Only x_t-1 and x_t-2.

**Concept: Population vs. Empirical Loss**
Why needed: Theoretical stationarity is proven for population loss (infinite data limit), while training uses finite samples. This gap explains why gradients might not be exactly zero in practice.
Quick check: Does a stationary point on population loss guarantee zero gradient on a finite training batch? No.

**Concept: Disentangled Transformers**
Why needed: The analysis uses modified architecture where heads concatenate rather than add outputs, decoupling heads for cleaner analysis but requiring mapping back to standard architectures.
Quick check: In standard transformers, residual streams are additive; how does the "disentangled" version keep head outputs separate? By concatenating instead of adding.

## Architecture Onboarding

**Component map:**
Orthogonal embeddings -> Layer 1 (n-1 heads, attend to relative positions -1, -2, ...) -> Concatenated outputs -> Layer 2 (1 head, match histories via Q,K, read skip connection via fixed V) -> Unembedding to vocabulary

**Critical path:**
1. Layer 1 aggregates x_t-1, ..., x_t-n+1 into vector r_1
2. Layer 2 computes attention scores based on dot-product similarity of history vectors
3. Attention aggregates "next tokens" from positions with matching histories

**Design tradeoffs:**
Disentangled (Concat) vs. Standard (Add): The paper uses concatenation for theoretical ease, claiming equivalent representational power but requiring more complex interference management in standard architectures. Fixed Value Matrix: Layer 2 Value is fixed to identity/skip, simplifying learning dynamics to focus on Q,K but removing a degree of freedom.

**Failure signatures:**
Infinite Plateaus: Model doesn't escape "near-stationary" region, permanently settling for sub-n-gram solution. Premature Deactivation: Poor initialization causes incorrect head deactivation, failing to construct necessary history.

**First 3 experiments:**
1. Loss Plateau Visualization: Train on synthetic n-gram data and plot test loss against theoretical k-gram estimator losses
2. Attention Head Analysis: Inspect attention matrix A^(h)_1 during plateau phases to verify head attending patterns
3. Gradient Norm Monitoring: Plot gradient norm during training to identify spikes at transitions and low norms during plateaus

## Open Questions the Paper Calls Out
None

## Limitations
- Proof assumptions (T, c → ∞) create gap between theory and practical finite-sample training
- Simplified "disentangled" architecture differs from standard transformers, limiting direct applicability
- Analysis assumes exact n-gram data generation, not addressing non-Markovian natural language structure

## Confidence

**High Confidence (☑️)**: Theoretical framework for analyzing stationary points of population loss is mathematically sound, with Theorem 4.1's gradient bound following logically from assumptions.

**Medium Confidence (⚠️)**: Empirical demonstration of stage-wise dynamics is convincing for synthetic setup, with good alignment between observed plateaus and theoretical k-gram losses.

**Low Confidence (❌)**: Claims about implications for real-world transformer training are speculative, lacking validation on actual language models or demonstration of simplified analysis mapping to standard architectures.

## Next Checks

1. **Finite-Sample Gradient Analysis**: Compute empirical gradient norm during training on synthetic n-gram data and compare against theoretical population gradient bound, plotting how the gap evolves with sequence length and parameter norm.

2. **Standard Architecture Mapping**: Implement k-gram stationary solutions in standard residual transformer architecture, train from initialization, and verify whether stage-wise dynamics emerge while comparing attention patterns and gradient norms between architectures.

3. **Natural Language Transfer**: Apply same analysis pipeline to real language modeling dataset (e.g., WikiText-2), training simplified transformer and tracking whether stage-like plateaus appear in loss curve while analyzing attention patterns for relative-position attending behavior.