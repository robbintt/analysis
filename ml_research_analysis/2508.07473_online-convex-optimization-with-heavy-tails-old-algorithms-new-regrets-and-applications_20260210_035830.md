---
ver: rpa2
title: 'Online Convex Optimization with Heavy Tails: Old Algorithms, New Regrets,
  and Applications'
arxiv_id: '2508.07473'
source_url: https://arxiv.org/abs/2508.07473
tags:
- theorem
- have
- which
- optimization
- stochastic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper examines the performance of classical online convex\
  \ optimization (OCO) algorithms under heavy-tailed noise, where stochastic gradients\
  \ only have finite p-th central moments for some p \u2208 (1,2]. The main result\
  \ is that three standard OCO algorithms\u2014Online Gradient Descent (OGD), Dual\
  \ Averaging (DA), and AdaGrad\u2014can achieve optimal in-expectation regret bounds\
  \ without any algorithmic modification, provided the feasible set is bounded."
---

# Online Convex Optimization with Heavy Tails: Old Algorithms, New Regrets, and Applications

## Quick Facts
- arXiv ID: 2508.07473
- Source URL: https://arxiv.org/abs/2508.07473
- Reference count: 40
- One-line primary result: Classical OCO algorithms achieve optimal heavy-tailed regret bounds without modification on bounded domains

## Executive Summary
This paper demonstrates that classical online convex optimization algorithms—OGD, Dual Averaging, and AdaGrad—can achieve optimal regret bounds under heavy-tailed noise without algorithmic modification, provided the feasible set is bounded. The key insight is that the bounded domain assumption allows these algorithms to implicitly handle infinite variance noise through geometric constraints, eliminating the need for gradient clipping or other modifications. Notably, AdaGrad achieves these bounds without requiring knowledge of problem-dependent parameters like the Lipschitz constant, noise level, or tail index.

## Method Summary
The paper analyzes three classical OCO algorithms under the assumption that stochastic gradients have finite p-th central moments for p ∈ (1,2]. For OGD and Dual Averaging, the analysis leverages Young's inequality to bound the interaction between noise and step size using the bounded domain diameter D, shifting the noise dependency from the second moment to the finite p-th moment. For AdaGrad, the adaptive stepsize automatically scales appropriately for heavy tails without tuning. The analysis establishes regret bounds of the form GD√T + σDT^{1/p}, which are optimal in all parameters. The bounded domain assumption is critical, as the projection operation onto this set provides the geometric constraint necessary to handle heavy-tailed noise.

## Key Results
- Three classical OCO algorithms (OGD, Dual Averaging, AdaGrad) achieve optimal in-expectation regret bounds GD√T + σDT^{1/p} under heavy-tailed noise without algorithmic modification
- AdaGrad achieves these bounds without knowing problem parameters like Lipschitz constant G, noise level σ, or tail index p
- The bounded domain assumption is essential for the theory to work; unbounded domains break the analysis
- Applications include optimal convergence rates for stochastic convex and nonconvex optimization under heavy tails without gradient clipping
- First convergence results for Hölder smooth nonconvex optimization under heavy tails via the O2NC framework

## Why This Works (Mechanism)

### Mechanism 1: Geometric Noise Truncation via Bounded Domain
- **Claim:** Classical algorithms implicitly handle infinite variance noise if the decision domain is bounded, rendering explicit gradient clipping unnecessary
- **Mechanism:** Standard OCO analysis often bounds noise by the term η∥εₜ∥², which diverges if variance is infinite. The paper utilizes a tighter bound on the update step ⟨gₜ, xₜ - xₜ₊₁⟩ coupled with the bounded diameter D. By applying Young's inequality, the noise dependency shifts from the second moment to the finite p-th moment (∥εₜ∥ᵖ), effectively "truncating" the heavy tail through the geometric constraint of the bounded set
- **Core assumption:** The feasible set X is bounded with diameter D, and the noise admits a finite p-th central moment for p ∈ (1,2]
- **Evidence anchors:** [abstract] "Under the standard bounded domain assumption, we establish new regrets... without any algorithmic modification." [section 3.1] "The basic idea... is to leverage the boundness property of X... Handle p<2 in a simple way... [using] Young's inequality."

### Mechanism 2: Adaptive Scale Normalization (AdaGrad)
- **Claim:** AdaGrad achieves optimal regret bounds without prior knowledge of the tail index p or noise scale σ
- **Mechanism:** AdaGrad adapts its step size inversely to the cumulative gradient norm. The analysis shows that the regret is bounded by the square root of the sum of squared gradients. When decomposed, the noise term becomes √(∑∥εₜ∥²). The paper argues this is upper-bounded by (∑∥εₜ∥ᵖ)¹/ᵖ, allowing the algorithm to automatically scale appropriately for heavy tails without tuning
- **Core assumption:** Assumption 1 (bounded domain, convex losses, heavy-tailed noise)
- **Evidence anchors:** [abstract] "Notably, AdaGrad achieves this bound without knowing problem parameters." [section 3.3] "AdaGrad is just OGD with an adaptive stepsize... it is this adaptive stepsize that can help us to overcome the above undesired point [of knowing parameters]."

### Mechanism 3: Online-to-Nonconvex Conversion (O2NC)
- **Claim:** Heavy-tailed nonsmooth nonconvex optimization can be reduced to an online linear optimization problem with K-shifting regret
- **Mechanism:** The O2NC framework converts the search for a (δ, ε)-stationary point into an online game. By plugging in the heavy-tailed OCO regret bounds from this paper, one derives convergence rates. Crucially, the "competing sequence" is chosen to align with the gradient direction, ensuring that the heavy-tailed noise affects only the variance term, not the structural bias
- **Core assumption:** The objective F is Lipschitz and well-behaved (Assumption 2)
- **Evidence anchors:** [abstract] "A particularly interesting one is the first provable and optimal convergence result for nonsmooth nonconvex optimization... without gradient clipping." [section 4.2.1] "Theorem 4 here provides a new and the first theoretical guarantee for O2NC under heavy tails."

## Foundational Learning

- **Concept:** **Online Convex Optimization (OCO) & Regret**
  - **Why needed here:** This is the core framework. The paper proves that "old algorithms" work by analyzing their *regret*—the difference between cumulative loss and the best fixed decision in hindsight
  - **Quick check question:** Can you explain why a sublinear regret bound (O(√T)) implies that the algorithm "learns" (average regret goes to 0)?

- **Concept:** **Heavy-Tailed Noise (Finite p-th Moment)**
  - **Why needed here:** This defines the difficulty. Unlike standard settings where noise has finite variance (p=2), heavy tails (1 < p < 2) allow for occasional massive outliers in gradient estimates, which historically broke convergence proofs
  - **Quick check question:** If a noise distribution has a finite 1.5-th moment but infinite variance, what happens to the standard SGD analysis that relies on E[∥ε∥²]?

- **Concept:** **Young's Inequality (for Products)**
  - **Why needed here:** This is the mathematical tool that enables Mechanism 1. It allows bounding a product (noise × step) by terms that separate the noise moment and the step size
  - **Quick check question:** How does Young's inequality allow us to trade off a dependency on ∥ε∥² for a dependency on ∥ε∥ᵖ?

## Architecture Onboarding

- **Component map:** Environment (adversary) -> Oracle (stochastic gradient generator) -> Learner (OGD, DA, or AdaGrad) -> Constraint (bounded domain X with diameter D)

- **Critical path:**
  1. Initialize x₁ ∈ X
  2. Receive gradient estimate gₜ
  3. Compute update yₜ₊₁ = xₜ - ηₜgₜ
  4. **Project:** xₜ₊₁ = Πₓ(yₜ₊₁). (This projection onto a bounded set is the "secret sauce" allowing the theory to work)
  5. Accumulate regret

- **Design tradeoffs:**
  - **OGD/DA vs. AdaGrad:** OGD/DA achieve optimal rates *if* you tune the step size η knowing p and σ. AdaGrad achieves the same rates *adaptively* (no hyperparameter knowledge required), making it robust for "unknown" heavy-tailed environments
  - **Bounded vs. Unbounded Domain:** The paper trades generality for stability. These guarantees **do not hold** if the domain is unbounded (Section 6)

- **Failure signatures:**
  - **Unbounded Domain:** Regret bounds may diverge
  - **Unknown σ in OGD:** If using OGD with a step size designed for finite variance (σ₂) while actual noise has infinite variance, the algorithm may exhibit high instability

- **First 3 experiments:**
  1. **Synthetic Validation:** Implement OGD and AdaGrad on a simple quadratic. Inject noise with finite p=1.5 moment but infinite variance. Plot regret vs. T to verify the T¹/ᵖ scaling
  2. **Ablation on Boundedness:** Run the same experiment but relax the projection constraint (effectively unbounded domain) to observe the break condition where the theory no longer applies
  3. **Nonconvex Stress Test:** Apply the O2NC framework to a nonsmooth nonconvex function (e.g., a ReLU network) with heavy-tailed noise, comparing "clipped" vs. "unclipped" AdaGrad to verify the paper's claim that clipping is unnecessary

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Is the regret bound of T²⁻ᵖ for Online Strongly Convex Optimization under heavy-tailed noise tight, or can it be improved to match the lower bound?
- **Basis in paper:** [explicit] Appendix A.2 states, "we suspect Theorem 8 is not tight in T for p ∈ (1,2)... we conjecture that a way to obtain a better regret bound than T²⁻ᵖ exists."
- **Why unresolved:** The current analysis yields a convergence rate of 1/Tᵖ⁻¹, which is worse than the known lower bound of 1/T²⁻²/ᵖ for the stochastic optimization equivalent
- **What evidence would resolve it:** A new analysis of OGD (or a new algorithm) achieving a regret bound consistent with the 1/T²⁻²/ᵖ rate, or a proof establishing that T²⁻ᵖ is indeed the lower bound for the online setting

### Open Question 2
- **Question:** Can classical OCO algorithms provably handle heavy-tailed noise under a weaker condition than a bounded domain?
- **Basis in paper:** [explicit] Section 6 states, "The main limitation of our work is that all the proof crucially relies on the bounded domain assumption... Finding a weaker sufficient condition... is a direction worth studying in the future."
- **Why unresolved:** The proofs utilize the domain diameter D to bound the interaction between the noise and the update step, specifically via the inequality ∥εₜ∥∥xₜ - xₜ₊₁∥
- **What evidence would resolve it:** A theoretical demonstration of finite regret for OGD, DA, or AdaGrad on unbounded domains under heavy-tailed noise without gradient clipping or algorithmic modification

### Open Question 3
- **Question:** Can the gap between the upper and lower bounds for nonsmooth nonconvex optimization under heavy tails be closed for general ε or the noiseless case?
- **Basis in paper:** [explicit] Section 4.2.3 notes, "for any general ε > 0 or the case σ=0, there is still a gap between the upper and lower bounds. Closing this gap could be an interesting direction."
- **Why unresolved:** The lower bound provided in Theorem 6 (and Theorem 12) is specific to the high-accuracy and noisy regime (small ε with σ > 0), leaving the deterministic or low-noise regimes uncharacterized
- **What evidence would resolve it:** A refined lower bound proof or an improved upper bound analysis that matches across all parameter regimes (ε, σ, δ)

## Limitations
- The bounded domain assumption is essential but restrictive; the analysis does not extend to unbounded domains where the projection step loses its noise-dampening effect
- While the paper claims AdaGrad works without knowing parameters, the practical sensitivity to initial conditions and parameter settings (like the initial accumulator value) remains unclear
- The theoretical analysis focuses on in-expectation regret; high-probability guarantees for heavy-tailed noise are not provided

## Confidence

- **High Confidence:** The core theoretical result that classical OCO algorithms achieve optimal regret bounds under heavy-tailed noise with bounded domains is well-supported by rigorous analysis and matching lower bounds
- **Medium Confidence:** The practical implications for stochastic optimization, particularly the claim that clipping is unnecessary, are theoretically sound but require empirical validation in realistic settings
- **Low Confidence:** The extension to nonsmooth nonconvex optimization via the O2NC framework, while promising, has limited empirical verification and relies on the heavy-tailed OCO results

## Next Checks

1. Implement a synthetic experiment comparing OGD with standard vs. heavy-tailed-aware stepsizes to empirically verify the regret scaling difference
2. Test the bounded vs. unbounded domain break condition by running algorithms with and without projection constraints
3. Evaluate the practical performance of AdaGrad vs. clipped variants on a real-world heavy-tailed dataset to assess the "clipping is unnecessary" claim