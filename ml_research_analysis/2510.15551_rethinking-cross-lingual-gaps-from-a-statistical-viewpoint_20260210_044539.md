---
ver: rpa2
title: Rethinking Cross-lingual Gaps from a Statistical Viewpoint
arxiv_id: '2510.15551'
source_url: https://arxiv.org/abs/2510.15551
tags:
- source
- target
- variance
- response
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Cross-lingual knowledge transfer in large language models suffers
  from performance gaps due to increased response variance in target languages, not
  from knowledge barriers. By formalizing this gap using bias-variance decomposition,
  the authors show that ensembling responses and targeted inference-time interventions
  can reduce cross-lingual divergence by 20-25%.
---

# Rethinking Cross-lingual Gaps from a Statistical Viewpoint

## Quick Facts
- **arXiv ID**: 2510.15551
- **Source URL**: https://arxiv.org/abs/2510.15551
- **Reference count**: 40
- **Primary result**: Cross-lingual gaps stem from response variance, not knowledge barriers; ensembling reduces gaps by 20-25% without retraining

## Executive Summary
This paper reinterprets cross-lingual performance gaps in large language models through a statistical lens. The authors argue that differences in accuracy between source (e.g., English) and target languages are primarily due to increased response variance in the target language, not a lack of knowledge. By applying bias-variance decomposition, they demonstrate that target-language responses are more noisy, and that this noise is unbiased—meaning it can be mitigated through ensembling or inference-time interventions. Their findings suggest that improving source-language accuracy or reducing variance in target responses can effectively close cross-lingual gaps, offering practical, model-agnostic solutions.

## Method Summary
The authors formalize cross-lingual gaps using bias-variance decomposition, showing that target-language responses exhibit higher variance but no systematic bias. They validate this by measuring variance across five state-of-the-art models on ECLeKTic and MMLU benchmarks. By applying ensembling and targeted inference-time interventions, they reduce cross-lingual divergence by 20-25%, confirming that variance—not knowledge—drives most gaps. The proportional relationship between source and target variance allows for predictable improvements.

## Key Results
- Cross-lingual gaps are primarily due to increased response variance in target languages, not knowledge barriers
- Ensembling responses reduces cross-lingual divergence by 20-25% without retraining
- Response variance in source and target languages are proportional, so improving source accuracy enhances target performance

## Why This Works (Mechanism)
The core mechanism is that cross-lingual gaps arise from unbiased noise in target-language responses. Because this noise is random (not systematic), it can be reduced through aggregation (ensembling) or targeted inference-time interventions. The proportional variance relationship means that interventions effective in the source language translate predictably to the target, enabling scalable mitigation strategies.

## Foundational Learning

**Bias-variance decomposition**: Separates error into systematic bias and random variance. Needed to quantify how much of cross-lingual gaps stem from noise vs. knowledge. Quick check: Verify decomposition holds across diverse tasks and languages.

**Ensemble methods**: Combine multiple model outputs to reduce variance. Needed to validate that noise is the dominant factor and can be mitigated. Quick check: Measure variance reduction with increasing ensemble size.

**Inference-time interventions**: Techniques like calibration or prompting applied during generation. Needed to test if gaps can be closed without retraining. Quick check: Apply targeted prompts and measure variance change.

## Architecture Onboarding

**Component map**: Source language responses → Variance analysis → Target language responses → Variance comparison → Ensemble/intervention → Reduced gap

**Critical path**: Data generation (source/target) → Variance decomposition → Ensemble/intervention application → Gap measurement

**Design tradeoffs**: Ensembling increases compute and latency; inference-time interventions may require task-specific tuning. Variance reduction is model-agnostic but depends on response quality.

**Failure signatures**: If variance is not the dominant factor, ensembling/interventions will yield minimal gains. If target responses have systematic bias, variance reduction alone won’t close gaps.

**First experiments**: 1) Measure variance in source/target for a held-out model. 2) Apply ensembling to target responses and quantify gap reduction. 3) Test if source accuracy improvements translate to target gains.

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis may overlook systematic biases conflated with random variance
- Experiments limited to five models and two benchmarks, limiting generalizability
- Does not address domain shift, instruction-following, or low-resource language quality

## Confidence

**High confidence**: Response variance significantly contributes to cross-lingual gaps (supported by statistical decomposition and empirical reduction via ensembling).

**Medium confidence**: Variance is the dominant factor (analysis does not fully exclude systematic causes; proportional variance relationship may vary).

**Medium confidence**: Mitigation strategies generalize (experiments constrained to specific models; effectiveness may depend on task and language similarity).

## Next Checks
1. Test variance-reduction across broader language pairs, including low-resource and structurally distant languages.
2. Conduct ablation studies isolating noise from systematic bias using synthetic data or perturbation analysis.
3. Evaluate proportional variance relationship for models with different pretraining objectives and scaling regimes.