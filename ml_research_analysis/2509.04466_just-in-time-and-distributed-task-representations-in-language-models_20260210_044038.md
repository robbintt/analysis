---
ver: rpa2
title: Just-in-time and distributed task representations in language models
arxiv_id: '2509.04466'
source_url: https://arxiv.org/abs/2509.04466
tags:
- task
- representations
- tasks
- transferrable
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies how language models represent and use tasks
  during in-context learning. It compares two types of task representations: one that
  identifies the task type, and another that is transferrable and enables models to
  perform the task without examples.'
---

# Just-in-time and distributed task representations in language models

## Quick Facts
- arXiv ID: 2509.04466
- Source URL: https://arxiv.org/abs/2509.04466
- Authors: Yuxuan Li; Declan Campbell; Stephanie C. Y. Chan; Andrew Kyle Lampinen
- Reference count: 28
- This paper studies how language models represent and use tasks during in-context learning, finding that transferrable task representations form sporadically just before answer generation while task identity representations are stable throughout prompts.

## Executive Summary
This paper investigates how language models represent and use tasks during in-context learning by comparing task identity representations (which identify the task type) with transferrable representations (which enable task performance without examples). The study reveals that transferrable task representations form sporadically and just before answer generation, while task identity representations remain stable throughout the prompt. Evidence accumulation across examples is reflected in transferrable representations through variance reduction, but this effect is inconsistent and often limited to small task scopes. For complex or multi-subtask cases, representations become more distributed or fail to transfer fully.

## Method Summary
The study extracts task vectors (residual activations) from the last colon token before answer generation in 8-shot prompts across 14 simple tasks. A layer search identifies the best restoration layer using 50 development queries, then task vectors are patched onto corresponding tokens in zero-shot prompts to measure recontextualized zero-shot accuracy. Task identity is probed using linear classifiers trained on non-key tokens. Evidence accrual is studied by extracting task vectors from different k-shot examples (k ∈ {0,1,2,4,8,16,32}) and measuring variance reduction and magnitude changes. Mixed-generation tasks are analyzed by intervening at comma tokens preceding subtask answers.

## Key Results
- Transferrable task representations activate sporadically at key tokens (primarily the colon before answer generation) rather than forming gradually across context
- Task identity is reliably decodable from any format token throughout the prompt, but only colon-token interventions restore task performance
- Evidence accrual manifests as variance reduction and magnitude decrease across k-shot examples, but counting tasks fail to show this pattern
- Complex tasks requiring state-tracking or multi-subtask structure need distributed representations across multiple token sites

## Why This Works (Mechanism)

### Mechanism 1: Just-in-Time Task Representation Formation
The model maintains passive task identity signals throughout context but actively constructs transferable task vectors only at specific positions—primarily the colon token immediately preceding answer generation. This creates a two-phase system: inert task sensitivity plus just-in-time active representation.

### Mechanism 2: Evidence Accrual via Variance Reduction
As k-shot examples increase, task vectors show reduced variance across samples and decreased L2-norm, suggesting denoising and convergence toward stable task encodings. The ratio of recontextualized-to-few-shot accuracy remains relatively stable across k.

### Mechanism 3: Distributed Representations for Complex Tasks
In mixed-generation tasks, task vectors from the colon token support only the first subtask. Subsequent subtask representations form at comma tokens prior to each answer. Full restoration requires interventions at multiple positions.

## Foundational Learning

- **Concept: Residual Stream Activation Patching**
  - Why needed: The entire methodology depends on extracting layer activations and overwriting them onto zero-shot forward passes
  - Quick check: Can you explain why patching middle-layer activations (vs. early/late layers) best restores task context?

- **Concept: Task Vectors vs. Function Vectors**
  - Why needed: Two extraction methods with different intervention types produce convergent findings on temporal locality
  - Quick check: What is the operational difference between Hendel-style task vectors and Todd-style function vectors?

- **Concept: Linear Probing for Task Identity**
  - Why needed: Demonstrates task identity is continuously decodable independent of transferability, establishing the two-representation distinction
  - Quick check: If a linear probe can decode task from token X, does that guarantee the representation can restore task performance when patched? (Answer: No—see Figure 3B.)

## Architecture Onboarding

- **Component map:** Residual stream layers → Key token positions (colon before answer, comma before subtask answers) → Format tokens (Q, 1st colon, A, newline)
- **Critical path:** 1) Layer search: Identify best restoration layer using 8-shot development set, 2) Extraction: Sample task vector from last colon token using dummy queries, 3) Intervention: Patch onto corresponding token position in zero-shot prompt, 4) Evaluation: Exact string match against ground truth
- **Design tradeoffs:** Single-layer vs. multi-layer intervention (paper uses single-layer; multi-layer methods may improve restoration but complicate dynamics analysis); Overwrite vs. additive patching (task vectors overwrite; function vectors add—both show locality but Gemma normalization may require scaling for additive approaches)
- **Failure signatures:** Counting tasks fail to show evidence accrual despite 32-shot context; Mixed-generation task accuracy decays over output units; Task vectors from non-key tokens show near-zero restoration
- **First 3 experiments:** 1) Replicate temporal locality: Extract task vectors from each format token in 8-shot prompts; verify only colon-token patches restore accuracy, 2) Test evidence accrual curve: Plot recontextualized accuracy and task vector variance across k ∈ {0,1,2,4,8,16,32}, 3) Probe task identity generalization: Train linear classifier on non-key tokens; test generalization to colon-token representations (expected: high for simple tasks, moderate for list-operations)

## Open Questions the Paper Calls Out

**Open Question 1:** What mechanistic factors or training pressures drive the emergence of strong temporal and scope locality in transferrable task representations? The paper characterizes the "just-in-time" phenomenon but only speculates on causes, such as the stability of the residual stream or implicit normative considerations to save capacity.

**Open Question 2:** Can advanced multi-layer or multi-token intervention methods successfully restore task contexts for "hard-to-transfer" tasks that require state tracking? The study relied primarily on single-token, single-layer patching, which may be insufficient for complex tasks where the task state is distributed across multiple layers.

**Open Question 3:** Do the observed dynamics of sporadic task representations generalize to naturalistic settings where tasks are not cleanly decomposable? The experiments utilized structured tasks with clear semantic boundaries, whereas real-world language use often involves ambiguous or overlapping task structures.

## Limitations

- Task vector extraction locality claims may be overly restrictive; alternative methods like function vectors or multi-layer interventions could potentially capture task representations at other token positions
- Evidence accrual findings show inconsistent patterns across model architectures, suggesting the mechanism may be architecture-dependent rather than universal
- Distributed representation claims for complex tasks have limited testing scope and incomplete restoration, suggesting the mechanism may be more nuanced than presented

## Confidence

- **High confidence:** Task identity is continuously decodable from any format token position throughout the prompt
- **Medium confidence:** Transferrable task representations form sporadically and just-in-time at specific token positions
- **Medium confidence:** Evidence accrual manifests as variance reduction and magnitude decrease across k-shot examples
- **Medium confidence:** Complex tasks requiring state-tracking or multi-subtask structure need distributed representations

## Next Checks

1. **Alternative extraction validation:** Test task vector extraction from multiple token positions (Q, 1st colon, A, newline) using both task vector and function vector methods. Compare restoration accuracy to verify that colon-token locality is extraction-method-independent.

2. **Architecture generalization study:** Repeat the evidence accrual analysis across a broader range of model architectures (different families, sizes, and training objectives). Map the variance reduction and magnitude decrease patterns to identify which architectural features correlate with consistent evidence accrual.

3. **Distributed representation completeness test:** For mixed-generation tasks, implement multi-token intervention strategies that patch both colon and comma positions simultaneously. Measure whether combined interventions achieve near-perfect restoration across all subtasks.