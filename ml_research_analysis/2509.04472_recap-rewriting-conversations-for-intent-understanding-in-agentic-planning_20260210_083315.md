---
ver: rpa2
title: 'RECAP: REwriting Conversations for Intent Understanding in Agentic Planning'
arxiv_id: '2509.04472'
source_url: https://arxiv.org/abs/2509.04472
tags:
- intent
- user
- plan
- plans
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "RECAP addresses the challenge of intent understanding in multi-agent\
  \ planning systems, where real-world dialogues are often ambiguous, underspecified,\
  \ or dynamic. The paper introduces RECAP, a benchmark designed to evaluate intent\
  \ rewriting\u2014reframing user-agent dialogues into concise, clarified representations\
  \ of user goals."
---

# RECAP: REwriting Conversations for Intent Understanding in Agentic Planning

## Quick Facts
- arXiv ID: 2509.04472
- Source URL: https://arxiv.org/abs/2509.04472
- Authors: Kushan Mitra; Dan Zhang; Hannah Kim; Estevam Hruschka
- Reference count: 23
- Primary result: Intent rewriting via prompt-based and DPO-trained approaches significantly improves downstream plan quality in multi-agent planning systems

## Executive Summary
RECAP addresses the challenge of intent understanding in multi-agent planning systems, where real-world dialogues are often ambiguous, underspecified, or dynamic. The paper introduces RECAP, a benchmark designed to evaluate intent rewriting—reframing user-agent dialogues into concise, clarified representations of user goals. The dataset captures diverse challenges such as ambiguity, intent drift, vagueness, and mixed-goal conversations. The authors develop prompt-based and DPO-trained rewriters that significantly improve downstream plan quality compared to baselines. Human and LLM-based evaluations show that plans from advanced rewrites are consistently preferred, especially in complex scenarios. DPO-trained models further enhance performance by aligning rewrites with human preferences. The results demonstrate that intent rewriting is a critical, tractable component for improving agentic planning in open-domain dialogue systems.

## Method Summary
RECAP introduces a benchmark for evaluating intent rewriting in multi-agent planning. The authors create a dataset capturing diverse conversational challenges including ambiguity, intent drift, vagueness, and mixed goals. They develop three rewriter variants (Dummy, Basic, Advanced) using GPT-4o with progressively more sophisticated prompts. The planner is a static GPT-4o that converts rewritten intents into DAG plans. A fine-tuned GPT-4.1 evaluator predicts plan preferences. They also train DPO rewriters using preference pairs derived from plan comparisons. The evaluation combines human judgments and LLM-based scoring across multiple metrics including plan preference, structural differences, and zero-shot plan ranking.

## Key Results
- Advanced rewriter achieves 50-63% win rates across conversation lengths on RECAP-toy, significantly outperforming IN3 baseline
- DPO:human achieves 48.88% win rate vs Advanced (22.22% loss) on human-preferred plans
- Fine-tuned gpt-4.1 evaluator reaches 65.01% test accuracy in predicting human plan preferences
- LLM evaluators successfully predict human preferences at scale while reducing annotation costs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicit intent rewriting improves downstream plan quality by reducing cognitive load on the planner.
- Mechanism: A dedicated rewriter module extracts the latest user goal, filters noise, and resolves ambiguity before the planner sees the input. This prevents the planner from misinterpreting agent suggestions as user requests or acting on outdated intents.
- Core assumption: Planners are sensitive to surface form and implicit signals in raw dialogue history.
- Evidence anchors:
  - [abstract] "prompt-based rewriting approach that outperforms baselines, in terms of plan preference"
  - [Section 5.2, Table 1] Advanced rewriter achieves 50-63% win rates across conversation lengths on RECAP-toy, while IN3 shows mostly ties due to lower complexity.
  - [corpus] Related work on prompt rewriting (arXiv:2503.16789) shows similar improvements in LLM response generation when user prompts are reformulated.
- Break condition: If plans generated from raw dialogue and rewritten intent show no structural or semantic differences, the rewriting module adds no value.

### Mechanism 2
- Claim: DPO-trained rewriters align output with human preferences for plan quality without requiring gold-standard rewrites.
- Mechanism: Preference pairs are constructed by tracing preferred plans back to their source rewrites. DPO optimizes the rewriter to generate outputs more likely to produce human-preferred plans, creating indirect supervision.
- Core assumption: Human plan preferences reliably reflect intent understanding quality.
- Evidence anchors:
  - [abstract] "fine-tuning two DPO-based rewriters yields additional utility gains"
  - [Section 5.5, Table 5] DPO:human achieves 48.88% win rate vs Advanced (22.22% loss); DPO:LLM achieves 28.88% win rate.
  - [corpus] Weak corpus signal—no directly comparable DPO-based rewriting work found in neighbors.
- Break condition: If preference labels are noisy or inconsistent, DPO training may amplify errors rather than improve alignment.

### Mechanism 3
- Claim: Fine-tuned LLM evaluators can predict human plan preferences at scale.
- Mechanism: Train a preference model on human-labeled plan pairs to predict which plan better fulfills user intent, reducing annotation cost while maintaining evaluation quality.
- Core assumption: LLMs can learn the rubrics humans use for plan comparison (latest intent, fabrication avoidance, task granularity, completeness, logical order).
- Evidence anchors:
  - [Section 5.4, Table 4] Fine-tuned gpt-4.1 achieves 65.01% test accuracy vs ~38-45% for zero-shot baselines.
  - [Section 3.3] "fine-tune two preference models using the collected human labels on RECAP-train with the majority vote"
  - [corpus] Related work (arXiv:2509.19700) shows contextual retrieval improvements via query rewriting, suggesting learned evaluators generalize across dialogue tasks.
- Break condition: If LLM evaluator diverges from human judgment on edge cases, automated evaluation becomes unreliable.

## Foundational Learning

- Concept: **Intent Drift**
  - Why needed here: Users frequently change goals mid-conversation; rewriters must track the most recent intent rather than aggregate all stated goals.
  - Quick check question: Given a dialogue where a user first requests Italian food then switches to Mexican, what should the rewritten intent contain?

- Concept: **Direct Preference Optimization (DPO)**
  - Why needed here: The paper uses DPO to train rewriters from preference pairs rather than supervised labels; understanding the loss function is critical for debugging training.
  - Quick check question: How does DPO differ from supervised fine-tuning when no gold rewrites exist?

- Concept: **Plan DAG Representation**
  - Why needed here: Plans are evaluated structurally (node/edge counts, graph edit distance); understanding DAG construction is essential for debugging planner output.
  - Quick check question: If two plans have identical nodes but different edge structures, are they functionally equivalent?

## Architecture Onboarding

- Component map:
  - Raw dialogue → Rewriter → Intent → Planner → DAG → Evaluator → Preference signal → DPO update

- Critical path: Raw dialogue → Rewriter → Intent → Planner → DAG → Evaluator → Preference signal → DPO update

- Design tradeoffs:
  - Dummy vs Basic vs Advanced rewriter: More guidance in prompts improves quality but requires careful prompt engineering
  - DPO:human vs DPO:LLM: Human labels are costly but higher quality; LLM labels are scalable but weaker
  - Temperature=0 for planner: Ensures deterministic outputs but may miss creative solutions

- Failure signatures:
  - Rewriter treats agent suggestions as user requests (e.g., "search for pizza" appears in plan when user wanted Mexican)
  - Rewriter misses intent shifts and includes outdated goals
  - Evaluator disagrees with human judgment on ambiguous cases (moderate inter-annotator agreement: 64-75%)

- First 3 experiments:
  1. Run Dummy vs Advanced rewriter on 20 dialogues; measure structural differences (Δnodes, Δedges, GED) to validate planner sensitivity.
  2. Fine-tune a small preference model on RECAP-train; compare accuracy against zero-shot GPT-4o baseline.
  3. Train DPO rewriter with 50 human-labeled pairs; evaluate win/tie/loss against Advanced rewriter on held-out test set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can intent rewriting approaches effectively extend to multi-modal input settings (visual context, system state, user behavior) while maintaining planning utility gains?
- Basis in paper: [explicit] The authors state in Limitations: "our experiments are restricted to text-only input representations. However, real-world task-oriented systems often involve multi-modal signals such as visual context, system state, or user behavior. Extending rewriting and planning approaches to such multi-modal input settings remains an important direction for future work."
- Why unresolved: The current RECAP benchmark and evaluation methodology only address text-based dialogues, leaving the integration of visual or behavioral signals as an unexplored frontier.
- What evidence would resolve it: A multi-modal extension of RECAP with visual/state inputs, plus experiments showing that multi-modal rewriters improve plan preference over text-only baselines.

### Open Question 2
- Question: Does incorporating structural supervision signals from the plan DAG into rewriter training improve downstream plan quality beyond preference-only supervision?
- Basis in paper: [explicit] The authors note: "we do not explicitly optimize for plan structure. Future work could explore structural supervision during rewrite training, incorporating signals from the plan itself into the rewriting loop."
- Why unresolved: Current DPO-trained rewriters optimize for human/LLM preferences but ignore structural plan metrics (node/edge counts, graph edit distance), potentially missing opportunities for more targeted improvements.
- What evidence would resolve it: Comparing preference-trained vs. structure-augmented rewriters on both preference and structural metrics, with significant gains on structural consistency without preference degradation.

### Open Question 3
- Question: What is the causal relationship between specific rewrite characteristics (e.g., length, specificity, intent clarity) and plan output quality?
- Basis in paper: [explicit] The authors mention: "Furthermore, a deeper analysis into the characteristics of the rewrites and planner signals (from open LLMs) can be made to study the causality between the rewriter and plan output."
- Why unresolved: The paper establishes correlation (better rewrites lead to better plans) but does not isolate which rewrite features causally drive planning improvements.
- What evidence would resolve it: Controlled ablation studies manipulating individual rewrite characteristics while holding others constant, combined with mechanistic analysis of planner attention to different rewrite components.

## Limitations
- Limited validation on non-GPT planners; results may not generalize to other planners or domains
- Human evaluator agreement (64-75%) suggests moderate subjectivity in plan preference judgments
- No ablation of intent drift vs. ambiguity handling; unclear which challenge RECAP addresses most effectively
- DPO training relies on preference pairs rather than gold rewrites, creating potential for compounding errors

## Confidence
- **High**: Intent rewriting improves downstream plan quality (supported by 50-63% win rates vs baselines)
- **Medium**: DPO-trained rewriters yield additional gains (48.88% win rate, but limited human-labeled data)
- **Medium**: LLM evaluators predict human preferences at scale (65.01% accuracy, but may diverge on edge cases)

## Next Checks
1. Test Advanced rewriter with alternative planners (e.g., Claude, open-source models) to assess generalizability
2. Conduct ablation studies isolating ambiguity resolution vs. intent drift tracking in RECAP dataset
3. Evaluate LLM evaluator performance on long-tail, ambiguous conversations where human agreement is lowest