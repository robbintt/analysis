---
ver: rpa2
title: 'RadEval: A framework for radiology text evaluation'
arxiv_id: '2509.18030'
source_url: https://arxiv.org/abs/2509.18030
tags:
- pairs
- radiology
- report
- metrics
- blocks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RadEval is a unified, open-source framework for evaluating radiology
  text generation, consolidating metrics from lexical overlap (BLEU, ROUGE) and contextual
  measures (BERTScore) to clinical concept-based scores (F1CheXbert, F1RadGraph, RaTEScore)
  and LLM-based evaluators (GREEN). It standardizes metric implementations, refines
  existing methods, extends GREEN to support multiple imaging modalities with a lightweight
  model, and pretrains a domain-specific radiology encoder showing strong zero-shot
  retrieval performance.
---

# RadEval: A framework for radiology text evaluation

## Quick Facts
- arXiv ID: 2509.18030
- Source URL: https://arxiv.org/abs/2509.18030
- Reference count: 16
- Primary result: Unified evaluation framework consolidating 14 metrics for radiology report generation with GREEN as most reliable for clinically significant errors

## Executive Summary
RadEval introduces a comprehensive, open-source framework for evaluating radiology report generation systems by consolidating 14 diverse evaluation metrics into a unified platform. The framework integrates lexical overlap metrics (BLEU, ROUGE), contextual measures (BERTScore), clinical concept-based scores (F1CheXbert, F1RadGraph, RaTEScore), and LLM-based evaluators (GREEN). A key contribution is the RadEval-expert dataset with 450+ clinically significant error labels across 208 studies, enabling rigorous validation of metric performance against radiologist judgment.

The framework demonstrates that no single metric excels at all clinically relevant error types, with GREEN showing the strongest overall alignment with radiologist assessments. Domain-specific embeddings (RadEvalBERTScore) improve zero-shot retrieval performance, while refined implementations address inconsistencies in metric calculations. The extensible architecture supports future metric additions and provides statistical testing tools for reliable benchmarking across MIMIC-CXR, CheXpert-Plus, and ReXGradient-160K datasets.

## Method Summary
RadEval consolidates multiple evaluation approaches for radiology report generation including lexical overlap (BLEU, ROUGE), contextual (BERTScore), clinical concept-based (F1CheXbert, F1RadGraph, RaTEScore, SRR-BERT, TemporalEntityF1), and LLM-based metrics (GREEN). The framework uses RadEvalBERTScore, a domain-specific encoder trained with SimCSE contrastive learning on findings and impression sections from multiple datasets. GREEN is refined using Gemma-2B fine-tuned on the original GREEN dataset plus 50,000 multi-modality report pairs. The framework includes an expert-annotated dataset with over 450 clinically significant error labels for validating metric performance. Statistical testing tools and baseline model evaluations enable reproducible benchmarking.

## Key Results
- GREEN demonstrates the strongest overall correlation with radiologist assessments of clinically significant errors
- Domain-specific embeddings improve zero-shot retrieval performance across radiology report sections
- No single metric excels at all error categories; metric selection should align with specific evaluation goals
- Refined implementations address inconsistencies in existing metric calculations

## Why This Works (Mechanism)
The framework's effectiveness stems from consolidating multiple evaluation paradigms that capture different aspects of radiology report quality. Lexical metrics assess surface-level similarity, while contextual measures evaluate semantic meaning. Clinical concept-based scores ensure medical accuracy, and LLM-based evaluators provide holistic assessment aligned with clinical practice. The expert-annotated dataset enables empirical validation of metric performance against human judgment.

## Foundational Learning
- **Metric consolidation**: Why needed - Reduces evaluation complexity and enables standardized benchmarking; Quick check - Verify all 14 metrics are accessible through unified interface
- **Domain-specific embeddings**: Why needed - General-purpose embeddings underperform on specialized medical text; Quick check - Compare retrieval performance with and without RadEvalBERTScore
- **Expert annotation validation**: Why needed - Ensures metrics align with clinical relevance rather than statistical artifacts; Quick check - Review correlation between metric scores and human error labels
- **Statistical testing integration**: Why needed - Enables reliable comparison of metric performance; Quick check - Verify p-values and confidence intervals for metric comparisons
- **Multi-modality support**: Why needed - Radiology reports cover various imaging types; Quick check - Test framework on CT, MRI, and ultrasound reports
- **Extensible architecture**: Why needed - Accommodates future metric developments; Quick check - Implement and test a new metric following framework guidelines

## Architecture Onboarding

**Component map**: Data preprocessing -> Metric computation -> Statistical analysis -> Results visualization

**Critical path**: Input reports → Metric computation pipeline → Expert annotation comparison → Statistical validation → Performance reporting

**Design tradeoffs**: Comprehensive metric coverage vs. computational efficiency; Open-source accessibility vs. proprietary LLM dependencies; Expert annotation quality vs. dataset size

**Failure signatures**: Inconsistent metric scores due to BERTScore layer mismatches; Poor clinical alignment from inadequate domain adaptation; Statistical significance errors from insufficient permutation testing

**Three first experiments**:
1. Run baseline evaluation suite on MIMIC-CXR test split and verify metric scores against published benchmarks
2. Test statistical testing tools with synthetic datasets to confirm correct p-value calculation
3. Evaluate model predictions from different RRG systems to assess metric sensitivity to clinical variations

## Open Questions the Paper Calls Out

### Open Question 1
Can GPT-based evaluation metrics (CheXprompt, FineRadScore, RadFact) be successfully integrated into a unified, reproducible framework despite their reliance on proprietary APIs and lack of standardization?
Basis: The authors state in the Limitations section that "recently proposed LLM-based metrics (e.g., CheXprompt, FineRadScore, and RadFact) are not currently implemented due to their reliance on LLM APIs or lack of standardization – though they remain valuable future additions."
Why unresolved: These metrics depend on external proprietary APIs, creating reproducibility and accessibility barriers that conflict with RadEval's open-source philosophy.
What evidence would resolve it: Successful implementation showing API-based metrics can be standardized with consistent interfaces, cost controls, and reproducible outputs within the framework.

### Open Question 2
Do current radiology report evaluation metrics generalize effectively across non-English languages and healthcare systems outside the United States?
Basis: The Limitations section notes that "current evaluations also focus primarily on chest X-ray radiology reports written in English from institutions in the U.S., limiting generalizability across languages and geographical regions."
Why unresolved: All benchmarking was conducted on English-language reports from U.S. institutions (MIMIC-CXR, CheXpert-Plus, ReXGradient-160K), leaving cross-lingual and cross-regional validity untested.
What evidence would resolve it: Systematic evaluation on non-English radiology datasets showing comparable metric-radiologist alignment scores across different languages and healthcare documentation practices.

### Open Question 3
Can a single metric reliably capture all clinically significant error categories, or is multi-metric combination fundamentally necessary for comprehensive evaluation?
Basis: Table 2 shows different metrics excel at different error types—SRR-BERT and CheXbert best detect omissions (τb ≈ -0.50), while GREEN excels overall but varies across categories. No single metric achieves strong alignment across all error types.
Why unresolved: The correlation analysis reveals inconsistent performance patterns, with some metrics even showing misalignment (CheXbert positive correlation with errors) or non-significance for certain error categories.
What evidence would resolve it: Either identification of a single metric with consistently strong negative correlations across all error categories, or formal analysis proving multi-metric ensembles are theoretically necessary.

### Open Question 4
What is the minimum model size and computational budget required for LLM-based evaluators to maintain clinical alignment while remaining practical for large-scale deployment?
Basis: The authors finetuned Gemma-2B for efficiency (2-3 seconds per report) but Appendix B notes the need to explore "even lighter architectures without sacrificing clinical alignment or interpretability."
Why unresolved: The trade-off space between model size, inference speed, computational cost, and clinical evaluation quality remains incompletely characterized.
What evidence would resolve it: Systematic benchmarking of models from 0.5B to 7B+ parameters showing performance curves for clinical alignment scores versus deployment costs.

## Limitations
- Incomplete training details for RadEvalBERTScore encoder and refined GREEN finetuning limit exact reproduction
- Expert annotation dataset consists of only 208 studies, potentially limiting error category coverage
- Framework performance across diverse clinical settings and patient populations remains unvalidated
- GPT-based metrics cannot be integrated due to API dependencies and standardization challenges

## Confidence

**High confidence**: The core claim that GREEN is most reliable for clinically significant errors is well-supported by expert annotation study; domain-specific embeddings improving retrieval performance is strongly validated

**Medium confidence**: Claims about framework extensibility and unification are supported but rely on assumed future implementations; refined metrics' effectiveness requires more extensive clinical validation

**Low confidence**: Performance claims for lightweight refined GREEN model across multiple imaging modalities lack comparative baseline data

## Next Checks

1. Conduct independent reproduction of RadEvalBERTScore encoder training using specified SimCSE contrastive learning approach, documenting exact hyperparameters and comparing retrieval performance against original results

2. Perform cross-institutional validation using radiology reports from different healthcare systems to assess generalizability of metric performance and clinically significant error detection

3. Implement A/B testing where clinicians compare system outputs evaluated by different metric combinations to empirically validate which metric combinations best predict clinical utility in real-world deployment scenarios