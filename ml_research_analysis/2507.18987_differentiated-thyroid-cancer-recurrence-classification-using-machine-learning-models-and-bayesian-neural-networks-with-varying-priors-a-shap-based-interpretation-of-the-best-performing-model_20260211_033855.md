---
ver: rpa2
title: 'Differentiated Thyroid Cancer Recurrence Classification Using Machine Learning
  Models and Bayesian Neural Networks with Varying Priors: A SHAP-Based Interpretation
  of the Best Performing Model'
arxiv_id: '2507.18987'
source_url: https://arxiv.org/abs/2507.18987
tags:
- recurrence
- cancer
- feature
- thyroid
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed a machine learning framework to classify differentiated
  thyroid cancer recurrence using 383 patient records and 16 clinical/pathological
  features. Initially, 11 classical ML models were evaluated, with SVM achieving the
  highest accuracy (94.81%).
---

# Differentiated Thyroid Cancer Recurrence Classification Using Machine Learning Models and Bayesian Neural Networks with Varying Priors: A SHAP-Based Interpretation of the Best Performing Model

## Quick Facts
- arXiv ID: 2507.18987
- Source URL: https://arxiv.org/abs/2507.18987
- Reference count: 40
- Primary result: BNN with Normal(0,10) prior achieved 98.70% accuracy after feature selection for DTC recurrence prediction

## Executive Summary
This study develops a machine learning framework to classify differentiated thyroid cancer recurrence using 383 patient records and 16 clinical/pathological features. After evaluating 11 classical ML models, SVM initially achieved the highest accuracy (94.81%), but feature selection via Boruta improved performance, making Logistic Regression the best classical model (96.11%). The key innovation was testing Bayesian Neural Networks with six prior distributions, where the Normal(0,10) prior achieved superior results—97.40% accuracy before feature selection and 98.70% after—while also providing epistemic and aleatoric uncertainty estimates. SHAP analysis identified "response to initial treatment," "cancer stage," and "risk type" as the most influential features, offering clinical interpretability alongside high accuracy.

## Method Summary
The study employed a 383-patient dataset with 16 clinical and pathological features for binary DTC recurrence classification. The methodology involved: (1) preprocessing with standardization, label/ordinal encoding, and SMOTE for class imbalance; (2) classical ML evaluation (SVM, Logistic Regression, etc.) with grid search and cross-validation; (3) Boruta feature selection reducing features from 16 to 7 (response to treatment, cancer stage, node status, tumor stage, risk type, adenopathy, age); (4) BNN architecture with 5 hidden units and ReLU activation; (5) testing six prior distributions (Normal(0,1), Normal(0,10), Laplace(0,1), Cauchy(0,1), Cauchy(0,2.5), Horseshoe(1)); (6) posterior inference via MCMC with NUTS sampler in Pyro; and (7) SHAP analysis for interpretability. The best model (Normal(0,10) prior BNN) achieved 98.70% accuracy after feature selection.

## Key Results
- BNN with Normal(0,10) prior achieved 98.70% accuracy after Boruta feature selection (vs 97.40% before selection)
- Classical ML models improved after feature selection: Logistic Regression accuracy increased from 95.51% to 96.11%
- SHAP analysis identified "response to initial treatment" as the most influential feature, followed by "cancer stage" and "risk type"
- The BNN provided both epistemic and aleatoric uncertainty estimates alongside predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reducing feature redundancy via Boruta algorithm improves classification accuracy by removing noise variables.
- Mechanism: Boruta compares original features against randomly permuted "shadow" counterparts; features that consistently outperform shadows are retained. This removes variables with no genuine predictive signal, reducing overfitting risk.
- Core assumption: The discarded features contain no synergistic interactions with retained features that would improve predictions.
- Evidence anchors:
  - [abstract] "Feature selection via Boruta reduced redundancy, after which Logistic Regression became the best classical model (96.11%)."
  - [section] Table 4 vs Table 5 show accuracy improvements for most models post-selection.
  - [corpus] No direct corpus validation for Boruta specifically on thyroid data; mechanism is methodologically general.
- Break condition: If retained features are highly collinear or if removed features contain interaction effects, performance may degrade or plateau.

### Mechanism 2
- Claim: Bayesian Neural Networks with wider priors (Normal(0,10)) outperform both classical ML and narrow-prior BNNs by allowing more flexible weight distributions.
- Mechanism: BNNs learn probability distributions over weights rather than point estimates. A Normal(0,10) prior permits larger weight values during posterior inference, enabling the network to capture stronger feature-outcome relationships without being overly constrained.
- Core assumption: The wider prior does not induce overfitting on this small dataset (n=383); the posterior will regularize appropriately via the likelihood.
- Evidence anchors:
  - [abstract] "The BNN using a Normal(0,10) prior achieved the best performance—97.40% accuracy before feature selection and 98.70% after."
  - [section] Table 6 shows Normal(0,10) consistently outperforms Normal(0,1), Laplace, Cauchy, and Horseshoe priors.
  - [corpus] No corpus papers directly compare BNN priors for thyroid recurrence; this is a novel contribution per the authors.
- Break condition: On larger or noisier datasets, wider priors may yield overconfident or unstable posteriors; narrow or sparsity-inducing priors (e.g., Horseshoe) may become preferable.

### Mechanism 3
- Claim: SHAP values identify "response to initial treatment" as the dominant predictor, aligning model explanations with known clinical risk factors.
- Mechanism: SHAP computes Shapley values from game theory, assigning each feature an additive contribution to the prediction. Features with larger mean absolute SHAP values have greater influence on model output across the population.
- Core assumption: The BNN's learned representations are sufficiently local-linear for SHAP's linear explanation model to approximate behavior faithfully.
- Evidence anchors:
  - [abstract] "SHAP analysis highlighted 'response to initial treatment,' 'cancer stage,' and 'risk type' as the most influential features."
  - [section] Figure 12 shows mean aggregated SHAP values; Figure 13 beeswarm plot visualizes directionality and distribution.
  - [corpus] SHAP is widely used for medical ML interpretability, though no thyroid-specific corpus paper validates these exact feature rankings.
- Break condition: If feature interactions are strong and non-additive, SHAP may misattribute importance; interaction-aware SHAP or alternative explainers may be needed.

## Foundational Learning

- Concept: **Bayesian Inference and Posterior Estimation**
  - Why needed here: BNNs require understanding how priors combine with likelihoods to form posterior distributions over weights, and why MCMC (HMC/NUTS) is used for intractable integrals.
  - Quick check question: Given a prior N(0,10) and observed data, does a wider prior encourage larger or smaller posterior weight values compared to N(0,1)?

- Concept: **Epistemic vs. Aleatoric Uncertainty**
  - Why needed here: The paper decomposes uncertainty into model-parameter uncertainty (epistemic) and inherent data noise (aleatoric); understanding this distinction is critical for clinical confidence interpretation.
  - Quick check question: If a model has low aleatoric but high epistemic uncertainty for a patient, what does that imply about data quality vs. model confidence?

- Concept: **Feature Selection via Boruta**
  - Why needed here: Understanding how shadow features and statistical testing identify truly important variables helps diagnose why feature reduction improved accuracy.
  - Quick check question: Why does Boruta use randomly permuted shadow features rather than a simple correlation threshold?

## Architecture Onboarding

- Component map: Input dim=d (7 post-selection features) -> 1 hidden layer with 5 units + ReLU -> Output logit -> Bernoulli likelihood for binary classification
- Critical path:
  1. Data preprocessing (standardization, encoding, SMOTE for class balance)
  2. Boruta feature selection → reduce 16 features to 7
  3. Define BNN architecture with chosen prior
  4. Run NUTS sampler to approximate posterior
  5. Compute predictions with uncertainty estimates (epistemic + aleatoric)
  6. Apply SHAP for post-hoc interpretability

- Design tradeoffs:
  - Simple 5-unit hidden layer limits capacity but reduces overfitting on n=383; deeper architectures may not converge well without more data
  - Normal(0,10) prior achieves best accuracy but may yield overconfident predictions; sparsity-inducing priors (Horseshoe) were worse here but could help with high-dimensional data
  - SHAP adds interpretability but is computed post-hoc and may not capture all interactions

- Failure signatures:
  - Posterior samples show poor mixing or divergences in NUTS → prior may be too diffuse or model misspecified
  - Epistemic uncertainty remains high across all samples → insufficient data for posterior contraction
  - SHAP values contradict clinical expectations → potential data leakage or model overfitting

- First 3 experiments:
  1. Replicate BNN with Normal(0,10) on held-out fold; verify accuracy and uncertainty calibration to confirm results generalize beyond single train/test split.
  2. Ablate the top SHAP feature ("response to initial treatment") and measure accuracy drop to quantify its true predictive contribution.
  3. Test deeper BNN (e.g., 2 hidden layers with 10 units each) to assess whether added capacity improves performance or causes overfitting on this small dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the BNN model with Normal(0,10) prior maintain its high accuracy (98.70%) and uncertainty calibration when validated on external, multi-center datasets?
- Basis in paper: [explicit] The authors state the lack of external validation and single-center data limits generalizability to diverse populations.
- Why unresolved: The study relied on a retrospective cohort from a single clinical center (383 patients), lacking diverse population data to test robustness.
- What evidence would resolve it: Performance metrics (accuracy, uncertainty plots) derived from applying the trained model to independent datasets from different geographical or clinical settings.

### Open Question 2
- Question: Does the integration of domain-informed or hierarchical priors outperform the standard Normal(0,10) distribution in terms of predictive performance and uncertainty estimation?
- Basis in paper: [explicit] The authors note that using standard priors "may limit the exploration and integration of expert knowledge" and suggest domain-informed priors as an extension.
- Why unresolved: The current study was restricted to testing six standard prior distributions (e.g., Normal, Laplace, Cauchy) without incorporating specific clinical expert opinion into the prior structure.
- What evidence would resolve it: A comparative study evaluating the BNN with expert-elicited priors against the current baseline using the same dataset.

### Open Question 3
- Question: Does increasing the BNN architecture depth (adding hidden layers) significantly improve the capture of complex patterns in DTC recurrence compared to the simple architecture used?
- Basis in paper: [explicit] The authors mention the study was limited to a "basic BNN architecture" and suggest future work evaluate deeper architectures.
- Why unresolved: The implemented model contained only a single hidden layer with five units, which might be insufficient to model highly complex nonlinear relationships compared to deeper networks.
- What evidence would resolve it: Performance benchmarks of deeper BNN variants on the same feature-selected dataset compared to the current simple model's accuracy and epistemic uncertainty metrics.

### Open Question 4
- Question: How does the performance of the BNN compare to other Bayesian approaches, such as Bayesian Logistic Regression, regarding the trade-off between classification accuracy and computational cost?
- Basis in paper: [explicit] The authors suggest using "other Bayesian models, such as Bayesian logistic regression," to compare performance in future studies.
- Why unresolved: The current framework only compared the BNN against classical machine learning models (like standard Logistic Regression), not against other Bayesian classification methods.
- What evidence would resolve it: A comparative analysis showing accuracy, F1-scores, and training/sampling times for Bayesian Logistic Regression versus the BNN on the DTC dataset.

## Limitations

- Dataset provenance unknown—without exact source details, faithful reproduction is blocked
- Small sample size (n=383) raises overfitting concerns despite reported validation
- Lack of external validation limits generalizability to diverse populations
- Clinical validation of feature importance rankings via SHAP is absent from the corpus

## Confidence

- Classical ML accuracy claims (SVM 94.81%, Logistic Regression 96.11%): **Medium** (benchmarked via cross-validation, but single split not reported)
- BNN Normal(0,10) superiority (97.40-98.70%): **Medium** (best of six priors tested, but MCMC tuning unknown)
- SHAP feature importance ("response to treatment" top): **Low-Medium** (methodologically sound but no external clinical validation cited)

## Next Checks

1. Locate and verify exact dataset source; reproduce feature selection multiple times to confirm stable 7-feature subset.
2. Re-run BNN with Normal(0,10) prior using NUTS; check posterior convergence diagnostics (R-hat, trace plots) and accuracy on held-out fold.
3. Perform ablation study removing "response to initial treatment" to quantify its true predictive contribution beyond other features.