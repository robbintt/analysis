---
ver: rpa2
title: 'Time Course MechInterp: Analyzing the Evolution of Components and Knowledge
  in Large Language Models'
arxiv_id: '2506.03434'
source_url: https://arxiv.org/abs/2506.03434
tags:
- proper
- heads
- count
- ffns
- relation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work analyzes the evolution of factual knowledge circuits
  in OLMo-7B during pre-training using Information Flow Routes to trace attention
  heads and FFNs across 40 training snapshots. Components are classified into general,
  entity, relation-answer, and fact-answer roles, revealing that LLMs first rely on
  stable general-purpose components before gradually developing specialized ones.
---

# Time Course MechInterp: Analyzing the Evolution of Components and Knowledge in Large Language Models

## Quick Facts
- **arXiv ID**: 2506.03434
- **Source URL**: https://arxiv.org/abs/2506.03434
- **Reference count**: 40
- **Primary result**: LLMs first rely on stable general-purpose components before gradually developing specialized ones, with attention heads showing high turnover while FFNs remain stable throughout training.

## Executive Summary
This work analyzes the evolution of factual knowledge circuits in OLMo-7B during pre-training using Information Flow Routes to trace attention heads and FFNs across 40 training snapshots. Components are classified into general, entity, relation-answer, and fact-answer roles, revealing that LLMs first depend on broad, general-purpose components which later specialize as training progresses. The study finds that attention heads display the highest turnover, especially in early and late layers, while FFNs remain more stable throughout training. Location-based relations are learned faster and with less volatility than name-based relations, indicating that task complexity shapes acquisition dynamics.

## Method Summary
The methodology extracts Information Flow Routes (IFRs) across 40 OLMo-7B training snapshots to classify attention heads and FFNs into roles based on their contribution to factual recall tasks. IFRs recursively trace pathways from output tokens backward using ALTI-based importance scoring, retaining only nodes/edges exceeding threshold θ. Components are classified into general, entity, relation-answer, and fact-answer roles based on their activation patterns. The study analyzes 160 facts across 10 relations, computing IoU between snapshot and final model component sets, role switch counts, and Markov chain transition probabilities to understand knowledge acquisition dynamics.

## Key Results
- LLMs initially depend on broad, general-purpose components, which later specialize as training progresses.
- Attention heads display the highest turnover during training, while FFNs remain more stable throughout.
- Location-based relations converge to high accuracy earlier in training than name-based relations, highlighting how task complexity shapes acquisition dynamics.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Factual knowledge acquisition follows a hierarchical learning trajectory where models first establish stable general-purpose components before gradually developing specialized ones.
- Mechanism: Early training is dominated by general attention heads that produce broad, context-independent representations. As training progresses, specialized heads emerge for precise fact retrieval, with answer-specific heads showing the highest turnover.
- Core assumption: General components provide a necessary foundation that enables later specialization.
- Evidence anchors: [abstract] "LLMs initially depend on broad, general-purpose components, which later specialize as training progresses"; [Section 4.3] "Early in training, the model primarily relies on general-purpose heads that generate broad, context-independent representations. As training progresses, specialized heads emerge to support more precise fact retrieval."
- Break condition: If early-stage ablation of specialized heads does not impair general-component formation, the hierarchy claim weakens.

### Mechanism 2
- Claim: Attention heads and FFNs exhibit fundamentally different stability profiles during training—attention heads are highly dynamic with frequent role transitions, while FFNs remain largely stable.
- Mechanism: Attention heads (especially in early layers 0–10 and late layers 18–31) frequently switch between roles. FFNs, with only 32 total versus 1024 attention heads, maintain consistent roles with minimal oscillation.
- Core assumption: The granularity of analysis affects perceived stability—classifying entire FFNs as single units may inflate apparent generality.
- Evidence anchors: [abstract] "Attention heads display the highest turnover. We also present evidence that FFNs remain more stable throughout training"; [Section 4.4] "Although the majority of FFNs remain general, we observe occasional role oscillations... FFNs rarely stay specialized; in most cases, they revert back to the general role"
- Break condition: If neuron-level FFN analysis reveals comparable turnover to attention heads, the stability distinction is a methodological artifact.

### Mechanism 3
- Claim: Task complexity shapes knowledge acquisition dynamics—simpler location-based relations converge faster and with less volatility than complex name-based relations.
- Mechanism: Location-based relations (LOC) reach 0.8 top-1 accuracy at step S5 (~104B tokens) versus name-based relations (NAME) at step S14 (~293B tokens). NAME relations also exhibit more frequent attention-head role transitions in middle layers (10–18) compared to LOC relations.
- Core assumption: The primary driver of complexity is answer-space size—more person names exist in training data than locations.
- Evidence anchors: [abstract] "location-based relations converge to high accuracy earlier in training than name-based relations, highlighting how task complexity shapes acquisition dynamics"; [Section 3.3] "LOC relations converge faster than NAME relations: A top-1 accuracy of 0.8 is first reached at S5 for LOC and at S14 for NAME. LOC also has less top-1 volatility than NAME."
- Break condition: If controlling for corpus frequency eliminates convergence differences, the complexity claim is confounded by exposure.

## Foundational Learning

- Concept: **Circuit Analysis & Minimal Computational Subgraphs**
  - Why needed here: The entire methodology depends on isolating task-specific circuits—subgraphs of attention heads and FFNs that faithfully reproduce model behavior on factual recall.
  - Quick check question: Can you explain why activation patching differs from gradient-based attribution methods for circuit identification?

- Concept: **Information Flow Routes (IFRs)**
  - Why needed here: IFRs are the primary tool for tracing knowledge circuits across 40 snapshots. They recursively trace pathways from output tokens backward, retaining only nodes/edges exceeding threshold θ.
  - Quick check question: How does the ALTI-based importance scoring in IFRs address self-repair mechanisms that challenge traditional circuit-finding methods?

- Concept: **Jaccard Similarity (IoU) for Component Stability**
  - Why needed here: IoU quantifies consistency between component roles at each snapshot versus the final model, enabling measurement of specialization dynamics.
  - Quick check question: What does an IoU of 0.2 between consecutive snapshots indicate about answer-specific head stability?

## Architecture Onboarding

- Component map:
  - **Attention Heads (1024 total)**: Classified into 5 roles—general (broad contribution), entity (subject/answer focus), relation-answer (relation-specific), fact-answer (fact-specific), deactivated. Early (0–10) and late (18–31) layers show highest turnover; middle layers (10–18) are more stable.
  - **FFNs (32 total, one per layer)**: Higher activation threshold (θ=0.90 vs. 0.10 for heads). Predominantly general role with occasional relation-answer transitions. Middle layers (10–31) show near-zero role switching.
  - **IFR Extraction Pipeline**: For each fact × snapshot × token position, extract circuit, compute activation scores, classify roles, compute IoU and transition probabilities.

- Critical path:
  1. **Dataset preparation** → Curate factual prompts with unambiguous answers, validate first-token probability >75%, second-token probability <10%
  2. **IFR extraction** → Run across 40 OLMo-7B snapshots (S1-20B through S39-838B), threshold θ for edge retention
  3. **Role classification** → Compute per-component activation scores (general, entity, relation-answer, fact-answer), apply threshold, define "proper" roles by subtracting broader categories
  4. **Dynamics analysis** → Calculate IoU vs. final model, accumulate role switches, fit Markov transition matrices

- Design tradeoffs:
  - **Granularity vs. interpretability**: Analyzing FFNs as monolithic units (vs. neuron-level) improves scalability but may mask internal heterogeneity
  - **Threshold selection**: θ=0.10 for attention heads, θ=0.90 for FFNs based on contribution distributions; varying these within reasonable ranges maintains trend patterns but affects absolute counts
  - **Snapshot density**: 40 snapshots at 5000-step intervals misses fine-grained transitions; computational constraints prevent finer resolution

- Failure signatures:
  - **Low IoU for answer-specific heads late in training** → Expected; these are highly dynamic even near convergence
  - **NAME relations never reaching 0.8 top-1 accuracy** → Check corpus frequency; if entity counts are orders of magnitude higher than LOC, model may require longer training
  - **IFR extraction returns empty circuits** → Verify tokenization matches expected SUBJECT/END/ANSWER positions; adjust threshold θ

- First 3 experiments:
  1. **Replicate IoU trajectories for a single relation**: Extract IFRs for COUNTRY_CAPITAL_CITY across all 40 snapshots, compute per-role IoU vs. final model, verify general-head IoU remains high (0.6+) throughout while answer-specific IoU fluctuates.
  2. **Ablate answer-specific heads at S20**: Identify answer-specific heads for NAME relations at step S20, zero their outputs, measure accuracy drop. Expect localized impact on NAME accuracy with minimal LOC effect.
  3. **Test convergence-rate prediction**: For a new relation type (e.g., SPORTS_TEAM), estimate corpus frequency via InfiniGram, predict accuracy curve slope based on LOC/NAME baselines, validate against actual probing.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the apparent stability of FFNs during pre-training mask internal volatility at the neuron level, or do individual neurons exhibit the same consistency as their parent layers?
- Basis in paper: [explicit] The authors explicitly state in the Limitations and Conclusion that they "could not extend our analysis to the neuron level, potentially missing finer-grained switching behaviors."
- Why unresolved: The paper treats FFNs as single monolithic components due to resource constraints, leaving the behavior of the underlying neurons—where distinct facts may be stored—unknown.
- What evidence would resolve it: Applying neuron-level interpretability methods (e.g., sparse autoencoders) to the OLMo training snapshots to track if individual neurons stabilize or fluctuate independently of the FFN layer.

### Open Question 2
- Question: Do the evolutionary trajectories of component roles (e.g., dynamic specialization of attention heads vs. stable FFNs) replicate in model architectures that compute attention and FFNs in parallel (e.g., GPT-J/Pythia) rather than sequentially?
- Basis in paper: [explicit] The authors note in the Limitations that the Pythia suite uses a parallel architecture which "prevents the use of IFRs for circuit extraction," making direct comparison with OLMo's sequential architecture impossible in this study.
- Why unresolved: The findings are specific to OLMo-7B's sequential architecture; it is unclear if the observed stability of FFNs is intrinsic to the model or a result of the specific architectural implementation.
- What evidence would resolve it: Developing or adapting circuit analysis methods suitable for parallel architectures to perform a similar time-course analysis on Pythia or GPT-J checkpoints.

### Open Question 3
- Question: To what extent can the attention heads classified as "deactivated" or frequently switching be pruned or ablated during pre-training without disrupting the hierarchical learning process?
- Basis in paper: [explicit] The Conclusion suggests future work should "assess redundancy among head roles."
- Why unresolved: The paper identifies that nearly 60% of heads remain deactivated and that many specialized heads are repurposed, but it does not test if these components are necessary backup mechanisms or simply redundant noise.
- What evidence would resolve it: Ablation studies on intermediate training snapshots that selectively remove heads identified as "deactivated" or "high turnover" to observe the impact on convergence speed and final accuracy.

### Open Question 4
- Question: How does the differential stability of attention heads (high turnover) versus FFNs (stable) impact the efficiency of post-training adaptation methods like fine-tuning or knowledge editing?
- Basis in paper: [inferred] The Limitations section states that "the long-term impact [of these dynamics] on fine-tuning, pruning, and continual learning is unclear."
- Why unresolved: The paper characterizes the pre-training dynamics but does not link these specific component behaviors (e.g., FFN stability) to the model's plasticity or resistance to catastrophic forgetting during downstream tasks.
- What evidence would resolve it: Targeted fine-tuning experiments that monitor component overlap (IoU) to see if stable FFNs resist updates while dynamic attention heads adapt to new tasks.

## Limitations
- **Granularity constraints**: FFNs analyzed as monolithic units, potentially masking neuron-level volatility
- **Sampling frequency**: 40 snapshots at 5000-step intervals may miss rapid transitions during early training
- **Complexity confounds**: Task difficulty interpretation confounded by entity frequency differences between LOC and NAME relations

## Confidence
- **High confidence**: Attention head turnover patterns and IoU trajectories; FFN general-role stability; LOC vs NAME convergence rate differences
- **Medium confidence**: Hierarchical learning trajectory claim; task complexity interpretation; stability distinction between attention heads and FFNs
- **Low confidence**: Neuron-level FFN dynamics; precise role transition timing; corpus frequency vs. complexity confounds

## Next Checks
1. **Neuron-level FFN analysis**: Decompose each FFN into individual neurons and repeat the role classification. Compare IoU and turnover metrics against monolithic FFN results to determine if stability is a methodological artifact.
2. **Controlled complexity experiment**: Create synthetic relations where LOC and NAME types have equal corpus frequency but differ in answer-space size. Train on these relations and verify that convergence rate differences persist independent of exposure.
3. **Fine-grained snapshot analysis**: Extract IFRs at 1000-step intervals for the first 20,000 training steps. Identify precise timing of first specialized head emergence and correlate with accuracy milestones to validate the hierarchical learning trajectory claim.