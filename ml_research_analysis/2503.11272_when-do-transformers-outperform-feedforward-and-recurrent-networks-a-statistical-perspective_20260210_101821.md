---
ver: rpa2
title: When Do Transformers Outperform Feedforward and Recurrent Networks? A Statistical
  Perspective
arxiv_id: '2503.11272'
source_url: https://arxiv.org/abs/2503.11272
tags:
- where
- have
- proof
- lemma
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies the statistical efficiency of transformers\
  \ versus classical architectures (feedforward and recurrent networks) for learning\
  \ sequence-to-sequence models where each output depends on a sparse subset of input\
  \ tokens described in the prompt. The authors propose the q-Sparse Token Regression\
  \ (qSTR) data generating model, where the output at each position depends only on\
  \ q relevant tokens with q \u226A N, and the positions of these tokens are described\
  \ in the input prompt."
---

# When Do Transformers Outperform Feedforward and Recurrent Networks? A Statistical Perspective

## Quick Facts
- arXiv ID: 2503.11272
- Source URL: https://arxiv.org/abs/2503.11272
- Reference count: 40
- Primary result: Transformers achieve sample complexity independent of sequence length for sparse token regression, while feedforward networks require N samples and recurrent networks require N^Ω(1) samples

## Executive Summary
This paper provides a rigorous statistical analysis comparing transformers to classical architectures for sequence-to-sequence learning with sparse dependencies. The authors introduce the q-Sparse Token Regression (qSTR) model where each output depends only on q relevant tokens from an input sequence of length N, with q ≪ N. They prove that transformers can learn this model with sample complexity nearly independent of N when using sufficient attention heads, while feedforward and recurrent networks require sample complexity that scales with N. The key insight is that transformers' attention mechanism allows them to dynamically adapt to sparse dependency structures, whereas other architectures cannot efficiently capture such patterns.

## Method Summary
The authors analyze a novel q-Sparse Token Regression (qSTR) data generating model where each output position depends on only q relevant input tokens specified in a prompt. They theoretically prove sample complexity bounds for single-layer transformers, feedforward networks, and recurrent networks on this task. The analysis shows transformers require samples independent of sequence length when using H ≥ q attention heads, while feedforward networks need N samples and recurrent networks need N^Ω(1) samples. The proof leverages the attention mechanism's ability to select relevant tokens regardless of their position in the sequence. Experiments on synthetic data validate these theoretical predictions by comparing learning curves across architectures.

## Key Results
- Single-layer transformers with H ≥ q heads achieve sample complexity independent of sequence length N for sparse token regression
- Feedforward networks require N samples, growing linearly with sequence length
- Recurrent networks require N^Ω(1) samples, with polynomial dependence on N
- Theoretical bounds are validated experimentally on synthetic sparse token selection tasks

## Why This Works (Mechanism)
Transformers can achieve better sample complexity than other architectures on sparse token selection tasks because their attention mechanism allows them to dynamically select relevant tokens from any position in the input sequence. This means transformers can learn to identify and focus on the q relevant tokens for each output position without requiring explicit positional encoding or sequential processing. The attention heads can independently learn to attend to different relevant tokens, and with H ≥ q heads, the transformer can cover all dependencies. In contrast, feedforward networks must process all N tokens equally and cannot selectively attend, while recurrent networks process tokens sequentially and must maintain information about relevant tokens across many time steps, making them inefficient for sparse dependencies.

## Foundational Learning
1. Statistical learning theory - why needed: to establish sample complexity bounds; quick check: understand VC dimension and Rademacher complexity
2. Attention mechanisms - why needed: core mechanism enabling transformers' advantages; quick check: verify understanding of scaled dot-product attention
3. Sequence-to-sequence models - why needed: problem setting being analyzed; quick check: confirm understanding of input-output dependency structures
4. Feedforward neural networks - why needed: baseline architecture for comparison; quick check: verify understanding of fixed receptive fields
5. Recurrent neural networks - why needed: baseline architecture for comparison; quick check: confirm understanding of sequential processing
6. Sparse dependency structures - why needed: key property enabling theoretical analysis; quick check: understand q-sparsity and prompt-based specification

## Architecture Onboarding
Component map: Input sequence -> Prompt + Token embeddings -> Attention layers -> Output layer
Critical path: Prompt provides sparse dependency structure -> Attention mechanism selects relevant tokens -> Output layer computes predictions
Design tradeoffs: H ≥ q attention heads required vs. computational cost; single-layer simplicity vs. expressivity; idealized assumptions vs. practical applicability
Failure signatures: When H < q, transformer sample complexity degrades to match feedforward networks; when prompt information is noisy or incomplete, sample complexity increases
First experiments: 1) Vary q and measure sample complexity scaling; 2) Test with H < q heads to observe degradation; 3) Compare learned vs. given attention patterns

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical results assume idealized conditions including infinite precision arithmetic and exact alignment with sparse structure
- Practical implementations require learning attention patterns simultaneously with output mappings, potentially increasing sample complexity
- Experiments only validate on synthetic data with clean sparse structure, not on real-world naturally occurring sparse dependencies
- Analysis focuses on single-layer architectures, while practical applications typically use deeper networks

## Confidence
- Sample complexity independent of N for transformers: High confidence (theoretical), Medium confidence (practical)
- N samples for feedforward networks: High confidence (theoretical), Medium confidence (practical)
- N^Ω(1) samples for recurrent networks: High confidence (theoretical), Medium confidence (practical)
- Experimental validation on synthetic data: Medium confidence

## Next Checks
1. Test sample complexity bounds on transformers with learned rather than given attention patterns on the same qSTR data, measuring additional data requirements when discovering sparse structure
2. Evaluate performance on real-world datasets with naturally occurring sparse dependencies (graph-structured data or text with explicit coreference) to assess practical advantages
3. Compare computational efficiency by measuring training time and inference latency across architectures for varying sequence lengths and sparsity levels