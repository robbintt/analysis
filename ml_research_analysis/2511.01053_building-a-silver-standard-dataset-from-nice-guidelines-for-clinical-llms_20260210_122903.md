---
ver: rpa2
title: Building a Silver-Standard Dataset from NICE Guidelines for Clinical LLMs
arxiv_id: '2511.01053'
source_url: https://arxiv.org/abs/2511.01053
tags:
- clinical
- guideline
- nice
- patient
- statements
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents the first publicly available benchmark for
  evaluating LLMs against clinical guidelines using realistic patient scenarios and
  validated NICE-based recommendations. The dataset was created using GPT-4 to extract
  actionable clinical statements from ten NICE guidelines, generate corresponding
  patient scenarios and questions, and filter outputs based on clinical relevance.
---

# Building a Silver-Standard Dataset from NICE Guidelines for Clinical LLMs

## Quick Facts
- **arXiv ID**: 2511.01053
- **Source URL**: https://arxiv.org/abs/2511.01053
- **Reference count**: 40
- **Primary result**: First publicly available benchmark for evaluating LLMs against clinical guidelines using NICE-based recommendations

## Executive Summary
This study presents a novel silver-standard dataset for evaluating large language models (LLMs) on their ability to follow clinical guidelines and provide appropriate medical recommendations. The dataset was constructed by extracting actionable clinical statements from ten NICE guidelines and generating corresponding patient scenarios and questions using GPT-4. Four popular LLMs were benchmarked on this dataset, with Qwen3-235b-a22b-instruct-2507 achieving the highest scores across all evaluation metrics, while DeepSeek-R1 performed worst. Manual inspection revealed that even top-performing models often selected incorrect clinical information or produced verbose outputs with excess information, highlighting the complexity of clinical reasoning tasks.

## Method Summary
The dataset creation process involved three main steps: first, GPT-4 extracted actionable clinical statements from ten NICE guidelines; second, patient scenarios and questions were generated based on these statements; third, outputs were filtered based on clinical relevance and feasibility. The benchmark evaluation used five metrics including ROUGE variants, BLEU, METEOR, and BERTScore to assess model performance. Four LLMs (Qwen3-235b-a22b-instruct-2507, Llama3-1-8b-instruct, Gemma2-9b-it, and DeepSeek-R1) were evaluated on their ability to select appropriate clinical information and produce accurate responses to the generated questions.

## Key Results
- Qwen3-235b-a22b-instruct-2507 achieved the highest scores: ROUGE-1: 0.6222, ROUGE-2: 0.5598, BLEU: 0.3031, METEOR: 0.6834, BERTScore-F1: 0.9227
- DeepSeek-R1 performed worst: ROUGE-1: 0.4278, BERTScore-F1: 0.8837
- Manual inspection of 20 random questions showed top models either selected incorrect clinical information or produced verbose outputs with excess information
- The dataset provides a robust foundation for assessing guideline adherence and clinical reasoning performance of LLMs

## Why This Works (Mechanism)
The approach leverages GPT-4's ability to parse structured clinical guidelines and generate realistic patient scenarios that test guideline adherence. By using automated extraction followed by human verification, the methodology creates a scalable way to build clinical reasoning benchmarks. The use of multiple evaluation metrics captures different aspects of model performance, from lexical similarity to semantic understanding.

## Foundational Learning
- **Clinical guideline structure**: Understanding how medical guidelines are organized into recommendations, evidence levels, and actionable statements - needed to parse guidelines effectively
- **Clinical reasoning principles**: Knowledge of how clinicians make decisions based on patient presentation, history, and guidelines - needed to evaluate model outputs
- **Evaluation metric selection**: Understanding the strengths and limitations of ROUGE, BLEU, METEOR, and BERTScore for clinical text - needed to choose appropriate assessment tools
- **LLM instruction following**: How models interpret and respond to medical questions based on provided context - needed to assess model capabilities
- **Silver-standard dataset creation**: Methods for using AI to create datasets with human verification - needed for scalable benchmark development
- **Clinical scenario generation**: Techniques for creating realistic patient presentations that test specific clinical guidelines - needed for relevant testing

## Architecture Onboarding

**Component Map**: GPT-4 extraction -> Scenario generation -> Question generation -> Model evaluation -> Metric calculation

**Critical Path**: NICE guideline input → GPT-4 extraction → Scenario/question generation → Model inference → Evaluation metrics → Results analysis

**Design Tradeoffs**: Automated extraction vs. manual curation balance - automation enables scalability but may miss nuanced clinical information; multiple evaluation metrics provide comprehensive assessment but may not capture clinical appropriateness; using GPT-4 for both dataset creation and evaluation may introduce systematic biases

**Failure Signatures**: Models selecting incorrect clinical information despite high metric scores; verbose outputs containing excess information; inability to handle temporal reasoning or comorbidities; poor performance on specific guideline types or medical specialties

**First 3 Experiments**: 1) Test additional guideline types beyond current specialties; 2) Compare automated vs. fully manual dataset creation; 3) Evaluate model performance on temporal reasoning and comorbidity scenarios

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset covers only ten NICE guidelines from specific medical domains, limiting generalizability
- Reliance on GPT-4 for both dataset creation and question generation may introduce systematic biases
- Manual inspection was limited to 20 random questions, potentially missing performance variations
- Evaluation metrics measure lexical and semantic similarity rather than clinical accuracy or appropriateness

## Confidence

**High confidence**: The dataset creation methodology is reproducible and the evaluation pipeline is technically sound.

**Medium confidence**: The benchmark results for the four tested models are reliable for the specific scenarios and guidelines covered.

**Low confidence**: The generalizability of findings to broader clinical contexts and other guideline sources.

## Next Checks
1. Test the benchmark with additional clinical guidelines from different medical specialties and international sources to assess generalizability.
2. Conduct expert clinician review of model outputs across a broader sample of questions to validate clinical appropriateness beyond automated metrics.
3. Evaluate model performance on scenarios requiring temporal reasoning and handling of multiple comorbidities not explicitly covered in the current dataset.