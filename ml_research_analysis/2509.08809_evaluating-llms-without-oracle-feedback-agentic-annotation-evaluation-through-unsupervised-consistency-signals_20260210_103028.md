---
ver: rpa2
title: 'Evaluating LLMs Without Oracle Feedback: Agentic Annotation Evaluation Through
  Unsupervised Consistency Signals'
arxiv_id: '2509.08809'
source_url: https://arxiv.org/abs/2509.08809
tags:
- annotation
- ratio
- student
- arxiv
- google
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper proposes an agentic annotation evaluation paradigm to\
  \ assess the quality of LLM-generated annotations in unsupervised environments without\
  \ oracle feedback. It introduces a student model that uses user-preference-based\
  \ majority voting to create annotations, which are then compared against a noisy\
  \ teacher LLM\u2019s outputs."
---

# Evaluating LLMs Without Oracle Feedback: Agentic Annotation Evaluation Through Unsupervised Consistency Signals

## Quick Facts
- arXiv ID: 2509.08809
- Source URL: https://arxiv.org/abs/2509.08809
- Reference count: 12
- Key outcome: Agentic annotation evaluation using unsupervised consistency signals achieves up to 0.93 Pearson correlation with annotation accuracy and correctly identifies best LLM 60% of the time without oracle feedback.

## Executive Summary
This paper introduces an agentic annotation evaluation paradigm for assessing LLM-generated annotations without requiring oracle feedback. The method employs a student model that uses user-preference-based majority voting to generate annotations, which are then compared against a noisy teacher LLM's outputs. A novel unsupervised metric called Consistent and Inconsistent (CAI) Ratio measures agreement between student and teacher models to evaluate annotation quality. The approach was validated across ten NLP datasets and four different LLMs, demonstrating strong correlation between CAI Ratio and actual annotation accuracy.

## Method Summary
The proposed method creates a self-contained evaluation framework where a student LLM generates annotations using majority voting based on user preferences, while a teacher LLM provides noisy reference annotations. The CAI Ratio metric captures the consistency between these two annotation sources to assess quality without ground truth labels. This agentic approach enables LLM evaluation in real-world scenarios where oracle feedback is unavailable, making it particularly valuable for practical deployment scenarios.

## Key Results
- CAI Ratio shows strong positive correlation (up to 0.93 Pearson correlation) with LLM annotation accuracy across tested datasets
- CAI Ratio effectively serves as a model selection criterion, correctly identifying the best-performing LLM in 60% of cases
- Method demonstrates robustness across ten NLP datasets and four different LLMs

## Why This Works (Mechanism)
The approach leverages the principle that high-quality LLM outputs should exhibit consistency when processed through different but complementary mechanisms. By having a student model generate annotations through preference-based voting and comparing these against a teacher model's outputs, the method captures intrinsic quality signals that correlate with true accuracy even without ground truth labels.

## Foundational Learning
- Consistency signals as quality proxies: Agreement between independently generated annotations serves as an unsupervised indicator of quality
  - Why needed: Enables evaluation without expensive human annotation or oracle feedback
  - Quick check: Measure correlation between consistency scores and actual accuracy across multiple models

- User-preference-based majority voting: Aggregating multiple annotation attempts weighted by user preferences improves robustness
  - Why needed: Reduces individual generation errors and captures preferred output characteristics
  - Quick check: Compare single-generation accuracy versus majority voting across different preference schemes

- Teacher-student evaluation framework: Using one model as reference for evaluating another creates self-supervised learning signal
  - Why needed: Provides ground for comparison without requiring external labels
  - Quick check: Test correlation between CAI Ratio and accuracy across different teacher-student pairs

## Architecture Onboarding

Component map: User preferences -> Student model (majority voting) -> Annotations -> CAI Ratio comparison with Teacher model outputs

Critical path: The evaluation pipeline flows from user preference input through the student model's majority voting mechanism to generate annotations, which are then compared against teacher model outputs to compute the CAI Ratio metric.

Design tradeoffs: The method trades computational cost of multiple generations and comparisons against the benefit of oracle-free evaluation. The teacher model introduces potential noise that could affect reliability but enables the self-supervised evaluation framework.

Failure signatures: Poor teacher model quality directly impacts CAI Ratio reliability. Inconsistent user preferences can lead to unreliable majority voting. Domain mismatch between student and teacher models may reduce correlation with actual accuracy.

First experiments:
1. Measure CAI Ratio correlation with accuracy across different teacher-student model pairs
2. Evaluate impact of user preference consistency on CAI Ratio reliability
3. Test CAI Ratio effectiveness across varying dataset sizes and annotation complexities

## Open Questions the Paper Calls Out
None

## Limitations
- Method performance heavily depends on teacher model quality, with noisy or biased outputs potentially compromising evaluation reliability
- Student model's reliance on user-preference-based majority voting may not generalize well across all annotation tasks or domains
- Current evaluation focuses on short-text classification, limiting applicability to more complex NLP tasks like long-form generation or multi-step reasoning

## Confidence
- High confidence in core correlation findings (Pearson up to 0.93) across tested datasets
- Medium confidence in model selection capability (60% success rate) given limited testing conditions
- Low confidence in generalization to other NLP tasks beyond short-text classification

## Next Checks
1. Test CAI Ratio effectiveness on long-form text generation and multi-step reasoning tasks
2. Evaluate performance across diverse domain distributions and annotation complexity levels
3. Compare CAI Ratio against traditional supervised evaluation metrics in oracle-available settings to establish baseline performance differences