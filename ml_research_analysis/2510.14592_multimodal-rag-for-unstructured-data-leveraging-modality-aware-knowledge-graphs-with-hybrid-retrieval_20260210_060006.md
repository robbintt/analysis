---
ver: rpa2
title: Multimodal RAG for Unstructured Data:Leveraging Modality-Aware Knowledge Graphs
  with Hybrid Retrieval
arxiv_id: '2510.14592'
source_url: https://arxiv.org/abs/2510.14592
tags:
- retrieval
- multimodal
- data
- knowledge
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of effectively retrieving and
  reasoning over unstructured multimodal documents, which combine text, images, tables,
  equations, and graphs, using Retrieval-Augmented Generation (RAG) systems. Current
  RAG systems are primarily unimodal and struggle with cross-modal dependencies.
---

# Multimodal RAG for Unstructured Data:Leveraging Modality-Aware Knowledge Graphs with Hybrid Retrieval

## Quick Facts
- arXiv ID: 2510.14592
- Source URL: https://arxiv.org/abs/2510.14592
- Reference count: 19
- Key outcome: MAHA achieves 72% ROUGE-L improvement over vector-only baselines and complete modality coverage (1.00) on multimodal retrieval benchmarks

## Executive Summary
This paper addresses the challenge of effectively retrieving and reasoning over unstructured multimodal documents, which combine text, images, tables, equations, and graphs, using Retrieval-Augmented Generation (RAG) systems. Current RAG systems are primarily unimodal and struggle with cross-modal dependencies. To overcome this, the authors propose MAHA, a Modality-Aware Hybrid retrieval Architecture. MAHA integrates dense vector retrieval with structured graph traversal over a modality-aware knowledge graph that captures cross-modal semantics and relationships. Evaluations on benchmark datasets demonstrate that MAHA substantially outperforms baseline methods, achieving a ROUGE-L score of 0.486 and complete modality coverage (1.00).

## Method Summary
The proposed MAHA architecture integrates dense vector retrieval with structured graph traversal over a modality-aware knowledge graph to handle multimodal documents. The system parses documents into text, tables, images, graphs, and equations, generates embeddings using text-embedding-3-small for text and CLIP for visuals, and stores them in a FAISS vectorstore. Simultaneously, it builds a knowledge graph with modality-aware relationships (HAS-*, NEXT-*) connecting content chunks. During retrieval, user queries are encoded and passed through both vector similarity search and KG traversal, with results fused via an unspecified hybrid strategy before LLM generation.

## Key Results
- MAHA achieves ROUGE-L score of 0.486, a 72% relative improvement over vector-only baseline (0.282)
- Complete modality coverage (1.00) demonstrates effective cross-modal retrieval
- Substantial improvements over graph-only variant (ROUGE-L 0.338) and other multimodal RAG approaches

## Why This Works (Mechanism)

### Mechanism 1: Complementary Retrieval Signals
Dense vector retrieval and structured graph traversal provide complementary information that, when combined, substantially outperforms either approach alone. Vector retrieval captures local semantic similarity but misses structural and cross-modal cues. Graph traversal provides structural navigation and relationship-based reasoning but struggles to surface rich evidence independently. The fusion integrates both signals without sacrificing relevance or coverage. Break condition: If queries are purely keyword-driven or require only single-modality evidence, graph traversal overhead provides diminishing returns.

### Mechanism 2: Modality-Aware Schema Extension
Explicitly encoding cross-modal relationships in the knowledge graph schema enables retrieval of multimodal evidence that modality-agnostic approaches miss. Traditional KG schemas are text-centric. Extending with modality-aware edge types (HAS-IMAGE, HAS-TABLE, HAS-FORMULA, NEXT-TEXT, NEXT-TABLE, NEXT-IMAGE, NEXT-FORMULA) allows traversal from any content unit to its related modalities. Break condition: If document modalities are decoratively unrelated (e.g., generic stock images), schema extension adds overhead without retrieval benefit.

### Mechanism 3: Dual-Index Coordination via Shared Chunk Identifiers
Maintaining both a vectorstore and knowledge graph with shared indexing enables hybrid retrieval without sacrificing either semantic precision or structural reasoning. Content chunks are indexed in the vectorstore for similarity search while simultaneously represented as nodes in the KG with relationship edges. The indexes serve as the common link—retrieval from either source can reference the same underlying content. Break condition: If vector and graph retrievers return conflicting or redundant results without proper deduplication or ranking fusion, answer quality degrades.

## Foundational Learning

- **Knowledge Graph Construction and Traversal**
  - Why needed here: The modality-aware KG is the core differentiator. Understanding node-edge schemas, relationship types, and traversal algorithms (e.g., multi-hop reasoning) is essential for extending text-centric KGs to multimodal content.
  - Quick check question: Given a document with a paragraph describing a table's findings, what relationship edges would connect these nodes, and how would a 2-hop traversal retrieve both?

- **Dense Embedding Models and Vector Similarity**
  - Why needed here: MAHA uses SBERT for text embeddings and CLIP for visual elements. Understanding embedding spaces, similarity metrics (cosine, dot product), and ANN indexing (FAISS) is required to implement and debug the vector retrieval component.
  - Quick check question: Why would a text-only embedding model fail to retrieve a relevant image even if the image contains the answer?

- **Hybrid Retrieval Fusion Strategies**
  - Why needed here: The paper explicitly notes the challenge of balancing vector and graph retrieval. Understanding reciprocal rank fusion (RRF), score normalization, and weighted combination strategies is critical for reproducing results.
  - Quick check question: If vector retrieval returns chunks with scores [0.92, 0.87, 0.65] and graph traversal returns different chunks with hop distances [1, 2, 3], how would you combine these into a single ranked list?

## Architecture Onboarding

- **Component map:** Assistant module (orchestrator) -> Process module (parser/segmentation) -> Vectorstore (FAISS) + Knowledge Graph (modality-aware) -> Query module (dual retrieval) -> LLM (synthesis)

- **Critical path:** Document ingestion → Process module parsing/segmentation → Chunk embedding (text + CLIP for visuals) + format conversion (HTML, LaTeX, base64) → Dual indexing: Vectorstore insertion + KG node/edge creation → Query encoding → Parallel retrieval (vector similarity + graph traversal) → Result fusion → LLM generation with retrieved context

- **Design tradeoffs:**
  - Schema granularity: More relationship types improve precision but increase KG construction complexity
  - Fusion strategy weighting: Over-prioritizing vector retrieval may miss structurally related evidence; over-prioritizing graph traversal may surface tangentially related content
  - Chunk size vs. modality alignment: Smaller chunks improve retrieval precision but may fragment cross-modal context

- **Failure signatures:**
  - Low modality coverage (<0.5): KG schema missing relevant edge types; graph traversal depth insufficient; parsing failing to extract non-text modalities
  - High recall but low MRR: Fusion strategy not properly ranking relevant results; vector and graph scores on different scales without normalization
  - Hallucinated cross-modal references: KG edges incorrectly linking unrelated content; LLM not grounded in retrieved evidence
  - Missing equation/table content: Parsing pipeline not extracting structured content; embeddings failing to capture semantic meaning

- **First 3 experiments:**
  1. Baseline replication: Implement vector-only and graph-only retrievers on UDA Benchmark; verify ROUGE-L ~0.28 and ~0.34 respectively
  2. Schema ablation: Test MAHA with only HAS-* edges vs. only NEXT-* edges vs. full schema; measure impact on modality coverage and ROUGE-L
  3. Fusion weight sweep: Systematically vary the balance between vector and graph retrieval scores; identify optimal weighting for different query types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a dynamic query router improve MAHA's efficiency by adapting retrieval strategies to question complexity in real-time?
- Basis in paper: The Conclusion states that "future research could focus on developing a more dynamic query router that can intelligently adapt to the complexity of a user’s question in real-time."
- Why unresolved: The current MAHA architecture uses a fixed hybrid fusion of vector and graph retrieval; it does not differentiate between simple lookup queries and complex multi-hop reasoning queries, potentially over-processing simple requests.
- What evidence would resolve it: A comparative analysis of latency and accuracy between the current static fusion mechanism and an adaptive router on a dataset with varying query complexity levels.

### Open Question 2
- Question: How can knowledge graph construction be fully automated for highly unstructured data without manual schema definition?
- Basis in paper: The Conclusion identifies "significant potential in exploring more advanced, automated methods for knowledge graph construction to handle highly unstructured data formats."
- Why unresolved: The paper describes the current construction as "schema-driven," which implies potential manual effort or rigidity in defining modality-aware relationships (e.g., "HAS-IMAGE") for new document types.
- What evidence would resolve it: Demonstration of a schema-agnostic pipeline that generates modality-aware graphs from raw unstructured data with comparable relationship extraction accuracy to the current schema-driven method.

### Open Question 3
- Question: Does converting non-textual data (tables, equations) into textual summaries or HTML result in information loss for retrieval?
- Basis in paper: The "Ingestion and Embedding" section notes that tables are converted to HTML, equations to LaTeX, and "Non-textual data is also summarized and embedded." This assumes textual summarization preserves all semantic cues needed for retrieval.
- Why unresolved: Summarizing complex visual elements like charts or equations into text may strip away spatial relationships or precise numerical values that a visual encoder might otherwise capture.
- What evidence would resolve it: An ablation study comparing the performance of text-summarized embeddings against direct multimodal embeddings (e.g., from a vision-language model) on tasks requiring precise structural or numerical reasoning.

## Limitations
- The fusion strategy between vector and graph retrieval is not fully specified, making exact replication challenging
- Knowledge graph construction relies on unspecified NER and entity linking tools, potentially limiting reproducibility
- Evaluation focuses on retrieval quality and generation metrics without analyzing computational overhead or scalability

## Confidence
- High confidence in the complementary retrieval signals mechanism, supported by strong quantitative improvements (72% ROUGE-L gain over vector-only baseline)
- Medium confidence in modality-aware schema extension benefits, as the approach aligns with but extends prior KG-RAG work
- Medium confidence in dual-index coordination claims, as implementation details remain underspecified in the paper

## Next Checks
1. Implement ablation studies removing either vector or graph retrieval components to verify the claimed 72% ROUGE-L improvement
2. Test the knowledge graph schema on documents with varying modality compositions to validate robustness across different multimodal distributions
3. Evaluate the fusion strategy's sensitivity to different weightings and normalization approaches to establish optimal configuration guidelines