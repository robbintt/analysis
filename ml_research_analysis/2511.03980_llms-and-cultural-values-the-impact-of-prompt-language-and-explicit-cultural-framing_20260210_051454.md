---
ver: rpa2
title: 'LLMs and Cultural Values: the Impact of Prompt Language and Explicit Cultural
  Framing'
arxiv_id: '2511.03980'
source_url: https://arxiv.org/abs/2511.03980
tags:
- cultural
- llms
- prompt
- values
- countries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines how prompt language and explicit cultural framing
  influence the cultural values exhibited by large language models (LLMs). Using questions
  from the Hofstede Values Survey Module and World Values Survey, the research tests
  10 LLMs across 11 languages and four prompting strategies.
---

# LLMs and Cultural Values: the Impact of Prompt Language and Explicit Cultural Framing

## Quick Facts
- arXiv ID: 2511.03980
- Source URL: https://arxiv.org/abs/2511.03980
- Reference count: 17
- Models exhibit persistent bias toward Netherlands, Germany, US, and Japan values regardless of prompting strategy

## Executive Summary
This study investigates how prompt language and explicit cultural framing influence the cultural values exhibited by large language models (LLMs). Using questions from the Hofstede Values Survey Module and World Values Survey, the research tests 10 LLMs across 11 languages and four prompting strategies. While both prompt language and cultural framing influence model responses, the most effective approach for cultural alignment is explicitly requesting a specific cultural perspective in English. Despite this, all models show a persistent bias toward the values of Germany, Japan, the Netherlands, and the United States. LLM responses tend to be neutral on most topics, with progressive stances on social tolerance. The findings highlight the limitations of current multilingual models in representing cultural diversity and suggest that explicit cultural prompting may be a partial solution.

## Method Summary
The study tests 10 LLMs (BLOOMZ, ChatGPT 3.5/4, Claude 3, ERNIE 3.5, GEMMA, GPT-4o, LLAMA, MISTRAL, QWEN, XIAOHONGSHU) across 11 languages using questions from Hofstede VSM and WVS surveys. Four prompting strategies are employed: no culture specification (ll_none), general cultural perspective (ll_general), cultural perspective in native language (ll_cultural), and cultural perspective in English (en_cultural). Responses are compared against human survey data using Pearson correlation coefficients across countries and cultural dimensions.

## Key Results
- Explicit English cultural prompts (en_cultural) achieve highest alignment with target culture values (r=.53 for VSM, .44 for WVS)
- Language-specific prompting (ll_general) shows minimal improvement over no cultural framing (average r improvement: +.08)
- All models exhibit systematic bias toward values associated with Netherlands, Germany, US, and Japan regardless of prompting strategy
- LLM responses are mostly neutral on cultural dimensions but show progressive stances on social tolerance questions

## Why This Works (Mechanism)

### Mechanism 1: Explicit Cultural Perspective Shifts Responses Toward Target Culture Values
- **Claim:** When prompts explicitly request responses "from the perspective of a [country adjective] person," LLM responses shift measurably toward the values of that culture, improving correlation with human survey data in those countries.
- **Mechanism:** The explicit instruction creates a semantic activation pattern that biases token selection toward representations associated with that cultural group in the training data. The model accesses stored knowledge about "how people from [country] typically respond" rather than defaulting to its dominant training distribution.
- **Core assumption:** Models encode sufficient cultural knowledge in English to differentiate cultural perspectives; this knowledge is more accessible through explicit semantic cues than implicit language cues.
- **Evidence anchors:**
  - [Abstract]: "Alignment with cultural values of human respondents is improved more with an explicit cultural perspective than with a targeted prompt language."
  - [Section 4.4.3]: "On average, the highest correlations are found when the prompt is written in English and explicitly requests the target culture's perspective (en_cultural: r=.53 for VSM, .44 for WVS)."
  - [Corpus]: Corpus lacks direct mechanistic studies; no neighboring papers isolate this causal pathway experimentally.
- **Break condition:** The mechanism fails when the target culture's values differ substantially from the model's dominant training distribution (e.g., Egypt, Iran, Turkey). Correlations remain weak (r ≈ .20-.48) even with explicit framing.

### Mechanism 2: Prompt Language Creates Orthogonal Variation Without Meaningful Alignment
- **Claim:** Changing prompt language (e.g., Arabic vs. English) creates variation in responses, but this variation is largely uncorrelated with the actual cultural values of speakers of those languages.
- **Mechanism:** Language-specific tokenization and representation spaces contain different probability distributions, but these distributions reflect "how this language is typically used" rather than "what values speakers of this language hold." Cultural knowledge about specific countries appears more robustly encoded in English representations.
- **Core assumption:** The variation stems from language-specific statistical patterns in training data rather than genuine cultural understanding.
- **Evidence anchors:**
  - [Abstract]: "While targeted prompting can, to a certain extent, steer LLM responses in the direction of the predominant values of the corresponding countries, it does not overcome the models' systematic bias toward the values associated with a restricted set of countries."
  - [Section 5.1]: "Cultural differences and values may be represented within the English language rather than their native languages... prompting in different languages is not enough to access these values."
  - [Corpus]: Neighboring paper "I Am Aligned, But With Whom?" confirms multilingual prompting inadequately captures regional values (FMR: 0.498).
- **Break condition:** Language-only prompting (ll_general variant) shows near-zero improvement in cultural alignment (average r improvement: +.08 vs. +.16 for explicit cultural framing).

### Mechanism 3: Training Data Distribution Dominates Over Prompt-Based Interventions
- **Claim:** All tested models exhibit persistent bias toward a specific set of countries (Netherlands, Germany, US, Japan) regardless of prompting strategy, suggesting training data composition creates an attractor state that weak interventions cannot escape.
- **Mechanism:** Models optimize for likelihood over the training distribution, where certain cultural perspectives are statistically overrepresented. Optimization toward "most probable outputs" treats underrepresented cultural views as noise to be minimized rather than signal to be preserved.
- **Core assumption:** The persistent bias toward these four specific countries reflects their proportional dominance in training corpora rather than model architecture or fine-tuning choices.
- **Evidence anchors:**
  - [Abstract]: "All tested models, regardless of their origin, exhibit remarkably similar patterns... systematic bias toward the values associated with a restricted set of countries... the Netherlands, Germany, the US, and Japan."
  - [Section 5.3]: "This pattern of alignment is consistent with most previous studies... tendency to favour the values of Western, secular and more prosperous societies stems from what Wang, Morgenstern, and Dickerson (2025) term 'flattening.'"
  - [Corpus]: Neighboring paper "Should LLMs be WEIRD?" directly confirms WEIRD (Western, Educated, Industrialized, Rich, Democratic) training data bias (FMR: 0.580).
- **Break condition:** Even when explicitly prompted for non-dominant cultures (Egypt, Iran, India), models fail to achieve alignment comparable to dominant cultures. The attractor state strength exceeds prompt-based perturbation magnitude.

## Foundational Learning

- **Concept: Hofstede Cultural Dimensions**
  - **Why needed here:** The study uses Hofstede's six dimensions (Power Distance, Individualism, Achievement, Uncertainty Avoidance, Long-Term Orientation, Indulgence) as structured measurement frameworks. Without understanding what these dimensions capture, you cannot interpret the correlation results.
  - **Quick check question:** Can you explain why Individualism Index (IDV) shows the highest correlation across all prompt variants (r=.36-.78) while Motivation Toward Achievement (MAS) shows the weakest (r=-.07-.37)?

- **Concept: Pearson Correlation for Cross-Cultural Alignment**
  - **Why needed here:** The paper uses r values to quantify how well LLM responses match human survey responses across countries. Understanding that r=.40 indicates moderate positive correlation while r=.80 indicates strong correlation is essential for interpreting practical significance.
  - **Quick check question:** If an en_cultural prompt achieves r=.44 correlation with human WVS responses, does this represent strong alignment? What would r=.95 indicate?

- **Concept: Secular-Rational vs. Traditional Values (Inglehart-Welzel)**
  - **Why needed here:** The paper frames its findings using Inglehart and Welzel's cultural map, explaining that high-alignment countries (DE, NL, JP, US) cluster toward secular-rational and self-expression poles. This theoretical framework explains why these specific countries align.
  - **Quick check question:** Why might models trained on internet-scale data exhibit secular-rational bias more strongly than traditional-religious bias?

## Architecture Onboarding

- **Component map:**
  - Input layer: Four prompt variants (ll_none, ll_general, ll_cultural, en_cultural) × 11 languages × 63 survey questions
  - Evaluation layer: Three correlation analyses (cross-country per question, cross-question per country, inter-country human correlation)
  - Output layer: Valid response rate + cultural alignment metrics + value profile characterization

- **Critical path:**
  1. Prompt formulation with controlled variables (language, perspective framing)
  2. Response extraction with format enforcement ("my answer is X")
  3. Normalization across heterogeneous response scales (1-4, 1-5, 1-10, binary)
  4. Correlation computation against human benchmark data
  5. Dimension-level aggregation for Hofstede VSM scores

- **Design tradeoffs:**
  - **Text-based output vs. logit probabilities:** Text extraction handles multilingual tokenization inconsistencies and unconventional model outputs, but introduces parsing complexity. Logits would be cleaner but unreliable across languages.
  - **Country-language pairing:** Simplifies analysis but obscures multilingual countries and diaspora populations. The paper acknowledges this as a "gross simplification."
  - **Temperature variation:** Testing at temp=0 and temp=1 provides robustness but increases experiment count by 6×.

- **Failure signatures:**
  - **Low valid reply rate (<80%):** Indicates model struggling with prompt interpretation or safety refusal. Most common in GPT-3.5, GPT-4, and for certain languages (Russian, Farsi).
  - **Near-zero cross-country variation (CoV <0.05):** Indicates model not differentiating between cultural perspectives despite explicit framing. Occurs primarily in WVS questions about universally-valued topics (family, friends, environment).
  - **Negative correlation with target culture:** Indicates model responding opposite to human patterns. Observed for MISTRAL and QWEN on Individualism Index with ll_general prompts.

- **First 3 experiments:**
  1. **Baseline alignment test:** Run en_cultural prompts for all 11 countries on a single WVS question with high human variation (WVS006: importance of religion). Calculate correlation. If r > .70, the model has accessible cultural knowledge for this question.
  2. **Language sensitivity test:** Run the same question with ll_general prompts across all 11 languages. Calculate cross-country variation (CoV). If CoV < 0.10, the model is language-insensitive for this question.
  3. **Dominant culture attractor test:** Prompt for Egypt or Iran perspectives using en_cultural, then compare correlation with (a) target country humans vs. (b) Netherlands/Germany/US/Japan humans. If correlation with dominant cluster exceeds target country, the attractor mechanism is overpowering explicit framing.

## Open Questions the Paper Calls Out
None

## Limitations
- Country-language pairing is a gross simplification that obscures multilingual populations and diaspora communities
- Cultural dimensions are not entirely independent, potentially affecting correlation interpretation
- Some countries (China, Indonesia) lack Hofstede VSM data, limiting dimension-level analysis
- All questions involve value judgments, potentially introducing systematic response biases

## Confidence
- High: Explicit English cultural framing improves alignment more than language-specific prompting
- High: All models show systematic bias toward specific Western and Asian cultures
- High: Models tend toward neutral/ambivalent responses on most cultural dimensions
- Medium: Language-specific prompting creates variation without meaningful cultural alignment
- Low: LLMs can reliably represent non-dominant cultural values through prompting

## Next Checks
1. Test whether explicit cultural framing in non-English languages can achieve comparable alignment to en_cultural prompts
2. Measure whether fine-tuning on culturally diverse training data reduces the persistent bias toward dominant cultures
3. Evaluate whether multi-turn conversations with explicit cultural context improve alignment compared to single-shot prompts