---
ver: rpa2
title: Optimization and generalization analysis for two-layer physics-informed neural
  networks without over-parametrization
arxiv_id: '2507.16380'
source_url: https://arxiv.org/abs/2507.16380
tags:
- training
- function
- loss
- neural
- wwwi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies stochastic gradient descent (SGD) training of
  two-layer physics-informed neural networks (PINNs) for solving Poisson's equation
  without requiring over-parameterization. The key innovation is to avoid the typical
  over-parameterization assumption by introducing a function class F for the target
  function and its discretization Fm.
---

# Optimization and generalization analysis for two-layer physics-informed neural networks without over-parametrization

## Quick Facts
- arXiv ID: 2507.16380
- Source URL: https://arxiv.org/abs/2507.16380
- Reference count: 3
- This paper studies SGD training of two-layer PINNs for Poisson's equation without requiring over-parameterization.

## Executive Summary
This paper provides theoretical analysis of two-layer physics-informed neural networks (PINNs) trained with stochastic gradient descent for solving Poisson's equation. The key contribution is showing that PINNs can achieve desired accuracy without requiring over-parameterization, where width scales with sample size. By restricting the target function to a specific class F, the authors prove that width can depend only on desired accuracy and problem structure, not on training data volume. The analysis combines approximation theory, optimization dynamics via linearization, and generalization bounds using Rademacher complexity.

## Method Summary
The method uses two-layer PINNs with ReLU³ activation to solve Δu(x) = f(x) on a unit ball with homogeneous Dirichlet boundary conditions. The network architecture is φ(x) = (∥x∥₂² - 1)·Σᵢ aᵢσ(wᵢᵀx + bᵢ), where σ(t) = max(0, t³), and the PINN is ψ = Δφ. Training uses SGD with frozen aᵢ, bᵢ parameters and only wᵢ updated. The analysis establishes approximation bounds for functions in class F, optimization convergence via pseudo-network linearization, and generalization bounds using Rademacher complexity. Experiments validate the theory by solving 3D Poisson's equation with various training set sizes.

## Key Results
- PINNs can achieve O(ε) training loss without over-parameterization if width exceeds threshold depending only on ε and problem structure
- SGD training succeeds by keeping actual network close to linearized pseudo-network
- Generalization bounds show expected loss can reach O(ε) with sufficient training data
- Numerical experiments on 3D Poisson equation demonstrate training loss between 10⁻³ and 10⁻⁴

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Network width requirements can be decoupled from the volume of training data (N) if the target function lies in a specific function class.
- **Mechanism:** The authors define a function class F (consisting of infinite linear combinations of random basis features) and its discretization Fₘ. Because F has specific spectral/compositional properties (analogous to Barron spaces), the error of approximating a function f ∈ F using m discrete neurons decreases as O(m⁻¹/²). This allows the required width m to depend on the desired accuracy ε and the complexity of f (measured by ‖f‖_F), rather than the sample count N.
- **Core assumption:** The target function (solution to the PDE) belongs to the function class F, or can be closely approximated by it.
- **Evidence anchors:**
  - [abstract] "width exceeds a threshold that depends only on ε and the problem... not on the number of training samples."
  - [section 3.1] Theorem 3.1 establishes the approximation error bound proportional to m⁻¹/².
  - [corpus] Corpus neighbors (e.g., "Convergence of Stochastic Gradient Methods...") primarily discuss standard or wide networks; specific evidence for the *non-over-parameterized* mechanism relies heavily on the theoretical derivation in this paper's Section 3.
- **Break condition:** If the PDE solution involves discontinuities or high-frequency features that are not well-represented in the function class F (e.g., outside the Barron-type space), the required width m may revert to scaling with N or fail to converge.

### Mechanism 2
- **Claim:** Stochastic Gradient Descent (SGD) succeeds by optimizing a linearized "pseudo-network" that resides in the local vicinity of the random initialization.
- **Mechanism:** The analysis tracks the "pseudo-network" g, which is the first-order Taylor expansion of the PINN ψ at initialization. Because g is linear in the weights, its optimization landscape is convex. The paper proves that SGD updates keep the actual PINN ψ close to this pseudo-network g, ensuring the non-convex loss of ψ decreases alongside the convex loss of g.
- **Core assumption:** The optimization dynamics remain bounded (Assumption 3.1: weights and outputs do not explode during iterations), and the learning rate η is appropriately scaled (η = Θ(ε/m)).
- **Evidence anchors:**
  - [abstract] "analyze the optimization dynamics of SGD, proving that the average training loss can be decreased..."
  - [section 3.3] Theorem 3.4 links the average training loss to the behavior of the pseudo-network and initialization.
  - [corpus] Neighbors like "Dual Natural Gradient Descent" suggest alternative optimization landscapes, validating that the standard SGD landscape requires specific structural assumptions (like linearity) to guarantee convergence.
- **Break condition:** If the learning rate is too high or initialization is poor, the weights may move too far from initialization, breaking the linear approximation (ψ ≈ g) and causing divergence or plateauing.

### Mechanism 3
- **Claim:** Generalization is guaranteed by the Rademacher complexity of the specific two-layer PINN architecture.
- **Mechanism:** The paper bounds the difference between empirical training loss and expected loss using Rademacher complexity. By exploiting the specific form of the PINN (ReLU³ activation and the specific structure of the Laplacian residual), they show the complexity is controlled by the norm of the weights ‖W‖₂,∞ and network width m, ensuring generalization if the sample size N is sufficiently large.
- **Core assumption:** The training data points are i.i.d. and the sample size N exceeds a threshold determined by the target accuracy.
- **Evidence anchors:**
  - [section 4] Theorem 4.2 proves the average expected loss is bounded by O(ε) given sufficient N.
  - [abstract] "similar generalization result is derived using Rademacher complexity."
  - [corpus] Explicit mechanism anchors for generalization in non-over-parameterized PINNs are specific to this theoretical treatment in the provided text.
- **Break condition:** If the sampling distribution is biased or N is too small (under-sampling the domain), the generalization gap will exceed the theoretical bound, even if training loss is low.

## Foundational Learning

- **Concept: Over-parameterization vs. Function Class Restrictions**
  - **Why needed here:** The paper's central thesis is that prior theory relied on "over-parameterization" (width ≫ samples), which is computationally wasteful. To escape this, one must understand that assuming a "nice" target function (in class F) allows for smaller networks.
  - **Quick check question:** Does a network need more neurons than training points to learn perfectly? (This paper argues "No," provided the target function is sufficiently smooth/regular).

- **Concept: Linearization and Neural Tangent Kernel (NTK)**
  - **Why needed here:** The proofs rely on approximating the network by its linearization at initialization (the "pseudo network"). This is the theoretical tool that converts a hard non-convex problem into a tractable convex one.
  - **Quick check question:** What is the "pseudo network" g defined in Eq (3.6), and why is its optimization easier than optimizing ψ?

- **Concept: Rademacher Complexity**
  - **Why needed here:** This is the metric used to bound generalization error. Understanding it is necessary to interpret why the "width independent of sample size" result holds for generalization.
  - **Quick check question:** How does the complexity of the function class relate to the number of samples needed to ensure the training error reflects the true error?

## Architecture Onboarding

- **Component map:** x ∈ Γ -> Two-layer network φ(x) = (∥x∥₂² - 1)·Σᵢ aᵢσ(wᵢᵀx + bᵢ) -> PINN ψ = Δφ -> Loss computation
- **Critical path:**
  1. Initialize weights wᵢ, bᵢ with specific scaling (m⁻ᵝ) and output weights aᵢ with scaling (m⁻ᵅ). (Eq 2.8).
  2. Construct the "Laplacian-activated" PINN ψ (Eq 2.7).
  3. Minimize MSE between ψ(x) and f(x) using SGD.
- **Design tradeoffs:**
  - **Width vs. Iteration:** You can use a smaller width m (avoiding over-parameterization), but you must ensure m still exceeds the threshold defined by the complexity of f (‖f‖_F), and you may need careful learning rate tuning.
  - **Activation:** ReLU³ is chosen to allow calculation of the 2nd-order derivative (Laplacian) required for Poisson's equation without vanishing gradients in the linearization terms.
- **Failure signatures:**
  - **Violating Assumption 3.1:** If gradients explode, the "boundedness" assumption fails, and the linearization analysis breaks down.
  - **Wrong Function Class:** If the PDE solution is not in F (e.g., lacks smoothness), the network might fail to decrease loss below ε regardless of iterations.
- **First 3 experiments:**
  1. **Verify Width Independence:** Train on the 3D Poisson equation (f = x₁² + x₂² + x₃²). Fix m=100 and vary N (e.g., 100, 1000, 10000). Confirm that training loss converges to O(10⁻³) in all cases, validating that width requirement is independent of N.
  2. **Test Boundedness:** Monitor the norm ‖w⁽ᵗ⁾ - w⁽⁰⁾‖₂ during training. Verify that it remains small (linear regime) as predicted by Theorem 3.3, rather than drifting arbitrarily.
  3. **Generalization Check:** Hold m and ε constant, but reduce N below the threshold suggested in Theorem 4.2. Observe if the gap between training loss and expected loss increases significantly.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the optimization and generalization analysis be extended to deep PINNs (e.g., three layers) where outer and inner layers have distinct gradient representations?
- Basis: [explicit] Section 6 identifies the consideration of only shallow PINNs as a limitation and states, "Future work could consider the behavior of gradient descent in training slightly deeper (e.g., three-layer) networks."
- Why unresolved: The current theoretical framework relies on the specific gradient representations of two-layer networks, which differ structurally from deep networks.
- What evidence would resolve it: A theoretical proof extending Theorems 3.4 and 4.2 to networks with three or more layers.

### Open Question 2
- Question: Can this analysis be applied to PDEs on general domains with coupled loss terms for boundary conditions?
- Basis: [explicit] Section 6 notes the analysis is restricted to the unit ball and suggests, "future work could also be studying PDEs on general domains with types of boundary or initial conditions."
- Why unresolved: The current model uses a unit ball to satisfy boundary conditions automatically (Eq. 2.3), avoiding the difficulty of analyzing losses with two or more coupled terms inherent to general domains.
- What evidence would resolve it: Derivation of optimization bounds for PINNs on complex geometries where the loss function includes separate, coupled terms for the domain and boundary.

### Open Question 3
- Question: Is the assumption that SGD parameters and PINN outputs remain bounded (Assumption 3.1) provable without manual hyperparameter tuning?
- Basis: [explicit] Section 3 states: "At present, we cannot provide a theoretical guarantee that the above assumption is valid."
- Why unresolved: The analysis depends on this assumption to avoid exploding gradients, but the authors currently treat it as a prerequisite condition rather than a derived property of the system.
- What evidence would resolve it: A rigorous proof demonstrating that ‖wᵢ⁽ᵗ⁾‖ ≤ O(1) and ψ(x; W(t)) ≤ O(1) hold intrinsically during SGD training.

## Limitations
- The analysis depends critically on the target function belonging to the specific function class F, which may not hold for all PDE solutions
- Theoretical bounds are asymptotic and may not accurately predict finite-sample performance
- Learning rate requirement (η = Θ(ε/m)) is not precisely specified in experiments, affecting reproducibility
- Assumption of bounded weights and outputs during training is critical but not rigorously enforced

## Confidence
- **High Confidence:** The approximation error bounds (Theorem 3.1) showing O(m⁻¹/²) convergence for functions in F, as this follows directly from established Barron space theory
- **Medium Confidence:** The SGD optimization analysis (Theorem 3.4), as it relies on multiple technical assumptions about weight bounds and linearization validity that may not hold in practice
- **Medium Confidence:** The generalization bounds (Theorem 4.2) using Rademacher complexity, as they depend on sample size requirements that may be conservative in practice

## Next Checks
1. **Function Class Membership:** Test the PINN on PDE solutions with known discontinuities or high-frequency features to verify when the required width scales with sample size instead of accuracy

2. **Linearization Breakdown:** Monitor weight trajectories during training to empirically measure when the pseudo-network approximation (ψ ≈ g) breaks down, comparing to theoretical bounds

3. **Sample Complexity Threshold:** Systematically vary N below and above the theoretical threshold in Theorem 4.2 to identify the precise sample complexity needed for the generalization gap to remain below O(ε)