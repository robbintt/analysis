---
ver: rpa2
title: Source-primed Multi-turn Conversation Helps Large Language Models Translate
  Documents
arxiv_id: '2503.10494'
source_url: https://arxiv.org/abs/2503.10494
tags:
- translation
- multi-turn
- machine
- segment-level
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a source-primed multi-turn conversation method
  for document-level machine translation using large language models (LLMs). The method
  decomposes documents into segments and iteratively translates them while maintaining
  previous turns, allowing access to full document context and efficient KV cache
  reuse.
---

# Source-primed Multi-turn Conversation Helps Large Language Models Translate Documents

## Quick Facts
- **arXiv ID**: 2503.10494
- **Source URL**: https://arxiv.org/abs/2503.10494
- **Reference count**: 14
- **Primary result**: Source-primed multi-turn conversation method outperforms single-turn and segment-level translation on WMT benchmarks

## Executive Summary
This paper introduces a novel approach for document-level machine translation using large language models through a source-primed multi-turn conversation framework. The method decomposes documents into segments and iteratively translates them while maintaining previous turns, enabling access to full document context and efficient KV cache reuse. By first providing the entire source document before starting multi-turn translation, the approach gives models access to future context. Experiments demonstrate consistent performance improvements across multiple evaluation metrics compared to baseline approaches.

## Method Summary
The proposed method decomposes documents into segments and employs a multi-turn conversation approach where each segment translation builds upon previous turns. The key innovation is the source-priming step, where the entire source document is provided before translation begins, giving the model access to future context. This approach maintains coherence across document segments while enabling efficient KV cache reuse during iterative translation. The method was evaluated on WMT-24 and WMT-23 datasets, showing superior performance compared to both single-turn document translation and segment-level approaches.

## Key Results
- Multi-turn conversation method outperforms single-turn document translation and segment-level approaches
- Source-primed variant achieves the best results among tested configurations
- Demonstrated improvements in translation coherence and reduced omission errors
- Consistent performance gains across multiple evaluation metrics on WMT benchmarks

## Why This Works (Mechanism)
The approach works by leveraging the document-level context through source-priming and maintaining conversation history across segments. By providing the entire source document upfront, the model gains access to future context that would otherwise be unavailable during sequential translation. The multi-turn conversation structure allows the model to build upon previous translations, maintaining coherence and reducing repetition or omission errors. The KV cache reuse mechanism provides computational efficiency by avoiding redundant context encoding across segments.

## Foundational Learning
- **Document-level context**: Why needed - captures cross-segment dependencies and coherence; Quick check - compare performance on documents with explicit inter-segment references
- **Multi-turn conversation**: Why needed - maintains translation state and consistency across segments; Quick check - measure coherence metrics across segment boundaries
- **KV cache reuse**: Why needed - reduces computational overhead in iterative translation; Quick check - compare memory usage and inference time with and without cache reuse
- **Source-priming**: Why needed - provides future context unavailable in sequential approaches; Quick check - evaluate impact on coherence when source-priming is removed
- **Segment decomposition**: Why needed - enables scalable processing of long documents; Quick check - test performance degradation with increasing segment count
- **Iterative refinement**: Why needed - allows progressive improvement of translation quality; Quick check - measure quality gains across conversation turns

## Architecture Onboarding
**Component map**: Source Document -> Source-Priming -> Multi-Turn Segment Translation -> Final Document
**Critical path**: The critical path involves source-priming the entire document, then sequentially translating segments while maintaining conversation context and KV cache state.
**Design tradeoffs**: Balances between full document context access (through source-priming) and computational efficiency (through segment-based processing and cache reuse). Tradeoff between translation quality and processing overhead.
**Failure signatures**: Degradation in coherence at segment boundaries, increased omission errors when context window is insufficient, computational bottlenecks with very long documents.
**First experiments**:
1. Compare single-turn vs multi-turn performance on documents with explicit inter-segment dependencies
2. Measure KV cache size and inference time across varying document lengths
3. Test source-priming impact by removing it from the multi-turn approach

## Open Questions the Paper Calls Out
None

## Limitations
- Performance on extremely long documents beyond typical benchmark lengths remains untested
- Computational overhead analysis during inference needs more detailed examination
- Impact on translation quality for languages with significantly different grammatical structures is not explored
- Real-world document testing beyond standard benchmarks is limited

## Confidence
- **High confidence**: Core experimental methodology and results showing improvement over single-turn baselines are robust and well-documented
- **Medium confidence**: Claimed efficiency gains from KV cache reuse are plausible but lack detailed computational analysis
- **Medium confidence**: Assertion that source-priming provides best results is supported by experiments but could benefit from additional ablation studies

## Next Checks
1. Conduct experiments on documents significantly longer than those in standard benchmarks to test scalability limits and KV cache management efficiency
2. Perform detailed computational analysis comparing inference time and memory usage between single-turn and multi-turn approaches across varying document lengths
3. Test the approach on a diverse set of language pairs with different grammatical structures to assess generalization beyond Indo-European languages