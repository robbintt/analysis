---
ver: rpa2
title: 'Stabilizing Knowledge, Promoting Reasoning: Dual-Token Constraints for RLVR'
arxiv_id: '2507.15778'
source_url: https://arxiv.org/abs/2507.15778
tags:
- tokens
- reasoning
- training
- arxiv
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces Archer, a dual-token constraint framework
  for RLVR that addresses the challenge of balancing knowledge preservation and reasoning
  exploration. Archer identifies reasoning-critical tokens using response-level entropy
  and applies differentiated training constraints: weaker KL regularization and higher
  clipping thresholds for reasoning tokens to encourage exploration, while stronger
  constraints are applied to knowledge tokens to maintain factual accuracy.'
---

# Stabilizing Knowledge, Promoting Reasoning: Dual-Token Constraints for RLVR

## Quick Facts
- arXiv ID: 2507.15778
- Source URL: https://arxiv.org/abs/2507.15778
- Authors: Jiakang Wang; Runze Liu; Fuzheng Zhang; Xiu Li; Guorui Zhou
- Reference count: 29
- Primary result: Dual-token constraint framework for RLVR that achieves +5.6% average accuracy gains on math reasoning and +3.0% on code generation

## Executive Summary
This paper introduces Archer, a dual-token constraint framework for RLVR that addresses the challenge of balancing knowledge preservation and reasoning exploration. Archer identifies reasoning-critical tokens using response-level entropy and applies differentiated training constraints: weaker KL regularization and higher clipping thresholds for reasoning tokens to encourage exploration, while stronger constraints are applied to knowledge tokens to maintain factual accuracy. The method synchronously updates all tokens to preserve semantic dependencies, unlike prior approaches that use gradient masking or asynchronous updates. Experimental results show Archer significantly outperforms DAPO and state-of-the-art models while using fewer training steps and GPU hours.

## Method Summary
Archer uses response-level entropy to classify tokens as reasoning (high-entropy) or knowledge (low-entropy) based on the 80th percentile threshold within each response. For reasoning tokens, it applies loose clipping (ε=0.5) and zero KL regularization to encourage exploration. For knowledge tokens, it uses tight clipping (ε=0.2) and KL regularization (β=0.001) to maintain factual accuracy. All tokens are updated synchronously to preserve semantic dependencies. The framework is implemented using the verl RL framework with DeepSeek-R1-Distill-Qwen-1.5B as base model, trained for 520 steps on math and 320 steps on code using 16× H800 GPUs.

## Key Results
- Achieves +5.6% average accuracy gains on mathematical reasoning benchmarks (AIME24/25) compared to DAPO
- Improves code generation tasks by +3.0% on LiveCodeBench v5/v6
- Outperforms state-of-the-art models like DeepScaleR-1.5B and DeepCoder-1.5B
- Uses fewer training steps and GPU hours than baseline approaches
- Improves reasoning through better structural organization, attention to details, and contextual consistency

## Why This Works (Mechanism)

### Mechanism 1: Response-Level Entropy Stratification
Using response-level entropy quantiles to classify tokens allows for more precise targeting of reasoning tokens compared to batch-level statistics. The model computes the 80th percentile of entropy within each individual response to set a threshold, isolating high-entropy "reasoning" tokens (logical connectors) from low-entropy "knowledge" tokens (facts/syntax).

### Mechanism 2: Synchronous Gradient Constraint Disentanglement
Applying differentiated clipping and KL constraints synchronously prevents semantic degradation caused by gradient masking while optimizing distinct token roles. The framework updates all tokens but applies loose constraints to reasoning tokens for exploration and tight constraints to knowledge tokens for stability.

### Mechanism 3: Knowledge-Stabilizing KL Regularization
Selective KL regularization on low-entropy tokens prevents model collapse without stifling exploration needed for reasoning. By setting β>0 only for low-entropy tokens, the model anchors factual knowledge to the reference policy, preventing entropy collapse and repetition loops.

## Foundational Learning

- **Proximal Policy Optimization (PPO) Clipping**: Archer modifies standard PPO clipping mechanism based on token type. Understanding the ratio r_t(θ) and how clipping limits policy deviation is required to interpret dual-token constraints. Quick check: How does changing the clipping threshold ε from 0.2 to 0.5 affect the range of allowed probability ratios r_t(θ)?

- **KL Divergence in RLHF**: The paper dynamically adjusts the KL coefficient (β) to balance exploration and stability. You must understand that KL divergence penalizes deviation from the initial reference policy. Quick check: If β is set to 0 for reasoning tokens, what constraint primarily prevents the policy from changing too rapidly on those tokens?

- **Entropy in Language Modeling**: The framework relies on entropy as a proxy for "reasoning" vs. "knowledge." You need to distinguish between token probability (confidence) and entropy (uncertainty). Quick check: In the context of this paper, does a low-entropy token likely represent a logical transition point or a factual entity?

## Architecture Onboarding

- **Component map**: Rollout Engine -> Entropy Analyzer -> Constraint Selector -> Optimizer
- **Critical path**: The Response-Level Entropy Calculation is critical. You must calculate statistics per generated sequence to correctly tag tokens before loss calculation.
- **Design tradeoffs**: Stability vs. Plasticity (setting ε_k too low or β_k too high freezes knowledge but may slow adaptation); Threshold (ρ=0.8 - lower treats more tokens as "reasoning" risking factual drift; higher treats more as "knowledge" risking stagnation)
- **Failure signatures**: Repetition Loops (β_k=0.0 or ε_k too high causes collapse); Stagnation (β_k too high flattens accuracy gains early); Semantic Incoherence (asynchronous updates degrade logical flow)
- **First 3 experiments**:
  1. Ablate KL Weight (β_k): Train with β_k ∈ {0.0, 0.001, 0.005} on validation set to confirm "repetition ratio" vs. "accuracy" trade-off
  2. Response vs. Batch Entropy: Compare training dynamics when calculating entropy threshold per batch vs. per response
  3. Clip Range Sensitivity: Run sweep on ε_r vs. ε_k to visualize "optimization regions" and confirm extending clip for high-entropy tokens improves convergence

## Open Questions the Paper Calls Out

- How does the optimal ratio of dual-token constraints (ε_r vs ε_k and β_r vs β_k) shift when scaling Archer to significantly larger models (e.g., 7B+) or different architectures?
- Would replacing the binary entropy threshold with a continuous, entropy-weighted constraint function improve the smoothing of gradient updates?
- Does the finding that "performance improvements correlate more strongly with intrinsic difficulty than topical categories" hold true for non-STEM domains like open-ended dialogue or creative writing?

## Limitations
- Method is specifically designed for RLVR tasks with verifiable rewards and untested on open-ended generation
- Limited evidence about transfer to domains beyond math reasoning and code generation
- Several critical implementation details are underspecified, including exact entropy calculation method
- Effectiveness on larger models (70B+) or smaller models (1B-) is not demonstrated

## Confidence

**High Confidence**: The mathematical framework for dual-token constraints is rigorously defined with clear equations; the response-level entropy stratification approach is novel and addresses a specific limitation; ablation studies systematically validate hyperparameter impact; synchronous gradient constraint mechanism is well-explained.

**Medium Confidence**: The correlation between token entropy and reasoning utility is primarily validated through downstream performance; comparison with state-of-the-art models is compelling but doesn't establish dominance across all metrics; computational efficiency claims are supported but baseline comparisons could be more comprehensive.

**Low Confidence**: The generalization of findings beyond specific math and code domains tested; long-term stability of trained models beyond training period; sensitivity of results to different initial model checkpoints or reward functions.

## Next Checks
1. Test Archer on non-mathematical, non-code reasoning tasks (e.g., commonsense reasoning, multi-hop QA) to verify entropy-based reasoning token identification generalizes beyond structured, verifiable reward domains.

2. Systematically vary the ρ quantile threshold (0.7, 0.8, 0.9) and measure its impact on both performance and token classification accuracy to confirm robustness of response-level entropy stratification.

3. Evaluate Archer on multiple model sizes (1B, 7B, 34B) to determine whether dual-token constraint benefits scale proportionally with model capacity or if there are diminishing returns.