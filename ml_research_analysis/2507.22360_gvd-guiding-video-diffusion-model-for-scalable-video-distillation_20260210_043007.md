---
ver: rpa2
title: 'GVD: Guiding Video Diffusion Model for Scalable Video Distillation'
arxiv_id: '2507.22360'
source_url: https://arxiv.org/abs/2507.22360
tags:
- video
- dataset
- distillation
- videos
- frames
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'GVD: Guiding Video Diffusion Model for Scalable Video Distillation
  proposes the first diffusion-based method for video dataset distillation. It addresses
  the challenge of efficiently capturing spatial and temporal information in large
  video datasets by guiding a pre-trained video diffusion model with class-specific
  cluster centers.'
---

# GVD: Guiding Video Diffusion Model for Scalable Video Distillation

## Quick Facts
- arXiv ID: 2507.22360
- Source URL: https://arxiv.org/abs/2507.22360
- Authors: Kunyang Li; Jeffrey A Chan Santiago; Sarinda Dhanesh Samarasinghe; Gaowen Liu; Mubarak Shah
- Reference count: 40
- Primary result: First diffusion-based method for video dataset distillation achieving 78.29% of original dataset performance on MiniUCF using only 1.98% of frames

## Executive Summary
GVD introduces the first diffusion-based approach for video dataset distillation, addressing the challenge of efficiently capturing spatial and temporal information in large video datasets. The method guides a pre-trained video diffusion model with class-specific cluster centers computed in latent space, introducing a frame-wise linear decay mechanism and multi-video instance composition to enhance diversity while preserving motion coherence. GVD achieves state-of-the-art results, reaching 78.29% of the original dataset's performance on MiniUCF using only 1.98% of frames and 73.83% on HMDB51 with 3.30% of frames, demonstrating superior scalability and cross-architecture generalization compared to existing approaches.

## Method Summary
GVD computes KMeans cluster centers on VQGAN-encoded latent features per video class, then guides ModelScope T2V diffusion during DDIM sampling using frame-wise linear decay guidance (λ_f = λ × (1 - f/F), stopping at t_stop=1000). The method applies Multi-Video Instance Composition (U=4 videos, 4 frames each) to integrate meaningful and diverse content while maintaining temporal coherence. Students are trained on the distilled data with soft labels from a pre-trained VideoMAE using KL loss (α=0.2, temperature=3.0), achieving Top-1 accuracy of 78.29% on MiniUCF and 73.83% on HMDB51 with minimal frame usage.

## Key Results
- Achieves 78.29% of original dataset performance on MiniUCF using only 1.98% of frames
- Reaches 73.83% of original performance on HMDB51 with 3.30% of frames
- Demonstrates superior scalability with linear batch size dependency rather than IPC
- Shows effective cross-architecture generalization across multiple student model architectures

## Why This Works (Mechanism)

### Mechanism 1: Latent Cluster Guidance
GVD modifies the diffusion denoising loop by injecting a guidance term $g_{\hat{t}} = m_k - \hat{x}_{0,\hat{t}}$ that pulls generated videos toward class-specific cluster centers in latent space. This steering creates more representative distilled datasets than unguided generation by ensuring outputs align with the semantic distribution of each class.

### Mechanism 2: Frame-wise Linear Decay
The guidance coefficient $\lambda_f$ decreases linearly as frame index $f$ increases (λ_f = λ × (1 - f/F)), applying strong guidance to early frames for content/structure while allowing later frames to rely on the diffusion model's native temporal priors. This prevents the noise introduced by excessive guidance from disrupting motion dynamics.

### Mechanism 3: Multi-Video Instance Composition
GVD concatenates frames from multiple generated videos (U×IPC strategy) to increase information density per distilled sample. This forces single distilled videos to contain multiple "views" or trajectories of the action, enhancing intra-class diversity while maintaining natural temporal coherence.

## Foundational Learning

- **Diffusion Guidance (Classifier-Free Guidance)**: Why needed - GVD modifies standard denoising loops by combining unconditional predictions with conditional signals to steer generation. Quick check - Can you explain how modifying the noise prediction $\epsilon_\theta$ with a guidance term $g$ shifts the sample distribution?

- **Latent Space Clustering (K-Means)**: Why needed - The quality of distillation relies entirely on "guiding vectors" ($m_k$) that capture semantic essence of video classes. Quick check - If a class has 10 distinct action styles but you set K=3, what is the likely result for the distilled dataset?

- **Dataset Distillation Objective**: Why needed - GVD condenses datasets, requiring understanding of trade-offs between representativeness and diversity. Quick check - Why does GVD use a pre-trained VideoMAE to compute soft labels for the student model rather than ground truth hard labels?

## Architecture Onboarding

- **Component map**: Encoder (VQGAN) -> Prototype Generator (K-Means) -> Diffusion Core (ModelScope T2V) -> Guidance Controller (injects $g_{\hat{t}}$ with decay $\lambda_f$) -> Post-Processor (MVIC) -> Evaluator (MiniC3D/CNN-GRU)

- **Critical path**: Correct calculation of guidance term $g_{\hat{t}} = m_k - \hat{x}_{0,\hat{t}}$ is most critical. If dimensions don't align or decay isn't applied frame-wise, the model fails to capture motion.

- **Design tradeoffs**: Guidance Strength ($\lambda$) - High ensures class alignment but reduces diversity; Low increases diversity but risks out-of-distribution samples. IPC vs Efficiency - GVD scales linearly with batch size, not IPC, making it highly scalable.

- **Failure signatures**: Static Output (over-guidance, λ too high), Semantic Drift (later frames mismatch class, decay too sharp), Low Diversity (identical videos, K-means failed or U=1).

- **First 3 experiments**: 1) Guidance Ablation - Compare λ=0 vs λ=0.1 on single MiniUCF class. 2) Component Isolation - Skip MVIC step and measure Top-1 accuracy drop. 3) Cross-Architecture Test - Train MiniC3D and CNN+RNN on same distilled set to confirm stable accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
Can the performance gap at IPC=1 be closed while retaining the scalability of diffusion-based distillation? The method underperforms at IPC 1 compared to optimization-based baselines like MTT, attributed to generative alignment limitations without a proposed solution for single-sample distillation.

### Open Question 2
How robust is the Frame-wise Linear Decay mechanism across video diffusion architectures other than ModelScope? The linear decay heuristic (1 - f/F) is derived empirically for ModelScope, leaving its universality to other U-Net architectures unproven.

### Open Question 3
Does reliance on single cluster centroids limit capture of multi-modal motion patterns within a class? While MVIC increases diversity, the guidance still pulls toward a single mean trajectory per instance, potentially missing distinct sub-actions within classes.

## Limitations
- Relies heavily on assumption that K-means cluster centers in latent space adequately represent prototypical class content, risking mode collapse with insufficient K
- MVIC mechanism's assumption that concatenating frames from multiple videos produces coherent temporal sequences is largely heuristic with limited validation
- Additional dependency on pre-trained VideoMAE for soft-labeling without clear ablation of its impact versus hard labels

## Confidence
- **High Confidence**: Quantitative results showing state-of-the-art performance (78.29% on MiniUCF, 73.83% on HMDB51) and scalability claims are well-supported
- **Medium Confidence**: Efficacy of frame-wise linear decay mechanism is supported by Figure 7(b) but requires further exploration of sensitivity
- **Low Confidence**: MVIC component's contribution is demonstrated through ablation but lacks theoretical justification for why frame concatenation aids temporal coherence

## Next Checks
1. **Cluster Center Quality Audit**: Visualize K-means cluster centers in latent space for subset of classes and compare to random samples from original distribution to quantify coverage and diversity
2. **MVIC Temporal Coherence Analysis**: Generate distilled videos with and without MVIC and perform qualitative/quantitative analysis (optical flow consistency, action recognition confidence) to assess temporal coherence
3. **Guidance Strength Sensitivity**: Conduct systematic sweep of λ (0.01, 0.05, 0.1, 0.2) and t_stop to identify optimal operating region and potential failure modes through accuracy, diversity metrics, and visual quality trade-offs