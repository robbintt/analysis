---
ver: rpa2
title: Structured Sparsity and Weight-adaptive Pruning for Memory and Compute efficient
  Whisper models
arxiv_id: '2510.12666'
source_url: https://arxiv.org/abs/2510.12666
tags:
- pruning
- weight
- layers
- whisper
- sparsity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of deploying Whisper models
  on resource-constrained edge devices by proposing a framework called TSPAR (fineTuning
  for Structured sParsity and Adaptive pRuning). The method enforces structured sparsity
  through Sparse Group LASSO regularization to reduce FLOPs and memory, and employs
  a weight-statistics-driven two-pass pruning algorithm that adapts dynamically to
  model parameters.
---

# Structured Sparsity and Weight-adaptive Pruning for Memory and Compute efficient Whisper models

## Quick Facts
- arXiv ID: 2510.12666
- Source URL: https://arxiv.org/abs/2510.12666
- Reference count: 0
- Primary result: TSPAR framework achieves up to 35.4% parameter reduction, 14.25% lower memory, and 18.5% fewer FLOPs on Whisper-small without WER degradation

## Executive Summary
This paper addresses the challenge of deploying Whisper models on resource-constrained edge devices by proposing TSPAR (fineTuning for Structured sParsity and Adaptive pRuning). The method combines Sparse Group LASSO regularization with a weight-statistics-driven two-pass pruning algorithm to achieve significant reductions in parameters, memory, and FLOPs while maintaining speech recognition accuracy. Experiments on the Common Voice 11.0 Hindi dataset demonstrate that TSPAR outperforms the state-of-the-art Iterative Magnitude Pruning method by pruning 18.7% more parameters and reducing WER by 12.31 points.

## Method Summary
The TSPAR framework employs Sparse Group LASSO regularization to enforce structured sparsity across model layers, reducing FLOPs and memory consumption. A two-pass weight-statistics-driven pruning algorithm dynamically adapts to model parameters, identifying and removing redundant connections. The approach includes a custom Hindi text normalizer for fair WER evaluation. The method integrates fine-tuning with structured sparsity enforcement and adaptive pruning to optimize Whisper models for edge deployment while preserving speech recognition performance.

## Key Results
- Whisper-small: 35.4% parameter reduction, 14.25% lower memory, 18.5% fewer FLOPs without WER degradation
- Whisper-medium: 31% parameter reduction, 15.29% lower memory, 16.95% fewer FLOPs without WER degradation
- Outperforms Iterative Magnitude Pruning by 18.7% more parameters pruned and 12.31 WER improvement points

## Why This Works (Mechanism)
The method leverages Sparse Group LASSO regularization to impose structured sparsity patterns that align with model architecture, ensuring computational savings translate to actual efficiency gains. The weight-statistics-driven two-pass pruning algorithm adapts dynamically to parameter distributions, identifying redundancies that traditional magnitude-based pruning might miss. By integrating fine-tuning with sparsity enforcement, the framework maintains model accuracy while aggressively reducing model size and computational requirements.

## Foundational Learning
- Sparse Group LASSO regularization: Combines L1 and group LASSO penalties to enforce structured sparsity; needed for computational efficiency on hardware with regular memory access patterns
- Weight-statistics-driven pruning: Uses parameter statistics to guide pruning decisions rather than static thresholds; needed for adaptive pruning that responds to model-specific characteristics
- Structured sparsity patterns: Organized sparsity that aligns with hardware acceleration capabilities; needed to ensure theoretical savings translate to practical efficiency gains

## Architecture Onboarding
- Component map: Input audio -> Encoder blocks -> Attention mechanisms -> Decoder blocks -> Output text (sparsity applied across all components)
- Critical path: Audio preprocessing -> Encoder feature extraction -> Attention-based contextualization -> Decoder transcription generation (with pruning applied during fine-tuning)
- Design tradeoffs: Aggressive sparsity vs. model accuracy; structured vs. unstructured sparsity patterns; fine-tuning duration vs. efficiency gains
- Failure signatures: WER degradation indicates over-pruning; minimal parameter reduction suggests insufficient sparsity enforcement; inconsistent improvements across layers indicate improper regularization
- First experiments: 1) Baseline WER measurement on unpruned model, 2) Parameter reduction analysis after initial sparsity application, 3) FLOPs calculation before and after pruning

## Open Questions the Paper Calls Out
None

## Limitations
- Limited to Hindi language and Common Voice 11.0 dataset, restricting generalizability
- Comparison only against Iterative Magnitude Pruning without broader state-of-the-art benchmarking
- Absence of real-device inference time measurements to validate practical deployment benefits

## Confidence
- Parameter and memory reduction claims: High confidence
- WER maintenance claims: High confidence  
- Practical deployment benefits claims: Medium confidence

## Next Checks
1. Conduct experiments on multiple languages and datasets beyond Hindi/Common Voice to verify framework generalizability across different ASR tasks and linguistic characteristics

2. Implement and compare TSPAR against additional state-of-the-art pruning methods including magnitude-based pruning with different fine-tuning strategies and other structured pruning approaches

3. Measure actual inference latency, power consumption, and memory usage on representative edge devices (e.g., Raspberry Pi, mobile CPU) to validate practical deployment benefits claimed through theoretical FLOPs and memory reductions