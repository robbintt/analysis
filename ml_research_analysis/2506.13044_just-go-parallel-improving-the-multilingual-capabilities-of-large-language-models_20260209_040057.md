---
ver: rpa2
title: 'Just Go Parallel: Improving the Multilingual Capabilities of Large Language
  Models'
arxiv_id: '2506.13044'
source_url: https://arxiv.org/abs/2506.13044
tags:
- data
- parallel
- translation
- language
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether incorporating parallel data into
  the training of decoder-based large language models (LLMs) improves their multilingual
  capabilities. While LLMs have demonstrated strong translation abilities, some models
  ignore available parallel data during training.
---

# Just Go Parallel: Improving the Multilingual Capabilities of Large Language Models

## Quick Facts
- arXiv ID: 2506.13044
- Source URL: https://arxiv.org/abs/2506.13044
- Reference count: 21
- Primary result: Parallel data significantly improves LLM multilingual translation and reasoning capabilities, with second-stage training most effective

## Executive Summary
This paper investigates whether incorporating parallel data into the training of decoder-based large language models (LLMs) improves their multilingual capabilities. While LLMs have demonstrated strong translation abilities, some models ignore available parallel data during training. The authors conduct controlled experiments with seven training setups: no parallel data, monolingual multilingual data, parallel data at the beginning, distributed parallel data, parallel data at the end (all directions), and parallel data at the end (one direction). They evaluate translation performance using BLEU scores across English, Chinese, and Indonesian language pairs, and also assess multilingual common-sense reasoning. Results show that adding parallel data significantly improves both translation and reasoning capabilities, with parallel data at the end of training being most effective. Training with unidirectional parallel data yields higher translation performance but creates specialized models unable to translate in other directions. The study demonstrates that leveraging parallel data—especially in second-stage training—substantially enhances LLM multilingual performance compared to models trained without such data.

## Method Summary
The study uses TinyLlama 1.1B as the base model, training on SlimPajama subset (~167B tokens, 82.35% English) with parallel corpora (4.5B tokens). Parallel data is formatted as "{source_lang}: {sentence}\n{target_lang}: {translation}" with `<\s>` separators and chunked to 16,384 tokens. Seven training setups are compared: NO PARALLEL, PARALLEL FIRST, PARALLEL DISTRIBUTED, PARALLEL NON-ADJACENT, PARALLEL LAST (ALL), PARALLEL LAST (UNI), and MULTILINGUAL. Training uses 8×H100 GPUs, global batch 512, LR 4e-4, AdamW optimizer, and checkpoints saved every 1,000 steps during parallel phase. Models are evaluated on WMT-2023 (ZH↔EN) and Flores-200 (ID↔EN) using BLEU scores, plus zero-shot and 5-shot accuracy on 11 common-sense reasoning benchmarks.

## Key Results
- Parallel data improves translation performance significantly: BLEU scores increase from ~1.0 to 21-42 across language pairs when using PARALLEL LAST (ALL)
- Parallel data at end of training (PARALLEL LAST) outperforms other placements, with slightly better reasoning performance than distributed parallel
- Unidirectional parallel training creates asymmetric translation capabilities: models achieve >40 BLEU in trained direction but <1.0 BLEU in reverse
- Quality-filtered parallel data improves zero-shot translation but slightly degrades few-shot performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Adjacent parallel sentence pairs create explicit cross-lingual mappings that monolingual data cannot establish alone.
- **Mechanism:** When source sentences and their translations appear consecutively in training (e.g., "EN: {sentence}\nZH: {translation}"), the model learns to associate semantically equivalent representations across languages through next-token prediction on aligned pairs.
- **Core assumption:** The model's attention mechanism can bind adjacent sentence pairs into cross-lingual associations during training.
- **Evidence anchors:**
  - [abstract]: "parallel data can significantly improve LLMs' multilingual capabilities"
  - [Section 5]: "PARALLEL DISTRIBUTED setting even outperforms BLOOM, despite BLOOM being trained with more Indonesian and Chinese data"
  - [corpus]: Related work FuxiMT confirms two-stage training with parallel data improves multilingual MT (FMR=0.61, moderate relevance)
- **Break condition:** If parallel sentences are non-adjacent (PARALLEL NON-ADJACENT), cross-lingual learning degrades substantially—BLEU scores drop from 21.95 to 1.98 for EN→ID.

### Mechanism 2
- **Claim:** Second-stage training with parallel data (end of training) avoids catastrophic forgetting while maximizing multilingual gains.
- **Mechanism:** Introducing parallel data after the model has learned general language representations from monolingual data allows specialization without overwriting foundational knowledge. Early parallel exposure causes interference as subsequent monolingual data erases the cross-lingual mappings.
- **Core assumption:** The model's capacity to retain early training signals is limited; later training data has disproportionate influence on final capabilities.
- **Evidence anchors:**
  - [Section 3.6]: "This approach can also be interpreted as second-stage training, where the model receives bilingual exposure after being pre-trained primarily on English data"
  - [Section 6.3]: "When examining the performance at later checkpoints, we observe that the translation capability completely disappears once the parallel data are exhausted"
  - [corpus]: No direct corpus support for this specific temporal mechanism
- **Break condition:** PARALLEL FIRST shows translation capability vanishes after parallel data is exhausted—demonstrating catastrophic forgetting in action.

### Mechanism 3
- **Claim:** Unidirectional parallel training creates asymmetric translation capabilities related to the "reversal curse" in autoregressive models.
- **Mechanism:** When trained only on EN→ZH pairs, the model learns source-to-target mappings but cannot invert them. The autoregressive factorization may prevent learning bidirectional associations from unidirectional exposure.
- **Core assumption:** The reversal curse phenomenon generalizes to translation tasks—learning "A translates to B" does not imply "B translates to A."
- **Evidence anchors:**
  - [Section 6.1]: "surprisingly, they also fail to translate in the opposite direction of the language pair they are trained on... even worse than models trained without parallel data"
  - [Section 6.1]: "This phenomenon may be related to the issue of autoregressive LLMs struggling with inverse relationships, dubbed the reversal curse"
  - [corpus]: Trans-Zero addresses catastrophic forgetting in multilingual MT but doesn't directly validate reversal curse mechanism
- **Break condition:** Unidirectional models score near-zero BLEU (<1.0) on reverse translation directions despite high scores on trained directions (>40).

## Foundational Learning

- **Concept: Parallel (Bilingual) Data**
  - Why needed here: Understanding that parallel corpora contain aligned sentence pairs across languages is foundational to interpreting all experimental conditions.
  - Quick check question: Can you explain why PARALLEL NON-ADJACENT differs fundamentally from PARALLEL DISTRIBUTED?

- **Concept: Catastrophic Forgetting**
  - Why needed here: The paper's central finding—that parallel data placement timing matters—depends on understanding how neural networks overwrite earlier learning.
  - Quick check question: If a model trained on parallel data first, then monolingual data, loses translation ability, what mechanism explains this?

- **Concept: Autoregressive Language Modeling**
  - Why needed here: Decoder-only LLMs predict tokens left-to-right; this directional bias may explain why reversal capabilities fail.
  - Quick check question: Why might an autoregressive model trained on "A→B" pairs fail to learn "B→A" translation?

## Architecture Onboarding

- **Component map:**
  TinyLlama 1.1B base model -> SlimPajama subset (167B tokens, 82% English) -> Parallel corpora (4.5B tokens) -> Seven training setups -> BLEU evaluation (WMT-2023, Flores-200) -> Common-sense reasoning benchmarks (ARC, HellaSwag, XNLI, XCOPA, etc.)

- **Critical path:**
  1. Pre-train base model on monolingual data
  2. Add parallel data at END of training (PARALLEL LAST ALL for general multilingual; PARALLEL LAST UNI for specialized translation)
  3. Select checkpoint with highest average BLEU on dev set
  4. Evaluate on WMT-2023 (ZH↔EN) and Flores-200 devtest; run LM-Eval Harness for reasoning benchmarks

- **Design tradeoffs:**
  - All-directions vs. unidirectional: All-directions preserves general multilingual capability; unidirectional achieves higher BLEU on target direction but loses reverse translation
  - Data quality filtering: Filtering parallel data by quality scores (CometKiwi) showed mixed results—LLMs appear resilient to noise at this scale
  - Distributed vs. last: Both effective; PARALLEL LAST slightly better for reasoning, PARALLEL DISTRIBUTED avoids catastrophic forgetting risk

- **Failure signatures:**
  - Translation capability drops to near-zero after initial parallel training → catastrophic forgetting (PARALLEL FIRST)
  - High BLEU on one direction but <1.0 on reverse → unidirectional training artifact
  - Poor translation despite multilingual monolingual data → parallel structure needed, not just language exposure

- **First 3 experiments:**
  1. **Baseline check:** Train NO PARALLEL and PARALLEL LAST (ALL) with identical token budgets; verify BLEU improvement exceeds 10 points on at least one language pair.
  2. **Position ablation:** Compare PARALLEL FIRST vs. PARALLEL LAST (ALL) on identical data; expect FIRST to show capability collapse after parallel data exhaustion.
  3. **Direction test:** Train unidirectional EN→ID model; evaluate both EN→ID and ID→EN to confirm reversal curse (expect >40 BLEU forward, <1 BLEU reverse).

## Open Questions the Paper Calls Out
The authors note that exploring additional languages and larger-scale models is left for future work, suggesting these extensions could provide valuable insights into the generalizability of their findings.

## Limitations
- Data composition uncertainty: The specific SlimPajama subset used is not detailed, limiting reproducibility and domain understanding
- Evaluation scope limited to three language pairs (EN-ZH, EN-ID) without testing cross-directional transfer (e.g., ZH-ID)
- Generalization boundaries unclear—findings may not transfer to larger models or different architectures

## Confidence
**High Confidence Claims:**
- Parallel data improves multilingual translation and reasoning capabilities
- PARALLEL LAST ALL consistently outperforms NO PARALLEL across all metrics
- Unidirectional training creates asymmetric translation capabilities (reversal curse)

**Medium Confidence Claims:**
- PARALLEL LAST ALL slightly outperforms PARALLEL DISTRIBUTED for reasoning tasks
- Quality filtering shows mixed results, suggesting LLMs are robust to parallel data noise
- Token count (4.5B) is sufficient for effective parallel training

**Low Confidence Claims:**
- Mechanism 2 (second-stage training timing) relies on single experimental setup without validation across model sizes
- Generalization to non-English-centric language pairs not evaluated
- Reasoning performance improvements directly attributable to parallel data vs. multilingual exposure alone

## Next Checks
1. **Scale Generalization Test:** Repeat PARALLEL LAST ALL training with 3B-10B parameter models using identical parallel data strategy. Measure whether BLEU improvements scale proportionally or show diminishing returns.

2. **Cross-Directional Transfer:** Train PARALLEL LAST ALL on ZH-ID parallel data (excluding English) and evaluate translation to/from English. This tests whether parallel structure learning transfers across language families beyond English-centric training.

3. **Forgetting Resistance Analysis:** Systematically vary parallel data placement (10% increments from 0-100% of training tokens) in PARALLEL DISTRIBUTED setup. Track translation capability across checkpoints to map the precise forgetting threshold and optimal distribution strategy.