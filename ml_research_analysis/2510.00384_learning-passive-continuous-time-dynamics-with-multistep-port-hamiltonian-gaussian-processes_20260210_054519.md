---
ver: rpa2
title: Learning Passive Continuous-Time Dynamics with Multistep Port-Hamiltonian Gaussian
  Processes
arxiv_id: '2510.00384'
source_url: https://arxiv.org/abs/2510.00384
tags:
- posterior
- hamiltonian
- multistep
- ms-phs
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the multistep port-Hamiltonian Gaussian process
  (MS-PHS GP) to learn continuous-time dynamics from noisy, irregularly sampled data
  while enforcing port-Hamiltonian structure. The method places a GP prior on the
  Hamiltonian and encodes multistep integration constraints as linear functionals,
  yielding closed-form posteriors for both the vector field and Hamiltonian surface.
---

# Learning Passive Continuous-Time Dynamics with Multistep Port-Hamiltonian Gaussian Processes

## Quick Facts
- arXiv ID: 2510.00384
- Source URL: https://arxiv.org/abs/2510.00384
- Reference count: 29
- Primary result: MS-PHS GP outperforms baselines in vector-field recovery and uncertainty calibration while enforcing port-Hamiltonian structure by construction

## Executive Summary
This paper introduces the multistep port-Hamiltonian Gaussian process (MS-PHS GP) to learn continuous-time dynamics from noisy, irregularly sampled data while enforcing port-Hamiltonian structure. The method places a GP prior on the Hamiltonian and encodes multistep integration constraints as linear functionals, yielding closed-form posteriors for both the vector field and Hamiltonian surface. MS-PHS enforces energy balance and passivity by construction and provides calibrated uncertainty quantification. Theoretical analysis decomposes estimation error into statistical and discretization components.

## Method Summary
MS-PHS GP learns continuous-time port-Hamiltonian dynamics by placing a GP prior on the Hamiltonian surface H with a physics-informed kernel k_phs that encodes the interconnection-dissipation structure. Variable-step linear multistep integrators (vLMM) project trajectory data into linear constraints on the vector field f(x) = [J(x)-R(x)]∇H(x), which are applied as linear functionals to the GP prior. The method jointly infers the vector field and Hamiltonian surface through an augmented GP that includes both gradient-derived constraints and an anchor constraint H(0)=H₀ to fix the additive constant. Training optimizes the negative log marginal likelihood via Adam, yielding calibrated posteriors for both f* and H*.

## Key Results
- MS-PHS outperforms MS-ODE and GP-PHS baselines in vector-field recovery with better directional alignment (cosine distance)
- Uncertainty calibration is maintained with H_mse/σ²_H ratios near 1 across varying noise and sampling jitter levels
- Theoretical error decomposition shows estimation error bounded by statistical error plus discretization bias C_bias h^(2p+2)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Placing a GP prior on the Hamiltonian surface H and enforcing port-Hamiltonian structure through a physics-informed kernel yields models that satisfy passivity and energy balance by construction.
- **Mechanism:** The PHS kernel k_phs(x,x') = σ²_f J_R(x)∇_x∇_x' k_base(x,x') J_R(x')^T encodes the interconnection-dissipation structure directly into the covariance. Since GPs are closed under affine transformations, the induced prior on the vector field f(x) = [J(x)-R(x)]∇H(x) automatically inherits skew-symmetry of J and positive semi-definiteness of R, guaranteeing passivity without post-hoc constraints.
- **Core assumption:** The true system admits a port-Hamiltonian representation with known parametric structures J(·), R(·), G(·) but unknown parameters.
- **Evidence anchors:**
  - [abstract]: "By placing a GP prior on the Hamiltonian surface H and encoding variable-step multistep integrator constraints as finite linear functionals, MS-PHS GP enables closed-form conditioning of both the vector field and the Hamiltonian surface without latent states, while enforcing energy balance and passivity by design."
  - [section II-B.1]: "Under the PHS dynamics (10), closure of GPs under affine transformations induces the matrix-valued PHS kernel."
  - [corpus]: Related work on physics-informed GP dynamics (arXiv:2512.24493) confirms that Hamiltonian GP priors naturally encode energy constraints for safe control.
- **Break condition:** If the true dynamics cannot be expressed in port-Hamiltonian form (e.g., non-passive or non-energy-based physics), the structural inductive bias becomes a misspecification rather than a useful constraint.

### Mechanism 2
- **Claim:** Encoding variable-step multistep integration constraints as linear functionals enables exact GP inference directly from irregularly-sampled, noisy trajectory data without derivative preprocessing.
- **Mechanism:** Variable-step linear multistep methods (vLMM) express dynamics as AX = Bf(X) where A, B are coefficient matrices parameterized by step sizes h_k = t_{k+1} - t_k. Since GPs are closed under linear functionals, applying these operators to the GP prior preserves Gaussianity, yielding closed-form posteriors. The label vector Y = A_I X̃ transforms noisy state observations into constraints on the stacked vector field.
- **Core assumption:** The numerical integration scheme provides a sufficiently accurate approximation of the continuous-time dynamics (bounded truncation error).
- **Evidence anchors:**
  - [abstract]: "encoding variable-step multistep integrator constraints as finite linear functionals, MS-PHS GP enables closed-form conditioning of both the vector field and the Hamiltonian surface without latent states."
  - [section III-B.1]: "Multistep integrators of order greater than or equal to 1 (e.g., Adams-Bashforth/Moulton, BDF etc.) form updates from linear combinations of past states and vector-field evaluations. This class of numerical integrators yields the stacked linear constraint AX = Bf(X) which preserves Gaussianity under a GP prior on f."
  - [corpus]: Limited direct corpus evidence; the multistep-GP integration appears novel, though discrete gradient methods for PHS (arXiv:2505.18810) address related structure-preserving discretization.
- **Break condition:** If step sizes become too large relative to the dynamics' timescales, truncation error dominates (C_LTE h^{p+1}) and the discretization bias overwhelms statistical accuracy—adding more data cannot remove this bias (Proposition 1).

### Mechanism 3
- **Claim:** Joint inference over the vector field and Hamiltonian surface with an anchor constraint yields calibrated uncertainty that accurately tracks reconstruction error.
- **Mechanism:** The augmented GP combines scalar Hamiltonian observations (including the anchor H(0)=H_0) with gradient-derived vector field constraints through cross-covariance blocks K_{Hf} = J_R(x)∇_x k_base(0,x) B_I^T. This couples surface and gradient information in a single Gaussian posterior. Uncertainty propagates from noisy observations through the multistep projection to both f and H, preventing the variance decoupling that occurs when smoothing is separated from inference.
- **Core assumption:** At least one noiseless Hamiltonian value (anchor) is available to fix the arbitrary additive constant.
- **Evidence anchors:**
  - [section III-C]: "Since derivative information alone can recover the integrated surface only up to an arbitrary constant, anchoring H(0) = H_0 is essential to fix the additive constant of the GP."
  - [section IV-C, Fig. 5-6]: "MS-PHS-ab-3 exhibits posterior variance that accurately tracks the realized error... σ²_H rises in step with H_mse across noise levels, indicating that the posterior dispersion scales with the actual reconstruction difficulty." GP-PHS with separate smoothing shows H_mse/σ²_H > 1 (overconfident) while MS-PHS stays near 1.
  - [corpus]: Plug-and-Play PHS learning (arXiv:2504.17966) similarly emphasizes uncertainty-quantified models for reliable prediction, though without the multistep integration mechanism.
- **Break condition:** If the anchor H_0 is corrupted or if the cross-covariance construction mis-specified J_R(·) structure, Hamiltonian uncertainty will be systematically miscalibrated.

## Foundational Learning

- **Concept:** Gaussian Process regression and kernel methods
  - **Why needed here:** MS-PHS GP relies on understanding that GPs are closed under linear functionals, enabling the multistep projection to preserve tractable posteriors. Without this, the connection between trajectory constraints and vector field inference is opaque.
  - **Quick check question:** If f ~ GP(0, k) and y = L[f] for linear operator L, what is the distribution of y?

- **Concept:** Port-Hamiltonian systems (energy function H, interconnection J, dissipation R)
  - **Why needed here:** The entire method assumes the true dynamics fit the PHS form ẋ = [J(x)-R(x)]∇H(x) + G(x)u. Understanding why J must be skew-symmetric and R positive semi-definite is essential to interpret what "passivity by construction" means.
  - **Quick check question:** For a mass-spring system with kinetic energy T = ½mẋ² and potential V = ½kx², what are J, R, and H?

- **Concept:** Linear multistep methods (Adams-Bashforth, BDF, truncation error)
  - **Why needed here:** The vLMM scheme determines how trajectory data constraints the vector field. Understanding order-p accuracy and local truncation error bounds explains why higher-order integrators improve performance and why step size matters.
  - **Quick check question:** For forward Euler (order 1), what is the local truncation error scaling in terms of step size h?

## Architecture Onboarding

- **Component map:**
  - Noisy trajectory data D = {(x̃_k, u_k, t_k)} with observation noise σ²_x
  - Base kernel k_base(x,x') (squared exponential with ARD length-scales)
  - PHS kernel k_phs = σ²_f J_R(x) ∇_x∇_x' k_base(x,x') J_R(x')^T (encodes physics structure)
  - Multistep projection: A, B matrices from vLMM scheme → K_Y = B_I K_phs B_I^T
  - Joint posterior: Augmented covariance K_gg combining H-anchor and f-constraints
  - Output: Posterior mean and covariance for both vector field f* and Hamiltonian H*

- **Critical path:**
  1. Choose vLMM order (1-3) based on data density and smoothness assumptions
  2. Initialize ARD length-scales and noise hyperparameters
  3. Construct A_I, B_I from timestamps {t_k}
  4. Optimize negative log marginal likelihood (Eq. 5) via Adam
  5. Compute posterior mean/covariance for f* (Eq. 20-21) and H* (Eq. 28-29)

- **Design tradeoffs:**
  - **Integrator order:** Higher order (AB-3) reduces discretization bias C_bias E[h^{2p+2}] but requires more startup points and is sensitive to irregular step ratios
  - **Anchor placement:** Noiseless H_0 is assumed; in practice, noisy anchors require jitter ϵ_H for numerical stability
  - **Input noise handling:** Current formulation assumes noiseless GP inputs; heteroscedastic noise from noisy states in f(X) is not fully corrected (NIGP mentioned as remedy)

- **Failure signatures:**
  - **Overconfident uncertainty:** H_mse/σ²_H >> 1 indicates smoothing was decoupled from inference (Fig. 6)
  - **Directional misalignment:** High cosine distance in vector field indicates missing PHS structure (Table I: MS-ODE shows 0.042 on Duffing vs. 0.003 for MS-PHS)
  - **Posterior variance insensitive to noise:** If σ²_H doesn't scale with observation noise σ²_x, the multistep projection isn't propagating uncertainty correctly

- **First 3 experiments:**
  1. **Mass-spring baseline (linear, conservative):** Start with σ²_x = 0.01, regular sampling. Verify MS-PHS-ab-1 matches MS-ODE on simple dynamics; check that both recover correct frequency and near-zero cosine distance.
  2. **Van der Pol (nonlinear, dissipative):** Add observation noise σ²_x ∈ {0.01, 0.02, 0.05}. Compare MS-PHS-ab-3 vs. MS-ODE-ab-3 on directional alignment; expect PHS prior to reduce spurious rotations in sparse regions (Fig. 2).
  3. **Duffing with jitter (double-well, damped):** Introduce sampling jitter σ_j = 0.05. Compute H_mse/σ²_H ratio across noise levels. Target: ratio near 1 for MS-PHS (calibrated) vs. >> 1 for GP-PHS with separate smoothing (overconfident).

## Open Questions the Paper Calls Out

- **Question:** How can the MS-PHS framework be extended to rigorously account for heteroscedastic input noise (noisy state observations used in the kernel) without relying on the "noiseless input" approximation?
  - **Basis in paper:** [explicit] Remark III-B.1 explicitly states that the method assumes noiseless inputs to the GP to maintain closure, deferring more complex noisy-input treatments (like NIGP) to future work.
  - **Why unresolved:** The current formulation ignores the uncertainty in the training inputs (states $x_k$) to maintain standard GP inference, which may bias the posterior uncertainty calibration.
  - **What evidence would resolve it:** A modified MS-PHS likelihood that incorporates state uncertainty or a demonstration of error propagation techniques preserving the port-Hamiltonian structure.

- **Question:** Can the MS-PHS kernel scale to high-dimensional systems via sparse variational approximations while preserving the passivity guarantees and theoretical error bounds?
  - **Basis in paper:** [inferred] The empirical evaluation is restricted to low-dimensional canonical oscillators (2D state spaces), and the method relies on exact GP inference which scales cubically with data points.
  - **Why unresolved:** It is untested whether the specific spectral and passivity properties of the MS-PHS kernel hold under the inducing-point approximations required for high-dimensional robotics or power grid applications.
  - **What evidence would resolve it:** Experiments on systems with state dimension $n > 10$ utilizing sparse GP techniques, verifying that passivity and calibration are maintained.

- **Question:** How does the choice of variable-step integrator (e.g., implicit BDF vs. explicit Adams-Bashforth) impact the vector-field recovery error when identifying stiff port-Hamiltonian systems?
  - **Basis in paper:** [inferred] The method section mentions alternative integrators like Adam-Moulton and BDF, but experiments are limited to explicit Adams-Bashforth methods on non-stiff oscillators (Van der Pol, Duffing).
  - **Why unresolved:** It is unclear if the stated discretization error bounds or directional alignment benefits hold for implicit schemes required for stiff dynamics.
  - **What evidence would resolve it:** Comparative analysis of explicit vs. implicit MS-PHS kernels on stiff benchmarks, analyzing the trade-off between integrator stability and kernel conditioning.

- **Question:** How can the calibrated Hamiltonian posterior be formally integrated into robust control design (e.g., stochastic MPC) to guarantee closed-loop stability?
  - **Basis in paper:** [explicit] The conclusion states that MS-PHS is "especially attractive for downstream control tasks," but the work is limited to identification without demonstrating control applications.
  - **Why unresolved:** The paper provides the model and uncertainty bounds but does not derive the control laws or stability certificates that utilize the specific "well-calibrated" variance of the Hamiltonian.
  - **What evidence would resolve it:** Derivation of a controller that leverages the MS-PHS posterior variance to bound the probability of constraint violation or instability.

## Limitations

- **Structural misspecification risk:** The method assumes the true system admits a port-Hamiltonian representation. For systems without energy-based structure, the PHS kernel becomes misspecified, potentially degrading performance.
- **Numerical stability with noisy anchors:** While the method theoretically requires a noiseless anchor H(0)=H₀, real applications may have noisy measurements. The paper mentions adding jitter ε_H for numerical stability, but doesn't explore how this affects posterior calibration or Hamiltonian reconstruction accuracy.
- **Computational scaling:** The joint GP formulation requires inverting K_gg ∈ ℝ^(n+1)×(n+1), where n is the number of trajectory points. While the paper reports reasonable training times, scaling to high-dimensional systems or long trajectories remains unaddressed.

## Confidence

- **Vector field recovery:** High - MS-PHS consistently outperforms MS-ODE and GP-PHS baselines on cosine distance metrics across all benchmarks with substantial improvements on challenging Duffing oscillator.
- **Uncertainty calibration:** High - The calibration ratio H_mse/σ²_H remains near 1 across noise levels for MS-PHS, while GP-PHS with separate smoothing shows ratios >> 1, demonstrating joint inference prevents overconfidence.
- **Energy balance preservation:** High - By construction, the method enforces passivity through the PHS kernel structure. Theoretical analysis provides rigorous support for claimed error bounds.

## Next Checks

1. **Non-PHS system validation:** Apply MS-PHS to a system without port-Hamiltonian structure (e.g., Lorenz attractor) to quantify performance degradation and identify when the structural inductive bias becomes harmful rather than helpful.

2. **Anchor noise sensitivity:** Systematically vary the noise level on the anchor constraint H(0) and measure impact on Hamiltonian posterior calibration. Compare different jitter strategies (additive vs. multiplicative) and their effect on numerical stability.

3. **High-dimensional scaling:** Test MS-PHS on systems with state dimension d > 2 (e.g., 4-6 dimensional mechanical systems) to evaluate computational scaling and identify bottlenecks in the joint GP inversion. Measure how kernel hyperparameter optimization time scales with state dimension.