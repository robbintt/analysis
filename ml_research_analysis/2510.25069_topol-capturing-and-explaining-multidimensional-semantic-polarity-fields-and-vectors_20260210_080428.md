---
ver: rpa2
title: 'TOPol: Capturing and Explaining Multidimensional Semantic Polarity Fields
  and Vectors'
arxiv_id: '2510.25069'
source_url: https://arxiv.org/abs/2510.25069
tags:
- polarity
- semantic
- topol
- each
- vector
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TOPol addresses the limitation of traditional sentiment analysis
  by modeling semantic polarity as multidimensional vector fields rather than unidimensional
  scales. The framework uses transformer embeddings, UMAP dimensionality reduction,
  and Leiden clustering to detect topic-level semantic shifts across human-on-the-loop
  defined contextual boundaries.
---

# TOPol: Capturing and Explaining Multidimensional Semantic Polarity Fields and Vectors

## Quick Facts
- arXiv ID: 2510.25069
- Source URL: https://arxiv.org/abs/2510.25069
- Authors: Gabin Taibi; Lucia Gomez
- Reference count: 3
- Primary result: Models semantic polarity as multidimensional vector fields, detecting non-affective and affective shifts with statistical significance (p < 0.001) against random boundaries

## Executive Summary
TOPol introduces a framework for reconstructing multidimensional semantic polarity fields as vector displacements between topic centroids across human-defined contextual boundaries. Unlike traditional sentiment analysis that reduces polarity to unidimensional scales, TOPol captures fine-grained semantic shifts through transformer embeddings, UMAP dimensionality reduction, and Leiden clustering. The framework demonstrates that different domains exhibit distinct polarity field structures: sentiment-dominant domains show aligned vectors (cosine similarity 0.66), while multidimensional policy discourse shows dispersed fields (cosine similarity 0.12). Experiments on central bank speeches and Amazon reviews validate that TOPol reliably detects meaningful semantic changes while random boundary definitions produce incoherent fields.

## Method Summary
TOPol processes text corpora by first generating high-dimensional embeddings using a transformer-based language model, then applying UMAP to preserve local topological structures while reducing dimensionality for clustering. Leiden partitioning segments documents into topically coherent communities, which are then partitioned by contextual boundaries (CBs) defined by human-on-the-loop (HoTL) into two regimes. For each topic with sufficient documents in both regimes, TOPol computes centroid displacement vectors representing directional semantic change. These polarity vectors form a field structure that quantifies fine-grained semantic displacement. LLM-based contrastive explainability interprets the dimensions by comparing document neighborhoods at vector endpoints, producing interpretable labels with coverage estimates.

## Key Results
- Statistical significance: Observed drift magnitude significantly exceeds random CB baselines (p < 0.001, N=1000 permutations)
- Domain-specific structures: Cosine similarity of 0.66 for sentiment-dominant Amazon reviews vs 0.12 for multidimensional central bank policy discourse
- Robustness: Polarity field coherence preserved under embedding and clustering perturbations, with contextual boundary definition being the primary driver of variation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: UMAP projection preserves local topological structure, enabling semantically coherent topic clustering
- Mechanism: The tLLM produces high-dimensional embeddings where document proximity reflects meaning. UMAP non-linearly projects these to a lower-dimensional manifold while preserving neighborhood relationships, creating a space where Leiden clustering can partition documents into topically coherent communities
- Core assumption: Semantic similarity is locally preserved through dimensionality reduction; documents near each other in embedding space share topical content
- Evidence anchors: [abstract] "applies neighbor-tuned UMAP projection, and segments topics via Leiden partitioning"; [section: Methodology] "To address this, we apply UMAP to preserve local topological structures, narrative latent topics, by mapping each zi to a low-dimensional representation"
- Break condition: If UMAP parameters destroy local structure (e.g., too few neighbors), topic clusters become semantically incoherent

### Mechanism 2
- Claim: Polarity vectors computed as centroid displacements capture directional semantic change across contextual boundaries
- Mechanism: For each topic cluster Ti, documents are split by CB into subsets A and B. Centroids μA_i and μB_i are computed; the vector vi = μB_i - μA_i encodes magnitude and direction of semantic shift. Random CBs produce incoherent fields (low cosine similarity), while meaningful CBs produce aligned vectors
- Core assumption: Centroid displacement encodes interpretable semantic polarity; topics exist in both regimes with sufficient documents for stable centroids
- Evidence anchors: [abstract] "computes directional vectors between corresponding topic-boundary centroids, yielding a polarity field that quantifies fine-grained semantic displacement"; [section: Experiments] "the observed average magnitude significantly exceeds that of the random CBs, yielding p-values of 0.000999"
- Break condition: Highly imbalanced or missing topics across regimes cause topic dropout; centroids become unstable with few documents

### Mechanism 3
- Claim: LLM-based contrastive explainability interprets polarity dimensions by comparing document neighborhoods at vector endpoints
- Mechanism: For each polarity vector, the top-n documents nearest to each centroid are extracted. An LLM (gemini-2.5-flash with 1M token context) is prompted to identify semantic distinctions, producing labels, coverage estimates, and exemplar sentences
- Core assumption: Documents near centroids represent the dominant semantic content of each regime; LLMs can reliably identify contrastive dimensions without hallucination
- Evidence anchors: [abstract] "To interpret identified polarity vectors, the tLLM compares their extreme points and produces contrastive labels with estimated coverage"; [section: Revealing TOPol Dimensions] "The ratio of supporting to contradicting sentences served as a proxy for interpretability reliability"
- Break condition: LLM hallucinations; empty responses when no coherent signal exists; non-deterministic outputs reduce reproducibility

## Foundational Learning

- Concept: **Manifold learning (UMAP)**
  - Why needed here: TOPol relies on UMAP to reduce high-dimensional embeddings while preserving local semantic neighborhoods for clustering
  - Quick check question: Can you explain why preserving local structure matters for downstream clustering quality?

- Concept: **Community detection (Leiden algorithm)**
  - Why needed here: Leiden partitions the UMAP manifold into topically coherent clusters with guaranteed connectivity
  - Quick check question: What does the resolution parameter control, and how does lowering it affect cluster granularity?

- Concept: **Vector fields and centroid displacement**
  - Why needed here: Polarity vectors are computed as centroid differences; the field structure encodes magnitude and directional alignment of semantic change
  - Quick check question: How would you interpret high vs. low pairwise cosine similarity among polarity vectors?

## Architecture Onboarding

- Component map: Corpus → tLLM embeddings (text-embedding-3-small) → UMAP projection → Leiden clustering → CB partitioning per cluster → Centroid computation → Polarity vectors → LLM explanation (gemini-2.5-flash)
- Critical path: **Contextual Boundary definition** is the primary driver of output variation. HoTL-defined CBs must reflect meaningful regime splits; random CBs produce incoherent fields
- Design tradeoffs:
  - UMAP dimensions (d=50 default): Higher dims capture more variance but risk noise
  - UMAP neighbors (k=100 default): Larger neighborhoods smooth topology but may blur fine distinctions
  - Leiden resolution (r=1.5 default): Lower resolution produces coarser clusters, potentially diluting multidimensional structure
- Failure signatures:
  - Low average pairwise cosine similarity (<0.1) suggests dispersed, unaligned polarity shifts (expected for multidimensional discourse like central bank speeches)
  - High similarity between HoTL and random CB metrics indicates CB is not meaningfully defined
  - Empty LLM responses or high contradicting-to-supporting sentence ratios signal unreliable explanations
- First 3 experiments:
  1. **Random CB baseline**: Run TOPOL with randomly permuted CB labels; verify that drift magnitude (m̄) and cosine similarity (s̄) drop significantly vs. HoTL-defined CB
  2. **UMAP perturbation**: Increase projection dimensionality (d=75) and neighborhood size (k=150); confirm vector field stability via m̄ and s̄ comparison
  3. **Sentiment benchmark**: On a sentiment-rich corpus (e.g., Amazon reviews), validate that TOPOL polarity dimensions align with NRC-VAD valence shifts using the community integration method

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the framework be extended beyond binary regime comparisons to handle sequential or parallel contextual boundaries for multi-point temporal analysis?
- Basis in paper: [explicit] The authors state that "Future extensions will explore sequential or parallel contextual boundaries, enabling multi-point temporal analysis or multidimensional polarity fields over 2D discourse partitions."
- Why unresolved: The current implementation is restricted to calculating vector displacements between two disjoint subsets (A and B), limiting the analysis to single-step regime shifts
- What evidence would resolve it: A demonstration of TOPol reconstructing trajectory fields across $t_0 \to t_1 \to t_2$ or complex 2D partitions without losing the vector field's geometric interpretability

### Open Question 2
- Question: How can the contrastive explainability layer be modified to mitigate the non-determinism and hallucination risks inherent in generative LLMs?
- Basis in paper: [explicit] The paper notes that "the non-deterministic nature of LLMs makes this part of the framework difficult to reproduce, and their outputs may be prone to hallucinations despite prompting safeguards."
- Why unresolved: The interpretation of polarity dimensions relies on stochastic generative models, creating a reproducibility bottleneck in the pipeline
- What evidence would resolve it: The integration of deterministic decoding strategies or ensemble methods that yield consistent polarity labels across repeated runs on the same data

### Open Question 3
- Question: What methodological refinements are required to robustly include topic clusters that are unbalanced or absent in one discourse regime?
- Basis in paper: [explicit] The authors identify as a limitation the "exclusion of topic clusters that are unbalanced or entirely absent from one regime, which may lead to underutilized polarity vectors or topic-level dropout."
- Why unresolved: The current centroid calculation method requires paired subsets, forcing the framework to discard topics that do not appear in both regimes
- What evidence would resolve it: An imputation technique or asymmetric vector calculation that retains semantic information from single-regime topics within the polarity field

### Open Question 4
- Question: How can the semantic validity of non-affective polarity vectors (e.g., "confidence-doubt") be independently verified without relying on the generative LLM used for interpretation?
- Basis in paper: [inferred] While the paper benchmarks against NRC valence for sentiment, the validation of non-affective shifts relies primarily on the internal coherence of the LLM's contrastive explanations, risking circularity
- Why unresolved: There is no external "ground truth" provided for abstract semantic dimensions, making it difficult to confirm if the vector directions correspond to actual semantic shifts perceived by humans
- What evidence would resolve it: A human annotation study confirming that the vector directions correlate with expert-assessed semantic changes in the source documents

## Limitations
- Contextual boundary definition sensitivity: Output quality depends entirely on human-on-the-loop judgments about regime splits
- LLM interpretability: Gemini-2.5-flash explanations are non-deterministic and lack ground truth validation
- Centroid stability: Framework assumes sufficient document counts in both regimes; topic dropout occurs when clusters are unbalanced or missing entirely in one regime

## Confidence
- **High Confidence**: Mathematical framework for computing polarity vectors and statistical validation against random CB baselines
- **Medium Confidence**: UMAP-based dimensionality reduction and Leiden clustering produce semantically coherent topics; statistical significance of observed shifts is established
- **Low Confidence**: LLM-based contrastive explanations lack ground truth validation and are inherently non-deterministic

## Next Checks
1. **Automated CB Validation Framework**: Implement objective metric for contextual boundary quality that doesn't rely on human judgment
2. **LLM Explanation Benchmarking**: Create labeled test corpus with known polarity dimensions to evaluate consistency and accuracy of gemini-2.5-flash explanations
3. **Cross-Domain Robustness Testing**: Apply TOPol to at least three additional domains to validate domain-specific cosine similarity patterns across diverse discourse types