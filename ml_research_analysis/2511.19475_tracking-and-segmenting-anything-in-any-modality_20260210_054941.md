---
ver: rpa2
title: Tracking and Segmenting Anything in Any Modality
arxiv_id: '2511.19475'
source_url: https://arxiv.org/abs/2511.19475
tags:
- tracking
- unified
- sata
- object
- modality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SATA, the first unified framework for tracking
  and segmentation that works with any input modality (RGB, RGB-T, RGB-D, RGB-E) and
  supports multi-task joint prediction (SOT, VOS, MOT, MOTS). The key innovation is
  a Decoupled Mixture-of-Expert (DeMoE) mechanism that learns both cross-modal shared
  knowledge and modality-specific information to bridge distribution and feature representation
  gaps.
---

# Tracking and Segmenting Anything in Any Modality

## Quick Facts
- **arXiv ID**: 2511.19475
- **Source URL**: https://arxiv.org/abs/2511.19475
- **Reference count**: 26
- **Primary result**: SATA achieves state-of-the-art performance across 18 benchmarks using a single unified model for SOT, VOS, MOT, and MOTS across RGB, RGB-T, RGB-D, and RGB-E modalities.

## Executive Summary
This paper introduces SATA, the first unified framework for tracking and segmentation that works with any input modality (RGB, RGB-T, RGB-D, RGB-E) and supports multi-task joint prediction (SOT, VOS, MOT, MOTS). The key innovation is a Decoupled Mixture-of-Expert (DeMoE) mechanism that learns both cross-modal shared knowledge and modality-specific information to bridge distribution and feature representation gaps. Additionally, a Task-aware MOT (TaMOT) pipeline unifies all task outputs into a single instance set with calibrated IDs, preventing task-specific knowledge degradation during multi-task training. SATA achieves state-of-the-art performance across 18 benchmarks, notably scoring 81.3% AO on GOT10K, 77.8% PR on LasHeR, 71.4% J&F on LLE-VOS, 59.7% HOTA on UniRTL, and 38.1% mMOTSA on BDD MOTS, all using a single model and parameter set.

## Method Summary
SATA builds upon SAM2's architecture with a HiViT-L backbone (214M parameters) and introduces a Decoupled Mixture-of-Expert (DeMoE) block that replaces the FFN layers. DeMoE consists of a Common-prompt MoE (CpMoE) with one frozen shared expert and four common experts, plus a Specific-activated MoE (SaMoE) with four specific experts per modality. The framework employs a Task-aware MOT (TaMOT) pipeline that reformulates all tracking and segmentation tasks as candidate generation + temporal association, using a Candidates Generation Module (CGM) based on SAM2's mask decoder and a Memory-enhanced Module (MEM) for fine-grained instance embeddings. Training follows a two-stage process: Stage I trains a detection head on COCO/UniTRL (180k iterations), while Stage II jointly trains DeMoE, CGM, and MEM on a mixture of SOT/VOS and MOT/MOTS datasets (360k iterations, 0.6:0.4 sampling ratio).

## Key Results
- Achieves 81.3% AO on GOT10K (SOT benchmark)
- Achieves 77.8% PR on LasHeR (RGB-T benchmark)
- Achieves 71.4% J&F on LLE-VOS (VOS benchmark)
- Achieves 59.7% HOTA on UniRTL (MOT benchmark)
- Achieves 38.1% mMOTSA on BDD MOTS (MOTS benchmark)
- All results achieved with a single unified model and parameter set

## Why This Works (Mechanism)

### Mechanism 1
The Decoupled Mixture-of-Expert (DeMoE) mechanism enables effective cross-modal knowledge sharing by explicitly separating modality-common and modality-specific representations. DeMoE replaces FFN layers with two parallel expert systems: (1) CpMoE with a frozen shared expert (preserving SAM2 pre-trained knowledge) and trainable common experts that generate prompts via element-wise multiplication of RGB and TDE (thermal/depth/event) features; (2) SaMoE with modality-specific experts activated by cross-modal routing. The unified representation combines both via residual connections (Eq. 6-7). Core assumption: Modality-common and modality-specific information are separable and complementary; freezing the shared expert prevents catastrophic forgetting of pre-trained knowledge. Evidence anchors: [abstract] claims cross-modal shared knowledge learning; [Page 4, Eq. 1-7] details formulation; [Page 7, Table 5] shows ablation degradation when removing either CpMoE or SaMoE. Break condition: If modalities are too disparate (e.g., audio vs. visual), the element-wise prompt multiplication may fail to capture meaningful cross-modal correlations.

### Mechanism 2
Cross-modal complementary and orthogonal losses prevent expert collapse and ensure diverse, complementary feature learning. L_CM (Eq. 8) masks one modality and reconstructs it from the other via MSE, forcing experts to learn complementary information. L_CE (Eq. 9) applies orthogonal projection loss between common and specific experts to prevent functional overlap. The combined L_MoE = μL_CM + λL_CE jointly optimizes both objectives. Core assumption: Masked cross-modal reconstruction is a viable proxy for learning complementary representations; orthogonal expert outputs correspond to functionally distinct capabilities. Evidence anchors: [Page 4, Eq. 8-9] provides mathematical formulation; [Page 7, Table 5, "W/o L_MoE"] shows consistent degradation across all benchmarks without MoE loss (e.g., 81.3→80.7 AO on GOT10K, 59.7→58.4 HOTA on UniRTL); [corpus] shows weak direct evidence—no neighbor papers explicitly use this dual-loss MoE formulation for multi-modal tracking. Break condition: If the masking ratio is too aggressive or if modalities lack sufficient correlation, L_CM may dominate and force trivial solutions.

### Mechanism 3
The Task-aware MOT (TaMOT) pipeline unifies heterogeneous tasks (SOT/VOS/MOT/MOTS) under a single MOT paradigm with calibrated IDs, preventing task-specific knowledge degradation. All tasks are reformulated as candidate generation + temporal association. For SOT/VOS, initial masks/boxes generate distractors as additional candidates (affinity > τ_mask=0.7). For MOT/MOTS, an added detection head predicts all objects. The Memory-enhanced Module (MEM) maintains fine-grained instance embeddings (using mask-weighted RoI features) and spatiotemporal tracklets via Q-Former-style learned queries. Bi-softmax matching with Hungarian algorithm produces final associations. Core assumption: Task differences can be abstracted to candidate generation priors while sharing a common temporal association backbone; maintaining distractors improves SOT/VOS robustness. Evidence anchors: [abstract] claims unified set of instances with calibrated ID information; [Page 5-6] details CGM and MEM architecture; [Page 7, Table 5] shows ablation performance collapse when removing MEM (e.g., 38.1→29.1 mMOTSA on BDD MOTS); [corpus] neighbor paper "SAM2RL" explores SAM 2 memory control with RL, suggesting memory design is critical for temporal consistency. Break condition: If candidate generation is too noisy (e.g., excessive false positives), the association module may be overwhelmed; if too restrictive, true targets may be missed early in the pipeline.

## Foundational Learning

- **Concept: Mixture-of-Experts (MoE) with Sparse Gating**
  - Why needed here: DeMoE relies on MoE principles—understanding router networks, expert specialization, and top-K activation is essential to grasp how SATA balances shared vs. modality-specific knowledge.
  - Quick check question: Can you explain why sparse gating (top-K activation) helps prevent expert collapse compared to dense averaging?

- **Concept: Foundation Model Initialization (SAM2 Architecture)**
  - Why needed here: SATA builds on SAM2's encoder-decoder structure, memory bank, and mask decoder. Understanding prompt encoding, memory attention, and mask prediction is prerequisite to comprehending CGM modifications.
  - Quick check question: Describe the role of SAM2's memory bank and how it differs from the MEM module in SATA.

- **Concept: Multi-object Tracking (Tracking-by-Detection Paradigm)**
  - Why needed here: TaMOT reformulates all tasks under MOT's detection-association framework. Concepts like affinity matrices, Hungarian matching, and IDF1/HOTA metrics are central.
  - Quick check question: How does the tracking-by-detection paradigm differ from tracking-by-query, and which does SATA employ?

## Architecture Onboarding

- **Component map**: Multi-modal input → HiViT encoder → DeMoE (unified embedding F^U) → CGM (generates candidates) → MEM (extracts embeddings and tracklets) → Instance matching (Hungarian algorithm) → Output unified instance set

- **Critical path**: 1) Multi-modal input → HiViT encoder → DeMoE (unified embedding F^U); 2) CGM generates candidates (M per frame) based on task priors; 3) MEM extracts fine-grained embeddings and maintains tracklets; 4) Instance matching associates candidates ↔ tracklets; 5) Output: unified instance set with calibrated IDs

- **Design tradeoffs**: Single shared model vs. task-specific models: ablation (Table 9) shows unified training outperforms separate models across all tasks, but efficiency suffers ("lacks interaction between objects" during multi-object tracking); Frozen shared expert vs. fully trainable: freezing preserves SAM2 knowledge but may limit adaptation, mitigated by common expert prompts; Joint training vs. multi-stage training: joint training (0.6:0.4 SOT/VOS : MOT/MOTS sampling) outperforms staged approaches (Table 9), but requires careful data balancing

- **Failure signatures**: Degrading multi-modal performance: if PR drops on LasHeR/DepthTrack/VisEvent, check L_MoE loss weighting (μ, λ) or expert activation patterns; ID switches in MOT/MOTS: if IDF1 or mIDF1 degrades, inspect MEM tracklet length (T) or matching threshold (τ_th); Poor cross-modal generalization: if RGB-only SOT works but RGB-T/RGB-D/RGB-E fail, verify cross-modal router (R_CM) routing accuracy

- **First 3 experiments**: 1) DeMoE ablation: remove CpMoE, SaMoE, and L_MoE individually on GOT10K/LasHeR/LLE-VOS/UniRTL/BDD MOTS; expect ~1-4% AO/HOTA drops as in Table 5; 2) Unified vs. separate training: compare SATA against separate SOT/VOS and MOT/MOTS models; expect 2-5% gains per Table 9; 3) Cross-modal transfer test: train on RGB+RGB-T data only, test on RGB-D/RGB-E; assess zero-shot generalization to unseen modalities

## Open Questions the Paper Calls Out
The paper's "Limitation" section explicitly states that the current strategy tracks and segments objects separately, which "lacks interaction between objects and affects the efficiency of the model to some extent." This limitation suggests that the unified framework could be further optimized to allow for interaction between multiple targets during tracking and segmentation, potentially improving both computational efficiency and accuracy through joint processing of object relationships.

## Limitations
- The unified framework sacrifices task-specific optimality for model simplicity, lacking interaction between multiple targets during multi-object tracking which affects efficiency
- The claim of "any modality" support is primarily demonstrated on four modalities (RGB, RGB-T, RGB-D, RGB-E), with no explicit testing on truly novel modalities like LiDAR or SAR
- Freezing the shared expert in DeMoE may constrain adaptation to non-RGB modalities that have different statistical distributions from the SAM2 pre-training data

## Confidence
- **High confidence**: Cross-modal performance improvements with DeMoE (verified by ablation Table 5 showing consistent gains when adding CpMoE, SaMoE, and L_MoE across all benchmarks)
- **Medium confidence**: Unified framework outperforming separate models (supported by Table 9 but with efficiency trade-offs noted)
- **Medium confidence**: State-of-the-art claims (based on reported metrics, but no public model/code for independent verification yet)

## Next Checks
1. **Zero-shot cross-modal transfer test**: Train on RGB+RGB-T data only, then evaluate on unseen RGB-D and RGB-E datasets to validate true modality generalization claims.
2. **Modality collapse monitoring**: Implement logging of DeMoE router activation patterns during training to verify that specific experts are properly activated across different input modalities and don't collapse to uniform behavior.
3. **Memory efficiency analysis**: Profile GPU memory usage during multi-object tracking with varying tracklet lengths (T) and candidate counts (M) to quantify the scalability limits of the MEM module for real-world deployment.