---
ver: rpa2
title: LLM-Guided Task- and Affordance-Level Exploration in Reinforcement Learning
arxiv_id: '2509.16615'
source_url: https://arxiv.org/abs/2509.16615
tags:
- learning
- exploration
- arxiv
- llm-tale
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LLM-TALE is a framework that uses large language models to guide
  reinforcement learning exploration at both task and affordance levels. The method
  generates task-level plans from language instructions and decomposes them into semantically
  meaningful affordances, enabling multimodal exploration.
---

# LLM-Guided Task- and Affordance-Level Exploration in Reinforcement Learning

## Quick Facts
- arXiv ID: 2509.16615
- Source URL: https://arxiv.org/abs/2509.16615
- Authors: Jelle Luijkx; Runyu Ma; Zlatan Ajanović; Jens Kober
- Reference count: 40
- Primary result: LLM-TALE achieves 93.3% success rate in real-robot pick-and-place tasks with zero-shot sim-to-real transfer, outperforming LLM-only controllers and strong RL baselines.

## Executive Summary
LLM-TALE is a framework that uses large language models to guide reinforcement learning exploration at both task and affordance levels. The method generates task-level plans from language instructions and decomposes them into semantically meaningful affordances, enabling multimodal exploration. A goal-conditioned residual policy learns to correct suboptimality in LLM-generated plans while the base policy drives exploration toward semantically meaningful regions. The framework improves sample efficiency by directing exploration through intrinsic rewards and value-guided affordance selection. Evaluated on six pick-and-place tasks from RLBench and ManiSkill, LLM-TALE achieves higher success rates and faster learning compared to strong baselines including Text2Reward and RLPD with demonstrations.

## Method Summary
LLM-TALE combines LLM planning with goal-conditioned residual reinforcement learning. The system first queries an LLM to decompose language instructions into task-level primitives (e.g., "pick", "transport") using prompt T. For each primitive, the LLM identifies affordance modalities (e.g., "pick from top", "pick from side") via prompt M, then generates specific affordance plans via prompt P. These plans are parsed into SE(3) goals that condition a PD controller (base policy). A goal-conditioned residual policy learns corrections to the LLM-generated actions, with exploration guided by intrinsic rewards (pose error + velocity penalties) and sparse external rewards. Affordance selection uses value estimates weighted by uncertainty to balance exploitation and exploration across modalities. The framework trains entirely from scratch without human demonstrations.

## Key Results
- Achieves 93.3% success rate on real-robot pick-and-place tasks with zero-shot sim-to-real transfer
- Outperforms LLM-only controllers (0% success due to collisions) and strong RL baselines including Text2Reward and RLPD with demonstrations
- Demonstrates faster learning and higher final success rates across all six tasks (PickCube, StackCube, PegInsert, TakeLid, OpenDrawer, PutBox) from RLBench and ManiSkill
- Shows effective multimodal affordance exploration, with agents learning to prefer high-value grasp modes (e.g., head grasps for PegInsert) while avoiding detrimental modes (e.g., top picks for PutBox)

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Semantic-to-Physical Decomposition
Guiding exploration through LLM-generated task and affordance abstractions improves sample efficiency over direct action-space exploration. LLM first decomposes language instructions into task-level primitives, then generates multiple affordance-level goal candidates. These goals are parsed into SE(3) poses that condition a base PD controller. The RL policy learns residual corrections in this semantically-biased action space. Core assumption: LLMs are more reliable at abstract planning levels than low-level control; affordances that are semantically plausible include at least one physically feasible mode.

### Mechanism 2: Goal-Conditioned Residual Policy Learning
A residual action formulation allows the RL agent to correct LLM suboptimality while maintaining exploration in semantically meaningful regions. The executed action is a = a_p + a_re, where a_p comes from a PD controller tracking LLM-generated goals and a_re is learned via RL. Dense intrinsic rewards guide the residual policy toward goal achievement while sparse external rewards signal task completion. Core assumption: The base policy provides a useful initialization that keeps exploration in productive regions; residual corrections are sufficient to overcome LLM planning errors.

### Mechanism 3: Value- and Uncertainty-Guided Multimodal Affordance Selection
Combining critic value estimates with uncertainty tracking enables adaptive selection among affordance modalities, avoiding fruitless exploration of infeasible options. For each primitive with m affordance modes, goal selection probability is p_sel(i) ∝ exp(βV(s, g_i)) * c_i, where c_i is an uncertainty term that decays when a mode is selected. Higher-value modes are exploited while uncertainty maintains exploration of less-visited alternatives. Core assumption: Value function approximations correlate with actual affordance feasibility; uncertainty decay rate is appropriately tuned.

## Foundational Learning

- **Goal-Conditioned Reinforcement Learning**
  - Why needed here: The framework conditions both value functions and policies on LLM-generated affordance goals; understanding how goals modulate RL is essential.
  - Quick check question: Can you explain how a goal-conditioned value function V(s, g) differs from an unconditional value function V(s)?

- **Residual Policy Learning**
  - Why needed here: LLM-TALE's core innovation is learning a residual policy on top of a base controller; understanding this decomposition is critical.
  - Quick check question: Given a base action a_p and residual action a_re, what happens to the learning signal if a_p is already near-optimal vs. if a_p is highly suboptimal?

- **Intrinsic Motivation / Reward Shaping**
  - Why needed here: The method uses dense intrinsic rewards (pose error, velocity penalties) to guide learning under sparse external rewards.
  - Quick check question: Why can naive intrinsic rewards lead to "noisy-TV" failures, and how does goal-conditioning mitigate this?

## Architecture Onboarding

- **Component map**: Task Planner (LLM + prompt T) -> Affordance Identifier (LLM + prompt M) -> Affordance Planner (LLM + prompt P) -> Goal Parser -> Base Policy (PD Controller) -> Residual Policy π_θ -> Value Function V_ϕ -> Uncertainty Tracker c

- **Critical path**: LLM planning (once, pre-training) -> Goal parsing (per-episode) -> Affordance selection via value + uncertainty (per-primitive) -> Base + residual action execution -> Intrinsic + extrinsic reward -> Policy/value update

- **Design tradeoffs**:
  - PD controller gain tuning: Higher gains = faster motion but less time for residual corrections; lower gains = smoother but slower
  - Temperature β: Higher = more exploitation of high-value affordances; lower = more uniform exploration
  - Uncertainty decay α: Faster decay = quicker commitment to modes; slower = prolonged exploration
  - On-policy (PPO) vs. off-policy (TD3): TD3 generally more sample-efficient; PPO simpler to tune

- **Failure signatures**:
  - LLM generates physically infeasible affordances for all modes -> exploration never finds solution (check affordance outputs manually)
  - Value function collapses or diverges -> affordance selection becomes random (check value magnitudes during training)
  - Residual policy never improves -> base policy may already be near-optimal OR residual action scale too small (check residual action norms)
  - Collision-heavy trajectories in deployment -> sim-to-real gap in dynamics or perception (check trajectory visualization)

- **First 3 experiments**:
  1. Validate planning pipeline in isolation: Run LLM prompts on 2-3 tasks, manually inspect that at least one affordance mode per primitive is physically reasonable; verify goal parsing produces valid SE(3) poses.
  2. Ablate affordance selection: Compare full value+uncertainty selection vs. random affordance selection vs. always-first selection; expect degraded sample efficiency without guidance.
  3. Compare base policy only vs. full residual learning: Run PD controller alone (no residual) and measure success rate; then add residual learning and verify improvement emerges within expected sample budget.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLM-TALE be extended to handle objects with complex geometric relationships such as articulated or deformable items?
- Basis in paper: The current planning framework does not handle objects with complex geometry and is limited to simpler items such as cubes, boxes, and drawer handles
- Why unresolved: The affordance representation (position, EE axes) assumes rigid objects with simple geometry; more complex objects require richer affordance specifications the current framework does not support.
- What evidence would resolve it: Successful extension to tasks involving articulated objects (e.g., doors with handles) or deformable objects (e.g., cloth folding) with comparable sample efficiency.

### Open Question 2
- Question: Can LLM-TALE operate effectively without explicit object state information using only learned perception?
- Basis in paper: The method also requires access to object state, which requires object state estimators for real-world deployment
- Why unresolved: Current affordance goals are derived from known object poses; removing this requirement while maintaining semantically meaningful exploration guidance is non-trivial.
- What evidence would resolve it: Integration with manipulation foundation models (e.g., AnyGrasp) demonstrating comparable performance without explicit object state access.

### Open Question 3
- Question: How well does LLM-TALE generalize to manipulation tasks beyond pick-and-place operations?
- Basis in paper: All six evaluation tasks are pick-and-place variants, and the framework centers on only "pick" and "transport" primitives
- Why unresolved: The affordance representation is tailored to grasping and placing; other manipulation types may require fundamentally different affordance specifications.
- What evidence would resolve it: Evaluation on diverse manipulation tasks (e.g., tool use, assembly, contact-rich manipulation) with task-appropriate primitive and affordance definitions.

### Open Question 4
- Question: Would dynamic LLM re-querying during training improve performance when initial plans are severely suboptimal?
- Basis in paper: We query the LLM before training and cache the outputs...for reuse during training - plans are generated once, requiring the residual policy to fully compensate for LLM errors
- Why unresolved: If the LLM generates fundamentally incorrect affordance modalities, no amount of residual correction may succeed.
- What evidence would resolve it: Ablation comparing static versus periodic re-planning on tasks with deliberately misleading initial LLM plans, measuring convergence speed and final success rate.

## Limitations
- Framework limited to objects with simple geometry (cubes, boxes, drawer handles) and cannot handle articulated or deformable items
- Requires explicit object state information, necessitating object state estimators for real-world deployment
- Only evaluated on pick-and-place tasks; generalizability to other manipulation tasks unknown

## Confidence
- High confidence in hierarchical semantic decomposition mechanism (supported by ablation results and corpus precedent)
- Medium confidence in residual policy effectiveness (real-robot improvement shown but sim-to-real gap not characterized)
- Low confidence in generalizability to non-pick-and-place tasks (method evaluated only on 6 structurally similar tasks)

## Next Checks
1. Ablation study validation: Replicate value-guided vs. random affordance selection ablation; verify reported sample efficiency gains hold across all 6 tasks
2. Sim-to-real gap characterization: Run additional real-robot trials with varying initial conditions and object placements; report success rate variance and failure mode distribution
3. Prompt template verification: Implement LLM planning pipeline with minimal prompts (e.g., "Plan task: {instruction}") and verify the method degrades or fails, establishing baseline prompt sensitivity