---
ver: rpa2
title: FormationEval, an open multiple-choice benchmark for petroleum geoscience
arxiv_id: '2601.02158'
source_url: https://arxiv.org/abs/2601.02158
tags:
- questions
- accuracy
- https
- arxiv
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FormationEval is a 505-question multiple-choice benchmark for evaluating
  language models on petroleum geoscience and subsurface disciplines. The benchmark
  covers seven domains including petrophysics, petroleum geology, and reservoir engineering,
  with questions derived from authoritative sources using a concept-based methodology
  that avoids verbatim copying of copyrighted text.
---

# FormationEval, an open multiple-choice benchmark for petroleum geoscience

## Quick Facts
- arXiv ID: 2601.02158
- Source URL: https://arxiv.org/abs/2601.02158
- Authors: Almaz Ermilov
- Reference count: 40
- Primary result: 505-question multiple-choice benchmark for petroleum geoscience with 72 models evaluated, top performers achieving over 97% accuracy

## Executive Summary
FormationEval is a comprehensive benchmark designed to evaluate language models on petroleum geoscience and subsurface disciplines. The benchmark comprises 505 questions across seven domains including petrophysics, petroleum geology, and reservoir engineering, with questions derived from authoritative sources using a concept-based methodology that avoids verbatim copying of copyrighted text. Each question includes source metadata for traceability and audit. The evaluation covers 72 models from major providers, with top performers achieving over 97% accuracy. Gemini 3 Pro Preview reaches 99.8% accuracy, while GLM-4.7 leads among open-weight models at 98.6%. The performance gap between open-weight and closed models is narrower than expected, with several lower-cost open-weight models exceeding 90% accuracy. Petrophysics emerges as the most challenging domain across all models.

## Method Summary
The benchmark was constructed through a multi-stage process involving concept-based question generation from three authoritative sources, bias mitigation, and evaluation across 72 models via API calls. Questions were generated using a reasoning model (GPT-5.2) that processed source chapters through extended chain-of-thought to extract concepts and formulate original questions. The evaluation pipeline involved API providers (Azure OpenAI, OpenRouter), response caching, answer extraction using regex patterns, and analysis of accuracy, bias, and domain breakdown. The dataset and evaluation code are publicly available for reproducibility.

## Key Results
- Gemini 3 Pro Preview achieves 99.8% accuracy, GLM-4.7 leads open-weight models at 98.6%
- Performance gap between open-weight and closed models narrower than expected, several open-weight models exceed 90% accuracy
- Petrophysics identified as most challenging domain across all models
- Residual length bias persists at 43.2% (vs 25% expected), with mitigation strategies documented

## Why This Works (Mechanism)

### Mechanism 1: Concept-Based Derivation Prevents Memorization Gaming
Questions generated from extracted concepts rather than source text paraphrasing test understanding rather than pattern recognition. A reasoning model (GPT-5.2) processes source chapters through extended chain-of-thought, extracting concepts and formulating original questions that require domain knowledge to answer without access to source material. This mechanism assumes models with genuine domain understanding will outperform those relying on surface-level text matching.

### Mechanism 2: Domain Stratification Exposes Knowledge Gaps
Performance variation across domains reveals genuine knowledge gaps, with petrophysics consistently harder due to integrated physics-geology reasoning. Seven domains with varying conceptual complexity require different levels of understanding, with petrophysics demanding integration of multiple concept types including well logging physics and formation evaluation.

### Mechanism 3: Bias Mitigation Reduces Statistical Gaming
Identifying and correcting length bias and qualifier-word patterns forces models toward reasoning rather than heuristics. Initial analysis showed correct answers uniquely longest in >55% of questions; mitigation expanded distractors and balanced hedging language, reducing to 43.2%. This mechanism assumes models exploit statistical patterns, and reducing these forces genuine knowledge application.

## Foundational Learning

- Concept: Multiple-choice benchmark validity threats
  - Why needed here: Length bias, position bias, and qualifier-word patterns can inflate scores without testing knowledge
  - Quick check question: If correct answers are longest 43.2% of the time (vs. 25% expected), what accuracy gain could a length-exploiting strategy achieve?

- Concept: Contamination risk in evaluation
  - Why needed here: Questions derived from textbooks may overlap with training data, inflating apparent capability
  - Quick check question: How would you interpret a model's 99.8% accuracy if 80% of questions were labeled "high contamination risk"?

- Concept: Open-weight vs. closed model tradeoffs
  - Why needed here: The paper shows narrower-than-expected gaps (GLM-4.7 at 98.6% vs. Gemini 3 Pro at 99.8%), affecting deployment decisions
  - Quick check question: Beyond accuracy, what operational factors (latency, cost, data privacy) would influence choosing an open-weight model?

## Architecture Onboarding

- Component map:
  Generation pipeline: Source PDFs → OCR (Mistral) → Chapter chunks → GPT-5.2 MCQ generation → Schema validation → Human spot-check → Dataset v0.1 (505 questions)
  Evaluation pipeline: Benchmark + model configs → API providers (Azure OpenAI, OpenRouter) → Response caching → Answer extraction (regex + reasoning tag removal) → Analysis (accuracy, bias, domain breakdown)

- Critical path:
  1. Concept-based generation must avoid verbatim copying to maintain copyright compliance and test understanding
  2. Bias mitigation must be applied before release to prevent gaming
  3. Answer extraction must handle varied formats (reasoning traces, explicit patterns) or count failures as incorrect

- Design tradeoffs:
  - Copyright compliance vs. fidelity: concept-based approach may miss nuanced formulations from sources
  - Domain balance vs. source availability: petrophysics dominates (54%) due to textbook depth
  - Bias elimination vs. practical thresholds: 43.2% length bias remains; complete elimination would require extensive rewrites

- Failure signatures:
  - Models selecting longest answers at >50% rate indicate length exploitation
  - Position bias (non-uniform A/B/C/D selection) suggests instruction-following issues
  - Small models near 25% accuracy indicate insufficient domain knowledge

- First 3 experiments:
  1. Run analyze-only mode on cached responses to verify reproducibility without API calls
  2. Stratify results by contamination risk (low/medium/high) to assess potential training data overlap effects
  3. Test whether model length-bias rates correlate with accuracy after mitigation to confirm residual bias impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can data contamination be reliably detected or quantified in specialized benchmarks when model training data is proprietary?
- Basis in paper: The authors state in the Limitations section that "Contamination risk cannot be fully ruled out" and list "developing contamination detection methods" as a specific goal in Future Work.
- Why unresolved: Current contamination risk labels are estimates based on topic commonality ("low", "medium", "high") rather than empirical verification, as actual training data overlap cannot be checked without access to model internals.
- What evidence would resolve it: Development of statistical tests or membership inference methods specifically validated on technical, domain-specific corpora to detect memorization of source concepts.

### Open Question 2
- Question: To what extent are model scores on FormationEval driven by genuine domain reasoning versus exploitation of residual length bias?
- Basis in paper: The Discussion notes that "Residual length bias persists" (43.2% vs 25% baseline) and that it is "difficult to distinguish length exploitation from genuine reasoning" in the analysis.
- Why unresolved: While mitigation reduced the bias, the correct answer is still uniquely longest in 43.2% of cases, making it unclear if high-accuracy models understand petrophysics or simply associate verbosity with correctness.
- What evidence would resolve it: A follow-up evaluation using a modified benchmark where answer lengths are strictly normalized, or an ablation study analyzing model attention patterns on length cues vs. technical terms.

### Open Question 3
- Question: Does high accuracy on the FormationEval benchmark correlate with competence in practical petroleum geoscience tasks?
- Basis in paper: The authors explicitly caution in Section 7.1 that "High scores indicate that a model can answer concept-based questions... but do not guarantee expertise in practical applications."
- Why unresolved: The benchmark uses multiple-choice questions to test conceptual knowledge, whereas practical workflows (e.g., well log interpretation) require integrating visual data, spatial reasoning, and numerical simulation not captured by the current format.
- What evidence would resolve it: A comparative study measuring the correlation between FormationEval scores and performance on open-ended, real-world tasks like calculating water saturation from raw log data or interpreting seismic slices.

### Open Question 4
- Question: How would the model leaderboard change if the benchmark's domain coverage were balanced rather than weighted 54% toward petrophysics?
- Basis in paper: The Limitations section notes "Domain coverage is uneven" and Future Work proposes "adding questions from more sources to balance domain coverage."
- Why unresolved: Because Petrophysics is the most challenging domain and represents the majority of the dataset, current aggregate accuracy scores are heavily dominated by performance in this single sub-field, potentially skewing the overall ranking.
- What evidence would resolve it: Re-evaluation of models on a stratified, balanced subset of questions where each of the seven domains contributes equally to the final score.

## Limitations

- Contamination risk cannot be fully ruled out due to inability to verify training data overlap with source material
- Residual length bias persists at 43.2%, potentially providing modest performance advantage to heuristic-driven models
- Domain coverage is uneven with petrophysics dominating 54% of questions, potentially skewing overall accuracy scores

## Confidence

- **High confidence**: Overall benchmark construction, evaluation methodology, and reported accuracy figures across 72 models
- **Medium confidence**: Concept-based derivation fully prevents memorization gaming, domain stratification analysis
- **Low confidence**: None explicitly stated, but contamination detection methods and bias elimination remain challenging

## Next Checks

1. **Contamination Risk Analysis**: Stratify model performance by contamination risk (low/medium/high) to quantify the impact of potential training data overlap. Compare accuracy gaps between contamination tiers to determine if high-risk questions inflate scores.

2. **Bias Exploitation Test**: For models showing >90% accuracy, analyze their selection patterns on questions where the correct answer is not the longest. Calculate the expected accuracy gain from length exploitation and compare against actual performance to assess residual bias impact.

3. **Generation Strategy Validation**: Conduct an ablation study comparing concept-based question generation against source-paraphrasing generation using the same question pool. Evaluate whether concept-based questions consistently require more complex reasoning by analyzing model performance differences and human validation of question difficulty.