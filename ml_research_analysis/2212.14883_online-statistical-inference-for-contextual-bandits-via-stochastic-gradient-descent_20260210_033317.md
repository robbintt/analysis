---
ver: rpa2
title: Online Statistical Inference for Contextual Bandits via Stochastic Gradient
  Descent
arxiv_id: '2212.14883'
source_url: https://arxiv.org/abs/2212.14883
tags:
- regression
- policy
- page
- assumption
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies online statistical inference for model parameters
  in a contextual bandit framework, where data is adaptively collected based on past
  observations. The authors propose a general framework for updating decision rules
  via weighted stochastic gradient descent (SGD), allowing different weighting schemes
  for the stochastic gradient.
---

# Online Statistical Inference for Contextual Bandits via Stochastic Gradient Descent

## Quick Facts
- arXiv ID: 2212.14883
- Source URL: https://arxiv.org/abs/2212.14883
- Reference count: 40
- This paper proposes a general framework for online statistical inference in contextual bandits using weighted stochastic gradient descent, establishing asymptotic normality and improved efficiency over previous methods.

## Executive Summary
This paper addresses the fundamental challenge of statistical inference for model parameters in contextual bandit settings, where data is collected adaptively based on past observations. The authors develop a general framework that allows for weighted stochastic gradient descent updates with various weighting schemes, establishing asymptotic normality of the resulting estimators. Their approach significantly improves upon previous inverse probability weighting methods by providing more efficient estimators with explicit formulas for asymptotic covariance matrices.

## Method Summary
The authors propose a general framework for updating decision rules in contextual bandits via weighted stochastic gradient descent (SGD). The key innovation is allowing different weighting schemes for the stochastic gradient updates, which enables more efficient parameter estimation compared to traditional inverse probability weighting approaches. The framework establishes asymptotic normality of the weighted SGD estimator and provides a Bahadur representation showing slower convergence rates due to adaptive data collection. The authors demonstrate their approach through modified ε-greedy and exponential policies in both linear and quantile regression settings, with online inference procedures using plug-in estimators for confidence intervals.

## Key Results
- Weighted SGD estimator achieves optimal asymptotic efficiency in linear regression settings, outperforming previous averaged SGD approaches
- Exponential policies admit uniform asymptotic normality, unlike ε-greedy strategies which may have non-uniform coverage
- Explicit formulas for asymptotic covariance matrices are derived for linear regression settings
- The estimator exhibits slower convergence rates (n^{-1/3}) compared to classical SGD (n^{-1/2}) due to adaptive data collection

## Why This Works (Mechanism)
The paper's framework works by carefully designing the weighting scheme in the SGD updates to account for the adaptive nature of data collection in contextual bandits. By allowing general weighting schemes rather than being restricted to inverse probability weights, the authors can optimize the estimator's efficiency. The asymptotic normality is established through careful analysis of the martingale structure induced by the stochastic gradients and the exploration properties of the policies. The slower convergence rates compared to classical SGD are a fundamental consequence of the data-dependent sampling, which creates additional variance that cannot be eliminated through standard SGD arguments.

## Foundational Learning

**Contextual Bandits**: Sequential decision-making framework where actions are chosen based on context, requiring understanding of the exploration-exploitation trade-off and its impact on inference.

**Stochastic Gradient Descent**: Optimization algorithm for minimizing expected loss functions, with specific modifications needed for the adaptive sampling setting.

**Inverse Probability Weighting**: Traditional approach for handling selection bias in observational data, shown to be suboptimal for contextual bandits compared to the proposed weighted SGD.

**Asymptotic Normality**: Theoretical property ensuring the estimator converges to a normal distribution, critical for constructing confidence intervals but with slower rates in adaptive settings.

**Martingale Theory**: Mathematical framework for analyzing the stochastic processes induced by the sequential decision-making, essential for establishing asymptotic properties.

## Architecture Onboarding

**Component Map**: Policy -> Context -> Action -> Reward -> Gradient Update -> Parameter Estimate -> Inference

**Critical Path**: The estimation pipeline follows: context generation → policy-based action selection → reward observation → weighted gradient computation → parameter update → asymptotic normality establishment → inference procedure

**Design Tradeoffs**: The paper trades computational simplicity (simple SGD updates) for statistical efficiency gains, with the weighting scheme providing a knob to balance exploration and estimation precision.

**Failure Signatures**: Non-smooth gradient functions, insufficient exploration rates, or decaying exploration parameters at incorrect rates can all violate the asymptotic normality assumptions.

**First Experiments**:
1. Compare weighted SGD estimator efficiency against inverse probability weighting in linear regression with varying exploration rates
2. Evaluate coverage properties of online confidence intervals in finite samples across different policies
3. Test robustness of asymptotic results when gradient functions have bounded but non-smooth derivatives

## Open Questions the Paper Calls Out
None

## Limitations
- Asymptotic normality results depend heavily on smoothness assumptions of gradient functions that may not hold in practice
- Slower convergence rates (n^{-1/3} vs n^{-1/2}) represent a fundamental trade-off that could limit practical utility
- Plug-in estimator approach for online inference relies on consistent estimation of unknown quantities, potentially introducing bias

## Confidence
- Asymptotic normality theory: High confidence, with rigorous mathematical proofs provided
- Practical efficiency gains: Medium confidence, as simulations are mentioned but detailed empirical validation is limited
- Plug-in inference procedure: Medium confidence, theoretical validity established but finite-sample performance unclear

## Next Checks
1. Conduct extensive Monte Carlo simulations comparing the proposed weighted SGD estimator against existing methods (inverse probability weighting, averaged SGD) across different exploration rates and problem dimensions
2. Implement the online inference procedure on real-world contextual bandit datasets to evaluate coverage properties of confidence intervals in finite samples
3. Test the robustness of asymptotic results when gradient functions are non-smooth or when the exploration parameters decay at different rates