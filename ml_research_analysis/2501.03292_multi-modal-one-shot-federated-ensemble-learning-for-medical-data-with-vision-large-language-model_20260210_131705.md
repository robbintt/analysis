---
ver: rpa2
title: Multi-Modal One-Shot Federated Ensemble Learning for Medical Data with Vision
  Large Language Model
arxiv_id: '2501.03292'
source_url: https://arxiv.org/abs/2501.03292
tags:
- learning
- federated
- medical
- textual
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FedMME, a one-shot federated ensemble learning
  framework for multi-modal medical image analysis. The method integrates vision large
  language models to generate textual reports from medical images, extracts visual
  and textual features, and combines them using feature fusion for enhanced diagnostic
  accuracy.
---

# Multi-Modal One-Shot Federated Ensemble Learning for Medical Data with Vision Large Language Model

## Quick Facts
- **arXiv ID:** 2501.03292
- **Source URL:** https://arxiv.org/abs/2501.03292
- **Reference count:** 15
- **Primary result:** FedMME achieves >17.5% higher accuracy than baselines on RSNA dataset under Dirichlet α=0.3

## Executive Summary
This paper introduces FedMME, a one-shot federated ensemble learning framework that integrates vision large language models with traditional visual feature extraction for multi-modal medical image analysis. The method generates textual reports from medical images using a VLLM, extracts both visual and textual features, and fuses them through dimensionality reduction to enhance diagnostic accuracy. Experiments across four medical datasets demonstrate that FedMME significantly outperforms existing one-shot federated learning methods, particularly in non-IID data scenarios, while maintaining low communication costs suitable for privacy-sensitive medical applications.

## Method Summary
FedMME operates in a one-shot federated learning paradigm where each client independently trains a multi-modal model using local data, then uploads the trained model to a central server. The client model architecture combines ResNet-18 for visual feature extraction (512-dim output), a VLLM (Llama-3.2-11B-Vision-Instruct) for generating textual medical reports, and BERT for textual feature extraction (768-dim output). Textual features are reduced to 128 dimensions to prevent overshadowing visual features, then concatenated with visual features and passed through a fully connected layer for classification. The server aggregates predictions through equal-weight voting across all client models. The framework is trained for 100 local epochs with SGD optimizer (lr=1e-3, batch size=128) on datasets including Blood, Derma, RSNA Pneumonia Detection, and Diabetic Retinopathy Detection.

## Key Results
- FedMME achieves >17.5% higher accuracy than baseline methods on the RSNA dataset under Dirichlet distribution with α=0.3
- The framework demonstrates robust performance across various non-IID data partitions (α ∈ {0.6, 0.3, 0.1})
- Accuracy remains stable across different numbers of clients while maintaining low communication costs
- Case study shows VLLM can identify subtle features like "retinal hemorrhages" even when uncertain about overall diagnosis

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal Semantic Enrichment via VLLM
The VLLM generates textual descriptions from medical images, encoding subtle diagnostic features (e.g., "microaneurysms") as text embeddings. These are fused with visual features to provide explicit reasoning context alongside visual patterns. This semantic enrichment provides information that might be ambiguous in raw pixel data.

### Mechanism 2: Modality Balancing via Dimensionality Reduction
BERT produces high-dimensional (768-dim) textual features that could dominate the 512-dim visual features during fusion. Reducing textual features to 128 dimensions ensures visual features maintain prominence as the primary signal, using text as an auxiliary cue rather than allowing it to overwhelm the image-based learning.

### Mechanism 3: Non-IID Robustness via One-Shot Ensemble
Instead of aggregating weights (FedAvg), FedMME retains diverse locally-trained models and aggregates predictions through voting. This preserves specialist knowledge about local data distributions and avoids forcing a single global model to average out distinct features of minority classes.

## Foundational Learning

- **One-Shot Federated Learning**: Clients communicate with server exactly once, making local training quality critical. Quick check: Can the server send updated weights back to clients? (No, violates one-shot constraint).
- **Feature-level Fusion**: The method fuses raw feature vectors (pixels + text embeddings) before the final classification layer, not decisions. Quick check: Where does concatenation occur—before or after final classification? (Before, as input to FC layer).
- **VLLM Prompt Engineering**: Text feature quality depends entirely on VLLM output. Quick check: Does the framework require diagnosis or description from VLLM? (Prompts request descriptions/diagnosis, but safety filters can block output).

## Architecture Onboarding

- **Component map:** ResNet-18 → Visual Feature Extractor (512-dim) → VLLM → BERT → Text Feature Extractor (768-dim) → Dimensionality Reduction (128-dim) → Concatenation → Fully Connected Layer → Classifier
- **Critical path:** 1) Design prompts for VLLM to generate detailed medical descriptions without refusals, 2) Run VLLM inference to build local text dataset, 3) Train fusion model on image+text pairs, 4) Upload trained model weights to server, 5) Server performs inference using equal-weight voting across all client models
- **Design tradeoffs:** Text dimensionality—higher preserves info but risks visual suppression (recommended 128 dims); Model choice—Llama-3.2 offers better compliance than ChatGPT-4o but requires significant GPU memory
- **Failure signatures:** Accuracy collapse suggests textual dimensions too high; low text quality indicates VLLM refusal or generic output; voting ties occur with few clients
- **First 3 experiments:** 1) Baseline sanity check: compare FedEnsemble (visual-only) vs. FedMME on validation set, 2) Dimensionality ablation: test {32, 128, 512} textual dimensions to find sweet spot, 3) VLLM compliance test: verify prompts generate valid reports before training

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in a dedicated section.

## Limitations
- VLLM prompt specificity and factual accuracy are not systematically evaluated for their impact on diagnostic performance
- Heavy local computational requirements (11B-parameter VLLM + BERT) may limit feasibility on edge devices
- Fixed dimensionality reduction (128 dims) may under-utilize semantic richness for tasks where textual descriptions carry more diagnostic weight than visual features

## Confidence
- **High Confidence**: Ensemble voting effectiveness, communication efficiency, privacy preservation
- **Medium Confidence**: VLLM-generated text quality impact, dimensionality reduction importance, non-IID robustness
- **Low Confidence**: Optimal text feature dimension (128), VLLM model choice generalizability

## Next Checks
1. Reproduce dimensionality ablation by systematically testing textual feature dimensions {32, 64, 128, 256, 512} on RSNA dataset to confirm the 128-dimension sweet spot
2. Test multiple VLLM models (Llama-3.2-11B, ChatGPT-4o-Vision, Gemini) on representative samples from each dataset to quantify refusal rates and text quality scores
3. Apply FedMME to a fifth medical dataset not in the original experiments to test generalization of the VLLM-generated text enrichment mechanism