---
ver: rpa2
title: 'Beyond Single-Reward: Multi-Pair, Multi-Perspective Preference Optimization
  for Machine Translation'
arxiv_id: '2510.13434'
source_url: https://arxiv.org/abs/2510.13434
tags:
- translation
- preference
- arxiv
- score
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'M2PO introduces a multi-perspective reward engine and multi-pair
  optimization strategy to address two key limitations in preference optimization
  for machine translation: unreliable quality estimation signals that overlook translation
  hallucinations, and inefficient data utilization that selects only single win-loss
  pairs. The framework combines a hallucination penalty with a dynamic quality score
  that adapts during training, while constructing multiple preference pairs from the
  entire candidate pool rather than just top-bottom selections.'
---

# Beyond Single-Reward: Multi-Pair, Multi-Perspective Preference Optimization for Machine Translation

## Quick Facts
- arXiv ID: 2510.13434
- Source URL: https://arxiv.org/abs/2510.13434
- Reference count: 19
- Primary result: M2PO achieves 91.05 XCOMET score and 97.3 coverage score across 10 translation directions, outperforming standard DPO and CPO variants

## Executive Summary
M2PO introduces a multi-perspective reward engine and multi-pair optimization strategy to address two key limitations in preference optimization for machine translation: unreliable quality estimation signals that overlook translation hallucinations, and inefficient data utilization that selects only single win-loss pairs. The framework combines a hallucination penalty with a dynamic quality score that adapts during training, while constructing multiple preference pairs from the entire candidate pool rather than just top-bottom selections. On WMT21-22 benchmarks, M2PO achieves 91.05 XCOMET score and 97.3 coverage score across 10 translation directions, outperforming standard DPO and CPO variants, and matching or exceeding leading proprietary models like GPT-4o-mini.

## Method Summary
The M2PO framework addresses two fundamental challenges in machine translation preference optimization. First, it introduces a hallucination penalty mechanism combined with dynamic quality scoring that adapts during training to better capture translation fidelity issues that standard quality estimation metrics miss. Second, it implements a multi-pair optimization strategy that constructs preference pairs from the entire candidate pool rather than just top-bottom selections, improving data utilization efficiency. The approach is designed to be algorithm-agnostic, working across multiple DPO-like methods while consistently improving performance by 0.5-2.88 XCOMET points. The method specifically targets the generation of translations that are both more accurate and more faithful to source content compared to baselines.

## Key Results
- Achieves 91.05 XCOMET score and 97.3 coverage score across 10 translation directions on WMT21-22 benchmarks
- Outperforms standard DPO and CPO variants by 0.5-2.88 XCOMET points
- Matches or exceeds performance of GPT-4o-mini on comparable metrics
- Demonstrates consistent improvements across multiple DPO-like methods while maintaining algorithm-agnostic properties

## Why This Works (Mechanism)
The framework's effectiveness stems from addressing the fundamental limitations of traditional preference optimization in machine translation. By introducing a hallucination penalty, M2PO directly tackles the problem of translation models generating content not supported by source text, which standard quality estimation metrics often fail to detect. The dynamic quality scoring mechanism adapts during training, allowing the model to better discriminate between subtle quality differences as it improves. The multi-pair optimization strategy maximizes information extraction from available candidate translations by creating multiple preference pairs rather than relying on single win-loss comparisons, leading to more robust learning signals and better generalization across diverse translation scenarios.

## Foundational Learning
- **Quality Estimation in MT**: Why needed - Standard metrics like BLEU don't capture nuanced translation quality; quick check - XCOMET and COMET-22 provide more comprehensive quality assessments
- **Preference Optimization**: Why needed - Supervised learning alone is insufficient for aligning model outputs with human preferences; quick check - DPO and CPO methods establish preference-based learning frameworks
- **Hallucination Detection**: Why needed - Translation models often generate content unsupported by source text; quick check - Coverage score and specific hallucination metrics measure source fidelity
- **Multi-Pair Selection**: Why needed - Single win-loss pairs waste information from remaining candidates; quick check - Constructing multiple pairs from candidate pools improves data utilization efficiency
- **Dynamic Scoring Adaptation**: Why needed - Static quality thresholds become less discriminative as model improves; quick check - Adaptive scoring maintains effective learning signals throughout training

## Architecture Onboarding

**Component Map**: Candidate Generator -> Quality Estimator (with Hallucination Penalty) -> Dynamic Quality Scorer -> Multi-Pair Constructor -> Preference Optimizer

**Critical Path**: The most critical path runs from candidate generation through quality estimation to multi-pair construction, as the quality of preference pairs directly determines optimization effectiveness. The hallucination penalty module is particularly crucial since it addresses a fundamental failure mode in translation systems.

**Design Tradeoffs**: The framework trades computational overhead from constructing multiple preference pairs against improved learning signal quality. While single-pair selection is computationally efficient, it discards valuable information from non-top/bottom candidates. The dynamic quality scoring adds complexity but maintains discriminative power throughout training.

**Failure Signatures**: If the hallucination detection component is weak, the model may still produce unfaithful translations despite overall quality improvements. Poor dynamic scoring adaptation can lead to premature convergence or inability to distinguish subtle quality differences. Inefficient multi-pair construction can overwhelm memory resources without proportional performance gains.

**First 3 Experiments**:
1. Compare XCOMET scores with and without hallucination penalty component across diverse language pairs
2. Test multi-pair versus single-pair selection efficiency by measuring XCOMET improvements per computational unit
3. Evaluate algorithm-agnostic properties by applying M2PO to three different DPO variants and measuring consistent improvement patterns

## Open Questions the Paper Calls Out
None identified in the provided information.

## Limitations
- Relies on XCOMET and COMET-22 metrics as proxy measures rather than ground-truth human judgments
- Computational overhead of multi-pair construction not fully characterized relative to performance gains
- Effectiveness of hallucination detection depends on quality of underlying quality estimation model
- Claims of matching GPT-4o-mini lack direct comparison on identical test sets

## Confidence

**Confidence Labels:**
- XCOMET/COMET-22 performance improvements: **High**
- Hallucination detection effectiveness: **Medium**
- Multi-pair optimization efficiency claims: **Medium**
- GPT-4o-mini comparison validity: **Low**
- Algorithm-agnostic improvements: **High**

## Next Checks

1. Conduct human evaluation studies on a subset of translations to validate XCOMET/COMET-22 improvements against actual translation quality, particularly focusing on hallucination reduction.

2. Perform ablation studies isolating each M2PO component (hallucination penalty, dynamic quality scoring, multi-pair selection) across diverse language pairs including low-resource and distant language combinations.

3. Measure and report computational overhead of multi-pair construction relative to performance gains, including memory usage and training time comparisons with single-pair baselines.