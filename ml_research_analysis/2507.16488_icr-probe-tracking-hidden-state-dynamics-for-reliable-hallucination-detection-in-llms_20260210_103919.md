---
ver: rpa2
title: 'ICR Probe: Tracking Hidden State Dynamics for Reliable Hallucination Detection
  in LLMs'
arxiv_id: '2507.16488'
source_url: https://arxiv.org/abs/2507.16488
tags:
- detection
- hidden
- score
- probe
- hallucination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses hallucination detection in large language
  models (LLMs), which undermines their reliability. The proposed ICR Probe method
  shifts focus from static hidden states to their dynamic update process across layers.
---

# ICR Probe: Tracking Hidden State Dynamics for Reliable Hallucination Detection in LLMs

## Quick Facts
- arXiv ID: 2507.16488
- Source URL: https://arxiv.org/abs/2507.16488
- Authors: Zhenliang Zhang; Xinyu Hu; Huixuan Zhang; Junzhe Zhang; Xiaojun Wan
- Reference count: 40
- Primary result: ICR Probe achieves AUROC scores up to 0.8436 for hallucination detection across multiple datasets and models

## Executive Summary
This paper addresses the critical challenge of hallucination detection in large language models (LLMs) by proposing ICR Probe, a novel method that tracks the dynamic evolution of hidden states across transformer layers. Unlike traditional approaches that focus on static hidden states, ICR Probe quantifies the contribution of each module to hidden state updates through the ICR Score, capturing cross-layer evolution of the residual stream. The method demonstrates strong performance on three mainstream open-source LLMs (Llama-3, Qwen2.5, Gemma-2) across four datasets, achieving state-of-the-art results with superior parameter efficiency compared to existing baselines like SAPLMA.

## Method Summary
ICR Probe introduces a dynamic approach to hallucination detection by shifting focus from static hidden states to their update process across transformer layers. The method introduces the ICR Score, which quantifies each module's contribution to hidden state updates, and captures the cross-layer evolution of the residual stream. By tracking these dynamics, ICR Probe identifies patterns that distinguish factual responses from hallucinated ones. The method is evaluated on three mainstream open-source LLMs (Llama-3, Qwen2.5, Gemma-2) and four datasets, demonstrating superior performance compared to baselines.

## Key Results
- Achieves AUROC scores up to 0.8436 on hallucination detection tasks
- Demonstrates strong generalization across datasets and models
- Shows superior parameter efficiency compared to alternatives like SAPLMA

## Why This Works (Mechanism)
ICR Probe works by tracking the dynamic evolution of hidden states across transformer layers rather than relying on static representations. The ICR Score quantifies how each module contributes to hidden state updates, capturing the cross-layer evolution of the residual stream. This approach identifies that factual and hallucinated responses exhibit distinct hidden state update patterns, allowing the model to detect hallucinations by monitoring these dynamic changes throughout the network's computation process.

## Foundational Learning

1. **Transformer Architecture**
   - Why needed: Understanding how transformers process information layer by layer is crucial for tracking hidden state dynamics
   - Quick check: Can you trace how input embeddings flow through attention and feed-forward layers?

2. **Residual Connections**
   - Why needed: Critical for understanding how information accumulates and evolves across layers
   - Quick check: Do you understand how residual streams maintain information flow?

3. **Hidden State Evolution**
   - Why needed: Core concept for understanding how information changes as it passes through transformer layers
   - Quick check: Can you explain how hidden states are updated in each transformer block?

## Architecture Onboarding

**Component Map:**
Input -> Layer 1 (Attention + FFN) -> Layer 2 (Attention + FFN) -> ... -> Output

**Critical Path:**
Input embeddings → Layer-wise processing → Residual stream evolution → ICR Score computation → Hallucination detection

**Design Tradeoffs:**
- Focuses on dynamic updates rather than static states for better detection accuracy
- Prioritizes parameter efficiency over raw performance
- Balances computational cost with detection reliability

**Failure Signatures:**
- May not generalize to proprietary models with different architectures
- Performance limited by dataset diversity in evaluation
- Computational overhead not fully characterized

**First 3 Experiments to Run:**
1. Evaluate on a proprietary model like GPT-4 to test architecture generalization
2. Conduct parameter count and computational complexity analysis against SAPLMA
3. Perform qualitative analysis of hidden state patterns identified by ICR Score

## Open Questions the Paper Calls Out
None

## Limitations
- Narrow focus on open-source models (Llama-3, Qwen2.5, Gemma-2) may limit generalization
- Evaluation relies on only four datasets, limiting real-world applicability
- Claims of superior parameter efficiency lack detailed comparative analysis

## Confidence
- **ICR Probe outperforms baselines**: High confidence - supported by quantitative AUROC scores
- **Strong generalization across datasets**: Medium confidence - demonstrated but limited by dataset diversity
- **Superior parameter efficiency**: Low confidence - claimed but not rigorously substantiated

## Next Checks
1. Test ICR Probe on proprietary models (e.g., GPT-4, Claude) to assess true generalization
2. Conduct detailed ablation study comparing parameter counts and computational overhead against SAPLMA
3. Perform qualitative analysis visualizing hidden state dynamics captured by ICR Score