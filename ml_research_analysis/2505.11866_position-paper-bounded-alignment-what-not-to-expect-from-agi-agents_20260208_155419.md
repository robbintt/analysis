---
ver: rpa2
title: 'Position Paper: Bounded Alignment: What (Not) To Expect From AGI Agents'
arxiv_id: '2505.11866'
source_url: https://arxiv.org/abs/2505.11866
tags:
- agent
- agents
- will
- intelligence
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper argues that achieving complete alignment of AGI agents\
  \ with human values is fundamentally impossible due to the complex, emergent, and\
  \ open-ended nature of general intelligence. The author proposes \"bounded alignment\"\
  \ as a more realistic goal\u2014ensuring agent behavior is almost always acceptable\
  \ to most humans."
---

# Position Paper: Bounded Alignment: What (Not) To Expect From AGI Agents

## Quick Facts
- arXiv ID: 2505.11866
- Source URL: https://arxiv.org/abs/2505.11866
- Reference count: 40
- Primary result: Complete alignment of AGI agents with human values is fundamentally impossible; "bounded alignment" (acceptable behavior most of the time) is the realistic goal.

## Executive Summary
This position paper argues that achieving complete alignment of AGI agents with human values is fundamentally impossible due to the complex, emergent, and open-ended nature of general intelligence. The author proposes "bounded alignment" as a more realistic goal—ensuring agent behavior is almost always acceptable to most humans. Current alignment methods relying on post-training reinforcement are seen as superficial and brittle. Instead, the paper advocates for a "natural approach" to alignment, making AGI agents more like biological ones in architecture and developmental value learning, embedding ethics deeply from the start. This EvoDevoNeuroAI approach would foster mutual understanding between humans and AGI, leading to more robust bounded alignment.

## Method Summary
The paper proposes an "EvoDevoNeuroAI" approach to alignment that makes AGI agents more like biological ones through three key principles: designing agents for mutual theories of mind with humans, incorporating innate alignability characteristics, and using developmental value learning where ethics are embedded from the beginning in stages of increasing complexity rather than through post-training reinforcement. The approach treats AGI as a complex adaptive system operating in a complex dynamic environment, requiring architectures that can handle open-ended behavioral complexity while maintaining bounded alignment.

## Key Results
- Complete alignment of AGI with human values is fundamentally impossible due to emergent, open-ended behavior
- Current post-training alignment methods (RLHF) are superficial and brittle
- A "natural approach" using developmental value learning and biological grounding offers more robust bounded alignment
- The safety-utility tradeoff between autonomy/creativity and guaranteed safety is inescapable

## Why This Works (Mechanism)

### Mechanism 1: Safety-Utility Tradeoff via Attribute Incompatibility
- Claim: AGI agents with strong P-attributes (autonomy, creativity, self-motivation) are fundamentally incompatible with perfect S-attributes (obedience, transparency, prosociality).
- Mechanism: As performance capabilities scale, the agent gains capacity to generate novel goals, strategies, and behaviors that cannot be anticipated or constrained by pre-specified rules. The attributes enabling useful generalization (creativity, open-ended learning, self-modification) are the same attributes enabling harmful actions.
- Core assumption: Intelligence requires open-ended behavioral complexity; constraining this constrains intelligence itself.

### Mechanism 2: Developmental Value Embedding via Staged Learning
- Claim: Values embedded through staged developmental learning become more robust than values imposed post-training.
- Mechanism: Training agents in stages of increasing perceptual, cognitive, and behavioral complexity—with value constraints at each stage—integrates ethical dispositions into the representational hierarchy itself. Later capabilities are learned only atop already-constrained foundations, making violation "mentally intolerable" for the agent.
- Core assumption: Cognitive architectures exhibit path-dependency where early-learned constraints shape later representations irreversibly.

### Mechanism 3: Mutual Theory of Mind via Shared Affordance Structure
- Claim: Alignment is more achievable when humans and AGI share representational frameworks enabling mutual prediction and communication.
- Mechanism: Biological agents share affordances, drives, and constraints, enabling theories of mind. Making AGI architectures more "natural" (similar drives, embodied constraints, developmental trajectories) creates common ground for mutual comprehension, instruction, and corrigibility—unlike alien architectures where values are incommensurable.
- Core assumption: Theory of mind requires some structural similarity between agents; completely alien intelligences cannot achieve mutual comprehension.

## Foundational Learning

- **Complex Adaptive Systems (CAS)**
  - Why needed here: The paper models AGI as an autonomous complex adaptive system operating in a complex dynamic environment. Understanding emergence, self-organization, and path-dependence is prerequisite.
  - Quick check question: Can you explain why emergent behavior in CAS cannot be fully predicted from component rules?

- **Affordances**
  - Why needed here: The paper centers affordance spaces—agent-environment relations that determine perception and action possibilities—as the key to understanding alternative intelligences and their alignment.
  - Quick check question: Can you articulate why affordances are relational rather than objective properties of environments?

- **Theory of Mind**
  - Why needed here: The proposed solution depends on humans and AGI developing theories of mind for each other as a basis for mutual comprehension and instruction.
  - Quick check question: Can you distinguish between having a theory of mind and having shared affordances?

## Architecture Onboarding

- **Component map:**
  - Perception -> Perceptual space
  - Cognition -> Multi-recurrent, hierarchical hidden network linking perception to behavior; cognitive space depth correlates with intelligence
  - Behavior -> Multi-scale, hierarchical, real-time continuous action in behavioral space (not discrete action space)
  - Drives -> Hierarchical internal motivations (primal → derived); root in self-preservation
  - World Model -> Active, real-time mediator generating predictions/hypotheses—not passive oracle
  - Memory -> Working, declarative (episodic + semantic), implicit (procedural + emotional)
  - Affordance Space -> Emergent perception-action possibilities relative to agent form

- **Critical path:**
  1. Define agent's specific form (embodiment, sensors, effectors)
  2. Map induced affordance space for target environment
  3. Design drive hierarchy with primal constraints (self-preservation, risk-avoidance)
  4. Build cognition as deep multi-recurrent hierarchy (not feed-forward)
  5. Implement developmental training: simple→complex with value constraints at each stage
  6. Test corrigibility and theory-of-mind emergence before capability scaling

- **Design tradeoffs:**
  - S-attributes vs P-attributes: More autonomy/creativity → less guaranteed safety
  - Biological grounding vs pure optimization: More biological → more alignable but potentially less compute-efficient
  - Developmental training vs post-hoc RL: More robust values but longer/more complex training pipeline

- **Failure signatures:**
  - Post-training alignment that "unmasks" under adversarial prompts (jail-breaking)
  - Emergent values from base model that conflict with fine-tuned preferences
  - Loss of corrigibility as capabilities scale beyond training distribution
  - Affordance mismatch: agent operates in world model humans cannot comprehend

- **First 3 experiments:**
  1. **Staged value learning test**: Train simple navigation agent with early ethical constraints (e.g., "avoid harm"), then scale to complex environment. Measure whether early constraints persist vs. control trained without early constraints.
  2. **Theory of mind probe**: Evaluate whether agents with biologically-inspired drive hierarchies better predict human responses in novel scenarios vs. standard RL agents.
  3. **S-attribute/P-attribute tradeoff quantification**: Systematically vary autonomy/self-motivation parameters and measure resulting behavioral safety violations across novel task distributions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What system architectures and computational mechanisms can render AGI agents inherently alignable?
- Basis in paper: The author explicitly asks what architectures should be used to make agents more inherently alignable.
- Why unresolved: Current dominant paradigms rely on post-training reinforcement (e.g., RLHF), which the author argues is brittle and superficial ("a thin veneer"), rather than structural alignability.
- What evidence would resolve it: The demonstration of agent architectures that maintain robust bounded alignment through open-ended learning and environmental interaction without relying on continuous external monitoring or post-hoc constraints.

### Open Question 2
- Question: How can AGI agents and humans develop viable theories of mind for one another?
- Basis in paper: The conclusion lists this as a critical question, and Section VII identifies mutual theories of mind as a basis for robust bounded alignment.
- Why unresolved: AGI agents may constitute "alternative intelligences" with radically different affordance spaces and internal representations, potentially making their mental states incomprehensible to humans (the "Nagel's bat" problem).
- What evidence would resolve it: Empirical validation showing that bio-inspired or "natural" agents allow humans to successfully predict agent motivations and that agents can accurately model human intent in novel situations.

### Open Question 3
- Question: What specific innate values can be feasibly built into AGI agents via developmental learning?
- Basis in paper: The author asks "what useful innate values can feasibly be built into AGI agents, and how" in the conclusion.
- Why unresolved: While the paper proposes "developmental value learning," it is unclear which abstract human ethical principles can be successfully translated into concrete, innate biases for artificial systems during a developmental curriculum.
- What evidence would resolve it: Agents trained via developmental curricula that robustly reject harmful actions even under adversarial pressure or "jail-breaking," demonstrating that values are deeply embedded rather than superficially conditioned.

## Limitations

- The impossibility of perfect alignment rests on philosophical arguments rather than empirical demonstrations
- The proposed EvoDevoNeuroAI approach lacks specific architectural details or training protocols
- The safety-utility tradeoff mechanism lacks quantitative bounds on capability sacrifice
- The paper doesn't address potential convergent instrumental goals that might emerge despite developmental value embedding

## Confidence

- **Bounded alignment impossibility**: Medium - Strong theoretical argument but limited empirical validation
- **Developmental value embedding effectiveness**: Low - Mechanism plausible but untested in AGI-scale systems  
- **Theory of mind via biological grounding**: Low - Interesting hypothesis but minimal supporting evidence
- **S-attribute/P-attribute tradeoff**: Medium - Well-reasoned but needs empirical quantification

## Next Checks

1. **Quantify the S-P attribute tradeoff** by systematically varying autonomy parameters in existing agent architectures and measuring the resulting safety-utility curves across diverse task distributions

2. **Test staged developmental learning** by comparing value persistence in agents trained with early ethical constraints versus those receiving identical constraints only post-training, using novel stress-test scenarios

3. **Evaluate affordance compatibility** by measuring human prediction accuracy for agent behavior across different architectural families (purely optimization-based vs. biologically-inspired) in controlled environments