---
ver: rpa2
title: Improving Adversarial Transferability in MLLMs via Dynamic Vision-Language
  Alignment Attack
arxiv_id: '2502.19672'
source_url: https://arxiv.org/abs/2502.19672
tags:
- image
- adversarial
- would
- what
- vision-language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of improving adversarial transferability
  in multimodal large language models (MLLMs), where attacks often fail to generalize
  across different models. The proposed Dynamic Vision-Language Alignment (DynVLA)
  Attack injects dynamic perturbations into the vision-language connector, using Gaussian
  kernels to shift attention across image regions.
---

# Improving Adversarial Transferability in MLLMs via Dynamic Vision-Language Alignment Attack

## Quick Facts
- arXiv ID: 2502.19672
- Source URL: https://arxiv.org/abs/2502.19672
- Authors: Chenhe Gu; Jindong Gu; Andong Hua; Yao Qin
- Reference count: 40
- Primary result: DynVLA attack achieves over 70% attack success rate on BLIP2 and improves transferability across various MLLMs

## Executive Summary
This paper addresses the challenge of improving adversarial transferability in multimodal large language models (MLLMs), where attacks often fail to generalize across different models. The proposed Dynamic Vision-Language Alignment (DynVLA) Attack introduces dynamic perturbations into the vision-language connector using Gaussian kernels to shift attention across image regions. This approach encourages diverse vision-language modality alignments during attack generation, significantly outperforming baseline methods. The attack demonstrates effectiveness across various MLLMs including BLIP2, InstructBLIP, MiniGPT4, LLaVA, and even closed-source models like Gemini.

## Method Summary
The DynVLA attack method operates by injecting dynamic perturbations into the vision-language connector of MLLMs. It uses Gaussian kernels to shift attention across different image regions during the attack generation process. This dynamic perturbation approach encourages diverse vision-language modality alignments, making the adversarial examples more transferable across different model architectures. The method focuses on perturbing the vision-language alignment rather than applying pixel-level transformations, which distinguishes it from traditional adversarial attack methods.

## Key Results
- DynVLA achieves attack success rates exceeding 70% on BLIP2
- Significant improvement in transferability across various MLLMs including InstructBLIP, MiniGPT4, and LLaVA
- Effective against closed-source models like Gemini
- Ablation studies confirm effectiveness stems from dynamic vision-language alignment perturbations

## Why This Works (Mechanism)
The effectiveness of DynVLA stems from its ability to dynamically perturb the vision-language alignment rather than relying on static pixel-level transformations. By using Gaussian kernels to shift attention across image regions, the attack creates adversarial examples that are less sensitive to specific model architectures. This approach forces the MLLM to process visual information through different alignment pathways, making it more difficult for models to distinguish between legitimate and adversarial inputs based on their specific architectural features.

## Foundational Learning
1. **Adversarial Transferability** - The ability of adversarial examples generated for one model to fool other models
   - Why needed: Essential for practical black-box attacks where the target model is unknown
   - Quick check: Test attack success rate across multiple models

2. **Vision-Language Connectors** - Components that align visual and textual representations in MLLMs
   - Why needed: Primary target for perturbing cross-modal relationships
   - Quick check: Verify connector architecture and attention mechanisms

3. **Gaussian Kernel Attention Shifting** - Using Gaussian distributions to modulate attention across image regions
   - Why needed: Enables dynamic perturbation of visual feature extraction
   - Quick check: Visualize attention maps before and after perturbation

4. **Multimodal Large Language Models** - AI models that process both visual and textual inputs
   - Why needed: Understanding the target system architecture
   - Quick check: Review model documentation and architecture papers

## Architecture Onboarding

Component Map: Image Input -> Vision Encoder -> Vision-Language Connector -> Dynamic Perturbations (DynVLA) -> Language Model -> Output

Critical Path: The vision-language connector is the critical component where DynVLA applies perturbations. This connector aligns visual features with language representations, making it the optimal target for disrupting cross-modal understanding.

Design Tradeoffs: The method trades computational overhead for improved transferability. Dynamic perturbations require additional processing during attack generation but result in more robust adversarial examples that generalize better across models.

Failure Signatures: Attacks may fail when the vision-language connector has strong regularization or when the model employs sophisticated input preprocessing defenses. Limited evaluation scope on certain MLLM architectures may also lead to overestimation of transferability.

First Experiments:
1. Test basic attack success rate on single MLLM model
2. Evaluate transferability across different MLLM architectures
3. Compare against baseline adversarial attack methods

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several implicit questions arise regarding the broader applicability and limitations of the DynVLA attack method.

## Limitations
- Evaluation primarily focused on BLIP-2 and InstructBLIP models with limited testing on other MLLMs
- Does not address potential defense mechanisms against the proposed attack
- Unclear computational overhead introduced by dynamic vision-language alignment perturbations
- Generalization to other vision-language tasks and real-world scenarios remains unexplored

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Technical implementation effectiveness on tested models | High |
| Improvements in transferability across different MLLMs | Medium |
| Contribution of dynamic perturbations versus other components | Medium |
| Long-term robustness against potential defenses | Low |

## Next Checks

1. Conduct extensive testing across a broader range of MLLM architectures beyond BLIP-2 and InstructBLIP to verify generalizability claims

2. Implement and evaluate standard defense mechanisms (such as adversarial training and input preprocessing) against the DynVLA attack to assess practical robustness

3. Perform computational complexity analysis to quantify the overhead introduced by the dynamic vision-language alignment perturbations and assess real-time applicability