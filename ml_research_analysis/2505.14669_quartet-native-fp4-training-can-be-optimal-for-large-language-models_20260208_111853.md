---
ver: rpa2
title: 'Quartet: Native FP4 Training Can Be Optimal for Large Language Models'
arxiv_id: '2505.14669'
source_url: https://arxiv.org/abs/2505.14669
tags:
- training
- arxiv
- quantization
- mxfp4
- scaling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Quartet, a new algorithm for fully end-to-end
  training of large language models (LLMs) using 4-bit floating-point precision (MXFP4)
  on NVIDIA Blackwell GPUs. Quartet addresses the accuracy degradation problem in
  low-precision training by combining a forward pass that minimizes quantization error
  (using QuEST) with a backward pass that balances error and bias (using round-to-nearest).
---

# Quartet: Native FP4 Training Can Be Optimal for Large Language Models

## Quick Facts
- arXiv ID: 2505.14669
- Source URL: https://arxiv.org/abs/2505.14669
- Reference count: 40
- Presents Quartet, achieving up to 1.6× end-to-end training speedup over FP8 and 2.9× over BF16 for LLM training using native MXFP4 on NVIDIA Blackwell GPUs

## Executive Summary
This paper introduces Quartet, a novel algorithm enabling fully end-to-end training of large language models using 4-bit floating-point precision (MXFP4) on NVIDIA Blackwell GPUs. Quartet addresses accuracy degradation in low-precision training through an asymmetric quantization strategy: forward pass uses QuEST (Hadamard transform + RMSE clipping) to minimize quantization error, while backward pass uses round-to-nearest (RTN) to balance error and bias. The method is supported by a new scaling law analysis that quantifies precision trade-offs, demonstrating that MXFP4 can be "optimal" in accuracy-efficiency trade-offs. Experimental results on Llama-family models show Quartet outperforms prior 4-bit methods with significant speedup gains over higher precisions.

## Method Summary
Quartet implements fully-quantized training using MXFP4 precision through a two-stage kernel pipeline optimized for Blackwell Tensor Cores. The forward pass applies a randomized Hadamard transform followed by QuEST quantization with block size g=32, while the backward pass uses RTN quantization with 3/4 and 16/9 scaling factors. The method employs a novel scaling law decomposition that separates parameter efficiency (effN) and data efficiency (effD) effects of quantization, enabling principled comparison across precisions. Training uses AdamW optimizer with weight decay 0.1, gradient clipping at 1.0, and learning rates scaled inversely to model size, with optimizer states maintained in FP32.

## Key Results
- Quartet achieves superior accuracy compared to prior 4-bit methods (LUQ, HALO, Jetfire, LSS) on Llama-family models
- Demonstrates up to 1.6× end-to-end training speedup over FP8 and 2.9× over BF16
- Scaling law analysis shows MXFP4 is "optimal" in terms of accuracy-efficiency trade-offs for LLM training
- Validates asymmetric quantization strategy: forward MSE minimization + backward bias balancing

## Why This Works (Mechanism)

### Mechanism 1: Scaling Law Decomposition
The paper proposes L(N, D, P_forward, P_backward) = (A/(N·effN)^α + B/(D·effD)^β)^γ + E, where forward precision affects effective parameter count and backward precision affects effective data requirements. A method is superior only if it improves both metrics simultaneously. Assumes multiplicative decomposition of precision effects on N and D terms is valid; assumes scaling law functional form transfers across precision levels. Follow-up work (Quartet II) extends method but doesn't independently verify scaling law form. If effN and effD cannot be independently fit or extrapolation shows systematic error, decomposition fails.

### Mechanism 2: Asymmetric Quantization Strategy
Forward pass quantization error correlates with parameter efficiency (MSE minimization preserves representational capacity). Backward pass requires balancing quantization error against gradient magnitude misalignment—RTN achieves better trade-off than stochastic rounding despite higher MSE because it maintains gradient direction better. Assumes misalignment metric (1−E[1/S]) accurately predicts training dynamics; assumes Hadamard rotation benefits transfer across model scales. TetraJet-v2 confirms oscillation suppression and outlier control are critical. If gradient estimator bias accumulates differently across architectures or training durations, RTN advantage may not generalize—note paper's erratum about initial RTN instability in longer runs.

### Mechanism 3: Hardware-Aware Kernel Fusion
Two-stage pipeline: Stage 1 fuses Hadamard (32×32 GEMM), quantization, and scale computation into single kernel using CUTLASS multilevel tiling; Stage 2 uses Blackwell's native tcgen05.mma instructions for block-scaled GEMM. The 32-element group size matches MXFP4 format, amortizing transform cost. Assumes Blackwell Tensor Core utilization reaches advertised throughput; assumes memory bandwidth not saturated by scale factor rearrangement. No independent kernel benchmarks from corpus; related work reports similar speedup ranges. If fused kernel register pressure exceeds capacity, or scale factor layout rearrangement becomes bottleneck, speedup degrades.

## Foundational Learning

- **Quantization-aware training (QAT) vs. fully-quantized training**: Quartet quantizes both forward and backward passes; most prior work only quantized forward. Quick check: Can you explain why stochastic rounding is unbiased but RTN is not, and why RTN may still be preferable for backward pass?

- **Hadamard transform for outlier mitigation**: Both forward (QuEST) and backward passes apply Hadamard transforms to redistribute outlier values before quantization. Quick check: Why does a randomized Hadamard transform reduce quantization error more than fixed transform for gradient computation?

- **MXFP4 block-scaled format**: The 1-sign + 1-mantissa + 2-exponent format with 32-element shared FP8 scales is hardware-native on Blackwell. Quick check: What is the representable range and precision of MXFP4 values, and how does block scaling affect dynamic range?

## Architecture Onboarding

- **Component map**: Input (BF16) → Hadamard Transform (32×32 GEMM) → QuEST Quantization → MXFP4 Values + E8M0 Scales + Masks → Native tcgen05.mma GEMM → Output
Backward: Gradient → Random Hadamard → RTN Quantization → MXFP4 GEMM → Rescale (16/9 factor) → Apply Masks → Inverse Hadamard → dX, dW

- **Critical path**: Start with CUTLASS 3.9 block-scaled GEMM templates (tcgen05.mma path). Implement fused Stage 1 kernel: Hadamard as small GEMM → FP32 accumulation in SMEM → PTX downcast to E2M1 → scale computation → mask generation. Implement scale factor rearrangement for tcgen05.mma K-dimension layout requirements. Validate against FP32 reference at each stage before integration.

- **Design tradeoffs**: Threadblock tile size (32×32 vs 128×32): Larger tiles increase arithmetic intensity but raise register pressure; quantization overhead drops from ~60% to ~20% with optimization. Hadamard group size: Must match MXFP4's 32-element blocks; larger groups require padding, smaller groups lose decorrelation benefit. RTN rescale factor (3/4 and 16/9): Empirically tuned; wrong factor causes gradient magnitude drift.

- **Failure signatures**: NaN/Inf in gradients: Check Hadamard input normalization; verify scale factors are E8M0 (power-of-two only). Loss divergence after warmup: Verify RTN rescale factors applied; check mask application order. No speedup vs FP8: Likely scale rearrangement bottleneck; profile MXFP rearrangement kernel separately. Accuracy gap >5%: Verify QuEST clipping masks applied to backward pass (Mx, Mw); check Hadamard group alignment.

- **First 3 experiments**: Single-layer sanity check: Isolate one linear layer (4096×4096), run forward+backward with Quartet vs FP32 reference. Verify gradient cosine similarity >0.99 and MSE <1e-3 before full model integration. Scaling law fit validation: Train 30M and 50M models with D/N=100 using Quartet; fit effN and effD independently. Cross-check: effN should correlate with forward MSE, effD with gradient misalignment metric. Kernel microbenchmark: Measure forward/backward timing for three Llama-7B linear layer shapes [(4096,4096), (4096,11008), (11008,4096)] with batch=64, seqlen=512. Target: >1.5× over FP8 on backward pass. If missed, profile quantization vs. GEMM ratio per Figure 7 breakdown.

## Open Questions the Paper Calls Out

### Open Question 1: Format Generalization
Can Quartet's approach be generalized to alternative low-precision formats such as NVFP4, or is the method fundamentally tied to MXFP4's specific block structure and scaling? Section 6 states: "One current limiting factor is that Quartet was designed with a specific (standard) data-type and compute architecture in mind. In future work, we plan to look into generalizing our approach to alternative formats." MXFP4 has 32-element blocks with shared FP8 scales, while NVFP4 uses 16-element blocks with different scale representation. The algorithm's Hadamard transform alignment with MXFP4's block size may not transfer directly.

### Open Question 2: Distributed Training Scalability
Does Quartet maintain its accuracy-efficiency advantages in distributed multi-GPU training across hundreds of GPUs, where communication overhead and gradient synchronization may interact with low-precision quantization differently? Section 6: "In future work, we plan to look into...larger-scale distributed model execution." Experiments were conducted on 8xH100 GPUs for pre-training and single RTX 5090 for speedup measurements. Distributed training introduces gradient compression, all-reduce operations, and synchronization that may amplify quantization errors differently than single-node training.

### Open Question 3: Theoretical Foundations
Can the theoretical foundations for the efficiency parameters (effN, effD) in the proposed scaling law be formalized, beyond the current empirical fitting approach? Section 4.1 introduces effN and effD as "fitted parameters" without theoretical derivation. The scaling law provides excellent empirical fit, but the underlying theory explaining why these specific parameterizations capture precision effects is not established. The paper connects them to forward MSE and backward misalignment empirically, but the mathematical relationship remains correlative.

## Limitations
- Scaling law decomposition assumes independent effects of forward and backward precision on parameter and data efficiency, which may not hold for all architectures or training durations
- Paper's erratum about RTN instability in longer runs suggests potential limitations in backward-pass assumptions
- Kernel optimization claims lack independent benchmarking from the corpus, making speedup validation dependent on reproduction efforts

## Confidence
- **High confidence**: Forward-pass quantization mechanism (QuEST + MXFP4) and its empirical validation across multiple model scales
- **Medium confidence**: Scaling law decomposition and backward-pass quantization strategy (RTN + Hadamard), pending longer-duration validation
- **Low confidence**: Absolute speedup claims (2.9× over BF16, 1.6× over FP8) without independent kernel benchmarks

## Next Checks
1. **Independent kernel benchmarking**: Profile Quartet's fused quantization kernels on Blackwell hardware for three Llama-7B layer shapes, measuring actual vs. claimed speedup (target >1.5× over FP8 backward pass)
2. **Long-duration stability test**: Train a 100M-parameter model for 150× tokens-per-parameter ratio (vs. reported 100×) to verify RTN backward-pass stability and detect any emerging divergence patterns
3. **Scaling law extrapolation validation**: Apply Quartet to model sizes and D/N ratios beyond the original training range (e.g., 15B models or D/N > 150×) to test whether the effN/effD decomposition remains predictive