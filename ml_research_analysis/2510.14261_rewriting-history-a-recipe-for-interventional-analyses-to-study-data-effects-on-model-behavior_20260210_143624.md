---
ver: rpa2
title: 'Rewriting History: A Recipe for Interventional Analyses to Study Data Effects
  on Model Behavior'
arxiv_id: '2510.14261'
source_url: https://arxiv.org/abs/2510.14261
tags:
- documents
- data
- items
- pretraining
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a systematic recipe for studying how training
  data affects language model behavior through interventional analyses. The approach
  involves selecting evaluation items where a model exhibits specific behavior, matching
  relevant training documents to those items using heuristics like term cooccurrence
  or information retrieval, and then modifying those documents before retraining to
  observe behavioral changes.
---

# Rewriting History: A Recipe for Interventional Analyses to Study Data Effects on Model Behavior

## Quick Facts
- **arXiv ID**: 2510.14261
- **Source URL**: https://arxiv.org/abs/2510.14261
- **Reference count**: 31
- **Primary result**: Systematic recipe for studying how training data affects language model behavior through interventional analyses

## Executive Summary
This paper introduces a systematic recipe for studying how training data affects language model behavior through interventional analyses. The approach involves selecting evaluation items where a model exhibits specific behavior, matching relevant training documents to those items using heuristics like term cooccurrence or information retrieval, and then modifying those documents before retraining to observe behavioral changes. Two case studies demonstrate the method: one relating term cooccurrences to fact learning in knowledge graphs, and another applying information retrieval techniques to complex multiple-choice question answering tasks. Results show that while removing cooccurring entity pairs reduces fact learning performance, it doesn't completely eliminate it, suggesting other factors contribute to knowledge acquisition. Information retrieval methods can identify relevant documents but often miss key details needed to fully explain model performance. The work provides a flexible framework for future research on data-behavior relationships in language models.

## Method Summary
The methodology centers on "rewriting history" by intervening on data batches and retraining model checkpoints to test hypotheses about data-behavior relationships. The process involves selecting evaluation items where the model exhibits target behavior, matching relevant training documents using heuristics (term cooccurrence or information retrieval), and then modifying those documents through removal or swapping before retraining from a specific checkpoint. This allows researchers to attribute changes in model behavior to specific data subsets. The approach is validated through two case studies: examining how cooccurring entity pairs in pretraining data relate to fact learning in knowledge graphs, and using information retrieval techniques to identify documents relevant to complex multiple-choice question answering tasks.

## Key Results
- Removing documents containing cooccurring entity pairs reduces fact learning performance but doesn't eliminate it, suggesting knowledge acquisition involves multiple mechanisms beyond simple cooccurrence patterns
- Information retrieval methods (BM25 and DPR) can identify relevant documents for MCQA tasks, but BM25 outperforms DPR for causal interventions in factual recall tasks
- Lexical matching remains more effective than semantic matching for identifying pretraining documents that influence knowledge acquisition in language models
- The intervention methodology successfully isolates the effects of specific data subsets on model behavior, providing a framework for causal attribution in language model research

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Targeted data modification at intermediate training stages allows for causal attribution of specific model behaviors to specific data subsets
- **Mechanism**: The recipe relies on "rewriting history" by initializing a model from a specific checkpoint, modifying the subsequent data batch, and retraining to observe behavioral changes
- **Core assumption**: The model's behavior is sufficiently plastic at the chosen checkpoint for the intervention to have a measurable effect
- **Evidence anchors**: Abstract and Section 2.2 demonstrate the counterfactual retraining approach
- **Break condition**: If the intervention fails to change model behavior, the hypothesized data-behavior link is incorrect or obscured

### Mechanism 2
- **Claim**: Term cooccurrence in pretraining data acts as a significant but incomplete mechanism for factual knowledge acquisition
- **Mechanism**: Removing documents containing cooccurring entity pairs reduces the model's ability to answer relational queries, but performance remains above baseline
- **Core assumption**: The matching heuristic successfully captures the majority of documents expressing the target relation
- **Evidence anchors**: Section 4.2 shows performance remains above majority baseline after cooccurrence removal
- **Break condition**: If removing cooccurrences caused performance to drop to random chance, cooccurrence would be proven as the sole mechanism

### Mechanism 3
- **Claim**: Lexical matching (BM25) is currently more effective than semantic matching (DPR) for identifying pretraining documents that influence knowledge acquisition
- **Mechanism**: When intervening on MCQA tasks, removing documents identified via BM25 suppresses learning more effectively than removing those identified via DPR
- **Core assumption**: The relevance scores correlate with the information density required to learn a fact
- **Evidence anchors**: Section 5.2 and 5.3 show BM25 outperforms DPR across datasets and model scales
- **Break condition**: If DPR identified semantically equivalent but lexically distinct documents, it should suppress learning; failure to do so suggests limitations in dense retrieval methods

## Foundational Learning

- **Concept: Counterfactual Retraining**
  - **Why needed here**: The core methodology depends on understanding that one can "branch" a training run from an intermediate checkpoint
  - **Quick check question**: If you retrain a model from step 2,000 to 2,500 with modified data, how does that differ from fine-tuning the final model on that data? (Answer: It isolates the effect of that specific training window on the learning process)

- **Concept: Information Retrieval (IR) Relevance Scoring**
  - **Why needed here**: The "Matching Documents" stage requires selecting documents from millions of candidates
  - **Quick check question**: Why might a document score high on BM25 but low on DPR for a specific question? (Answer: Lexical keyword overlap vs. semantic vector similarity)

- **Concept: Evaluation Metrics for Knowledge (OLMES)**
  - **Why needed here**: To measure the effect of the intervention, one must define what "learning" means
  - **Quick check question**: In the context of this paper, what defines an item as "permanently learned" at a checkpoint? (Answer: It is answered correctly at that checkpoint and all subsequent ones)

## Architecture Onboarding

- **Component map**:
  - Pretraining corpus (partitioned into batches) -> Intermediate model checkpoints -> Evaluation dataset -> Selector function -> Matcher heuristic -> Intervenor function -> Modified data batch -> Counterfactual model checkpoint

- **Critical path**:
  1. Select items the model gets right at step t (but not t-1)
  2. Match: Scan data batch Dt for relevant docs (using BM25/cooccurrence)
  3. Intervene: Swap/Remove relevant docs; Retrain M(t-1) â†’ M(int)t
  4. Evaluate: Compare accuracy of Mt vs M(int)t on target items

- **Design tradeoffs**:
  - Intervention Granularity: Token-level editing (high precision, low impact) vs. Document removal (high impact, introduces domain shift/confounds)
  - Matching Scale: Intervening on large batches (computationally expensive) vs. small slices (risk of missing relevant docs)
  - Matching Method: Cooccurrence (fast, good for facts) vs. IR (computationally heavier, required for abstract knowledge)

- **Failure signatures**:
  - High Variance: Irreducible noise in distributed training leads to different results for identical runs (mitigate by repeated runs)
  - Incomplete Suppression: Performance remains above baseline after removing matched docs (indicates missing relevant docs or multi-hop learning)
  - Control Group Drift: Interventions accidentally degrade performance on unrelated items (implies intervention was too aggressive/broad)

- **First 3 experiments**:
  1. Replication (ParaRel): Using the open-sourced code/data, replicate the "suppress learning" experiment on OLMo-1B for a single relation (e.g., "P17: located in") to validate the pipeline setup
  2. Matching Ablation: Run the "promote learning" experiment on a subset of SciQ, comparing BM25 vs. DPR retrieval to verify the finding that BM25 is more effective for causal intervention
  3. Timing Check: Select a set of items learned late in pretraining and attempt to "promote" them at an earlier checkpoint to test the limits of the model's capacity to learn early

## Open Questions the Paper Calls Out

- **Open Question 1**: What specific latent mechanisms allow language models to retain factual knowledge when explicit subject-object cooccurrences are removed from the immediate training data?
  - **Basis in paper**: The authors observe that models maintain performance above the majority baseline even after cooccurrences are removed
  - **Why unresolved**: The study design isolates interventions to the data batch immediately preceding the learning checkpoint
  - **What evidence would resolve it**: Applying the intervention recipe to remove multi-hop reasoning chains or semantically similar terms in earlier data batches

- **Open Question 2**: Can more precise document matching heuristics fully explain an LM's ability to answer complex knowledge queries?
  - **Basis in paper**: The authors note that "extant methods for identifying relevant training documents do not fully explain an LM's ability"
  - **Why unresolved**: Current heuristics (BM25, DPR) are approximations; it remains unclear if a theoretical "perfect" matching method would close the explanatory gap
  - **What evidence would resolve it**: Developing a high-precision matching method that identifies only documents entailing the specific answer

- **Open Question 3**: To what extent does the accumulation of information in data batches prior to the intervention window contribute to the learning of specific facts?
  - **Basis in paper**: The authors hypothesize that the "remaining performance gap in our experiments may be due to information in documents from past data batches that we did not intervene on"
  - **Why unresolved**: Due to computational constraints, the interventions were limited to specific time windows
  - **What evidence would resolve it**: Applying the intervention recipe to modify relevant documents across the entire history of pretraining

## Limitations

- The methodology's core limitation lies in the imperfect mapping between evaluation items and training documents, meaning observed effects may underestimate true data-behavior relationships
- Distributed training environment introduces irreducible noise requiring 3-5 repeated runs for statistical significance, making experiments computationally expensive
- The causal attribution relies on specific training checkpoints being sufficiently plastic - if a model has already encoded knowledge before the intervention batch, the effect may be muted

## Confidence

**High Confidence**: The basic retraining methodology (rewriting history from intermediate checkpoints) is technically sound and produces reproducible results across multiple datasets. The observation that cooccurrence-based interventions reduce but don't eliminate fact learning is robust and clearly demonstrated.

**Medium Confidence**: The relative effectiveness of BM25 versus DPR for document retrieval in causal interventions is supported by experimental results but may depend on dataset characteristics and model scale.

**Low Confidence**: The paper's interpretation of "multi-hop learning" and knowledge acquisition from non-cooccurring sources remains speculative. While the data shows performance doesn't drop to baseline after cooccurrence removal, the mechanism for this residual learning is not definitively established.

## Next Checks

1. **Matching Completeness Analysis**: For a subset of ParaRel items, manually annotate whether the documents identified by cooccurrence matching actually express the target relation. This would quantify the false negative rate and help interpret why interventions don't fully suppress learning.

2. **Intervention Granularity Study**: Replicate the MCQA experiments comparing document removal versus token-level editing for a small set of items. This would test whether the paper's choice of document-level intervention is optimal or if finer-grained editing could achieve stronger effects.

3. **Cross-Dataset Transfer Test**: Apply the exact same intervention methodology (BM25-based document removal) to a different knowledge-intensive task like Natural Questions or HotpotQA. This would validate whether the observed effectiveness of lexical matching generalizes beyond the current MCQA datasets.