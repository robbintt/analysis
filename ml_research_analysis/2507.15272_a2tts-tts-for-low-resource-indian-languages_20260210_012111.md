---
ver: rpa2
title: 'A2TTS: TTS for Low Resource Indian Languages'
arxiv_id: '2507.15272'
source_url: https://arxiv.org/abs/2507.15272
tags:
- speaker
- speech
- dataset
- languages
- indicsuperb
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of generating high-quality, speaker-consistent
  speech for unseen speakers in low-resource Indian languages using a diffusion-based
  TTS system. The core method integrates a speaker encoder with a cross-attention-based
  duration predictor to condition both prosody and timing on reference audio, enabling
  zero-shot speaker adaptation.
---

# A2TTS: TTS for Low Resource Indian Languages

## Quick Facts
- arXiv ID: 2507.15272
- Source URL: https://arxiv.org/abs/2507.15272
- Reference count: 3
- Average speaker similarity score (SIM-O) of 0.7548, outperforming baseline Grad-TTS

## Executive Summary
This work addresses the challenge of generating high-quality, speaker-consistent speech for unseen speakers in low-resource Indian languages using a diffusion-based TTS system. The core method integrates a speaker encoder with a cross-attention-based duration predictor to condition both prosody and timing on reference audio, enabling zero-shot speaker adaptation. Classifier-free guidance is used during inference to enhance pronunciation and speaker consistency. Evaluated on the IndicSUPERB dataset across seven Indian languages, the model achieves an average speaker similarity score (SIM-O) of 0.7548, outperforming the baseline Grad-TTS.

## Method Summary
The system uses a diffusion-based TTS architecture built on Grad-TTS with 2× channel capacity for multi-speaker generation. A pre-trained speaker encoder (UnitSpeech) extracts embeddings from 2-second reference audio samples to condition the DDPM decoder. Cross-attention between text embeddings and reference mel spectrograms drives the duration predictor, enabling speaker-adaptive prosody modeling. The model is trained on IndicSUPERB data (7 Indian languages) for 1,500 epochs plus 1,000 fine-tuning epochs, with inference using classifier-free guidance and HiFi-GAN vocoder.

## Key Results
- Average SIM-O speaker similarity score of 0.7548 across 7 Indian languages
- Outperforms baseline Grad-TTS with speaker conditioning on the same dataset
- Achieves zero-shot speaker adaptation without fine-tuning on target speakers

## Why This Works (Mechanism)

### Mechanism 1
Speaker encoder conditioning enables zero-shot voice cloning by extracting identity embeddings from short reference audio. A pre-trained speaker encoder (UnitSpeech, trained on VoxCeleb2) extracts fixed-dimensional embeddings that condition the DDPM decoder's score function, steering the diffusion process toward target speaker characteristics without fine-tuning. Core assumption: embeddings generalize to unseen Indian language speakers. Evidence: abstract states "speaker encoder extracts embeddings from short reference audio samples to condition the DDPM decoder for multispeaker generation."

### Mechanism 2
Cross-attention between text embeddings and reference mel spectrogram improves speaker-adaptive duration modeling. Text embeddings serve as queries while reference mel spectrogram provides keys/values, allowing prosodic timing patterns from reference to influence phoneme durations without requiring linguistic content overlap. Core assumption: speaker-specific prosody transfers across utterances via attention. Evidence: "reference mel spectrogram is chosen to be unrelated to the input text" to avoid memorizing durations.

### Mechanism 3
Classifier-free guidance at inference amplifies speaker and text conditioning without retraining. The score function is modified by adding the difference between conditional and unconditional score predictions, where the unconditional condition is the dataset-wide mean mel spectrogram. Core assumption: mean mel spectrogram is sufficient unconditional baseline. Evidence: "This inference-time modification amplifies the influence of the speaker and text conditions without requiring changes to the training process."

## Foundational Learning

- **Denoising Diffusion Probabilistic Models (DDPMs)**: The mel-spectrogram decoder iteratively denoises Gaussian noise over ~N steps. Understanding the forward/reverse process is essential for debugging convergence issues. Quick check: Can you explain why the reverse process requires learning a score function rather than directly predicting X₀?

- **Cross-Attention in Sequence-to-Sequence Models**: The duration predictor uses cross-attention to align phoneme queries with acoustic reference keys. Misalignment here produces timing artifacts. Quick check: What would happen if queries and keys came from the same modality (both text or both mel)?

- **Speaker Embeddings (d-vectors/x-vectors)**: The system relies on fixed-dimensional speaker representations for conditioning. Understanding their properties (speaker-discriminative, text-independent) is critical for evaluating similarity metrics. Quick check: Why might cosine similarity of embeddings be insufficient for perceptual speaker similarity?

## Architecture Onboarding

- **Component map**: Text Input → Text Encoder → E_t (phoneme embeddings) → Cross-Attention → Duration Predictor → D → DDPM Decoder ← (conditioned on e_s, aligned text) → X₀ (mel) → HiFi-GAN Vocoder → Audio

- **Critical path**: 1) Reference audio quality → speaker embedding accuracy, 2) Cross-attention alignment → duration naturalness, 3) CFG scale γ → speaker consistency vs. intelligibility tradeoff

- **Design tradeoffs**: Double channels vs. memory (2× channels vs. Grad-TTS for multi-speaker capacity), Language-specific vs. multilingual (7 separate models vs. unified model), Training duration (~2,500 total epochs is computationally expensive)

- **Failure signatures**: Robotic/artificial timing (cross-attention not learning speaker prosody), Wrong speaker identity (speaker encoder failing on out-of-domain voices), Poor pronunciation (CFG scale too high or text encoder alignment issues)

- **First 3 experiments**: 1) Baseline reproduction: Train Grad-TTS with speaker conditioning (no cross-attention duration, no CFG) on Hindi to establish SIM-O baseline (~0.72), 2) Ablation: Cross-attention vs. standard duration (compare with/without reference mel cross-attention), 3) CFG sweep: Vary guidance scale γ ∈ {0.5, 1.0, 1.5, 2.0} and measure SIM-O vs. CER tradeoff

## Open Questions the Paper Calls Out

- **Scalability to additional languages**: Can the cross-attention-based duration predictor effectively generalize to additional low-resource languages while maintaining high speaker similarity and expressiveness? Current study restricted to seven languages within IndicSUPERB dataset.

- **Training efficiency improvement**: Can training efficiency be improved to reduce the high computational cost associated with the model's slow convergence? The model requires approximately 2000 epochs to converge, resulting in "slow training times and demands resource, which may limit scalability."

- **Fine-tuning for out-of-distribution speakers**: Would integrating fine-tuning strategies significantly improve voice matching for speakers with characteristics distinct from the training data? The model struggles with "mimicking speaker characteristics for voices that are significantly different from those seen during training."

## Limitations

- Cross-lingual speaker embedding generalization uncertainty: UnitSpeech trained on English/multilingual data may not generalize well to Indian language speakers without validation against language-specific speaker encoders.

- Computational cost barrier: 2,500 total training epochs on 8x H100 GPUs represents significant expense that may limit practical applicability in low-resource settings.

- CFG scale sensitivity not quantified: The paper uses classifier-free guidance but doesn't report optimal guidance scale values or sensitivity analysis, leaving the SIM-O vs. CER tradeoff unexplored.

## Confidence

- **High confidence**: The core diffusion-based architecture and integration of speaker conditioning via UnitSpeech embeddings is technically sound and well-established in prior work.
- **Medium confidence**: SIM-O scores and CER metrics are reported, but evaluation methodology lacks depth (only 20 speakers per language, no statistical significance testing).
- **Low confidence**: Practical deployment considerations (inference speed, memory requirements, guidance scale optimization) are not thoroughly addressed.

## Next Checks

1. **Speaker embedding ablation**: Train and evaluate A2TTS using a speaker encoder fine-tuned on target Indian languages versus pre-trained UnitSpeech model to quantify cross-lingual transfer performance.

2. **CFG scale sensitivity analysis**: Systematically vary classifier-free guidance scale γ and plot the tradeoff between SIM-O (speaker similarity) and CER (pronunciation accuracy) on held-out speakers.

3. **Resource efficiency comparison**: Benchmark A2TTS against simpler adaptation methods using the same computational budget, measuring not just final performance but training convergence speed and inference latency.