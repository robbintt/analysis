---
ver: rpa2
title: 'LyS at SemEval 2025 Task 8: Zero-Shot Code Generation for Tabular QA'
arxiv_id: '2508.09012'
source_url: https://arxiv.org/abs/2508.09012
tags:
- code
- tabular
- data
- question
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a zero-shot code generation approach for Tabular
  Question Answering (Tabular QA) in SemEval 2025 Task 8. The system leverages a Large
  Language Model (LLM) to dynamically generate executable Python code for extracting
  information from tabular data based on natural language questions.
---

# LyS at SemEval 2025 Task 8: Zero-Shot Code Generation for Tabular QA

## Quick Facts
- arXiv ID: 2508.09012
- Source URL: https://arxiv.org/abs/2508.09012
- Reference count: 14
- Primary result: Zero-shot code generation for tabular QA achieving 65-68 points accuracy

## Executive Summary
This paper presents a zero-shot code generation approach for Tabular Question Answering (Tabular QA) in SemEval 2025 Task 8. The system leverages a Large Language Model (LLM) to dynamically generate executable Python code for extracting information from tabular data based on natural language questions. The approach features a modular pipeline consisting of a column selector, answer generator, and iterative code fixer to handle runtime errors. While demonstrating competent performance, the system faces limitations with complex data types and schema variations.

## Method Summary
The LyS system implements a zero-shot code generation framework for tabular QA that converts natural language questions into executable Python code. The method uses an LLM to analyze table structure and question semantics, generating code that can extract answers directly from tabular data. The system employs a three-stage pipeline: column selection to identify relevant table attributes, code generation for answer extraction, and an iterative error correction mechanism. This approach operates without task-specific fine-tuning, relying instead on the LLM's general capabilities for code generation and reasoning.

## Key Results
- Achieved 65 points accuracy in Subtask 1 during test phase
- Achieved 68 points accuracy in Subtask 2 during test phase
- Ranked 32nd and 31st respectively among 49 participating teams

## Why This Works (Mechanism)
The system works by leveraging LLMs' ability to understand both natural language and programming syntax, creating a bridge between user questions and data extraction logic. The zero-shot approach eliminates the need for task-specific training while maintaining flexibility across different table schemas. The modular design allows for targeted improvements - the column selector handles semantic understanding of table structure, the answer generator produces executable code, and the iterative fixer addresses runtime errors through repeated attempts.

## Foundational Learning
1. **Tabular QA fundamentals** - Understanding how natural language questions map to structured data queries; needed to bridge semantic understanding with data extraction; quick check: test with simple "count" or "sum" questions
2. **LLM code generation capabilities** - Leveraging large language models for generating executable code from natural language; needed for zero-shot operation without fine-tuning; quick check: validate generated code runs correctly on sample tables
3. **Schema adaptation techniques** - Methods for handling varying table structures and column naming conventions; needed for generalization across datasets; quick check: test on tables with different organizational patterns
4. **Error handling in automated code execution** - Strategies for detecting and fixing runtime errors in generated code; needed to improve reliability; quick check: introduce controlled errors and verify correction
5. **Iterative refinement processes** - Multi-pass approaches for improving initial outputs; needed to handle complex edge cases; quick check: compare single-pass vs multi-pass performance
6. **Zero-shot learning principles** - Operating without task-specific training while maintaining performance; needed for rapid deployment across domains; quick check: validate performance on unseen table types

## Architecture Onboarding

**Component map:** User Question -> Column Selector -> Answer Generator -> Iterative Code Fixer -> Execution Environment -> Answer

**Critical path:** Question understanding flows through column selection to code generation, with error correction providing fallback mechanism for failed executions.

**Design tradeoffs:** Zero-shot approach trades task-specific optimization for generalization capability, accepting moderate performance in exchange for flexibility across domains.

**Failure signatures:** Complex data types (lists, dictionaries) cause execution failures; schema variations lead to incorrect column selection; ambiguous questions result in semantically incorrect code generation.

**3 first experiments:**
1. Test generated code execution success rate on simple vs complex table structures
2. Measure performance degradation when removing the iterative code fixer component
3. Evaluate accuracy on tables with non-standard column naming conventions

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Struggles with complex data types like lists and dictionaries, limiting effectiveness on real-world tabular data
- Middle-ranking performance (31st-32nd) indicates room for improvement compared to top systems
- Schema adaptation capabilities remain uncertain without testing on diverse table structures

## Confidence
- **High confidence**: Zero-shot code generation framework using LLMs is technically sound and produces working code in many cases
- **Medium confidence**: Ranking of 31st-32nd out of 49 teams reflects competitive but not leading performance
- **Low confidence**: Claims about "strong performance" are questionable given middle-ranking position and explicit limitations

## Next Checks
1. Test the system on benchmark datasets with explicitly defined complex data types (nested lists, dictionaries, mixed-type columns) to quantify the performance gap
2. Conduct ablation studies removing the iterative code fixer to measure its actual contribution to overall accuracy
3. Evaluate schema adaptation capabilities by testing on tables with varying structures and column naming conventions outside the competition dataset