---
ver: rpa2
title: Incentivizing Agentic Reasoning in LLM Judges via Tool-Integrated Reinforcement
  Learning
arxiv_id: '2510.23038'
source_url: https://arxiv.org/abs/2510.23038
tags:
- reasoning
- response
- arxiv
- learning
- judges
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TIR-Judge, a framework that trains LLM judges
  with tool-integrated reinforcement learning to improve accuracy in evaluating responses.
  By combining code execution with iterative RL, TIR-Judge outperforms strong reasoning-based
  judges by up to 6.4% (pointwise) and 7.7% (pairwise), and matches 96% of Claude-Opus-4's
  performance in listwise ranking with only 8B parameters.
---

# Incentivizing Agentic Reasoning in LLM Judges via Tool-Integrated Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2510.23038
- **Source URL**: https://arxiv.org/abs/2510.23038
- **Reference count**: 25
- **Primary result**: TIR-Judge achieves up to 6.4% pointwise and 7.7% pairwise gains over strong reasoning-based judges, matching 96% of Claude-Opus-4's listwise ranking performance with only 8B parameters.

## Executive Summary
This paper introduces TIR-Judge, a framework that trains LLM judges with tool-integrated reinforcement learning to improve accuracy in evaluating responses. By combining code execution with iterative RL, TIR-Judge outperforms strong reasoning-based judges by up to 6.4% (pointwise) and 7.7% (pairwise), and matches 96% of Claude-Opus-4's performance in listwise ranking with only 8B parameters. Notably, TIR-Judge-Zero, trained without distilled data, matches or exceeds its distilled variant, demonstrating that tool-augmented judges can self-improve through RL. The approach achieves consistent gains across seven benchmarks, showing strong generalization and efficiency.

## Method Summary
TIR-Judge trains LLM judges to integrate reasoning with code execution through iterative reinforcement learning. The framework uses DAPO (a GRPO variant) to train Qwen3-4B/8B models that generate reasoning trajectories with code blocks, execute them in a sandbox, and receive structured rewards based on correctness, format compliance, and tool efficiency. The training data combines verifiable domains (math, competitive programming) with non-verifiable domains (helpfulness, safety). For the Zero variant, iterative cycles of rejection sampling, supervised fine-tuning, and RL enable self-bootstrapping without teacher distillation.

## Key Results
- TIR-Judge achieves 6.4% pointwise and 7.7% pairwise gains over strong reasoning-based judges
- Matches 96% of Claude-Opus-4's listwise ranking performance with only 8B parameters
- TIR-Judge-Zero, trained without distilled data, matches or exceeds the performance of distilled variants
- Consistent improvements across seven benchmarks including PPE, IFBench, CodeJudgeBench, and RewardBench

## Why This Works (Mechanism)

### Mechanism 1
Iterative RL with code execution feedback enables judges to learn when and how to invoke tools, overcoming limitations of inference-time-only tool augmentation. The model samples reasoning trajectories with code generation, executes code in a sandbox, receives structured rewards (correctness × format × tool efficiency), and updates policy via DAPO/GRPO-style clipped gradients. Execution outputs are masked during loss to prevent memorization, forcing the model to learn tool-use policies rather than output patterns.

### Mechanism 2
Rejection sampling followed by SFT creates progressively higher-quality training data, enabling self-bootstrapping without distillation. After initial RL, sample multiple trajectories per prompt, filter for correctness + format + error-free execution, retain only the shortest valid trajectory per prompt, use this curated data for SFT before the next RL round. This RS→SFT→RL cycle repeats, with each iteration producing cleaner training signals.

### Mechanism 3
Diverse task mixing across verifiable and non-verifiable domains teaches the model to discriminate when tool invocation is beneficial versus unnecessary. Training data combines verifiable domains (math, competitive programming, instruction-following with constraints) where code execution provides ground truth, with non-verifiable domains (helpfulness, safety) where pure reasoning suffices. A format reward heuristic penalizes unnecessary tool calls for non-reasoning tasks.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO/DAPO)**: TIR-Judge uses DAPO, a GRPO variant, for RL training. Understanding group-based advantage estimation and clipping is essential before implementing or debugging the training loop.
  - Quick check: Can you explain why DAPO normalizes rewards within groups of rollouts rather than across batches?

- **Tool-Integrated Reasoning (TIR) Trajectories**: The training data format interleaves reasoning steps, code blocks, and execution outputs. Understanding this multi-turn structure is critical for data preparation and trajectory filtering.
  - Quick check: Given a trajectory `s_k = {r_1, c_1, o_1, ..., r_k, c_k, o_k}`, which components are masked during loss computation and why?

- **Rejection Sampling for SFT Data Curation**: The iterative training pipeline depends on filtering trajectories by correctness, format, and tool efficiency before SFT. Understanding filtering criteria is essential for reproducing results.
  - Quick check: When multiple valid trajectories exist for the same prompt, which one should be retained for SFT and what is the rationale?

## Architecture Onboarding

- **Component map**: Policy Model (J_θ) -> Code Executor (I) -> Reward Computation -> RL Framework (DAPO) -> SFT Pipeline
- **Critical path**: 1) Data preparation: Curate ~26k preference pairs spanning verifiable/non-verifiable domains, apply 8-gram decontamination; 2) Cold start (optional): SFT on distilled trajectories from teacher model, masking execution outputs; 3) RL training: Sample G=8 rollouts per prompt with tool access, compute rewards, update policy; 4) Iterative refinement (Zero variant): RS→SFT→RL cycles, selecting best checkpoint on validation set
- **Design tradeoffs**: Distill vs. Zero (distillation provides faster convergence but requires teacher model; Zero is self-sufficient but needs more iterations); Tool call budget (max 3 calls balances verification capability against inference cost; increasing may help complex tasks but risks over-reliance); Error message truncation (reduces context length but may lose debugging information)
- **Failure signatures**: Tool overuse on non-reasoning tasks (check if format reward heuristic is applied correctly for safety/helpfulness prompts); Format collapse (model generates invalid output structures; verify reward penalization is working); Execution errors in valid trajectories (sandbox may differ from training environment; check library availability and timeout settings); No improvement across iterations (validation accuracy plateau suggests RS filtering is too strict or data diversity insufficient)
- **First 3 experiments**: 1) Ablate tool access: Train identical model without code executor on same data; expect <1% gain vs. ~6-8% with tools (confirms RL×tool synergy); 2) Single-domain training: Train on only reasoning or only helpfulness data; expect poor cross-domain transfer (validates task diversity mechanism); 3) Zero vs. Distill comparison: Run both variants with matched compute; expect Zero to match or exceed Distill after 2-3 iterations (validates self-bootstrapping)

## Open Questions the Paper Calls Out

### Open Question 1
Can the TIR-Judge framework effectively integrate non-code tools, such as web search or external APIs, while maintaining the stability and accuracy observed with code execution? The Conclusion states: "In future work, we aim to expand the range of tools and training tasks used in RL..." The current implementation and benchmarks exclusively utilize a Python code executor; the interaction dynamics and error handling for non-deterministic tools like search remain unexplored.

### Open Question 2
Does integrating a tool-augmented judge into the RLHF loop for policy model training introduce specific optimization instabilities compared to using standard reward models? The Conclusion notes a future direction is to "explore using TIR-Judge to enhance policy model training." While the paper demonstrates TIR-Judge improves Best-of-N selection, the feedback loop dynamics during actual policy gradient updates are not analyzed.

### Open Question 3
Is there an optimal curriculum or ratio of verifiable versus non-verifiable training data that maximizes the transfer of tool-use reasoning to subjective domains? Figure 3 demonstrates that training on specific task types leads to poor transfer, and mixing is essential, but the precise boundary for generalization is not defined. The paper establishes that data diversity is critical but does not determine if the 26k pair mixture is optimal or merely sufficient.

## Limitations

- Evaluation against strong baselines (Claude-Opus-4, GPT-4.5) that are not open-weight models makes it difficult to assess whether gains are due to the method or model competitiveness
- Lack of detailed ablation on rejection sampling criteria and iteration count limits full confidence in self-bootstrapping claims
- Benchmarks vary in task type and difficulty, but the paper does not provide per-task breakdown of gains or error analysis

## Confidence

- **High Confidence**: The core mechanism of using iterative RL with code execution feedback is well-supported by empirical results showing significant gains over inference-time-only tool augmentation
- **Medium Confidence**: The self-bootstrapping capability of TIR-Judge-Zero is plausible given the results, but the lack of detailed ablation on rejection sampling criteria and iteration count limits full confidence
- **Low Confidence**: The generalization claim across seven benchmarks is reasonable, but the benchmarks vary in task type and difficulty, and the paper does not provide per-task breakdown of gains or error analysis

## Next Checks

1. **Ablate Tool Access**: Train an identical model without code executor on the same data to confirm that the ~6-8% gains are due to RL×tool synergy rather than other factors

2. **Single-Domain Training**: Train on only reasoning or only helpfulness data to test whether task diversity is essential for cross-domain transfer, as claimed

3. **Zero vs. Distill Iteration Count**: Run both TIR-Judge-Zero and TIR-Judge-Distill with matched compute and monitor performance over iterations to verify that Zero matches or exceeds Distill after 2-3 iterations