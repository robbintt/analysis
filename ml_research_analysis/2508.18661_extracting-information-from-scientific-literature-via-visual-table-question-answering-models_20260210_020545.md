---
ver: rpa2
title: Extracting Information from Scientific Literature via Visual Table Question
  Answering Models
arxiv_id: '2508.18661'
source_url: https://arxiv.org/abs/2508.18661
tags:
- table
- document
- information
- documents
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study explored three approaches for processing table data
  in scientific papers to improve extractive question answering for systematic reviews.
  The methods evaluated were: (1) Optical Character Recognition (OCR) for text extraction,
  (2) Pre-trained vision-language models for document visual question answering, and
  (3) Table detection and structure recognition models to extract and merge table
  content with surrounding text.'
---

# Extracting Information from Scientific Literature via Visual Table Question Answering Models

## Quick Facts
- **arXiv ID**: 2508.18661
- **Source URL**: https://arxiv.org/abs/2508.18661
- **Reference count**: 10
- **Primary result**: Structure preservation approaches outperform others in representing and organizing table content for scientific DocQA

## Executive Summary
This study evaluates three approaches for processing table data in scientific papers to improve extractive question answering for systematic reviews: Optical Character Recognition (OCR), pre-trained vision-language models, and table detection with structure recognition. Using ten RF-EMF-related documents and seven predefined question-answer pairs, the results demonstrate that approaches preserving table structure outperform others, particularly in representing and organizing table content. The study identifies accurate recognition of specific notations and symbols as critical for improved results in scientific document processing.

## Method Summary
The paper compares three approaches for extracting information from scientific tables: (1) OCR-based extraction using Tesseract and LaTeX OCR for text conversion, (2) end-to-end vision-language models including Donut, Pix2Struct, and GPT-4 for direct question answering from document images, and (3) table detection and structure recognition using Table Transformer to extract and merge table content with surrounding text. The evaluation uses ten RF-EMF-related documents with seven predefined question-answer pairs, focusing on both text extraction accuracy and question-answering performance.

## Key Results
- Structure preservation approaches outperform others in representing and organizing table content
- Accurate recognition of specific notations and symbols emerges as critical for improved results
- OCR-based approaches are affected by the complexity of documents containing specific symbols like exponential and temperature symbols

## Why This Works (Mechanism)

### Mechanism 1: Structure Preservation Enables Relational Reasoning
- Claim: Maintaining table structural integrity improves extractive QA performance on scientific documents, conditional on accurate cell-level content extraction
- Mechanism: When row/column relationships are preserved, the QA model can correctly associate values with their headers and contextual metadata. Flattening tables into linear text disrupts these spatial relationships, causing misattribution of values to wrong variables
- Core assumption: The downstream QA module can reason over structured representations when explicitly prompted about format
- Evidence anchors: [abstract] "approaches preserving table structure outperform the others"; [section 5.1] "pseudo-text outperformed expectations in understanding structured data"; [corpus] confirms structure preservation is critical across domains
- Break condition: If OCR fails to correctly extract cell boundaries or content, structure preservation alone cannot recover accuracy

### Mechanism 2: Symbol Recognition Quality Gates Scientific Extraction
- Claim: Accurate recognition of domain-specific symbols is a necessary precondition for correct answer extraction in scientific DocQA
- Mechanism: Scientific tables encode critical quantitative information using specialized symbols. Misrecognition at the OCR stage propagates as hallucinated or incorrect answers downstream
- Core assumption: Symbol recognition errors cannot be corrected by downstream language model reasoning without explicit correction mechanisms
- Evidence anchors: [abstract] "Accurately recognizing specific notations and symbols... emerged as a critical factor"; [section 5.1] "OCR-based approaches are affected by... specific symbols"; [corpus] identifies this as a literature gap
- Break condition: If document images are degraded or low-resolution, symbol recognition degrades independently of model choice

### Mechanism 3: Multi-stage Pipelines Accumulate Stage-specific Errors
- Claim: Table structure-aware approaches suffer from error accumulation across stages, with each stage's accuracy dependent on prior stage correctness
- Mechanism: The pipeline is serial—errors in table detection propagate to structure recognition; structure errors affect OCR region selection; OCR errors affect QA
- Core assumption: Each stage operates independently without cross-stage error recovery
- Evidence anchors: [section 3.3] "subject to accuracy at various stages of the processing pipeline"; [section 5.3] "non-trivial tabular layout" create cascading failures; [corpus] proposes uncertainty quantification at each stage
- Break condition: If any single stage fails catastrophically (e.g., merged cells misparsed), downstream stages cannot recover correct answers

## Foundational Learning

- **Concept: Document Visual Question Answering (DocQA)**
  - Why needed here: This is the core task—extracting answers from document images containing both text and structured tables
  - Quick check question: Can you explain why DocQA is harder than standard VQA for scientific documents?

- **Concept: Table Structure Recognition (TSR)**
  - Why needed here: TSR identifies rows, columns, and cell boundaries. Without this, tables flatten into unstructured text, breaking relational reasoning
  - Quick check question: What happens to a merged cell in a standard TSR pipeline designed for regular grids?

- **Concept: OCR Limitations in Scientific Contexts**
  - Why needed here: Standard OCR models are trained on natural text, not scientific notation. Understanding failure modes informs model selection
  - Quick check question: Why would LaTeX OCR potentially outperform standard text OCR for scientific tables?

## Architecture Onboarding

- **Component map:** Document Image → [Preprocessing: enhancement] → [Path A: OCR-only] → Text/LaTeX → GPT-4 QA → [Path B: End-to-end VLM] → Donut/Pix2Struct/GPT-4 → Answer → [Path C: Structure-aware] → Table Transformer → Cell OCR → GPT-4 QA

- **Critical path:** Table detection accuracy → Structure recognition (rows/columns) → Cell-level OCR → QA prompt engineering

- **Design tradeoffs:**
  - OCR-based: Fast, interpretable, but loses structure and struggles with symbols
  - End-to-end VLM: Preserves layout implicitly, but requires fine-tuning for domain-specific terms
  - Structure-aware: Explicit structure preservation, but error accumulation and failure on non-trivial layouts

- **Failure signatures:**
  - Merged cells or irregular row/column counts → TSR failure → garbage extraction
  - Dense tables with cell borders close to text → OCR misreads borders as characters
  - Domain-specific symbols (°C, Hz, p-values) → OCR substitution errors → incorrect QA answers
  - Low-resolution images → all paths degrade, but GPT-4 shows relative robustness

- **First 3 experiments:**
  1. **Baseline OCR comparison:** Run Tesseract vs. LaTeX OCR on 5 sample tables; manually inspect symbol recognition accuracy
  2. **Structure preservation test:** Flatten tables to linear text vs. structured HTML/markdown; measure QA accuracy on Q7 (variable extraction from table structure)
  3. **End-to-end VLM zero-shot evaluation:** Test Donut, Pix2Struct, and GPT-4 on all 7 predefined questions; identify which question types fail and categorize by failure mode

## Open Questions the Paper Calls Out
None

## Limitations
- Conclusions depend heavily on specific document domain (RF-EMF) and small sample size (10 documents)
- Doesn't systematically characterize which table features most affect accuracy or provide error analysis for specific failure modes
- Doesn't address computational efficiency tradeoffs between approaches or evaluate cross-domain robustness

## Confidence
- **High confidence**: Structure preservation improves performance when cell-level extraction is accurate
- **Medium confidence**: Symbol recognition quality is critical for scientific DocQA
- **Low confidence**: End-to-end VLMs are inherently superior to multi-stage pipelines

## Next Checks
1. **Cross-domain robustness test**: Evaluate all three approaches on 10 documents from different scientific domains to assess whether structure preservation advantages hold beyond RF-EMF
2. **Error propagation analysis**: Instrument each pipeline stage to measure how errors at each stage affect final accuracy, quantifying the error accumulation hypothesis
3. **Symbol recognition benchmark**: Create a controlled test set of tables containing domain-specific symbols with ground truth OCR outputs to isolate symbol recognition as a bottleneck