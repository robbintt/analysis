---
ver: rpa2
title: 'LLaDA2.0: Scaling Up Diffusion Language Models to 100B'
arxiv_id: '2512.15745'
source_url: https://arxiv.org/abs/2512.15745
tags:
- diffusion
- training
- language
- block
- llada2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents LLaDA2.0, a paradigm for scaling discrete diffusion
  language models to 100B parameters by converting pre-trained auto-regressive models
  rather than training from scratch. The authors propose a 3-phase Warmup-Stable-Decay
  training strategy to enable smooth AR-to-dLLM conversion, implement document-level
  attention masks to maintain semantic coherence during packed sequence training,
  and introduce post-training techniques including complementary masking, confidence-aware
  parallel training, and diffusion-adapted DPO.
---

# LLaDA2.0: Scaling Up Diffusion Language Models to 100B

## Quick Facts
- arXiv ID: 2512.15745
- Source URL: https://arxiv.org/abs/2512.15745
- Reference count: 13
- 100B-parameter diffusion language model achieves competitive performance with state-of-the-art auto-regressive models

## Executive Summary
This paper presents LLaDA2.0, a paradigm for scaling discrete diffusion language models to 100B parameters by converting pre-trained auto-regressive models rather than training from scratch. The authors propose a 3-phase Warmup-Stable-Decay training strategy to enable smooth AR-to-dLLM conversion, implement document-level attention masks to maintain semantic coherence during packed sequence training, and introduce post-training techniques including complementary masking, confidence-aware parallel training, and diffusion-adapted DPO. The resulting LLaDA2.0-mini (16B) and LLaDA2.0-flash (100B) models achieve competitive performance with state-of-the-art auto-regressive models, with the 100B model matching Qwen3-30B-A3B-Instruct-2507 at 73.18 average score across 47 benchmarks.

## Method Summary
LLaDA2.0 converts pre-trained auto-regressive models to discrete diffusion language models using a 3-phase Warmup-Stable-Decay (WSD) strategy. The approach begins with AR-to-BDLM conversion through progressive block-size scaling (1→4→32→64→4096), followed by full-sequence MDLM training, then decay back to compact blocks. Document-level attention masks prevent cross-document contamination in packed training sequences. Post-training includes SFT with complementary masking, confidence-aware parallel training (CAP) for accelerated inference, and diffusion-adapted DPO using ELBO-substituted log-likelihoods.

## Key Results
- LLaDA2.0-flash (100B) achieves 73.18 average score across 47 benchmarks, matching Qwen3-30B-A3B-Instruct-2507
- Superior performance on coding tasks: HumanEval 94.51, MBPP 88.29
- Strong agent capabilities: BFCL v3 score of 75.43
- CAP training improves inference throughput from 383 to 535 TPS while maintaining quality

## Why This Works (Mechanism)

### Mechanism 1: Warmup-Stable-Decay Block Sizing
Progressive block-size scaling enables stable AR-to-dLLM conversion by gradually expanding the receptive field. The model starts with block size 1 (equivalent to AR), progressively expands to 4096 for full bidirectional context, then contracts back to efficient sizes. This bridges the distribution gap between left-to-right generation and bidirectional denoising, allowing incremental adaptation of internal representations.

### Mechanism 2: Document-Level Attention Isolation
For packed sequences containing multiple documents, specialized attention masks enforce block-diagonal attention within noisy sequences and cross-attention only from noisy to earlier clean blocks. This prevents semantic contamination by ensuring cross-document attention is explicitly zeroed, maintaining coherent denoising signals.

### Mechanism 3: Confidence-Aware Parallel Training (CAP)
CAP adds an auxiliary entropy-minimizing loss on correctly predicted tokens, selectively sharpening output distributions only for correct predictions. This compels higher certainty in predictions, enabling more tokens to exceed confidence thresholds per parallel decoding step, thereby improving efficiency without quality loss.

## Foundational Learning

- **Masked Discrete Diffusion (MDLM)**: Understanding how tokens are randomly masked and reconstructed via iterative denoising is prerequisite to grasping the entire conversion paradigm. Quick check: Can you explain why MDLM enables parallel decoding but AR models cannot?

- **Block Diffusion Language Models (BDLM)**: LLaDA2.0 operates as BDLM with configurable block sizes; the WSD strategy manipulates this parameter directly. Quick check: How does block-wise generation differ from full-sequence diffusion, and what efficiency trade-offs exist?

- **Mixture-of-Experts (MoE) Scaling**: Both LLaDA2.0-mini (16B) and LLaDA2.0-flash (100B) are MoE variants; understanding expert routing is relevant to inference optimization. Quick check: Why might MoE architectures be preferred for diffusion LLMs at frontier scale?

## Architecture Onboarding

- **Component map**: Input packed document sequences → Document-level attention mask constructor → Block partitioning → AR base model → WSD scheduler (controls block size LB) → BDLM/MDLM training with masked token reconstruction → SFT with complementary masking → CAP training → DPO with ELBO-substituted log-likelihoods → Block-wise threshold decoder with hybrid acceptance

- **Critical path**: Masked token embedding initialization (add Gaussian noise to avoid gradient explosion) → Block size scheduling during WSD phases → Attention mask construction ensuring document isolation → Confidence loss integration during SFT

- **Design tradeoffs**: Block size 16 vs 32 yields quality/TPF tradeoff (16: 70.26/2.44 vs 32: 70.15/2.55); Threshold 0.95 vs 0.85 maximizes quality vs speed (quality drops at threshold 0.85); Complementary masking effective for CPT with <100B tokens but not for larger corpora

- **Failure signatures**: Gradient explosion at high mask ratios → Check masked embedding initialization; Cross-document contamination → Verify attention mask indices align with document boundaries; Poor parallel decoding speed → Check CAP loss weight λ and inference threshold calibration

- **First 3 experiments**: 1) Block size sweep: Train LLaDA2.0-mini variant with block sizes [16, 32, 64] on held-out benchmarks; measure score/TPF tradeoff. 2) Attention mask ablation: Disable document-level masking on small CPT run; compare perplexity and contamination metrics. 3) CAP loss calibration: Vary λ in L(θ) = L_SFT + λL_conf across [0.0, 0.5, 1.0]; plot TPS vs quality score.

## Open Questions the Paper Calls Out

1. **Coding and Agent Performance Advantage**: What mechanisms explain LLaDA2.0-flash's superior performance on coding and agent benchmarks compared to AR models? The paper notes advantages in structured domains without providing causal explanation, leaving this empirical observation unexplained.

2. **SFT, RL, and Inference Interaction**: How do SFT, RL, and inference acceleration techniques interact when scaling to hundreds of billions of parameters? The paper applies these techniques sequentially but doesn't study their combinatorial effects at the 100B scale.

3. **Complementary Masking Scalability**: Why does complementary masking only improve CPT efficiency on corpora below 100B tokens? The paper acknowledges this limitation without exploring why the technique loses effectiveness at larger data scales.

4. **AR-to-dLLM Conversion Generalization**: Does the conversion paradigm generalize across different base AR architectures and training regimes? The paper only demonstrates conversion from specific MoE architectures, leaving architectural requirements unexplored.

## Limitations

- **Scale vs. Generalization Trade-off**: Notable performance drop when scaling from 16B to 100B in some benchmarks, suggesting optimization challenges at frontier scale
- **Context Window Limitations**: Performance degrades substantially beyond native 32k windows despite YaRN context extension to 64k
- **DPO Adaptation Uncertainty**: Diffusion-adapted DPO is presented as an initial attempt without systematic ablation studies or comparative analysis against standard DPO

## Confidence

- **High Confidence**: Claims about Warmup-Stable-Decay training strategy effectiveness and document-level attention mask necessity are well-supported by experimental validation
- **Medium Confidence**: Claims regarding coding task superiority are supported by benchmarks but lack comparative analysis against specialized coding models
- **Low Confidence**: Claims about CAP training's universal effectiveness across scales are primarily validated on the 100B model, with smaller-scale benefits underexplored

## Next Checks

1. **Block Size Scalability Validation**: Implement WSD training on a 7B parameter model with systematic block size variation [8, 16, 32, 64] to validate whether progressive scaling generalizes across scales.

2. **Document Mask Boundary Stress Test**: Design ablation study with intentionally misaligned document boundaries and block partitions to quantify mask effectiveness and identify failure modes.

3. **CAP Training Hyperparameter Sensitivity**: Conduct comprehensive sweep of confidence loss weight λ across [0.0, 0.1, 0.5, 1.0, 2.0] on 16B model, measuring both inference TPS and quality score at multiple confidence thresholds.