---
ver: rpa2
title: 'Overcoming Algorithm Aversion with Transparency: Can Transparent Predictions
  Change User Behavior?'
arxiv_id: '2508.03168'
source_url: https://arxiv.org/abs/2508.03168
tags:
- algorithm
- participants
- transparency
- aversion
- predictions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of algorithm aversion by examining
  whether transparency and adjustability can improve user acceptance of machine learning
  predictions. The research conceptually replicates and extends Dietvorst et al.'s
  (2018) work through a pre-registered user study with 280 participants, introducing
  an interpretable GAM model that provides visual explanations of its decision logic.
---

# Overcoming Algorithm Aversion with Transparency: Can Transparent Predictions Change User Behavior?

## Quick Facts
- arXiv ID: 2508.03168
- Source URL: https://arxiv.org/abs/2508.03168
- Reference count: 7
- Primary result: Adjustability significantly reduces algorithm aversion (73.9% vs 51.1% model usage) but transparency alone shows modest, insignificant effects

## Executive Summary
This study examines whether transparency and adjustability can reduce algorithm aversion - the tendency for people to reject algorithmic predictions even when they outperform human judgment. The researchers conducted a pre-registered user study with 280 participants who made bike rental predictions under different conditions of transparency (white-box vs black-box models) and adjustability (can't-change, adjust-by-50, use-freely). The study introduced an interpretable GAM model that provides visual explanations of its decision logic. Results show that allowing users to modify predictions significantly increases model acceptance, while transparency alone shows only a modest impact. The findings suggest that providing users with control over predictions is more effective than simply revealing the model's internal logic.

## Method Summary
The study conceptually replicated and extended Dietvorst et al.'s (2018) work through a pre-registered user study with 280 participants recruited via Prolific. Participants made bike rental predictions in three phases: practice, control, and treatment. The experimental design used a 2x3 factorial structure varying transparency (white-box GAM with visual explanations vs black-box model) and adjustability (can't-change, adjust-by-50, use-freely). The GAM model used meteorological features and provided visual representations of feature effects and interaction effects. Task performance was measured by the average absolute error of participants' predictions, and algorithm aversion was operationalized as the proportion of trials where participants chose to use their own predictions rather than the model's predictions.

## Key Results
- Adjustability significantly reduces algorithm aversion: 73.9% model usage in adjust-by-50 condition vs 51.1% in can't-change condition (χ²(1, N=180)=8.98, p=.003)
- Transparency alone showed only a modest, statistically insignificant effect on model choice and task performance
- Participants in the adjust-by-50 condition performed slightly better (MAE = 62.94) than those in the can't-change condition (MAE = 63.80), though the difference was not statistically significant
- The study replicates Dietvorst et al.'s (2018) adjustability effect but finds no significant interaction between transparency and adjustability

## Why This Works (Mechanism)
The study reveals that algorithm aversion stems primarily from a perceived loss of control rather than from a lack of understanding of the model's decision-making process. When users can adjust predictions, even within limited bounds, they feel more ownership over the final decision, which reduces their tendency to reject the model. The transparency mechanism (visual explanations of GAM decision logic) provides some benefit but is insufficient on its own to overcome the psychological barrier of algorithm aversion. The mechanism suggests that user acceptance of algorithmic predictions depends more on perceived agency than on comprehension of the underlying model.

## Foundational Learning

**Algorithm Aversion Concept**
- Why needed: Understanding why users reject algorithmic predictions despite their superiority is crucial for designing human-AI systems
- Quick check: Users consistently perform worse when they reject algorithmic predictions in favor of their own judgment

**Interpretability vs. Control Tradeoff**
- Why needed: Determines whether explaining model logic or providing user control is more effective for adoption
- Quick check: Adjustability shows stronger effects than transparency in reducing algorithm aversion

**GAM Model Interpretability**
- Why needed: Provides a concrete example of how white-box models can be designed to support user understanding
- Quick check: Visual representations of feature effects and interactions help users understand model predictions

## Architecture Onboarding

**Component Map**
User Interface -> GAM Model -> Visual Explanations -> Prediction Adjustment Interface -> Performance Metrics

**Critical Path**
User receives prediction with visual explanation → User decides to accept or adjust → Adjustment is applied within bounds → Performance is measured

**Design Tradeoffs**
Transparency (explanatory power) vs. Simplicity (cognitive load) - Visual explanations provide interpretability but may overwhelm users with information

**Failure Signatures**
- Users ignore visual explanations entirely
- Users over-adjust predictions, reducing overall accuracy
- Transparency features increase cognitive load without improving acceptance

**First Experiments**
1. Test whether adding explanatory tooltips to visual elements improves transparency effects
2. Vary adjustment bounds to find optimal level of user control
3. Compare GAM interpretability with other white-box models like decision trees

## Open Questions the Paper Calls Out
The study identifies several open questions: whether transparency effects emerge over repeated interactions with the algorithm, how these findings generalize to high-stakes domains like healthcare or finance, and whether there are interaction effects between transparency and adjustability that require larger sample sizes to detect. The authors also note the need to examine long-term adaptation effects and real-world deployment scenarios where stakes might be higher.

## Limitations
- Transparency effects were modest and statistically insignificant, suggesting visual explanations alone may not substantially overcome algorithm aversion
- The bike rental prediction task may not generalize to high-stakes domains where algorithm aversion is most pronounced
- The study population consisted of Prolific participants, limiting generalizability to professional decision-makers

## Confidence

**Adjustability effect on model usage**: High (statistically significant result with pre-registration)
**Transparency effect on algorithm aversion**: Low-Medium (small effect size, not statistically significant)
**Transparency effect on task performance**: Low (no significant difference found)

## Next Checks

1. Replicate the study in a high-stakes domain (e.g., medical diagnosis prediction) to test generalizability of adjustability benefits
2. Conduct a longitudinal study examining whether transparency effects emerge over repeated interactions with the algorithm
3. Test the combined effect of transparency and adjustability in a 2x2 factorial design with larger sample size to detect smaller interaction effects