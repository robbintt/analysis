---
ver: rpa2
title: 'BOOM: Beyond Only One Modality KIT''s Multimodal Multilingual Lecture Companion'
arxiv_id: '2512.02817'
source_url: https://arxiv.org/abs/2512.02817
tags:
- translation
- text
- multimodal
- lecture
- slides
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces BOOM, a multimodal multilingual lecture
  companion that addresses the challenge of localizing educational content for global
  audiences. BOOM combines three synchronized outputs: translated text, localized
  slides with preserved visual elements, and synthesized speech.'
---

# BOOM: Beyond Only One Modality KIT's Multimodal Multilingual Lecture Companion

## Quick Facts
- arXiv ID: 2512.02817
- Source URL: https://arxiv.org/abs/2512.02817
- Reference count: 19
- This paper introduces BOOM, a multimodal multilingual lecture companion that addresses the challenge of localizing educational content for global audiences.

## Executive Summary
This paper introduces BOOM, a multimodal multilingual lecture companion that addresses the challenge of localizing educational content for global audiences. BOOM combines three synchronized outputs: translated text, localized slides with preserved visual elements, and synthesized speech. The system leverages multimodal speech translation by incorporating slide screenshots into the translation pipeline, using the OmniFusion model to improve accuracy through visual context. Experiments demonstrate that slide-aware transcripts enhance downstream tasks like summarization and question answering across multiple languages.

## Method Summary
BOOM is a multimodal lecture translation system that synchronizes translated transcripts, localized slides, and synthesized speech. The core approach uses OmniFusion for multimodal speech translation, incorporating slide screenshots to improve translation quality through visual context. The slide translation pipeline employs OCR (PaddleOCR v5), layout analysis (Hi-SAM), multimodal translation, inpainting (Simple-LaMa), and heuristic rendering to preserve visual coherence while localizing embedded text. The system is evaluated on VISTRA benchmark and MCIF dataset, showing improvements in translation quality and downstream task performance when incorporating visual context.

## Key Results
- BOOM achieves BLEU scores of up to 18.4 on summarization and 35.7 on question answering for English
- Incorporating slide screenshots into translation improves accuracy through visual disambiguation
- Block-level layout analysis outperforms line-level segmentation for slide text translation
- Slide-aware transcripts yield cascading benefits for downstream tasks across multiple languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incorporating slide screenshots into speech translation improves translation quality and downstream task performance through visual disambiguation.
- Mechanism: The OmniFusion model receives both audio segments and corresponding slide screenshots extracted mid-segment. Visual context helps resolve ambiguities in technical terminology, definitions, and domain-specific language that may be unclear from audio alone.
- Core assumption: Slides contain semantically relevant information (formulas, diagrams, labels) that disambiguates spoken content.
- Evidence anchors:
  - [abstract] "leveraging multimodal speech translation by incorporating slide screenshots into the translation pipeline, using the OmniFusion model to improve accuracy through visual context"
  - [Page 3] "Accurate visual context is crucial for effective translation... allowing us to extract a screenshot from the middle of the segment and feed it to the ST model"
  - [corpus] Related work on slide-LLM integration (FMR=0.55, "From Slides to Chatbots") supports slide content as valuable signal for educational tasks, though direct citation linkage is weak.
- Break condition: If slides are minimally used, out of sync with speech, or contain primarily decorative images without text, visual context provides negligible benefit.

### Mechanism 2
- Claim: Improved multimodal translations cascade to better performance on downstream tasks like summarization and QA.
- Mechanism: Higher-quality translations (via visual context) serve as input to LLMs for summarization and RAG-based QA. Better input quality yields better outputs even when downstream models are text-only.
- Core assumption: Translation quality is a bottleneck for downstream task performance; improving it propagates benefits.
- Evidence anchors:
  - [abstract] "slide-aware transcripts also yield cascading benefits for downstream tasks such as summarization and question answering"
  - [Page 6] "summaries generated from audio+image input consistently outperform those based on audio-only across most languages and models"
  - [corpus] No direct corpus validation of cascaded benefits; this remains an internal finding requiring external replication.
- Break condition: If downstream tasks are primarily extractive rather than abstractive, or if translation errors are not the dominant failure mode, cascade effects may be minimal.

### Mechanism 3
- Claim: A modular pipeline (OCR → layout analysis → translation → inpainting → heuristic rendering) preserves slide visual coherence while localizing embedded text.
- Mechanism: PaddleOCR extracts text with bounding boxes; Hi-SAM groups text into semantic blocks; OmniFusion translates using image context; Simple-LaMa inpaints removed text regions; heuristic drawing re-renders translated text with estimated styling.
- Core assumption: Accurate OCR and layout grouping are prerequisites for translation quality; inpainting preserves background; heuristics suffice for font/alignment estimation.
- Evidence anchors:
  - [Page 4, Table 1] PaddleOCR v5 achieves 13.48 CER vs. 56.44 for EasyOCR, validating OCR selection.
  - [Page 5, Table 2] "Block-level segmentation improves translation over line-level segmentation, confirming that coherent sentence-like units are critical"
  - [corpus] Corpus lacks direct benchmarking of slide-specific image translation; VISTRA (street signs) is used as proxy per authors' limitations section.
- Break condition: If slides contain complex graphics, equations rendered as images, or non-Latin scripts with poor OCR support, inpainting artifacts and misalignment degrade output quality.

## Foundational Learning

- Concept: Multimodal fusion in sequence-to-sequence models
  - Why needed here: Understanding how OmniFusion combines visual encoders with text decoders is essential for debugging translation quality and latency.
  - Quick check question: Can you explain the difference between early fusion (visual features concatenated to text embeddings) vs. late fusion (cross-attention at decoder layers)?

- Concept: Streaming speech translation with voice activity detection
  - Why needed here: BOOM targets live lectures; understanding segmentation policies (Local-Agreement) explains how audio is chunked and synchronized with slides.
  - Quick check question: What is the trade-off between translation latency and quality in streaming ST systems?

- Concept: OCR evaluation metrics (CER, TER)
  - Why needed here: Selecting and tuning OCR directly impacts downstream translation; CER quantifies character-level accuracy.
  - Quick check question: If CER is low but translation quality is poor, where in the pipeline should you investigate next?

## Architecture Onboarding

- Component map: PDF Viewer / Slide Tracker → identifies active slide per audio segment; Audio Segmentation (VAD + Local-Agreement) → chunks streaming audio; OmniFusion (multimodal ST) → translates audio + slide screenshot; Slide Translation Pipeline: PaddleOCR v5 → text extraction + bounding boxes → Hi-SAM → layout analysis (block/line grouping) → OmniFusion / SeedX → multimodal text translation → Simple-LaMa → inpainting (text removal) → Heuristic Drawing → re-render translated text; Downstream LLMs → summarization (chaptered) + RAG-based QA; TTS (VITS/Kokoro) → synthesized speech output

- Critical path: Audio → Segmentation → OmniFusion (with slide screenshot) → Translated transcript → Downstream LLMs. For slides: Image → OCR → Layout → Translation → Inpainting → Render.

- Design tradeoffs:
  - Multimodal vs. unimodal translation for editable slide text: Authors use unimodal MT for efficiency (computational cost).
  - Block-level vs. line-level segmentation: Block-level improves translation but requires accurate layout analysis.
  - Heuristic rendering vs. diffusion-based: Diffusion degraded clarity over repeated edits; heuristics chosen for stability.

- Failure signatures:
  - Low translation BLEU but high OCR accuracy → investigate layout grouping or multimodal model context integration.
  - Inpainting artifacts visible in output → Simple-LaMa struggles with complex backgrounds; consider larger mask handling.
  - QA performance regression with visual context → LLM receives only text, not images; visual signal may not propagate (per authors' note on Chinese results).

- First 3 experiments:
  1. Reproduce OCR benchmark (Table 1) on your own slide dataset; confirm PaddleOCR v5 latency/accuracy trade-off holds.
  2. A/B test multimodal vs. audio-only translation on 10 lectures; measure BLEU and human judgment of technical term accuracy.
  3. End-to-end latency profiling: measure each pipeline component (Table 4) on target hardware; identify bottleneck (likely layout analysis at 2.93s or translation at 3.10s).

## Open Questions the Paper Calls Out

- Question: How do human subjects rate the rendering quality, layout preservation, and translation accuracy of the localized slides compared to the automated VISTRA benchmark metrics?
- Basis in paper: [explicit] The Limitations section states that the VISTRA benchmark "does not fully reflect translation quality in the lecture domain" and "Human evaluation is therefore needed to assess the rendering quality."
- Why unresolved: The current study relies on proxy metrics (BLEU, ChrF) for image translation rather than direct assessment of the visual output in the educational context.
- What evidence would resolve it: Results from a user study where annotators evaluate the visual coherence and usability of the translated lecture slides.

- Question: Does the multimodal input improve performance on downstream tasks like Summarization and Question Answering when evaluated in a live scenario with timeline-aligned questions?
- Basis in paper: [explicit] The authors note they "conduct evaluation only after the entire talk has been translated" and suggest "Benchmarks with questions aligned to the lecture timeline would provide more realistic... evaluations."
- Why unresolved: The current evaluation method does not simulate the temporal constraints or segment-by-segment nature of a live lecture experience.
- What evidence would resolve it: Performance scores on a benchmark specifically designed for streaming or segment-aligned lecture comprehension.

- Question: Can the latency of the Layout Analysis and Multimodal Translation components be reduced to enable seamless, real-time slide translation during live lectures?
- Basis in paper: [inferred] Table 4 shows Layout Analysis and Translation take ~3 seconds each. The text notes that "optimizing efficiency for these would provide the largest latency gains."
- Why unresolved: While the speech translation is streaming, the total image translation pipeline time (~7 seconds per image) may cause significant lag in synchronizing slides with live audio.
- What evidence would resolve it: Latency measurements of an optimized pipeline successfully running in sync with a live audio stream.

## Limitations
- Limited downstream task validation: The paper claims cascading benefits to summarization and QA, but this is evaluated internally rather than through external benchmarks.
- Corpus representativeness: The VISTRA benchmark uses street signs rather than actual educational slides, limiting ecological validity.
- Reproducibility constraints: Critical implementation details are missing, including the fine-tuning protocol for OmniFusion on lecture speech and downstream LLM configurations.

## Confidence
- High confidence: The core multimodal translation pipeline (OCR → layout analysis → translation → inpainting → rendering) is well-specified and demonstrates measurable improvements in BLEU scores when incorporating visual context.
- Medium confidence: The claimed benefits for downstream tasks (summarization, QA) are supported by internal experiments but lack external validation across diverse lecture types and languages.
- Medium confidence: The superiority of block-level layout analysis over line-level is demonstrated on their test set, but the generalizability to slides with different design conventions remains uncertain.

## Next Checks
1. Cross-domain evaluation: Test the complete BOOM pipeline on educational slides from different domains (STEM, humanities, business) to assess generalizability beyond ACL talks and VISTRA street signs.
2. Human evaluation of visual coherence: Conduct user studies comparing the heuristic rendering approach against alternative methods (diffusion-based, template-based) for preserving slide aesthetics and readability.
3. Latency-latency quality trade-off analysis: Profile the complete pipeline (Table 4 components) on production hardware to identify bottlenecks and determine acceptable quality-latency trade-offs for live lecture scenarios.