---
ver: rpa2
title: Signal or Noise? Evaluating Large Language Models in Resume Screening Across
  Contextual Variations and Human Expert Benchmarks
arxiv_id: '2507.08019'
source_url: https://arxiv.org/abs/2507.08019
tags:
- human
- llms
- recruitment
- expert
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates whether large language models (LLMs) exhibit
  consistent behavior or random variation in resume screening across different contextual
  inputs, comparing their performance to human experts. Using a controlled experimental
  design, three LLMs (Claude, GPT, and Gemini) and three human recruitment experts
  evaluated resumes under four context conditions: no company information, multinational
  corporation, startup, and reduced context.'
---

# Signal or Noise? Evaluating Large Language Models in Resume Screening Across Contextual Variations and Human Expert Benchmarks

## Quick Facts
- arXiv ID: 2507.08019
- Source URL: https://arxiv.org/abs/2507.08019
- Reference count: 0
- Large language models exhibit significant context sensitivity in resume screening, with GPT showing strongest adaptation and Claude minimal responsiveness

## Executive Summary
This study investigates whether large language models (LLMs) exhibit consistent behavior or random variation in resume screening across different contextual inputs, comparing their performance to human experts. Using a controlled experimental design, three LLMs (Claude, GPT, and Gemini) and three human recruitment experts evaluated resumes under four context conditions: no company information, multinational corporation, startup, and reduced context. Statistical analysis revealed significant mean differences in LLM evaluations across four of eight conditions, with LLM scores consistently differing from human experts (p < 0.01). GPT showed strong context sensitivity, Gemini exhibited partial adaptation, and Claude demonstrated minimal responsiveness. The findings indicate that LLMs produce interpretable patterns with detailed prompts but diverge substantially from human judgment, suggesting hybrid approaches combining LLM efficiency with human oversight may be optimal for recruitment automation.

## Method Summary
The study employed a controlled experimental design where three LLMs (Claude, GPT, and Gemini) and three human recruitment experts evaluated 30 anonymized Product Manager resumes across four context conditions: no company information, multinational corporation (MNC), startup, and reduced context. Each resume was evaluated under all four conditions using standardized prompts that included job descriptions and company profiles. The evaluation process involved 20,400 total evaluations (8 evaluators × 30 resumes × 4 conditions × 10 randomized iterations). Statistical analysis included one-way ANOVA for inter-evaluator differences, paired t-tests for within-model context effects, and Cohen's d effect sizes to measure practical significance.

## Key Results
- GPT demonstrated strong contextual sensitivity with significant score increases from No Company (50.5) to Firm1 (76.1), t(9) = -6.07, p < 0.001, Cohen's d = -1.92
- LLM scores consistently differed from human experts (p < 0.01) across four of eight conditions
- Claude showed minimal responsiveness to organizational context with no significant score changes (p > 0.1)
- GPT exhibited 28.9% score inflation under reduced context conditions, while human experts maintained stable scoring

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exhibit model-specific context sensitivity that affects resume evaluation scores systematically.
- Mechanism: When organizational context (MNC vs. startup) is added to prompts, models redistribute evaluation weights across resume categories. GPT adjusts weights (e.g., increasing leadership/soft skills from 15% to 25% for MNC context), producing significant score changes. Claude maintains stable technical-focused weighting regardless of context.
- Core assumption: Weight redistribution reflects meaningful adaptation to organizational requirements rather than arbitrary output variation.
- Evidence anchors:
  - [abstract] "Paired t-tests showed GPT adapts strongly to company context (p < 0.001), Gemini partially (p = 0.038 for Firm1), and Claude minimally (p > 0.1)"
  - [section: Study 1 Results] "GPT demonstrated strong contextual sensitivity, with significant increases from No Company (50.5) to Firm1 (76.1), t(9) = -6.07, p < 0.001, Cohen's d = -1.92"
  - [corpus] Related work on context-aware LLM hiring frameworks (arxiv 2504.02870) supports context-sensitivity as a design dimension, though specific model comparisons are not directly addressed.
- Break condition: If score changes across contexts do not follow interpretable patterns (e.g., random direction/magnitude), mechanism fails.

### Mechanism 2
- Claim: Information scarcity triggers score inflation in LLMs but not in human experts.
- Mechanism: Under reduced context conditions, LLMs (particularly GPT) interpret missing information optimistically rather than conservatively, inflating scores by up to 28.9%. Human experts maintain or reduce scores, applying risk-averse professional judgment developed through experience.
- Core assumption: The score inflation pattern reflects LLM architectural tendency to resolve ambiguity toward positive evaluation rather than explicit uncertainty acknowledgment.
- Evidence anchors:
  - [abstract] "LLM scores consistently differed from human experts (p < 0.01)"
  - [section: Study 3] "GPT's increase from 64.3 in the No Company condition to 82.9 in Reduced Context represented a 28.9% score inflation... Human experts did not exhibit the score inflation patterns observed in LLMs"
  - [corpus] Corpus does not directly address information scarcity effects on LLM resume scoring; this remains an understudied area.
- Break condition: If LLM score distributions under reduced context show increased variance without systematic upward shift, the inflation mechanism does not hold.

### Mechanism 3
- Claim: LLMs and human experts employ fundamentally different evaluation decomposition strategies.
- Mechanism: LLMs apply explicit percentage allocations to discrete categories (analytical decomposition) with adaptive but mechanical weight redistribution. Human experts integrate multiple factors holistically through professional judgment, incorporating qualitative organizational understanding (e.g., "in a startup the person should be more mature") that LLMs do not replicate.
- Core assumption: The qualitative-expertise gap is not merely a prompt-engineering limitation but reflects deeper differences in reasoning architecture.
- Evidence anchors:
  - [abstract] "Meta-cognition analysis highlighted adaptive weighting patterns that differ markedly from human evaluation approaches"
  - [section: Meta-Cognition Analysis] "Expert 2 dramatically shifted to experience emphasis (75%) and industry fit (25%)... demonstrated qualitative understanding of organizational dynamics that LLMs did not capture"
  - [corpus] Related bias assessment work (FAIRE, arxiv 2504.01420) examines LLM evaluation fairness but does not address meta-cognitive decomposition differences.
- Break condition: If improved prompts elicit equivalent qualitative reasoning from LLMs, the mechanism would reflect implementation rather than architectural limitation.

## Foundational Learning

- Concept: **Signal Detection Theory (SDT)**
  - Why needed here: The paper frames LLM reliability as distinguishing meaningful patterns (signal) from random variation (noise). SDT provides the theoretical vocabulary for sensitivity vs. response bias in evaluation systems.
  - Quick check question: If an LLM consistently scores all candidates at 75 regardless of qualifications, is this a sensitivity problem or a response bias problem?

- Concept: **Effect Size (Cohen's d)**
  - Why needed here: The study reports effect sizes (d = -1.92 to -2.02 for GPT context adaptation) alongside p-values. Understanding practical significance vs. statistical significance is critical for deployment decisions.
  - Quick check question: A paired t-test shows p < 0.001 with Cohen's d = 0.15. Is this result practically meaningful?

- Concept: **Context Window and Position Sensitivity**
  - Why needed here: Literature cited shows LLMs struggle with information position in long inputs (best at beginning/end). Resume and JD structure may affect which details models prioritize.
  - Quick check question: If critical job requirements appear in the middle of a long job description, how might this affect LLM evaluation accuracy?

## Architecture Onboarding

- Component map:
  Input Layer (Job description + company context + candidate resume) -> Model Layer (LLM processes prompt, generates numerical score + weighting rationale) -> Evaluation Layer (Score aggregation, context-sensitivity analysis, human-AI comparison) -> Output Layer (Ranked candidate list with confidence indicators)

- Critical path:
  1. Prompt standardization (identical format across models is essential for comparison validity)
  2. Context condition randomization (prevents order effects)
  3. Score normalization (LLMs use 0-100 scale but with different distributions than humans)
  4. Statistical comparison (ANOVA for inter-model, paired t-tests for within-model context effects)

- Design tradeoffs:
  - Detailed prompts vs. reduced context: Detailed prompts improve LLM consistency but diverge further from human judgment patterns
  - Single-model vs. ensemble: Different models show different context sensitivities; ensemble may average out biases but lose interpretability
  - Automation vs. human oversight: Full automation risks systematic misalignment; hybrid approaches add latency and cost

- Failure signatures:
  - Score inflation under reduced context: GPT increases scores by 28.9% when information is scarce
  - Non-adaptation to organizational context: Claude shows minimal score changes across MNC/startup conditions (p > 0.1)
  - Selective context sensitivity: Gemini adapts to MNC but not startup contexts
  - Human expert fatigue: Expert 3 provided zero scores in later conditions, indicating engagement breakdown

- First 3 experiments:
  1. Baseline calibration: Run identical resumes through all three models under No Company condition; verify score distributions and inter-model agreement (expect F > 10, p < 0.01 per paper)
  2. Context sensitivity test: Add Firm1 (MNC) and Firm2 (startup) context to prompts; measure score shifts using paired t-tests; confirm GPT shows largest effect sizes (d > 1.5)
  3. Information scarcity stress test: Use Reduced Context condition; flag if any model shows >20% score inflation; this identifies deployment risk for incomplete job descriptions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do LLM-human expert evaluation differences and context sensitivity patterns observed for Product Manager roles generalize to other job functions with different skill requirements?
- Basis in paper: [explicit] "Focusing exclusively on Product Manager positions limits the external validity of the results, as patterns of LLM-human expert alignment may differ across roles with varying skill requirements and evaluation criteria."
- Why unresolved: The study tested only one job type; it is unknown whether GPT's strong adaptation, Claude's rigidity, and human conservatism persist across technical, creative, or executive roles with different evaluation criteria.
- What evidence would resolve it: Replication of the experimental design across diverse job roles (engineering, design, sales, executive) showing convergence or divergence in LLM-human alignment patterns.

### Open Question 2
- Question: Do systematic differences between LLM and human expert evaluations create disparate impacts on protected demographic groups, particularly when contextual information is limited?
- Basis in paper: [explicit] "Understanding whether systematic LLM-human differences create disparate impacts on protected groups is essential for responsible AI deployment" and "LLMs' varying sensitivity to organizational context may result in different evaluation outcomes for candidates applying to startups versus multinational corporations."
- Why unresolved: The study used anonymized resumes and did not test demographic-based differences; bias mechanisms remain unexamined.
- What evidence would resolve it: Controlled experiments with resumes varying only demographic indicators across different context conditions, measuring disparate impact ratios.

### Open Question 3
- Question: Can calibration mechanisms effectively align LLM resume screening scores with human expert judgment while maintaining efficiency?
- Basis in paper: [explicit] "Research should also explore calibration mechanisms to better align LLM outputs with human expert judgment and investigate ways to improve LLM contextual understanding and resilience to information scarcity."
- Why unresolved: The study documented systematic 15-25 point gaps but tested no interventions to reduce them.
- What evidence would resolve it: Intervention studies comparing baseline LLM scores against calibrated outputs (via prompt engineering, fine-tuning, or score transformation) with human expert agreement as the benchmark.

### Open Question 4
- Question: How should hybrid human-LLM recruitment systems be optimally structured to leverage complementary strengths?
- Basis in paper: [inferred] The paper concludes that "hybrid approaches combining LLM efficiency with human oversight may be optimal" but does not specify design parameters, thresholds, or division of labor.
- Why unresolved: The study compared LLM and human performance separately but did not test integrated workflows.
- What evidence would resolve it: Experiments testing different hybrid architectures (LLM pre-screening with human review, parallel evaluation with conflict resolution, tiered complexity routing) measuring accuracy, fairness, and efficiency trade-offs.

## Limitations
- The study uses synthetic resumes and controlled company profiles rather than real-world hiring data, limiting ecological validity.
- The evaluation framework assumes numerical scoring (0-100) captures the full complexity of candidate assessment, potentially missing qualitative factors.
- The study does not address potential biases in LLM evaluations across demographic characteristics.

## Confidence
- High confidence: LLM context sensitivity exists and varies by model (Mechanism 1) - supported by statistically significant p-values (p < 0.001 for GPT) and effect sizes (d > 1.5).
- Medium confidence: Information scarcity triggers systematic score inflation in LLMs (Mechanism 2) - while the 28.9% inflation is observed, the mechanism's universality across different LLM architectures remains uncertain.
- Low confidence: The fundamental architectural gap between LLM and human evaluation strategies (Mechanism 3) - the qualitative differences could potentially be reduced through prompt engineering improvements not explored in this study.

## Next Checks
1. Cross-organizational validation: Test the same framework across 3-5 additional company profiles (different industries, sizes, and cultures) to verify whether context sensitivity patterns hold beyond the MNC/startup dichotomy.
2. Longitudinal stability assessment: Run identical resume sets through the same models weekly for 4 weeks to measure score stability and detect any systematic drift or temperature-dependent variation.
3. Human-AI agreement calibration: Implement a hybrid evaluation where LLM scores are adjusted based on human expert feedback on a validation set, measuring whether this improves alignment while maintaining efficiency gains.