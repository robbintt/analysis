---
ver: rpa2
title: 'Decoupling Template Bias in CLIP: Harnessing Empty Prompts for Enhanced Few-Shot
  Learning'
arxiv_id: '2512.08606'
source_url: https://arxiv.org/abs/2512.08606
tags:
- template
- bias
- few-shot
- learning
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the issue of template-induced bias in CLIP,\
  \ where the similarity between text templates and image samples can lead to misclassification\
  \ by causing the model to rely on template proximity rather than true sample-to-category\
  \ alignment. The proposed method uses \"empty prompts\" \u2014 textual inputs that\
  \ convey the idea of \"emptiness\" without category information \u2014 to capture\
  \ unbiased template features and offset this bias."
---

# Decoupling Template Bias in CLIP: Harnessing Empty Prompts for Enhanced Few-Shot Learning

## Quick Facts
- arXiv ID: 2512.08606
- Source URL: https://arxiv.org/abs/2512.08606
- Reference count: 40
- This paper addresses template-induced bias in CLIP by using empty prompts to capture unbiased template features and improve few-shot classification accuracy.

## Executive Summary
This paper tackles the problem of template-sample similarity (TSS) bias in CLIP-based few-shot image classification, where the semantic proximity between text templates and image samples can lead to misclassification by causing the model to rely on template proximity rather than true sample-to-category alignment. The authors propose a novel approach using "empty prompts" — textual inputs that convey the idea of "emptiness" without category information — to capture unbiased template features and offset this bias. The framework employs a two-stage training process: pretraining with empty prompts to reveal and reduce template-induced bias within the CLIP encoder, followed by few-shot fine-tuning with a bias calibration loss to enforce correct alignment between images and their categories. Experiments across multiple benchmarks show that this template correction method significantly reduces performance fluctuations caused by template-sample similarity, yielding higher classification accuracy and stronger robustness.

## Method Summary
The method uses a two-stage training framework to address template-sample similarity bias in CLIP. First, pretraining with empty prompts (like "None", "Vacant", "Blank") captures unbiased template features by computing a template bias loss that measures the similarity between empty prompt embeddings and image embeddings across pseudo-classes formed from the dataset samples. Second, few-shot fine-tuning combines cross-entropy loss with the template bias loss to calibrate the model's alignment between images and their true categories. The approach uses LoRA adapters on CLIP's query/key/value matrices and demonstrates that this template correction significantly reduces performance fluctuations caused by template-sample similarity across 11 diverse datasets.

## Key Results
- The proposed method significantly reduces template-sample similarity-induced performance fluctuations in few-shot image classification
- Average accuracy improvement of approximately 1.5 points over LoRA baseline across 11 datasets
- Demonstrates stronger robustness to template choices compared to baseline methods
- Successfully lowers the correlation between template-sample similarity and classification accuracy to near zero

## Why This Works (Mechanism)
The method works by explicitly modeling and compensating for template bias through empty prompts. During pretraining, empty prompts reveal the inherent template-induced bias in CLIP's embedding space by measuring how image embeddings correlate with these category-free text prompts. This creates a baseline template bias signature that the model learns to recognize. During fine-tuning, the template bias loss forces the model to align image embeddings with their true categories rather than with the template structure, effectively decoupling the semantic content from the template form. The empty prompts act as a bias reference point, allowing the model to calibrate its predictions based on the difference between biased (template-influenced) and unbiased (empty prompt-influenced) embeddings.

## Foundational Learning
- **Template-Sample Similarity (TSS) bias**: The phenomenon where text templates used in CLIP classification influence results based on their semantic similarity to image content rather than true category alignment. Needed to understand why standard CLIP approaches fail in few-shot scenarios. Quick check: Measure correlation between template similarity scores and classification accuracy.
- **Empty prompts as bias anchors**: Using text inputs that convey "emptiness" without category information to establish a baseline for template-free embeddings. Needed to create a reference point for measuring and correcting template bias. Quick check: Verify empty prompts have low similarity to actual category names.
- **Two-stage training with bias calibration**: Separating the bias modeling (pretraining) from the category alignment (fine-tuning) to ensure proper bias compensation. Needed to prevent the model from conflating bias correction with category learning. Quick check: Ensure pretraining completes before fine-tuning begins.

## Architecture Onboarding
**Component map**: CLIP ViT-B/16 encoder -> LoRA adapters (Q/K/V) -> Empty prompt embeddings -> Template bias loss -> Cross-entropy loss -> Classification output

**Critical path**: Image → CLIP encoder → Image embedding → [Bias calibration] → Classification logits → Cross-entropy loss → Accuracy

**Design tradeoffs**: Uses empty prompts instead of category names to avoid introducing new biases, but requires careful prompt selection. Employs LoRA for parameter efficiency rather than full fine-tuning, trading some representational capacity for computational efficiency.

**Failure signatures**: High correlation (>0.3) between TSS and accuracy after training indicates bias not properly compensated; performance degradation vs. baseline suggests pretraining stage was skipped or α hyperparameter is too high.

**First experiments**: 1) Test empty prompt pretraining alone to verify it reduces TSS-accuracy correlation; 2) Run ablation with different empty prompt sets to confirm their importance; 3) Compare with baseline using category names instead of empty prompts to validate the empty prompt strategy.

## Open Questions the Paper Calls Out
None

## Limitations
- The reported 1.5-point average accuracy improvement is modest and may be sensitive to implementation details
- Lacks statistical significance testing or variance reporting across multiple random seeds
- Does not provide ablation studies on the template correction mechanism's specific contribution
- Implementation details like data augmentation specifics and softmax temperature are not fully specified

## Confidence
- **High confidence**: The two-stage training framework and template bias correction objective are clearly specified and theoretically sound
- **Medium confidence**: The 1.5-point average accuracy improvement claim is plausible but may vary with implementation details and random seeds
- **Medium confidence**: The TSS-accuracy correlation analysis demonstrates the problem exists but doesn't conclusively prove the solution's generality

## Next Checks
1. Verify that the empty prompt pretraining stage actually reduces TSS-accuracy correlation on held-out datasets
2. Test whether the template correction mechanism improves robustness across different template formulations beyond "a photo of a {}"
3. Measure the variance of accuracy improvements across multiple random seeds to assess statistical significance of the reported gains