---
ver: rpa2
title: 'MirrorMamba: Towards Scalable and Robust Mirror Detection in Videos'
arxiv_id: '2511.06716'
source_url: https://arxiv.org/abs/2511.06716
tags:
- mirror
- detection
- depth
- video
- flow
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of video mirror detection, where
  existing methods suffer from limited performance due to over-reliance on single
  cues and architectural limitations. The authors propose MirrorMamba, a novel framework
  that leverages multiple complementary cues (perceived depth, correspondence, and
  optical flow) and introduces the first Mamba-based architecture for mirror detection.
---

# MirrorMamba: Towards Scalable and Robust Mirror Detection in Videos

## Quick Facts
- **arXiv ID:** 2511.06716
- **Source URL:** https://arxiv.org/abs/2511.06716
- **Reference count:** 14
- **Primary result:** Proposes first Mamba-based architecture for mirror detection, achieving state-of-the-art on VMD-D and MMD benchmarks by fusing depth, correspondence, and flow cues.

## Executive Summary
This paper addresses video mirror detection by introducing MirrorMamba, a novel framework that leverages multiple complementary cues (perceived depth, correspondence, and optical flow) and implements the first Mamba-based architecture for this task. The approach tackles limitations of existing methods that over-rely on single cues and suffer from architectural constraints. The Mamba-based Multi-direction Correspondence Extractor (MMCE) captures global correspondence properties, while the Layer-wise Boundary Enforcement Decoder (BED) refines boundary details. Experiments demonstrate state-of-the-art performance on video mirror detection benchmarks and strong results on image-based mirror detection, proving both effectiveness and generalizability.

## Method Summary
MirrorMamba processes RGB, relative depth, and optical flow inputs through a shared VMamba-T backbone to extract multi-scale features. The MMCE module fuses these features using four Mamba scanning blocks that scan in opposite horizontal and vertical directions to capture flip correspondence properties. The BED module progressively refines boundaries by using high-level semantic features to guide the reconstruction of low-level spatial detail features through cross-Mamba guidance and cross-channel attention. The model is trained with AdamW optimizer, polynomial decay schedule, and BCE loss for 40 epochs on 4 GPUs.

## Key Results
- Achieves state-of-the-art performance on VMD-D and MMD video mirror detection benchmarks
- Demonstrates strong generalizability by achieving competitive results on image-based PMD dataset
- Shows that multi-cue fusion (depth + correspondence + flow) outperforms single-cue approaches
- Proves Mamba architecture effectiveness for video mirror detection tasks

## Why This Works (Mechanism)

### Mechanism 1: Multi-cue Fusion for Robustness
Combining perceived depth, correspondence, and optical flow cues provides more robust mirror detection than relying on a single cue, because different cues are effective in different scenarios. The model fuses three complementary cues as inputs to a shared backbone. Perceived depth provides initial screening via discontinuity, correspondence verifies mirror existence via inside-outside symmetry, and optical flow adds dynamic information about motion inconsistencies between real and reflected objects. These are fused in the MMCE module. Core assumption: At least one of the three cues will be discriminative in any given challenging scenario, and their failure modes are not perfectly correlated.

### Mechanism 2: Mamba-based Correspondence Extraction
The Mamba-based Multi-direction Correspondence Extractor (MMCE) effectively captures long-range spatial correspondence properties using the selective state space model's global receptive field and scanning strategy. The MMCE uses four Mamba scanning blocks (M1-M4) to scan the fused feature tensor in opposite horizontal and vertical directions. The selective scan mechanism (SSM) allows the model to capture global dependencies linearly. By comparing features across these directions, it generates attention maps that highlight horizontally or vertically flipped correspondences, a key mirror characteristic. Core assumption: Mirrors exhibit a flip correspondence with the scene, and Mamba's sequential scanning can effectively capture this by comparing features from opposite directions.

### Mechanism 3: Progressive Boundary Refinement
The Layer-wise Boundary Enforcement Decoder (BED) refines blurry or inaccurate mirror boundaries by using high-level semantic features to guide the reconstruction of low-level spatial detail features. BED operates progressively. At each layer, it takes the current layer's feature and a global feature from the previous decoder step. A cross-Mamba module uses the high-level features to guide the reconstruction of the low-level features during the selective scanning process. A cross-channel attention module then further refines inter-channel dependencies. This process is repeated layer-by-layer to produce sharp boundaries. Core assumption: High-level semantic features contain sufficient boundary-relevant information to guide refinement, and the cross-Mamba mechanism can effectively transfer this guidance to low-level features.

## Foundational Learning

### State Space Models (SSMs) / Mamba
Why needed: This is the core architectural innovation. Mamba is a type of SSM designed for efficient sequence modeling with linear complexity, replacing the quadratic self-attention of Transformers. Quick check: Can you explain, in simple terms, how a selective state space model (like Mamba) processes a 1D sequence differently from a Transformer's self-attention mechanism, particularly regarding computational complexity?

### Multi-modal / Multi-cue Fusion
Why needed: The paper's central premise is that robust mirror detection requires fusing information from different modalities (RGB, depth, flow). Understanding fusion strategies is critical. Quick check: What are two common approaches for fusing features from different modalities (e.g., early vs. late fusion), and what is a key trade-off of each?

### Feature Pyramids / Encoder-Decoder Architectures
Why needed: The model uses a backbone to create a feature hierarchy and a decoder (BED) to progressively restore resolution. This is a standard paradigm in segmentation tasks. Quick check: In an encoder-decoder network for segmentation, what type of information is typically more salient in the deeper/encoder layers versus the shallower/decoder layers, and why are skip connections often used?

## Architecture Onboarding

### Component map
Input RGB/Depth/Flow -> Shared VMamba-T Backbone -> MMCE (Fusion & Correspondence) -> BED (Boundary Refinement) -> Segmentation Map

### Critical path
Input -> VMamba Backbone -> MMCE (Fusion & Correspondence) -> BED (Boundary Refinement) -> Segmentation Map

### Design tradeoffs
VMamba-T vs. Transformer: Trades potentially higher raw Transformer capacity for linear complexity and scalability in video tasks. Multi-cue vs. Single-cue: Trades off increased inference cost (depth/flow estimation) and system complexity for robustness across diverse scenarios. Shared vs. Separate Backbone Weights: Trades off potential modality-specific feature optimization for parameter efficiency by using a shared-weight backbone for all inputs.

### Failure signatures
Input Modality Failure: If depth estimation is very poor or optical flow is unreliable, and RGB correspondence is also absent, the model will produce false negatives. Mamba Scanning Failure: If the mirror's correspondence is not aligned with the four scanning directions, the MMCE may fail to capture it. Boundary Blurring: If high-level features in BED are too coarse, the output may have imprecise boundaries. Over-reliance on Depth: In scenes with depth-like artifacts, the model might produce false positives if the correspondence and flow cues are insufficient.

### First 3 experiments
1. Reproduce Multi-Cue Ablation: Run the model on the VMD-D dataset with three input configurations: (a) RGB only, (b) RGB + Depth, (c) RGB + Depth + Flow. Quantify the performance gain from adding each modality.
2. Visualize MMCE Attention Maps: For a sample image with a known mirror, extract and visualize the horizontal and vertical attention maps from the MMCE module. Verify that they correctly highlight the mirror region and its corresponding "flipped" context.
3. Component Ablation on BED: Evaluate performance (IoU, boundary F-measure) with and without the BED module. Test variants removing the cross-Mamba guidance and the cross-channel attention to understand their individual contributions to boundary refinement.

## Open Questions the Paper Calls Out

### Open Question 1
How can a unified mirror detection framework be developed to automatically process both image and video inputs without requiring manual architectural adjustments? The introduction states that the adaptability of the current method "opens promising avenues for future research on unified mirror detection frameworks." Currently, adapting the model for image-based detection requires the manual removal of the dynamic optical flow branch; a truly unified architecture would need to handle the presence or absence of temporal cues dynamically. Evidence: A single model configuration that achieves state-of-the-art performance on both video (VMD-D) and image (PMD) benchmarks without manual modality-specific ablation.

### Open Question 2
Can the Mamba architecture be intrinsically modified to effectively model inter-channel relationships, eliminating the need for external attention mechanisms? The methodology section notes that "Mamba lacks the ability to model inter-channel relationships," which forced the authors to enhance the BED module with a separate cross-channel attention module. The current solution relies on a hybrid approach (Mamba + Attention) rather than solving the limitation within the core state space model structure itself. Evidence: A variation of the MirrorMamba using a modified Mamba block that achieves equivalent or superior performance without the auxiliary cross-channel attention components.

### Open Question 3
How can video mirror detection be improved in scenarios where optical flow is unreliable, such as pure camera rotation or extremely slow movement? The paper states that the optical flow "will fail when the camera only rotates or moves at a very slow speed," limiting the utility of this dynamic cue in specific motion conditions. While the paper uses multiple cues to mitigate this, it accepts the failure of flow in these scenarios rather than proposing a method to extract dynamic information from rotational or minimal motion. Evidence: Qualitative and quantitative results on a test subset specifically composed of rotational or low-motion videos where the method maintains high IoU despite the degradation of flow cues.

## Limitations
- Specific Mamba hyperparameters for VSS blocks are not provided, making exact reproduction challenging
- Performance gain from Mamba modules over traditional CNN-based counterparts is not quantified
- Lacks comparison to Transformer-based baselines to validate Mamba's efficiency advantage
- Requires external depth estimation (MiDaS) and flow estimation (FlowDiffuser) tools

## Confidence
- **High Confidence:** The multi-cue fusion approach demonstrably improves performance over single-cue methods, supported by ablation results on VMD-D and MMD datasets
- **Medium Confidence:** The Mamba-based MMCE architecture effectively captures global correspondence properties, but the mechanism's superiority over alternative global attention methods remains unverified without Transformer comparisons
- **Medium Confidence:** The BED module improves boundary refinement, though the specific contribution of cross-Mamba guidance versus standard feature fusion is not isolated

## Next Checks
1. **Ablation Study on Mamba Components:** Conduct controlled experiments removing MMCE and BED modules separately to quantify their individual contributions to the performance gain, comparing against equivalent CNN-based replacements
2. **Complexity-Performance Analysis:** Measure FLOPs and inference time per frame for MirrorMamba versus a Transformer-based mirror detection baseline to validate the claimed efficiency advantage
3. **Generalization Test:** Evaluate MirrorMamba on out-of-distribution mirror types (e.g., curved mirrors, transparent surfaces) not well-represented in training data to assess robustness claims