---
ver: rpa2
title: 'Towards Scalable SOAP Note Generation: A Weakly Supervised Multimodal Framework'
arxiv_id: '2506.10328'
source_url: https://arxiv.org/abs/2506.10328
tags:
- clinical
- soap
- notes
- note
- structured
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a weakly supervised multimodal framework for
  generating structured SOAP notes from lesion images and limited clinical text in
  dermatology. The method uses retrieval-augmented generation and fine-tuning of Vision-LLaMA
  to synthesize clinically relevant notes without large-scale annotations.
---

# Towards Scalable SOAP Note Generation: A Weakly Supervised Multimodal Framework

## Quick Facts
- arXiv ID: 2506.10328
- Source URL: https://arxiv.org/abs/2506.10328
- Reference count: 40
- Primary result: Weakly supervised multimodal framework generates structured SOAP notes from lesion images and limited clinical text with semantic alignment scores of 0.76-0.86 across sections

## Executive Summary
This paper introduces a weakly supervised multimodal framework for generating structured SOAP notes from dermatology lesion images and sparse clinical text. The method employs retrieval-augmented generation and Vision-LLaMA fine-tuning to produce clinically grounded documentation without requiring large-scale expert annotations. Evaluation demonstrates strong semantic alignment with expert medical concepts through MedConceptEval and Clinical Coherence Score metrics, with LLM-as-a-Judge assessment outperforming GPT-4o, Claude, and DeepSeek Janus Pro. The approach offers a scalable solution for dermatology documentation while reducing clinician burden.

## Method Summary
The framework uses three phases: (1) Pre-training where GPT-3.5 generates clinical captions from structured metadata, RAG retrieves relevant medical passages, and Vision-LLaMA produces weak SOAP labels; (2) Fine-tuning with QLoRA (r=8, α=16) and cross-entropy loss on image-caption-SOAP triplets; (3) Inference producing structured notes. Training uses the PAD-UFES-20 dataset with 2,298 dermoscopic images and 26 clinical features per lesion. The system reduces annotation requirements through synthetic data generation while maintaining clinical fidelity.

## Key Results
- MedConceptEval similarity scores range from 0.76-0.86 across SOAP sections
- Clinical Coherence Score averages 0.88-0.93, showing strong alignment between captions and generated notes
- LLM-as-Judge evaluation gave the model a perfect 20/20 score, outperforming GPT-4o, Claude, and DeepSeek Janus Pro
- ClinicalBERT F1 scores range 0.76-0.79, matching or exceeding strong baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-augmented generation grounds SOAP outputs in authoritative medical knowledge, reducing hallucination risk.
- Mechanism: Clinical captions generated from structured features serve as queries against a curated vector database (South Texas Skin Cancer Institute, NCI, ACS, NHS sources). Retrieved passages are concatenated with captions before being passed to Vision-LLaMA, providing domain-specific context that guides structured generation.
- Core assumption: The retrieved passages contain sufficient relevant information for the specific lesion type and clinical scenario to inform each SOAP section.
- Evidence anchors:
  - [section] "The generated caption is used as a query to retrieve semantically relevant passages from a curated vector database... The retrieved context is concatenated with the original caption and provided as input to the pre-trained Vision-LLaMA 3.2 model"
  - [corpus] CLI-RAG paper corroborates RAG utility for clinical text generation, though it addresses general clinical notes rather than dermatology-specific SOAP structures.
- Break condition: Retrieval fails when query captions lack specificity or when the knowledge base lacks coverage for rare conditions (e.g., lower CCS scores observed for SCC and BCC in MedConceptEval).

### Mechanism 2
- Claim: Weak supervision via synthetic SOAP generation enables training without large-scale expert annotations.
- Mechanism: GPT-3.5 generates initial clinical captions from structured metadata (lesion diameter, biopsy status, symptoms). Vision-LLaMA then produces weakly supervised SOAP notes using RAG-augmented prompts. These synthetic notes serve as training targets for fine-tuning, bypassing the need for manually annotated SOAP corpora.
- Core assumption: Synthetic SOAP notes generated through this pipeline are sufficiently clinically accurate to serve as reliable supervision signal.
- Evidence anchors:
  - [abstract] "Our approach reduces reliance on manual annotations, enabling scalable, clinically grounded documentation while alleviating clinician burden"
  - [section] "Due to the limited availability of large-scale annotated SOAP notes in dermatology, we employed a weak supervision strategy to synthesize training data"
  - [corpus] Related papers (K-SOAP, pediatric rehabilitation SOAP) rely on extensive annotated dialogues; this work explicitly targets the annotation bottleneck.
- Break condition: If synthetic notes contain systematic errors or hallucinations, these propagate through fine-tuning. The paper acknowledges this limitation.

### Mechanism 3
- Claim: Parameter-efficient fine-tuning (QLoRA) adapts a general vision-language model to structured clinical documentation while preserving computational feasibility.
- Mechanism: Low-rank adapters (r=8, α=16) are injected into transformer projections (query, key, value, output, gate, up, down). The model learns to map multimodal inputs (image + clinical caption) to structured SOAP format through cross-entropy loss minimization.
- Core assumption: The low-rank decomposition captures sufficient task-specific information without full parameter updates.
- Evidence anchors:
  - [section] "Fine-tuning is performed for 500 epochs... completed in 1.5 hours on an NVIDIA A100 GPU with 80 GB of VRAM"
  - [section] ClinicalBERT F1 scores range 0.76-0.79, outperforming or matching GPT-4o, Claude, and DeepSeek Janus Pro
  - [corpus] Limited direct corpus evidence on QLoRA specifically for SOAP generation; most related work uses full fine-tuning or prompting.
- Break condition: Low-rank constraint may limit adaptation to complex clinical reasoning patterns; ANOVA shows section type significantly affects semantic quality (F(3,20)=3.88, p=0.024), suggesting uneven learning across SOAP components.

## Foundational Learning

- Concept: **SOAP Note Structure (Subjective, Objective, Assessment, Plan)**
  - Why needed here: The entire framework is designed to generate sectioned clinical notes; understanding what belongs in each section is prerequisite to evaluating model outputs.
  - Quick check question: Given a patient presenting with a bleeding lesion, which SOAP section would contain "patient reports itching and bleeding for 2 weeks"?

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: Core mechanism for knowledge grounding; understanding query-document retrieval and context concatenation is essential for debugging generation quality.
  - Quick check question: If retrieved passages discuss melanoma treatment but the input image shows seborrheic keratosis, what failure mode would you expect?

- Concept: **Parameter-Efficient Fine-Tuning (LoRA/QLoRA)**
  - Why needed here: The training pipeline uses QLoRA; understanding low-rank adaptation helps diagnose underfitting vs. rank insufficiency.
  - Quick check question: If validation loss plateaus but training loss continues decreasing, would you increase or decrease the LoRA rank?

## Architecture Onboarding

- Component map:
  Input Layer (lesion image + 26 structured clinical features) -> GPT-3.5 Caption Generation -> RAG Retrieval (vector database query) -> Vision-LLaMA 3.2 (multimodal encoder) -> QLoRA Fine-Tuning (low-rank adapters) -> Structured SOAP Output (Subjective, Objective, Assessment, Plan)

- Critical path:
  1. Structured features → GPT-3.5 caption → RAG retrieval
  2. Image + retrieved context → Vision-LLaMA encoder
  3. QLoRA-adapted decoder → SOAP sections
  4. Evaluation: MedConceptEval (descriptor bank similarity), CCS (caption-to-section alignment), ClinicalBERT F1

- Design tradeoffs:
  - Single dataset (PAD-UFES-20) enables controlled experiments but limits generalization claims
  - Weak supervision reduces annotation cost but introduces synthetic data quality risk
  - QLoRA enables single-GPU training but may constrain model capacity for complex clinical reasoning
  - LLM-as-Judge evaluation scales well but lacks human validation

- Failure signatures:
  - Low CCS in Plan section (0.88-0.90 vs. 0.93+ for Subjective): Plan-specific keywords absent from captions, requiring pure generation without input grounding
  - Section placement errors: Diagnosis appearing in Chief Complaint (Subjective) instead of Assessment (observed in Figure 4a)
  - Lower MedConceptEval for SCC/BCC (max ~0.86 vs. 0.90+ for Melanoma): Subtle clinical features less captured

- First 3 experiments:
  1. Baseline reproduction: Fine-tune Vision-LLaMA on PAD-UFES-20 using paper-specified QLoRA settings (r=8, α=16, 500 epochs, lr=2e-4); verify MedConceptEval scores fall within reported 0.76-0.86 range
  2. Ablation: RAG removal: Disable retrieval module, generate SOAP notes using caption-only inputs; compare ClinicalBERT F1 and hallucination rates to quantify RAG contribution
  3. Section-level analysis: Compute per-section CCS across all 6 lesion types; identify which sections/conditions show weakest alignment and hypothesize whether retrieval coverage or model capacity is the bottleneck

## Open Questions the Paper Calls Out

- Can human-in-the-loop refinement strategies improve structural accuracy and reduce placement errors in generated SOAP notes?
  - Basis in paper: Authors state: "In Future work, we will focus on... incorporating human-in-the-loop refinement strategies."
  - Why unresolved: Current framework exhibits placement errors (e.g., diagnosis appearing prematurely in Chief Complaint instead of Assessment section), but no interactive correction mechanism exists.
  - What evidence would resolve it: A/B comparison showing reduced placement errors and higher Flow-Judge scores when clinicians iteratively correct model outputs during training.

- How does the framework generalize across diverse dermatology datasets with varying metadata schemas?
  - Basis in paper: Authors note: "variations in metadata across different sources posed challenges for standardization" and they "utilized only a single dataset."
  - Why unresolved: PAD-UFES-20 contains specific structured features; unclear if the caption synthesis and retrieval mechanisms adapt to datasets with different clinical attribute formats.
  - What evidence would resolve it: Evaluation on additional datasets (e.g., ISIC, HAM10000) with heterogeneous metadata, reporting MedConceptEval scores across data sources.

- Can evaluation benchmarks capturing longitudinal clinical reasoning across multiple patient encounters improve utility for real-world decision-making?
  - Basis in paper: Authors state future work includes "developing evaluation benchmarks that capture the progression of clinical reasoning across multiple encounters and support decision-making."
  - Why unresolved: Current metrics (MedConceptEval, CCS) assess single-visit notes; real clinical workflows require tracking condition evolution and treatment response over time.
  - What evidence would resolve it: Correlation between multi-visit benchmark scores and clinician utility ratings in prospective workflow studies.

## Limitations

- Ground-truth annotations: The paper relies on 3 expert-annotated SOAP notes for evaluation but does not release these annotations, creating a fundamental barrier to exact reproduction.
- Weak supervision quality: Synthetic SOAP notes may contain hallucinations or inaccuracies, with uncertainty about whether errors in synthetic data propagate to the final model.
- RAG implementation specifics: Critical details including embedding model, chunk size, top-k parameters, and descriptor bank construction are underspecified.

## Confidence

- High confidence in the overall architectural approach and core claims about RAG grounding reducing hallucinations
- Medium confidence in the weak supervision strategy's effectiveness given acknowledged synthetic data quality risks
- Medium confidence in clinical fidelity for common conditions (melanoma) while recognizing lower performance for rare conditions (SCC, BCC)

## Next Checks

1. **Section-wise hallucination analysis**: Systematically evaluate generated SOAP notes for section-specific hallucinations by checking if clinical concepts appear in semantically inappropriate sections (e.g., diagnosis in Subjective vs. Assessment) across all 6 lesion types.

2. **RAG contribution quantification**: Implement an ablation study comparing model performance with and without the retrieval module to isolate the specific contribution of knowledge grounding to semantic quality and hallucination reduction.

3. **Rare condition performance profiling**: Conduct detailed analysis of MedConceptEval scores for SCC and BCC cases to identify whether retrieval coverage gaps or model capacity limitations drive the observed performance drop for less common dermatological conditions.