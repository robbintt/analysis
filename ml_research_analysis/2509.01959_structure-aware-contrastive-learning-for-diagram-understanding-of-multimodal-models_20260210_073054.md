---
ver: rpa2
title: Structure-aware Contrastive Learning for Diagram Understanding of Multimodal
  Models
arxiv_id: '2509.01959'
source_url: https://arxiv.org/abs/2509.01959
tags:
- hard
- negative
- samples
- clip
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a structure-aware contrastive learning approach
  for enhancing diagrammatic understanding in multimodal models like CLIP. The method
  addresses limitations of existing models when interpreting structured visual content
  such as flowcharts, which differ significantly from natural imagery.
---

# Structure-aware Contrastive Learning for Diagram Understanding of Multimodal Models

## Quick Facts
- arXiv ID: 2509.01959
- Source URL: https://arxiv.org/abs/2509.01959
- Authors: Hiroshi Sasaki
- Reference count: 32
- Primary result: Proposed structure-aware contrastive learning achieves substantial improvements on flowchart understanding tasks, outperforming standard CLIP and hard negative contrastive learning baselines.

## Executive Summary
This paper introduces a structure-aware contrastive learning approach to enhance diagrammatic understanding in multimodal models like CLIP. The method addresses limitations of existing models when interpreting structured visual content such as flowcharts, which differ significantly from natural imagery. The proposed approach uses synthetic hard positive and negative samples generated through diagram code manipulation, combined with two specialized loss functions: structure-aware contrastive loss and distinct factor orthogonal loss. These losses leverage the inherent structural properties of diagrams to improve semantic understanding.

## Method Summary
The method combines granulation-based decomposition of diagrams into adjacent node triplets with synthetic hard positive and negative sample generation. Hard positives are created by reversing flow direction while preserving node relationships, and hard negatives through label swaps, arrow reversals, or arrow removals. The training uses two specialized losses: Structure-aware Contrastive (SC) Loss that computes pairwise similarities across inter-modal and intra-modal spaces, and Distinct factor Orthogonal (DO) Loss that separates embeddings into shared and distinct factors to preserve semantic elements while enabling discrimination. The approach is evaluated on flowchart datasets and demonstrates substantial improvements over standard CLIP and conventional hard negative learning paradigms.

## Key Results
- Achieves higher performance in both image-text matching and visual question answering tasks on flowchart datasets
- Outperforms standard CLIP and conventional hard negative CLIP learning paradigms
- Demonstrates effectiveness of tailored training strategies for specialized visual domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing complex diagrams into simpler triplets of adjacent nodes enables standard CLIP architectures to process structured visual content that would otherwise exceed their input capacity constraints.
- Mechanism: The granulation process extracts all combinations of adjacent node triplets from diagram definition code (e.g., Mermaid), reconstructs them as simplified sub-diagrams, normalizes flow direction (top-down), and generates synthetic captions using templates like "An arrow points from node ⟨A⟩ to node ⟨B⟩." This transforms a single complex diagram into multiple trainable instances that fit within CLIP's input size limits.
- Core assumption: The structural semantics of a diagram can be preserved through local triplet relationships rather than requiring global context.
- Evidence anchors:
  - [abstract] "...hard positive and negative samples generated through a granulation process that decomposes diagrams into simpler subparts..."
  - [section 3.1] "The granulation process... involves extracting all combinations of adjacent triplets of nodes from each diagram code."
  - [corpus] Weak direct corpus support for granulation specifically; neighbor papers focus on hard negative sampling (MMGeoLM, TripletCLIP) rather than structural decomposition.
- Break condition: If diagrams require long-range dependencies (e.g., a node's meaning depends on distant context), triplet-level granulation may lose critical semantic information.

### Mechanism 2
- Claim: Hard positive samples (visually distinct but semantically identical) combined with hard negative samples (visually similar but semantically distinct) force the model to learn structural rather than superficial visual features.
- Mechanism: Hard positive images are created by reversing flow direction (top-down to bottom-up), preserving node labels and relationships while altering visual appearance. Hard negative images apply perturbations: swapping node labels, reversing arrow directions, removing arrows, or combining these operations. This creates a curriculum where the model must distinguish based on semantic structure, not visual similarity.
- Core assumption: Flow direction reversal is perceptually significant but semantically neutral; the model will learn to prioritize relational structure over visual orientation.
- Evidence anchors:
  - [abstract] "...manipulates them to create challenging training examples..."
  - [section 3.2] "Rule for Hard Positive Images... Reverse the flow direction from top-down to bottom-up"; "Rule for Hard Negative Images... Randomly swap the labels of selected nodes. Reverse the direction of randomly selected arrows."
  - [corpus] "MMGeoLM: Hard Negative Contrastive Learning for Fine-Grained Geometric Understanding" supports hard negative effectiveness for geometric/structural tasks.
- Break condition: If diagram semantics inherently depend on flow direction (e.g., strictly top-down conventions in certain domains), reversing direction may introduce semantic confounds rather than creating true positives.

### Mechanism 3
- Claim: Combining Structure-aware Contrastive (SC) Loss with Distinct factor Orthogonal (DO) Loss enables discrimination between semantically correct and incorrect samples while preserving shared information (e.g., node names) that should not be disrupted.
- Mechanism: SC Loss extends NegCLIP/TripletCLIP by computing similarity across all pairwise relations (original-hard positive, original-hard negative) in both inter-modal and intra-modal spaces, pulling hard positives closer while pushing hard negatives apart. DO Loss assumes embeddings decompose into shared vectors (zs*) and distinct vectors (z'*), uses Thales's theorem to approximate orthogonality between distinct components, and applies Moore-Penrose inverse to solve for the shared factor, preserving common semantic elements.
- Core assumption: Embedding space is locally Euclidean, and shared/distinct factors are linearly decomposable.
- Evidence anchors:
  - [abstract] "...incorporates two specialised loss functions that leverage the inherent structural properties of diagrams."
  - [section 3.3] "the DO loss, assumes that the embedding vectors are approximately situated within Euclidean space... which can limit the generalisability of the approach."
  - [corpus] "Can Visual Encoder Learn to See Arrows?" corroborates structural understanding challenges in VLMs; "TripletCLIP" validates hard negative contrastive learning foundations.
- Break condition: If embedding space exhibits significant curvature or non-Euclidean structure, Thales's theorem approximation in DO Loss becomes invalid, potentially degrading rather than improving representations.

## Foundational Learning

- Concept: **Contrastive Learning (InfoNCE Loss)**
  - Why needed here: The paper builds directly on CLIP's contrastive training; understanding how cosine similarity, positive/negative pairs, and temperature-scaled logits work is prerequisite to grasping SC Loss extensions.
  - Quick check question: Can you explain why increasing the distance between an anchor and hard negatives improves fine-grained discrimination compared to random negatives?

- Concept: **Triplet Loss and Hard Negative Mining**
  - Why needed here: The paper positions itself as an extension of TripletCLIP and NegCLIP; understanding the anchor-positive-negative triplet structure is essential for understanding the multi-pair SC Loss formulation.
  - Quick check question: What makes a hard negative "hard," and why might too-hard negatives collapse training?

- Concept: **Disentangled Representations**
  - Why needed here: DO Loss explicitly separates embeddings into shared and distinct factors; familiarity with disentanglement concepts (e.g., VAE latent spaces) helps understand why orthogonal distinct components preserve shared semantics.
  - Quick check question: Why would orthogonal distinct factors help a model distinguish between two diagrams that share node names but differ in arrow direction?

## Architecture Onboarding

- Component map:
  - Diagram codes (Mermaid) -> Granulation module -> Triplet extraction -> SVG rasterization -> Image encoder (CLIP ViT-L/14@336px)
  - P_v (flow reversal), N_v (label/arrow perturbations), P_t (original code), N_t (semantic distortions) -> Dual Encoder
  - Shared CLIP vision encoder (F_v) + text encoder (F_t), both fine-tuned via LoRA -> Loss Aggregator
  - L_CL (standard InfoNCE) + λ_SC * L_SC + λ_DO * L_DO -> backprop -> Fine-tuned encoder plugged into LLaVA-v1.6-Mistral-7B for VQA evaluation

- Critical path:
  1. Parse Mermaid diagrams -> granulate into triplets -> validate triplet coverage
  2. Generate hard positives/negatives dynamically per batch -> verify perturbation rules
  3. Initialize from CLIP ViT-L/14@336px -> apply LoRA (α=256, r=64)
  4. Train 3 epochs, LR=0.0004, warmup=8 steps -> monitor L_SC and L_DO convergence
  5. Evaluate on FlowVQA: image-text matching (R@1, R@5, MRR) + VQA (BERTScore)

- Design tradeoffs:
  - Granulation granularity: Triplets capture local structure but may miss global context; larger subgraphs increase computational cost and may exceed input size
  - λ_SC vs. λ_DO balance: Paper tests λ_SC=0.1 with λ_DO∈{0, 0.01, 0.1}; higher λ_DO improves VQA F1 but may over-regularize, reducing retrieval sharpness
  - Hard negative intensity: More aggressive perturbations (multiple edits) create harder negatives but risk semantic incoherence; paper uses combinations but does not ablate edit complexity
  - Euclidean assumption: DO Loss depends on local Euclidean geometry; alternative disentanglement losses (e.g., mutual information minimization) could be more robust but add complexity

- Failure signatures:
  - Collapsed embeddings: If SC Loss pushes negatives too aggressively without DO regularization, shared semantic features (node names) may be erased, causing retrieval failures
  - Granulation artifacts: If triplet extraction misses critical edges, the model will never see those relationships during training -> test-time failures on multi-hop queries
  - Hard negative leakage: If perturbation rules accidentally preserve semantic equivalence (e.g., swapping semantically identical labels), the model learns to ignore hard negatives
  - DO Loss divergence: If Z* matrix is ill-conditioned, Moore-Penrose inverse produces unstable zs* estimates -> loss spikes or NaN values

- First 3 experiments:
  1. Ablate granulation: Compare triplet-level vs. full-diagram training on a held-out flowchart subset with complex structures (4+ branches); measure R@1 gap to quantify local vs. global context tradeoff
  2. Stress-test hard negatives: Create a diagnostic set where hard negatives differ by exactly one perturbation type (label swap only, arrow reverse only, arrow removal only); isolate which perturbations contribute most to SC Loss gains
  3. Probe DO Loss geometry: Extract embeddings for original/hard negative pairs; compute actual angles between distinct factor approximations vs. random vector pairs; verify if Thales's theorem holds empirically or if curvature is significant

## Open Questions the Paper Calls Out
None

## Limitations
- Granulation granularity tradeoff: The triplet-level decomposition may fail to capture long-range dependencies in complex flowcharts, potentially limiting performance on multi-hop reasoning tasks.
- Euclidean space assumption: The Distinct Factor Orthogonal loss relies on Thales's theorem assuming locally Euclidean embedding geometry, which may not hold in practice.
- Synthetic data dependence: Hard sample generation depends on controlled perturbations of diagram code, but the perturbation rules may not fully capture the complexity of real-world diagram variations.

## Confidence
- **High confidence**: The core mechanism of using hard positive/negative contrastive learning for structured visual content is well-supported by existing literature and experimental improvements are substantial and consistent.
- **Medium confidence**: The specific combination of granulation + SC loss + DO loss represents a novel synthesis, but the individual contribution of DO loss is less clear since its effectiveness depends on the Euclidean assumption that isn't empirically validated.
- **Low confidence**: The generalization of this approach beyond flowcharts to other diagram types is not demonstrated, and the perturbation rules may not translate directly to domains with different structural conventions.

## Next Checks
1. Granulation granularity ablation: Systematically vary the subgraph size used in granulation (pairs, triplets, quadruples, full diagrams) and measure the performance tradeoff on complex flowcharts requiring multi-hop reasoning.
2. DO loss geometry validation: Extract embeddings from original/hard negative pairs and compute actual angles between distinct factor approximations versus random vector baselines to empirically verify the Euclidean assumption and Thales's theorem approximation.
3. Perturbation rule robustness test: Create diagnostic datasets where hard negatives differ by exactly one perturbation type (label swap only, arrow reverse only, arrow removal only) to isolate which perturbation rules contribute most to the SC loss improvements and identify potential semantic leakage.