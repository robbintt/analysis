---
ver: rpa2
title: Efficient Sliced Wasserstein Distance Computation via Adaptive Bayesian Optimization
arxiv_id: '2509.17405'
source_url: https://arxiv.org/abs/2509.17405
tags:
- bosw
- optimization
- bayesian
- directions
- wasserstein
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using Bayesian optimization (BO) to learn projection
  directions for sliced Wasserstein (SW) distance estimation, rather than relying
  on random or quasi-Monte Carlo sampling. The core idea is that in settings where
  SW appears inside an optimization loop, BO can adaptively select informative projection
  directions by treating the 1D Wasserstein cost as a black-box function on the sphere.
---

# Efficient Sliced Wasserstein Distance Computation via Adaptive Bayesian Optimization

## Quick Facts
- arXiv ID: 2509.17405
- Source URL: https://arxiv.org/abs/2509.17405
- Reference count: 32
- Primary result: Bayesian optimization can adaptively learn informative projection directions for sliced Wasserstein estimation, improving convergence in optimization-in-the-loop tasks.

## Executive Summary
This paper introduces Bayesian optimization (BO) as a method to learn projection directions for sliced Wasserstein (SW) distance estimation, rather than relying on random or quasi-Monte Carlo sampling. The approach treats the per-slice 1D Wasserstein cost as a black-box function on the sphere and uses a Gaussian Process surrogate to adaptively select informative directions. Four variants are proposed: BOSW (one-shot BO), RBOSW (periodic refresh), ABOSW (QSW-seeded refinement), and ARBOSW (restarted hybrid). Experiments on point-cloud interpolation, image style transfer, and deep autoencoders show that ABOSW and ARBOSW achieve competitive convergence with quasi-Monte Carlo baselines while offering modest runtime overhead.

## Method Summary
The method uses Bayesian optimization to learn projection directions for sliced Wasserstein distance estimation. A Gaussian Process with angular RBF kernel models the landscape of 1D Wasserstein costs across the unit sphere. The Upper Confidence Bound acquisition function selects directions that balance exploitation of high-cost regions and exploration of uncertain areas. Four variants are introduced: BOSW performs one-shot BO, RBOSW periodically refreshes directions, ABOSW seeds with quasi-Monte Carlo sequences and refines with BO, and ARBOSW combines periodic restarts with hybrid seeding. The approach integrates into existing SW pipelines without altering downstream losses or gradients.

## Key Results
- ABOSW and ARBOSW achieve convergence competitive with best quasi-Monte Carlo baselines on ShapeNet point-cloud interpolation
- BOSW underperforms QSW in pure integration tasks but excels when SW appears inside optimization loops
- ABOSW achieves lowest reconstruction losses in deep point-cloud autoencoders, even surpassing state-of-the-art CQSW
- ARBOSW shows best overall performance across dynamic tasks by combining periodic restarts with hybrid seeding

## Why This Works (Mechanism)

### Mechanism 1: Informative Projection Selection via Surrogate Modeling
The method uses a Gaussian Process with angular RBF kernel to model the landscape of 1D Wasserstein costs across the sphere. An acquisition function (UCB) prioritizes directions where the surrogate predicts high cost (exploitation) or high uncertainty (exploration), focusing the projection budget on "high-signal" regions. This works when the projection landscape is smooth and informative directions exist.

### Mechanism 2: Hybrid Seeding for Bias-Variance Control
ABOSW initializes with quasi-Monte Carlo sequences ensuring uniform coverage, then uses BO to refine by replacing worst-performing slices. This stabilizes training while improving upon pure QSW or pure BO baselines. The approach trades strict unbiasedness for faster convergence.

### Mechanism 3: Dynamic Re-alignment in Gradient Flows
In optimization loops where distributions evolve, periodically re-learning directions maintains alignment between the projection set and changing geometry. As point clouds move, "important" directions shift, and the ARBOSW variant periodically restarts BO to adapt to new geometric structure.

## Foundational Learning

- **Gaussian Processes on Riemannian Manifolds**: Standard BO assumes Euclidean input space, but projection directions lie on a unit sphere requiring kernels that respect geodesic distance. Quick check: Why use arccos(⟨θ,θ'⟩) instead of Euclidean distance in the RBF kernel?

- **Sliced Wasserstein Distance (SW)**: This is the objective function being optimized. SW turns an O(n³) OT problem into L sorting operations (O(n log n)). Quick check: How does SW complexity scale with points n and projections L?

- **Exploration vs. Exploitation in BO**: The UCB acquisition function μ(θ) + βσ(θ) balances sampling known high-cost areas vs. uncertain areas. Quick check: In SW context, what does "exploitation" mean vs. "exploration"?

## Architecture Onboarding

- **Component map**: Data Source -> Projection Selector (GP Surrogate + Acquisition Function) -> SW Core (1D sort-and-sum) -> Outer Optimizer (gradient descent)

- **Critical path**:
  1. Initialize Θ_L (Random or QSW)
  2. Evaluate SW distance and per-slice costs f(θ)
  3. Fit GP to (θ, f(θ)) pairs
  4. Optimize acquisition function on sphere → propose new θ
  5. Replace worst θ (or refresh all) in Θ_L
  6. Return SW estimate and gradients to outer optimizer

- **Design tradeoffs**:
  - Bias vs. Variance: Pure BOSW is biased; QSW is unbiased; ABOSW/ARBOSW trade unbiasedness for convergence
  - Runtime vs. Convergence: RBOSW has high overhead but faster convergence; BOSW has low overhead but may stagnate

- **Failure signatures**:
  - Mode Collapse: BO selects identical directions (cosine similarity ≈ 1.0)
  - Stagnation: GP overfits and fails to propose new useful directions
  - Dimensionality Curse: In high dimensions (d > 100), BO sampling becomes ineffective

- **First 3 experiments**:
  1. Sanity Check: Replicate "Peaks" landscape experiment to verify BO finds global maximum
  2. Static Integration: Replicate ShapeNet Fig 2 to confirm BOSW underperforms QSW in pure integration
  3. Dynamic Flow: Run point-cloud interpolation comparing ARBOSW vs. CQSW to verify refresh mechanism improves final W₂ distance

## Open Questions the Paper Calls Out

### Open Question 1
Can theoretical performance bounds be established for BOSW methods with finite projection budgets L? The paper notes this remains an interesting open question since BOSW with UCB is not an unbiased estimator, so standard MC convergence rates don't apply.

### Open Question 2
Does an annealing schedule for the UCB exploration parameter β restore asymptotic unbiasedness while preserving early-stage convergence advantages? The paper proposes mixing acquisition with uniform sampling via ϵ_t = t^(-γ) but notes this remains untested.

### Open Question 3
How do BO-based SW methods scale to higher-dimensional ambient spaces (d ≫ 3)? The paper acknowledges traditional concerns about BO's dimensionality scaling and plans to test on higher-dimensional datasets.

### Open Question 4
Can domain-specific constraints or priors be incorporated into the BO formulation to improve performance over data-agnostic QSW? The paper suggests this could give further advantage but presents no experiments exploring informed priors or constrained acquisition.

## Limitations
- Runtime overhead from BO steps (GP fitting, acquisition optimization) may be significant, especially with frequent refreshes
- BOSW is explicitly biased, limiting use in statistical estimation tasks where unbiasedness is required
- Dynamic adaptation granularity depends on unspecified refresh intervals, making effectiveness assessment difficult across different task timescales

## Confidence
- **High confidence**: BOSW provides valid framework for adaptive direction selection when bias is acceptable; GP + angular RBF + UCB is technically sound
- **Medium confidence**: ABOSW and ARBOSW improve convergence over QSW in optimization-in-the-loop settings based on empirical results, though runtime trade-offs are not fully quantified
- **Low confidence**: Claim that BO can universally replace QSW in static SW estimation tasks is weakly supported; BOSW underperforms QSW in pure approximation benchmarks

## Next Checks
1. Runtime profiling: Measure wall-clock time per iteration for BOSW, ABOSW, and ARBOSW against QSW baselines on ShapeNet interpolation (500 steps, L=100)
2. Bias quantification: Compute and compare bias and variance of BOSW vs. QSW across multiple random seeds on fixed SW estimation task
3. Refresh interval sensitivity: Systematically vary refresh interval R in RBOSW and restart frequency in ARBOSW to identify optimal balance between tracking performance and computational cost