---
ver: rpa2
title: 'Why LVLMs Are More Prone to Hallucinations in Longer Responses: The Role of
  Context'
arxiv_id: '2510.20229'
source_url: https://arxiv.org/abs/2510.20229
tags:
- image
- hallucinations
- hallucination
- arxiv
- objects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates why large vision-language models (LVLMs)
  hallucinate more in longer responses. The authors propose that the key factor is
  not response length itself, but rather the model's increased reliance on context
  for coherence and completeness.
---

# Why LVLMs Are More Prone to Hallucinations in Longer Responses: The Role of Context

## Quick Facts
- arXiv ID: 2510.20229
- Source URL: https://arxiv.org/abs/2510.20229
- Reference count: 40
- This paper proposes that longer responses trigger more hallucinations not due to length itself, but because LVLMs increasingly rely on context for coherence and completeness.

## Executive Summary
This paper challenges the common belief that hallucination frequency in LVLM responses directly correlates with response length. Instead, the authors propose that hallucinations arise from the model's increased reliance on context to maintain coherence and completeness when generating longer outputs. Based on this insight, they develop HalTrapper, a three-step framework that induces, detects, and suppresses hallucinations. Their approach achieves significant improvements in hallucination detection (up to 12% AUROC increase) and mitigation (up to 10% reduction in CHAIR hallucination metrics) across multiple benchmarks and models.

## Method Summary
HalTrapper is a training-free framework that operates in three stages. First, it induces hallucinations by appending "There is also" to initial responses and extracting attention patterns from induced objects. Second, it detects potential hallucinations using two complementary methods: Internal Grounding (IG) which computes attention similarity between induced and existing objects, and External Expansion (EE) which prompts the model to imagine objects outside the frame across 8 spatial directions. Third, it suppresses detected hallucinations during decoding using Contrastive Contextual Decoding (CCD), which constructs a contrastive branch encoding detected hallucination candidates as context tokens, amplifying their likelihood in the contrastive branch and subtracting it from the main branch to reduce their occurrence.

## Key Results
- Proposes that hallucination risk is driven by context reliance for coherence/completeness rather than response length itself
- Achieves up to 12% AUROC increase in hallucination detection across LLaVA v1.5 7B, MiniGPT-4, Qwen VL Chat, Qwen2 VL 7B, and Janus Pro 7B
- Achieves up to 10% reduction in CHAIR hallucination metrics through contrastive contextual decoding
- Demonstrates effectiveness across COCO and AMBER benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Attention Similarity as Hallucination Signal
- Claim: Hallucinated objects exhibit dispersed attention patterns with high pairwise similarity, distinguishable from focused attention on real objects.
- Mechanism: Contextual coherence creates tension—models must maintain consistency with prior outputs while avoiding repetition. This conflict disperses attention to ungrounded regions, producing similar noisy attention maps across hallucinated tokens.
- Core assumption: Attention similarity correlates with hallucination likelihood; the pattern generalizes beyond the tested models (LLaVA, MiniGPT-4, Qwen-VL).
- Evidence anchors:
  - [abstract] "detects potential hallucinations using attention similarity and consistency across prompts"
  - [section 4.1] "hallucinated objects typically manifest diffuse, noisy attention patterns, making attention similarity a robust metric for their detection"
  - [corpus] Related work on sparse autoencoders for hallucination mitigation (arXiv:2505.16146) suggests mechanistic interpretability approaches align with attention-based detection.
- Break condition: If hallucinated tokens do NOT show higher intra-set attention similarity than non-hallucinated tokens in your model, this mechanism fails.

### Mechanism 2: Completeness-Driven Extrapolation
- Claim: Hallucinations arise from contextual completeness pressure—when models exhaust grounded content but must continue generating coherent, comprehensive responses.
- Mechanism: Models compensate for informational/structural incompleteness by extrapolating beyond recognized content. This produces consistent hallucinations across varied prompts for the same image.
- Core assumption: Hallucinations are image-context-dependent rather than purely random or language-prior-driven.
- Evidence anchors:
  - [abstract] "risk of hallucinations is not caused by length itself but by the increased reliance on context for coherence and completeness"
  - [section 4.2] "all models exhibit a high degree of repetitiveness in hallucinated objects, with objects appearing in only one response accounting for merely 30% on average"
  - [corpus] Weak direct corpus support; most related work focuses on contrastive decoding or external tools rather than completeness mechanisms.
- Break condition: If hallucinated objects vary randomly across prompts (low repeatability), the completeness-driven extrapolation hypothesis is invalid.

### Mechanism 3: Contrastive Contextual Decoding (CCD)
- Claim: Encoding detected hallucination candidates as contrastive context tokens reduces their probability during decoding.
- Mechanism: CCD constructs a contrastive branch by concatenating CCT tokens with image input. The contrastive logits amplify hallucinated object likelihood in the contrastive branch, which then gets subtracted from the main branch—suppressing hallucinations.
- Core assumption: Detected hallucination candidates are accurate enough that suppressing them doesn't harm recall of real objects.
- Evidence anchors:
  - [abstract] "suppresses these hallucinations during decoding...achieves up to 10% reduction in CHAIR hallucination metrics"
  - [section 5.2] "By treating CCT tokens as complementary to image content, the model naturally increases the likelihood of potential hallucinated objects...thereby effectively reducing their occurrence"
  - [corpus] VCD (arXiv:2311.16922) and related contrastive decoding methods validate the broader contrastive approach; CCD adapts it for context-driven hallucinations.
- Break condition: If precision drops significantly (false positives in detection suppress real content), detection thresholds need recalibration.

## Foundational Learning

- **Concept: Autoregressive Generation and Error Accumulation**
  - Why needed here: The paper explicitly contrasts its context-based hypothesis with the prior belief that hallucinations accumulate with length.
  - Quick check question: Can you explain why cropping an image (reducing content) causes hallucinations to appear *earlier* rather than later?

- **Concept: Contrastive Decoding in LVLMs**
  - Why needed here: CCD builds directly on contrastive decoding; understanding logit manipulation is essential for implementation.
  - Quick check question: What happens if α in the contrastive formula is set too high or too low?

- **Concept: Attention Map Analysis**
  - Why needed here: IGScore relies on computing cosine similarity between attention maps of generated objects.
  - Quick check question: Given two attention maps A and B for objects in the same response, how would you compute their similarity?

## Architecture Onboarding

- **Component map:** Internal Grounding (IG) -> External Expansion (EE) -> Contrastive Contextual Decoding (CCD)
- **Critical path:** Detection quality → CCT construction → CCD hyperparameters (α=1.0, β=0.1). Detection errors propagate directly to suppression.
- **Design tradeoffs:**
  - Higher θ_IG reduces false positives but may miss hallucinations
  - EE alone captures image-contextual hallucinations but may miss intra-response ones
  - CCT truncation priority (IG > EE) favors attention-based detection over consistency-based
- **Failure signatures:**
  - Recall drops → detection too aggressive (lower θ thresholds)
  - Hallucinations persist → detection missing candidates (raise thresholds or check attention computation)
  - Fluency degrades → α too high in CCD
- **First 3 experiments:**
  1. Reproduce Fig. 3: Compute S_H and S_N distributions on a small sample; verify hallucinated pairs show higher similarity.
  2. Ablate IG vs. EE: Run HalTrapper with only IG, only EE, and both; compare CHAIR scores.
  3. Threshold sweep: Test θ_IG ∈ {0.65, 0.75, 0.85} and θ_EE ∈ {0, 1, 2} on 100 images; plot AUROC vs. F1 tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the mechanisms of contextual coherence and completeness driving hallucinations in LVLMs apply equally to attribute-level and relational-level hallucinations?
- Basis in paper: [explicit] Section F (Limitations) explicitly states the work primarily addresses object-level hallucinations and notes the models are susceptible to failures in instruction following and hallucinations at attribute and relational levels.
- Why unresolved: The proposed HalTrapper framework and the statistical analysis of attention similarity (IG Score) were designed and validated specifically for object entities using metrics like CHAIR, which do not capture attribute or relation errors.
- What evidence would resolve it: Extending the analysis to benchmarks like AMBER (which includes attribute/relation annotations) to determine if hallucinated attributes/relations also exhibit high attention similarity or consistent repetition across prompts.

### Open Question 2
- Question: Can the HalTrapper framework maintain efficacy in open-ended, multi-turn conversational scenarios where context evolves dynamically over time?
- Basis in paper: [explicit] Section F notes that current evaluations are mainly on image captioning benchmarks which "do not adequately cover more open-ended generative scenarios."
- Why unresolved: The current framework relies on inducing hallucinations via single-turn manipulations (e.g., "There is also..."), whereas multi-turn dialogues accumulate context and potential error propagation differently.
- What evidence would resolve it: Testing the suppression method on multi-turn dialogue benchmarks (e.g., MM-Vet conversation subsets) to see if CCD prevents hallucinations when the "context" includes long conversational history.

### Open Question 3
- Question: Is the "contextual completeness" mechanism independent of linguistic co-occurrence priors, or does it merely amplify existing language biases when visual content is exhausted?
- Basis in paper: [inferred] Section 4.2 discusses "contextual extrapolation" as a compensatory strategy, while Section 2.2 cites co-occurrence patterns [32, 99] as a known factor. The interaction between the pressure for completeness and statistical priors is not isolated.
- Why unresolved: The "External Expansion" experiments show consistent hallucinations, but the paper does not disentangle whether the model generates specific objects because they are visually plausible or simply because they statistically co-occur with the prompt content.
- What evidence would resolve it: Ablation studies analyzing hallucination rates on images specifically designed to contradict common linguistic priors, checking if the "completeness" drive overrides visual grounding.

## Limitations
- The framework targets object-level hallucinations and has not been validated for attribute-level or relational-level hallucinations
- Detection performance heavily depends on threshold parameters that may need model-specific tuning
- Implementation complexity includes computational overhead from multiple decoding passes and attention map computations

## Confidence

**High confidence**: The core hypothesis that context reliance—not response length—drives hallucinations is well-supported by the observed pattern that cropping images causes earlier hallucinations. The mechanism of attention similarity correlating with hallucination likelihood is empirically validated in Fig. 3.

**Medium confidence**: The completeness-driven extrapolation mechanism (Mechanism 2) has moderate support from the repeatability analysis showing 70% of hallucinated objects appear consistently across prompts. However, this could also reflect language prior biases rather than genuine completeness pressure.

**Low confidence**: The contrastive contextual decoding mechanism's effectiveness relies on the assumption that detected candidates are sufficiently accurate. While CHAIR metrics improve, the potential for over-suppression harming recall is not extensively explored.

## Next Checks

1. **Cross-model attention pattern validation**: Extract attention maps from three different LVLM architectures (beyond the tested ones) for hallucinated vs. grounded objects. Compute IGScore distributions to verify the similarity pattern generalizes.

2. **Hallucination type ablation study**: Apply HalTrapper to responses containing different hallucination types (object errors, factual errors, reasoning errors). Measure which types benefit most from the framework and identify failure modes.

3. **Threshold-accuracy tradeoff analysis**: Systematically sweep θ_IG and θ_EE across their plausible ranges on a held-out validation set. Plot detection F1 score against CHAIR hallucination reduction to identify optimal operating points and quantify precision-recall tradeoffs.