---
ver: rpa2
title: '4,500 Seconds: Small Data Training Approaches for Deep UAV Audio Classification'
arxiv_id: '2505.23782'
source_url: https://arxiv.org/abs/2505.23782
tags:
- data
- audio
- dataset
- augmentations
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates deep learning approaches for UAV audio
  classification under extreme data scarcity, using a 9-class dataset totaling 4,500
  seconds of audio. It compares CNNs and pre-trained AST transformers, leveraging
  parameter-efficient fine-tuning (PEFT) and data augmentations to mitigate limited
  data.
---

# 4,500 Seconds: Small Data Training Approaches for Deep UAV Audio Classification

## Quick Facts
- **arXiv ID:** 2505.23782
- **Source URL:** https://arxiv.org/abs/2505.23782
- **Reference count:** 10
- **Primary result:** CNNs outperform pre-trained transformers by 1-2% accuracy on 9-class UAV audio classification with only 4,500 seconds of training data

## Executive Summary
This study investigates deep learning approaches for UAV audio classification under extreme data scarcity, using a 9-class dataset totaling 4,500 seconds of audio. It compares CNNs and pre-trained AST transformers, leveraging parameter-efficient fine-tuning (PEFT) and data augmentations to mitigate limited data. Results show CNNs outperform transformers by 1-2% accuracy while being more computationally efficient. The best CNN configuration achieved 98.88% accuracy with augmentations, while AST with ia3 adapters reached 97.83%. Despite CNNs' current superiority, transformer performance suggests potential with more data. Key augmentations included sin-distortion, tanh-distortion, and time stretch. The study demonstrates effective small-data training techniques and highlights transformers' promise for future scalability.

## Method Summary
The research tackles 9-class UAV audio classification using only 4,500 seconds of total training data. The methodology involves training a custom CNN architecture from scratch alongside a pre-trained Audio Spectrogram Transformer (AST) using parameter-efficient fine-tuning. Both models process Mel-spectrogram features, with the CNN using a 3-layer convolutional architecture and the AST leveraging ia3 adapters for efficient adaptation. Data augmentation techniques including sin-distortion, tanh-distortion, and time stretch are applied to expand the training data. The CNN achieves 98.88% accuracy while the AST reaches 97.83%, demonstrating that simpler architectures can outperform more complex ones when data is extremely limited.

## Key Results
- Custom CNN achieved 98.88% accuracy with augmentations, outperforming pre-trained AST (97.83%)
- CNNs trained faster and required less computational resources than transformers
- Augmentations (sin-distortion, tanh-distortion, time stretch) were critical for performance
- Transformers show promise but require more data to surpass CNN performance
- Small data training techniques enable high accuracy with minimal training data

## Why This Works (Mechanism)
The success stems from combining appropriate model architecture selection with effective data augmentation for extreme data scarcity. CNNs provide efficient feature extraction for audio spectrograms without requiring large pre-training datasets. Parameter-efficient fine-tuning allows transformers to adapt to new tasks without overfitting on limited data. The augmentation techniques artificially expand the dataset by introducing controlled variations in the audio signals, helping models generalize better from limited examples.

## Foundational Learning
- **Mel-spectrogram conversion**: Transforms audio signals into 2D representations suitable for CNN processing. Why needed: Provides visual-like features that capture temporal and frequency patterns in UAV sounds. Quick check: Verify spectrogram dimensions match model input requirements.
- **Parameter-efficient fine-tuning (PEFT)**: Adapts pre-trained models by training only small adapter modules instead of full weights. Why needed: Prevents overfitting when fine-tuning large models on limited data. Quick check: Ensure base model weights are frozen during adapter training.
- **Data augmentation for audio**: Applies transformations like distortion and time stretching to expand training data. Why needed: Compensates for limited samples by creating realistic variations. Quick check: Confirm augmentations are applied only to training set, not test set.
- **Transfer learning vs scratch training**: Compares benefits of pre-trained models against custom architectures. Why needed: Determines optimal approach based on available data volume. Quick check: Compare training curves for both approaches.
- **Model efficiency metrics**: Evaluates both accuracy and computational requirements. Why needed: Identifies practical solutions for real-world deployment. Quick check: Monitor GPU memory usage and training time.

## Architecture Onboarding

**Component Map:** Raw audio -> Mel-spectrogram -> CNN (Conv2d→ReLU→MaxPool→BatchNorm→FC) OR AST with ia3 adapters -> Classification

**Critical Path:** Data preprocessing → Model training → Augmentation application → Performance evaluation

**Design Tradeoffs:** CNNs offer faster training and better performance on limited data, while transformers provide better scalability with more data but require careful fine-tuning to avoid overfitting

**Failure Signatures:** Dimension mismatches in CNN input layers, overfitting in transformer adapters, data leakage through improper augmentation application

**First Experiments:**
1. Train CNN with default parameters to establish baseline performance
2. Implement AST with ia3 adapters using standard configurations
3. Apply augmentations systematically to measure impact on accuracy

## Open Questions the Paper Calls Out
- At what specific data quantity does the transformer architecture become more viable and accurate than a CNN for UAV audio classification? The experiments were restricted to a "small data" scenario (4,500 seconds), where CNNs consistently outperformed transformers.
- How do pre-trained CNN architectures (e.g., ResNet, MobileNet) compare to the custom CNN and pre-trained transformer when fine-tuned on this dataset? The study only implemented a custom CNN trained from scratch.
- Can the proposed models be effectively pruned and quantized for deployment on multi-modal embedded hardware without significant accuracy loss? The current research focused on training performance rather than edge-device deployment constraints.

## Limitations
- Proprietary dataset access prevents exact replication of results
- Specific augmentation parameters were not fully detailed in the text
- Only tested on extreme data scarcity scenario, not intermediate data volumes
- Limited comparison to other pre-trained CNN architectures
- No evaluation of model compression for edge deployment

## Confidence
- **High:** Core CNN architecture and training setup are directly specified
- **Medium:** AST implementation with ia3 adapters using standard defaults
- **Low:** Proprietary dataset composition and exact augmentation parameters

## Next Checks
1. Verify Mel-spectrogram dimensions produce exactly 19,456 flattened features before the first FC layer in the CNN
2. Implement the AST with ia3 adapters using standard defaults, then test with alternative adapter configurations
3. Create a synthetic validation set by applying documented augmentations to a public UAV dataset and compare baseline performance