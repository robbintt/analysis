---
ver: rpa2
title: Distillation of Large Language Models via Concrete Score Matching
arxiv_id: '2509.25837'
source_url: https://arxiv.org/abs/2509.25837
tags:
- bolts
- takes
- total
- teacher
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Concrete Score Distillation (CSD), a novel
  knowledge distillation objective for large language models (LLMs) that addresses
  the limitations of existing approaches. Traditional distillation losses either suffer
  from softmax-induced smoothing, which obscures valuable logit information, or restrict
  the optimal solution set by enforcing logit shift invariance.
---

# Distillation of Large Language Models via Concrete Score Matching

## Quick Facts
- arXiv ID: 2509.25837
- Source URL: https://arxiv.org/abs/2509.25837
- Authors: Yeongmin Kim, Donghyeok Shin, Mina Kang, Byeonghu Na, Il-Chul Moon
- Reference count: 40
- Proposes CSD to overcome softmax smoothing and logit shift invariance in LLM distillation

## Executive Summary
This paper introduces Concrete Score Distillation (CSD), a novel knowledge distillation objective for large language models that addresses fundamental limitations of existing approaches. Traditional distillation methods either suffer from softmax-induced smoothing that obscures valuable logit information or restrict the optimal solution set through logit shift invariance. CSD overcomes both issues by matching relative logit differences between student and teacher models across all vocabulary pairs using a discrete score-matching framework.

The method provides a broader solution space and allows flexible weighting of vocabulary pairs while maintaining computational efficiency through O(|V|) gradient computation. Experiments across task-agnostic instruction-following and task-specific settings (summarization, math, translation) demonstrate that CSD consistently outperforms recent distillation objectives, achieves favorable fidelity-diversity trade-offs, and yields complementary gains when combined with on-policy techniques. Results show improvements of up to 2.0% in ROUGE-L scores across multiple benchmarks and model sizes up to 7B parameters.

## Method Summary
CSD reformulates knowledge distillation as a concrete score-matching problem between student and teacher logits. Instead of directly matching probabilities or logits, it matches the relative differences between logits for all vocabulary pairs. This is achieved through a novel objective function that computes pairwise logit differences and minimizes their divergence. The method uses an O(|V|) efficient computation scheme through gradient calculation. CSD also introduces flexible weighting mechanisms for vocabulary pairs, allowing prioritization of more informative tokens during distillation.

## Key Results
- CSD outperforms recent distillation objectives by up to 2.0% in ROUGE-L scores across summarization tasks
- Consistent improvements across multiple benchmarks including GSM8K (math) and WMT (translation)
- Achieves favorable fidelity-diversity trade-offs compared to existing methods
- Yields complementary gains when combined with on-policy distillation techniques
- Effective across model sizes up to 7B parameters

## Why This Works (Mechanism)
CSD works by addressing two fundamental limitations in existing distillation methods: softmax-induced smoothing and logit shift invariance. Softmax smoothing occurs when probability distributions are used for distillation, which can obscure important logit-level information. Logit shift invariance refers to the problem where adding a constant to all logits doesn't change the probability distribution but can significantly affect the distillation objective. By focusing on relative logit differences rather than absolute values or probabilities, CSD preserves more information and provides a broader solution space.

## Foundational Learning

**Score Matching**: A statistical technique for learning probability distributions by matching score functions. Why needed: Provides the theoretical foundation for comparing distributions without computing normalization constants. Quick check: Verify that the score function properly captures the direction of maximum probability change.

**Knowledge Distillation**: The process of transferring knowledge from a large teacher model to a smaller student model. Why needed: Enables deployment of efficient models while preserving teacher performance. Quick check: Confirm that student performance improves relative to training from scratch.

**Logit Shift Invariance**: The property that adding constants to all logits doesn't change output probabilities. Why needed: Understanding this limitation helps design objectives that capture more meaningful information. Quick check: Verify that the objective changes when logits are shifted.

**Relative Logit Differences**: Comparing logit values relative to each other rather than in absolute terms. Why needed: Preserves information about token relationships while being invariant to scale. Quick check: Confirm that token ranking information is preserved.

**Pair-wise Vocabulary Comparison**: Evaluating all possible token pairs for distillation. Why needed: Ensures comprehensive coverage of vocabulary relationships. Quick check: Verify that all vocabulary pairs are included in the computation.

## Architecture Onboarding

**Component Map**: Teacher Model -> CSD Objective -> Student Model Training -> Performance Evaluation

**Critical Path**: The forward pass through both teacher and student models, computation of relative logit differences, gradient calculation via CSD objective, and parameter updates in the student model.

**Design Tradeoffs**: CSD trades some computational complexity (pair-wise comparisons) for better information preservation and solution space. The O(|V|) complexity is acceptable given the performance gains.

**Failure Signatures**: Poor distillation performance when vocabulary weighting is misconfigured, or when the student model architecture is too dissimilar from the teacher. Computational bottlenecks may occur with extremely large vocabularies.

**First Experiments**: 1) Compare CSD against standard KL divergence distillation on a small dataset, 2) Test CSD with different vocabulary weighting schemes, 3) Evaluate the impact of temperature scaling on CSD performance.

## Open Questions the Paper Calls Out
None

## Limitations
- Does not address potential computational overhead from pair-wise comparisons at larger scales
- Limited evaluation on frontier-scale models (tested up to 7B parameters only)
- Insufficient ablation studies on hyperparameter sensitivity, particularly vocabulary weighting
- Lacks quantitative diversity metrics beyond qualitative assessment

## Confidence

| Claim Cluster | Confidence |
|---------------|------------|
| CSD overcomes softmax smoothing and logit shift invariance | High |
| CSD provides broader solution space than existing methods | Medium |
| CSD achieves consistent improvements across all tested settings | High |
| CSD yields complementary gains with on-policy techniques | Medium |

## Next Checks

1. Benchmark CSD against existing distillation methods on frontier-scale models (60B+ parameters) to verify scalability and performance retention at extreme model sizes.

2. Conduct comprehensive ablation studies on vocabulary pair weighting schemes and their impact on final performance across different task types.

3. Implement runtime profiling to quantify the computational overhead of CSD's pair-wise comparison mechanism and compare wall-clock training times with existing distillation objectives.