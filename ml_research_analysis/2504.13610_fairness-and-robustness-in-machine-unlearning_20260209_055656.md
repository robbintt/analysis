---
ver: rpa2
title: Fairness and Robustness in Machine Unlearning
arxiv_id: '2504.13610'
source_url: https://arxiv.org/abs/2504.13610
tags:
- unlearning
- robustness
- fairness-gap
- data
- fairness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study addresses fairness and robustness concerns in machine
  unlearning, which have been overlooked in prior research. The authors propose two
  conjectures based on the variance-bias trade-off principle: first, that a robust
  unlearning algorithm should preserve the fairness-gap of the original model; second,
  that a higher fairness-gap makes models more vulnerable to adversarial attacks.'
---

# Fairness and Robustness in Machine Unlearning

## Quick Facts
- **arXiv ID:** 2504.13610
- **Source URL:** https://arxiv.org/abs/2504.13610
- **Authors:** Khoa Tran; Simon S. Woo
- **Reference count:** 25
- **Primary result:** Approximated unlearning methods significantly increase fairness-gap and reduce adversarial robustness compared to exact unlearning

## Executive Summary
This study reveals a critical connection between fairness and robustness in machine unlearning that has been overlooked in prior research. Through theoretical analysis and empirical experiments on ResNet50 and SmallViT models using CIFAR-10, the authors demonstrate that existing unlearning methods (RL, SALUN, SCRUB) not only fail to preserve model fairness but also increase vulnerability to adversarial attacks. The work introduces the concept of "fairness-gap" measured through variance in normalization layers and shows that preserving this gap is essential for maintaining robustness. The authors propose that unlearning in intermediate and last layers is sufficient and cost-effective for achieving exact unlearning.

## Method Summary
The authors propose a variance-based fairness metric measured through normalization layers, defining "fairness-gap" as the difference between maximum and minimum feature variances across classes. They test this hypothesis on ResNet50 and SmallViT models trained on CIFAR-10, comparing exact unlearning (retraining from scratch) against approximated methods (fine-tuning variants like CF, RL, BS, SALUN, SCRUB). The study evaluates both fairness-gap preservation and adversarial robustness under FGSM attacks, examining how variance changes across network layers and correlating these changes with model vulnerability.

## Key Results
- Approximated unlearning methods (RL, SALUN, SCRUB) significantly increase fairness-gap compared to exact unlearning, with up to 50% accuracy drops under adversarial attacks
- Methods preserving fairness-gap (CF and BS) maintain better robustness while being computationally efficient
- Fairness-gap instability primarily occurs in intermediate and last layers, suggesting selective unlearning in these layers is sufficient and more efficient
- Strong correlation observed between higher fairness-gap and increased vulnerability to adversarial attacks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The variance of feature distributions in normalization layers serves as a proxy for model fairness ("fairness-gap"), and preserving this gap is necessary for a robust unlearning algorithm.
- **Mechanism:** Normalization layers constrain feature vectors to a unit Gaussian distribution. By measuring variance of these features per class, we define a "fairness-gap" that approximated unlearning methods disturb, causing high variance in specific classes.
- **Core assumption:** Class distributions are independent, allowing variance to serve as a valid metric for differential treatment within the model's latent space.
- **Evidence anchors:** [Section 3, Definition 1] defines fairness-gap mathematically; [Section 3, Conjecture 1] proposes robust unlearning should preserve original fairness-gap; "Bias-Aware Machine Unlearning" supports link between unlearning and bias mitigation.
- **Break condition:** If classes are not independent or normalization layers are absent, this variance-based fairness metric may not hold.

### Mechanism 2
- **Claim:** A higher fairness-gap correlates with increased vulnerability to adversarial attacks (robustness decay).
- **Mechanism:** High variance (indicated by high fairness-gap) implies the model has become sensitive to minor input fluctuations, manifesting as susceptibility to adversarial perturbations.
- **Core assumption:** Variance property measured in normalization layers directly translates to generalization sensitivity and adversarial vulnerability.
- **Evidence anchors:** [Section 3, Conjecture 2] states higher fairness-gap increases sensitivity; [Section 4.2.2] shows high fairness-gap methods suffer lower adversarial accuracy; "Group-robust Machine Unlearning" corroborates difficulty maintaining robustness during unlearning.
- **Break condition:** If adversarial attacks target structural model weights rather than input gradients, the link between feature variance and robustness might decouple.

### Mechanism 3
- **Claim:** Unlearning effectiveness and robustness are primarily determined by changes in intermediate and last layers, allowing for cost-effective selective fine-tuning.
- **Mechanism:** Fairness-gap instability and feature variance fluctuations are most prominent in deeper layers, so freezing early layers and restricting unlearning to later layers can preserve performance while reducing computational overhead.
- **Core assumption:** Critical "forget" features and robustness properties are predominantly encoded or easily modified in later stages of the network.
- **Evidence anchors:** [Section 4.2.1] notes instability appears in intermediate and last layers; [Section 6] concludes unlearning in these layers is sufficient and more efficient; "Soft Weighted Machine Unlearning" discusses efficiency via weight selection.
- **Break condition:** In architectures with heavy skip connections or where low-level features are heavily shared across classes, freezing early layers might prevent effective unlearning.

## Foundational Learning

- **Concept: Variance-Bias Trade-off**
  - **Why needed here:** This principle is the theoretical backbone for the authors' definition of "fairness-gap" (variance) and their justification for why high variance leads to overfitting and adversarial vulnerability.
  - **Quick check question:** How does increasing model variance typically affect sensitivity to noise in the input data?

- **Concept: Normalization Layers (Batch/Layer Norm)**
  - **Why needed here:** The proposed "fairness-gap" metric is calculated specifically using feature vectors output by normalization layers. Understanding how these layers standardize distributions is required to interpret the metric.
  - **Quick check question:** Does Batch Normalization normalize across the batch dimension or the feature dimension, and how does this impact the calculation of class-specific variance?

- **Concept: Approximated vs. Exact Unlearning**
  - **Why needed here:** The paper critiques "approximated" methods for failing to preserve robustness compared to "exact" methods, framing the trade-off not just as speed vs. accuracy, but speed vs. security.
  - **Quick check question:** Why is "Exact Unlearning" (retraining from scratch) often considered prohibitively expensive or impractical for large web-scale models?

## Architecture Onboarding

- **Component map:** Original Model (f_θ_D) -> Normalization Layers -> Unlearning Algorithms (A) -> Adversarial Attacker
- **Critical path:**
  1. Train base model (f_θ_D) on full dataset (CIFAR-10)
  2. Select "Forget Set" (e.g., class "trucks")
  3. Apply Unlearning Algorithm (e.g., fine-tune on Retain Set)
  4. **Evaluation Step:** Calculate Fairness-Gap (ε) on normalized features of retain set
  5. **Robustness Check:** Perform FGSM attack on unlearned model; verify if High ε correlates with Low Accuracy

- **Design tradeoffs:**
  - **Exact vs. Approximated:** Retraining guarantees fairness/robustness preservation but is computationally infeasible; approximated methods (BS, CF) are fast but risk increasing fairness-gap and vulnerability
  - **Retain vs. Forget Data:** Methods like Boundary Shrink (BS) use only forget set (efficient but risks retain accuracy), while others use both

- **Failure signatures:**
  - **High Fairness-Gap:** If variance difference between classes spikes in later layers, model is flagged as "unfair" and likely non-robust
  - **Adversarial Collapse:** If accuracy under FGSM attack drops significantly (>30%) compared to retrained baseline, unlearning algorithm has compromised robustness

- **First 3 experiments:**
  1. **Baseline Verification:** Train ResNet50 on CIFAR-10, apply "Retraining" (Exact Unlearning) to establish optimal "Robustness" and "Fairness-Gap" baselines
  2. **Algorithm Comparison:** Apply RL, SALUN, and SCRUB to same model. Plot Fairness-Gap per layer to identify which methods cause variance spikes in intermediate/last layers
  3. **Robustness Correlation:** Execute FGSM attacks on unlearned models. Plot "Fairness-Gap" vs. "Adversarial Accuracy" to verify inverse correlation proposed in Conjecture 2

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the correlation between fairness-gap and adversarial robustness hold across larger-scale datasets?
- Basis in paper: [explicit] Authors state in Limitations that "it would be more convincing to validate this on larger datasets such as Cifar100, Tiny-Imagenet, and Imagenet"
- Why unresolved: Current empirical support for conjectures is restricted to relatively small CIFAR-10 dataset
- What evidence would resolve it: Replicating experiments on ImageNet or Tiny-ImageNet to observe if high fairness-gap consistently predicts vulnerability to adversarial attacks

### Open Question 2
- Question: How do fairness and robustness dynamics manifest during unlearning in Large Language Models (LLMs) or Vision Language Models (VLMs)?
- Basis in paper: [explicit] Paper acknowledges it only focuses on classification problems, noting that "unlearning in Large Language Models (LLMs) or Vision Language Models (VLMs) is crucial" for addressing real-world issues
- Why unresolved: Proposed conjectures rely on normalization layer properties specific to ResNet and ViT architectures used in classification; unclear if they apply to generative or decoder-based architectures
- What evidence would resolve it: Adapting fairness-gap metric for generative models and evaluating robustness post-unlearning in LLMs

### Open Question 3
- Question: Can a specific unlearning algorithm be designed to explicitly minimize the fairness-gap to enhance robustness?
- Basis in paper: [explicit] Authors emphasize the "strong need to propose a robust unlearning method that mitigates the underlying fairness-gap and enhances the safety and robustness of unlearned models"
- Why unresolved: Paper analyzes existing methods (which generally fail to preserve fairness) rather than proposing new optimization technique to solve the issue
- What evidence would resolve it: A new loss function or regularization term that successfully constrains fairness-gap during unlearning process while maintaining accuracy

## Limitations
- The variance-based fairness metric assumes class independence and the presence of normalization layers, limiting generalizability to architectures without batch normalization
- Adversarial attack evaluation uses only FGSM, a relatively weak attack, limiting confidence in robustness claims under stronger attacks
- Current empirical support is restricted to CIFAR-10 dataset, with no validation on larger-scale datasets or architectures without normalization layers

## Confidence

- **High**: Core correlation between fairness-gap preservation and robustness maintenance
- **Medium**: Effectiveness of selective unlearning in intermediate/last layers
- **Low**: Generalizability to architectures without normalization layers

## Next Checks

1. **Replicate with stronger adversarial attacks** (e.g., PGD, AutoAttack) to verify robustness claims under more sophisticated threat models.

2. **Test the variance-based fairness metric** on architectures without normalization layers (e.g., vanilla MLPs, ViTs without LayerNorm) to assess metric limitations.

3. **Validate on non-independent class distributions** by constructing synthetic datasets where classes share common features to test the independence assumption underlying fairness-gap calculation.