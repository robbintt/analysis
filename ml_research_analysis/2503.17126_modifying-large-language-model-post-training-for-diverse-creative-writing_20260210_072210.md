---
ver: rpa2
title: Modifying Large Language Model Post-Training for Diverse Creative Writing
arxiv_id: '2503.17126'
source_url: https://arxiv.org/abs/2503.17126
tags:
- diversity
- prompt
- quality
- instances
- while
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work tackles the challenge of diversifying LLM creative writing
  outputs while maintaining quality. Current post-training methods often boost quality
  but suppress diversity, leading to homogenized outputs that limit creative expression.
---

# Modifying Large Language Model Post-Training for Diverse Creative Writing

## Quick Facts
- arXiv ID: 2503.17126
- Source URL: https://arxiv.org/abs/2503.17126
- Reference count: 40
- Key outcome: Deviation-aware extensions to DPO and ORPO improve LLM creative writing diversity while maintaining quality

## Executive Summary
This work addresses the challenge of balancing quality and diversity in LLM creative writing post-training. Standard approaches like DPO improve quality but suppress diversity, leading to homogenized outputs. The authors introduce deviation-aware extensions (DDPO, DORPO) that weight preference loss by the mean pairwise distance between responses to the same prompt. Experiments on r/writingPrompts show DDPO achieves diversity scores approaching human-written data while maintaining quality comparable to GPT-4o, with human evaluation confirming superior diversity and quality over both GPT-4o and DPO.

## Method Summary
The approach modifies standard DPO/ORPO by incorporating deviation - the mean pairwise cosine distance between responses to the same prompt in embedding space. Each training pair's loss is scaled by the winning response's deviation score, encouraging the model to learn from rare high-quality examples. Semantic and style embeddings provide complementary diversity signals. When responses are sparse (≤4 per prompt), a minimum deviation threshold prevents quality collapse. The method is evaluated on Llama-3.1-8B and Mistral-7B-v0.3 trained on r/writingPrompts preference data.

## Key Results
- DDPO-both achieves semantic and style diversity approaching human-written data while maintaining quality comparable to GPT-4o
- Human evaluation confirms DDPO outputs are both higher quality and more diverse than GPT-4o and standard DPO
- Ablation studies show robustness across dataset sizes and superiority over DivPO
- Minimum deviation threshold successfully mitigates quality degradation with sparse data

## Why This Works (Mechanism)

### Mechanism 1
Weighting preference loss by deviation encourages learning from rare high-quality examples. Each (prompt, winner, loser) training pair's loss is scaled by δ_w, the mean pairwise distance between the winning response and other responses to the same prompt. This upweights training on atypical outputs, pushing the model to generate more diverse valid responses. Core assumption: the dataset contains multiple responses per prompt, and "deviation" meaningfully captures diversity-relevant variation.

### Mechanism 2
Semantic and style embeddings provide complementary diversity signals. Deviation is computed as mean cosine distance in embedding space. Jina-embeddings-v3 captures semantic content; Style-Embedding captures authorship-like features. Combining them (δ_sem^0.5 × δ_style^0.5) targets both diversity types simultaneously. Core assumption: embedding distances meaningfully reflect human-perceived diversity in creative writing.

### Mechanism 3
Enforcing a minimum deviation threshold mitigates quality collapse when data is sparse. When δ_w would be zero (common with few samples per prompt), replacing it with a floor value (0.1) prevents the training signal from being zeroed out entirely. Core assumption: quality degradation with sparse data stems from zero-weighted loss terms.

## Foundational Learning

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: DDPO is a deviation-weighted extension of DPO; understanding the base objective is prerequisite.
  - Quick check question: Can you explain why DPO avoids training a separate reward model?

- Concept: Pairwise distance in embedding space
  - Why needed here: Deviation is computed as mean cosine distance between embeddings; intuition about embedding geometry is essential.
  - Quick check question: Given four text embeddings, how would you compute their mean pairwise cosine distance?

- Concept: Preference data construction from noisy signals
  - Why needed here: The paper converts Reddit upvote scores to preference pairs with careful filtering.
  - Quick check question: Why might raw upvote scores be unreliable for constructing preference pairs?

## Architecture Onboarding

- Component map:
  1. Dataset preprocessing → preference pairs with upvote-derived winners/losers
  2. Embedding computation → per-response deviation scores (semantic + style)
  3. Deviation normalization → min-zero, sum-equals-count per prompt
  4. Modified DPO/ORPO training → deviation-weighted loss
  5. Evaluation → reward model + embedding-based diversity metrics

- Critical path: Deviation calculation is the key step—if embeddings fail or normalization is wrong, downstream training is meaningless.

- Design tradeoffs:
  - Semantic-only vs. style-only vs. combined deviation—trades off which diversity dimension is prioritized
  - Minimum δ threshold vs. quality-filtered data—different approaches to handling sparse-per-prompt data
  - LoRA rank 128/alpha 256 trades off expressiveness vs. memory

- Failure signatures:
  - Quality drops below baseline DPO → check if average responses per prompt < 6; consider minimum δ or quality filtering
  - Diversity doesn't improve → verify embedding models are correctly loaded; check deviation distribution isn't collapsed
  - Training unstable → check for NaN in deviation-weighted loss; ensure normalization prevents division by zero

- First 3 experiments:
  1. Replicate DDPO-sem vs. DPO comparison on a small validation split to verify deviation weighting is implemented correctly (should see semantic diversity increase).
  2. Ablate minimum δ threshold (0.0, 0.05, 0.1, 0.2) on a subset with ≤6 responses per prompt to find quality-diversity sweet spot.
  3. Test transfer: train on r/writingPrompts, evaluate on an out-of-distribution creative writing task to assess generalization.

## Open Questions the Paper Calls Out

### Open Question 1
Can deviation-aware training approaches be adapted to online RLHF settings (e.g., online PPO) where models are updated dynamically during training? The current work only validates DDPO/DORPO in offline settings; online training has different dynamics where deviation would need to be recalculated continuously.

### Open Question 2
Do deviation-aware approaches transfer to non-creative-writing tasks where multiple valid outputs exist (e.g., code generation, reasoning, dialogue)? The paper doesn't test whether the method works equally well in tasks with more structured solution spaces.

### Open Question 3
How can deviation be incorporated into training methods that use numerical rewards rather than preference pairs (e.g., reinforcement learning with scalar reward models)? Current formulations weight loss terms by deviation of the winning response; numerical reward settings lack this binary structure.

### Open Question 4
What is the minimum number of responses per prompt needed to reliably estimate deviation without quality degradation, and can this threshold be lowered through better deviation estimators? The paper identifies quality drops with sparse data but doesn't fully solve the fundamental limitation of deviation estimation.

## Limitations

- Evaluation relies entirely on r/writingPrompts dataset, limiting generalizability to other creative writing domains
- Choice of jina-embeddings-v3 and Style-Embedding as diversity proxies isn't fully validated
- Method demonstrated only on Llama-3.1-8B and Mistral-7B-v0.3, not tested on larger models

## Confidence

**High confidence**: The core algorithmic contribution is technically sound and the implementation appears correct. Quality maintenance on datasets with sufficient responses per prompt is well-established.

**Medium confidence**: Superiority over DivPO and effectiveness of minimum δ threshold are demonstrated but rest on specific dataset conditions. Human evaluation involves only 300 samples.

**Medium confidence**: Combined semantic+style approach shows benefits, but relative importance of each dimension isn't thoroughly explored.

## Next Checks

1. **Domain transfer experiment**: Apply the trained DDPO-both model to an out-of-domain creative writing task and measure whether diversity improvements transfer.

2. **Embedding ablation study**: Systematically replace jina-embeddings-v3 and Style-Embedding with alternative semantic and style embeddings to determine sensitivity to embedding choice.

3. **Quality-diversity Pareto analysis**: Systematically vary the minimum δ threshold (0.0 to 0.3) and plot quality vs. diversity trade-offs to provide clearer guidance on application.