---
ver: rpa2
title: Language Models Can Learn from Verbal Feedback Without Scalar Rewards
arxiv_id: '2509.22638'
source_url: https://arxiv.org/abs/2509.22638
tags:
- feedback
- arxiv
- training
- learning
- scalar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel approach to training language models
  directly from verbal feedback without converting it to scalar rewards. The core
  idea is to treat verbal feedback as a conditioning signal, learning a feedback-conditional
  policy that approximates the posterior distribution over responses given feedback.
---

# Language Models Can Learn from Verbal Feedback Without Scalar Rewards

## Quick Facts
- arXiv ID: 2509.22638
- Source URL: https://arxiv.org/abs/2509.22638
- Reference count: 33
- Key outcome: Proposes training language models from verbal feedback as conditioning signals, matching or exceeding scalar-based RLHF baselines on mathematical and reasoning tasks without verifiers or data filtering.

## Executive Summary
This paper introduces Feedback-Conditional Policy (FCP), a method for training language models directly from verbal feedback without converting it to scalar rewards. Instead of optimizing reward functions, FCP treats feedback as a conditioning signal and learns a feedback-conditional posterior distribution through maximum likelihood training on response-feedback pairs. The approach leverages the compositional nature of language to generalize to positive feedback conditions, and includes an online bootstrapping stage where the model generates under positive constraints and refines itself with fresh feedback. Experiments show FCP matches or exceeds strong scalar-reward baselines like offline RFT and online GRPO on mathematical reasoning and general reasoning benchmarks.

## Method Summary
FCP trains a feedback-conditional policy π_θ(o|x,c) that maps inputs and feedback to responses, rather than optimizing scalar rewards. The method uses two stages: offline training where responses are generated from a reference policy and paired with feedback, then trained via maximum likelihood estimation with feedback as conditioning; and online bootstrapping where the model generates under positive feedback conditions and refines itself with fresh feedback. The feedback is prepended to the input using special wrapper tokens, and training uses cross-entropy loss. The approach relies on the compositional generalization of language priors to handle positive feedback conditions not seen in training.

## Key Results
- FCP matches or exceeds scalar-reward baselines (offline RFT and online GRPO) on mathematical reasoning tasks (AIME24/25, MATH500, OlympiadBench) and general reasoning benchmarks (GPQA-Diamond, MMLU-Pro, TheoremQA)
- The method achieves this without requiring verifiers, data filtering, or scalar reward computation
- FCP demonstrates improved controllability, with significant accuracy differences between contradictory feedback conditions (68.5% vs 17.1% in Table 3)
- Performance remains stable with noisy user-style feedback, showing only ~2.5% drop compared to professional feedback

## Why This Works (Mechanism)

### Mechanism 1: Feedback-Conditional Posterior Approximation
FCP works by training the model to approximate the posterior distribution of responses given specific feedback conditions, rather than optimizing a scalar reward function. The policy π_θ(o|x,c) is trained via Maximum Likelihood Estimation (MLE) on offline pairs (x, o, c) collected from a reference policy and an environment. This minimizes the forward KL divergence between the policy and the target posterior P_off(o|x,c), effectively "re-weighting" the reference policy based on the likelihood of the feedback.

### Mechanism 2: Compositional Generalization via Language Priors
FCP leverages the pre-existing language priors of LLMs to generalize to unseen positive feedback conditions (c^+) by composing features learned from mixed feedback examples. Analogous to text-to-image models generating "a banana surfing" from separate concepts, the model learns to map distinct verbal attributes (e.g., "correct," "verbose," "efficient") to response behaviors. It can then combine these attributes at inference time to generate high-quality responses for purely positive prompts (c^+) even if such perfect pairs were rare in training.

### Mechanism 3: Self-Refinement via Positive-Condition Bootstrapping
Online bootstrapping improves performance by forcing the model to generate under positive constraints and correcting deviations with fresh feedback. The model generates samples conditioned on a target positive feedback c^+. These samples are then critiqued by the environment (p_env), producing actual feedback c. Training on the pair (o, c) reinforces the link between the generated behavior and the resulting critique, iteratively aligning the model's "positive" generation mode with the environment's actual standards.

## Foundational Learning

- **Conditional Language Modeling**: Why needed: This is the core architecture shift. Instead of modeling P(o|x), you must understand modeling P(o|x, c) where c is the feedback text used as a conditioning prompt. Quick check: How does concatenating feedback to the prompt change the model's output distribution compared to standard prompting?

- **Forward vs. Reverse KL Divergence**: Why needed: The paper explicitly chooses Forward KL (MLE) over Reverse KL (common in RLHF/PPO) to avoid the intractability of computing log-probabilities in the feedback environment. Understanding this distinction explains why FCP is "offline-first." Quick check: Why does Forward KL divergence enable Maximum Likelihood Estimation (MLE) while Reverse KL typically requires reinforcement learning?

- **The Reward Hypothesis**: Why needed: The paper challenges Sutton's hypothesis that goals must be scalar signals. You need to understand the baseline RL paradigm (scalar rewards) to grasp why treating feedback as "verbal conditioning" is a fundamental shift. Quick check: What specific limitation of scalar rewards (e.g., information loss, ambiguity) does verbal feedback preserve?

## Architecture Onboarding

- **Component map**: Reference Policy (π_ref) -> Feedback Environment (p_env) -> Buffer -> Policy (π_θ)
- **Critical path**:
  1. Offline Phase: Generate responses with π_ref → Get feedback c from p_env → Train π_θ on (x, o, c) via MLE
  2. Online Phase: Sample condition c^+ → Generate response o using π_θ(·|x, c^+) → Get fresh feedback c_new → Train π_θ on (x, o, c_new)

- **Design tradeoffs**:
  - MLE vs. RL: FCP uses MLE (stable, simple) but relies on the quality of the behavioral data. It avoids complex RL value functions but requires the model to "invent" better responses via its prior rather than explicit gradient ascent on reward.
  - Feedback Style: Real-world user feedback is noisy/cheap; Professional feedback is high-quality/expensive. The paper suggests FCP can work with noisy feedback but performs better with professional reviews.

- **Failure signatures**:
  - Length Collapse: If "conciseness" is a feedback condition, the model may iteratively shorten responses until they are empty or useless.
  - Mode Collapse: If conditioning on c^+ leads to repetitive outputs that stylistically match the condition but lack semantic diversity.

- **First 3 experiments**:
  1. Verify Conditional Control: Train offline FCP and evaluate on test sets with contradictory conditions (e.g., "fully_positive" vs. "fully_negative"). Confirm accuracy diverges significantly (e.g., Table 3 results: 68.5% vs 17.1%).
  2. Ablate Feedback Source: Compare training with "Real-World User" style feedback vs. "Professional Reviewer" style feedback to measure robustness to noise (expect ~2.5% drop as per Table 5).
  3. Check Bootstrapping Stability: Run online bootstrapping with and without "conciseness" related conditions to reproduce the length collapse phenomenon (Figure 3) and verify that filtering these conditions stabilizes training.

## Open Questions the Paper Calls Out

### Open Question 1
Can combining FCP with verifiable scalar rewards (e.g., by treating absent feedback as a neutral condition) improve performance beyond either method alone? The authors explicitly propose combining it with verifiable rewards, but empirical comparisons of hybrid models remain unexplored.

### Open Question 2
How can FCP be extended to multi-turn interactive settings where feedback arrives incrementally across conversation turns? The authors list extending FCP to multi-turn interactions as a future direction, but the current formulation assumes single-turn response-feedback pairs.

### Open Question 3
What mechanisms can mitigate the instability caused by length-related feedback conditions during FCP bootstrapping? The paper documents that length-related conditions cause response length to shrink over time while loss becomes unstable, but does not propose a principled solution beyond filtering.

## Limitations
- Performance relies heavily on availability of high-quality offline response-feedback pairs and reliable feedback environments, which may be costly to obtain
- Bootstrapping phase introduces potential instability when conditioning on positive feedback that is not well-defined or when feedback environment provides inconsistent signals
- Experiments are conducted on specific benchmarks and may not generalize to all types of tasks or feedback styles

## Confidence

- **High**: The core mechanism of using verbal feedback as a conditioning signal is well-supported by experimental results and aligns with established conditional language modeling principles
- **Medium**: The compositional generalization claim is supported by ablation studies but requires further validation on more diverse feedback types and tasks
- **Medium**: The self-refinement via bootstrapping shows promise but is sensitive to feedback quality and conditioning stability, as evidenced by the length collapse phenomenon

## Next Checks

1. **Generalization to Diverse Feedback**: Test FCP on tasks with highly non-compositional or ambiguous feedback to evaluate its robustness beyond the current benchmarks
2. **Feedback Environment Consistency**: Conduct controlled experiments to measure the impact of inconsistent or deceptive feedback on the stability and performance of the online bootstrapping phase
3. **Scalability to Larger Models**: Validate whether the proposed method scales effectively to larger language models (e.g., 70B+ parameters) and whether the feedback-conditional policy remains efficient and effective at scale