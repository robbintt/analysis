---
ver: rpa2
title: Solving Situation Puzzles with Large Language Model and External Reformulation
arxiv_id: '2503.18394'
source_url: https://arxiv.org/abs/2503.18394
tags:
- llms
- player
- situation
- questions
- guess
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of using large language models
  (LLMs) to solve situation puzzles that require multi-turn dialogue and creative
  reasoning. LLMs often struggle in these scenarios, asking overly detailed or repetitive
  questions that hinder progress.
---

# Solving Situation Puzzles with Large Language Model and External Reformulation

## Quick Facts
- arXiv ID: 2503.18394
- Source URL: https://arxiv.org/abs/2503.18394
- Reference count: 24
- Primary result: 4/1 win/lose ratio vs 2/3 baseline with fewer questions and guesses

## Executive Summary
This study addresses the challenge of using large language models (LLMs) to solve situation puzzles through multi-turn dialogue. LLMs often struggle by asking overly detailed or repetitive questions that hinder progress. The authors propose an external reformulation methodology where puzzle descriptions are updated with selected Q&A interactions after several rounds of dialogue or an incorrect guess. This reformulated description is then used to restart a new chat session. Experiments on five situation puzzles show that this approach significantly improves performance compared to using LLMs directly, achieving a higher win rate with fewer questions and guesses.

## Method Summary
The method involves playing situation puzzles with an LLM as the player and a human as the host, using Yes/No/Irrelevant responses. When the conversation reaches 5 questions or an incorrect guess, the system reformulates the puzzle by selecting top Q&A pairs (prioritizing Yes answers) and converting them into declarative hints. These hints are appended to the original puzzle description, and a new chat session begins with this reformulated description. This process iterates until the puzzle is solved or the game ends. The approach is compared against a baseline of continuous dialogue without reformulation.

## Key Results
- Win/lose ratio improved from 2/3 to 4/1
- Questions reduced from 28.6 to 22.6 per game
- Guesses reduced from 2.8 to 2.4 per game
- Yes-questions increased from 3.6 to 5.0
- No/Irrelevant questions decreased from 18.2/6.8 to 12.2/5.4

## Why This Works (Mechanism)

### Mechanism 1: Context Consolidation via Session Reset
The method resets chat sessions with consolidated hints to improve reasoning efficiency by reducing attention degradation in long dialogues. Instead of maintaining extensive dialogue history, key Q&A pairs are extracted and reformulated as explicit hints in a fresh problem statement. This creates a clean attentional slate where accumulated discoveries are presented as premises rather than requiring retrieval from long-context history. The core assumption is that LLMs exhibit attention degradation or fixation patterns in extended multi-turn dialogues, causing them to focus on narrow details or repeat questions.

### Mechanism 2: Priority-Based Information Filtering
The method selects only high-value Q&A pairs (prioritizing "Yes" answers) to improve solution trajectory by focusing on confirmed relevant information. Questions are ranked by answer type ("Yes" > "No" > "Irrelevant") and only top-M pairs are selected for reformulation. This filters noise and presents the LLM with the most constraint-satisfying information as explicit hints. The core assumption is that "Yes" answers provide stronger constraint signals toward the solution than negative or irrelevant information.

### Mechanism 3: Interrupting Fixation Loops
Forcing session restarts after K questions or incorrect guesses breaks potential fixation loops where the LLM gets stuck asking similar questions or focusing on irrelevant details. By resetting the context with reformulated hints, the model is forced to reconsider the problem from a broader perspective rather than continuing down a potentially unproductive line of questioning.

## Foundational Learning

### Session Reset for Context Management
**Why needed**: LLMs exhibit attention degradation in long dialogues, causing fixation on narrow details or repetitive questioning.
**Quick check**: Compare Yes/No/Irrelevant question ratios in baseline vs reformulation after 10+ rounds.

### Selective Information Distillation
**Why needed**: Long dialogue histories contain noise; filtering to high-value Q&A pairs focuses reasoning on confirmed constraints.
**Quick check**: Verify that reformulation sessions have higher Yes-question ratios than baseline sessions.

### Hint Generation from Q&A
**Why needed**: Converting Q&A pairs to declarative hints provides the model with explicit premises rather than implicit context.
**Quick check**: Ensure generated hints are accurate, concise, and capture essential information from selected Q&A pairs.

## Architecture Onboarding

### Component Map
Puzzle Description → Chat Session → Host Responses → Question/Guess Tracking → Reformulation Trigger (K=5 questions OR wrong guess) → Q&A Selection (M=3, Yes>priority) → Hint Generation → Reformulated Description → New Chat Session

### Critical Path
Puzzle → Continuous Dialogue (baseline) OR Reformulation Loop (experimental) → Win/Lose → Metrics

### Design Tradeoffs
- **Session reset vs continuous dialogue**: Reset provides clean context but loses conversational flow; continuous maintains flow but risks attention degradation
- **Selective vs complete Q&A preservation**: Selective filtering reduces noise but may lose valuable negative constraints
- **Hint generation vs direct Q&A inclusion**: Hints are more declarative and concise but require additional LLM processing

### Failure Signatures
- LLM asks redundant questions after 10+ rounds (baseline failure)
- Reformulated hints contain inaccuracies or omit crucial information
- No improvement in win rate despite fewer questions/guesses
- Reformulation triggers too frequently, creating excessive session overhead

### First 3 Experiments
1. Implement baseline continuous dialogue on 3 puzzles, measure win rate, questions, and guess metrics
2. Implement reformulation with K=5 questions trigger, measure same metrics and compare to baseline
3. Test different values of M (1-5) to find optimal number of Q&A pairs to preserve

## Open Questions the Paper Calls Out

### Open Question 1
Does the external reformulation methodology improve performance in complex, multi-step cognitive tasks other than situation puzzles? The conclusion suggests potential application for enhancing reasoning capabilities in complex, multi-step cognitive tasks, but the experiments were restricted to lateral thinking puzzles. This remains untested for other interactive reasoning domains.

### Open Question 2
Is the reported performance improvement statistically significant when evaluated on a larger dataset of situation puzzles? The study reports a win/lose ratio of 4/1 compared to 2/3, but this result is derived from experiments on only five distinct puzzles, which is insufficient to establish statistical significance.

### Open Question 3
How does the reformulation strategy perform when applied to open-source Large Language Models (LLMs) with different parameter scales or architectures? The experiments were conducted exclusively using ChatGPT without specifying the version or testing on other models, leaving unclear whether the benefits are universal or ChatGPT-specific.

## Limitations
- Sample size of only 5 puzzles limits generalizability across puzzle types
- Priority-based filtering heuristic (Yes > No > Irrelevant) lacks corpus-backed justification
- Exact prompt engineering details for hint generation are underspecified
- Comparison may be confounded by additional LLM queries and context resets

## Confidence
- **High**: Core mechanism of session reset and reformulation is theoretically sound
- **Medium**: Substantial win-rate improvement reported, but limited to 5 puzzles
- **Low**: Potential confounding factors not adequately addressed (additional LLM queries, context resets)

## Next Checks
1. **Generalization Test**: Apply the method to a larger dataset of 50+ puzzles spanning different categories to verify that the 2x win rate improvement holds across diverse puzzle types.

2. **Ablation Study**: Compare three variants: (a) reformulation with all Q&A pairs preserved, (b) reformulation with selective M=3 pairs, and (c) baseline continuous dialogue. This would isolate whether the improvement comes from session reset, selective filtering, or both.

3. **Negative Constraint Analysis**: Design puzzles where "No" answers provide critical information, then test whether the Yes-priority scheme remains optimal or if a weighted scoring system would perform better.