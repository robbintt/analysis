---
ver: rpa2
title: 'BASIC: Boosting Visual Alignment with Intrinsic Refined Embeddings in Multimodal
  Large Language Models'
arxiv_id: '2508.06895'
source_url: https://arxiv.org/abs/2508.06895
tags:
- visual
- embeddings
- vision
- arxiv
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the limited visual alignment in Multimodal\
  \ Large Language Models (MLLMs), where visual embeddings are treated as contextual\
  \ cues without direct supervision. The authors propose BASIC, a method that leverages\
  \ refined visual embeddings from the LLM\u2019s shallow layers as supervision to\
  \ guide initial visual embeddings from the vision projector."
---

# BASIC: Boosting Visual Alignment with Intrinsic Refined Embeddings in Multimodal Large Language Models

## Quick Facts
- **arXiv ID**: 2508.06895
- **Source URL**: https://arxiv.org/abs/2508.06895
- **Authors**: Jianting Tang; Yubo Wang; Haoyu Cao; Linli Xu
- **Reference count**: 40
- **Primary result**: Improved visual alignment in MLLMs using intrinsic refined embeddings from LLM shallow layers as supervision, boosting performance on VQAv2 and MMBench-CN benchmarks without additional annotations.

## Executive Summary
This paper addresses the challenge of limited visual alignment in Multimodal Large Language Models (MLLMs), where visual embeddings from vision encoders are treated as contextual cues without direct supervision. The authors propose BASIC (Boosting visual Alignment with intrinsic refined visual embeddings), a method that leverages refined visual embeddings from the LLM's shallow layers as supervision to guide initial visual embeddings from the vision projector. This guidance is applied through directional alignment (minimizing angular distances) and semantic distribution matching (minimizing KL divergence). Without additional supervisory models or annotations, BASIC demonstrates improved performance across multiple benchmarks, enhancing the semantic quality of initial visual embeddings.

## Method Summary
BASIC addresses the problem of limited visual alignment in MLLMs by using intrinsic refined embeddings from the LLM's shallow layers as supervision to guide initial visual embeddings. The method consists of two main alignment objectives: directional alignment, which minimizes the angular distance between initial and refined embeddings, and semantic distribution matching, which minimizes the KL divergence between their semantic distributions. The approach works by first generating initial visual embeddings through a vision projector, then processing these through the LLM to obtain refined embeddings. The alignment objectives are then applied to minimize the difference between these embedding spaces, creating a self-supervised learning framework that requires no additional annotations or supervisory models.

## Key Results
- On VQAv2 benchmark: BASIC increases accuracy from 78.5 to 79.2 for 7B model and from 80.0 to 80.6 for 13B model
- On MMBench-CN benchmark: BASIC increases accuracy from 58.3 to 62.1 for 7B model and from 63.6 to 64.9 for 13B model
- Demonstrates improved semantic quality of initial visual embeddings across multiple model configurations

## Why This Works (Mechanism)
The method works by recognizing that LLM shallow layers produce refined visual embeddings that capture better semantic representations than initial projections. By using these refined embeddings as intrinsic supervision, the vision projector can learn to generate embeddings that are already more aligned with the LLM's processing space. The directional alignment ensures embeddings are positioned correctly in the semantic space, while KL divergence matching ensures their semantic distributions are consistent. This creates a self-reinforcing training loop where the vision encoder learns to produce embeddings that require less adaptation by the LLM, improving overall model efficiency and performance.

## Foundational Learning
**Vision-Language Model Architecture**: Understanding how vision encoders and language models are integrated in MLLMs is crucial, as BASIC specifically targets the alignment between visual embeddings and LLM processing spaces.
*Why needed*: Provides context for where and how BASIC intervenes in the MLLM pipeline.
*Quick check*: Verify understanding of how visual features are typically projected into the LLM's embedding space.

**Self-Supervised Learning**: BASIC creates a self-supervised learning framework using intrinsic model outputs as supervision.
*Why needed*: The core innovation relies on using refined embeddings as supervision without external annotations.
*Quick check*: Confirm understanding of how intrinsic supervision differs from traditional supervised learning approaches.

**KL Divergence and Directional Alignment**: The method employs both KL divergence minimization and angular distance minimization for alignment.
*Why needed*: These mathematical tools are the core mechanisms by which BASIC achieves alignment between embedding spaces.
*Quick check*: Verify understanding of when to use KL divergence versus directional alignment in embedding space optimization.

## Architecture Onboarding

**Component Map**: Vision Encoder -> Vision Projector -> LLM Shallow Layers (Refined Embeddings) -> BASIC Alignment Objectives (Directional + KL) -> Updated Vision Projector

**Critical Path**: The critical path involves generating initial visual embeddings, processing them through the LLM to obtain refined embeddings, and then applying the alignment objectives to update the vision projector parameters.

**Design Tradeoffs**: BASIC trades additional computational overhead during training (multiple forward passes) for improved visual alignment without requiring external annotations or supervisory models. The method prioritizes semantic quality over inference efficiency.

**Failure Signatures**: Potential failures could include misalignment between the vision projector and LLM processing spaces, inadequate semantic preservation during refinement, or instability in the self-supervised learning loop if the refined embeddings are not sufficiently informative.

**First Experiments**:
1. Verify that BASIC improves visual embedding quality on a simple semantic similarity task before full benchmark evaluation
2. Test the directional alignment component alone versus the full BASIC approach to isolate its contribution
3. Evaluate the method's sensitivity to the relative weighting of directional versus KL divergence objectives

## Open Questions the Paper Calls Out
None

## Limitations
- Limited empirical validation scope, with effectiveness on other important multimodal tasks untested
- Architectural specificity that may not generalize to all MLLM architectures
- Potential computational overhead during inference due to multiple forward passes, though not quantified
- Limited theoretical grounding for why intrinsic embeddings serve as effective supervision

## Confidence

**High confidence**: The core claim that BASIC improves visual alignment metrics on tested benchmarks is well-supported by the presented results (VQAv2 and MMBench-CN improvements across both 7B and 13B models).

**Medium confidence**: The claim that BASIC improves the semantic quality of initial visual embeddings is supported by benchmark results but would benefit from more direct semantic quality evaluations.

**Medium confidence**: The claim of broad applicability across model configurations is suggested by results on two model sizes but would require testing on diverse architectural variants to be fully substantiated.

## Next Checks

1. **Cross-task generalization test**: Evaluate BASIC on a broader suite of multimodal benchmarks including image captioning datasets (COCO, Flickr30k), visual reasoning tasks (GQA, NLVR2), and zero-shot multimodal transfer tasks to establish comprehensive performance improvements.

2. **Architectural robustness evaluation**: Test BASIC on different MLLM architectures (e.g., models without explicit shallow layer embeddings, different vision encoder configurations) to validate the claimed broad applicability and identify architectural constraints.

3. **Ablation study of alignment objectives**: Conduct detailed ablation studies isolating the contributions of directional alignment versus semantic distribution matching, including intermediate analyses of embedding space geometry and semantic preservation metrics to better understand the mechanism of improvement.