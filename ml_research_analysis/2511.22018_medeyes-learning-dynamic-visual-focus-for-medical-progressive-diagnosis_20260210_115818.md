---
ver: rpa2
title: 'MedEyes: Learning Dynamic Visual Focus for Medical Progressive Diagnosis'
arxiv_id: '2511.22018'
source_url: https://arxiv.org/abs/2511.22018
tags:
- reasoning
- visual
- medical
- expert
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MedEyes introduces a hybrid reinforcement learning framework that
  addresses the challenge of enabling medical vision models to perform progressive
  visual focusing and iterative diagnostic reasoning similar to expert clinicians.
  The core innovation lies in combining off-policy expert trajectories generated through
  a Gaze-guided Reasoning Navigator (GRN) with on-policy exploration, using a dual-stream
  Group Relative Policy Optimization (GRPO) architecture.
---

# MedEyes: Learning Dynamic Visual Focus for Medical Progressive Diagnosis

## Quick Facts
- arXiv ID: 2511.22018
- Source URL: https://arxiv.org/abs/2511.22018
- Authors: Chunzheng Zhu; Yangfang Lin; Shen Chen; Yijun Wang; Jianxin Lin
- Reference count: 5
- Primary result: Hybrid RL framework for progressive medical diagnosis with +8.5% accuracy improvement over existing methods

## Executive Summary
MedEyes introduces a novel hybrid reinforcement learning framework that enables medical vision models to perform progressive visual focusing and iterative diagnostic reasoning similar to expert clinicians. The approach combines off-policy expert trajectories generated through a Gaze-guided Reasoning Navigator (GRN) with on-policy exploration, using a dual-stream Group Relative Policy Optimization (GRPO) architecture. This framework addresses the challenge of enabling medical models to dynamically adjust their visual attention and reasoning steps based on both initial observations and intermediate conclusions, while maintaining interpretability through structured visual-textual grounding.

## Method Summary
The method employs a hybrid reinforcement learning approach that integrates expert-guided trajectories with autonomous exploration. The Gaze-guided Reasoning Navigator (GRN) generates off-policy expert trajectories that guide the model's initial behavior, while nucleus sampling and adaptive termination mechanisms enable on-policy exploration. The dual-stream GRPO architecture decouples on-policy and off-policy learning signals to prevent reward collapse and ensure stable training. This allows the model to learn progressive visual focusing - dynamically adjusting attention based on intermediate diagnostic reasoning - while maintaining interpretability through explicit reasoning paths that ground visual observations to diagnostic conclusions.

## Key Results
- Achieves state-of-the-art performance across five medical visual question answering benchmarks
- Demonstrates +8.5% average accuracy improvement over existing methods
- Provides interpretable diagnostic reasoning paths through structured visual-textual grounding

## Why This Works (Mechanism)
The framework works by combining the stability of expert-guided learning with the adaptability of autonomous exploration. The off-policy expert trajectories from GRN provide reliable initial guidance based on domain expertise, while the on-policy components with nucleus sampling allow the model to discover novel diagnostic strategies. The decoupled GRPO architecture prevents the common reinforcement learning problem of reward collapse by maintaining separate learning signals for expert imitation and autonomous discovery. This hybrid approach enables the model to develop progressive visual focusing capabilities that mimic expert clinician behavior while remaining flexible enough to adapt to new diagnostic scenarios.

## Foundational Learning

### Reinforcement Learning with Expert Trajectories
- Why needed: Enables learning from both expert demonstrations and autonomous exploration
- Quick check: Verify that off-policy expert trajectories are properly integrated with on-policy learning

### Progressive Visual Focusing
- Why needed: Mimics how clinicians iteratively refine their diagnostic attention based on intermediate findings
- Quick check: Confirm that visual attention maps change meaningfully across reasoning steps

### Dual-Stream Policy Optimization
- Why needed: Prevents reward collapse by maintaining separate learning signals for different training modes
- Quick check: Ensure that on-policy and off-policy gradients don't interfere destructively

## Architecture Onboarding

### Component Map
GRN (Gaze-guided Reasoning Navigator) -> Off-policy trajectory generation -> Dual-stream GRPO -> Progressive visual focusing -> Diagnostic reasoning output

### Critical Path
The critical path involves: 1) Initial image observation and question encoding, 2) GRN-guided off-policy trajectory generation, 3) Dual-stream GRPO optimization, 4) Progressive visual focusing across reasoning steps, 5) Final diagnostic reasoning output with visual-textual grounding.

### Design Tradeoffs
The framework trades computational complexity for improved diagnostic accuracy and interpretability. The dual-stream architecture increases training overhead but provides better stability and prevents reward collapse. The combination of expert trajectories and autonomous exploration balances reliability with adaptability, though it requires careful tuning of the nucleus sampling and adaptive termination parameters.

### Failure Signatures
Potential failure modes include: 1) Over-reliance on expert trajectories limiting novel discovery, 2) Unstable training if on-policy and off-policy signals interfere, 3) Inadequate progressive focusing leading to superficial reasoning, 4) Poor generalization if the model memorizes expert patterns rather than learning underlying diagnostic principles.

### 3 First Experiments
1. Ablation study removing nucleus sampling to quantify its impact on exploration and performance
2. Comparison of visual attention maps across reasoning steps to verify progressive focusing behavior
3. Evaluation of model stability across different GRPO hyperparameter settings to identify optimal configurations

## Open Questions the Paper Calls Out

None specified in the provided content.

## Limitations
- Lacks comparison against established clinical reasoning baselines or human expert performance
- Limited methodological detail on parameter selection for nucleus sampling and adaptive termination
- No systematic evaluation of interpretability claims through human studies or automated metrics

## Confidence

**High confidence**: The hybrid reinforcement learning framework combining off-policy expert trajectories with on-policy exploration is technically sound and well-motivated

**Medium confidence**: The reported performance improvements (+8.5% average accuracy) are likely valid within the evaluated benchmarks, but clinical significance remains unclear

**Medium confidence**: The dual-stream GRPO architecture with decoupled learning signals appears implementable, though training stability claims need verification

## Next Checks

1. Conduct ablation studies isolating the contributions of nucleus sampling, adaptive termination, and decoupled learning signals to verify their individual impact on performance

2. Evaluate model outputs with clinical experts to assess whether the interpretable reasoning paths align with actual diagnostic decision-making processes

3. Test model generalization by evaluating on out-of-distribution medical imaging cases and comparing against human expert performance on the same cases