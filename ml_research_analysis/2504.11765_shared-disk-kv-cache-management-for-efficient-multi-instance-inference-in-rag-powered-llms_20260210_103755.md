---
ver: rpa2
title: Shared Disk KV Cache Management for Efficient Multi-Instance Inference in RAG-Powered
  LLMs
arxiv_id: '2504.11765'
source_url: https://arxiv.org/abs/2504.11765
tags:
- cache
- query
- rag-dcache
- documents
- caches
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the growing inference latency in large language
  models (LLMs) caused by longer input contexts in retrieval-augmented generation
  (RAG). To reduce time-to-first-token (TTFT) and improve throughput, the authors
  propose a disk-based key-value (KV) cache management system called Shared RAG-DCache.
---

# Shared Disk KV Cache Management for Efficient Multi-Instance Inference in RAG-Powered LLMs

## Quick Facts
- **arXiv ID**: 2504.11765
- **Source URL**: https://arxiv.org/abs/2504.11765
- **Reference count**: 27
- **Key outcome**: Throughput improvements of 15-71% and latency reductions of 12-65% in multi-instance RAG-powered LLM inference using disk-based KV cache sharing and proactive prefetching

## Executive Summary
This paper addresses the growing inference latency in large language models (LLMs) caused by longer input contexts in retrieval-augmented generation (RAG). To reduce time-to-first-token (TTFT) and improve throughput, the authors propose a disk-based key-value (KV) cache management system called Shared RAG-DCache. The system precomputes and stores KV caches for frequently retrieved documents on disk, enabling reuse across multiple LLM instances. A shared KV cache manager and a proactive cache generator prefetch caches during query wait times. Experiments on a dual-GPU, single-CPU server showed throughput improvements of 15-71% and latency reductions of 12-65% depending on resource configuration.

## Method Summary
The method involves precomputing KV caches for document chunks during offline preparation and storing them in a KV-augmented vector database. At inference time, the system loads precomputed caches instead of recomputing attention over document text, reducing TTFT by eliminating redundant prefill computation. For multi-instance deployments, a shared KV cache manager with CPU RAM caching enables multiple LLM instances to access common disk-resident caches. A proactive cache generator monitors query queues and prefetches KV caches for waiting queries during queue wait times. The approach leverages query locality and idle compute time to reduce redundant prefill computations in multi-instance RAG-powered LLM services.

## Key Results
- 15-71% throughput improvement and 12-65% latency reduction in multi-instance RAG inference
- 10-20% TTFT reduction in single-instance experiments
- Document access locality analysis shows 3.1-31.4% of documents account for 50% of queries
- Performance benefits depend heavily on query locality and resource configuration

## Why This Works (Mechanism)

### Mechanism 1: Precomputed Document KV Cache Reuse
Storing precomputed KV caches for document chunks on disk and reusing them during inference reduces TTFT by eliminating redundant prefill computation. During offline preparation, the KV Cache Manager computes key-value tensors for each document chunk using the target LLM and persists them in a KV-augmented vector database. At inference time, when a query retrieves document(s), the system loads the precomputed KV cache instead of recomputing attention over the document text. The prefill phase then only processes the user query tokens. Core assumption: Document content changes infrequently; cached KV tensors remain valid across queries; query locality exists such that a subset of documents accounts for a large portion of queries.

### Mechanism 2: Shared Disk-Based Cache with Centralized Manager
A centralized Shared KV Cache Manager enables multiple LLM instances to access a common disk-resident KV cache, eliminating duplicate prefill work across instances. All LLM instances query a single Shared KV Cache Manager rather than maintaining independent caches. The manager implements a CPU RAM caching layer with LRU eviction to accelerate repeated accesses. When any instance or the KV Cache Generator creates a cache, it becomes immediately available system-wide through the shared disk store. Core assumption: Multi-instance deployment where instances process queries referencing overlapping document sets; network/IPC overhead for cache distribution is acceptable.

### Mechanism 3: Proactive Prefetching During Queue Wait
Utilizing queue wait time to precompute KV caches for waiting queries improves throughput by transforming idle compute into useful work. The KV Cache Generator monitors the query queue. When a query exceeds a wait threshold, it preemptively performs embedding, retrieval, and KV cache generation using available resources (idle GPU or CPU). By the time the query reaches an LLM instance, its document KV caches are prepared, allowing immediate decode-focused processing. Core assumption: Sufficient queue depth exists under load; idle compute resources (CPU or secondary GPU) are available; prefetching completes before query dispatch.

## Foundational Learning

- **Concept: KV Cache in Transformer Decoding**
  - Why needed here: The entire approach hinges on understanding that prefill generates KV tensors reusable across decode steps and across queries sharing the same context.
  - Quick check question: During autoregressive generation, why must the model retain KV tensors from prior tokens rather than recomputing attention from scratch each step?

- **Concept: RAG Prefill vs. Decode Phases**
  - Why needed here: TTFT is dominated by prefill over retrieved documents; caching targets this specific bottleneck, not decode latency.
  - Quick check question: In a RAG system with a 500-token retrieved document and 20-token query, which phase accounts for most TTFT, and why?

- **Concept: Query Locality in Information Retrieval**
  - Why needed here: Cache effectiveness depends on document reuse frequency; the paper's locality analysis (3.1–31.4% of documents serving 50% of queries) justifies the caching investment.
  - Quick check question: If your workload shows flat document access distribution, would precomputed KV caching still be beneficial?

## Architecture Onboarding

- **Component map**: Offline Cache Preparation -> KV Cache Manager -> Vector Database <- Query Queue -> KV Cache Generator -> Shared KV Cache Manager -> LLM Instances

- **Critical path**:
  1. Offline: Precompute KV caches for all documents → populate KV-augmented vector DB
  2. Query arrives → embed → vector search → retrieve document IDs
  3. Request KV caches from Manager → load from RAM cache or disk
  4. Construct prompt with cached past-key-values + query tokens
  5. LLM performs abbreviated prefill (query only) + decode

- **Design tradeoffs**:
  - Disk vs. RAM cache: Disk offers capacity for large corpora; RAM cache accelerates hot entries but is capacity-limited
  - GPU-only vs. CPU-based KV generation: GPU generation is faster but reduces inference capacity; CPU generation preserves GPU for inference but is slower
  - Top-k handling: Precomputing per-document caches independently loses cross-document attention; joint precomputation for top-k combinations maintains accuracy but increases storage

- **Failure signatures**:
  - Accuracy drop with top-k > 1: Indicates missing cross-attention; solution is joint KV computation for retrieved document combinations
  - High TTFT despite cache hits: Disk I/O latency or RAM cache misses; check NVMe bandwidth and cache eviction policy
  - No throughput improvement: Query locality may be low; analyze document access distribution before caching investment
  - Queue-based prefetch not triggering: Query rate too low to create queue wait; adjust threshold or workload

- **First 3 experiments**:
  1. Baseline vs. RAG-DCache single-instance: Measure TTFT breakdown (prefill time vs. cache load time) across batch sizes using a fixed model (e.g., OPT-2.7B) on SQuAD queries to validate that cache load + reduced prefill < full prefill.
  2. Locality analysis on your corpus: Compute document access CDF from query logs; if 50% of queries require more than ~30% of documents, cache ROI decreases—adjust top-k or accept lower hit rates.
  3. Configuration (A) vs. (B) resource allocation: Run multi-instance workload at high QPS; compare dedicating one GPU to KV generation versus CPU-based generation. Monitor GPU utilization, queue depth, and cache hit rate to identify the bottleneck (inference GPU saturation vs. cache generation latency).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can cross-attention between retrieved documents be efficiently preserved while maintaining the latency benefits of precomputed KV caches?
- **Basis in paper**: The authors acknowledge that "when top-k > 1, the accuracy was... lower than when RAG-DCache was not applied" because "we do not calculate cross-attention between the documents." They propose generating KV caches for document combinations as a workaround but note this increases storage complexity.
- **Why unresolved**: The fundamental tradeoff between approximate caching (missing cross-document attention) and storage explosion (caching all k-document combinations) remains unaddressed.
- **What evidence would resolve it**: An algorithm that achieves comparable accuracy to full RAG while preserving most latency benefits, tested across k=2,3,5 retrieval settings.

### Open Question 2
- **Question**: What is the optimal prefetching threshold that maximizes cache hit benefits without wasting compute on queries that will timeout or be canceled?
- **Basis in paper**: The KV Cache Generator activates when "a query's wait time exceeds a configured threshold," but threshold tuning methodology is absent. The paper notes that under light loads, "queries won't wait in queue and the KV Cache Generator might not trigger."
- **Why unresolved**: The threshold likely depends on query arrival rate distribution, compute resource availability, and average prefill time—all variable across deployments.
- **What evidence would resolve it**: A systematic study of throughput/latency under varying thresholds (e.g., 0ms, 50ms, 200ms, 500ms) across different query arrival rates.

### Open Question 3
- **Question**: How does Shared RAG-DCache scale in true multi-host deployments with network latency and distributed coordination overhead?
- **Basis in paper**: The authors state they "explicitly support multi-instance and multi-host LLM service environments," but acknowledge experiments were "performed on a single-host server" with 2 GPUs and 1 CPU.
- **Why unresolved**: Network transfer of KV caches between hosts could dominate the latency savings from avoided prefill computation, especially for larger caches (16GB for opt-6.7b in Table II).
- **What evidence would resolve it**: Experiments measuring end-to-end latency when KV Cache Manager serves caches across 10Gbps/100Gbps networks to multiple host nodes.

### Open Question 4
- **Question**: How do storage-latency tradeoffs evolve when serving production-scale document corpora (millions of chunks) versus the relatively small SQuAD dataset?
- **Basis in paper**: Storage overhead grows dramatically with model size (0.5MB vector DB → 16GB KV cache for opt-6.7b). The authors note "adjusting the disk-based KV Cache... may represent a tradeoff between query throughput and the necessary disk size" but provide no analysis of eviction policies or cache replacement strategies for storage-constrained scenarios.
- **Why unresolved**: Production RAG systems may have corpora orders of magnitude larger than SQuAD, making full precomputation infeasible.
- **What evidence would resolve it**: Experiments with million-document corpora showing throughput under fixed disk budgets with LRU/LFU/random eviction policies.

## Limitations
- Accuracy degradation with top-k > 1 due to missing cross-document attention, with proposed joint KV computation solution increasing storage complexity quadratically
- Performance improvements heavily depend on specific resource configurations and query locality assumptions that may not generalize across workloads
- Missing ablation study on queue wait threshold parameter, making it difficult to predict performance without empirical tuning

## Confidence
- **High confidence** in single-instance RAG-DCache TTFT reduction mechanism (15-71% throughput improvement): Well-supported by controlled experiments with clear baseline comparisons and consistent results across different batch sizes and model scales
- **Medium confidence** in multi-instance Shared RAG-DCache latency reduction (12-65% improvement): Validated through multi-instance experiments, but the performance gain heavily depends on specific resource configurations and query locality assumptions that may not generalize
- **Low confidence** in accuracy preservation with top-k > 1: The paper explicitly notes accuracy degradation but doesn't provide comprehensive measurements or clear mitigation strategies for the multi-document case

## Next Checks
1. **Accuracy vs. top-k tradeoff analysis**: Measure F1/EM scores for Shared RAG-DCache across different top-k values (1, 3, 5) on a standard RAG benchmark. Compare against baseline RAG to quantify accuracy loss and determine the practical maximum k before accuracy degradation becomes unacceptable.

2. **Resource configuration sensitivity testing**: Run the same workload under three configurations: (A) 1 GPU inference + 1 GPU cache generation, (B) 2 GPUs inference + CPU cache generation, and (C) all GPUs for inference without cache sharing. Measure throughput, latency, and GPU utilization to identify the optimal configuration for different query arrival rates and document locality patterns.

3. **Queue threshold parameter sweep**: Implement the KV Cache Generator with configurable wait time thresholds (0ms, 50ms, 100ms, 200ms). Run experiments at varying QPS levels (low, medium, high) and measure prefetch hit rates, queue depth, and overall latency. Determine the threshold that maximizes performance improvement while minimizing wasted cache generation.