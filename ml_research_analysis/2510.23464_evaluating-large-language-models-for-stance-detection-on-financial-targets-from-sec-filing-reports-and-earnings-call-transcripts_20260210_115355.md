---
ver: rpa2
title: Evaluating Large Language Models for Stance Detection on Financial Targets
  from SEC Filing Reports and Earnings Call Transcripts
arxiv_id: '2510.23464'
source_url: https://arxiv.org/abs/2510.23464
tags:
- financial
- context
- performance
- few-shot
- stance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a sentence-level corpus for stance detection\
  \ on three financial targets\u2014debt, EPS, and sales\u2014extracted from SEC Form\
  \ 10-K reports and earnings call transcripts. The dataset was labeled using ChatGPT-o3-pro\
  \ with human validation."
---

# Evaluating Large Language Models for Stance Detection on Financial Targets from SEC Filing Reports and Earnings Call Transcripts

## Quick Facts
- arXiv ID: 2510.23464
- Source URL: https://arxiv.org/abs/2510.23464
- Reference count: 40
- Primary result: Few-shot with Chain-of-Thought prompting achieved best stance detection performance on financial targets from SEC filings and earnings calls.

## Executive Summary
This paper introduces a sentence-level corpus for stance detection on financial targets (debt, EPS, sales) extracted from SEC Form 10-K reports and earnings call transcripts. The dataset was labeled using ChatGPT-o3-pro with human validation and used to evaluate four large language models under different prompting strategies. The study demonstrates that few-shot learning with Chain-of-Thought prompting achieves the best performance, with GPT-4.1-Mini consistently outperforming other models including Llama3.3, Gemma3, and Mistral 3. The research highlights the viability of using LLMs for target-specific stance detection in finance without requiring extensive labeled data.

## Method Summary
The authors created a stance detection corpus by extracting sentences containing financial targets from SEC Form 10-K reports and earnings call transcripts of two manufacturing companies. ChatGPT-o3-pro was used to label the stance of each sentence toward three targets (debt, EPS, sales) as positive, negative, or neutral, with human validation ensuring high agreement. Four large language models were evaluated under zero-shot, few-shot, and Chain-of-Thought prompting strategies. The models were tested on both SEC filing data and earnings call transcripts to compare performance across different financial document types.

## Key Results
- Few-shot learning with Chain-of-Thought prompting achieved the highest performance across all models and document types
- GPT-4.1-Mini consistently outperformed Llama3.3, Gemma3, and Mistral 3 on both SEC reports and earnings call transcripts
- Models showed significantly better performance on earnings call transcripts compared to SEC reports, attributed to the numerically dense and complex syntax of SEC documents

## Why This Works (Mechanism)
The success of LLMs in stance detection stems from their ability to understand contextual relationships between sentences and financial targets through natural language processing. Chain-of-Thought prompting enables step-by-step reasoning that helps models navigate the complex quantitative relationships present in financial documents. The few-shot approach provides relevant examples that help models adapt to the specific language and terminology used in financial reporting. The combination of these techniques allows models to capture nuanced sentiment toward financial metrics even in documents with varying syntactic complexity.

## Foundational Learning
- **Stance Detection**: Determining whether a sentence expresses positive, negative, or neutral sentiment toward a specific target - needed to understand the core task and evaluation metrics
- **Chain-of-Thought Prompting**: A technique where models are prompted to show their reasoning step-by-step - needed to understand why CoT improved performance on complex SEC documents
- **Zero-shot vs Few-shot Learning**: Zero-shot uses no examples while few-shot provides a small number of examples - needed to compare prompting strategies and understand their relative effectiveness
- **SEC Filing Structure**: Understanding the format and content of Form 10-K reports - needed to appreciate why models perform differently on SEC vs earnings call data
- **Earnings Call Transcript Analysis**: Understanding the conversational nature of earnings calls - needed to explain performance differences between document types
- **Financial Target Identification**: Recognizing specific financial metrics (debt, EPS, sales) as targets - needed to understand the domain-specific focus of the study

## Architecture Onboarding
**Component Map**: Data Extraction -> Automated Labeling -> Human Validation -> Model Evaluation -> Performance Analysis
**Critical Path**: SEC/ECT Document Processing → Sentence Extraction → Target Identification → Stance Labeling → LLM Evaluation → Performance Comparison
**Design Tradeoffs**: Automated labeling with ChatGPT-o3-pro provides scalability but may introduce bias toward OpenAI models; focusing on three financial targets limits scope but enables depth; using two companies enables controlled comparison but limits generalizability
**Failure Signatures**: Poor performance on SEC reports indicates difficulty with numerical reasoning; inconsistent results across document types suggest sensitivity to syntactic complexity; potential labeling bias may inflate performance for OpenAI models
**Three First Experiments**:
1. Test model performance on a new financial target (e.g., revenue growth) to assess scalability
2. Apply the same models to financial documents from a different industry (e.g., technology) to evaluate generalizability
3. Implement specialized numerical reasoning prompts for SEC documents to bridge the performance gap

## Open Questions the Paper Calls Out
### Open Question 1
- **Question:** Can the observed performance of LLMs on stance detection generalize to companies in diverse industries outside of the manufacturing sector?
- **Basis in paper:** The authors explicitly state in the Limitations section that the study focused exclusively on two companies (MATIV Holdings Inc. and 3M Co.), which "may constrain the generalizability of the findings across broader financial contexts."
- **Why unresolved:** The current dataset is restricted to a specific industry profile, leaving the model's ability to handle varying financial jargon and reporting styles in other sectors (e.g., tech, pharmaceuticals) untested.
- **What evidence would resolve it:** Evaluation results from the same models applied to a new corpus of SEC filings and ECTs derived from companies in distinct sectors, such as energy or biotechnology.

### Open Question 2
- **Question:** To what degree does using ChatGPT-o3-pro for data annotation bias the evaluation in favor of OpenAI models like GPT-4.1-mini?
- **Basis in paper:** The authors acknowledge that while human validation agreement was high, "the use of a language model for data annotation may introduce model-induced biases," potentially inflating performance for models within the same family.
- **Why unresolved:** The paper validates the methodology but does not quantify the specific advantage the annotator model's "DNA" gives to the evaluated OpenAI model compared to the open-source competitors.
- **What evidence would resolve it:** A comparative experiment where the test set is re-annotated by human experts or a different state-of-the-art model, followed by a re-evaluation of GPT-4.1-mini to measure performance degradation.

### Open Question 3
- **Question:** How can prompting strategies be optimized to bridge the performance gap between conversational transcripts and numerically dense SEC reports?
- **Basis in paper:** The results show a consistent performance gap where models score higher on ECT data than SEC data; the authors attribute this to SEC reports requiring "deeper quantitative reasoning" and containing "numerically dense" complex syntax.
- **Why unresolved:** The study evaluates standard prompting (Zero/Few/CoT) but does not propose specific mechanisms to handle the quantitative reasoning failures observed in SEC sentences (e.g., calculating ratio changes).
- **What evidence would resolve it:** An analysis of performance on SEC data using specialized prompts that explicitly require step-by-step numerical calculations or the integration of external calculator tools.

## Limitations
- The study's focus on two manufacturing companies limits generalizability to other industries with different financial reporting styles
- Automated labeling using ChatGPT-o3-pro may introduce bias favoring OpenAI models, despite human validation
- The evaluation framework lacks statistical significance testing to confirm performance differences between models and prompting strategies

## Confidence
- Model performance superiority (Medium): The claim that GPT-4.1-Mini consistently outperforms other models is supported by the reported results, but lacks statistical validation and may be influenced by implementation factors
- Few-shot with CoT effectiveness (Medium): The assertion that few-shot with Chain-of-Thought prompting achieves the best performance is based on observed results, but requires statistical confirmation
- LLM viability for financial stance detection (High): The demonstration that LLMs can perform target-specific stance detection in finance without extensive labeled data is well-supported, though the quality of the automated labels remains uncertain

## Next Checks
1. Conduct statistical significance testing on model performance differences to verify that observed improvements are not due to random variation
2. Implement a stratified human validation process where a random sample of labeled data is independently verified by multiple annotators to establish inter-annotator agreement and label reliability
3. Expand the evaluation to include additional financial targets beyond debt, EPS, and sales, and test the models on different types of financial documents to assess generalizability