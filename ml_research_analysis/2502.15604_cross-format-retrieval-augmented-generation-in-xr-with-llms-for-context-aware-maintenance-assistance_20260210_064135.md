---
ver: rpa2
title: Cross-Format Retrieval-Augmented Generation in XR with LLMs for Context-Aware
  Maintenance Assistance
arxiv_id: '2502.15604'
source_url: https://arxiv.org/abs/2502.15604
tags:
- system
- data
- information
- response
- maintenance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates a Retrieval-Augmented Generation (RAG) system
  that integrates large language models (LLMs) to support maintenance tasks by processing
  multi-format data sources (PDFs, CSVs, text files). Eight LLMs were tested on three
  query complexity scenarios, with performance measured by BLEU/METEOR accuracy scores,
  success rates, and response times.
---

# Cross-Format Retrieval-Augmented Generation in XR with LLMs for Context-Aware Maintenance Assistance

## Quick Facts
- **arXiv ID**: 2502.15604
- **Source URL**: https://arxiv.org/abs/2502.15604
- **Reference count**: 30
- **Primary result**: GPT-4 and GPT-4o-mini consistently outperformed other models in cross-format maintenance queries, with success rates reaching 97-98% for simpler queries and 93% for complex scenarios.

## Executive Summary
This study evaluates a Retrieval-Augmented Generation (RAG) system that integrates large language models (LLMs) to support maintenance tasks by processing multi-format data sources (PDFs, CSVs, text files). Eight LLMs were tested on three query complexity scenarios, with performance measured by BLEU/METEOR accuracy scores, success rates, and response times. GPT-4 and GPT-4o-mini consistently outperformed other models, particularly in complex scenarios requiring cross-format data integration. The RAG system successfully delivered accurate responses in most cases, with success rates reaching 97-98% for simpler queries. However, challenges remain with complex queries and local-model deployment, as Llama models showed lower accuracy and higher hallucination rates.

## Method Summary
The RAG system processes heterogeneous data sources (PDFs, CSVs, text) using format-specific handlers with unified LLM access. PDFs are parsed using the `pypdf` library, while CSVs are queried via DuckDB using SQL generated by the LLM based on provided schema summaries. The system employs multi-path query decomposition for complex queries, where the LLM analyzes input queries and generates JSON-formatted subqueries mapped to specific knowledge bases. Three evaluation scenarios test performance: (A) simple queries on text/PDF, (B) simple queries on CSV, and (C) complex queries across both formats. Eight LLMs were evaluated across 50 runs per scenario, measuring success rates, response times, and BLEU/METEOR accuracy scores.

## Key Results
- GPT-4o-mini achieved the highest success rate (93%) in complex cross-format scenarios
- Llama models struggled significantly, with Llama 2 achieving only 2% success rate on complex queries
- GPT-4o models showed higher hallucination rates (up to 8%) but maintained superior accuracy overall
- Response times varied widely, with GPT-3.5 fastest (3.32s) but 80% success vs GPT-4o-mini slower (9.90s) but 93% success

## Why This Works (Mechanism)

### Mechanism 1: Cross-Format Data Processing
The system enables retrieval from heterogeneous data sources (PDF, CSV) through format-specific handlers with unified LLM access. PDFs are parsed using the `pypdf` library for text extraction; CSVs are queried via DuckDB using SQL generated by the LLM based on provided schema summaries (column headers + sample rows). The LLM receives a Knowledge Base Summary in JSON format describing available data. Core assumption: LLMs can reliably generate valid SQL queries when given schema context and natural language intent. Evidence anchors: [abstract] "Cross-Format Functionality: The system can process and retrieve data from various file types, such as plain text, PDFs, CSVs, and databases." Break condition: Schema summaries exceed context window limits; SQL generation fails on complex joins or ambiguous column references.

### Mechanism 2: Multi-Path Query Decomposition
Complex queries referencing multiple knowledge bases are decomposed into correlated subqueries that execute in parallel. LLM analyzes input query → generates JSON-formatted subqueries mapped to specific knowledge bases → Cross-Format Retrieval module processes each → results synthesized into unified response. Core assumption: The orchestrating LLM can accurately identify which knowledge bases are relevant and maintain logical coherence across subqueries. Evidence anchors: [abstract] "multi-path routing for complex queries... GPT-4o-mini achieved a 93% success rate in the most complex scenario." Break condition: Query ambiguity causes incorrect knowledge base routing; subquery dependencies are not properly sequenced.

### Mechanism 3: Model-Task Complexity Alignment
Larger parameter models (GPT-4, GPT-4o-mini) demonstrate stronger reasoning for multi-source synthesis; local Llama variants show higher hallucination rates and slower response times under complexity. Core assumption: The performance differential stems from reasoning capability and context handling, not retrieval infrastructure. Evidence anchors: [Section 4.3] "Scenario C evaluated complex queries... GPT-4o-mini scored the highest with a success rate of 93%... LLama models struggled significantly, with LLama 2 achieving a success rate of only 2%." Break condition: Cost/latency constraints preclude advanced models; domain-specific fine-tuning requirements favor local deployment.

## Foundational Learning

- **Concept: RAG Architecture Fundamentals**
  - Why needed here: Core pattern grounding LLM outputs in external knowledge to reduce hallucination.
  - Quick check question: Can you explain why RAG reduces hallucination compared to vanilla LLM prompting, and what retrieval granularity choices affect?

- **Concept: Structured Data Querying via LLM**
  - Why needed here: CSV retrieval requires LLM-to-SQL translation; understanding schema prompting is critical.
  - Quick check question: What information must a schema summary contain to enable reliable SQL generation for a maintenance parts database?

- **Concept: Evaluation Metrics (BLEU/METEOR)**
  - Why needed here: Paper relies on these for accuracy quantification; understanding their limitations is essential for interpreting results.
  - Quick check question: Why might a correct but verbose response score lower on BLEU than a concise but incomplete one?

## Architecture Onboarding

- **Component map:** Voice + images (AI Vision extraction) → Query → Knowledge Base Summary lookup → Subquery decomposition (LLM) → Format-specific retrieval (pypdf/DuckDB) → Response synthesis

- **Critical path:** Query → Knowledge Base Summary lookup → Subquery decomposition (LLM) → Format-specific retrieval (pypdf/DuckDB) → Response synthesis

- **Design tradeoffs:** Local (Llama) vs Cloud (GPT): Data sovereignty vs accuracy/speed; Verbosity: GPT-4o models produce enriched responses that lower BLEU scores but increase completeness; Speed vs accuracy: GPT-3.5 fastest (3.32s) but 80% success vs GPT-4o-mini slower (9.90s) but 93% success on complex queries

- **Failure signatures:** Hallucination: Llama models up to 8% rate; Incorrect routing: 66% incorrect answer rate for Llama 3 on complex scenarios; Verbose responses: Lower BLEU despite correctness (GPT-4o patterns)

- **First 3 experiments:**
  1. Replicate Scenario A (single-format text retrieval) with your chosen model to establish baseline latency and accuracy.
  2. Test schema summary variations for CSV querying—measure SQL generation success rate across different summary detail levels.
  3. Compare cloud vs local model performance on your domain's cross-format queries to validate the paper's complexity-performance relationship for your use case.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can an adaptive context-based filtering mechanism be implemented to mitigate false positives where LLMs rely on general training data rather than the specific maintenance knowledge base?
- Basis in paper: [explicit] The Discussion section identifies false positives as a critical challenge, noting that models often draw from "generalized understanding" rather than the curated database, and proposes "adaptive context-based filtering" as a solution.
- Why unresolved: Current state-of-the-art LLMs frequently misalign with specific operational contexts, particularly during routine inquiries, without a defined filtering architecture.
- What evidence would resolve it: A comparative evaluation showing a statistical reduction in hallucination rates and increased adherence to the provided knowledge base in the system's outputs.

### Open Question 2
- Question: Can local models (e.g., Llama) be optimized to match the latency and accuracy of cloud-based models (e.g., GPT-4o-mini) in complex, multi-format retrieval scenarios?
- Basis in paper: [inferred] The Discussion on "Local-Model Concerns" highlights the trade-off between the data security of local deployment and the "marked discrepancies in response accuracy and speed" compared to cloud counterparts.
- Why unresolved: Local resource constraints (computational power and memory) currently limit the real-time processing capabilities required for sophisticated, multi-tiered queries.
- What evidence would resolve it: Benchmark results where fine-tuned or hardware-optimized local models achieve comparable success rates (>90%) and response times (<15s) in Scenario C (complex queries).

### Open Question 3
- Question: What specific retrieval technique refinements are required to improve response generation for "intricate scenarios" involving multi-format data integration?
- Basis in paper: [explicit] The Conclusion states that future research will prioritize "refining retrieval techniques" specifically to enhance system responsiveness to "less common queries" and complex situations.
- Why unresolved: The study observed a drop in success rates for complex queries (Scenario C) compared to simple ones, indicating the current multi-path routing logic struggles with higher complexity.
- What evidence would resolve it: Improved BLEU/METEOR scores and success rates in complex scenarios using advanced retrieval methods (e.g., REAPER-style planning) compared to the current baseline.

## Limitations

- The evaluation relies heavily on proprietary cloud models, raising questions about scalability and cost in real-world deployments
- Local Llama models showed significantly degraded performance, suggesting potential domain-specific challenges or insufficient fine-tuning
- Evaluation metrics (BLEU/METEOR) may not fully capture semantic correctness, as evidenced by verbose responses scoring lower despite higher success rates

## Confidence

- **High Confidence**: Cross-format retrieval architecture and basic performance differentials between model families (GPT vs Llama) are well-supported by empirical results
- **Medium Confidence**: Claims about multi-path query decomposition effectiveness are supported but lack detailed analysis of routing accuracy and subquery correlation mechanisms
- **Low Confidence**: Assertions about optimal model selection for production scenarios (GPT-4o-mini) are based on limited cost-performance tradeoffs without addressing deployment constraints or domain-specific tuning requirements

## Next Checks

1. Replicate the evaluation with domain-specific maintenance datasets to verify performance generalization across different technical domains and knowledge base sizes
2. Conduct detailed error analysis comparing hallucination patterns between cloud and local models to identify specific failure modes and mitigation strategies
3. Test alternative evaluation metrics (semantic similarity, task completion) to validate whether BLEU/METEOR scores accurately reflect real-world maintenance assistance effectiveness