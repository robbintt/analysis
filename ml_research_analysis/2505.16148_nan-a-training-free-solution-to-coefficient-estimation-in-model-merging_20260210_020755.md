---
ver: rpa2
title: 'NAN: A Training-Free Solution to Coefficient Estimation in Model Merging'
arxiv_id: '2505.16148'
source_url: https://arxiv.org/abs/2505.16148
tags:
- merging
- arxiv
- task
- preprint
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of coefficient estimation in
  model merging, a technique for combining independently fine-tuned models into a
  unified one without access to raw data. Existing approaches often rely on heuristics
  or additional statistics to determine merging coefficients, limiting their scalability
  and generality.
---

# NAN: A Training-Free Solution to Coefficient Estimation in Model Merging

## Quick Facts
- **arXiv ID**: 2505.16148
- **Source URL**: https://arxiv.org/abs/2505.16148
- **Reference count**: 15
- **Primary result**: NAN is a training-free method that estimates merging coefficients via the inverse of parameter norm, improving baseline merging performance by 2.5% on average across language tasks.

## Executive Summary
Model merging is a technique for combining independently fine-tuned models into a unified one without access to raw data. Traditional approaches often rely on heuristics or additional statistics to determine merging coefficients, limiting their scalability and generality. NAN introduces a training-free method that estimates merging coefficients via the inverse of parameter norm, based on a theoretical framework derived from least-squares optimization. The method is simple, plug-and-play, and applicable to a wide range of merging strategies. Experiments on vision, language, and VLM tasks demonstrate that NAN consistently improves the performance of baseline methods, with an average improvement of 2.5% across four language benchmarks compared to the best baseline.

## Method Summary
NAN proposes a training-free approach to coefficient estimation in model merging by leveraging the inverse of parameter norms as merging weights. The method is grounded in a theoretical framework that connects parameter norms to the amount of task-specific information encoded in each model. By computing the Frobenius norm of weight updates for each task, NAN estimates the relative importance of each fine-tuned model without requiring additional training or access to original data. The merging coefficients are then set as the inverse of these norms, normalized to sum to one. This approach is model-agnostic and can be applied to various merging strategies, including single-layer, cross-layer, and LoRA-based merging. The simplicity and plug-and-play nature of NAN make it a practical solution for improving the performance of merged models across diverse tasks and domains.

## Key Results
- NAN improves baseline merging performance by 2.5% on average across four language benchmarks.
- The method consistently enhances performance across vision, language, and VLM tasks.
- NAN demonstrates scalability and effectiveness in merging models with different architectures and fine-tuning strategies.

## Why This Works (Mechanism)
NAN's effectiveness stems from its theoretical foundation in least-squares optimization, which establishes that optimal merging weights should scale with the amount of task-specific information encoded in each model. The method leverages the Frobenius norm of weight updates as a proxy for this information, under the assumption that larger norm updates indicate more significant task-specific adaptation. By setting merging coefficients as the inverse of these norms, NAN effectively balances the contribution of each model based on its relative importance to the target task. This approach eliminates the need for additional training or heuristic-based weight selection, providing a training-free and generalizable solution to coefficient estimation in model merging.

## Foundational Learning
- **Model Merging**: Combining independently fine-tuned models into a unified one without raw data access; needed to understand the problem NAN addresses.
- **Parameter Norms**: The Frobenius norm of weight updates as a proxy for task-specific information; critical for understanding NAN's weighting mechanism.
- **Least-Squares Optimization**: Theoretical framework connecting parameter norms to optimal merging coefficients; provides the mathematical foundation for NAN.
- **Fine-Tuning**: Process of adapting pre-trained models to specific tasks; relevant context for understanding model merging scenarios.
- **Plug-and-Play Methods**: Approaches that can be easily integrated into existing workflows; describes NAN's practical applicability.

## Architecture Onboarding
- **Component Map**: Pre-trained models → Fine-tuning → Weight update computation → Norm calculation → Inverse norm weighting → Model merging
- **Critical Path**: The theoretical derivation (Section 3.1) establishes the connection between parameter norms and optimal merging coefficients, which is the core of NAN's effectiveness.
- **Design Tradeoffs**: NAN trades off theoretical optimality for practical simplicity and training-free operation, making it more accessible but potentially less precise than training-based methods.
- **Failure Signatures**: Performance degradation when weight updates are not zero-centered or when task-specific information is not well-captured by parameter norms.
- **3 First Experiments**:
  1. Apply NAN to merge two models fine-tuned on different subsets of a language task and compare performance to baseline merging strategies.
  2. Test NAN's effectiveness in merging models with different fine-tuning strategies (e.g., full fine-tuning vs. LoRA) on a vision task.
  3. Evaluate NAN's scalability by merging models with increasing parameter counts (e.g., 1B, 8B, 30B parameters) on a VLM task.

## Open Questions the Paper Calls Out
### Open Question 1
- **Question**: Can NAN be effectively adapted for merging models with heterogeneous architectures rather than just those sharing a pre-trained backbone?
- **Basis in paper**: [explicit] The "Limitations" section explicitly states the method "currently focuses on merging models with a shared pre-trained backbone and may require adaptation for merging across heterogeneous architectures."
- **Why unresolved**: The theoretical derivation assumes a shared parameter space $W$ to compute norms and merge weights, which breaks down when model structures differ.
- **What evidence would resolve it**: A modified NAN mechanism applied to models with different architectures (e.g., merging a ResNet with a ViT) showing improved performance over baseline merging strategies.

### Open Question 2
- **Question**: To what extent does the linear least-squares theoretical framework fail to capture the dynamics of non-linear deep networks?
- **Basis in paper**: [inferred] The method is derived using a linear formulation ($Y = XW$) in Section 3.1, whereas the actual experiments utilize highly non-linear architectures (Transformers, ViTs).
- **Why unresolved**: The paper does not provide a theoretical bound on the error introduced when mapping the linear solution to non-linear transformers.
- **What evidence would resolve it**: A theoretical analysis or empirical study correlating the degree of model non-linearity with the deviation from NAN's predicted optimal coefficients.

### Open Question 3
- **Question**: Under what conditions does the parameter norm fail as a proxy for data volume due to violations of the zero-mean assumption?
- **Basis in paper**: [inferred] Section 3.3 relies on the assumption that weight updates are approximately zero-centered to equate variance with the Frobenius norm. Specific results in Table 1 (e.g., SVHN performance dropping from 80.2 to 76.2) suggest this proxy can misestimate task importance.
- **Why unresolved**: The paper does not analyze why the norm-proxy fails on specific datasets like SVHN where performance degrades.
- **What evidence would resolve it**: An ablation study analyzing the distribution of weight updates for failing tasks to determine if non-zero means correlate with performance drops.

## Limitations
- NAN currently focuses on merging models with a shared pre-trained backbone and may require adaptation for merging across heterogeneous architectures.
- The method's performance gains, while consistent, are sometimes modest (e.g., 2.5% average improvement), suggesting potential room for refinement.
- The evaluation focuses primarily on merging two models, leaving open questions about scalability to multi-model merging scenarios.

## Confidence
- **High confidence**: The empirical results showing NAN's consistent performance improvements across vision, language, and VLM tasks are well-supported by the experimental data.
- **Medium confidence**: The theoretical framework connecting parameter norms to optimal merging coefficients is plausible but relies on simplifying assumptions that may not capture all practical scenarios.
- **Medium confidence**: The claim of NAN being "training-free" is accurate, though the method does require access to pre-computed parameter norms and merged model evaluation, which involves some computational overhead.

## Next Checks
1. Test NAN's scalability and performance on models with significantly more parameters (e.g., 30B+ parameter models) to assess its applicability to state-of-the-art architectures.
2. Evaluate NAN in multi-model merging scenarios (merging more than two models) to determine whether the coefficient estimation method scales effectively.
3. Conduct ablation studies to quantify the contribution of the inverse norm weighting versus other potential weighting schemes, and assess sensitivity to different fine-tuning hyperparameters.