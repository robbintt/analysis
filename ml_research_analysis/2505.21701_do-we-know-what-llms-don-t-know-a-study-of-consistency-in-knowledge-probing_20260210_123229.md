---
ver: rpa2
title: Do We Know What LLMs Don't Know? A Study of Consistency in Knowledge Probing
arxiv_id: '2505.21701'
source_url: https://arxiv.org/abs/2505.21701
tags:
- one-shot
- space
- typo
- methods
- consistency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the consistency of knowledge probing methods
  used to identify knowledge gaps in large language models (LLMs). Through systematic
  evaluation, it reveals alarming inconsistencies within and across probing methods,
  with decision consistency dropping as low as 7% and intra-method consistency reaching
  only 2%.
---

# Do We Know What LLMs Don't Know? A Study of Consistency in Knowledge Probing

## Quick Facts
- arXiv ID: 2505.21701
- Source URL: https://arxiv.org/abs/2505.21701
- Reference count: 40
- Primary result: Knowledge probing methods show alarming inconsistency, with decision consistency dropping as low as 7% and intra-method consistency reaching only 2%.

## Executive Summary
This study systematically evaluates the consistency of knowledge probing methods used to identify knowledge gaps in large language models. Through systematic evaluation across multiple models, datasets, and prompt perturbations, it reveals that current probing methods are highly unreliable, with decision consistency dropping to as low as 7% in cross-method comparisons. The paper identifies three key failure mechanisms: prompt sensitivity cascade, signal disagreement across probing paradigms, and threshold calibration fragility. These findings challenge the validity of current probing methods and highlight the urgent need for more robust, perturbation-resistant frameworks to reliably assess LLM knowledge.

## Method Summary
The study evaluates six knowledge probing methods (TOKPROB, ASKCAL, EMBEDDING, NOTA, MOREINFO, SELFREF) across multiple models (Mistral-7B, LLaMA-3.x, OLMo-2) and datasets (MMLU, HellaSwag). Methods are tested under four prompt perturbations: space insertion, option shuffling, typos, and one-shot variants. Consistency is measured using IoU_cons (intra-method) and DecCons (cross-method), comparing rejection decisions across prompt variants and between methods. Threshold-based methods (TOKPROB, ASKCAL) use calibration on validation sets with a 0.5 clamping safeguard for unreasonable thresholds.

## Key Results
- Intra-method consistency drops to 7% under prompt perturbations, with option shuffling reducing agreement to around 40%.
- Cross-method decision consistency ranges from 7% to 98%, with methods using similar signals (NOTA/MOREINFO) showing highest agreement.
- Scaling rule does not consistently hold—some methods perform worse on larger models (e.g., NOTA peaks at 3B and declines at 70B).
- Established metrics like Abstain F1 fail to capture probing method reliability, masking significant variations in rejection behavior.

## Why This Works (Mechanism)

### Mechanism 1: Prompt Sensitivity Cascade
Knowledge probing methods inherit and amplify the underlying LLM's sensitivity to surface-level prompt variations. When prompts undergo non-semantic perturbations, the model's output distribution shifts, propagating to the probing signal and causing different classifications of the same knowledge. EMBEDDING maintains relatively higher consistency under option shuffling, suggesting representation-based approaches may partially mitigate this.

### Mechanism 2: Signal Disagreement Across Probing Paradigms
Different probing methods capture fundamentally different aspects of model behavior, not a unified "knowledge" construct. TOKPROB captures output probability mass, ASKCAL captures self-reported confidence, EMBEDDING captures hidden state patterns, and NOTA/MOREINFO capture behavioral choices. These signals originate from different mechanisms and are not guaranteed to correlate, leading to legitimate disagreement about "knowledge."

### Mechanism 3: Threshold Calibration Fragility
Calibration-based probing methods are highly sensitive to threshold selection, which can produce unstable and extreme rejection rates. Validation set optimization can converge to extreme values (e.g., 0.98 rejecting nearly all, or 0.01 accepting nearly all) when validation data doesn't cleanly separate correct/incorrect predictions. Small distribution shifts between validation and test exacerbate this.

## Foundational Learning

- **Intersection over Union (IoU) as Consistency Metric**: IoU_cons, IoU_acc, and IoU_rej quantify overlap in accept/reject decisions across prompt variants or methods. Understanding IoU is essential to interpret Tables 1, 7, 8. Quick check: If Method A rejects 100 questions and Method B rejects 100 questions with 50 overlap, what is IoU_rej? (Answer: 50/150 = 0.33)

- **Calibration in Uncertainty Estimation**: TOKPROB and ASKCAL rely on calibrating model confidence against empirical accuracy. Without calibration, raw probabilities don't reflect true accuracy rates. Quick check: If a model assigns 0.8 confidence to 100 predictions but only 60% are correct, is it well-calibrated? (Answer: No, overconfident; 0.8 confidence should correspond to ~80% accuracy.)

- **Hidden State Probing**: EMBEDDING trains a classifier on LLM hidden states to predict knowledge gaps. This assumes internal representations encode uncertainty or knowledge boundaries. Quick check: Why might hidden states be more robust to prompt perturbation than output probabilities? (Answer: Hidden states capture deeper semantic patterns; output probabilities are more exposed to surface-level tokenization and formatting effects.)

## Architecture Onboarding

- **Component map**:
  ```
  Input Layer: Prompt + Question + Options
       ↓
  Perturbation Module: Space/Options Shuffle/Typo/One-shot variants
       ↓
  LLM Forward Pass: Mistral-7B / LLaMA-3.x / OLMo-2
       ↓
  Signal Extraction: 
    - TOKPROB → output token probabilities
    - ASKCAL → self-reported confidence score
    - EMBEDDING → last-layer hidden states → trained classifier
    - NOTA → "None of the above" selection rate
    - MOREINFO → "Do you need more info?" response
    - SELFREF → self-verification True/False
       ↓
  Decision Module: Threshold comparison (calibration) or behavioral check
       ↓
  Output: Accept (model knows) / Reject (knowledge gap)
  ```

- **Critical path**:
  1. Define probing method(s) to evaluate
  2. Generate prompt variants (original + perturbations)
  3. Run all methods across all variants on same model+dataset
  4. Compute IoU_cons for intra-method comparisons
  5. Compute DecCons for cross-method comparisons
  6. Validate with Abstain F1 but recognize it masks inconsistency

- **Design tradeoffs**:
  - Representation-based vs. output-based signals: EMBEDDING requires training data but shows higher robustness; TOKPROB/ASKCAL are zero-shot but more fragile
  - Threshold correction vs. pure optimization: Clamping thresholds to 0.5 improves consistency but may reduce accuracy; pure optimization can yield unstable extremes
  - Multiple-choice vs. open-ended: Paper focuses on multiple-choice for easier evaluation; open-ended generation requires different consistency metrics

- **Failure signatures**:
  - IoU_cons < 0.5 under option shuffling → prompt sensitivity dominant
  - Cross-method DecCons < 0.2 → methods measuring different constructs
  - Abstain F1 stable but IoU_cons drops → metric masking
  - Threshold values at extremes (< 0.1 or > 0.9) → calibration failure

- **First 3 experiments**:
  1. Reproduce intra-method inconsistency on Mistral-7B + MMLU subset (100 examples), run TOKPROB with original and shuffled-options prompts. Compute IoU_cons. Expected: ~0.40.
  2. Test threshold correction impact: Run ASKCAL with automatic threshold selection, check if thresholds fall in reasonable range (0.3-0.7). Implement 0.5 clamping safeguard. Compare IoU_cons before/after. Expected: improvement of +0.20 to +0.40.
  3. Cross-method comparison on LLaMA-3-8B + MMLU: Run all six methods on identical prompts. Build DecCons heatmap. Identify which method pairs show highest/lowest agreement. Test hypothesis: methods using similar signals should show DecCons > 0.7.

## Open Questions the Paper Calls Out
- Do the observed inconsistencies persist in open-ended text generation tasks, or are they specific to the multiple-choice format?
- What underlying mechanisms cause the consistency of certain probing methods (such as NOTA) to degrade as model scale increases to 70B parameters?
- How can evaluation metrics be redesigned to effectively capture the reliability and consistency of probing methods, rather than just aggregate abstention performance?

## Limitations
- Ground-truth ambiguity: No objective ground truth for "what the model does not know," undermining the validity of consistency metrics.
- Signal validity: Cross-method low consistency could reflect construct divergence rather than method unreliability.
- Model and dataset scope: Experiments cover limited models and two multiple-choice datasets, limiting generalization.

## Confidence

- **High confidence**: Intra-method inconsistency under prompt perturbations - directly observable from IoU_cons values across variants.
- **Medium confidence**: Cross-method inconsistency is fundamental - supported by DecCons heatmap and signal divergence, but ground-truth ambiguity limits definitive claims.
- **Low confidence**: Threshold correction (clamping to 0.5) is a robust solution - shows improvement in single case but lacks systematic evaluation.

## Next Checks

1. **Ground-truth validation study**: Create synthetic test cases where ground-truth knowledge gaps are known. Evaluate all six probing methods against this ground truth to determine which consistently identify actual gaps.

2. **Threshold stability analysis**: Systematically vary validation set size, composition, and distribution shift for TOKPROB and ASKCAL. Track threshold convergence and IoU_cons stability. Test whether 0.5 clamping generalizes.

3. **Cross-task generalization test**: Apply the probing consistency framework to open-ended generation tasks (e.g., summarization, code generation) where "knowledge gaps" manifest as factual errors. Compare consistency patterns to multiple-choice tasks.