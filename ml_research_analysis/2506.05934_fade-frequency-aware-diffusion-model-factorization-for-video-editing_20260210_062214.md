---
ver: rpa2
title: 'FADE: Frequency-Aware Diffusion Model Factorization for Video Editing'
arxiv_id: '2506.05934'
source_url: https://arxiv.org/abs/2506.05934
tags:
- video
- editing
- blocks
- diffusion
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "FADE addresses the challenge of editing video content, particularly\
  \ for motion adjustments, by leveraging frequency-aware factorization of pre-trained\
  \ text-to-video (T2V) diffusion models. The method analyzes attention patterns within\
  \ the T2V model to identify how video priors are distributed across components,\
  \ enabling a factorization strategy that optimizes each block\u2019s specialized\
  \ role."
---

# FADE: Frequency-Aware Diffusion Model Factorization for Video Editing

## Quick Facts
- arXiv ID: 2506.05934
- Source URL: https://arxiv.org/abs/2506.05934
- Reference count: 40
- One-line primary result: FADE achieves CLIP score of 0.3561 and OSV of 43.57 on DA VIS benchmark with strong input fidelity (Mask-PSNR: 21.54, LPIPS: 0.3297) and human preference scores, completing edits in ~3 minutes per video without training.

## Executive Summary
FADE introduces a training-free video editing framework that leverages frequency-aware factorization of pre-trained text-to-video diffusion models. By analyzing attention patterns across transformer blocks, FADE identifies that early blocks capture low-frequency spatial-temporal structure while later blocks refine high-frequency details. This enables selective use of blocks based on editing requirements, with spectrum-guided modulation preserving source structure while allowing flexible detail editing.

The method operates by first inverting the source video to obtain a latent trajectory, then running modified DDIM sampling with guidance derived from 3D DFT analysis of sketching block outputs. This approach achieves state-of-the-art results on the DAVIS benchmark, demonstrating high-quality, temporally coherent edits with strong input fidelity metrics and human preference scores, all while avoiding sample-specific optimization and completing edits in approximately 3 minutes per video.

## Method Summary
FADE factorizes pre-trained text-to-video diffusion models by analyzing attention patterns to separate "sketching blocks" (first 4 of 48) that encode low-frequency spatial-temporal structure from "sharpening blocks" that refine high-frequency details. The method employs DDIM inversion to obtain source video trajectories, then uses 3D DFT on sketching block outputs to generate spectrum-guided modulation during sampling. This modulates the DDIM denoising step with L2 differences between source and target frequency spectra, preserving foundational structure while enabling flexible edits. The approach is training-free, leveraging pretrained T2V model priors, and achieves efficient processing (~3 min/video) with competitive quality metrics on DAVIS benchmark.

## Key Results
- Achieves CLIP score of 0.3561 and OSV of 43.57 on DA VIS benchmark
- Maintains strong input fidelity with Mask-PSNR of 21.54 and LPIPS of 0.3297
- Delivers high-quality, temporally coherent edits with strong human preference scores
- Operates without training, completing edits in approximately 3 minutes per video

## Why This Works (Mechanism)

### Mechanism 1: Frequency-Aware Block Factorization
- **Claim**: Early transformer blocks encode low-frequency spatial layouts and temporal dynamics ("sketching blocks"), while later blocks refine high-frequency details ("sharpening blocks").
- **Mechanism**: Analysis of attention patterns shows early blocks produce blurred outputs with diagonal attention patterns (indicating focus on foundational structure and temporal correspondence), while later blocks show sparse, distributed attention for detail refinement.
- **Core assumption**: Frequency-role differentiation across blocks is consistent across sampling timesteps and generalizes across diverse video content types.
- **Evidence anchors**: Attention pattern analysis in Section 3.2; weak/no direct evidence in corpus.
- **Break condition**: If attention patterns vary significantly across video domains or the block-function mapping shifts across sampling steps, the fixed 4-block factorization may underperform.

### Mechanism 2: Spectrum-Guided Modulation via 3D DFT
- **Claim**: Frequency-domain guidance from sketching blocks modulates the sampling trajectory to preserve source structure while allowing flexible detail editing.
- **Mechanism**: 3D DFT transforms attention outputs to frequency domain; low-pass filtering isolates spatial-temporal structure. The L2 difference between source and target spectra provides a gradient that nudges the edited video toward preserving foundational layout/motion while freeing high-frequency details for modification.
- **Core assumption**: Low-frequency components capture essential structure/motion that should remain invariant; high-frequency components contain editable details and noise that need not be preserved.
- **Evidence anchors**: Equation 6-7 define guidance G_t and modulated DDIM step; reports using ~2/3 of frequency components for optimal balance.
- **Break condition**: If critical structural changes require high-frequency components, low-pass-only guidance may over-preserve, limiting edit flexibility.

### Mechanism 3: Training-Free Exploitation of Pretrained Video Priors
- **Claim**: Pretrained T2V models contain sufficient spatial-temporal priors to enable coherent editing without sample-specific optimization.
- **Mechanism**: DDIM inversion produces an approximate source trajectory. The dual-branch strategy uses this trajectory to enforce consistency while the pretrained model generates edited content.
- **Core assumption**: The base T2V model's priors generalize to the input video and edit type, including motion changes.
- **Evidence anchors**: ~3 min/video vs. >15 min for optimization-based methods; competitive CLIP/OSV/Mask-PSNR/LPIPS on DAVIS.
- **Break condition**: Edits requiring domain-specific knowledge not in pretraining, or heavy occlusions demanding advanced temporal reasoning, may exceed model priors.

## Foundational Learning

- **Concept**: DDIM sampling and inversion
  - Why needed here: Core to dual-branch strategy—inversion obtains source trajectory; sampling generates edited video with guidance.
  - Quick check question: Why does DDIM inversion introduce reconstruction errors, and how does spectrum-guided modulation mitigate them?

- **Concept**: 3D Discrete Fourier Transform (3D DFT)
  - Why needed here: Enables frequency-domain analysis of spatial and temporal content for modulation.
  - Quick check question: What spatial and temporal features does the low-frequency spectrum capture in video tokens?

- **Concept**: Attention mechanisms in transformers
  - Why needed here: Understanding diagonal vs. distributed attention patterns underpins block factorization.
  - Quick check question: What do diagonal patterns in temporal attention maps indicate about inter-frame correspondence?

## Architecture Onboarding

- **Component map**: Input video → 3D VAE encoder → latent z_0 → DDIM inversion → trajectory {z*_t} → Sampling branch with spectrum guidance → Modified DDIM with sketching blocks → Sharpening blocks → VAE decoder → edited video

- **Critical path**:
  1. Invert source video to latent trajectory
  2. During sampling steps t ∈ [0, 0.6T], compute spectrum guidance from sketching blocks
  3. Apply modulated DDIM step: z_{t-1} = DDIM(...) - λ·Norm(∇_{z_t} G_t)
  4. Decode final latent to output video

- **Design tradeoffs**:
  - Number of sketching blocks: 4 blocks empirically optimal; more improves preservation but reduces edit flexibility and increases compute
  - Guidance weight λ ∈ [10, 15] balances preservation vs. text alignment
  - Low-pass filter cutoff (~2/3 spectrum) trades structure preservation for edit flexibility

- **Failure signatures**:
  - Incorrect object orientation: Guidance lacks directional constraints
  - Artifacts under heavy occlusion: Base model priors insufficient for complex temporal reasoning
  - Text-video misalignment: λ too high or low-pass filter too restrictive

- **First 3 experiments**:
  1. Visualize attention outputs and spectra across blocks to confirm frequency progression
  2. Ablate sketching block count {2, 4, 6, 8} and measure CLIP/OSVR/M.PSNR/LPIPS
  3. Sweep λ ∈ {5, 10, 12, 15, 20} to characterize preservation vs. editability trade-off

## Open Questions the Paper Calls Out

- **Open Question 1**: How can the framework be improved to effectively handle video editing scenarios involving heavy object occlusions?
  - Basis in paper: Explicitly stated in Limitations section (4.3) and Supplementary Material (F) that the method "struggles with heavy occlusions requiring advanced temporal reasoning."
  - Why unresolved: The current methodology relies on attention patterns and frequency modulation which may fail to maintain object permanence or coherence when visual features are temporarily obscured.
  - What evidence would resolve it: Qualitative and quantitative results on benchmark datasets featuring frequent occlusions, showing maintained object identity and structure.

- **Open Question 2**: How can the sampling trajectory be constrained to prevent random variations in generated object orientation?
  - Basis in paper: Supplementary material notes "incorrect object orientation" arises due to the "absence of specific constraints to guide object placement."
  - Why unresolved: While the spectrum-guided modulation preserves low-frequency structural content, it lacks semantic spatial constraints during the generative denoising process.
  - What evidence would resolve it: A modified guidance mechanism that successfully preserves directional poses in the edited output relative to the source video.

- **Open Question 3**: Does the frequency-aware factorization strategy generalize to other T2V architectures?
  - Basis in paper: Implementation details specifically designate the "first 4 blocks" as sketching blocks for CogVideoX, but it is unclear if this specific block-level frequency distribution is universal or model-dependent.
  - Why unresolved: The distinct separation between "sketching" (low-frequency) and "sharpening" (high-frequency) blocks is empirically observed in one specific DiT architecture.
  - What evidence would resolve it: Analysis of attention patterns and successful application of the specific block-index factorization on diverse T2V architectures.

## Limitations

- The method struggles with heavy object occlusions requiring advanced temporal reasoning, as base model priors may be insufficient for complex occlusion scenarios.
- Performance depends on pretrained T2V model generalization, which may fail for domain shifts or edits requiring specialized knowledge not in pretraining.
- Spectrum-guided modulation parameters (4 sketching blocks, 2/3 frequency cutoff, λ=10-15) are empirically tuned without theoretical guarantees, and exact implementation details are not fully specified.

## Confidence

- **High Confidence**: The frequency-aware block factorization mechanism and its role in separating spatial-temporal structure from detail refinement is well-supported by attention pattern analysis and ablation studies.
- **Medium Confidence**: The spectrum-guided modulation approach and its impact on preserving structure while enabling edits is validated empirically but depends on specific filter parameters not fully specified.
- **Low Confidence**: Claims about the auxiliary guidance term and exact implementation details (low-pass filter cutoff, attention extraction) lack sufficient detail for full reproduction.

## Next Checks

1. **Ablation of Sketching Block Count**: Systematically vary the number of sketching blocks (2, 4, 6, 8) and measure CLIP, OSV, Mask-PSNR, and LPIPS to confirm 4 blocks is optimal across diverse video types.

2. **Low-Pass Filter Sensitivity**: Test different low-pass filter cutoffs (e.g., 1/2, 2/3, 3/4 of frequency spectrum) to quantify trade-offs between structure preservation and edit flexibility.

3. **Cross-Dataset Generalization**: Evaluate FADE on additional video datasets (e.g., Kinetics, Something-Something) with varying motion complexity to test model prior robustness beyond DAVIS.