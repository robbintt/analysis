---
ver: rpa2
title: 'Evaluating Hybrid Retrieval Augmented Generation using Dynamic Test Sets:
  LiveRAG Challenge'
arxiv_id: '2506.22644'
source_url: https://arxiv.org/abs/2506.22644
tags:
- retrieval
- question
- hybrid
- questions
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The LiveRAG Challenge 2025 evaluated retrieval-augmented generation
  (RAG) systems on dynamic test sets using the FineWeb-10BT corpus. This paper presents
  a hybrid approach combining sparse (BM25) and dense (E5) retrieval with Falcon3-10B-Instruct
  for answer generation.
---

# Evaluating Hybrid Retrieval Augmented Generation using Dynamic Test Sets: LiveRAG Challenge

## Quick Facts
- arXiv ID: 2506.22644
- Source URL: https://arxiv.org/abs/2506.22644
- Reference count: 17
- Key outcome: 4th place in faithfulness and 11th place in correctness among 25 teams in LiveRAG Challenge 2025 using hybrid BM25+E5 retrieval with Falcon3-10B-Instruct

## Executive Summary
This paper presents a hybrid retrieval-augmented generation system that combines sparse (BM25) and dense (E5) retrieval methods to address the LiveRAG Challenge 2025. The system achieved strong performance with 4th place in faithfulness and 11th place in correctness among 25 competing teams. Key findings include that neural re-ranking with RankLLaMA significantly improves retrieval quality (MAP from 0.523 to 0.797) but introduces prohibitive computational costs, while DSPy-optimized prompting achieves higher semantic similarity at the risk of over-confidence. The study identifies vocabulary alignment between questions and documents as the strongest predictor of performance.

## Method Summary
The system employs hybrid retrieval combining OpenSearch BM25 and Pinecone E5-base-v2 embeddings, retrieving 30 documents from each index and fusing them via normalized score combination to select the top 10 documents. Documents are processed into 512-token chunks using a sentence-aware splitter. Answer generation uses Falcon3-10B-Instruct with temperature 0.6, top_p 0.9, and 200-token limit. Optional components include RankLLaMA-7B for neural re-ranking and DSPy for prompt optimization using BootstrapFewShot with 160/40 train/validation splits. The system was evaluated on FineWeb-10BT corpus using DataMorgana's synthetic questions and real-time test sets during the competition.

## Key Results
- Neural re-ranking with RankLLaMA improved MAP from 0.523 to 0.797 (52% relative improvement)
- DSPy-optimized prompting achieved higher semantic similarity (0.771 vs 0.668 baseline) but showed 0% refusal rates
- Vocabulary alignment was strongest predictor of performance: document-similar phrasing achieved 0.762 cosine similarity vs 0.562 for distant phrasing
- System achieved 4th place in faithfulness and 11th place in correctness among 25 teams

## Why This Works (Mechanism)

### Mechanism 1: Hybrid Retrieval Fusion
Combining sparse (BM25) and dense (E5) retrieval creates a more robust foundation for downstream generation than either method alone. Sparse retrieval handles exact term matching for specific factual queries while dense retrieval captures semantic similarity. Normalized score fusion selects top documents from both streams, providing complementary signals for re-ranking or generation. Core assumption: sparse and dense methods return partially overlapping but not identical result sets, with each capturing unique relevant documents.

### Mechanism 2: Neural Re-ranking Quality Improvement
Generative re-ranking provides substantial retrieval quality gains but at computational costs that may be prohibitive for production time constraints. RankLLaMA performs pointwise scoring where each passage is independently evaluated against the query using a generative model, providing more nuanced relevance assessment than lexical or embedding similarity alone. Core assumption: the re-ranker's training distribution generalizes to the target corpus (FineWeb-10BT).

### Mechanism 3: Vocabulary Alignment Effect
Query-document vocabulary alignment is the strongest predictor of RAG performance in this system. When query terminology matches document terminology, both sparse (exact term matching) and dense (embedding similarity) retrieval benefit simultaneously, with compounding effects on downstream generation quality. Core assumption: the embedding model (E5) preserves vocabulary-level similarity signals in its semantic space.

## Foundational Learning

- **Sparse vs Dense Retrieval Characteristics**
  - Why needed here: The hybrid architecture requires understanding when BM25 excels (exact term matching) versus when E5 excels (semantic similarity) to diagnose retrieval failures.
  - Quick check question: Given a paraphrased query with no overlapping keywords, which retrieval method would you expect to perform better and why?

- **Refusal Rate as Calibration Signal**
  - Why needed here: DSPy-optimized prompts achieved 0% refusal rates, which was interpreted as over-confidence rather than perfect calibration—a critical distinction for trustworthy RAG.
  - Quick check question: If two systems achieve identical semantic similarity scores but one has 0% refusal and the other has 15% refusal, which would you deploy and why?

- **Pointwise vs Listwise Re-ranking**
  - Why needed here: The paper uses pointwise re-ranking (each passage scored independently); understanding this distinction is necessary for selecting appropriate re-ranking strategies.
  - Quick check question: Why might pointwise re-ranking be more robust than listwise re-ranking, and what efficiency tradeoff does it introduce?

## Architecture Onboarding

- **Component map**: Query → Parallel Sparse/Dense Retrieval → Score Fusion → (Optional: Re-ranking) → Context Assembly → Answer Generation
- **Critical path**: Query → BM25(k=30) and E5(k=30) → Normalized score fusion → Top-10 selection → Falcon3-10B-Instruct generation
- **Design tradeoffs**:
  - Re-ranking: +52% MAP vs 48x latency increase (0.797 vs 0.523 MAP; 84s vs 1.74s)
  - DSPy prompting: +15% semantic similarity (0.771 vs 0.668) vs loss of refusal behavior (0% vs 17%)
  - Hybrid vs Sparse-only: Identical MAP (0.523) but potentially better downstream generation robustness
- **Failure signatures**:
  - doc2query enhancement underperformed baseline (MAP 0.321 vs 0.523)—likely domain mismatch with FineWeb corpus
  - Short search queries: 26.1% refusal rate vs 8.9% for verbose natural questions
  - Document-distant phrasing: 25.5% refusal vs 9.4% for document-similar phrasing
- **First 3 experiments**:
  1. Establish retrieval baselines: Compare sparse-only, dense-only, and hybrid on a held-out set (n=50+) measuring MAP, Recall@10, and downstream generation metrics (ROUGE, cosine similarity, refusal rate).
  2. Latency budget analysis for re-ranking: Measure RankLLaMA per-question latency at varying batch sizes to determine if batching can make it feasible for your production constraints.
  3. Vocabulary sensitivity test: Create matched question pairs differing only in document-similar vs document-distant phrasing to quantify the alignment effect on your specific corpus and user population.

## Open Questions the Paper Calls Out

- **Open Question 1**: How can the efficiency of generative re-rankers like RankLLaMA be improved to make them computationally feasible for large-scale, time-constrained RAG applications? Basis: The conclusion explicitly encourages "future work to focus on strategies to increase efficiency of generative re-rankers like RankLLaMA." Unresolved because while RankLLaMA provided a 52% relative improvement in MAP, the authors found it computationally prohibitive (84s/query), forcing them to use a weaker baseline for the competition deadline.

- **Open Question 2**: Can adaptive retrieval strategies successfully adjust computational overhead based on real-time question complexity analysis? Basis: The authors state that "Future work could also focus on adaptive retrieval strategies that adjust computational overhead based on question complexity." Unresolved because the current system relies on a static hybrid approach, whereas performance varied significantly (0.479 to 0.822 cosine similarity) across different question types, suggesting fixed resource allocation is suboptimal.

- **Open Question 3**: How can automated prompt optimization frameworks like DSPy be regularized to prevent over-confidence and maintain necessary refusal behaviors? Basis: The paper notes that DSPy-optimized prompts achieved 0% refusal rates, raising "concerns about over-confidence and generalizability" which resulted in the authors rejecting the optimized prompts for submission. Unresolved because maximizing semantic similarity metrics during optimization appears to come at the cost of calibration, causing models to answer even when evidence is insufficient.

## Limitations

- The 52% MAP improvement from re-ranking comes at prohibitive computational cost (84s vs 1.74s per question), limiting practical deployment
- DSPy optimization achieving 0% refusal rates while maintaining high semantic similarity suggests potential over-confidence rather than perfect calibration
- Vocabulary alignment effect, while statistically significant, may reflect artifact rather than causal mechanism

## Confidence

- **High Confidence**: Hybrid retrieval combining BM25 and E5 provides complementary coverage; computational cost-benefit analysis of re-ranking is empirically validated; vocabulary alignment correlates strongly with performance metrics.
- **Medium Confidence**: The mechanism by which DSPy-optimized prompts eliminate refusals without degrading semantic similarity; the extent to which corpus-specific findings generalize to other domains.
- **Low Confidence**: That vocabulary alignment represents a causal mechanism rather than correlation artifact; that DSPy's 0% refusal rates indicate over-confidence rather than optimal calibration.

## Next Checks

1. **Domain Transfer Test**: Replicate the hybrid retrieval architecture on a different corpus (e.g., biomedical or legal) to validate whether the 52% MAP improvement from re-ranking holds across domains.

2. **Calibration Assessment**: Design experiments to distinguish between optimal calibration (correct 0% refusal) and over-confidence (inappropriate 0% refusal) by measuring error rates on questions with known incorrect answers.

3. **Vocabulary Alignment Causality**: Conduct controlled experiments varying query phrasing while holding document content constant to determine whether vocabulary alignment causally improves retrieval performance or merely correlates with other factors.