---
ver: rpa2
title: Boosting Adversarial Transferability via Residual Perturbation Attack
arxiv_id: '2508.05689'
source_url: https://arxiv.org/abs/2508.05689
tags:
- adversarial
- respa
- gradient
- attack
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel attack method, Residual Perturbation
  Attack (ResPA), for improving adversarial transferability in black-box settings.
  The key insight is that existing methods often search for perturbed points in overly
  sharp regions of the loss landscape, which hinders transferability.
---

# Boosting Adversarial Transferability via Residual Perturbation Attack

## Quick Facts
- **arXiv ID:** 2508.05689
- **Source URL:** https://arxiv.org/abs/2508.05689
- **Reference count:** 40
- **Primary result:** Proposes ResPA, a novel attack method that uses residual gradients to improve adversarial transferability in black-box settings, outperforming state-of-the-art methods across multiple model architectures.

## Executive Summary
This paper introduces Residual Perturbation Attack (ResPA), a novel method for generating adversarial examples with improved transferability in black-box settings. The key insight is that existing methods often overfit to sharp regions of the loss landscape, hindering transferability to different models. ResPA addresses this by using the residual gradient—the difference between the current and historical gradients—as the perturbation direction, effectively guiding the search toward flatter regions that generalize better across models.

The method employs exponential moving average to maintain a reference gradient from historical gradients and uses the residual gradient to maximize the loss function while flattening the loss surface. Extensive experiments demonstrate that ResPA significantly outperforms existing state-of-the-art transfer-based attack methods across multiple models, including convolutional neural networks and vision transformers. The approach also shows improved performance when combined with input transformation techniques and achieves superior results against advanced defense mechanisms.

## Method Summary
ResPA generates adversarial examples by computing the residual gradient (current gradient minus historical EMA gradient) and using this as the perturbation direction. The method samples N neighboring points around the current adversarial example to stabilize gradient estimation, computes the residual gradient to avoid sharp regions, and optimizes a combined objective that balances maximizing loss while maintaining flatness. The attack uses momentum to smooth the optimization process and employs a regularization term that encourages the loss surface to remain flat in the perturbation direction. Key hyperparameters include the EMA decay factor θ, the regularization coefficient γ, and the number of sampled neighbors N.

## Key Results
- ResPA achieves significantly higher attack success rates (ASR) than state-of-the-art transfer-based methods across 8 different model architectures
- The method demonstrates superior performance against advanced defense mechanisms including HGD, NRP, FD, and RS
- Visualization confirms ResPA adversarial examples reside in broader, smoother flat areas of the loss landscape compared to baselines
- ResPA shows improved transferability when combined with input transformation techniques

## Why This Works (Mechanism)

### Mechanism 1: Residual Gradient Suppression of Sharp Regions
- **Claim:** Using the residual gradient (current minus historical gradient) as the search direction prevents the attack from overfitting to locally sharp regions of the loss landscape.
- **Mechanism:** Standard methods use the immediate gradient to find the next perturbed point, which often lies in the "sharpest" direction. By subtracting the EMA of historical gradients, ResPA suppresses abrupt local fluctuations, steering the perturbation toward directions that represent global changes rather than local noise.
- **Core assumption:** Adversarial transferability is hindered when perturbations overfit to sharp local maxima specific to the surrogate model.
- **Evidence anchors:** [abstract] "ResPA... uses the residual gradient—the difference between the current and historical gradients—as the perturbation direction." [section 2.2.1] Eq. (6) defines $g^{res}$ as the difference $\nabla J - M$, explicitly to capture changes in global direction.

### Mechanism 2: EMA Reference Gradient Provides Stable Baseline
- **Claim:** A reference gradient derived from Exponential Moving Average (EMA) provides a stable baseline that dampens the impact of high-curvature (sharp) regions.
- **Mechanism:** In sharp regions, gradient direction changes rapidly. The EMA ($M_{t+1}$) aggregates history and does not fluctuate violently with the current gradient. Consequently, the residual ($g_{res}$) becomes small in sharp regions (suppressing the perturbation) and more significant in flat regions, effectively prioritizing flat areas for the attack.
- **Core assumption:** Flat regions of the loss landscape on the surrogate model correlate with transferable features on the target model.
- **Evidence anchors:** [section 2.2.2] "When the current gradient... exhibits large fluctuations, the residual gradient... can effectively suppress the current gradient... avoiding searching for perturbation points in the sharpest regions." [section 3.8] Visualization confirms ResPA adversarial examples reside in "broader and smoother flat areas."

### Mechanism 3: Flatness Regularization as Objective Term
- **Claim:** Treating the flatness constraint as a regularization term allows the attack to maximize loss while explicitly enforcing surface flatness, rather than implicitly hoping for it.
- **Mechanism:** ResPA modifies the objective function $L$ to balance maximizing the standard loss $J$ and the loss at the residual-perturbed point $x^*$. This forces the optimization to find points where the loss is not only high but remains high in the residual direction, mathematically encouraging a flat local surface.
- **Core assumption:** A weighted combination ($\gamma$) of current loss and perturbed-point loss better approximates the ideal flat maximum than optimizing either alone.
- **Evidence anchors:** [section 2.2.2] Eq. (8) defines the objective $L = (1-\gamma)J + \gamma J(x^*)$. [section 3.7] Hyper-parameter analysis shows $\gamma \in [0.2, 0.9]$ yields stable high transferability.

## Foundational Learning

### Concept: Transfer-based Black-box Attacks
- **Why needed here:** The paper assumes the attacker has no access to the target model. Understanding the "surrogate-to-target" gap is essential to grasp why "flatness" is the proxy for transferability.
- **Quick check question:** Why does high attack success on a surrogate model not guarantee success on a target model?

### Concept: Loss Landscape Sharpness vs. Flatness
- **Why needed here:** The core hypothesis relies on "flat" maxima generalizing better across models than "sharp" maxima. You must understand curvature to interpret the visualization results.
- **Quick check question:** In the context of this paper, does a "sharp" region imply high or low transferability?

### Concept: Exponential Moving Average (EMA)
- **Why needed here:** EMA is the mathematical tool used to create the "reference gradient." Without understanding how EMA smooths time-series data (gradients over iterations), the definition of "residual gradient" is opaque.
- **Quick check question:** How does the decay factor $\theta$ in EMA affect the sensitivity of the reference gradient to sudden gradient changes?

## Architecture Onboarding

### Component map:
- **Input** -> **Sampler** -> **Gradient History** -> **Residual Calculator** -> **Optimizer** -> **Adversarial example**

### Critical path:
Generating the adversarial example requires iterating T times. In each step, you must compute the current gradient, update the EMA reference, calculate the residual, determine the perturbed point $x^*$, compute the regularization loss, and update the image.

### Design tradeoffs:
- **Sampling (N):** Higher N (e.g., 20 vs 5) stabilizes the gradient but increases computational cost linearly.
- **Decay (θ):** High θ makes the reference gradient stale (more history); low θ makes it too sensitive to current noise.

### Failure signatures:
- **Stagnation:** If the residual gradient becomes zero (current equals history), the attack stops updating. This usually implies the optimizer is stuck in a flat basin or a cycle.
- **Overfitting:** High success on surrogate (>99%) but low success on target (<40%) indicates the flatness regularization (Mechanism 3) is failing or γ is too low.

### First 3 experiments:
1. **Hyper-parameter Sweep:** Vary γ (regularization) and θ (history decay) on a single surrogate (Inc-v3) to find the "flatness" sweet spot before full evaluation.
2. **Loss Surface Visualization:** Replicate Figure 4. Plot the loss surface of ResPA vs. standard MI-FGSM to visually confirm the "flatness" claim. This is the strongest sanity check.
3. **Ablation on Components:** Test "ResPA without EMA" (just current gradient) vs. "ResPA without Residual" (just EMA direction) to isolate the contribution of the residual subtraction.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the hyperparameters θ (exponential decay factor) and γ (penalty coefficient) be adaptively adjusted during the iterative attack process to optimize transferability, rather than remaining fixed?
- **Basis in paper:** [inferred] Section 3.7 analyzes hyperparameters, showing that transferability improves as θ increases but drops sharply when γ > 0.9, suggesting a fixed value may be suboptimal across different iterations or surrogate models.
- **Why unresolved:** The paper establishes a "satisfactory" static interval for these parameters but does not explore a dynamic scheduling mechanism that could respond to the local loss landscape during optimization.
- **What evidence would resolve it:** Experiments showing that an adaptive schedule for γ or θ (e.g., based on loss convergence) yields higher Attack Success Rates (ASR) than the static defaults (θ=0.6, γ=0.6) across diverse target models.

### Open Question 2
- **Question:** Is there a formal theoretical bound that mathematically connects the magnitude of the "residual gradient" to the sharpness of the loss landscape?
- **Basis in paper:** [inferred] Section 2.2.2 provides an intuitive explanation claiming the residual gradient suppresses sharp regions, supported by visualization (Fig. 4), but lacks a formal theorem or proof.
- **Why unresolved:** The paper relies on empirical observation and geometric intuition rather than deriving a provable relationship between the residual perturbation direction and the curvature of the loss function.
- **What evidence would resolve it:** A theoretical derivation defining the specific conditions under which the residual gradient direction guarantees convergence to a flat maximum, or a mathematical proof linking the residual norm to the Hessian spectrum.

### Open Question 3
- **Question:** What is the precise computational trade-off between the gain in transferability and the increased overhead of sampling N points and maintaining gradient moments?
- **Basis in paper:** [inferred] Section 3.7 notes that transferability continues to increase with the sampling number N (stabilizing only when N > 25), implying a significant computational cost for optimal performance.
- **Why unresolved:** While the paper demonstrates superior attack success rates, it does not report the wall-clock time or floating-point operations (FLOPs) required relative to baselines like MI or VMI.
- **What evidence would resolve it:** A comparative efficiency analysis detailing the time-to-attack and memory footprint required for ResPA to achieve a specific ASR compared to existing state-of-the-art methods.

## Limitations
- The core claim relies heavily on the assumption that flat regions correlate with cross-model generalizability, which is not rigorously proven.
- The choice of EMA parameters (θ=0.6) appears tuned for specific models, and the decay factor's sensitivity is not extensively explored.
- The method's performance on non-ImageNet datasets and real-world applications remains untested.
- The paper does not address computational overhead from sampling N=5 points per iteration compared to single-point methods.

## Confidence
- **High confidence:** Experimental results showing ResPA outperforming baselines on standard ImageNet transferability benchmarks across diverse architectures (CNNs and Vision Transformers).
- **Medium confidence:** The theoretical mechanism linking residual gradients to flat regions and improved transferability, as supported by visualization but not rigorously proven.
- **Medium confidence:** Claims about robustness against advanced defenses, though evaluated on a limited set of known techniques.

## Next Checks
1. **Cross-Dataset Generalization:** Test ResPA on non-ImageNet datasets (e.g., CIFAR-10, Places365) to validate whether flatness correlates with transferability beyond the training domain.
2. **Ablation of EMA Parameters:** Systematically vary θ and N to determine sensitivity to history length and sampling noise, isolating the contribution of each component.
3. **Real-World Defense Testing:** Evaluate against practical black-box defenses (e.g., input preprocessing, model ensembles in production) to assess real-world applicability beyond synthetic attacks.