---
ver: rpa2
title: 'AURA: A Diagnostic Framework for Tracking User Satisfaction of Interactive
  Planning Agents'
arxiv_id: '2505.01592'
source_url: https://arxiv.org/abs/2505.01592
tags:
- user
- agent
- task
- agents
- aura
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "AURA provides a domain-agnostic evaluation framework for task\
  \ planning agents by conceptualizing user satisfaction across behavioral stages,\
  \ moving beyond conventional task completion metrics. It defines five atomic LLM\
  \ evaluation criteria\u2014state consistency, tool efficiency, observation alignment,\
  \ policy adherence, and task completion\u2014aligned with the POMDP paradigm."
---

# AURA: A Diagnostic Framework for Tracking User Satisfaction of Interactive Planning Agents

## Quick Facts
- arXiv ID: 2505.01592
- Source URL: https://arxiv.org/abs/2505.01592
- Reference count: 40
- Primary result: AURA reveals that different models excel at different satisfaction stages, showing that task completion alone poorly predicts user satisfaction.

## Executive Summary
AURA is a diagnostic framework for evaluating interactive planning agents that moves beyond conventional task completion metrics to track user satisfaction through five atomic LLM evaluation criteria. By decomposing agent evaluation along POMDP components (states, actions, observations, policy, reward), AURA enables targeted diagnosis of satisfaction-relevant failures. Experimental results show that different models excel at different behavioral stages, revealing a disconnect between task completion and overall user satisfaction. Human studies confirm that intermediate behaviors strongly influence satisfaction, with AURA better capturing these nuances.

## Method Summary
AURA evaluates task planning agents through five atomic criteria aligned with POMDP components: state consistency (checking alignment between intermediate reasoning and user requests), tool efficiency (measuring unnecessary or failed API calls), observation alignment (verifying retrieved entities match user needs), policy adherence (tracking compliance with defined rules), and task completion (goal achievement). Four criteria use LLM-as-judge with boolean outputs, while task completion leverages existing benchmark metrics. The framework provides both individual metric scores and an average score, enabling diagnosis of specific failure modes across the agent's decision-making pipeline.

## Key Results
- Different models excel at different behavioral stages - DeepSeek-v3 achieves top state consistency (.81) while Qwen2.5-72B leads in task completion (.38).
- Human studies show AURA average score independently predicts user satisfaction beyond task completion (p=0.049), confirming satisfaction-completion decoupling.
- Agent mixing experiments demonstrate that substituting stronger intermediate-understanding agents can improve overall performance.
- Analysis reveals systematic failure patterns: state consistency failures occur when agents reason correctly but execute wrong actions, while observation alignment failures involve retrieving correct information but presenting it incorrectly.

## Why This Works (Mechanism)

### Mechanism 1: POMDP-Aligned Behavioral Decomposition
Decomposing evaluation along POMDP components enables targeted diagnosis of satisfaction-relevant failures. Each metric maps to a decision pipeline stage - state consistency checks intermediate reasoning alignment, tool efficiency penalizes unnecessary API calls, observation alignment verifies retrieved entities match needs, policy adherence tracks rule compliance, and task completion captures goal achievement. This separation isolates where agents fail. The core assumption is that user satisfaction emerges cumulatively across behavioral stages, not solely from final outcomes. Evidence shows this decomposition successfully identifies stage-specific weaknesses across different models.

### Mechanism 2: LLM-as-Judge with Atomic Boolean Criteria
Boolean LLM evaluations targeting single attributes yield more reliable diagnostic signals than holistic assessments. Each metric uses an IsConsistent/IsAligned/IsAdherent function implemented as LLM prompts with binary outputs. For example, observation alignment checks whether each retrieved entity matches conversational context. Aggregating across turns/steps produces scalar scores. The core assumption is that LLM judges can reliably detect specific failure modes when evaluation scope is narrow and criteria are explicit. The framework employs a single model (llama-3.3-70B) for consistency, though ensemble approaches may reduce bias.

### Mechanism 3: Satisfaction-Completion Decoupling
Task completion is necessary but insufficient for predicting user satisfaction; intermediate behavioral quality provides independent predictive signal. Human study compares preference between agent pairs controlling for R vs. AURA average. When R is held equal, higher AURA average predicts preference (p=0.049). When AURA average is equal, higher R still predicts preference (p=0.000). Both contribute independently. The core assumption is that user satisfaction is a multi-factor construct where process quality and outcome quality have separable effects. This challenges conventional evaluation that relies primarily on task completion metrics.

## Foundational Learning

- **Partially Observable Markov Decision Process (POMDP)**
  - Why needed here: AURA's theoretical foundation; agents act under partial observability of user intent/satisfaction.
  - Quick check question: Can you explain why task planning is naturally modeled as POMDP rather than full MDP?

- **LLM-as-Judge Evaluation**
  - Why needed here: All AURA metrics except R use LLM evaluators; understanding strengths/limitations is critical for implementation.
  - Quick check question: What makes boolean criteria more reliable than open-ended LLM evaluations according to the paper?

- **Agent Pipeline Decomposition**
  - Why needed here: AURA requires separating understanding, tool-calling, response generation, and policy-checking stages.
  - Quick check question: Which AURA metric would catch an agent that retrieves correct information but presents it in a way that contradicts earlier statements?

## Architecture Onboarding

- **Component map**: Input (dialogue history, states, tool traces, policy) -> State Consistency Module -> Tool Efficiency Module -> Observation Alignment Module -> Policy Adherence Module -> Task Completion Module -> Aggregation (individual scores + average)

- **Critical path**: Start with existing benchmark -> extract dialogue logs + tool traces -> implement Tool Efficiency (deterministic) first -> add LLM-judge modules for S/O/P -> validate against human annotations -> compare R vs. AURA average correlation with satisfaction.

- **Design tradeoffs**:
  - Single LLM judge vs. ensemble: Paper uses single model for consistency; ensemble may reduce bias but increases cost/complexity.
  - Turn-level vs. session-level evaluation: State consistency and observation alignment are turn-level; policy adherence is session-level. Aggregation strategy affects sensitivity to intermittent failures.
  - Benchmark-specific R vs. unified R: Paper retains existing completion metrics for compatibility; tradeoff is cross-benchmark comparability.

- **Failure signatures**:
  - State Consistency failure: Agent reasons correctly but executes wrong action (e.g., calls get_order_details when user asked about product availability).
  - Tool Efficiency failure: Premature tool calls before gathering required parameters; generates hallucinated values.
  - Observation Alignment failure: Agent claims refund processed to gift card when user specified certificate.
  - Policy Adherence failure: Skips required step (e.g., asking for user ID before trip details).

- **First 3 experiments**:
  1. **Baseline correlation check**: Run AURA on your agent across TravelPlanner and Ï„-Bench; compute correlation between R and AURA average to quantify decoupling.
  2. **Ablation by metric**: Remove one metric at a time and re-run human preference study to validate each metric's independent contribution.
  3. **Agent mixing pilot**: Test whether substituting a stronger intermediate-understanding agent (per AURA S/A scores) improves overall performance, following Section 5.2 methodology.

## Open Questions the Paper Calls Out
None

## Limitations
- Framework assumes consistent LLM judge behavior across criteria, though evaluation reliability for boolean tasks remains understudied.
- Study focuses on retail/travel domains, limiting generalizability to other interactive planning contexts.
- Human study sample size (21 participants) constrains statistical power for preference comparisons.

## Confidence

- POMDP-aligned decomposition mechanism: **High** - Strong theoretical grounding and clear empirical support for stage-specific failure detection.
- LLM-as-judge reliability: **Medium** - Boolean criteria show promise, but limited validation against human annotations and no comparison with alternative evaluation approaches.
- Satisfaction-completion decoupling: **Medium** - Human study shows independent effects, but controlled conditions and sample size limit generalizability.

## Next Checks
1. **Cross-domain generalization**: Apply AURA to non-retail domains (healthcare scheduling, technical support) to test metric validity beyond current scope.
2. **Judge consistency validation**: Compare AURA scores across multiple LLM judges and human annotators to quantify inter-rater reliability for each atomic criterion.
3. **Longitudinal performance tracking**: Evaluate whether AURA scores correlate with user satisfaction changes over multiple interaction sessions rather than single encounters.