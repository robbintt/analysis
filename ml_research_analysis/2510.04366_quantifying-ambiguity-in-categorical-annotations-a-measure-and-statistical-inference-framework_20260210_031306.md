---
ver: rpa2
title: 'Quantifying Ambiguity in Categorical Annotations: A Measure and Statistical
  Inference Framework'
arxiv_id: '2510.04366'
source_url: https://arxiv.org/abs/2510.04366
tags:
- ambiguity
- distribution
- probability
- measure
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a novel scalar measure, amb(q), to quantify\
  \ aleatoric uncertainty in categorical soft labels. The measure extends quadratic\
  \ entropy by asymmetrically incorporating an explicit \u201Ccan\u2019t solve\u201D\
  \ category, separating uncertainty from class-level indistinguishability versus\
  \ task unresolvability."
---

# Quantifying Ambiguity in Categorical Annotations: A Measure and Statistical Inference Framework

## Quick Facts
- **arXiv ID**: 2510.04366
- **Source URL**: https://arxiv.org/abs/2510.04366
- **Reference count**: 40
- **Primary result**: Introduces amb(q), a scalar measure for aleatoric uncertainty in categorical soft labels that separates class-level indistinguishability from task unresolvability

## Executive Summary
This paper introduces amb(q), a novel scalar measure for quantifying ambiguity in categorical annotations. The measure extends quadratic entropy by asymmetrically incorporating an explicit "can't solve" category, providing interpretable probabilities of label disagreement or abstention. The authors develop a Bayesian Dirichlet-multinomial inference framework with closed-form expressions for expected value and variance, enabling principled separation of aleatoric from epistemic uncertainty. The work positions amb(q) as a practical baseline for data curation, benchmark stratification, and active learning workflows.

## Method Summary
The method defines ambiguity as the probability that two independent annotators disagree or that one abstains, expressed as amb = q_cs + (1-q_cs)·P(Y1≠Y2|Y1,Y2≠cs). This equals 1-(1/(1-q_cs))·Σq_k² for q_cs<1, and 1 for q_cs=1. The measure treats "can't solve" as a distinct source of ambiguity rather than distributing abstention mass among proper categories. For inference, given count vectors n over C categories plus abstention, the posterior follows Dir(α₀+n) under uniform prior β=1. Closed-form expressions for E[amb] and Var[amb] are derived analytically, allowing uncertainty quantification without sampling.

## Key Results
- The amb(q) measure provides interpretable probability values in [0,1] representing label-flip likelihood
- Bayesian inference under Dirichlet-multinomial yields closed-form posterior mean and variance for amb(q)
- Plug-in estimators systematically underestimate ambiguity compared to Bayesian approaches
- The measure separates aleatoric uncertainty (inherent task ambiguity) from epistemic uncertainty (finite sample effects)

## Why This Works (Mechanism)

### Mechanism 1: Label-Flip Probability Core
The ambiguity measure amb(q) captures the probability that two independent annotators disagree or that one abstains, providing an interpretable scalar in [0,1]. For probability vector q over C categories plus "can't solve" (cs), amb = q_cs + (1-q_cs)·P(Y1≠Y2|Y1,Y2≠cs). The second term equals 1-Σp_k² where p is the conditional probability vector over proper categories, which is the quadratic entropy (Gini impurity) of the non-abstention distribution. Core assumption: Annotators are statistically indistinguishable operators drawing from the same categorical distribution Cat(q).

### Mechanism 2: Asymmetric Abstention Separation
Explicitly modeling a "can't solve" category separates class-level indistinguishability from task-level unresolvability. The measure treats q_cs as a distinct source of ambiguity rather than distributing abstention mass among proper categories. This allows amb = 1 when q_cs = 1 (complete unsolvability) while amb < 1 for uniform distributions over proper categories alone. Core assumption: The "can't solve" option reflects genuine task unresolvability rather than annotator laziness.

### Mechanism 3: Dirichlet-Posterior Uncertainty Quantification
Bayesian inference under Dir(α₀+n) provides closed-form posterior mean and variance for amb(q), separating aleatoric ambiguity from epistemic uncertainty due to finite samples. Given observed counts n = (n₁,...,n_C, n_cs), the posterior over q is Dirichlet. Since amb is a function of q, E[amb(q)] and Var[amb(q)] can be computed analytically without sampling. This yields credible intervals that shrink as sample size increases. Core assumption: Dirichlet prior is appropriate; authors use β = 1 (uniform over simplex) as default.

## Foundational Learning

- **Concept: Dirichlet-Multinomial Conjugacy**
  - Why needed here: The entire inference framework relies on Dirichlet priors combining with multinomial likelihoods to yield Dirichlet posteriors analytically.
  - Quick check question: If you observe counts (5, 3, 2) with prior Dir(1, 1, 1), what is the posterior?

- **Concept: Quadratic Entropy / Gini Impurity**
  - Why needed here: The term 1-Σp_k² appears in the ambiguity definition; understanding this as "probability two random draws differ" makes the measure interpretable.
  - Quick check question: For a 3-class uniform distribution, what is the probability two independent draws yield different classes?

- **Concept: Aleatoric vs. Epistemic Uncertainty**
  - Why needed here: The paper's central contribution is separating inherent task ambiguity (aleatoric) from estimation uncertainty due to finite annotation samples (epistemic).
  - Quick check question: If you double the number of annotators, which type of uncertainty decreases?

## Architecture Onboarding

- **Component map**: Raw annotation responses Y₁,...,Y_R per task → Count vector n = (n₁,...,n_C, n_cs) → Posterior update: α = α₀ + n (Dirichlet parameters) → Ambiguity computation: E[amb], Var[amb] via closed-form expressions OR sampling from Dir(α) → Output: Point estimate + uncertainty interval per task

- **Critical path**: 1) Validate annotation schema includes explicit "can't solve" option; 2) Compute count vectors per item; 3) Choose prior (default: β = 1, uniform); 4) Apply Equations 14 and 22 for mean/variance; or sample if full posterior shape needed; 5) Filter/rank items by posterior mean or credible interval width

- **Design tradeoffs**: 
  - amb vs. amb̃ (modified): Original has direct probability interpretation; modified normalizes to reach 1 for uniform non-cs distributions but inflates values quickly
  - Prior β: Smaller β → broader prior, more epistemic uncertainty; larger β → prior concentrated toward higher ambiguity
  - Bayesian vs. plug-in: Plug-in underestimates ambiguity (negative bias); Bayesian estimators biased but consistent

- **Failure signatures**: 
  - Very few annotations (R < 5): Posterior extremely wide; credible intervals nearly span [0,1]
  - All responses in one category: Posterior collapses; amb = 0 if non-cs, amb = 1 if all cs
  - No "can't solve" responses observed: q_cs posterior still nontrivial; amb may be underestimated if unresolvable items exist but weren't labeled as such
  - Highly skewed annotation protocols: If annotators rarely use cs even when appropriate, unresolvability component is muted

- **First 3 experiments**:
  1. Synthetic validation: Generate data from known q vectors; verify posterior means converge to true amb as R increases; check coverage of 95% credible intervals
  2. Sensitivity to prior: For a fixed dataset, recompute posteriors with β ∈ {0.5, 1.0, 2.0}; quantify how ranking of items by ambiguity shifts
  3. Downstream utility: Split a real annotation dataset by posterior mean amb (low/medium/high); train classifiers on each split and compare calibration/error patterns—hypothesis: high-amb items yield noisier labels and less reliable models

## Open Questions the Paper Calls Out

### Open Question 1
Can the Bayesian inference framework be extended to incorporate informative or hierarchical priors that reflect historical annotator behavior rather than uniform assumptions? The authors explicitly acknowledge that "improved priors or more elaborate inference techniques are promising directions for future work, but lie beyond the scope of this paper." The current analysis relies primarily on a uniform Dirichlet prior (β=1), which influences posterior shape and bias but may not be optimal for sparse annotation data.

### Open Question 2
How does the ambiguity measure integrate with latent-class or latent-rater models that explicitly account for annotator heterogeneity? The Discussion states the Dirichlet-multinomial baseline "does not model rater-specific heterogeneity," positioning the measure as a complement to "richer approaches." The current formulation assumes annotators are statistically indistinguishable operators, ignoring potential variance in worker reliability or expertise.

### Open Question 3
To what extent does training on data filtered or weighted by this measure improve the calibration and robustness of downstream classifiers? The Abstract claims utility for "active learning" and "data curation," yet the paper focuses on measure definition and statistical inference rather than end-to-end ML experiments. The practical benefit of using the specific scalar ambiguity score for dataset curation is asserted but not empirically verified against downstream performance metrics.

## Limitations

- The exchangeability assumption may not hold with systematic rater effects or expertise differences
- The measure's interpretability depends on consistent and appropriate use of the "can't solve" option
- Prior strength β remains largely arbitrary without systematic guidance for selection

## Confidence

- **High confidence**: Mathematical derivation of amb(q) as label-flip probability and relationship to quadratic entropy
- **Medium confidence**: Bayesian inference framework provides useful uncertainty quantification
- **Low confidence**: Real-world utility claims for data curation and active learning lack systematic empirical validation

## Next Checks

1. Implement a simple hierarchical Bayesian model allowing annotator-specific distributions to test whether the exchangeability assumption significantly impacts ambiguity estimates on real multi-rater datasets.

2. Conduct a systematic grid search over β values on a standard annotation benchmark to quantify how ranking of items by ambiguity shifts and impacts downstream classifier performance.

3. Design an active learning experiment where models are trained on increasingly large subsets selected by posterior mean amb(q), comparing against random and entropy-based selection baselines to quantify practical benefits for reducing annotation costs while maintaining model quality.