---
ver: rpa2
title: 'LiveRAG: A diverse Q&A dataset with varying difficulty level for RAG evaluation'
arxiv_id: '2511.14531'
source_url: https://arxiv.org/abs/2511.14531
tags:
- question
- questions
- difficulty
- document
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The LiveRAG benchmark introduces a publicly available dataset of
  895 synthetic question-answer pairs for evaluating Retrieval-Augmented Generation
  (RAG) systems. Generated using the DataMorgana tool, questions cover diverse topics
  and varying difficulty levels, with each associated with difficulty and discriminability
  scores derived from an Item Response Theory (IRT) model trained on system performance
  data.
---

# LiveRAG: A diverse Q&A dataset with varying difficulty level for RAG evaluation

## Quick Facts
- arXiv ID: 2511.14531
- Source URL: https://arxiv.org/abs/2511.14531
- Reference count: 40
- Introduces a publicly available dataset of 895 synthetic question-answer pairs for evaluating Retrieval-Augmented Generation (RAG) systems

## Executive Summary
LiveRAG presents a novel benchmark for evaluating Retrieval-Augmented Generation systems through a carefully constructed dataset of 895 synthetic question-answer pairs. The dataset covers diverse topics and varying difficulty levels, with questions generated using the DataMorgana tool and associated with difficulty and discriminability scores derived from Item Response Theory (IRT) modeling. The benchmark includes ground-truth answers, supporting documents, and answer claims for comprehensive evaluation. The dataset demonstrates strong coverage across difficulty levels and shows that multi-document questions present significantly greater challenges than single-document queries.

## Method Summary
The LiveRAG dataset was generated using the DataMorgana tool to create synthetic questions covering diverse topics. Each question was assigned difficulty and discriminability scores through an IRT model trained on system performance data across multiple RAG implementations. The dataset includes supporting documents (primarily Wikipedia-based), ground-truth answers, and answer claims for evaluation purposes. Questions were categorized by document complexity (single vs multi-document) and difficulty levels were validated through correlation with actual system performance across different large language models. The linguistic diversity of the dataset was compared against popular QA benchmarks to demonstrate broader coverage.

## Key Results
- The dataset spans a wide difficulty range with strong correlation between IRT-based difficulty scores and actual system performance
- Multi-document questions show significantly higher difficulty compared to single-document questions
- The dataset exhibits greater linguistic diversity than popular QA benchmarks
- IRT-based difficulty scores demonstrate generalization across different LLM architectures

## Why This Works (Mechanism)
LiveRAG's effectiveness stems from its systematic approach to question generation and difficulty calibration. By using DataMorgana to generate synthetic questions across diverse topics, the benchmark ensures broad coverage of potential query types. The IRT-based scoring mechanism captures the relative difficulty of questions based on actual system performance patterns, creating a meaningful difficulty scale. The inclusion of both single and multi-document questions allows for nuanced evaluation of retrieval capabilities. The linguistic diversity exceeds that of existing benchmarks, providing more robust testing conditions for RAG systems.

## Foundational Learning
- **Item Response Theory (IRT)**: Statistical framework for modeling item difficulty and discriminability; needed to create meaningful difficulty scores that correlate with system performance; quick check: verify correlation coefficients between IRT scores and actual system accuracy
- **DataMorgana tool**: Synthetic data generation system for creating diverse question-answer pairs; needed to generate a large, varied dataset without relying on manual annotation; quick check: assess question diversity metrics across generated samples
- **Discriminability scoring**: Measure of how well a question distinguishes between high-performing and low-performing systems; needed to identify questions that effectively differentiate system capabilities; quick check: calculate correlation between discriminability scores and performance variance across systems
- **Multi-document retrieval complexity**: Understanding that questions requiring information from multiple documents are inherently more difficult; needed to create realistic evaluation scenarios; quick check: compare accuracy rates between single and multi-document question sets
- **Linguistic diversity metrics**: Measures of vocabulary, syntax, and semantic variation across questions; needed to ensure benchmark comprehensiveness; quick check: compare diversity indices against established QA benchmarks
- **Ground-truth verification**: Process of establishing correct answers through reliable sources; needed for accurate evaluation; quick check: validate answer accuracy through multiple independent sources

## Architecture Onboarding

**Component Map:**
DataMorgana -> Question Generation -> IRT Model Training -> Difficulty Assignment -> Benchmark Creation -> System Evaluation

**Critical Path:**
Question generation (DataMorgana) → IRT model training → Difficulty assignment → System evaluation → Performance analysis

**Design Tradeoffs:**
- Synthetic vs. real questions: Synthetic generation allows for controlled diversity but may miss real-world query patterns
- Wikipedia focus: Provides reliable ground truth but limits domain generalizability
- IRT-based scoring: Captures relative difficulty but assumes performance differences reflect question difficulty rather than system implementation variations

**Failure Signatures:**
- Poor correlation between IRT scores and system performance indicates issues with difficulty calibration
- Low discriminability scores suggest questions fail to differentiate system capabilities
- Limited linguistic diversity may indicate insufficient coverage of real-world query patterns

**3 First Experiments:**
1. Evaluate system performance correlation with IRT-based difficulty scores across 5+ different RAG implementations
2. Compare accuracy rates between single-document and multi-document question sets to validate complexity assumptions
3. Measure linguistic diversity metrics and compare against established QA benchmarks

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic question generation may not capture the full complexity of real user queries encountered in practice
- The dataset relies on Wikipedia-based documents, limiting applicability to domains with different retrieval characteristics
- IRT-based difficulty scoring may be influenced by system architecture and implementation choices rather than purely reflecting question difficulty

## Confidence
- Dataset creation and composition: **High** - Methodology is clearly documented and reproducible
- IRT difficulty scoring validity: **Medium** - Strong correlations exist but may be system-dependent
- Linguistic diversity claims: **Medium** - Comparative analysis provided but relies on specific metrics
- Generalization across domains: **Low** - Limited to Wikipedia-based documents and synthetic generation

## Next Checks
1. Test the IRT-based difficulty scores on a broader range of RAG systems, including those with different retrieval strategies, to assess score stability and generalizability
2. Evaluate the dataset with real user queries collected from actual RAG deployments to assess coverage and relevance to practical use cases
3. Extend benchmark evaluation to non-Wikipedia domains (e.g., scientific literature, news articles, technical documentation) to validate cross-domain applicability