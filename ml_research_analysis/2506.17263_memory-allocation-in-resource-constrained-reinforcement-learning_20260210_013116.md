---
ver: rpa2
title: Memory Allocation in Resource-Constrained Reinforcement Learning
arxiv_id: '2506.17263'
source_url: https://arxiv.org/abs/2506.17263
tags:
- agent
- memory
- learning
- plan
- reinforcement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper examines how memory-constrained agents must allocate\
  \ limited memory between building a world model and planning with it. The authors\
  \ formalize this as a trade-off between the memory for transition model estimation\
  \ (Np) and the memory for planning (N\u03C0), where Np + N\u03C0 = N."
---

# Memory Allocation in Resource-Constrained Reinforcement Learning

## Quick Facts
- arXiv ID: 2506.17263
- Source URL: https://arxiv.org/abs/2506.17263
- Reference count: 1
- Agents must allocate limited memory between building world models and planning with them, with peak performance at approximately equal splits.

## Executive Summary
This paper examines how memory-constrained agents must allocate limited memory between building a world model and planning with it. The authors formalize this as a trade-off between memory for transition model estimation (Np) and planning memory (Nπ), where Np + Nπ = N. Using MCTS-based agents in a MiniGrid environment, they find that performance peaks when Nπ ≈ Np ≈ N/2, with some asymmetry favoring planning. When data quality is poor, agents need more memory for planning to compensate for unreliable models. The authors also extend this analysis to DQN agents using a PT-DQN architecture, showing that optimal memory allocation between permanent and transient value functions can significantly improve learning performance. These results demonstrate that careful memory allocation is crucial for resource-constrained agents and can recover near-optimal performance even with severe memory limits.

## Method Summary
The paper formalizes memory allocation in resource-constrained RL as a trade-off between transition model memory (Np) and planning memory (Nπ), with total budget N = Np + Nπ. For MCTS experiments, agents use N=500 units of memory in MiniGrid CorridorEnv, varying Nπ while building transition models from datasets with different quality levels (optimal trajectories vs. random noise). The PT-DQN extension uses a 4-layer architecture with hidden units split between permanent Q(P) and transient Q(T) value functions, tested in a Jelly Bean World environment with non-stationary rewards. Performance is measured through average returns (MCTS) and per-step rewards (PT-DQN) across multiple seeds.

## Key Results
- In memory-constrained MCTS, performance peaks when Nπ ≈ Np ≈ N/2, with slight asymmetry favoring planning memory
- When training data contains more noise, optimal allocation shifts toward more planning memory to compensate for unreliable models
- PT-DQN with 10% permanent/90% transient split outperforms 50-50 split by 50% in per-step reward, achieving near-optimal performance with limited memory

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** When memory is constrained (N = 500 units), splitting memory approximately evenly between model estimation (Np̂) and planning (Nπ) yields peak performance, with slight asymmetry favoring planning.
- **Mechanism:** The trade-off creates a saddle point: too little model memory (Np̂ << N/2) produces unreliable transition estimates that mislead planning; too little planning memory (Nπ << N/2) prevents agents from computing action sequences long enough to reach goals. The inverse-U performance curves in Figure 2 emerge because both capacities must simultaneously clear their respective thresholds.
- **Core assumption:** The environment requires multi-step planning; single-step greedy policies would show different allocation optima.
- **Evidence anchors:**
  - [abstract]: "performance peaks when Nπ ≈ Np̂ ≈ N/2, with some asymmetry favoring planning"
  - [section: Results and Discussion]: "Performance roughly peaks when Nπ ≈ 250 ≈ Np̂. The curves are not symmetric, hinting that the plan may need more resources than the model estimate."
  - [corpus]: Weak direct support; neighboring papers address multi-agent resource allocation but not the internal model/planning split.
- **Break condition:** When the planning horizon required to reach any rewarding state exceeds Nπ even at maximum allocation (Nπ = N), the curve flattens at near-zero returns regardless of split.

### Mechanism 2
- **Claim:** Noisier training data shifts the optimal allocation toward more planning memory, as agents must explore more "dead branches" in the search tree to compensate for unreliable model estimates.
- **Mechanism:** Random transitions in the dataset D create spurious state-action-outcome associations in p̂. With more planning capacity, MCTS can roll out multiple trajectories and discover which paths actually reach goals, partially overriding bad model predictions through sampling.
- **Core assumption:** The agent cannot collect additional data; it must work with a fixed dataset D of size |D| that may exceed Np̂.
- **Evidence anchors:**
  - [section: Results and Discussion]: "If D contains a lot of random transitions, the agent may need more planning capacity to account for 'dead branches' in the MCTS tree."
  - [section: Results and Discussion]: Yellow line (Dronly, random-only data) always achieves minimum returns—"the agent may waste resources by remembering useless transitions."
  - [corpus]: No direct validation in neighboring papers; this compensation mechanism appears novel to this work.
- **Break condition:** If D contains no complete trajectory to any goal, no allocation strategy can recover positive returns (as shown by Dronly dataset).

### Mechanism 3
- **Claim:** For PT-DQN architectures under memory constraints, allocating most hidden units (≈90%) to the transient value function Q(T) rather than the permanent function Q(P) improves continual learning performance.
- **Mechanism:** In non-stationary environments with task-specific reward structures (e.g., swapping red/blue goal values), the transient function needs capacity to rapidly adapt, while the permanent function only needs enough capacity to encode slowly-changing cross-task knowledge. Oversizing Q(P) creates "memory debt" that constrains adaptation.
- **Core assumption:** Task structure varies such that some knowledge generalizes (permanent) while much is task-specific (transient); if all knowledge were task-specific, Q(P) ≈ 0 would be optimal.
- **Evidence anchors:**
  - [section: Question Two Results]: "50–50 PT-split proposed by Anand and Precup... only reaches 0.2 per-step reward, while the best split (yellow line; 10%...for the permanent value function) reaches 0.3 reward."
  - [section: Question Two Results]: "allocating only 10% of hidden units to the permanent value function...can almost equal the performance of the larger network."
  - [corpus]: Neighboring papers on continual learning and multi-agent RL do not address this PT-split allocation question.
- **Break condition:** When tasks share substantial structure (high transfer), a larger Q(P) allocation may become beneficial; the 10% optimum is environment-dependent.

## Foundational Learning

- **Concept: Monte Carlo Tree Search (MCTS)**
  - **Why needed here:** The paper uses MCTS as its primary model-based planning algorithm; understanding how MCTS allocates tree nodes (Nπ) vs. transition storage (Np̂) is essential to grasp the memory trade-off.
  - **Quick check question:** Can you explain why MCTS needs both a transition model and a tree structure to plan?

- **Concept: Model-Based vs. Model-Free RL**
  - **Why needed here:** The paper explicitly contrasts model-based approaches (MCTS with p̂) against value-based approaches (DQN); the PT-DQN extension bridges these paradigms by splitting value functions.
  - **Quick check question:** What information does a transition model provide that a pure value function does not?

- **Concept: Continual Learning and Catastrophic Forgetting**
  - **Why needed here:** The PT-DQN experiment addresses continual learning under non-stationary rewards; the permanent/transient split is a mechanism to balance stability (retaining cross-task knowledge) vs. plasticity (adapting to new task specifics).
  - **Quick check question:** Why would a standard DQN struggle when reward contingencies suddenly reverse?

## Architecture Onboarding

- **Component map:**
  ```
  [Dataset D] → [Memory Split Module] → Np̂ + Nπ = N
                    ↓                    ↓
            [MLE Model Builder]    [MCTS Planner]
            (stores Np̂ trans.)    (builds Nπ nodes)
                    ↓                    ↓
               [Transition Model p̂] ←→ [Policy π]
  ```
  For PT-DQN:
  ```
  [Experience Buffer] → [Q(P) Network] + [Q(T) Network] → Q = Q(P) + Q(T)
                     (permanent)        (transient)
  ```

- **Critical path:**
  1. Determine total memory budget N (hardware constraint, fixed at 500 in experiments)
  2. Select memory split based on expected data quality and task horizon
  3. Build model p̂ from available transitions (streaming, one-pass in MCTS setting)
  4. Execute planning using constrained tree size
  5. For PT-DQN: allocate hidden units between Q(P) and Q(T) based on task variability

- **Design tradeoffs:**
  | Decision | Options | Impact |
  |----------|---------|--------|
  | Nπ vs. Np̂ split | 0%–100% planning | Controls planning depth vs. model accuracy |
  | Dataset curation | Random, goal-directed, mixed | Determines minimum Np̂ needed for useful model |
  | PT-split ratio | 10%–50% permanent | Affects adaptation speed vs. cross-task retention |
  | Network architecture | [128,256,64,4] vs. larger | Constrained architectures force allocation decisions |

- **Failure signatures:**
  - **Flat low returns + high Nπ:** Np̂ insufficient; model too sparse to simulate transitions accurately
  - **Declining returns at high Nπ (near N):** Insufficient planning depth to reach any goal
  - **Zero returns regardless of split:** Dataset D contains no goal-reaching trajectories (Dronly scenario)
  - **Slow adaptation in PT-DQN:** Q(P) oversized; stale permanent knowledge interferes with transient learning

- **First 3 experiments:**
  1. **Replicate the Nπ sweep in MiniGrid CorridorEnv:** Fix N = 500, vary Nπ from 0 to 500 in steps of 50; use Doa dataset (trajectories to all goals); confirm inverse-U shape with peak near Nπ = 250.
  2. **Test data quality sensitivity:** Compare Nπ sweeps using Dra20 (20% random noise) vs. Doa (clean data); verify that noisier data shifts optimal Nπ rightward (more planning needed).
  3. **Validate PT-DQN split in Jelly Bean World:** Implement Q(P) with 10%, 25%, 50% of hidden units; compare per-step reward curves over 300k steps; confirm 10% split achieves ≈0.3 reward while 50% achieves ≈0.2.

## Open Questions the Paper Calls Out

- **How can a resource-constrained agent autonomously determine the optimal memory allocation between competing processes without exhaustive search?**
  - The paper evaluates performance across all Nπ and N̂p combinations but provides no mechanism for an agent to dynamically discover or adapt its allocation.

- **What formal definitions should govern what constitutes a "unit" of memory across different architectures?**
  - The paper deliberately abstracts from implementation details, but this limits cross-architecture comparison and practical deployment.

- **What mechanisms drive the asymmetric optimal allocation favoring planning memory (Nπ > N̂p)?**
  - The authors observe asymmetric U-shaped curves but only speculate about "dead branches" in MCTS trees without systematic testing.

- **Does the memory allocation trade-off generalize to partially observable settings (POMDPs)?**
  - The paper states the formalism "can easily be extended to POMDPs" but does not test this extension.

## Limitations

- The exact MCTS hyperparameters (simulations per step, UCB coefficient, rollout policy) are not specified, which may affect the shape and location of the performance peaks in the Nπ sweep.
- The buffer size of 52 for PT-DQN under N=500 memory constraint lacks explicit derivation, making it difficult to verify the memory accounting.
- The claim that optimal PT-split is ~10% permanent is based on a single environment (Jelly Bean World), limiting generalizability to other continual learning scenarios.

## Confidence

- **High confidence**: The general inverse-U relationship between Nπ and performance in MCTS (Mechanism 1) is well-supported by multiple experiments across different data qualities.
- **Medium confidence**: The data quality compensation mechanism (Mechanism 2) is theoretically sound but lacks direct empirical validation in the paper.
- **Medium confidence**: The PT-DQN memory allocation results (Mechanism 3) show clear performance differences but are demonstrated in only one environment with a specific task structure.

## Next Checks

1. **MCTS hyperparameter sensitivity**: Systematically vary the number of simulations per action and UCB constant to determine their effect on the optimal Nπ value.
2. **Cross-environment PT-split validation**: Test the PT-DQN architecture in multiple non-stationary environments with different levels of task similarity to determine if the 10% optimum generalizes.
3. **Memory accounting verification**: Implement exact parameter counting for the PT-DQN architecture to verify that the 50-50 split truly uses twice as much memory as the 10-90 split.