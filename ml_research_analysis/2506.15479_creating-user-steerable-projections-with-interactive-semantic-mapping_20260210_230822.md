---
ver: rpa2
title: Creating User-steerable Projections with Interactive Semantic Mapping
arxiv_id: '2506.15479'
source_url: https://arxiv.org/abs/2506.15479
tags:
- data
- projection
- projections
- labels
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a user-steerable projection framework that
  enables customizable, interpretable data visualizations via zero-shot classification
  with Multimodal Large Language Models (MLLMs). The core method fuses original data
  embeddings with textual label embeddings generated through Qwen-based zero-shot
  classification, allowing users to dynamically steer projections using natural-language
  guiding prompts.
---

# Creating User-steerable Projections with Interactive Semantic Mapping

## Quick Facts
- arXiv ID: 2506.15479
- Source URL: https://arxiv.org/abs/2506.15479
- Reference count: 40
- Primary result: Framework enables semantic steering of data projections via zero-shot MLLM classification

## Executive Summary
This paper presents a novel framework for creating user-steerable projections that combine traditional dimensionality reduction with interactive semantic mapping. The approach uses zero-shot classification via Multimodal Large Language Models (MLLMs) to generate semantic labels that can be fused with original data embeddings, allowing users to dynamically steer projections using natural language prompts. The method was evaluated across four datasets (AG News, MNIST, Fashion-MNIST, CIFAR-10) using three projection techniques (Isomap, t-SNE, UMAP), demonstrating enhanced cluster separation and the ability to create hierarchical groupings that reflect semantic relationships specified by prompts.

## Method Summary
The framework works by first generating semantic labels for data points through zero-shot classification using Qwen-based MLLMs. These textual labels are then embedded into the same semantic space as the original data using a pre-trained text encoder. The method fuses the original data embeddings with these semantic label embeddings through a weighted average, where the fusion weight controls the influence of semantic information on the final projection. Users can then steer the projection by providing natural language prompts that guide the semantic classification process, effectively controlling how data points are grouped and separated in the reduced dimensional space. The fused embeddings are then passed to standard dimensionality reduction techniques like Isomap, t-SNE, or UMAP to create the final visualization.

## Key Results
- Enhanced cluster separation compared to baseline projections across all tested datasets
- Projections successfully reflected semantic relationships specified by user prompts, including hierarchical groupings
- Quality metrics showed preservation of data structure: trustworthiness (0.77-0.99), continuity (0.93-0.99), and Shepard diagram correlation (0.38-0.87)
- Method captured qualitative attributes not explicitly encoded in the data, demonstrating flexibility for human-centered exploration

## Why This Works (Mechanism)
The method works by leveraging the semantic understanding capabilities of MLLMs to create interpretable groupings that go beyond purely geometric distance relationships in the original feature space. By fusing semantic embeddings with original data embeddings, the framework effectively adds a layer of semantic reasoning to the dimensionality reduction process. This allows users to guide the projection based on conceptual relationships rather than just numerical similarity, creating visualizations that align with human intuition about the data structure. The weighted fusion approach provides a smooth control over how much influence semantic information has versus the original data structure.

## Foundational Learning

**Multimodal Large Language Models (MLLMs)**: AI models that can process and generate both text and images, understanding semantic relationships across modalities. Why needed: They provide the semantic understanding required to generate meaningful labels for steering projections. Quick check: Can the MLLM accurately classify samples into relevant categories based on natural language prompts?

**Semantic Embedding Fusion**: The process of combining embeddings from different sources (original data + semantic labels) into a single representation. Why needed: Enables integration of semantic information with geometric data structure for enhanced interpretability. Quick check: Does the fusion weight appropriately balance semantic vs. original information for different use cases?

**Dimensionality Reduction Quality Metrics**: Quantitative measures (trustworthiness, continuity, silhouette score, Shepard diagram correlation) that evaluate how well a low-dimensional projection preserves relationships from the original high-dimensional space. Why needed: Provides objective assessment of projection quality beyond visual inspection. Quick check: Do metric improvements correlate with perceived interpretability gains?

## Architecture Onboarding

**Component Map**: Data → MLLM Zero-shot Classification → Semantic Embedding Generation → Embedding Fusion → Dimensionality Reduction → Projection Visualization

**Critical Path**: The semantic embedding generation and fusion steps are critical, as errors in label generation or inappropriate fusion weights will propagate through to the final projection quality.

**Design Tradeoffs**: The method trades computational overhead (additional MLLM inference) for enhanced interpretability. The fusion weight represents a key design choice between faithful data representation and semantic steering capability.

**Failure Signatures**: Poor prompt formulation leading to irrelevant semantic labels, MLLM classification errors producing noisy semantic embeddings, or inappropriate fusion weights causing either loss of data structure or insufficient semantic influence.

**First Experiments**: 1) Test different prompt formulations on a small dataset to evaluate semantic label quality, 2) Vary fusion weights to find optimal balance for interpretability vs. data fidelity, 3) Compare quality metrics across different MLLM models to identify most effective semantic understanding.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can this framework be adapted for high-dimensional tabular or numerical data that lacks inherent semantic descriptions?
- Basis in paper: The conclusion identifies extending the method to any high-dimensional dataset "beyond text and images" as a key future objective.
- Why unresolved: The current pipeline relies on Multimodal LLMs (MLLMs) like CLIP and Qwen, which require image or text inputs to generate semantic embeddings.
- What evidence would resolve it: A modified pipeline that effectively translates numerical features into semantic space or utilizes a different embedding strategy for non-semantic data.

### Open Question 2
- Question: Does semantic steering improve user task performance and insight generation compared to traditional hyperparameter tuning?
- Basis in paper: The paper claims to bridge the gap to "human-centered data exploration" but evaluates the method solely through automated quality metrics and author-generated visualizations, without a user study.
- Why unresolved: Improved cluster separation metrics do not automatically guarantee that users will find the system more usable or interpretable than existing DR controls.
- What evidence would resolve it: A controlled user study measuring task completion time, accuracy, and subjective workload when using natural language prompts versus traditional parameter sliders.

### Open Question 3
- Question: How can the system be evolved to support bi-directional conversational interactions, such as explaining user-selected clusters?
- Basis in paper: The conclusion suggests future work should allow users to "select a group of points in the projection and freely ask the system to explain, in natural language, what those samples have in common."
- Why unresolved: The current implementation is unidirectional: user prompts steer the projection, but the system cannot yet interpret user selections to generate new semantic insights.
- What evidence would resolve it: An interactive interface that accepts point selections as input and generates accurate natural language summaries of the selected data distribution.

## Limitations

- Evaluation relies on relatively small, well-structured benchmark datasets, limiting generalizability to real-world high-dimensional data with complex noise patterns
- Does not provide systematic comparisons of interpretability gains across different prompt types or user expertise levels
- Zero-shot classification accuracy of the Qwen-based MLLM is not explicitly evaluated, potentially impacting reliability of semantic embeddings

## Confidence

**High Confidence**: The technical implementation of semantic embedding fusion and the reported quality metric improvements (trustworthiness, continuity, Shepard diagram correlation, silhouette score) are methodologically sound and reproducible.

**Medium Confidence**: The claim of "enhanced interpretability" through user steering relies heavily on qualitative examples without systematic user studies or standardized interpretability benchmarks to quantify perceptual improvements.

**Low Confidence**: The generalizability of results to real-world datasets with unstructured noise and the robustness of semantic mapping across diverse prompt types remain unverified.

## Next Checks

1. **Benchmark Real-World Data**: Validate the framework on larger, noisier real-world datasets (e.g., multi-modal medical imaging or social media data) to assess scalability and robustness.

2. **Evaluate MLLM Classification Accuracy**: Measure the zero-shot classification accuracy of the Qwen-based MLLM on the datasets to quantify the reliability of generated semantic embeddings.

3. **User Study for Interpretability**: Conduct a controlled user study comparing interpretability gains across different prompt types and user expertise levels to empirically validate the claimed improvements in data exploration.