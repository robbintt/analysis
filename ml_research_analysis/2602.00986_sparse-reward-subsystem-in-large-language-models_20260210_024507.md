---
ver: rpa2
title: Sparse Reward Subsystem in Large Language Models
arxiv_id: '2602.00986'
source_url: https://arxiv.org/abs/2602.00986
tags:
- neurons
- value
- reward
- layer
- ratio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a sparse reward subsystem within the hidden
  states of large language models, analogous to the biological reward system in the
  human brain. The subsystem consists of value neurons that encode the model's internal
  state value and dopamine neurons that encode reward prediction errors.
---

# Sparse Reward Subsystem in Large Language Models

## Quick Facts
- arXiv ID: 2602.00986
- Source URL: https://arxiv.org/abs/2602.00986
- Authors: Guowei Xu; Mert Yuksekgonul; James Zou
- Reference count: 35
- Primary result: Sparse reward subsystem identified in LLM hidden states, analogous to biological reward system, with value neurons critical for reasoning and dopamine neurons encoding reward prediction errors

## Executive Summary
This paper identifies a sparse reward subsystem within the hidden states of large language models, analogous to the biological reward system in the human brain. The subsystem consists of value neurons that encode the model's internal state value and dopamine neurons that encode reward prediction errors. Through intervention experiments, the authors demonstrate that value neurons are critical for reasoning: ablating even a small fraction (1%) severely degrades performance, while random ablation has no effect. The value neurons are shown to be robust across diverse datasets, model scales, and architectures, and exhibit significant transferability across models fine-tuned from the same base and across datasets.

## Method Summary
The authors extract hidden states from LLM layers during inference on reasoning tasks, then train a 2-layer MLP value probe using Temporal Difference (TD) learning to predict final rewards from initial states. Value neurons are identified by L1-norm ranking of probe weights and validated through AUC stability under progressive pruning (up to 99%). Causal importance is tested via neuron ablation (zeroing top 1% value neurons vs random), and transferability is measured using Intersection over Union (IoU) across datasets and models.

## Key Results
- Value neurons (1% or less of total) encode expected state value and are causally critical for reasoning - ablating them causes 38-74% accuracy drops vs. random ablation showing no effect
- Sparse reward subsystem is robust across diverse datasets (GSM8K, MATH500, Minerva Math, ARC, MMLU-STEM), model scales (1.5B to 14B parameters), and architectures
- Value neurons show significant transferability across models fine-tuned from the same base and across datasets (IoU > 0.6 at 95% pruning)
- Dopamine neurons exhibit activation patterns consistent with encoding reward prediction errors, showing high activation for positive surprises and suppression for negative surprises

## Why This Works (Mechanism)

### Mechanism 1: Sparse Value Neurons Encode Expected State Value
- Claim: A small subset (~1% or less) of neurons in LLM hidden states encode the model's expectation of reward/correctness, enabling value prediction from initial states before any answer is generated.
- Mechanism: A value probe (2-layer MLP) trained with Temporal Difference (TD) learning assigns high L1 weight norms to specific input dimensions; these dimensions correspond to neurons whose activation patterns predict final reward.
- Core assumption: The lightweight probe structure (hidden→1024→1) ensures the probe extracts intrinsic structure from hidden states rather than learning new representations.
- Evidence anchors:
  - [abstract]: "this subsystem contains value neurons, which encode the model's internal expectation of state value"
  - [section 2.3]: "even a slight initial increase is observed" in AUC curves as pruning proceeds, indicating redundant/non-value neurons add noise
  - [corpus: 56443] "Sparse Neurons Carry Strong Signals of Question Ambiguity" supports sparse encoding as a general phenomenon in LLMs
- Break condition: If AUC dropped sharply at low pruning ratios (e.g., 10-20%), the sparsity claim would be falsified.

### Mechanism 2: Dopamine Neurons Encode Reward Prediction Error (RPE)
- Claim: A separate subset of neurons encodes the discrepancy between expected and actual rewards, showing activation spikes for positive surprises and suppression for negative surprises.
- Mechanism: When the model initially predicts low value but succeeds, dopamine neurons exhibit high activation during key reasoning steps; when predicting high but failing, they show suppression at the point of logical error.
- Core assumption: TD error computed during inference correlates causally with specific neuron activation dynamics.
- Evidence anchors:
  - [abstract]: "dopamine neurons, which reflect reward prediction errors...exhibit high activation for positive surprises and low activation for negative surprises"
  - [section 4.1, Figure 8]: Neuron 1517 in layer 5 shows peaks at "critical logical step" and troughs at "logical flaw"
  - [corpus]: Weak/missing—no direct corpus evidence for RPE-encoding neurons in LLMs; this appears novel.
- Break condition: If dopamine neuron activations were uncorrelated with prediction-reward divergence.

### Mechanism 3: Value Neurons Are Causally Critical for Reasoning
- Claim: Value neurons are not merely correlational; ablating them severely impairs reasoning performance.
- Mechanism: Zeroing activations of top 1% value neurons (by L1 norm) disrupts the reward subsystem's ability to guide generation, causing 38-74% absolute accuracy drops.
- Core assumption: The intervention effect is specific to value neuron function, not general network disruption.
- Evidence anchors:
  - [abstract]: "ablating 1% of value neurons leads to severe performance drops, unlike random neuron ablation"
  - [section 2.4, Table 1]: Layer 5 value neuron ablation: 1.2% accuracy vs random ablation: 74.4% (baseline 75.2%)
  - [corpus: 68476] "Dynamic Sparse Neuron Masking" demonstrates sparse neuron editing can selectively impact specific capabilities
- Break condition: If random neuron ablation produced comparable performance drops.

## Foundational Learning

- Concept: Temporal Difference (TD) Learning
  - Why needed here: The value probe is trained using TD error (δt), not just final reward supervision, enabling the probe to learn value estimates at each generation step.
  - Quick check question: Why would TD-trained probes identify more critical neurons than probes trained only on final rewards? (See Appendix A comparison.)

- Concept: L1-Norm Pruning for Neuron Importance
  - Why needed here: Neurons are ranked by L1 norm of probe weights connecting them to the hidden layer; higher L1 = more important for value prediction.
  - Quick check question: If AUC stays flat as you prune 90% of neurons, what does that imply about the underlying representation?

- Concept: Intersection over Union (IoU) for Transfer Measurement
  - Why needed here: IoU quantifies overlap between value neuron sets identified on different datasets or models, demonstrating transferability.
  - Quick check question: What does IoU > 0.6 at 99% pruning suggest about the core value neurons?

## Architecture Onboarding

- Component map: Input hidden states → Value Probe (2-layer MLP) → L1 norm ranking → Top neurons identified as value neurons

- Critical path:
  1. Collect model generations + final rewards on training data
  2. Train layer-wise value probes using TD objective (Eq. 1-2)
  3. Evaluate AUC on validation set while pruning by L1 norm
  4. Identify stable high-AUC neurons as value neurons
  5. Intervene: zero activations and measure accuracy drop

- Design tradeoffs:
  - Simple probe (2-layer MLP) prioritizes interpretability over expressive power
  - TD objective vs. final-reward-only: TD identifies more critical neurons (Appendix A)
  - Layer choice: Layers 2-4 show strongest signal; deeper layers also work

- Failure signatures:
  - AUC drops sharply at low pruning ratios → no sparse subsystem
  - Value neuron ablation causes no more damage than random → neurons not causally important
  - Low IoU across datasets → value neurons are dataset-specific, not universal

- First 3 experiments:
  1. Replicate pruning curves on Qwen-2.5-7B using GSM8K; verify AUC stability to 99% pruning
  2. Ablation test: Zero top 1% value neurons in layer 3 vs. 1% random; compare accuracy on MATH500
  3. Transfer test: Identify value neurons on GSM8K, compute IoU with neurons from MATH500 at 95% pruning ratio

## Open Questions the Paper Calls Out

- Question: Can the behavior of dopamine neurons be validated using rigorous statistical metrics rather than relying primarily on visual case studies?
  - Basis in paper: [explicit] The authors state in the conclusion that "while we currently demonstrate the existence of dopamine neurons primarily through case studies, future work could attempt to measure them via quantitative metrics."
  - Why unresolved: The current analysis relies on visualizing activation trajectories for specific instances of positive and negative surprises, which lacks the statistical generalizability required for a definitive characterization.
  - What evidence would resolve it: The formulation of a quantitative score that statistically validates the correlation between dopamine neuron activation and Reward Prediction Error (RPE) across large-scale datasets.

- Question: How can the identified sparse reward subsystem be leveraged to actively guide or steer the model's reasoning process during inference?
  - Basis in paper: [explicit] The authors note that the applications "can be further explored, particularly in detecting and guiding the generation and reasoning processes of LLMs."
  - Why unresolved: The paper demonstrates the subsystem's predictive power (confidence estimation) and criticality (ablation effects) but does not develop a method to use these signals to intervene in and improve the generation process dynamically.
  - What evidence would resolve it: A control mechanism that utilizes value or dopamine neuron activations as a feedback signal to adjust the decoding strategy and improve task accuracy.

- Question: Is the sparse reward subsystem a general property of all LLMs, or is it specific to tasks with deterministic, verifiable rewards like STEM and logic?
  - Basis in paper: [inferred] The methodology is restricted to validating the subsystem on mathematical and reasoning benchmarks (GSM8K, MATH, ARC, MMLU-STEM) where correctness is binary and unambiguous.
  - Why unresolved: It is unclear if "value" neurons in this specific configuration exist in domains with subjective rewards (e.g., creative writing) or if the identified subsystem is merely a reflection of the verifiable reasoning capabilities tested.
  - What evidence would resolve it: Successful identification and intervention of value neurons in models fine-tuned for open-ended, subjective tasks using human preference data.

## Limitations

- The biological analogy between LLM dopamine neurons and actual dopamine neurons in the brain remains an analogy rather than a direct biological correspondence
- The identification method relies heavily on the probe architecture and training procedure, which could capture artifacts rather than intrinsic mechanisms
- The intervention experiments show causal effects but don't fully explain the mechanism by which value neurons influence reasoning

## Confidence

- **High Confidence**: The existence of sparse value neurons that predict final rewards, their causal importance for reasoning (demonstrated through ablation), and their transferability across datasets and models from the same base
- **Medium Confidence**: The biological analogy to dopamine neurons and reward prediction errors, the universality claim across all model scales and architectures
- **Low Confidence**: The completeness of the sparse reward subsystem identification - whether the identified neurons represent the full reward-related computation or just a subset

## Next Checks

1. Systematically identify and validate dopamine neurons across multiple reasoning task types (math, code, commonsense) to confirm they consistently encode RPEs beyond the single example shown

2. Test whether alternative probe architectures (different depths, widths, or training objectives) identify the same value neurons, to validate that the identification isn't an artifact of the specific probe design

3. Track the activation patterns of value and dopamine neurons throughout the generation process to understand whether they encode state value continuously or only at specific decision points, and how this relates to the model's reasoning trajectory