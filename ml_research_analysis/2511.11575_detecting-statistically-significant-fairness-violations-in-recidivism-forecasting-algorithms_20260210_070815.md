---
ver: rpa2
title: Detecting Statistically Significant Fairness Violations in Recidivism Forecasting
  Algorithms
arxiv_id: '2511.11575'
source_url: https://arxiv.org/abs/2511.11575
tags:
- test
- individuals
- used
- protected
- positive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of detecting statistically significant
  fairness violations in recidivism forecasting algorithms. It introduces a rigorous
  framework leveraging k-fold cross-validation to generate sampling distributions
  of fairness metrics, enabling statistical inference for various fairness definitions
  including group fairness, predictive parity, predictive equality, equalized odds,
  conditional use accuracy equality, overall accuracy equality, treatment equality,
  calibration, and causal discrimination.
---

# Detecting Statistically Significant Fairness Violations in Recidivism Forecasting Algorithms

## Quick Facts
- arXiv ID: 2511.11575
- Source URL: https://arxiv.org/abs/2511.11575
- Reference count: 19
- Models forecast recidivism with 65.6%-73.7% accuracy while showing statistically significant bias against Black individuals across multiple fairness metrics

## Executive Summary
This study introduces a rigorous statistical framework for detecting fairness violations in recidivism forecasting algorithms. Using 250-fold cross-validation on a 250,000-observation NIJ dataset, the research trains four machine learning models and tests them against 13+ fairness definitions. The key innovation is generating sampling distributions of fairness metrics to enable statistical inference about systematic bias versus random variation. Results reveal that most models exhibit statistically significant bias against Black individuals, though some show reverse discrimination against White individuals under certain fairness definitions, highlighting the inherent incompatibilities between different fairness criteria.

## Method Summary
The methodology employs 250-fold cross-validation to generate sampling distributions for fairness metrics, enabling statistical inference about bias. Four models (Logistic Regression, LDA, Random Forest, XGBoost) are trained on the NIJ Recidivism Challenge dataset. Fairness is evaluated across 13+ definitions including group fairness, predictive parity, equalized odds, and causal discrimination. Statistical tests include two-sample t-tests, chi-squared tests, and McNemar's mid-p test with appropriate corrections for multiple comparisons. Counterfactual race changes are used to detect causal discrimination.

## Key Results
- Most models show statistically significant bias against Black individuals across multiple fairness metrics
- Accuracy ranges from 65.6% to 73.7% across all models
- Some models demonstrate reverse discrimination against White individuals under certain fairness definitions
- Different fairness metrics yield contradictory conclusions about bias direction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: K-fold cross-validation generates sampling distributions of fairness metrics, enabling statistical inference to distinguish systematic bias from random variation.
- Mechanism: By partitioning data into K folds (K=250) and training/evaluating models K times, each fold produces metric values (e.g., FPR by race). The resulting n=250 observations per group form empirical distributions. Two-sample t-tests then compare distribution means to assess whether disparities exceed chance-level variation.
- Core assumption: The cross-validation folds produce approximately independent samples of each fairness metric; metric distributions are sufficiently normal for t-test validity.
- Evidence anchors:
  - [abstract]: "leveraging k-fold cross-validation to generate sampling distributions of fairness metrics"
  - [PAGE 2, Section A]: "250-fold cross validation is used to create 250 potential test datasets... distributions for model statistics... can be generated using K-Fold cross validation"
  - [corpus]: Related work on statistical fairness auditing (Empirical Likelihood-Based Fairness Auditing) supports distribution-free approaches when normality is questionable, suggesting this assumption may need validation.
- Break condition: If folds are not sufficiently independent (e.g., temporal or geographic clustering), or if sample sizes within groups per fold are too small, distributions may not converge to normality, invalidating t-tests.

### Mechanism 2
- Claim: Multiple fairness definitions applied to the same model yield contradictory conclusions about bias direction, revealing inherent incompatibilities in fairness criteria.
- Mechanism: The paper tests each model against 13+ fairness definitions (group fairness, predictive parity, equalized odds, etc.). Different definitions measure different conditional probabilities (e.g., P(ŷ|race) vs. P(y|ŷ, race) vs. P(ŷ|y, race)). Base rate differences between groups propagate through these conditionals differently, causing some metrics to show anti-Black bias while others show "reverse discrimination" against White individuals.
- Core assumption: No single model can simultaneously satisfy all fairness definitions when base rates differ between groups—consistent with impossibility theorems in fairness literature.
- Evidence anchors:
  - [abstract]: "different fairness metrics can lead to contradictory conclusions about bias"
  - [PAGE 3, Section C]: "there is statistically significant violation of predictive parity in a manner that is unfair to white individuals"
  - [PAGE 6, Section N]: "these models are not discriminatory and instead may be practicing reverse discrimination"
  - [corpus]: Weak corpus linkage on impossibility theorems explicitly—corpus papers focus on detection/monitoring rather than theoretical constraints.
- Break condition: If group base rates are equal, multiple fairness definitions may become simultaneously satisfiable, reducing contradictory conclusions.

### Mechanism 3
- Claim: Counterfactual simulation (race change) combined with McNemar's test detects causal discrimination by identifying whether race directly affects predictions when all other attributes are held constant.
- Mechanism: For each observation, predictions are generated under original race and counterfactual race. Discordant pairs (prediction changes when race flips) are tabulated. McNemar's mid-p test evaluates whether the proportion of (1→0) discordances equals (0→1) discordances. Asymmetric proportions indicate causal effect of race on predictions.
- Core assumption: The model does not have race as an explicit input feature, but may infer race proxy from correlated features; counterfactual simulation only changes the race label, not the proxy-correlated covariates.
- Evidence anchors:
  - [PAGE 6, Section N]: "a race change can be simulated to generate counterfactual predictions... McNemar's test identifies whether there are equal proportions of both types of discordant pairs"
  - [PAGE 7 Conclusion]: "some models showcased reverse discrimination when individuals with identical attributes but different races were compared"
  - [corpus]: Related paper "Uncovering Discrimination Clusters" uses similar individual fairness testing with counterfactuals, validating this approach.
- Break condition: If race-correlated features (e.g., zip code, income) are not also intervened upon in counterfactuals, the test may underestimate causal effect of race.

## Foundational Learning

- Concept: **Hypothesis testing with multiple comparisons (Bonferroni correction)**
  - Why needed here: Equalized odds and conditional use accuracy equality require simultaneous testing of two conditions. The paper applies Bonferroni correction (α/2 = 0.025) to maintain family-wise error rate.
  - Quick check question: If you test 5 independent fairness metrics at α=0.05 each without correction, what is the probability of at least one false positive by chance alone?

- Concept: **Confusion matrix decomposition by protected group**
  - Why needed here: All outcome-based fairness metrics derive from group-stratified confusion matrices. TPR, FPR, PPV, NPV must be computed separately for protected (Black) and privileged (White) groups.
  - Quick check question: For Equal Opportunity, you compare FNR across groups. Is this derived from P(ŷ=1|y=0, race) or P(y=0|ŷ=1, race)?

- Concept: **McNemar's test for paired binary outcomes**
  - Why needed here: Used for causal discrimination and fairness through awareness testing, where each observation has paired predictions (original vs. counterfactual, or individual vs. nearest neighbor).
  - Quick check question: In a 2×2 table with discordant counts b=30 and c=10, does McNemar's test focus on the (b+c) diagonal or the (b vs. c) asymmetry?

## Architecture Onboarding

- Component map:
Raw data (250K observations)
    ↓
250-fold CV split
    ↓
[Loop K times] → Train model on K-1 folds → Generate predictions on holdout fold
    ↓
Aggregate predictions across folds
    ↓
┌─────────────────────────────────────────────────────────┐
│ Fairness Metric Calculator                              │
│  - Group-stratified confusion matrices → TPR/FPR/PPV/NPV│
│  - Probability bins → Calibration curves by group       │
│  - Counterfactual generation → Discordance tables       │
│  - Mahalanobis matching → Neighbor comparison tables    │
└─────────────────────────────────────────────────────────┘
    ↓
Statistical Test Engine
  - Two-sample t-tests for distribution comparisons
  - Chi-squared tests for calibration
  - McNemar's mid-p test for paired outcomes
    ↓
Results aggregation: p-values, disparities, significance flags

- Critical path:
  1. Implement 250-fold CV prediction loop (cache all predictions; recompute is expensive)
  2. Build group-stratified metric extractors (must handle missing groups in some folds)
  3. Implement statistical tests with proper one-tailed/two-tailed selection per metric
  4. Apply Bonferroni correction for composite metrics (equalized odds, conditional use accuracy)

- Design tradeoffs:
  - **K value selection**: Higher K (250) produces more stable sampling distributions but increases compute. K=5 or K=10 may be insufficient for reliable inference.
  - **McNemar's exact vs. mid-p vs. asymptotic**: Mid-p recommended for low discordance counts (see Fagerland et al. citation); exact test may be overly conservative.
  - **One-tailed vs. two-tailed tests**: Paper uses directional tests based on which disparity direction constitutes harm. This increases power but requires clear normative justification.

- Failure signatures:
  - **Empty group in fold**: If a fold has zero protected or privileged individuals in a condition (e.g., y=1 ∧ race=Black), metrics cannot be computed. Mitigation: stratified sampling by race × outcome.
  - **Zero discordance in McNemar's table**: Test has no power. Paper notes this for logistic regression counterfactuals (see low counts in Tables XVI–XIX).
  - **p-value = 0.0 or 1.0**: Likely indicates test statistic far in tail or wrong tail direction; verify test direction matches hypothesis.

- First 3 experiments:
  1. **Reproduce group fairness test on a single model**: Implement 10-fold CV, compute P(ŷ=0|race=Black) and P(ŷ=0|race=White) distributions, run two-sample t-test. Compare p-value direction with paper's Table II.
  2. **Validate calibration test sensitivity**: Bin predictions into deciles, compute chi-squared statistic for group difference in positive outcome frequency. Test with and without the paper's standardization formula (λ = θ/α × β).
  3. **Pilot counterfactual analysis with feature importance**: Select 100 observations, flip race label, record prediction changes. If model has no race input but predictions change, identify which features act as proxies via SHAP or permutation importance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can strict definitions or hierarchies of fairness metrics be established to resolve contradictory conclusions where models appear biased against Black individuals under some metrics but show "reverse discrimination" against White individuals under others?
- Basis in paper: [explicit] The Conclusion explicitly calls for "establishing more strict definitions... to avoid the issue of different definitions leading to different conclusions."
- Why unresolved: The study demonstrates that the same models violate different fairness definitions in opposing directions, creating ambiguity regarding the true fairness status of the algorithm.
- What evidence would resolve it: A theoretical or regulatory framework that prioritizes specific fairness definitions (e.g., weighting Predictive Parity over Equalized Odds) within the criminal justice context.

### Open Question 2
- Question: Can a valid causal graph for recidivism be constructed to enable the application of the proposed statistical significance framework to metrics based on causal inference?
- Basis in paper: [explicit] The authors state they "refrain from identifying statistically significant violations of metrics based on causal graphs" because "There is no causal graph that can be drawn using current literature."
- Why unresolved: The statistical methodology for testing causal fairness exists within the paper, but the necessary domain-specific causal structure for recidivism is missing from current literature.
- What evidence would resolve it: A validated structural causal model (SCM) for recidivism that allows the k-fold framework to test the statistical significance of causal discrimination.

### Open Question 3
- Question: Do bias mitigation techniques applied to the detected statistically significant violations degrade the predictive accuracy (65.6%–73.7%) or stability of the recidivism forecasting models?
- Basis in paper: [inferred] The paper focuses entirely on a rigorous framework for *detecting* violations but does not explore the trade-offs involved in *correcting* these violations.
- Why unresolved: It is unclear if the identified distributional disparities (e.g., in False Positive Rates or Calibration) can be reduced without negatively impacting the utility of the forecasting tools used by judges and parole boards.
- What evidence would resolve it: Post-mitigation results showing the trade-off curves between fairness metric alignment and overall accuracy using the proposed k-fold sampling distributions.

## Limitations
- Computational intensity of 250-fold cross-validation requires substantial resources
- Assumption of metric distribution normality for t-test validity not explicitly validated
- Analysis restricted to Black and White individuals, excluding other racial groups
- Counterfactual race change test may underestimate causal effects if race-correlated features aren't also intervened upon

## Confidence
- **High confidence**: The mechanism of using k-fold cross-validation to generate sampling distributions for statistical inference is well-grounded and methodologically sound.
- **Medium confidence**: The application of multiple fairness definitions and the resulting contradictory conclusions align with known impossibility theorems, though the paper does not explicitly cite these theoretical constraints.
- **Medium confidence**: The counterfactual analysis using McNemar's test is appropriate for detecting causal discrimination, but the test's power is limited when discordant pairs are few, as noted for logistic regression.

## Next Checks
1. **Reproduce group fairness test on a single model**: Implement 10-fold CV, compute P(ŷ=0|race=Black) and P(ŷ=0|race=White) distributions, run two-sample t-test. Compare p-value direction with paper's Table II.
2. **Validate calibration test sensitivity**: Bin predictions into deciles, compute chi-squared statistic for group difference in positive outcome frequency. Test with and without the paper's standardization formula (λ = θ/α × β).
3. **Pilot counterfactual analysis with feature importance**: Select 100 observations, flip race label, record prediction changes. If model has no race input but predictions change, identify which features act as proxies via SHAP or permutation importance.