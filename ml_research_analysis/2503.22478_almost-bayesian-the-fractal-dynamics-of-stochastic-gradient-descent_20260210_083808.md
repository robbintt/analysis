---
ver: rpa2
title: 'Almost Bayesian: The Fractal Dynamics of Stochastic Gradient Descent'
arxiv_id: '2503.22478'
source_url: https://arxiv.org/abs/2503.22478
tags:
- learning
- diffusion
- dimension
- which
- fractal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper connects the dynamics of stochastic gradient descent
  (SGD) in deep learning to Bayesian statistics by showing that SGD behaves like diffusion
  on a fractal landscape. The fractal dimension of the loss landscape, which determines
  how "accessible" different regions are, can be described using the local learning
  coefficient from singular learning theory.
---

# Almost Bayesian: The Fractal Dynamics of Stochastic Gradient Descent

## Quick Facts
- arXiv ID: 2503.22478
- Source URL: https://arxiv.org/abs/2503.22478
- Authors: Max Hennick; Stijn De Baerdemacker
- Reference count: 28
- Primary result: SGD exhibits anomalous subdiffusion on fractal loss landscapes, with spectral dimension bounded above by the local learning coefficient

## Executive Summary
This paper establishes a unified framework connecting stochastic gradient descent (SGD) dynamics to Bayesian statistics through fractal geometry. The authors demonstrate that SGD weight trajectories exhibit anomalous subdiffusion described by the fractional Fokker-Planck equation, rather than standard Brownian motion. The fractal dimension of the loss landscape, which determines accessibility of different regions, is shown to be mathematically equivalent to the local learning coefficient (LLC) from singular learning theory. Experiments on MNIST with various architectures validate that the spectral dimension governing particle movement on fractals is bounded above by the LLC, providing empirical support for the theoretical framework.

## Method Summary
The authors validate their fractal diffusion theory by training fully connected networks on MNIST with ReLU activations and batch normalization. LLC is computed every 100 training steps using the devinterp library with default hyperparameters. Weight displacement R(t) is tracked over time, and spectral dimension d_s is extracted via linear regression on log(R(t)) vs. log(t). The key relationship d_s ≤ λ is tested across 50 identical FC networks with different seeds and 18 architectures varying in depth and width. SGD is used with learning rate 0.001, zero weight decay, and batch size 256. Final LLC values are averaged over the last 10 estimates.

## Key Results
- SGD weight trajectories exhibit anomalous subdiffusion following power-law scaling rather than Brownian motion
- The local learning coefficient from singular learning theory equals the Minkowski-Bouligand fractal dimension of the loss landscape
- Spectral dimension d_s governing particle movement on fractals is bounded above by the LLC: d_s ≤ λ(w(t))
- The relationship holds across multiple architectures and training configurations on MNIST

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Neural network weight trajectories under SGD exhibit anomalous subdiffusion described by power-law scaling rather than normal Brownian diffusion.
- Mechanism: The standard Fokker-Planck equation fails to capture observed weight dynamics. Introducing a Caputo fractional time derivative D^α_t (0 < α < 1) into the Fractional Fokker-Planck Equation (FFPE) accounts for memory-dependent subdiffusive behavior, analogous to diffusion through porous media with variable absorption rates.
- Core assumption: Large batch regime where gradient estimation noise doesn't dominate dynamics; weight displacements don't follow heavy-tailed (Lévy) distributions.
- Evidence anchors:
  - [abstract] "the fractional Fokker-Planck equation (FFPE), which includes a fractional time derivative, better captures the anomalous diffusion behavior observed on neural network weight trajectories"
  - [section 3] "it has been observed that the mean squared displacement of neural network weights (when trained using SGD) describes an anomalous diffusion process"
  - [corpus] Related work on transient learning dynamics (arXiv:2601.10962) supports non-equilibrium mechanism in SGD solution selection
- Break condition: Small batch sizes induce heavy-tailed gradient noise; extreme hyperparameter choices where noise dominates dynamics.

### Mechanism 2
- Claim: The local learning coefficient λ(w) from singular learning theory corresponds to the Minkowski-Bouligand fractal dimension d_f of the loss landscape.
- Mechanism: In singular learning theory, the volume of parameters with loss below ε scales as V(ε) ∝ ε^λ. This scaling relationship is mathematically equivalent to the mass fractal dimension definition for porous media, where accessible "pore" volume scales with the characteristic length scale. The LLC thus measures the effective dimensionality of accessible low-loss regions.
- Core assumption: Near Stability Hypothesis (Hypothesis 3.1): For any point w_1 encountered during training, there exists a nearby metastable point w_2 such that d_f(w_1) ≈ λ(w_2).
- Evidence anchors:
  - [abstract] "the fractal dimension of the loss landscape, which determines how 'accessible' different regions are, can be described using the local learning coefficient from singular learning theory"
  - [section 3.1] Equation (4) and (5) show formal equivalence: V(B(w*,ε)) ∝ ε^λ(w*) and M(ε) ∝ ε^d_f(w*)
  - [corpus] Corpus evidence is weak—no direct prior work establishing this LLC-fractal dimension equivalence
- Break condition: Points far from any metastable state; non-degenerate minima-dominated landscapes where spectral dimension approximation fails.

### Mechanism 3
- Claim: The spectral dimension d_s, governing actual particle movement on fractals, is bounded above by the local learning coefficient: d_s ≤ λ(w(t)).
- Mechanism: The spectral dimension measures scaling of reachable states (V_s(t) ~ t^{d_s/2}), which is strictly smaller than the fractal dimension measuring geometrically accessible states. The walker dimension d_walk = 2λ/d_s ≥ 2 in subdiffusive regimes, establishing the bound. This implies SGD concentrates time in relatively flat regions.
- Core assumption: Asymptotic spectral dimension exists; d_walk ≥ 2 (subdiffusive regime verified empirically).
- Evidence anchors:
  - [abstract] "spectral dimension (which governs particle movement on fractals) is bounded above by the local learning coefficient"
  - [section 4, Figure 2-3] Experimental results on MNIST show all tested architectures satisfy d_s ≤ λ(w(t)) and d_s ≤ λ̄(w(t))
  - [corpus] Related work on SGD dynamics (arXiv:2510.20905) confirms SGD preference for flatter solutions
- Break condition: Early-time dynamics where subdiffusion appears linear; grokking regimes with sudden weight jumps requiring spatial fractional derivatives.

## Foundational Learning

- Concept: **Fokker-Planck Equation**
  - Why needed here: The paper reformulates SGD dynamics as diffusion on fractal geometry, requiring understanding of how probability densities evolve under drift and diffusion terms.
  - Quick check question: Can you explain why the standard FPE with potential V(w) fails to capture anomalous diffusion?

- Concept: **Fractal Dimensions (Mass/Box-Counting vs. Spectral)**
  - Why needed here: The paper distinguishes between the mass dimension (LLC, measuring accessible volume) and spectral dimension (measuring reachable states under diffusion), both critical for the theoretical framework.
  - Quick check question: If a fractal has mass dimension d_f = 3 and spectral dimension d_s = 2, what does this tell you about the diffusive process?

- Concept: **Singular Learning Theory Basics (RLCT/LLC)**
  - Why needed here: The LLC is the central quantity connecting Bayesian complexity measures to fractal geometry; understanding its role in WBIC is essential.
  - Quick check question: Why does the BIC fail for neural networks, and how does the LLC address this?

## Architecture Onboarding

- Component map:
  Input (empirical loss) -> LLC estimator (devinterp) -> Weight displacement computation -> Spectral dimension extraction -> d_s ≤ λ validation

- Critical path:
  1. Implement LLC estimator (use devinterp library: `pip install devinterp`)
  2. Train models with varying architectures while logging weight displacements
  3. Compute d_s from displacement power-law fit
  4. Verify d_s ≤ λ bound holds across architectures

- Design tradeoffs:
  - **Length scale ξ**: Too small captures noise; too large smooths meaningful structure. Paper uses δ built into LLC estimator.
  - **Batch size**: Large batches needed for theory validity; small batches induce grokking/jump behavior requiring spatial fractional derivatives (Appendix D)
  - **Optimizer**: Theory assumes SGD with low learning rate, negligible weight decay; AdamW shows similar dynamics but less theoretically grounded

- Failure signatures:
  - Negative LLC estimates (indicates insufficient proximity to local minima)
  - Linear (rather than power-law) weight displacement (breaks fractal assumption)
  - Large sudden weight jumps (indicates need for spatial fractional derivative; current theory incomplete)

- First 3 experiments:
  1. **Replicate MNIST validation**: Train 50 identical FC networks on MNIST subset (10k images, 50/50 split), compute LLC every 100 steps, verify d_s ≤ λ concentration
  2. **Architecture sweep**: Vary depth and width across 18 architectures, confirm bound holds universally; plot LLC vs. generalization error to validate linear relationship
  3. **Hyperparameter stress test**: Systematically vary batch size, learning rate, weight decay for fixed architecture; identify regime boundaries where power-law breaks down and grokking emerges

## Open Questions the Paper Calls Out
None

## Limitations
- Theory requires large batch regimes where gradient noise doesn't dominate dynamics
- LLC-fractal dimension equivalence lacks direct prior corpus support
- Theory doesn't address early-time dynamics where subdiffusion appears linear

## Confidence

- LLC-spectral dimension relationship (d_s ≤ λ): **High confidence** - supported by mathematical derivation and empirical validation
- Fractional Fokker-Planck equation as complete model: **Medium confidence** - assumes Markovian dynamics while SGD exhibits memory effects
- LLC-fractal dimension equivalence: **Medium confidence** - mathematically elegant but lacks direct prior corpus support

## Next Checks

1. **Architecture generalization**: Test the d_s ≤ λ bound on convolutional networks (CNNs) and transformers trained on CIFAR-10/100 to verify architectural universality beyond FC networks.

2. **Hyperparameter boundary detection**: Systematically vary batch size, learning rate, and weight decay to identify precise thresholds where power-law scaling breaks down and alternative dynamics (grokking, heavy-tailed behavior) emerge.

3. **Temporal dynamics verification**: Track the LLC and spectral dimension evolution throughout training to determine if the asymptotic spectral dimension exists and whether the walker dimension d_walk ≥ 2 holds consistently across training stages.