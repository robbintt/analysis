---
ver: rpa2
title: 'SALMONN-omni: A Standalone Speech LLM without Codec Injection for Full-duplex
  Conversation'
arxiv_id: '2505.17060'
source_url: https://arxiv.org/abs/2505.17060
tags:
- speech
- salmonn-omni
- full-duplex
- arxiv
- thinking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SALMONN-omni introduces a standalone full-duplex speech LLM without
  audio codec injection, integrating a streaming speech encoder, LLM backbone, and
  synthesizer with a novel explicit "thinking" strategy for autonomous dialogue state
  transitions. It achieves at least 30% relative performance improvement over open-source
  full-duplex models and performs competitively to half-duplex systems despite less
  training data, demonstrating strong capabilities in turn-taking, backchanneling,
  echo cancellation, and context-dependent barge-in, with further gains via reinforcement
  learning.
---

# SALMONN-omni: A Standalone Speech LLM without Codec Injection for Full-duplex Conversation

## Quick Facts
- arXiv ID: 2505.17060
- Source URL: https://arxiv.org/abs/2505.17060
- Authors: Wenyi Yu; Siyin Wang; Xiaoyu Yang; Xianzhao Chen; Xiaohai Tian; Jun Zhang; Guangzhi Sun; Lu Lu; Yuxuan Wang; Chao Zhang
- Reference count: 40
- Primary result: Achieves at least 30% relative performance improvement over open-source full-duplex models while competing with half-duplex systems despite less training data

## Executive Summary
SALMONN-omni introduces a standalone full-duplex speech LLM that processes streaming speech input and output simultaneously without audio codec injection. The model integrates a streaming speech encoder, LLM backbone, and synthesizer with a novel explicit "thinking" strategy for autonomous dialogue state transitions. It achieves at least 30% relative performance improvement over open-source full-duplex models and performs competitively to half-duplex systems despite less training data, demonstrating strong capabilities in turn-taking, backchanneling, echo cancellation, and context-dependent barge-in, with further gains via reinforcement learning.

## Method Summary
SALMONN-omni is trained in three stages: Stage 1 connects a Mamba streaming encoder to Llama-3-8B-Instruct via MLP with LoRA for ASR and QA tasks; Stage 2 adds a CosyVoice2 synthesizer with linear adapter for streaming speech output; Stage 3 applies Direct Preference Optimization to improve barge-in and backchanneling behaviors. The model uses explicit thinking tokens to manage turn-taking, operates on 80ms time blocks with 320ms latency for semantic coherence, and processes dual-channel input (environment + assistant streams) without discrete audio codecs.

## Key Results
- At least 30% relative performance improvement over open-source full-duplex models on QA and dialogue tasks
- Competitive performance to half-duplex systems despite significantly less training data
- F1 score for barge-in detection improves from 0.86 (SFT) to 0.90 (DPO) after reinforcement learning
- Strong performance in turn-taking, backchanneling, echo cancellation, and context-dependent barge-in

## Why This Works (Mechanism)

### Mechanism 1: Explicit Thinking for State Transition
The LLM is trained to output special tokens (`<think`, `<shift>`) to represent internal states, forcing it to autoregressively model time and conversation dynamics as a text generation task. This explicit strategy outperforms implicit prediction for turn-taking management.

### Mechanism 2: Codec-Free Continuous Embedding Interleaving
SALMONN-omni avoids discrete audio codec tokens by connecting streaming encoder and synthesizer via hidden states (adapters/MLPs), reducing the "modality gap" and preserving textual reasoning capabilities in the continuous latent space.

### Mechanism 3: Reinforcement Learning for Contextual Dynamics
Direct Preference Optimization (DPO) corrects the SFT bias toward being interrupted by training on preference pairs that distinguish true interruptions from noise, improving barge-in detection precision from ~0.5 to ~0.9.

## Foundational Learning

- **Concept**: Full-Duplex vs. Half-Duplex Interaction
  - Why needed: To understand why simultaneous listening and speaking is architecturally distinct from turn-based systems
  - Quick check: Can the model listen to a new user query while it is still generating the audio for the previous answer?

- **Concept**: Stream Interleaving (Environment vs. Assistant)
  - Why needed: The model processes two streams that must be distinguished by the LLM
  - Quick check: How does the LLM distinguish between the sound of its own voice (echo) and the user's voice?

- **Concept**: Autoregression with Soft Tokens
  - Why needed: The LLM generates discrete text tokens while conditioning on continuous speech vectors
  - Quick check: Does the LLM generate audio waveforms directly, or does it generate text tokens that condition a separate synthesizer?

## Architecture Onboarding

- **Component map**: Log-Mel features → Mamba Streaming Encoder → Linear Adapter → LLM Backbone (Llama-3) → CosyVoice2 Synthesizer → Audio
- **Critical path**: Synchronization of the 80ms time blocks; the LLM must process environment stream and generate response within this window
- **Design tradeoffs**: 320ms fixed delay ensures semantic coherence but impacts responsiveness; explicit thinking adds tokens but improves turn-taking accuracy
- **Failure signatures**:
  - "Yes-Man" Effect: Model stops speaking immediately upon any user noise (low precision in barge-in)
  - "Deaf Ear": Model ignores user interruptions and continues speaking (barge-in failure)
  - Echo Loop: Model responds to its own voice (assistant stream not properly labeled)
- **First 3 experiments**:
  1. Implicit vs. Explicit Ablation: Verify explicit tokens improve turn-taking success rates
  2. Echo Cancellation Stress Test: Ensure model doesn't hallucinate inputs from its own output
  3. DPO Batch Size Sweep: Confirm RL dynamics function as described with the "conservative dip"

## Open Questions the Paper Calls Out

1. What causes the model to become extremely conservative in early DPO training, and what mechanisms enable gradual recovery and improvement over SFT?
2. Why does increasing diversity of "thinking" content during listening state degrade performance compared to simple explicit thinking strategy?
3. What explains why middle-to-late LLM layers (16-24) provide optimal embeddings for speech synthesis while early layers and final layer degrade performance?
4. How can emotional expression be made consistent and contextually appropriate given current intermittent inappropriate or mismatched emotions?

## Limitations
- Synthetic training data quality may not capture real-world conversational richness
- Performance comparisons lack direct benchmarking against commercial-grade closed-source systems
- DPO reinforcement learning stability and reproducibility across hardware setups remains uncertain
- Fixed 320ms delay may impact responsiveness in rapid conversational exchanges

## Confidence
- **High Confidence**: Architectural design and quantitative improvements over existing open-source models are well-defined and measurable
- **Medium Confidence**: Claims about outperforming half-duplex systems and achieving state-of-the-art performance lack direct comparison to commercial systems
- **Medium Confidence**: Effectiveness of explicit thinking and codec-free design demonstrated through ablation studies, but long-term generalization uncertain

## Next Checks
1. Deploy SALMONN-omni in live multi-turn conversations with human users, measuring turn-taking success rate and barge-in detection F1 compared to commercial baseline
2. Retrain variant using real human-human conversational data instead of synthetic data, comparing performance on turn-taking and barge-in metrics
3. Implement and evaluate adaptive speech synthesis delay strategy that adjusts based on conversational context against fixed 320ms baseline