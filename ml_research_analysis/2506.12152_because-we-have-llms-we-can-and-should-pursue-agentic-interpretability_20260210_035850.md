---
ver: rpa2
title: Because we have LLMs, we Can and Should Pursue Agentic Interpretability
arxiv_id: '2506.12152'
source_url: https://arxiv.org/abs/2506.12152
tags:
- interpretability
- agentic
- human
- mental
- humans
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper argues that large language models (LLMs) enable a new
  approach to interpretability: agentic interpretability. Unlike traditional "inspective"
  methods that open the black box, agentic interpretability leverages the model itself
  as a cooperative agent to help humans understand it through multi-turn conversation.'
---

# Because we have LLMs, we Can and Should Pursue Agentic Interpretability

## Quick Facts
- arXiv ID: 2506.12152
- Source URL: https://arxiv.org/abs/2506.12152
- Authors: Been Kim; John Hewitt; Neel Nanda; Noah Fiedel; Oyvind Tafjord
- Reference count: 16
- Primary result: LLMs enable agentic interpretability through multi-turn conversation where the model proactively helps humans understand it by developing mental models of users

## Executive Summary
This paper proposes agentic interpretability as a new approach to understanding AI systems, where large language models serve as cooperative agents in multi-turn conversations to help humans develop mental models of the machine. Unlike traditional "inspective" methods that attempt to open the black box directly, agentic interpretability leverages the LLM's ability to maintain context, infer user knowledge states, and proactively tailor explanations. The approach enables potentially superhuman concept discovery while acknowledging trade-offs in completeness and suitability for high-stakes safety scenarios.

## Method Summary
The framework involves implementing conversational agents that maintain user state representations (either implicit through context or explicit via knowledge graphs) and generate explanations calibrated to inferred user understanding levels. The core components include user state tracking, explanation generation, dialogue management, and concept inventory. Evaluation relies on proxy metrics measuring end-task performance or prediction accuracy, as direct mental model assessment is impossible. The method is positioned as complementary to traditional interpretability approaches rather than a replacement.

## Key Results
- Agentic interpretability enables bidirectional mental model construction between LLM and human
- The approach can discover potentially superhuman concepts that improve human understanding
- Evaluation challenges arise from the human-entangled-in-the-loop nature of the method

## Why This Works (Mechanism)

### Mechanism 1: Mutual Mental Model Construction
- Claim: LLMs can form implicit mental models of users through conversational context and tailor explanations accordingly
- Evidence: MindGames paper suggests LLMs display Theory of Mind-like abilities in multi-step tasks
- Break condition: User knowledge is highly idiosyncratic or LLM fails to track context over long conversations

### Mechanism 2: Zone of Proximal Development Targeting
- Claim: LLMs can identify user's learning frontier and introduce concepts incrementally
- Evidence: Neologism learning work shows new words scaffold learning of machine concepts
- Break condition: Concepts are genuinely superhuman and human cognitive architecture lacks hooks for certain abstractions

### Mechanism 3: Human-Entangled Evaluation Feedback Loop
- Claim: Evaluation must measure end-task outcomes rather than intermediate explanation quality
- Evidence: HCI/XAI literature supports application-grounded metrics for explanation evaluation
- Break condition: Proxy metrics don't capture genuine understanding and lead to misleading signals

## Foundational Learning

- **Mental Models (Cognitive Science)**: Why needed - entire framework hinges on bidirectional mental model construction. Quick check: Can you explain the difference between an implicit and explicit mental model in this context?
- **Zone of Proximal Development (Vygotsky)**: Why needed - provides theoretical basis for how agentic system should pace and scaffold concept introduction. Quick check: How would you identify a user's ZPD from a single conversation turn?
- **Theory of Mind / Recursive Reasoning**: Why needed - RSA-style recursive reasoning is proposed as necessary for efficient communication. Quick check: What's the minimum recursion depth needed for effective teaching?

## Architecture Onboarding

- **Component map**: User query -> Infer user knowledge state from context -> Identify relevant concept(s) -> Generate explanation at appropriate level -> User response -> Update user model -> Loop or terminate with summary artifact
- **Critical path**: 1. User query → 2. Infer user knowledge state from context → 3. Identify relevant concept(s) → 4. Generate explanation at appropriate level → 5. User response → 6. Update user model → 7. Loop or terminate with summary artifact
- **Design tradeoffs**: Completeness vs. interactivity (agentic methods may miss edge cases); Implicit vs. explicit user models (efficient but opaque vs. inspectable but costly); Human vs. LLM proxy evaluation (expensive but generalizable vs. fast but potentially unreliable)
- **Failure signatures**: Repeated explanations (user model not updating); Verbose generic responses (user model too vague); User confusion on "simple" follow-ups (explanation level mismatch); Inability to transfer learned concepts to new examples (rote memorization)
- **First 3 experiments**: 1. Baseline comparison: Single-turn explanation vs. multi-turn agentic dialogue on concept learning; 2. User model ablation: Compare implicit context-only vs. explicit knowledge graph tracking; 3. Case improve validation: Measure whether agentic dialogue leads to better prompt modifications than static documentation

## Open Questions the Paper Calls Out

- Can LLMs serve as reliable proxy evaluators for human responses in agentic interpretability settings? The paper proposes LLM proxies as practical but acknowledges human evaluation remains the "eventual end goal" without validating correlation between proxy and actual human learning outcomes.

- How can we quantitatively measure whether a human has developed an accurate mental model of a machine concept? The paper resorts to proxy metrics like prediction accuracy but acknowledges mental models are internal and unobservable, with proposed proxies potentially not capturing full richness of understanding.

- Can agentic mechanistic interpretability reliably expose deceptive or misaligned behavior? The paper suggests this approach could reveal discrepancies but states agentic interpretability may not be the right tool for deliberately deceptive models without empirical evidence.

- How should researchers navigate cases where concepts blur the boundary between human-defined and machine-defined categories? The paper acknowledges conceptual ambiguity but offers only pragmatic guidance rather than a principled framework for classification or methodology selection.

## Limitations

- The framework's core assumption that LLMs can form accurate user mental models through conversational cues lacks empirical validation
- Agentic methods may not be suitable for high-stakes safety scenarios with potentially deceptive models
- Human-entangled evaluation introduces confounding factors that current proxy metrics may not adequately control for

## Confidence

- **High confidence**: Theoretical foundation linking mental models, ZPD, and recursive reasoning to interpretability goals is well-established in cognitive science
- **Medium confidence**: Proposed evaluation metrics (case improve, case learn) represent reasonable approaches to measuring mental model formation
- **Low confidence**: Core mechanism claim that LLMs can proactively develop and leverage user mental models to improve interpretability lacks empirical evidence

## Next Checks

1. **Mental Model Fidelity Test**: Implement controlled study comparing agentic dialogue vs. static explanations on concept learning, measuring both immediate prediction accuracy and 24-hour retention to validate ZPD targeting claims.

2. **User Model Accuracy Benchmark**: Create benchmark dataset where ground truth user knowledge states are known, then evaluate how accurately different LLM agents can infer these states from conversational cues alone.

3. **Deception Vulnerability Assessment**: Design experiments testing whether agentic interpretability methods can be gamed by deceptive models, directly addressing the framework's limitation in high-stakes safety scenarios.