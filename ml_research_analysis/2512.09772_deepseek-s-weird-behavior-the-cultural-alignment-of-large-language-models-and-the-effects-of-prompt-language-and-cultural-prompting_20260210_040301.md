---
ver: rpa2
title: 'DeepSeek''s WEIRD Behavior: The cultural alignment of Large Language Models
  and the effects of prompt language and cultural prompting'
arxiv_id: '2512.09772'
source_url: https://arxiv.org/abs/2512.09772
tags:
- cultural
- alignment
- please
- dimensions
- importance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates cultural alignment of six large language models
  (GPT-4, GPT-4.1, GPT-4o, GPT-5, DeepSeek-V3, and DeepSeek-V3.1) with the United
  States and China using Hofstede's VSM13 survey framework. Models were tested using
  different prompt languages (English and Simplified Chinese) and cultural prompting
  strategies (US or China).
---

# DeepSeek's WEIRD Behavior: The cultural alignment of Large Language Models and the effects of prompt language and cultural prompting

## Quick Facts
- arXiv ID: 2512.09772
- Source URL: https://arxiv.org/abs/2512.09772
- Authors: James Luther; Donald Brown
- Reference count: 40
- All tested models showed stronger alignment with United States cultural values than Chinese cultural values, with DeepSeek models exhibiting particularly strong US alignment despite Chinese development.

## Executive Summary
This study evaluates cultural alignment of six large language models (GPT-4, GPT-4.1, GPT-4o, GPT-5, DeepSeek-V3, and DeepSeek-V3.1) with the United States and China using Hofstede's VSM13 survey framework. Models were tested using different prompt languages (English and Simplified Chinese) and cultural prompting strategies (US or China). Results show all models exhibited stronger alignment with the United States than China, with DeepSeek models showing particularly strong US alignment despite being Chinese-developed. GPT-4o and GPT-4.1 demonstrated the most flexibility, successfully aligning with both cultures when prompted appropriately. Cultural prompting proved highly effective in English but less so in Simplified Chinese. GPT-4 showed unexpected closer alignment to China when prompted in English without cultural prompting. The findings highlight significant cultural biases in LLMs and suggest that prompt language and cultural prompting can be effective tools for cultural adaptation in AI applications.

## Method Summary
The study used Hofstede's VSM13 24-question survey instrument to measure cultural alignment across six dimensions (Power Distance Index, Individualism, Masculinity, Uncertainty Avoidance, Long-term Orientation, and Indulgence). Six LLMs were tested under six prompting conditions (English, English with US cultural prompting, English with China cultural prompting, Simplified Chinese, Simplified Chinese with US cultural prompting, Simplified Chinese with China cultural prompting). For each condition, 20 complete survey responses were generated using temperature=2 to simulate population diversity. The mean responses were converted to Hofstede dimension values using established formulas, and alignment distance was calculated as the sum of absolute differences from reference country values.

## Key Results
- All models showed stronger alignment with United States cultural values than Chinese cultural values
- DeepSeek models (Chinese-developed) exhibited particularly strong US alignment despite their origin
- GPT-4o and GPT-4.1 demonstrated the most cultural flexibility, successfully aligning with both cultures when prompted
- Cultural prompting in English improved China alignment by 29.8% but only 4.9% when using Simplified Chinese prompts
- GPT-4 showed closer alignment to China when prompted in English without cultural prompting

## Why This Works (Mechanism)

### Mechanism 1: Training Data Composition Drives Default Cultural Alignment
- Claim: LLMs default to WEIRD cultural patterns primarily due to English-dominated training corpora, not model architecture or developer origin.
- Mechanism: Models learn statistical patterns from training data; when English-language, Western-authored content dominates, cultural values embedded in that content become the model's default response patterns.
- Evidence anchors: DeepSeek models showing US bias despite Chinese origin; "LLMs tend to perpetuate the biases in the data on which they are trained."

### Mechanism 2: Cultural Prompting Activates Contextual Persona Adoption
- Claim: System prompts specifying cultural identity cause models to surface latent cultural patterns associated with that identity from training data associations.
- Mechanism: When prompted "You are an average person from China," models retrieve statistically-associated cultural response patterns linked to "China" in training data.
- Evidence anchors: 29.8% improvement in China alignment with English cultural prompting; effectiveness varies by model.

### Mechanism 3: Language-Specific Prompting Activates Cultural Context Frames
- Claim: Querying in a non-English language surfaces culturally-specific response patterns because multilingual training creates language-indexed cultural clusters.
- Mechanism: Models trained on multilingual data develop separate (but overlapping) response distributions per language; prompting in Simplified Chinese activates the Chinese-language cultural subspace.
- Evidence anchors: 15.2% improvement in China alignment when switching from English to Simplified Chinese; cultural prompting in English more effective than language switching alone.

## Foundational Learning

- **Hofstede's Cultural Dimensions**: Six dimensions (PDI, IDV, MAS, UAI, LTO, IVR) quantify cultural values; needed to understand the measurement framework and interpret alignment results.
  - Quick check: Can you explain why Power Distance Index (PDI) measures hierarchy acceptance and why the US and China would differ on this dimension?

- **WEIRD Bias in AI Systems**: "Western, Educated, Industrialized, Rich, Democratic" societies are overrepresented in AI training data, explaining systematic cultural skew across all tested models.
  - Quick check: Why would a Chinese-developed model like DeepSeek still exhibit US-aligned cultural values according to this paper's findings?

- **Temperature Sampling and Response Variance**: Temperature=2 generates varied responses simulating population diversity; critical for understanding methodology and reproducing results.
  - Quick check: Why would higher temperature (2.0 vs. 0.0) be necessary for simulating survey responses from a "population" rather than a single deterministic answer?

## Architecture Onboarding

- **Component map**: Survey prompt system → API batch executor → Dimension calculator → Alignment distance metric
- **Critical path**: 1. Define cultural condition (language × cultural prompt = 6 conditions per model) 2. Execute 20 survey completions per condition with temperature=2 3. Compute mean response per question 4. Apply dimension equations to calculate Hofstede values 5. Compute alignment distance to target country 6. Compare across conditions to isolate effects
- **Design tradeoffs**: Population size (n=20) limits statistical power; temperature=2 generates diversity but may introduce noise; two countries only limits generalizability; survey modification may shift model interpretation
- **Failure signatures**: Cultural prompting ineffectiveness (rigid cultural encoding); language switching backfires (poor multilingual training); negative improvement (ceiling effects)
- **First 3 experiments**: 1. Reproduce baseline alignment on single model 2. Test additional models (Claude, Llama, Gemini) 3. Isolate language vs. prompt effects through ablation study

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on self-reported cultural dimensions through survey prompts introduces measurement uncertainty
- Temperature=2 setting generates high response variance that may obscure true cultural alignment patterns
- Focus on only two countries (US and China) limits generalizability to other cultural contexts
- Modification of survey questions from individual to collective framing may alter model interpretation

## Confidence

*High confidence:* All models exhibit stronger alignment with US cultural values than Chinese cultural values, and this bias persists across models from different developers including Chinese-developed DeepSeek models.

*Medium confidence:* Effectiveness of cultural prompting strategies, particularly that English-language cultural prompts are significantly more effective than Simplified Chinese prompts for improving China alignment.

*Low confidence:* Specific numerical values for alignment distances and percentage improvements, as these depend on exact model versions, API implementations, and potential post-processing steps not fully specified.

## Next Checks

1. **Ablation study on prompt language vs. cultural framing**: Run controlled experiments keeping language constant while varying only the cultural framing intensity to isolate whether language switching or cultural prompting drives the observed effects.

2. **Cross-cultural generalization test**: Apply the same methodology to models from other cultural contexts (e.g., Japanese, Indian, or Arabic-speaking regions) to determine whether the WEIRD bias pattern extends beyond US-China comparisons.

3. **Temporal stability analysis**: Test the same models at different time points or with different random seeds to quantify the stability of cultural alignment scores and determine whether the observed patterns represent consistent cultural encoding or sampling artifacts.