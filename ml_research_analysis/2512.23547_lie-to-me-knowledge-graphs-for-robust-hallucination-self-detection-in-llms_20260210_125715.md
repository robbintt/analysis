---
ver: rpa2
title: 'Lie to Me: Knowledge Graphs for Robust Hallucination Self-Detection in LLMs'
arxiv_id: '2512.23547'
source_url: https://arxiv.org/abs/2512.23547
tags:
- knowledge
- hallucination
- graphs
- methods
- self-detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of hallucination detection in Large
  Language Models (LLMs), where models generate convincing but factually incorrect
  statements. The core method introduces knowledge graphs (KGs) to improve self-detection
  by structuring LLM responses into entities and relations, enabling more granular
  and precise analysis of factual claims.
---

# Lie to Me: Knowledge Graphs for Robust Hallucination Self-Detection in LLMs

## Quick Facts
- arXiv ID: 2512.23547
- Source URL: https://arxiv.org/abs/2512.23547
- Authors: Sahil Kale; Antonio Luca Alfeo
- Reference count: 5
- Primary result: KG integration improves hallucination detection F1 scores by up to 20% and AUC-PR by 21%

## Executive Summary
This paper addresses the challenge of hallucination detection in LLMs by introducing knowledge graphs to structure model outputs into atomic facts. The core innovation lies in decomposing responses into (subject, relation, object) triples, enabling more granular and precise analysis of factual claims. Experiments demonstrate that this approach consistently improves detection accuracy, F1 scores, and AUC-PR across multiple datasets and models, with Self-Confidence achieving the highest absolute F1 scores. The findings suggest hallucinations often stem from incorrect fact associations rather than missing facts, highlighting the importance of how models link information.

## Method Summary
The method extracts (subject, relation, object) triples from LLM outputs using a deterministic prompt (temperature=0.0), then scores each triple independently for factual consistency or confidence using the LLM (temperature=1.0). Scores are averaged to produce a final hallucination likelihood. The approach was evaluated on SimpleQA and WikiBio datasets using GPT-4o and Gemini 2.5 Flash, comparing Self-Confidence, Self-Questioning, and SelfCheckGPT methods with and without KG integration. Performance was measured using Accuracy, F1-score, and AUC-PR, with optimal thresholds determined through hyperparameter search.

## Key Results
- KG integration improved F1 scores by up to 20% and AUC-PR by 21% compared to baseline methods
- Self-Confidence achieved the highest absolute F1 scores across datasets and models
- Multi-sample approaches did not outperform single-sample ones, indicating structured fact-level analysis is more efficient
- Improved performance with KGs indicates hallucinations often arise from incorrect fact associations rather than missing atomic facts

## Why This Works (Mechanism)

### Mechanism 1
Decomposing LLM outputs into atomic knowledge graph triples enables more precise hallucination detection than evaluating responses as undifferentiated text blocks. A single prompt instructs the LLM to extract (subject, relation, object) triples from its output. Each triple is then scored independently for consistency or confidence, and scores are averaged to produce a final hallucination likelihood. This isolates specific claims rather than forcing global assessments. Core assumption: LLMs can more accurately evaluate individual atomic facts than composite statements containing multiple interdependent claims.

### Mechanism 2
Structured representations activate latent reasoning capabilities in LLMs that remain dormant when processing unstructured text. Knowledge graphs impose a fixed schema (entities as nodes, relations as edges) that constrains the verification task to direct entity-relation comparisons. This reduction in problem complexity enables more reliable self-interrogation and consistency checking. Core assumption: LLMs possess reasoning capabilities that are reliably triggered by structured prompts, and this activation is consistent across domains.

### Mechanism 3
Hallucinations frequently stem from incorrect fact associations rather than missing atomic facts, making association-level analysis more diagnostic than presence/absence checks. The paper's Self-Questioning and Self-Confidence methods probe whether the model can verify its own claimed relationships. Improved performance with KGs suggests models often retain correct entity knowledge but generate false associations between them. Core assumption: Hallucination patterns are predominantly associative errors (linking correct entities via incorrect relations) rather than fabrication of entirely unknown entities.

## Foundational Learning

- **Knowledge Graph Triple Extraction**: Decomposing text into (subject, relation, object) triples is the foundation of the entire method. Poor extraction quality propagates errors through all downstream scoring. Quick check: Given "Marie Curie discovered polonium in 1898," can you identify subject, relation, and object? What ambiguities arise with "Apple acquired NeXT in 1996"?

- **Self-Detection vs. External Verification Paradigms**: This paper focuses on self-detection (generator = detector). Understanding the trade-offs explains why multi-sample methods underperformed—external consistency offers limited signal when model outputs are already stable. Quick check: Why might a model consistently generate the same hallucination across multiple samples? What does this imply for consistency-based detection?

- **Threshold Calibration for Detection Scores**: The paper performs hyperparameter search to find optimal classification thresholds per method. Scores are not directly comparable across methods without this calibration step. Quick check: If Method A produces scores in [0.4, 0.9] and Method B in [0.1, 0.6], how would you fairly compare their binary classification performance?

## Architecture Onboarding

- **Component map**: Output text → KG extraction prompt → Triple set → Per-triple scoring → Score aggregation → Threshold comparison → Binary label
- **Critical path**: Output text → KG extraction prompt → Triple set → Per-triple scoring → Score aggregation → Threshold comparison → Binary label
- **Design tradeoffs**: Single-sample (Self-Confidence/Self-Questioning) vs. Multi-sample (SelfCheckGPT): Paper finds single-sample + KG outperforms multi-sample at lower computational cost for capable models (GPT-4o, Gemini 2.5 Flash). Temperature settings: KG construction uses temperature=0.0 (deterministic); detection uses temperature=1.0 (varied reasoning paths). Triple-level vs. sentence-level granularity: Triple-level improves precision but increases LLM API calls proportionally to fact density.
- **Failure signatures**:
  1. Extraction failures: Complex sentences with coreference or nested clauses produce incomplete/incorrect triples
  2. Score miscalibration: Per-method thresholds don't transfer across models; re-calibration required for each new LLM
  3. Domain mismatch: Entity extraction quality degrades for specialized domains not well-represented in model training data
- **First 3 experiments**:
  1. Baseline comparison: Run Self-Confidence and Self-Questioning (with/without KG) on 50 samples from target domain; measure F1, accuracy, and AUC-PR to establish domain-specific gains
  2. Extraction quality audit: Manually inspect triple extraction accuracy for 20 representative outputs; identify systematic extraction errors (e.g., missing relations, entity boundary issues)
  3. Threshold sensitivity analysis: Plot F1 vs. threshold curves for each method; verify optimal thresholds are stable across different random seeds and sample subsets

## Open Questions the Paper Calls Out
- **Open Question 1**: Does the effectiveness of KG-driven self-detection generalize to low-resource languages where LLM capabilities for structured entity-relation extraction may be less reliable? The study focused solely on English (SimpleQA, WikiBio), and the authors explicitly identify this linguistic boundary as a limitation.

- **Open Question 2**: Can the scoring mechanisms be calibrated to perform robustly without requiring a dataset-specific hyper-parameter search for optimal decision thresholds? The authors note that their analysis is "limited to best-performing thresholds" identified via search, and that "score interpretation is method dependent."

- **Open Question 3**: Would a larger evaluation dataset reveal significant performance gains for multi-sample methods that were obscured by the current study's "relatively small multi-sample test set"? The paper highlights the "relatively small multi-sample test set" as a limitation and calls for "multi-sample robustness" exploration.

## Limitations
- Exact prompt templates for KG extraction and scoring are not provided, preventing exact replication
- Evaluation focused on relatively constrained datasets (SimpleQA, WikiBio), limiting generalizability to open-domain tasks
- Multi-sample methods underperformed single-sample approaches, but the paper doesn't fully explore whether this reflects model capabilities or suboptimal sampling strategies

## Confidence
- **High**: Knowledge graphs improve detection precision over raw text baselines; KG benefits are consistent across Self-Confidence and Self-Questioning methods; computational efficiency gains from single-sample approaches
- **Medium**: Hallucinations primarily result from incorrect fact associations rather than missing facts; KG structure activates latent reasoning capabilities; multi-sample methods underperform due to model capabilities rather than sampling strategy
- **Low**: Exact prompt formulations are sufficient for reliable triple extraction across all domains; optimal thresholds are stable across different models and datasets without re-calibration

## Next Checks
1. **Prompt Transferability Test**: Implement the KG extraction prompt on a held-out domain (e.g., technical documentation) and measure extraction accuracy. Compare against the paper's reported performance on SimpleQA to assess domain generalization limits.

2. **Threshold Stability Analysis**: Run threshold optimization on 5 random 80/20 train/test splits of the SimpleQA dataset. Measure coefficient of variation in optimal thresholds across splits to quantify calibration stability.

3. **Associative Error Isolation**: Create synthetic test cases where entity knowledge is correct but relations are deliberately corrupted. Measure detection accuracy to directly test whether KG methods excel at identifying associative errors versus entity-level hallucinations.