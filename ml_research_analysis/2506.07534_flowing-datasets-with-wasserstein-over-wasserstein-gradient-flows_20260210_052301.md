---
ver: rpa2
title: Flowing Datasets with Wasserstein over Wasserstein Gradient Flows
arxiv_id: '2506.07534'
source_url: https://arxiv.org/abs/2506.07534
tags:
- wasserstein
- gradient
- then
- cited
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a principled framework for optimizing functionals
  over the space of probability measures on probability measures, using the Wasserstein
  over Wasserstein (WoW) gradient flow approach. The authors represent labeled datasets
  as mixture distributions supported on class-conditional distributions, and endow
  this space with the WoW distance, deriving a differential structure to define WoW
  gradient flows that decrease a given objective functional.
---

# Flowing Datasets with Wasserstein over Wasserstein Gradient Flows

## Quick Facts
- **arXiv ID:** 2506.07534
- **Source URL:** https://arxiv.org/abs/2506.07534
- **Reference count:** 40
- **One-line primary result:** Introduces WoW gradient flows for optimizing functionals over spaces of probability measures on probability measures

## Executive Summary
This paper presents a principled framework for optimizing functionals over the space of probability measures on probability measures, using Wasserstein over Wasserstein (WoW) gradient flows. The authors represent labeled datasets as mixture distributions supported on class-conditional distributions, and endow this space with the WoW distance, deriving a differential structure to define WoW gradient flows that decrease a given objective functional. The core method involves defining a Wasserstein differentiable functional (specifically, a Maximum Mean Discrepancy with Sliced-Wasserstein based kernels) and simulating gradient flows on the WoW space to optimize it. This enables structured transitions of classes toward other classes, with applications to transfer learning and dataset distillation.

## Method Summary
The method represents labeled datasets as mixture distributions P = (1/C) Σ δ_{μc} where each class c is represented by its conditional distribution μc of features. The full dataset space is endowed with the WoW distance, enabling gradient flow optimization of functionals like MMD. The authors use a forward Euler discretization scheme to approximate the WoW gradient flow, updating particles via x^{c}_{i,k+1} = x^{c}_{i,k} - τ ∇_{WW²} F(P_k)(μc_k)(x^{c}_{i,k}). The gradient is computed via autodifferentiation rescaled by nC. The MMD functional uses a Riesz Sliced-Wasserstein kernel K(μ, ν) = -SW₂(μ, ν) that avoids bandwidth hyperparameter tuning.

## Key Results
- On MNIST-to-FashionMNIST domain adaptation, achieves 100% accuracy with 200 samples per class
- On dataset distillation, outperforms existing approaches by 2-4% accuracy with 1-10 samples per class
- The approach is significantly faster (13-294x speedup) than comparable methods for transfer learning tasks

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Measure Representation of Labeled Datasets
Labeled datasets can be represented as probability distributions over probability distributions, enabling geometric operations that respect class structure. Each class c is represented by its conditional distribution μc of features, creating a two-level structure where the WoW distance computes optimal transport between distributions (inter-class) while using Wasserstein distance between samples within each class (intra-class). This fails when class-conditional distributions are highly multimodal or when class boundaries are not semantically meaningful.

### Mechanism 2: Forward Euler Discretization with WoW Gradients
The WoW gradient flow can be approximated tractably via explicit forward Euler steps on particles, avoiding costly JKO optimization. At iteration k, update P via P_{k+1} = exp_{P_k}(-τ ∇_{WW²} F(P_k)). This reduces to particle updates and remains stable for suitable step sizes τ with momentum (m = 0.9 used in experiments). Instability occurs for large τ requiring careful tuning.

### Mechanism 3: MMD with Riesz Sliced-Wasserstein Kernel
The MMD with kernel K(μ, ν) = -SW₂(μ, ν) provides a tractable discrepancy with interpretable gradients and no bandwidth hyperparameter. The Riesz kernel avoids bandwidth selection and converges well in general. Performance degrades for very high-dimensional data without sufficient projections L (L ≥ 300 needed for MNIST generation).

## Foundational Learning

- **Wasserstein-2 Geometry and Gradient Flows**: Why needed here: The entire WoW framework builds on the Riemannian structure of (P₂(M), W₂), including tangent spaces, exponential maps, and gradient flows defined via the continuity equation. Quick check question: Can you derive the Wasserstein gradient of a potential energy F(μ) = ∫ V dμ and explain why exp_μ(v) = (exp_Id ∘ v)_#μ?

- **Optimal Transport Maps and Kantorovich Potentials**: Why needed here: The WoW gradient flow requires computing gradients of SW₂, which involves Kantorovich potentials ψ_θ between projected measures. Understanding the duality is essential. Quick check question: For μ absolutely continuous, what is the form of the optimal transport map T from μ to ν, and how does φ_{μ,ν} relate to it?

- **Maximum Mean Discrepancy (MMD) and Kernel Methods**: Why needed here: The objective functional F(P) = ½MMD²(P, Q) decomposes into potential and interaction energies; its WoW gradient involves kernel derivatives. Quick check question: For MMD²_K(μ, ν) with positive definite kernel K, write the U-statistic estimator and identify the witness function?

## Architecture Onboarding

- Component map: Input: Source P₀ = (1/C)Σ δ_{μc,n}, Target Q = (1/C)Σ δ_{νc,n} -> Kernel Layer: Compute K(μc, νc') = -SW₂(μc, νc') for all pairs via L projections -> MMD Objective: F(P) = ½MMD²_K(P, Q) = V(P) + W(P) + const -> Gradient Computation: ∇_{WW²} F(P)(μc) via autodiff, rescaled by Cn -> Forward Update: x^{c}_{i,k+1} = x^{c}_{i,k} - τ·(gradient) + m·v_k (momentum) -> Output: Flowed particles {x^{c}_{i,K}} approximating target class structure

- Critical path: The SW distance approximation with L projections is the computational bottleneck (O(C² L n log n) per iteration). Insufficient L causes noisy gradients; too large L is costly.

- Design tradeoffs:
  - Riesz vs Gaussian SW kernel: Riesz avoids bandwidth tuning but is not positive definite; Gaussian requires bandwidth selection
  - Number of projections L: Low L → fast but noisy; high L → accurate but slow. Paper uses L = 500 for most experiments
  - Momentum m: Accelerates convergence (m = 0.9) but may overshoot; no momentum is more stable but slower
  - Ambient vs embedded space: Embedding (ψ_θ) reduces dimension but adds complexity; ambient space is simpler but may struggle in high dimensions

- Failure signatures:
  - Class collapse: All particles converge to same location → learning rate too high or kernel bandwidth too narrow
  - Non-convergence with different class counts: If source has N classes and target M < N, standard flow cannot merge mass (Figure 17). Requires weight adjustment or unbalanced OT
  - Blurry generated images: Insufficient projections L (Figure 14) or insufficient iterations

- First 3 experiments:
  1. Synthetic rings validation: Replicate the three-rings experiment (Figure 1) with K(μ, ν) = -SW₂(μ, ν), τ = 0.1, L = 500. Verify that each distribution forms a ring and converges to target
  2. MNIST→FMNIST domain adaptation: Flow n = 200 samples per class, τ = 0.05, m = 0.9, L = 500. Measure pretrained classifier accuracy on flowed samples. Expect convergence to 100% accuracy as in Figure 3
  3. Ablation on projections L: On MNIST generation from noise, sweep L ∈ {10, 50, 100, 300, 500, 1000} with fixed iterations. Plot image quality vs L to identify minimum viable L for your dimensionality

## Open Questions the Paper Calls Out

- **Can quantitative convergence guarantees be established for the proposed discretized WoW gradient flow schemes?** While the existence of the continuous flow is proven via the JKO scheme, the practical forward Euler scheme relies on heuristic discretization without formal rate analysis. Rigorous theoretical bounds on the error between the continuous WoW gradient flow and its discretized particle approximation would resolve this.

- **Can the differential structure and gradient flow theory be extended to non-compact Riemannian manifolds?** The theoretical derivations rely on the assumption that the manifold M is compact and connected to ensure the existence and uniqueness of optimal transport maps. A formal extension of the WoW differential structure and flow existence theorems to spaces like ℝᵈ without compactness assumptions would resolve this.

- **Is the WoW gradient flow well-defined and tractable for internal energies or f-divergences?** The paper focuses on potential and interaction energies (MMD), but internal energies (e.g., entropy) introduce regularization constraints that complicate the differential structure. Derivation of the WoW gradient for specific internal energy functionals and an analysis of their convexity along generalized geodesics would resolve this.

## Limitations

- The empirical validation relies heavily on specific synthetic datasets and standard vision benchmarks, which may not generalize to more complex domain shifts or high-dimensional distributions
- Computational complexity scales as O(C²Ln log n) per iteration due to the SW distance approximation, which could become prohibitive for large C or n
- The stability of the forward Euler scheme for WoW gradient flows is assumed but not rigorously proven beyond numerical experiments

## Confidence

- **High Confidence**: The theoretical framework for WoW gradient flows and their relationship to the continuity equation is mathematically sound and well-established in OT literature
- **Medium Confidence**: The computational tractability claims (speedups of 13-294x) are supported by experiments but depend on specific implementation details and hardware
- **Medium Confidence**: The effectiveness for dataset distillation and transfer learning is demonstrated but may be sensitive to hyperparameters like L, τ, and m

## Next Checks

1. **High-dimensional robustness**: Test the method on higher-dimensional datasets (e.g., CIFAR-100 or medical imaging) to verify whether L = 500 projections remain sufficient or if performance degrades with dimensionality
2. **Theoretical stability analysis**: Conduct a formal analysis of the forward Euler scheme's stability conditions for WoW gradient flows, including explicit bounds on τ as a function of the functional's smoothness
3. **Alternative kernels**: Compare the Riesz SW kernel against other sliced-Wasserstein based kernels (e.g., Gaussian SW kernel) across all applications to quantify the impact of kernel choice on performance and hyperparameter sensitivity