---
ver: rpa2
title: Quantile Advantage Estimation for Entropy-Safe Reasoning
arxiv_id: '2509.22611'
source_url: https://arxiv.org/abs/2509.22611
tags:
- entropy
- advantage
- zhang
- reasoning
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of unstable entropy dynamics in
  reinforcement learning with verifiable rewards (RLVR), where training often oscillates
  between entropy collapse and explosion. The core issue is traced to the mean baseline
  in value-free RL methods like GRPO and DAPO, which improperly penalizes negative-advantage
  samples under reward outliers.
---

# Quantile Advantage Estimation for Entropy-Safe Reasoning

## Quick Facts
- arXiv ID: 2509.22611
- Source URL: https://arxiv.org/abs/2509.22611
- Reference count: 40
- Primary result: Minimal modification replacing mean baseline with K-quantile baseline stabilizes entropy, prevents collapse/explosion, and yields sustained pass@1 gains on math reasoning benchmarks

## Executive Summary
This paper addresses entropy instability in reinforcement learning with verifiable rewards (RLVR), where training oscillates between entropy collapse and explosion due to mean baseline design in value-free methods like GRPO and DAPO. The core insight is that mean baselines improperly penalize negative-advantage samples under reward outliers, causing unstable entropy dynamics. The proposed Quantile Advantage Estimation (QAE) replaces the mean baseline with a group-wise K-quantile baseline, creating a two-regime gate that selectively reinforces rare successes on hard queries and suppresses residual failures on easy ones. Under first-order softmax updates, QAE provides two-sided entropy safety with provable bounds on one-step entropy change, preventing both collapse and explosion. Empirically, QAE stabilizes entropy, sparsifies credit assignment (roughly 80% of responses receive zero advantage with tuned K), and yields sustained pass@1 gains on Qwen3-8B/14B-Base across math reasoning benchmarks.

## Method Summary
QAE is a drop-in modification to value-free RL methods (GRPO/DAPO) that replaces the mean baseline in advantage calculation with a K-quantile baseline. For each query group with G sampled responses and binary rewards, the method computes empirical success rate p(q) = mean({R_i}), determines the regime (hard if p ≤ 1-K, easy if p > 1-K), sets baseline b_K = 0 for hard or b_K = 1 for easy, and computes standardized advantage Â_i = (R_i - b_K) / (std({R_i}) + ε). This creates a response-level, two-regime gate: on hard queries it reinforces rare successes while on easy queries it targets remaining failures. The method maintains all other components of existing frameworks (clipping, token-level loss normalization, dynamic sampling) unchanged.

## Key Results
- Entropy stabilization: Replaces unstable entropy dynamics with controlled trajectories, preventing both collapse and explosion
- Sustained accuracy gains: Qwen3-8B/14B-Base achieves consistent pass@1 improvements across AIME 2024/2025 and AMC 2023 benchmarks
- Sparsified credit assignment: Tuned K (~0.4) results in roughly 80% of responses receiving zero advantage, concentrating learning on informative samples
- Two-sided entropy safety: Under first-order softmax updates, QAE provides provable bounds on one-step entropy change in both high- and low-success regimes

## Why This Works (Mechanism)

### Mechanism 1: Two-Regime Gating via Quantile Baseline
Replacing the mean baseline with a K-quantile creates a query-difficulty-dependent gate that selectively reinforces rare successes on hard problems and suppresses residual failures on easy ones. For binary rewards with group success rate p(q), the K-quantile baseline becomes 0 when p(q) ≤ 1-K (hard queries) and 1 when p(q) > 1-K (easy queries). This masks either all negative-advantage or all positive-advantage samples per regime, concentrating signal where it's most informative. The mechanism assumes binary or near-binary reward structure with reliable empirical success rates from group sampling.

### Mechanism 2: Two-Sided Entropy Safety via Extremal Baseline Selection
Under first-order softmax updates, the K-quantile baseline achieves extremal one-step entropy changes—minimum in the low-success regime (explosion-proof) and maximum in the high-success regime (collapse-proof). Entropy change ∆H(q;b) is strictly increasing in baseline b (via covariance structure). Setting b_K = 0 for hard queries minimizes entropy increase; setting b_K = 1 for easy queries maximizes it. Token-level clipping cannot achieve this because it rescales steps without changing the response-level baseline. The mechanism assumes first-order approximation holds and policy remains non-uniform.

### Mechanism 3: Sparsified Credit Assignment Concentrates Learning
With appropriately tuned K (~0.4), roughly 80% of responses receive zero advantage, focusing gradient computation on the most informative ~20%. The quantile threshold naturally zeros out advantages for the majority class in each regime (failures on hard queries, successes on easy ones). This reduces gradient noise and computational cost while amplifying signal from informative samples. The mechanism assumes the informative minority lies at regime boundaries near-threshold success rates.

## Foundational Learning

- **Concept: Advantage estimation with baselines**
  - Why needed: QAE modifies the baseline in advantage calculation; understanding why baselines reduce variance is essential
  - Quick check: Why does subtracting a baseline from rewards not bias policy gradient estimates, and what property must the baseline satisfy?

- **Concept: Entropy as exploration control in policy gradient methods**
  - Why needed: The paper frames baseline design as the primary entropy control mechanism; distinguishing token-level from response-level entropy effects is critical
  - Quick check: How does policy entropy relate to exploration, and what happens at the extremes (near-zero vs. very high entropy)?

- **Concept: Value-free RL methods (GRPO, DAPO)**
  - Why needed: QAE is a drop-in modification to these algorithms; understanding their group-relative advantage computation is prerequisite
  - Quick check: In GRPO/DAPO, how is the advantage computed without a value network, and what role does group sampling play?

## Architecture Onboarding

- **Component map:** Query q, group of G sampled responses {o_i} with binary rewards {R_i} -> Compute empirical success rate p(q) = mean({R_i}) -> Determine regime: hard (p ≤ 1-K) vs. easy (p > 1-K) -> Set baseline: b_K = 0 (hard) or b_K = 1 (easy) -> Compute standardized advantage: Â_i = (R_i - b_K) / (std({R_i}) + ε) -> Feed to existing policy gradient objective (DAPO/GRPO)

- **Critical path:** The single-line change is the baseline calculation. Everything else (sampling, clipping, token-level loss) remains unchanged.

- **Design tradeoffs:** K selection: Lower K → more positive-advantage samples → lower entropy (collapse risk); higher K → more negative-advantage samples → higher entropy (explosion risk). Paper recommends K=0.4 with Clip-Higher. Group size G: Larger G gives more reliable quantiles but increases sampling cost. Integration with token-level controls: QAE is orthogonal to Clip-Higher, Clip-Cov, KL-Cov; compose by keeping their hyperparameters fixed.

- **Failure signatures:** Entropy still collapses: K too low; increase toward 0.6. Entropy explodes: K too high; decrease toward 0.3-0.4. Performance plateaus early: Check if dynamic sampling constraint (0 < correct < G) is violated; may need curriculum or data filtering. Uneven token distributions: Token-level diagnostics show homogenization → entropy regime may be misaligned with actual difficulty distribution.

- **First 3 experiments:**
  1. **Baseline replication:** Implement DAPO with mean baseline on Qwen3-8B-Base on AIME'24; verify entropy dynamics match Figure 1 (left). This establishes the instability you're trying to fix.
  2. **Drop-in QAE:** Replace mean with K-quantile (K=0.4); compare entropy trajectory and pass@1. Expected: flatter entropy curve, sustained accuracy gains (Figure 5).
  3. **K sensitivity sweep:** Run K ∈ {0.2, 0.4, 0.6, 0.8} with fixed other hyperparameters; plot entropy, accuracy, response length (Figure 9 pattern expected). Identify stable operating range for your model/dataset combination.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can dynamic or scheduled K (e.g., two-phase curricula) outperform fixed K for balancing exploration-exploitation across training stages?
- Basis: Section 8 explicitly calls out exploring schedules or two-phase curricula
- Why unresolved: Current experiments use a single fixed K=0.4 throughout training
- What evidence would resolve it: Ablations comparing fixed K to scheduled K on pass@1 and entropy trajectories

### Open Question 2
- Question: Can K be adapted automatically based on model state (success rate, entropy, gradient variance) to eliminate manual tuning?
- Basis: Section 8 suggests adapting K to model state to remove manual tuning
- Why unresolved: K is currently a hyperparameter set by inspecting baseline policy entropy
- What evidence would resolve it: An adaptive K scheme matching or exceeding fixed-K performance without per-baseline tuning

### Open Question 3
- Question: Does quantile-based advantage estimation transfer to algorithms with learned value functions like PPO?
- Basis: Section 8 proposes embedding quantile-baseline idea into PPO's whitening/normalization
- Why unresolved: QAE is formulated and tested only for value-free methods
- What evidence would resolve it: PPO+QAE experiments showing comparable entropy safety and performance gains

### Open Question 4
- Question: Does the two-regime entropy safety guarantee extend to non-binary or continuous reward settings?
- Basis: Theoretical analysis and empirical evaluation assume binary rewards; generalization is unexplored
- Why unresolved: Quantile baseline behavior under sparse or continuous rewards may differ from hard/easy threshold structure
- What evidence would resolve it: Experiments on tasks with partial-credit or multi-level rewards

## Limitations

- **Theoretical scope limits:** Entropy safety proof relies on first-order softmax updates and binary rewards; extension to continuous rewards requires additional analysis
- **Empirical validation gaps:** Results demonstrated only on Qwen3 models and DAPO-Math-17K; generalization to other model families and datasets remains untested
- **Reproducibility concerns:** Key hyperparameters underspecified (ε value, exact training steps, dynamic sampling implementation); dataset access to DAPO-Math-17K is not public

## Confidence

**High confidence:** The mechanism connecting mean baseline to entropy instability is well-founded—binary rewards create systematic bias when group success rates are imbalanced, and the empirical observation of entropy collapse/explosion patterns is reproducible.

**Medium confidence:** The extremal entropy bounds hold under stated assumptions, but real-world violations (large step sizes, non-softmax policies) may reduce effectiveness. The 80% sparsity claim appears robust across reported experiments but depends critically on K tuning.

**Low confidence:** Generalization to non-binary rewards and different model architectures lacks theoretical or empirical support. The interaction between QAE and token-level entropy controls is described as orthogonal but not experimentally validated in combination.

## Next Checks

1. **Cross-dataset validation:** Implement QAE on a public RLVR dataset (e.g., RewardMath or RewardBench) with non-binary rewards. Measure whether quantile baselines maintain entropy safety and accuracy gains when reward distributions deviate from binary.

2. **Architectural generalization:** Apply QAE to Llama-3 or Claude models using the same DAPO framework. Compare entropy trajectories and pass@1 to establish whether baseline design effects are model-family dependent or universal.

3. **Dynamic sampling failure analysis:** Systematically test scenarios where dynamic sampling constraint (0 < correct < G) is violated (e.g., all-fail or all-pass groups). Measure QAE performance degradation and whether fallback mechanisms restore stability.