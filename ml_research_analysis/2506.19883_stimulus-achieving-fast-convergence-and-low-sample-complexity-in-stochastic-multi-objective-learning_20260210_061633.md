---
ver: rpa2
title: 'STIMULUS: Achieving Fast Convergence and Low Sample Complexity in Stochastic
  Multi-Objective Learning'
arxiv_id: '2506.19883'
source_url: https://arxiv.org/abs/2506.19883
tags:
- stimulus
- stimulus-m
- follows
- where
- sample
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces STIMULUS, a variance-reduced stochastic multi-gradient
  descent (SMGD) algorithm for multi-objective optimization (MOO) that achieves fast
  convergence and low sample complexity. Unlike existing SMGD methods that suffer
  from slow convergence due to noisy gradient estimates, STIMULUS employs a recursive
  variance reduction framework with periodic full gradient evaluations to construct
  more accurate stochastic multi-gradient estimators.
---

# STIMULUS: Achieving Fast Convergence and Low Sample Complexity in Stochastic Multi-Objective Learning

## Quick Facts
- arXiv ID: 2506.19883
- Source URL: https://arxiv.org/abs/2506.19883
- Reference count: 40
- Primary result: STIMULUS achieves O(1/T) convergence for non-convex MOO and O(exp(-μT)) for strongly convex MOO with sample complexities matching deterministic methods

## Executive Summary
STIMULUS introduces a variance-reduced stochastic multi-gradient descent algorithm that addresses the fundamental challenge of slow convergence in multi-objective optimization. By combining recursive variance reduction with periodic full gradient evaluations, STIMULUS constructs more accurate stochastic multi-gradient estimators that achieve convergence rates comparable to deterministic methods while maintaining computational efficiency. The algorithm demonstrates significant improvements in both convergence speed and sample complexity across convex and non-convex settings, with enhanced momentum variants and adaptive batching versions that eliminate the need for periodic full gradient evaluations.

## Method Summary
The STIMULUS algorithm employs a recursive variance reduction framework that periodically evaluates full gradients to construct more accurate stochastic multi-gradient estimators. Unlike traditional stochastic multi-gradient descent methods that suffer from slow convergence due to noisy gradient estimates, STIMULUS maintains momentum-based recursive estimates that reduce variance over time. The algorithm operates in epochs where each epoch begins with a full gradient evaluation, followed by stochastic updates using the improved gradient estimates. The enhanced momentum variant STIMULUS-M accelerates convergence further by incorporating momentum into the variance reduction process, while adaptive batching versions STIMULUS+ and STIMULUS-M+ eliminate the periodic full gradient requirement by adjusting batch sizes dynamically based on gradient variance estimates.

## Key Results
- Achieves O(1/T) convergence rate for non-convex MOO and O(exp(-μT)) for strongly convex settings
- Sample complexities of O(n + √nε⁻¹) and O(n + √n ln(μ/ε)) match deterministic MGD convergence rates
- Outperforms existing methods on MultiMNIST and CelebA datasets in both convergence speed and sample complexity
- Enhanced momentum variant STIMULUS-M provides additional acceleration without sacrificing theoretical guarantees

## Why This Works (Mechanism)
The algorithm works by addressing the high variance in stochastic multi-gradient estimates that plague traditional SMGD methods. By maintaining recursive variance-reduced estimates through periodic full gradient evaluations, STIMULUS creates a more stable optimization trajectory that converges faster. The momentum variant further accelerates convergence by building on these stable estimates. The key insight is that variance reduction in the multi-objective setting requires different techniques than single-objective optimization due to the complex interactions between multiple objectives.

## Foundational Learning
**Multi-Objective Optimization (MOO)**: Optimization problems involving multiple competing objectives that must be optimized simultaneously.
*Why needed*: Understanding the fundamental challenge of balancing multiple objectives is crucial for appreciating the algorithm's contribution.
*Quick check*: Can you explain the Pareto optimality concept in MOO?

**Stochastic Multi-Gradient Descent (SMGD)**: Extension of SGD to multi-objective settings where gradients from multiple objectives are combined.
*Why needed*: This is the baseline method that STIMULUS improves upon.
*Quick check*: What causes the high variance in traditional SMGD methods?

**Variance Reduction Techniques**: Methods that reduce the variance in stochastic gradient estimates to improve convergence.
*Why needed*: These techniques are central to STIMULUS's improved performance.
*Quick check*: How do variance reduction techniques differ between single and multi-objective settings?

**Full Gradient Computation**: Periodic evaluation of exact gradients across all objectives.
*Why needed*: This is the key mechanism that enables variance reduction in STIMULUS.
*Quick check*: Why is full gradient computation expensive in large-scale MOO?

**Momentum Methods**: Techniques that incorporate historical gradient information to accelerate convergence.
*Why needed*: The STIMULUS-M variant builds on momentum to further improve performance.
*Quick check*: How does momentum interact with variance reduction in multi-objective settings?

## Architecture Onboarding

**Component Map**: Data -> Stochastic Gradients -> Variance Reduction Module -> Momentum Update -> Parameter Update -> New Data

**Critical Path**: The algorithm's critical path involves the periodic full gradient computation followed by recursive variance reduction updates. Each epoch begins with a computationally expensive full gradient evaluation, but subsequent updates use the improved estimates, making the overall approach efficient.

**Design Tradeoffs**: The main tradeoff is between computational cost (periodic full gradients) and convergence speed. The algorithm trades occasional expensive computations for significantly faster overall convergence. The adaptive variants eliminate this tradeoff but may require more sophisticated hyperparameter tuning.

**Failure Signatures**: Poor performance may manifest as slow convergence when gradient noise is extremely high, or computational bottlenecks when the number of objectives is very large. The algorithm may also struggle when the assumptions about bounded gradients or unbiased estimates are violated.

**First Experiments**:
1. Implement STIMULUS on a simple multi-objective convex problem to verify the O(1/T) convergence rate
2. Compare STIMULUS against standard SMGD on a synthetic non-convex MOO problem to demonstrate variance reduction benefits
3. Test the adaptive batching variants (STIMULUS+, STIMULUS-M+) on a medium-scale problem to evaluate their effectiveness without full gradient computations

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Assumes access to unbiased stochastic gradients and bounded gradient norms, which may not hold in all practical scenarios
- Periodic full gradient evaluations could become prohibitive for very large-scale problems with millions of objectives
- While theory covers both convex and non-convex settings, practical benefits may vary depending on problem structure and gradient noise characteristics

## Confidence
High confidence in: Theoretical convergence rates and sample complexity bounds
Medium confidence in: Practical significance of improvements across diverse problem types
Low confidence in: Scalability to extremely high-dimensional multi-objective problems with millions of objectives

## Next Checks
1. Empirical evaluation on larger-scale multi-objective problems with thousands of objectives to test scalability of periodic full gradient computation requirement
2. Investigation of algorithm performance under biased or heavy-tailed gradient noise to assess robustness beyond bounded noise regime
3. Comparative analysis of momentum variant STIMULUS-M against other accelerated methods in multi-objective setting to quantify practical benefits of variance reduction approach