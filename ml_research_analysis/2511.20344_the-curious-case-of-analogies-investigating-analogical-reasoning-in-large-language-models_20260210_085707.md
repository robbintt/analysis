---
ver: rpa2
title: 'The Curious Case of Analogies: Investigating Analogical Reasoning in Large
  Language Models'
arxiv_id: '2511.20344'
source_url: https://arxiv.org/abs/2511.20344
tags:
- information
- analogical
- reasoning
- layers
- relational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work investigates the internal mechanisms of large language
  models (LLMs) in analogical reasoning, focusing on how models encode and apply relational
  information across different entities. Using attention knockout, patchscopes, and
  structural alignment metrics, the study identifies three key findings: (1) mid-upper
  layers in LLMs encode both attributive and relational information critical for analogical
  reasoning, with relational information showing a sharp decline in incorrect cases;
  (2) models struggle not only with extracting relational information but also applying
  it to new entities, with performance improvements of up to 38.1% observed when patching
  representations from the second entity into the linking position; (3) successful
  analogical reasoning is associated with strong structural alignment between source
  and target stories, as measured by mutual alignment scores, while failures reflect
  weaker or distractor-biased alignment.'
---

# The Curious Case of Analogies: Investigating Analogical Reasoning in Large Language Models

## Quick Facts
- arXiv ID: 2511.20344
- Source URL: https://arxiv.org/abs/2511.20344
- Reference count: 23
- Key outcome: LLMs encode relational information in mid-upper layers critical for analogical reasoning, with failures stemming from both missing relational encoding and ineffective application to new entities

## Executive Summary
This work investigates how large language models perform analogical reasoning by examining internal mechanisms through attention knockout, Patchscopes probing, and structural alignment metrics. The study reveals that while LLMs encode both attributive and relational information in mid-upper layers, they struggle not only with extracting relational information but also applying it to new entities. Performance improvements of up to 38.1% were observed when patching representations from the second entity into the linking position, indicating this as a critical transfer bottleneck. The findings demonstrate that successful analogical reasoning correlates with strong structural alignment between source and target stories, while failures reflect weaker or distractor-biased alignment.

## Method Summary
The researchers conducted mechanistic analysis of analogical reasoning using proportional analogies (e1:e2::e3:e4) and story analogies. They employed three analysis methods: attention knockout to identify critical positions and layers, Patchscopes probing to decode attributive vs relational information at different layers, and structural alignment metrics to measure token-level similarity between source and target stories. Datasets were filtered to isolate reasoning from knowledge (knowledge filter) and from shortcut inference (shortcut filter). Models tested included Llama-2-13B, Gemma-7B, Qwen2.5-14B for proportional analogies and their chat/instruction variants for story analogies.

## Key Results
- Mid-upper layers in LLMs encode both attributive and relational information critical for analogical reasoning, with relational information showing a sharp decline in incorrect cases
- Models struggle not only with extracting relational information but also applying it to new entities, with performance improvements of up to 38.1% observed when patching representations from the second entity into the linking position
- Successful analogical reasoning is associated with strong structural alignment between source and target stories, as measured by mutual alignment scores, while failures reflect weaker or distractor-biased alignment

## Why This Works (Mechanism)

### Mechanism 1: Mid-Upper Layer Relational Encoding
- **Claim:** LLMs encode both attributive and relational information in mid-upper layers, with relational encoding being the critical differentiator between correct and incorrect analogical reasoning.
- **Mechanism:** Entity representations in the e2 and e3 positions progressively encode attributive information through early layers, then relational information emerges in mid-upper layers (~layers 20-35 in Qwen2.5-14B). This relational encoding is what enables answer resolution at upper layers.
- **Core assumption:** The Patchscopes decoding method accurately reflects what information is represented, not just what can be linearly probed.
- **Evidence anchors:** [abstract] "mid-upper layers in LLMs encode both attributive and relational information critical for analogical reasoning, with relational information showing a sharp decline in incorrect cases"; [Section 4.2, Figure 3] "attributive information persists across mid-upper layers regardless of correctness, while relational information shows a sharp decline in incorrect cases"

### Mechanism 2: Link Position as Relational Transfer Bottleneck
- **Claim:** The "link" position ("as" in proportional analogies) serves as a critical transfer point for relational information, and failures often stem from ineffective information propagation through this position rather than missing relations.
- **Mechanism:** Information from e2 (which encodes both attributive and relational content) must propagate through the link to enable answer resolution at the final token. Patching e2 representations into the link position in early-to-middle layers allows relational information to be properly contextualized and transferred downstream.
- **Core assumption:** The patching intervention reveals a genuine functional role rather than an artifact of the intervention method.
- **Evidence anchors:** [abstract] "performance improvements of up to 38.1% observed when patching representations from the second entity into the linking position"; [Section 5.2, Table 1] "patching the representations of e2 to the link leads to noticeable performance gains up to 38.1%"

### Mechanism 3: Structural Alignment via Mutual Best Matching
- **Claim:** Successful analogical reasoning correlates with stronger token-level structural alignment between source and target stories, measurable via Mutual Alignment Score (MAS)—the proportion of token pairs that are mutual best matches in cosine similarity.
- **Mechanism:** During story analogy processing, correct cases show higher MAS between source-target pairs than source-distractor pairs, especially in middle layers (~layers 20-30). This suggests models develop abstract relational representations that capture structural parallels despite minimal lexical overlap.
- **Core assumption:** MAS reflects meaningful structural alignment rather than spurious similarity patterns.
- **Evidence anchors:** [abstract] "successful analogical reasoning is associated with strong structural alignment between source and target stories, as measured by mutual alignment scores"; [Section 6.2, Figure 5] "for correct cases, the MAS between source and target stories consistently exceeds that between source and distractor stories"

## Foundational Learning

- **Concept: Attention Knockout**
  - **Why needed here:** Understanding which token positions and layers are causally necessary for analogical reasoning requires selectively disabling attention pathways.
  - **Quick check question:** If you block attention from the resolution token to e2 and accuracy drops, what does this imply about e2's role?

- **Concept: Linear Probing**
  - **Why needed here:** Determining whether analogical structure is linearly separable from lexical similarity in hidden representations.
  - **Quick check question:** If a linear probe achieves 82.9% accuracy at layer 25 but ~50% at layer 5, what does this suggest about where analogical distinctions emerge?

- **Concept: Activation Patching**
  - **Why needed here:** Testing causal claims about information flow by injecting representations from one position/layer into another.
  - **Quick check question:** Why patch into early layers of the link rather than late layers? (Hint: representations need contextualization.)

## Architecture Onboarding

- **Component map:** Entity tokens → mid-upper layer relational encoding → link transfer → resolution token answer prediction
- **Critical path:** e1,e2 (first entity pair) → mid-upper layer encoding → link position → e3,e4 (second entity pair) → resolution token
- **Design tradeoffs:**
  - Attention knockout window size (k=1/5 layers): Balances precision vs. capturing cross-layer information propagation
  - Patchscopes target prompts: Custom prompts extract relational vs. attributive info; design affects interpretation quality
  - Two-option story analogy format: Reduces noise but may not reflect real-world complexity
- **Failure signatures:**
  - Missing relational info: Relational Patchscopes decoding fails at mid-upper layers; attributive info persists
  - Link transfer failure: Attention knockout shows strong link influence in incorrect cases; patching e2→link helps
  - Distractor-biased alignment: Source-distractor MAS ≥ source-target MAS; middle-layer probe accuracy near chance
- **First 3 experiments:**
  1. **Attention knockout sweep:** Block resolution token attention to e1, e2, e3, link across all layers; measure accuracy drop (correct cases) or generation change (incorrect cases)
  2. **Relational vs. attributive Patchscopes:** Decode hidden states at e2, e3 across layers using custom prompts; quantify the gap between correct and incorrect cases
  3. **Intervention patching:** For incorrect cases where e2 replacement doesn't help, patch mid-layer e2 representations into early link layers; measure correction rate

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What mechanisms account for the ~62% of incorrect analogical reasoning cases where patching representations from the second entity to the link fails to correct model outputs?
- **Basis in paper:** [explicit] The authors report that patching representations from e₂ to the link improves performance in up to 38.1% of remaining incorrect cases (Table 1), implicitly acknowledging that the majority of cases remain unexplained by this intervention.
- **Why unresolved:** The paper identifies the link as a transfer bottleneck but does not investigate why some failures are resistant to this intervention or what alternative failure modes exist.
- **What evidence would resolve it:** Ablation studies targeting other components (e.g., attention heads in later layers, feed-forward networks) combined with failure mode analysis on the resistant cases.

### Open Question 2
- **Question:** Can the structural alignment mechanisms identified in this work be leveraged to improve analogical reasoning through targeted training objectives rather than inference-time interventions?
- **Basis in paper:** [inferred] The authors demonstrate that mutual alignment scores correlate with successful reasoning and that patching can help at inference time, but explicitly state their work "paves the way for future research into understanding and improving" these capabilities.
- **Why unresolved:** The paper focuses on mechanistic analysis and inference-time interventions rather than training methodologies or architectural modifications.
- **What evidence would resolve it:** Experiments training models with alignment-based loss functions or fine-tuning objectives that explicitly reward higher mutual alignment scores between source and target representations.

### Open Question 3
- **Question:** Do the information flow patterns and structural alignment mechanisms identified in proportional and story analogies generalize to other analogical reasoning tasks such as long-text analogies or cross-domain analogies?
- **Basis in paper:** [inferred] The paper limits analysis to two analogy types and acknowledges in related work that analogies take "several forms, including word analogies, proportional analogies, story analogies, and long-text analogies."
- **Why unresolved:** The mechanistic findings are demonstrated only on the specific datasets studied; whether the mid-upper layer encoding and MAS correlation patterns hold for other analogy forms remains untested.
- **What evidence would resolve it:** Replication of the attention knockout, Patchscopes, and MAS analysis on diverse analogy benchmarks such as AnaloBench or cross-domain analogy tasks.

### Open Question 4
- **Question:** Why do LLMs struggle more than humans with the application phase of analogical reasoning, given that humans find relation extraction more difficult?
- **Basis in paper:** [explicit] The authors state "unlike humans, LLMs often struggle not only when relational information is missing, but also when attempting to apply it to new entities" and highlight this as a key gap with human cognition.
- **Why unresolved:** The paper documents this divergence but does not investigate its cognitive or computational origins.
- **What evidence would resolve it:** Comparative analysis of attention patterns and activation trajectories between human-aligned models or neuro-symbolic systems versus standard LLMs on the same analogy tasks.

## Limitations

- The analysis relies heavily on synthetic interventions (attention knockout, activation patching) whose effects may not cleanly map to the underlying causal mechanisms
- The Patchscopes decoding method assumes that custom target prompts reliably elicit the intended information types, but this assumption is not independently verified
- The structural alignment findings depend on MAS as a proxy for "structural" similarity, which may conflate lexical overlap with abstract relational alignment
- Knowledge and shortcut filtering steps are critical for isolating reasoning from memorization, but exact prompt templates and thresholds are underspecified

## Confidence

- **High confidence:** Mid-upper layer relational encoding - supported by consistent layer-wise Patchscopes patterns and attention knockout effects showing relational information is critical for correct cases
- **Medium confidence:** Link position as relational transfer bottleneck - patching interventions show strong performance gains, but the causal interpretation depends on assumptions about information flow that are not independently verified
- **Medium confidence:** Structural alignment via MAS - the correlation between MAS and correctness is robust, but MAS itself is a noisy proxy for abstract relational alignment

## Next Checks

1. **Prompt sensitivity analysis for Patchscopes:** Systematically vary the custom target prompts (e.g., "This is the author of" vs "This person wrote") and measure how decoding accuracy changes for both attributive and relational information. This would test whether Patchscopes reliably captures the intended information types or is sensitive to prompt phrasing.

2. **MAS ablation with lexical control:** Create a lexical control condition where source and target stories have matched token overlap but no meaningful relational structure (e.g., swapped character names), then measure whether MAS still predicts correctness. This would test whether MAS captures genuine structural alignment or is confounded by surface similarity.

3. **Multi-hop attention knockout validation:** Extend the attention knockout analysis to use a larger window (e.g., 10 layers) and test whether the critical positions identified (e2, link, e3) remain consistent. This would test whether the current 1-5 layer window captures all relevant dependencies or misses longer-range information flow.