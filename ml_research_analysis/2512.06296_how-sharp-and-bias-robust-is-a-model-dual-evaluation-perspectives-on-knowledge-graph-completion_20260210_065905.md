---
ver: rpa2
title: How Sharp and Bias-Robust is a Model? Dual Evaluation Perspectives on Knowledge
  Graph Completion
arxiv_id: '2512.06296'
source_url: https://arxiv.org/abs/2512.06296
tags:
- uni00000013
- uni00000011
- evaluation
- knowledge
- uni00000018
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies two critical limitations in existing knowledge
  graph completion (KGC) evaluation: (1) predictive sharpness - how strictly we evaluate
  individual predictions, and (2) popularity-bias robustness - how well models perform
  on low-popularity entities. The authors propose PROBE, a novel evaluation framework
  consisting of a rank transformer (RT) that converts prediction ranks to scores based
  on desired sharpness levels, and a rank aggregator (RA) that weights scores by entity
  popularity.'
---

# How Sharp and Bias-Robust is a Model? Dual Evaluation Perspectives on Knowledge Graph Completion

## Quick Facts
- arXiv ID: 2512.06296
- Source URL: https://arxiv.org/abs/2512.06296
- Authors: Sooho Moon; Yunyong Ko
- Reference count: 37
- Primary result: Introduces PROBE framework addressing predictive sharpness and popularity-bias robustness in KGC evaluation

## Executive Summary
This paper identifies critical limitations in existing knowledge graph completion (KGC) evaluation: predictive sharpness (how strictly we evaluate individual predictions) and popularity-bias robustness (how well models perform on low-popularity entities). The authors propose PROBE, a novel evaluation framework that generalizes traditional metrics through adjustable parameters. Experiments on FB15k237 and WN18RR datasets demonstrate that PROBE provides more reliable and comprehensive evaluation results compared to existing metrics.

## Method Summary
PROBE is a two-component evaluation framework consisting of a Rank Transformer (RT) that converts prediction ranks to scores based on desired sharpness levels, and a Rank Aggregator (RA) that weights scores by entity popularity. The RT applies an affine transformation f*(r, α) = (f(r,α) - 1)/(1 - |E|^(-α) + 1) where f(r,α) = 1/r^α, allowing adjustable strictness through parameter α. The RA computes weighted averages using w_q = 1/(ε + δ(q)_train)^β, where β controls popularity-bias robustness. The framework was evaluated on FB15k237 and WN18RR datasets using four KGC models (RotatE, TuckEr, pLogicNet, RNNLogic) across multiple α and β parameter settings.

## Key Results
- PROBE reveals that RNNLogic's strong performance in traditional metrics is primarily due to success on popular entities
- RotatE demonstrates better popularity-bias robustness that was previously undervalued by standard metrics
- Different α values enable evaluation that matches varying real-world requirements (strict for drug discovery, lenient for recommendations)
- PROBE provides fixed bounds [0,1] for scores, enabling more reliable cross-dataset comparison than unbounded metrics

## Why This Works (Mechanism)

### Mechanism 1: Sharpness-Controlled Rank Transformation
- Claim: Adjusting sharpness control factor α enables evaluation reflecting varying strictness requirements
- Mechanism: RT applies f*(r, α) = (f(r,α) - 1)/(1 - |E|^(-α) + 1) where f(r,α) = 1/r^α, with higher α imposing exponentially larger penalties on lower ranks
- Core assumption: Different applications require different tolerance for near-miss predictions
- Evidence anchors: [abstract], [section 3, Eq. 1-2]
- Break condition: If α ≤ 0, transformation becomes sensitive to outliers and loses anti-monotonicity

### Mechanism 2: Popularity-Aware Weighted Aggregation
- Claim: Weighting predictions inversely to entity popularity exposes models relying on high-popularity entities
- Mechanism: RA assigns w_q = 1/(ε + δ(q)_train)^β, with higher β aggressively down-weighting popular entity predictions
- Core assumption: Entity popularity distributions in training and test sets are similar
- Evidence anchors: [abstract], [section 3, Eq. 3-4]
- Break condition: If training-test popularity distributions diverge significantly, weighting may penalize wrong predictions

### Mechanism 3: Fixed Bounds for Distinguishability
- Claim: Enforcing fixed bounds [0,1] improves model comparison across datasets
- Mechanism: Affine transformation maps best rank (1) to score 1.0 and worst rank (|E|) to score 0.0
- Core assumption: Fixed bounds enable more consistent comparison than relative or unbounded scoring
- Evidence anchors: [section 3, Property 1-2]
- Break condition: If |E| varies dramatically across datasets without normalization, distinguishability may still suffer

## Foundational Learning

- Concept: Rank-based evaluation metrics (MRR, Hits@K, Mean Rank)
  - Why needed here: PROBE generalizes and extends these metrics; understanding baseline behavior is essential
  - Quick check question: Given ranks [1, 5, 10], compute MRR and explain why it equals α=1 in PROBE's RT

- Concept: Power-law degree distributions in real-world graphs
  - Why needed here: Justifies popularity-bias mechanism; most entities have low degree while few dominate
  - Quick check question: In a KG with 10,000 entities where top 1% have 50% of connections, explain why standard MRR might overestimate model quality

- Concept: Open-world assumption (OWA) in KGC
  - Why needed here: Explains why rank-based metrics are preferred over classification metrics
  - Quick check question: Why can't we use precision/recall directly for KGC evaluation under OWA?

## Architecture Onboarding

- Component map: Trained KGC Model θ(·) → Prediction ranks → Rank Transformer RT → Rank Aggregator RA → Final PROBE Score

- Critical path:
  1. Load trained model and test triples
  2. Generate queries (h,r,?) and (?,r,t) for each triple
  3. Compute ranks for true entities
  4. Apply RT with chosen α (start with α=1 for MRR-equivalent)
  5. Compute popularity weights with chosen β (start with β=0 for uniform weighting)
  6. Aggregate to final score

- Design tradeoffs:
  - High α (>1): Strict evaluation, penalizes near-misses heavily. Use for high-stakes domains (medical, drug discovery)
  - Low α (<1): Lenient evaluation, rewards consistent good predictions. Use for recommendation, exploration
  - High β (>0.4): Strong popularity-bias robustness. Use when low-popularity entity coverage matters
  - Low β (≈0): Standard evaluation, may overestimate models that exploit popularity

- Failure signatures:
  - Scores clustering near 0 or 1 without differentiation → α too extreme or |E| too small
  - Model rankings inconsistent with application needs → α/β misaligned with use case
  - Division-by-zero errors → ε not set for entities with zero training occurrence

- First 3 experiments:
  1. Replicate Table 2: Run PROBE on FB15k237 with α ∈ {0.25, 0.5, 1.0, 2.0}, β=0. Confirm MRR matches at α=1
  2. Popularity stratification: Bin entities by training degree, compute PROBE scores per bin with β ∈ {0, 0.2, 0.4, 0.8}. Visualize performance gaps
  3. Model re-ranking: Compare RotatE vs. RNNLogic rankings under (α=2, β=0.8) vs. (α=0.25, β=0). Verify claims that RNNLogic is overestimated by standard metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well does PROBE generalize to a broader range of knowledge graphs and KGC model architectures beyond the two datasets (FB15k237, WN18RR) and four models evaluated in this study?
- Basis in paper: [explicit] The conclusion states: "In future work, we plan to explore how well PROBE generalizes to other real-world KGs and KG models."
- Why unresolved: Experiments were limited to two benchmarks and four specific models, leaving framework's behavior on diverse graph structures and emerging architectures unverified
- What evidence would resolve it: Systematic evaluation across additional KG datasets and wider variety of model types

### Open Question 2
- Question: Can principled or automated methods be developed for selecting appropriate values of the predictive sharpness (α) and popularity-bias robustness (β) parameters for specific application domains?
- Basis in paper: [inferred] The paper provides heuristic guidelines but offers no systematic framework for parameter selection across diverse real-world scenarios
- Why unresolved: Current recommendations rely on qualitative domain knowledge rather than quantitative or automated approaches
- What evidence would resolve it: Development of data-driven or task-driven parameter selection methods; empirical studies linking parameter choices to downstream application performance

### Open Question 3
- Question: How sensitive is PROBE to the specific functional forms of the rank transformer (Eq. 2) and popularity-weighting function (Eq. 3), and would alternative formulations yield different model rankings?
- Basis in paper: [inferred] The paper proposes specific mathematical forms without comparing to alternative formulations or justifying these choices theoretically
- Why unresolved: Different transformation or weighting functions could potentially lead to different conclusions about model performance
- What evidence would resolve it: Ablation studies comparing alternative functional forms and analysis of ranking stability across these variants

## Limitations
- Limited empirical validation across diverse KGC model architectures beyond four evaluated models
- Choice of ε in popularity weighting function is not explicitly specified, potentially affecting reproducibility
- Generalizability to entirely different domains (e.g., social networks) beyond knowledge graphs is not established

## Confidence

- **High Confidence**: Mathematical formulation of RT and RA, including fixed bounds properties and anti-monotonicity guarantees, demonstrates theoretical soundness
- **Medium Confidence**: Empirical claims about model ranking reversals under different α and β values are supported by presented experiments but require independent validation
- **Low Confidence**: Generalizability of PROBE's advantages to entirely different KGC architectures or domains beyond knowledge graphs is not established

## Next Checks

1. **Cross-Architecture Validation**: Apply PROBE to evaluate additional KGC models (e.g., ConvE, ComplEx) on FB15k237 and WN18RR to verify claimed model ranking stability across architectures

2. **Dataset Distribution Sensitivity**: Test PROBE on KGs with varying degree distributions (e.g., synthetic power-law vs. uniform) to assess robustness of popularity-bias weighting

3. **Parameter Sensitivity Analysis**: Systematically vary ε in the weight function and analyze its impact on model rankings to determine optimal parameter settings for different KG characteristics