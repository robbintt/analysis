---
ver: rpa2
title: 'EFIM: Efficient Serving of LLMs for Infilling Tasks with Improved KV Cache
  Reuse'
arxiv_id: '2505.21889'
source_url: https://arxiv.org/abs/2505.21889
tags:
- prefix
- cache
- suffix
- efim
- reuse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EFIM introduces a transformed prompt format that enables significant
  KV cache reuse for infilling tasks, addressing inefficiencies where the prefix or
  suffix parts are frequently invalidated. The method also includes a fragment tokenization
  training approach to improve subtoken generation capabilities in LLMs.
---

# EFIM: Efficient Serving of LLMs for Infilling Tasks with Improved KV Cache Reuse

## Quick Facts
- arXiv ID: 2505.21889
- Source URL: https://arxiv.org/abs/2505.21889
- Authors: Tianyu Guo; Hande Dong; Yichong Leng; Feng Liu; Cheater Lin; Nong Xiao; Xianwei Zhang
- Reference count: 40
- Key outcome: EFIM reduces average latency by 52% and improves throughput by 98% compared to standard FIM

## Executive Summary
EFIM introduces a transformed prompt format that enables significant KV cache reuse for infilling tasks, addressing inefficiencies where the prefix or suffix parts are frequently invalidated. The method also includes a fragment tokenization training approach to improve subtoken generation capabilities in LLMs. Experimental results show that EFIM reduces average latency by 52% and improves throughput by 98% compared to standard FIM, while maintaining model infilling capability. The additional training overhead for fragment tokenization is minimal, requiring only 0.74% of the cost of training a model like Llama3.1-8B, making it a cost-effective enhancement for LLM serving in infilling applications.

## Method Summary
EFIM addresses the inefficiency of KV cache invalidation in standard FIM prompt formats by transforming the prompt structure and introducing fragment tokenization training. The core innovation is relocating incremental content to the end of the prompt (`<P>prefix<S>suffix<M>inc` instead of `<P>prefix+inc<S>suffix<M>`), which preserves both prefix and suffix KV caches across requests. The fragment tokenization training randomly splits text into segments during training, exposing the model to subtoken generation scenarios it would rarely encounter otherwise. A per-user session pool tracks prefix/suffix states to enable intelligent prompt transformation routing, maximizing cache reuse without requiring user-facing changes.

## Key Results
- 52% average latency reduction and 98% throughput improvement compared to standard FIM
- KV cache reuse rate reaches ~80% at low user counts (vs <40% for FIM)
- Fragment tokenization training adds only 0.74% of full model training cost
- Subtoken generation quality improves from 52.44% to 75.61% pass@1 on random-span tasks

## Why This Works (Mechanism)

### Mechanism 1
Transforming FIM prompt format to EFIM format enables KV cache reuse for both prefix and suffix during infilling tasks. Standard FIM format (`<P>prefix<S>suffix<M>`) invalidates suffix KV cache when prefix grows incrementally because the suffix's preceding tokens change. EFIM relocates the incremental prefix content to the end (`<P>prefix<S>suffix<M>inc`), keeping both prefix and suffix stable across requests. This allows the prefill stage to skip recomputation for cached prefix and suffix tokens.

### Mechanism 2
Fragment tokenization training enables universal subtoken generation capability required by EFIM's transformed prompt format. Standard LLM training rarely encounters subtokens (partial words like "pri" in "print") except near FIM special tokens. EFIM requires generating subtokens after incremental content (`inc`), a context lacking FIM tokens. Fragment tokenization randomly splits text into multiple segments (1-200 tokens each) before tokenization, distributing subtoken cases throughout training data. The model learns to complete partial words from arbitrary positions.

### Mechanism 3
Per-user session pooling with prompt transformation routing maximizes KV cache reuse without user-facing changes. A session pool tracks each user's last prefix/suffix. On new requests, the system compares against stored sessions: (1) if no match, use PSM and create session; (2) if prefix grew, apply EFIM with extracted increment; (3) if suffix grew, use PSM (still reuses prefix cache). This routing logic is transparent to users.

## Foundational Learning

- **KV Cache and Prefill/Decode Stages**
  - Why needed here: Understanding that LLM inference has two phases—prefill (processing input tokens, compute-intensive) and decode (generating output tokens, memory-bound)—is essential to grasp why KV cache reuse matters. EFIM targets prefill reduction.
  - Quick check question: If prefill takes 114s and decode takes 12s without cache reuse, what percentage of total inference time is prefill? (Answer: ~90%)

- **Fill-in-the-Middle (FIM) Format**
  - Why needed here: FIM is the standard prompt structure for infilling tasks. Knowing PSM (`<P>prefix<S>suffix<M>middle`) vs SPM variants explains why cache invalidation occurs and how EFIM's transformation differs.
  - Quick check question: In PSM format, if the user adds tokens to the end of the prefix, which part's KV cache is invalidated and why? (Answer: Suffix cache—because its preceding tokens change)

- **Tokenization and Subtokens**
  - Why needed here: BPE/tokenizers split text into tokens that don't align with word boundaries. "print" might tokenize as ["pri", "nt"]. LLMs trained on clean token boundaries struggle to generate "nt" given "pri" as context end.
  - Quick check question: Why might an LLM fail to complete "comp" to "completion" in EFIM but succeed in standard FIM? (Answer: FIM has FIM special tokens providing subtoken context; EFIM places increment after, lacking those cues)

## Architecture Onboarding

- **Component map:**
  - Session Pool -> Prompt Transformer -> Inference Engine -> Enhanced LLM

- **Critical path:**
  1. Request arrives → 2. Session lookup → 3. Prefix/suffix diff → 4. Prompt format selection → 5. KV cache match → 6. Prefill (partial if cached) → 7. Decode → 8. Session update

- **Design tradeoffs:**
  - **GPU memory vs reuse rate:** Larger KV cache capacity improves reuse but limits concurrent users (Figure 12 shows reuse drops at 64+ users)
  - **Training cost vs inference savings:** Fragment tokenization adds 0.74% of full training cost; breaks even in ~1 day of serving (Section 5.3)
  - **PSM fallback vs EFIM-only:** System keeps PSM for suffix-growth cases; pure EFIM would lose those reuse opportunities

- **Failure signatures:**
  - **Garbled subtoken output:** Model generates incoherent text after increment → likely using oLLM (original) instead of eLLM (enhanced with fragment tokenization)
  - **Low reuse rate despite EFIM:** Check session pool eviction, GPU memory pressure, or non-sequential user behavior
  - **Latency spike at high concurrency:** Expected per Figure 12; consider horizontal scaling or cache tiering (see LMCache in corpus)

- **First 3 experiments:**
  1. **Baseline latency comparison:** Run infilling workload (avg 2100 input / 32 output tokens, 16 users, 5 rounds) comparing: (a) PSM no reuse, (b) PSM with reuse, (c) EFIM with reuse. Expect 52% latency reduction for (c) vs (a).
  2. **Subtoken generation test:** Create test cases where prefix ends mid-word (e.g., "code comp[] models"). Compare pass@1 between oLLM w/EFIM and eLLM w/EFIM. Expect 20+ percentage point gap on random-span tasks.
  3. **Cache reuse rate scaling:** Increment concurrent users from 8 to 128. Plot reuse rate and latency. Identify the inflection point where GPU memory pressure causes cache eviction (expected ~40-60 users for 8B model on A100).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does EFIM's efficiency compare to standard formats specifically in scenarios where the suffix grows incrementally, rather than the prefix?
- Basis in paper: Section 4.2 states that the efficiency experiments were conducted "in a scenario where tokens are appended to the prefix," despite Figure 3 claiming benefits for suffix growth as well.
- Why unresolved: The quantitative results for latency and throughput (Figure 11) focus exclusively on prefix growth, leaving the suffix-growth performance gain unverified.
- What evidence would resolve it: Benchmark results comparing EFIM against PSM/SPM specifically on incremental suffix generation tasks.

### Open Question 2
- Question: Does the fragment tokenization training method degrade the model's general reasoning or natural language capabilities outside of infilling tasks?
- Basis in paper: The evaluation in Section 5.1 is restricted to infilling benchmarks (HumanEval, CCEval) and pass@1 rates, without measuring potential negative transfer on general benchmarks.
- Why unresolved: Randomly splitting text into segments during training (Figure 9) introduces noise that could theoretically harm the model's coherence on standard tasks.
- What evidence would resolve it: A comparison of accuracy on general benchmarks (e.g., MMLU or GSM8K) between the baseline and the fragment-tokenized model.

### Open Question 3
- Question: What distinct cache eviction strategies are required to prevent the sharp decline in KV cache reuse rates observed in EFIM under high concurrency?
- Basis in paper: Figure 12 shows EFIM's reuse rate drops significantly as user count increases because the total capacity exceeds GPU memory, yet the paper does not propose a specific solution.
- Why unresolved: The current implementation relies on standard memory management which struggles to maintain the high reuse rates EFIM is capable of achieving under load.
- What evidence would resolve it: An analysis of latency and reuse rates using custom eviction policies (e.g., priority-based retention of prefix/suffix blocks) at high user concurrency.

## Limitations
- Evaluation restricted to coding tasks without testing on general language tasks
- No ablation studies for training hyperparameters affecting fragment tokenization effectiveness
- Session pool assumes sequential user interactions without handling concurrent requests
- GPU memory vs reuse tradeoff degrades significantly at 64+ concurrent users

## Confidence

**High Confidence (Level 3):** The core mechanism of EFIM's prompt transformation is well-established and theoretically sound. The 40% prefill reduction demonstrated in Figure 4 provides strong empirical support for this mechanism.

**Medium Confidence (Level 2):** The fragment tokenization training approach shows promising results but lacks detailed hyperparameter specifications. The session pooling mechanism's performance at scale is also medium confidence, as the 80% reuse rate at low user counts degrades significantly under high concurrency.

**Low Confidence (Level 1):** The generalization claims to other model families and task domains are speculative. The concurrent user handling assumptions are particularly weak, with no quantitative analysis of race conditions or cache invalidation scenarios.

## Next Checks

1. **Ablation Study on Training Hyperparameters:** Run the fragment tokenization training with varying learning rates (1e-5, 5e-5, 1e-4) and batch sizes (128, 256, 512) while keeping all other parameters constant. Measure the impact on subtoken generation quality (pass@1 on random-span tasks) and serving latency reduction.

2. **Cross-Domain Task Evaluation:** Test EFIM on general language tasks including summarization (CNN/DailyMail), question answering (SQuAD), and creative writing prompts. Compare latency reduction and subtoken generation quality against the coding benchmarks.

3. **Concurrent User Stress Test with Cache Tiering:** Simulate 128+ concurrent users with varying request patterns (sequential vs. bursty) and implement a two-level cache system (GPU-level + CPU-level). Measure reuse rates, latency, and memory usage across different tiering configurations.