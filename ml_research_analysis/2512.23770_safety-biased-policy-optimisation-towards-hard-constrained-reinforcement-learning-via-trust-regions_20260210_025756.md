---
ver: rpa2
title: 'Safety-Biased Policy Optimisation: Towards Hard-Constrained Reinforcement
  Learning via Trust Regions'
arxiv_id: '2512.23770'
source_url: https://arxiv.org/abs/2512.23770
tags:
- reward
- cost
- safe
- safety
- sb-trpo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SB-TRPO introduces a trust-region method for hard-constrained reinforcement
  learning that dynamically balances cost reduction with reward improvement. By combining
  reward and cost natural policy gradients via a convex combination, the method ensures
  a fixed fraction of optimal cost reduction at each update while using remaining
  capacity for reward improvement.
---

# Safety-Biased Policy Optimisation: Towards Hard-Constrained Reinforcement Learning via Trust Regions

## Quick Facts
- arXiv ID: 2512.23770
- Source URL: https://arxiv.org/abs/2512.23770
- Authors: Ankit Kanwar; Dominik Wagner; Luke Ong
- Reference count: 40
- Primary result: Introduces SB-TRPO, a trust-region method for hard-constrained RL that dynamically balances cost reduction with reward improvement, achieving state-of-the-art safety-performance trade-offs on Safety Gymnasium tasks

## Executive Summary
SB-TRPO introduces a trust-region method for hard-constrained reinforcement learning that dynamically balances cost reduction with reward improvement. By combining reward and cost natural policy gradients via a convex combination, the method ensures a fixed fraction of optimal cost reduction at each update while using remaining capacity for reward improvement. Theoretical guarantees show that every update yields local cost reduction and reward improvement when gradients are suitably aligned. Experiments on Safety Gymnasium tasks demonstrate that SB-TRPO consistently achieves the best balance of safety and task performance compared to state-of-the-art methods, avoiding the over-conservatism seen in CPO-style approaches while maintaining high safety.

## Method Summary
SB-TRPO is a hard-constrained RL algorithm that performs trust-region updates using a convex combination of reward and cost natural policy gradients. The method computes separate trust-region steps for reward and cost optimization, then combines them using a safety bias parameter β to guarantee a fixed fraction of optimal cost reduction per update. Unlike CPO-style methods, SB-TRPO does not switch to separate recovery phases, avoiding over-conservatism. The algorithm uses Monte Carlo gradient estimation without critics, which proves more effective for sparse cost signals. Theoretical analysis guarantees local cost reduction and reward improvement under suitable gradient alignment conditions.

## Key Results
- SB-TRPO achieves state-of-the-art safety-performance trade-offs on Safety Gymnasium tasks
- Eliminates over-conservatism of CPO-style methods by avoiding separate recovery phases
- Monte Carlo estimation without critics proves more effective than critic-based approaches for sparse cost signals
- Theoretical guarantees show every update yields local cost reduction and reward improvement when gradients are suitably aligned

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamically combining reward and cost gradients with safety bias β ensures guaranteed cost reduction while preserving reward improvement potential.
- Mechanism: SB-TRPO computes separate trust-region steps Δr (reward-optimal) and Δc (cost-optimal), then forms Δ = (1-μ)·Δr + μ·Δc where μ is the minimum coefficient satisfying ⟨gc, Δ⟩ ≤ -ε, with ε = β·⟨gc, Δc⟩. This guarantees at least a β-fraction of optimal cost reduction per update.
- Core assumption: Gradients gr and gc are sufficiently accurate approximations of true policy performance changes within the trust region.
- Evidence anchors:
  - [abstract] "performs trust-region updates using a convex combination of the natural policy gradients of cost and reward, ensuring a fixed fraction of optimal cost reduction at each step"
  - [section 4.2] Equation (4) defines μ and Lemma B.3 proves optimality
  - [corpus] Limited direct corpus support; related PPO-BR paper addresses adaptive trust regions but not constraint-aware gradient combination
- Break condition: When gc ≈ 0 (near-feasible), μ → 0 and updates become purely reward-directed; when reward/cost gradients are misaligned (>90°), reward improvement is not guaranteed.

### Mechanism 2
- Claim: Eliminating separate recovery phases prevents collapse into overly conservative, task-ineffective policies.
- Mechanism: Unlike CPO and C-TRPO which switch to pure cost minimization when constraints are violated, SB-TRPO always optimizes within the joint (Update 1) formulation. This allows temporary cost increases if they enable eventual reward improvement, avoiding local minima in "trivially safe" regions.
- Core assumption: Hard-constrained regime where zero-cost threshold is the appropriate formulation, not positive thresholds that permit some violations.
- Evidence anchors:
  - [section 1] "Unlike CPO and related methods such as C-TRPO... SB-TRPO does not switch into separate feasibility-recovery phases, avoiding over-conservatism"
  - [section 6] "CPO gets 'stuck' near low-cost but task-ineffective policies"
  - [corpus] Weak corpus support; related work on safe RL (Mirror Descent Policy Optimisation) addresses robustness but not recovery-phase elimination
- Break condition: If β = 1, SB-TRPO recovers CPO behavior exactly, re-introducing over-conservatism risk.

### Mechanism 3
- Claim: Monte Carlo gradient estimation without critics improves safety under sparse cost signals while reducing computational cost.
- Mechanism: The ablation study shows critics accelerate reward learning but increase incurred costs. Sparse/binary cost signals provide weak feedback for cost critics, biasing updates toward riskier behavior. Monte Carlo returns (GAE with λ=1) provide more stable gradient estimates for safety-critical sparse costs.
- Core assumption: Cost signals are sparse or binary, making critic bootstrap estimates unreliable or harmful.
- Evidence anchors:
  - [section 5.4] "learning is dominated by relatively accurate reward critic information, whereas cost critics provide weak or delayed feedback for rare unsafe events"
  - [Table 5] SB-TRPO without critics: 0.26s/epoch vs TRPO-Lag with critics: 5.85s/epoch
  - [corpus] No corpus papers directly address critic-free safe RL
- Break condition: In environments with dense cost signals, critics may provide value; this design choice is specifically justified for sparse safety constraints.

## Foundational Learning

- Concept: **Constrained Markov Decision Processes (CMDPs)**
  - Why needed here: SB-TRPO targets Problem 2 formulation with Jc(π) = 0 constraint; understanding the distinction between soft constraints (positive threshold) and hard constraints (zero threshold) is essential for applying the method correctly.
  - Quick check question: Can you explain why setting cost threshold d > 0 conflates problem specification with algorithmic hyperparameter?

- Concept: **Natural Policy Gradients and Fisher Information Matrix**
  - Why needed here: The algorithm computes F⁻¹gr and F⁻¹gc via conjugate gradient; understanding why natural gradients are preconditioned by Fisher information is necessary for debugging convergence issues.
  - Quick check question: Why does the KL constraint D_max_KL(π_old || π) ≤ δ translate to ½ΔᵀFΔ ≤ δ in the quadratic approximation?

- Concept: **Trust Region Methods (TRPO specifically)**
  - Why needed here: SB-TRPO inherits TRPO's conjugate gradient solver and line search; the surrogate objectives Lr,π_old(π) and Lc,π_old(π) rely on TRPO's policy improvement bounds.
  - Quick check question: What happens to surrogate accuracy if the KL constraint is violated during updates?

## Architecture Onboarding

- Component map: Rollout Collection → Gradient Estimation (gr, gc via Monte Carlo) → Conjugate Gradient Solver → Δr (reward step), Δc (cost step) → Convex Combination Calculator → μ via Equation (4) → Line Search → η scaling for KL + cost constraint satisfaction → Parameter Update: θ ← θ + η·Δ

- Critical path: The conjugate gradient solver (lines 6-8 in Algorithm 1) is the computational bottleneck; incorrect Fisher matrix estimation or ill-conditioning causes numerical instability. The μ calculation (line 7) determines safety-reward trade-off—if κ is too small or ⟨gc, Δr⟩ ≈ ⟨gc, Δc⟩, division instability occurs.

- Design tradeoffs:
  - β ∈ [0.6, 0.9]: Higher β → more safety, lower reward; lower β → inverse. Paper uses β=0.75 uniformly.
  - Critics vs Monte Carlo: Critics faster but less safe for sparse costs; Monte Carlo slower but more reliable.
  - Target KL δ: Must be small enough for surrogate accuracy but large enough for meaningful progress.

- Failure signatures:
  - Cost plateau with low reward: β too high or gradient misalignment persistent
  - Oscillating costs: Line search failing to find valid η; check KL constraint satisfaction
  - Division by zero in μ: ⟨gc, Δr⟩ ≈ ⟨gc, Δc⟩; increase κ or check gradient estimates
  - Negative safe reward with high safety probability: Policy in trivially safe region; decrease β

- First 3 experiments:
  1. **Hyperparameter sweep β ∈ [0.6, 0.9]** on a single task (e.g., CarCircle): Plot reward vs safety probability Pareto frontier to confirm paper's Figure 3 behavior and select appropriate β for your safety requirements.
  2. **Ablate critic vs Monte Carlo** on a task with sparse costs (e.g., PointButton): Compare final costs and rewards to verify that critic-free approach yields lower costs despite slower reward accumulation.
  3. **Gradient alignment analysis**: Log angles between Δ and gr/gc during training to verify updates remain below 90° to reward gradient (confirming Theorem 4.2 behavior) and compare against CPO baseline which should show orthogonal updates during recovery.

## Open Questions the Paper Calls Out

- Can SB-TRPO be extended to handle hybrid settings or CMDPs with positive cost thresholds?
  - Basis in paper: [explicit] The Limitations section states: "We leave hybrid methods replacing conventional recovery phases (e.g. in CPO/C-TRPO) with our (Update 3) to future work."
  - Why unresolved: The current formulation is mathematically specific to the zero-cost (hard-constraint) regime and is not directly applicable to soft constraints.
  - What evidence would resolve it: A modified version of the convex combination update rule that provably converges for positive cost limits.

- Does the theoretical performance guarantee (Theorem 4.2) hold robustly under the noise of stochastic gradient estimates?
  - Basis in paper: [inferred] The authors note in the Limitations that the theorem "assumes exact gradients and holds only approximately with estimates."
  - Why unresolved: Practical deep RL relies on noisy Monte Carlo estimates, which may violate the local progress assumptions required for guaranteed cost/reward improvement.
  - What evidence would resolve it: A theoretical analysis bounding the error induced by gradient estimation noise on the local improvement guarantees.

- How can the method be modified to guarantee almost-sure safety on the most challenging tasks?
  - Basis in paper: [explicit] The authors admit that "it does not guarantee almost-sure safety on the most challenging tasks" in the Limitations.
  - Why unresolved: On difficult navigation tasks, safety probabilities remain below 1.0 (e.g., 0.68–0.79), indicating the agent still encounters unsafe states.
  - What evidence would resolve it: Integration with shielding mechanisms or an analysis proving convergence to strictly zero-cost policies in these complex environments.

## Limitations

- The method's reliance on accurate gradient estimation is critical - when gradients are poorly estimated due to sparse rewards or costs, the trust-region approximation breaks down.
- The choice of β=0.75 is somewhat arbitrary; while the paper shows performance is stable across [0.6, 0.9], the optimal value likely depends on task characteristics.
- The theoretical guarantees assume small KL steps, but practical δ=0.01 may violate this assumption in high-dimensional spaces.

## Confidence

- **High confidence**: The basic mechanism of combining reward and cost gradients with guaranteed cost reduction (Mechanism 1) - this follows directly from the convex combination formulation and Lemma B.3.
- **Medium confidence**: The claim that eliminating recovery phases prevents over-conservatism (Mechanism 2) - supported by qualitative observations but not rigorously quantified against alternative approaches.
- **Medium confidence**: The critic-free approach improves safety for sparse costs (Mechanism 3) - supported by ablation but based on limited empirical evidence without theoretical grounding.

## Next Checks

1. **Gradient alignment monitoring**: Instrument SB-TRPO to log the angle between Δ and gr/gc during training. Verify that angles remain below 90° to reward gradient (confirming Theorem 4.2) and compare against CPO baseline which should show orthogonal updates during recovery phases.

2. **β sensitivity analysis**: Conduct systematic sweeps across β ∈ [0.5, 0.9] on multiple tasks, measuring both final performance and learning speed. This validates whether β=0.75 is truly optimal or if task-specific tuning is necessary.

3. **Line search robustness test**: Create controlled scenarios with known gradient noise and verify that SB-TRPO's line search reliably finds η satisfying both KL and cost constraints. Measure how often constraints are violated and identify conditions causing line search failure.