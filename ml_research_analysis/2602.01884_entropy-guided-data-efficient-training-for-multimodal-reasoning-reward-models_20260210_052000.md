---
ver: rpa2
title: Entropy-Guided Data-Efficient Training for Multimodal Reasoning Reward Models
arxiv_id: '2602.01884'
source_url: https://arxiv.org/abs/2602.01884
tags:
- entropy
- reward
- reasoning
- training
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of training multimodal reasoning
  reward models, specifically the issues of noisy preference datasets and inefficient
  training methods that ignore sample difficulty. The authors propose EGT (Entropy-Guided
  Training), which leverages the observation that response entropy correlates strongly
  with accuracy, making it a reliable unsupervised proxy for both annotation noise
  and sample difficulty.
---

# Entropy-Guided Data-Efficient Training for Multimodal Reasoning Reward Models

## Quick Facts
- **arXiv ID**: 2602.01884
- **Source URL**: https://arxiv.org/abs/2602.01884
- **Authors**: Shidong Yang; Tongwen Huang; Hao Wen; Yong Wang; Li Chen; Xiangxiang Chu
- **Reference count**: 0
- **Primary result**: Achieves state-of-the-art performance on multimodal reward benchmarks with 17.50% improvement over GPT-4o baseline using only 15% of lowest-entropy data

## Executive Summary
This paper addresses the challenge of training multimodal reasoning reward models by leveraging response entropy as an unsupervised proxy for both annotation noise and sample difficulty. The authors propose EGT (Entropy-Guided Training), which combines entropy-guided data curation to remove unreliable samples with an entropy-guided training curriculum that progressively introduces more complex examples. The method achieves state-of-the-art performance on three multimodal reward benchmarks while demonstrating remarkable data efficiency.

## Method Summary
The EGT method employs a three-stage pipeline: First, a base model is fine-tuned on reasoning trajectories to create a reasoning reward model. Second, this model calculates answer token entropy scores for each sample in the preference dataset, enabling the selection of 2,500 lowest-entropy samples. Third, the final model is trained using reinforcement learning with StableReinforce, processing samples in ascending entropy order (easy-to-hard curriculum). The approach uses a composite reward function combining accuracy, logic, and format rewards, and was tested on Qwen2.5-VL-7B-Instruct with extensive ablation studies validating each component.

## Key Results
- Achieved 17.50% improvement over GPT-4o baseline on multimodal reward benchmarks
- Comparable performance using only 15% of lowest-entropy data versus full dataset
- Answer token entropy outperforms reasoning sentence entropy (77.15% vs 69.37% accuracy)
- Entropy-guided curriculum (+ Selection & Sort) yields 77.15% accuracy versus 71.13% without sorting

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Response entropy serves as a reliable unsupervised proxy for sample difficulty and annotation noise
- **Mechanism**: High entropy indicates model uncertainty when facing ambiguous preferences or factual errors, while low entropy reflects confident, clear-cut cases
- **Core assumption**: Model uncertainty correlates with objective "noisiness" or "difficulty" of the sample
- **Evidence anchors**: Strong correlation between response entropy and accuracy observed in experiments; answer token entropy performs better than reasoning sentence entropy
- **Break condition**: Under-trained or miscalibrated models may reflect their own ignorance rather than data noise

### Mechanism 2
- **Claim**: Pruning high-entropy samples creates a compact, high-quality training set
- **Mechanism**: Discarding samples where model is undecided prevents noisy gradient signals from entering training
- **Core assumption**: High-entropy samples are unreliable rather than valuable hard negatives
- **Evidence anchors**: 15% lowest-entropy subset outperforms full dataset; low-entropy subsets significantly outperform high-entropy subsets
- **Break condition**: If dataset requires learning subtle distinctions that appear initially ambiguous but are solvable

### Mechanism 3
- **Claim**: Low-to-high entropy curriculum stabilizes reinforcement learning optimization
- **Mechanism**: Training begins on easy samples to establish stable decision boundary before introducing complex examples
- **Core assumption**: Optimization landscape is smoother when easy samples anchor policy before hard samples perturb it
- **Evidence anchors**: + Selection & Sort yields 77.15% accuracy versus 71.13% for unsorted selection
- **Break condition**: If easy samples are trivial and hard samples provide only new information, model may overfit before curriculum advances

## Foundational Learning

- **Concept**: Shannon Entropy & Probability Calibration
  - **Why needed here**: Method relies on extracting $-\sum p \log p$ from model logits to measure uncertainty
  - **Quick check question**: If a model outputs probabilities [0.48, 0.52] versus [0.99, 0.01], which has higher entropy and what does that imply for data curation?

- **Concept**: Reward Modeling (Bradley-Terry/Preference Pairs)
  - **Why needed here**: Understanding input structure $(I, x, y_a, y_b)$ and objective of predicting preference $l$
  - **Quick check question**: Why is annotation noise particularly damaging for preference pairs compared to standard classification tasks?

- **Concept**: Curriculum Learning
  - **Why needed here**: Mechanism 3 relies on easy-to-hard heuristic to impact convergence speed and stability
  - **Quick check question**: Why might training on hard samples first cause gradient instability in early training epochs?

## Architecture Onboarding

- **Component map**: Base Model (Instruct) -> SFT Stage -> Reasoning RM -> Probing Stage -> Curation Module -> RL Trainer -> Final model
- **Critical path**: Calculation of Answer Token Entropy (Eq. 3) drives both curation and curriculum
- **Design tradeoffs**:
  - Data Efficiency vs. Coverage: Aggressive pruning risks losing edge-case knowledge but boosts stability
  - Entropy Type: Answer token entropy performs better than reasoning sentence entropy
- **Failure signatures**:
  - High-entropy collapse: All samples may look high-entropy if model is poorly initialized
  - Stagnation: Model may fail to generalize if easy subset contains systematic bias
- **First 3 experiments**:
  1. Validate entropy-accuracy correlation on held-out validation set
  2. Ablate entropy source: compare $e_{answer}$ vs $e_{reasoning}$ vs $e_{mix}$
  3. Scale data experiment: train on 15% vs 100% (random vs entropy-curated)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can reasoning sentence entropy be effectively aggregated without diluting the information signal?
- **Basis in paper**: Authors note calculating entropy over entire reasoning sentences provided minimal gain, hypothesizing averaging dilutes information signal
- **Why unresolved**: Current method relies solely on answer token entropy because averaging method failed
- **What evidence would resolve it**: Study comparing various aggregation methods (weighted averaging, max-pooling, attention-based) against answer-only baseline

### Open Question 2
- **Question**: Does aggressive pruning limit model's ability to generalize to inherently ambiguous or out-of-distribution queries?
- **Basis in paper**: Method prunes "unreliable and extremely difficult samples" assuming high entropy always indicates noise
- **Why unresolved**: Paper validates on standard benchmarks but doesn't evaluate robustness on novel, highly complex scenarios
- **What evidence would resolve it**: Evaluation on dataset of "hard" examples to see if model fails silently compared to full distribution model

### Open Question 3
- **Question**: Can entropy threshold be determined dynamically rather than using fixed percentile cutoffs?
- **Basis in paper**: Conclusion suggests pruning effectiveness "opens up new possibilities for adopting advanced training strategies like adaptive sampling"
- **Why unresolved**: Current implementation uses fixed heuristic (2,500 lowest entropy samples)
- **What evidence would resolve it**: Experiments using adaptive algorithms that adjust cutoff based on validation loss, comparing performance and efficiency against fixed 15% heuristic

## Limitations
- Entropy proxy depends critically on base model being well-calibrated and sufficiently trained
- Pruning strategy assumes high-entropy samples are predominantly noisy rather than genuinely difficult but valuable
- Performance on reasoning tasks beyond multimodal preferences remains untested

## Confidence
- **High Confidence**: Entropy-guided curriculum improves training stability and achieves state-of-the-art performance on tested multimodal reward benchmarks
- **Medium Confidence**: Answer token entropy outperforms reasoning sentence entropy is well-supported, but broader applicability requires validation
- **Medium Confidence**: Data efficiency claim is empirically demonstrated but may depend heavily on dataset characteristics and base model calibration

## Next Checks
1. **Calibration Analysis**: Verify entropy-accuracy correlation on held-out validation set across different base model scales
2. **Pruning Strategy Validation**: Conduct experiments on datasets where high-entropy samples contain valuable edge cases
3. **Cross-Domain Generalization**: Apply EGT to non-multimodal reasoning task to test generalization beyond original domain