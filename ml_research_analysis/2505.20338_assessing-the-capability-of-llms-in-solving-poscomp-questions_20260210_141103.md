---
ver: rpa2
title: Assessing the Capability of LLMs in Solving POSCOMP Questions
arxiv_id: '2505.20338'
source_url: https://arxiv.org/abs/2505.20338
tags:
- poscomp
- questions
- exam
- chatgpt-4
- gemini
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study assesses the ability of Large Language Models (LLMs)
  to solve POSCOMP exam questions, a challenging Brazilian graduate admissions test
  in computer science. We evaluated ChatGPT-4, Gemini 1.0 Advanced, Claude 3 Sonnet,
  and Le Chat Mistral on 2022 and 2023 exams, followed by recent models (o1, Gemini
  2.5 Pro, Claude 3.7 Sonnet, o3-mini-high) on 2022-2024 exams.
---

# Assessing the Capability of LLMs in Solving POSCOMP Questions
## Quick Facts
- arXiv ID: 2505.20338
- Source URL: https://arxiv.org/abs/2505.20338
- Authors: Cayo Viegas; Rohit Gheyi; Márcio Ribeiro
- Reference count: 25
- Primary result: LLMs match or surpass human performance on POSCOMP exam

## Executive Summary
This study evaluates the performance of large language models (LLMs) on POSCOMP, a challenging Brazilian graduate admissions test in computer science. The researchers tested multiple models including ChatGPT-4, Gemini 1.0, Claude 3, Mistral, and newer versions (o1, Gemini 2.5 Pro, o3-mini-high) across 2022-2024 exams. ChatGPT-4 consistently outperformed earlier models, particularly in Mathematics and Computer Science Fundamentals. Newer models demonstrated even stronger performance, with some achieving or exceeding 90% accuracy across topics and surpassing both average and top human scores. The study found LLMs excel at text-based reasoning but struggle with image interpretation, and confirmed robustness through metamorphic testing.

## Method Summary
The researchers evaluated LLMs on POSCOMP exams (2022-2024) using zero-shot prompting with zero-shot instruction prompts. Questions were translated from Portuguese to English via DeepL, and images were extracted for circuit diagrams, class hierarchies, and automata. Exams were segmented into ≤4-page chunks due to context window constraints. Performance was measured against official answer keys, with per-topic disaggregation (Mathematics, CS Fundamentals, Computing Technology) and comparison to human benchmark data. Metamorphic testing was applied to verify reasoning rather than memorization.

## Key Results
- ChatGPT-4 consistently outperformed other models across all exam years and topics
- Newer models (o1, Gemini 2.5 Pro, o3-mini-high) achieved 90%+ accuracy, surpassing human performance
- LLMs showed strong text-based reasoning (57.9% accuracy for ChatGPT-4) but struggled with image interpretation (15.9% for Gemini 1.0)
- Metamorphic testing confirmed robustness in newer models against semantic-preserving perturbations

## Why This Works (Mechanism)

### Mechanism 1: Text-Dominant Question Processing
Models leverage pre-trained text representations where semantic relationships between domain concepts are well-encoded. Text inputs allow direct token-level processing without information loss from vision-to-text translation steps.

### Mechanism 2: Language-Transfer Through English Mediation
English-language training data dominates LLM pre-training corpora, resulting in denser concept representations and more reliable reasoning chains in English. Translation shifts tasks from low-resource to high-resource language reasoning.

### Mechanism 3: Robustness Through Semantic Equivalence Testing
Metamorphic transformations alter surface features while preserving underlying problem structure. Consistent performance indicates generalization to problem schema rather than retrieval of specific training examples.

## Foundational Learning

- **Concept: Zero-shot vs. Few-shot Prompting**
  - Why needed here: The study uses zero-shot prompting exclusively; understanding this baseline is critical before experimenting with chain-of-thought or few-shot enhancements
  - Quick check question: If you provide three solved POSCOMP-style questions before the target question, are you still using zero-shot prompting?

- **Concept: Data Contamination and Memorization Risk**
  - Why needed here: The paper explicitly identifies training data overlap as a validity threat and uses metamorphic testing to mitigate it
  - Quick check question: If a model scores 95% on an exam published in 2022, what additional test would increase confidence that this reflects reasoning rather than memorization?

- **Concept: Multi-Modal Input Limitations**
  - Why needed here: Image-based prompts underperformed dramatically (15.9% vs. 57.9% for Gemini)
  - Quick check question: A question contains a circuit diagram with three logic gates and no accompanying text. Should you submit the image directly to a text-focused LLM, or first transcribe the gate configuration?

## Architecture Onboarding

- **Component map:** PDFs segmented into ≤4-page chunks (≤20 questions) -> Zero-shot prompt submission -> Model response recording -> Official answer key comparison -> Per-topic accuracy calculation

- **Critical path:** 1) Download official POSCOMP PDFs and answer keys 2) Segment exams into 5 PDF chunks 3) Submit each chunk via model web interface with Portuguese instruction prompt 4) Record responses and flag multi-answer/non-answer cases 5) Score against answer key and compute per-topic accuracy 6) Compare to published human score distributions

- **Design tradeoffs:** PDF chunking enables evaluation but loses inter-question context; Portuguese maintains exam authenticity while English may improve accuracy; zero-shot provides clean baseline while CoT may improve math reasoning

- **Failure signatures:** Multi-answer responses (Claude 3-5 instances, Mistral 1-5 per exam year); non-responses (ChatGPT-4 left 5 unanswered in 2022); calculation-interpretation mismatch (all models calculated 15.56 ns but selected wrong option)

- **First 3 experiments:** 1) Run ChatGPT-4 and Gemini 2.5 Pro on POSCOMP 2022 to verify score replication within ±2 questions 2) Compare accuracy between Portuguese-only, DeepL-translated English, and human-translated English prompts on 10-question sample 3) Test image-containing questions across image-only, text-only, and combined image+text conditions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do advanced prompt engineering techniques (few-shot, chain-of-thought) affect LLM performance on graduate-level computer science assessments compared to the zero-shot approach used in this study?
- Basis in paper: [explicit] The authors state: "We also aim to explore the impact of advanced prompt engineering techniques... including few-shot and chain-of-thought prompting, on the models' ability to solve complex academic tasks."
- Why unresolved: Only zero-shot prompting was evaluated; the study did not compare different prompting strategies.
- What evidence would resolve it: A controlled experiment comparing zero-shot, few-shot, and chain-of-thought prompting on the same POSCOMP questions.

### Open Question 2
- Question: Can more extensive metamorphic testing with diverse transformation strategies reveal hidden brittleness or memorization in LLMs solving POSCOMP questions?
- Basis in paper: [explicit] "We intend to expand our use of metamorphic testing by applying a larger and more diverse set of systematically transformed questions" and "applying metamorphic testing to a limited subset of questions does not fully eliminate the threat posed by variations in the training data."
- Why unresolved: Only 10 questions underwent metamorphic testing with limited transformation types.
- What evidence would resolve it: Systematic application of metamorphic transformations to all questions across multiple syntactic and semantic variation types.

### Open Question 3
- Question: How do LLMs from diverse architectural backgrounds (e.g., DeepSeek) perform on POSCOMP compared to the evaluated models from OpenAI, Google, Anthropic, and Mistral?
- Basis in paper: [explicit] "For future work, we plan to broaden our evaluation by incorporating additional LLMs, such as DeepSeek, to further investigate model diversity and performance."
- Why unresolved: Only four LLM families were evaluated; model diversity effects remain unexplored.
- What evidence would resolve it: Evaluation of additional LLMs from different developers on the same POSCOMP benchmarks.

### Open Question 4
- Question: To what extent does training data contamination inflate LLM performance metrics on standardized exams like POSCOMP?
- Basis in paper: [inferred] The paper identifies data contamination as "a key threat to validity when evaluating foundation models" and acknowledges that metamorphic testing on a limited subset "does not fully eliminate the threat."
- Why unresolved: No direct assessment of training data overlap with POSCOMP questions was conducted.
- What evidence would resolve it: Analysis of training data composition or evaluation on newly created questions unavailable during model training.

## Limitations
- Translation pipeline impact on accuracy remains uncertain due to lack of direct Portuguese vs English comparison
- Image processing limitations create significant performance gaps for visual question types
- Explanation quality assessment relies on qualitative scoring rather than standardized metrics

## Confidence
- High: Overall performance ordering (ChatGPT-4 > Gemini > Claude > Mistral) and superiority of newer models
- Medium: Image vs text performance gap and specific numerical accuracy values
- Low: Explanation quality assessment

## Next Checks
1. **Translation validation**: Re-run a subset of questions using both automated DeepL translation and human-verified Portuguese prompts to isolate the translation effect on accuracy
2. **Image processing evaluation**: Systematically compare performance on image-containing questions across three conditions: image-only, text-only transcription, and combined image+text to quantify information loss
3. **Context window analysis**: Test whether full-exam context (70 questions in single prompt) improves accuracy compared to the 4-page chunking methodology, particularly for questions requiring cross-reference between topics