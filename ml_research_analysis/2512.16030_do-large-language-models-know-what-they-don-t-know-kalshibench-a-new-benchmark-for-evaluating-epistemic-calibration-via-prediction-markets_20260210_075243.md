---
ver: rpa2
title: 'Do Large Language Models Know What They Don''t Know? Kalshibench: A New Benchmark
  for Evaluating Epistemic Calibration via Prediction Markets'
arxiv_id: '2512.16030'
source_url: https://arxiv.org/abs/2512.16030
tags:
- calibration
- confidence
- accuracy
- questions
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KalshiBench, a novel benchmark for evaluating
  epistemic calibration of large language models (LLMs) using prediction market questions
  with verifiable real-world outcomes. The key challenge addressed is that traditional
  benchmarks measure accuracy on static knowledge, while KalshiBench tests whether
  models can appropriately quantify uncertainty about genuinely unknown future events
  by using temporally-filtered questions that resolve after model training cutoffs.
---

# Do Large Language Models Know What They Don't Know? Kalshibench: A New Benchmark for Evaluating Epistemic Calibration via Prediction Markets

## Quick Facts
- arXiv ID: 2512.16030
- Source URL: https://arxiv.org/abs/2512.16030
- Authors: Lukas Nel
- Reference count: 40
- Primary result: All evaluated frontier models show systematic overconfidence on prediction market questions, with Expected Calibration Error ranging from 0.120 to 0.395

## Executive Summary
This paper introduces KalshiBench, a novel benchmark for evaluating whether large language models can appropriately quantify uncertainty about genuinely unknown future events. Unlike traditional benchmarks that test static knowledge recall, KalshiBench uses temporally-filtered prediction market questions that resolve after all model training cutoffs, ensuring models cannot have memorized outcomes. The study evaluates five frontier models on 300 binary prediction market questions, revealing that all models are systematically overconfident, with only one achieving positive Brier Skill Score. Critically, reasoning-enhanced models like GPT-5.2-XHigh perform worse on calibration despite comparable accuracy, suggesting that scaling and enhanced reasoning do not automatically confer calibration benefits.

## Method Summary
The benchmark uses 300 binary prediction market questions from Kalshi, temporally filtered to resolve after October 1, 2025. Five frontier models (Claude Opus 4.5, GPT-5.2-XHigh, DeepSeek-V3.2, Qwen3-235B, Kimi-K2) are prompted with explicit calibration instructions to provide yes/no predictions with 0-100 confidence scores. Models are evaluated using Brier Score, Brier Skill Score, Expected Calibration Error (ECE), Maximum Calibration Error (MCE), and accuracy metrics. The temporal filtering ensures questions genuinely require epistemic uncertainty quantification rather than knowledge retrieval.

## Key Results
- All models exhibit systematic overconfidence with ECE ranging from 0.120 to 0.395
- Only Claude Opus 4.5 achieves positive Brier Skill Score (0.057), meaning most models perform worse than simply predicting base rates
- At 90%+ confidence levels, models are wrong 15-32% of the time, far exceeding the <10% expected for well-calibrated systems
- Reasoning-enhanced models like GPT-5.2-XHigh show worse calibration (ECE=0.395) despite comparable accuracy to less reasoning-intensive models

## Why This Works (Mechanism)

### Mechanism 1
Temporal filtering prevents memorization-based false calibration signals by selecting only questions that resolve after all model training cutoffs. This ensures models cannot have memorized outcomes and must quantify genuine uncertainty about unknowns. The core assumption is that models cannot systematically predict future outcomes through pattern extrapolation from training data. Break condition: If models can predict outcomes through learned priors (e.g., election favorites win ~80% of the time), temporal filtering alone may not eliminate contamination.

### Mechanism 2
Extended reasoning chains can amplify rather than correct overconfidence by generating longer argument chains that tend to support initial hypotheses through confirmation bias. This produces the counterintuitive result that more compute/reasoning worsens calibration. The core assumption is that the reasoning process is optimizing for persuasive coherence rather than epistemic accuracy. Break condition: If reasoning were explicitly trained with calibration-aware objectives, this mechanism could reverse.

### Mechanism 3
Calibration and accuracy are decoupled capabilities requiring separate optimization. Standard language modeling objectives reward correct predictions without penalizing miscalibrated confidence, and RLHF may pressure models toward confident-sounding responses. This creates models that are accurate but systematically overconfident. The core assumption is that human raters prefer confident responses over appropriately hedged ones. Break condition: If training explicitly incorporated calibration loss, accuracy and calibration could co-improve.

## Foundational Learning

- **Expected Calibration Error (ECE)**: The primary metric measuring average gap between confidence and accuracy. Understanding binning is essential for interpreting reliability diagrams. Quick check: If a model has ECE=0.12, what does that mean in practical terms? (Answer: On average, its confidence differs from accuracy by 12 percentage points.)

- **Brier Skill Score (BSS)**: Determines whether a model beats naive baseline prediction. Only Claude Opus 4.5 achieved positive BSS (0.057), meaning all others performed worse than simply predicting the base rate. Quick check: What does a negative BSS indicate? (Answer: The model's probability estimates are worse than always predicting the dataset's base rate.)

- **Reliability Diagrams**: Visual diagnostic showing where calibration fails. The paper reveals all models become catastrophically overconfident at 90%+ confidence (15-32% error rates vs. expected <10%). Quick check: In a reliability diagram, what does the diagonal line represent? (Answer: Perfect calibration—predicted confidence equals actual accuracy.)

## Architecture Onboarding

- **Component map**: Kalshi API → temporal filter → deduplication (max 2 per series_ticker) → 300-question sample → Prompt templates with explicit calibration instruction → Model inference → Structured output parsing → Metrics calculation
- **Critical path**: The temporal filtering logic (Equation 1) is the integrity gate—if cutoffs are incorrect, the entire evaluation is contaminated
- **Design tradeoffs**: Sample size (300) vs. statistical power at category level (some categories have <5 questions); binary outcomes only—excludes continuous forecasts and multi-outcome markets; self-reported confidence (0-100) may not reflect true internal probability estimates
- **Failure signatures**: Model concentrates predictions in 90-100% confidence bin with <50% accuracy; model shows higher confidence when wrong than when correct; positive accuracy but negative BSS (beating random classification but losing to base-rate prediction)
- **First 3 experiments**: 1) Reproduce ECE calculation on a 50-question subset to validate binning logic before full evaluation; 2) Test prompt sensitivity: compare "be calibrated" instruction vs. neutral prompt to isolate instruction-following vs. capability gap; 3) Run post-hoc temperature scaling on model confidences to determine if calibration is recoverable without retraining

## Open Questions the Paper Calls Out

1. **What specific mechanisms cause extended reasoning to worsen calibration rather than improve it?** The paper shows GPT-5.2-XHigh uses 26× more tokens yet achieves worse calibration (ECE=0.395 vs 0.120), but the causal mechanism remains speculative. Evidence would require ablation studies comparing reasoning chains that actively seek disconfirming evidence vs. confirming evidence, measuring calibration changes.

2. **Can calibration-aware training objectives or explicit uncertainty modeling architectures substantially improve epistemic calibration?** Current training objectives reward correct predictions without penalizing miscalibrated confidence; no such modified training was tested. Evidence would require training models with proper scoring rule objectives and comparing resulting calibration on held-out prediction market questions.

3. **Do alternative confidence elicitation methods such as betting mechanisms or proper scoring rule incentives yield better-calibrated probability estimates?** All experiments used direct confidence elicitation (0-100 scale); no incentive-aligned methods were tested. Evidence would require comparing calibration when models are prompted with hypothetical betting scenarios vs. direct confidence requests on the same questions.

## Limitations
- Temporal filtering cannot completely eliminate the possibility that models infer future outcomes through learned priors or patterns in training data
- Relatively small sample size (300 questions) distributed across 7 categories, with some categories having fewer than 5 questions, limiting statistical power for granular analysis
- Binary outcome restriction excludes continuous and multi-outcome markets that may present different calibration challenges

## Confidence
- **High confidence**: Models exhibit systematic overconfidence (ECE 0.120-0.395) and only one model achieves positive BSS
- **Medium confidence**: Reasoning-enhanced models perform worse on calibration despite comparable accuracy
- **Medium confidence**: Temporal filtering successfully isolates genuinely unknown questions, though perfect elimination of inference contamination cannot be guaranteed

## Next Checks
1. **Temporal filtering robustness test**: Conduct ablation studies comparing results from questions resolving immediately after training cutoffs versus those resolving 3+ months later to quantify contamination from short-term pattern inference.

2. **Cross-dataset replication**: Apply the same evaluation protocol to alternative prediction markets (PredictIt, Manifold Markets) and natural event prediction datasets to verify whether overconfidence patterns generalize beyond Kalshi.

3. **Calibration intervention study**: Test whether post-hoc temperature scaling or explicit calibration-aware fine-tuning can recover well-calibrated behavior, distinguishing between capability gaps versus training objective misalignment.