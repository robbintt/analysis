---
ver: rpa2
title: 'HyperCore: The Core Framework for Building Hyperbolic Foundation Models with
  Comprehensive Modules'
arxiv_id: '2504.08912'
source_url: https://arxiv.org/abs/2504.08912
tags:
- hyperbolic
- hypercore
- foundation
- graph
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "HyperCore is a comprehensive framework for building hyperbolic\
  \ foundation models across diverse modalities, addressing the lack of essential\
  \ components in existing tools. The framework provides intuitive and flexible modules\
  \ for constructing novel hyperbolic models, supporting both Lorentz hyperboloid\
  \ and Poincar\xE9 ball models."
---

# HyperCore: The Core Framework for Building Hyperbolic Foundation Models with Comprehensive Modules

## Quick Facts
- arXiv ID: 2504.08912
- Source URL: https://arxiv.org/abs/2504.08912
- Authors: Neil He; Menglin Yang; Rex Ying
- Reference count: 40
- Primary result: Framework for building hyperbolic foundation models with modules supporting Lorentz hyperboloid and Poincaré ball models, demonstrated by constructing the first fully hyperbolic vision transformer (LViT) and CLIP model (L-CLIP).

## Executive Summary
HyperCore is a comprehensive framework that addresses the lack of essential components for building hyperbolic foundation models across diverse modalities. The framework provides intuitive and flexible modules for constructing novel hyperbolic models, supporting both Lorentz hyperboloid and Poincaré ball models. HyperCore's core includes hyperbolic linear layers, convolutional networks, transformers, attention mechanisms, and graph encoders with per-layer variable curvatures. The framework was demonstrated by building the first fully hyperbolic vision transformer (LViT) and CLIP model (L-CLIP), as well as a hybrid GraphRAG with hyperbolic graph encoder. LViT outperformed its Euclidean counterpart on image classification tasks, while L-CLIP showed successful training on multi-modal learning.

## Method Summary
HyperCore provides a modular framework for building hyperbolic foundation models using Riemannian geometry principles. The framework implements hyperbolic linear layers, transformers, attention mechanisms, and graph encoders that operate directly on the manifold rather than using tangent space projections. Key innovations include per-layer variable curvatures allowing geometric adaptation across network depths, and support for both Lorentz hyperboloid and Poincaré ball models. The framework employs Riemannian optimization techniques (Riemannian Adam) for manifold parameters while maintaining standard optimizers for Euclidean components. The architecture follows a standard path: input projection to manifold via exponential map, backbone processing through hyperbolic layers with residuals and attention, and output through pooling and manifold-based classification.

## Key Results
- LViT (fully hyperbolic vision transformer) achieved 79.4% accuracy on ImageNet-1K compared to 77.91% for Euclidean HVT baseline.
- L-CLIP (fully hyperbolic CLIP model) successfully trained on multi-modal learning tasks.
- Hyperbolic GNNs with variable curvatures showed significant performance improvements across link prediction, node classification, and graph reconstruction tasks.
- The framework enables comprehensive benchmarking of hyperbolic models without requiring researchers to construct modules from scratch.

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Data Embedding Efficiency
Hyperbolic spaces provide superior embedding efficiency for data with scale-free or hierarchical properties compared to Euclidean spaces. Hyperbolic space has constant negative curvature, causing volume to grow exponentially with distance from the origin, allowing "tree-like" or hierarchical data (which also grows exponentially) to be embedded with significantly lower distortion than in Euclidean space (polynomial growth). Core assumption: input data (e.g., token distributions in LLMs, graph structures) possesses latent hierarchical or scale-free geometry. Evidence: token distributions in foundation models exhibit "scale-free properties" and hyperbolic space is a "more suitable ambient space." Break condition: if underlying data is Euclidean (flat) or lacks hierarchical structure, hyperbolic inductive bias may overfit or add unnecessary complexity.

### Mechanism 2: Manifold-Native Operations
Fully hyperbolic operations (operating directly on the manifold) preserve geometric integrity better than hybrid approaches relying on tangent space projections. Operations like linear transforms or attention are mathematically formulated to occur directly on the Lorentz hyperboloid or Poincaré ball, avoiding the distortion-prone step of projecting to a flat tangent space and back. Core assumption: the geometry of the manifold provides a stronger prior than the linearity of the tangent space for these specific operations. Evidence: fully hyperbolic LViT outperforms HVT, which contains tangent space based operations. Break condition: if curvature is extremely high or numerical instability arises, standard tangent-space approximations might act as a necessary regularizer.

### Mechanism 3: Per-Layer Variable Curvatures
Per-layer variable curvatures allow the model to adapt the geometric "spread" of embeddings at different depths of the network. Instead of fixing curvature K=-1, the framework allows K to be a learnable parameter, letting early layers capture local neighborhoods (flatter) and deeper layers capture global hierarchies (more curved), or vice versa. Core assumption: different layers in a deep network represent different levels of abstraction, each with a potentially optimal geometric curvature. Evidence: hyperparameter search over curvatures "benefited significantly" for tasks like link prediction on the Disease dataset. Break condition: if learnable curvature collapses to zero (Euclidean) or infinite (singularity), geometric benefits are lost.

## Foundational Learning

- **Concept: Riemannian Manifolds (Hyperbolic Geometry)**
  - **Why needed here:** To understand that standard matrix multiplication (Wx) does not keep data on the hyperbolic surface. You must define operations (exp/log maps) to enter the space and "parallel transport" to move vectors correctly.
  - **Quick check question:** Can you explain why you cannot simply add two vectors in hyperbolic space without leaving the manifold?

- **Concept: Lorentz vs. Poincaré Models**
  - **Why needed here:** HyperCore supports both. Lorentz (Hyperboloid) is often numerically stable and unbounded, while Poincaré (Ball) is bounded and easier to visualize. They are isometric (mathematically equivalent), but code differs significantly.
  - **Quick check question:** Why does the Lorentz model typically use one extra dimension (ambient space n+1) compared to the Poincaré ball for the same effective embedding dimension?

- **Concept: Riemannian Optimization**
  - **Why needed here:** Standard SGD/Adam assumes Euclidean gradients. In hyperbolic space, the gradient direction depends on the current point's curvature. You must use Riemannian Adam or project gradients.
  - **Quick check question:** What happens if you apply standard Euclidean Adam to update parameters directly on the hyperboloid manifold without projection?

## Architecture Onboarding

- **Component map:** Input tensor -> Exponential map (project to Manifold) -> Stack of LorentzLinear/Conv -> LResNet (skip connection) -> LorentzMultiheadAttention -> Pooling -> LorentzMLR (classification)

- **Critical path:**
  1. **Input:** Euclidean tensor
  2. **Embedding:** exponential_map (project to Manifold)
  3. **Backbone:** Stack of LorentzLinear (or Conv) → LResNet (skip connection) → LorentzMultiheadAttention
  4. **Head:** Pooling → LorentzMLR (Multinomial Logistic Regression on the manifold)

- **Design tradeoffs:**
  - **Lorentz vs. Poincaré:** Lorentz handles "center" operations (like centroids) better, while Poincaré is standard for embeddings
  - **Speed vs. Geometry:** Fully hyperbolic layers are computationally heavier but offer better performance (79.4% vs 77.91% on ImageNet)
  - **Ambient Dimension:** Lorentz layers output d+1 dimensions because of the time-like coordinate

- **Failure signatures:**
  - **Dimension Mismatch:** Forgetting the extra time-like dimension in Lorentz layers (input d, expected d+1)
  - **Numerical Collapse:** NaN appearing if curvature becomes extremely small or operations push points "past" infinity
  - **Gradient Issues:** Exploding gradients if Riemannian optimizers are not used or if curvature is not clipped

- **First 3 experiments:**
  1. **Sanity Check (MLP):** Build 2-layer LorentzLinear MLP for MNIST classification to verify exp_map and LorentzMLR connectivity
  2. **GNN Reproduction:** Reproduce link prediction task on Disease dataset using HGCN or HyboNet layers to validate graph encoder modules
  3. **LViT Fine-tuning:** Run LViT fine-tuning pipeline on CIFAR-10 using HypLoRA to test vision transformer assembly and residual connections

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the choice of curvature (fixed vs. learnable) specifically influence the behavior and performance of hyperbolic foundation models? The authors list "Curvature Sensitivity" as a future direction, noting that while some works fix curvature at -1 and others make it learnable, the actual impact remains underexplored.

- **Open Question 2:** What are the geometric implications of designing non-linear hyperbolic operations as projections of their Euclidean counterparts? The authors point out that many modules are implemented by performing Euclidean operations on the time-like dimension and projecting back, stating the "geometric meaning... are usually not discussed."

- **Open Question 3:** Do hyperbolic foundation models require specialized training schemes to stabilize pre-training at scale? The paper lists "Hyperbolic Training Schemes" as a future analysis direction, stating that "training dynamics... remain poorly understood, particularly in the context of pre-training large-scale models."

- **Open Question 4:** Why does high performance on link prediction and node classification fail to correlate with graph reconstruction performance in hyperbolic GNNs? The authors note that "performance for link prediction and node classification tasks are not necessarily good indicators for the model performance on graph reconstruction."

## Limitations

- **Generalization Gap:** Superior performance is demonstrated primarily on structured datasets with hierarchical properties; extent to unstructured data remains untested.
- **Numerical Stability:** Paper acknowledges but doesn't thoroughly analyze numerical instability in Poincaré models and high-curvature regimes.
- **Computational Overhead:** Fully hyperbolic models require additional manifold operations and extra time-like dimension, but comprehensive speed analysis is lacking.

## Confidence

**High Confidence:** The core claim that hyperbolic spaces can embed hierarchical data more efficiently than Euclidean spaces is well-established in geometry literature and supported by theoretical framing.

**Medium Confidence:** The claim that fully hyperbolic operations outperform hybrid tangent-space approaches is supported by specific comparisons but lacks comprehensive ablation studies.

**Medium Confidence:** The framework's engineering implementation appears sound with clear modular design, though some hyperparameters are underspecified.

**Low Confidence:** The claim that this framework will enable "future research to focus on deeper analysis rather than construction" is speculative.

## Next Checks

1. **Cross-domain validation:** Test LViT and L-CLIP on datasets without clear hierarchical structure (e.g., CIFAR-100 fine-grained classes, medical imaging without organ hierarchy) to assess generalization limits.

2. **Numerical stability analysis:** Systematically evaluate model performance across the full curvature parameter space (K ∈ [-1, 0]) for different tasks to identify stability boundaries and determine if automatic curvature regularization is needed.

3. **Computational overhead quantification:** Measure wall-clock training time, memory usage, and inference latency for fully hyperbolic vs. hybrid vs. Euclidean baselines across different hardware configurations to provide complete cost-benefit analysis.