---
ver: rpa2
title: Pretraining Frequency Predicts Compositional Generalization of CLIP on Real-World
  Tasks
arxiv_id: '2502.18326'
source_url: https://arxiv.org/abs/2502.18326
tags:
- uni00000014
- uni00000004
- uni00000012
- uni00000049
- uni00000052
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "We investigate CLIP\u2019s compositional generalization by curating\
  \ real-world retrieval test sets containing only novel object combinations not seen\
  \ during pretraining. We find that CLIP\u2019s performance on these combinations\
  \ is consistently high and can be accurately predicted from the pretraining frequencies\
  \ of individual objects."
---

# Pretraining Frequency Predicts Compositional Generalization of CLIP on Real-World Tasks

## Quick Facts
- **arXiv ID:** 2502.18326
- **Source URL:** https://arxiv.org/abs/2502.18326
- **Reference count:** 40
- **Primary result:** CLIP's compositional generalization on novel object combinations can be accurately predicted from the geometric mean of individual object pretraining frequencies.

## Executive Summary
This paper investigates whether CLIP exhibits compositional generalization - the ability to correctly retrieve images containing novel object combinations not seen during pretraining. The authors curate real-world retrieval test sets where objects co-occur zero times in pretraining data, then demonstrate that CLIP's performance on these combinations can be predicted from individual object frequencies using the geometric mean. Across five CLIP architectures and four pretraining datasets, performance scales predictably with object frequency, suggesting CLIP learns independent object representations that can be recomposed. The findings imply that balancing object frequencies during data curation could improve generalization efficiency without scaling data volume.

## Method Summary
The study evaluates CLIP's compositional generalization on Text-to-Image (T2I) and Image-to-Text (I2T) retrieval tasks. The authors extract 945 tangible nouns from captions, verify object presence in pretraining and test images using RAM++ visual tagging, and filter test sets to retain only samples where objects co-occur zero times in pretraining ($f_{\cap,D}(x) = 0$) but individual objects appear ($f_D > 0$). They compute average sample frequency using the geometric mean of individual object frequencies and fit logistic regression models to predict retrieval success probability from this frequency metric. The methodology is applied across multiple CLIP architectures (ResNet-50, ResNet-50×4, ViT-B-16, ViT-B-32, ViT-L-14) and pretraining datasets (CC-3M, CC-12M, YFCC-15M, LAION400M, SynthCI-30M).

## Key Results
- CLIP's performance on novel object combinations matches or nearly matches known combinations across architectures and datasets
- The geometric mean of individual object frequencies accurately predicts compositional generalization performance via linear regression
- Underrepresented objects act as bottlenecks for compositional generalization
- The frequency-performance relationship holds across architectures, parameter counts, and data scales

## Why This Works (Mechanism)

### Mechanism 1
CLIP's compositional generalization performance on novel object combinations can be predicted from the geometric mean of individual object pretraining frequencies. Objects are learned independently during contrastive pretraining, so when encountering a novel combination (o₁, o₂), retrieval probability factorizes: P(correct|o₁, o₂) ≈ P(correct|o₁) × P(correct|o₂). The geometric mean captures this multiplicative dependency, where the weakest object (lowest frequency) disproportionately limits performance. This assumes object representations are sufficiently disentangled that their contributions to retrieval are approximately independent.

### Mechanism 2
CLIP generalizes to novel object combinations at near-ceiling performance when constituent objects have been observed sufficiently often, even with zero co-occurrence in pretraining. Web-scale pretraining exposes the model to objects across diverse contexts, enabling context-agnostic feature extraction. When evaluating novel combinations, existing object embeddings can be composed without requiring joint observation during training. This assumes frequency correlates with representation quality; higher exposure yields more robust embeddings.

### Mechanism 3
Underrepresented objects act as bottlenecks for compositional generalization; balancing object frequencies improves efficiency without increasing data volume. Since compositional performance depends multiplicatively on constituent frequencies, the lowest-frequency object dominates the geometric mean. Increasing rare object counts yields larger marginal gains than further increasing common objects. This assumes the frequency-performance relationship is approximately logarithmic/saturating for individual objects.

## Foundational Learning

- **Compositional Generalization (in VLMs):**
  - Why needed here: The paper's central construct; understanding that novel combinations ≠ novel concepts
  - Quick check question: Can you explain why zero co-occurrence (f_∩,D(x) = 0) doesn't imply the model has never seen the objects involved?

- **Geometric vs. Arithmetic Mean for Multiplicative Aggregation:**
  - Why needed here: The paper uses geometric mean specifically because object contributions multiply; arithmetic mean would obscure bottleneck effects
  - Quick check question: For frequencies [10, 1000], which mean better reflects the limiting factor for composition performance?

- **Co-occurrence Frequency (f_∩,D) vs. Individual Frequency (f_D):**
  - Why needed here: The experimental design hinges on filtering test samples where f_∩,D(x) = 0 but f_D > 0 for all objects
  - Quick check question: Why must both conditions hold to claim "compositional generalization" rather than mere memorization?

## Architecture Onboarding

- **Component map:**
  - RAM++ image tagging + caption lemmatization → 945 nouns → concept extraction pipeline
  - Pretraining corpus → individual object frequencies (f_D) and co-occurrence counts (f_∩,D)
  - Flickr-1K/COCO-5K → filtered test sets with f_∩,D(x) = 0, f_D > 0 ∀ objects
  - CLIP models → Recall@k evaluation
  - f_avg (geometric mean) → logistic regression prediction model

- **Critical path:**
  1. Obtain pretraining corpus with caption/image pairs
  2. Run concept extraction (text + visual verification) on full corpus
  3. Compute per-object frequencies and co-occurrence statistics
  4. Filter retrieval benchmarks to isolate compositional generalization samples
  5. Evaluate CLIP variants; fit f_avg → performance predictor

- **Design tradeoffs:**
  - Concept granularity: Limited to 945 tangible nouns; abstract concepts excluded due to annotation difficulty
  - Strict novelty: Requiring f_∩,D(x) = 0 reduces test set size (17-44% T2I, 89-98% I2T retained)
  - Visual verification: Using RAM++ reduces false positives from caption-only presence

- **Failure signatures:**
  - Predictor fails on high-performing models (ceiling effects flatten relationship)
  - I2T shows larger known/unknown gap than T2I (caption inconsistency with image background objects)
  - Synthetic pretraining data shows fewer novel compositions but similar scaling

- **First 3 experiments:**
  1. **Reproduce on a single model-dataset pair:** Take ViT-B-16 on CC-3M; compute object frequencies; filter COCO-5K for f_∩ = 0; verify linear f_avg → Recall@10 relationship
  2. **Ablate aggregation function:** Compare geometric mean vs. arithmetic mean vs. minimum frequency as predictors; expect geometric mean to best capture bottleneck
  3. **Pilot balanced curation:** Subset CC-12M to balance object frequencies (upsample rare objects); compare compositional generalization vs. unbalanced baseline of same size

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the linear relationship between pretraining frequency and compositional generalization hold for generative models like diffusion models or Large Language Models (LLMs)? The study exclusively evaluated discriminative CLIP architectures on retrieval tasks.
- **Open Question 2:** Can compositional generalization be accurately predicted for complex relationships like attribute-binding with multiple objects? The independence assumption may fail when modeling complex interactions or spatial bindings.
- **Open Question 3:** How does the partial co-occurrence of objects in the pretraining data influence the model's ability to generalize to novel combinations? The current methodology filters strictly for samples with zero joint co-occurrence, ignoring sub-combinations that might aid generalization.

## Limitations
- The study is limited to 945 tangible nouns, excluding abstract concepts, actions, and styles which could behave differently
- The test sets are filtered to only include truly novel combinations (f_∩,D = 0), which may not reflect real-world distribution shifts
- The paper proposes but does not empirically validate that frequency-balanced pretraining improves downstream performance

## Confidence
- **High Confidence:** The geometric mean frequency predicting compositional generalization performance - supported by consistent results across 5 architectures and 4 pretraining datasets
- **Medium Confidence:** The claim that frequency balancing improves efficiency without scaling data volume - proposed but not empirically validated with balanced pretraining experiments
- **Medium Confidence:** The independence assumption underlying the factorization model - while empirical fit is good, the paper explicitly notes this as an open question for complex compositions

## Next Checks
1. **Relational Composition Test:** Design experiments with compositions requiring explicit object relationships (e.g., "cat under table" vs "table under cat") to test the independence assumption's limits and measure performance degradation

2. **Frequency-Balanced Pretraining Validation:** Implement a frequency-balanced version of CC-12M by upsampling underrepresented objects and train CLIP from scratch to verify whether the proposed efficiency gains materialize in practice

3. **Cross-Domain Generalization:** Evaluate the frequency-performance relationship on non-photographic domains (medical imaging, satellite imagery) where object co-occurrence patterns differ systematically from web-scale data to test the model's robustness to domain shift