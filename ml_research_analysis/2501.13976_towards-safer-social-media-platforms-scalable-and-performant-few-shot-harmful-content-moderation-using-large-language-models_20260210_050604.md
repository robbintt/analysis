---
ver: rpa2
title: 'Towards Safer Social Media Platforms: Scalable and Performant Few-Shot Harmful
  Content Moderation Using Large Language Models'
arxiv_id: '2501.13976'
source_url: https://arxiv.org/abs/2501.13976
tags:
- content
- title
- classification
- harmful
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of scalable and dynamic content
  moderation on social media platforms by employing Large Language Models (LLMs) for
  few-shot harmful content detection. The core method involves using in-context learning
  with minimal supervision, where LLMs classify content as harmful or harmless based
  on a small set of exemplars provided within the prompt.
---

# Towards Safer Social Media Platforms: Scalable and Performant Few-Shot Harmful Content Moderation Using Large Language Models

## Quick Facts
- arXiv ID: 2501.13976
- Source URL: https://arxiv.org/abs/2501.13976
- Reference count: 27
- Primary result: GPT-4o-Mini achieves 78.57% accuracy in 12-shot FS-ICL, outperforming proprietary baselines and demonstrating strong few-shot harm detection capability.

## Executive Summary
This paper addresses scalable content moderation on social media by employing Large Language Models (LLMs) for few-shot harmful content detection. The core approach uses in-context learning with minimal supervision, where LLMs classify content as harmful or harmless based on a small set of exemplars provided within the prompt. The study evaluates various LLM models, including open-source and proprietary ones, across zero-shot, few-shot in-context learning (FS-ICL), and multimodal FS-ICL settings. Results show that LLMs, even in zero-shot settings, outperform existing proprietary baselines like Perspective API and OpenAI Moderation API, with significant improvements in few-shot configurations and multimodal approaches.

## Method Summary
The method employs in-context learning where LLMs classify YouTube videos as "Harmful" or "Harmless" based on title text and thumbnail images. Three approaches are tested: zero-shot learning (direct classification without exemplars), few-shot in-context learning with 8-14 exemplars selected via coverage-based selectors (BERTScore, Cosine, BM25), and multimodal FS-ICL using either caption generation (BLIP) or direct image input with multimodal LLMs. Models evaluated include Mistral-7B, Llama2-13B, GPT-3.5-Turbo, and GPT-4o-Mini, with accuracy as the primary metric compared against commercial baselines.

## Key Results
- Zero-shot LLMs achieve 65-70% accuracy, outperforming proprietary baselines (50-56%)
- GPT-4o-Mini reaches 78.57% accuracy in 12-shot configuration using BERTScore selector
- Multimodal approach with GPT-4o-Mini achieves 80% accuracy in 14-shot configuration
- Open-source multimodal models (LLaVa, OpenFlamingo) perform near random (~50%) on this task

## Why This Works (Mechanism)

### Mechanism 1: Zero-Shot Harm Classification via Pre-trained Knowledge
- Claim: LLMs can classify harmful content without explicit examples because their pre-training corpora contain implicit knowledge about harm categories.
- Mechanism: The LLM's pre-training on diverse web text exposes it to patterns of harmful language and harmless discourse. When prompted with a classification task, the model retrieves this latent knowledge to map inputs to binary labels without gradient updates.
- Core assumption: Harmful content patterns in pre-training data sufficiently overlap with the target moderation task.
- Evidence anchors:
  - [abstract]: "LLMs, even in zero-shot settings, outperform existing proprietary baselines like Perspective API and OpenAI Moderation API."
  - [section 4.3]: "Our ZSL approach... was able to achieve accuracies of 65% (Mistral-7B), 69% (Llama2-13B; GPT-4o-Mini), and 70% (GPT-3.5-Turbo)" vs. 50-56% for proprietary baselines.
  - [corpus]: Limited direct corpus support; related work focuses on transformer-based moderation rather than zero-shot mechanisms.
- Break condition: If harmful content patterns diverge significantly from pre-training distribution, zero-shot performance degrades.

### Mechanism 2: Few-Shot In-Context Learning with Coverage-Based Exemplar Selection
- Claim: Providing 8-14 carefully selected labeled examples in the prompt improves classification by anchoring the model's reasoning to task-relevant features.
- Mechanism: Coverage-based selectors (BERTScore, Cosine, BM25) identify exemplars that maximize coverage of salient aspects in the test sample. The LLM attends to these demonstrations during inference, using them as implicit gradient signals to refine its internal representation of the decision boundary.
- Core assumption: The selector captures relevant semantic similarity, and the LLM can generalize from the exemplar distribution to the test sample.
- Evidence anchors:
  - [abstract]: "LLMs achieve significant improvements, with GPT-4o-Mini reaching an accuracy of 78.57% using 12 shots and the BERTScore selector."
  - [section 3.3]: "Coverage-based ICL approaches ensure maximally informative demonstrations are selected by submodular optimization."
  - [section 5]: "Imbalanced selection generally results in better performance across the board" than balanced class selection.
- Break condition: If exemplars are irrelevant or the selector fails to capture task-specific semantics, performance may not improve (or could degrade via context window noise).

### Mechanism 3: Multimodal Signal Integration via Vision-Language Models
- Claim: Direct visual input (video thumbnails) enhances harm detection when using capable multimodal LLMs, by capturing signals absent from text alone.
- Mechanism: Native multimodal LLMs (e.g., GPT-4o-Mini) process visual and textual inputs jointly, enabling detection of harm cues present only in images (e.g., suggestive thumbnails paired with innocuous titles).
- Core assumption: The multimodal model has sufficient vision-language grounding to extract harm-relevant visual features.
- Evidence anchors:
  - [abstract]: "Multimodal approaches incorporating visual information further enhance performance... GPT-4o-Mini achieves an accuracy of 80% in the 14-shot configuration."
  - [section 4.3]: "GPT-4o-Mini in the 14-shot configuration achieves the best performance... However, this is not the case for LLaVa and OpenFlamingo" (≈50% accuracy).
  - [corpus]: Related work on multimodal moderation systems (e.g., MTikGuard) supports the utility of visual signals but does not address the closed-vs-open-source performance gap.
- Break condition: Open-source multimodal models with weaker vision-language alignment may not benefit; caption-based multimodal fusion can introduce error propagation.

## Foundational Learning

- **In-Context Learning (ICL)**:
  - Why needed here: The paper's core method relies on few-shot ICL to improve LLM moderation without fine-tuning.
  - Quick check question: Can you explain why ICL does not require gradient updates, and what factors influence exemplar selection quality?

- **Coverage-Based Submodular Optimization**:
  - Why needed here: BERTScore and related selectors are framed as coverage maximization problems over exemplar sets.
  - Quick check question: How does BERTScore-Recall differ from cosine similarity in capturing semantic coverage?

- **Multimodal Fusion Architectures**:
  - Why needed here: The paper compares caption-based vs. direct-image-input multimodal strategies, with performance implications.
  - Quick check question: What are the trade-offs between caption-based fusion (BLIP + text-only LLM) and native multimodal LLMs for this task?

## Architecture Onboarding

- **Component map**:
  Exemplar selector (BERTScore/Cosine/BM25) → Prompt constructor → LLM (text-only or multimodal) → Binary classifier output

- **Critical path**:
  1. Curate labeled exemplar pool (balanced or imbalanced; paper suggests imbalanced may be better)
  2. For each test sample, select k exemplars via coverage-based selector
  3. Construct prompt with task instruction + exemplars + test sample
  4. Query LLM; parse binary output

- **Design tradeoffs**:
  - Closed-source vs. open-source LLMs: Closed-source (GPT-4o-Mini) achieves higher accuracy but incurs API costs and latency (10-30s per multimodal inference per [section 7]).
  - Shot count: 8-14 shots yield similar results; diminishing returns beyond 8 due to limited salient aspects in short titles.
  - Selector choice: BERTScore consistently outperforms Cosine and BM25 in the paper's experiments.

- **Failure signatures**:
  - Open-source multimodal models (LLaVa, OpenFlamingo) perform near random (≈50% accuracy) on this task.
  - Caption-based fusion (FS-ICL-CG) can reduce performance if generated captions fail to capture visual harm signals.
  - Balanced exemplar selection may underperform vs. imbalanced selection (per ablation in section 5).

- **First 3 experiments**:
  1. Replicate zero-shot baseline: Compare GPT-4o-Mini vs. Perspective/OpenAI Moderation APIs on a held-out subset of the YouTube dataset to validate reported accuracy gaps.
  2. Ablate selector and shot count: Run FS-ICL with BERTScore vs. Cosine vs. BM25 at k=8, 12, 14 shots to confirm BERTScore superiority and shot-count sensitivity.
  3. Test multimodal strategies: Compare FS-ICL-CG (BLIP captions) vs. FS-ICL-DII (native VLM) on a subset of visually distinctive harmful content (e.g., sexual/clickbait thumbnails) to diagnose caption error propagation vs. direct vision grounding.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can specific fine-tuning or architectural modifications bridge the performance gap between open-source and closed-source multimodal models in harmful content detection?
- **Basis in paper:** [explicit] The authors note that while GPT-4o-Mini achieves 80% accuracy, open-source models like LLaVA and OpenFlamingo perform poorly (~50-54%), concluding that performance gains are "contingent on the LLM being used."
- **Why unresolved:** The paper evaluates existing models but does not propose a solution to improve the visual reasoning capabilities of the underperforming open-source Vision-Language Models (VLMs).
- **What evidence would resolve it:** Experiments demonstrating that fine-tuning open-source VLMs on harm-specific datasets or utilizing improved visual encoders can raise their accuracy to compete with proprietary models.

### Open Question 2
- **Question:** How can visual captioning models be enhanced to capture subtle harm indicators that text-only LLMs currently miss?
- **Basis in paper:** [inferred] The authors observe that the Caption Generation (FS-ICL-CG) method often reduces performance compared to text-only baselines, hypothesizing that generated captions fail to capture "subtle harm information" present in the visual domain.
- **Why unresolved:** The study tests the pipeline but does not isolate whether the failure is due to the specific BLIP model used or a fundamental limitation of textual descriptions for visual harm.
- **What evidence would resolve it:** A comparative study using specialized captioning models trained on harmful visual content to see if they yield positive performance gains over the general-purpose BLIP model.

### Open Question 3
- **Question:** Do few-shot ICL approaches for harm detection generalize effectively to low-resource languages without requiring large labeled datasets?
- **Basis in paper:** [explicit] The authors explicitly state in the Limitations section that they restricted analyses to English and "it is important to extend these efforts to other languages, especially low-resource ones."
- **Why unresolved:** The current study relies on English datasets, and the efficacy of the prompt engineering and in-context learning strategies used is unknown for languages with different syntactic structures or less training data in the base LLMs.
- **What evidence would resolve it:** Zero-shot and few-shot evaluation results of the proposed models on translated or native low-resource social media datasets.

## Limitations
- Results rely on proprietary LLM APIs, raising concerns about reproducibility and cost scalability
- Dataset access uncertainty (YouTube harms dataset, Jo et al., 2024) limits exact reproduction
- Open-source multimodal models perform near chance level (~50%), limiting multimodal approach applicability

## Confidence
- **High confidence**: Zero-shot LLM performance exceeding proprietary baselines (65-70% vs. 50-56%), and BERTScore-based exemplar selection superiority
- **Medium confidence**: Specific accuracy numbers (78.57% for GPT-4o-Mini in 12-shot) depend on proprietary model versions and dataset access
- **Low confidence**: Multimodal performance claims, particularly 80% accuracy with GPT-4o-Mini, are harder to verify given dependence on specific visual encoding capabilities

## Next Checks
1. **Baseline replication**: Run zero-shot experiments with GPT-4o-Mini vs. Perspective API and OpenAI Moderation API on held-out validation subset
2. **Selector ablation study**: Compare BERTScore, Cosine, and BM25 selectors across k=8, 12, and 14 exemplars
3. **Caption quality assessment**: Evaluate BLIP caption quality on visually distinctive harmful thumbnails before full multimodal runs