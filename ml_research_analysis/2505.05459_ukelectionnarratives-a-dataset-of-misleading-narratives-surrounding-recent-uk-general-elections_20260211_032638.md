---
ver: rpa2
title: 'UKElectionNarratives: A Dataset of Misleading Narratives Surrounding Recent
  UK General Elections'
arxiv_id: '2505.05459'
source_url: https://arxiv.org/abs/2505.05459
tags:
- narratives
- political
- tweets
- data
- narrative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces UKElectionNarratives, the first dataset of
  human-annotated misleading narratives surrounding UK General Elections (2019 and
  2024), built using a novel multi-level taxonomy of 10 super-narratives and 32 narratives
  derived from European election disinformation studies. The dataset was constructed
  through a filtering pipeline using high-retweet tweets and LLM pre-annotation, followed
  by a three-stage human annotation process.
---

# UKElectionNarratives: A Dataset of Misleading Narratives Surrounding Recent UK General Elections

## Quick Facts
- **arXiv ID**: 2505.05459
- **Source URL**: https://arxiv.org/abs/2505.05459
- **Reference count**: 40
- **Primary result**: First human-annotated dataset of misleading narratives in UK General Elections (2019, 2024) with 2,000 tweets across 32 narrative classes

## Executive Summary
This work introduces UKElectionNarratives, the first dataset of human-annotated misleading narratives surrounding UK General Elections (2019 and 2024), built using a novel multi-level taxonomy of 10 super-narratives and 32 narratives derived from European election disinformation studies. The dataset was constructed through a filtering pipeline using high-retweet tweets and LLM pre-annotation, followed by a three-stage human annotation process. Topic modeling with BERTopic and GPT-4o confirmed alignment between detected themes and annotated narratives, revealing persistent topics such as EU relations, immigration, gender, antisemitism, and Islamophobia across both elections. Benchmarking with RoBERTa and GPT-4o showed that GPT-4o significantly outperformed RoBERTa models in narrative detection, with zero-shot performance improved further by including narrative descriptions. The dataset and methodology support future research on election misinformation detection and analysis.

## Method Summary
The UKElectionNarratives dataset was constructed using a three-stage pipeline: (1) Data collection from Twitter API using election-related keywords and hashtags for both 2019 and 2024 UK General Elections; (2) LLM-based filtering where three open-source models (Gemma-2, Llama-3.1, Mistral-7B) pre-annotated high-retweet tweets, with only consensus-agreement tweets (all three LLMs agreeing) proceeding to human annotation; (3) Three-stage human annotation (initial dual annotation, disagreement resolution with third annotator, consolidation by expert) using Teamware tool with confidence scores. The taxonomy includes 32 fine-grained narratives organized under 10 super-narratives. BERTopic topic modeling with GPT-4o labeling validated narrative-topic alignment across elections. RoBERTa models were fine-tuned on the annotated data, and GPT-4o was evaluated using zero-shot and few-shot approaches with and without narrative descriptions.

## Key Results
- GPT-4o significantly outperformed RoBERTa models in narrative detection, achieving higher Macro-F1 scores across both zero-shot and few-shot approaches
- Including narrative descriptions in prompts improved GPT-4o's zero-shot performance, with Macro-F1 increasing from 0.444 to 0.479 on the test set
- Topic modeling revealed consistent narrative themes across both elections, including EU relations, immigration, gender issues, antisemitism, and Islamophobia
- The multi-stage annotation process achieved fair inter-annotator agreement (kappa=0.34) in initial stages, highlighting the need for consolidation

## Why This Works (Mechanism)

### Mechanism 1: LLM-Assisted Filtering with Consensus Agreement
Using multiple LLMs to pre-annotate and filter high-retweet tweets improves annotation efficiency by identifying tweets with clear narrative signals. Three open-source LLMs (Gemma-2, Llama-3.1, Mistral-7B) independently classify tweets into narrative categories. Only tweets where all three models agree on a label are selected for human annotation, reducing the pool from 40,000 to ~5,976 tweets. Core assumption: High inter-LLM agreement correlates with human-annotatable narrative content; the LLMs' shared biases do not systematically exclude valid narratives. Evidence: Cohen's kappa between the LLMs label and the humans label is 0.70, which indicates substantial agreement.

### Mechanism 2: Multi-Stage Human Annotation with Consolidation
A three-stage annotation process (initial annotation, disagreement resolution, consolidation) improves label reliability for fine-grained, subjective narrative categories. Stage 1 involves dual independent annotation with qualification tests. Disagreements go to Stage 3 (after Stage 2's third annotator), where a consolidator makes a final decision. Confidence scores are collected to surface uncertainty. Core assumption: The consolidator's judgment is more reliable than majority voting; inter-annotator difficulty reflects task subjectivity rather than unclear guidelines. Evidence: The overall Cohen's kappa in Stage 1 is 0.34 which indicates fair agreement... This score highlights the importance of the multi-stage annotation process.

### Mechanism 3: Zero-Shot Classification with Narrative Descriptions
Providing narrative descriptions in the prompt improves GPT-4o's zero-shot and few-shot performance on narrative detection. Prompts include both narrative labels and their detailed definitions from the codebook. This contextual information helps the model distinguish between similar categories. Core assumption: GPT-4o's pre-trained knowledge can leverage explicit definitions to map text to nuanced categories; the descriptions are comprehensive and unambiguous. Evidence: Table 4 shows Macro-F1 improves from 0.444 to 0.479 on the test set when descriptions are added to zero-shot.

## Foundational Learning

- **Concept: Misleading Narratives vs. Disinformation** - Why needed here: The paper adopts the EDMO definition ("consistent set of contents that can be demonstrated as false"), distinct from broader disinformation. Understanding this boundary is critical for annotating the "None" class and for model evaluation. Quick check question: Can a misleading narrative be fact-checked as false, or does it rely primarily on framing/emotion?

- **Concept: Super-Narratives vs. Fine-Grained Narratives** - Why needed here: The taxonomy uses a hierarchical structure (10 super-narratives, 32 narratives). Model benchmarking could target either level, affecting performance metrics and task difficulty. Quick check question: Should the model predict at the super-narrative level first, then refine to specific narratives, or treat all 32 classes as flat?

- **Concept: Topic-Narrative Alignment via BERTopic** - Why needed here: The paper uses BERTopic + GPT-4o to validate that annotated narratives align with detected topics. This provides an external consistency check on the annotation quality. Quick check question: Does high topic-narrative alignment indicate good annotation, or could it reflect circularity if the same data is used for both?

## Architecture Onboarding

- **Component map**: Data Collection (keywords/hashtags) -> Filtering (high-retweet selection → LLM consensus pre-annotation) -> Annotation (Teamware tool → 3-stage process) -> Validation (BERTopic + GPT-4o labeling) -> Benchmarking (RoBERTa fine-tuning vs. GPT-4o zero-shot/few-shot)

- **Critical path**: Data Collection → Filtering (breaks if LLMs exclude valid narratives) → Annotation (breaks if annotator agreement is too low) → Benchmarking (depends on dataset quality and splits)

- **Design tradeoffs**: 
  1. Filtering aggressiveness: Stricter consensus reduces annotation cost but risks missing edge-case narratives
  2. Annotation granularity: 32 fine-grained narratives enable detailed analysis but lower inter-annotator agreement; super-narratives are more reliable but less informative
  3. Model choice: GPT-4o outperforms RoBERTa but is costlier and less interpretable; RoBERTa is faster for real-time deployment

- **Failure signatures**: 
  1. High "None" rate (39% of dataset) suggests many filtered tweets don't fit the taxonomy, potentially reflecting real-world noise or taxonomy gaps
  2. Fair agreement (kappa=0.34) in Stage 1 indicates subjective difficulty; if not resolved by later stages, labels may be unreliable
  3. Three narrative classes missing from training data prevent PLMs from learning them, biasing evaluation toward majority classes

- **First 3 experiments**: 
  1. Replicate filtering pipeline on a held-out sample to verify LLM consensus correlates with human annotatability
  2. Train RoBERTa on super-narratives (10 classes) to establish a more robust baseline before fine-grained classification
  3. Conduct error analysis on GPT-4o's misclassifications, focusing on tweets with multiple narrative labels or high annotator uncertainty

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Which prompt engineering techniques beyond zero-shot and few-shot would most improve GPT-4o's narrative detection performance?
- **Basis in paper**: Authors state "we believe more investigation exploring other prompt engineering techniques may further improve the results"
- **Why unresolved**: Only basic zero-shot and few-shot prompts with narrative descriptions were tested; chain-of-thought, self-consistency, and other advanced techniques were not explored
- **What evidence would resolve it**: Systematic benchmarking of advanced prompt engineering techniques on the test set showing statistically significant Macro-F1 improvements

### Open Question 2
- **Question**: What data augmentation approaches (human-in-the-loop, automatic annotation, synthetic data) would most effectively expand UKElectionNarratives while maintaining annotation quality?
- **Basis in paper**: Authors state "we plan to augment our dataset adopting different techniques including a human-in-the-loop approach, automatic data annotation, and generating synthetic data"
- **Why unresolved**: Dataset is small (2,000 tweets) with severe class imbalance; comparative effectiveness of augmentation strategies is unknown
- **What evidence would resolve it**: Experiments comparing augmentation methods with inter-annotator agreement metrics and downstream classifier performance

### Open Question 3
- **Question**: Would class imbalance mitigation techniques (oversampling, weighted loss, focal loss) substantially improve detection of minority narrative classes?
- **Basis in paper**: Authors state "techniques to mitigate the class imbalance should be investigated"
- **Why unresolved**: 39% of tweets are labeled "None"; some narratives have only 1-4 examples, which hampered RoBERTa models
- **What evidence would resolve it**: Benchmarking experiments with imbalance mitigation techniques showing improved per-class F1 on minority narratives

### Open Question 4
- **Question**: Does LLM-based pre-filtering introduce systematic bias by excluding valid narrative-containing tweets?
- **Basis in paper**: Filtering retained only tweets where all three LLMs agreed; 0.70 Cohen's kappa with humans indicates substantial disagreement was discarded
- **Why unresolved**: Certain narrative types that LLMs consistently misclassify or disagree on may have been systematically excluded
- **What evidence would resolve it**: Comparison of narrative distributions between pre-filtered data and randomly sampled data annotated without LLM pre-filtering

## Limitations

- Three narrative classes have zero training examples, severely biasing model evaluation toward majority classes
- Fair inter-annotator agreement (kappa=0.34) in initial stages suggests the fine-grained narrative taxonomy may be too subjective for reliable distinction
- The LLM consensus filtering mechanism may systematically exclude emerging narratives not well-represented in pre-training data

## Confidence

- **High confidence**: Dataset construction methodology, BERTopic topic modeling validation, and the general trend that GPT-4o outperforms RoBERTa on narrative detection
- **Medium confidence**: Specific performance metrics (Macro-F1 values) due to unknown random seeds and exact hyperparameter choices
- **Low confidence**: The reliability of fine-grained narrative classification given the low inter-annotator agreement, and whether the LLM filtering systematically excludes valid narratives

## Next Checks

1. **Replicate the LLM consensus filtering pipeline** on a held-out sample of high-retweet tweets to verify that inter-LLM agreement correlates with human annotatability and does not systematically exclude novel narratives
2. **Conduct an ablation study on the annotation stages** by comparing Stage 1 agreement with final consolidated labels to quantify how much the multi-stage process improves reliability for fine-grained narratives
3. **Test model performance on super-narratives (10 classes) versus fine-grained narratives (32 classes)** to establish whether the hierarchical structure improves classification reliability while maintaining analytical value