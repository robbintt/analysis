---
ver: rpa2
title: 'OpenCSG Chinese Corpus: A Series of High-quality Chinese Datasets for LLM
  Training'
arxiv_id: '2501.08197'
source_url: https://arxiv.org/abs/2501.08197
tags:
- uni00000048
- uni00000013
- data
- uni0000004c
- uni00000052
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the OpenCSG Chinese Corpus, a series of high-quality
  datasets designed to address the scarcity of large-scale, diverse Chinese data for
  training large language models (LLMs). The corpus includes Fineweb-edu-chinese,
  Fineweb-edu-chinese-v2, Cosmopedia-chinese, and Smoltalk-chinese, each targeting
  different training needs such as pretraining, knowledge-intensive tasks, and instruction
  fine-tuning.
---

# OpenCSG Chinese Corpus: A Series of High-quality Chinese Datasets for LLM Training

## Quick Facts
- arXiv ID: 2501.08197
- Source URL: https://arxiv.org/abs/2501.08197
- Reference count: 28
- A series of high-quality Chinese datasets (Fineweb-edu-chinese, Cosmopedia-chinese, Smoltalk-chinese) for LLM training and alignment

## Executive Summary
This paper introduces the OpenCSG Chinese Corpus, addressing the scarcity of large-scale, diverse Chinese data for training large language models. The corpus comprises three datasets: Fineweb-edu-chinese for pretraining, Cosmopedia-chinese for knowledge generation, and Smoltalk-chinese for instruction fine-tuning. Each dataset uses scalable, reproducible curation processes involving LLM-guided filtering, synthetic generation, and deduplication. Experimental evaluations on smaller Chinese LLMs show significant performance improvements, with models trained on Fineweb-edu-chinese outperforming baselines by large margins on benchmarks like C-Eval and CMMLU.

## Method Summary
The OpenCSG Chinese Corpus was created through a multi-stage pipeline. For Fineweb-edu-chinese, raw Chinese web data was filtered using an LLM-trained "educational value" scorer (Qwen2-7b-instruct) to train a BERT-based filter, followed by MinHash deduplication. Cosmopedia-chinese was generated synthetically from curated seed data (BaiduBaike, Zhihu) using a long-context LLM (glm4-9b-longwriter) to create textbook-style content. Smoltalk-chinese was created through system-prompt-driven instruction tuning, where advanced LLMs generated multi-turn conversations across diverse task categories, with automated quality scoring and embedding-based deduplication. Models were trained on a 2B Llama architecture with specific hyperparameters for pretraining (50k steps, LR 1e-3) and SFT (2 epochs, LR 3e-4).

## Key Results
- Models trained on Fineweb-edu-chinese significantly outperformed baselines on C-Eval and CMMLU benchmarks
- Smoltalk-chinese showed strong gains in alignment tasks on the Alignbench benchmark
- Cosmopedia-chinese produced well-structured, knowledge-rich outputs despite limited benchmark gains
- The corpus provides an openly accessible resource for advancing Chinese NLP research

## Why This Works (Mechanism)

### Mechanism 1: LLM-Guided Quality Filtering
- Claim: Filtering raw web data using an LLM-trained "educational value" scorer improves pretraining efficiency.
- Mechanism: A powerful LLM (Qwen2-7b-instruct) scores a sample of raw text on educational value. These scores train a smaller, faster BERT-based filter. Applying this filter to a large raw corpus removes low-quality content (e.g., ads, incoherent text), concentrating the pretraining signal on high-value samples.
- Core assumption: An LLM's notion of "educational value" aligns with features that improve downstream task performance.
- Evidence anchors:
  - [abstract] "Fineweb-edu datasets focus on filtered, high-quality content... derived from diverse Chinese web sources..."
  - [section 3.1] "...used Qwen2-7b-instruct... to evaluate the educational value... This scoring data was then used to fine-tune a bge-rerank-zh model..."
  - [section 4.1] "As illustrated in Figure 5, the Fineweb-Edu-Chinese model experienced a sharp accuracy increase... surpassing the baseline by a significant margin."
  - [corpus] Evidence is weak. No corpus papers explicitly validate this specific filtering technique.
- Break condition: The teacher LLM may transfer its biases to the student filter, inadvertently removing valuable but stylistically different text.

### Mechanism 2: Synthetic Textbook Generation from Curated Seeds
- Claim: Synthetic data generated from high-quality seeds produces well-structured, knowledge-rich text but may lack diversity for general benchmark gains.
- Mechanism: Seed data is sourced from knowledge-dense repositories (BaiduBaike, Zhihu). A long-context LLM (glm4-9b-longwriter) is prompted to generate detailed textbook-style content from these seeds, creating a synthetic corpus with controlled style and depth.
- Core assumption: The utility of synthetic pretraining data is more dependent on seed quality and content depth than on the raw volume of diverse web text.
- Evidence anchors:
  - [abstract] "Cosmopedia-chinese provides synthetic, textbook-style data..."
  - [section 3.3] "...collected seed samples from high-quality sources... we resort to glm4-9b-longwriter... which can generate long answers with sufficiently detailed content."
  - [section 4.2] "...benchmark results indicated only a little accuracy gains... human evaluators noted that the Cosmopedia-Chinese model produced consistently well-structured and knowledge-rich responses..."
  - [corpus] Evidence is weak. No corpus papers address this synthetic data approach.
- Break condition: Over-reliance on limited genres (textbook, story, wikihow) creates homogeneous data, failing to improve generalization on diverse benchmarks like C-Eval.

### Mechanism 3: System-Prompt-Driven Instruction Tuning
- Claim: Instruction fine-tuning with diverse, synthetically generated multi-turn conversations improves model alignment on complex tasks.
- Mechanism: Advanced LLMs are guided by detailed system prompts to generate conversations across a wide taxonomy of tasks (e.g., reasoning, coding, advice). Automated quality scoring and embedding-based deduplication are applied to create a diverse, high-quality SFT dataset.
- Core assumption: A broad, predefined taxonomy of task types in the instruction data is essential for teaching a model to follow complex, multi-step instructions.
- Evidence anchors:
  - [abstract] "Smoltalk-chinese also showed strong gains in alignment tasks..."
  - [section 3.4] "This expansion ensures broader coverage of tasks relevant to natural language understanding... Using advanced LLMs... we generate 3-turn conversations for the 11 task categories..."
  - [section 4.3] "As shown in Figure 8, the Smoltalk-Chinese dataset yielded the strongest overall performance gains."
  - [corpus] Evidence is weak. Corpus mentions related alignment work but not this specific method.
- Break condition: If the generating LLMs have a shared stylistic bias, the fine-tuned model may learn this artifact, becoming verbose or unnatural.

## Foundational Learning
- Concept: LLM-as-a-Judge / Scorer
  - Why needed here: This is the core engine for quality control in the Fineweb-edu pipeline. Understanding how a large model can train a smaller filter is essential.
  - Quick check question: Can you explain the two-step process (labeling then training) used to create the filter for Fineweb-edu-chinese?

- Concept: Synthetic Data Generation (SDG)
  - Why needed here: A major part of the corpus (Cosmopedia-chinese) is synthetic. Understanding its strengths (structure) and weaknesses (homogeneity) is critical for its use.
  - Quick check question: What are the two primary reasons cited in the paper for Cosmopedia-chinese failing to show significant gains on general benchmarks?

- Concept: Instruction Fine-Tuning (SFT) vs. Pretraining
  - Why needed here: The Smoltalk-chinese dataset is designed for SFT, a distinct stage from pretraining. Grasping this difference is key to the corpus architecture.
  - Quick check question: How does the evaluation of a model fine-tuned on Smoltalk-chinese (using Alignbench) differ from evaluating a model pretrained on Fineweb-edu-chinese?

## Architecture Onboarding
- Component map:
  1.  **Fineweb-edu Pipeline:** Raw Corpora → Qwen2 Scorer → BERT Filter Training → MinHash Deduplication
  2.  **Cosmopedia Pipeline:** Curated Seeds → GLM4-Longwriter → MinHash Deduplication
  3.  **Smoltalk Pipeline:** System Prompts → Teacher LLM Generation → Qwen2 Quality Scorer → Embedding Deduplication

- Critical path: For a new engineer, the highest-impact starting point is the **Fineweb-edu pipeline**. Validating the quality filter on a sample of raw data is the most direct way to influence pretraining quality.

- Design tradeoffs:
  - **Filter Threshold:** A higher educational score threshold yields cleaner data but reduces corpus size. The tradeoff is between data volume and average sample quality.
  - **Synthetic Blend:** Using Cosmopedia-chinese adds structure but risks homogeneity. The tradeoff is between controlled content and natural diversity.
  - **Generation Temperature:** For Smoltalk, a higher temperature increases diversity but may reduce coherence. The tradeoff is between task coverage and sample quality.

- Failure signatures:
  - **Benchmark Stagnation:** If pretraining loss drops but benchmark accuracy (C-Eval) doesn't improve, the filter may be too aggressive or the data source too narrow.
  - **Homogeneous Outputs:** If a model trained on Cosmopedia produces repetitive, textbook-style answers for creative prompts, it has overfit to the synthetic style.
  - **Instruction Following Failure:** If a model fine-tuned on Smoltalk fails to follow specific formatting constraints, the SFT data may lack sufficient examples of that task type.

- First 3 experiments:
  1.  **Filter Ablation:** Train two small models: one on raw data, one on Fineweb-edu filtered data. Compare their loss curves and scores on a small benchmark set.
  2.  **Synthetic Blend Test:** Pretrain a model on a mixed dataset (e.g., 80% Fineweb-edu, 20% Cosmopedia) and compare its factual accuracy vs. a baseline on a closed-book QA task.
  3.  **SFT Task Coverage:** Fine-tune a pretrained model on a subset of Smoltalk containing only reasoning tasks. Evaluate if its performance on reasoning benchmarks improves disproportionately.

## Open Questions the Paper Calls Out
- Question: Does blending real-world data and removing markdown formatting improve the generalization capability of models pretrained on Cosmopedia-Chinese?
  - Basis in paper: [explicit] The Conclusion states the dataset "could benefit from real-world data blending to mitigate over-homogeneity and from further reductions in markdown tags" to allow the model to focus on semantic richness.
  - Why unresolved: The authors identified that Cosmopedia-Chinese produced coherent text but yielded only marginal benchmark gains, hypothesizing that markdown overuse and homogeneity were the causes, but they did not test the proposed corrections.
  - What evidence would resolve it: An ablation study showing benchmark performance (e.g., C-Eval scores) of models trained on a modified version of Cosmopedia-Chinese stripped of markdown and mixed with non-synthetic data.

- Question: Which specific evaluation metrics beyond C-Eval and CMMLU are required to accurately capture the "factual correctness, reasoning depth, and safety alignment" of models trained on these corpora?
  - Basis in paper: [explicit] The Conclusion explicitly suggests that "Further research should also explore alternative or complementary evaluation metrics... including factual correctness, reasoning depth, and safety alignment."
  - Why unresolved: The current evaluation suite showed Cosmopedia-Chinese models performed poorly on benchmarks despite human evaluators finding the outputs "knowledge-rich," indicating a mismatch between current metrics and actual utility.
  - What evidence would resolve it: A correlation analysis between human evaluations of model outputs and scores from new, specialized benchmarks designed for the traits listed.

- Question: Is the lack of benchmark improvement in Cosmopedia-Chinese primarily caused by rhetorical homogeneity or the distraction of markdown tokens?
  - Basis in paper: [inferred] Section 4.2 attributes the lack of improvement to "two factors... homogeneity of synthesized data" and "overuse of markdown formatting," but the study does not isolate these variables to determine which is the dominant negative factor.
  - Why unresolved: The paper presents these as simultaneous hypotheses without experimental disentanglement.
  - What evidence would resolve it: A controlled experiment comparing a model trained on markdown-free synthetic data against a model trained on diverse-genre synthetic data containing markdown.

## Limitations
- The evaluation framework relies on relatively small (2B parameter) models rather than state-of-the-art LLMs, limiting generalizability to larger architectures
- The synthetic generation pipeline (Cosmopedia) lacks validation of factual accuracy and potential hallucination rates
- The quality filtering mechanism depends heavily on the alignment between the scoring LLM's notion of "educational value" and downstream task performance, which remains an untested assumption

## Confidence
- **High Confidence:** The filtering methodology (LLM-judge to BERT-filter) is technically sound and reproducible. The deduplication approach (MinHash for pretraining, embedding-based for SFT) follows established practices. The observation that Fineweb-edu improves benchmark performance is well-supported by experimental results.
- **Medium Confidence:** The synthetic data generation approach for Cosmopedia shows promise but lacks rigorous validation of factual consistency. The instruction fine-tuning pipeline for Smoltalk is well-designed but may overfit to the generating LLM's stylistic biases.
- **Low Confidence:** Claims about the corpus's impact on larger LLMs remain speculative. The long-term stability and scalability of the quality filtering approach across diverse domains is unproven.

## Next Checks
1. **Filter Generalization Test:** Evaluate the Fineweb-edu filter on an external Chinese corpus (e.g., a subset of CommonCrawl) and measure how well the quality scores correlate with downstream task performance on that corpus.
2. **Synthetic Factuality Audit:** Implement a fact-checking pipeline to measure the hallucination rate in Cosmopedia-generated content, comparing it against human-written reference materials on the same topics.
3. **Large Model Scaling Study:** Train a 7B parameter model (using the same architecture) on the Fineweb-edu corpus and evaluate whether the performance gains scale proportionally compared to the 2B model results.