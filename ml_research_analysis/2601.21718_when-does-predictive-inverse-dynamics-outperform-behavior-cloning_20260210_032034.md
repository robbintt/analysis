---
ver: rpa2
title: When does predictive inverse dynamics outperform behavior cloning?
arxiv_id: '2601.21718'
source_url: https://arxiv.org/abs/2601.21718
tags:
- state
- pidm
- demonstrations
- predictor
- future
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates why predictive inverse dynamics models
  (PIDM) outperform behavior cloning (BC) in offline imitation learning, especially
  with limited expert demonstrations. PIDM decomposes decision-making into a state
  predictor and inverse dynamics model (IDM), conditioning IDM on predicted future
  states.
---

# When does predictive inverse dynamics outperform behavior cloning?
## Quick Facts
- arXiv ID: 2601.21718
- Source URL: https://arxiv.org/abs/2601.21718
- Reference count: 40
- Primary result: PIDM requires 3-5× fewer expert demonstrations than BC in 2D navigation tasks

## Executive Summary
This paper investigates when Predictive Inverse Dynamics Models (PIDM) outperform Behavior Cloning (BC) in offline imitation learning. PIDM decomposes decision-making into a state predictor and inverse dynamics model, conditioning the IDM on predicted future states. The authors theoretically analyze a bias-variance tradeoff where future conditioning reduces variance (improving sample efficiency) while imperfect state prediction introduces bias that diminishes gains. Empirical validation shows PIDM requires 3-5× fewer demonstrations than BC in 2D navigation tasks, with even greater advantages (66% more samples) in complex 3D video game environments.

## Method Summary
The paper proposes Predictive Inverse Dynamics Models (PIDM) as an alternative to Behavior Cloning for offline imitation learning. PIDM works by decomposing the decision-making problem into two components: a state predictor that forecasts future states, and an inverse dynamics model (IDM) that predicts actions conditioned on both current states and predicted future states. This decomposition creates a bias-variance tradeoff - conditioning on predicted future states reduces variance and improves sample efficiency, but imperfect state prediction introduces bias that can limit performance gains. The authors derive theoretical conditions for when PIDM is more sample-efficient than BC and validate these findings through experiments in both 2D navigation tasks and a complex 3D video game with high-dimensional visual inputs.

## Key Results
- In 2D navigation tasks, PIDM requires 3× fewer demonstrations on average (up to 5× in some cases) compared to BC
- In complex 3D video game environments with high-dimensional visual inputs, BC needs over 66% more samples than PIDM
- The theoretical bias-variance tradeoff analysis explains PIDM's advantages under limited expert data conditions

## Why This Works (Mechanism)
PIDM outperforms BC by leveraging predicted future states to condition action predictions, creating a bias-variance tradeoff that improves sample efficiency. When future state predictions are accurate, conditioning on them reduces the variance of the inverse dynamics model, requiring fewer expert demonstrations to achieve comparable performance. However, when predictions are imperfect, the introduced bias can limit or eliminate these gains. The key insight is that PIDM is most advantageous when state prediction is reasonably accurate but not perfect, creating a sweet spot where reduced variance outweighs the bias penalty.

## Foundational Learning
**Bias-Variance Tradeoff**: The fundamental tension between model simplicity (high bias, low variance) and complexity (low bias, high variance). Understanding this tradeoff is crucial for explaining PIDM's performance advantages under limited data conditions.

**Sample Efficiency**: The ability to achieve good performance with limited training data. PIDM's conditioning on predicted future states directly addresses this by reducing the variance of action predictions, requiring fewer expert demonstrations.

**Inverse Dynamics Modeling**: Learning to predict actions from state transitions, which is the core mechanism behind PIDM's decomposition approach to imitation learning.

## Architecture Onboarding
**Component Map**: State Predictor -> Inverse Dynamics Model (IDM) -> Action Prediction

**Critical Path**: The state predictor generates future state estimates, which are fed into the IDM along with current states to produce action predictions. The quality of state predictions directly impacts IDM performance through the bias-variance tradeoff.

**Design Tradeoffs**: The architecture trades off between state prediction accuracy and action prediction quality. Better state prediction reduces IDM variance but requires more computational resources and introduces potential error propagation.

**Failure Signatures**: PIDM performance degrades when state predictions become highly inaccurate, as the introduced bias overwhelms the variance reduction benefits. This manifests as action predictions that diverge from expert behavior despite having access to predicted future states.

**First Experiments**:
1. Compare PIDM vs BC sample efficiency curves on 2D navigation tasks with varying amounts of expert data
2. Analyze the impact of state prediction error magnitude on PIDM performance
3. Test PIDM performance under different levels of environmental stochasticity

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Theoretical conditions assume deterministic dynamics and perfect state prediction, which rarely hold in real-world scenarios
- Empirical validation limited to relatively simple 2D navigation tasks and one complex 3D game environment
- Does not extensively explore PIDM performance across diverse domains or with varying levels of environmental stochasticity

## Confidence
- **High confidence** in the core observation that PIDM generally requires fewer demonstrations than BC, supported by both theory and experiments
- **Medium confidence** in the theoretical conditions for when PIDM outperforms BC, given simplifying assumptions
- **Medium confidence** in the 3-5× improvement factor, as this is based on limited experimental domains

## Next Checks
1. Test PIDM against BC across a broader range of benchmark control environments (MuJoCo, Atari, etc.) to verify generalizability
2. Systematically vary levels of state prediction error and environmental stochasticity to map the boundary conditions for PIDM's advantage
3. Compare PIDM with other state-of-the-art offline IL methods (e.g., CQL, BCQ) to establish relative performance across sample efficiency regimes