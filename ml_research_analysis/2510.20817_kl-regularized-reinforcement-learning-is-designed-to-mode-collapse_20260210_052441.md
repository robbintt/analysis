---
ver: rpa2
title: KL-Regularized Reinforcement Learning is Designed to Mode Collapse
arxiv_id: '2510.20817'
source_url: https://arxiv.org/abs/2510.20817
tags:
- reward
- arxiv
- distribution
- solution
- diversity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes mode collapse in KL-regularized reinforcement
  learning (RL) for large language models (LLMs). The authors show that commonly used
  RL objectives often have unimodal optimal solutions by construction, explaining
  observed diversity collapse during post-training.
---

# KL-Regularized Reinforcement Learning is Designed to Mode Collapse

## Quick Facts
- arXiv ID: 2510.20817
- Source URL: https://arxiv.org/abs/2510.20817
- Reference count: 40
- Key outcome: Shows commonly used RL objectives have unimodal optimal solutions, explaining diversity collapse in LLM post-training; proposes Mode Anchored Reward Augmentation (MARA) to improve both quality and diversity

## Executive Summary
This paper reveals that KL-regularized reinforcement learning objectives for large language models are fundamentally designed to produce mode collapse. The authors prove that standard RL with equal rewards preserves reference policy probability ratios, never promoting lower-support correct answers. They show that mode coverage depends primarily on regularization strength and reward scales, not on the choice of forward vs. reverse KL as commonly believed. To address this, they introduce MARA, a simple reward augmentation method that constructs a target distribution with uniform high mass over all high-quality modes by minimally modifying rewards. MARA improves both quality and diversity on creative QA and chemical language model tasks without requiring external diversity signals.

## Method Summary
The paper analyzes KL-regularized RL objectives, showing they implicitly define a target distribution Gβ(y) ∝ πref(y)exp(R(y)/β). Under equal rewards, this target preserves reference policy ratios exactly, preventing low-support modes from gaining probability. MARA addresses this by augmenting rewards with a term that equalizes probabilities for all high-reward samples above a threshold τ, using an anchor sample z with high πref support. The method requires only two lines of pseudocode and works with both KL types.

## Key Results
- KL-regularized RL objectives have unimodal optimal solutions by construction, not by optimization failure
- Mode coverage depends primarily on regularization strength β and relative reward scales, not KL direction choice
- MARA improves both quality and diversity on creative QA and chemical language model tasks
- Standard RL with equal rewards never promotes lower-support correct answers to global optimum

## Why This Works (Mechanism)

### Mechanism 1
- Claim: KL-regularized RL implicitly defines a target distribution whose shape determines mode coverage, not the choice of forward vs. reverse KL itself
- Mechanism: The optimization objective Jβ(πθ) = Eπθ[R(y)] - β·DKL(πθ||πref) has closed-form solution Gβ(y) ∝ πref(y)exp(R(y)/β). Gradient descent on this objective is mathematically equivalent to minimizing DKL(πθ||Gβ) toward this target distribution
- Core assumption: Policy family is sufficiently expressive to approximate Gβ; optimization converges near-globally
- Evidence anchors: [abstract] and [Section 3.1, Remark 3.2] show theoretical equivalence; APO paper notes similar divergence-family effects

### Mechanism 2
- Claim: Under equal rewards for correct answers, the optimal solution preserves πref's probability ratios exactly, never promoting low-support modes
- Mechanism: For any two samples with R(y1)=R(y2), Proposition 4.1 gives Gβ(y1)/Gβ(y2) = πref(y1)/πref(y2), independent of β. Low-support correct answers stay low-probability in the global optimum
- Core assumption: Rewards are verifiable and discrete (e.g., math correctness = 1, incorrect = 0)
- Evidence anchors: [Section 4.2, Remark 4.3] proves the preservation property; Figure 4 empirically confirms; "Expected Return Causes Outcome-Level Mode Collapse" independently identifies outcome-level collapse

### Mechanism 3
- Claim: MARA constructs a multimodal target by reward augmentation that equalizes probabilities for all high-reward samples
- Mechanism: For samples with R(y) ≥ τ, augmented reward R̄(y) = R(z) + β(log πref(z) - log πref(y)) cancels the πref ratio term in Gβ's closed form, yielding uniform probability Gβ(y) ∝ πref(z)exp(R(z)/β) across all above-threshold samples
- Core assumption: Threshold τ can be set meaningfully; anchor sample z with high πref exists among high-reward samples
- Evidence anchors: [Section 5, Algorithm 1] provides complete pseudocode; Figure 5 shows uniform high mass production; ADPO paper uses anchoring concept but for preference optimization

## Foundational Learning

- Concept: Variational inference and KL divergence direction
  - Why needed here: The paper reframes KL-regularized RL as distribution matching; understanding DKL(q||p) vs. DKL(p||q) behaviors is essential
  - Quick check question: Why does reverse KL avoid regions where p≈0 while forward KL covers them?

- Concept: Exponential reward scaling in probability space
  - Why needed here: Small reward differences (ΔR=0.1) with typical β=0.001 produce probability ratios of ~10^43 (Figure 3)
  - Quick check question: What β value makes two samples with ΔR=1.0 have equal probability under πref?

- Concept: Policy gradient as implicit distribution matching
  - Why needed here: Standard RL intuition (maximize reward) obscures that gradient = KL gradient toward implicit target Gβ
  - Quick check question: What target distribution does your current RL objective implicitly define?

## Architecture Onboarding

- Component map: Reward function R(y) → Reference policy πref → KL coefficient β → Threshold τ → Anchor sample z

- Critical path:
  1. Identify diversity requirement (does task have multiple valid solutions?)
  2. Diagnose current target: compute Gβ ratios for known correct answers
  3. If ratios are highly imbalanced, apply MARA augmentation
  4. Monitor diversity metrics alongside reward during training

- Design tradeoffs:
  - Lower β → sharper reward exploitation, faster collapse
  - Higher β → better mode coverage but lower peak reward
  - Lower τ → more samples equalized, potential quality dilution
  - Higher τ → preserves quality, may miss secondary modes

- Failure signatures:
  - High reward but entropy → 0: objective has unimodal target by construction
  - Low-support correct answers never improve: equal-reward scenario (Remark 4.3)
  - MARA doesn't help: check if πref(z) >> πref(other correct answers)

- First 3 experiments:
  1. Replicate Figure 3: vary β on synthetic task with known reward landscape; observe probability ratios
  2. Diagnose your task: compute Gβ ratios for 10-20 known correct answers; if ratios >100:1, target is effectively unimodal
  3. Apply MARA with τ at 90th percentile of sampled rewards; compare entropy and reward curves against vanilla

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis assumes exact global convergence to optimal distribution Gβ, but practical RL uses gradient-based optimization with finite samples
- MARA effectiveness depends on finding anchor samples with high πref support among high-reward samples, which may not exist in all domains
- Analysis focuses on discrete reward scenarios, while many real-world LLM tasks involve continuous or structured rewards

## Confidence

**High Confidence** - The closed-form solution Gβ(y) ∝ πref(y)exp(R(y)/β) and its implications for mode coverage are mathematically rigorous and independent of optimization details. The proof that equal-reward samples preserve πref ratios in the optimum follows directly from the exponential form.

**Medium Confidence** - Empirical validation on creative QA and chemical language models demonstrates MARA's effectiveness, but sample sizes and evaluation protocols would benefit from additional validation. The claim that β and reward scale, not KL direction, determine mode coverage is theoretically sound but may have practical nuances.

**Low Confidence** - Generalization to all possible reward structures and reference policies is not fully explored. Behavior in continuous action spaces and with non-stationary reward functions remains unclear.

## Next Checks

1. **Cross-task diversity validation**: Apply MARA to at least 3 additional diverse LLM tasks (e.g., code generation with multiple valid solutions, story continuation with different plot directions) and measure both quality and diversity metrics across all tasks.

2. **Anchor sample sensitivity analysis**: Systematically vary the anchor selection criteria (different z candidates, multiple anchors, anchor-free variants) and measure the resulting diversity and quality trade-offs.

3. **Convergence behavior study**: Track the KL divergence to Gβ and reward curves throughout training for both vanilla RL and MARA, measuring how quickly each approach reaches its final mode coverage.