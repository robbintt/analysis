---
ver: rpa2
title: Constructing a Question-Answering Simulator through the Distillation of LLMs
arxiv_id: '2509.09226'
source_url: https://arxiv.org/abs/2509.09226
tags:
- student
- knowledge
- concept
- concepts
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes LDSim, a method that distills the domain knowledge
  and reasoning capabilities of large language models (LLMs) into a lightweight network
  to construct a question-answering (QA) simulator. The QA simulator predicts student
  responses to questions and interacts with educational recommender systems (ERS)
  to collect training data without real student interaction, thereby preventing harmful
  recommendations.
---

# Constructing a Question-Answering Simulator through the Distillation of LLMs

## Quick Facts
- arXiv ID: 2509.09226
- Source URL: https://arxiv.org/abs/2509.09226
- Authors: Haipeng Liu; Ting Long; Jing Fu
- Reference count: 27
- Key outcome: LDSim achieves 2-20% improvement over state-of-the-art methods in both simulation and knowledge tracing tasks.

## Executive Summary
This paper proposes LDSim, a method that distills the domain knowledge and reasoning capabilities of large language models (LLMs) into a lightweight network to construct a question-answering (QA) simulator. The QA simulator predicts student responses to questions and interacts with educational recommender systems (ERS) to collect training data without real student interaction, thereby preventing harmful recommendations. LDSim consists of three modules: knowledge distillation, reasoning distillation, and simulation. The knowledge distillation module constructs a concept relation graph, the reasoning distillation module infers students' mastery of concepts, and the simulation module uses this distilled information to predict correctness. Experiments on four datasets (Junyi, Assist09, Assist12, Algebra) show that LDSim achieves 2-20% improvement over state-of-the-art methods in both simulation and knowledge tracing tasks.

## Method Summary
LDSim uses three modules to construct a QA simulator. The knowledge distillation module builds a concept relation graph by querying an LLM for prerequisite relationships between concepts. The reasoning distillation module uses the LLM to infer students' mastery scores for each concept based on their interaction history. The simulation module combines these components with a Graph Attention Network (GAT) and attention mechanism over question-answer history to predict student correctness. The method uses a two-stage training approach: first optimizing mastery prediction via MSE loss, then optimizing final prediction performance.

## Key Results
- LDSim achieves 0.8179 accuracy on Junyi, 0.8260 on Assist09, 0.7887 on Assist12, and 0.8457 on Algebra
- Outperforms all baselines including LLM-based methods while maintaining computational efficiency (0.73s vs 3117s for Agent4Edu)
- 2-20% improvement over state-of-the-art methods in both simulation and knowledge tracing tasks
- Removing the world knowledge distillation module results in performance decrease
- Removing the reasoning capability distillation module causes the largest performance drop

## Why This Works (Mechanism)

### Mechanism 1: Explicit Concept Relation Graphs via LLM Extraction
The system queries an LLM to determine relevance and dependency between concept pairs, encoding this as a graph $G_c$. A Graph Attention Network (GAT) then propagates information along these edges, allowing the model to update a concept's embedding based on its neighbors. This structure improves the model's ability to reason about student errors compared to treating concepts as independent IDs.

### Mechanism 2: Dense Mastery Supervision via Reasoning Distillation
The LLM acts as a "teacher" to label each interaction step with an estimated mastery level $m_i$ based on historical context. The simulator is trained via MSE loss to replicate these soft labels before being fine-tuned for the binary prediction task. This provides a richer learning signal than binary cross-entropy on correctness alone.

### Mechanism 3: Hierarchical Semantic Encoding for Generalization
The model builds a hierarchical concept-question graph where embeddings for questions are informed by the concept relation graph $G_c$. This allows the model to reason about a new question if it understands the concepts attached to it, enabling generalization to unseen questions that share known concepts.

## Foundational Learning

- **Concept: Knowledge Tracing (KT)**
  - Why needed here: The core task is modeling a student's knowledge state over time to predict future performance
  - Quick check question: Given a sequence of [Q1(Correct), Q2(Incorrect), Q3(?)], does the model predict Q3 based solely on Q2, or the whole history?

- **Concept: Knowledge Distillation (Teacher-Student)**
  - Why needed here: The architecture relies on a large "Teacher" LLM generating labels/reasoning for a smaller "Student" network
  - Quick check question: Why would minimizing the difference between a teacher's softmax output and a student's softmax output be better than training the student on hard labels?

- **Concept: Graph Attention Networks (GAT)**
  - Why needed here: The structural knowledge of concepts is encoded via GATs
  - Quick check question: In a GAT layer, how does the model decide that "Algebra" is more relevant to "Calculus" than "Geometry" is?

## Architecture Onboarding

- **Component map:** LLM Inference Engine (Offline) -> Data Augmentation (Offline) -> Simulator Network (Online)
- **Critical path:** The Reasoning Distillation data generation is the bottleneck. You cannot train the Simulator without the pre-processed mastery labels $m_t$ and credit scores $s_t$ from the LLM.
- **Design tradeoffs:** High upfront cost (LLM API calls to label the dataset) trades for extremely low inference latency (0.73s vs 3117s for Agent4Edu). The model uses a "sampling" step ($k \sim f_m(s_i)$) for mastery levels, requiring multiple runs to average performance.
- **Failure signatures:** High GPU memory during inference indicates using the LLM instead of the distilled lightweight model. Poor performance on new concepts suggests the LLM failed to link them to the existing graph. Divergence during training suggests incorrect two-stage training separation.
- **First 3 experiments:**
  1. Sanity Check (Overfit): Train the Sim module on a single student's history. Can it perfectly predict their responses?
  2. Distillation Ablation: Train two modelsâ€”one with standard Cross-Entropy only, and one with the Mastery Distillation loss ($L_b$). Compare convergence speed and accuracy.
  3. Graph Validation: Visualize the Concept Relation Graph generated by the LLM. Manually inspect if "Prerequisites" make sense or if the LLM hallucinated edges.

## Open Questions the Paper Calls Out

### Open Question 1
How does the accuracy of the lightweight simulator degrade when the teacher LLM exhibits hallucinations or reasoning errors during mastery inference? The paper evaluates final performance against ground-truth correctness but does not analyze the fidelity of the intermediate mastery scores generated by the LLM or the simulator's robustness to noisy distillation signals.

### Open Question 2
Does the LDSim framework generalize to non-STEM educational domains where concept relationships are less structured than in mathematics? The paper limits experimental validation to mathematics-related datasets, which possess clear prerequisite structures.

### Open Question 3
Is the performance improvement dependent on the specific capabilities of the GLM-4-Flash model, or does it hold across different LLM architectures? The paper specifies that the method is implemented exclusively using GLM-4-Flash as the foundation LLM.

## Limitations
- The method relies heavily on a proprietary GLM-4-Flash API for knowledge extraction, creating potential reproducibility barriers and cost constraints
- The exact number of discrete mastery levels and GAT architecture details are unspecified, requiring arbitrary parameter choices
- The method assumes LLM-generated prerequisite graphs are pedagogically accurate, but LLMs may hallucinate relationships
- The credit score mechanism is mentioned but not utilized in the implementation, suggesting incomplete design specification

## Confidence

- **High Confidence:** The two-stage training methodology is sound and well-specified. The ablation study results showing performance drops when removing KD or RD modules are compelling.
- **Medium Confidence:** The reported 2-20% improvements over baselines are likely accurate for the tested datasets, but generalizability to other educational contexts is uncertain.
- **Low Confidence:** The claim that this approach prevents harmful recommendations in ERS is not directly validated - only simulation accuracy is measured, not actual recommendation safety.

## Next Checks

1. **Ablation Stress Test:** Systematically vary the number of discrete mastery levels (l) and GAT layers to determine sensitivity to these unspecified hyperparameters.
2. **LLM Hallucination Audit:** Manually verify a sample of prerequisite relationships generated by GLM-4-Flash to quantify hallucination rates and their impact on GAT performance.
3. **Cross-Dataset Transfer:** Train the complete LDSim pipeline on one dataset (e.g., Algebra) and evaluate zero-shot on another (e.g., Assist09) to test generalization of the distilled knowledge graph.