---
ver: rpa2
title: 'MDPO: Multi-Granularity Direct Preference Optimization for Mathematical Reasoning'
arxiv_id: '2506.15706'
source_url: https://arxiv.org/abs/2506.15706
tags:
- reasoning
- llms
- mdpo
- math
- mathematical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of improving mathematical reasoning
  in large language models (LLMs), where errors in intermediate reasoning steps can
  lead to incorrect final answers. While supervised fine-tuning struggles to suppress
  incorrect outputs and DPO shows limited effectiveness in long-chain reasoning, the
  authors propose Multi-Granularity Direct Preference Optimization (MDPO).
---

# MDPO: Multi-Granularity Direct Preference Optimization for Mathematical Reasoning

## Quick Facts
- arXiv ID: 2506.15706
- Source URL: https://arxiv.org/abs/2506.15706
- Reference count: 6
- Multi-granularity preference optimization improves mathematical reasoning accuracy by 1.7-2.3% on GSM8K and 0.9-2.3% on MATH datasets.

## Executive Summary
This paper addresses the challenge of improving mathematical reasoning in large language models, where errors in intermediate reasoning steps can lead to incorrect final answers. While supervised fine-tuning struggles to suppress incorrect outputs and standard Direct Preference Optimization (DPO) shows limited effectiveness for long-chain reasoning, the authors propose Multi-Granularity Direct Preference Optimization (MDPO). MDPO provides three levels of supervision: Solution2Solution (entire solution correctness), Inference2Inference (logical correctness between steps), and Step2Step (computational accuracy within steps). The method automatically constructs training data without manual annotation costs and demonstrates accuracy improvements on GSM8K and MATH datasets when applied to Qwen2 and Llama3 models.

## Method Summary
MDPO introduces a three-level granularity framework for preference optimization in mathematical reasoning. The approach constructs preference pairs at three levels: Solution2Solution compares entire reasoning solutions based on final answer correctness, Inference2Inference evaluates logical correctness between reasoning steps, and Step2Step focuses on computational accuracy within individual steps. The method uses SimPO-style length-normalized rewards with a unified loss function trained across all granularities. Data construction is automated using LLMs to generate multiple reasoning paths per problem, identify unreliable steps based on error rates, and correct computational errors. The training procedure uses AdamW optimizer with cosine scheduler, batch size 128, and learning rate 5e-7 across 8 epochs.

## Key Results
- Accuracy improvements of 1.7-2.3% on GSM8K dataset compared to baseline DPO methods
- Accuracy improvements of 0.9-2.3% on MATH dataset compared to baseline DPO methods
- Performance gains achieved using Qwen2-7B-Instruct and Llama3-8B-Instruct models
- Automatic data construction pipeline eliminates manual annotation costs

## Why This Works (Mechanism)
MDPO works by providing more granular and targeted supervision during preference optimization. Standard DPO only compares complete solutions, but mathematical reasoning requires multiple correct intermediate steps. By breaking down the optimization into three levels - solution-level, inference-level, and step-level - MDPO can identify and correct errors at the appropriate granularity. The Solution2Solution level ensures final answer correctness, Inference2Inference improves logical flow between steps, and Step2Step targets computational accuracy within individual steps. This multi-faceted approach addresses the limitation of traditional preference optimization methods that struggle with long-chain reasoning tasks where intermediate errors cascade to incorrect final answers.

## Foundational Learning

**Direct Preference Optimization (DPO)**: A method for aligning language models with human preferences by optimizing the difference between log-likelihoods of preferred and dispreferred responses. Why needed: Provides the foundation for preference-based learning without requiring explicit reward modeling. Quick check: Verify understanding of the basic DPO loss function and its application to pairwise comparisons.

**Preference Pair Construction**: The process of creating training data by comparing two responses and labeling one as preferred over the other. Why needed: Forms the basis of all preference optimization methods, including MDPO's multi-granularity approach. Quick check: Understand how preference pairs are constructed differently across the three granularity levels.

**SimPO Length Normalization**: A technique that normalizes rewards by sequence length to prevent bias toward longer responses. Why needed: Ensures fair comparison between responses of different lengths during preference optimization. Quick check: Verify that length normalization is correctly implemented in the reward calculation.

## Architecture Onboarding

**Component Map**: Base LLM -> Preference Pair Generator -> Multi-Granularity Loss Function -> Optimized LLM
- Preference Pair Generator: Creates Sol2Sol, Infer2Infer, and Step2Step pairs
- Multi-Granularity Loss Function: Combines three levels of preference optimization
- Optimized LLM: Final model with improved mathematical reasoning

**Critical Path**: Data Construction (Sol2Sol/Infer2Infer/Step2Step) -> Preference Pair Formatting -> MDPO Training -> Evaluation
The most critical steps are accurate error detection in the data construction phase and proper implementation of the length-normalized reward function.

**Design Tradeoffs**: MDPO trades computational complexity for improved performance. The three-level granularity approach requires more sophisticated data construction and training procedures compared to standard DPO, but provides better alignment for complex reasoning tasks. The automated data construction pipeline eliminates manual annotation costs but introduces potential LLM generation biases.

**Failure Signatures**: Low win rate (pθ(yw|x) > pθ(yl|x)) indicates misalignment between reward and generation metrics. Poor performance on Infer2Infer suggests incorrect unreliable-step detection logic. Computational errors persisting despite Step2Step optimization indicate issues with the error localization mechanism.

**Three First Experiments**:
1. Verify basic DPO training works on the target models before implementing MDPO's multi-granularity approach
2. Test data construction pipeline by manually inspecting generated preference pairs for correctness
3. Run ablation study with only one granularity level to establish baseline performance

## Open Questions the Paper Calls Out
The paper explicitly states in the Conclusion that due to resource limits, experiments were restricted to 7B/8B models, and the authors "plan to enrich our experimental results by testing on a wider range of large-scale models." This directly calls out the need to validate whether the 1.7-2.3% accuracy gains persist or grow in larger models (70B+ parameters). The paper also implicitly leaves open questions about the method's generalizability to non-mathematical reasoning domains and the robustness of the automated data construction pipeline to noise.

## Limitations
- Evaluation limited to two small-scale models (7B/8B parameters) and two mathematical datasets, limiting generalizability
- Lack of comprehensive ablation studies to quantify individual contributions of each granularity level
- Heavy reliance on GPT-4 for automated data construction introduces generation biases and limits reproducibility
- Evaluation metrics limited to accuracy without deeper analysis of model behavior or reasoning consistency

## Confidence

**High Confidence**: Core methodology is clearly defined with explicit equations and training procedures; three-level granularity framework is well-motivated and technically sound.

**Medium Confidence**: Experimental results show statistically meaningful improvements but may not represent substantial practical gains; lack of statistical significance testing and comparison to more recent methods limits absolute performance claims.

**Low Confidence**: Generalizability beyond tested model sizes and datasets remains uncertain; reliance on GPT-4 for data construction raises questions about reproducibility with smaller models.

## Next Checks

1. **Ablation Study Validation**: Conduct systematic ablation experiments to quantify the individual contribution of each granularity level by training separate models with only one or two granularity types included.

2. **Statistical Significance Testing**: Perform paired statistical tests (e.g., McNemar's test) on the accuracy improvements across test sets to establish whether observed gains are statistically significant.

3. **Generalizability Assessment**: Validate the approach on larger models (70B+ parameters) and additional mathematical reasoning datasets to assess whether performance improvements scale with model size and transfer to more challenging problems.