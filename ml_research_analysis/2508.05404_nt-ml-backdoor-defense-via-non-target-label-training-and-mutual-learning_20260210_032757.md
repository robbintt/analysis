---
ver: rpa2
title: 'NT-ML: Backdoor Defense via Non-target Label Training and Mutual Learning'
arxiv_id: '2508.05404'
source_url: https://arxiv.org/abs/2508.05404
tags:
- backdoor
- attacks
- training
- poisoned
- defense
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the vulnerability of deep neural networks
  to backdoor attacks, where triggers embedded in training data cause misclassification.
  The proposed defense, NT-ML, uses a two-step training approach followed by mutual
  learning.
---

# NT-ML: Backdoor Defense via Non-target Label Training and Mutual Learning

## Quick Facts
- **arXiv ID**: 2508.05404
- **Source URL**: https://arxiv.org/abs/2508.05404
- **Reference count**: 40
- **Primary result**: Proposes NT-ML defense achieving lowest ASR against six backdoor attacks, especially effective with limited clean data

## Executive Summary
This paper addresses the vulnerability of deep neural networks to backdoor attacks, where triggers embedded in training data cause misclassification. The proposed defense, NT-ML, uses a two-step training approach followed by mutual learning. First, a model is trained on poisoned data (target training), then another model is trained using the outputs of the first model as soft labels, focusing on non-target classes (non-target training). Finally, mutual learning between these two models, with distillation of predictions and feature representations, purifies the student model. Experiments on CIFAR-10, CIFAR-100, and GTSRB datasets show NT-ML effectively defends against six advanced backdoor attacks, achieving the lowest attack success rates compared to five state-of-the-art defenses, especially when limited clean data is available.

## Method Summary
NT-ML employs a two-stage defense: (1) Two-step training where a teacher model is first trained on poisoned data with standard cross-entropy, then a student model is trained using the teacher's soft labels but excluding the target class probabilities (non-target training); (2) Mutual learning where both models are trained on a small clean set, with the student mimicking the teacher's predictions and the teacher mimicking the student's feature maps. This approach leverages the observation that poisoned models often retain the true label as the second-highest probability prediction, allowing the student to learn semantic relationships without reinforcing the backdoor.

## Key Results
- NT-ML achieves the lowest attack success rates (ASR) against all six evaluated backdoor attacks (BadNets, WaNet, BPP, LC, Refool, Narcissus)
- Outperforms five state-of-the-art defenses across CIFAR-10, CIFAR-100, and GTSRB datasets
- Maintains effectiveness even with only 1% clean data for mutual learning phase
- Shows particular robustness when the small clean set is limited, compared to other defenses requiring larger trusted datasets

## Why This Works (Mechanism)

### Mechanism 1: Non-target Training Suppresses Trigger Association
Training a model using only non-target class probabilities (soft labels) suppresses the trigger-to-target association while retaining semantic knowledge. In a poisoned model, the trigger forces a high probability for the target class, but the model often still assigns the second highest probability to the true label. By discarding the target class logit and training a second model to match the remaining distribution, the student learns inter-class relationships without reinforcing the backdoor shortcut. Core assumption: poisoned teacher model retains sufficient residual information about the true class in its non-target logits. Break condition: if the attack suppresses all non-target probabilities, soft labels become uninformative.

### Mechanism 2: Mutual Learning Purifies Feature Representations
Mutual learning allows a high-accuracy backdoored model to "forget" the backdoor by aligning its features with a robust, purified student model. The standard trained model (Teacher) has high accuracy but contains the backdoor. The Non-Target trained model (Student) has lower accuracy but is robust to the backdoor. By enforcing a feature distance loss where the Teacher mimics the Student's feature maps, the backdoor neurons in the Teacher are effectively "pruned" or updated to match the robust Student. Core assumption: the Student's feature maps effectively separate trigger features from semantic features better than the Teacher's. Break condition: if the Student's feature maps are too noisy, the Teacher may degrade without effectively removing the backdoor.

### Mechanism 3: Diversity Creates Stable Defense Intersection
Decoupling the training process into Target Training (TT) and Non-Target Training (NT) creates necessary diversity for defense. Standard defenses often fail because they rely on a single model to self-correct. NT-ML creates two distinct optimization landscapes: one biased toward the trigger (TT) and one biased away from it (NT). The intersection of these landscapes via Mutual Learning creates a stable point where accuracy is maintained but the trigger pathway is deprecated. Core assumption: the trigger relies on distinct neurons or pathways that can be isolated through contrast between TT and NT models. Break condition: if the backdoor is deeply entangled with robust features, the diversity between TT and NT may be insufficient to disentangle them.

## Foundational Learning

- **Concept: Knowledge Distillation (KD)**
  - Why needed here: The method relies on "soft labels" (probability distributions) rather than "hard labels" (one-hot vectors). You must understand that soft labels carry information about the similarity between classes.
  - Quick check question: If a teacher model outputs `[0.7 (Dog), 0.2 (Cat), 0.1 (Truck)]`, how does this help the student compared to a hard label `[1, 0, 0]`?

- **Concept: Backdoor Attack Topology**
  - Why needed here: Specifically distinguishing between *poisoned-label* (label is flipped) and *clean-label* (label stays, but input is perturbed) attacks. The Non-Target mechanism works differently for each.
  - Quick check question: Why might removing the target class logit specifically hurt a poisoned-label attack more than a clean-label attack?

- **Concept: Mutual Learning vs. Distillation**
  - Why needed here: Standard distillation assumes a fixed Teacher and a learning Student. Mutual Learning implies both networks update.
  - Quick check question: In a standard Teacher-Student setup, the Teacher is frozen. Why does NT-ML allow the Teacher to update its weights based on the Student's features?

## Architecture Onboarding

- **Component map**: ResNet-18 trained on poisoned data (TT) → ResNet-18 trained on teacher outputs excluding target class (NT) → Mutual Learning on clean data with prediction and feature distillation
- **Critical path**: The generation of soft labels in Stage 1 (NT). If the teacher model has low Top-2 accuracy (totally confused), the soft labels become noise and the defense fails.
- **Design tradeoffs**: Alpha ($\alpha$) controls how much the Student trusts the Teacher's predictions; Beta ($\beta$) controls how much the Teacher trusts the Student's features; Clean Data Ratio affects stability.
- **Failure signatures**: High ASR, Low BA (mutual learning collapsed); Low ASR, Low BA (Student forgot original task); High ASR, High BA (defense failed to disentangle features).
- **First 3 experiments**:
  1. Top-2 Check: Train teacher on poisoned dataset and verify true label is 2nd highest prediction to validate non-target training mechanism.
  2. Ablation on Loss: Run NT-ML using all classes vs. non-target classes only to confirm necessity of dropping target logit.
  3. Hyperparameter Sweep: Tune $\alpha$ on clean-label attack (Narcissus) vs. poisoned-label attack (BadNets) to observe stability difference.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the NT-ML defense mechanism be modified to function effectively without any trusted clean samples? The paper intends to investigate how to implement a defense without additional datasets for users without their own data.

- **Open Question 2**: How can an adaptive attacker bypass NT-ML if they are aware that mutual learning is being used as a defense? The paper identifies it as an interesting research direction to explore how to destroy the defense system assuming the attacker knows distillation is used.

- **Open Question 3**: How robust is NT-ML if the small "clean" dataset provided for mutual learning is partially compromised by label noise or poisoning? The paper assumes the defender has a small local clean set but doesn't test sensitivity to corruption within this trusted set.

## Limitations

- Relies on the assumption that poisoned models maintain high Top-2 accuracy on poisoned data, which is critical for non-target training but has limited empirical validation.
- Hyperparameter sensitivity analysis is incomplete, particularly for $\alpha$ and $\beta$ parameters across the full attack space.
- Focuses on six specific backdoor attacks without testing generalization to other attack types or sophisticated variations that might circumvent the non-target mechanism.

## Confidence

**High Confidence**: The basic defense pipeline works as described—NT-ML consistently reduces ASR compared to baseline defenses across multiple datasets and attack types when adequate clean data is available.

**Medium Confidence**: The specific mechanism of non-target training (removing target class logits) provides the claimed benefit over standard distillation. The ablation study showing TS and ST separately don't work as well as mutual learning is suggestive but not comprehensive.

**Low Confidence**: The defense's effectiveness with only 1% clean data is overstated. The paper reports this as a key advantage, but performance drop compared to 5% clean data suggests this claim needs more rigorous validation.

## Next Checks

1. **Top-2 Accuracy Validation**: Train the initial poisoned model on each attack type and measure Top-2 accuracy on poisoned data. Document cases where this drops below 80% and measure corresponding NT-ML performance degradation.

2. **Non-target vs Full Distillation Ablation**: Implement NT-ML with standard KD loss (all classes) and compare against non-target version. Measure ASR, BA, and PA for both variants across all six attack types to confirm necessity of dropping target logit.

3. **Feature Layer Sensitivity Analysis**: Systematically vary which ResNet-18 layers are used for feature distillation MSE loss. Test combinations of early, middle, and late layers to identify optimal configurations and measure performance sensitivity to architectural choice.