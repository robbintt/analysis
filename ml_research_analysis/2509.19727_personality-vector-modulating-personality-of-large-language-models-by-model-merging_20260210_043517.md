---
ver: rpa2
title: 'Personality Vector: Modulating Personality of Large Language Models by Model
  Merging'
arxiv_id: '2509.19727'
source_url: https://arxiv.org/abs/2509.19727
tags:
- personality
- merging
- vectors
- trait
- traits
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose personality vector merging, a method that modulates
  personality traits in large language models (LLMs) without additional training.
  They construct personality vectors by subtracting the weights of a pre-trained model
  from those of a personality-fine-tuned model and merge them into other models to
  induce desired personality traits.
---

# Personality Vector: Modulating Personality of Large Language Models by Model Merging
## Quick Facts
- arXiv ID: 2509.19727
- Source URL: https://arxiv.org/abs/2509.19727
- Reference count: 28
- Primary result: Achieves Pearson correlations > 0.9 for single-trait personality modulation in LLMs without additional training

## Executive Summary
This paper introduces personality vector merging, a method to modulate personality traits in large language models by subtracting the weights of a base model from those of a personality-fine-tuned model and merging the resulting vector into other models. The approach enables continuous control over personality trait intensity, supports multi-trait composition, and transfers across domains including role-playing agents, multilingual models, and vision-language models. Experiments demonstrate strong alignment between intended personality modulation and observed outcomes, with average Pearson correlations exceeding 0.9 for single-trait control and 0.6 for multi-trait composition after applying DaRE.

## Method Summary
Personality vector merging works by first fine-tuning a base LLM on specific personality traits, then computing the difference between the fine-tuned model weights and the original base model weights to create a personality vector. This vector is then merged into other models through weight addition, allowing the transfer of personality traits without requiring additional training. The method enables continuous control by scaling the personality vector magnitude and supports composition of multiple personality traits. The approach was validated across diverse model types including standard LLMs, role-playing agents, multilingual models, and vision-language models, demonstrating both quantitative alignment with trait scores and qualitative behavioral changes.

## Key Results
- Merging personality vectors into a role-playing agent increased extraversion score from 2.4 to 3.9
- Cross-modal transfer demonstrated trait-specific differences in visual interpretation between personality-modulated vision-language models
- Achieved average Pearson correlations exceeding 0.9 for single-trait control and 0.6 for multi-trait composition after applying DaRE

## Why This Works (Mechanism)
Personality vector merging leverages the hypothesis that personality traits manifest as systematic, learnable shifts in model weights that can be captured as additive differences between personality-conditioned and base models. By representing personality as weight-space vectors rather than prompt-based conditioning, the method enables direct manipulation of internal model representations to induce desired behavioral traits.

## Foundational Learning
- Weight-space personality representation - The concept that personality traits can be encoded as differences in neural network weights rather than just surface-level behaviors
  - Why needed: Enables direct manipulation of personality through weight merging rather than prompt engineering
  - Quick check: Verify that personality vector subtraction produces meaningful differences in personality trait scores

- Linear separability assumption - The hypothesis that personality-conditioned weights can be represented as linear combinations of base and fine-tuned weights
  - Why needed: Underpins the validity of the subtraction method for personality vector creation
  - Quick check: Test whether personality vectors maintain effectiveness when scaled continuously

- Cross-modal personality transfer - The ability to transfer personality traits from language models to multimodal models through weight merging
  - Why needed: Demonstrates the general applicability of personality vectors beyond pure language tasks
  - Quick check: Verify that vision-language models exhibit trait-specific differences in visual interpretation after personality vector merging

## Architecture Onboarding
- Component map: Base LLM -> Personality Fine-tuning -> Weight Subtraction -> Personality Vector -> Target Model Weight Addition -> Personality-modulated Model
- Critical path: The weight subtraction step is critical as it defines the personality vector; errors here propagate to all downstream merging
- Design tradeoffs: Simple weight addition vs. more sophisticated merging techniques (LoRA, adapters); simplicity favors interpretability but may miss complex trait interactions
- Failure signatures: Poor personality alignment indicates non-linear personality representations or inadequate fine-tuning; inconsistent cross-domain transfer suggests domain-specific personality encoding
- First experiments: 1) Single-trait personality vector merging with continuous scaling, 2) Multi-trait composition with DaRE post-processing, 3) Cross-modal transfer to vision-language models

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Method's effectiveness for less structured personality dimensions or extreme trait intensities remains unvalidated
- Reliance on linear weight subtraction may not capture complex, nonlinear personality representations
- Experimental scope limited to English-language models, leaving cross-linguistic consistency unexplored

## Confidence
- High: Single-trait modulation in controlled settings (Pearson > 0.9 alignment)
- Medium: Multi-trait composition (correlation drops to 0.6 post-DaRE)
- Low: Cross-modal transfer claims (limited to single traits, lacks systematic evaluation)

## Next Checks
1. Evaluate personality vector merging across broader personality frameworks (HEXACO, Dark Triad) and extreme trait intensities
2. Conduct ablation studies comparing personality vector merging against LoRA and prefix tuning on identical benchmarks
3. Test cross-linguistic generalization by applying English-trained personality vectors to non-English LLMs