---
ver: rpa2
title: Controlling Gender Bias in Retrieval via a Backpack Architecture
arxiv_id: '2511.00875'
source_url: https://arxiv.org/abs/2511.00875
tags:
- gender
- ranking
- bias
- uni00000013
- uni00000011
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of gender bias in document ranking
  systems that use large language models (LLMs), which can propagate harmful stereotypes
  embedded in training data. The authors propose a bias-controllable ranking framework
  based on Backpack Language Models, which represent tokens as weighted combinations
  of non-contextual sense vectors (aspects).
---

# Controlling Gender Bias in Retrieval via a Backpack Architecture

## Quick Facts
- **arXiv ID**: 2511.00875
- **Source URL**: https://arxiv.org/abs/2511.00875
- **Reference count**: 33
- **Primary result**: Reduces gender bias in document ranking via weighted sense vectors without retraining

## Executive Summary
This paper addresses the problem of gender bias in document ranking systems that use large language models (LLMs), which can propagate harmful stereotypes embedded in training data. The authors propose a bias-controllable ranking framework based on Backpack Language Models, which represent tokens as weighted combinations of non-contextual sense vectors (aspects). Their method disentangles sense vectors by measuring their sensitivity to gender attributes using cosine similarity with gendered word pairs, then selectively reweights the two most gender-sensitive senses via a control parameter λ to suppress gender-related information during inference without requiring model retraining. Experiments on MS MARCO show their approach effectively reduces gender bias (RaB/ARaB metrics) across different cutoffs while maintaining strong ranking performance (MRR@10/NDCG@10 up to 0.334/0.402 with λ=1.0), demonstrating a controllable fairness-performance trade-off.

## Method Summary
The authors develop a bias-controllable ranking framework that extends Backpack Language Models to address gender bias in document retrieval. The method works by representing each token as a weighted combination of non-contextual sense vectors (aspects), then disentangling these sense vectors based on their sensitivity to gender attributes. The disentanglement process uses cosine similarity between sense vectors and gendered word pairs (he/she, him/her, his/her) to identify gender-sensitive dimensions. During inference, the framework reweights the two most gender-sensitive sense vectors using a control parameter λ, effectively suppressing gender-related information in the ranking process without requiring model retraining. This approach enables controllable trade-offs between fairness and ranking performance through λ adjustment.

## Key Results
- Effectively reduces gender bias as measured by RaB/ARaB metrics across different cutoffs on MS MARCO
- Maintains strong ranking performance (MRR@10/NDCG@10 up to 0.334/0.402) with λ=1.0
- Demonstrates controllable fairness-performance trade-off through λ parameter adjustment
- Achieves bias reduction without requiring model retraining, operating purely through inference-time reweighting

## Why This Works (Mechanism)
The approach works by exploiting the inherent structure of Backpack Language Models, where tokens are represented as weighted combinations of distinct sense vectors. By identifying and reweighting the most gender-sensitive sense vectors, the method can selectively suppress gender-related information while preserving other semantic content. The use of cosine similarity with gendered word pairs provides a quantitative measure of gender sensitivity for each sense vector, enabling targeted intervention. The control parameter λ allows for graduated suppression of gender information, creating a tunable balance between bias reduction and ranking effectiveness. Operating at inference time rather than requiring retraining makes the approach computationally efficient and adaptable to different fairness requirements.

## Foundational Learning

**Backpack Language Models**: Represent tokens as weighted combinations of non-contextual sense vectors (aspects)
- Why needed: Provides the architectural foundation for disentangling and selectively reweighting sense representations
- Quick check: Verify that sense vectors capture distinct semantic aspects and can be meaningfully separated

**Cosine Similarity for Gender Sensitivity**: Measures alignment between sense vectors and gendered word pairs
- Why needed: Quantifies the degree to which each sense vector encodes gender-related information
- Quick check: Confirm that gendered word pairs adequately represent the gender dimensions of interest

**Two-Sense Reweighting**: Selectively adjusts the two most gender-sensitive sense vectors using parameter λ
- Why needed: Provides targeted control over gender bias while minimizing impact on other semantic content
- Quick check: Validate that two-sense adjustment is sufficient for meaningful bias reduction

## Architecture Onboarding

**Component Map**: Token representations (Backpack) -> Sense vector disentanglement (cosine similarity) -> Gender-sensitive sense identification -> λ-controlled reweighting -> Inference output

**Critical Path**: The disentanglement and reweighting process must operate efficiently at inference time to avoid latency issues in real-world retrieval systems

**Design Tradeoffs**: The choice to operate at inference time rather than retraining enables flexibility but may limit the depth of bias mitigation compared to model-level interventions

**Failure Signatures**: If λ=0.0 fails to eliminate gender bias or λ=1.0 significantly degrades ranking performance, this suggests either insufficient sense disentanglement or overly aggressive suppression of relevant semantic content

**First Experiments**:
1. Test the framework with varying λ values on a small subset of MS MARCO to observe the fairness-performance trade-off curve
2. Validate the sense disentanglement process by examining whether identified gender-sensitive senses align with linguistic intuition
3. Compare the two-sense reweighting approach against single-sense or multi-sense alternatives to quantify the optimal number of adjustments

## Open Questions the Paper Calls Out
None

## Limitations
- The two-sense reweighting approach may not capture the full complexity of gender-related information in embeddings
- The evaluation focuses primarily on binary gender pairs, potentially overlooking broader gender bias manifestations
- Experiments are conducted on a single dataset (MS MARCO), limiting generalizability to other retrieval tasks

## Confidence

**High confidence**: The core methodology and implementation details are clearly described and reproducible

**Medium confidence**: The effectiveness of the approach in reducing gender bias metrics on MS MARCO

**Medium confidence**: The preservation of ranking performance across different λ values

## Next Checks

1. Test the Backpack Architecture approach on additional retrieval datasets (e.g., TREC collections, academic search) to assess cross-domain robustness and identify potential dataset-specific behaviors

2. Extend the gender bias evaluation framework to include non-binary gender terms and intersectional identity considerations, examining whether the two-sense reweighting captures these more complex bias patterns

3. Conduct ablation studies removing the sense disentanglement step to quantify its contribution to bias reduction versus alternative reweighting strategies based on different sensitivity metrics