---
ver: rpa2
title: What's in a prompt? Language models encode literary style in prompt embeddings
arxiv_id: '2505.17071'
source_url: https://arxiv.org/abs/2505.17071
tags:
- embeddings
- style
- information
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates how information from an entire prompt\
  \ is encoded in the embeddings of large language models (LLMs), focusing on intangible\
  \ aspects like literary style. Using short excerpts (10\u2013100 tokens) from various\
  \ 19th- and early 20th-century novels, the authors show that embeddings from later\
  \ transformer layers contain distinctive stylistic features specific to individual\
  \ authors."
---

# What's in a prompt? Language models encode literary style in prompt embeddings

## Quick Facts
- **arXiv ID:** 2505.17071
- **Source URL:** https://arxiv.org/abs/2505.17071
- **Reference count:** 17
- **Primary result:** LLM embeddings encode author-specific literary style, enabling >90% accuracy in authorship attribution from short excerpts.

## Executive Summary
This study demonstrates that large language models (LLMs) encode distinctive literary styles of individual authors in their deep layer embeddings. Using short excerpts from 19th- and early 20th-century novels, the authors show that embeddings from later transformer layers contain stylistic features specific to authors. They achieve over 90% accuracy in classifying excerpts by authorship using an MLP probe, with performance increasing as context length and layer depth increase. The effect persists across multiple LLM architectures and is stronger between different authors than between books by the same author, suggesting embeddings capture lexical and syntactic style rather than factual content.

## Method Summary
The authors extracted embeddings from short literary excerpts (10-100 tokens) from 13 novels by 7 authors. For each excerpt, they ran forward passes through four LLM architectures (Llama-3.2-1B, gemma-3-1b-pt, Qwen3-1.7B-Base, SmolLM2-1.7B) and captured the last token's hidden state at each transformer layer. They used these embeddings for binary classification (SVM with PCA) and multiclass classification (MLP probe) tasks. The SVM probe reduced embeddings to 64 dimensions and used a linear kernel with 70/30 train/validation split. The MLP probe had a 32-dimensional penultimate layer with cross-entropy loss and Adam optimizer.

## Key Results
- SVM classifier achieved >90% accuracy for GE vs VW classification with N ≥ 64 tokens and L ≥ 10 layers
- Multiclass MLP probe achieved similar high accuracy across all tested architectures
- Books by the same author showed more entanglement than books by different authors in embedding space
- Shuffling token order preserved separability, indicating style is more lexical than syntactic
- Effect is robust across multiple LLM architectures including Llama-3.2-1B, gemma-3-1b-pt, Qwen3-1.7B-Base, and SmolLM2-1.7B

## Why This Works (Mechanism)
LLMs learn distributed representations that capture both semantic content and linguistic patterns during pretraining. As text passes through transformer layers, information is progressively transformed from surface-level features to abstract representations. Deep layers capture higher-level linguistic patterns including stylistic choices, vocabulary preferences, and syntactic tendencies that are characteristic of individual authors. The causal attention mechanism ensures the final token's embedding aggregates information from the entire prompt context.

## Foundational Learning
- **Transformer embeddings**: Token representations that evolve through network layers to capture increasingly abstract features
  - Why needed: Form the basis for analyzing how stylistic information is encoded
  - Quick check: Verify embeddings change meaningfully across layers for the same input
- **Causal attention**: Attention mechanism where each position can only attend to previous positions
  - Why needed: Determines which token's embedding captures full context
  - Quick check: Confirm final token attends to entire sequence
- **Linear separability**: Whether data points from different classes can be separated by a linear boundary
  - Why needed: Indicates whether stylistic features are encoded in a way that supports classification
  - Quick check: Test SVM performance on embeddings vs random projections
- **Intrinsic dimension**: The minimum number of parameters needed to approximate a dataset
  - Why needed: Measures complexity of the embedding manifold
  - Quick check: Compare ID estimates across different context lengths
- **Probing classifiers**: Simple models trained on embeddings to test for specific information
  - Why needed: Provide interpretable metrics for what information is encoded
  - Quick check: Test classifier performance on embeddings vs shuffled controls
- **Tokenization**: Process of converting text to model input tokens
  - Why needed: Affects how text is represented in embeddings
  - Quick check: Verify consistent tokenization across all texts

## Architecture Onboarding

**Component Map:** Text → Tokenizer → LLM (multiple layers) → Final token embedding → Probe classifier → Authorship prediction

**Critical Path:** Prompt → LLM forward pass → Extract last token embedding at layer L → Train probe classifier → Evaluate accuracy

**Design Tradeoffs:** The choice to use the final token's embedding assumes it captures the full context, trading completeness for computational efficiency. Using non-overlapping chunks prioritizes simplicity over preserving natural sentence boundaries.

**Failure Signatures:** Low accuracy despite deep layers suggests either incorrect token position extraction or insufficient stylistic variation in the corpus. High accuracy on shuffled inputs indicates style dominates over syntax.

**First Experiments:**
1. Extract embeddings from a single short excerpt and visualize how they change across layers
2. Train SVM classifier on embeddings from layer 1 vs layer 16 for binary classification
3. Test classifier performance on original vs shuffled token sequences

## Open Questions the Paper Calls Out

**Open Question 1:** Can the stylistic signatures identified in LLM embeddings be decomposed into interpretable, fine-grained features (e.g., sentence length, vocabulary rarity, specific syntactic patterns)?

The paper demonstrates that style is encoded and is more lexical than syntactic, but does not identify which specific features drive separability. Use sparse autoencoders or probing classifiers with curated stylistic feature datasets to map embedding dimensions to specific attributes.

**Open Question 2:** Does the cross-lingual geometric alignment of stylistic representations (e.g., French-English author pairs) generalize to non-translation settings and typologically distant languages?

The experiment uses translated texts where semantic content is matched; it remains unclear whether style-specific geometry transfers when both content and language differ. Train probes on original-language texts from multilingual authors and test on their works in other languages without translation correspondence.

**Open Question 3:** Why does the intrinsic dimension of embedding ensembles remain constant (~20) regardless of context length, even as classification accuracy improves with more tokens?

The paper documents the phenomenon but does not investigate why adding tokens translates embeddings without increasing manifold complexity. Analyze how centroids shift in the principal component subspace as N increases; test whether displacement magnitude scales with N while local neighborhood structure remains fixed.

## Limitations
- MLP probe hyperparameters (depth, learning rate, epochs) are unspecified beyond the 32-dimensional penultimate layer
- Dataset sizes are only given as relative splits (70/30), so absolute excerpt counts per novel are unknown
- Analysis focuses on English 19th-early 20th-century prose; results may not generalize to other languages, genres, or contemporary writing

## Confidence

**High:** LLM embeddings encode author-specific stylistic features at deep layers, given the >90% accuracy and cross-model consistency.

**Medium:** The claim that lexical rather than syntactic features dominate, since shuffling preserves separability but does not exclude all syntactic signals.

**Medium:** The effect is due to style and not content, given the entanglement of same-author works and the robustness across architectures, though finer-grained ablation studies would strengthen this claim.

## Next Checks
1. Replicate the binary SVM classification with a fixed random seed and report variance across runs to assess stability
2. Perform ablation by training on shuffled-token inputs for each N and L, confirming that accuracy remains high and is not due to sentence structure
3. Extend the probe to a different language model (e.g., Mistral-7B or Phi-3) to confirm that the stylistic encoding effect is not specific to the models tested