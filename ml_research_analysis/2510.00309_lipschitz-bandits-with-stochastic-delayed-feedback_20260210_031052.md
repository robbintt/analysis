---
ver: rpa2
title: Lipschitz Bandits with Stochastic Delayed Feedback
arxiv_id: '2510.00309'
source_url: https://arxiv.org/abs/2510.00309
tags:
- algorithm
- delayed
- delay
- regret
- lipschitz
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses Lipschitz bandits with stochastic delayed\
  \ feedback, where rewards are observed after random delays. For bounded delays,\
  \ a delay-aware zooming algorithm is proposed that achieves regret \xD5(T(dz+1)/(dz+2)\
  \ + \u03C4max T^dz/(dz+2)), recovering the optimal delay-free rate with an additive\
  \ term scaling with the maximal delay \u03C4max."
---

# Lipschitz Bandits with Stochastic Delayed Feedback

## Quick Facts
- **arXiv ID**: 2510.00309
- **Source URL**: https://arxiv.org/abs/2510.00309
- **Authors**: Zhongxuan Liu, Yue Kang, Thomas C. M. Lee
- **Reference count**: 40
- **Primary result**: Algorithms achieving near-optimal regret bounds for Lipschitz bandits with stochastic delayed feedback

## Executive Summary
This paper addresses the challenge of Lipschitz bandits where rewards are observed after random delays. For bounded delays, a delay-aware zooming algorithm is proposed that achieves regret Õ(T^(dz+1)/(dz+2) + τmax·T^dz/(dz+2)), recovering the optimal delay-free rate with an additive term scaling with the maximal delay τmax. For unbounded delays, a novel Delayed Lipschitz Phased Pruning (DLPP) algorithm accumulates reliable feedback over scheduled intervals, achieving regret Õ(min_p∈(0,1]{(1/p)T^(dz+1)/(dz+2) + Q(p)}), where Q(p) is the p-th quantile of the delay distribution. A lower bound nearly matching these upper bounds is established, demonstrating the algorithms' near-optimality.

## Method Summary
The paper proposes two algorithms for Lipschitz bandits with stochastic delayed feedback. For bounded delays, Delayed Zooming uses lazy updates to maintain stable confidence radii despite delayed observations, substituting observed reward counts for pull counts in confidence radius computation. For unbounded delays, DLPP employs phased elimination with exponentially shrinking radii, accumulating feedback over scheduled intervals and pruning suboptimal regions based on quantile-dependent confidence bounds. Both algorithms assume access to a covering oracle and operate under Lipschitz continuity, sub-Gaussian noise, and either bounded or i.i.d. delay assumptions.

## Key Results
- For bounded delays, delay-aware zooming achieves regret Õ(T^(dz+1)/(dz+2) + τmax·T^dz/(dz+2))
- For unbounded delays, DLPP achieves regret Õ(min_p∈(0,1]{(1/p)T^(dz+1)/(dz+2) + Q(p)})
- A lower bound nearly matching the upper bounds is established, proving near-optimality
- Experiments demonstrate sublinear regret under various reward functions and delay distributions

## Why This Works (Mechanism)

### Mechanism 1: Lazy Update for Confidence Radius Stability Under Bounded Delays
The delay-aware zooming algorithm maintains confidence balls over active arms. When delayed feedback arrives for an unpulled arm, a lazy update rule caches observations until they exceed 4× the count from the arm's last pull. This prevents confidence radii from shrinking too rapidly due to delayed arrivals, preserving the critical sub-optimality bound ∆(x) ≤ 6r_t(x). The algorithm substitutes observed reward counts v_t(x) for pull counts n_t(x) in confidence radius computation.

### Mechanism 2: Phased Pruning with Quantile-Based Feedback Accumulation
DLPP operates in phases with exponentially shrinking radii (r_m = 2^(-m)). Each phase uses round-robin sampling from surviving balls until v_m observations accumulate per ball. Balls with empirical mean ≥4r_m below the best are eliminated. Critically, after Q(p) additional rounds, the algorithm guarantees ≥(p/2)·n_t(B) observations with high probability, converting delay uncertainty into a quantile-dependent additive term.

### Mechanism 3: Lower Bound via Bernoulli Reduction
The lower bound proof constructs a hard delay distribution where delay = τ₀ with probability p and ∞ otherwise. A reduction simulates a non-delayed Lipschitz bandit algorithm using Bernoulli sampling (parameter p). The lower bound combines: (1) the standard Lipschitz bandit lower bound scaled by 1/p, and (2) unavoidable regret τ₀·∆̄ during initial rounds with zero feedback.

## Foundational Learning

- **Concept: Zooming Dimension (d_z) vs. Covering Dimension (d)**
  - Why needed here: Regret bounds scale with d_z (instance-dependent) not d (space-dependent). Understanding this distinction is critical for algorithm selection and bound interpretation.
  - Quick check question: If the optimal arm lies in a small "easy" region of the action space, which dimension characterizes regret, and why can d_z << d?

- **Concept: Quantile Function Q(p) for Delay Characterization**
  - Why needed here: Unbounded delay analysis relies on quantiles, not moments. The bound optimizes over p ∈ (0,1], trading off delay magnitude against effective round count.
  - Quick check question: Why does the regret bound minimize over p rather than using a fixed quantile like the median?

- **Concept: Covering Oracle for Adaptive Discretization**
  - Why needed here: Both algorithms require a covering oracle to identify uncovered arms (zooming) or construct r-coverings (DLPP). This oracle assumption is standard but non-trivial to implement.
  - Quick check question: What does a covering oracle return, and how does it differ between the two algorithms?

## Architecture Onboarding

- **Component map**: Active set A -> Covering oracle -> Selection (argmax UCB) -> Lazy update cache Q[x] -> Confidence radius update
- **Critical path**: Initialize covering/delay parameters → Process incoming delayed feedback → Update statistics → Select action → Schedule observation → Lazy update/phase completion check → Elimination/pruning decisions
- **Design tradeoffs**: Zooming offers tighter regret for 1D problems but requires bounded delays; DLPP handles unbounded delays with superior multi-dimensional performance but has piecewise-linear regret curves due to phase structure
- **Failure signatures**: Confidence radius collapses under delay floods; phase never completes due to heavy-tailed delays; pruning eliminates optimal region due to incorrect Lipschitz constant
- **First 3 experiments**:
  1. Implement Delayed Zooming on [0,1] with triangle reward and uniform delays, verifying regret scales as Õ(T^(2/3) + τmax·T^(1/3))
  2. Implement DLPP with geometric delays, comparing regret against theoretical bound using different quantiles p
  3. Test both algorithms on ([0,1]², ||·||_∞) with 2D reward function, verifying DLPP outperforms zooming

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Delayed Zooming Algorithm be extended to handle unbounded delays without requiring additional assumptions about the delay distribution?
- Basis in paper: [explicit] The conclusion states, "extending it to the unbounded delay case without additional assumptions on the delay distribution remains an open and challenging problem."
- Why unresolved: The algorithm's theoretical guarantees currently rely on a bounded support (τmax) to manage the confidence radius updates via the "lazy update" mechanism; unbounded delays disrupt the stability of these confidence estimates.
- What evidence would resolve it: A theoretical proof establishing a regret bound for the Delayed Zooming Algorithm (or a variant) that holds for unbounded delay distributions (e.g., geometric), specifically removing the dependence on a maximum delay constant.

### Open Question 2
- Question: Can these algorithms be adapted to remain optimal without prior knowledge of the zooming dimension (dz) or delay statistics (τmax, Q(p))?
- Basis in paper: [inferred] Theorems 1 and 3 express regret bounds in terms of dz and delay quantiles, and the analysis assumes specific noise parameters (σ=1), implying the current theoretical guarantees depend on these known environmental constants.
- Why unresolved: The analysis relies on these parameters to set confidence radii and pruning thresholds; achieving the same rates "parameter-free" (without knowing dz or delay distributions) is a standard but unaddressed challenge in bandit literature.
- What evidence would resolve it: An algorithm design and analysis that achieves the stated regret rates (or near-optimal variants) while treating dz, τmax, and delay quantiles as unknown variables to be learned online.

### Open Question 3
- Question: How do these Lipschitz bandit algorithms perform under heavy-tailed reward distributions rather than sub-Gaussian noise?
- Basis in paper: [inferred] The problem setting explicitly assumes the noise εt is sub-Gaussian, a standard assumption that does not hold in all real-world applications (e.g., financial data) cited in the introduction.
- Why unresolved: The confidence radius and empirical mean calculations (μ̂) used in both algorithms rely on concentration inequalities (Hoeffding) that fail for heavy-tailed distributions.
- What evidence would resolve it: A modification of the proposed algorithms using robust estimators (e.g., median-of-means or truncated mean) with a corresponding regret analysis proving sublinear regret for heavy-tailed delayed feedback.

## Limitations

- **Covering oracle assumption**: Both algorithms require access to a covering oracle, which is standard but non-trivial to implement in practice
- **Lazy update mechanism**: The bounded delay algorithm relies on cache management that isn't fully specified in the paper
- **Quantile dependence**: The unbounded delay algorithm's performance depends on delay quantiles that may not be well-characterized in all applications

## Confidence

- **High confidence**: Regret bounds for bounded delays (Theorem 1), as they extend well-established zooming techniques with clear modifications
- **Medium confidence**: DLPP regret analysis, as it builds on known phased elimination methods but adapts them to the quantile-based delay framework
- **Medium confidence**: Lower bound construction, as it relies on a specific hard instance that may not capture all delay distributions

## Next Checks

1. Implement the covering oracle for 1D and 2D spaces to verify the activation conditions in Delayed Zooming work as intended
2. Test DLPP with various delay distributions to confirm the regret bound depends on the claimed quantile Q(p) rather than other delay statistics
3. Compare empirical regret against theoretical bounds across different Lipschitz functions to verify the zooming dimension dz captures problem difficulty accurately