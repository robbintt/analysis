---
ver: rpa2
title: 'Seeing but Not Believing: Probing the Disconnect Between Visual Attention
  and Answer Correctness in VLMs'
arxiv_id: '2510.17771'
source_url: https://arxiv.org/abs/2510.17771
tags:
- evidence
- visual
- attention
- layers
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "VLMs often answer incorrectly despite perceiving the correct visual\
  \ evidence. We find that shallow layers focus on text, while deeper layers sparsely\
  \ attend to evidence regions\u2014yet models still fail to use this information."
---

# Seeing but Not Believing: Probing the Disconnect Between Visual Attention and Answer Correctness in VLMs

## Quick Facts
- arXiv ID: 2510.17771
- Source URL: https://arxiv.org/abs/2510.17771
- Reference count: 35
- VLMs often answer incorrectly despite perceiving correct visual evidence; VEA improves Exact Match by up to +11.1 points and Token F1 by +17.3

## Executive Summary
VLMs frequently answer questions incorrectly even when they have correctly perceived the relevant visual evidence, a phenomenon termed "seeing but not believing." Through systematic analysis of attention patterns across model layers, we find that while deeper layers do identify and attend to correct evidence regions, this information is often not effectively utilized in the final answer generation. We introduce VEA (Visual Evidence Augmentation), an inference-time method that highlights these evidence regions via attention-based masking without requiring any training. VEA consistently improves performance across eight models from four different families, demonstrating that explicitly bridging the gap between visual perception and reasoning can significantly enhance VQA accuracy.

## Method Summary
VEA is an inference-time method that improves VQA performance by highlighting evidence regions identified by deep layers through attention-based masking. The approach profiles models to identify "visual grounding layers" that best align with ground-truth evidence, extracts attention maps from these layers, denoises and smooths them to create highlight masks, and blends these masks with the original image before feeding it to the VLM. This forces the visual encoder to emphasize regions the model has already identified as important, amplifying the signal-to-noise ratio without any retraining.

## Key Results
- VEA improves Exact Match by up to +11.1 points and Token F1 by +17.3 across eight models from four families
- Consistently outperforms baseline methods including CLIP-filtered inputs and attention-guided masks
- Robustly handles noisy inputs and maintains effectiveness across different VQA datasets (InfoVQA, DocVQA, SROIE, TextVQA)
- Particularly effective for models with larger visual encoders relative to their LLM backbones

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** VLMs process modalities sequentially, shifting focus from linguistic parsing to visual grounding as depth increases.
- **Mechanism:** Shallow layers prioritize text tokens to understand the query structure. As layers deepen, the attention mass shifts progressively toward image tokens. If this transition fails or is insufficient, the model defaults to language priors.
- **Core assumption:** The architecture follows a standard Transformer decoder pattern where self-attention evolves layer-by-layer.
- **Evidence anchors:**
  - [Section 2.1] "Early layers focus overwhelmingly on the question text... As depth increases... deeper layers allocating relatively more attention to image tokens."
  - [Section 2.2] "Deeper layers... display sparse yet highly concentrated attention, consistently highlighting regions aligned with ground-truth evidence."

### Mechanism 2
- **Claim:** Errors in VQA often stem from a failure to propagate perceived evidence to the generation phase ("seeing but not believing"), rather than a failure of perception itself.
- **Mechanism:** Deep layers successfully identify and attend to the correct visual evidence regions (high attribution scores). However, the "visual grounding" signal is under-utilized by the final generation layers, which may be dominated by textual priors or hallucinations.
- **Core assumption:** Attention weights in deep layers correlate with the model's internal "perception" of relevant evidence.
- **Evidence anchors:**
  - [Abstract] "Surprisingly, VLMs often perceive the visual evidence when outputting incorrect answers... a phenomenon we term 'seeing but not believing'."
  - [Figure 4 / Section 2.3] "Deeper layers pay much greater attention to crucial evidence... even when VLM responds incorrectly."

### Mechanism 3
- **Claim:** Explicitly highlighting evidence regions via attention-based masking (VEA) bridges the gap between perception and reasoning.
- **Mechanism:** The method extracts the attention map from the identified "visual grounding layers," denoises it, and applies a Gaussian-smoothed mask to the original image. This forces the visual encoder to emphasize the regions the model *already* identified as important, amplifying the signal-to-noise ratio.
- **Core assumption:** The visual encoder processes the augmented image differently, boosting the activation of previously suppressed evidence.
- **Evidence anchors:**
  - [Section 3.3] "This simple augmentation makes evidence more salient without retraining, steering inference toward highlighted regions."
  - [Table 1] VEA consistently improves Exact Match and Token F1 across LLaVA, Qwen, Gemma, and InternVL.

## Foundational Learning

- **Concept:** **Transformer Self-Attention Mechanics**
  - **Why needed here:** The entire VEA method relies on extracting and interpreting raw attention weights ($a^{(\ell,h)}$) from specific layers.
  - **Quick check question:** Can you explain how Query, Key, and Value matrices interact to produce an attention map?

- **Concept:** **Vision-Language Model (VLM) Architecture**
  - **Why needed here:** You must distinguish between the Visual Encoder (processes patches) and the LLM Backbone (processes tokens) to understand where attention leakage occurs.
  - **Quick check question:** In a typical VLM, are image patches treated as distinct tokens in the LLM's context window, or are they processed separately?

- **Concept:** **Visual Grounding & Attribution Metrics**
  - **Why needed here:** To evaluate if the method works, you need to measure alignment between model attention and ground truth (AUROC/NDCG).
  - **Quick check question:** Why is Exact Match alone insufficient to determine if a model "saw" the evidence?

## Architecture Onboarding

- **Component map:** Image + Question -> Layer Profiling -> Attention Extractor -> Mask Generator -> Augmented Inference -> Answer
- **Critical path:**
  1. **Layer Profiling:** Selecting the wrong layers (e.g., shallow text-focused layers) will generate a useless mask. The paper finds middle-to-deep layers are critical (e.g., Layers 14-19 for LLaVA-7B).
  2. **Denoising:** Raw attention often contains scattered high-value artifacts. The neighborhood filtering step is required to prevent highlighting noise.
- **Design tradeoffs:**
  - **Static vs. Adaptive Layers:** The paper uses a static set of "visual grounding layers" per model to save inference time, rather than adapting per sample.
  - **Efficiency:** VEA requires an extra forward pass (or partial pass) to extract attention before generating the answer.
  - **Delegate Models:** For models with memory leaks (e.g., LLaVA-Next, InternVL), the paper uses Qwen2.5-VL-7B as a *delegate* to generate the mask. This assumes attention patterns are somewhat transferable or at least better than no mask.
- **Failure signatures:**
  - **Memory Leakage:** Extracting attention maps requires `attn_implementation='eager'`, which may cause OOM errors on 80GB GPUs for certain models (noted in Appendix A.2).
  - **Textual Bias:** If the model ignores the highlighted image region entirely, VEA will fail; the paper notes this is reduced but not eliminated.
- **First 3 experiments:**
  1. **Layer Ablation:** Profile the target model to identify $L_{VG}$. Visualize attention maps from shallow vs. deep layers to confirm the "text-to-image" shift.
  2. **Mask Quality Check:** Run VEA on a small VisualCoT subset. Calculate AUROC between the generated mask and the ground-truth bounding boxes to ensure the mask actually covers the evidence.
  3. **Delegate Verification:** If running on a memory-constrained model, test if the mask generated by a smaller "delegate" model (e.g., Qwen-7B) improves the larger model's accuracy, or if it introduces misalignment.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can deep-layer attention signals effectively guide targeted image manipulations, such as dynamic cropping or zooming, to facilitate reasoning in long-horizon or agentic multimodal tasks?
- **Basis in paper:** [explicit] Appendix B.3 proposes leveraging attention to trigger adaptive image refinement (e.g., "dynamically crop or zoom into the relevant region") for multi-stage processing.
- **Why unresolved:** The current study focuses on static image highlighting (masking) for single-turn VQA; it does not explore sequential or self-triggered visual refinement in complex reasoning pipelines.
- **What evidence would resolve it:** Empirical results demonstrating that attention-guided visual refinement improves performance on multi-hop visual reasoning benchmarks or long-horizon agentic tasks compared to static augmentation.

### Open Question 2
- **Question:** To what extent is the "seeing but not believing" phenomenon caused by architectural imbalance (e.g., large LLM backbone vs. small visual encoder) versus training data priors?
- **Basis in paper:** [inferred] Section 2.4 discusses "Textual Information Dominance" and "Architectural Imbalance" as perspectives but does not isolate the primary causal factor.
- **Why unresolved:** The paper introduces an inference-time intervention (VEA) to mitigate the symptom but does not determine if the root cause is model scale disparity or linguistic bias in training.
- **What evidence would resolve it:** Ablation studies systematically varying the ratio of LLM size to visual encoder capacity and analyzing the correlation with the rate of evidence-ignorance errors.

### Open Question 3
- **Question:** Do gradient-based saliency methods or probing features provide more robust or complementary evidence localization signals compared to the raw attention maps used in VEA?
- **Basis in paper:** [explicit] Appendix B.2 states, "attention may not always fully capture all the internal mechanisms... alternative attribution signals (e.g., gradient-based saliency or probing features) could provide complementary insights."
- **Why unresolved:** The proposed method relies exclusively on attention weights; the utility of other interpretability signals for this specific intervention remains untested.
- **What evidence would resolve it:** Comparative analysis evaluating VEA against analogous interventions based on GradCAM or feature probing on the same VQA datasets.

## Limitations

- The "seeing but not believing" interpretation relies on the assumption that deep-layer attention weights directly reflect internal perception, which is not definitively proven
- Fixed "visual grounding layers" identified on TextVQA may not generalize optimally across all datasets or tasks
- Delegate model approach assumes attention patterns are transferable, which lacks rigorous validation
- Memory leakage issues require using delegate models for certain architectures, adding complexity

## Confidence

- **High Confidence:** The empirical results showing VEA's consistent improvement across eight models and four families (+11.1 EM, +17.3 Token F1). The method's mechanism (attention-based masking) is well-defined and reproducible.
- **Medium Confidence:** The interpretation of the "text-to-image attention shift" as the cause of VQA errors. While supported by layer-wise attention visualizations, this is a correlational observation, not a proven causal mechanism.
- **Low Confidence:** The claim that deep-layer attention perfectly aligns with ground-truth evidence during errors. The paper shows high attribution scores, but doesn't rule out the possibility that the model is "seeing" the wrong thing or that attention is influenced by confounders.

## Next Checks

1. **Causal Intervention Test:** Perform an ablation where attention from non-evidence regions is artificially suppressed in deep layers. If this degrades performance more than suppressing evidence regions, it would support the "seeing but not believing" hypothesis.

2. **Cross-Dataset Layer Profiling:** Re-profile L_VG on DocVQA or SROIE instead of TextVQA. If VEA's performance drops significantly, it would indicate the current L_VG selection is overfit to the profiling dataset.

3. **Attention Attribution Robustness:** Calculate the correlation between VEA's performance gain and the initial AUROC of a model's deep-layer attention. If the correlation is weak, it would suggest VEA is helping for reasons beyond simply amplifying good attention.