---
ver: rpa2
title: 'Landing with the Score: Riemannian Optimization through Denoising'
arxiv_id: '2509.23357'
source_url: https://arxiv.org/abs/2509.23357
tags:
- manifold
- optimization
- data
- gradient
- riemannian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a data-driven framework for Riemannian optimization\
  \ when the manifold is only implicitly available through samples. The core innovation\
  \ is the \"link function\" \u2113\u03C3, which connects the data distribution to\
  \ manifold operations; its gradient and Hessian recover projections to the manifold\
  \ and tangent space as \u03C3\u21920."
---

# Landing with the Score: Riemannian Optimization through Denoising

## Quick Facts
- **arXiv ID**: 2509.23357
- **Source URL**: https://arxiv.org/abs/2509.23357
- **Reference count**: 40
- **Primary result**: Introduces a data-driven framework for Riemannian optimization using learned score functions, with applications to orthogonal groups and data-driven control tasks.

## Executive Summary
This paper introduces a novel framework for Riemannian optimization when the manifold is only implicitly available through samples, using denoising score matching from generative modeling. The core innovation is the "link function" ℓ_σ that connects the data distribution to manifold operations, with its gradient and Hessian recovering projections to the manifold and tangent space as σ→0. By leveraging diffusion model score functions, the authors recover these operations from data without explicit manifold knowledge. The framework enables Riemannian optimization in applications like generative AI and data-driven design where explicit manifold representations are unavailable.

## Method Summary
The method connects score functions from diffusion models to Riemannian geometry through the link function ℓ_σ. As the noise level σ decreases, the gradient and Hessian of ℓ_σ converge to the orthogonal projection onto the manifold M and its tangent space T_x M, respectively. This allows recovery of manifold operations (projection and tangent space projection) directly from data samples without explicit manifold knowledge. Two algorithms are proposed: Denoising Landing Flow (DLF) for finding points on the manifold, and Denoising Riemannian Gradient Descent (DRGD) for optimization on the manifold. Both algorithms use the score function as a surrogate for manifold operations, enabling data-driven Riemannian optimization.

## Key Results
- Successfully recovers manifold projections from data samples using learned score functions
- Demonstrates convergence of DLF and DRGD algorithms with non-asymptotic guarantees for approximate optimality and feasibility
- Achieves lower objective values than available training samples on Brockett cost minimization for orthogonal groups
- Successfully tracks reference trajectories in data-driven control tasks for pendulum and unicycle systems

## Why This Works (Mechanism)
The framework works by establishing a precise connection between denoising score matching and Riemannian geometry. The link function ℓ_σ serves as a bridge: its gradient provides the projection to the manifold, while its Hessian gives the projection to the tangent space. As the noise level σ approaches zero, these approximations become exact, enabling recovery of manifold operations from data. The score function, trained via denoising score matching, acts as a learned oracle for these geometric operations, making Riemannian optimization possible without explicit manifold representation.

## Foundational Learning

**Riemannian Geometry Basics**: Understanding manifolds, tangent spaces, and projections is essential for grasping how the score function approximates geometric operations. Quick check: Verify that projection operators satisfy π^2 = π and P^2 = P.

**Diffusion Models and Score Matching**: Knowledge of how score functions are learned through denoising score matching is crucial for understanding the data-driven aspect. Quick check: Confirm that score matching minimizes E[∥∇_x log p_σ(x) - s_σ(x)∥²].

**Optimization on Manifolds**: Familiarity with Riemannian gradient descent and its convergence properties is needed to appreciate the algorithmic contributions. Quick check: Ensure understanding of retraction and vector transport operations.

## Architecture Onboarding

**Component Map**: Data samples → Score Network Training → Learned Score Function → Manifold Operations → Riemannian Optimization

**Critical Path**: The critical path flows from generating manifold samples, training the score network via denoising score matching, using the learned score to approximate manifold projections, and finally applying DLF or DRGD for optimization.

**Design Tradeoffs**: The framework trades explicit manifold knowledge for data-driven approximation, requiring sufficient samples and accurate score estimation. Higher noise levels in score training provide stability but reduce approximation accuracy.

**Failure Signatures**: Optimization iterates diverging from the manifold, poor score function approximation at small σ, or slow convergence in DRGD indicate implementation issues.

**First Experiments**: 1) Verify projection recovery on synthetic manifolds by comparing s(x) to true π(x) on held-out points. 2) Test DLF convergence on Brockett cost for increasing n. 3) Compare DRGD performance with different step sizes on control tracking tasks.

## Open Questions the Paper Calls Out

**Open Question 1**: Can non-asymptotic convergence guarantees be established for the proposed algorithms when the denoising score is trained with a non-zero L² error rather than the uniform L^∞ approximation assumption? The current analysis requires strong L^∞-approximation assumptions while standard score matching minimizes L² error.

**Open Question 2**: How can the approximate manifold operations derived from the score function be integrated into second-order Riemannian optimization algorithms, such as Newton or trust region methods? The paper currently focuses only on first-order methods.

**Open Question 3**: How does the performance and stability of the proposed denoising flow degrade when applied to non-compact manifolds, which contradict the compactness assumption of Theorem 1? The theoretical guarantees rely on compactness while some applications involve non-compact manifolds.

**Open Question 4**: How can the convergence speed of Denoising Riemannian Gradient Descent (DRGD) be accelerated while maintaining feasibility constraints? Current experiments required large iteration budgets (e.g., 2500–3000 steps).

## Limitations

- Limited experimental validation to relatively simple manifold structures and low-dimensional control problems
- Non-asymptotic convergence guarantees rely on idealized assumptions about score function quality and Lipschitz constants
- Performance ultimately bounded by quality of underlying generative model, which is not extensively characterized
- Optimization methods assume access to noiseless score gradients, while real implementations use stochastic estimates

## Confidence

**High**: The theoretical framework connecting score functions to manifold projections is mathematically sound and the basic algorithmic mechanisms are correct.

**Medium**: The empirical results on orthogonal groups and simple control tasks demonstrate feasibility but are limited in scope and dimensionality.

**Low**: Claims about broad applicability to "generative AI and data-driven design" exceed what the current experimental validation supports.

## Next Checks

1. **Scalability test**: Apply the framework to higher-dimensional manifolds (e.g., Stiefel or Grassmannian manifolds) with dimension n≥50 to assess computational and statistical scaling.

2. **Score network sensitivity**: Systematically vary score network architecture, training data size, and noise levels σ to quantify the impact on optimization performance and identify failure modes.

3. **Real-world data manifold**: Test on a natural data manifold (e.g., pose manifolds from 3D shape data or neural network loss landscapes) to validate applicability beyond synthetic examples.