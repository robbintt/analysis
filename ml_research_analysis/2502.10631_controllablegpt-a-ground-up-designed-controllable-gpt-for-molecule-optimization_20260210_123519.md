---
ver: rpa2
title: 'ControllableGPT: A Ground-Up Designed Controllable GPT for Molecule Optimization'
arxiv_id: '2502.10631'
source_url: https://arxiv.org/abs/2502.10631
tags:
- mask
- masked
- molecule
- original
- controllable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces CONTROLLABLE GPT, a bidirectional causally
  masked seq2seq GPT model designed for controllable generation in drug optimization.
  It combines the strengths of masked language modeling, causal language modeling,
  and sequence-to-sequence modeling into a unified framework that allows precise control
  over sequence expansion, contraction, and mutation while preserving specified structures.
---

# ControllableGPT: A Ground-Up Designed Controllable GPT for Molecule Optimization

## Quick Facts
- arXiv ID: 2502.10631
- Source URL: https://arxiv.org/abs/2502.10631
- Reference count: 40
- Achieves higher average normalized rewards and Tanimoto similarity than competing baselines on viral and cancer benchmarks

## Executive Summary
ControllableGPT is a bidirectional causally masked seq2seq GPT model designed specifically for controllable generation in drug optimization. The model combines masked language modeling, causal language modeling, and sequence-to-sequence modeling into a unified framework, enabling precise control over sequence expansion, contraction, and mutation while preserving specified molecular structures. Using a novel Causally Masked Seq2seq (CMS) objective and a three-phase training approach, ControllableGPT demonstrates superior performance in improving drug properties compared to competing baselines, achieving higher average normalized rewards and Tanimoto similarity scores while reducing toxicity.

## Method Summary
ControllableGPT uses a novel Causally Masked Seq2seq (CMS) objective that combines bidirectional context during encoding with autoregressive control during generation. The model trains in three progressive phases: Phase 1 uses causal language modeling (CLM) for molecular representation learning, Phase 2 adds causally masked objectives for expansion control, and Phase 3 integrates sequence-to-sequence spans for mutation and contraction. Mask tokens include explicit size hints (e.g., <mask_i:n>) that guide generation length. At inference, the model generates replacement spans autoregressively, which are then manually reintegrated into the original sequence.

## Key Results
- Outperforms competing baselines on viral and cancer benchmarks
- Achieves higher average normalized rewards and Tanimoto similarity scores
- Demonstrates ability to preserve beneficial molecular structures while improving drug properties and reducing toxicity

## Why This Works (Mechanism)

### Mechanism 1: Causally Masked Seq2seq (CMS) Objective
The CMS objective enables bidirectional context during encoding while maintaining autoregressive control during generation by repositioning masked spans to the sequence end. Given a SMILES string, randomly selected spans are replaced with size-hinted mask tokens and repositioned to the end of the sequence. The model learns to generate replacement tokens autoregressively while the unmasked prefix retains bidirectional attention context. Core assumption: Repositioning masked spans does not destroy structural semantics needed for valid molecule generation.

### Mechanism 2: Size-Hint Embedded Mask Tokens
Embedding explicit length targets within mask tokens enables controllable generation at specified lengths. Mask tokens are formatted as <mask_i:n> where n specifies the target generation length. The model learns to condition its generation on this numerical hint during training. Core assumption: The numerical hint can be learned as a conditional signal without confusing it with SMILES numerical syntax.

### Mechanism 3: Three-Phase Progressive Pre-training
Incremental training builds molecular understanding before adding controllable generation capabilities. Phase 1 trains with CLM for molecular representation learning. Phase 2 adds causally masked objectives for expansion control. Phase 3 integrates seq2seq spans for mutation and contraction. Core assumption: Each phase establishes necessary foundations without catastrophic forgetting of prior capabilities.

## Foundational Learning

- **SMILES Notation**: Why needed here: The entire model operates on SMILES string representations of molecules; understanding token-level structure (atoms, bonds, rings, stereochemistry markers) is essential for interpreting mask positions and generated outputs. Quick check question: Can you identify which tokens in "CC[C@@H]1C(=O)" represent a chiral center and a carbonyl group?

- **Causal vs. Masked Language Modeling**: Why needed here: The CMS objective explicitly merges these paradigms; understanding that CLM is unidirectional (left-to-right only) while MLM uses bidirectional context explains why both are combined. Quick check question: Why can't standard GPT (CLM-only) use future context when generating a token at position i?

- **Sequence-to-Sequence Conditioning**: Why needed here: The s2s spans (<s2s_i_Lt:...>) enable mutation conditioned on existing subsequences; understanding encoder-decoder patterns helps explain how the model transforms input spans to target outputs. Quick check question: In a standard seq2seq model, what information flows from encoder to decoder, and how does CMS modify this with bidirectional context?

## Architecture Onboarding

- **Component map**: Tokenizer (BPE) -> GPT-2-style transformer backbone -> Three-phase training pipeline (CLM → πCM → πCMS) -> Prompt construction with mask/s2s tokens -> Autoregressive generation -> Manual span reintegration

- **Critical path**: 1) Construct training corpus with repositioned masked spans 2) Train three phases sequentially with specified mask configurations 3) At inference, construct prompts with mask/s2s tokens at desired modification positions 4) Generate and manually reintegrate spans into original SMILES

- **Design tradeoffs**: Manual reintegration vs. end-to-end (simpler training but adds post-processing step), size hints in token vs. separate embedding (avoids model confusion but requires tokenizer handling), three-phase vs. single-phase (may improve stability but increases training complexity)

- **Failure signatures**: Invalid SMILES output (~90% validity only when generated length is within 5-10 tokens of masked length), generated length not matching size hint, beneficial substructures not preserved (low Tanimoto similarity), catastrophic forgetting of molecular validity after Phase 3

- **First 3 experiments**: 1) Validity vs. mask length test (generate with varying mask lengths and measure SMILES validity rates) 2) Size hint adherence test (generate with fixed mask positions but varying size hints; measure correlation between hint and actual generated length) 3) Ablation on phases (train only Phase 1+2 and compare to full model on contraction/expansion tasks to isolate seq2seq contribution)

## Open Questions the Paper Calls Out
### Open Question 1
Can the ControllableGPT framework be effectively transferred to non-molecular domains, such as protein engineering or natural language processing, while maintaining its capability for expansion, contraction, and mutation? Basis: Authors state "we encourage applying CONTROLLABLE GPT in fields beyond our current research scope." Why unresolved: The model's architecture and training strategy were designed specifically for SMILES representation using biological evolution metaphors. What evidence would resolve it: Successful application to a distinct domain with quantitative results showing comparable controllability and performance improvements.

### Open Question 2
Can the generation process be fully automated to replace the current requirement for manual reintegration of generated segments? Basis: Figures 4 and 5 captions describe the process where segments generated by the model are "manually repositioned" or "manually added" back into the molecule structure. Why unresolved: The current workflow requires human intervention to place generated tokens back into specified mask locations, creating a bottleneck for automated high-throughput optimization. What evidence would resolve it: An automated decoding algorithm capable of filling the masked spans in-place that achieves the same structural validity and rewards as the manual process.

### Open Question 3
Can the model's optimization targets be refined to improve synthesizability scores without compromising the gains observed in docking and drug-likeness? Basis: Table 1 shows ControllableGPT underperforms compared to REINVENT 4 baselines in Synthesizability (scoring ~2.75 vs ~2.68, where lower is better). Why unresolved: The paper focuses on "average normalized reward," but synthesizability metric lags behind state-of-the-art, suggesting potential trade-offs in current reward weighting or model biases. What evidence would resolve it: Experiments with adjusted reward functions resulting in synthesizability scores statistically significantly lower than current REINVENT 4 baselines while maintaining high Tanimoto similarity scores.

## Limitations
- Limited evaluation scope with lack of comparison against state-of-the-art graph-based molecular optimization methods
- Manual span reintegration creates a non-differentiable bottleneck that prevents end-to-end optimization
- Token-level control shows ~90% validity only when generated tokens are within 5-10 of the masked length hint

## Confidence

**High Confidence (4-5 evidence anchors)**: The CMS objective architecture is correctly implemented and trains successfully through three phases; the model achieves higher average normalized rewards and Tanimoto similarity than baselines on evaluated tasks; the three-phase training approach enables incremental learning of controllable generation capabilities

**Medium Confidence (2-3 evidence anchors)**: Size-hint embedded mask tokens effectively control generation length; causally masked objectives improve controllability over pure causal or masked approaches; the model preserves beneficial molecular substructures while improving drug properties

**Low Confidence (0-1 evidence anchors)**: Performance generalizes beyond the specific viral and cancer benchmarks tested; the model would maintain validity when scaling to larger molecule sizes; the size-hint mechanism would work robustly across diverse molecular scaffolds

## Next Checks
1. **Robustness to Size-Hint Deviation**: Systematically test SMILES validity across a range of mask length hints (e.g., ±20 tokens from target) to quantify the operational limits of the size-hint control mechanism and identify failure patterns.

2. **End-to-End Differentiable Alternative**: Implement a soft-attention based span reintegration mechanism that allows gradient flow through the entire generation process, then compare performance to the current manual approach to assess whether differentiable control improves learning efficiency.

3. **Cross-Scaffold Generalization Test**: Evaluate the model on a held-out set of molecular scaffolds not present in training data, measuring both property improvement and substructure preservation to assess whether learned controllability generalizes beyond memorized patterns.