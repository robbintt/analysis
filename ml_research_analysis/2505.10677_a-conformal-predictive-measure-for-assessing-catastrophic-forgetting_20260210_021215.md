---
ver: rpa2
title: A Conformal Predictive Measure for Assessing Catastrophic Forgetting
arxiv_id: '2505.10677'
source_url: https://arxiv.org/abs/2505.10677
tags:
- tasks
- cpcf
- training
- data
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces the Conformal Prediction Confidence Factor
  (CPCF), a novel metric for assessing catastrophic forgetting (CF) in continual learning.
  CPCF leverages conformal prediction to monitor model confidence on previously learned
  tasks by analyzing the average length of prediction sets.
---

# A Conformal Predictive Measure for Assessing Catastrophic Forgetting

## Quick Facts
- arXiv ID: 2505.10677
- Source URL: https://arxiv.org/abs/2505.10677
- Reference count: 18
- Primary result: CPCF shows strong correlation (0.4996-0.8012) with accuracy on previous tasks across four benchmark datasets

## Executive Summary
This work introduces the Conformal Prediction Confidence Factor (CPCF), a novel metric for quantifying catastrophic forgetting in continual learning. CPCF measures the average length of conformal prediction sets on previously learned tasks—shorter sets indicate higher confidence and better retention, while longer sets suggest increased uncertainty due to forgetting. The method leverages conformal prediction theory to provide distribution-free uncertainty quantification with statistical coverage guarantees. Experimental results on MNIST, CIFAR-10, KMNIST, and FashionMNIST demonstrate that CPCF strongly correlates with actual accuracy degradation on previous tasks, validating its reliability as a forgetting metric. The approach remains effective under various calibration ratios and significance levels, and shows enhanced sensitivity when combined with Elastic Weight Consolidation regularization.

## Method Summary
CPCF quantifies catastrophic forgetting by computing the average length of conformal prediction sets on previously learned tasks after training on new tasks. The method splits training data into effective training and calibration sets, trains a 3-layer MLP incrementally on task sequences, and computes conformal scores as cumulative softmax probabilities needed to include the true label. A quantile threshold derived from calibration scores determines prediction set boundaries. CPCF is calculated as the mean prediction set size across all test samples from previous tasks. The framework supports both standard cross-entropy and EWC-regularized training, with experimental validation across four benchmark datasets using varying calibration ratios and significance levels.

## Key Results
- Strong correlation (0.4996-0.8012) between CPCF and accuracy of previous tasks across all datasets
- CPCF becomes more sensitive to forgetting under EWC regularization, showing higher correlation with accuracy
- Robust performance across calibration ratios (0.05-0.20) with no consistent trend in metric values
- Higher significance levels (α=0.15-0.20) yield stronger correlation with accuracy than lower levels (α=0.05-0.10)

## Why This Works (Mechanism)

### Mechanism 1: Prediction Set Length as Knowledge Retention Proxy
- Claim: The average length of conformal prediction sets on previously learned tasks serves as a reliable proxy for catastrophic forgetting.
- Mechanism: When a model retains knowledge, its softmax distribution on old task data remains peaked (high confidence), resulting in smaller prediction sets. Forgetting flattens distributions, requiring more classes to reach the cumulative probability threshold, producing larger sets.
- Core assumption: Model uncertainty on previous tasks correlates monotonically with actual performance degradation.
- Evidence anchors: [abstract] strong correlation between CPCF and accuracy of previous tasks; [section II.D] shorter sets indicate higher confidence and stronger retention; [corpus] limited direct support but related CP uncertainty work exists

### Mechanism 2: Calibration-Quantile Thresholding for Distribution-Free Uncertainty
- Claim: Computing conformal scores on held-out calibration data and deriving a quantile threshold enables distribution-free, model-agnostic uncertainty quantification.
- Mechanism: For each calibration sample, the conformal score Ei equals the cumulative softmax probability needed to include the true label. The quantile threshold qα captures the (1-α) percentile of these scores, establishing the prediction set boundary with statistical coverage guarantees.
- Core assumption: The calibration and test data are exchangeable (no covariate or concept shift between them).
- Evidence anchors: [section II.B] determine adjusted quantile threshold using computed conformal scores; [section II.C] construct conformal prediction set with cumulative probability mass ≥ threshold; [corpus] weak corpus support, general CP literature establishes foundations

### Mechanism 3: CPCF Sensitivity Amplification Under Regularization
- Claim: CPCF exhibits stronger correlation with accuracy when Elastic Weight Consolidation (EWC) regularization is applied, making it more sensitive to detecting forgetting.
- Mechanism: EWC constrains parameter updates to protect important weights for previous tasks. This creates more structured uncertainty patterns—when forgetting does occur, it manifests as clearer shifts in prediction set lengths rather than diffuse noise, improving signal-to-noise ratio for CPCF.
- Core assumption: Regularization produces more interpretable uncertainty patterns that better reflect actual knowledge degradation.
- Evidence anchors: [abstract] CPCF becomes more sensitive to forgetting under EWC regularization; [section III.E, Table II-III] distance correlation consistently higher for EWC vs standard MLP across all datasets; [corpus] corpus contains related forgetting mitigation work but not this sensitivity-amplification effect

## Foundational Learning

**Concept: Conformal Prediction and Prediction Sets**
- Why needed here: CPCF is built entirely on conformal prediction—understanding how prediction sets differ from point predictions is essential to interpret CPCF values.
- Quick check question: Given a 10-class classifier with α=0.1, what does a prediction set of length 2 vs length 8 tell you about model confidence?

**Concept: Catastrophic Forgetting in Sequential Learning**
- Why needed here: The entire goal of CPCF is to quantify CF; understanding why neural networks forget (weight interference, representational drift) contextualizes why prediction sets capture this.
- Quick check question: After training on task 5, why might accuracy on task 1 drop even though task 1 was "learned successfully" initially?

**Concept: Exchangeability in Statistical Learning**
- Why needed here: CP's coverage guarantees depend on exchangeability; understanding this clarifies when CPCF remains reliable vs when it may fail.
- Quick check question: If your calibration data comes from tasks 1-2 but you evaluate on task 5 test data, what assumption might be violated?

## Architecture Onboarding

**Component map:**
Training Data → Data Splitter → Effective Training → Model Training
                                  → Calibration → Conformal Score Calculator
                                                          ↓
                                              Quantile Threshold Computer ← qα from scores
                                                      ↓
                                              Prediction Set Constructor ← Test data from tasks 1 to j-1
                                                      ↓
                                              CPCF Aggregator → Average set length = CPCF_j

**Critical path:**
1. Train model on task j (using only effective training portion)
2. Compute conformal scores Ei using calibration data from ALL previous tasks (1 to j-1)
3. Derive quantile threshold qα^j from combined calibration scores
4. Construct prediction sets for test samples from tasks 1 to j-1
5. Aggregate: CPCF_j = (1/|test samples|) × Σ|C(xt)|

**Design tradeoffs:**
- **Calibration ratio (0.05-0.20)**: Higher values → more reliable qα estimates but less training data. Paper shows CPCF is robust to this choice (no consistent trend in Table II).
- **Significance level α (0.05-0.20)**: Higher α → larger prediction sets, stronger correlation with aprev (Table III), but potentially less precise signals.
- **Single vs Multi-penalization EWC**: Paper found identical Ω metrics (Table I), chose single-penalization for computational efficiency.

**Failure signatures:**
- CPCF-aprev correlation < 0.3 (indicates metric isn't capturing forgetting)
- Prediction sets uniformly maximal (|C(xt)| ≈ K) across all samples (model confidence collapsed everywhere)
- CPCF insensitive to α changes (suggests calibration failure or model miscalibration)

**First 3 experiments:**
1. **Baseline replication**: Run MNIST with calibration_ratio=0.1, α=0.1, 5 incremental classes. Measure distance correlation between CPCF_j and aprev,j across tasks. Expect correlation ~0.55 (MLP) or ~0.68 (EWC).
2. **α sensitivity test**: Fix calibration_ratio=0.1, vary α ∈ {0.05, 0.10, 0.15, 0.20}. Verify correlation increases with α (as per Table III: MNIST MLP 0.53→0.59, EWC 0.63→0.80).
3. **Regularization comparison**: Train identical architectures with cross-entropy vs EWC loss. Compute CPCF for both; verify EWC shows consistently higher correlation (expect 0.1-0.2 absolute improvement across datasets).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the CPCF framework be effectively adapted for regression tasks in continual learning?
- Basis: [explicit] The paper states the method "can naturally be extended to regression tasks by transforming CP prediction sets into CP interval estimates."
- Why unresolved: The current metric relies on prediction set lengths (classification); adapting this for continuous intervals requires defining a new aggregation logic for the confidence factor.
- What evidence would resolve it: Experiments on sequential regression benchmarks showing a correlation between interval-based CPCF and regression error metrics.

### Open Question 2
- Question: How does CPCF perform when applied to multimodal, imbalanced, or non-stationary data streams?
- Basis: [explicit] The authors identify extending the approach to "diverse and complex data distributions" as a specific goal for future work.
- Why unresolved: Standard CP assumes exchangeability, which is often violated in non-stationary streams or highly imbalanced scenarios, potentially degrading calibration.
- What evidence would resolve it: Evaluating CPCF on streaming datasets with concept drift or modalities beyond simple image classification.

### Open Question 3
- Question: Does the correlation between CPCF and accuracy persist in modern, deep architectures like Transformers?
- Basis: [inferred] The experimental scope is limited to a simple three-layer Multi-Layer Perceptron (MLP).
- Why unresolved: Uncertainty dynamics and overconfidence issues in deep, over-parameterized networks (e.g., Vision Transformers) may differ significantly from shallow MLPs.
- What evidence would resolve it: Replicating the analysis using deep architectures on complex benchmarks like Split CIFAR-100 or TinyImageNet.

## Limitations

- Assumes exchangeability between calibration and test data, which may not hold under concept drift or distribution shift
- Effectiveness depends on proper model calibration—poorly calibrated models may produce unreliable prediction sets
- Computationally heavier than direct accuracy metrics due to conformal score computation across all previous tasks
- Limited to classification settings where prediction sets are meaningful

## Confidence

**Major Claim Confidence**:
- CPCF reliably correlates with forgetting (High): Strong experimental support across multiple datasets and regularization methods
- Prediction set length proxies confidence (Medium): Mechanistically sound but depends on calibration quality
- CPCF is robust to calibration ratio/α choices (Medium): Supported in experiments but may vary across problem domains
- EWC amplifies CPCF sensitivity (Medium): Observed in experiments but mechanism needs further validation

## Next Checks

1. Test CPCF on non-i.i.d. task sequences (e.g., with concept drift) to verify exchangeability assumption holds
2. Compare CPCF against established forgetting metrics (e.g., weight importance changes, representation similarity) on the same datasets
3. Evaluate CPCF sensitivity on real-world continual learning scenarios beyond benchmark datasets (e.g., class-incremental image streams)