---
ver: rpa2
title: LLMs are Bayesian, in Expectation, not in Realization
arxiv_id: '2507.11768'
source_url: https://arxiv.org/abs/2507.11768
tags:
- length
- theoretical
- martingale
- bayesian
- log2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper resolves a theoretical paradox in large language models:\
  \ transformers violate martingale properties (a requirement for Bayesian inference\
  \ on exchangeable data) yet achieve Bayesian-level compression efficiency. The authors\
  \ show that positional encodings fundamentally alter the learning problem\u2014\
  transformers minimize expected conditional Kolmogorov complexity over permutations\
  \ rather than permutation-invariant complexity."
---

# LLMs are Bayesian, in Expectation, not in Realization

## Quick Facts
- arXiv ID: 2507.11768
- Source URL: https://arxiv.org/abs/2507.11768
- Reference count: 40
- One-line primary result: Transformers violate martingale properties yet achieve Bayesian-level compression efficiency through positional encodings that alter the learning problem

## Executive Summary
This paper resolves a fundamental paradox in large language models: transformers simultaneously violate martingale properties (required for Bayesian inference on exchangeable data) while achieving near-optimal compression efficiency. The authors demonstrate that positional encodings fundamentally change the learning problem from minimizing permutation-invariant complexity to minimizing expected conditional complexity over orderings. This theoretical framework explains why transformers can achieve MDL optimality with excess risk O(n^-1/2) despite violating exchangeability for any specific ordering. The paper provides four key theoretical results and empirical validation on GPT-3 confirming the Î˜(log n/n) scaling of martingale violations.

## Method Summary
The authors analyze transformer behavior on exchangeable data through information-theoretic and statistical frameworks. They quantify martingale violations induced by positional encodings using Lipschitz continuity arguments, prove MDL optimality by distinguishing between permutation-invariant and expected conditional complexity, and derive optimal chain-of-thought length through balancing reasoning cost against positional degradation. Empirical validation uses GPT-3 API to measure martingale gaps on balanced binary sequences, applying FFT-based debiasing to remove RoPE periodicity artifacts. The study employs 19,000+ API calls across 100 sequences per length for n âˆˆ {10, 12, ..., 198}.

## Key Results
- Transformers exhibit martingale violations scaling as Î˜(log n/n) due to positional encodings
- Despite violating exchangeability, transformers achieve MDL optimality with excess risk O(n^-1/2) in expectation over orderings
- Optimal chain-of-thought length scales as k* = Î˜(âˆšn log(1/Îµ)) with explicit constants
- GPT-3 achieves 99% of theoretical compression efficiency within just 20 examples

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Positional encodings induce martingale violations scaling as Î˜(log n/n) in transformers processing exchangeable data.
- **Mechanism:** Positional encodings break permutation invariance by making predictions depend on token ordering. For sequences with identical sufficient statistics (e.g., same number of 1s in a binary sequence), different orderings produce different positional embedding distances, causing prediction variance. The log n factor emerges from the expected distance between random permutations in permutation space.
- **Core assumption:** The transformer's forward pass is L-Lipschitz with respect to input embeddings, and positional encoding variance is finite.
- **Evidence anchors:** [abstract]: "positional encodings induce martingale violations of order Î˜(log n/n)"; [Section 3.2, Theorem 3.4]: Provides the bound âˆ†n â‰¤ (LÂ²f ÏƒÂ²PE / 2) Â· (log n / n) + O(n^-3/2); [Section 5.2.1]: Empirical validation on GPT-3 shows RÂ² = 0.759 for log n/n model vs. RÂ² < 0.3 for 1/n model.

### Mechanism 2
- **Claim:** Transformers achieve Minimum Description Length (MDL) optimality with excess risk O(n^-1/2) when averaged over orderings, despite violating martingale properties for any specific ordering.
- **Mechanism:** Transformers minimize expected conditional Kolmogorov complexity E_Ï€[K(X|Ï€)] rather than permutation-invariant complexity K(X). Attention mechanisms compute running sufficient statistics (counts), and MLP layers approximate posterior moments. The key insight is that optimality holds "in expectation over orderings, not for any specific ordering."
- **Core assumption:** Data sequences are i.i.d. from a distribution with finite sufficient statistics; the model has learned to approximate counting operations through attention.
- **Evidence anchors:** [abstract]: "transformers achieve information-theoretic optimality with excess risk O(n^-1/2) in expectation over orderings"; [Section 3.3, Theorem 3.7]: E_{X,Ï€}[MDL_n(T_Î¸*, X_Ï€(1:n))] = nH(p) + O(âˆš(n log n)); [Section 5.3]: "GPT-3 achieves 99% of optimal compression efficiency within just 20 examples".

### Mechanism 3
- **Claim:** Optimal chain-of-thought length scales as k* = Î˜(âˆšn log(1/Îµ)) with explicit constants derivable from model architecture.
- **Mechanism:** The âˆšn scaling emerges from balancing three factors: (1) reasoning cost kÂ·H_CoT (linear in k), (2) prediction benefit Î± log(1 + k/kâ‚€) (logarithmic), and (3) positional penalty Î²Â·k log(n+k)/(n+k). The square root represents the optimal trade-off between compression benefit and positional degradation. Additionally, an incompleteness theorem proves CoT is theoretically necessary for functions exceeding the model's parameter complexity budget.
- **Core assumption:** The chain-of-thought process is Ï†-mixing with exponential decay; the benefit function has bounded second derivative.
- **Evidence anchors:** [abstract]: "optimal chain-of-thought length as k* = Î˜(âˆšn log(1/Îµ)) with explicit constants"; [Section 4.3, Theorem 4.5]: Provides full formula k* = âˆš(Î±n / (H_CoT(B(0)-B_opt))) Â· logâ‚‚(1/Îµ) Â· (1 + Î¾_n); [Section 4.6, Theorem 4.8]: Incompleteness theoremâ€”transformers with H bits cannot compute predicates with K(Ï€) > H without CoT.

## Foundational Learning

- **Concept: Martingale property and exchangeability**
  - **Why needed here:** The central paradox is that transformers violate martingale properties (a mathematical consequence of Bayesian updating on exchangeable data) while achieving Bayesian-level compression. Understanding this requires grasping that exchangeability means P(xâ‚...xâ‚™) = P(x_Ï€(â‚)...x_Ï€(â‚™)) for any permutation Ï€, and that Bayesian posteriors on exchangeable data must satisfy E[f(X_{n+1})|Xâ‚...Xâ‚™] = E[f(X_{n+1})|X_Ï€(â‚)...X_Ï€(â‚™)].
  - **Quick check question:** If a model assigns different probabilities to sequences (0,1,0,1) and (1,0,1,0), does it violate exchangeability? Does it necessarily violate the martingale property?

- **Concept: Kolmogorov complexity and MDL principle**
  - **Why needed here:** The paper's resolution hinges on distinguishing K(X) (permutation-invariant complexity) from E_Ï€[K(X|Ï€)] (expected conditional complexity given ordering). MDL provides the information-theoretic foundation showing why transformers achieve near-optimal compression despite violating exchangeability.
  - **Quick check question:** Why does minimizing E_Ï€[K(X|Ï€)] allow for better finite-sample performance than minimizing K(X) when the model has access to positional information?

- **Concept: Sufficient statistics in Bayesian inference**
  - **Why needed here:** The paper proves transformers learn implicit posterior representations "in the space of sufficient statistics"â€”attention heads compute counts S_n = Î£xáµ¢, which are sufficient statistics for Bernoulli sequences. The MLP approximates posterior moments (e.g., Beta distribution parameters) from these statistics.
  - **Quick check question:** For Bernoulli(p) sequences, why is the count S_n = Î£xáµ¢ a sufficient statistic, and what does this imply about what the transformer needs to compute?

## Architecture Onboarding

- **Component map:** Input tokens â†’ Embedding layer â†’ + Positional Encoding (source of martingale violations) â†’ Multi-head Attention (computes sufficient statistics via counting heads) â†’ MLP layers (approximate posterior moments from aggregated statistics) â†’ Output layer (sigmoid for binary prediction)
- **Critical path:** 1. Understand how positional encodings inject ordering information (sinusoidal, learned, RoPE) 2. Trace how attention weights can implement counting: Î±^count_t,i â‰ˆ ðŸ™[xáµ¢=1]/S_t 3. Map MLP computation to posterior moment approximation (Beta distribution parameters) 4. Connect to Theorem 3.4 proof showing how positional variance propagates to prediction variance
- **Design tradeoffs:** Position-awareness vs. exchangeability: More expressive positional encodings increase martingale violations but improve sequential modeling; Model depth vs. mixing time: Deeper models mix more slowly (Ï closer to 1), affecting CoT entropy estimation; Parameter budget H vs. computable functions: Functions with K(Ï€) > H require external chain-of-thought
- **Failure signatures:** Periodic artifacts at positions matching RoPE period (64 in GPT-3) â†’ requires debiasing via harmonic fitting; Variance in predictions for sequences with identical sufficient statistics â†’ confirms positional encoding effects; Poor calibration on short contexts (n < 20) â†’ martingale violations are larger at O(log n/n)
- **First 3 experiments:** 1. Martingale violation scaling test: Generate balanced binary sequences with n âˆˆ {10, 20, ..., 200}, measure |log P(x_n|xâ‚:nâ‚‹â‚) - log P(x_n|xâ‚:nâ‚‹â‚‚)|, fit both log(n)/n and 1/n models. Expect RÂ² > 0.7 for log(n)/n model. 2. Permutation averaging validation: For fixed sequence, sample k âˆˆ {1, 5, 10, 20, 50} random permutations, average predictions, measure variance reduction. Expect Ïƒ âˆ k^-0.5 scaling. 3. Sufficient statistic conditioning: Compare prediction variance across sequences with identical S_n but different orderings. Apply sufficient statistic conditioning (condition on count) and measure bias reduction. Expect ~85% reduction per Section 3.6.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do the theoretical bounds on optimal chain-of-thought length (k* = Î˜(âˆšn log(1/Îµ))) hold empirically across model scales and task types?
- **Basis in paper:** [explicit] "The empirical validation of the optimal chain-of-thought bounds derived in Section 4 is deferred to follow-up work, as it requires extensive computational resources and access to multiple model scales."
- **Why unresolved:** Validating requires 100-1000Ã— the computational resources already expended, and access to multiple model scales not available in current study.
- **What evidence would resolve it:** Experiments measuring performance vs. chain length across varying context lengths n, error tolerances Îµ, and model sizes; verification that empirical k* matches âˆšn scaling with predicted constants.

### Open Question 2
- **Question:** How do martingale violations scale with model sizeâ€”do larger models become more or less exchangeable?
- **Basis in paper:** [explicit] "Larger models may better approximate exchangeable behavior through increased capacity to model complex interactions, or they may exhibit stronger positional biases due to their ability to memorize fine-grained patterns. Understanding these scaling laws is crucial for predicting the behavior of future, more powerful systems."
- **Why unresolved:** The paper only validates on GPT-3 (text-davinci-002); systematic investigation across model scales remains unexplored.
- **What evidence would resolve it:** Measurements of Î˜(log n/n) violation magnitudes across model families and sizes, correlating violation coefficients with model dimension d, depth L, and parameter count.

### Open Question 3
- **Question:** Can position encodings be redesigned to reduce martingale violations below Î˜(log n/n) while preserving sequential modeling capability?
- **Basis in paper:** [explicit] "Can we design encodings that achieve smaller martingale gaps without sacrificing sequential modeling capacity? This optimization problem sits at the intersection of architecture design and statistical theory."
- **Why unresolved:** Current work characterizes existing encodings (sinusoidal, RoPE, ALiBi) but does not propose or test novel designs optimizing the expressiveness-exchangeability trade-off.
- **What evidence would resolve it:** Novel position encoding schemes with theoretically bounded martingale violations smaller than Î˜(log n/n), validated on both synthetic (exchangeability tests) and language modeling benchmarks.

## Limitations

- Theoretical framework relies on Lipschitz continuity assumption for transformers that may not hold for deeper networks where gradients become sparse
- Finite positional encoding variance assumption relies on bounded sinusoidal ranges, but learned positional embeddings could violate this
- i.i.d. data assumption is violated in natural language, though paper claims bounded violations for "sufficiently local" dependencies

## Confidence

**High Confidence (Likelihood > 90%):** The martingale violation scaling as Î˜(log n/n) is well-supported by both theoretical derivation and empirical validation (RÂ² = 0.759 for log n/n model vs. RÂ² < 0.3 for 1/n model). The MDL optimality result with O(n^-1/2) excess risk follows from standard information-theoretic arguments.

**Medium Confidence (Likelihood 70-90%):** The implicit Bayesian representation through sufficient statistics (attention heads computing counts) is mechanistically plausible but relies on specific architectural choices. The chain-of-thought optimality formula k* = Î˜(âˆšn log(1/Îµ)) follows from the derived optimization problem but assumes the benefit function's logarithmic form holds empirically.

**Low Confidence (Likelihood < 70%):** The incompleteness theorem's implications for transformer limitations may be overly pessimistic given that transformers can learn hierarchical representations beyond simple counting. The debiasing method's effectiveness across different positional encoding schemes (RoPE vs. learned embeddings) is untested.

## Next Checks

1. **Architecture-specific martingale violations:** Test martingale gap scaling across different positional encoding schemes (sinusoidal, learned, RoPE) and model depths. Hypothesis: Learned positional embeddings have lower ÏƒÂ²_PE, reducing violations by factor 2-3 compared to sinusoidal encodings.

2. **Natural language extension:** Measure martingale violations on real text sequences using perplexity variance across permutations of identical contexts (e.g., "the cat sat on the mat" vs. "the mat on sat cat the"). Expected result: Violations persist but with O(1) vs. Î˜(log n) dependence due to local dependency structure.

3. **Chain-of-thought generalization:** Test CoT length scaling on tasks requiring tree-structured reasoning (e.g., logic puzzles with branching) rather than linear reasoning. Hypothesis: k* underestimates required computation by factor 3-5 for tasks requiring non-linear reasoning structures.