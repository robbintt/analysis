---
ver: rpa2
title: 'GUI Knowledge Bench: Revealing the Knowledge Gap Behind VLM Failures in GUI
  Tasks'
arxiv_id: '2510.26098'
source_url: https://arxiv.org/abs/2510.26098
tags:
- knowledge
- task
- type
- question
- interface
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces GUI Knowledge Bench, a benchmark designed
  to systematically evaluate the GUI knowledge encoded in vision-language models (VLMs).
  The authors categorize GUI knowledge into three dimensions: interface knowledge
  (recognizing widget functions, layout semantics, and system states), interaction
  knowledge (understanding GUI interaction conventions and effects), and procedure
  knowledge (knowing task objectives and workflow sequences).'
---

# GUI Knowledge Bench: Revealing the Knowledge Gap Behind VLM Failures in GUI Tasks

## Quick Facts
- arXiv ID: 2510.26098
- Source URL: https://arxiv.org/abs/2510.26098
- Reference count: 40
- Key outcome: Introduces GUI Knowledge Bench to evaluate GUI knowledge in VLMs across three dimensions: interface, interaction, and procedure knowledge

## Executive Summary
This paper introduces GUI Knowledge Bench, a benchmark designed to systematically evaluate the GUI knowledge encoded in vision-language models (VLMs). The authors categorize GUI knowledge into three dimensions: interface knowledge (recognizing widget functions, layout semantics, and system states), interaction knowledge (understanding GUI interaction conventions and effects), and procedure knowledge (knowing task objectives and workflow sequences). The benchmark consists of 3,483 multiple-choice and yes/no questions across six platforms (Web, Android, MacOS, Windows, Linux, IOS) and 292 applications. Evaluation results show that current VLMs generally understand widget functions but struggle with perceiving system states, predicting GUI interactions, and verifying task completion progress.

## Method Summary
The authors developed a comprehensive taxonomy of GUI knowledge organized into three dimensions: interface knowledge (recognizing widget functions, layout semantics, and system states), interaction knowledge (understanding GUI interaction conventions and effects), and procedure knowledge (knowing task objectives and workflow sequences). They manually annotated questions across six platforms and 292 applications, creating 3,483 multiple-choice and yes/no questions. The benchmark evaluates VLMs' ability to answer questions about widget functions, system states, interaction outcomes, and task completion verification. The authors tested various VLMs and demonstrated that GUI knowledge is a necessary condition for successful real-world GUI task execution.

## Key Results
- VLMs generally understand widget functions but struggle with perceiving system states and predicting GUI interactions
- Current VLMs show significant deficits in verifying task completion progress and understanding interaction conventions
- GUI knowledge gaps identified through the benchmark correlate with failures in actual GUI task completion
- The benchmark reveals platform-specific strengths and weaknesses in GUI knowledge across VLMs

## Why This Works (Mechanism)
The benchmark works by isolating GUI-specific knowledge from general visual understanding, creating a structured framework to identify precise knowledge gaps that cause VLM failures in GUI tasks. By focusing on three distinct dimensions of GUI knowledge, the benchmark can pinpoint whether failures stem from interface recognition, interaction prediction, or procedural understanding.

## Foundational Learning
- GUI Knowledge Taxonomy: Three-dimensional framework (interface, interaction, procedure) needed to systematically assess what VLMs understand about GUIs
- Multiple-choice evaluation methodology: Allows precise measurement of knowledge gaps while maintaining consistency across diverse platforms
- Platform-specific knowledge assessment: Reveals how GUI knowledge varies across different operating systems and application types

## Architecture Onboarding
- Component map: GUI Knowledge Taxonomy -> Question Generation -> VLM Evaluation -> Performance Analysis
- Critical path: Question annotation → benchmark creation → VLM testing → gap analysis → insights for GUI agent improvement
- Design tradeoffs: Manual annotation ensures quality but limits scalability; multiple-choice format enables consistent evaluation but may not capture full task complexity
- Failure signatures: Consistent failure patterns across VLMs in system state perception, interaction prediction, and task verification
- First experiments: 1) Test VLMs on widget function recognition, 2) Evaluate system state perception capabilities, 3) Assess interaction prediction accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- Manual annotation process limits scalability and may introduce subjectivity in question formulation
- Three-dimensional taxonomy may not capture all nuances of GUI knowledge, particularly emergent behaviors in complex multi-modal interfaces
- Focus on multiple-choice and yes/no questions may not fully represent the complexity of real-world GUI tasks

## Confidence
- High: VLMs generally understand widget functions but struggle with system states and interaction predictions
- Medium: GUI knowledge is a necessary condition for successful GUI task execution
- Medium: Benchmark coverage of six platforms and 292 applications provides good breadth

## Next Checks
1. Test the benchmark with a larger and more diverse set of VLMs, including models specifically trained for GUI tasks
2. Conduct ablation studies to isolate the impact of different GUI knowledge dimensions on actual task completion rates
3. Evaluate the benchmark's effectiveness by correlating knowledge gaps with actual performance deficits in GUI task completion benchmarks