---
ver: rpa2
title: 'Learning While Staying Curious: Entropy-Preserving Supervised Fine-Tuning
  via Adaptive Self-Distillation for Large Reasoning Models'
arxiv_id: '2602.02244'
source_url: https://arxiv.org/abs/2602.02244
tags:
- arxiv
- entropy
- preprint
- zhang
- exploration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the problem of entropy collapse during supervised
  fine-tuning (SFT) in large reasoning models, which limits exploration capability
  and constrains downstream reinforcement learning (RL) performance. The proposed
  method, CurioSFT, uses entropy-preserving SFT through two key components: Self-Exploratory
  Distillation, which aligns the model with a self-generated, temperature-scaled teacher
  distribution to encourage exploration within the model''s capability, and Entropy-Guided
  Temperature Selection, which dynamically adjusts token-level temperatures to selectively
  amplify exploration at reasoning tokens while stabilizing factual tokens.'
---

# Learning While Staying Curious: Entropy-Preserving Supervised Fine-Tuning via Adaptive Self-Distillation for Large Reasoning Models

## Quick Facts
- **arXiv ID:** 2602.02244
- **Source URL:** https://arxiv.org/abs/2602.02244
- **Reference count:** 26
- **Key outcome:** CurioSFT improves SFT accuracy by +2.5 ID and +2.9 OOD points, with +5.0 point RL-stage gains

## Executive Summary
This paper addresses entropy collapse during supervised fine-tuning (SFT) in large reasoning models, which limits exploration capability and constrains downstream reinforcement learning (RL) performance. The proposed method, CurioSFT, uses entropy-preserving SFT through two key components: Self-Exploratory Distillation, which aligns the model with a self-generated, temperature-scaled teacher distribution to encourage exploration within the model's capability, and Entropy-Guided Temperature Selection, which dynamically adjusts token-level temperatures to selectively amplify exploration at reasoning tokens while stabilizing factual tokens. Experimental results show that CurioSFT outperforms vanilla SFT by 2.5 points on in-distribution tasks and 2.9 points on out-of-distribution tasks during SFT, and successfully translates preserved exploration into a 5.0-point improvement in the RL stage. The method demonstrates robustness across different model architectures and hyperparameter settings.

## Method Summary
CurioSFT addresses entropy collapse in SFT through two complementary mechanisms. First, Self-Exploratory Distillation aligns the student model with a teacher model (synchronized via EMA every 5 steps) using K2 loss to a temperature-scaled distribution, encouraging exploration within the model's capability. Second, Entropy-Guided Temperature Selection computes token-wise temperatures via binary search to amplify exploration for reasoning tokens while stabilizing factual tokens. The total loss combines standard SFT loss with the adaptive distillation term. The method is evaluated on the OpenR1-Math-46K dataset using Qwen2.5-Math-7B, showing consistent improvements across both in-distribution and out-of-distribution benchmarks.

## Key Results
- +2.5 accuracy@8 points on in-distribution tasks during SFT
- +2.9 accuracy@8 points on out-of-distribution tasks during SFT
- +5.0 point improvement in RL stage, successfully translating preserved exploration
- Demonstrated robustness across different model architectures (Qwen2.5-Coder-6.7B)

## Why This Works (Mechanism)
Entropy collapse in standard SFT leads models to prematurely converge to deterministic, low-entropy distributions that limit exploration capability needed for downstream RL. By preserving entropy through adaptive self-distillation, CurioSFT maintains a distribution over possible solutions that enables more effective exploration during RL. The temperature-guided mechanism selectively amplifies uncertainty at reasoning tokens while preserving factual knowledge, creating a balance between exploration and exploitation that standard SFT cannot achieve.

## Foundational Learning
- **Entropy Collapse:** The phenomenon where SFT drives model probabilities toward 0 or 1, eliminating useful uncertainty signals needed for exploration.
  - *Why needed:* Understanding why standard SFT fails for reasoning tasks that benefit from exploration.
  - *Quick check:* Monitor average token entropy during training; if it drops below 0.5 nats, entropy collapse is occurring.

- **Self-Distillation with EMA:** Using a teacher model that's a slowly-updated version of the student to provide stable training targets.
  - *Why needed:* Prevents teacher-student divergence while allowing the teacher to represent the student's evolving capability.
  - *Quick check:* Verify teacher-student KL divergence stays below 0.5 nats on average.

- **Temperature Scaling for Exploration:** Adjusting the softmax temperature to control the entropy of the output distribution.
  - *Why needed:* Higher temperatures increase entropy, promoting exploration; lower temperatures increase determinism.
  - *Quick check:* With temperature=1.5, output entropy should increase by ~0.3 nats compared to temperature=1.0.

## Architecture Onboarding
**Component Map:** Student Model -> K2 Loss + Standard SFT Loss -> Teacher Model (EMA-synced) -> Temperature-Guided Distillation
**Critical Path:** Token generation → Entropy computation → Temperature selection (binary search) → K2 distillation loss → Parameter update
**Design Tradeoffs:** The entropy-guided temperature mechanism trades computational overhead (binary search per token) for selective exploration preservation, while the EMA teacher trades recency for stability.
**Failure Signatures:** 
- Entropy explosion (>1.5 nats average) indicates ∆_max or α too high
- Knowledge forgetting (>20% probability shift on factual tokens) indicates H_pivot too low
- Training instability (spiking KL divergence) indicates EMA decay µ too slow

**First Experiments:**
1. Implement baseline SFT with OpenR1-Math-46K and verify standard entropy collapse (<0.5 nats average)
2. Add Self-Exploratory Distillation with fixed temperature to verify entropy preservation mechanism
3. Implement Entropy-Guided Temperature Selection with binary search to verify selective exploration

## Open Questions the Paper Calls Out
None

## Limitations
- The entropy preservation mechanism's effectiveness heavily depends on several unspecified hyperparameters (binary search stopping criterion, H_pivot sensitivity)
- Claims of successful RL translation assume compatibility without addressing potential conflicts between entropy objectives and RL rewards
- Robustness claims across architectures are based on a single alternative model without exploring architectural diversity

## Confidence
- **High Confidence:** Experimental methodology and baseline comparisons are clearly specified and reproducible
- **Medium Confidence:** Performance improvements appear credible but hyperparameter sensitivity is not explored
- **Low Confidence:** Cross-architecture robustness and RL compatibility claims lack sufficient validation

## Next Checks
1. Test binary search temperature selection with varying ϵ thresholds to determine robustness to convergence criteria
2. Apply CurioSFT to non-mathematical reasoning tasks to evaluate cross-domain generalization of the entropy signal
3. Systematically vary entropy regularization in RL to test whether SFT-preserved exploration benefits persist under different reward structures