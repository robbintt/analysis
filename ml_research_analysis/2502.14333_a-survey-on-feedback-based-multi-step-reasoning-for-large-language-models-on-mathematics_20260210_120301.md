---
ver: rpa2
title: A Survey on Feedback-based Multi-step Reasoning for Large Language Models on
  Mathematics
arxiv_id: '2502.14333'
source_url: https://arxiv.org/abs/2502.14333
tags:
- arxiv
- reasoning
- preprint
- solution
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper surveys strategies for enhancing multi-step mathematical
  reasoning in large language models (LLMs) through feedback mechanisms. The work
  categorizes approaches into training-based and training-free methods, with training-based
  techniques using step-level or outcome-level feedback to improve reasoning accuracy.
---

# A Survey on Feedback-based Multi-step Reasoning for Large Language Models on Mathematics

## Quick Facts
- arXiv ID: 2502.14333
- Source URL: https://arxiv.org/abs/2502.14333
- Authors: Ting-Ruen Wei; Haowei Liu; Xuyang Wu; Yi Fang
- Reference count: 39
- One-line primary result: This survey categorizes feedback-based approaches for improving LLM mathematical reasoning, identifying key challenges and future research directions.

## Executive Summary
This paper surveys strategies for enhancing multi-step mathematical reasoning in large language models (LLMs) through feedback mechanisms. The work categorizes approaches into training-based and training-free methods, with training-based techniques using step-level or outcome-level feedback to improve reasoning accuracy. Training-free approaches leverage frozen LLMs or external tools to provide feedback without model updates. The survey focuses on math problems due to their logical structure, presenting recent datasets at college and competition levels. Key challenges identified include optimizing efficiency for different problem difficulties, addressing reward hacking, and mitigating inverse scaling law effects. The work establishes a foundation for understanding feedback-based multi-step reasoning and aims to guide future research in this rapidly evolving field.

## Method Summary
The survey examines feedback mechanisms for multi-step mathematical reasoning by categorizing approaches based on whether they involve model training and the granularity of feedback. Training-based methods include Process Reward Models (PRMs) that provide step-level feedback and Outcome Reward Models (ORMs) that evaluate complete solutions. Training-free approaches use frozen LLM capabilities or external verification tools. The core methodology involves generating multiple solution paths, evaluating them with appropriate reward models, and selecting final answers through aggregation strategies like voting or search algorithms. Key datasets discussed include GSM8K, MATH, PRM800K, and Math-Shepherd.

## Key Results
- Process Reward Models (PRMs) significantly outperform Outcome Reward Models (ORMs) in mathematical reasoning tasks
- Training-free feedback approaches can provide competitive performance without model updates
- Inverse scaling law poses a significant challenge when policy models deviate from reward model training distributions
- Multiple solution generation combined with best-of-N selection improves answer accuracy

## Why This Works (Mechanism)

### Mechanism 1: Process-Level Feedback Enables Credit Assignment Across Reasoning Steps
Providing rewards at each reasoning step, rather than only at the final answer, improves the model's ability to identify and reinforce correct intermediate reasoning. Process Reward Models (PRMs) assign scalar scores to individual reasoning steps, which are aggregated to weight solution paths in voting schemes or guide search algorithms. This creates denser feedback signals compared to sparse outcome-only rewards.

### Mechanism 2: Outcome-Level Feedback Provides Lower-Cost Alternative with Aggregation Strategies
Outcome-level rewards, while less granular, can achieve competitive performance when combined with sampling-based selection strategies like Best-of-N or self-consistency voting. Outcome Reward Models (ORMs) output a single correctness probability for complete solution paths, and multiple solutions are sampled with final selection using majority voting, weighted voting, or Best-of-N selection.

### Mechanism 3: Training-Free Feedback Leverages Frozen LLM Capabilities or External Tools
Feedback can be obtained without training reward models by exploiting LLM internal signals (logits, self-evaluation responses) or external verification tools. Three approaches include LLM response-based evaluation, logit-based uncertainty detection, and external tool verification using theorem provers or code execution to provide ground-truth verification.

## Foundational Learning

- **Chain-of-Thought (CoT) Prompting**:
  - Why needed here: CoT establishes the baseline paradigm of multi-step decomposition that all feedback mechanisms build upon
  - Quick check question: Can you explain why prompting an LLM to "think step by step" changes its output distribution over solutions?

- **Preference Learning and Reward Modeling**:
  - Why needed here: Understanding how models learn scalar reward functions from preference or correctness data is essential for PRM/ORM training
  - Quick check question: What is the difference between training a reward model via discriminative classification versus generative next-token prediction?

- **Tree Search Algorithms (Beam Search, MCTS)**:
  - Why needed here: Step-level rewards are commonly used as heuristics to guide tree-based solution exploration
  - Quick check question: How does the UCT formula in MCTS balance exploitation (high-value nodes) versus exploration (less-visited nodes)?

## Architecture Onboarding

- **Component map**: Question input -> Policy model (LLM) generates multiple solution paths -> Reward model (PRM or ORM) evaluates solutions -> Aggregation/Selection module (voting, Best-of-N, or search) -> Final answer output

- **Critical path**: 1. Question input â†’ Policy model generates M solution paths (each with n reasoning steps) 2. Reward model evaluates: PRM scores each step; ORM scores complete paths 3. Aggregation: Combine scores (product/min/last for PRM; direct for ORM) 4. Selection: Apply voting, Best-of-N, or guided search to select final answer 5. (Optional) Use low-scoring steps to trigger refinement prompts

- **Design tradeoffs**:
  - PRM vs ORM: PRM provides finer feedback but requires expensive step-level annotation; ORM is cheaper but less precise
  - Training-based vs Training-free: Training improves performance but adds complexity; training-free is simpler but relies on frozen model capabilities
  - Search vs Sampling: Beam/MCTS provides structured exploration but is computationally expensive; Best-of-N with sampling is simpler but less directed
  - Assumption: Snell et al. (2024) find beam search outperforms Best-of-N under low compute budgets, but the reverse holds at high budgets

- **Failure signatures**:
  - Reward hacking: Model generates repetitive "correct" steps to maximize reward without progress
  - Inverse scaling: Policy deviates from PRM's training distribution over iterations, degrading feedback quality
  - Verification collapse: Self-evaluation prompts produce confident but incorrect assessments
  - Poor identification rate: Sampling produces correct solutions but ORM fails to rank them highest

- **First 3 experiments**:
  1. Baseline comparison: Implement Best-of-N with a pretrained ORM on GSM8K; measure accuracy vs N (sample count) to verify identification rate scales with sampling
  2. PRM vs ORM ablation: Train both on identical data with step-level vs outcome-level labels; compare Best-of-N and beam search performance to quantify granularity benefits
  3. Training-free sanity check: Implement logit-based uncertainty filtering (accept steps until confidence drops); compare against random step selection to verify signal validity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can models dynamically determine problem difficulty to switch between direct answering and computationally expensive multi-step reasoning?
- Basis in paper: Section 6 states that multi-step reasoning is inefficient for easier problems and determining this distinction is a challenge for optimizing efficiency
- Why unresolved: Current systems often apply uniform reasoning strategies regardless of problem complexity, leading to unnecessary resource consumption
- What evidence would resolve it: A mechanism that accurately classifies problem difficulty prior to generation and reduces inference latency on simple datasets (like GSM8K) without sacrificing accuracy on complex datasets (like OlympiadBench)

### Open Question 2
- Question: What effective methods exist to mitigate "reward hacking" in Process Reward Models (PRMs), such as models repeating correct steps to artificially inflate rewards?
- Basis in paper: Section 6 identifies studying reward hacking, specifically the repetition of correct steps to gain high rewards without progress, as essential for improving PRMs
- Why unresolved: Current reward models may assign high probabilities to valid but redundant steps, failing to penalize lack of forward progress
- What evidence would resolve it: The development of reward shaping techniques or constraints that penalize repetitive reasoning paths while maintaining or improving final answer accuracy

### Open Question 3
- Question: How can the "inverse scaling law" be mitigated when policy models trained on feedback deviate from the distribution the original reward model was trained on?
- Basis in paper: Section 6 notes the challenge where the policy model deviates from the original distribution over training iterations, causing the reward model to provide less useful feedback
- Why unresolved: As the policy updates, the fixed reward model becomes less reliable, creating a distribution shift that degrades the training signal
- What evidence would resolve it: A dynamic training framework where the reward model is updated or adapted alongside the policy to maintain alignment with the current reasoning distribution

### Open Question 4
- Question: To what extent can feedback-based multi-step reasoning frameworks transfer from mathematics to domains with less rigid logical structures, such as image captioning or cross-lingual tasks?
- Basis in paper: Section 6 mentions that while math has attracted research due to its logical nature, the technique has potential to expand to other domains like image captioning and cross-lingual tasks
- Why unresolved: Mathematical reasoning relies on verifiable steps, whereas domains like image captioning lack the same objective ground truth for intermediate feedback
- What evidence would resolve it: Successful application of process reward mechanisms to non-mathematical benchmarks (e.g., visual reasoning) yielding performance improvements comparable to those seen in mathematical tasks

## Limitations
- The survey lacks quantitative benchmarking across different feedback approaches
- Training-free section particularly suffers from sparse evidence with limited detailed examples
- Inverse scaling law discussion remains largely theoretical with limited experimental validation
- Most claims rely on qualitative comparisons from cited papers rather than direct empirical validation

## Confidence

**High Confidence**: The taxonomy of training-based vs training-free feedback approaches, and the distinction between PRMs and ORMs as fundamental categories. The mechanism descriptions for beam search and MCTS with step-level rewards are well-established.

**Medium Confidence**: Claims about reward hacking and inverse scaling effects, as these are documented phenomena but their prevalence and severity in current math reasoning systems require more systematic study.

**Low Confidence**: Direct performance comparisons between training-based and training-free methods, and the assertion that training-free approaches can match or exceed training-based performance in practice.

## Next Checks

1. **Reward Hacking Detection**: Implement automated detection of repetitive reasoning patterns in PRM-scored solutions. Compare frequency of reward hacking behaviors between PRMs trained on step-level vs outcome-level data to quantify the vulnerability gap.

2. **Distribution Shift Quantification**: Design a controlled experiment tracking PRM accuracy degradation over iterative policy updates. Measure performance drop when policy distribution shifts beyond PRM's training data coverage, validating inverse scaling concerns.

3. **Training-Free vs Training-Based Benchmark**: Conduct head-to-head comparison of logit-based uncertainty filtering against trained PRMs on identical math problem sets. Measure accuracy, computational cost, and robustness to noise to establish practical tradeoffs.