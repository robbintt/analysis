---
ver: rpa2
title: 'Align-then-Unlearn: Embedding Alignment for LLM Unlearning'
arxiv_id: '2506.13181'
source_url: https://arxiv.org/abs/2506.13181
tags:
- unlearning
- embedding
- forget
- align-then-unlearn
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Align-then-Unlearn, a framework for unlearning
  sensitive information from large language models by operating in the semantic embedding
  space rather than directly on output tokens. The method first augments the LLM with
  an embedding prediction module trained to anticipate future context representations,
  then performs unlearning by fine-tuning the model to minimize similarity between
  predicted embeddings and a target embedding representing the concept to be removed.
---

# Align-then-Unlearn: Embedding Alignment for LLM Unlearning

## Quick Facts
- arXiv ID: 2506.13181
- Source URL: https://arxiv.org/abs/2506.13181
- Reference count: 8
- Achieves 13.5% forget accuracy while preserving 64.5% MMLU performance

## Executive Summary
This paper proposes Align-then-Unlearn, a framework for removing targeted conceptual knowledge from large language models by operating in semantic embedding space rather than directly on output tokens. The method augments an LLM with an embedding prediction module trained to anticipate future context representations, then performs unlearning by fine-tuning the model to minimize similarity between predicted embeddings and a target embedding representing the concept to be removed. Experiments on the RWKU benchmark show the approach effectively removes targeted knowledge while maintaining model utility and providing robustness to prompt rephrasing compared to token-level methods.

## Method Summary
Align-then-Unlearn operates in two phases: (1) Alignment pre-training where a 6-layer embedding prediction head is trained to map hidden states to predicted embeddings of future token windows using a frozen text encoder as reference, and (2) Unlearning where the LLM is fine-tuned to minimize cosine similarity between predicted embeddings and a target concept embedding, with an iterative process of LLM updates and head realignment. The approach uses a margin threshold τ to control the aggressiveness of unlearning, only penalizing predictions with similarity above this threshold to preserve unrelated knowledge.

## Key Results
- Achieves 13.5% forget accuracy on RWKU benchmark while maintaining 64.5% MMLU performance
- Demonstrates robustness to prompt rephrasing compared to token-level unlearning methods
- Shows layer-specific effectiveness (layer 10 achieved 54% forget accuracy for "Warren Buffett" vs 12% at layer 20)

## Why This Works (Mechanism)

### Mechanism 1
Operating in embedding space enables unlearning that generalizes across prompt rephrasings. The embedding prediction module learns to map hidden states to semantic representations of future token windows rather than individual tokens. By minimizing similarity between predicted embeddings and a target concept embedding, the model suppresses the underlying conceptual representation rather than specific output sequences. This assumes semantic similarity in the pre-trained text encoder's embedding space meaningfully corresponds to conceptual relatedness for the target knowledge.

### Mechanism 2
The iterative align-unlearn process creates an adversarial dynamic that deepens forgetting beyond surface-level output masking. After unlearning reduces similarity below threshold τ, the prediction head is realigned to the updated LLM representations. This forces representational change rather than output suppression, as the LLM must further obscure the target to keep predictions distant from the unlearned concept. This assumes the LLM's hidden representations can be modified to make the target concept unrecoverable even by an optimally realigned predictor.

### Mechanism 3
Margin threshold τ provides fine-grained control over unlearning scope, preserving neighbor knowledge. The loss only penalizes predictions with similarity above τ, allowing predictions already distant from the target to incur zero loss. This prevents unnecessary updates to unrelated representations, assuming the embedding space is structured such that unrelated concepts naturally fall below the similarity threshold.

## Foundational Learning

- Concept: Cosine similarity and embedding space geometry
  - Why needed here: The entire unlearning objective operates on angular distance between predicted and target embeddings
  - Quick check question: If two vectors have cosine similarity 0.9, what does that imply about their semantic relationship? How would a margin threshold of τ=0.5 affect the loss?

- Concept: Frozen modules vs. fine-tunable parameters in multi-component systems
  - Why needed here: The text encoder remains frozen throughout; the embedding head is frozen during unlearning but realigned between rounds; only the LLM is updated during unlearning
  - Quick check question: During unlearning phase, which components' parameters are updated: (a) text encoder, (b) embedding prediction head, (c) LLM backbone?

- Concept: Trade-offs between forgetting depth and model utility
  - Why needed here: The paper explicitly shows that stronger forgetting correlates with neighbor knowledge loss
  - Quick check question: At checkpoint 20% forget accuracy, what is the MMLU score? What does this suggest about the forgetting-utility frontier?

## Architecture Onboarding

- Component map:
Input tokens (x_1,...,x_T) -> [Pre-trained LLM M] -> Hidden states (h_1,...,h_T) -> [Embedding Prediction Module E] -> Predicted embeddings ê_t -> Compare to target e_unlearn -> [Frozen Text Encoder] <- Concept description

- Critical path:
1. Train E on alignment loss L_align using frozen encoder reference embeddings
2. Compute target embedding e_unlearn from concept description via frozen encoder
3. Fine-tune M on L_unlearn until similarity drops below τ
4. Realign E to updated M representations
5. Repeat steps 3-4 until desired forget accuracy achieved

- Design tradeoffs:
  - Layer selection: Applying unlearning at layer 10, 20, or 30 shows target-specific effectiveness
  - Threshold τ: Lower τ = more aggressive unlearning but more utility loss
  - Head capacity: 6-layer head with 768 hidden dim used; capacity affects alignment quality

- Failure signatures:
  - Over-unlearning: Neighbor scores drop significantly while forget accuracy remains moderate
  - Early convergence: Similarity drops below τ too quickly, halting unlearning prematurely
  - Layer mismatch: Different layers show varying effectiveness for specific targets
  - Quantization/retraining attacks: Unlearned knowledge can be restored post-unlearning

- First 3 experiments:
1. Single-target unlearning with layer sweep: Run unlearning at layers 10, 20, 30 with fixed τ and plot forget accuracy vs. neighbor/utility scores
2. Threshold calibration on held-out target: Train τ on 2-3 targets, evaluate generalization to unseen targets
3. Adversarial robustness test: After unlearning, probe with paraphrased questions and multi-hop queries from RWKU adversarial attack set

## Open Questions the Paper Calls Out

1. Can a dynamic adjustment strategy for the margin threshold τ improve unlearning consistency across diverse target concepts? The authors note that applying a τ tuned for a single target to others reduced effectiveness and suggest future work could explore dynamic τ adjustment.

2. Does the effectiveness of unlearning vary systematically based on the specific internal layer targeted, and can a multi-layer strategy optimize results? The authors observe that unlearning effectiveness depends on the layer and suggest target-specific layer selection or multi-layer strategies as future research.

3. Is Align-then-Unlearn suitable for removing verbatim text sequences, such as passwords or PII, or is it restricted to conceptual knowledge? The authors state that evaluating effectiveness for unlearning exact text sequences could assess its suitability for such scenarios.

## Limitations

- The approach relies on the assumption that semantic embeddings from a frozen text encoder capture all relevant conceptual variations, which may not hold for all knowledge domains
- The iterative process introduces multiple hyperparameters (τ, layer selection, realignment frequency) that require manual tuning for each target
- The method's vulnerability to activation steering and knowledge restoration attacks suggests unlearning at the embedding level may not provide stronger guarantees than token-level methods against sophisticated adversaries

## Confidence

**High Confidence:** The basic mechanism of embedding prediction followed by similarity-based fine-tuning is clearly specified and the experimental results are reproducible given the described architecture.

**Medium Confidence:** The claim that embedding-space unlearning generalizes better to rephrasings than token-level methods is supported by comparative analysis but the evaluation set may be too small to establish general robustness.

**Low Confidence:** The claim about fine-grained control through τ threshold is weakened by the paper's own observation that single-threshold generalization fails.

## Next Checks

1. Cross-domain threshold generalization test: Select 10 targets from diverse knowledge domains and measure how much threshold tuning is required per domain when applying optimal τ values from training targets to held-out targets.

2. Adversarial rephrasing robustness benchmark: Compare Align-then-Unlearn against token-level unlearning methods across multiple rephrasing types from the RWKU adversarial attack set to identify specific failure modes.

3. Hidden-layer sensitivity analysis: Systematically evaluate unlearning effectiveness across all 32 layers of Phi-3-mini-4k-instruct for 5 diverse targets to identify patterns in which knowledge types are most vulnerable at different representational depths.