---
ver: rpa2
title: "$\u03C1$-$\\texttt{EOS}$: Training-free Bidirectional Variable-Length Control\
  \ for Masked Diffusion LLMs"
arxiv_id: '2601.22527'
source_url: https://arxiv.org/abs/2601.22527
tags:
- length
- masked
- arxiv
- density
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the fixed-length limitation in masked diffusion\
  \ large language models (dLLMs), where a predefined generation length forces a trade-off\
  \ between output quality and computational efficiency. The authors propose \u03C1\
  -EOS, a training-free, single-stage strategy that enables bidirectional variable-length\
  \ generation by leveraging the implicit density (\u03C1) of end-of-sequence (EOS)\
  \ tokens as a signal of generation sufficiency."
---

# $ρ$-$\texttt{EOS}$: Training-free Bidirectional Variable-Length Control for Masked Diffusion LLMs

## Quick Facts
- arXiv ID: 2601.22527
- Source URL: https://arxiv.org/abs/2601.22527
- Reference count: 14
- Enables bidirectional variable-length generation for masked diffusion LLMs without training

## Executive Summary
This paper addresses the fixed-length limitation in masked diffusion large language models (dLLMs), where a predefined generation length forces a trade-off between output quality and computational efficiency. The authors propose ρ-EOS, a training-free, single-stage strategy that enables bidirectional variable-length generation by leveraging the implicit density (ρ) of end-of-sequence (EOS) tokens as a signal of generation sufficiency. During denoising, high ρ-EOS triggers MASK token contraction (indicating excessive length), while low ρ-EOS induces expansion (indicating insufficient length). Extensive experiments on mathematics and code benchmarks demonstrate that ρ-EOS achieves comparable performance to fixed-length baselines and two-stage methods while substantially improving inference efficiency and token utilization.

## Method Summary
ρ-EOS introduces a training-free, single-stage approach for bidirectional variable-length generation in masked diffusion LLMs. The method computes the implicit EOS density ρ at each denoising step as the ratio of implicit EOS tokens to remaining MASK tokens. Based on threshold comparisons, it dynamically adjusts sequence length: if ρ < ρ_low, the model appends additional MASK tokens (expansion); if ρ > ρ_high, it truncates trailing MASK tokens (contraction); otherwise, it holds the current length. This approach enables the model to generate sequences of appropriate length without requiring retraining or multi-stage inference, addressing the fixed-length limitation inherent to current masked diffusion architectures.

## Key Results
- Achieves comparable accuracy to fixed-length baselines and two-stage methods on GSM8K, MATH500, MBPP, and HumanEval benchmarks
- Substantially improves inference efficiency by avoiding excessive token generation
- Provides flexible length control suitable for practical deployment scenarios requiring adaptive generation lengths

## Why This Works (Mechanism)
The method works by exploiting the statistical relationship between EOS token density and generation sufficiency. During denoising, as the model progressively reconstructs the sequence, the implicit density of EOS tokens serves as a signal for whether the generation is complete or requires adjustment. High EOS density indicates the model is producing EOS tokens too early, suggesting truncation is needed, while low density suggests insufficient information has been generated, requiring expansion. This creates a self-regulating mechanism where the model can adapt its output length dynamically based on the internal state of the denoising process.

## Foundational Learning
- **Implicit token extraction**: Converting model logits to predicted tokens and identifying which tokens occupy MASK positions. *Why needed*: ρ-EOS relies on counting implicit EOS tokens at MASK positions to compute density. *Quick check*: Verify that argmax(softmax(logits)) correctly identifies tokens at MASK indices and that implicit token counting matches expected values.
- **Density computation**: Calculating ρ_EOS = (# implicit EOS tokens) / (# remaining MASK tokens). *Why needed*: This ratio serves as the core signal for length adjustment decisions. *Quick check*: Implement density calculation and verify it produces values in [0,1] that correlate with generation completeness.
- **Bidirectional adjustment logic**: Implementing conditional expansion and contraction based on threshold comparisons. *Why needed*: The core mechanism for dynamic length control. *Quick check*: Test adjustment logic with synthetic ρ_EOS values to verify correct expansion/contraction behavior.
- **Confidence-based remasking**: Using a threshold to decide which predicted tokens to keep versus re-mask during expansion. *Why needed*: Controls the stability of the expansion process and prevents oscillation. *Quick check*: Implement remasking with different τ_high values and observe the effect on length adjustment patterns.

## Architecture Onboarding

**Component Map**
LLaDA Model -> Forward Pass -> Implicit Token Extraction -> ρ_EOS Computation -> Threshold Comparison -> Length Adjustment (Expansion/Contraction/Hold) -> Remask -> Next Denoising Step

**Critical Path**
1. Forward pass through LLaDA to get logits
2. Extract implicit tokens at MASK positions
3. Compute ρ_EOS density
4. Compare to [ρ_low, ρ_high] thresholds
5. Apply expansion, contraction, or hold
6. Remask selected tokens
7. Repeat until completion

**Design Tradeoffs**
- Fixed vs variable length: Fixed length provides stability but wastes tokens; variable length saves computation but requires dynamic adjustment logic
- Symmetric vs asymmetric thresholds: Symmetric thresholds are simpler but asymmetric bounds may better handle the asymmetric nature of generation sufficiency
- Constant vs adaptive expansion: Constant expansion (as used) is simpler but adaptive factors could potentially optimize adjustment speed

**Failure Signatures**
- Premature contraction: ρ_EOS spikes early, causing the model to truncate before completing the response (the "EOS trap")
- Oscillatory behavior: ρ_EOS repeatedly crosses thresholds, causing the sequence length to fluctuate
- Insufficient expansion: ρ_EOS remains below threshold despite inadequate generation, resulting in incomplete outputs

**Exactly 3 First Experiments**
1. Implement baseline fixed-length denoising with LLaDA-Instruct-8B on GSM8K to establish reference performance
2. Implement ρ-EOS with asymmetric thresholds [0.4, 0.8] and constant expansion, measuring accuracy and token utilization
3. Log ρ_EOS trajectories across all denoising steps to diagnose stability and identify potential oscillation or premature contraction

## Open Questions the Paper Calls Out

### Open Question 1
Can ρ-EOS be effectively integrated with reinforcement learning fine-tuning for masked dLLMs, where diverse reasoning depths and response structures could facilitate exploration? The paper notes that length flexibility plays a critical role in RL fine-tuning scenarios but does not investigate this application.

### Open Question 2
How does ρ-EOS generalize to open-ended generation tasks beyond structured reasoning benchmarks? The evaluation is limited to mathematics and code generation, which have relatively well-defined termination conditions.

### Open Question 3
Can ρ-EOS be combined with inference acceleration techniques (e.g., caching, speculative decoding) to further reduce latency while maintaining adaptive length control? The paper does not explore integration with existing acceleration methods.

### Open Question 4
Does the implicit EOS density signal transfer effectively to other masked diffusion architectures beyond LLaDA? All experiments use LLaDA models, leaving cross-architecture generalization untested.

## Limitations
- Underspecified implementation details for Remask function and confidence threshold τ_high
- Asymmetric threshold selection [0.4, 0.8] lacks theoretical justification beyond empirical observation
- Quantitative claims about efficiency improvements depend heavily on unspecified parameters (L_max, N) and lack comprehensive ablation

## Confidence

**High confidence**: The core mathematical framework for ρ-EOS density computation and bidirectional adjustment logic is well-defined and reproducible.

**Medium confidence**: The effectiveness of asymmetric thresholds and the claim that ρ-EOS achieves comparable accuracy to fixed-length baselines.

**Low confidence**: The specific quantitative claims about "doubling" inference efficiency and "substantial" token utilization improvements.

## Next Checks

1. **Remask Function Sensitivity Analysis**: Implement and test multiple Remask variants with different confidence thresholds τ_high (0.5, 0.7, 0.9) and selection criteria to quantify their impact on ρ-EOS stability and final generation quality.

2. **Threshold Parameter Ablation**: Systematically vary the ρ_low and ρ_high thresholds across the range [0.3, 0.9] in 0.1 increments, measuring accuracy, Eratio, and Ntoken for each configuration on both GSM8K and HumanEval.

3. **Oscillation Prevention Testing**: Implement diagnostic logging of ρ_EOS trajectories across all denoising steps, measuring the frequency of threshold crossings and computing a "stability score" (inverse of oscillation count). Test whether widening the threshold gap or implementing hysteresis prevents the "EOS trap" failure mode.