---
ver: rpa2
title: XAI for Point Cloud Data using Perturbations based on Meaningful Segmentation
arxiv_id: '2507.22020'
source_url: https://arxiv.org/abs/2507.22020
tags:
- data
- point
- input
- segmentation
- cloud
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel segmentation-based explainable AI (XAI)
  method for neural networks working on point cloud classification. The method introduces
  perturbations into the input data using meaningful segments generated by segmentation
  models, and computes saliency attributions based on the change in the output value
  of the target class.
---

# XAI for Point Cloud Data using Perturbations based on Meaningful Segmentation

## Quick Facts
- **arXiv ID:** 2507.22020
- **Source URL:** https://arxiv.org/abs/2507.22020
- **Reference count:** 32
- **Primary result:** Introduces semantic segmentation-based perturbation method for explainable 3D point cloud classification

## Executive Summary
This paper presents a novel approach for explaining neural network decisions on point cloud classification tasks using perturbations based on semantically meaningful segments. The method leverages pre-trained part segmentation models to partition point clouds into human-interpretable parts (like wings, fuselage, wheels), then measures how perturbing these segments affects classification output. Two perturbation strategies—absence of feature and presence of feature—provide complementary views of feature importance. The approach addresses limitations of existing methods that use arbitrary clustering, which can produce inconsistent and difficult-to-interpret saliency maps.

## Method Summary
The method uses semantic segmentation models to divide point clouds into meaningful parts, then perturbs these segments using a novel point-shifting mechanism. Instead of removing points or shifting them to centroids (which can create artificial structures), points are shifted to random locations within the retained structure, effectively neutralizing their influence. Saliency attributions are computed by measuring the change in classification score when segments are perturbed. The method is model-agnostic and works with any classification model operating on point clouds. Evaluation on the ShapeNetCore dataset demonstrates that semantic segmentation produces more stable and interpretable explanations compared to clustering-based approaches.

## Key Results
- Semantic segmentation produces more interpretable saliency maps than arbitrary clustering (KMeans with varying cluster counts)
- Point-shifting to random retained points avoids artificial structure creation compared to centroid shifting
- The two perturbation strategies (absence and presence) reveal complementary aspects of feature importance
- Method is model-agnostic and applicable to any point cloud classification architecture

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Meaningful semantic segments produce more interpretable saliency maps than arbitrary clustering
- Mechanism: Uses pre-trained segmentation models to partition point clouds into human-interpretable parts, then perturbs these segments to measure contribution to classification
- Core assumption: Segments that are semantically meaningful to humans align with features the model actually uses for classification
- Evidence anchors: Shows KMeans clustering produces inconsistent saliency maps across different cluster counts; related work supports structured segmentation aids interpretability
- Break condition: Poor segmentation model accuracy (<65%) or misalignment between semantic parts and model's actual decision features

### Mechanism 2
- Claim: Shifting points to random locations within retained structure neutralizes their influence without introducing artifacts
- Mechanism: Points are shifted to a random point from the retained segment rather than to the centroid or being removed
- Core assumption: Classification model does not treat point density or local concentration as meaningful features
- Evidence anchors: Demonstrates centroid shifting creates artificial clusters; shows different random point selections yield identical output scores
- Break condition: If model uses local point density as a feature, or retained structure has insufficient points to absorb shifted points

### Mechanism 3
- Claim: Two complementary perturbation strategies reveal different aspects of feature importance
- Mechanism: "Absence of feature" removes one segment while keeping others; "Presence of feature" keeps only one segment
- Core assumption: Both removal-isolation and isolation-only perspectives provide valid, complementary views of feature importance
- Evidence anchors: Shows chair with seat removed still retains structural identity; demonstrates that single segments may not carry standalone information
- Break condition: When segments are highly interdependent—removing one changes the meaning of others

## Foundational Learning

- **Point cloud representation basics**: Point clouds are unordered sets of $(x, y, z)$ coordinates where structural information emerges from spatial distribution. *Quick check:* If you randomly shuffle a point cloud's point order, should the classification output change? (No—PointNet and similar architectures are permutation-invariant.)

- **Perturbation-based XAI fundamentals**: The methodology relies on measuring output change when input is modified. *Quick check:* What is the baseline requirement for any perturbation to be valid for attribution? (The perturbation must remove or isolate the feature's influence without introducing confounding artifacts.)

- **Semantic segmentation for 3D data**: The method depends on pre-trained part segmentation models that assign per-point labels like "wing," "wheel," "seat." *Quick check:* Why does the paper train 16 separate segmentation models instead of one unified model? (Better specialization per category; dataset imbalance across categories.)

## Architecture Onboarding

- **Component map**: Input Point Cloud → Classification Model → Class Prediction → Segmentation Model → Segments → Perturbation Module → Perturbed Inputs → Re-classification → Output Scores → Saliency Computation

- **Critical path**: Classification model prediction determines which segmentation model to load; segmentation quality directly determines explanation coherence; point-shifting implementation must correctly identify retained structure

- **Design tradeoffs**:
  - Segmentation accuracy vs. interpretability guarantee: Higher accuracy segmentation models produce more reliable explanations but require more training data
  - Segmentation-only vs. Segmentation+Clustering: Segmentation-only treats all wheels as one feature; segmentation+clustering is more granular but increases computation
  - Absence vs. Presence perturbation: Absence is more intuitive; Presence reveals standalone discriminative power but may produce uninterpretable results for interdependent features

- **Failure signatures**:
  - Segmentation errors: Mislabeled parts lead to attributions assigned to wrong regions (check by visualizing segmentation output)
  - Classification errors: Wrong segmentation model loads if classifier misclassifies input (requires human-in-the-loop correction)
  - Noise sensitivity: Above ~10% noise, both segmentation and classification degrade (monitor input quality)

- **First 3 experiments**:
  1. Validate point-shifting neutrality by confirming identical classification scores across 5 different random point selections
  2. Compare clustering vs. segmentation by quantifying attribution consistency across random seeds
  3. Test Absence vs. Presence consistency by checking if high $S_{AF}$ segments have reasonable $S_{PF}$ values

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the XAI pipeline be decoupled from the initial classification model's accuracy to prevent the selection of incorrect segmentation models?
- Basis in paper: Section 4.6 states that the method currently depends on the classification output to select the segmentation model, meaning an incorrect classification results in an inappropriate segmentation model being chosen
- Why unresolved: The current implementation relies on a hard dependency where the explainer trusts the classifier's prediction, creating a failure mode for misclassified inputs
- What evidence would resolve it: A modified pipeline that utilizes class-agnostic segmentation or a confidence-based selection mechanism that remains robust despite classification errors

### Open Question 2
- Question: Can the method be extended to explain classifications for object categories outside the segmentation training set without requiring new labeled data?
- Basis in paper: Section 4.6 lists the "requirement of a labeled dataset for segmentation" as a limitation when adding new 3D object categories
- Why unresolved: The current architecture requires training specific segmentation models for specific object types, limiting scalability to open-world datasets
- What evidence would resolve it: An adaptation of the method using unsupervised or zero-shot segmentation techniques that maintains explanation quality without category-specific training

### Open Question 3
- Question: Is the "random point shifting" perturbation mechanism universally robust across diverse neural network architectures?
- Basis in paper: The paper claims the method is "model-agnostic" but experimental validation is restricted to PointNet-based architectures
- Why unresolved: It is unverified if the assumption that "shifted points do not provide additional structural information" holds true for architectures with different inductive biases
- What evidence would resolve it: Evaluation on non-PointNet architectures (e.g., DGCNN, PointCNN) showing perturbations consistently remove feature influence

## Limitations
- Segmentation model accuracy (65-80% reported) directly impacts attribution quality with no error propagation analysis
- Core assumption that semantic segments align with model's actual decision features remains unproven
- Method assumes point density isn't used by classifier, but this is unverified
- Multi-category setup requires switching between 16 segmentation models, creating human-in-the-loop dependencies

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Point-shifting mechanism works as described and avoids centroid artifact | High |
| Semantic segments produce more interpretable results than clustering | Medium |
| Method's explanations are actually aligned with classifier's decision process | Low |

## Next Checks

1. **Test segmentation error propagation**: Introduce controlled noise in segmentation labels and measure how much attribution quality degrades
2. **Validate feature alignment**: Use feature visualization (CAM/grad-CAM) to check if segments with high saliency actually correspond to model attention regions
3. **Stress test point-shifting**: Systematically vary the number of points shifted and measure classification stability to confirm perturbation doesn't introduce artifacts