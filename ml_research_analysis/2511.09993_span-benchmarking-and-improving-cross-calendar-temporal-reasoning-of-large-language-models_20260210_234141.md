---
ver: rpa2
title: 'SPAN: Benchmarking and Improving Cross-Calendar Temporal Reasoning of Large
  Language Models'
arxiv_id: '2511.09993'
source_url: https://arxiv.org/abs/2511.09993
tags:
- calendar
- date
- reasoning
- temporal
- gregorian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SPAN, a benchmark for evaluating large language
  models' (LLMs) cross-calendar temporal reasoning capabilities. SPAN requires models
  to perform both intra-calendar reasoning and inter-calendar date conversion across
  ten directions, two reasoning types (date-based and festival-based), and two question
  formats (polar and content) using six calendars.
---

# SPAN: Benchmarking and Improving Cross-Calendar Temporal Reasoning of Large Language Models

## Quick Facts
- arXiv ID: 2511.09993
- Source URL: https://arxiv.org/abs/2511.09993
- Reference count: 40
- Primary result: LLMs achieve only 34.5% average accuracy on cross-calendar temporal reasoning benchmark; Time Agent with tool-augmented code generation achieves 95.31% accuracy

## Executive Summary
SPAN introduces a comprehensive benchmark for evaluating large language models' cross-calendar temporal reasoning capabilities across six calendar systems (Gregorian, Chinese Lunar, Islamic, Hebrew, Shaka, Persian). The benchmark requires both intra-calendar reasoning and inter-calendar date conversion across ten directions, two reasoning types, and two question formats. Experiments on 21 evaluation dates from 1960 to 2060 reveal state-of-the-art LLMs achieve only 34.5% average accuracy, with none exceeding 80%. The study identifies Future-Date Degradation (accuracy drops for future dates) and Calendar Asymmetry Bias (better performance converting from Gregorian to other calendars). To address these limitations, the authors develop an LLM-powered Time Agent using tool-augmented code generation that achieves 95.31% average accuracy, significantly outperforming competitive baselines.

## Method Summary
The SPAN benchmark employs a template-driven dynamic instance generation protocol that enables time-variant evaluation and mitigates contamination risks. The protocol involves four stages: calendar conversion to generate cross-calendar entries, template matching to build question-code template pairs, template instantiation with user-specified Gregorian dates, and code execution to obtain ground truth answers. The Time Agent combines GPT-4o's code-generation capabilities with a cross-calendar conversion interface (search_calendar) that wraps Python libraries (datetime, convertdate, LunarCalendar) to handle the actual calendar arithmetic, while the LLM focuses on parsing questions and generating executable code. This tool-augmented approach achieves 95.31% accuracy compared to 34.5% for direct LLM reasoning.

## Key Results
- State-of-the-art LLMs achieve only 34.5% average accuracy on cross-calendar temporal reasoning tasks
- No model exceeds 80% accuracy across the 1960-2060 evaluation period
- Time Agent with tool-augmented code generation achieves 95.31% average accuracy
- Calendar Asymmetry Bias: models perform 4-17% better on Gregorian-to-Others conversions
- Future-Date Degradation: accuracy drops significantly for dates beyond 2020

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tool-augmented code generation dramatically outperforms direct LLM reasoning for cross-calendar temporal conversion.
- Mechanism: The Time Agent delegates calendar arithmetic to deterministic Python libraries via a search_calendar interface, while the LLM handles only question parsing and code generation—not the conversion itself.
- Core assumption: Calendar conversions are algorithmically well-defined and better suited to symbolic computation than neural parametric memory.
- Evidence anchors:
  - [abstract]: "Time Agent... leverages tool-augmented code generation... achieves an average accuracy of 95.31%, outperforming several competitive baselines"
  - [section]: "We develop an LLM-powered Time Agent that combines LLM code-generation capabilities with our cross-calendar conversion interface search_calendar"
  - [corpus]: Weak direct evidence; corpus neighbors focus on other benchmarks and temporal alignment, not tool-augmented approaches.

### Mechanism 2
- Claim: Template-driven dynamic instance generation mitigates data contamination and enables time-variant evaluation.
- Mechanism: Question-code template pairs with placeholders are instantiated at evaluation time using a user-specified Gregorian date, producing fresh test instances whose ground truth is computed by executing the paired code snippet.
- Core assumption: Contamination risk stems from static benchmark datasets being absorbed into training corpora; dynamic generation avoids this.
- Evidence anchors:
  - [abstract]: "template-driven protocol for dynamic instance generation that enables assessment on a user-specified Gregorian date"
  - [section]: Figure 1 shows the four-stage protocol: Calendar Conversion → Template Matching → Template Instantiation → Code Execution
  - [corpus]: Corpus neighbors show related benchmarking efforts (MatSciBench, AutoGEEval) but no direct comparison to template-driven protocols.

### Mechanism 3
- Claim: LLMs exhibit Calendar Asymmetry Bias due to pretraining data distribution favoring Gregorian-origin expressions.
- Mechanism: Modern web documents predominantly use Gregorian timestamps as primary temporal anchors, so LLMs receive more exposure to conversions from Gregorian than to Gregorian, creating directional performance gaps.
- Core assumption: The asymmetry reflects training data distribution, not an intrinsic property of calendar systems.
- Evidence anchors:
  - [abstract]: "Calendar Asymmetry Bias (better performance converting from Gregorian to other calendars)"
  - [section]: Figure 5 shows all LLMs perform better on Gregorian-to-Others (e.g., DeepSeek-V3: 57.32% vs 39.83%)
  - [corpus]: Temporal Alignment paper (ID 95327) notes LLMs suffer temporal misalignment over long time spans, supporting data-driven limitations.

## Foundational Learning

- Concept: **Intra-calendar vs. Inter-calendar Reasoning**
  - Why needed here: SPAN requires both reasoning within a calendar (e.g., "7 days later") and converting between calendars (e.g., Gregorian to Islamic).
  - Quick check question: Given "2024-06-01 Gregorian," what's the Islamic date 10 days later? (Requires intra-calendar offset + inter-calendar conversion.)

- Concept: **Deterministic Calendar Conversion Libraries**
  - Why needed here: The Time Agent's success hinges on reliable external libraries; you must understand their API and limitations.
  - Quick check question: Does convertdate handle Hebrew leap years correctly for 5785?

- Concept: **Template Placeholders and Code-Question Alignment**
  - Why needed here: Each question template has a corresponding code template; variables must align semantically (e.g., "{nd} days later" ↔ timedelta(days={nd})).
  - Quick check question: For a festival-based polar question, what variables are required vs. optional?

## Architecture Onboarding

- Component map: Cross-calendar entries → search_calendar interface → Template Engine → Template Instantiation → Code Execution → Evaluation

- Critical path:
  1. User specifies Gregorian date → search_calendar produces cross-calendar entry
  2. Template matching builds (cs, dr_cs, fcs, ct) pairs
  3. Manual variable specification (de_ct, nd, nw, ny)
  4. Code execution yields ground truth; LLM generates response

- Design tradeoffs:
  - **Dynamic vs. Static Benchmarks**: Dynamic avoids contamination but requires maintaining template-code alignment
  - **Tool-augmented vs. Direct Reasoning**: Tools give 95.31% accuracy but add infrastructure complexity (code interpreter, library dependencies)
  - **Polar vs. Content Questions**: Polar easier to evaluate but lower ceiling (50% chance guessing)

- Failure signatures:
  - **Future-Date Degradation**: Accuracy drops for dates >2020 (see Figure 2, all models)
  - **Calendar Asymmetry**: Others-to-Gregorian consistently worse than Gregorian-to-Others
  - **Code Execution Errors**: Time Agent's ~4.7% failures stem from syntax bugs or logical errors

- First 3 experiments:
  1. **Reproduce baseline**: Run SPAN evaluation on GPT-4o and Claude-3.7-Sonnet for dates 1960-2060; confirm ~34.5% average accuracy.
  2. **Ablate Time Agent**: Remove search_calendar and force direct LLM reasoning; expect accuracy drop to baseline levels.
  3. **Test asymmetry reduction**: Fine-tune a smaller model on balanced Gregorian↔non-Gregorian conversion pairs; measure if Calendar Asymmetry Bias decreases.

## Open Questions the Paper Calls Out
None

## Limitations

- Cross-Calendar Entry Generation: The benchmark relies on accurate cross-calendar entries generated by the search_calendar interface, but the paper doesn't fully validate whether the underlying Python libraries produce consistent festival date mappings across all six calendars, particularly for edge cases like leap months or regional variations.

- Template Coverage: With only 12 question templates (6 date-based, 6 festival-based), the benchmark may not comprehensively test all temporal reasoning scenarios. The templates might miss complex edge cases such as century transitions, religious calendar exceptions, or cultural variations in festival observance.

- Evaluator Reliability: While human agreement rates (0.95-0.98) suggest the GPT-4o evaluator is reliable, the paper doesn't address potential systematic biases in how the evaluator interprets ambiguous temporal expressions or cultural context in festival-based questions.

## Confidence

**High Confidence**: The claim that template-driven dynamic instance generation mitigates data contamination is well-supported by the methodology description and addresses a recognized issue in LLM benchmarking.

**Medium Confidence**: The identification of Future-Date Degradation and Calendar Asymmetry Bias is empirically demonstrated across multiple LLMs, though the exact causes (data distribution vs. architectural limitations) remain speculative.

**Low Confidence**: The assertion that tool-augmented code generation is the optimal solution for cross-calendar reasoning lacks comparative analysis against other approaches like fine-tuning or specialized calendar reasoning modules.

## Next Checks

1. **Reproduce the Calendar Asymmetry Gap**: Test a subset of LLMs on both directions of conversion (Gregorian-to-Others and Others-to-Gregorian) for multiple evaluation dates to verify the 4-17% performance differential holds across the full 1960-2060 range.

2. **Validate Template Edge Cases**: Manually construct test cases for leap years, century transitions, and festival date variations (e.g., Chinese Lunar leap months) to assess whether the 12 templates adequately cover temporal reasoning complexity.

3. **Ablate Tool Dependencies**: Implement a version of the Time Agent that uses direct LLM reasoning for calendar conversions instead of code generation, measuring the accuracy drop to quantify the true contribution of the tool-augmented approach versus prompt engineering.