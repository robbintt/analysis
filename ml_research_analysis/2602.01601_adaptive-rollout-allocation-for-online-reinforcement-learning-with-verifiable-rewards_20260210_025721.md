---
ver: rpa2
title: Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable
  Rewards
arxiv_id: '2602.01601'
source_url: https://arxiv.org/abs/2602.01601
tags:
- gradient
- rloo
- training
- variance
- grpo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the inefficiency in group-based reinforcement
  learning methods, such as GRPO, which uniformly allocate rollouts across all training
  prompts regardless of their informativeness. To improve sampling efficiency, the
  authors introduce VIP, a Variance-Informed Predictive allocation strategy.
---

# Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards

## Quick Facts
- **arXiv ID:** 2602.01601
- **Source URL:** https://arxiv.org/abs/2602.01601
- **Reference count:** 40
- **Primary result:** VIP improves sampling efficiency and performance over uniform rollout allocation in RL with verifiable rewards

## Executive Summary
This paper addresses the inefficiency of uniform rollout allocation in group-based reinforcement learning methods like GRPO. The authors introduce VIP (Variance-Informed Predictive allocation), which predicts per-prompt success probabilities using a lightweight Gaussian process model and solves a convex optimization problem to allocate rollouts that minimize expected gradient variance under a fixed compute budget. VIP consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies across multiple benchmarks, with less than 1.2% runtime overhead.

## Method Summary
VIP uses a Gaussian process surrogate to predict success probabilities for each training prompt, then solves a convex optimization problem to allocate rollouts in a way that minimizes expected gradient variance given a fixed compute budget. The approach addresses the fundamental inefficiency of uniform rollout allocation in GRPO-like methods, where uninformative prompts consume resources that could be better spent on more promising ones. The method maintains computational efficiency through its lightweight GP surrogate while providing theoretical guarantees on variance reduction.

## Key Results
- VIP improved Pass@32 and Mean@32 by up to 12.3 and 6.3 percentage points on AIME2024 and AIME2025 benchmarks
- Achieved consistent improvements over uniform and heuristic allocation strategies
- Runtime overhead remained below 1.2% of total RL training time

## Why This Works (Mechanism)
The method works by predicting which prompts will yield the most informative gradients and allocating more rollouts to those prompts. By minimizing expected gradient variance through optimal allocation, VIP ensures that each rollout contributes maximally to the learning signal. The lightweight Gaussian process surrogate enables rapid prediction of prompt success probabilities, while the convex optimization formulation guarantees efficient allocation under budget constraints.

## Foundational Learning

**Variance reduction in RL**: Why needed - to improve sample efficiency; Quick check - compare gradient variance with/without allocation

**Gaussian process regression**: Why needed - to predict prompt success probabilities; Quick check - GP prediction accuracy vs ground truth

**Convex optimization**: Why needed - to solve rollout allocation problem efficiently; Quick check - solution feasibility and convergence

**Gradient-based RL**: Why needed - framework for understanding allocation impact; Quick check - gradient magnitude correlation with performance

**Rollout budgeting**: Why needed - to operate under compute constraints; Quick check - allocation sensitivity to budget changes

## Architecture Onboarding

**Component map**: GP surrogate -> Convex optimizer -> Rollout allocator -> RL trainer

**Critical path**: Prompt features → GP prediction → Allocation optimization → Rollout execution → Gradient update

**Design tradeoffs**: GP complexity vs prediction accuracy; Optimization precision vs runtime overhead; Allocation granularity vs computational cost

**Failure signatures**: GP predictions diverging from actual performance; Optimization becoming infeasible under tight budgets; Allocation instability across training iterations

**First experiments**:
1. Test GP prediction accuracy on held-out prompts
2. Verify convex optimization produces feasible allocations
3. Measure gradient variance reduction compared to uniform allocation

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Assumes verifiable rewards with known means and variances, limiting generalization to noisy or subjective reward signals
- Gaussian process surrogate assumes smooth, continuous reward surfaces that may not hold in complex reasoning domains
- Performance gains demonstrated primarily on AIME benchmarks, raising questions about broader generalization

## Confidence

**High confidence**: The mathematical formulation of variance-minimizing rollout allocation is sound and correctly implemented

**Medium confidence**: Empirical improvements on AIME benchmarks are real but may not transfer to other domains

**Medium confidence**: The 1.2% runtime overhead claim appears reasonable given the lightweight GP surrogate

## Next Checks

1. Test VIP on reward functions with higher noise levels and non-stationary characteristics to evaluate robustness
2. Compare against production-grade sampling strategies like prioritized experience replay or adaptive temperature-based selection
3. Evaluate performance degradation when rollout budgets are severely constrained (e.g., <16 total rollouts)