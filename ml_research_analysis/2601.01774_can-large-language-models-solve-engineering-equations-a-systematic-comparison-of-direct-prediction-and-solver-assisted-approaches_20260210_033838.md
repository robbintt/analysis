---
ver: rpa2
title: Can Large Language Models Solve Engineering Equations? A Systematic Comparison
  of Direct Prediction and Solver-Assisted Approaches
arxiv_id: '2601.01774'
source_url: https://arxiv.org/abs/2601.01774
tags:
- direct
- numerical
- problems
- equation
- equations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper systematically compares two approaches for using large
  language models (LLMs) to solve transcendental engineering equations: direct prediction
  (LLMs generate numerical solutions) versus solver-assisted computation (LLMs formulate
  equations and provide initial conditions while Newton-Raphson iteration computes
  the solution). The study evaluates six state-of-the-art models on 100 problems across
  seven engineering domains.'
---

# Can Large Language Models Solve Engineering Equations? A Systematic Comparison of Direct Prediction and Solver-Assisted Approaches

## Quick Facts
- arXiv ID: 2601.01774
- Source URL: https://arxiv.org/abs/2601.01774
- Reference count: 1
- Direct prediction yields mean relative errors of 0.765-1.262; solver-assisted computation achieves 0.225-0.301

## Executive Summary
This paper systematically compares two approaches for using large language models (LLMs) to solve transcendental engineering equations: direct prediction (LLMs generate numerical solutions) versus solver-assisted computation (LLMs formulate equations and provide initial conditions while Newton-Raphson iteration computes the solution). The study evaluates six state-of-the-art models on 100 problems across seven engineering domains. Direct prediction yields mean relative errors of 0.765-1.262, while solver-assisted computation achieves 0.225-0.301, representing 67.9-81.8% error reduction. The improvement is particularly dramatic in domains with exponential sensitivity (Electronics: 93.1%) but minimal in domains where LLMs have learned effective pattern recognition (Fluid Mechanics: 7.2%).

## Method Summary
The study evaluates six state-of-the-art LLMs (GPT-4, GPT-3.5, Claude 2, Llama 2, Mixtral 8x7B, and PaLM 2) on 100 transcendental engineering equations across seven domains: Thermodynamics, Fluid Mechanics, Heat Transfer, Materials Science, Electronics, Vibrations, and Machine Design. Two approaches are compared: direct prediction where models generate numerical solutions, and solver-assisted computation where models formulate equations and provide initial conditions for Newton-Raphson iteration. All models are tested using identical prompt formats with fixed temperature and repetition penalties. Performance is measured using mean relative error between predicted and actual solutions.

## Key Results
- Direct prediction yields mean relative errors of 0.765-1.262 across all models and domains
- Solver-assisted computation achieves mean relative errors of 0.225-0.301, representing 67.9-81.8% error reduction
- Domain-specific improvements vary widely: Electronics shows 93.1% improvement while Fluid Mechanics shows only 7.2%

## Why This Works (Mechanism)
Contemporary LLMs excel at symbolic manipulation and domain knowledge retrieval but struggle with precision-critical iterative arithmetic. The solver-assisted approach leverages LLMs' strengths in understanding engineering concepts, recognizing problem patterns, and formulating mathematical relationships while delegating numerical computation to classical solvers that handle iterative arithmetic with high precision. This division of labor plays to each system's strengths: LLMs as intelligent interfaces that understand context and generate symbolic representations, and numerical solvers as reliable engines for precise arithmetic operations.

## Foundational Learning
- Transcendental equations: Non-algebraic equations involving transcendental functions like exponential, logarithmic, or trigonometric functions. Why needed: These equations cannot be solved analytically and require numerical methods, making them ideal test cases for LLM performance. Quick check: Can the model correctly identify whether an equation is transcendental?
- Newton-Raphson iteration: A numerical method for finding roots of equations using iterative refinement. Why needed: Serves as the gold standard numerical solver in the solver-assisted approach. Quick check: Does the model understand the convergence criteria and iteration process?
- Relative error: The ratio of absolute error to the true value, used to measure solution accuracy. Why needed: Provides a scale-invariant metric for comparing performance across different magnitude problems. Quick check: Can the model calculate and interpret relative error values?
- Symbolic manipulation: The ability to work with mathematical expressions and equations in their symbolic form. Why needed: LLMs must correctly formulate equations from problem descriptions. Quick check: Can the model correctly rearrange equations to isolate variables?
- Domain-specific knowledge: Engineering principles and formulas specific to different fields. Why needed: LLMs must recognize which equations apply to different problem types. Quick check: Can the model correctly identify the governing equation for a given engineering scenario?

## Architecture Onboarding
Component map: Problem description -> LLM symbolic formulation -> Initial condition generation -> Numerical solver (Newton-Raphson) -> Final solution
Critical path: The solver-assisted approach succeeds when LLMs correctly identify the problem type, select the appropriate governing equation, formulate it symbolically, and provide reasonable initial conditions that ensure Newton-Raphson convergence.
Design tradeoffs: Direct prediction trades computational accuracy for simplicity, while solver-assisted computation trades model complexity for precision. The optimal approach depends on whether the application prioritizes speed or accuracy.
Failure signatures: Direct prediction fails through arithmetic errors and convergence to incorrect values. Solver-assisted computation fails when LLMs misidentify problem types or provide poor initial conditions leading to solver divergence.
First experiments:
1. Test a simple transcendental equation (e.g., x = e^(-x)) using both approaches to verify basic functionality
2. Evaluate model performance on equations from different domains to assess domain-specific capabilities
3. Vary initial conditions in the solver-assisted approach to determine sensitivity to starting values

## Open Questions the Paper Calls Out
None

## Limitations
- Study focuses exclusively on transcendental equations with single solutions; performance on systems of equations, multiple solutions, or differential equations remains unknown
- Results based on a curated dataset of 100 problems across seven domains, which may not capture full diversity of real-world engineering challenges
- Assumes Newton-Raphson as gold standard solver, which may introduce bias toward problems where this method performs well

## Confidence
High: The comparative methodology between direct prediction and solver-assisted approaches is sound, and the reported error reductions (67.9-81.8%) are statistically robust within the tested domain. The conclusion that LLMs struggle with precision-critical iterative arithmetic is well-supported by the data.

Medium: The domain-specific findings (e.g., 93.1% improvement in Electronics vs. 7.2% in Fluid Mechanics) are based on a limited sample size per domain and may reflect domain-specific dataset characteristics rather than fundamental model limitations.

Low: The extrapolation of results to broader engineering applications beyond transcendental equations with single solutions is speculative and requires additional validation.

## Next Checks
1. Test both approaches on systems of transcendental equations and problems with multiple solutions to determine if the solver-assisted advantage persists in more complex mathematical scenarios
2. Evaluate alternative numerical solvers (e.g., bisection method, secant method) in the solver-assisted framework to assess whether the observed performance gap is specific to Newton-Raphson or represents a more general pattern
3. Conduct experiments with varied prompting strategies, including few-shot examples, chain-of-thought prompting, and different temperature settings, to determine if direct prediction performance can be substantially improved through better prompting