---
ver: rpa2
title: 'HIDE and Seek: Detecting Hallucinations in Language Models via Decoupled Representations'
arxiv_id: '2506.17748'
source_url: https://arxiv.org/abs/2506.17748
tags:
- hide
- hallucination
- output
- detection
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of detecting hallucinations in
  large language models, which generate fluent but factually incorrect content. The
  authors propose HIDE (Hallucination Detection via Decoupled Representations), a
  single-pass, training-free method that measures statistical decoupling between input
  and output representations using the Hilbert-Schmidt Independence Criterion (HSIC).
---

# HIDE and Seek: Detecting Hallucinations in Language Models via Decoupled Representations

## Quick Facts
- **arXiv ID**: 2506.17748
- **Source URL**: https://arxiv.org/abs/2506.17748
- **Reference count**: 16
- **Primary result**: HIDE achieves ~29% relative AUC-ROC improvement over single-pass baselines and ~3% over multi-pass methods while reducing computation time by ~51%

## Executive Summary
HIDE (Hallucination Detection via Decoupled Representations) addresses the critical problem of detecting hallucinations in large language models by measuring statistical decoupling between input and output representations. The method uses the Hilbert-Schmidt Independence Criterion (HSIC) to quantify the statistical independence between hidden states of key tokens in the input and output sequences. By extracting embeddings during a single forward pass and computing a HIDE score, the approach achieves state-of-the-art performance while being significantly more efficient than multi-pass methods. The technique works across multiple question-answering datasets and demonstrates strong performance across different model architectures.

## Method Summary
HIDE operates by extracting hidden state representations of salient tokens from both input and output sequences during a single autoregressive generation pass. It uses KeyBERT to identify 20 semantically important tokens from each sequence, then computes the HSIC between these representations using an adapted estimator that ensures numerical stability. The method measures statistical dependence in Reproducing Kernel Hilbert Space (RKHS) using an RBF kernel, with lower HSIC scores indicating greater decoupling and higher likelihood of hallucination. Detection occurs through thresholding the HIDE score, typically around 0.12, without requiring any additional model training or multiple generations.

## Key Results
- Achieves ~29% relative improvement in AUC-ROC over best single-pass baseline across six models and four QA datasets
- Shows ~3% improvement over multi-pass state-of-the-art methods while reducing computation time by ~51%
- Demonstrates effectiveness in detecting both faithfulness and factuality hallucinations
- Performance remains largely agnostic to layer selection and kernel parameters

## Why This Works (Mechanism)

### Mechanism 1: Statistical Decoupling as Hallucination Signal
Hallucinations manifest as measurable statistical decoupling between input and output representations in LLM hidden states. When models generate hallucinated content, the output representation (H_out^ℓ) becomes statistically independent from the input representation (H_in^ℓ). HSIC quantifies this by computing the squared Hilbert-Schmidt norm of the cross-covariance operator between these representations in RKHS space—lower HSIC scores indicate greater independence (decoupling). This assumes that hallucinated outputs diverge from input context in a way captured by representational geometry rather than surface form.

### Mechanism 2: Single-Pass Efficiency via Representation Extraction
Detection occurs in one forward pass by extracting hidden states during generation, eliminating multi-generation overhead while preserving accuracy. During standard autoregressive generation, HIDE simultaneously captures H_in^ℓ and H_out^ℓ at layer ℓ, then computes the adapted HSIC estimator. No additional sampling or model forward passes are required—the information is already present in hidden states. This assumes single-generation hidden states contain sufficient signal for detection without needing contrastive comparison across multiple samples.

### Mechanism 3: Keyword-Based Token Alignment for Robust Measurement
KeyBERT selects representative tokens using maximal-marginal-relevance ranking, creating aligned sample sets for HSIC computation across variable-length sequences. This resolves dimension mismatch between input/output sequences while focusing computation on meaningful tokens. The approach assumes that important semantic tokens carry the primary hallucination signal, though performance is nearly identical between keyword and SVD-based alignment strategies.

## Foundational Learning

- **Concept: Hilbert-Schmidt Independence Criterion (HSIC)**
  - Why needed here: HSIC quantifies whether input and output representations are statistically dependent or independent, enabling detection of representational decoupling by measuring cross-covariance in RKHS space
  - Quick check question: Given two sets of embeddings X (input tokens) and Y (output tokens), would you expect HSIC to be high or low if the model is faithfully conditioning its output on the input context?

- **Concept: Reproducing Kernel Hilbert Spaces (RKHS)**
  - Why needed here: HSIC operates in RKHS to detect non-linear dependencies that simple correlation would miss. The RBF kernel implicitly maps representations to an infinite-dimensional space where independence becomes tractable
  - Quick check question: Why compute HSIC in RKHS instead of directly calculating Pearson correlation between flattened input/output embeddings?

- **Concept: Hidden State Representations Across Transformer Layers**
  - Why needed here: HIDE extracts representations from specific decoder layers, with different layers encoding different abstraction levels. Early layers capture syntax, later layers capture task-specific semantics
  - Quick check question: Based on the near-agnostic layer performance, would you expect dramatically different results using layer 5 vs layer 25 of a 32-layer model?

## Architecture Onboarding

- **Component map**: Prompt → Generation (capture hidden states) → Token Selection → HSIC Computation → Threshold Decision → Hallucination Label
- **Critical path**: Single forward pass extracts H_in^ℓ and H_out^ℓ, KeyBERT selects neff salient tokens, HSIC estimator computes adapted \HSIC^HIDE, binary decision via threshold τ
- **Design tradeoffs**:
  - Keyword vs SVD alignment: Keywords enable interpretability; SVD guarantees mathematical orthogonality. Performance equivalent, paper defaults to keywords
  - Layer selection: Middle layers chosen based on prior work; ablation shows near-agnostic performance
  - Token budget (neff): 15-20 near-optimal with diminishing returns beyond; higher budgets increase compute without accuracy gain
- **Failure signatures**:
  - Single-token outputs: Lemma 3 forces score=0 for neff=1, requiring fallback to perplexity
  - Input verbatim copying: Artificial HSIC inflation when output repeats input keywords
  - Extreme sequence length imbalance: NQ shows earlier plateau when output length constrains effective neff
- **First 3 experiments**:
  1. Validate on controlled hallucinations: Generate outputs with deliberately incorrect facts; verify HIDE scores <0.12 threshold
  2. Layer sensitivity check: Extract representations from layers 1, 8, 16, 24, 32; plot AUC-ROC vs layer to confirm agnostic behavior
  3. Token budget calibration: Sweep neff ∈ {5, 10, 15, 20, 30, 50} on domain's sequence length distribution; identify plateau point

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to question-answering datasets with open-source LMs; effectiveness on other tasks like summarization or code generation remains unproven
- Default neff=20 parameter optimized on English QA datasets; different domains or languages may require different token budgets
- Performance may degrade with significant sequence length imbalance between inputs and outputs, limiting applicability to constrained-output tasks

## Confidence
- **High Confidence (8-10/10)**: Claims about computational efficiency gains (~51% reduction in time vs multi-pass methods) and general effectiveness across tested model/dataset combinations
- **Medium Confidence (5-7/10)**: Claims about HIDE being "largely agnostic to layer and kernel selection" supported by ablation studies but not fully characterized
- **Low Confidence (1-4/10)**: Claims about distinguishing factuality vs faithfulness hallucinations lack clear empirical separation in results

## Next Checks
1. **Cross-Task Validation**: Evaluate HIDE on non-QA tasks like summarization where hallucinations manifest differently; generate summaries with controlled factual errors and measure HIDE scores
2. **Layer and Kernel Sensitivity Analysis**: Conduct granular ablation across all layers and multiple kernel bandwidths; plot AUC-ROC vs layer depth and kernel parameter with confidence intervals
3. **Multilingual and Cross-Domain Transfer**: Test HIDE on non-English QA datasets or cross-domain knowledge bases to verify whether statistical decoupling signal transfers across languages and domains