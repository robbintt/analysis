---
ver: rpa2
title: 'TSVer: A Benchmark for Fact Verification Against Time-Series Evidence'
arxiv_id: '2511.01101'
source_url: https://arxiv.org/abs/2511.01101
tags:
- time
- series
- evidence
- data
- claims
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TSVer introduces a new benchmark for fact verification grounded
  in real-world time-series evidence, addressing the challenge of reasoning over temporal
  and numerical data. The dataset comprises 287 real-world claims from 38 fact-checking
  organizations, paired with 400 curated time series covering diverse domains.
---

# TSVer: A Benchmark for Fact Verification Against Time-Series Evidence

## Quick Facts
- arXiv ID: 2511.01101
- Source URL: https://arxiv.org/abs/2511.01101
- Authors: Marek Strong; Andreas Vlachos
- Reference count: 21
- Key outcome: 287 real-world claims benchmark with 400 time series; baseline shows even state-of-the-art models achieve only 63.37 accuracy on verdicts and 48.63 Ev2R on justifications.

## Executive Summary
TSVer introduces a new benchmark for fact verification grounded in real-world time-series evidence, addressing the challenge of reasoning over temporal and numerical data. The dataset comprises 287 real-world claims from 38 fact-checking organizations, paired with 400 curated time series covering diverse domains. Each claim is annotated with relevant time frames, verdicts, and justifications, using an LLM-assisted multi-step annotation process achieving a kappa of 0.745 on verdicts. A baseline pipeline shows that even state-of-the-art models like Gemini-2.5-Pro struggle with this task, achieving 63.37 accuracy on verdicts and 48.63 Ev2R score on justifications. The dataset supports evaluation of evidence selection and reasoning quality, advancing research in explainable, evidence-based fact verification.

## Method Summary
The paper introduces TSVer, a benchmark for fact verification against time-series evidence. The dataset includes 287 real-world claims paired with 400 time series from Our World in Data, annotated for verdicts (SUPPORTS, REFUTES, NOT ENOUGH INFO, CONFLICTING EVIDENCE) and justifications. Claims are processed through a pipeline: (1) time-series retrieval using few-shot LLM prompting, (2) filtering for relevant countries and time ranges, (3) verdict and justification generation via zero-shot LLM prompting. The baseline uses few-shot examples for retrieval and zero-shot for reasoning, with metrics including TSCS for retrieval quality and Ev2R for justification evaluation.

## Key Results
- TSVer dataset: 287 claims, 400 time series, 4-class verdict labels
- Baseline accuracy: Gemini-2.5-Pro achieves 63.37 accuracy and 48.63 Ev2R score
- Retrieval bottleneck: Llama-3.1-8B fails on 78.85% of instances due to context overflow
- Inter-annotator agreement: kappa=0.745 on verdicts using LLM-assisted multi-step annotation

## Why This Works (Mechanism)

### Mechanism 1
LLM-assisted multi-step annotation improves annotation quality for time-series evidence alignment by decomposing the task into pre-alignment, Phase 1 (evidence selection with article access), and Phase 2 (verdict/justification without article access). This staged approach with different information access levels produces higher-quality ground truth than single-pass annotation, achieving kappa=0.745 inter-annotator agreement.

### Mechanism 2
Time-series retrieval quality acts as a bottleneck for downstream verification performance because poor retrieval cascades through the pipeline. Over-retrieval exceeds context windows causing inference failures, while under-retrieval omits evidence needed for correct verdicts. The TSCS metric captures this by jointly measuring dataset selection accuracy (F1) and temporal alignment (Jaccard Index).

### Mechanism 3
Raw time-series tokenization via byte-pair encoding (BPE) creates inefficient representations that impair reasoning. Floating-point numbers tokenized through BPE produce inconsistent, non-semantic token sequences. Combined with large series length (average ~20,000 records), this creates context window pressure and disrupts numerical reasoning patterns.

## Foundational Learning

- **Time-series representation for LLMs**: Why needed - Raw time series exceed context limits; baseline uses markdown tables but this is inefficient. Understanding alternatives (quantization, dedicated encoders, statistical summaries) is essential for system design. Quick check - Can you explain why converting a 50,000-record time series to markdown format might fail, and name two alternative representations mentioned in the paper?

- **Evidence retrieval as a multi-stage process**: Why needed - The baseline pipeline separates retrieval (series + countries + time ranges) from reasoning. Understanding this decomposition helps debug where failures originate. Quick check - What three pieces of information does the retrieval stage need to identify before verdict generation can proceed?

- **Inter-annotator agreement metrics (Randolph's kappa)**: Why needed - The paper reports kappa=0.745 as evidence of annotation quality. Understanding what this means (substantial agreement) helps evaluate benchmark reliability claims. Quick check - Why might Randolph's free-marginal kappa be preferred over Fleiss' kappa for this annotation task?

## Architecture Onboarding

- **Component map**: Claim preprocessing (HeidelTime, spaCy, Llama-3.1-8B) -> Evidence database (400 OWID time series) -> Retrieval stage (few-shot LLM for series, countries, time ranges) -> Verdict generation (zero-shot LLM with markdown tables) -> Evaluation (TSCS, Ev2R, accuracy/F1)

- **Critical path**: Claim → Time series retrieval → Country filtering → Time range selection → Data slicing → Verdict + Justification generation

- **Design tradeoffs**: Few-shot retrieval vs. end-to-end (few-shot for retrieval due to prompt space limits), precomputed statistics vs. raw data (statistics aid annotators but not models), Markdown tables vs. alternative encodings (baseline uses pandas markdown for simplicity)

- **Failure signatures**: Context length overflow (Llama-3.1-8B failed 78.85% of instances), low TSCS with reasonable accuracy (would indicate shortcuts), high METEOR with low Ev2R (observed with Ministral-3B vs GPT-4)

- **First 3 experiments**: 
  1. Retrieval ablation: Test verdict accuracy with gold-standard time series/time ranges vs. model-retrieved evidence
  2. Context length stress test: Progressively truncate time series and measure accuracy degradation
  3. Tokenization comparison: Compare BPE tokenization vs. quantized discrete tokens on a subset of claims

## Open Questions the Paper Calls Out

- **Can alternative input representations improve verification accuracy?** The paper suggests exploring quantization or dedicated time-series encoders over raw text linearization to handle continuous numerical data better.

- **How can retrieval mechanisms be optimized?** The discussion highlights that time-series data cannot be easily truncated without losing critical patterns, leading to context length errors that smaller models cannot overcome.

- **Does integrating visual chart representations help?** The limitations note that excluding multi-modal evidence like charts may miss important reasoning cues used in real-world fact-checking.

## Limitations
- Dataset size is small (287 claims) and domain-specific, limiting generalizability
- Annotation pipeline relies on LLM assistance with only moderate inter-rater agreement (kappa=0.745)
- Baseline pipeline asymmetry (few-shot retrieval, zero-shot reasoning) may not reflect optimal design
- LLM-as-judge evaluation (Ev2R with Gemini-2.5-Flash) may introduce scoring biases
- Limited prior work directly addressing time-series fact verification suggests this is an emerging research area

## Confidence
- **High confidence**: Benchmark construction methodology, dataset creation process, and evaluation metrics are clearly specified and reproducible. Retrieval bottleneck finding is directly supported by experimental results.
- **Medium confidence**: LLM-assisted annotation quality improvement claim is supported by kappa=0.745 but lacks comparative analysis with single-pass approaches. Tokenization inefficiency hypothesis is mentioned but not experimentally validated.
- **Low confidence**: Ev2R score's ability to capture true justification quality is uncertain, as evidenced by Ministral-3B and GPT-4 having similar METEOR scores despite large accuracy gaps.

## Next Checks
1. **Gold-standard retrieval ablation**: Provide models with ground-truth time series and time ranges for each claim, then measure verdict accuracy to isolate retrieval vs. reasoning bottlenecks.

2. **Annotation pipeline comparison**: Re-annotate 50 claims using multi-step approach versus traditional single-pass annotation, comparing inter-rater agreement and annotation time.

3. **Tokenization intervention experiment**: Implement quantized time-series representation (K-means or VQ-VAE) and compare against raw BPE tokenization on a subset of 20 claims, measuring changes in accuracy and context window utilization.