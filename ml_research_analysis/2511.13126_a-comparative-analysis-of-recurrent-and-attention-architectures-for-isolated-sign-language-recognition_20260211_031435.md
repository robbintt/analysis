---
ver: rpa2
title: A Comparative Analysis of Recurrent and Attention Architectures for Isolated
  Sign Language Recognition
arxiv_id: '2511.13126'
source_url: https://arxiv.org/abs/2511.13126
tags:
- sign
- language
- recognition
- accuracy
- architectures
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a systematic comparison of recurrent and attention-based
  architectures for isolated sign language recognition, evaluating ConvLSTM and Vanilla
  Transformer models on Azerbaijani and American Sign Language datasets. The attention-based
  Transformer consistently outperformed the recurrent ConvLSTM across both datasets,
  achieving 76.8% Top-1 accuracy on AzSLD and 88.3% on WLASL, compared to ConvLSTM's
  70.5% and 85.3% respectively.
---

# A Comparative Analysis of Recurrent and Attention Architectures for Isolated Sign Language Recognition

## Quick Facts
- **arXiv ID**: 2511.13126
- **Source URL**: https://arxiv.org/abs/2511.13126
- **Reference count**: 22
- **Primary result**: Attention-based Transformer outperformed recurrent ConvLSTM on sign language recognition, achieving 76.8% vs 70.5% Top-1 accuracy on AzSLD and 88.3% vs 85.3% on WLASL

## Executive Summary
This study presents a systematic comparison of recurrent (ConvLSTM) and attention-based (Vanilla Transformer) architectures for isolated sign language recognition. The authors evaluate both architectures on Azerbaijani Sign Language Dataset (AzSLD) and the large-scale WLASL dataset using signer-independent evaluation. The Transformer consistently outperforms ConvLSTM across both datasets, achieving 76.8% Top-1 accuracy on AzSLD and 88.3% on WLASL, compared to ConvLSTM's 70.5% and 85.3% respectively. The performance gap narrows with larger datasets, suggesting Transformers exhibit better data efficiency while ConvLSTMs scale more effectively with increased data. These findings indicate that architectural selection should depend on specific application requirements, with Transformers optimal for high-accuracy applications and ConvLSTMs better suited for real-time, resource-constrained scenarios.

## Method Summary
The study employs a controlled comparison of ConvLSTM and Vanilla Transformer architectures on isolated sign language recognition. Both models process 64-frame sequences of 63-dimensional MediaPipe Holistic features (21 hand landmarks × 3 channels) extracted from sign language videos. The ConvLSTM uses 3×3 convolutional filters with 128 channels followed by 256 LSTM units, while the Transformer employs a 6-layer encoder with 8 attention heads and 512-dimensional embeddings. Training uses TensorFlow 2.15.0 with Adam optimizer, cyclical learning rates, curriculum learning (progressive frame count increase), and extensive augmentation. Evaluation employs signer-independent 5-fold cross-validation on AzSLD (100 classes, 1,800 samples, 8 signers) and WLASL2000 (2,000 classes, 21,083 samples, 119 signers).

## Key Results
- Transformer achieved 76.8% Top-1 accuracy on AzSLD versus ConvLSTM's 70.5% (6.3pp gap)
- On WLASL, Transformer reached 88.3% Top-1 accuracy versus ConvLSTM's 85.3% (3.0pp gap)
- Performance gap narrowed with larger datasets, suggesting ConvLSTM scales better with data
- Transformer excelled in signer-independent evaluation while ConvLSTM offered computational efficiency advantages

## Why This Works (Mechanism)

### Mechanism 1: Self-Attention for Global Temporal Dependencies
The attention-based Transformer captures global dependencies across sign language video frames more effectively than sequential processing. Self-attention computes weighted relationships between all frame pairs simultaneously, regardless of temporal distance, enabling the model to connect distant gesture components that jointly define sign meaning. Core assumption: sign semantics require understanding relationships between temporally distant frames, not just local transitions. Evidence: Transformer consistently outperformed ConvLSTM across datasets, achieving up to 76.8% Top-1 accuracy on AzSLD. Break condition: if signs are primarily local or computational budget prohibits full-sequence attention.

### Mechanism 2: Data Efficiency Through Attention Inductive Biases
Transformer architectures demonstrate superior performance under data-constrained conditions compared to recurrent architectures. Attention directly models frame-to-frame relationships without requiring gradient flow through sequential timesteps, potentially requiring fewer samples to learn effective temporal representations. Core assumption: attention's inductive biases are better matched to sign language's multimodal temporal structure than sequential processing. Evidence: ConvLSTM lags in recognition accuracy, particularly on smaller datasets. Break condition: with very limited data (<100 samples per class), Transformers may overfit due to larger parameter counts.

### Mechanism 3: Computational Efficiency Through Sequential Processing
ConvLSTM provides computational advantages suitable for real-time applications despite lower accuracy. Sequential frame-by-frame processing with fixed-size hidden states limits memory and computation growth with sequence length (linear complexity), unlike attention's quadratic scaling with sequence length. Core assumption: real-time applications have latency thresholds where accuracy trade-offs become acceptable. Evidence: ConvLSTM, while more computationally efficient, lags in recognition accuracy. Break condition: if sequences are short (≤32 frames) or hardware supports efficient attention kernels.

## Foundational Learning

- **Concept**: Self-Attention Mechanism
  - **Why needed here**: Understanding how Transformers process 64-frame sequences requires grasping pairwise attention computation across all timesteps.
  - **Quick check question**: Given 64 frames, how many pairwise attention scores does a single head compute per forward pass?

- **Concept**: ConvLSTM (Convolutional LSTM)
  - **Why needed here**: This hybrid architecture differs from sequential CNN→LSTM pipelines by integrating convolutions directly within LSTM cells.
  - **Quick check question**: How does ConvLSTM's internal structure differ from applying CNN feature extraction followed by standard LSTM?

- **Concept**: Signer-Independent Evaluation
  - **Why needed here**: The paper uses 5-fold cross-validation where test signers never appear in training—a critical real-world generalization constraint.
  - **Quick check question**: Why might a model achieve high signer-dependent accuracy but fail signer-independent evaluation?

## Architecture Onboarding

- **Component map**: MediaPipe Holistic → 63-dim feature vectors → (ConvLSTM or Transformer) → Classification head → Top-1/Top-5 predictions

- **Critical path**: MediaPipe Holistic extracts 21 hand landmarks × 3 channels = 63 features/frame → Temporal alignment to exactly 64 frames via cubic spline interpolation → Either ConvLSTM sequential processing OR Transformer parallel attention → Classification produces Top-1/Top-5 predictions

- **Design tradeoffs**:
  - **Accuracy vs. Latency**: Transformer (+3–6pp accuracy) vs ConvLSTM (lower inference time, linear vs quadratic scaling)
  - **Data Scale**: Transformers preferred for small-medium datasets; ConvLSTMs close gap at scale
  - **Deployment Context**: Offline/archival (Transformer) vs real-time/mobile (ConvLSTM)

- **Failure signatures**:
  - Transformer underfitting small datasets may indicate over-regularization or insufficient warmup
  - ConvLSTM confusing signs with similar starts but different endings suggests long-range dependency failure
  - Both failing signer-independence signals overfitting to signer-specific visual features

- **First 3 experiments**:
  1. **Baseline replication**: Train both architectures on a WLASL subset (e.g., 500 classes) with identical hyperparameters; expect ~3–4pp Transformer advantage.
  2. **Data scaling curve**: Vary training data from 10%–100% and plot accuracy for both; verify gap narrows as data increases.
  3. **Latency benchmark**: Measure per-sample inference time on identical hardware; quantify ConvLSTM efficiency advantage.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: At what dataset scale would the performance gap between Transformer and ConvLSTM architectures fully converge, if at all?
- **Basis in paper**: Authors state "the improvement curve appears steeper for recurrent models, suggesting potential convergence of performance with sufficiently large datasets" but the exact convergence point remains unknown.
- **Why unresolved**: The study only evaluated two datasets of limited size (1,800 and 21,083 samples), and the 6.3-point gap on AzSLD reduced to only 3.0 points on WLASL—insufficient data to extrapolate convergence behavior.
- **What evidence would resolve it**: Systematic evaluation across a wider range of dataset sizes (e.g., 50K, 100K, 500K samples) with controlled vocabulary complexity.

### Open Question 2
- **Question**: Can hybrid architectures that dynamically allocate processing between attention and recurrent modules achieve both the accuracy of Transformers and the efficiency of ConvLSTMs?
- **Basis in paper**: Authors explicitly call for "hybrid systems that dynamically allocate processing between attention and recurrent modules based on sign complexity—a promising direction for achieving both accuracy and efficiency."
- **Why unresolved**: The study only evaluated pure architectures; no hybrid model was implemented or tested to validate this hypothesis.
- **What evidence would resolve it**: Implementation and evaluation of adaptive hybrid architectures with complexity-based routing mechanisms on the same datasets.

### Open Question 3
- **Question**: What are the quantitative computational trade-offs (inference latency, memory footprint, FLOPs) between these architectures under real-time deployment constraints?
- **Basis in paper**: The paper claims ConvLSTM offers "computational efficiency advantages" and is "better suited for real-time, resource-constrained scenarios," but provides no actual latency measurements or resource utilization metrics.
- **Why unresolved**: Only accuracy metrics were reported; computational efficiency claims remain qualitative assertions without empirical support.
- **What evidence would resolve it**: Benchmarking both architectures on standardized hardware with measurements of inference time per sample, peak memory usage, and energy consumption.

## Limitations
- **Data scope limitations**: Both datasets use MediaPipe Holistic skeletal features rather than raw video frames, limiting generalizability to other feature extraction methods.
- **Hyperparameter uncertainty**: Exact learning rate schedule parameters and internal MediaPipe Holistic configuration remain unspecified, potentially affecting training stability.
- **Mechanism validation gaps**: The study lacks ablation studies isolating specific architectural components or direct analysis of learned temporal dependencies to confirm hypothesized mechanisms.

## Confidence
- **High confidence**: The comparative accuracy findings between ConvLSTM and Transformer architectures are well-supported by the reported results across two independent datasets.
- **Medium confidence**: The hypothesized mechanisms explaining performance differences are theoretically plausible but lack direct empirical validation within this study.
- **Low confidence**: Claims about optimal deployment contexts are extrapolations from synthetic benchmarks rather than real-world deployment evidence.

## Next Checks
1. **Cross-dataset generalization**: Train the winning architecture (Transformer) on AzSLD and evaluate directly on WLASL (or vice versa) to measure true cross-dataset performance and signer generalization beyond the 5-fold splits.

2. **Ablation analysis**: Systematically remove architectural components (positional encoding, attention heads, feed-forward layers) from the Transformer to quantify their individual contributions to the accuracy advantage over ConvLSTM.

3. **Real-time deployment benchmark**: Measure actual inference latency on mobile/embedded hardware (e.g., NVIDIA Jetson, smartphone) for both architectures using the reported models to validate computational efficiency claims beyond theoretical complexity analysis.