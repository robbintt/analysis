---
ver: rpa2
title: 'ReLAM: Learning Anticipation Model for Rewarding Visual Robotic Manipulation'
arxiv_id: '2509.22402'
source_url: https://arxiv.org/abs/2509.22402
tags:
- learning
- relam
- reward
- anticipation
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ReLAM, a framework for visual robotic manipulation
  that addresses the challenge of reward design by automatically generating dense,
  structured rewards from action-free video demonstrations. ReLAM extracts task-relevant
  keypoints from videos, learns an anticipation model to predict intermediate subgoals,
  and uses keypoint distances to provide continuous rewards for training a low-level
  goal-conditioned policy under hierarchical reinforcement learning.
---

# ReLAM: Learning Anticipation Model for Rewarding Visual Robotic Manipulation

## Quick Facts
- arXiv ID: 2509.22402
- Source URL: https://arxiv.org/abs/2509.22402
- Reference count: 28
- Achieves 100% success rate on Meta-World Drawer Open and Door Open tasks, outperforming Diffusion Reward (80% and 100%) and DACfO (71.3% and 70%)

## Executive Summary
ReLAM addresses the challenge of reward design in visual robotic manipulation by automatically generating dense, structured rewards from action-free video demonstrations. The framework extracts task-relevant keypoints from videos, learns an anticipation model to predict intermediate subgoals, and uses keypoint distances to provide continuous rewards for training a low-level goal-conditioned policy under hierarchical reinforcement learning. By eliminating the need for manually designed reward functions and action labels, ReLAM achieves state-of-the-art performance on long-horizon manipulation tasks while significantly accelerating learning compared to existing methods.

## Method Summary
ReLAM operates in two stages: first, it processes action-free video demonstrations to extract task-relevant keypoints and train an anticipation model that predicts intermediate subgoals; second, it uses these predictions to construct dense, structured rewards for training a low-level goal-conditioned policy via hierarchical reinforcement learning. The anticipation model employs EVF-SAM for segmentation, CoTracker3 for point tracking, and an autoregressive transformer to predict keypoint sequences. The policy receives continuous rewards based on keypoint distances, with additional stage-success and final-success bonuses, enabling efficient learning of complex manipulation tasks from raw visual inputs.

## Key Results
- Achieves 100% success rate on Meta-World Drawer Open and Door Open tasks
- Outperforms Diffusion Reward (80% and 100%) and DACfO (71.3% and 70%) on the same tasks
- Demonstrates strong generalization with 89.3% and 88.0% success rates on ManiSkill Push Cube and Pick Cube tasks respectively

## Why This Works (Mechanism)
The method works by converting unstructured video demonstrations into structured, learnable rewards through keypoint-based subgoal prediction. By extracting semantically meaningful keypoints that track object parts across demonstrations, ReLAM creates a representation that captures task-relevant information while being robust to visual variations. The anticipation model learns to predict intermediate states, providing the policy with intermediate targets that guide learning toward successful task completion. This approach transforms the sparse, binary success signal into dense, continuous rewards that accelerate policy learning.

## Foundational Learning
- **Hierarchical Reinforcement Learning**: Why needed: Enables decomposition of complex tasks into manageable subgoals. Quick check: Verify hierarchical structure improves sample efficiency vs flat RL.
- **Keypoint Detection and Tracking**: Why needed: Provides task-relevant visual features for reward computation. Quick check: Ensure keypoints consistently track object parts across demonstrations.
- **Video Anticipation Models**: Why needed: Predicts intermediate states to guide policy learning. Quick check: Validate anticipation accuracy on held-out demonstration frames.
- **Distance-based Reward Shaping**: Why needed: Converts visual similarity into continuous learning signals. Quick check: Confirm reward correlates with task progress and improves learning speed.

## Architecture Onboarding
- **Component Map**: Video Demos -> Keypoint Extraction -> Anticipation Model -> Subgoal Prediction -> Policy Training -> Manipulation Execution
- **Critical Path**: Keypoint Extraction -> Anticipation Model -> Reward Computation -> Policy Optimization
- **Design Tradeoffs**: Dense rewards vs computational overhead of keypoint processing; hierarchical control vs increased system complexity; off-policy vs on-policy learning efficiency
- **Failure Signatures**: Inconsistent keypoint detection leads to noisy rewards; poor anticipation accuracy causes policy confusion; incorrect threshold parameters result in sparse or uninformative rewards
- **First Experiments**: 1) Validate keypoint extraction quality on held-out demonstrations; 2) Test anticipation model accuracy for predicting subgoals; 3) Verify reward computation produces meaningful gradients for policy learning

## Open Questions the Paper Calls Out
None specified in the paper.

## Limitations
- Critical hyperparameters (motion threshold Θ, keyframe bounds m/M, stage-success threshold θₛ) are unspecified, creating uncertainty in reproduction
- Performance depends heavily on high-quality video demonstrations with consistent camera viewpoints and lighting
- Assumes task-relevant keypoints can be reliably extracted across all demonstrations, which may not hold for complex or deformable objects

## Confidence
- **High Confidence**: Overall framework design and reported performance trends are consistent across multiple environments
- **Medium Confidence**: Quantitative results due to critical unspecified hyperparameters that could significantly impact reproduction success
- **Low Confidence**: Real-world applicability without additional robustness testing for varying environmental conditions

## Next Checks
1. Conduct hyperparameter sensitivity analysis by systematically varying motion threshold Θ and keyframe selection bounds (m, M) to identify their impact on keypoint quality and downstream policy performance

2. Test alternative reward formulations (e.g., Gaussian vs. piecewise linear) and validate the impact of stage-success threshold θₛ on learning stability and final performance

3. Evaluate ReLAM on tasks with different camera viewpoints, lighting conditions, and object properties to assess robustness beyond controlled Meta-World and ManiSkill environments