---
ver: rpa2
title: 'Revisiting Rule-Based Stuttering Detection: A Comprehensive Analysis of Interpretable
  Models for Clinical Applications'
arxiv_id: '2508.16681'
source_url: https://arxiv.org/abs/2508.16681
tags:
- detection
- speech
- clinical
- stuttering
- rule-based
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Rule-based stuttering detection offers complete interpretability
  crucial for clinical applications. The proposed framework uses acoustic feature
  extraction and hierarchical decision structures with speaking-rate normalization
  to detect prolongations, repetitions, and blocks.
---

# Revisiting Rule-Based Stuttering Detection: A Comprehensive Analysis of Interpretable Models for Clinical Applications

## Quick Facts
- arXiv ID: 2508.16681
- Source URL: https://arxiv.org/abs/2508.16681
- Reference count: 40
- Rule-based stuttering detection achieves 97-99% accuracy for prolongation detection with 6% F1 gap to neural models

## Executive Summary
This paper presents a rule-based stuttering detection framework that prioritizes interpretability and clinical auditability over marginal accuracy gains. The system uses hierarchical decision structures with speaking-rate normalization to detect prolongations, repetitions, and blocks through multi-level acoustic feature analysis. While achieving slightly lower F1 scores than neural models (0.86 vs 0.87 average), the approach maintains complete transparency in decision-making, making it suitable for clinical applications where explanations are critical. The framework demonstrates particular strength in prolongation detection and robustness across varying speaking rates.

## Method Summary
The framework implements a hierarchical detection cascade with three stages: prolongation detection using spectral similarity, F0 stability, and harmonic-to-noise ratio; repetition detection via autocorrelation and DTW distance; and block detection through silence and struggle pattern identification. Key innovations include speaking-rate normalized duration thresholds (T_min = 1.2/speaking_rate) and multi-feature fusion requiring concurrent spectral, prosodic, and voicing evidence. The system processes audio at 16kHz with pre-emphasis 0.97, extracts MFCCs, F0, HNR, spectral flux, and energy features, then applies precedence-based conflict resolution (Blocks > Sound Repetitions > Prolongations > Word Repetitions) with 100ms minimum separation constraints.

## Key Results
- Prolongation detection achieves 97-99% accuracy across datasets
- Overall F1 scores: 0.86 (UCLASS), 0.83 (FluencyBank), 0.74 (SEP-28k)
- 6% performance gap to neural models considered acceptable given clinical priority of transparency
- Speaking-rate normalization maintains robust performance across 0.5× to 2.0× rate variations
- Hierarchical structure excels particularly at prolongation detection

## Why This Works (Mechanism)

### Mechanism 1: Speaking-Rate Normalization for Adaptive Duration Thresholds
Adaptive thresholds that scale inversely with speaking rate maintain detection accuracy across rate variations that would catastrophically degrade fixed-threshold systems. Compute normalized duration threshold as T_min = α/SR where α ≈ 1.2 and SR is estimated syllables/second. A segment lasting 400ms qualifies as a prolongation at fast speech rates but not at slow rates, matching clinical judgment.

### Mechanism 2: Hierarchical Detection Cascade with Precedence-Based Conflict Resolution
Processing dysfluency types in sequence with explicit precedence rules prevents overlapping detections from producing contradictory outputs. Stage 1 detects prolongations via spectral stability; Stage 2 identifies repetitions via autocorrelation and DTW; Stage 3 flags blocks via silence/struggle patterns. Overlapping events resolve by precedence: Blocks > Sound Repetitions > Prolongations > Word Repetitions.

### Mechanism 3: Multi-Feature Fusion for Prolongation Discrimination
Combining spectral similarity, F0 stability, and harmonic-to-noise ratio reduces false positives from natural vowel extensions. For each frame, compute: (1) MFCC correlation > 0.92, (2) |F0[t+1] - F0[t]| < 15 Hz, (3) HNR > 10 dB. All three conditions must hold to extend a prolongation segment, ensuring acoustic evidence across spectral, prosodic, and voicing domains.

## Foundational Learning

- Concept: **MFCC (Mel-Frequency Cepstral Coefficients)**
  - Why needed here: Core feature for spectral similarity measurement; frame-to-frame correlation indicates prolongation when sustained >0.92.
  - Quick check question: If two consecutive frames have MFCC correlation of 0.95 for 300ms at speaking rate 3 syll/sec, does this exceed the normalized threshold?

- Concept: **Forced Alignment**
  - Why needed here: Provides phone-level temporal boundaries required for word repetition detection and duration measurement.
  - Quick check question: How would forced alignment output enable distinguishing "word repetition" from "sound repetition"?

- Concept: **Autocorrelation Function (ACF) for Periodicity Detection**
  - Why needed here: Repetition detection relies on identifying quasi-periodic patterns in amplitude/spectral domains via ACF peaks.
  - Quick check question: If ACF(τ) peaks at lag τ=150ms, what repetition rate does this suggest?

## Architecture Onboarding

- Component map:
Audio Input → Preprocessing (16kHz, -20dB LUFS, pre-emphasis 0.97) → Feature Extraction (MFCC, F0/HNR, Spectral, Energy) → Speaking Rate Estimation (syllable nuclei detection) → Hierarchical Detection: Stage 1: Prolongation (rate-normalized, multi-feature) → Stage 2: Repetition (ACF + DTW) → Stage 3: Blocks (silence + struggle detection) → Conflict Resolution (precedence + separation constraints) → Output: Timestamped dysfluency segments with confidence

- Critical path:
1. Speaking rate estimation accuracy directly determines threshold calibration quality
2. MFCC frame-to-frame correlation is the primary prolongation indicator
3. Conflict resolution precedence determines final labels for overlapping events

- Design tradeoffs:
  - Interpretability vs. accuracy: 6% F1 gap to neural models traded for full decision traceability
  - Prolongation precision vs. block detection: Rule system excels at prolongations (0.98 F1) but underperforms on blocks (0.69 F1 vs 0.79 neural)
  - Fixed vs. adaptive thresholds: Adaptive enables cross-rate robustness but requires reliable rate estimation
  - Strict multi-feature fusion vs. recall: Three-condition AND logic reduces false positives but may miss edge cases

- Failure signatures:
  - Boundary imprecision (31% of errors): Onset/offset within 50ms of ground truth
  - Missed coarticulation (24%): SR immediately before prolongation obscures secondary event
  - False positive hesitations (18%): Natural pauses flagged as blocks when duration near threshold
  - Environmental noise (12%): Background speech interferes with syllable nuclei detection

- First 3 experiments:
1. Rate robustness validation: Apply WSOLA time-stretching at 0.5×, 0.75×, 1.0×, 1.5×, 2.0× to UCLASS samples; compare fixed-threshold vs. normalized F1 scores. Expect fixed to collapse at extremes, normalized to remain stable.
2. Per-dysfluency threshold sensitivity: Sweep θ_mfcc from 0.88-0.96, θ_f0 from 10-20 Hz on FluencyBank validation set. Plot F1 contours to verify optimal values (0.92, 15 Hz) generalize beyond UCLASS.
3. Cross-corpus generalization: Train thresholds on UCLASS, evaluate on FluencyBank and SEP-28k without modification. Target: <5% F1 degradation confirming acoustic principles transfer.

## Open Questions the Paper Calls Out

### Open Question 1
How can patient-specific rule parameters be learned from few-shot data while remaining constrained to acoustically meaningful ranges? The current framework relies on manual calibration by SLPs (required in 28% of sessions) rather than automated patient-specific adaptation. Target: An algorithm that optimizes thresholds using <5 minutes of patient speech to match SLP-validated baselines.

### Open Question 2
Does the integration of visual cues (facial tension) into the rule framework significantly improve silent block detection? The acoustic-only system achieves only 0.69 F1 for blocks, struggling with silent/inaudible struggle behaviors that lack acoustic energy. Target: A multimodal system improving block detection F1 to >0.80 on video-audio datasets like FluencyBank while maintaining rule-based transparency.

### Open Question 3
Which hybrid integration strategy (proposal generators vs. constraints) yields the best trade-off between accuracy and interpretability? The 6% F1 gap to neural models might be bridged by hybrids, but the optimal architectural configuration for clinical auditability is not determined. Target: Ablation studies comparing "Rules-as-Proposal" vs. "Rules-as-Constraint" architectures, measuring both F1 scores and explanation quality metrics.

## Limitations

- Performance gap: 6% F1 gap to neural models represents fundamental tradeoff between transparency and accuracy
- Single point of failure: Speaking rate estimation accuracy directly impacts threshold calibration quality
- Edge case misses: Strict multi-feature fusion may miss voiced fricative prolongations and laryngeal tension patterns

## Confidence

- **High Confidence**: Prolongation detection accuracy (97-99% F1), speaking rate normalization mechanism, and hierarchical detection cascade structure
- **Medium Confidence**: Generalization across datasets (F1 0.86→0.83→0.74) and cross-corpus performance
- **Low Confidence**: Environmental robustness claims and performance in naturalistic conditions

## Next Checks

1. **Rate Robustness Stress Test**: Apply controlled time-stretching (0.5×, 2.0×) to validate normalized thresholds maintain stability while fixed thresholds fail catastrophically
2. **Per-Dysfluency Threshold Sensitivity**: Systematically sweep MFCC, F0, and HNR thresholds to verify claimed optimal values (0.92, 15Hz, 10dB) generalize beyond training data
3. **Cross-Corpus Transfer**: Train on UCLASS, evaluate on FluencyBank and SEP-28k without adaptation to confirm acoustic principles transfer across recording conditions and speaker populations