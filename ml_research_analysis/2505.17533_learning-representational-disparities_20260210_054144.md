---
ver: rpa2
title: Learning Representational Disparities
arxiv_id: '2505.17533'
source_url: https://arxiv.org/abs/2505.17533
tags:
- loss
- logit
- biasr
- disparity
- case
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a fair machine learning algorithm to model interpretable
  differences between observed and desired human decision-making, with the goal of
  reducing disparity in downstream outcomes. The method learns a shallow neural network
  where a portion of the hidden layer models representational disparities - the differences
  in input representations seen by observed and desired decision-makers.
---

# Learning Representational Disparities

## Quick Facts
- **arXiv ID:** 2505.17533
- **Source URL:** https://arxiv.org/abs/2505.17533
- **Authors:** Pavan Ravishankar; Rushabh Shah; Daniel B. Neill
- **Reference count:** 40
- **Primary result:** A fair ML algorithm that learns interpretable differences between observed and desired human decision-making to reduce outcome disparity, achieving lower disparity than LFR while maintaining higher accuracy and consistency.

## Executive Summary
This paper proposes a novel fair machine learning algorithm that models interpretable differences between observed and desired human decision-making. The method uses a shallow neural network where a portion of the hidden layer captures "representational disparities" - the differences in input representations seen by observed and desired decision-makers. Under simplifying assumptions, the authors prove that the learned weights are interpretable and can fully mitigate outcome disparity. Experiments on German Credit, Adult, and Heritage Health datasets show that the proposed method (LRD) achieves substantially reduced disparity compared to a competing fair representation learning approach (LFR), while also achieving higher accuracy and greater consistency in its recommendations.

## Method Summary
The method uses a 4-layer shallow neural network with an input layer (S and X), a representation layer (R) with ReLU activation, a decision layer (H) with sigmoid activation, and an outcome layer (Y) with sigmoid activation. The representation layer is split into nodes used by the observed decision-maker (R₁ to Rₘ') and nodes that capture representational disparities (Rₘ'+₁ to Rₘ). The training involves two phases: first training the observed decision and outcome models separately, then training the full model with a weighted multi-objective loss that balances disparity reduction, interpretability (via L1 regularization), and accuracy. The method uses 5-fold cross-validation to select hyperparameters and 10-fold cross-validation for final evaluation.

## Key Results
- LRD achieves substantially reduced outcome disparity compared to LFR baseline on all three datasets
- LRD maintains higher accuracy than LFR while reducing disparity
- LRD shows greater consistency in recommendations compared to LFR
- The learned disparity-node weights are interpretable and capture meaningful corrections to input representations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Separating the internal representation layer into "observed" and "desired" decision-maker pathways isolates the bias source.
- **Mechanism:** The architecture dedicates nodes R₁ to Rₘ' for the observed decision-maker and nodes Rₘ'+₁ to Rₘ for the desired decision-maker. The latter captures *representational disparities*—learnable, interpretable corrections to the input representation that explain outcome gaps. L1 regularization on disparity-node weights encourages sparsity, so only necessary corrections activate.
- **Core assumption:** Observed and desired decision-makers differ only in which representation nodes they use, not in how those nodes connect to the final decision.
- **Evidence anchors:**
  - [abstract]: "We model the outcome disparities as arising due to the different representations of the input seen by the observed and desired decision-maker, which we term representational disparities."
  - [section 3]: "We assume that the former uses only a subset of the representation nodes, R1 to Rm′, to decide H, while the latter uses all of the representation nodes R1 to Rm to decide H. Thus, nodes Rm′+1 to Rm capture the representational disparities."
- **Break condition:** If the outcome Y depends directly on latent factors not captured in X or S, the disparity nodes cannot model the full correction.

### Mechanism 2
- **Claim:** Multi-objective loss balances outcome disparity reduction against interpretability and predictive accuracy.
- **Mechanism:** The total loss L = aA + bB + cC + dD combines: (A) absolute difference in Pr(Y=1|S=1) vs Pr(Y=1|S=0) under the desired decision; (B) L1 penalty on disparity-node weights; (C) cross-entropy for observed decision; (D) cross-entropy for outcome. Setting c ≫ a, b ensures the observed decision model stays accurate while disparity is reduced.
- **Core assumption:** The outcome model Pr(Y|H,S,X) is fixed and can be learned from data; only the decision H can be modified.
- **Evidence anchors:**
  - [abstract]: "we frame this as a multi-objective optimization problem using a neural network."
  - [section 3]: "c ≫ a, c = d, and b = 1 − a with 0 < a < 1 to capture trade-off between Objectives A and B."
- **Break condition:** If c and d are not sufficiently large, the model may distort the observed decision or outcome processes to artificially reduce disparity.

### Mechanism 3
- **Claim:** Under simplifying assumptions, gradient descent converges to interpretable weights that fully mitigate outcome disparity.
- **Mechanism:** Theorem 4.1 shows that with proper initialization (e.g., w > 0, w_SR' > 0 for δ < 0), the loss surface becomes convex within feasible regions, guaranteeing convergence to global optimum w_min = {-sign(δ)√|δ|, √|δ|, 0}. The learned weights satisfy ww_SR' = -δ, explicitly compensating for observed unfairness.
- **Core assumption:** (A1) observed/outcome weights are fixed; (A2) S ⊥ X; (A3) Y ⊥ S | H; (A4-A6) simplified single-node, disparity-dominated settings.
- **Evidence anchors:**
  - [section 4]: "Under reasonable simplifying assumptions, we prove that our neural network model of the representational disparity learns interpretable weights that fully mitigate the outcome disparity."
  - [section 4]: "Lmin = 2√|δ| ... The weights to which the network converges are interpretable."
- **Break condition:** Theorems 4.2-4.3 relax assumptions but do not guarantee convergence; multiple random initializations may be needed.

## Foundational Learning

- **Concept:** Demographic parity in fair ML
  - **Why needed here:** Objective A directly measures outcome disparity as |Pr(Y=1|S=1) - Pr(Y=1|S=0)|, a form of demographic parity; understanding this fairness criterion is essential.
  - **Quick check question:** Given two groups with Pr(Y=1|S=0)=0.7 and Pr(Y=1|S=1)=0.4, what is the outcome disparity?

- **Concept:** L1 regularization and sparsity
  - **Why needed here:** Objective B uses L1 penalties to force disparity-node weights toward zero, yielding interpretable, minimal corrections.
  - **Quick check question:** Why does L1 regularization tend to produce sparse weights compared to L2?

- **Concept:** Sigmoid and logit functions
  - **Why needed here:** The decision H uses sigmoid activation; Theorem 4.1 derives solutions in logit space (δ = logit(H=1|S=1) - logit(H=1|S=0)).
  - **Quick check question:** If σ(x) = 0.8, what is the approximate logit value?

## Architecture Onboarding

- **Component map:** S,X (input) -> R₁...Rₘ (representation) -> H (decision) -> Y (outcome), where Rₘ'+₁...Rₘ are disparity nodes
- **Critical path:**
  1. Train 100 fits on Objectives C + D only (freeze observed/outcome weights)
  2. Select fit with minimum total loss
  3. Train full model with Adam, a ≫ b (e.g., a=0.99, b=0.01, c=d=1000)
  4. Run 100 fits with different initializations; select minimum-loss fit

- **Design tradeoffs:**
  - Larger a → more disparity reduction but potentially less interpretable weights
  - Larger m' → better observed-decision modeling but risk of overfitting
  - Single disparity node (m = m' + 1) maximizes interpretability; multiple nodes may capture more complex biases

- **Failure signatures:**
  - Disparity loss A not decreasing: check that Y depends on H (if Y ⊥ H, modifying H cannot affect disparity)
  - All disparity-node weights near zero in Case I/II/IV: initialization may be trapped in local minimum; try different seeds
  - High variance in corrections across splits: increase a or reduce b to prioritize disparity reduction

- **First 3 experiments:**
  1. Validate on synthetic data satisfying Theorem 4.2 assumptions (S ⊥ X, Y ⊥ S | H, δ-unfairness); confirm single disparity node activates with ww_SR' ≈ δ
  2. Sweep hyperparameter a ∈ {0.1, 0.5, 0.9, 0.99} on synthetic data; plot losses A, B, C, D vs a to verify tradeoffs
  3. Run semi-synthetic Case V (Y = H) on Adult dataset; compare outcome disparity and consistency (CM metric) vs LFR baseline

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical guarantees rely on highly simplified assumptions (single-node representation, δ-dominated setting) that may not hold in real-world datasets
- Training procedure details are underspecified (learning rate, initialization, outcome head architecture)
- Semi-synthetic data generation assumes linear relationships between sensitive attributes and outcomes that may not reflect real-world bias patterns

## Confidence

| Claim | Confidence |
|-------|------------|
| Multi-objective optimization framework | High |
| Architectural design separating observed/desired pathways | High |
| Theoretical convergence guarantees | Medium (due to simplifying assumptions) |
| Empirical results on real datasets | Medium (due to underspecified training) |

## Next Checks

1. **Theoretical relaxation test**: Implement synthetic experiments that relax assumptions A1-A3 to verify Theorem 4.2-4.3 convergence behavior
2. **Training sensitivity analysis**: Systematically vary learning rate, initialization, and outcome head architecture to assess robustness of reported performance
3. **Real-world bias validation**: Apply the method to real datasets with known bias patterns (not just semi-synthetic) to verify that disparity-node corrections are interpretable and meaningful