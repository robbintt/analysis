---
ver: rpa2
title: On-device System of Compositional Multi-tasking in Large Language Models
arxiv_id: '2510.13848'
source_url: https://arxiv.org/abs/2510.13848
tags:
- lora
- on-device
- compositional
- arxiv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an on-device system for compositional multi-tasking
  with large language models, focusing on the practical application of summarizing
  conversations and translating them into another language. The authors propose a
  projection merge approach that adds a learnable projection layer on top of combined
  summarization and translation adapters, achieving efficient integration with minimal
  additional parameters.
---

# On-device System of Compositional Multi-tasking in Large Language Models

## Quick Facts
- arXiv ID: 2510.13848
- Source URL: https://arxiv.org/abs/2510.13848
- Reference count: 5
- The paper introduces an on-device system for compositional multi-tasking with large language models, focusing on summarizing conversations and translating them into another language.

## Executive Summary
This paper presents an efficient on-device system for compositional multi-tasking in large language models, specifically targeting the practical application of summarizing conversations and translating them into another language. The authors propose a projection merge approach that combines task-specific LoRA adapters through learnable projection layers, achieving compositional capabilities with minimal additional parameters. The method enables a single inference pass to perform both summarization and translation, outperforming standard baselines while maintaining efficiency. The system is demonstrated through a fully functional Android application running on a Samsung S23 Ultra device.

## Method Summary
The method builds on pre-trained task-specific LoRA adapters for summarization and translation, then introduces a learnable projection layer to combine these adapters for compositional multi-tasking. The projection merge approach computes ΔW = P₂P₁(0.5·B₁A₁ + 0.5·B₂A₂), where the averaged LoRA parameters are projected through low-rank matrices (rank s=4) to create aligned parameter adjustments for the compositional objective. The projection parameters are shared across transformer layers and components with matching dimensions, resulting in only 0.1M additional parameters (0.4% overhead). The model is trained on direct input-output pairs (English conversation → Spanish summary) using 10K compositional examples created by translating SAMSum summaries to Spanish via Opus-MT.

## Key Results
- Achieves ROUGE-1/2/L scores of 37.21/14.41/30.21 on the compositional task
- Performs comparably to inefficient but well-performing baselines while being significantly faster
- Requires only one inference pass with negligible storage overhead (0.4% additional parameters)
- Completes inference in approximately 24 seconds on Samsung S23 Ultra device

## Why This Works (Mechanism)

### Mechanism 1: Projection Merge for Adapter Combination
The method computes ΔW = P₂P₁(0.5·B₁A₁ + 0.5·B₂A₂), where averaged LoRA parameters from summarization and translation tasks are projected through low-rank matrices to create aligned parameter adjustments. This enables compositional multi-tasking in a single inference pass. The core assumption is that averaged LoRA parameters contain sufficient signal for both tasks such that lightweight projection can extract and recombine them without catastrophic interference.

### Mechanism 2: Cross-Layer Parameter Sharing
Projection parameters P₁ and P₂ are shared across all transformer layers and across components with identical input/output dimensions, constraining total parameters to 0.1M (0.2MB). This sharing enables compositional capability with negligible storage overhead. The core assumption is that the transformation required to align averaged LoRA parameters is sufficiently universal across layers to permit aggressive sharing.

### Mechanism 3: Direct Compositional Mapping via Single-Pass Inference
The model learns to map directly from conversation to translated summary by training on direct input-output pairs, bypassing explicit intermediate generation. Ground-truth data is created by translating SAMSum summaries to Spanish, and the model learns the compositional transformation end-to-end. The core assumption is that the compositional task can be learned as an end-to-end mapping rather than requiring explicit intermediate reasoning steps.

## Foundational Learning

- **LoRA (Low-Rank Adaptation)**
  - Why needed here: The entire method builds on pre-trained task-specific LoRAs. Understanding that LoRA adds low-rank matrices B∈ℝ^(d×r), A∈ℝ^(r×k) to modify frozen weights W₀ is essential.
  - Quick check question: If a LoRA has rank r=32 and targets attention projections with d=k=1024, how many parameters per target layer?

- **Task Arithmetic and Model Merging**
  - Why needed here: The method averages LoRA weights (0.5·B₁A₁ + 0.5·B₂A₂). Understanding why naive averaging often fails and how learnable corrections help is key.
  - Quick check question: Why might averaging two fine-tuned models perform worse than either individually on their respective tasks?

- **Compositional vs. Sequential Multi-Tasking**
  - Why needed here: The paper distinguishes performing tasks separately (multi-task) vs. simultaneously (compositional). Sequential approaches (T₂(T₁(x))) require two inference passes; compositional approaches learn T_C(x) directly.
  - Quick check question: For tasks T₁(x)=summarize and T₂(y)=translate, what is T_C(x) and why is it harder than T₁ or T₂ alone?

## Architecture Onboarding

- **Component map:**
  Base LLM (Llama-3.2-1B-Instruct, Q4_K_M quantized) -> Pre-trained task LoRAs (Summarization, Translation) -> Projection layer (P₁, P₂ matrices) -> Compositional output

- **Critical path:**
  1. Train individual LoRAs on single-task data (SAMSum for summarization, TEDTalks for translation)
  2. Create compositional ground truth (translate summaries with external MT system)
  3. Train projection parameters P₁, P₂ on 10K compositional examples
  4. Export quantized model + adapters + projection params to device
  5. Runtime: Load base model → merge adapters via projection → single inference pass

- **Design tradeoffs:**
  - Projection rank s: Lower s → fewer params but potentially underfitting; paper uses s=4
  - Storage budget: Each additional compositional task requires new P₁, P₂ (~0.2MB each)
  - Training data: 10K examples used; unclear sensitivity to data quantity

- **Failure signatures:**
  - ROUGE scores similar to primary/secondary-task LoRA alone (~23-28) → projection not learning compositional behavior
  - Inference time ~2× single-pass → LoRA merging not working, falling back to sequential
  - Memory errors on device → model not properly quantized or loaded inefficiently
  - Generated output missing summarization OR translation → task interference dominant

- **First 3 experiments:**
  1. **Sanity check:** Run zero-shot, primary LoRA, secondary LoRA, and two-step baseline on held-out compositional test set to establish performance bounds.
  2. **Projection rank ablation:** Train projection merge with s∈{1, 2, 4, 8, 16} to find knee point between efficiency and performance.
  3. **On-device latency profiling:** Measure inference time breakdown on target device to identify bottlenecks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can advanced quantization or OS-integrated LLMs reduce on-device inference times below the reported 24 seconds to support real-time usage?
- Basis: Page 7 states "inference times of over 20 seconds may be too long" and suggests "more aggressive quantization" or "running an LLM integrated into the mobile operating system."
- Why unresolved: The current standalone app architecture on a Samsung S23 Ultra faces compute constraints that limit speed.

### Open Question 2
- Question: Does the projection merge strategy generalize effectively to other compositional tasks, such as reply suggestion or tone adjustment?
- Basis: Page 7 claims the "modular design... supports straightforward extension to additional languages and compositional tasks."
- Why unresolved: The paper evaluates the method strictly on the summarization-translation pipeline using SAMSum and TEDTalks datasets.

### Open Question 3
- Question: How significantly does parameter merging degrade the safety alignment of the original base model?
- Basis: Page 8 warns that "fine-tuning it via LoRA and performing subsequent LoRA merging can diminish the robustness of the safeguard mechanisms."
- Why unresolved: The paper identifies the risk but does not provide a safety evaluation or quantification of alignment loss in the merged model.

## Limitations
- The method's generalization capability to tasks beyond summarization-translation remains unverified
- Reliance on external machine translation for creating compositional training data introduces potential noise
- The 10K training examples may be insufficient for more complex compositional tasks
- On-device implementation lacks detailed profiling of memory usage patterns during different inference phases

## Confidence
- **High Confidence**: The projection merge mechanism for combining LoRA adapters, the efficiency claims (0.4% parameter overhead), and the single-inference-pass benefit are well-supported by experimental results
- **Medium Confidence**: The method's effectiveness on the specific summarization-to-Spanish task is demonstrated, but generalization to other compositional tasks remains untested
- **Low Confidence**: Claims about the approach working for arbitrary compositional task pairs and the sufficiency of the 0.4% parameter overhead for maintaining performance across diverse tasks

## Next Checks
1. **Cross-Task Generalization Test**: Apply the projection merge approach to a different compositional task pair (e.g., question-answering + summarization) using the same methodology to verify generalization beyond summarization-translation.

2. **Projection Rank Sensitivity Analysis**: Systematically evaluate the method across a broader range of projection ranks (s=1, 2, 4, 8, 16, 32) on the current task to identify the optimal tradeoff between parameter efficiency and compositional performance.

3. **Real-Time On-Device Stress Test**: Deploy the system on multiple Android devices with varying hardware capabilities and run sustained inference sessions while monitoring memory usage, CPU/GPU utilization, and thermal performance to verify stability under continuous operation.