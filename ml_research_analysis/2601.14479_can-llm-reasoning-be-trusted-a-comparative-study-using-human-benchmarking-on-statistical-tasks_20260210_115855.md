---
ver: rpa2
title: 'Can LLM Reasoning Be Trusted? A Comparative Study: Using Human Benchmarking
  on Statistical Tasks'
arxiv_id: '2601.14479'
source_url: https://arxiv.org/abs/2601.14479
tags:
- statistical
- reasoning
- human
- llms
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the ability of large language models (LLMs)
  to solve statistical tasks and assess reasoning quality. The authors fine-tuned
  three 7B-parameter LLMs (LLaMA-3, Mistral-Nemo, DeepSeek-R1-Qwen) on a curated dataset
  of 2,000 statistical questions using LoRA with 8-bit quantization.
---

# Can LLM Reasoning Be Trusted? A Comparative Study: Using Human Benchmarking on Statistical Tasks

## Quick Facts
- arXiv ID: 2601.14479
- Source URL: https://arxiv.org/abs/2601.14479
- Reference count: 0
- Primary result: Fine-tuning improves statistical reasoning, with LLM-as-a-Judge correlating better with human evaluation than traditional metrics.

## Executive Summary
This paper investigates the ability of large language models to solve statistical tasks and assess reasoning quality. The authors fine-tuned three 7B-parameter LLMs on a curated dataset of 2,000 statistical questions using LoRA with 8-bit quantization. Models were evaluated on 50 held-out questions across correctness, step-by-step explanation, and reasoning dimensions, with human judges providing benchmark scores. Results show fine-tuning significantly improves performance, with Mistral achieving a 34.18% weighted score increase and DeepSeek outperforming both base models. The study also demonstrates that LLM-based evaluation metrics (LLM-as-a-Judge) correlate better with human judgments than traditional metrics like BLEU or BERTScore, though they tend to be overly lenient with incorrect answers.

## Method Summary
The study fine-tuned three 7B-parameter LLMs (LLaMA-3, Mistral-Nemo, DeepSeek-R1-Qwen) on a curated dataset of 2,000 statistical questions using LoRA with 8-bit quantization. Models were evaluated on 50 held-out questions across three dimensions (correctness, explanation, reasoning) using human judges and automated metrics. The evaluation compared base vs. fine-tuned models and tested whether LLM-as-a-Judge metrics correlate better with human judgments than traditional metrics like BLEU or BERTScore.

## Key Results
- Fine-tuning significantly improves statistical reasoning performance, with Mistral achieving a 34.18% weighted score increase and LLaMA improving by 14.02%.
- DeepSeek-R1-Qwen outperformed both base models, demonstrating superior statistical reasoning capabilities.
- LLM-as-a-Judge metrics correlate more strongly with human judgments (Kendall's τ up to 0.536) than traditional metrics (BLEU: τ=0.098, BERTScore: τ=0.022), though they exhibit leniency bias with incorrect answers.

## Why This Works (Mechanism)

### Mechanism 1: Parameter-efficient fine-tuning with domain-specific data
Parameter-efficient fine-tuning with LoRA adapters and 8-bit quantization enables training 7B models on modest hardware while adapting to statistical reasoning patterns. The study shows architecture-dependent improvements, with Mistral showing substantial gains post-fine-tuning. However, pre-optimized models like DeepSeek show minimal fine-tuning gains, suggesting domain adaptation may have diminishing returns.

### Mechanism 2: LLM-as-a-Judge correlation advantages
Fine-tuned LLMs can evaluate reasoning quality across multiple dimensions (correctness, explanation, reasoning) more effectively than traditional lexical metrics. The study demonstrates that LLM judges correlate better with human judgments than BLEU or BERTScore, but exhibit "sycophancy"—overly lenient scoring of incorrect answers, such as giving 3.5/5 to a fundamentally flawed Markov chain solution.

### Mechanism 3: Architecture choice affects reasoning performance
Different model architectures encode different inductive biases that affect how statistical reasoning patterns are acquired during fine-tuning. The study shows Mistral surpassing LLaMA post-fine-tuning despite a weaker base version, while DeepSeek shows no fine-tuning delta as it was pre-optimized by developers for mathematical reasoning.

## Foundational Learning

- **LoRA (Low-Rank Adaptation)**
  - Why needed here: Core technique enabling efficient fine-tuning by learning rank-decomposed weight matrices (A×B) instead of full weight updates.
  - Quick check question: If LoRA rank r=8 and original weight is 4096×4096, how many trainable parameters does LoRA add per weight matrix?

- **Quantization (8-bit)**
  - Why needed here: Reduces VRAM from ~32GB to ~8GB for 7B models, enabling training on consumer-grade GPUs.
  - Quick check question: Why does quantizing frozen base weights while keeping LoRA adapters in higher precision preserve task performance?

- **Kendall's τ (Rank Correlation)**
  - Why needed here: Primary metric for meta-evaluation of how well automated scores align with human judgments.
  - Quick check question: If automated metric A scores [1,2,3] and humans score [1,3,2] for three items, what is Kendall's τ?

## Architecture Onboarding

- **Component map:** Dataset (2,000 questions) → Base LLM (7B) → LoRA adapters (trainable) → 8-bit quantized weights (frozen) → Human judges + Automated metrics → Kendall's τ correlation analysis

- **Critical path:**
  1. Dataset construction with topic/complexity stratification
  2. LoRA fine-tuning with 8-bit quantization on NVIDIA T4
  3. Inference on 50 held-out questions (zero-shot)
  4. Multi-dimensional human scoring (Correctness 40%, Explanation 35%, Reasoning 25%)
  5. Compute automated metrics and measure correlation with human scores

- **Design tradeoffs:**
  - 7B model size chosen for accessibility vs. potential performance gains from larger models
  - Fine-tuned models as judges eliminate API dependencies but introduce self-evaluation bias
  - Three rubric dimensions capture more signal but increase inter-rater variance

- **Failure signatures:**
  - LLM judges consistently over-score incorrect answers (sycophancy)
  - BLEU correlates negatively with some model evaluations (τ = -0.125 for Mistral FT)
  - Pre-optimized models show minimal fine-tuning gains (DeepSeek)

- **First 3 experiments:**
  1. Baseline replication: Load base LLaMA-3 8B, run inference on 5 statistical questions, compute BLEU vs reference solutions.
  2. LoRA fine-tuning sweep: Fine-tune with rank ∈ {4, 8, 16}, learning rate ∈ {1e-4, 3e-4, 1e-3} on 100-question subset, evaluate on 10 held-out questions using weighted rubric.
  3. Judge calibration: Implement LLM-as-Judge prompt with held-out model, score 10 known-incorrect solutions, measure leniency rate.

## Open Questions the Paper Calls Out

### Open Question 1
How do multilingual and cross-cultural variations affect the statistical reasoning capabilities of fine-tuned LLMs? The current study restricted evaluation to English-language statistical problems, limiting generalizability to global educational contexts. Evidence needed: Evaluation of fine-tuned models on translated datasets with varying cultural contexts.

### Open Question 2
Can robust uncertainty quantification methods be integrated to ensure reliability in high-stakes decision-making? LLMs generate plausible-sounding but potentially incorrect interpretations without flagging low confidence. Evidence needed: New architectures producing calibrated confidence scores alongside reasoning, validated against human expert uncertainty benchmarks.

### Open Question 3
How can the "overly lenient" bias and sycophancy of LLM-as-a-Judge be mitigated? The study shows LLM judges systematically fail to penalize severe conceptual errors while correlating well with human judgments on correct answers. Evidence needed: Adversarial training methods or calibrated prompts that align error penalization rates with human strictness.

## Limitations
- Limited human evaluation with only three PhD students and no reported inter-rater agreement scores
- Dataset composition transparency issues with unclear question-by-question difficulty distribution
- Narrow generalization scope with evaluation limited to 50 held-out questions across five statistical domains

## Confidence

- **High confidence:** Architecture-dependent fine-tuning improvements (Mistral +34.18%, LLaMA +14.02%) empirically supported by clear before/after comparisons
- **Medium confidence:** LLM-as-a-Judge correlation advantages demonstrated, but sycophancy suggests relationship isn't monotonic
- **Low confidence:** Claims about pre-optimized models showing minimal fine-tuning gains conflate architectural properties with optimization history

## Next Checks

1. Compute Cohen's κ or Krippendorff's α for the three human judges across at least 20 randomly sampled questions to quantify agreement reliability.

2. Categorize incorrect LLM responses (conceptual misunderstanding vs. calculation error) to determine if LLM-as-a-Judge leniency is systematic across error types.

3. Evaluate fine-tuned models on a distinct statistics dataset (e.g., StatEval benchmark) to verify transfer beyond the original 2,000-question domain.