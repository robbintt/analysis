---
ver: rpa2
title: 'Agent-as-a-Graph: Knowledge Graph-Based Tool and Agent Retrieval for LLM Multi-Agent
  Systems'
arxiv_id: '2511.18194'
source_url: https://arxiv.org/abs/2511.18194
tags:
- agent
- graph
- retrieval
- tool
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of routing queries to the correct
  agent in large-scale multi-agent systems where traditional methods either obscure
  fine-grained tool capabilities or discard agent-level context. It introduces Agent-as-a-Graph,
  a knowledge graph retrieval augmented generation approach that represents tools
  and agents as co-equal nodes in a unified graph structure with explicit ownership
  edges.
---

# Agent-as-a-Graph: Knowledge Graph-Based Tool and Agent Retrieval for LLM Multi-Agent Systems

## Quick Facts
- **arXiv ID**: 2511.18194
- **Source URL**: https://arxiv.org/abs/2511.18194
- **Reference count**: 2
- **Primary result**: 14.9% and 14.6% improvements in Recall@5 and nDCG@5 over prior state-of-the-art retrievers on LiveMCPBench

## Executive Summary
Agent-as-a-Graph introduces a knowledge graph-based retrieval augmented generation approach for multi-agent systems that addresses the challenge of routing queries to the correct agent in large-scale environments. The method represents tools and agents as co-equal nodes in a unified graph structure with explicit ownership edges, enabling fine-grained capability matching while preserving agent-level context. By combining vector search, type-specific weighted reciprocal rank fusion, and graph traversal from tools to parent agents, the approach achieves significant improvements over traditional RAG methods on the LiveMCPBench benchmark.

## Method Summary
The approach builds a bipartite knowledge graph G=(A,T,E) indexing tools and agents together, with ownership edges linking tools to their parent agents. During retrieval, queries are processed through vector search against both tool and agent corpora, then reranked using type-specific weighted reciprocal rank fusion (wRRF) with optimal weights α_A=1.5 and α_T=1.0. Finally, graph traversal follows ownership edges from high-ranked tools to their parent agents, ensuring all retrieved entities resolve to executable agents. The method was evaluated on LiveMCPBench with eight embedding models, showing substantial improvements in Recall@5 and nDCG@5 over baseline retrievers.

## Key Results
- Achieves 14.9% and 14.6% improvements in Recall@5 and nDCG@5 over prior state-of-the-art retrievers
- Optimal configuration (α_A=1.5, α_T=1.0) outperforms standard weighted RRF by 2.4% in Recall@5
- 39.13% of retrieved items originate from agent nodes and 34.44% from tool nodes via graph edges
- Consistent performance across eight embedding models with standard deviations below 0.02 in Recall metrics

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Representing tools and agents as co-equal nodes in a knowledge graph with explicit ownership edges improves retrieval accuracy by preserving both fine-grained tool capabilities and agent-level context.
- **Mechanism**: A bipartite graph G=(A,T,E) indexes tools and agents together. Vector search retrieves from both corpora simultaneously, then ownership edges enable traversal from matched tools to their parent agents, ensuring executable endpoints are always returned.
- **Core assumption**: Queries benefit from both granularity levels—tool-level specificity for precise capability matching and agent-level context for execution requirements (auth, policies, parameters).
- **Evidence anchors**: [abstract] "represents both tools and their parent agents as nodes and edges in a knowledge graph"; [section 3.1.1] "Retrieving solely by agent descriptions can miss fine-grained functional capabilities at the tool level, while retrieving tools independently discards important execution context"; [corpus] Related work "Graph RAG-Tool Fusion" (FMR=0.57) confirms graph structure captures tool dependencies traditional RAG misses.
- **Break condition**: If tools have no meaningful parent-agent relationship (standalone tools), or if agent descriptions already contain exhaustive tool details, graph structure adds overhead without gain.

### Mechanism 2
- **Claim**: Type-specific weighted reciprocal rank fusion (wRRF) outperforms standard RRF by decoupling tool and agent scoring at rank fusion time.
- **Mechanism**: Instead of blending ranks from heterogeneous retrievers, wRRF applies piecewise weights: s(e) = α_T/(k+r(e)) for tools, α_A/(k+r(e)) for agents. This preserves within-type ordering while allowing interpretable control over granularity emphasis.
- **Core assumption**: Tool and agent similarity scores have different calibration distributions; treating them uniformly in fusion introduces noise.
- **Evidence anchors**: [abstract] "we apply a type-specific weighted reciprocal rank fusion (wRRF) for reranking tools and agents"; [section 4.4, Table 3] Optimal configuration (α_A=1.5, α_T=1.0) achieves Recall@5 of 0.85 vs. 0.79 for standard weighted RRF; [corpus] No direct corpus comparison to wRRF variants; evidence primarily internal to paper.
- **Break condition**: If retrieval scores are already well-calibrated across types, or if the optimal weight ratio varies significantly by query type without adaptive tuning, static weights may underperform.

### Mechanism 3
- **Claim**: Graph traversal from retrieved tools to parent agents ensures all retrieved entities resolve to executable agents.
- **Mechanism**: After vector search + reranking produces a ranked node list, the algorithm traverses ownership edges: if a tool is ranked high, its parent agent is added to the final set. This guarantees every retrieval result maps to an agent capable of execution.
- **Core assumption**: Users ultimately need agents (not raw tools) to execute requests; tool-only matches are incomplete without agent context.
- **Evidence anchors**: [section 3.1.1] "every query or sub-query ultimately resolves to an executable agent through the graph edges"; [section 4.2.2] "34.44% of matched tool nodes trace back to agent nodes through graph edges"; [corpus] "Tool-to-Agent Retrieval" paper (FMR=0.56) independently validates bridging tools to agents improves routing.
- **Break condition**: If the ownership graph is incomplete (orphan tools) or contains incorrect mappings, traversal produces wrong agents.

## Foundational Learning

- **Concept: Bipartite Knowledge Graphs**
  - **Why needed here**: The core data structure assumes familiarity with nodes, edges, and traversal. Without this, the ownership relationship between tools and agents is opaque.
  - **Quick check question**: Given nodes {A1, A2, T1, T2, T3} and edges {(T1,A1), (T2,A1), (T3,A2)}, which tools belong to agent A2?

- **Concept: Reciprocal Rank Fusion (RRF)**
  - **Why needed here**: The wRRF mechanism extends standard RRF. Understanding the base formula s(e)=Σ 1/(k+r) is prerequisite to grasping the type-weighted variant.
  - **Quick check question**: If document X ranks #2 in list A and #5 in list B (k=60), what is its RRF score?

- **Concept: Dense Vector Retrieval**
  - **Why needed here**: The first retrieval stage uses embedding similarity. You need to understand how queries are encoded and compared to corpus vectors.
  - **Quick check question**: Why might a tool named "execute_sql_query" fail to match a user query "get data from database" using pure lexical matching but succeed with dense retrieval?

## Architecture Onboarding

- **Component map**: User Query → Query Processor (direct or step-wise decomposition) → Vector Search → Top-N nodes from unified corpus (C_A ∪ C_T) → wRRF Reranker → Type-weighted scoring (α_A, α_T knobs) → Graph Traversal → Tool→Agent edge following → Top-K Agent Set → Returned to orchestration layer

- **Critical path**:
  1. **Graph construction** (Section 3.1): Ingest MCP manifests → extract agent/tool metadata → build ownership edges → store in graph DB (e.g., Neo4j with Cypher support)
  2. **Indexing** (Section 3.1.2): Embed tool names+descriptions and agent names+descriptions separately but in shared vector space
  3. **Retrieval** (Algorithm 1): Lines 1-6 handle vector search + wRRF; lines 9-15 handle traversal

- **Design tradeoffs**:
  - **Agent emphasis (α_A) vs. tool emphasis (α_T)**: Paper finds 1.5:1 optimal; higher agent bias (3:1) drops Recall@5 to 0.76; tool bias (1:3) achieves 0.80
  - **Direct vs. step-wise querying**: Step-wise used in evaluations (avg 2.68 steps/query); direct may suffice for simple queries
  - **N vs. K selection**: Retrieve N≫K nodes before traversal; paper doesn't specify exact N, assumes sufficient buffer for traversal deduplication

- **Failure signatures**:
  - **Low Recall@5 despite good embedding**: Check ownership edge completeness—orphan tools bypass traversal
  - **High variance across embedding models**: If std≫0.02 in Recall, verify corpus indexing consistency
  - **Tool-heavy queries returning wrong agents**: α_T may be too low; tool matches aren't propagating to parent agents

- **First 3 experiments**:
  1. **Reproduce baseline comparison**: Run Agent-as-a-Graph vs. MCPZero on LiveMCPBench subset with OpenAI ada-002; target Recall@5 improvement ~13-19%
  2. **Ablate wRRF**: Set α_A=α_T=1.0 (unweighted) and compare against optimal (1.5, 1.0); expect ~2% drop
  3. **Test edge case**: Query with only tool-level specificity (no agent description match) and verify traversal successfully maps tool→agent

## Open Questions the Paper Calls Out

- **Open Question 1**: Can dynamic, query-adaptive weighting of agents and tools (α_A, α_T) outperform the static optimal configuration found via grid search?
  - **Basis in paper**: [explicit] The conclusion explicitly motivates "future research into query-adaptive type weights" beyond the static weights evaluated.
  - **Why unresolved**: The paper establishes a globally optimal static weight (1.5:1), but it remains untested whether adjusting weights per query based on intent or granularity yields further improvements.
  - **What evidence would resolve it**: Experiments demonstrating that a lightweight classifier predicting optimal weights per query improves Recall@K and nDCG@K over the static baseline.

- **Open Question 2**: To what extent do structure-aware graph priors (e.g., tool-tool dependencies or agent-agent hierarchies) improve retrieval accuracy in multi-step orchestration?
  - **Basis in paper**: [explicit] The conclusion identifies "structure-aware graph priors for multi-agent orchestration" as a key area for future research.
  - **Why unresolved**: The current implementation utilizes a bipartite ownership graph; it does not leverage other structural relationships, such as tool co-usage or functional similarity edges, which could aid complex reasoning.
  - **What evidence would resolve it**: Ablation studies extending the graph schema to include dependency edges and measuring the resulting performance delta on multi-hop reasoning tasks.

- **Open Question 3**: How does the retrieval performance and latency of Agent-as-a-Graph scale when expanding to thousands of agents and tens of thousands of tools?
  - **Basis in paper**: [inferred] The introduction defines the problem context as systems with "thousands" of tools, but the evaluation is restricted to LiveMCPBench with 527 tools.
  - **Why unresolved**: It is uncertain if the vector search and graph traversal efficiency remains robust without degradation in significantly larger, denser knowledge graphs representative of enterprise scale.
  - **What evidence would resolve it**: Benchmarks on synthetic or larger real-world datasets (e.g., >10,000 nodes) showing latency and Recall stability as the graph size increases.

## Limitations
- The approach is evaluated on a specific MCP benchmark with 527 tools, limiting generalizability to other multi-agent systems
- Step-wise query decomposition procedure remains underspecified, potentially affecting reproducibility
- No ablation studies on the necessity of graph traversal vs. direct tool-to-agent mapping alternatives

## Confidence
- **High confidence**: Graph traversal mechanism (proven by 34.44% tool-to-agent tracing), bipartite graph construction, and LiveMCPBench dataset validity
- **Medium confidence**: wRRF performance claims - while the 2.4% improvement is documented, the optimal weight configuration (1.5:1) lacks robustness testing across query types
- **Low confidence**: Generalizability beyond MCP servers - results are based on a specific benchmark that may not represent all multi-agent systems

## Next Checks
1. **Parameter sensitivity analysis**: Test Agent-as-a-Graph across different N values (20, 50, 100) to establish optimal retrieval buffer size
2. **Weight configuration robustness**: Evaluate wRRF performance across diverse query types (tool-heavy vs. agent-heavy) to validate static weights remain optimal
3. **Cross-domain applicability**: Apply the approach to non-MCP multi-agent systems (e.g., LangChain agents, AutoGen groups) to test generalizability claims