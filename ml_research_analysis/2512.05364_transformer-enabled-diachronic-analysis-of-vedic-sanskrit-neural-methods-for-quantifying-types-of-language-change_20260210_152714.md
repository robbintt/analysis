---
ver: rpa2
title: 'Transformer-Enabled Diachronic Analysis of Vedic Sanskrit: Neural Methods
  for Quantifying Types of Language Change'
arxiv_id: '2512.05364'
source_url: https://arxiv.org/abs/2512.05364
tags:
- sanskrit
- vedic
- features
- feature
- diachronic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a hybrid neural-symbolic framework for analyzing
  morphological change in low-resource historical languages, specifically Vedic Sanskrit.
  The method combines regex-based symbolic rules with a fine-tuned multilingual BERT
  model through confidence-weighted ensembling, addressing data scarcity through weak
  supervision.
---

# Transformer-Enabled Diachronic Analysis of Vedic Sanskrit: Neural Methods for Quantifying Types of Language Change

## Quick Facts
- arXiv ID: 2512.05364
- Source URL: https://arxiv.org/abs/2512.05364
- Reference count: 0
- This study introduces a hybrid neural-symbolic framework for analyzing morphological change in low-resource historical languages, specifically Vedic Sanskrit.

## Executive Summary
This study presents a hybrid neural-symbolic framework for analyzing morphological change in low-resource historical languages, specifically Vedic Sanskrit. The method combines regex-based symbolic rules with a fine-tuned multilingual BERT model through confidence-weighted ensembling, addressing data scarcity through weak supervision. Applied to a 1.47-million-word diachronic corpus spanning 2,000+ years, the ensemble achieves 52.4% feature detection rate with strong calibration (Pearson r = 0.92, ECE = 0.043). The analysis reveals that Sanskrit's morphological complexity redistributes rather than simplifies over time, challenging conventional assumptions about linguistic simplification. The framework demonstrates effectiveness for low-resource diachronic analysis and is released with trained models and corpus for reproducible research in computational philology.

## Method Summary
The methodology employs a hybrid neural-symbolic approach to morphological analysis, combining regex-based symbolic rules with a fine-tuned multilingual BERT model through confidence-weighted ensembling. The symbolic component uses handcrafted regex patterns to identify morphological features in Vedic Sanskrit, while the neural component leverages pre-trained language models fine-tuned on weak supervision from the symbolic rules. The ensemble combines predictions using weighted confidence scores, with weights optimized through grid search. The framework processes a 1.47-million-word diachronic corpus spanning 2,000+ years, tracking morphological changes across temporal segments. Weak supervision addresses the data scarcity inherent in historical language analysis, while the ensemble approach compensates for individual component limitations.

## Key Results
- Ensemble achieves 52.4% feature detection rate on morphological tagging task
- Strong calibration metrics: Pearson correlation r = 0.92 and Expected Calibration Error = 0.043
- Analysis reveals morphological complexity redistribution rather than simplification over 2,000+ years
- Framework successfully handles low-resource diachronic analysis of historical Sanskrit

## Why This Works (Mechanism)
The framework succeeds by combining complementary approaches: symbolic rules provide interpretable, linguistically-informed feature extraction that works well for regular morphological patterns, while the fine-tuned mBERT model captures complex exceptions and contextual variations. The confidence-weighted ensembling leverages the strengths of both components while mitigating their individual weaknesses. Weak supervision enables training with limited annotated data by bootstrapping from the symbolic rules, and the ensemble calibration ensures reliable probability estimates. This hybrid approach is particularly effective for historical languages where traditional supervised learning is infeasible due to data scarcity.

## Foundational Learning
- **Weak supervision**: Training models using noisy, automatically generated labels instead of expensive manual annotation; needed because historical Sanskrit lacks large annotated corpora; quick check: verify label quality and noise levels
- **Confidence-weighted ensembling**: Combining multiple models' predictions using their confidence scores as weights; needed to balance strengths/weaknesses of symbolic vs neural approaches; quick check: test ensemble calibration across confidence thresholds
- **Morphological feature detection**: Identifying grammatical features like case, number, and tense from surface forms; needed for tracking diachronic linguistic changes; quick check: validate feature extraction against linguistic gold standards
- **Multilingual BERT fine-tuning**: Adapting pre-trained transformer models to specific linguistic tasks with limited data; needed to leverage transfer learning for low-resource settings; quick check: monitor fine-tuning convergence and overfitting
- **Diachronic corpus analysis**: Studying language change patterns across temporal segments of text; needed to quantify linguistic evolution over 2,000+ years; quick check: verify temporal segmentation and coverage
- **Regex-based symbolic rules**: Handcrafted pattern matching for morphological feature identification; needed for interpretable, rule-based analysis of Sanskrit's complex morphology; quick check: test rule coverage and exception handling

## Architecture Onboarding

**Component Map**: Corpus -> Symbolic Rules -> Regex Patterns -> Morphological Features -> Neural Model (mBERT) -> Fine-tuned Predictions -> Ensemble -> Weighted Confidence Scores -> Final Output

**Critical Path**: Corpus → Symbolic Rules → Neural Model → Ensemble → Final Output

**Design Tradeoffs**: Symbolic rules provide interpretability and domain expertise integration but require manual effort and may miss exceptions; neural models handle complexity and exceptions but need data and lack interpretability; weak supervision enables training with limited resources but introduces noise; ensembling balances complementary strengths but adds complexity.

**Failure Signatures**: 
- Low recall: Regex patterns miss morphological variants or neural model underfits
- High false positives: Overconfident symbolic rules or neural model overfitting
- Poor calibration: Mismatch between predicted confidence and actual accuracy
- Domain mismatch: Symbolic rules designed for Classical Sanskrit may not transfer to Vedic variants

**3 First Experiments**:
1. Test symbolic rules in isolation on held-out validation set to establish baseline performance
2. Fine-tune mBERT on weak supervision labels and evaluate calibration curves
3. Perform ablation study comparing ensemble performance against individual components

## Open Questions the Paper Calls Out
The paper does not explicitly identify open questions in the report, though the analysis of morphological complexity redistribution rather than simplification over time suggests several research directions: How generalizable is this finding to other historical languages? What specific mechanisms drive the redistribution of morphological complexity? Can the framework be extended to capture finer-grained temporal patterns within the 2,000-year span?

## Limitations
- Reliance on weak supervision may introduce systematic labeling biases that propagate through the ensemble model
- Moderate 52.4% detection rate indicates substantial room for accuracy improvement in low-resource settings
- Framework tested exclusively on Vedic Sanskrit, limiting generalizability to languages with different morphological typologies
- Temporal granularity of 2,000+ year corpus may obscure finer-grained patterns of linguistic change

## Confidence

**High confidence**: The methodology combining symbolic rules with neural models is technically sound and the ensemble calibration metrics (Pearson r = 0.92, ECE = 0.043) demonstrate reliable probability estimates. The finding that Sanskrit's morphological complexity redistributes rather than simplifies is well-supported by the corpus evidence.

**Medium confidence**: The weak supervision approach and the specific detection rate of 52.4% are appropriate given data constraints, but the absolute accuracy figures should be interpreted cautiously due to potential label noise and the limited availability of gold-standard annotations for historical Sanskrit.

**Medium confidence**: The broader claim about challenging conventional assumptions regarding linguistic simplification is valid for this specific case study but requires additional diachronic analyses of other languages to establish as a general principle.

## Next Checks

1. Conduct ablation studies comparing the ensemble performance with individual components (symbolic rules vs. fine-tuned mBERT) to quantify the contribution of each method and identify optimal weight configurations.

2. Apply the framework to a diachronic corpus of another morphologically rich language (e.g., Classical Arabic or Ancient Greek) to test generalizability and identify language-specific challenges.

3. Perform qualitative error analysis on false positives and false negatives to characterize systematic weaknesses in the symbolic rules and neural model predictions, informing targeted improvements to both components.