---
ver: rpa2
title: Explaining How Quantization Disparately Skews a Model
arxiv_id: '2509.07222'
source_url: https://arxiv.org/abs/2509.07222
tags:
- quantization
- weights
- network
- neural
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how quantization disproportionately affects\
  \ different classes in neural networks. The authors analyze how quantization-induced\
  \ changes in weights and activations propagate through the network, leading to lower\
  \ variance in logits, increased loss, and reduced accuracy\u2014particularly for\
  \ minority groups."
---

# Explaining How Quantization Disparately Skews a Model

## Quick Facts
- arXiv ID: 2509.07222
- Source URL: https://arxiv.org/abs/2509.07222
- Authors: Abhimanyu Bellam; Jung-Eun Kim
- Reference count: 11
- Primary result: Shows quantization disproportionately harms minority groups in neural networks and proposes FairQAT mitigation.

## Executive Summary
This paper demonstrates that quantization disproportionately affects different classes in neural networks, leading to lower accuracy and increased loss for minority groups. Through analysis of logit variance, gradient norms, and Hessian eigenvalues, the authors show that quantization creates sharper loss landscapes and removes fine-grained weights critical for minority features. They propose FairQAT, combining mixed-precision quantization-aware training with dataset sampling and weighted loss functions, to reduce fairness violations while maintaining overall accuracy.

## Method Summary
The study analyzes Post-Training Quantization (PTQ) effects on ResNet18 using the UTKFace dataset for ethnicity classification. The method involves applying per-tensor uniform quantization to weights only (int8, int4, int2), measuring fairness violation observed (FVO) as the maximum accuracy difference between groups, and analyzing gradient norms and Hessian eigenvalues. The FairQAT mitigation combines mixed-precision QAT (first/last layers int8, activations fp32) with weighted cross entropy using specific weights [0.1, 0.1, 0.1, 0.1, 0.6] for the "Others" class plus under/over-sampling.

## Key Results
- Quantization reduces logit variance, pushing predictions closer to decision boundaries and disproportionately increasing loss for minority groups
- Minority classes exhibit larger gradient norms and higher Hessian eigenvalues (λmax), indicating sharper loss landscapes
- FairQAT reduces fairness violations while maintaining overall accuracy compared to standard QAT and PTQ
- Zero-bin induced sparsity from uniform quantization removes critical features for minority groups

## Why This Works (Mechanism)

### Mechanism 1: Logit Variance Collapse and Decision Boundary Shift
Reducing bit-precision forces weights into discrete bins, creating a "high-temperature" effect where logit differences shrink. This flattens softmax probabilities, reducing model confidence and pushing minority group predictions toward low-confidence regions. Break condition: Activation quantization alone or high precision (fp16) may prevent variance collapse.

### Mechanism 2: Optimization Landscape Sharpening (Hessian Eigenvalues)
Quantization moves weights to sub-optimal points with sharper loss landscapes for minority classes. Higher Hessian eigenvalues (λmax) indicate steeper loss valleys, suggesting poor generalization for these groups. Break condition: QAT allows weight updates to "flatten" sharp regions.

### Mechanism 3: Zero-Bin Induced Sparsity
Uniform quantization maps small weights to absolute zero, removing fine-grained weights essential for distinguishing minority class features. Majority classes rely on larger, more robust weights that survive quantization. Break condition: Non-uniform or learned quantization scales that preserve small non-zero weights.

## Foundational Learning

- **Concept: Post-Training Quantization (PTQ) vs. Quantization Aware Training (QAT)**
  - Why needed here: Core diagnosis applies to PTQ (static weights), while solution relies on QAT (trainable weights)
  - Quick check: Does the mechanism rely on model failing to adapt to new precision (PTQ) or failing to learn precision during training (QAT)? (Answer: The former)

- **Concept: Hessian Eigenvalues (λmax)**
  - Why needed here: Used as proxy for "landscape sharpness" to prove worse optimization state for minorities
  - Quick check: If λmax increases for specific group, does loss surface become flatter or sharper? (Answer: Sharper)

- **Concept: Weighted Cross Entropy (WCR) & Sampling**
  - Why needed here: FairQAT relies on addressing "example difficulty" and class imbalance using weighted loss and sampling
  - Quick check: Why would balancing dataset alone be insufficient if "example difficulty" varies between groups?

## Architecture Onboarding

- **Component map:** Input Dataset (D) -> ResNet18 -> Quantizer (T) -> Diagnosis Tools (Gradient Norm/Hessian) -> Mitigation Layer (FairQAT)

- **Critical path:** 1. Train standard model -> 2. Apply PTQ -> 3. Measure FVO -> 4. Measure λmax and Gradient Norms -> 5. Retrain using FairQAT

- **Design tradeoffs:** PTQ offers speed and compression but high FVO; FairQAT requires retraining but lowers FVO. Lower bits increase sparsity and disparity; system balances compression ratio against FVO metric.

- **Failure signatures:** Logit nullification in int2/int3, inverse gradient trends (high norms but low accuracy), disparity spikes when moving from int8 to int4.

- **First 3 experiments:**
  1. Baseline disparity check: Quantize ResNet18 on UTKFace to int8, int4, int2; plot FVO and accuracy per group
  2. Landscape analysis: Compute gradient norms and λmax for minority vs. majority groups on int4 model
  3. Mitigation validation: Implement FairQAT on int4 model and compare FVO vs. OA reduction

## Open Questions the Paper Calls Out

- **Open Question 1:** How does activation quantization influence disparate impact compared to weight-only quantization analyzed?
  - Basis: Study focused solely on weight quantization; hypothesizes activation quantization could expand observed disparity
  - Evidence needed: Comparative experiments measuring FVO and gradient norms with both weight and activation quantization

- **Open Question 2:** Do optimal mitigation strategies vary significantly across different quantization bit-widths?
  - Basis: Hyperparameters tuned for int4 were reused for int2/int8 without verification
  - Evidence needed: Hyperparameter search for int2 and int8 models to determine distinct optimal settings

- **Open Question 3:** Do observed optimization shifts generalize to Large Language Models (LLMs)?
  - Basis: Introduction highlights quantization's prominence in LLMs, but analysis relies on ResNet18/CNN architectures
  - Evidence needed: Applying gradient norm/Hessian analysis to quantized generative language models

## Limitations

- Core mechanisms demonstrated on single dataset (UTKFace) and architecture (ResNet18), limiting generalizability
- FairQAT implementation details (sampling ratios, PTQ calibration) are underspecified
- Reliance on maximum Hessian eigenvalues as fairness proxy lacks broader validation across diverse architectures

## Confidence

- **High Confidence:** Observed accuracy disparity between groups after PTQ is robustly demonstrated
- **Medium Confidence:** Zero-bin induced sparsity mechanism is plausible but needs more empirical validation
- **Low Confidence:** Specific quantitative relationship between λmax values and group-specific fairness violations

## Next Checks

1. Generalizability test: Replicate disparity analysis on CelebA dataset and EfficientNet architecture
2. Ablation study: Apply quantization to activations without weights to isolate logit variance collapse source
3. Hessian validation: Implement power iteration approximation for λmax on larger model beyond ResNet18 scale