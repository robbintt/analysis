---
ver: rpa2
title: Ontology-Guided Query Expansion for Biomedical Document Retrieval using Large
  Language Models
arxiv_id: '2508.11784'
source_url: https://arxiv.org/abs/2508.11784
tags:
- query
- retrieval
- biomedical
- expansion
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of retrieving relevant biomedical
  documents in response to user queries, which is complicated by domain-specific vocabulary
  and semantic ambiguity. The proposed method, BMQExpander, integrates structured
  biomedical ontologies with large language models to enhance query expansion.
---

# Ontology-Guided Query Expansion for Biomedical Document Retrieval using Large Language Models

## Quick Facts
- **arXiv ID:** 2508.11784
- **Source URL:** https://arxiv.org/abs/2508.11784
- **Reference count:** 40
- **Primary result:** BMQExpander improves biomedical document retrieval by up to 22.1% in NDCG@10 over sparse baselines and reduces hallucinations compared to other LLM-based methods.

## Executive Summary
This paper addresses the challenge of retrieving relevant biomedical documents in response to user queries, which is complicated by domain-specific vocabulary and semantic ambiguity. The proposed method, BMQExpander, integrates structured biomedical ontologies with large language models to enhance query expansion. It extracts key medical terms from queries, maps them to UMLS concepts, and retrieves definitions and relationships from trusted biomedical vocabularies. These are serialized into a prompt for the LLM, which generates a medically grounded pseudo-document used to expand the original query. Experiments on NFCorpus, TREC-COVID, and SciFact show that BMQExpander outperforms sparse baselines by up to 22.1% in NDCG@10 and strong dense retrievers by up to 6.5%. It also generalizes better under query perturbations, achieving up to 15.7% improvement over the strongest baseline.

## Method Summary
BMQExpander is a five-stage pipeline that enhances biomedical document retrieval by grounding LLM-generated query expansions in structured medical ontologies. The method extracts medical entities from queries using GPT-4o, maps them to UMLS Concept Unique Identifiers (CUIs) via exact string matching, retrieves definitions and semantic relationships from vocabularies like MeSH and SNOMED-CT, and serializes this context into a structured prompt. The LLM then generates a medically grounded pseudo-document (max 512 tokens), which is concatenated with the original query (weighted 5:1) and used to retrieve documents via BM25. This approach bridges semantic gaps while reducing hallucinations through ontology grounding.

## Key Results
- BMQExpander improves NDCG@10 by up to 22.1% over sparse baselines and 6.5% over dense retrievers
- Robustness under query perturbation achieves up to 15.7% improvement over strongest baseline
- Qualitative analysis shows 0.07 hallucination score vs. 0.80 for Corpus-Steered baseline

## Why This Works (Mechanism)

### Mechanism 1: Ontology-Guided Hallucination Reduction
- **Claim:** Injecting structured medical definitions and relationships into the LLM prompt reduces factual errors and "hallucinations" compared to unguided generation.
- **Mechanism:** The pipeline retrieves curated definitions from authoritative vocabularies (MeSH, SNOMED-CT, NCI) via UMLS. By forcing these definitions into the prompt context, the LLM shifts from open-ended generation to a synthesis task grounded in provided facts, limiting its ability to invent incorrect medical terminology.
- **Core assumption:** The LLM prioritizes and utilizes the provided context over its internal pre-trained knowledge when generating the pseudo-document.
- **Evidence anchors:**
  - [abstract]: "Qualitative analysis shows that BMQExpander has fewer hallucinations compared to other LLM-based query expansion baselines."
  - [section 5.5]: Expert evaluation (Table 6) shows BMQExpander achieves a hallucination score of 0.07 (lower is better) vs. 0.80 for the Corpus-Steered baseline, and higher medical accuracy.
  - [corpus]: Doc2Query++ notes that uncontrolled generation produces "hallucinated or redundant queries," supporting the need for the grounding mechanism used here.
- **Break condition:** If the UMLS mapping retrieves irrelevant definitions (concept drift), the LLM will generate a grounded but irrelevant pseudo-document, degrading retrieval precision.

### Mechanism 2: Semantic Gap Bridging via Weighted Expansion
- **Claim:** Concatenating the original query (repeated α times) with a generated pseudo-document improves sparse retrieval (BM25) performance by alleviating vocabulary mismatch while preserving query intent.
- **Mechanism:** Biomedical queries often use lay terms while documents use technical jargon. The LLM-generated pseudo-document serves as a "bridge," likely containing document-side keywords. Repeating the original query (α=5) ensures the BM25 term frequency scoring remains anchored to the user's specific intent, preventing the expansion from diluting the core signal.
- **Core assumption:** The generated pseudo-document contains lexically matching terms present in relevant target documents but absent in the original query.
- **Evidence anchors:**
  - [section 3.5]: Defines q' = q ⊕ ... ⊕ p_q and states the pseudo-document serves as a generator of likely document-side keywords.
  - [abstract]: Reports improvements of up to 22.1% in NDCG@10 over sparse baselines.
  - [corpus]: "LLM-based Query Expansion Fails..." notes failure on ambiguous queries; this mechanism mitigates that by weighting the original query heavily.
- **Break condition:** If α is too low, the verbose pseudo-document may overwhelm the original query keywords, shifting the retrieval topic.

### Mechanism 3: Robustness via Explicit Concept Normalization
- **Claim:** Mapping query terms to Concept Unique Identifiers (CUIs) creates a stable semantic representation that resists query perturbations (paraphrasing).
- **Mechanism:** The pipeline normalizes entities to UMLS CUIs before retrieving context. Even if a paraphrased query uses different lexical forms (e.g., "Risk of Cancer" vs. "Cancer Risk"), the mapping anchors the expansion to the same underlying medical concept, making the retrieval robust to syntactic variations that typically degrade dense retrievers.
- **Core assumption:** The entity recognition step successfully identifies the core medical concepts regardless of query phrasing.
- **Evidence anchors:**
  - [abstract]: "BMQExpander generalizes robustly under query perturbation settings... achieving up to 15.7% improvement."
  - [section 3.1]: Describes the extraction of key biomedical terms to resolve ambiguity.
  - [corpus]: TCDE discusses semantic misalignment in expansion; explicit normalization helps prevent this.
- **Break condition:** Failure of the entity recognition module to spot specific medical terms in paraphrased text will result in an empty set T_q, bypassing the expansion logic.

## Foundational Learning

- **Concept: UMLS Metathesaurus & CUIs**
  - **Why needed here:** The system relies on the Unified Medical Language System (UMLS) to translate ambiguous text into standardized Concept Unique Identifiers (CUIs). Understanding this mapping is critical to debugging why certain definitions are retrieved.
  - **Quick check question:** If the system maps "Cold" to a CUI, does it refer to the temperature or the viral infection, and which source vocabulary determines this?

- **Concept: Sparse vs. Dense Retrieval**
  - **Why needed here:** The paper benchmarks against both BM25 (sparse/lexical) and embedding models (dense/semantic). The proposed method enhances *sparse* retrieval (BM25) using *dense* generation capabilities, bridging the two paradigms.
  - **Quick check question:** Why does the author repeat the original query 5 times instead of just using the expanded pseudo-document for BM25?

- **Concept: Prompt Engineering (Grounding)**
  - **Why needed here:** The efficacy of the method depends on how definitions and relationships are serialized into the prompt. The structure of this prompt dictates the LLM's adherence to facts.
  - **Quick check question:** What specific formatting does the prompt use to distinguish between the user's query and the retrieved medical definitions?

## Architecture Onboarding

- **Component map:** Entity Recognizer (LLM) -> UMLS Linker -> Context Retriever -> Prompt Serializer -> Generator (LLM) -> Retrieval Scorer
- **Critical path:** The **UMLS Linker** is the most fragile component. It uses an *exact-match* strategy. If the user's phrasing does not exactly match a string in the UMLS lookup, the pipeline returns an empty CUI set, effectively disabling the expansion.
- **Design tradeoffs:**
  - **Precision vs. Recall (Linking):** The paper discards terms with no exact UMLS match to prevent noise (high precision), risking missed expansion opportunities for valid but phrased-differently terms.
  - **Efficiency vs. Accuracy:** Relying on LLM generation adds latency compared to standard BM25, but avoids the "training data" requirements of dense retrievers.
- **Failure signatures:**
  - **Empty Expansion:** Output is just the original query (likely due to unrecognized terms in the Entity Recognizer or Linker).
  - **Topic Drift:** The pseudo-document discusses a broader or related disease (e.g., generic "infections" instead of specific "opportunistic infections"), causing BM25 to retrieve general textbooks rather than specific papers.
- **First 3 experiments:**
  1. **Unit Test Linker:** Input specific medical slang vs. formal terms to verify if the exact-match UMLS lookup succeeds or fails silently.
  2. **Ablation on α:** Run retrieval with α=1 (low original weight) vs. α=10 (high original weight) on a sample of short, ambiguous queries to visualize the drift vs. precision tradeoff.
  3. **Perturbation Stress Test:** Feed the system the "Paraphrased" queries from the paper's new dataset (NFCorpus-P) and compare the retrieved CUIs against the original queries to validate the robustness claim.

## Open Questions the Paper Calls Out
- The authors acknowledge that a larger qualitative study with multiple medical experts is needed to generalize the clinical safety and fidelity of the generated pseudo-documents.
- The paper does not confirm whether the improved retrieval metrics directly translate to better performance in downstream biomedical QA tasks.
- The computational overhead and latency of the multi-stage pipeline for real-time clinical search are not benchmarked against highly efficient sparse baselines.

## Limitations
- The ontology-mapping step relies on exact-string matching against UMLS, which may silently fail for valid biomedical terms not present in the string index.
- The claim of "fewer hallucinations" is based on a single expert evaluation across 100 queries with a binary scoring rubric.
- The generalizability of hallucination reduction claims is limited by the small evaluation sample size and lack of comparison to newer grounding techniques.

## Confidence
- **High confidence:** The core mechanism of using UMLS context to ground LLM generation is technically sound and supported by both qualitative analysis and quantitative metrics.
- **Medium confidence:** The robustness claims under query perturbation are compelling but limited to a specific dataset (NFCorpus-P).
- **Low confidence:** The generalizability of hallucination reduction claims is limited by the small evaluation sample size and lack of comparison to newer grounding techniques.

## Next Checks
1. **UMLS Linking Robustness Test:** Create a test suite with biomedical queries containing both formally defined terms and plausible synonyms. Measure the exact-match success rate of the UMLS linker and analyze the correlation between linking success and expansion quality.
2. **Cross-Dataset Generalization:** Apply BMQExpander to biomedical retrieval datasets outside the paper's scope (e.g., BioASQ, TREC Genomics) to verify whether the 22.1% improvement holds across different document collections and query distributions.
3. **Ablation on Context Selection:** Compare BMQExpander against variants that use only definitions (no relations) or only relations (no definitions) to isolate which components of the ontology context contribute most to the performance gains and hallucination reduction.