---
ver: rpa2
title: 'ALiiCE: Evaluating Positional Fine-grained Citation Generation'
arxiv_id: '2406.13375'
source_url: https://arxiv.org/abs/2406.13375
tags:
- citation
- fine-grained
- node
- positional
- claim
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces ALiiCE, the first automatic evaluation framework
  for positional fine-grained citation generation in long-form question answering.
  The method parses sentences into atomic claims using dependency trees and evaluates
  citation quality with three metrics: positional fine-grained citation recall, precision,
  and coefficient of variation of citation positions (CVCP).'
---

# ALiiCE: Evaluating Positional Fine-grained Citation Generation

## Quick Facts
- arXiv ID: 2406.13375
- Source URL: https://arxiv.org/abs/2406.13375
- Reference count: 37
- Primary result: ALiiCE framework evaluates positional fine-grained citation quality in long-form QA using dependency parsing and NLI, revealing limited citation generation in current LLMs.

## Executive Summary
This paper introduces ALiiCE, the first automatic evaluation framework for positional fine-grained citation generation in long-form question answering. The method parses sentences into atomic claims using dependency trees and evaluates citation quality with three metrics: positional fine-grained citation recall, precision, and coefficient of variation of citation positions (CVCP). Experiments on ASQA and ELI5 datasets show that existing LLMs generate limited positional fine-grained citations, with GPT-3.5 (10-psg) achieving the best citation recall (86.1%) and precision (51.1%) on ASQA. Human evaluation demonstrates high consistency between ALiiCE and human judgment, with Cohen's kappa coefficients of 0.71 for recall and 0.62 for precision.

## Method Summary
ALiiCE evaluates positional fine-grained citations by first parsing sentences into atomic claims using dependency trees. The algorithm identifies citation nodes, computes their Lowest Common Ancestor (LCA), and prunes or replaces subtrees to isolate the text span belonging to each citation group. For each atomic claim, Natural Language Inference (NLI) models judge whether cited passages entail the claim (recall) and whether individual citations are necessary (precision). The framework also introduces CVCP, which measures citation dispersion by computing the normalized standard deviation of citation positions within sentences.

## Key Results
- Existing LLMs generate limited positional fine-grained citations: GPT-3.5 (10-psg) achieves best citation recall (86.1%) and precision (51.1%) on ASQA
- Open-source LLaMA-3-8B achieves highest CVCP (0.44-0.61), indicating better mid-sentence citation dispersion
- Human evaluation shows high consistency with ALiiCE: Cohen's kappa of 0.71 for recall and 0.62 for precision
- NLI-based evaluation is reliable: <1% inconsistency between ALiiCE and GPT-4 refined claims

## Why This Works (Mechanism)

### Mechanism 1: Dependency Tree-Based Atomic Claim Decomposition
The algorithm identifies citation nodes in dependency trees, computes LCA between citation pairs, and prunes subtrees to isolate atomic claims. This converts multi-citation sentences into fine-grained atomic claims like "Cups can be made of glass" and "Cups can be made of plastic." Core assumption: dependency parse accurately reflects semantic constituency. Evidence: ALiiCE successfully decomposes 99% of sentences with <1% NLI inconsistency. Break condition: malformed sentences or non-standard citation placements cause parsing errors.

### Mechanism 2: NLI-Based Citation Recall and Precision Scoring
For each atomic claim and citation group, NLI models judge entailment (recall) and individual citation necessity (precision). This provides principled quality metrics based on whether cited passages actually support the claims. Core assumption: NLI models generalize well to claim-passage pairs. Evidence: Human evaluation shows kappa of 0.71 (recall) and 0.62 (precision). Break condition: NLI context length limits or ambiguous entailment cases reduce reliability.

### Mechanism 3: CVCP Quantifies Citation Dispersion
CVCP computes normalized standard deviation of citation marker positions per sentence, then averages across the response. Higher CVCP indicates citations appear mid-sentence rather than clustered at the end. Core assumption: dispersion correlates with fine-grained citation utility. Evidence: LLaMA-3-8B achieves highest CVCP (0.44-0.61) while GPT-3.5 shows lower dispersion (0.10-0.15). Break condition: High CVCP doesn't guarantee utility—some high-CVCP citations were redundant or unhelpful.

## Foundational Learning

- **Concept: Dependency Parsing and LCA**
  - Why needed: The core algorithm relies on identifying grammatical structure and subtree relationships to decompose claims.
  - Quick check: Given a sentence with two citation groups, can you manually identify the LCA node and which subtrees belong to each claim?

- **Concept: Natural Language Inference (NLI)**
  - Why needed: Citation recall and precision depend on entailment judgments between claims and passages.
  - Quick check: Given the claim "Cups can be made of glass" and passage "One raw material of cups is glass," would an NLI model predict entailment, contradiction, or neutral?

- **Concept: Coefficient of Variation (CV)**
  - Why needed: CVCP normalizes position dispersion to compare across sentences of varying lengths.
  - Quick check: If all citations appear at position 0.9 of sentence length for every sentence, what is the CVCP?

## Architecture Onboarding

- **Component map**: Text Cleaner -> Dependency Parser (SpaCy) -> Citation Node Matcher -> LCA Subtree Modifier -> Tree-to-Text Converter -> NLI Evaluator (TRUE/T5-11B) -> CVCP Calculator -> Aggregator

- **Critical path**: Raw response → Text cleaning → Dependency parsing → Citation node matching → LCA-based subtree modification → Claim text extraction → NLI evaluation → Metric aggregation

- **Design tradeoffs**: SpaCy vs. more accurate parsers (efficiency vs. accuracy); TRUE (T5-11B) vs. smaller NLI models (performance vs. compute); sentence-level vs. atomic-level evaluation (speed vs. precision).

- **Failure signatures**: Grammatical errors → malformed dependency trees → incorrect claim parsing; overlapping evidence → false precision penalties; NLI context overflow → unreliable judgments; high CVCP with low utility → redundant citations.

- **First 3 experiments**:
  1. Run ALiiCE on held-out set with manually annotated atomic claims; compute precision/recall of claim extraction against gold labels.
  2. Compare TRUE (T5-11B) against smaller NLI models (e.g., DeBERTa-base) on citation recall/precision correlation with human judgments.
  3. Manually annotate citation utility for 100+ responses; compute correlation between CVCP and utility to validate whether dispersion proxies usefulness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can evaluation metrics be designed to assess "citation utility"—the usefulness of a citation to the user—rather than just citation correctness?
- Basis: Section 7 notes existing metrics fail to assess citation utility, which is critical for future exploration.
- Why unresolved: Current metrics verify support but cannot distinguish necessary citations from redundant ones offering no verification value.
- What evidence would resolve it: A new framework or dataset where models are scored on qualitative utility of citations, validated by human judgment.

### Open Question 2
- Question: What are effective methods for constructing supervised training data for positional fine-grained citation generation?
- Basis: Section 7 highlights creating labeled data presents significant challenge as existing sentence-level annotation algorithms are insufficient.
- Why unresolved: Complexity of placing citations mid-sentence makes it difficult to generate ground-truth labels at scale, limiting supervised learning.
- What evidence would resolve it: Development of automated annotation pipeline (e.g., using multi-hop QA datasets) producing high-quality positional labels, demonstrated through successful model fine-tuning.

### Open Question 3
- Question: Does integrating logical reasoning paths or multi-step retrieval into the generation process enhance the quality of positional fine-grained citations?
- Basis: Section 7 suggests constructing reasoning paths can establish clearer logical relationships, thereby promoting fine-grained citations.
- Why unresolved: Authors observe useful fine-grained citations exhibit logical structures but didn't experimentally verify if enforcing these structures improves generation performance.
- What evidence would resolve it: Experimental results showing models guided by Chain-of-Thought reasoning achieve higher CVCP and utility scores than standard models.

### Open Question 4
- Question: How robust is the dependency tree-based ALiiCE framework when applied to languages other than English?
- Basis: Limitations section states dependency analysis may be primarily applicable to mainstream languages such as English, and transferring it might result in reduced evaluation accuracy.
- Why unresolved: Parsing algorithm relies on SpaCy's English dependency trees; languages with different syntactic structures may cause LCA method to fail at extracting atomic claims correctly.
- What evidence would resolve it: Successful application of ALiiCE to multilingual long-form QA datasets, showing high consistency with human evaluation across diverse linguistic structures.

## Limitations
- Dependency parsing robustness may degrade on complex sentence structures or non-standard citation placements
- NLI model generalization is limited to ASQA and ELI5 datasets; performance may vary across domains
- CVCP metric validity is unproven—high CVCP doesn't necessarily indicate citation utility

## Confidence
- **High confidence**: Core claim that existing LLMs produce limited positional fine-grained citations is well-supported by experimental results
- **Medium confidence**: Open-source models narrowing gap with closed-source models is supported but based on single model comparison
- **Low confidence**: ALiiCE provides comprehensive evaluation framework is limited by validation scope (two datasets, specific NLI model, single dependency parser)

## Next Checks
1. Apply ALiiCE to independent long-form QA dataset with manual atomic claim annotations to verify parsing accuracy across domains
2. Systematically compare TRUE against smaller NLI models on human judgment correlation to assess necessity of T5-11B model
3. Conduct controlled human evaluation rating citation utility independently of position, then compute correlation between CVCP and utility ratings