---
ver: rpa2
title: Multilingual and Multi-Accent Jailbreaking of Audio LLMs
arxiv_id: '2504.01094'
source_url: https://arxiv.org/abs/2504.01094
tags:
- audio
- arxiv
- multilingual
- reverb
- inputs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MULTI-AUDIO JAIL, a novel framework exposing
  vulnerabilities in Large Audio Language Models (LALMs) through multilingual and
  multi-accent jailbreaking attacks. The authors construct a comprehensive dataset
  of adversarially perturbed audio prompts across six languages (English, German,
  Spanish, French, Italian, Portuguese) and multiple accents, then apply acoustic
  perturbations including reverberation, echo, and whisper effects.
---

# Multilingual and Multi-Accent Jailbreaking of Audio LLMs

## Quick Facts
- arXiv ID: 2504.01094
- Source URL: https://arxiv.org/abs/2504.01094
- Reference count: 40
- Primary result: Audio-based attacks achieve 3.1× higher jailbreak success rates than text-only attacks across multilingual/accents

## Executive Summary
This paper exposes significant vulnerabilities in Large Audio Language Models (LALMs) through multilingual and multi-accent jailbreaking attacks. The authors construct a comprehensive dataset of adversarially perturbed audio prompts across six languages and multiple accents, then systematically evaluate how acoustic perturbations amplify jailbreak success. Their key finding reveals that multimodal LLMs are inherently more vulnerable than text-only systems, with attackers able to exploit the weakest link - non-English audio inputs - to achieve dramatically higher success rates. The study demonstrates that acoustic perturbations like reverberation can increase jailbreak success by up to 57.25 percentage points, while introducing an inference-time text-based defense that reduces but doesn't eliminate these vulnerabilities.

## Method Summary
The researchers generated 102,720 audio files from 520 harmful prompts translated to five languages (German, Spanish, French, Italian, Portuguese) plus English. They created natural accents (Australia, Singapore, South Africa, Philippines, Kenya, Nigeria) and synthetic accents (Chinese, Korean, Japanese, Arabic, Portuguese, Spanish, Tamil) using TTSMaker API. Three acoustic perturbations were applied: reverberation (via impulse response convolution with Teisco, Room, Railway IRs), echo (delayed copy with decay), and whisper effects (amplitude reduction, low-pass filtering, noise injection). They evaluated five LALMs (Qwen2-Audio, DiVA, MERaLiON, MiniCPM-o-2.6, Ultravox) using Llama Guard 3 for safety classification and Whisper-large-v3 for transcription quality measurement.

## Key Results
- Audio-only attacks achieve 3.1× higher jailbreak success rates than text-only attacks across all evaluated models
- Reverberation perturbations cause extreme JSR increases, with Reverb Teisco boosting Qwen2's German performance from 9.71% to 57.79%
- Synthetic accents show significantly higher baseline JSRs (11.42% avg) compared to natural accents (2.54% avg)
- Text-based inference defense reduces JSR by 5-20 percentage points but doesn't eliminate vulnerabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual audio inputs bypass text-centric safety filters more effectively than text inputs.
- Mechanism: LALMs transcribe audio inputs via ASR before processing through text-based LLMs. Safety alignment training primarily targets text; audio pathways lack equivalent safety conditioning. Slight transcription errors or language-specific phonetic characteristics may allow adversarial cues to pass through unfiltered.
- Core assumption: Safety alignment is predominantly trained on text inputs, leaving audio modalities under-defended.
- Evidence anchors: Abstract statement about multimodal vulnerability; section 5.1 showing German audio reaching 12.31% vs 3.92% for text; corpus evidence from audio injection attacks.

### Mechanism 2
- Claim: Acoustic perturbations amplify jailbreak success by degrading ASR transcription quality and introducing cross-lingual phonetic ambiguity.
- Mechanism: Perturbations convolve audio signals with impulse responses or add delayed/filtered copies, increasing word error rate (WER). Higher WER correlates with elevated JSR because safety filters receive garbled or ambiguous text. Reverberation shows strongest effect (+27.41 pp average with Reverb Teisco).
- Core assumption: Safety filters depend on clean transcriptions; degraded transcriptions evade pattern matching.
- Evidence anchors: Abstract showing JSR surges up to +57.25 pp; section 5.3.1 demonstrating Reverb Teisco causing extreme increases; corpus lacks direct perturbation-WER-JSR causal evidence.

### Mechanism 3
- Claim: Accent variations exploit inconsistent multilingual safety coverage across accent representations.
- Mechanism: Natural accents and synthetic accents produce different phonetic patterns. Synthetic accents show higher baseline JSRs (11.42% avg) vs. natural (2.54% avg), suggesting acoustic artifacts from cross-language voice generation create additional ambiguity.
- Core assumption: Accent-specific phonetics introduce transcription variability that safety training doesn't cover uniformly.
- Evidence anchors: Section 5.2 showing synthetic accents increasing JSRs dramatically; section 5.3.2 demonstrating Kenyan inputs triggering remarkable 50.88% JSR; corpus doesn't contradict findings from concurrent accent modification work.

## Foundational Learning

- **Audio-language model architecture (encoder → LLM pipeline)**: Understanding that audio is first transcribed by ASR (e.g., Whisper) before text LLM processing explains why audio-specific attacks work—the safety filter sees only the transcribed text, not the original audio. Quick check: Can you trace where in the pipeline the safety filter is applied—on raw audio, transcription, or both?

- **Word Error Rate (WER) as proxy for attack surface**: The paper correlates higher WER under perturbations with higher JSR. Understanding this relationship helps predict which acoustic modifications will be most effective. Quick check: If a perturbation increases WER from 0.10 to 0.70, would you expect JSR to increase, decrease, or stay the same? Why?

- **Multimodal "weakest link" vulnerability**: The core insight is that multimodal systems inherit the weakest safety coverage across all modalities. Adding capabilities (languages, audio) expands attack surface disproportionately. Quick check: If a model has 99% text safety coverage but 60% audio safety coverage, what's the effective system safety level?

## Architecture Onboarding

- **Component map**: Audio prompts (multilingual/multi-accent) → TTS generation → ASR encoder (Whisper) → Text safety filter → LLM backbone → Response → JSR measurement
- **Critical path**: Audio input → ASR transcription (perturbation degrades quality here) → Text safety filter (evaded by ambiguous transcription) → LLM response → JSR measurement
- **Design tradeoffs**: Robust ASR vs. utility (lower WER may have lower JSR but better legitimate performance); Text-only defense vs. audio-specific defense (inference-time text defense reduces JSR ~5-20 pp but doesn't address root audio vulnerability); Dataset coverage vs. attack generalization (training on more languages/accents improves robustness but expands attack surface)
- **Failure signatures**: High JSR with low WER indicates safety filter gap rather than ASR weakness; Disproportionate non-English JSR confirms multilingual safety coverage gap; Accent-specific JSR spikes suggest training data distribution mismatch
- **First 3 experiments**: (1) Reproduce baseline JSR gap: Run text-only vs. audio-only attacks on Qwen2 across German, Spanish, Portuguese to confirm 3.1× multiplier; (2) Perturbation sensitivity test: Apply Reverb Teisco to 50 samples per language, measure WER and JSR correlation; (3) Defense ablation: Test text-based inference defense on perturbed inputs to quantify residual vulnerability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can audio-specific adversarial training or input preprocessing substantially reduce jailbreak success rates against LALMs without degrading transcription accuracy?
- Basis in paper: Ethics statement notes methods tailored to audio inputs should be further developed since text-based defense only reduces JSR inconsistently
- Why unresolved: Authors test only inference-time text-based defense that doesn't modify audio inputs or model architecture; no audio-native defenses are evaluated
- What evidence would resolve it: Comparative experiments applying audio-domain defenses (adversarial audio augmentation during training, real-time denoising pipelines) measuring JSR reduction alongside WER and SQA accuracy preservation

### Open Question 2
- Question: What mechanistic interaction between reverberation perturbations and language-specific phonetics causes Romance languages and German to exhibit far greater JSR increases than English?
- Basis in paper: Paper reports reverberation causes German JSR to surge +48.08 pp while English increases only +20.96 pp, hypothesizing factors like training data disparities and phonetic characteristics
- Why unresolved: Analysis remains correlational; relative contributions of phonetic structure, prosody, training data volume, and safety-conditioning coverage are not disentangled through controlled experiments
- What evidence would resolve it: Ablation studies with matched training data sizes across languages, phonetic feature analyses, and layer-wise probing to identify where in the audio-text pipeline the vulnerability emerges

### Open Question 3
- Question: Would gradient-based optimization of audio perturbations yield substantially higher JSRs than the non-optimization approach used in this study?
- Basis in paper: Limitations section states they don't employ optimization techniques used in prior works and acknowledges such methods could potentially yield even higher JSRs
- Why unresolved: Paper intentionally restricts attacks to signal-level perturbations without iterative optimization to maintain black-box realism; upper bound of attack success remains unknown
- What evidence would resolve it: Experiments applying white-box or query-based optimization to craft adversarial audio for same LALMs and languages, comparing optimized JSRs to non-optimized baselines

## Limitations
- The attack success rates may be specific to Whisper-based ASR pipelines and may not generalize to LALMs with different ASR backends
- The causal relationship between acoustic perturbations and jailbreak success remains correlative rather than definitively established
- The text-based defense mechanism provides incomplete protection, reducing JSR by only 5-20 percentage points while leaving significant residual vulnerability

## Confidence
- **High Confidence**: Multilingual audio vs. text JSR gap (3.1× higher) is well-supported by systematic evaluation across six languages and five models
- **Medium Confidence**: Acoustic perturbation mechanism (WER → ASR degradation → safety filter evasion) is plausible and supported by correlation data but requires additional validation
- **Medium Confidence**: Claim that multimodal systems are inherently more vulnerable due to exploiting "weakest link" is conceptually sound but specific quantification may vary

## Next Checks
1. **Architecture-specific vulnerability mapping**: Test the same attack methodology across LALMs with different ASR backends to determine whether observed vulnerabilities are specific to the Whisper pipeline or represent broader architectural weakness

2. **Controlled perturbation experiment**: Conduct ablation study where perturbed audio is first transcribed with and without perturbations, then fed to text-only LLM with identical safety filters to isolate whether vulnerability stems from degraded transcriptions or other perturbation effects

3. **Defense mechanism expansion**: Implement and evaluate audio-specific safety filters that process original audio signal using features like prosodic patterns, speaker verification, or direct audio classification to quantify residual vulnerability gap compared to current text-only defense