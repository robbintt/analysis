---
ver: rpa2
title: Measurement Score-Based MRI Reconstruction with Automatic Coil Sensitivity
  Estimation
arxiv_id: '2509.18402'
source_url: https://arxiv.org/abs/2509.18402
tags:
- diffusion
- measurement
- c-msm
- reconstruction
- csms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of compressed-sensing parallel
  MRI reconstruction when both pre-calibrated coil sensitivity maps (CSMs) and ground-truth
  images are unavailable. The authors propose C-MSM, a self-supervised diffusion-based
  framework that jointly learns CSM estimation and measurement score functions directly
  from undersampled k-space data.
---

# Measurement Score-Based MRI Reconstruction with Automatic Coil Sensitivity Estimation

## Quick Facts
- arXiv ID: 2509.18402
- Source URL: https://arxiv.org/abs/2509.18402
- Reference count: 13
- Primary result: C-MSM achieves PSNR of 32.7 dB and SSIM of 0.853 under ×4 acceleration, approaching supervised diffusion methods while requiring no ground-truth images or pre-calibrated CSMs.

## Executive Summary
This paper addresses the challenge of compressed-sensing parallel MRI reconstruction when both pre-calibrated coil sensitivity maps (CSMs) and ground-truth images are unavailable. The authors propose C-MSM, a self-supervised diffusion-based framework that jointly learns CSM estimation and measurement score functions directly from undersampled k-space data. The method uses a two-part network: a CSM prediction module that estimates coil sensitivities from auto-calibration signal regions, and a measurement denoising module that learns to denoise corrupted measurements in image space using the predicted CSMs. During training, both components are optimized together using a combination of measurement denoising loss and gradient penalty to enforce spatial smoothness of CSMs. At inference, C-MSM approximates the posterior distribution through stochastic sampling over partial measurement scores while refining CSMs. Experiments on the multi-coil fastMRI dataset show that C-MSM achieves PSNR of 32.7 dB and SSIM of 0.853 under ×4 acceleration, approaching the performance of supervised diffusion methods that require clean training data and pre-calibrated CSMs, while outperforming other self-supervised baselines.

## Method Summary
C-MSM is a self-supervised diffusion-based framework for compressed-sensing parallel MRI reconstruction that jointly estimates coil sensitivity maps (CSMs) and learns measurement score functions from undersampled k-space data without requiring ground-truth images or pre-calibrated CSMs. The method consists of two networks: a CSM prediction module that estimates coil sensitivities from auto-calibration signal (ACS) regions, and a measurement denoising module based on diffusion models that denoises corrupted measurements in image space. During training, both components are optimized jointly using a measurement denoising loss and a gradient penalty to enforce spatial smoothness of CSMs. At inference, C-MSM approximates the posterior distribution through stochastic sampling over partial measurement scores while refining CSMs, using weighted aggregation across multiple random subsampling masks to improve reconstruction quality.

## Key Results
- C-MSM achieves PSNR of 32.7 dB and SSIM of 0.853 under ×4 acceleration on multi-coil fastMRI brain data
- The method approaches performance of supervised diffusion methods that require ground-truth images and pre-calibrated CSMs
- Outperforms other self-supervised baselines including UNN and C-SPIRiT on the same dataset

## Why This Works (Mechanism)

### Mechanism 1: Joint CSM Estimation and Diffusion Training from Undersampled k-space
The method predicts coil sensitivity maps from ACS regions while simultaneously learning measurement denoising, enabling self-supervised reconstruction without ground-truth images or pre-calibrated CSMs. The CSM prediction network extracts spatial sensitivity patterns from the fully-sampled ACS region of k-space, which then serve as the coil combination operator for transforming multi-coil k-space data into a composite image representation. The diffusion model learns to denoise in this image space, with gradients backpropagating through both networks to jointly optimize CSM accuracy and denoising fidelity.

### Mechanism 2: Spatial Smoothness Constraint on Coil Sensitivity Maps via Gradient Penalty
Adding a gradient penalty to the CSM prediction loss enforces physically-motivated smoothness, stabilizing joint optimization and preventing degenerate solutions. The L_CSM loss directly penalizes the L2 norm of the spatial gradient of predicted CSMs, reflecting the physical property that coil sensitivity varies smoothly across space. This constraint counteracts potential overfitting or high-frequency artifacts in CSM estimates that could arise from fitting undersampled measurements alone.

### Mechanism 3: Stochastic Mini-batch Posterior Sampling with Weighted Aggregation
Approximating the full posterior by aggregating denoised estimates from multiple random subsampling masks improves reconstruction quality over single-mask sampling. During inference, C-MSM applies w random subsampling operators to the current diffusion iterate, denoises each subsampled measurement using the learned score network, enforces data consistency with observed measurements, and aggregates via the weighting scheme. This ensemble approach provides implicit regularization by averaging over multiple measurement perspectives.

## Foundational Learning

- **Parallel MRI forward model (y = Ax where A includes coil sensitivities, Fourier transform, and undersampling)**
  - Why needed: The entire method is built on decomposing and inverting this acquisition model. Without understanding how coil sensitivities modulate the image and how undersampling restricts k-space, the motivation for joint CSM estimation and the structure of the denoising pipeline will be opaque.
  - Quick check: Given a 4-coil acquisition with known CSMs and 4× random undersampling, can you write out the dimensions of y, C_k, F, and S, and explain what information is missing from the undersampled measurements?

- **Score-based diffusion models as learned priors (∇_x log p(x))**
  - Why needed: C-MSM uses a diffusion model not for image generation but as a prior that scores the plausibility of intermediate noisy estimates. Understanding how score functions guide denoising and how they can be conditioned on measurements is essential to grasp the training and sampling algorithms.
  - Quick check: If a diffusion model learns ∇_x_t log p(x_t) at noise level σ_t, how would you use this score to denoise a corrupted image x_t = x + σ_t·n? What does the score point toward?

- **Auto-calibration signal (ACS) in compressed sensing MRI**
  - Why needed: The ACS region—the center of k-space that is fully sampled even in accelerated acquisitions—is the only source of coil sensitivity information available to C-MSM. Understanding why ACS exists and what it encodes is critical to assessing when C-MSM's CSM estimation can succeed.
  - Quick check: Why is the center of k-space prioritized in accelerated MRI acquisitions, and what spatial information does it primarily contain? How might ACS lines relate to coil sensitivity estimation?

## Architecture Onboarding

- **Component map**: 
  - CSM Prediction Network (f_ϕ) <- F^H s_ACS -> Ĉ_ϕ (estimated CSMs)
  - Diffusion Backbone (D_θ) <- F^H Ĉ_ϕ^H s_t -> denoised image estimate
  - Joint Loss: L_total = L_MSM + λ·L_CSM -> updates both ϕ and θ

- **Critical path**:
  1. Input undersampled multi-coil k-space y, extract y_ACS
  2. CSM network predicts Ĉ_ϕ from F^H y_ACS → normalize → (used in all subsequent steps)
  3. Initialize diffusion with z_T ~ N(0, I)
  4. For t = T to 1: apply w subsampling masks → denoise each → data consistency step → aggregate → sample z_{t-1}
  5. Final coil-combined image from z_0

- **Design tradeoffs**:
  - λ (CSM smoothness weight) = 1000: Higher values enforce smoother CSMs but may underfit true coil variations; lower values risk high-frequency CSM artifacts. Paper does not report ablations—tuning may be dataset-dependent.
  - Mini-batch size w = 10: More samples improve ensemble averaging but increase inference compute linearly. Paper does not report sensitivity to this hyperparameter.
  - Diffusion backbone choice: Paper uses standard architecture; specialized medical image architectures could improve performance but require additional engineering.
  - ACS region size: Paper uses 20 ACS lines at test time; fewer lines may degrade CSM estimation, more lines reduce acceleration benefit.

- **Failure signatures**:
  - CSM estimation collapse: If predicted CSMs are all near-zero or highly irregular, check gradient penalty implementation and λ scaling relative to L_MSM magnitude.
  - Measurement inconsistency artifacts: If reconstructed images show aliasing or structured noise inconsistent with undersampling pattern, verify data consistency step is correctly restricting to observed coordinates.
  - Slow convergence or unstable training: Joint optimization of two networks can be unstable; monitor both L_MSM and L_CSM separately. If L_CSM dominates, reduce λ; if L_MSM stalls, check CSM normalization.

- **First 3 experiments**:
  1. **Baseline replication with fixed pre-calibrated CSMs**: Implement C-MSM training loop but provide ground-truth CSMs. Compare PSNR/SSIM against paper's C-MSM results to isolate the contribution of joint CSM estimation vs. the measurement-score learning framework.
  2. **ACS ablation (10, 20, 40 lines)**: Train and evaluate C-MSM with varying ACS region sizes to quantify sensitivity of CSM estimation quality to calibration data availability. Plot CSM estimation error and final reconstruction metrics.
  3. **λ sensitivity sweep (10, 100, 1000, 10000)**: Train separate models with different smoothness penalties to characterize the tradeoff between CSM smoothness and reconstruction fidelity. Visualize predicted CSMs for qualitative assessment of over/under-smoothing.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the reconstruction quality of C-MSM degrade as the number of available ACS lines decreases, particularly at extreme acceleration factors?
- Basis in paper: While the method is "calibration-free" regarding separate scans, the CSM prediction module relies explicitly on extracting s_ACS from the undersampled data, and experiments used a fixed set of 20 ACS lines.
- Why unresolved: The paper does not ablate the number of ACS lines or test scenarios where the ACS region is highly corrupted or missing.
- What evidence would resolve it: An ablation study showing PSNR/SSIM curves as the number of ACS lines varies from 0 to 32.

### Open Question 2
- Question: Can the C-MSM framework be adapted for non-Cartesian sampling trajectories (e.g., radial or spiral) where distinct ACS lines are not available?
- Basis in paper: The methodology formulates the CSM input based on the ACS region and assumes a discrete Fourier transform suitable for Cartesian grids.
- Why unresolved: Non-Cartesian trajectories sample k-space differently, often lacking the central "lines" used for ACS extraction in this implementation.
- What evidence would resolve it: Implementation and evaluation of C-MSM on radial or spiral fastMRI data, potentially modifying the CSM input to use a low-resolution resampled grid.

### Open Question 3
- Question: Does the explicit gradient penalty for CSM smoothness inadvertently limit the model's ability to estimate sharp sensitivity transitions required by complex coil geometries?
- Basis in paper: The paper introduces a gradient penalty L_CSM with a high weight (λ=1000) to enforce spatial smoothness.
- Why unresolved: While smoothness is a known property of CSMs, rigid enforcement via L2 gradient penalty might over-smooth sensitivities for flexible coil arrays or surface coils with rapid spatial fall-off.
- What evidence would resolve it: Visualizing the estimated Ĉ_φ for surface coils and comparing reconstruction fidelity near the coil surface against methods with less restrictive smoothness constraints.

## Limitations
- Heavy dependency on quality and representativeness of ACS region data, with no analysis of performance degradation when ACS lines are limited or corrupted
- Potential over-smoothing of coil sensitivity maps due to aggressive gradient penalty (λ=1000), which may not generalize well to complex coil geometries
- Joint optimization of CSM estimation and diffusion training may be unstable, but the paper provides limited diagnostic information about training dynamics or hyperparameter sensitivity

## Confidence

- **High confidence**: The core measurement-score diffusion framework and CSM estimation mechanism are technically sound and implementable based on the descriptions provided.
- **Medium confidence**: The claim that C-MSM approaches supervised diffusion performance without ground-truth images is supported by reported metrics, but the comparison is limited to self-supervised baselines without direct supervised diffusion ablations.
- **Low confidence**: The assertion that gradient penalty alone sufficiently regularizes CSM estimation without additional constraints requires empirical validation across diverse acquisition scenarios.

## Next Checks

1. **Fixed-CSM ablation**: Implement C-MSM with ground-truth ESPIRiT CSMs to isolate the measurement-score learning contribution from joint CSM estimation—this will reveal whether the self-supervised aspect adds value beyond the measurement denoising framework.

2. **ACS line sensitivity**: Systematically vary ACS line counts (10, 20, 40) during both training and testing to quantify the tradeoff between calibration data availability and reconstruction quality, with CSM estimation error metrics if ground-truth CSMs are available.

3. **Gradient penalty sensitivity**: Train models with λ∈{10, 100, 1000, 10000} and visualize predicted CSMs alongside reconstruction metrics to characterize the smoothness-generalization tradeoff and identify potential over-regularization regimes.