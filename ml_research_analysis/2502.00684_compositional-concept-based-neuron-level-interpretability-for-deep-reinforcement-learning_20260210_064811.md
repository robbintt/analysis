---
ver: rpa2
title: Compositional Concept-Based Neuron-Level Interpretability for Deep Reinforcement
  Learning
arxiv_id: '2502.00684'
source_url: https://arxiv.org/abs/2502.00684
tags:
- neuron
- concept
- concepts
- neurons
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the lack of interpretability and transparency
  in deep reinforcement learning (DRL) models, which hinders trust in high-stakes
  applications. The authors propose a novel concept-based interpretability method
  that operates at the neuron level, formalizing atomic concepts as binary functions
  over the state space and constructing complex concepts through logical operations.
---

# Compositional Concept-Based Neuron-Level Interpretability for Deep Reinforcement Learning

## Quick Facts
- arXiv ID: 2502.00684
- Source URL: https://arxiv.org/abs/2502.00684
- Reference count: 3
- Primary result: Novel concept-based interpretability method using Jaccard similarity and beam search to find compositional concepts that explain individual neuron activations in DRL policy/value networks

## Executive Summary
This paper addresses the critical challenge of interpretability in deep reinforcement learning (DRL) by proposing a novel method for neuron-level explanations. The approach formalizes atomic concepts as binary functions over the state space and constructs complex concepts through logical operations. By analyzing the correspondence between neuron activations and concept functions using Jaccard similarity and beam search optimization, the method provides fine-grained, human-interpretable explanations for individual neurons in policy/value networks. The approach was validated through experiments on discrete (Blackjack-v1, LunarLander-v3) and continuous (LunarLander-Continuous-v2) environments, successfully identifying meaningful concepts that align with human understanding and demonstrating reliable causal relationships through targeted perturbation experiments.

## Method Summary
The method involves collecting 10K state samples from trained DQN or PPO agents, extracting activations from the second hidden layer, binarizing these activations at threshold β=0, and using beam search (beam width=10, max formula length=5) to find compositional concepts maximizing Jaccard similarity with each neuron's activation pattern. Only neurons activating in >5% of samples are analyzed. Atomic concepts are manually defined as interval-based predicates over state variables. The approach focuses on finding the optimal compositional concept C* that maximizes Jaccard similarity J(ai,l, c) = |ai,l ∩ c| / |ai,l ∪ c| between binarized activation vectors and concept vectors. Validation involves targeted perturbations to test causal relationships between concept satisfaction, neuron activation, and action selection.

## Key Results
- Identified meaningful concepts aligning with human understanding: Neuron 28 in Blackjack detects high sums (18-21) with Jaccard 0.997, Neuron 41 in LunarLander manages attitude control
- Jaccard similarity scores ranged from 0.60 to 0.997 for identified neuron-concept mappings
- Perturbation experiments validated causal relationships: violating a neuron's concept reliably leads to predictable changes in both neuron activation and action selection
- Method successfully applied across discrete (Blackjack-v1, LunarLander-v3) and continuous (LunarLander-Continuous-v2) environments

## Why This Works (Mechanism)

### Mechanism 1: Binary Concept-Activation Alignment via Jaccard Similarity
- Claim: Neurons can be interpreted by matching binarized activation patterns to compositional logical expressions over state variables
- Mechanism: Define atomic concepts as binary functions C: S → {0,1}, binarize neuron activations using threshold Tβ, compute Jaccard similarity J(ai,l, c) = |ai,l ∩ c| / |ai,l ∪ c|, use beam search to find optimal compositional concept maximizing similarity
- Core assumption: Individual neurons encode semantically meaningful patterns rather than distributed representations
- Evidence anchors: Abstract states "analyzing the correspondence between neuron activations and concept functions using Jaccard similarity and beam search optimization"; Section 3.2 Equation 5 formalizes finding C* = argmax J(ai,l, c); Related work (Mu and Andreas, 2020) shows compositional explanations work for vision models
- Break condition: If neurons exhibit highly distributed representations (no single neuron achieves Jaccard > ~0.5 with any concept), method fails to produce meaningful interpretations

### Mechanism 2: Compositional Concept Construction via Boolean Operations
- Claim: Complex decision-relevant concepts emerge from logical combinations (AND, OR, NOT) of atomic state predicates
- Mechanism: Build concept space C through recursive composition using conjunction (min), disjunction (max), negation (1-C); beam search explores compositions up to length k (max 5 in experiments)
- Core assumption: Decision boundaries in RL can be expressed as Boolean combinations of thresholded state variables
- Evidence anchors: Section 3.1 formalizes compositional concepts Ck = op1(op2(...opk-1(C1, C2), ..., Ck)); Figure 1 shows example: "(c1 ∨ c2) ∧ c3" explains a neuron; Limited direct evidence for RL specifically
- Break condition: If policies require continuous fuzzy logic or attention over many variables simultaneously, Boolean compositions may be insufficiently expressive

### Mechanism 3: Perturbation-Based Causal Validation
- Claim: Identified concept-neuron mappings causally influence action selection; modifying concept-relevant features predictably changes both neuron activation and downstream behavior
- Mechanism: Identify neurons with high connection weights to specific actions; select states where concept is satisfied and neuron active; perturb state to violate concept; observe activation drop and action change
- Core assumption: Relationship between concept satisfaction, neuron activation, and action selection is monotonic and predictable
- Evidence anchors: Section 5.1 LunarLander Neuron 5: perturbing x from 0.27 to -0.24 changed activation from +4.00 to -4.48 and switched action; Section 5.2 Blackjack Table 2 shows consistent activation-to-inhibition transitions; Limited external validation
- Break condition: If perturbations trigger compensatory activation in other neurons (distributed redundancy), effects may not be predictable from single-neuron analysis

## Foundational Learning

- Concept: **Jaccard Similarity for Binary Vectors**
  - Why needed here: Core metric for measuring alignment between neuron activation patterns and concept functions; interprets the "goodness of fit"
  - Quick check question: Given vectors a = [1,0,1,1,0] and c = [1,1,1,0,0], compute J(a,c). (Answer: |{1,3}∩{1,2,3}| / |{1,2,3}| = 2/3 ≈ 0.67)

- Concept: **Beam Search for Discrete Optimization**
  - Why needed here: The concept search space grows exponentially; beam search provides tractable approximate optimization over Boolean formulas
  - Quick check question: With beam width 3 and candidates {A,B,C,D,E} with scores {0.9, 0.8, 0.7, 0.6, 0.5}, which candidates survive? (Answer: A, B, C)

- Concept: **DRL Value/Policy Network Architecture**
  - Why needed here: Interpretation targets specific layers (penultimate hidden layer); requires understanding how activations propagate to action outputs
  - Quick check question: In DQN, what does the Q-network output for each action? (Answer: Estimated expected return Q(s,a) for discrete actions)

## Architecture Onboarding

- Component map: Atomic Concept Definitions -> Activation Binarizer -> Jaccard Scorer -> Beam Search Optimizer -> Perturbation Validator
- Critical path:
  1. Collect 10K state samples from trained agent
  2. For each neuron in layer 2: extract activations → binarize → run beam search → store best concept + score
  3. Filter neurons with >5% activation rate
  4. Validate top neurons via targeted perturbations
- Design tradeoffs:
  - Hand-crafted vs. learned atomic concepts: Manual design ensures interpretability but limits scalability; future work suggests automated discovery
  - Formula length cap (5): Balances expressiveness vs. human interpretability; longer formulas become inscrutable
  - Single-layer focus (layer 2): Captures high-level features but misses hierarchical representations across layers
  - Jaccard vs. correlation: Jaccard is set-based (binary); correlation would capture graded relationships but loses crisp interpretability
- Failure signatures:
  - Low Jaccard scores (<0.4) across all concepts → neuron may encode distributed or non-compositional pattern
  - High Jaccard but perturbations fail to change behavior → concept is correlational, not causal
  - Concept formula exceeds length 5 or uses many negations → likely overfitting to sample noise
  - Fewer than 5% of neurons activatable → sampling coverage insufficient or network too sparse
- First 3 experiments:
  1. Reproduce LunarLander discrete results: Train DQN (64-64-64), sample 10K states, run concept matching on layer 2, verify Jaccard > 0.7 for top neurons (e.g., Neuron 19 landing detector should score ~0.98 as reported)
  2. Ablate beam width: Compare beam widths {1, 5, 10, 20} on same network; measure how Jaccard scores and formula complexity change; expect diminishing returns beyond 10
  3. Test generalization to unseen environment variant: Apply learned concept interpretations from LunarLander-v3 to LunarLander-Continuous-v2; assess whether similar conceptual structure emerges despite different action space and training algorithm (PPO vs. DQN)

## Open Questions the Paper Calls Out

- Can automated concept discovery methods be developed to eliminate the need for manual atomic concept design in DRL interpretability?
  - Basis: The conclusion states "While our current implementation requires manual design of atomic concepts, future work could focus on developing automated methods for concept identification."
  - Why unresolved: Current method relies on domain expertise to pre-define atomic concepts for each environment, limiting scalability and applicability to unfamiliar domains where relevant concepts are unknown.
  - What evidence would resolve it: An algorithm that automatically discovers meaningful atomic concepts from state trajectories without human input, achieving comparable Jaccard similarity scores and passing similar perturbation validation tests.

- Can concept-based neuron interpretations be leveraged for effective network pruning while maintaining policy performance?
  - Basis: The conclusion explicitly identifies "exploring applications in network pruning" as future work direction.
  - Why unresolved: While interpretable neurons can be identified, the relationship between concept semantic importance and policy-critical functionality remains unclear—pruning neurons with "simple" concepts may inadvertently remove essential control mechanisms.
  - What evidence would resolve it: Demonstrating that concept-guided pruning (e.g., removing neurons with low-complexity concepts) achieves comparable or better compression ratios than magnitude-based pruning while maintaining task performance.

- Does the compositional concept framework scale to high-dimensional state spaces and complex control tasks beyond the tested toy environments?
  - Basis: Experiments are limited to simple environments (Blackjack with 3 features, LunarLander with 8 state variables), and beam search over compositional concepts has combinatorial complexity that may become prohibitive.
  - Why unresolved: The search space for compositional concepts grows exponentially with the number of atomic concepts, and coverage of relevant state-concept combinations requires exponentially more samples in higher dimensions.
  - What evidence would resolve it: Successful application to high-dimensional domains (e.g., Atari from pixels, robotic manipulation with proprioceptive and visual inputs) with maintained interpretability quality and reasonable computational cost.

## Limitations
- Method's reliance on manually crafted atomic concepts limits scalability to complex state spaces with many continuous variables
- Assumes monotonic relationships between concept satisfaction, neuron activation, and action selection, which may not hold in networks with distributed representations
- Binary activation threshold (β=0) may miss graded relationships, and beam search with length cap (k≤5) may oversimplify complex decision boundaries

## Confidence
- High confidence: The Jaccard similarity framework for matching binarized activations to compositional concepts is well-defined and mathematically sound (tested across three environments with consistent results)
- Medium confidence: The causal interpretations from perturbation experiments, while showing strong qualitative patterns, may not fully account for network redundancy or distributed representations
- Low confidence: The generalizability to environments with higher-dimensional or continuous action spaces beyond LunarLander-Continuous-v2

## Next Checks
1. Test the method on a more complex environment (e.g., Breakout or CartPole) to assess scalability and identify failure modes in higher-dimensional state spaces
2. Compare Jaccard similarity with alternative metrics (correlation, mutual information) to evaluate robustness to the binarization threshold choice
3. Implement an ablation study removing beam search width and formula length constraints to quantify their impact on interpretability quality