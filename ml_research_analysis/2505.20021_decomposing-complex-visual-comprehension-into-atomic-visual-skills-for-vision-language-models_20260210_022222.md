---
ver: rpa2
title: Decomposing Complex Visual Comprehension into Atomic Visual Skills for Vision
  Language Models
arxiv_id: '2505.20021'
source_url: https://arxiv.org/abs/2505.20021
tags:
- determine
- given
- line
- which
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces the Atomic Visual Skills Dataset (AVSD) to
  evaluate vision-language models on fundamental, indivisible visual perception skills
  in 2D Euclidean geometry. The authors systematically categorized 36 atomic visual
  skills and constructed three sub-datasets: AVSD-h (5,163 handcrafted problems),
  AVSD-s (5,400 procedurally generated problems), and AVSD-c (2,625 style-augmented
  problems).'
---

# Decomposing Complex Visual Comprehension into Atomic Visual Skills for Vision Language Models

## Quick Facts
- **arXiv ID:** 2505.20021
- **Source URL:** https://arxiv.org/abs/2505.20021
- **Reference count:** 40
- **Primary result:** VLMs achieve only 30-34% accuracy on fundamental geometric perception tasks, showing they lack robust atomic visual skills despite strong performance on higher-level tasks.

## Executive Summary
This paper introduces the Atomic Visual Skills Dataset (AVSD) to systematically evaluate vision-language models on fundamental, indivisible visual perception skills in 2D Euclidean geometry. The authors categorized 36 atomic visual skills and constructed three sub-datasets: AVSD-h (5,163 handcrafted problems), AVSD-s (5,400 procedurally generated problems), and AVSD-c (2,625 style-augmented problems). Evaluation reveals that even state-of-the-art VLMs struggle with these basic tasks, achieving only 30-34% accuracy across the full dataset, with the best models scoring below 75%. Domain-specific models trained on composite geometry do not perform better than general VLMs. Fine-tuning on atomic visual skill data improves performance, while training on composite geometry alone does not transfer, indicating that VLMs lack robust atomic visual perception skills despite strong performance on higher-level tasks.

## Method Summary
The authors systematically categorize 36 atomic visual skills in 2D Euclidean geometry and construct three sub-datasets: AVSD-h with 5,163 handcrafted problems, AVSD-s with 5,400 procedurally generated problems from AlphaGeometry, and AVSD-c with 2,625 style-augmented problems using ControlNet. They evaluate state-of-the-art VLMs on these datasets using binary accuracy metrics. For training experiments, they fine-tune LLaVA-Next-13B on AVSD-s-train (360K atomic problems) and compare it with training on MathV360k (composite geometry). Evaluation uses GPT-4o mini for answer extraction and scoring with specific few-shot prompts. The study demonstrates that atomic skill training improves performance while composite geometry training does not transfer.

## Key Results
- VLMs achieve only 30-34% accuracy across the full AVSD dataset, struggling with basic geometric perception tasks
- Best models (GPT-4o, o3, Gemini 2.5) score below 75% on atomic skills despite strong performance on higher-level tasks
- Fine-tuning on atomic visual skill data improves performance (33% → 45%), while training on composite geometry alone shows no improvement (31% → 32%)
- Style augmentation consistently degrades performance by 10-25%, revealing robustness issues in geometric perception

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing visual tasks into atomic skills isolates fundamental perception failures that composite benchmarks miss.
- Mechanism: The 36 atomic visual skills are individually tested with targeted questions, preventing models from using linguistic shortcuts or composite reasoning to mask perception deficits. Each skill has ~366 problems across three sub-datasets.
- Core assumption: Atomic skills are truly indivisible and transfer to composite tasks; perception failures at atomic level propagate upward.
- Evidence anchors: [abstract] "systematically categorize the fundamental, indivisible visual perception skills, which we refer to as atomic visual skills" [Section 4] "each skill is intuitive and trivial for adult humans, (ii) each skill cannot be decomposed further"
- Break condition: If atomic skills are learnable from composite training alone, decomposition becomes unnecessary.

### Mechanism 2
- Claim: Training on atomic visual skill data improves performance, while training on composite geometry does not transfer.
- Mechanism: Fine-tuning LLaVA-Next-13B on AVSD-s-train (360K atomic problems) improved AVSD-h accuracy from 33% to 45%, while training on MathV360k (composite geometry) showed no improvement (31% → 32%).
- Core assumption: Atomic perception skills must be explicitly trained; they do not emerge implicitly from composite problem exposure.
- Evidence anchors: [Section 5.2] "fine-tuned LLaVA-Next-13B on the MathV360k dataset... observed that additional training on composite diagrams alone did not improve the performance" [Section 5.2] "training LLaVA-Next-13B on the atomic perception tasks of the AVSD-s-train dataset leads to clear improvements"
- Break condition: If pre-training data already contains sufficient atomic skill examples, additional fine-tuning may show diminishing returns.

### Mechanism 3
- Claim: Style augmentation via ControlNet reveals VLMs' lack of robustness in geometric perception.
- Mechanism: AVSD-c applies style transformations (whiteboard, chalkboard, paper textures, neon displays) using ControlNet conditioned on Canny edges. Performance drops consistently across all models (e.g., o3: 73.8% AVSD-h → 50.3% AVSD-c).
- Core assumption: Robust visual perception should be invariant to superficial style changes when geometric content is preserved.
- Evidence anchors: [Section 4.3] "A VSD-c consists of 2,625 synthetically generated images with diverse styles imbued with ControlNet" [Table 3] Shows consistent performance degradation from AVSD-s to AVSD-c across all models
- Break condition: If style sensitivity is task-specific rather than a fundamental perception gap, augmentation strategies may need domain-specific calibration.

## Foundational Learning

- Concept: **Euclidean geometry primitives** (points, lines, circles, angles, tangency)
  - Why needed here: The 36 atomic skills are defined entirely within 2D Euclidean geometry. Without understanding what "tangent," "parallel," or "congruent" mean geometrically, you cannot interpret the benchmark or failure modes.
  - Quick check question: Can you distinguish between two circles that intersect vs. two circles that are tangent at exactly one point?

- Concept: **Vision-language model architecture** (vision encoder + projector + LLM)
  - Why needed here: The paper's hypothesis is that vision encoders pre-trained on natural images lack geometric precision. Understanding where perception vs. reasoning occurs in the architecture informs where interventions should target.
  - Quick check question: In a typical VLM pipeline, which component is responsible for extracting visual features before they reach the language model?

- Concept: **Diffusion-based image generation with ControlNet**
  - Why needed here: AVSD-c uses ControlNet for style augmentation while preserving geometric structure via Canny edge conditioning. Understanding this helps replicate or extend the dataset.
  - Quick check question: How does ControlNet differ from standard text-to-image diffusion when you need to preserve spatial structure?

## Architecture Onboarding

- Component map:
  AVSD-h (5,163 handcrafted) -> AVSD-s (5,400 synthetic) -> AVSD-c (2,625 style-augmented) -> AVSD-s-train (360K training)

- Critical path:
  1. Evaluate baseline VLM on AVSD-h to identify weak skills
  2. Fine-tune on AVSD-s-train targeting those skills
  3. Validate on AVSD-c for robustness; iterate if style sensitivity remains high

- Design tradeoffs:
  - Handcrafted vs. synthetic: AVSD-h has higher quality/diversity but limited scale; AVSD-s is scalable but may have template artifacts
  - Skill isolation vs. natural composition: Atomic tasks are artificial but diagnostic; real tasks require skill composition
  - Training efficiency: 360K atomic problems vs. unknown pre-training data requirements

- Failure signatures:
  - Models perform well on OCR, absolute position, shapes (>80% on some) but fail on tangency, parallel, angle (<40%)
  - Chain-of-thought provides no benefit or worsens performance (perception doesn't benefit from reasoning steps)
  - Consistent 10-25% accuracy drop from AVSD-s to AVSD-c indicates style overfitting

- First 3 experiments:
  1. Reproduce baseline evaluation on AVSD-h for your target VLM; identify which of the 36 skills have <50% accuracy.
  2. Fine-tune on AVSD-s-train for 1-2 epochs on 8 GPUs (per paper's setup); measure improvement on AVSD-h to confirm transfer.
  3. Ablate training data: compare atomic-only (AVSD-s-train) vs. composite-only (MathV360k) fine-tuning on the same model to validate the core claim.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does incorporating atomic visual perception data into large-scale pre-training (rather than fine-tuning) yield significant improvements in VLMs' geometric perception and downstream reasoning abilities?
- Basis in paper: [explicit] "We hypothesize that incorporating such atomic perception data into the large-scale pre-training will lead to significant improvements in VLMs' geometric perception and reasoning abilities."
- Why unresolved: The authors only conducted fine-tuning experiments; pre-training experiments would require substantially more computational resources and are outside the scope of this work.
- What evidence would resolve it: Pre-training a VLM from scratch with atomic visual skill data included in the training corpus, then evaluating on AVSD and higher-level geometry benchmarks.

### Open Question 2
- Question: Does the taxonomy of 36 atomic visual skills for 2D Euclidean geometry generalize to 3D spatial reasoning, and what additional or modified skills would be required?
- Basis in paper: [explicit] "One limitation of this work is that we focus solely on 2D geometric perception. Expanding our approach to include 3D spatial reasoning would be an interesting direction for future research."
- Why unresolved: The authors intentionally constrained scope to 2D geometry; systematically defining and testing 3D atomic skills remains unexplored.
- What evidence would resolve it: Extending the decomposition methodology to 3D contexts, creating a 3D atomic skills dataset, and benchmarking VLMs to identify failure patterns.

### Open Question 3
- Question: Do atomic visual skills for mathematical diagrams transfer to diagram and illustration perception in broader scientific and everyday contexts?
- Basis in paper: [explicit] "Such tasks may require a related but distinct set of atomic skills compared to the 36 skills we consider in this work."
- Why unresolved: The current 36 skills were derived specifically for high-school-level geometry; whether this skill set applies to scientific figures, charts, or everyday visual tasks is unknown.
- What evidence would resolve it: Creating analogous atomic skill datasets for scientific figures (e.g., molecular diagrams, circuit schematics) and evaluating whether training on AVSD transfers.

## Limitations

- The atomic decomposition framework relies on manual skill identification that may not capture all relevant geometric perception abilities
- The handcrafted dataset covers only 36 skills and may have selection bias toward specific geometric configurations
- The synthetic dataset generation depends on AlphaGeometry, which may introduce structural biases in problem templates

## Confidence

- **High Confidence:** The systematic evaluation reveals that current VLMs struggle with basic geometric perception tasks, as evidenced by consistent low accuracy (<40%) on fundamental skills like tangency and parallelism across multiple model families.
- **Medium Confidence:** The superiority of atomic skill training over composite geometry training is demonstrated, though the magnitude of improvement and its general applicability to other VLM architectures requires further validation.
- **Medium Confidence:** Style augmentation reveals robustness issues in geometric perception, but the specific mechanisms causing performance degradation need more detailed investigation.

## Next Checks

1. **Architectural Intervention Test:** Modify the vision encoder architecture (e.g., add geometric attention mechanisms or domain-specific pre-training) and evaluate whether baseline AVSD-h performance improves without requiring AVSD-specific fine-tuning data.

2. **Cross-Skill Transfer Analysis:** Systematically measure performance changes when training on individual atomic skills versus skill combinations to determine if certain skills transfer more effectively than others.

3. **Natural Image Generalization:** Test AVSD-trained models on geometric perception tasks within natural images (rather than diagrams) to assess whether atomic skill learning generalizes beyond synthetic/canonical visual formats.