---
ver: rpa2
title: 'SMT-EX: An Explainable Surrogate Modeling Toolbox for Mixed-Variables Design
  Exploration'
arxiv_id: '2503.19496'
source_url: https://arxiv.org/abs/2503.19496
tags:
- variables
- surrogate
- shap
- variable
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents SMT-EX, an extension of the open-source Surrogate
  Modeling Toolbox (SMT) that integrates explainability methods into a state-of-the-art
  surrogate modeling framework. The tool incorporates three key explainability methods:
  Shapley Additive Explanations (SHAP), Partial Dependence Plots (PDP), and Individual
  Conditional Expectations (ICE), along with Sobol'' indices and uncertainty quantification
  through conformal prediction.'
---

# SMT-EX: An Explainable Surrogate Modeling Toolbox for Mixed-Variables Design Exploration

## Quick Facts
- arXiv ID: 2503.19496
- Source URL: https://arxiv.org/abs/2503.19496
- Reference count: 40
- Primary result: SMT-EX integrates SHAP, PDP, ICE, Sobol' indices, and conformal prediction into a GPR framework for mixed-variable design exploration, achieving RMSE of 0.144 on a 10-variable wing weight problem and 10^-4 on a 3-variable cantilever beam problem while providing interpretable feature importance and uncertainty quantification.

## Executive Summary
SMT-EX is an extension of the open-source Surrogate Modeling Toolbox (SMT) that integrates explainability methods into state-of-the-art surrogate modeling. The tool incorporates SHAP, PDP, ICE, Sobol' indices, and uncertainty quantification through conformal prediction, enabling engineers to gain deeper insights into surrogate model behavior beyond simple prediction. Demonstrated on a 10-variable wing weight problem with continuous variables and a 3-variable mixed-categorical cantilever beam bending problem, SMT-EX successfully revealed variable importance, interaction effects, and nonlinear relationships while maintaining strong predictive accuracy.

## Method Summary
SMT-EX combines Gaussian Process Regression (GPR) with multiple explainability techniques through a unified framework. The toolbox supports mixed-categorical variables using specialized kernels (Generalized Dice, Continuous Relaxation, and Extended Hypersphere) that learn correlation structures between categorical levels. For explainability, it implements SHAP for feature attribution, PDP and ICE for trend visualization, and Sobol' indices for global sensitivity analysis. Uncertainty quantification is provided through split conformal prediction, which offers distribution-free, calibrated prediction intervals. The framework is demonstrated through two test cases: a 10-variable wing weight function and a 3-variable cantilever beam problem with mixed continuous and categorical inputs.

## Key Results
- GPR model achieved RMSE of 0.144 for the 10-variable wing weight problem and 10^-4 for the 3-variable cantilever beam problem
- SHAP and PDP/ICE methods revealed similar feature importance rankings while providing complementary insights on interactions
- Conformal prediction intervals were more reasonable than GP native variance, reducing negative tip deflection predictions in the beam problem
- Correlation matrices for categorical variables showed learned relationships between cross-section types, with thickness having greater influence than cross-sectional shape

## Why This Works (Mechanism)

### Mechanism 1
Combining SHAP, PDP, and ICE provides complementary perspectives on feature importance and interaction effects. PDP marginalizes over other features to show average effects, ICE exposes heterogeneity by plotting individual prediction paths, and SHAP computes game-theoretic contribution values considering all feature coalitions. Together, they distinguish between main effects and interactions—PDP aggregation hides interactions while ICE dispersion and SHAP value spread reveal them. This works under the assumption that the surrogate model has achieved sufficient predictive accuracy before explainability methods are applied.

### Mechanism 2
Mixed-categorical GPR kernels learn correlation structures between categorical levels through the correlation matrix R_cat, which encodes learned similarity between levels (e.g., cross-section types). During training, kernel hyperparameters are optimized via maximum likelihood, revealing which categorical values produce similar outputs. This matrix can be directly inspected post-training for engineering insight. The mechanism assumes categorical levels can be meaningfully embedded in a correlation structure using appropriate kernels like homoscedastic hypersphere or continuous relaxation.

### Mechanism 3
Split conformal prediction provides distribution-free, calibrated uncertainty bounds that adapt to local prediction difficulty better than native GP variance estimates. After training, a held-out calibration set computes prediction residuals, and the (1-α) quantile of these residuals defines a prediction interval with guaranteed coverage under exchangeability. Unlike GP variance (which depends on kernel assumptions and training data density), conformal intervals directly reflect empirical error distribution. This works when the calibration data is exchangeable with test data.

## Foundational Learning

- **Gaussian Process Regression fundamentals**: Why needed - GPR is the core surrogate model; understanding mean prediction, covariance functions, and variance estimation is prerequisite to interpreting explainability outputs. Quick check - Can you explain why a GP provides both a prediction and uncertainty estimate at any input point?

- **Variance-based sensitivity analysis (Sobol' indices)**: Why needed - Sobol' indices are implemented as a GSA baseline; distinguishing first-order from total-order indices is essential for interpreting interaction effects. Quick check - What does it mean if the first-order Sobol' index for a variable is low but its total-order index is high?

- **Shapley value theory**: Why needed - SHAP is the primary local explainability method; understanding coalitional game theory foundations helps interpret why SHAP values sum to the prediction. Quick check - Why do SHAP values require a reference/baseline value, and how does this choice affect interpretation?

## Architecture Onboarding

- **Component map**: smt.surrogate_models.KRG (GPR implementation) -> smt.explainability (SHAP, PDP, ICE, Sobol' submodules) -> smt.sampling_methods (LHS for training data) -> Correlation matrices R_cat (categorical relationships) -> Conformal prediction wrapper (uncertainty quantification)

- **Critical path**: 1) Generate training data via LHS covering input space, 2) Fit GPR model (specify kernel), 3) Validate predictive accuracy (RMSE on test set) - do not proceed if accuracy is poor, 4) Initialize explainability module with trained model, 5) Run PDP/ICE for trend visualization, SHAP for feature attribution, inspect R_cat for categorical relationships

- **Design tradeoffs**: Exact SHAP vs. KernelSHAP (exact is accurate but O(2^m); KernelSHAP is faster but approximate for m > 10 variables), GP native uncertainty vs. conformal prediction (GP variance is faster but conformal provides guaranteed coverage), training/calibration split ratio (paper uses 80/20; more calibration data improves conformal intervals but reduces model accuracy)

- **Failure signatures**: SHAP values appear random/uniform (surrogate model likely underfit; increase training data or adjust kernel), PDP shows flat line but SHAP shows large dispersion (strong interactions present; ICE curves will show crossing patterns), conformal intervals much wider than GP variance (calibration set contains outliers or model extrapolation regions), R_cat matrix shows near-uniform correlations (insufficient data to distinguish categorical effects)

- **First 3 experiments**: 1) Reproduce the 10-variable wing weight problem with 300 samples, comparing SHAP feature importance against Sobol' indices to validate that Nz, A, and tc are top-ranked, 2) On the cantilever beam problem, train three GPR models with different categorical kernels (GD, CR, EHH) and compare R_cat matrices to observe how kernel choice affects learned categorical correlations, 3) For a new mixed-variable problem, run both GP native confidence intervals and split conformal prediction; compare interval widths and coverage on held-out test data to assess which method better captures uncertainty

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the initial sampling strategy and data distribution specifically influence the stability and reliability of explainability results (SHAP, PDP) in surrogate modeling? The authors state that "For explainability, the initial distribution of points can significantly impact the results" and suggest that "choosing the data sample may be a whole field of research to investigate." This remains unresolved as the current work relies on standard Latin Hypercube Sampling without analyzing sensitivity to sampling density or distribution.

- **Open Question 2**: How can engineers systematically resolve discrepancies when different explainability methods (e.g., SHAP vs. PDP) provide conflicting feature importance rankings? In the cantilever beam problem, the authors note that "both metrics do not fully agree on the importance level of input variables," with SHAP ranking length and cross-section equally while PDP ranks length highest. The paper presents divergent results as observations but does not offer a theoretical framework for determining which metric better reflects ground truth in mixed-variable contexts.

- **Open Question 3**: How does the computational efficiency and fidelity of SMT-EX's explainability module scale to high-dimensional problems (>20 variables) compared to the low-dimensional test cases presented? While KernelSHAP is mentioned for acceleration, the computational overhead and visualization clarity for truly high-dimensional design spaces remain unquantified in the results, despite the paper acknowledging that gaining understanding is "not a trivial task... for problems with high-dimensional input."

## Limitations

- Method generalization remains unproven across diverse engineering domains with mixed-variable spaces; effectiveness may vary with complexity of variable interactions and proportion of categorical variables
- Explainability method reliability is sensitive to sample size and input space coverage; for high-dimensional problems, KernelSHAP approximations may introduce significant errors and PDP may suffer from the curse of dimensionality
- Conformal prediction assumptions may not hold with limited data; the 80/20 split may not be optimal for all problems, potentially leading to unreliable prediction intervals or reduced model accuracy

## Confidence

- **High confidence**: GPR predictive accuracy claims (RMSE values) - directly verifiable from test set performance
- **Medium confidence**: Complementary nature of explainability methods - supported by qualitative comparisons but not rigorously quantified
- **Low confidence**: Mixed-categorical kernel performance across diverse real-world problems - based on single test case demonstration

## Next Checks

1. **Generalization test**: Apply SMT-EX to a new mixed-variable engineering problem with at least 5 categorical variables and evaluate whether the same explainability insights (feature importance rankings, interaction detection) hold as in the demonstration cases

2. **Robustness analysis**: Systematically vary the train/calibration split ratio (60/40, 70/30, 90/10) and measure the impact on conformal prediction coverage and interval width to identify the optimal trade-off for different data regimes

3. **Comparison benchmark**: Compare SMT-EX explainability outputs (SHAP, PDP, ICE) against a non-GPR surrogate model (e.g., Random Forest) on the same problems to assess whether the insights are model-specific or represent true underlying relationships