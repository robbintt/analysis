---
ver: rpa2
title: 'SOLD: SELFIES-based Objective-driven Latent Diffusion'
arxiv_id: '2509.25198'
source_url: https://arxiv.org/abs/2509.25198
tags:
- molecules
- diffusion
- latent
- molecule
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents SOLD (SELFIES-based Objective-driven Latent
  Diffusion), a novel latent diffusion model for de novo drug design. The model generates
  molecules in a latent space derived from 1D SELFIES strings and conditioned on target
  proteins, offering a simpler and more efficient alternative to existing 3D-based
  approaches.
---

# SOLD: SELFIES-based Objective-driven Latent Diffusion

## Quick Facts
- arXiv ID: 2509.25198
- Source URL: https://arxiv.org/abs/2509.25198
- Authors: Elbert Ho
- Reference count: 40
- Primary result: 1D SELFIES latent diffusion model generating protein-conditioned molecules with competitive binding affinity and faster generation times than 3D approaches

## Executive Summary
SOLD introduces a novel latent diffusion framework for de novo drug design that operates in a compressed 1D SELFIES latent space rather than explicit 3D coordinates. The model uses a transformer-based encoder trained with multi-task learning to map molecules to latent vectors, which are then processed by a 1D U-Net diffusion model conditioned on protein embeddings from ESM-2. This approach offers significant computational efficiency while maintaining competitive binding affinity performance compared to state-of-the-art 3D-based methods.

## Method Summary
The method trains a transformer encoder-decoder on ChEMBL to map SELFIES strings to 128-dimensional latent vectors, using multi-task learning to predict five chemical descriptors simultaneously. The 1D U-Net diffusion model (360M parameters) is then trained on PDBbind protein-ligand pairs, conditioning generation on ESM-2 protein embeddings. At inference, molecules are generated by sampling noise through the diffusion process with classifier-free guidance (w=5), decoding to SELFIES strings, and converting to 3D coordinates for docking evaluation.

## Key Results
- Generated molecules for COVID-3CLpro protease demonstrated high binding affinity and drug-likeness
- Top-10% Vina scores comparable to or better than state-of-the-art 3D models
- Significantly faster generation times (seconds vs hours) compared to 3D-based approaches
- Higher molecular diversity than competing methods while maintaining binding affinity

## Why This Works (Mechanism)

### Mechanism 1: Latent Compression of 1D Molecular Grammars
Mapping molecules to compressed 1D latent space via Transformers reduces computational complexity compared to 3D coordinate-based methods. The model encodes SELFIES strings into continuous latent vectors, allowing the denoising U-Net to operate on compact representations rather than high-dimensional 3D atomic space.

### Mechanism 2: Protein-Conditioned Guidance via Language Models
Pre-trained protein language models (ESM-2) serve as effective conditioning agents to steer molecular generation toward specific biological targets. The ESM-2 embeddings of target amino acid sequences guide the diffusion process, pulling the latent distribution toward the target protein's chemical neighborhood.

### Mechanism 3: Multi-Task Latent Space Regularization
Training the molecular encoder on multiple auxiliary chemical property tasks stabilizes the latent space for diffusion. The encoder simultaneously reconstructs SELFIES and predicts specific chemical descriptors, forcing the latent space to organize semantically and making diffusion more effective.

## Foundational Learning

**Concept: SELFIES (Self-Referencing Embedded Strings)**
- Why needed: Unlike SMILES, SELFIES guarantees valid strings during generative sampling, reducing the diffusion model's burden to learn grammar
- Quick check: Why does a deterministic grammar help a diffusion model compared to a probabilistic one like SMILES?

**Concept: Latent Diffusion vs. Pixel/Atomic Diffusion**
- Why needed: This architecture separates understanding (Encoder) from generation (Diffusion), explaining why SOLD is faster than 3D models that diffuse directly over atomic coordinates
- Quick check: What is the trade-off of performing diffusion in a learned latent space versus the original data space (e.g., 3D coordinates)?

**Concept: Classifier-Free Guidance**
- Why needed: This control mechanism balances generating diverse molecules (unconditioned) versus generating specific binders (conditioned on protein)
- Quick check: How does the parameter w (guidance weight) affect the trade-off between diversity and binding affinity?

## Architecture Onboarding

**Component map:** ESM-2 (Protein Embedder) -> Custom Transformer (SELFIES Encoder) -> 1D U-Net (Diffusion Model) -> Custom Transformer Decoder -> RDKit (SELFIES -> 3D/SMILES)

**Critical path:**
1. Encoder Training: Train Transformer Encoder/Decoder on ChEMBL to minimize reconstruction loss + multi-task property loss
2. Diffusion Training: Freeze Encoder, train 1D U-Net on PDBbind to denoise latent vectors conditioned on ESM-2 embeddings
3. Inference: Sample noise -> Denoise with Guidance -> Decode Latent -> Convert to Molecule

**Design tradeoffs:**
- 1D vs. 3D: Trades spatial accuracy for speed and exploration capability
- Transformer vs. VAE: Uses deterministic transformer for reconstruction fidelity over variational latent space smoothness

**Failure signatures:**
- Property Drift: High binding affinity but lower QED/SA scores than SOTA
- Optimization Collapse: Evolutionary algorithm for property optimization failed, destroying binding affinity

**First 3 experiments:**
1. Guidance Sweep: Vary guidance weight (w) and plot Average Vina Score vs. Diversity to find optimal balance
2. Reconstruction Validity Check: Pass held-out molecules through Encoder-Decoder to verify 0% reconstruction error
3. Target Ablation: Generate molecules for 3CLpro using specific vs. null ESM-2 embeddings to verify conditioning effectiveness

## Open Questions the Paper Calls Out
- Can the latent space representation be modified to support effective property optimization without degrading binding affinity?
- Does scaling the training dataset beyond 15,000 protein-ligand pairs resolve the suboptimal average binding affinity?
- Can the 1D generative framework be extended to accurately predict or minimize off-target interactions and toxicity?

## Limitations
- Architecture specification gaps, particularly for the 1D U-Net design details
- Training hyperparameter ambiguities in the multi-task balancing algorithm
- Property optimization failure where evolutionary methods degraded binding affinity while improving drug-likeness

## Confidence
- SOLD generates molecules significantly faster than 3D-based models (High)
- Top-10% Vina scores are comparable to or better than state-of-the-art 3D models (Medium)
- Generated molecules for COVID-3CLpro show high binding affinity and drug-likeness (Medium)
- 1D latent space approach is simpler and more efficient than 3D approaches (High)

## Next Checks
1. Systematically vary the guidance weight parameter w from 1 to 15 and measure the trade-off curve between average Vina score and diversity
2. Test SOLD on at least 5-10 diverse protein targets beyond the COVID-3CLpro example to assess generalizability
3. For one target with available crystal structure, generate molecules and perform molecular docking with a more accurate scoring function to validate binding site accuracy