---
ver: rpa2
title: Advancing Speech Understanding in Speech-Aware Language Models with GRPO
arxiv_id: '2509.16990'
source_url: https://arxiv.org/abs/2509.16990
tags:
- grpo
- speech
- arxiv
- bleu
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using Group Relative Policy Optimization (GRPO)
  with BLEU reward to improve Speech-Aware Large Language Models (SALLMs) on open-format
  speech understanding tasks. The authors address the challenge of improving SALLMs
  on tasks like Spoken Question Answering (SQA) and Automatic Speech Translation (AST),
  where open-ended responses are required rather than multiple-choice answers.
---

# Advancing Speech Understanding in Speech-Aware Language Models with GRPO

## Quick Facts
- **arXiv ID**: 2509.16990
- **Source URL**: https://arxiv.org/abs/2509.16990
- **Reference count**: 0
- **Primary result**: GRPO with BLEU reward improves SALLM performance on open-format speech understanding tasks, achieving up to 151% BLEU improvement over baseline on LibriSQA.

## Executive Summary
This paper introduces Group Relative Policy Optimization (GRPO) with BLEU reward to train Speech-Aware Large Language Models (SALLMs) on open-format speech understanding tasks. The authors address the challenge of improving SALLMs on tasks like Spoken Question Answering (SQA) and Automatic Speech Translation (AST), where open-ended responses are required rather than multiple-choice answers. They propose training SALLMs using GRPO with BLEU as the reward signal, demonstrating empirically that this approach surpasses standard supervised fine-tuning across several key metrics including BLEU, BERTScore, ROUGE, and METEOR.

## Method Summary
The authors train Granite Speech 2B/8B models on LibriSQA (SQA) and CoVoST2 (AST) using GRPO with BLEU as the reward signal. The method samples G responses per prompt, computes BLEU scores, estimates advantages via z-score normalization within groups, and applies DAPO loss with KL regularization. They explore mixed-policy GRPO incorporating ground truth as off-policy samples. Training uses AdamW optimizer with temperature=1.0 during training, evaluates with temperature=0.9, top-p=0.9, and requires 4×H100 GPUs for up to 24 hours.

## Key Results
- GRPO improved BLEU scores by 61.8% over baseline and 9.8% over SFT for the 2B model on LibriSQA
- GRPO achieved 151% BLEU improvement over baseline and 6% over SFT for the 8B model on LibriSQA
- On CoVoST2 English to German translation, GRPO achieved 8.2% BLEU improvement over baseline and 3.2% over SFT for the 2B model
- Mixed-policy GRPO improved AST performance but degraded SQA performance

## Why This Works (Mechanism)

### Mechanism 1: Group-Relative Advantage Estimation
GRPO improves sample efficiency by normalizing rewards within groups of generated outputs, enabling relative quality assessment without a learned value function. For each prompt, the policy samples G responses. Rewards are computed for each, then advantages are estimated via z-score normalization: Âi = (ri − mean(R)) / std(R). This centers learning on relative improvements rather than absolute reward values.

### Mechanism 2: Differentiable Text Similarity as Reward
Using BLEU as a continuous reward signal provides richer gradient signal for open-ended generation than binary correctness rewards. BLEU computes n-gram overlap between generated and reference text, yielding scores in [0,1]. This allows partial credit—unlike binary rewards used in prior SQA work—enabling learning on tasks with multiple valid outputs.

### Mechanism 3: Off-Policy Anchoring with Ground Truth
Incorporating ground truth as an off-policy sample (MP-GRPO) can accelerate early training by providing high-quality anchors, but effectiveness depends on task and baseline capability. The ground truth reference is added to the sample group with importance sampling weight πφ = 1 and clipping disabled.

## Foundational Learning

- **Concept: Policy Gradient Methods**
  - Why needed: GRPO is fundamentally a policy gradient algorithm. Understanding how log-probability gradients flow through sampled outputs is essential for debugging reward hacking or gradient vanishing.
  - Quick check: Can you explain why GRPO doesn't require a separate value network unlike PPO?

- **Concept: Importance Sampling in RL**
  - Why needed: The DAPO loss uses importance sampling weights si,t(θ) to correct for distribution shift between old and current policy. Mixed-policy GRPO extends this with off-policy samples.
  - Quick check: What happens to the importance sampling ratio when the policy changes too rapidly?

- **Concept: Text Generation Metrics (BLEU/ROUGE/BERTScore)**
  - Why needed: These metrics serve as reward functions. Understanding their biases (e.g., BLEU favors n-gram precision, penalizes paraphrasing) is critical for reward design.
  - Quick check: Why might BLEU be suboptimal for evaluating a correct but creative answer to an open-ended question?

## Architecture Onboarding

- **Component map**: Audio Input → Speech Encoder (CTC-based) → Projector (Window Q-Former) → LLM Backbone (Granite 3.3) → Policy πθ → Sample G outputs → Compute Rewards (BLEU, etc.) → Group Advantage Estimation → DAPO Loss with KL Penalty

- **Critical path**: The reward computation (BLEU vs. reference) and group normalization. If rewards are sparse or uninformative, the entire advantage estimation fails. The KL penalty β prevents excessive drift—paper notes β = 0 causes divergence.

- **Design tradeoffs**:
  - Group size G: Larger G (e.g., 8) provides better advantage estimates but increases compute. Paper uses G = 8.
  - Reward function: BLEU yields best average performance but may not align with human judgment for all tasks. Neural rewards (BERTScore) are more expensive.
  - Off-policy samples: MP-GRPO helps AST but hurts SQA—suggests task-dependent utility based on base model prior capability.

- **Failure signatures**:
  - Training divergence with β = 0 (KL penalty too low)
  - Reward exploitation: model optimizes metric without semantic improvement
  - Performance degradation with MP-GRPO on tasks where base model has weak prior (observed on SQA)

- **First 3 experiments**:
  1. Baseline replication: Train Granite Speech 2B on LibriSQA with GRPO (G=8, BLEU reward, β=0.02). Validate that BLEU improves ~17 points over SFT baseline.
  2. Ablate reward function: Compare BLEU vs. ROUGE-L vs. BERTScore as reward on same task. Expect BLEU to yield highest average metric.
  3. Test MP-GRPO on both tasks: Add ground truth as off-policy sample for both AST and SQA. Confirm AST improves while SQA degrades.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does Mixed-Policy GRPO (MP-GRPO) degrade performance on Spoken Question Answering (SQA) while improving Automatic Speech Translation (AST), and how can this instability be resolved?
- Basis: Authors note in Section 5.3 that MP-GRPO improves AST but degrades SQA, stating that "further investigation is required" to understand this divergence.
- Why unresolved: The paper hypothesizes that the base model's prior exposure to AST (but not SQA) made off-policy samples act as beneficial anchors for the former but destabilizing for the latter, but this mechanism is not verified.

### Open Question 2
- Question: Can GRPO-based training be effectively adapted for Automatic Speech Recognition (ASR) tasks where outputs are constrained to a single valid transcription?
- Basis: Section 4.1 reports that preliminary experiments with ASR yielded "only minor improvements" and explicitly states that "further work needs to be done on the abilities of RL/GRPO in tasks such as ASR."
- Why unresolved: The GRPO advantage estimation relies on group sampling, which is naturally suited for open-ended generative tasks but struggles with the "unique valid response" characteristic of ASR.

### Open Question 3
- Question: Do neural-based rewards or composite reward functions provide superior performance and stability compared to the simple BLEU metric used in this study?
- Basis: Section 3.3 notes that "Neural-based rewards, such as BERT-score, or possibly a combination of rewards may also be considered," but the authors "leave other possibilities for future work" due to computational constraints.
- Why unresolved: The paper relies solely on BLEU (and similar n-gram metrics) for optimization, leaving the potential benefits of semantic or neural-based reward signals unexplored.

## Limitations
- Effectiveness depends heavily on reference quality and diversity in training datasets
- Method's performance sensitivity to hyperparameters and task characteristics isn't fully characterized
- Results are demonstrated on specific domains (book excerpts, news translation) with unknown generalization to more diverse speech domains

## Confidence

**High confidence** in the core mechanism of GRPO with group-relative advantage estimation. The mathematical formulation is sound, the training pipeline is well-specified, and the empirical improvements over SFT are substantial and consistent across multiple metrics.

**Medium confidence** in the mixed-policy approach (MP-GRPO). While the method is correctly implemented, the task-dependent results suggest effectiveness is not universal. The authors provide a reasonable hypothesis about base model priors, but don't fully validate this explanation.

**Medium confidence** in BLEU as the optimal reward function. The paper shows BLEU yields highest average performance across metrics, but this may reflect metric gaming rather than true semantic improvement.

## Next Checks

1. **Cross-dataset generalization**: Evaluate GRPO-trained models on a held-out dataset from a different domain (e.g., MuST-C for speech translation, or Common Voice for diverse speech understanding). This tests whether improvements transfer beyond the training distribution and helps validate that metric gains reflect genuine understanding rather than overfitting.

2. **Ablation of reference quality**: Systematically vary reference quality in the reward computation (e.g., use paraphrased references, synthetic references, or no references for some groups) to test whether GRPO's effectiveness depends on reference quality. This would help determine if the method is robust to reference variability or if it's exploiting specific reference characteristics.

3. **Human evaluation of open-ended responses**: Conduct human evaluation on SQA outputs from GRPO vs. SFT models, focusing on semantic correctness, relevance, and fluency rather than n-gram overlap. This would validate whether BLEU-based improvements correlate with human judgment and help identify cases where the model optimizes metrics without improving actual understanding.