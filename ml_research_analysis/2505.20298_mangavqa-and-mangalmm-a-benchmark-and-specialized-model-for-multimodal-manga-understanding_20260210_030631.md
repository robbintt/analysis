---
ver: rpa2
title: 'MangaVQA and MangaLMM: A Benchmark and Specialized Model for Multimodal Manga
  Understanding'
arxiv_id: '2505.20298'
source_url: https://arxiv.org/abs/2505.20298
tags:
- text
- manga
- mangavqa
- mangaocr
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MangaVQA and MangaOCR, two benchmarks designed
  to evaluate multimodal understanding of Japanese comics (manga). MangaVQA consists
  of 526 manually constructed question-answer pairs that test a model's ability to
  comprehend narrative context from both visual and textual information in manga spreads.
---

# MangaVQA and MangaLMM: A Benchmark and Specialized Model for Multimodal Manga Understanding

## Quick Facts
- **arXiv ID**: 2505.20298
- **Source URL**: https://arxiv.org/abs/2505.20298
- **Reference count**: 40
- **Primary result**: Introduces MangaVQA and MangaOCR benchmarks; develops MangaLMM model achieving 71.5% OCR Hmean and competitive VQA performance on manga understanding tasks.

## Executive Summary
This paper addresses the challenge of multimodal understanding in Japanese manga, which combines complex visual layouts with text in stylized forms. The authors introduce two new benchmarks—MangaVQA for visual question answering and MangaOCR for text detection/recognition—to evaluate models' ability to comprehend narrative context from manga spreads. They develop MangaLMM, a specialized model fine-tuned from Qwen2.5-VL that jointly handles OCR and VQA tasks. Extensive experiments show that while state-of-the-art proprietary models struggle with these tasks, MangaLMM achieves over 70% OCR performance and competitive VQA results, demonstrating effective multimodal manga understanding through joint task training.

## Method Summary
The authors develop MangaLMM by finetuning Qwen2.5-VL-7B-Instruct on a combination of MangaOCR text detection/recognition data (~170K instances) and synthetic VQA data (39,837 QA pairs generated by GPT-4o with OCR annotations). The training uses a single epoch with batch size 32 on 4x A100 GPUs. OCR task outputs include bounding box coordinates and text content, while VQA outputs are free-form answers. The model is evaluated on MangaOCR using Hmean (IoU>0.5 matching) and NED metrics, and on MangaVQA using LLM-as-a-judge scoring via Gemini 2.5 Flash.

## Key Results
- Proprietary models (GPT-4o, Gemini 2.5 Flash, Claude Sonnet 4.5) score 0.0 on MangaOCR text detection/recognition task
- MangaLMM achieves 71.5% Hmean on MangaOCR and competitive VQA performance
- Joint OCR+VQA training yields slight VQA improvement (6.57 → 6.68 LLM score) over VQA-only training
- OCR annotations significantly improve synthetic VQA data quality (6.68 vs 6.00 without annotations)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: OCR annotations provided to GPT-4o during synthetic VQA generation improve downstream VQA performance.
- Mechanism: Text annotations enable GPT-4o to generate factually grounded questions tied to actual in-image text. This produces training data that teaches the model to locate and reason over specific textual content rather than hallucinating answers.
- Core assumption: The quality and factual accuracy of synthetic QA pairs depend on grounding in verified text content.
- Evidence anchors:
  - [abstract]: "MangaVQA consists of 526 high-quality, manually constructed question-answer pairs."
  - [section] Table 5: Without OCR annotations, model scores 5.64; with OCR annotations, it scores 6.68, surpassing GPT-4o's own performance (6.00).
  - [corpus]: Weak corpus support; related work on comics humor and narrative reasoning does not directly validate the OCR-annotation mechanism.
- Break condition: If OCR annotations contain systematic errors (e.g., misrecognized characters), synthetic QA quality degrades and may introduce noisy supervision.

### Mechanism 2
- Claim: Joint finetuning on OCR and VQA tasks yields slight VQA improvement over VQA-only training.
- Mechanism: Enhanced OCR capability provides more accurate textual cues during VQA inference. Although task interference typically reduces performance in multi-task learning, here the OCR-to-VQA knowledge transfer appears to outweigh interference effects.
- Core assumption: Text detection and recognition errors are a bottleneck for VQA in text-heavy domains like manga.
- Evidence anchors:
  - [section] Table 3: TVQA-only training yields 6.57 LLM score; TOCR+TVQA joint training yields 6.68.
  - [section] §6.1: "Although the models fail to explicitly output the correct OCR results, they appear to capture some textual semantics from the image."
  - [corpus] StripCipher (arXiv:2502.13925) examines sequential image understanding but does not isolate OCR-to-VQA transfer.
- Break condition: If OCR training introduces output format conflicts (e.g., bounding box generation interferes with free-form answer generation), joint training may harm both tasks.

### Mechanism 3
- Claim: Domain-specific finetuning dramatically improves OCR performance on stylized visual content where general LMMs fail.
- Mechanism: General LMMs lack exposure to manga-specific visual patterns (stylized onomatopoeia, vertical text, panel layouts). Finetuning on manga data adapts the visual encoder and language model to this distribution.
- Core assumption: The visual and textual patterns in manga are sufficiently different from natural images and standard documents to cause significant distribution shift.
- Evidence anchors:
  - [section] Table 2: GPT-4o, Gemini 2.5 Flash, Claude Sonnet 4.5 all score 0.0 on MangaOCR; MangaLMM scores 71.5% Hmean.
  - [section] §6.1: "Most of their predictions consist of meaningless repetitions or short repeated tokens."
  - [corpus] Re:Verse (arXiv:2508.08508) confirms general VLMs struggle with manga narrative understanding.
- Break condition: If the domain is similar to training data (e.g., standard printed Japanese text), gains from specialization diminish.

## Foundational Learning

- Concept: Optical Character Recognition (OCR) as structured grounding
  - Why needed here: Manga understanding requires detecting text bounding boxes and recognizing characters; VQA depends on accurate text extraction.
  - Quick check question: Can you explain why positional information matters for OCR evaluation in manga?

- Concept: Multi-task learning and task interference
  - Why needed here: MangaLMM is jointly trained on OCR and VQA; understanding when tasks help vs. hurt each other is critical.
  - Quick check question: What symptoms would indicate task interference is occurring during joint training?

- Concept: LLM-as-a-judge evaluation
  - Why needed here: MangaVQA uses Gemini 2.5 Flash to score model responses on a 1-10 scale.
  - Quick check question: Why might using GPT-4o as the judge introduce circular bias in this study?

## Architecture Onboarding

- Component map:
  - Qwen2.5-VL-7B-Instruct (base model) -> TOCR (MangaOCR data) + TVQA (synthetic data) -> Finetuned model -> OCR/VQA outputs

- Critical path:
  1. Prepare MangaOCR annotations (dialogue + onomatopoeia with bounding boxes)
  2. Generate synthetic VQA using GPT-4o with OCR text annotations
  3. Continual finetune Qwen2.5-VL-7B on combined TOCR + TVQA (1 epoch, batch size 32)
  4. Evaluate with task-specific prompts

- Design tradeoffs:
  - Domain specialization vs. general capability: MangaLMM drops to 25.8% MMMU accuracy vs. 58.6% for base Qwen2.5-VL; recoverable via joint training with natural image data (Table J).
  - OCR-only vs. joint training: OCR-only yields 74.5% Hmean; joint training slightly reduces to 71.5% but improves VQA.
  - Synthetic vs. human QA: Human validation shows ~80% synthetic QA quality; manual benchmark (526 pairs) provides ground truth.

- Failure signatures:
  - Near-zero OCR scores with repetitive token outputs → model not adapted to manga distribution
  - VQA answers that ignore in-image text → insufficient text grounding in training
  - Bounding boxes in nonsensical locations (as in GPT-4o outputs) → visual-textual alignment failure

- First 3 experiments:
  1. Replicate Table 3: Train separate models on TOCR-only, TVQA-only, and TOCR+TVQA to isolate task interference and transfer effects.
  2. Ablate OCR annotation content: Compare VQA data generated with text-only vs. text+position annotations (Table G).
  3. Test cross-domain generalization: Evaluate MangaLMM on eBDtheque (non-Japanese comics) to measure domain boundary (Table K).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LMMs be optimized to achieve OCR inference speeds comparable to dedicated OCR models (e.g., DeepSolo at 10+ FPS) while maintaining accuracy on manga text detection and recognition?
- Basis in paper: [explicit] The Limitation section states: "LMMs are inherently slower than dedicated OCR models; for instance, processing 1,166 test images containing 25,651 text instances takes several hours on an A100 GPU. In contrast, a dedicated OCR model such as DeepSolo... can complete the same task in about two minutes."
- Why unresolved: The architectural characteristics that make LMMs slow for OCR (large output tokens, repetition/looping during inference) are fundamental to current LMM designs.
- What evidence would resolve it: Development of LMM architectures or inference techniques that process manga pages at comparable speeds to dedicated OCR systems without sacrificing detection/recognition accuracy.

### Open Question 2
- Question: How can positional OCR information be effectively leveraged to improve synthetic VQA data generation quality for manga understanding?
- Basis in paper: [explicit] Appendix E.2 states: "Although our current approach did not benefit from positional information, leveraging it remains a promising direction for future work."
- Why unresolved: The authors found that using only text content (without bounding boxes) produced better VQA training data (6.68 vs 6.46 LLM score) than including both text and positional information—counter to expectations.
- What evidence would resolve it: Identification of prompt engineering or data formatting strategies that successfully exploit spatial information to generate higher-quality VQA pairs.

### Open Question 3
- Question: How do proprietary LMMs (e.g., GPT-4o) extract sufficient semantic information from manga text to answer VQA questions despite producing near-zero OCR scores?
- Basis in paper: [inferred] Section 6.1 notes this is "counterintuitive"—models with 0.0 Hmean on MangaOCR still answer VQA questions requiring text understanding, suggesting they "capture some textual semantics from the image" without explicit OCR output capability.
- Why unresolved: The mechanism by which LMMs internally represent text semantics without producing structured OCR outputs remains unexplained in current literature.
- What evidence would resolve it: Probing studies or attention analysis revealing how text regions are processed for semantic extraction vs. explicit transcription within LMM architectures.

### Open Question 4
- Question: What training modifications would enable MangaLMM to generalize effectively across diverse comic styles (European, American, Japanese) while maintaining manga-specific performance?
- Basis in paper: [explicit] Appendix E.4 states: "While MangaLMM is intentionally specialized for manga, extending it toward a general comic-capable LMM is a promising direction for future work," noting that eBDtheque evaluation reveals domain gaps from page format and color differences.
- Why unresolved: Current training on black-and-white, two-page manga spreads doesn't transfer well to single-page color comics from other traditions.
- What evidence would resolve it: Systematic experiments measuring OCR/VQA performance on diverse comic datasets after incorporating multi-format, multi-color training data.

## Limitations

- Strong domain specificity limits generalization to non-Japanese comics (eBDtheque performance drops significantly)
- LLM-as-a-judge evaluation introduces potential circular bias and subjectivity
- Key training hyperparameters not fully specified, limiting exact reproduction
- Marginal joint training improvements may be within noise bounds

## Confidence

**High Confidence**:
- Domain-specific finetuning dramatically improves OCR on manga-specific visual patterns
- OCR annotations significantly improve synthetic VQA data quality
- General LMMs fail on manga OCR and VQA tasks

**Medium Confidence**:
- Joint OCR+VQA training provides slight VQA improvement
- The proposed benchmarks are necessary for evaluating manga understanding
- The model captures "some textual semantics" from images during joint training

**Low Confidence**:
- The 80% synthetic QA quality claim without detailed validation methodology
- The assumption that task interference is outweighed by transfer effects
- The scalability of this approach to larger manga corpora

## Next Checks

1. **Ablation Study on OCR Annotations**: Systematically compare VQA data generated with text-only vs. text+position annotations to isolate the contribution of spatial grounding to downstream performance.

2. **Cross-Domain Generalization Test**: Evaluate MangaLMM on non-Japanese comics (eBDtheque) and other text-heavy visual domains to quantify domain boundary and transfer capability.

3. **Task Interference Quantification**: Train separate models on TOCR-only, TVQA-only, and TOCR+TVQA with multiple random seeds to establish statistical significance of the claimed joint training benefits and identify interference effects.