---
ver: rpa2
title: 'MM-Skin: Enhancing Dermatology Vision-Language Model with an Image-Text Dataset
  Derived from Textbooks'
arxiv_id: '2505.06152'
source_url: https://arxiv.org/abs/2505.06152
tags:
- mm-skin
- image
- medical
- dataset
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the lack of specialized dermatology vision-language
  models by introducing MM-Skin, a large-scale multimodal dermatology dataset with
  nearly 10,000 high-quality image-text pairs across clinical, dermoscopic, and pathological
  modalities. Using this dataset, the authors fine-tuned SkinVL, a domain-specific
  model that achieved state-of-the-art performance on visual question answering tasks,
  surpassing both general and medical LVLMs with BLEU-4 scores over 70 on pathology
  and clinical images.
---

# MM-Skin: Enhancing Dermatology Vision-Language Model with an Image-Text Dataset Derived from Textbooks

## Quick Facts
- arXiv ID: 2505.06152
- Source URL: https://arxiv.org/abs/2505.06152
- Reference count: 40
- The paper introduces MM-Skin, a large-scale multimodal dermatology dataset, and demonstrates that fine-tuning SkinVL on this data achieves state-of-the-art performance on dermatology visual question answering tasks.

## Executive Summary
This paper addresses the critical gap in specialized dermatology vision-language models by introducing MM-Skin, a dataset derived from medical textbooks containing nearly 10,000 high-quality image-text pairs across clinical, dermoscopic, and pathological modalities. The authors fine-tune SkinVL, a domain-specific model that achieves state-of-the-art performance on visual question answering tasks, surpassing both general and medical LVLMs with BLEU-4 scores over 70 on pathology and clinical images. The model also demonstrates strong zero-shot classification performance, reaching 82.34% accuracy on pathology datasets, validating the effectiveness of domain-specific training data in improving dermatology AI reasoning and generalization.

## Method Summary
The authors created MM-Skin by extracting image-text pairs from 15 dermatology textbooks, generating 27,000 instruction-following QA pairs using Llama-3.1, and combining this with 9 public dermatology datasets. They fine-tuned LLaVA-Med-v1.5-7B using LoRA (rank=128, alpha=64) with frozen vision encoder and LLM, training visual projection layers and LoRA adapters on 8× NVIDIA 3090 GPUs. Three model variants were trained: SkinVL-MM (MM-Skin only), SkinVL-Pub (public data only), and SkinVL-PubMM (combined). Evaluation included VQA metrics (BLEU-4, METEOR, ROUGE-L, BERTScore, Recall), supervised fine-tuned classification (logistic regression on frozen features), and zero-shot classification (prompt-based querying).

## Key Results
- SkinVL-MM achieved state-of-the-art VQA performance with BLEU-4 scores over 70 on pathology and clinical images, surpassing LLaVA-Med-7B (7.70) and Qwen2.5-VL-7B (1.97)
- Strong zero-shot classification performance with accuracy up to 82.34% on pathology datasets (Patch16)
- Multi-modality training enhanced generalization across clinical (63%), dermoscopic (10%), and pathological (27%) imaging types
- Instruction-tuned models demonstrated superior conversational QA capabilities compared to descriptive-only approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific fine-tuning with high-quality dermatology image-text pairs substantially improves VLM performance over general and general-medical models.
- Mechanism: Textbook-derived captions provide semantically rich, professionally accurate descriptions that align visual features with precise medical terminology. This alignment enables the projection layer to learn domain-specific visual-language mappings that general models lack.
- Core assumption: Textbook descriptions accurately reflect clinically relevant visual features, and the quality of text supervision transfers to improved reasoning.
- Evidence anchors:
  - SkinVL achieved state-of-the-art performance on visual question answering tasks with BLEU-4 scores over 70 on pathology and clinical images
  - Table 3: SkinVL-MM achieves Bleu-4 of 22.04 (pathology) vs LLaVA-Med-7B's 7.70 and Qwen2.5-VL-7B's 1.97
  - Limited direct corpus evidence; DermaBench paper notes evaluation gaps in dermatology VLMs, indirectly supporting need for domain-specific approaches
- Break condition: If textbook captions contain systematic biases, outdated terminology, or poor visual-text alignment, the learned mappings would degrade or mislead.

### Mechanism 2
- Claim: Training across multiple imaging modalities (clinical, dermoscopic, pathological) enhances zero-shot generalization and transfer learning.
- Mechanism: Multi-modality exposure forces the visual encoder to learn modality-agnostic disease representations while preserving modality-specific features. This redundancy creates robust feature embeddings that transfer to unseen datasets.
- Core assumption: Shared semantic concepts (e.g., "melanoma") manifest consistently enough across modalities for cross-modal learning to occur.
- Evidence anchors:
  - Strong zero-shot classification performance, with accuracy up to 82.34% on pathology datasets
  - Table 5: SkinVL-PubMM achieves 82.34% on Patch16 (pathology), 51.29% on HAM10000 (dermoscopy)—both datasets include modalities with varying representation in training
  - No direct corpus comparison of multi-modality vs single-modality training in dermatology VLMs
- Break condition: If disease presentations are too modality-specific with minimal shared features, multi-modality training could introduce noise rather than transfer benefits.

### Mechanism 3
- Claim: LLM-generated instruction-following QA pairs from expert captions improve open-ended response quality.
- Mechanism: Llama-3.1 reformats dense textbook captions into multi-turn conversational QA, exposing the model to diverse question phrasings while preserving factual grounding. This instruction-tuning bridges the gap between descriptive captions and interactive clinical dialogue.
- Core assumption: The LLM generator introduces linguistic diversity without hallucinating information beyond the source caption (the paper includes explicit rules to prevent this).
- Evidence anchors:
  - Llama-3.1 was employed as a generator to curate diverse instruction-following data, generating 27k QA pairs
  - Figure 3c: Caption texts average ~22 words with high lexical diversity (Herdan's C metric)
  - Weak corpus evidence; related papers don't isolate LLM-generated QA as a causal factor
- Break condition: If the LLM systematically introduces medical inaccuracies or if generated questions don't match real clinical query distributions, instruction-tuning could misalign model behavior.

## Foundational Learning

- Concept: **Vision-Language Model Architecture (Encoder-Projector-LLM)**
  - Why needed here: SkinVL uses a frozen CLIP-ViT-L/14 visual encoder, a trainable projection layer, and a frozen LLaVA-Med LLM with LoRA adapters. Understanding this separation is essential for debugging which component causes failures.
  - Quick check question: Which components are frozen vs. trainable in SkinVL, and what does each component output?

- Concept: **LoRA (Low-Rank Adaptation) Fine-tuning**
  - Why needed here: The paper uses LoRA with rank=128, alpha=64 to efficiently adapt the LLM without full retraining. Misconfiguring LoRA parameters is a common failure mode.
  - Quick check question: What do rank and alpha control in LoRA, and what happens if rank is set too low?

- Concept: **Dermatology Imaging Modalities**
  - Why needed here: The dataset spans clinical photos (external appearance), dermoscopy (magnified surface/subsurface features), and pathology (histological slides). Performance varies significantly by modality.
  - Quick check question: Which modality showed highest VQA performance, and why might image quality matter for recall metrics?

## Architecture Onboarding

- Component map:
Image Input → CLIP-ViT-L/14 (frozen) → Visual Projection Layer (trainable) → LLaVA-Med-v1.5/Mistral-7B (frozen) with LoRA adapters (trainable) → Text Output

Training updates: projection layer weights + LoRA adapters only.

- Critical path:
  1. Data preparation: Convert classification datasets to QA format; ensure MM-Skin captions are properly aligned
  2. Modality classification: Verify images are correctly labeled as clinical/dermoscopic/pathological before training
  3. LoRA configuration: Set rank=128, alpha=64, lr=5e-5 with cosine decay
  4. Evaluation pipeline: VQA (BLEU-4, BERTScore), SFT classification (logistic regression on frozen features), zero-shot (prompt-based)

- Design tradeoffs:
  - SkinVL-MM (MM-Skin only) vs SkinVL-Pub (public data only) vs SkinVL-PubMM (combined): MM excels at VQA metrics; PubMM best for zero-shot classification due to label exposure
  - Freezing encoder/LLM limits capacity but enables efficient training on 8×3090 GPUs; full fine-tuning would require significantly more compute
  - LLM-generated QA scales data efficiently but requires prompt engineering to prevent hallucination

- Failure signatures:
  - Low BLEU-4 but high recall: Model generates relevant content but with poor lexical precision (seen in general LVLMs like LLaVA-v1.6)
  - Near-zero accuracy on specific modalities: Modality mismatch between training and test (e.g., LLaVA-1.6 scores 3.18% on HAM10000 dermoscopy)
  - High BertScore but low BLEU-4: Semantic alignment present but surface-form mismatch

- First 3 experiments:
  1. **Ablation on training data source**: Train separate models on MM-Skin only, public data only, and combined; compare VQA metrics and zero-shot classification to isolate contribution of textbook-derived captions.
  2. **Modality-specific evaluation**: Evaluate each SkinVL variant on held-out test sets split by modality (pathology: Patch16, dermoscopy: HAM10000/ISIC2019, clinical: DDI/Fitzpatrick) to identify modality gaps.
  3. **LoRA rank sensitivity**: Test rank values {32, 64, 128, 256} on a validation subset to verify the paper's claim that rank=128 is optimal; monitor for underfitting (low rank) or overfitting (high rank).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can detailed, long-text descriptions in dermatology datasets specifically drive the development of reasoning capabilities in next-generation medical Large Vision-Language Models (LVLMs)?
- Basis in paper: The conclusion states, "Further research into next-generation medical LVLMs with better reasoning capabilities will benefit from the improvement in the generation of detailed responses."
- Why unresolved: While the paper demonstrates that SkinVL generates better responses, the specific mechanisms by which long-text descriptions enhance complex clinical reasoning (versus just descriptive accuracy) remain an area for future development.
- What evidence would resolve it: Future studies isolating "reasoning" metrics (e.g., diagnostic logic evaluation rather than just text overlap scores like BLEU) comparing models trained on long-form vs. short-form data.

### Open Question 2
- Question: To what extent does the reliance on LLM-generated instruction-tuning data introduce systematic hallucinations or biases into the dermatological reasoning of models like SkinVL?
- Basis in paper: The authors use Llama-3.1 to generate 27k QA pairs from captions. While rules are set to prevent introducing outside info (Appendix B), the paper does not quantitatively evaluate the "grounding" of the model to ensure it isn't hallucinating features implied by the generator rather than present in the image.
- Why unresolved: The evaluation relies on text-generation metrics (BLEU, METEOR) and classification accuracy, but does not explicitly measure the factual consistency of the generated free-text explanations against the visual content.
- What evidence would resolve it: A human or automated evaluation specifically scoring the "hallucination rate" of clinical features in the model's free-text responses.

### Open Question 3
- Question: Can the sparse demographic annotations in the MM-Skin dataset (particularly for pathology images) support statistically significant fairness evaluations across diverse patient populations?
- Basis in paper: The paper introduces "MM-Skin-D" for fairness evaluation. However, Table 2 shows that only 14 gender attributes and 6 age attributes were extracted for the 3,016 pathology images, which may be insufficient for robust subgroup analysis.
- Why unresolved: The paper proposes the task of fairness evaluation but acknowledges limited metadata extraction for certain modalities, leaving the viability of fairness benchmarking on this specific subset uncertain.
- What evidence would resolve it: A performance breakdown of SkinVL by demographic subgroups to show if the limited sample size allows for meaningful fairness conclusions.

### Open Question 4
- Question: How does the significant modality imbalance in the MM-Skin dataset affect the model's robustness and generalizability in underrepresented imaging types like dermoscopy?
- Basis in paper: Figure 3(b) and Table 2 show that clinical images comprise 63% of the dataset, while dermoscopic images make up only 10% (1,039 images).
- Why unresolved: While the model performs well overall, it is unclear if the performance on dermoscopic tasks is fundamentally limited by the lower volume of training data compared to clinical images.
- What evidence would resolve it: Ablation studies showing performance scaling on dermoscopic tasks as the number of dermoscopic training samples increases to match the clinical sample size.

## Limitations

- Limited external validation of textbook caption quality and clinical accuracy across modalities
- Absence of human evaluation for the 27k LLM-generated QA pairs to verify medical accuracy
- Significant modality imbalance (63% clinical, 10% dermoscopic) may limit performance on underrepresented imaging types

## Confidence

- **High**: Claims about SkinVL-MM achieving superior VQA performance metrics (BLEU-4 >70 on pathology) compared to general and medical LVLMs
- **Medium**: Claims about multi-modality training benefits for zero-shot generalization, supported by strong accuracy numbers but lacking ablation comparisons
- **Low**: Claims about instruction-tuning benefits, as the contribution of LLM-generated QA is not isolated from other factors in the evaluation

## Next Checks

1. **Human Evaluation of Textbook Caption Quality**: Have dermatology experts rate a random sample of MM-Skin captions for clinical accuracy, completeness, and alignment with actual image content. This would validate whether the "high-quality" descriptor is empirically justified and whether caption biases could explain performance differences.

2. **Modality-Specific Ablation Study**: Train separate SkinVL variants on each imaging modality independently (clinical only, dermoscopic only, pathological only) and compare their zero-shot transfer performance to the multi-modal model. This would quantify whether cross-modal learning genuinely enhances generalization or if single-modality specialization performs comparably.

3. **LLM-Generated QA Validation**: Select 100 MM-Skin captions and generate multiple LLM prompt variants to create QA pairs. Compare these outputs against expert-generated questions to measure hallucination rates and question diversity. This would isolate whether the instruction-tuning approach introduces systematic errors that could affect model reliability.