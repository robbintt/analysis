---
ver: rpa2
title: Semi-gradient DICE for Offline Constrained Reinforcement Learning
arxiv_id: '2506.08644'
source_url: https://arxiv.org/abs/2506.08644
tags:
- policy
- distribution
- semidice
- correction
- cost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Semi-gradient DICE methods excel in offline RL but fail at off-policy
  evaluation, making them unsuitable for constrained RL where cost estimation is critical.
  This paper identifies that semi-gradient optimization produces policy corrections
  instead of stationary distribution corrections, breaking Bellman flow constraints.
---

# Semi-gradient DICE for Offline Constrained Reinforcement Learning

## Quick Facts
- arXiv ID: 2506.08644
- Source URL: https://arxiv.org/abs/2506.08644
- Authors: Woosung Kim; JunHo Seo; Jongmin Lee; Byung-Jun Lee
- Reference count: 40
- Primary result: CORSDICE consistently satisfies cost constraints while maximizing returns, outperforming baselines in 27/38 DSRL tasks

## Executive Summary
This paper addresses the critical limitation of semi-gradient DICE methods in offline constrained reinforcement learning, where they excel at return maximization but fail at cost estimation due to producing policy corrections instead of stationary distribution corrections. The authors propose CORSDICE, which extracts valid stationary distributions from policy corrections through a novel convex optimization method. On the DSRL benchmark, CORSDICE achieves state-of-the-art performance by consistently satisfying cost constraints while maximizing returns, outperforming baselines in 27 out of 38 tasks.

## Method Summary
CORSDICE is an offline constrained RL algorithm that iterates between: (1) SemiDICE optimization on penalized reward using Q/ν networks via semi-gradient losses to compute policy correction ratios, (2) Stationary distribution extraction via auxiliary A/µ networks using a novel convex optimization to recover state distribution corrections, (3) Lagrange multiplier λ update for cost limits, and (4) Policy extraction via weighted behavior cloning. The method leverages semi-gradient optimization's efficiency while correcting its OPE limitations through stationary distribution extraction.

## Key Results
- CORSDICE achieves state-of-the-art performance on DSRL benchmark, outperforming baselines in 27/38 tasks
- Consistently satisfies cost constraints while maximizing returns across Safety-Gymnasium, BulletGym, and MetaDrive environments
- Demonstrates stable convergence and accurate cost evaluation through stationary distribution extraction
- Shows significant improvement in off-policy evaluation accuracy compared to raw SemiDICE weights

## Why This Works (Mechanism)

### Mechanism 1
Semi-gradient optimization in DICE algorithms yields policy correction ratios (π(a|s)/πD(a|s)) rather than stationary distribution corrections (dπ(s,a)/dD(s,a)), violating Bellman flow constraints. By omitting the gradient from the next-state dual variable ν(s'), the optimization loses Bellman flow enforcement and instead enforces normalization conditions, solving a behavior-regularized MDP rather than the intended stationary distribution problem.

### Mechanism 2
Semi-gradient methods avoid the "state distribution sparsity" problem of full-gradient DICE by enforcing valid probability distributions over actions for all states in the dataset. This ensures the policy remains defined even in continuous domains, whereas full-gradient DICE can produce states with no defined actions, making the optimal policy undefined.

### Mechanism 3
A valid stationary distribution correction can be recovered from a policy correction via novel convex optimization. The method isolates state stationary distribution ratio w(s) by solving a constrained optimization where Bellman flow is calculated using the policy correction w(a|s), with an auxiliary network A(s) reducing bias in sample-based estimation.

## Foundational Learning

- **Stationary Distribution Correction vs. Policy Correction**
  - Why needed here: The paper distinguishes these to explain why semi-gradient methods fail at cost estimation
  - Quick check question: If I have a policy correction w(a|s), do I know the relative probability of visiting state s1 vs s2? (Answer: No)

- **Bellman Flow Constraint**
  - Why needed here: This constraint ensures estimated distributions respect environment transition dynamics
  - Quick check question: Does a policy distribution π(a|s) inherently satisfy Bellman flow? (Answer: No)

- **Convex Conjugates (f*)**
  - Why needed here: DICE methods use Lagrangian duality to convert constrained optimization into tractable loss functions
  - Quick check question: Why is the dual form preferred here? (Answer: Converts constrained problem to unconstrained saddle-point suitable for gradient descent)

## Architecture Onboarding

- **Component map:**
  Dataset D, Initial state buffer p0 -> SemiDICE Core (Q/ν networks) -> Extraction Module (A/µ networks) -> Policy (πθ) with Lagrange multiplier λ

- **Critical path:**
  1. Update νψ, Qφ using semi-gradient losses on penalized reward r-λc, derive w(a|s)
  2. Update Aξ, μζ to compute state stationary distribution correction w(s)
  3. Compute total correction w(s,a) = w(s)w(a|s), update λ using gradient descent
  4. Update πθ by maximizing likelihood weighted by w(a|s)

- **Design tradeoffs:**
  - Auxiliary network A(s) adds complexity but reduces bias in stochastic environments
  - Semi-gradient vs. full-gradient: retains RL performance/stability while paying extraction computational cost

- **Failure signatures:**
  - High OPE error: A(s) fails to converge, causing biased cost estimates
  - Unstable λ: Oscillating Lagrange multiplier suggests noisy stationary distribution extraction
  - Policy collapse: α too small causes overfitting to sparse rewards

- **First 3 experiments:**
  1. Sanity Check: Run SemiDICE vs. CORSDICE on grid-world, verify Bellman flow violation vs. satisfaction
  2. OPE Validation: Compare RMSE of return estimation using raw vs. extracted weights on D4RL
  3. Constraint Satisfaction: Run on Safety-Gymnasium, plot normalized return vs. normalized cost to verify cost limits

## Open Questions the Paper Calls Out

### Open Question 1
Can CORSDICE's stationary distribution extraction method be effectively extended to other DICE-based algorithms requiring OPE, such as ROI maximization, without compromising performance?
- Basis in paper: "While this work focuses specifically on constrained RL problems, the proposed method can be readily applied to enhance other DICE-based algorithms for problems requiring OPE, such as ROI maximization."
- Why unresolved: Paper proposes extension but provides no empirical or theoretical validation beyond constrained RL
- What evidence would resolve it: Empirical results on ROI maximization benchmarks showing CORSDICE maintains accurate evaluation while preserving performance

### Open Question 2
How does the additional function approximator A(s) for bias reduction in stationary distribution extraction scale with state space complexity, and what are optimal architectural choices for high-dimensional problems?
- Basis in paper: "Extracting valid stationary distribution requires an additional approximator in our method, potentially leading to increased training complexity."
- Why unresolved: Acknowledges increased complexity but provides no analysis of computational overhead or scaling behavior
- What evidence would resolve it: Systematic ablation studies comparing training time, memory usage, and performance across varying state space dimensions

### Open Question 3
What are the theoretical error bounds for the bias introduced by CORSDICE's stationary distribution extraction, particularly compared to direct semi-gradient OPE attempts?
- Basis in paper: "While approximation errors may introduce some additional bias, our empirical observations suggest this bias is significantly smaller than that introduced by relying on the naive single-sample estimator."
- Why unresolved: Claim supported only empirically; no formal analysis of convergence properties or error bounds provided
- What evidence would resolve it: Theoretical derivation of error bounds with empirical validation across diverse MDP structures

## Limitations
- The extraction module introduces approximation errors through the auxiliary network A(s)
- The approach assumes offline dataset covers state space sufficiently for policy correction to be meaningful
- Out-of-distribution states remain problematic regardless of the method
- Additional computational complexity from the extraction step

## Confidence

- Mechanism 1 (Semi-gradient produces policy not stationary corrections): High - Proven mathematically with clear empirical validation
- Mechanism 2 (State distribution sparsity avoidance): Medium - Theoretical proof exists but real-world applicability depends on dataset coverage
- Mechanism 3 (Extraction recovers stationary distributions): Medium - Convex formulation is sound but auxiliary network introduces potential bias

## Next Checks

1. **Dataset Coverage Analysis**: Systematically evaluate CORSDICE performance as a function of dataset coverage/overlap with optimal policy states. Quantify the breakdown point where offline data becomes insufficient.

2. **A(s) Network Sensitivity**: Ablate the auxiliary network by varying its architecture, training schedule, and initialization. Measure impact on extracted stationary distribution accuracy and downstream cost estimation.

3. **Multi-Task Generalization**: Test CORSDICE on tasks requiring generalization to unseen dynamics or reward structures within the same environment family. Validate whether extracted stationary distributions transfer appropriately.