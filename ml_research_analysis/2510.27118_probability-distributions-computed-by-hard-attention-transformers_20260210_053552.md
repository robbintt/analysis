---
ver: rpa2
title: Probability Distributions Computed by Hard-Attention Transformers
arxiv_id: '2510.27118'
source_url: https://arxiv.org/abs/2510.27118
tags:
- language
- next
- weighted
- which
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the expressivity of transformer language models
  in defining probability distributions over strings, contrasting with prior work
  that focused on Boolean language recognition. The authors define and analyze unique-hard-attention
  transformers (UHA Ts) as classifiers and autoregressors over both Boolean and real
  semirings.
---

# Probability Distributions Computed by Hard-Attention Transformers

## Quick Facts
- **arXiv ID:** 2510.27118
- **Source URL:** https://arxiv.org/abs/2510.27118
- **Reference count:** 19
- **Key outcome:** This paper studies the expressivity of transformer language models in defining probability distributions over strings, contrasting with prior work that focused on Boolean language recognition. The authors define and analyze unique-hard-attention transformers (UHA Ts) as classifiers and autoregressors over both Boolean and real semirings. They establish that in the Boolean semiring, UHA Ts, LTL, and counter-free DFAs are equivalent both as classifiers and as autoregressors. However, in the real semiring, while LTL and counter-free DFAs remain equivalent as classifiers (defining aperiodic step functions), they are strictly less expressive than counter-free NFAs as autoregressors. The paper also shows that certain fragments of LTL (e.g., TL[∅]) are more expressive as autoregressors than classifiers, and that temporal logic with counting operators is more expressive as autoregressors. The main contribution is a detailed characterization of which probability distributions can be expressed by transformers in their common autoregressive language modeling setting.

## Executive Summary
This paper provides a rigorous theoretical analysis of what probability distributions over strings can be expressed by transformer language models, specifically focusing on unique-hard-attention transformers (UHA Ts). Unlike previous work that studied transformers as Boolean classifiers for formal languages, this research examines their ability to define actual probability distributions in autoregressive language modeling settings. The authors establish a comprehensive hierarchy showing which formal languages and distributions can be captured by different computational models.

The key insight is that transformers' expressivity depends critically on whether they operate in Boolean or real semirings, and whether they function as classifiers or autoregressors. In the Boolean setting, UHA Ts align perfectly with linear temporal logic (LTL) and counter-free deterministic finite automata (DFAs). However, in the real semiring, important distinctions emerge: while classifiers remain equivalent to LTL and counter-free DFAs (defining aperiodic step functions), autoregressors gain additional expressivity, with counter-free NFAs becoming strictly more powerful than their DFA counterparts.

## Method Summary
The paper establishes formal models for studying transformer expressivity by defining unique-hard-attention transformers (UHA Ts) as a tractable theoretical abstraction. UHA Ts are characterized by their attention mechanism that selects exactly one previous token at each step, making their computation path deterministic and analyzable. The authors then systematically compare UHA Ts to three other formal models: linear temporal logic (LTL), deterministic finite automata (DFAs), and nondeterministic finite automata (NFAs) across two algebraic settings - the Boolean semiring (for classification tasks) and the real semiring (for probability distributions).

The analysis proceeds by constructing explicit mappings between these formalisms and proving equivalence or strict inclusion relationships. For the Boolean semiring, the authors show that UHA Ts, LTL, and counter-free DFAs are mutually equivalent both as language classifiers and as probability distributions (where all accepted strings have probability 1 and rejected strings have probability 0). For the real semiring, they demonstrate that while LTL and counter-free DFAs remain equivalent as classifiers, counter-free NFAs can express probability distributions that counter-free DFAs cannot capture as autoregressors.

## Key Results
- In the Boolean semiring, UHA Ts, LTL, and counter-free DFAs are equivalent both as classifiers and as autoregressors
- In the real semiring, LTL and counter-free DFAs remain equivalent as classifiers (defining aperiodic step functions) but are strictly less expressive than counter-free NFAs as autoregressors
- Certain LTL fragments (e.g., TL[∅]) are more expressive as autoregressors than classifiers
- Temporal logic with counting operators is more expressive as autoregressors than as classifiers

## Why This Works (Mechanism)
The paper's theoretical framework works by leveraging the deterministic nature of hard attention to create analyzable computation paths that can be mapped to formal language theory models. By restricting attention to unique-hard-attention transformers, the authors can precisely characterize the probability distributions these models can express. The key mechanism is the decomposition of transformer computation into sequential decisions about which previous token to attend to, which maps naturally to state transitions in finite automata.

The semiring distinction is crucial: the Boolean semiring captures simple acceptance/rejection decisions, while the real semiring enables the assignment of actual probability values. This shift from classification to probabilistic modeling reveals new expressivity gaps, particularly between deterministic and nondeterministic automata when functioning as autoregressors rather than classifiers.

## Foundational Learning

**Linear Temporal Logic (LTL):** A formal system for specifying properties of sequences using operators like "always," "eventually," and "until." *Why needed:* LTL provides a logical framework for describing regular languages that can be directly compared to transformer computation paths. *Quick check:* Verify that any LTL formula can be translated to an equivalent counter-free automaton.

**Semirings:** Algebraic structures with two operations (addition and multiplication) satisfying certain axioms. *Why needed:* The Boolean and real semirings provide different mathematical foundations for classification versus probabilistic modeling. *Quick check:* Confirm that the Boolean semiring {0,1} with OR and AND satisfies semiring axioms.

**Counter-free Automata:** Finite automata whose transition monoids are aperiodic (no cycles of length > 1). *Why needed:* These automata characterize exactly what UHA Ts can compute in the Boolean setting. *Quick check:* Test that counter-free DFAs cannot count modulo any number greater than 1.

**Autoregressive Modeling:** Sequential probability modeling where each token is predicted conditioned on previous tokens. *Why needed:* This is the standard language modeling paradigm for transformers. *Quick check:* Verify that the chain rule of probability decomposes joint distributions into sequential conditional probabilities.

**Transition Monoids:** Algebraic structures capturing the composition of state transitions in automata. *Why needed:* Aperiodicity of transition monoids characterizes counter-free automata. *Quick check:* Compute the transition monoid of a simple automaton and verify its aperiodicity.

## Architecture Onboarding

**Component Map:** Input string → Hard attention mechanism → Token selection → State update → Output distribution, where the attention mechanism deterministically selects one previous position to attend to at each step.

**Critical Path:** The critical computational path is the sequence of attention choices made by the hard attention mechanism, which determines the state trajectory through the implicit finite automaton representation.

**Design Tradeoffs:** The paper trades generality (soft attention) for theoretical tractability (hard attention), enabling precise characterization of expressivity at the cost of modeling practical transformer implementations.

**Failure Signatures:** Failure to express certain probability distributions occurs when the hard attention mechanism cannot create the necessary conditional dependencies, particularly those requiring counting or complex temporal patterns that counter-free automata cannot capture.

**First Experiments:**
1. Implement a counter-free NFA as an autoregressive model and verify it can express distributions that counter-free DFAs cannot.
2. Construct an explicit probability distribution that requires counting and show neither counter-free DFAs nor NFAs can express it.
3. Translate a simple LTL formula into its equivalent UHA T representation and verify the computation path matches the automaton structure.

## Open Questions the Paper Calls Out
None

## Limitations
- The analysis focuses specifically on unique-hard-attention transformers and does not address the expressivity of standard transformers with soft attention mechanisms
- The paper assumes access to arbitrary real values in the real semiring setting, which may not reflect practical implementation constraints
- The characterization of autoregressors versus classifiers reveals interesting asymmetries, but the practical significance for real-world language modeling tasks is not fully explored

## Confidence

**High confidence** in the equivalence results between UHA Ts, LTL, and counter-free DFAs in the Boolean semiring, as these follow from well-established formal language theory results.

**Medium confidence** in the real semiring results showing the gap between counter-free DFAs and NFAs as autoregressors, as these require careful analysis of probabilistic distributions that may have subtle edge cases.

**Medium confidence** in the expressivity hierarchies involving LTL fragments and counting operators, as these results depend on specific technical constructions that would benefit from additional verification.

## Next Checks

1. Verify the gap between counter-free DFAs and NFAs as autoregressors by constructing explicit probability distributions that can be represented by the latter but not the former, ensuring no technical errors in the proof constructions.

2. Test the practical implications by implementing a subset of the theoretically expressible distributions from the paper and measuring whether standard transformer training can actually learn these distributions from data.

3. Extend the analysis to soft-attention transformers by either proving upper bounds on their expressivity or identifying specific distributions they can represent that UHA Ts cannot, helping bridge the theory-practice gap.