---
ver: rpa2
title: 'LuxInstruct: A Cross-Lingual Instruction Tuning Dataset For Luxembourgish'
arxiv_id: '2510.07074'
source_url: https://arxiv.org/abs/2510.07074
tags:
- luxembourgish
- instruction
- language
- tuning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LUXINSTRUCT, a cross-lingual instruction
  tuning dataset for Luxembourgish designed to overcome the scarcity of high-quality
  instruction data for low-resource languages. Instead of relying on machine translation,
  the dataset is constructed using aligned data from English, French, and German,
  preserving linguistic and cultural nuances.
---

# LuxInstruct: A Cross-Lingual Instruction Tuning Dataset For Luxembourgish

## Quick Facts
- arXiv ID: 2510.07074
- Source URL: https://arxiv.org/abs/2510.07074
- Reference count: 40
- Key outcome: Cross-lingual instruction tuning with aligned data from English, French, and German improves representational alignment and generative capabilities in Luxembourgish compared to monolingual instruction tuning.

## Executive Summary
This paper introduces LUXINSTRUCT, a cross-lingual instruction tuning dataset for Luxembourgish designed to overcome the scarcity of high-quality instruction data for low-resource languages. Instead of relying on machine translation, the dataset is constructed using aligned data from English, French, and German, preserving linguistic and cultural nuances. The approach involves leveraging Wikipedia, news articles, and an online dictionary to generate instruction-output pairs. Empirical results show that cross-lingual instruction tuning improves representational alignment between Luxembourgish and other languages, and enhances the model's generative capabilities in Luxembourgish. The dataset is evaluated using G-Eval with three LLM judges, demonstrating superior performance compared to monolingual instruction tuning.

## Method Summary
The LUXINSTRUCT dataset is constructed by extracting human-written Luxembourgish content from Wikipedia, RTL News, and the Luxembourg Online Dictionary, then generating instructions in English, French, or German using GPT-4.1-mini, paired with native Luxembourgish outputs. Data is filtered through heuristic-based methods and aligned using embedding-based retrieval. Models are fine-tuned using LoRA on query, key, and value projections with specific hyperparameters (rank=8, α=16, dropout=0.05). Evaluation uses CKA scores on FLORES-200 for alignment and G-Eval with three LLM judges for generation quality.

## Key Results
- Cross-lingual instruction tuning (EN-LB and FR-LB) yields higher CKA alignment improvements than monolingual LB-LB tuning
- English and French instruction configurations outperform German in alignment and generation quality
- G-Eval scores demonstrate superior performance compared to monolingual instruction tuning baselines

## Why This Works (Mechanism)

### Mechanism 1: Cross-Lingual Representational Alignment
- Claim: Training with high-resource language instructions paired with native low-resource outputs improves embedding space alignment more effectively than monolingual low-resource training.
- Mechanism: The model associates well-formed instruction representations from English, French, or German with Luxembourgish response representations, creating stronger cross-lingual bridges in shared embedding space.
- Core assumption: High-resource instruction representations provide more robust semantic anchors than low-resource ones, enabling better transfer.
- Evidence anchors: [section 4.1] EN-LB and FR-LB configurations yield highest CKA alignment improvements; DE-LB often underperforms monolingual LB-LB tuning.

### Mechanism 2: Avoiding Translation Artifact Propagation
- Claim: Using human-generated Luxembourgish outputs instead of machine-translated ones preserves linguistic integrity and avoids cascading errors.
- Mechanism: Machine translation into low-resource languages introduces semantic drift, idiomatic errors, and cultural mismatches; native outputs ground the model in authentic language patterns.
- Core assumption: The quality gap between MT output and native text is larger for low-resource languages than for high-resource ones.
- Evidence anchors: [abstract] "Traditional reliance on machine translation often introduces semantic misalignment and cultural inaccuracies."

### Mechanism 3: Linguistic Distance Affects Cross-Lingual Transfer Efficiency
- Claim: Pairing low-resource languages with more linguistically distant high-resource languages during instruction tuning may yield better alignment than using closely related languages.
- Mechanism: Closely related languages (German–Luxembourgish) may have overlapping but misaligned representations that interfere; more distant languages (English, French) force the model to learn cleaner cross-lingual mappings.
- Core assumption: The model's pre-existing multilingual representations for similar languages have entangled features that reduce effective transfer.
- Evidence anchors: [section 4.1] "DE-LB performs often worse than monolingual (LB-LB) tuning... EN-LB and FR-LB configurations yield the highest alignment improvements."

## Foundational Learning

- Concept: **Centered Kernel Alignment (CKA)**
  - Why needed here: Used to measure representational similarity between embedding spaces across languages; understanding this metric is essential for interpreting alignment improvements.
  - Quick check question: If CKA between Luxembourgish and English embeddings increases from 0.23 to 0.36 after fine-tuning, what does that indicate about the model's cross-lingual representations?

- Concept: **Cross-Lingual Instruction Tuning vs. Multilingual Instruction Tuning**
  - Why needed here: The paper distinguishes cross-lingual (instruction and output in different languages) from multilingual (same-language instruction-output pairs); this design choice drives the entire approach.
  - Quick check question: If you train with English instructions and Luxembourgish outputs, is this cross-lingual or multilingual instruction tuning?

- Concept: **Few-Shot In-Context Learning**
  - Why needed here: The evaluation methodology relies on providing k examples (4-shot, 8-shot) to steer model behavior; understanding how instruction language in these examples affects performance is central to the results.
  - Quick check question: Why might 8-shot examples in English outperform 8-shot examples in Luxembourgish for generating Luxembourgish outputs?

## Architecture Onboarding

- Component map:
  - Wikipedia (Open-Ended) -> Instruction generation (GPT-4.1-mini) -> English instructions
  - RTL News (Article-Title) -> Instruction generation (GPT-4.1-mini) -> French instructions
  - Luxembourg Online Dictionary (LOD) -> Instruction generation (GPT-4.1-mini) -> German instructions
  - All sources -> LoRA fine-tuning (query, key, value projections)
  - All sources -> Embedding-based retrieval (text-embedding-3-small) for alignment
  - FLORES-200 -> CKA alignment evaluation
  - 50 test instructions -> G-Eval with GPT-5 mini, Gemini 2.5 Flash-Lite, DeepSeek-V3

- Critical path:
  1. Extract human-written Luxembourgish content from sources
  2. Generate English/French/German instructions paired with native Luxembourgish outputs
  3. Filter via heuristics (length, language detection, extraction consistency)
  4. Fine-tune base model with cross-lingual instruction-output pairs
  5. Evaluate alignment (CKA on FLORES-200) and generation quality (G-Eval)

- Design tradeoffs:
  - Quality vs. diversity: Dataset covers only 7 task types due to scarcity of Luxembourgish resources; prioritized human-generated quality over synthetic breadth
  - Cross-lingual vs. monolingual: Trading instruction-language familiarity for better alignment and output quality
  - Judge reliability vs. cost: Three LLM judges improve robustness but increase evaluation complexity

- Failure signatures:
  - Low CKA alignment gains with DE-LB training despite linguistic similarity
  - MT-based baselines showing semantic drift in cultural/idiomatic expressions
  - Single-judge evaluation producing inconsistent rankings in low-resource settings

- First 3 experiments:
  1. Replicate alignment experiment: Fine-tune a multilingual model (e.g., Gemma 3 1B) on EN-LB, FR-LB, DE-LB, and LB-LB subsets; compute CKA scores on FLORES-200 to validate linguistic distance hypothesis.
  2. Ablate translation: Create a machine-translated Luxembourgish instruction subset and compare generation quality against native Luxembourgish outputs using G-Eval.
  3. Scale test: Apply the same cross-lingual pipeline to another low-resource language with available parallel data to test generalization of the mechanism.

## Open Questions the Paper Calls Out

- How does expanding the dataset to include additional task types and domains beyond the current seven categories impact model generalization in Luxembourgish?
- Does linguistic distance between the instruction and target language correlate with the effectiveness of cross-lingual instruction tuning?
- How can evaluation benchmarks for low-resource languages be standardized to reduce reliance on proprietary LLM-as-a-judge models?

## Limitations
- Dataset covers only 7 task types, limiting generalizability
- No systematic comparison between cross-lingual and MT-augmented approaches
- Linguistic distance effects remain hypothesized rather than proven
- Evaluation relies on LLM judges, which may have biases in low-resource settings

## Confidence

- Cross-lingual alignment improvements: Medium
- Linguistic distance hypothesis: Low
- Cultural preservation through native outputs: Medium

## Next Checks
1. Conduct ablation study comparing cross-lingual tuning against MT-based instruction tuning using the same dataset size
2. Extend experiments to multiple low-resource languages with varying linguistic distances from high-resource languages
3. Implement human evaluation study to validate LLM judge scores and assess cultural nuance preservation