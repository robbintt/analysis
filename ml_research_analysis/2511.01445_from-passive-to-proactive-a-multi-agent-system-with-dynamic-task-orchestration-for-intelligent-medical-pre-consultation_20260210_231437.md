---
ver: rpa2
title: 'From Passive to Proactive: A Multi-Agent System with Dynamic Task Orchestration
  for Intelligent Medical Pre-Consultation'
arxiv_id: '2511.01445'
source_url: https://arxiv.org/abs/2511.01445
tags:
- task
- medical
- history
- system
- pre-consultation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study introduces a hierarchical multi-agent framework that
  transforms passive medical AI systems into proactive inquiry agents through autonomous
  task orchestration. The framework decomposes pre-consultation into four primary
  tasks: Triage, History of Present Illness collection, Past History collection, and
  Chief Complaint generation, with the first three tasks further divided into 13 domain-specific
  subtasks.'
---

# From Passive to Proactive: A Multi-Agent System with Dynamic Task Orchestration for Intelligent Medical Pre-Consultation

## Quick Facts
- arXiv ID: 2511.01445
- Source URL: https://arxiv.org/abs/2511.01445
- Authors: ChengZhang Yu; YingRu He; Hongyan Cheng; nuo Cheng; Zhixing Liu; Dongxu Mu; Zhangrui Shen; Zhanpeng Jin
- Reference count: 37
- One-line primary result: Hierarchical multi-agent system achieves 87.0% triage accuracy and 98.2% task completion rate through dynamic task orchestration

## Executive Summary
This paper introduces a hierarchical multi-agent framework that transforms passive medical AI systems into proactive inquiry agents for intelligent medical pre-consultation. The system decomposes pre-consultation into four primary tasks (Triage, History of Present Illness, Past History, Chief Complaint) with 13 domain-specific subtasks, using an 8-agent architecture to orchestrate dynamic task selection and incremental context accumulation. Evaluated on 1,372 Chinese EHRs across multiple foundation models, the framework demonstrates superior task completion rates (98.2% vs 93.1% sequential) and clinical quality scores (4.48-4.69/5.0) while maintaining efficiency within 12.7-16.9 dialogue rounds.

## Method Summary
The framework implements hierarchical task decomposition with four primary tasks: T1 (Triage), T2 (HPI collection), T3 (Past History collection), and T4 (Chief Complaint generation). T1-T3 are further divided into 13 domain-specific subtasks. An 8-agent system (Controller, Monitor, Inquirer, Prompter, Recipient, Triager, Virtual Patient, Evaluator) orchestrates the consultation process through centralized control. The Controller selects the highest-priority incomplete subtask based on Monitor evaluations using a 0.85 threshold for clinical semantic validity and task completeness. The Recipient incrementally updates HPI, PH, and CC records across dialogue turns to prevent information loss in extended conversations.

## Key Results
- Achieved 87.0% accuracy for primary department triage and 80.5% for secondary department classification
- Task completion rates reached 98.2% using agent-driven scheduling versus 93.1% with sequential processing
- Clinical quality scores averaged 4.56 for Chief Complaints, 4.48 for History of Present Illness, and 4.69 for Past History on 5-point scale
- Consultations completed within 12.7 rounds for HPI and 16.9 rounds for Past History

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Task Scheduling
The Controller agent maintains a pending task set and selects the highest-priority incomplete subtask after each dialogue round. The Monitor evaluates subtasks against a 0.85 threshold combining clinical semantic validity and task completeness, creating an adaptive loop that prioritizes information gaps rather than following fixed sequences.

### Mechanism 2: Hierarchical Task Decomposition
The framework decomposes pre-consultation into 4 primary tasks with 13 domain-specific subtasks, enabling comprehensive structured inquiry while maintaining global coherence. This hierarchy allows microscopic symptom detail collection within macroscopic diagnostic progression.

### Mechanism 3: Incremental Context Accumulation
The Recipient agent incrementally updates HPI, PH, and CC records across dialogue turns using a function F that maintains condensed clinical summaries. This mitigates "loss-in-middle-turns" degradation in extended multi-turn medical dialogues by providing stable context for downstream agents.

## Foundational Learning

- **Concept: Hierarchical Task Networks (HTN)**
  - Why needed here: The framework decomposes high-level pre-consultation goals into primitive subtasks with ordering constraints. Understanding HTN decomposition helps grasp why T4 (CC generation) cannot be further subdivided—it's an integrative synthesis task, not a collection task.
  - Quick check question: Given a new medical domain (e.g., pediatrics), which existing subtasks would transfer directly vs. require domain-specific definition?

- **Concept: Completion Threshold Calibration**
  - Why needed here: The 0.85 Monitor threshold directly trades off dialogue length against information completeness. Engineers tuning this system must understand how threshold changes affect both metrics.
  - Quick check question: If clinical evaluation reveals missing allergy information in 15% of cases, should the threshold for the "Allergy History" subtask be raised or lowered?

- **Concept: Multi-Agent Message Passing Patterns**
  - Why needed here: The 8-agent architecture uses specific information flows (Controller→Prompter→Inquirer, Patient→Recipient→Prompter). Understanding these dependencies prevents circular dependencies or information bottlenecks.
  - Quick check question: Which agents must complete their work before the Inquirer can generate a contextually appropriate question?

## Architecture Onboarding

- **Component map:** Patient → Recipient → Monitor → Controller → Prompter → Inquirer → Patient (loop until all subtasks ≥0.85 or 30 rounds)
- **Critical path:** Patient input → Recipient updates records → Monitor evaluates pending subtasks → Controller selects next subtask → Prompter generates guidance → Inquirer produces question → Patient responds (loop until all subtasks ≥0.85 or 30 rounds)
- **Design tradeoffs:** Centralized Controller enables coherent global coordination but creates single point of failure; 0.85 threshold empirically effective but requires validation per deployment context; model-agnostic design sacrifices potential fine-tuning gains for portability
- **Failure signatures:** Stuck in Triage (T1 never reaches 0.85), redundant questioning (same subtask repeatedly selected), incomplete PH at round limit (T3 requires most rounds), hallucinated records (Recipient introduces information not stated by patient)
- **First 3 experiments:** 1) Baseline replication with GPT-OSS 20B on 50-sample subset, 2) Threshold sensitivity testing at 0.70, 0.80, 0.85, 0.90, 3) Scheduling strategy comparison implementing both Agent Driven and Default Order strategies

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the triage component be refined to better distinguish between clinically overlapping specialties like Psychiatry and Neurology? The results show Psychiatry had the lowest classification accuracy (65.2%), with errors primarily attributed to the "inherent overlap" with Neurology.

- **Open Question 2:** Is the framework's performance robust across different languages and clinical documentation standards? The study uses a strictly Chinese dataset and notes that Qwen3-8B's efficiency was likely due to its "stronger Chinese language capabilities."

- **Open Question 3:** How does the system handle the noise and non-cooperation inherent in real-world patient interactions compared to the simulated environment? The methodology relies on a "Virtual Patient" agent simulating dialogue based on clean, structured Electronic Health Records (EHRs).

## Limitations

- Clinical validity outside Chinese EHRs: Framework validated exclusively on 1,372 Chinese electronic health records from a single public platform
- LLM-specific behavior: Performance varied significantly across foundation models without fine-tuning strategies
- Missing comparative baselines: No comparison against single-agent approaches or state-of-the-art medical dialogue systems

## Confidence

- **High confidence**: Task decomposition effectiveness (87.0% triage accuracy, 4.69/5.0 clinical quality scores) and agent-driven scheduling superiority (98.2% vs 93.1% completion) are well-supported by direct experimental results
- **Medium confidence**: Hierarchical task orchestration mechanism works as described, though threshold calibration and subtask boundary definitions may require context-specific adjustment
- **Low confidence**: Claims about "transforming passive systems into proactive inquiry agents" lack comparative baseline data showing what passive systems would achieve under identical conditions

## Next Checks

1. **Cross-institutional validation**: Deploy the framework on 200-500 EHRs from a different healthcare system or country to verify the 87.0%/80.5% accuracy rates and 4.25-4.69/5.0 clinical quality scores generalize beyond the original dataset

2. **Threshold sensitivity analysis**: Systematically test Monitor thresholds from 0.70 to 0.95 in 0.05 increments, measuring the trade-off between completion rates and dialogue efficiency. Identify if 0.85 represents optimal balance or dataset-specific artifact

3. **Single-agent baseline comparison**: Implement a non-hierarchical, single-agent medical pre-consultation system using identical foundation models and evaluation criteria. Compare task completion rates, clinical quality scores, and dialogue efficiency to quantify the specific contribution of multi-agent orchestration