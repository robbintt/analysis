---
ver: rpa2
title: 'Zero-RAG: Towards Retrieval-Augmented Generation with Zero Redundant Knowledge'
arxiv_id: '2511.00505'
source_url: https://arxiv.org/abs/2511.00505
tags:
- knowledge
- corpus
- retrieval
- zero-rag
- pruning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Zero-RAG addresses the problem of knowledge redundancy between
  retrieval-augmented generation (RAG) corpora and large language models (LLMs). The
  core method uses a Mastery-Score metric to identify and prune redundant passages
  from the corpus, then employs a Query Router to bypass retrieval for questions the
  LLM can answer directly, and Noise-Tolerant Tuning to improve robustness against
  irrelevant documents.
---

# Zero-RAG: Towards Retrieval-Augmented Generation with Zero Redundant Knowledge

## Quick Facts
- arXiv ID: 2511.00505
- Source URL: https://arxiv.org/abs/2511.00505
- Authors: Qi Luo; Xiaonan Li; Junqi Dai; Shuang Cheng; Xipeng Qiu
- Reference count: 14
- Primary result: Zero-RAG can prune 30% of Wikipedia corpus, accelerate retrieval by 22%, and retain RAG performance with <3-point EM drop.

## Executive Summary
Zero-RAG addresses knowledge redundancy between retrieval-augmented generation corpora and large language models by identifying and removing mastered content. The approach combines corpus pruning via Mastery-Score, query routing to bypass retrieval for mastered queries, and noise-tolerant tuning to improve robustness. Experiments show significant efficiency gains while maintaining competitive performance on four QA benchmarks.

## Method Summary
Zero-RAG operates through three core mechanisms: (1) Corpus pruning using a Mastery-Score metric that identifies redundant knowledge the LLM has already mastered, (2) Query routing that bypasses retrieval for questions the LLM can answer directly, and (3) Noise-tolerant tuning that improves robustness against irrelevant documents. The system prunes 30% of Wikipedia sentences, accelerates retrieval by 22%, and reduces overall processing time by over 60% while retaining RAG performance with minimal EM degradation.

## Key Results
- Prunes 30% of Wikipedia corpus without substantial performance loss
- Accelerates retrieval by 22% through smaller index
- Retains RAG performance with less than 3-point EM drop
- Reduces overall processing time by over 60% in some cases

## Why This Works (Mechanism)

### Mechanism 1: Mastery-Score Corpus Pruning
Removes corpus passages the LLM has "mastered" by generating QA pairs for each sentence and computing average EM when the LLM answers without context. High EM indicates redundancy. Core assumption: QA performance reflects flexible knowledge utilization after fine-tuning.

### Mechanism 2: Query Router
Routes queries the LLM can answer independently directly to generation, bypassing retrieval. Evaluates queries with and without retrieval using a Noise-Tolerant model to determine mastery status. Core assumption: Router generalizes beyond training queries and LLM self-assessment aligns with actual competence.

### Mechanism 3: Noise-Tolerant Tuning
Trains with noisy/relevant/no-retrieval samples to teach the LLM to downweight irrelevant documents and leverage internal knowledge when appropriate. Constructs three sample types and optimizes a unified loss. Core assumption: Model can learn to distinguish signal from noise and switch strategies based on context utility.

## Foundational Learning

### Concept: Parametric vs. Non-Parametric Knowledge
**Why needed**: Zero-RAG's premise relies on substantial parametric knowledge overlapping with the corpus. Understanding this distinction is required to interpret Mastery-Scores and routing decisions.
**Quick check**: If an LLM correctly answers "What is the capital of France?" without retrieval, is this parametric knowledge? What about "What is the capital of South Sudan?" (established 2011)—how might training cutoffs affect this?

### Concept: Attention Distraction from Redundant Context
**Why needed**: The paper claims redundant retrieved passages hurt performance on mastered questions. Understanding attention mechanisms and limited context capacity clarifies why more is not always better.
**Quick check**: In a standard RAG setup, why might adding a retrieved document that merely repeats what the model already knows degrade its answer quality?

### Concept: Dense Retrieval Efficiency
**Why needed**: Zero-RAG explicitly targets dense retrieval workload. The reported 22% retrieval speedup assumes understanding embedding-based retrieval pipelines.
**Quick check**: How does reducing the corpus size from 138M to ~97M sentences (30% pruning) directly affect retrieval latency?

## Architecture Onboarding

### Component map
Corpus Pruner (offline) -> Query Router (inference-time) -> Retriever -> Noise-Tolerant LLM -> Answer

### Critical path
(1) Offline: Mastery-Score annotation → Corpus Pruner training → Corpus pruning → Dretained. (2) Training: Noise-Tolerant LLM fine-tuning + Query Router training. (3) Inference: Router decision → retrieval (if needed) → generation.

### Design tradeoffs
- Pruning ratio vs. EM drop: 30% pruning ≈ <3 EM point drop; 70% pruning ≈ 3–4 points on some datasets
- Router aggressiveness vs. coverage: More queries routed to "no retrieval" → lower latency but higher risk of false negatives
- Noise-tolerant training mix: Proportion of noise vs. relevant vs. no-RAG samples affects robustness vs. internal-knowledge bias

### Failure signatures
- Pruner over-prunes: Sudden performance collapse on specific topics not captured by QA probes
- Router over-bypasses: Retrieval rate drops sharply but unmastered-query EM degrades
- Noise-tolerant tuning overfits to internal knowledge: EM on unmastered queries approaches no-retrieval baseline
- Domain shift: System trained on Wikipedia fails on specialized corpora

### First 3 experiments
1. **Mastery-Score calibration**: Sample 500–1000 sentences; manually verify whether LLM answers derived QA correctly. Correlate with predicted Mastery-Scores. Target: ≥0.75 correlation before full pruning.
2. **Router threshold sweep**: On held-out queries, vary routing threshold; plot EM vs. retrieval rate and latency. Identify operating point matching your latency budget with acceptable EM drop.
3. **Component ablation replication**: Replicate Table 3 on your dataset—remove Pruner, Router, or Noise-Tolerant Tuning individually. Ensure each component provides measurable benefit before integration.

## Open Questions the Paper Calls Out

### Open Question 1
**Question**: How effective is Zero-RAG when applied to domain-specific corpora (e.g., biomedical or legal documents) or multimodal databases?
**Basis**: Authors explicitly state effectiveness on other database types "remains to be explored."
**Why unresolved**: Wikipedia's specific knowledge density and structure may not generalize to specialized domains where LLMs have weaker internal knowledge.

### Open Question 2
**Question**: How does the presence of noise or inconsistencies in the raw corpus affect the pruning mechanism's ability to retain critical information?
**Basis**: Authors note performance relies on initial data quality and noisy input may cause pruning to "inadvertently remove valuable information."
**Why unresolved**: Experiments use relatively clean Wikipedia; robustness against messy, real-world data remains untested.

### Open Question 3
**Question**: Does Noise-Tolerant Tuning impede the model's ability to update its internal knowledge when retrieved context contradicts mastered but outdated beliefs?
**Basis**: Method tunes model to rely on mastered internal knowledge, creating risk of reinforcing hallucinations if internal knowledge is stale.
**Why unresolved**: Paper evaluates static fact retrieval but not temporal generalization or capacity to accept corrective information over pre-trained memory.

### Open Question 4
**Question**: Is the Mastery-Score and resulting pruned corpus transferable across different LLM architectures?
**Basis**: Authors train regression model for specific target LLM but don't investigate cross-model transferability.
**Why unresolved**: Different models have different internal knowledge boundaries; passage redundant for 70B model might be essential for smaller 8B model.

## Limitations
- Limited validation to Wikipedia-based QA datasets, raising generalizability concerns
- Single-sentence QA probes may not capture complex knowledge utilization for multi-hop reasoning
- Pruning threshold and query router calibration parameters not fully specified for exact reproduction
- Effectiveness depends heavily on quality and distribution of noise samples in training

## Confidence

**High confidence**: Retrieval latency reduction (22%) and pruning efficiency (30% corpus reduction) are directly measurable from methodology.

**Medium confidence**: Exact Match retention (<3-point drop) is supported by experiments but depends on dataset-specific factors not fully characterized.

**Low confidence**: Generalizability to non-Wikipedia domains and long-term stability of Mastery-Scores across model updates or knowledge cutoffs.

## Next Checks

1. **Dataset generalization**: Apply Zero-RAG to specialized corpus (medical literature or legal documents) and measure performance degradation relative to Wikipedia baseline.

2. **Temporal robustness**: Test Mastery-Score stability by introducing recently created knowledge (post-training cutoff) and measuring whether system correctly identifies it as unmastered.

3. **Component interaction analysis**: Systematically vary pruning ratios, router thresholds, and noise sample proportions to map Pareto frontier of EM vs. latency across all three components.