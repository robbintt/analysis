---
ver: rpa2
title: 'Exploiting the Randomness of Large Language Models (LLM) in Text Classification
  Tasks: Locating Privileged Documents in Legal Matters'
arxiv_id: '2512.08083'
source_url: https://arxiv.org/abs/2512.08083
tags:
- privileged
- documents
- document
- classification
- randomness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how randomness in large language models
  (LLMs) can improve the detection of legally privileged documents in legal document
  review. Using GPT-4.1, researchers applied zero-shot learning with multiple submissions
  per document and varied randomness parameters (Temperature, Top-P) to assess classification
  consistency.
---

# Exploiting the Randomness of Large Language Models (LLM) in Text Classification Tasks: Locating Privileged Documents in Legal Matters

## Quick Facts
- arXiv ID: 2512.08083
- Source URL: https://arxiv.org/abs/2512.08083
- Reference count: 17
- Primary result: Multiple LLM submissions per document improve privileged document recall by up to 7% in legal review tasks

## Executive Summary
This study investigates how the inherent randomness of large language models can be leveraged to improve privileged document detection in legal review tasks. Using GPT-4.1, researchers applied zero-shot learning with multiple submissions per document and varied randomness parameters (Temperature, Top-P) to assess classification consistency. Results showed that while randomness control parameters had minimal impact on performance, submitting each document multiple times significantly improved recall—up to 7% in some configurations—with only modest precision losses. The method leverages classification variability to rank documents by confidence, offering a practical way to enhance accuracy in high-stakes legal review tasks.

## Method Summary
The researchers conducted binary classification of legal documents as "Privileged" or "Not Privileged" using GPT-4.1 via zero-shot prompting. They processed 1,206 documents from six real-world legal matters (201 sampled per matter), with each document submitted multiple times (5-60 submissions) across varied Temperature (0-2) and Top-P (0.2-0.9) configurations while keeping Top-K fixed at 50. Confidence scores were calculated as the ratio of "Privileged" classifications to total submissions, and documents were ranked by these scores to optimize the precision-recall trade-off. The primary goal was to improve recall for privileged document detection while accepting modest precision losses.

## Key Results
- Multiple submissions per document improved recall by up to 7% compared to single submissions
- Temperature and Top-P parameters had minimal impact on classification consistency, with inherent variability remaining around 4%
- Confidence scoring based on classification frequency enabled effective ranking for threshold-based precision-recall optimization
- Recall gains were achieved with only modest precision losses, making the approach viable for legal review workflows

## Why This Works (Mechanism)
The methodology exploits the inherent randomness in LLM outputs by submitting each document multiple times and aggregating results. When an LLM's confidence in a classification is low, repeated submissions produce varied outputs. Documents consistently classified as "Privileged" across multiple submissions receive higher confidence scores, while those with mixed classifications receive lower scores. This approach transforms classification uncertainty into a ranking mechanism, allowing reviewers to focus on high-confidence privileged documents while accepting that some uncertain cases may be missed.

## Foundational Learning
**Zero-shot learning**: Why needed - Enables classification without model retraining; Quick check - Verify model can correctly classify held-out examples without examples in prompt
**Temperature parameter**: Why needed - Controls randomness in sampling; Quick check - Compare output diversity at T=0 vs T=2 for identical prompts
**Top-P sampling**: Why needed - Alternative to temperature for controlling output randomness; Quick check - Measure vocabulary coverage across samples at different P values
**Confidence scoring**: Why needed - Aggregates multiple classifications into a single metric; Quick check - Verify score distribution matches expected binomial variance
**Precision-recall trade-off**: Why needed - Balances false positives against false negatives; Quick check - Plot PR curve across confidence thresholds to identify optimal operating point

## Architecture Onboarding

**Component Map**: Document -> LLM API (Temperature/Top-P) -> Classification Output -> Aggregation Engine -> Confidence Score -> Ranking System

**Critical Path**: Document ingestion → Multiple LLM submissions → Classification aggregation → Confidence calculation → Document ranking

**Design Tradeoffs**: 
- More submissions → higher recall but increased API costs and processing time
- Higher confidence thresholds → better precision but lower recall
- Zero-shot vs. few-shot prompting → simplicity vs. potential accuracy gains

**Failure Signatures**:
- Classification variability near zero despite parameter tuning → API parameters not being applied
- Recall improvement below 2% despite 25+ submissions → inherent randomness rate too low
- Precision drops >20% → confidence scoring threshold too low

**First Experiments**:
1. Test 5 submissions per document at Temperature=0 and Temperature=2, verify 4% variability rate
2. Implement confidence scoring and compute precision/recall at 0.1 intervals from 0.0-1.0
3. Compare single-submission vs. multi-submission recall gains on a small test set

## Open Questions the Paper Calls Out
**Open Question 1**: What techniques can effectively increase the inherent randomness rate of LLM classification outputs beyond the observed ~4% variability? The authors note that Temperature and Top-P parameters did not significantly alter randomness rates, and they plan to investigate techniques for increasing randomness rates.

**Open Question 2**: Does incorporating matter-specific context into prompts significantly improve privileged document detection performance? The authors suggest that conducting experiments on a matter-by-matter basis and incorporating matter-specific knowledge into prompts could yield significantly improved results.

**Open Question 3**: How well does the multiple-submission methodology generalize to sanctions compliance screening and other high-stakes legal domains? This section outlines future work exploring how the inherent randomness of large language models can be leveraged to enhance sanction compliance.

## Limitations
- Reliance on proprietary legal documents not publicly available for validation
- Use of unreleased GPT-4.1 model variant, limiting generalizability to current LLM versions
- Lack of F1-score analysis or cost-benefit quantification for practical legal applications

## Confidence
- **High Confidence**: Multiple submissions improve recall in privileged document detection
- **Medium Confidence**: Randomness control parameters have minimal impact on classification consistency
- **Low Confidence**: Practical utility in real-world legal review settings without cost-benefit analysis

## Next Checks
1. Replicate methodology using publicly available legal corpus (e.g., EDRM XML datasets) to verify recall improvements without proprietary data dependencies
2. Test same methodology using GPT-4o or GPT-4-turbo to determine if 4% classification variability and recall gains are consistent across model versions
3. Implement simulation comparing single-submission vs. multi-submission approaches across varying document volumes to quantify trade-off between improved recall and increased review costs