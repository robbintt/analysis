---
ver: rpa2
title: Bayesian Concept Bottleneck Models with LLM Priors
arxiv_id: '2410.15555'
source_url: https://arxiv.org/abs/2410.15555
tags:
- concepts
- bc-llm
- concept
- does
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BC-LLM, a novel method for learning Concept
  Bottleneck Models (CBMs) that addresses the accuracy-interpretability tradeoff in
  interpretable machine learning. The key innovation is using Large Language Models
  (LLMs) within a Bayesian framework to iteratively search over a potentially infinite
  set of interpretable concepts rather than relying on a predefined finite set.
---

# Bayesian Concept Bottleneck Models with LLM Priors

## Quick Facts
- arXiv ID: 2410.15555
- Source URL: https://arxiv.org/abs/2410.15555
- Authors: Jean Feng; Avni Kothari; Luke Zier; Chandan Singh; Yan Shuo Tan
- Reference count: 40
- Primary result: Introduces BC-LLM, a Bayesian framework using LLMs to iteratively discover interpretable concepts for Concept Bottleneck Models

## Executive Summary
This paper introduces BC-LLM, a novel method for learning Concept Bottleneck Models (CBMs) that addresses the accuracy-interpretability tradeoff in interpretable machine learning. The key innovation is using Large Language Models (LLMs) within a Bayesian framework to iteratively search over a potentially infinite set of interpretable concepts rather than relying on a predefined finite set. The method employs a split-sample Metropolis update where the LLM proposes concepts based on a subset of data while the remaining data is used to accept or reject these proposals.

Across image, text, and tabular datasets, BC-LLM outperforms interpretable baselines and even black-box models in certain settings. It converges more rapidly towards relevant concepts, provides robust uncertainty quantification for out-of-distribution samples, and demonstrates strong performance in clinical applications where interpretability is crucial. The method is particularly effective in settings with limited prior knowledge about which concepts are relevant.

## Method Summary
BC-LLM uses a split-sample Metropolis-Hastings framework to iteratively discover concepts. The algorithm partitions data into subset S (for LLM concept proposals) and held-out complement S^c (for Metropolis-Hastings acceptance). The LLM generates M candidate concepts in batches, which are then batch-annotated across all observations. Acceptance is determined by comparing partial Bayes factors computed via Laplace approximation. The method includes a warm-start greedy initialization phase followed by T=5 outer loops of Gibbs sampling, cycling through K=4-10 concept slots.

## Key Results
- BC-LLM achieves superior accuracy-interpretability tradeoffs compared to interpretable baselines across image, text, and tabular datasets
- The method converges more rapidly than existing approaches, requiring only O(KT) LLM queries
- BC-LLM provides robust uncertainty quantification for out-of-distribution samples and demonstrates strong clinical application performance
- Outperforms both interpretable baselines and certain black-box models in settings with limited prior knowledge about relevant concepts

## Why This Works (Mechanism)

### Mechanism 1: Split-sample validation
- Claim: Split-sample data partitioning enables valid Bayesian inference even when using LLM proposals that may be miscalibrated or hallucinate.
- Mechanism: The algorithm partitions data into subset S (for LLM concept proposals) and held-out complement S^c (for Metropolis-Hastings acceptance). The acceptance probability reduces to the partial Bayes factor p(y_{S^c} | y_S, candidate_concepts) / p(y_{S^c} | y_S, current_concepts), which compares held-out likelihoods. This creates a statistical guardrail: poor LLM proposals are rejected when they fail to generalize to S^c.
- Core assumption: The LLM's proposal distribution is approximately consistent with some prior p(C) across iterations.
- Evidence anchors: Section 3.2 shows the split-sample update provides an opportunity to correct inconsistencies between the LLM's proposal and the actual posterior distribution.

### Mechanism 2: Multiple-try batching
- Claim: Multiple-try Metropolis-Hastings batching amortizes LLM query costs while maintaining valid posterior inference.
- Mechanism: Rather than proposing one concept at a time, the LLM generates M candidates in a single batched query. The algorithm samples from this batch proportionally to posterior weights, then applies a modified acceptance ratio that accounts for the multiple-try sampling scheme.
- Core assumption: The first few LLM-proposed candidates are highest quality; diminishing returns for additional candidates.
- Evidence anchors: Section 3.2, Algorithm 3 shows the multiple-try pseudocode with weight computation and modified acceptance ratio.

### Mechanism 3: Warm-start initialization
- Claim: Warm-start with greedy initialization enables practical convergence with T≈5 outer loops despite exponentially large concept space.
- Mechanism: Rather than pure random-walk MCMC, BC-LLM precedes Gibbs sampling with a greedy warm-start phase that selects concepts maximizing posterior probability. This initializes the Markov chain near high-probability regions.
- Core assumption: Greedy initialization provides a reasonable starting point; the concept space has exploitable structure.
- Evidence anchors: Appendix B describes running warm-start for one epoch and retaining only the last 20 iterates as posterior samples.

## Foundational Learning

- Concept: **Metropolis-Hastings acceptance ratio**
  - Why needed here: Core mathematical object determining whether proposed concepts are accepted; must understand how α = min(p(proposed)/p(current), 1) creates a valid stationary distribution.
  - Quick check question: If the acceptance ratio simplifies to the partial Bayes factor, what does this mean for how held-out data influences concept selection?

- Concept: **Concept Bottleneck Model architecture**
  - Why needed here: BC-LLM modifies the standard CBM pipeline (input → concepts → label) by making concept discovery iterative rather than pre-specified. Understanding the bottleneck constraint (K concepts, typically ≤20) is essential.
  - Quick check question: Why does constraining to binary/hard concept values prevent information leakage that plagues "soft" CBM variants?

- Concept: **Bayesian variable selection as Gibbs sampling**
  - Why needed here: The algorithm extends Bayesian sparse regression (spike-and-slab priors over features) to infinite concept spaces. Understanding how Gibbs cycles through variables clarifies why each concept is updated conditionally on others.
  - Quick check question: In Metropolis-within-Gibbs, why can't we sample directly from p(C_k | c_{-k}, y, X) when the concept space is infinite?

## Architecture Onboarding

- Component map: Step 0 (Initializer) -> Step 1 (Summary Generator) -> Step 2 (Proposer) -> Step 3 (Annotator) -> Step 4 (Acceptor) -> Outer Loop
- Critical path: Step 3 (annotation) dominates LLM API costs at O(nT) queries; Step 4 acceptance computation requires fitting logistic regression on S and S^c splits.
- Design tradeoffs:
  - **K (concept count)**: Larger K improves accuracy but reduces interpretability and increases annotation cost. Paper uses K∈[4,10].
  - **ω (split fraction)**: Controls exploration vs validation. Paper uses 0.5.
  - **T (outer loops)**: More samples improve uncertainty quantification but linearly increase cost. Paper finds T=5 sufficient for prediction, T=10 for uncertainty.
  - **Hard vs soft concepts**: Hard (binary) values preserve interpretability and prevent leakage; soft values improve accuracy but compromise auditability.
- Failure signatures:
  - **Low acceptance rate**: LLM proposals consistently rejected → likely ω too small or keyphrase summary uninformative.
  - **Concept drift without convergence**: Posterior samples show high variance across unrelated concepts → may need more iterations or better warm-start.
  - **OOD overconfidence**: Model confident on out-of-distribution samples → check that concept annotations allow "uncertain" probability outputs for ambiguous cases.
- First 3 experiments:
  1. **Sanity check**: Run BC-LLM on MIMIC simulation with known ground-truth concepts. Verify concept recovery improves with n (100→800 samples) and that BC-LLM approaches oracle performance.
  2. **Ablation of split fraction**: Compare ω∈{0.3, 0.5, 0.7} on a single bird family task. Plot acceptance rate and final accuracy.
  3. **Comparison to non-iterative baseline**: Run LLM+CBM (single-pass concept generation) vs BC-LLM on same task with same K. Quantify accuracy gap attributable to iterative refinement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the computational efficiency of BC-LLM be maintained when scaling to significantly larger concept spaces (K > 20) without compromising the interpretability-accuracy tradeoff?
- Basis in paper: [explicit] The Discussion section states, "BC-LLM is currently designed for learning highly interpretable CBMs where the number of concepts K is typically no more than 20... future directions can consider further speeding up posterior inference."
- Why unresolved: The current multiple-try Metropolis-within-Gibbs sampling requires O(nTK) LLM queries. While cost-effective for small K, this iterative process may become prohibitively slow or expensive as the model complexity increases.
- What evidence would resolve it: Empirical benchmarks on datasets requiring hundreds of concepts to achieve high accuracy.

### Open Question 2
- Question: How does the theoretical guarantee of convergence change if the LLM's proposal distribution fails to cover the optimal concepts with non-zero probability?
- Basis in paper: [inferred] Assumption G.1 (item 7) requires that the proposal distribution Q suggests the optimal concept c*_l with probability η > 0.
- Why unresolved: If the LLM's prior knowledge is fundamentally limited or biased such that it never proposes a specific causal concept relevant to the prediction task, the Bayesian framework cannot correct this omission.
- What evidence would resolve it: A theoretical extension of Theorem 3.1 relaxing the coverage assumption.

### Open Question 3
- Question: To what extent does the Laplace approximation used in the accept/reject step bias the concept selection in finite-sample regimes?
- Basis in paper: [inferred] Step 4 employs a Laplace-like approximation to estimate the split-sample posterior, noting that it "becomes increasingly accurate as n → ∞."
- Why unresolved: In small sample settings, the posterior may be highly non-Gaussian. Approximating it as such could lead to incorrect acceptance ratios.
- What evidence would resolve it: Ablation studies comparing the current implementation against exact posterior sampling methods on small datasets.

## Limitations

- The method's performance heavily depends on LLM quality for both concept proposal and annotation tasks.
- The greedy warm-start initialization may converge to local optima when true relevant concepts require complex multi-hop reasoning.
- The Laplace approximation assumes the posterior is approximately Gaussian, which may not hold for sparse concept sets with strong correlations between concepts.

## Confidence

- **High Confidence**: The split-sample Metropolis-Hastings framework for Bayesian concept selection is mathematically sound and well-supported by theoretical convergence results (Theorem 3.1).
- **Medium Confidence**: The empirical performance claims across multiple datasets are reasonable but would benefit from more extensive ablation studies.
- **Medium Confidence**: The cost-effectiveness claims relative to existing methods are plausible given the O(KT) complexity.

## Next Checks

1. **Concept recovery validation**: Run BC-LLM on a controlled simulation where ground-truth concepts are known (e.g., synthetic MIMIC data with predefined concept combinations) and measure concept precision/recall across different sample sizes.

2. **LLM dependence study**: Compare BC-LLM performance using different LLM providers/models (e.g., GPT-4o vs Claude) to quantify sensitivity to LLM quality and identify failure modes when proposals are suboptimal.

3. **Ablation of Metropolis steps**: Run BC-LLM with Metropolis acceptance disabled (accept all proposals) versus full Bayesian updates to isolate the contribution of the split-sample validation mechanism to overall performance.