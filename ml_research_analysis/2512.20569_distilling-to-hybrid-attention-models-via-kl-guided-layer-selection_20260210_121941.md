---
ver: rpa2
title: Distilling to Hybrid Attention Models via KL-Guided Layer Selection
arxiv_id: '2512.20569'
source_url: https://arxiv.org/abs/2512.20569
tags:
- layer
- attention
- softmax
- layers
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles the challenge of selecting which softmax attention\
  \ layers to preserve when distilling a pretrained Transformer into a more efficient\
  \ hybrid architecture that interleaves softmax and linear attention. The authors\
  \ propose a simple yet effective KL-guided layer selection method: they first distill\
  \ to an all-linear student, then restore each layer one at a time to softmax and\
  \ measure the resulting reduction in KL divergence to the teacher\u2014treating\
  \ this as a layer importance score."
---

# Distilling to Hybrid Attention Models via KL-Guided Layer Selection

## Quick Facts
- **arXiv ID**: 2512.20569
- **Source URL**: https://arxiv.org/abs/2512.20569
- **Reference count**: 40
- **Primary result**: KL-guided layer selection for hybrid softmax/linear attention distillation outperforms uniform and heuristic baselines by 0.03-0.22 absolute points on recall benchmarks at 25% softmax ratio.

## Executive Summary
This paper addresses the challenge of selecting which softmax attention layers to preserve when distilling a pretrained Transformer into a hybrid architecture that interleaves softmax and linear attention. The authors propose a simple yet effective KL-guided layer selection method: they first distill to an all-linear student, then restore each layer one at a time to softmax and measure the resulting reduction in KL divergence to the teacher—treating this as a layer importance score. They select the top-K layers by these scores and run a final distillation step to produce the hybrid model. Experiments on two 3B models (Qwen2.5-3B-Instruct and Llama-3.2-3B-Instruct) across multiple benchmarks show that this approach consistently outperforms strong baselines including uniform interleaving, heuristic task-guided selectors, and model-signal-based methods.

## Method Summary
The method uses a three-phase approach to create hybrid softmax/linear attention models. First, the teacher model is distilled to an all-linear student via a two-stage RADLADS pipeline: Stage 1 aligns hidden states with L2 loss (freezing FFN, training attention only), and Stage 2 performs temperature-scaled KL matching (training all student parameters). Second, for each layer ℓ in the student, the method creates a one-swap variant by restoring layer ℓ to the teacher's softmax, runs both distillation stages, and computes an importance score I(ℓ) = -E[L_KL] based on KL reduction. Third, the top-K layers by importance score are selected, the hybrid model is instantiated with those softmax layers, and final distillation is performed. The greedy addition strategy (starting from all-linear and adding layers) outperforms greedy removal, and GDN linear attention serves as a better "probe" for identifying transferable softmax layers than GLA.

## Key Results
- KL-guided selection (GA-S2) achieves 0.8631 on RULER for Qwen2.5-3B at 25% softmax, outperforming best baselines by 0.03-0.22 absolute points.
- Greedy addition strategy outperforms greedy removal by 0.17+ points on RULER at 25% softmax for both teacher models.
- GDN-selected layers transfer better to GLA students (0.8407 vs 0.6921) than GLA-native selections, suggesting GDN is a superior probing architecture.
- Early stopping can reduce token budget by 58-74% while maintaining selection quality, using rolling Jaccard similarity checks.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The reduction in KL divergence when restoring a single layer to softmax in an otherwise all-linear student serves as an effective proxy for that layer's importance to long-context recall.
- Mechanism: By distilling briefly with each layer individually restored to softmax, the KL reduction directly measures marginal utility under the distillation objective—layers that reduce KL more contribute more to matching teacher behavior. This one-swap approach avoids combinatorial search while remaining sensitive to both teacher and student architecture.
- Core assumption: KL divergence reduction correlates with downstream task performance, particularly on recall-intensive benchmarks.
- Evidence anchors: [abstract] "layer importance scores derived from a small amount of training on generic text data"; [section 5.1] "Stage-2 (KL-based) methods consistently and dramatically outperform their Stage-1 (MSE-based) counterparts"; [corpus] Related papers confirm KL as standard distillation metric but do not validate the one-swap marginal utility approach.
- Break condition: If KL reduction does not correlate with RULER/SWDE performance, the metric is not a valid importance proxy.

### Mechanism 2
- Claim: Greedy addition (starting from all-linear, adding top-K softmax layers) outperforms greedy removal (starting from all-softmax, removing least important).
- Mechanism: Identifying the most impactful layer to add from a constrained baseline yields a stronger signal than identifying the least harmful layer to remove. The all-linear baseline forces the selector to find layers essential for recovering teacher-like behavior.
- Core assumption: The all-linear student provides meaningful gradients for measuring marginal improvement.
- Evidence anchors: [section 5.1] "our greedy addition strategy (GA-S2) is more effective than greedy removal (GR-S2)"; [table 2] GA-S2 achieves 0.8631 vs GR-S2's 0.8259 on RULER for Qwen2.5-3B at 25% softmax; [corpus] No corpus papers directly compare greedy addition vs. removal for layer selection.
- Break condition: If the all-linear baseline is too degraded to train stably, gradients become uninformative.

### Mechanism 3
- Claim: The choice of linear attention variant during selection acts as a "probe," with GDN producing layer sets that transfer better to other architectures than GLA-derived selections.
- Mechanism: Layer importance is not purely a teacher property—it interacts with student architecture. GDN's stronger recall capabilities make it a more reliable probe for identifying universally important softmax layers.
- Core assumption: Some linear attention variants are better diagnostic tools than others.
- Evidence anchors: [section 5.2] "GDN and GLA selection trajectories exhibit only moderate overlap: mean Jaccard similarity 0.54–0.65"; [table 4] GDN-selected layers achieve 0.8407 on GLA student vs. 0.6921 from GLA-native selection; [corpus] Neighbor papers discuss hybrid architectures but do not analyze cross-architecture selection transfer.
- Break condition: If selections are entirely architecture-specific, each variant requires expensive re-probing.

## Foundational Learning

- Concept: **Softmax vs. Linear Attention Trade-offs**
  - Why needed here: The entire paper assumes understanding that softmax attention provides accurate long-context recall at O(n) memory cost, while linear attention offers O(1) memory but struggles with associative recall.
  - Quick check question: Why does Figure 1 show sliding window performance dropping on RULER but not commonsense tasks?

- Concept: **KL Divergence with Temperature Scaling**
  - Why needed here: Stage 2 distillation uses temperature-scaled KL to provide stronger gradients on non-argmax tokens, which is the core importance metric.
  - Quick check question: What happens to the gradient signal if temperature τ is too low vs. too high?

- Concept: **Combinatorial vs. Greedy Search**
  - Why needed here: The paper explicitly avoids combinatorial search (intractable for choosing K layers from L) by using greedy one-swap scoring.
  - Quick check question: Why can't we simply try all possible K-subsets of layers?

## Architecture Onboarding

- Component map:
  - Teacher (M_teacher): Pretrained softmax Transformer (Qwen2.5-3B or Llama-3.2-3B)
  - All-linear student (M_all-linear): All L layers converted to GDN or GLA
  - One-swap variants (M^(-ℓ)_all-linear): All-linear with layer ℓ restored to teacher's softmax
  - Final hybrid (M_hybrid-K): Top-K softmax layers + (L−K) linear layers

- Critical path:
  1. Distill teacher → all-linear (Stage 1: 100M tokens, Stage 2: 600M tokens)
  2. For each layer ℓ ∈ [1,L]: create one-swap, run Stage 1+2, compute I(ℓ) = −E[L_KL]
  3. Select S_softmax = top-K by I(ℓ)
  4. Instantiate hybrid and run final distillation (Stage 1+2)

- Design tradeoffs:
  - Token budget: Full pipeline uses ~5.6B tokens; early stopping can reduce by 58–74%
  - Softmax ratio: Lower ratios (12.5–25%) show largest gains over baselines; 50% recovers most teacher performance
  - Linear variant: GDN yields more transferable selections; GLA requires native re-probing for optimal results

- Failure signatures:
  - Uniform interleaving at low softmax ratios: RULER drops 0.2+ points below KL-guided selection
  - MSE-based (Stage 1) importance scores: Dramatically underperform KL-based scores
  - Early-selection instability: Top-K set unstable before ~1500 steps; use rolling Jaccard check

- First 3 experiments:
  1. Run one-swap scoring for a single layer (e.g., layer 0) to validate the pipeline produces a scalar I(ℓ)
  2. Compare GA-S2 vs. UNIFORM at 25% softmax on RULER to reproduce the reported 0.17+ gap for Qwen2.5-3B
  3. Implement early stopping: track rolling Jaccard (R=10 windows) and stop when backbone ≥ K−1 and JaccardMean ≥ 0.90

## Open Questions the Paper Calls Out

- **Can layer importance be derived directly from the teacher model's activations or gradients without requiring a full distillation pass for each layer?**
  - Basis in paper: [explicit] Conclusion states "future work could explore even cheaper proxies for layer importance, potentially derived directly from the teacher model's activations or gradients."
  - Why unresolved: Current method requires running Stage-1 and Stage-2 distillation for each layer individually to compute importance scores, which remains computationally expensive.
  - What evidence would resolve it: A proxy metric computed from teacher activations/gradients alone that correlates strongly with KL-guided importance scores and achieves comparable RULER performance when used for selection.

- **Can the layer selection framework be extended to fine-grained, head-level hybridization while maintaining efficiency?**
  - Basis in paper: [explicit] Conclusion identifies as a "promising direction" extending "this selection framework from the layer level to a more fine-grained, head-level hybridization."
  - Why unresolved: Current method operates at layer granularity; head-level selection would require substantially more importance evaluations and complexity.
  - What evidence would resolve it: Demonstration of head-level selection method that improves recall performance at fixed softmax budget compared to layer-level selection, with tractable computational cost.

- **What mechanisms enable GDN to serve as a more effective "probe" for identifying transferable softmax layer selections compared to GLA?**
  - Basis in paper: [inferred] Section 5.2 shows GDN-selected layers transfer well to GLA students (outperforming GLA-GLA selections), but the paper does not explain why GDN yields superior probing behavior.
  - Why unresolved: The architectural properties that make one linear attention variant a better probe for layer importance remain unclear.
  - What evidence would resolve it: Analysis of how GDN's recurrence structure, gating, or state representation differs from GLA in ways that correlate with softmax layer utility identification.

- **How can hybrid models maintain performance parity with teachers at extreme context lengths (131k+ tokens) where needle retrieval currently degrades?**
  - Basis in paper: [explicit] Appendix F reports hybrid student accuracy drops to 0.684 at 131k tokens versus teacher's 0.954, noting "further improvements at extreme lengths as an interesting direction for future work."
  - Why unresolved: Distillation uses shorter sequences and benchmarks evaluate below 10k tokens; behavior at extreme lengths is unexplored.
  - What evidence would resolve it: Modified distillation or architecture that achieves <5% relative degradation versus teacher at 131k+ token contexts.

## Limitations
- KL reduction as proxy validity: The assumption that KL divergence reduction correlates with downstream task performance remains untested against alternative metrics or ablation studies on layer utility.
- Temperature τ specification: The exact temperature value for KL divergence is not specified, which is a critical hyperparameter affecting gradient magnitudes and importance score calibration.
- Generalization beyond 3B models: While experiments show results on 1.5B and 7B Qwen2.5 models, the primary claims focus on 3B models, and effectiveness on much larger models remains untested.

## Confidence
- **High confidence**: The greedy addition strategy (GA-S2) outperforms greedy removal and baselines is well-supported by Table 2 results showing consistent improvements across both teacher models and multiple softmax ratios.
- **Medium confidence**: The claim that GDN-selected layers transfer better to GLA architectures than GLA-native selections is supported by Table 4, but the mechanism (why GDN is a better "probe") is speculative and not empirically validated.
- **Low confidence**: The claim that KL-guided selection is particularly effective at low softmax ratios is primarily based on comparison with uniform baselines. The paper does not explore whether this advantage persists when compared to more sophisticated alternatives or if the effect is specific to the benchmark suite used.

## Next Checks
1. **Temperature sensitivity analysis**: Systematically vary τ (e.g., τ ∈ {0.5, 1.0, 2.0}) and measure the correlation between KL reduction scores and downstream RULER performance. This validates whether the temperature choice materially affects selection quality and whether the metric is robust to hyperparameter choice.

2. **Cross-task transferability**: Test whether layers selected via KL reduction on generic text (DCLM) transfer to specialized domains (e.g., medical, legal) or whether domain-specific distillation would yield better hybrid architectures for those tasks.

3. **Layer clustering stability**: Analyze whether the "somewhat clustered" layer selections (mentioned in text) are statistically significant by computing layer depth distributions across multiple random seeds and teacher-student pairs, then testing for non-uniformity via chi-square or permutation tests.