---
ver: rpa2
title: Resource-Efficient Fine-Tuning of LLaMA-3.2-3B for Medical Chain-of-Thought
  Reasoning
arxiv_id: '2510.05003'
source_url: https://arxiv.org/abs/2510.05003
tags:
- reasoning
- medical
- fine-tuning
- language
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates the feasibility of fine-tuning LLaMA-3.2-3B
  for medical chain-of-thought reasoning using resource-constrained environments.
  The model was adapted using QLoRA and the Unsloth framework on the FreedomIntelligence/medical-o1-reasoning-SFT
  dataset, achieving stable ROUGE-L scores of 0.3052 while preserving reasoning transparency.
---

# Resource-Efficient Fine-Tuning of LLaMA-3.2-3B for Medical Chain-of-Thought Reasoning

## Quick Facts
- arXiv ID: 2510.05003
- Source URL: https://arxiv.org/abs/2510.05003
- Reference count: 28
- Key outcome: Achieved ROUGE-L 0.3052 on medical chain-of-thought reasoning while preserving reasoning transparency under resource constraints

## Executive Summary
This study demonstrates the feasibility of fine-tuning LLaMA-3.2-3B for medical chain-of-thought reasoning using resource-constrained environments. The model was adapted using QLoRA and the Unsloth framework on the FreedomIntelligence/medical-o1-reasoning-SFT dataset, achieving stable ROUGE-L scores of 0.3052 while preserving reasoning transparency. Despite limited computational resources, the approach maintained interpretable reasoning traces without catastrophic forgetting. The fine-tuned model and training pipeline were released on Hugging Face Hub, providing an accessible baseline for future research in efficient, domain-specific medical AI development.

## Method Summary
The research employed QLoRA with 4-bit quantization and LoRA adapters to fine-tune LLaMA-3.2-3B Instruct on medical chain-of-thought reasoning tasks. Using the Unsloth framework, the model was trained on NVIDIA T4/P100 GPUs with 15-16GB VRAM, utilizing a LoRA rank of 16, batch size of 4, and learning rate of 2×10⁻⁴ over 2 epochs. The FreedomIntelligence/medical-o1-reasoning-SFT dataset was split 90/10 for training and evaluation, with inputs reformatted as prompt-response pairs and processed with a maximum sequence length of 2048 tokens.

## Key Results
- Achieved stable ROUGE-L score of 0.3052 for medical chain-of-thought reasoning
- Maintained interpretable reasoning traces without catastrophic forgetting
- Successfully demonstrated resource-efficient fine-tuning on constrained hardware (Kaggle GPUs)

## Why This Works (Mechanism)
The approach leverages parameter-efficient fine-tuning through QLoRA, which freezes most model parameters while training only low-rank adapter matrices. This dramatically reduces memory requirements, enabling fine-tuning of large language models on consumer-grade GPUs. The Unsloth framework provides optimized kernels for faster training, while the 4-bit quantization further reduces memory footprint without significant performance degradation.

## Foundational Learning
- **QLoRA**: Why needed - Enables fine-tuning large models on limited hardware by quantizing to 4-bit and using LoRA adapters. Quick check - Verify GPU memory usage stays under 16GB during training.
- **LoRA (Low-Rank Adaptation)**: Why needed - Reduces trainable parameters from billions to millions while maintaining performance. Quick check - Confirm adapter weights are being updated during training.
- **Unsloth**: Why needed - Provides optimized training kernels for faster fine-tuning with reduced memory overhead. Quick check - Monitor training speed and GPU utilization.
- **ROUGE-L**: Why needed - Measures n-gram overlap between generated and reference text, suitable for evaluating reasoning outputs. Quick check - Validate ROUGE-L score consistency across evaluation runs.
- **Chain-of-Thought Prompting**: Why needed - Guides the model to generate step-by-step reasoning rather than direct answers. Quick check - Inspect generated outputs for logical reasoning traces.
- **Gradient Accumulation**: Why needed - Enables effective batch sizes larger than GPU memory constraints. Quick check - Verify that effective batch size matches intended value.

## Architecture Onboarding

**Component Map**: Dataset -> Tokenizer -> LLaMA-3.2-3B -> QLoRA Adapter -> Optimizer -> Evaluation

**Critical Path**: Data preprocessing → Model loading with QLoRA → Training loop → Evaluation → Model saving

**Design Tradeoffs**: QLoRA vs full fine-tuning (memory vs potential performance), 2 epochs vs longer training (speed vs convergence), ROUGE-L vs human evaluation (efficiency vs accuracy)

**Failure Signatures**: OOM errors during training, flat ROUGE-L curves, loss of reasoning structure in outputs, failure to load model due to quantization mismatches

**First Experiments**:
1. Load model with QLoRA and verify GPU memory stays under 16GB
2. Train on single batch and inspect adapter weight updates
3. Generate outputs and manually verify reasoning trace preservation

## Open Questions the Paper Calls Out
1. **Expert human evaluation**: Does expert human evaluation reveal measurable improvements in clinical reasoning accuracy that ROUGE-L failed to capture? The authors note that ROUGE-L remained unchanged while qualitative inspection showed improved interpretability, and list expert human evaluation as future work.
2. **Training duration and dataset size**: Would extended training beyond 2 epochs on larger, more diverse medical datasets yield measurable ROUGE-L improvements? The authors attribute flat scores to limited training duration and small dataset size.
3. **Retrieval-augmented generation**: Can retrieval-augmented generation (RAG) enhance factual accuracy while preserving the chain-of-thought reasoning style? Listed as future work, with RAG potentially improving factual grounding without disrupting reasoning traces.

## Limitations
- Limited training duration (2 epochs) may not achieve optimal performance
- ROUGE-L metric cannot capture semantic correctness or logical consistency of medical reasoning
- Lack of expert human evaluation to validate clinical reasoning quality improvements

## Confidence
- **High**: Resource constraints were respected; model and code were released publicly; reasoning traces were preserved qualitatively
- **Medium**: ROUGE-L score of 0.3052 is stable; the pipeline is reproducible in similar environments
- **Low**: The claim of avoiding catastrophic forgetting and maintaining reasoning transparency lacks rigorous validation

## Next Checks
1. Test the fine-tuned model on held-out medical reasoning datasets (e.g., MedQA, MedAlpaca) to quantify reasoning performance and domain transfer
2. Compare ROUGE-L and reasoning coherence with a baseline fine-tuned without QLoRA (full fine-tuning) to isolate the impact of parameter-efficient methods
3. Perform ablation studies on LoRA rank, batch size, and gradient accumulation to identify the most resource-efficient configuration without sacrificing reasoning quality