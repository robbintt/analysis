---
ver: rpa2
title: 'AutoLogi: Automated Generation of Logic Puzzles for Evaluating Reasoning Abilities
  of Large Language Models'
arxiv_id: '2502.16906'
source_url: https://arxiv.org/abs/2502.16906
tags:
- inputs
- constraint
- return
- constraints
- arrangement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AutoLogi introduces an automated method for generating open-ended
  logic puzzles to evaluate Large Language Models' reasoning abilities. The approach
  uses program-based verification and controllable difficulty levels to create a bilingual
  benchmark that mitigates random guessing and provides more reliable assessment than
  traditional multiple-choice formats.
---

# AutoLogi: Automated Generation of Logic Puzzles for Evaluating Reasoning Abilities of Large Language Models

## Quick Facts
- arXiv ID: 2502.16906
- Source URL: https://arxiv.org/abs/2502.16906
- Authors: Qin Zhu; Fei Huang; Runyu Peng; Keming Lu; Bowen Yu; Qinyuan Cheng; Xipeng Qiu; Xuanjing Huang; Junyang Lin
- Reference count: 40
- Primary result: Program-based verification enables open-ended logic puzzle evaluation with 35%-73% performance range vs 21%-37% on multiple-choice, better reflecting true LLM reasoning capabilities.

## Executive Summary
AutoLogi introduces an automated pipeline for generating open-ended logic puzzles that overcome limitations of traditional multiple-choice reasoning benchmarks. By using program-based verification instead of LLM-as-judge, the approach achieves more reliable evaluation with F1 scores of 0.96 versus 0.76 for human-aligned assessments. The method generates a bilingual benchmark (1,575 English + 883 Chinese puzzles) with controllable difficulty levels, showing wider performance distributions across eight modern LLMs that better reflect true reasoning abilities.

## Method Summary
AutoLogi operates through a three-stage pipeline: (1) Puzzle Formulation extracts background and logical constraints from source corpora using GPT-4, (2) Format & Verifiers Generation creates Python verification functions and traversal logic with cross-validation to ensure correctness, and (3) Data Augmentation adjusts puzzle difficulty via constraint addition/removal. The system generates high-quality training data through rejection sampling with verifiers, producing D_sft and D_dpo datasets that improve model performance on independent reasoning benchmarks by 4-7%.

## Key Results
- Program-based verification achieves F1 = 0.96 vs GPT-4 judge F1 = 0.76 on human-aligned samples
- Performance scores span 35% to 73% range compared to 21% to 37% on source multiple-choice dataset
- DPO training improves Qwen2.5-72b on LiveBench from 46% to 52% (+6%)
- 23% of initially generated puzzles were identified as unsolvable through cross-validation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Open-ended format with program-based verification yields more reliable LLM reasoning evaluation than multiple-choice.
- Mechanism: Requiring full solution construction eliminates 25% random-guess baseline inherent to 4-option multiple-choice; deterministic code execution replaces noisy model-based scoring.
- Core assumption: Verification functions correctly encode all logical constraints from puzzle text.
- Evidence anchors:
  - [abstract] "performance scores spanning from 35% to 73% compared to the narrower range of 21% to 37% on the source multiple-choice dataset"
  - [section 5.4] Program-based Verifier F1 = 0.96 vs LLM judge F1 = 0.76 on 90 human-verified samples
  - [corpus] ZebraLogic (arXiv:2502.01100) similarly uses constraint satisfaction for logic evaluation, supporting program-verification validity
- Break condition: If verifier code has bugs or doesn't capture edge cases, evaluation becomes unreliable (paper acknowledges ~3% error rate in Section 5.4).

### Mechanism 2
- Claim: Cross-validation between traversal and verification functions catches synthesis errors.
- Mechanism: Traversal function enumerates all valid solutions; verification function validates each. Empty solution space or syntax errors trigger regeneration. This catches LLM-generated bugs in ~23% of initial puzzles (Chinese subset).
- Core assumption: Traversal function correctly implements exhaustive search over domain space.
- Evidence anchors:
  - [section 3.2] "we propose a cross-validation method to check the correctness of LLM-generated Verifiers and Traversal Function"
  - [section 5.4] "among the 139 problems initially generated from 147 information units, 23% were identified as unsolvable"
  - [corpus] No direct corpus evidence for this specific cross-validation technique
- Break condition: If both traversal and verifier share the same conceptual error, cross-validation fails to detect it.

### Mechanism 3
- Claim: Constraint augmentation controls difficulty and improves training data quality via rejection sampling.
- Mechanism: Adding constraints reduces solution space (harder); removing constraints expands it (easier). Rejection sampling with verifiers ensures only provably-correct responses enter training data, eliminating false positives from lucky guesses.
- Core assumption: Constraint count correlates with human-perceived difficulty.
- Evidence anchors:
  - [section 3.3] "Expansion... terminates when either the maximum number of attempts is reached or the solution space size reduces to one"
  - [section 5.3] DPO training improves Qwen2.5-72b on LiveBench from 46% to 52%
  - [corpus] PHANTOM RECALL (arXiv:2510.11812) warns that models may memorize puzzle templates rather than reason—suggests augmentation may not fully address memorization risk
- Break condition: If generated constraints are semantically redundant or contradictory, difficulty control fails.

## Foundational Learning

- Concept: Constraint Satisfaction Problems (CSPs)
  - Why needed here: All AutoLogi puzzles are CSPs—objects assigned values subject to logical constraints. Understanding backtracking search and constraint propagation helps debug traversal functions.
  - Quick check question: Given variables {A, B, C} with domains {1, 2} and constraints "A ≠ B" and "B ≠ C," how many valid solutions exist?

- Concept: Rejection Sampling with Verifiers
  - Why needed here: Training data generation depends on this pattern. Without understanding why correct answers ≠ correct reasoning, you may miss the point of program-based verification.
  - Quick check question: Why might a model produce a correct answer through flawed reasoning, and how does a verifier catch this?

- Concept: LLM-as-Judge Limitations
  - Why needed here: The paper explicitly positions program verification as superior to LLM-as-judge. Understanding where LLM judges fail (complex logic, long contexts) contextualizes this design choice.
  - Quick check question: What types of reasoning errors might an LLM judge miss that a program verifier would catch?

## Architecture Onboarding

- Component map: Puzzle Formulation -> Verifier Generation -> Augmentation -> Training Pipeline
- Critical path: Verifier correctness → evaluation validity → training data quality. A buggy verifier corrupts both benchmark scores and training labels.
- Design tradeoffs:
  - Exhaustive traversal guarantees correctness but becomes computationally intractable for large domain spaces (paper limits to ~7 objects with binary values)
  - Relying on LLMs (GPT-4) for verifier generation introduces dependency on proprietary models; paper acknowledges failure risk with weaker LLMs (Section: Limitations)
- Failure signatures:
  - Empty solution space after constraint addition → verifier or traversal bug
  - High variance across evaluation runs → verifier non-determinism or prompt instability
  - Training degradation despite verified data → verifier accepting incorrect solutions (false positives)
- First 3 experiments:
  1. Reproduce the human alignment experiment (Section 5.4): Sample 50 puzzles, run Claude-3.5-Sonnet, compare program verifier vs GPT-4 judge against human ground truth. Target: verifier F1 > 0.90.
  2. Ablate cross-validation: Generate 100 puzzles with/without traversal validation, measure unsolvable puzzle rate. Expect ~20% increase in invalid puzzles without validation.
  3. Difficulty calibration test: Evaluate a single model (e.g., Qwen2.5-7B) across puzzles binned by constraint count (2, 4, 6 constraints). Expect monotonically decreasing accuracy.

## Open Questions the Paper Calls Out

- Can hybrid approaches combining LLMs with formal methods (e.g., SMT solvers) eliminate the residual 3% error rate in program-based verification?
  - Basis: [explicit] The authors explicitly state in the Limitations section that "about 3% of validation results contain errors" due to the imperfection of LLM-generated verification functions.
  - Why unresolved: The current method relies solely on LLMs (GPT-4/GPT-4o) to generate Python verifiers, which can hallucinate constraints or fail to check edge cases.
  - What evidence would resolve it: A comparative study showing that integrating a formal solver into the pipeline reduces the verification False Positive/Negative rate to near zero.

- Can the synthesis pipeline be decoupled from frontier models like GPT-4 to allow smaller, open-source models to generate high-quality logic puzzles autonomously?
  - Basis: [explicit] The paper lists "Dependence on LLMs" as a primary limitation, noting that the method "may suffer from a high failure rate" if only limited-capacity models are accessible.
  - Why unresolved: The current three-stage pipeline requires advanced instruction-following and code-generation capabilities found primarily in proprietary models.
  - What evidence would resolve it: Demonstrating that a 7B or 13B parameter model can execute the full synthesis pipeline with comparable puzzle validity and diversity metrics.

- Does increasing the number of logical constraints accurately proxy for reasoning difficulty, or does it primarily increase search space complexity?
  - Basis: [inferred] The paper controls difficulty via "Expansion" (adding constraints) and analyzes performance degradation. However, the error analysis (Appendix A.2) suggests failures often stem from "Incorrect Logical Inference" (e.g., misinterpreting negations) rather than the sheer volume of constraints.
  - Why unresolved: It is unclear if high scores on dense puzzles reflect superior logical deduction or simply better error-free execution of many simple steps.
  - What evidence would resolve it: An ablation study controlling for "logical complexity" (e.g., nested negations) separately from "constraint density" to see which factor better predicts model failure.

## Limitations
- Reliance on GPT-4 for generating verification functions and traversal logic creates dependency on proprietary models
- Cross-validation mechanism may not catch shared conceptual errors between traversal and verification functions
- Assumption that constraint count directly correlates with difficulty lacks rigorous empirical validation against human judgment

## Confidence

- **High Confidence**: The core mechanism of using program-based verification to replace LLM-as-judge (F1 improvement from 0.76 to 0.96) is well-supported by experimental evidence. The wider performance distribution (35%-73% vs 21%-37%) convincingly demonstrates the value of open-ended evaluation.

- **Medium Confidence**: The cross-validation between traversal and verification functions appears theoretically sound and addresses a real problem (23% initial failure rate), but the method's effectiveness depends on the traversal function being exhaustively correct.

- **Low Confidence**: The difficulty calibration via constraint augmentation is reasonable but not rigorously validated against human judgment of puzzle difficulty. The assumption that more constraints always means harder puzzles may not hold for all logical structures.

## Next Checks

1. **Human Alignment Verification**: Sample 50 AutoLogi puzzles and have 3 human experts solve them without model assistance. Compare human accuracy distribution to LLM performance (35%-73%) to validate that the benchmark appropriately challenges human-level reasoning.

2. **Memorization Resistance Test**: Train a model on AutoLogi training data, then evaluate it on a held-out test set where puzzle templates are systematically varied (e.g., object names changed, constraint order shuffled). Measure performance drop to assess whether the model learned reasoning patterns or memorized specific puzzles.

3. **Verification Function Robustness**: Introduce subtle logical bugs into a subset of AutoLogi verifiers (e.g., off-by-one errors, incorrect inequality operators) and measure how often these go undetected by the traversal validation. This would quantify the false negative rate of the cross-validation system.