---
ver: rpa2
title: Adapting Lightweight Vision Language Models for Radiological Visual Question
  Answering
arxiv_id: '2506.14451'
source_url: https://arxiv.org/abs/2506.14451
tags:
- image
- https
- saliency
- evaluation
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of adapting lightweight vision-language
  models for radiological visual question answering, where domain-specific data scarcity
  and complex medical imagery hinder model performance. The authors propose a two-stage
  fine-tuning pipeline with synthetic question-answer generation and dataset annealing,
  using a 3B parameter PaliGemma model and parameter-efficient LoRA adaptation on
  curated radiological datasets (SLAKE, ROCO v2.0, MedPix v2.0).
---

# Adapting Lightweight Vision Language Models for Radiological Visual Question Answering

## Quick Facts
- arXiv ID: 2506.14451
- Source URL: https://arxiv.org/abs/2506.14451
- Authors: Aditya Shourya; Michel Dumontier; Chang Sun
- Reference count: 40
- Primary result: 3B parameter PaliGemma achieves up to 79% accuracy on SLAKE radiological VQA

## Executive Summary
This paper addresses the challenge of adapting lightweight vision-language models for radiological visual question answering, where domain-specific data scarcity and complex medical imagery hinder model performance. The authors propose a two-stage fine-tuning pipeline with synthetic question-answer generation and dataset annealing, using a 3B parameter PaliGemma model and parameter-efficient LoRA adaptation on curated radiological datasets (SLAKE, ROCO v2.0, MedPix v2.0). Despite operating at a fraction of the scale of larger models like LLaVA-Med, their approach achieves competitive accuracy on both open- and closed-ended radiological QA tasks (up to 79% on SLAKE, 41% on annealed ROCO+MedPix). They also introduce a lightweight saliency-based diagnostic tool for interpretability, enabling domain experts to inspect model predictions and identify failure modes.

## Method Summary
The authors propose a two-stage fine-tuning pipeline for adapting a 3B parameter PaliGemma model to radiological visual question answering. Stage 1 involves training only the projection head (a single linear layer) while freezing all other parameters, using the SLAKE dataset to align visual features with medical vocabulary. Stage 2 applies LoRA parameter-efficient fine-tuning to the attention heads of both the vision tower and LLM, using an annealed dataset combining ROCO v2.0 with MedPix v2.0 for higher-quality supervision. The approach also includes synthetic question-answer generation using LLaMA-8B from image-caption pairs and introduces a lightweight attention-based saliency diagnostic tool for interpretability.

## Key Results
- Achieves up to 79% accuracy on the SLAKE dataset for closed-ended questions
- Reaches 41% accuracy on annealed ROCO+MedPix dataset
- Outperforms larger models like LLaVA-Med on specific radiological VQA tasks despite using only 3B parameters
- Demonstrates effectiveness of dataset annealing with high-quality MedPix cases improving performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A two-stage fine-tuning process, beginning with projection-head-only alignment, establishes foundational visual-textual mapping before full-model reasoning adaptation.
- Mechanism: In Stage 1, a single linear layer (the projection head) is trained to map features from a frozen, pre-trained vision tower to the token space of a frozen LLM using a curated, semantically-labeled dataset (SLAKE). This creates a domain-aligned embedding space. In Stage 2, parameter-efficient LoRA is applied to the attention heads of both the vision tower and LLM, allowing the entire model to adapt its reasoning to the medical domain without full parameter updates.
- Core assumption: The pre-trained vision tower and LLM possess sufficient general representational power; the primary bottleneck for medical tasks is the initial misalignment between their feature spaces.
- Evidence anchors: [abstract] "We propose a two-stage fine-tuning pipeline with synthetic question-answer generation and dataset annealing..."; [section 4.4] "In this initial phase, we fine-tune only the projection head of the model while keeping all other parameters frozen. We train the projection layer for 5 epochs on SLAKE... aligning with curriculum learning principles."

### Mechanism 2
- Claim: Dataset annealing with a high-quality, semantically enriched corpus improves model generalization more efficiently than training on a larger, noisier dataset alone.
- Mechanism: A smaller, high-quality dataset (MedPix v2.0) is mixed into a larger, general radiological corpus (ROCO v2.0). This increases the density of high-quality supervision signals (clinical reasoning, case-based contexts), helping the model learn robust patterns that might be obscured by noise or simpler artifacts in the larger dataset. This is particularly effective for smaller models which are more susceptible to noise.
- Core assumption: The smaller annealing dataset is genuinely higher-quality and its reasoning patterns are representative of the desired model behavior.
- Evidence anchors: [abstract] "...pipeline from synthetic question-answer pair generation to multi-stage fine-tuning... Our results show that despite operating at a fraction of the scale... our model achieves promising performance..."; [section 4.3] "Annealing improves model performance by incrementally incorporating small, high-quality subsets into a larger training set. ...small or mid-sized models, such as our 4B parameter VLM, are receptive to annealing."

### Mechanism 3
- Claim: A lightweight, attention-based saliency diagnostic enables expert identification of model failure modes by visualizing cross-modal reasoning pathways.
- Mechanism: The tool visualizes attention weights from the LLM's attention heads. By using "rollout attention," it recursively aggregates attention weights across all layers to show how information flows from input image patches to output tokens and vice-versa. This allows experts to see if the model's prediction is based on a clinically relevant region (e.g., a fracture site) or a spurious correlation.
- Core assumption: The aggregated attention weights meaningfully correlate with the model's reasoning process and are interpretable by domain experts.
- Evidence anchors: [abstract] "We introduce a lightweight saliency-based diagnostic tool that enables domain experts to inspect VQA model performance and identify ill-conditioned failure modes through saliency analysis."; [section 3.2] "Although saliency is not the same as explainability, experts can often identify diagnostic indicators, as saliency is fundamentally tied to the learned weights of the model."

## Foundational Learning

- Concept: **Vision-Language Model (VLM) Architecture**
  - Why needed here: The proposed method is a training recipe for a specific VLM architecture. Understanding the components (Vision Tower, Projection Head, LLM) is essential to understand *what* is being fine-tuned in each stage.
  - Quick check question: In the PaliGemma architecture used, what is the specific role of the single linear layer in the projection head, and which component's weights are frozen during the first stage of fine-tuning?

- Concept: **Parameter-Efficient Fine-Tuning (PEFT) & LoRA**
  - Why needed here: The paper's central claim of a "lightweight" and "cost-effective" approach relies on using LoRA to adapt a 3B-parameter model. One must understand that LoRA allows adaptation without updating all parameters.
  - Quick check question: According to the paper, which components of the VLM are targeted by LoRA adapters during the second stage of fine-tuning?

- Concept: **Attention Saliency vs. Explainability**
  - Why needed here: The paper introduces a diagnostic tool based on attention. It's critical to understand its limitation: saliency shows where the model "looked," but does not prove *why* it made a decision, which is a key distinction for clinical validation.
  - Quick check question: The authors state that "saliency is not the same as explainability." Based on the text, what is the primary function of their saliency diagnostic tool for a domain expert?

## Architecture Onboarding

- Component map:
  - **Base VLM:** PaliGemma-mix-448 (3B params total).
    - **Vision Tower:** SigLIP (400M params) - Encodes images into feature vectors independent of text.
    - **Projection Head:** Single Linear Layer - Aligns vision features to the LLM's token space.
    - **LLM:** Gemma-2B (2B params) - Generates text based on joint visual and textual input.
  - **Training Pipeline:** Two-stage fine-tuning + Synthetic QA Generation + Dataset Annealing.
  - **Diagnostic Tool:** Lightweight, attention-based saliency visualizer (raw & rollout attention).

- Critical path: The **two-stage fine-tuning pipeline** is the most critical path for model performance.
  1.  **Stage 1 (Alignment):** Train only the **Projection Head** on SLAKE to map visual concepts to medical vocabulary.
  2.  **Stage 2 (Specialization):** Apply **LoRA** to the vision tower and LLM attention heads using an annealed mix of ROCO+MedPix to build clinical reasoning.

- Design tradeoffs:
  - **Scale vs. Curation:** The paper argues that a small model (3B) with a highly curated, annealed training pipeline can be competitive with much larger models (e.g., LLaVA-Med). The tradeoff is increased effort in data generation and curation.
  - **Interpretability vs. Performance:** The saliency diagnostic is a separate module. While it adds interpretability, it does not improve performance and requires expert evaluation to be meaningful.
  - **Cost vs. Generality:** The pipeline is designed for low-resource specialization. The tradeoff is potentially weaker performance on out-of-distribution tasks compared to a larger, more general model.

- Failure signatures:
  - **Inconsistent anatomical recognition:** Model misidentifies organs. This suggests a failure in **Stage 1** alignment.
  - **Robust but incorrect reasoning:** Model confidently answers using plausible but wrong clinical logic. This suggests a failure in **Stage 2** data quality (e.g., noisy annealing set).
  - **Saliency maps highlight irrelevant regions:** The diagnostic tool reveals the model is using visual shortcuts. This indicates a need for better data curation or annealing to focus learning on meaningful features.

- First 3 experiments:
  1.  **Reproduce Ablation Study:** Implement the two-stage fine-tuning (with Stage 1 vs. without Stage 1) on the SLAKE dataset to verify the performance gain from the projection-head alignment step.
  2.  **Annealing Sensitivity Analysis:** Train models on ROCO v2.0 with varying percentages of MedPix v2.0 data (e.g., 0%, 10%, 25%, 50%) to measure the impact of the annealing ratio on evaluation loss and accuracy.
  3.  **Diagnostic Tool Validation:** Use the saliency tool on a set of known failure cases to confirm it visually highlights the image regions contributing to the incorrect answers, as claimed by the authors.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the lightweight model perform in multi-turn clinical dialogue scenarios compared to the evaluated single-turn QA tasks?
- Basis in paper: [explicit] The authors state in the conclusion that "our evaluation framework focused only on single-turn QA, whereas real-world clinical workflows involve multi-turn interactions."
- Why unresolved: The current study only validates static question-answer pairs, failing to measure the model's ability to maintain context or reasoning consistency over a multi-turn conversation typical of clinical practice.
- What evidence would resolve it: Evaluation results on a multi-turn radiological dialogue benchmark or a user study involving interactive clinical sessions.

### Open Question 2
- Question: Do domain experts verify the fine-grained anatomical and pathological relevance of the model's saliency maps?
- Basis in paper: [explicit] The paper notes that "saliency analysis was conducted without expert involvement" and suggests "Future work should involve clinicians to evaluate fine-grained anatomical and pathological relevance."
- Why unresolved: The current diagnostic validation was performed by the authors on broad organ-level cases, leaving the clinical utility of the interpretability tool for specific pathologies unconfirmed.
- What evidence would resolve it: A qualitative or quantitative study where radiologists assess whether the attention heatmaps accurately highlight clinically relevant regions for diagnosis.

### Open Question 3
- Question: To what extent does the scaling of the vision encoder contribute to overall performance relative to the LLM scaling investigated in this study?
- Basis in paper: [explicit] The authors list as a limitation: "our ablation analysis focused primarily on LLM scaling, with a limited investigation into the vision encoder."
- Why unresolved: While the study demonstrates the efficacy of adapting a lightweight LLM (Gemma 2B), it leaves the impact of the vision tower's capacity (SigLIP 400M) on domain adaptation unexplored.
- What evidence would resolve it: Ablation experiments swapping the SigLIP vision tower for larger or domain-specific encoders while keeping the language model constant.

## Limitations

- The diagnostic tool's utility depends heavily on expert interpretation without systematic validation of expert reliability
- The study lacks evaluation of the model's performance in multi-turn clinical dialogue scenarios
- The ablation analysis focused primarily on LLM scaling while leaving vision encoder capacity largely unexplored

## Confidence

- **Two-stage fine-tuning pipeline**: Medium confidence - The ablation study shows clear performance gains, but generalizability beyond specific datasets remains uncertain
- **Dataset annealing benefits**: Medium confidence - Performance improvements are demonstrated, but relative contribution versus other factors is not isolated
- **Diagnostic tool utility**: Low confidence - While described and applied, minimal evidence of systematic validation with domain experts
- **Lightweight competitiveness claim**: Medium confidence - Competitive results on specific datasets, but broader benchmark performance unconfirmed

## Next Checks

1. **Cross-dataset validation of the two-stage approach**: Apply the exact two-stage fine-tuning pipeline (with and without Stage 1) to a different medical VQA dataset (e.g., VQA-RAD or SLAKE subset with different splits) to test whether the projection-head-first mechanism generalizes beyond the original experimental setup.

2. **Controlled annealing experiment**: Train identical models on ROCO v2.0 alone, MedPix v2.0 alone, and various annealing ratios (0%, 25%, 50%, 75% MedPix). Measure not just final accuracy but also learning curves to quantify how quickly the annealed models converge versus the baseline, and perform statistical significance testing on performance differences.

3. **Expert validation study for the saliency tool**: Recruit 3-5 radiologists to use the saliency tool on a held-out set of 50 model predictions (25 correct, 25 incorrect). Measure inter-rater reliability for whether they can correctly identify failure modes from the visualizations, and compare their diagnostic accuracy against a baseline where they only see the image and question without saliency maps.