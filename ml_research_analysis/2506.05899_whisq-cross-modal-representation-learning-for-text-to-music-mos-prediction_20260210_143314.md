---
ver: rpa2
title: 'WhisQ: Cross-Modal Representation Learning for Text-to-Music MOS Prediction'
arxiv_id: '2506.05899'
source_url: https://arxiv.org/abs/2506.05899
tags:
- whisq
- audio
- arxiv
- transport
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: WhisQ introduces a multimodal architecture for text-to-music Mean
  Opinion Score (MOS) prediction, addressing the dual challenge of assessing overall
  musical quality and text-prompt alignment. The method employs Whisper-Base for audio
  encoding and Qwen-3 for text encoding, maintaining sequence structure for fine-grained
  cross-modal modeling.
---

# WhisQ: Cross-Modal Representation Learning for Text-to-Music MOS Prediction

## Quick Facts
- arXiv ID: 2506.05899
- Source URL: https://arxiv.org/abs/2506.05899
- Reference count: 25
- Primary result: 7% improvement in SRCC for OMQ and 14% improvement for TA prediction on MusicEval Track-1 dataset

## Executive Summary
WhisQ introduces a multimodal architecture for text-to-music Mean Opinion Score (MOS) prediction, addressing the dual challenge of assessing overall musical quality and text-prompt alignment. The method employs Whisper-Base for audio encoding and Qwen-3 for text encoding, maintaining sequence structure for fine-grained cross-modal modeling. It features specialized prediction pathways using sequence co-attention and Sinkhorn optimal transport regularization to enforce semantic alignment in the shared embedding space. On the MusicEval Track-1 dataset, WhisQ achieves a 7% improvement in Spearman correlation for overall musical quality and a 14% improvement for text alignment compared to the baseline.

## Method Summary
WhisQ uses frozen Whisper-Base and Qwen-3 encoders with 4-head bidirectional sequence co-attention to fuse audio and text representations. The architecture has task-specific prediction pathways: OMQ uses pooled audio features alone, while TA uses concatenated co-attended representations. Training employs a dual objective combining Huber loss with Sinkhorn optimal transport regularization. The model is trained on MusicEval Track-1 data with audio resampled to 16 kHz and padded to 3000 log-mel frames, using only 2.66M trainable parameters out of 617M total.

## Key Results
- 7% improvement in Spearman correlation for Overall Musical Quality (OMQ) prediction
- 14% improvement in Spearman correlation for Text Alignment (TA) prediction
- Optimal transport regularization provides largest performance gain at 10% SRCC improvement
- Achieves 0.6823 utterance-level SRCC for OMQ and 0.6109 for TA

## Why This Works (Mechanism)

### Mechanism 1: Bidirectional Sequence Co-Attention for Fine-Grained Cross-Modal Correspondence
- **Claim:** Maintaining sequence structure enables fine-grained audio-text alignment that improves textual alignment prediction.
- **Mechanism:** 4-head multi-head attention computes bidirectional attended representations at token/frame level before pooling, preserving temporal correspondence signals.
- **Core assumption:** Text-to-music alignment manifests at the subsequence level, not just global semantics.
- **Evidence anchors:** Abstract mentions "maintaining sequence structure for fine-grained cross-modal modeling"; equations 3-5 define bidirectional co-attention.

### Mechanism 2: Sinkhorn Optimal Transport as Cross-Modal Alignment Regularizer
- **Claim:** Explicit distributional matching between audio and text embeddings via optimal transport provides the largest single performance gain (10% SRCC improvement).
- **Mechanism:** Sinkhorn loss computes entropy-regularized optimal transport distance between audio and text embedding distributions during training.
- **Core assumption:** Audio and text encoder outputs inhabit different regions of shared space; explicit distributional matching is required beyond attention-weighted mixing.
- **Evidence anchors:** Abstract states "optimal transport regularization provides the largest performance gain (10% SRCC improvement)"; ablation shows TA SRCC drops from 0.6109 to 0.4895 without OT.

### Mechanism 3: Task-Decoupled Prediction Pathways
- **Claim:** OMQ and TA benefit from structurally different feature aggregation strategies.
- **Mechanism:** OMQ uses pooled audio features alone; TA uses concatenated co-attended representations.
- **Core assumption:** OMQ is largely independent of text while TA explicitly requires cross-modal comparison.
- **Evidence anchors:** Equations 6-7 show distinct input paths and separate MLP architectures; problem formulation distinguishes intrinsic quality vs. prompt consistency.

## Foundational Learning

- **Concept: Optimal Transport and Sinkhorn Distance**
  - **Why needed here:** The Sinkhorn loss is the single largest contributor to performance.
  - **Quick check question:** Can you explain why Sinkhorn distance approximates Wasserstein distance, and what the blur parameter (0.05) controls in entropy regularization?

- **Concept: Multi-Head Co-Attention vs Cross-Attention**
  - **Why needed here:** The paper distinguishes between "sequence co-attention" (bidirectional), "vanilla co-attention," and "cross-attention" with different performance results.
  - **Quick check question:** What is the difference between cross-attention (one modality queries the other) and bidirectional co-attention (both modalities query each other)?

- **Concept: MOS Prediction as Regression with Correlation Metrics**
  - **Why needed here:** The paper reports SRCC, LCC, KTAU, and MSE. Understanding why correlation metrics matter more than MSE for subjective quality assessment is essential.
  - **Quick check question:** Why might a model with lower MSE still have worse SRCC than another model?

## Architecture Onboarding

- **Component map:**
Audio (16kHz, 3000 log-mel frames) -> Whisper-Base Encoder (frozen) -> Ha ∈ R^(Ta×512)
Text Prompt -> Qwen-3-0.6B (frozen) -> Linear Projection -> Ht ∈ R^(Tt×512)
Ha, Ht -> 4-Head Co-Attention -> Hatt_a, Hatt_t
OMQ Branch: MeanPool(Ha) -> MLP_OMQ (512→256→64→1)
TA Branch: [Pool(Hatt_a); Pool(Hatt_t)] -> MLP_TA (1024→256→64→1)
Loss: Huber(ŷ, y*) + λ·Sinkhorn(Ha, Ht)

- **Critical path:** Audio → Whisper encoder → Ha → [co-attention] → pooled representation → MLP → MOS score. The co-attention module and OT loss are the only novel components; backbones are frozen.

- **Design tradeoffs:**
  - Frozen backbones (617M params, only 2.66M trainable) → faster training, lower compute, but prevents end-to-end adaptation to music domain
  - Sinkhorn OT regularization → strong alignment gains, but adds computational overhead and hyperparameter sensitivity
  - Sequence-level attention → fine-grained alignment, but requires more memory than utterance-level pooling

- **Failure signatures:**
  - TA SRCC drops significantly without OT → check if λ is too low or Sinkhorn implementation is incorrect
  - OMQ underperforms baseline → verify audio is correctly resampled to 16kHz and padded/truncated to 3000 frames
  - Negative SRCC (as with Wav2Vec2 + ModernBERT) → encoder outputs may be in incompatible embedding spaces

- **First 3 experiments:**
  1. **Reproduce ablation:** Train WhisQ w/ OT vs w/o OT on validation split. Confirm ~10% SRCC gap on TA.
  2. **Attention variant comparison:** Replace 4-head co-attention with vanilla co-attention and cross-attention.
  3. **OT hyperparameter sweep:** Test λ ∈ {1e-6, 1e-5, 4e-5, 1e-4} and blur ∈ {0.01, 0.05, 0.1}.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do alternative alignment objectives, such as Gromov-Wasserstein distance or contrastive learning, offer superior cross-modal alignment compared to the Sinkhorn optimal transport used in WhisQ?
- **Basis in paper:** The conclusion suggests exploring "different optimal transport variants (e.g., Gromov-Wasserstein) or contrastive learning approaches" to further enhance alignment.
- **Why unresolved:** The current study isolates Sinkhorn optimal transport as the primary alignment mechanism; other alignment strategies remain untested.
- **What evidence would resolve it:** Comparative benchmarks on MusicEval dataset using Gromov-Wasserstein or contrastive losses versus Sinkhorn baseline.

### Open Question 2
- **Question:** To what extent does WhisQ generalize to broader audio quality datasets and related tasks like text-to-speech (TTS) evaluation?
- **Basis in paper:** The authors explicitly state the importance of "investigating WhisQ's generalization," including evaluation on diverse audio datasets and adaptation to TTS.
- **Why unresolved:** Current experiments are restricted to MusicEval Track-1 text-to-music dataset.
- **What evidence would resolve it:** Evaluation of trained model's performance on out-of-domain TTS or general audio-quality datasets without retraining.

### Open Question 3
- **Question:** Does end-to-end fine-tuning of the Whisper and Qwen backbones yield performance gains that outweigh the associated computational costs?
- **Basis in paper:** The paper identifies reliance on frozen backbones as a limitation that "prevents end-to-end fine-tuning that could potentially unlock further performance gains."
- **Why unresolved:** Model is currently designed with frozen encoders to reduce computational footprint.
- **What evidence would resolve it:** Ablation study comparing performance and training cost of frozen vs. fully fine-tuned backbones.

## Limitations
- Architecture sensitivity to OT regularization parameter λ may limit generalizability of the 10% SRCC improvement
- Domain specificity to MusicEval Track-1 dataset raises questions about generalization to different TTM models and musical genres
- Frozen encoders prevent end-to-end adaptation to music domain, potentially limiting performance ceiling

## Confidence
- **High Confidence:** Ablation demonstrating OT regularization provides largest performance gain (10% SRCC improvement)
- **Medium Confidence:** Bidirectional sequence co-attention mechanism's effectiveness based on related work
- **Medium Confidence:** Task-decoupled prediction pathways are logically consistent but have weaker empirical validation

## Next Checks
1. **OT Hyperparameter Sensitivity Analysis:** Systematically vary λ across three orders of magnitude and blur parameters while monitoring both OMQ and TA performance to establish robustness of the 10% gain.

2. **Cross-Domain Generalization Test:** Evaluate WhisQ on independent text-to-music MOS dataset with different TTM models or musical styles to verify the 7% and 14% improvements hold beyond MusicEval Track-1 corpus.

3. **Fine-tuning vs. Frozen Encoders:** Train variant where either Whisper-Base or Qen-3 (or both) are fine-tuned on MOS prediction task and compare performance to frozen architecture.