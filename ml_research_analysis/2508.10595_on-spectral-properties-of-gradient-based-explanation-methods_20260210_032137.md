---
ver: rpa2
title: On Spectral Properties of Gradient-based Explanation Methods
arxiv_id: '2508.10595'
source_url: https://arxiv.org/abs/2508.10595
tags:
- arxiv
- perturbation
- explanation
- spectral
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a spectral analysis of gradient-based explanation
  methods for deep neural networks, revealing a pervasive spectral bias in gradient
  explanations. The authors introduce a novel probabilistic framework to analyze how
  gradient operators and perturbations interact to create band-pass filters in the
  frequency domain.
---

# On Spectral Properties of Gradient-based Explanation Methods

## Quick Facts
- arXiv ID: 2508.10595
- Source URL: https://arxiv.org/abs/2508.10595
- Reference count: 40
- This paper provides a spectral analysis of gradient-based explanation methods for deep neural networks, revealing a pervasive spectral bias in gradient explanations.

## Executive Summary
This paper provides a spectral analysis of gradient-based explanation methods for deep neural networks, revealing a pervasive spectral bias in gradient explanations. The authors introduce a novel probabilistic framework to analyze how gradient operators and perturbations interact to create band-pass filters in the frequency domain. They identify inconsistencies in explanation methods due to frequency-dependent attribution and propose two solutions: (1) determining an optimal perturbation scale based on cosine similarity between the perturbation kernel and classifier's power spectral density, and (2) SpectralLens, an aggregation method that combines information across frequency bands. Experimental evaluations using pixel removal strategies show that SpectralLens improves robustness against inconsistencies while maintaining or enhancing quantitative performance metrics on ImageNet and Food101 datasets.

## Method Summary
The paper proposes two methods to address spectral inconsistencies in gradient-based explanations. The first method determines an optimal perturbation scale σ by maximizing the cosine similarity between the classifier's power spectral density and the perturbation kernel's power spectral density in the frequency domain. The second method, SpectralLens, aggregates explanations across multiple perturbation scales using a uniform prior. Both methods build on SmoothGrad-Squared, which computes the expected squared gradient under Gaussian perturbations. The optimal scale selection involves estimating the power spectral density of the classifier and finding the noise level that best aligns with it, while SpectralLens combines information from multiple scales to provide more robust attributions.

## Key Results
- Spectral analysis reveals gradient operators act as high-pass filters while perturbations act as low-pass filters, creating a band-pass effect in explanations
- Cosine similarity-based optimal scale selection significantly improves deletion/insertion scores (0.465 vs 0.212 for SG2_Opt vs standard SG)
- SpectralLens aggregation method enhances robustness against hyperparameter sensitivity while maintaining or improving quantitative performance
- Theoretical justification for using squared gradients over vanilla gradients due to reduced phase sensitivity and better spectral properties

## Why This Works (Mechanism)

### Mechanism 1: Spectral Filtering (Band-Pass Effect)
Gradient-based explanation methods with perturbation act as band-pass filters, where gradients amplify high frequencies and perturbations dampen them. The paper proves that the gradient operator scales with frequency (∥ω∥²), functioning as a high-pass filter (Theorem 2), which explains the noisy nature of VanillaGrad. Conversely, any perturbation kernel (e.g., Gaussian noise) acts as a low-pass filter (Theorem 1), attenuating high frequencies. The interaction creates a band-pass filter ∥ω∥²e^(-8π²σ²∥ω∥²) (Eq. 4), meaning the explanation only captures features within a specific frequency range determined by the hyperparameter σ.

### Mechanism 2: Spectral Alignment via Cosine Similarity
Determining the optimal perturbation scale by maximizing cosine similarity between the classifier and kernel in the frequency domain improves explanation consistency. Squared gradients can be expressed as an inner product of the classifier's Power Spectral Density (PSD) Sf and the kernel's PSD S√p (Eq. 3). The paper proposes finding the scale σ that maximizes the cosine similarity (Eq. 6) between these two spectra. This aligns the "band-pass filter" with the frequencies most relevant to the classifier, reducing arbitrary results from manual hyperparameter tuning.

### Mechanism 3: Squared Gradient Explainers
Squared gradients (▽f)² provide more reliable attributions than vanilla gradients because they extract information about the Power Spectral Density without complex phase dependencies. Vanilla gradients (Eq. 2) extract asymmetric information (odd parts) and suffer from complex symmetry issues (Remark 1), making them sensitive to phase shifts caused by perturbations (Remark 5). Squared gradients (Eq. 3) rely on the auto-correlation of the gradient signal, resulting in a real-valued, non-negative representation that aggregates the PSD Sf directly, avoiding destructive interference from phase mismatches.

## Foundational Learning

- **Concept: Fourier Transform (FT) and Power Spectral Density (PSD)**
  - Why needed here: The paper models explanations entirely in the frequency domain. You must understand that multiplication in the spatial domain (perturbation) becomes convolution/filtering in the frequency domain.
  - Quick check question: How does a Gaussian function in the spatial domain behave in the frequency domain?

- **Concept: Signal Processing Filters (High-pass vs. Low-pass)**
  - Why needed here: The core diagnosis of the paper is that gradients act as high-pass filters (noise) and perturbations act as low-pass filters (smoothing).
  - Quick check question: If you apply a high-pass filter to a step function, what kind of artifacts (ringing/overshoot) appear in the output?

- **Concept: Inner Product & Cosine Similarity**
  - Why needed here: The proposed solution for fixing hyperparameters relies on measuring the alignment (angle) between two vectors in the frequency domain (Eq. 6).
  - Quick check question: Why is cosine similarity preferred over Euclidean distance when comparing the "shape" of two frequency distributions regardless of their magnitude?

## Architecture Onboarding

- **Component map:**
  Input -> Sampler (Perturbs with Kernel p(x̃)) -> Explainer (Computes E_p(x̃)[(▽f(x̃))²]) -> Spectral Analyzer (Estimates Sf and S√p) -> Optimizer (Maximizes Cosine Similarity) -> Aggregator (SpectralLens)

- **Critical path:**
  1. Implement the squared gradient expectation (Eq. 3)
  2. Implement the frequency-domain cosine similarity (Eq. 6) to find the optimal noise scale
  3. Compare single-scale vs. aggregated (SpectralLens) outputs

- **Design tradeoffs:**
  - Vanilla vs. Squared: Squared is robust but loses directionality (sign)
  - Fixed σ vs. Optimal σ: Fixed is faster; Optimal requires spectral estimation (computationally heavier)
  - SpectralLens: Most robust but requires calculating explanations for multiple noise scales, increasing runtime significantly

- **Failure signatures:**
  - Noisy explanations: σ is too small (High-pass dominance)
  - Overly smooth/blank explanations: σ is too large (Low-pass dominance)
  - Inconsistent rankings: Rashomon effect observed when changing σ slightly (Proposition 1)

- **First 3 experiments:**
  1. Verify Spectral Bias: Generate explanations for a single image using VanillaGrad vs. SmoothGrad-Squared and visualize the noise reduction
  2. Optimal Scale Check: Plot the Cosine Similarity (Eq. 6) against increasing noise scale σ to see if a clear peak exists
  3. Robustness Test: Run deletion/insertion benchmarks comparing standard SmoothGrad vs. the proposed SpectralLens (SL2) to verify quantitative improvement

## Open Questions the Paper Calls Out

### Open Question 1
How do the spectral properties of explanation methods interact with the pixel removal evaluation strategy, and can this analysis yield improved evaluation metrics? The paper states in Section 5, "The spectral analysis of pixel removal strategy and possible solutions are deferred to future research." The authors note that evaluation methods (like pixel removal) might suffer from their own spectral biases, but they did not formalize this interaction in the current work.

### Open Question 2
Can the proposed spectral framework be extended to function-space perturbation methods (e.g., NoiseGrad, SAM) to unify them with input-space methods? The authors acknowledge in Section 6 that while their focus is on input-space perturbations, "we defer this aspect [function space perturbation] to future investigations." The current mathematical formulation relies on input-space perturbations; extending it to function-space requires bridging the gap between input signals and model weight distributions.

### Open Question 3
How can high-frequency attributions be standardized to ensure generalizability across different data points? Section 5 notes that "Identifying generalizable attributions across images remains a subject for future investigation" because high-frequency regions in one image may not align with those in another. The current framework defines frequency locally per pixel relative to the sample, making dataset-level comparisons of "robustness" difficult.

## Limitations
- The paper's analysis relies on local linearity assumptions that may not hold for highly non-linear models at the perturbation scale
- The unimodal PSD assumption for classifiers is not rigorously validated across diverse architectures
- Computational cost of SpectralLens (requiring multiple explanations across scales) may limit practical adoption
- The theoretical framework focuses on squared gradients, leaving vanilla gradient behavior less well-characterized

## Confidence

- **High confidence**: The spectral filtering mechanism (band-pass effect) and its mathematical derivation (Theorems 1-2)
- **Medium confidence**: The cosine similarity alignment method for optimal scale selection, though practical implementation details are sparse
- **Medium confidence**: The superiority of squared gradients over vanilla gradients for explanation quality, based on empirical results

## Next Checks

1. **Phase sensitivity test**: Validate whether vanilla gradients consistently show destructive interference patterns across different perturbation phases, confirming Remark 5
2. **PSD shape validation**: Empirically verify the unimodal PSD assumption for multiple classifier architectures by computing and visualizing actual power spectral densities
3. **Runtime efficiency analysis**: Measure the actual computational overhead of SpectralLens compared to single-scale methods to assess practical feasibility tradeoffs