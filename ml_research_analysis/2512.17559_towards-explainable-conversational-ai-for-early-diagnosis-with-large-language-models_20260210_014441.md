---
ver: rpa2
title: Towards Explainable Conversational AI for Early Diagnosis with Large Language
  Models
arxiv_id: '2512.17559'
source_url: https://arxiv.org/abs/2512.17559
tags:
- system
- diagnostic
- symptoms
- accuracy
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses limitations in current AI diagnostic systems
  by introducing an explainable conversational diagnostic chatbot powered by GPT-4o,
  Retrieval-Augmented Generation (RAG), and Chain-of-Thought prompting. The system
  dynamically extracts and normalizes patient symptoms, prioritizes diagnoses through
  similarity matching, and adapts follow-up questions based on symptom context.
---

# Towards Explainable Conversational AI for Early Diagnosis with Large Language Models

## Quick Facts
- arXiv ID: 2512.17559
- Source URL: https://arxiv.org/abs/2512.17559
- Authors: Maliha Tabassum; M Shamim Kaiser
- Reference count: 8
- Primary result: LLM-based system achieves 90% Top-1 accuracy and 100% Top-3 accuracy in conversational disease diagnosis

## Executive Summary
This paper presents an explainable conversational diagnostic chatbot that leverages GPT-4o, Retrieval-Augmented Generation (RAG), and Chain-of-Thought prompting to identify diseases from patient symptom descriptions. The system dynamically extracts and normalizes symptoms, uses hybrid similarity scoring to prioritize diagnoses, and adapts follow-up questions based on symptom context. When evaluated against traditional ML models, the LLM-based system demonstrates superior performance with 90% Top-1 accuracy and 100% Top-3 accuracy, offering a promising approach for transparent and interactive AI in healthcare.

## Method Summary
The system combines GPT-4o with RAG using FAISS vector store and All-MiniLM-L6-v2 embeddings to ground responses in medical knowledge from WHO, CDC, and Mayo Clinic sources. It employs hybrid similarity scoring (70% global/30% local) and non-linear confidence calibration to rank diagnoses. The architecture includes symptom extraction, dynamic question generation, and explainable outputs. Evaluation used synthetic patient cases and compared against traditional ML models (Naive Bayes, Logistic Regression, SVM, Random Forest, KNN) using 5-fold cross-validation.

## Key Results
- Achieved 90% Top-1 accuracy and 100% Top-3 accuracy in disease diagnosis
- Outperformed traditional ML models (F1-score 86.1% vs 61.6-86.4%)
- Hybrid similarity scoring improved Top-1 accuracy from 74-89% to 90% compared to single-method approaches

## Why This Works (Mechanism)

### Mechanism 1: Hybrid Similarity Scoring for Differential Diagnosis
- Combines local (symptom-level) and global (disease-profile) similarity for more accurate rankings
- Uses weighted averaging (70/30 or 50/50) to balance fine-grained matching with disease-level context
- Evidence: Hybrid_70_30 achieved Top-1 0.90 vs. Local_Only 0.89 vs. Global_Only 0.74

### Mechanism 2: RAG Grounding Reduces Hallucination
- Retrieves medical context from curated knowledge base before generation
- Uses FAISS vector search and validation layer to constrain outputs to evidence-based content
- Evidence: Grounded Generation ensures outputs are based on medically validated context

### Mechanism 3: Non-Linear Confidence Calibration Prevents Overconfidence
- Uses logarithmic and exponential transformations with hard caps (max 95%)
- Implements diminishing returns and symptom denial penalties
- Evidence: Nonlinear scaling and capped confidence avoids overconfidence in diagnostic reasoning

## Foundational Learning

- **Retrieval-Augmented Generation (RAG)**: Core architecture choice for grounding responses; understand embedding, retrieval, and context injection
  - Quick check: Given "chest tightness" query, what steps retrieve relevant medical context before LLM generates response?

- **Cosine Similarity and Jaccard Similarity**: Used for symptom-disease matching (cosine) and knowledge base analysis (Jaccard heatmap)
  - Quick check: Why prefer cosine for semantic matching and Jaccard for set-based symptom overlap analysis?

- **Non-Linear Confidence Calibration**: Explains why raw symptom counts don't translate linearly to diagnostic confidence
  - Quick check: If 10 symptoms match a disease, why might confidence not be 10× higher than with 1 symptom matched?

## Architecture Onboarding

- **Component map**: Input Preprocessing -> RAG Retrieval -> Symptom Extraction/Normalization -> Diagnostic Engine -> Conversation Manager -> Explainability Layer -> Output

- **Critical path**: User input → RAG retrieval → Symptom extraction → Hybrid scoring → Confidence calculation → Question generation (loop until MinS=8 or MaxQ=20) → Final diagnosis synthesis with evidence

- **Design tradeoffs**: Higher similarity threshold (0.85) improves precision but increases latency (4.01s vs 1.55s); lower MinS (2) speeds conversations but accuracy drops to 0.60; local-only scoring is faster but less contextually aware

- **Failure signatures**: Top-1 accuracy drops for high-overlap diseases (Malaria 83.3%, Viral Fever 33.3%); CountVectorizer-based ML models collapse (F1=0.616) vs TF-IDF (F1=0.864); Text-Embedding-Ada-002 shows higher latency vs All-MiniLM-L6-v2

- **First 3 experiments**:
  1. Ablate MinS parameter: Test MinS=4, 6, 8 on held-out disease category to find optimal balance
  2. Swap embedding model: Compare All-MiniLM-L6-v2 vs Text-Embedding-Ada-002 on latency and accuracy
  3. Stress-test retrieval grounding: Withhold key knowledge base documents for target disease and measure hallucination rate

## Open Questions the Paper Calls Out

- How does the system's diagnostic performance compare when evaluated on real-life patient interactions versus the synthetic dialogues used in this study?
  - Basis: Section 4.7 states test data were synthetic dialogues lacking real life patient interactions
  - Resolution: Validation study using real clinical consultation transcripts or live deployment trial

- To what extent does integration of Electronic Health Records (EHR) improve diagnostic precision by incorporating longitudinal patient history?
  - Basis: Listed as future research direction in Section 5
  - Resolution: Comparative performance metrics between current RAG-only system and EHR-integrated version

- Can reinforcement learning (RL) effectively replace fixed symptom weights to better handle symptom severity and unpredictable clinical contexts?
  - Basis: Section 4.7 notes system relied heavily on fixed weights; Section 5 proposes RL to handle unpredictable scenarios
  - Resolution: Ablation study comparing static weighting system against RL-based adaptive weighting model

## Limitations

- Synthetic evaluation data: Results based on ChatGPT-generated cases rather than real clinical conversations
- Knowledge base construction: Specific documents, parsing methodology, and symptom-weighting schema not detailed
- Limited disease scope: Evaluation covers only 14 diseases, predominantly infectious diseases and common chronic illnesses

## Confidence

- **High Confidence**: Hybrid similarity scoring demonstrably improves diagnostic accuracy (90% Top-1 vs 74-89% for single methods)
- **Medium Confidence**: RAG grounding effectively reduces hallucination, but extent depends on retrieval quality and knowledge base completeness
- **Medium Confidence**: LLM-based system significantly outperforms traditional ML models, but synthetic data generation may inflate real-world performance

## Next Checks

1. Deploy system in controlled clinical setting with actual patient interactions to measure real-world diagnostic accuracy
2. Systematically audit knowledge base for disease coverage gaps, symptom-weighting biases, and representation disparities
3. Design test cases with ambiguous, overlapping, or atypical symptom presentations to stress-test hybrid scoring mechanism