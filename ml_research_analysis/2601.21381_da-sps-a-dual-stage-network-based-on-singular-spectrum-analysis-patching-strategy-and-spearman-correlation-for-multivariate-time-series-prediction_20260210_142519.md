---
ver: rpa2
title: 'DA-SPS: A Dual-stage Network based on Singular Spectrum Analysis, Patching-strategy
  and Spearman-correlation for Multivariate Time-series Prediction'
arxiv_id: '2601.21381'
source_url: https://arxiv.org/abs/2601.21381
tags:
- prediction
- target
- variables
- variable
- lstm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The DA-SPS model addresses multivariate time-series forecasting
  by separately processing the target variable and extraneous variables. It uses Singular
  Spectrum Analysis (SSA) to decompose the target into trend and seasonality components,
  which are then processed by P-Conv-LSTM and LSTM respectively.
---

# DA-SPS: A Dual-stage Network based on Singular Spectrum Analysis, Patching-strategy and Spearman-correlation for Multivariate Time-series Prediction

## Quick Facts
- arXiv ID: 2601.21381
- Source URL: https://arxiv.org/abs/2601.21381
- Authors: Tianhao Zhang; Shusen Ma; Yu Kang; Yun-Bo Zhao
- Reference count: 30
- Primary result: DA-SPS outperforms seven state-of-the-art baselines on four public datasets and one private dataset, achieving lower MAE (0.0253 vs 0.0311-0.0368) and higher CORR (0.9374 vs 0.8696-0.9102) compared to CNN, LSTM, DA-RNN, LSTNet, DA-Conv-LSTM, TS-Conv-LSTM, and TCLN.

## Executive Summary
DA-SPS is a dual-stage neural network for multivariate time-series forecasting that processes the target variable and extraneous variables separately. The model uses Singular Spectrum Analysis (SSA) to decompose the target into trend and seasonality components, which are processed by P-Conv-LSTM and LSTM respectively. For extraneous variables, Spearman correlation analysis filters relevant variables, followed by an L-Attention module. The model combines these processed features through weighted summation and linear mapping, achieving superior performance on both public and private datasets.

## Method Summary
DA-SPS processes multivariate time-series data through two distinct stages: Target Variable Processing Stage (TVPS) and Extraneous Variables Processing Stage (EVPS). TVPS uses SSA to decompose the target variable into trend and seasonality components, with trend processed by P-Conv-LSTM (a convolutional LSTM with patching strategy) and seasonality by standard LSTM. EVPS filters extraneous variables using Spearman correlation (threshold 0.5) and processes remaining variables through L-Attention module. The outputs from both stages are combined via weighted summation and linear mapping. The model is trained with MAE loss using Adam optimizer, lr=0.001, batch size=64, and 200 epochs.

## Key Results
- Outperforms seven state-of-the-art baselines (CNN, LSTM, DA-RNN, LSTNet, DA-Conv-LSTM, TS-Conv-LSTM, TCLN) on four public datasets
- Achieves lower MAE (0.0253 vs 0.0311-0.0368) and higher CORR (0.9374 vs 0.8696-0.9102) compared to baselines
- Ablation studies confirm P-Conv-LSTM module has the most significant impact on performance
- Shows consistent improvement across multiple forecasting horizons (3, 6, 12, 24 steps)

## Why This Works (Mechanism)

### Mechanism 1: SSA Decomposition
Decomposing the target variable isolates predictive signals (trend/seasonality) from stochastic noise, improving feature extraction efficiency. SSA embeds the target sequence into a trajectory matrix, applies SVD, and groups components. Crucially, it discards the noise component rather than feeding it to the network, ensuring downstream LSTM and P-Conv-LSTM only process structured patterns.

### Mechanism 2: Patching Strategy
Segmentation (patching) of the trend component preserves local structural dependencies that point-wise LSTMs miss. The P-Conv-LSTM module segments the trend sequence into non-overlapping patches, which are processed using convolution operations inside LSTM gates, allowing the model to encode spatial hierarchies within local time windows.

### Mechanism 3: Spearman Correlation Filtering
Filtering extraneous variables via Spearman correlation reduces interference from irrelevant variables, enhancing generalization. The EVPS calculates Spearman coefficient between target and every other variable, applying a hard threshold (ρ' = 0.5) to discard variables before passing data to L-Attention module.

## Foundational Learning

- **Singular Spectrum Analysis (SSA)**
  - Why needed here: You must understand how the trajectory matrix works to diagnose why the model might be failing to capture trend or seasonality effectively.
  - Quick check question: If the window length m is set too short, which component (trend or seasonality) will the SSA module fail to extract?

- **Spearman vs. Pearson Correlation**
  - Why needed here: The model relies on Spearman to filter variables. Understanding that Spearman captures monotonic rank relationships (unlike Pearson's linear constraint) explains why certain non-linear variables might still be selected.
  - Quick check question: Why is Spearman preferred over Pearson if the relationship between two variables is exponential rather than linear?

- **Conv-LSTM (Convolutional LSTM)**
  - Why needed here: The P-Conv-LSTM is the most impactful module. Standard LSTMs use fully connected layers; Conv-LSTMs use convolution. You need to know how the * operator changes the inductive bias.
  - Quick check question: In a standard LSTM, input x_t is a vector. In Conv-LSTM, input X_t is a tensor/grid. How does this change the memory retention of spatial features?

## Architecture Onboarding

- **Component map:** TVPS (SSA → LSTM for seasonality, P-Conv-LSTM for trend) + EVPS (Spearman filter → L-Attention encoder-decoder) → Weighted Sum → Linear Projection

- **Critical path:** The P-Conv-LSTM module is the performance bottleneck. Ablation studies show that replacing this with a standard LSTM causes the largest performance degradation.

- **Design tradeoffs:**
  - SSA vs. STL: SSA is claimed superior because it allows explicit noise removal, whereas STL (based on Loess smoothing) leaves noise in the components.
  - Hard Thresholding: Setting ρ' = 0.5 is aggressive, reducing computation but risking dropping weakly correlated but causally relevant variables.

- **Failure signatures:**
  - Mode Collapse: If the Spearman threshold is too high for a dataset with weak correlations, the EVPS contribution becomes zero, and the model collapses to a univariate predictor.
  - Noise Overfitting: If SSA parameters are misconfigured, the "Trend" component will contain high-frequency noise, confusing the P-Conv-LSTM.

- **First 3 experiments:**
  1. Sensitivity Analysis on ρ': Run the EVPS with thresholds [0.3, 0.5, 0.7] to verify if the fixed 0.5 threshold is optimal or if the model is missing critical variables.
  2. Patching Size Ablation: Vary the subsequence length l (currently 24) relative to the dataset's dominant frequency to verify alignment with local pattern periods.
  3. SSA Reconstruction Validation: Visualize the SSA output vs. raw data to ensure decomposition isn't "eating" signal peaks through over-smoothing.

## Open Questions the Paper Calls Out

- Can the DA-SPS architecture be effectively extended to handle multivariate input with multivariate output forecasting?
- Does the static Spearman correlation filter limit the model's ability to capture time-varying relationships between extraneous and target variables?
- How sensitive is the prediction accuracy to the choice of the Spearman correlation threshold and SSA window length?

## Limitations

- SSA implementation relies on hard threshold for noise removal without validating grouping criteria for trend/seasonality/noise components
- Fixed Spearman threshold of 0.5 may be dataset-specific and could eliminate causally relevant variables with non-linear or lagged relationships
- Patching length of 24 is set without justification for non-hourly datasets, potentially misaligning local pattern extraction

## Confidence

- **High Confidence**: Performance improvements over baselines are well-supported by quantitative results (MAE and CORR metrics)
- **Medium Confidence**: Mechanism for filtering extraneous variables via Spearman correlation is logically sound but may be overly aggressive
- **Low Confidence**: SSA decomposition parameters (window size, component grouping) are underspecified, making decomposition effectiveness difficult to fully evaluate

## Next Checks

1. **Spearman Threshold Sensitivity**: Test EVPS with thresholds [0.3, 0.5, 0.7] to determine if the fixed 0.5 value is optimal or if the model is discarding valuable variables on certain datasets.

2. **Patching Length Validation**: Vary the subsequence length l (currently 24) relative to the dominant frequency of each dataset to verify alignment with local pattern periods.

3. **SSA Decomposition Visualization**: Generate plots comparing SSA outputs (trend and seasonality components) against raw data for each dataset to verify the decomposition isn't over-smoothing signal peaks or failing to capture important patterns.