---
ver: rpa2
title: Evaluating Multilingual and Code-Switched Alignment in LLMs via Synthetic Natural
  Language Inference
arxiv_id: '2508.14735'
source_url: https://arxiv.org/abs/2508.14735
tags:
- language
- multilingual
- languages
- across
- cross-lingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a controlled evaluation framework for multilingual
  natural language inference (NLI) using synthetic, logic-based premise-hypothesis
  pairs translated into typologically diverse languages. The approach enables precise
  control over semantic relations and tests reasoning in both monolingual and code-switched
  (mixed-language) conditions.
---

# Evaluating Multilingual and Code-Switched Alignment in LLMs via Synthetic Natural Language Inference

## Quick Facts
- arXiv ID: 2508.14735
- Source URL: https://arxiv.org/abs/2508.14735
- Authors: Samir Abdaljalil; Erchin Serpedin; Khalid Qaraqe; Hasan Kurban
- Reference count: 0
- Primary result: Code-switching in multilingual NLI does not degrade performance and can improve it, suggesting translation-induced lexical variation acts as a regularization signal

## Executive Summary
This study introduces a controlled evaluation framework for multilingual natural language inference (NLI) using synthetic, logic-based premise-hypothesis pairs translated into typologically diverse languages. The approach enables precise control over semantic relations and tests reasoning in both monolingual and code-switched (mixed-language) conditions. Surprisingly, code-switching does not degrade performance and can even improve it, suggesting translation-induced lexical variation may act as a regularization signal. Semantic preservation was validated through embedding-based similarity analyses, confirming the fidelity of translated pairs. Across six languages and six models, results show substantial variation in reasoning performance, with code-switching offering unexpected robustness.

## Method Summary
The evaluation framework uses synthetic NLI pairs generated from logic-based templates (entailment, contradiction, neutral) with placeholder instantiation. These English pairs are translated into six languages (Arabic, German, French, Hindi, Swahili) using high-performance neural machine translation. The study constructs all pairwise language combinations (36 total) for code-switched evaluation. Six multilingual LLMs (Fanar-9B, Gemma-7B, LLaMA-3-8B, Mistral-7B-v0.3, Phi-4, Qwen3-7B) are evaluated in zero-shot mode using structured prompts with greedy decoding. Semantic preservation is validated using LaBSE embeddings and UMAP visualizations to confirm cross-lingual alignment.

## Key Results
- Code-switching does not degrade performance and can improve accuracy, suggesting translation-induced lexical variation acts as a regularization signal
- Translation-induced variation produces tighter semantic clusters across typologically distant languages (cosine similarity >0.80)
- Performance varies substantially across models and languages, with some non-English hypothesis languages yielding higher accuracy than English

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Translation-induced lexical variation can serve as a regularization signal, improving cross-lingual reasoning in some configurations.
- **Mechanism:** When NLI pairs are translated across typologically diverse languages, the resulting lexical and syntactic variation may reduce overfitting to language-specific surface patterns, forcing models to rely more heavily on abstract semantic structure. This effect is observed when code-switched (mixed-language) premises and hypotheses yield accuracy comparable to or exceeding monolingual baselines.
- **Core assumption:** The translation process introduces controlled variability without fundamentally altering the underlying logical relation (entailment, contradiction, neutrality).
- **Evidence anchors:**
  - [abstract] "Surprisingly, code-switching does not degrade, and can even improve, performance, suggesting that translation-induced lexical variation may serve as a regularization signal."
  - [Section 5.2] "Several models outperform their monolingual baselines in specific code-switched configurations. For example, Gemma-7B achieves markedly higher accuracy on many bilingual pairs than on English–English (e.g., En–Hi: 32.9% vs. En–En: 17.0%)."
  - [corpus] Related work (arXiv:2601.07153, arXiv:2506.14012) confirms LLM robustness in code-switched settings remains underexplored, but does not directly validate the regularization hypothesis.
- **Break condition:** If translation quality degrades significantly (cosine similarity <0.75) or introduces systematic semantic drift, the regularization effect may become noise, reducing performance.

### Mechanism 2
- **Claim:** Multilingual LLMs encode semantically equivalent sentences from different languages in proximate embedding space regions, enabling cross-lingual inference.
- **Mechanism:** Models with aligned multilingual representations (e.g., via shared tokenization or multilingual pretraining) map translations to similar vector-space positions. When premise and hypothesis are in different languages, the model can compare their embeddings to determine logical relations, provided the embedding space exhibits cross-lingual cohesion.
- **Core assumption:** The embedding encoder (e.g., LaBSE) faithfully represents semantic content across languages, and the LLM's internal representations align sufficiently with this encoder.
- **Evidence anchors:**
  - [abstract] "Semantic preservation was validated through embedding-based similarity analyses and cross-lingual alignment visualizations, confirming the fidelity of translated pairs."
  - [Section 6.1] "Translations of the same sentence form tight clusters, even across typologically distant languages. This indicates high semantic consistency."
  - [corpus] arXiv:2507.14900 examines cross-lingual alignment via neuron-level analysis, suggesting embedding-based metrics may not fully capture internal alignment.
- **Break condition:** If the LLM's internal representations are poorly aligned with the external encoder used for validation, or if low-resource languages exhibit systematic drift, cross-lingual inference accuracy will degrade.

### Mechanism 3
- **Claim:** Model performance exhibits asymmetric dependency on hypothesis language, with some non-English hypothesis languages yielding higher accuracy.
- **Mechanism:** Instruction-tuned LLMs may overfit to statistical artifacts in high-resource languages (e.g., English), causing ambiguous or noisy hypothesis interpretation. More literal or syntactically constrained translations in lower-resource languages can reduce this noise, improving classification accuracy when these languages appear as the hypothesis.
- **Core assumption:** Translation to lower-resource languages produces more deterministic or less ambiguous surface forms for the hypothesis.
- **Evidence anchors:**
  - [Section 5.2] "Using Hindi, Swahili, or Arabic as the hypothesis language yields higher performance than English, suggesting potential advantages from morphologically richer or syntactically simpler constructions."
  - [Section 5.2] "This pattern suggests a disproportionate reliance on hypothesis surface forms, with syntactic or lexical ambiguity in English degrading performance."
  - [corpus] Corpus neighbors do not directly confirm this hypothesis; validation would require controlled studies of hypothesis-language effects.
- **Break condition:** If translation introduces systematic errors or the hypothesis language has tokenization issues, the asymmetry may reverse or become inconsistent.

## Foundational Learning

- **Concept:** Natural Language Inference (NLI)
  - **Why needed here:** NLI (entailment, contradiction, neutrality) is the core task used to probe cross-lingual semantic alignment. Understanding NLI structure is prerequisite to interpreting the experimental design and results.
  - **Quick check question:** Given the premise "All cats are mammals" and hypothesis "Some cats are mammals," what is the correct NLI label?

- **Concept:** Code-Switching
  - **Why needed here:** The paper's key innovation is evaluating NLI under code-switched conditions (premise and hypothesis in different languages). This tests whether models maintain logical consistency across linguistic boundaries.
  - **Quick check question:** If a premise is in Arabic and the hypothesis is in French, what additional challenge does this introduce compared to both being in French?

- **Concept:** Cross-Lingual Embedding Alignment
  - **Why needed here:** The validation methodology relies on embedding-based similarity (LaBSE, UMAP projections) to confirm semantic preservation across translations. Understanding embedding spaces is essential for interpreting Section 6.
  - **Quick check question:** Why would translations of the same sentence into different languages cluster together in a well-aligned embedding space?

## Architecture Onboarding

- **Component map:**
  Synthetic NLI generator -> Neural MT translation -> Code-switch constructor -> LLM evaluator -> Validation layer

- **Critical path:**
  1. Template design must ensure logical relations are unambiguous and compositionally structured
  2. Translation quality must be verified via embedding similarity (threshold: avg cosine >0.80)
  3. Prompt formatting must be consistent across all languages and models
  4. Exact string matching for label extraction must handle multilingual output variations

- **Design tradeoffs:**
  - Synthetic templates provide precise control but limit ecological validity (natural discourse complexity is absent)
  - Machine translation enables scale but may introduce subtle semantic drift in low-resource languages
  - Zero-shot evaluation avoids fine-tuning confounds but may underestimate model potential

- **Failure signatures:**
  - Monolingual accuracy near 33% (random chance): model not following NLI task structure
  - Asymmetric performance (e.g., En→Hi high, Hi→En low): hypothesis-language over-reliance
  - Low embedding similarity (<0.75): translation quality insufficient for valid inference
  - Consistent underperformance on specific languages: tokenization or script encoding issues

- **First 3 experiments:**
  1. **Monolingual baseline by language:** Evaluate all six models on monolingual NLI pairs (En-En, Ar-Ar, etc.) to establish per-language reasoning capacity and identify high/low-resource disparities.
  2. **Code-switched pair matrix:** Run the full 6×6 language-pair grid for at least two models (highest and lowest monolingual performers) to confirm whether code-switching improves accuracy and identify asymmetric patterns.
  3. **Translation fidelity validation:** Compute LaBSE cosine similarity for a held-out sample of 100 premise-hypothesis pairs per target language; visualize with UMAP to confirm cross-lingual clustering before proceeding to large-scale evaluation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanism causes code-switching to improve reasoning performance in some models—is it translation-induced regularization, improved cross-lingual alignment, or translation simplification effects?
- Basis in paper: [explicit] The authors state: "Possible explanations include translation-induced lexical or syntactic variation acting as a regularization signal, improved alignment within the multilingual representation space, or simplification effects from translation."
- Why unresolved: The study observes the phenomenon but does not isolate which mechanism drives the improvement across different model architectures.
- What evidence would resolve it: Controlled experiments manipulating lexical variation, alignment quality, and syntactic complexity independently across code-switched pairs; ablation studies comparing artificially perturbed translations.

### Open Question 2
- Question: Do the performance patterns observed with synthetic, template-based NLI data generalize to natural multilingual discourse with real-world ambiguity and complexity?
- Basis in paper: [explicit] The limitations section states: "The templates, while semantically well-formed, cannot fully capture the diversity and ambiguity of natural multilingual discourse. Consequently, performance on these tasks may not directly translate to real-world reasoning ability."
- Why unresolved: The study prioritizes control over ecological validity, leaving the generalization gap untested.
- What evidence would resolve it: Comparative evaluation on naturally occurring multilingual NLI data with equivalent logical structures, measuring correlation with synthetic benchmark performance.

### Open Question 3
- Question: To what extent do translation artifacts—meaning shifts, simplifications, or structural divergences—influence the observed code-switching benefits, particularly for low-resource languages?
- Basis in paper: [inferred] The paper relies on machine translation and acknowledges that "neural translation systems—particularly for low-resource languages—can introduce meaning shifts, simplifications, or structural divergences." Semantic similarity scores vary by language (French: 0.912 vs. Arabic: 0.811).
- Why unresolved: While embedding similarity was measured, causality between translation quality variations and code-switched performance gains was not established.
- What evidence would resolve it: Human verification of translation pairs; experiments comparing machine-translated vs. human-authored multilingual examples; controlled degradation of translation quality to measure impact.

## Limitations
- Synthetic templates cannot fully capture the diversity and ambiguity of natural multilingual discourse
- Translation quality, particularly for low-resource languages like Swahili, may introduce semantic drift affecting results
- The hypothesis-language asymmetry finding is observed but not mechanistically explained

## Confidence
- High confidence: Semantic preservation via embedding similarity validation; monolingual baseline establishment across six languages; code-switching performance comparison methodology
- Medium confidence: Code-switching regularization hypothesis (plausible but not directly tested); hypothesis-language asymmetry interpretation (data-supported but mechanistically unclear)
- Low confidence: Generalization to natural NLI datasets; translation quality for low-resource languages; extent to which findings apply beyond the six studied languages

## Next Checks
1. Conduct human evaluation of 100 randomly sampled translations per language to verify semantic preservation independently of embedding metrics
2. Test the regularization hypothesis by creating controlled synthetic pairs with varying translation quality (e.g., word-for-word vs. fluent) and measuring impact on code-switched performance
3. Evaluate the same models on a natural multilingual NLI dataset (e.g., XNLI) to test whether synthetic results generalize to real-world complexity