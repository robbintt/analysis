---
ver: rpa2
title: Intent Recognition and Out-of-Scope Detection using LLMs in Multi-party Conversations
arxiv_id: '2507.22289'
source_url: https://arxiv.org/abs/2507.22289
tags:
- llms
- intent
- label
- bert
- intents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a hybrid method combining BERT and LLMs for
  intent recognition and out-of-scope detection in multi-party conversations. The
  approach routes uncertain BERT predictions to LLMs, using BERT's probability outputs
  to dynamically reduce the label space in LLM prompts.
---

# Intent Recognition and Out-of-Scope Detection using LLMs in Multi-party Conversations

## Quick Facts
- **arXiv ID:** 2507.22289
- **Source URL:** https://arxiv.org/abs/2507.22289
- **Reference count:** 12
- **Primary result:** Hybrid BERT-LLM approach achieves improved in-scope precision and OOS detection while reducing computational costs by 40%+

## Executive Summary
This paper addresses the challenge of intent recognition and out-of-scope detection in multi-party conversations by proposing a hybrid approach that combines BERT and large language models (LLMs). The method leverages BERT for initial classification and routes uncertain predictions to LLMs, with a novel label space reduction strategy that dynamically narrows the LLM's candidate labels based on BERT's probability outputs. The approach aims to balance accuracy with computational efficiency in low-resource intent classification scenarios.

## Method Summary
The proposed hybrid method routes uncertain BERT predictions to LLMs, using BERT's probability outputs to dynamically reduce the label space in LLM prompts. This label space reduction strategy helps maintain high in-scope precision while preserving strong out-of-scope detection performance. The approach was evaluated on two datasets, demonstrating that sharing information between models can effectively balance accuracy and efficiency compared to using either model in isolation.

## Key Results
- Label space reduction strategy improves in-scope precision while maintaining strong OOS detection
- Computational costs reduced by over 40% compared to full LLM inference
- DeepSeek-R1 with label space reduction achieved the best overall results

## Why This Works (Mechanism)
The approach works by leveraging BERT's strong performance on in-scope intents while using LLMs for cases where BERT expresses uncertainty. The label space reduction strategy dynamically narrows the candidate labels for LLM processing, which improves precision and efficiency by focusing the LLM's attention on the most relevant options. This hybrid architecture allows the system to benefit from BERT's efficiency for confident predictions while reserving LLM resources for challenging cases that require more sophisticated reasoning.

## Foundational Learning
1. **Multi-party conversation dynamics** - Understanding interactions between multiple speakers is crucial for accurate intent recognition in conversational AI systems
   * Why needed: Different speakers and conversation contexts can affect intent interpretation
   * Quick check: Test on datasets with varying numbers of speakers

2. **Out-of-scope detection** - Identifying when user input falls outside predefined intent categories
   * Why needed: Critical for maintaining system reliability and user experience
   * Quick check: Evaluate false positive/negative rates on OOS detection

3. **Uncertainty estimation** - Using probability outputs to determine when predictions are unreliable
   * Why needed: Enables intelligent routing between models based on confidence levels
   * Quick check: Analyze calibration of BERT's probability estimates

## Architecture Onboarding

**Component Map:** BERT -> Uncertainty Threshold -> Label Space Reduction -> LLM Prompt

**Critical Path:** Input utterance → BERT classification → Uncertainty check → Dynamic label space reduction → LLM inference → Final prediction

**Design Tradeoffs:** 
- Accuracy vs. computational cost (BERT for confident cases, LLM for uncertain ones)
- Precision vs. recall in OOS detection (tighter thresholds improve precision but may miss some OOS cases)
- Label space reduction granularity (more reduction saves computation but risks missing correct labels)

**Failure Signatures:**
- High false positive rate in OOS detection when uncertainty threshold is too low
- Reduced recall when label space reduction is too aggressive
- Increased computational costs when uncertainty threshold is too high

**First 3 Experiments to Run:**
1. Baseline comparison: BERT-only vs. LLM-only vs. hybrid approach
2. Sensitivity analysis: Varying uncertainty thresholds and label space reduction ratios
3. Cross-dataset validation: Testing on additional conversational datasets

## Open Questions the Paper Calls Out
None identified in the provided information.

## Limitations
- Focus on only two specific datasets may limit generalizability to broader conversational contexts
- Heavy dependence on BERT's uncertainty estimates, which may vary across different tasks
- Computational cost reduction is relative to full LLM inference, lacking absolute performance metrics

## Confidence
- **High:** Label space reduction strategy's effectiveness in improving in-scope precision and maintaining OOS detection
- **Medium:** Computational cost reduction claims (relative comparisons that vary by implementation)
- **Medium:** Generalizability of the approach across different conversational domains

## Next Checks
1. Test the hybrid approach on additional datasets from different domains (e.g., customer service, healthcare, education) to assess generalizability and identify potential failure modes.
2. Compare the proposed method against other state-of-the-art efficient intent recognition approaches using standardized benchmarks to establish relative performance.
3. Conduct ablation studies to determine the impact of different uncertainty thresholds and label space reduction strategies on both accuracy and computational efficiency across varying conversation lengths and complexities.