---
ver: rpa2
title: Can Large Language Models Capture Video Game Engagement?
arxiv_id: '2502.04379'
source_url: https://arxiv.org/abs/2502.04379
tags:
- uni00000008
- uni00000010
- uni00000014
- uni00000015
- uni00000013
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether large language models (LLMs) can
  accurately predict viewer engagement from gameplay videos. The authors conducted
  over 4,800 experiments using 80 minutes of annotated first-person shooter gameplay
  from the GameVibe corpus, testing multiple LLM architectures, model sizes, prompting
  strategies, and input modalities.
---

# Can Large Language Models Capture Video Game Engagement?

## Quick Facts
- arXiv ID: 2502.04379
- Source URL: https://arxiv.org/abs/2502.04379
- Reference count: 40
- One-line primary result: LLMs can predict viewer engagement from gameplay videos but generally fall short of human-level performance

## Executive Summary
This study investigates whether large language models can accurately predict viewer engagement from gameplay videos. Using 80 minutes of annotated first-person shooter gameplay from the GameVibe corpus, the authors tested multiple LLM architectures, model sizes, prompting strategies, and input modalities across over 4,800 experiments. The key finding is that while LLMs can outperform traditional machine learning baselines, they generally fall short of human-level annotation performance, with GPT-4o using multimodal few-shot prompting achieving up to 47% improvement over baseline in certain games.

## Method Summary
The study uses the GameVibe-LLM corpus containing 80 minutes of FPS gameplay with human annotations of engagement. The task is formulated as ordinal learning, predicting whether engagement increases or decreases between consecutive 3-second video windows. The authors tested zero-shot and few-shot prompting strategies with multiple LLM architectures (GPT-4o, LLaVA, Gemma 3) using both multimodal and text-only inputs. They applied temporal shifting to align video frames with annotations and measured performance as relative accuracy gain over a Zero Rule Classifier baseline.

## Key Results
- GPT-4o with multimodal few-shot prompting achieved up to 47% improvement over baseline in certain games
- Average improvement across games was 6% over baseline
- Text-based frame descriptions did not improve performance compared to direct multimodal prompting
- Model size significantly impacts performance, with larger models generally performing better
- Games with clearer visuals and more dynamic gameplay were easier for LLMs to predict

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If a Large Multimodal Model (LMM) is provided with concrete positive and negative examples of engagement changes (Few-Shot) alongside the query, it likely anchors its reasoning to specific visual features rather than relying solely on general priors.
- **Mechanism:** Multimodal Few-Shot Prompting. The model uses the provided examples to establish a local decision boundary for "engaging" vs. "disengaging" within the specific visual context of the game being analyzed.
- **Core assumption:** The model possesses sufficient visual recognition capability to transfer patterns from the few-shot examples to the target query.
- **Evidence anchors:**
  - [abstract] "The best performance... was achieved with GPT-4o using multimodal few-shot prompting."
  - [section V.D] States that Multimodal Few-Shot experiments showed "overall improvement" and helped difficult games like *Counter Strike 1.6*.
- **Break condition:** Performance degrades if the visual style of the few-shot examples differs too drastically from the target game, or if the examples are contradictory.

### Mechanism 2
- **Claim:** If the engagement task is formulated as a pairwise preference (ordinal comparison) rather than an absolute rating, the system is more robust to individual annotator bias.
- **Mechanism:** Ordinal Learning / Preference Learning. Instead of predicting "Engagement = 7/10", the model predicts "Frame B > Frame A". This simplifies the task to recognizing relative changes in intensity.
- **Core assumption:** Engagement changes are perceptible in visual cues (e.g., action spikes, lighting) between consecutive frames.
- **Evidence anchors:**
  - [section I] "We view player modelling as an ordinal learning paradigm... We task LLMs to label increases or decreases of engagement across frames."
  - [section III.B] Describes the conversion of interval signals into "discrete ordinal signals by comparing pairs of consecutive time windows."
- **Break condition:** Fails if the visual changes between frames are negligible (static scenes) or if the "ground truth" noise (disagreement between human annotators) is too high.

### Mechanism 3
- **Claim:** If the ground truth label is temporally shifted backward relative to the visual frame, the alignment between visual stimulus and reported affect improves.
- **Mechanism:** Temporal Shift ($\Delta t$) for Reaction Lag Compensation. The paper introduces a hyperparameter $\Delta t$ to align the video frame with the annotation window, accounting for the delay in human reaction/annotation speed.
- **Core assumption:** A visual event (e.g., an explosion) causes a spike in engagement that is recorded by a human annotator *after* the event appears on screen.
- **Evidence anchors:**
  - [section V.A] "We introduce... A temporal shift compared to the observed video frame ($\Delta t$). This is similar to what the literature often refers to as input lag."
  - [section V.B] Reports that larger negative shifts (specifically $\Delta t = -2$) tend to yield higher performance.
- **Break condition:** Breaks if the temporal shift over-corrects or if the gameplay is so chaotic that reaction lag is inconsistent.

## Foundational Learning

- **Concept: Chain-of-Thought (CoT) Prompting**
  - **Why needed here:** The paper explicitly requires the LLM to output a "description," "comparison," and "reasoning" before the final decision. This forces the model to attend to visual details sequentially.
  - **Quick check question:** Can you design a prompt that forces the model to list three visual differences between two images before classifying them?

- **Concept: Multimodal Embedding Alignment**
  - **Why needed here:** To understand why "Text Descriptions" (converting images to text first) didn't outperform direct multimodal input. The models use vision encoders (like SigLIP or CLIP) to project images into the same vector space as text tokens.
  - **Quick check question:** Does the model process the image pixels directly, or does it "read" a caption generated by a separate tool? (In this paper, it processes pixels directly in the Multimodal settings).

- **Concept: The Zero Rule Classifier (ZeroR) Baseline**
  - **Why needed here:** To interpret the results meaningfully. The paper measures success ($\Delta A$) not by raw accuracy, but by improvement over ZeroR (always guessing the majority class).
  - **Quick check question:** If a game has 80% "decreasing engagement" labels, and the model predicts "decrease" every time, what is the ZeroR accuracy? (Answer: 80%).

## Architecture Onboarding

- **Component map:** GameVibe-LLM Dataset -> Frame Extraction -> Ground Truth Alignment (Shift) -> Prompt Assembly (few-shot selection) -> Inference -> Parsing -> Metric Calculation
- **Critical path:** Frame Extraction -> Ground Truth Alignment (Shift) -> Prompt Assembly (few-shot selection) -> Inference -> Parsing -> Metric Calculation
- **Design tradeoffs:**
  - **Multimodal vs. Text:** Direct image input is simpler and performed comparably or better than generating text descriptions first (Section V.C)
  - **One-Shot vs. Few-Shot:** Few-Shot improves accuracy but increases token costs (requires 6 images per query: 4 for examples + 2 for query) and latency
  - **Resolution:** The study crops to 512x512. Higher resolution might improve performance on "cluttered" games (like *Counter Strike 1.6*) but would increase cost/VRAM
- **Failure signatures:**
  - **Visual Clutter:** Models consistently failed on dark or visually complex games where foreground/background separation was hard (e.g., *Corridor 7*, *HROT*)
  - **Formatting Errors:** LLMs occasionally failed to follow the strict JSON output requirement or refused analysis ("I'm unable to analyse...")
  - **Class Imbalance:** Models struggle when the ground truth is heavily skewed (e.g., mostly "decreasing engagement"), making ZeroR a hard baseline to beat
- **First 3 experiments:**
  1. **Sanity Check (One-Shot):** Run GPT-4o or LLaVA on 10 pairs of frames from *Apex Legends* (an "easy" game) using $\Delta t = -2$ and $\theta = 0.05$. Verify you can parse the JSON output and calculate accuracy.
  2. **Ablation (Temporal Shift):** Run the same 10 pairs with $\Delta t = 0$ vs. $\Delta t = -2$. Observe if the alignment shift actually changes the prediction accuracy.
  3. **Modality Test (Text vs. Vision):** For a single frame pair, first run the Multimodal One-Shot prompt. Then, describe the images in text yourself and run the "Text Description" prompt. Compare the reasoning traces to see which modality captures more "action" detail.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does providing direct video input or a higher density of frames improve LLM engagement prediction accuracy compared to sparse static image sampling?
- Basis in paper: [explicit] Section VI (A.1) states that testing video input or providing more frames per query to capture action context "remains to be tested in future studies."
- Why unresolved: The study prioritized low-resolution, single-frame inputs sampled at 3-second intervals due to cost and availability, likely losing temporal context.
- What evidence would resolve it: An experimental comparison between the current static frame method and native video processing capabilities of multimodal LLMs on the GameVibe corpus.

### Open Question 2
- Question: Can efficient parameter tuning (e.g., Low-Rank Adaptation) outperform the best out-of-the-box few-shot prompting strategies for affect modeling?
- Basis in paper: [explicit] Section VI (A.2) identifies the lack of fine-tuning as a limitation and explicitly calls for future studies to investigate efficient parameter tuning.
- Why unresolved: The paper focused exclusively on "out-of-the-box" knowledge priors to evaluate zero/few-shot capabilities rather than specialized adaptation.
- What evidence would resolve it: Benchmarking LoRA-adapted open-source models against the GPT-4o few-shot baseline on the same engagement prediction task.

### Open Question 3
- Question: Can retrieval-augmented generation (RAG) or memory mechanisms mitigate the performance drop observed in visually obscure or less popular games?
- Basis in paper: [inferred] Results show LLMs struggle with games lacking a strong online presence or clear visuals; Section VI (B) suggests implementing RAG or memory mechanisms to improve context.
- Why unresolved: Current models rely heavily on pre-trained priors, failing on unfamiliar stimuli without external context injection or domain knowledge.
- What evidence would resolve it: A study implementing RAG to inject game-specific context for low-performing titles (e.g., Corridor 7) to measure performance recovery.

## Limitations

- The study uses a relatively small dataset (80 minutes total) from a single game genre (FPS), limiting generalizability
- Performance is evaluated against human annotations that themselves show variability, making the "ground truth" somewhat uncertain
- The paper relies on synthetic few-shot examples rather than human-curated ones, which may limit the quality of learned patterns
- The study does not explore why certain models (like Gemma 3) underperform despite being multimodal

## Confidence

- **High Confidence**: LLMs can outperform traditional baselines for engagement prediction; model size affects performance; few-shot prompting improves results
- **Medium Confidence**: Text descriptions do not improve over direct multimodal prompting; games with clearer visuals are easier for models to predict
- **Low Confidence**: The specific mechanisms for why Gemma 3 underperforms; generalizability to non-FPS games and other engagement contexts

## Next Checks

1. **Cross-genre validation**: Test the best-performing models (GPT-4o with few-shot) on gameplay from other genres (strategy, RPG, platformers) to assess generalizability beyond FPS games
2. **Human-in-the-loop refinement**: Have human experts review the few-shot examples generated by the system to assess whether synthetic examples capture the right engagement patterns
3. **Ground truth uncertainty analysis**: Quantify the inter-annotator agreement and its impact on model performance metrics to better understand the ceiling for automated engagement prediction