---
ver: rpa2
title: Speech-Aware Long Context Pruning and Integration for Contextualized Automatic
  Speech Recognition
arxiv_id: '2511.11139'
source_url: https://arxiv.org/abs/2511.11139
tags:
- speech
- keywords
- contextual
- context
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of leveraging long-context information
  in contextualized automatic speech recognition (ASR), particularly in scenarios
  like conference presentations where extensive OCR-derived contextual keywords are
  available but contain significant noise. The core method, SAP2 (Speech-Aware Context
  Pruning with Speech-Driven Attention-based Pooling), is a two-stage framework that
  dynamically prunes irrelevant contextual keywords using a SpeechLLM and integrates
  them into ASR systems.
---

# Speech-Aware Long Context Pruning and Integration for Contextualized Automatic Speech Recognition

## Quick Facts
- arXiv ID: 2511.11139
- Source URL: https://arxiv.org/abs/2511.11139
- Reference count: 25
- Key outcome: SAP2 framework achieves 7.71% WER on SlideSpeech, 41.1% reduction in biased keyword error rates compared to non-contextual baselines

## Executive Summary
This paper addresses the challenge of leveraging long-context information in contextualized automatic speech recognition (ASR), particularly in scenarios like conference presentations where extensive OCR-derived contextual keywords are available but contain significant noise. The core method, SAP2 (Speech-Aware Context Pruning with Speech-Driven Attention-based Pooling), is a two-stage framework that dynamically prunes irrelevant contextual keywords using a SpeechLLM and integrates them into ASR systems. The key innovation is the Speech-Driven Attention-based Pooling mechanism, which compresses extensive textual inputs into concise, speech-relevant context embeddings by weighting contextual keywords with speech attention scores. Experimental results demonstrate state-of-the-art performance on the SlideSpeech and LibriSpeech datasets, achieving word error rates (WER) of 7.71% and 1.12%, respectively.

## Method Summary
The method decomposes contextualized ASR into explicit pruning followed by recognition, using a SpeechLLM trained with LoRA adapters. Stage 1 receives raw OCR-derived keywords and acoustic features, outputting pruned keywords via speech-conditioned selection. Stage 2 conditions the ASR model only on the pruned keywords, reducing noise interference. The Speech-Driven Attention-based Pooling mechanism computes cross-modal attention between speech embeddings and text tokens, followed by window-wise weighted pooling to compress context length while preserving speech-relevant information. The framework is implemented using Qwen2-Audio-7B-Instruct as the base model with LoRA adapters on the multimodal projector and LLM backbone.

## Key Results
- Achieves 7.71% WER on SlideSpeech dataset, outperforming non-contextual baselines by 41.1% in biased keyword error rates
- Demonstrates robust scalability, maintaining consistent performance as context length increases from 1 to 25 slides
- Reduces training time by 24.97% and inference time by 20.20% through speech-driven pooling compared to baseline approaches

## Why This Works (Mechanism)

### Mechanism 1: Two-Stage Context Pruning Pipeline
The framework decomposes contextualized ASR into explicit pruning followed by recognition, improving biased keyword accuracy over joint or direct-prompt approaches. Stage 1 uses SpeechLLM to prune irrelevant keywords from extensive OCR-derived contexts, while Stage 2 conditions only on the pruned keywords. This separation reduces noise interference and enables more accurate keyword selection, achieving 93.85-94.48% F1-score for keyword selection versus 20.45-21.19% for joint training approaches.

### Mechanism 2: Speech-Driven Attention-Based Pooling
Cross-modal attention between speech embeddings and text tokens, followed by window-wise weighted pooling, compresses context length while preserving speech-relevant information. The mechanism computes attention scores between speech queries and context keys, then applies softmax-normalized pooling within windows of size n to produce compressed embeddings. This approach reduces training time by 24.97% and inference time by 20.20% while maintaining recognition accuracy.

### Mechanism 3: Context Length Scalability via Pruning
The pruning mechanism enables consistent ASR performance as input context length increases from single slides to 25+ slides. By reducing extensive contexts (median 332-1474 tokens for 5-25 slides) to speech-relevant subsets before ASR, the model avoids context window overflow and noise accumulation. The framework maintains stable WER (~7.67-7.71%) even as information rate decreases from 2.56% to 0.87% with longer contexts.

## Foundational Learning

- **Cross-modal attention mechanisms**
  - Why needed: The pooling mechanism computes attention between speech (audio encoder output) and text (keyword embeddings); understanding Query/Key/Value attention patterns is prerequisite.
  - Quick check: Given speech embedding hx ∈ R^(T×d) and context embedding hz ∈ R^(C×d), how would you compute per-token relevance scores α ∈ R^C?

- **Autoregressive language modeling with conditioning**
  - Why needed: The SpeechLLM generates text conditioned on both audio features and textual context; understanding p(yj|X, Z, y<j) formulation is essential.
  - Quick check: In Eq. 1, what happens to the conditional probability if Z contains irrelevant keywords?

- **Context window limitations in Transformers**
  - Why needed: The paper addresses scenarios where context length (402-1562 tokens median) approaches or exceeds LLM context limits, motivating the compression approach.
  - Quick check: If an LLM has a 2048-token context window and you have 500 speech tokens plus 1500 context tokens, what happens without compression?

## Architecture Onboarding

- **Component map:**
  ```
  Audio Input → Speech Encoder (frozen) → Speech Embedding hx
  OCR Keywords → Text Embedding Layer → Context Embedding hz
  [hx, hz] → Cross-Modal Attention (Eq. 8) → Attention Weights α
  [α, hz] → Window-wise Pooling (Eqs. 11-13) → Compressed ĥz
  
  Stage 1: [Audio, ĥz] → SpeechLLM (LoRA-tuned) → Pruned Keywords Z̃
  Stage 2: [Audio, ĥz from Z̃] → SpeechLLM (LoRA-tuned) → Transcription Y
  ```

- **Critical path:**
  1. Stage 1 pruning accuracy (F1-score 93.85-94.48%) directly determines Stage 2 input quality
  2. Pooling window size n=2 balances compression vs. information retention
  3. LoRA adapters (rank 8, 20.03M parameters) applied to projector + LLM backbone only

- **Design tradeoffs:**
  - TPI (two-stage) vs. PC (prompt concatenation): TPI adds inference latency (~20% overhead per Table 6 Stage I) but improves B-WER by 1.52-3.20%
  - Pooling window size: n=2 vs. n=4 vs. n=8 (Figure 4); n=2 yields lowest WER
  - Training data scale: L95 (473h) vs. S95 (161h); larger training data improves generalization (Table 1)

- **Failure signatures:**
  - Low F1-score in Stage 1 (<80%): Over-pruning removes relevant keywords → B-WER increases despite correct ASR
  - WER degrades with longer contexts: Pooling window too small or attention scores uninformative
  - U-WER increases: Context over-influences non-biased words (SAP2-TPI maintains U-WER ~8.08%, Table 1)

- **First 3 experiments:**
  1. **Baseline replication:** Fine-tune Qwen2-Audio with prompt concatenation (PC) on SlideSpeech S95; measure WER/B-WER/Recall with 1-slide vs. 5-slide contexts. Expected: WER ~7.85-7.93%, B-WER ~6.24-6.36% (Table 3).
  2. **Pooling ablation:** Compare Qwen2-Audio-TPI (no pooling) vs. SAP2-TPI (with pooling) on 5-slide L95; measure training time and B-WER. Expected: Training time reduction ~24.97%, B-WER improvement from 4.12% to 4.54% (Tables 5, 6).
  3. **Context length stress test:** Evaluate SAP2-TPI (trained on 5-slide) on 1/3/5/7/9/15/25-slide test sets. Expected: WER remains stable (~7.67-7.71%) or improves as context increases (Figure 5).

## Open Questions the Paper Calls Out

- **How does the performance of the SAP2 framework change when utilizing semantically meaningful phrasal contexts instead of contexts split as single words?**
  - The authors note that only single words were used due to resource limitations, but future work will explore semantically meaningful phrasal contexts. This leaves the handling of multi-word expressions or noun phrases untested.

- **Can the integration of visual embeddings directly into the SAP2 architecture further enhance ASR performance in multimodal scenarios?**
  - The authors list "multimodal fusion with visual embeddings" as a specific future direction. The current method relies exclusively on textual OCR data and audio, ignoring the potential visual features present in the slides.

- **What specific architectural modifications are necessary to enable a single Joint Pruning-Integration (JPI) model to match the performance of the two-stage pipeline?**
  - The ablation study shows the SAP2-JPI method suffers from performance degradation due to interference between keyword selection and recognition objectives. The paper identifies the interference but does not propose a method to mitigate it within a single model.

## Limitations
- Scalability uncertainty to extremely long contexts (e.g., multi-hour recordings with hundreds of slides) where keyword relevance becomes increasingly ambiguous
- Dependence on OCR quality with no systematic analysis of how OCR errors propagate through the pruning and recognition pipeline
- Limited evaluation scope across diverse ASR domains (medical, legal, conversational) where keyword distributions and contextual relevance patterns may differ substantially

## Confidence
- **High Confidence (90-95%)**: The core mechanism of Speech-Driven Attention-based Pooling is well-validated with consistent WER improvements and computational efficiency gains demonstrated experimentally.
- **Medium Confidence (70-85%)**: Context length scalability claims show promising stability but are based on a limited range (1-25 slides), requiring further validation for much longer contexts.
- **Low Confidence (50-65%)**: Generalization to diverse real-world OCR quality levels and domain-specific terminology remains uncertain with no systematic analysis of failure modes when OCR quality degrades.

## Next Checks
1. **Context Length Stress Test**: Evaluate SAP2-TPI on extended context scenarios (50-100 slides or equivalent token counts) to verify whether pruning F1-score and B-WER improvements maintain stability, including qualitative analysis of pruning failures.

2. **OCR Quality Sensitivity Analysis**: Systematically degrade OCR keyword quality (introduce typos, remove keywords, add distractors) and measure impact on Stage 1 pruning F1-score and downstream Stage 2 B-WER to reveal robustness thresholds.

3. **Cross-Domain Generalization Study**: Apply SAP2 to ASR datasets from different domains (medical transcription, legal proceedings, conversational dialogue) with their own contextual keyword sources to quantify generalization capabilities and identify domain-specific challenges.