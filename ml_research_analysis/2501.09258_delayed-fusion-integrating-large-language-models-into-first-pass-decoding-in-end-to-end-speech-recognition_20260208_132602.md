---
ver: rpa2
title: 'Delayed Fusion: Integrating Large Language Models into First-Pass Decoding
  in End-to-end Speech Recognition'
arxiv_id: '2501.09258'
source_url: https://arxiv.org/abs/2501.09258
tags:
- fusion
- decoding
- delayed
- language
- shallow
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes delayed fusion, a method for integrating large
  language models (LLMs) into end-to-end speech recognition (E2E-ASR) decoding. The
  approach addresses two challenges with using LLMs in ASR: high computational cost
  and vocabulary mismatch between ASR models and LLMs.'
---

# Delayed Fusion: Integrating Large Language Models into First-Pass Decoding in End-to-end Speech Recognition

## Quick Facts
- arXiv ID: 2501.09258
- Source URL: https://arxiv.org/abs/2501.09258
- Reference count: 36
- Primary result: 4-13% WER reduction from baseline and 3-7% WER reduction from NLM shallow fusion

## Executive Summary
This paper introduces delayed fusion, a method for integrating large language models (LLMs) into end-to-end speech recognition (E2E-ASR) decoding that addresses two key challenges: high computational cost and vocabulary mismatch. The approach applies LLM scores after beam pruning during decoding, reducing both the number of hypotheses requiring LLM evaluation and the total number of LLM inference calls. It also handles different tokenizations by re-tokenizing hypotheses when word boundaries are detected, enabling cross-vocabulary integration. Experiments on the LibriHeavy corpus using three public LLMs show that delayed fusion achieves significant WER reductions while being 2.2 times faster than standard LLM shallow fusion.

## Method Summary
Delayed fusion integrates LLMs into ASR decoding by scoring hypotheses after beam pruning rather than at every expansion step. The method first prunes hypotheses using only E2E model scores, then triggers LLM scoring when a fusion condition is met (either the shortest hypothesis grows or a fixed time interval elapses). To handle vocabulary mismatch between ASR and LLM tokenizers, the decoder identifies word-complete prefixes and re-tokenizes them using the LLM's tokenizer before scoring. Hypothesis batching with key-value caching maximizes GPU utilization by computing LLM scores for all surviving hypotheses at once while reusing attention states for shared prefix tokens. The approach is evaluated on CTC-AED models trained on LibriHeavy with three public LLMs (OpenLLaMA 3B/7B and Mistral 7B).

## Key Results
- 4-13% WER reduction from baseline CTC-AED decoding on LibriHeavy test sets
- 3-7% WER reduction from NLM shallow fusion baselines
- 2.2× faster than standard LLM shallow fusion while achieving lower WER than N-best rescoring for the same decoding time
- Particularly effective for streaming applications where lattice/N-best rescoring is not feasible

## Why This Works (Mechanism)

### Mechanism 1: Post-Pruning LLM Scoring Reduces Computational Overhead
- Claim: Applying LLM scores after beam pruning reduces both the number of hypotheses requiring LLM evaluation and the total number of LLM inference calls.
- Mechanism: Standard shallow fusion calls the LLM for every partial hypothesis extension during beam search. Delayed fusion inverts this: the beam is first pruned using only the E2E model scores, and LLM scoring is triggered only when a fusion condition is met. This dramatically reduces redundant LLM invocations.
- Core assumption: The E2E ASR model's scores are sufficiently discriminative to retain correct hypotheses during pruning before LLM refinement.
- Evidence anchors: [abstract] "Delayed fusion applies LLM scores during decoding after pruning hypotheses, reducing both the number of hypotheses scored and the number of LLM inference calls."

### Mechanism 2: Word-Boundary Triggered Re-tokenization Enables Cross-Vocabulary Integration
- Claim: Detecting word boundaries and re-tokenizing only completed words allows correct LLM scoring even when ASR and LLM use incompatible tokenizers.
- Mechanism: The decoder identifies the longest tokenizable prefix ending at a word boundary. This subsequence is re-tokenized using the LLM's SentencePiece tokenizer, producing a unique token sequence for which the LLM can compute a valid probability score. Incomplete words are excluded until completed.
- Core assumption: SentencePiece tokenization is deterministic and produces unique token sequences for completed words, enabling consistent re-tokenization.
- Evidence anchors: [abstract] "The method also handles different tokenizations by re-tokenizing hypotheses when word boundaries are detected."

### Mechanism 3: Hypothesis Batching with KV-Cache Maximizes GPU Utilization
- Claim: Batching all current hypotheses for LLM scoring with key-value caching enables efficient GPU-accelerated inference despite delayed scoring.
- Mechanism: When the fusion condition triggers, all surviving hypotheses are submitted to the LLM as a batch. The KV-cache stores computed attention states for shared prefix tokens, avoiding redundant computation. New scores are computed only for the delta tokens beyond the cached prefix.
- Core assumption: Hypotheses share substantial prefix overlap, making KV-cache reuse effective; GPU memory is sufficient for batched inference.
- Evidence anchors: [Section III-B] "We compute LLM scores for all the current hypotheses at once, where we use hypothesis batching and a key-value cache to take full advantage of GPU acceleration."

## Foundational Learning

- **Beam search and pruning in ASR decoding**: Understanding beam search is prerequisite to understanding where the delay is inserted. Quick check: In a beam of size K=10, what happens to a hypothesis ranked 11th after a pruning step?
- **Shallow fusion vs. N-best rescoring**: The paper positions delayed fusion as a middle ground—incorporating LLM signals earlier than rescoring but more efficiently than shallow fusion. Quick check: Why does shallow fusion require identical vocabularies between ASR and LM, while N-best rescoring does not?
- **Subword tokenization (SentencePiece/BPE)**: The re-tokenization mechanism depends on understanding how subword tokenizers work and why word boundaries are necessary for consistent cross-tokenizer mapping. Quick check: If an ASR uses 6K wordpieces and an LLM uses 32K tokens, why can't you directly score ASR hypotheses with the LLM?

## Architecture Onboarding

- **Component map**: Audio → E2E encoder → frame-level logits → Beam search decoder → Word boundary detector → Re-tokenizer → LLM → Score combiner
- **Critical path**:
  1. Audio → E2E encoder → frame-level logits
  2. Beam search extends hypotheses token-by-token
  3. Pruning retains top-K by combined E2E + cached LLM scores
  4. On fusion trigger (shortest-hypothesis growth OR fixed interval):
     - Detect word-complete prefixes
     - Re-tokenize with LLM tokenizer
     - Batch all hypotheses → LLM with KV-cache
     - Update LLM scores for new tokens only
  5. Final hypothesis selection by combined score
- **Design tradeoffs**:
  - Shortest-hypothesis vs. fixed-interval fusion: Shortest-hypothesis minimizes LLM calls but may delay scoring; fixed-interval offers tunable speed/accuracy tradeoff
  - Beam size: Larger beams reduce pruning errors but increase LLM batch size and memory
  - With/without NLM shallow fusion: Combining NLM shallow fusion with LLM delayed fusion reduces pruning errors but adds NLM overhead
- **Failure signatures**:
  - Pruning errors manifest as: Correct hypothesis eliminated before LLM scoring → higher WER than N-best rescoring on long utterances
  - Re-tokenization failures manifest as: Jagged WER patterns on specific word types; errors at word boundaries
  - Latency spikes indicate: KV-cache invalidation due to early hypothesis divergence; consider reducing beam or increasing interval
- **First 3 experiments**:
  1. Reproduce baseline comparison: Run CTC prefix beam search with/without NLM shallow fusion on LibriHeavy test-clean; verify WER and RTF match baseline before adding LLM.
  2. Ablate fusion interval: Test fixed-interval delayed fusion with I={16, 32, 64} frames; plot WER vs. RTF curve to identify optimal operating point for your latency budget.
  3. Validate re-tokenization: Compare delayed fusion with matched-vocabulary shallow fusion (requires retraining ASR with LLM tokenizer); quantify any accuracy gap attributable to re-tokenization approximation.

## Open Questions the Paper Calls Out
- How does delayed fusion perform on RNN Transducer architectures compared to the CTC-AED models tested in this study?
- What is the GPU memory consumption of delayed fusion compared to shallow fusion and N-best rescoring, particularly for long utterances?
- How does true streaming performance of delayed fusion compare to full-utterance decoding in terms of latency and accuracy?
- How does delayed fusion generalize to different domains and languages beyond English audiobook data?

## Limitations
- Re-tokenization mechanism depends on accurate word boundary detection and deterministic mapping between ASR and LLM tokenizers, which may not work reliably across all languages and tokenization schemes
- Efficiency gains measured only on LibriHeavy corpus with specific beam sizes and fusion intervals; may vary significantly under different conditions
- All experiments use English audiobooks; effectiveness for streaming applications, conversational speech, or non-English languages is not validated

## Confidence
- **High Confidence**: Delayed fusion achieves lower WER than N-best rescoring for the same decoding time on LibriHeavy test sets; successfully handles vocabulary mismatch through word-boundary-triggered re-tokenization for the tested English audiobook domain; is 2.2× faster than LLM shallow fusion on tested configurations
- **Medium Confidence**: Delayed fusion provides particular advantages for streaming applications (claimed but not experimentally validated); efficiency gains from hypothesis batching with KV-cache are significant (no ablation studies provided); combining NLM shallow fusion with LLM delayed fusion improves accuracy (tested but not extensively analyzed)
- **Low Confidence**: Re-tokenization mechanism will work reliably across all languages and tokenization schemes (not tested beyond English); optimal fusion interval can be determined without extensive tuning for new applications; method will scale efficiently to much larger beam sizes or different ASR architectures

## Next Checks
1. **Streaming Performance Validation**: Implement delayed fusion in a streaming ASR system and measure real-time WER degradation, latency, and memory usage as a function of look-ahead buffer size. Compare against standard shallow fusion and CTC-only streaming baselines on conversational speech datasets.

2. **Ablation Study on Fusion Interval and Beam Size**: Systematically vary beam size (5, 10, 20, 50) and fixed fusion intervals (8, 16, 32, 64, 128 frames) on LibriHeavy to generate WER vs. RTF tradeoff curves. Identify Pareto-optimal operating points and test whether the 2.2× speedup holds across this parameter space.

3. **Cross-Lingual and Cross-Genre Generalization**: Apply delayed fusion to ASR models trained on non-English languages (e.g., Mandarin, Spanish) and conversational speech datasets (e.g., Switchboard, CHiME). Measure WER degradation attributable to re-tokenization failures and test whether word boundary detection mechanisms need adaptation for morphologically rich languages.