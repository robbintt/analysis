---
ver: rpa2
title: Generative AI for Named Entity Recognition in Low-Resource Language Nepali
arxiv_id: '2503.09822'
source_url: https://arxiv.org/abs/2503.09822
tags:
- entity
- nepali
- language
- tion
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the effectiveness of generative Large Language
  Models (LLMs) for Named Entity Recognition (NER) in Nepali, a low-resource language.
  The researchers conducted experiments using state-of-the-art LLMs with various prompting
  techniques, including zero-shot and few-shot settings with random and semantic example
  selection.
---

# Generative AI for Named Entity Recognition in Low-Resource Language Nepali

## Quick Facts
- arXiv ID: 2503.09822
- Source URL: https://arxiv.org/abs/2503.09822
- Reference count: 4
- Primary result: GPT-4o with semantic example selection achieves F1-micro 0.64 on Nepali NER

## Executive Summary
This study evaluates generative Large Language Models (LLMs) for Named Entity Recognition (NER) in Nepali, a low-resource language. The researchers conducted experiments using GPT-4o with various prompting techniques, including zero-shot and few-shot settings with random and semantic example selection. They also explored prompt language (English vs. Nepali) and implemented a self-verification mechanism to improve prediction accuracy. Results showed that semantic selection and English prompts yielded better performance than random selection and Nepali prompts, with F1-micro scores of 0.64 and 0.57, respectively. Self-verification improved precision but reduced recall. Overall, generative LLMs demonstrated promise for NER in low-resource languages, particularly for common entities like person names, though traditional methods still outperformed them in accuracy.

## Method Summary
The study uses GPT-4o with few-shot prompting to perform NER on the EverestNER dataset containing 24,587 annotated entities across 5 types: LOCATION, ORGANIZATION, PERSON, DATE, and EVENT. The methodology involves semantic example selection using NPVec1 BERT embeddings to retrieve similar training sentences, construction of English prompts with @@Entity## delimiters, and optional self-verification. Two evaluation modes are used: merged (with priority ordering LOC>ORG>PER>DATE>EVT) and entity-wise. The approach is compared against traditional discriminative models and evaluated using CoNLL metrics including Precision, Recall, and F1-micro.

## Key Results
- Semantic example selection (k=10) achieved F1-micro of 0.64 vs. 0.55 for random selection
- English prompts outperformed Nepali prompts with F1-micro scores of 0.64 vs. 0.57
- Self-verification improved precision from 0.50 to 0.66 but reduced recall from 0.85 to 0.71
- DATE entity recall dropped significantly from 0.71 to 0.23 during verification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic example selection in few-shot prompting appears to improve NER performance consistency compared to random selection for low-resource languages.
- Mechanism: The system uses a sentence encoder (NPVec1 BERT) to embed the test sentence and retrieves *k* most similar examples from the training data using cosine similarity. This aligns the context window with structurally or topically similar instances, likely activating relevant linguistic patterns in the LLM.
- Core assumption: The embedding space of the retriever accurately reflects the linguistic features required for the entity recognition task.
- Evidence anchors:
  - [abstract] "Results showed that semantic selection... yielded better performance than random selection."
  - [page 5, table 5] Semantic selection (k=10) achieved an F1-micro of 0.64 vs. 0.55 for random selection in the non-merged setting.
  - [corpus] *Tokenization Matters: Improving Zero-Shot NER for Indic Languages* suggests that vocabulary and segmentation significantly impact low-resource performance; semantic selection may implicitly bridge tokenization gaps better than random sampling.
- Break condition: If the embedding model fails to capture semantic nuances specific to the target entity types (e.g., rare events), retrieval quality degrades, negating the advantage.

### Mechanism 2
- Claim: English-language prompts may serve as more effective instruction carriers than native-language (Nepali) prompts for multilingual models heavily pre-trained on English.
- Mechanism: The prompt instructions and delimiters are kept in English while the target sentence remains in Nepali. This leverages the model's stronger adherence to English instructions for formatting and task logic, reducing syntactic ambiguity in the command structure.
- Core assumption: The LLM has sufficient cross-lingual alignment capabilities to map English instructions onto non-English content tokens.
- Evidence anchors:
  - [abstract] "...English prompts yielded better performance... with F1-micro scores of 0.64 and 0.57, respectively."
  - [page 5] "English prompts outperformed Nepali prompts across all metrics... likely due to the model’s extensive pre-training on English data."
  - [corpus] Evidence regarding the specific mechanism of English vs. Native prompting efficiency is weak in the provided corpus neighbors.
- Break condition: If the target language lacks robust representation in the model's latent space, English instructions may fail to ground the reasoning process effectively in the target text.

### Mechanism 3
- Claim: Self-verification acts as a secondary filter to increase precision, but introduces a recall penalty.
- Mechanism: The LLM generates an initial prediction, then is prompted a second time to validate its own output with a simple Yes/No question. This creates a gating mechanism where low-confidence or "hallucinated" entities are rejected.
- Core assumption: The model possesses sufficient self-calibration to distinguish between valid and invalid extractions when explicitly queried.
- Evidence anchors:
  - [abstract] "Self-verification improved precision but reduced recall."
  - [page 6, table 9] Precision improved from 0.50 to 0.66, while recall dropped from 0.85 to 0.71.
  - [corpus] *Assessment of Generative Named Entity Recognition in the Era of Large Language Models* investigates generative NER capabilities, supporting the premise that generation-to-validation is a distinct paradigm from direct classification.
- Break condition: If the model is systematically overconfident in its errors, self-verification will simply reinforce false positives rather than filtering them.

## Foundational Learning

- Concept: **In-Context Learning (Few-Shot)**
  - Why needed here: The methodology relies entirely on prompting without weight updates. Understanding how *k* examples define the task boundary is critical.
  - Quick check question: Does increasing *k* (examples) always improve performance, or is there a saturation point where context window noise increases?

- Concept: **BIO Tagging Scheme**
  - Why needed here: The paper converts generative output (text with delimiters) into the standard Beginning-Inside-Outside format for evaluation.
  - Quick check question: How does the system handle a generated entity that spans tokens unpredictably when mapping to BIO labels?

- Concept: **Generative vs. Discriminative NER**
  - Why needed here: The study compares GPT-4o (generative) against BERT-based (discriminative/encoder) baselines.
  - Quick check question: Why might a generative model struggle with precision compared to a discriminative model trained on the full dataset?

## Architecture Onboarding

- Component map:
  - NPVec1 BERT -> k-NN Retriever -> Prompt Constructor -> GPT-4o -> Parser -> Validator (optional)

- Critical path:
  1. Embed test sentence -> Retrieve similar examples
  2. Construct English prompt with examples
  3. Generate output with @@Entity## format
  4. Parse to BIO format (handle conflicts via priority: LOC > ORG > PER)

- Design tradeoffs:
  - **Merged vs. Entity-wise Evaluation**: Evaluating all entities in one pass (Merged) causes performance drops due to label conflicts (e.g., "Real" as Location vs. Organization). The paper suggests Entity-wise (separate prompts per entity type) is more accurate but computationally 5x more expensive.
  - **Prompt Language**: English prompts yield higher accuracy (0.64 F1) but reduce accessibility for native pipelines; Nepali prompts (0.57 F1) are lower latency in terms of cognitive load for native speakers but less effective for the model.

- Failure signatures:
  - **Output Inconsistency**: Smaller models (LLAMA-3-8B, Mistral) failed to adhere to delimiter formats.
  - **Hallucination**: The model may tag words as entities even when none exist (addressed partially by self-verification).
  - **Entity Confusion**: Ambiguity between Organization and Location names (e.g., sports teams vs. cities) causes precision drops in merged settings.

- First 3 experiments:
  1. **Baseline Calibration**: Run zero-shot prompting (k=0) on a small sample to verify the LLM understands the delimiter format instructions.
  2. **Retrieval Ablation**: Compare Random vs. Semantic selection with k=5 to quantify the signal lift from the NPVec1 embedder.
  3. **Self-Verification Pilot**: Run the verification step on 50 random samples to measure the Precision/Recall trade-off specific to your data domain.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the self-verification mechanism be refined to boost precision without incurring the significant loss of recall observed in this study?
- Basis in paper: [explicit] The authors explicitly state in the conclusion that "Further research should refine self-verification method to boost precision without sacrificing recall," noting that DATE entity recall dropped from 0.71 to 0.23 during verification.
- Why unresolved: The current binary verification process acts as a strict filter, successfully removing false positives (improving precision) but aggressively discarding valid entities (lowering recall).
- What evidence would resolve it: A modified verification strategy that maintains the precision gains (e.g., >0.66 micro precision) while stabilizing recall closer to the non-verified baseline.

### Open Question 2
- Question: To what extent does instruction fine-tuning improve generative LLM performance on Nepali NER compared to the few-shot prompting methods evaluated?
- Basis in paper: [explicit] The conclusion lists "instruction fine-tuning to enhance LLM’s understanding of Nepali entity patterns" as a specific direction for future work to address current limitations.
- Why unresolved: The current study relied solely on in-context learning (few-shot prompting); the potential gains from updating model weights via fine-tuning remain untested.
- What evidence would resolve it: A comparative study benchmarking a Nepali-instruction-tuned LLM against the few-shot semantic selection baseline (F1-micro 0.64) on the EverestNER dataset.

### Open Question 3
- Question: Can dedicated pre-training or prompt engineering strategies close the performance gap between English and Nepali prompts?
- Basis in paper: [inferred] The paper reports that English prompts outperformed Nepali prompts (0.64 vs 0.57 F1-micro), suggesting the model's pre-training data imbalance limits its ability to follow native-language instructions effectively.
- Why unresolved: The paper identifies the symptom (English prompts are more effective) but does not test methods to improve the model's responsiveness to low-resource language prompts.
- What evidence would resolve it: An experiment demonstrating that a domain-adapted or language-specialized model can achieve comparable or superior performance using Nepali prompts compared to English prompts.

### Open Question 4
- Question: Can open-source LLMs (e.g., Llama-3, Mistral) be stabilized to match the output consistency of proprietary models like GPT-4o for low-resource NER?
- Basis in paper: [inferred] The authors excluded Llama-3 and Mistral from the final evaluation because they generated "very inconsistent output" compared to GPT-4o, leaving their true performance potential unresolved.
- Why unresolved: The inconsistency prevented evaluation; it is unclear if the failure was due to the models' intrinsic capabilities or simply a lack of robustness in the specific prompting format used.
- What evidence would resolve it: An experiment where smaller, open-source models are fine-tuned or prompted with robust formatting constraints to achieve the parsing consistency required for NER evaluation.

## Limitations
- The F1-micro score of 0.64 remains substantially below traditional discriminative models on the same dataset
- Self-verification mechanism creates significant recall penalty, particularly for DATE entities (recall drops from 0.71 to 0.23)
- The approach relies on a single low-resource language (Nepali), limiting generalizability

## Confidence

- **High Confidence**: The observation that semantic example selection outperforms random selection (F1-micro 0.64 vs. 0.55) is well-supported by the experimental results and follows logically from the retrieval mechanism described.
- **Medium Confidence**: The superiority of English prompts over Nepali prompts (0.64 vs. 0.57 F1) is demonstrated, but the underlying mechanism relies on assumptions about the model's cross-lingual capabilities that could vary across different LLM architectures.
- **Medium Confidence**: The self-verification mechanism's precision-recall tradeoff is clearly demonstrated, but the optimal balance point and entity-specific effectiveness require further investigation.

## Next Checks

1. **Entity-Wise Performance Analysis**: Conduct detailed ablation studies to determine which entity types benefit most from semantic selection versus English prompts, as current results suggest variable effectiveness across entity categories.

2. **Verification Threshold Optimization**: Implement graduated self-verification with adjustable confidence thresholds to identify optimal precision-recall balance points for different entity types rather than the binary approach tested.

3. **Cross-Lingual Prompt Generalization**: Test the English prompt advantage across multiple low-resource languages to determine whether this represents a generalizable pattern or is specific to Nepali's linguistic features and the model's training corpus.