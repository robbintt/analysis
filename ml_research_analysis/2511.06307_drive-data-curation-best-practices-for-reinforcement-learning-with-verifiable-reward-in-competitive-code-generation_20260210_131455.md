---
ver: rpa2
title: 'DRIVE: Data Curation Best Practices for Reinforcement Learning with Verifiable
  Reward in Competitive Code Generation'
arxiv_id: '2511.06307'
source_url: https://arxiv.org/abs/2511.06307
tags:
- training
- performance
- stage
- arxiv
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the underexplored area of competitive programming\
  \ in reinforcement learning with verifiable rewards (RLVR), where models must generate\
  \ executable, efficient code to solve complex algorithmic problems. The authors\
  \ identify key limitations in standard RLVR approaches\u2014low entropy, repetitive\
  \ generation, and poor handling of challenging problems\u2014and propose a two-stage\
  \ framework: first, entropy expansion using uniformly distributed problems with\
  \ moderate rollout budgets to enhance exploration; second, a hard-focus curriculum\
  \ with large rollout budgets (64 per prompt) on progressively difficult problems."
---

# DRIVE: Data Curation Best Practices for Reinforcement Learning with Verifiable Reward in Competitive Code Generation

## Quick Facts
- **arXiv ID:** 2511.06307
- **Source URL:** https://arxiv.org/abs/2511.06307
- **Reference count:** 40
- **Primary result:** Two-stage RLVR framework achieves state-of-the-art competitive programming performance with 58% relative improvement over SFT baselines

## Executive Summary
This paper addresses key limitations in reinforcement learning with verifiable rewards (RLVR) for competitive code generation, including low entropy, repetitive generation, and poor handling of challenging problems. The authors propose a two-stage framework: entropy expansion using uniformly distributed problems with moderate rollout budgets to enhance exploration, followed by a hard-focus curriculum with large rollout budgets on progressively difficult problems. Implemented on Qwen2.5-32B, the method achieves state-of-the-art performance among models of similar scale, with extensive ablation studies confirming the effectiveness of entropy expansion and curriculum design.

## Method Summary
The approach consists of three phases: (1) SFT on Qwen2.5-32B-Instruct with 470K curated prompts distilled from DeepSeekR1-0528, (2) Stage 1 RL using GRPO on 9K uniformly distributed competitive programming problems with 8 rollouts and 24k context to expand entropy, and (3) Stage 2 RL with a hard-focus curriculum on LiveCode V6 (175 problems) filtered through Pre-GRPO to retain only the hardest cases, using 64 rollouts and 32k context across three progressive phases (72→50→25 hardest cases).

## Key Results
- Achieves 58% relative improvement on Codeforces contests compared to SFT baseline
- Demonstrates competitive performance against much larger models like DeepSeek v3.1
- Extensive ablation studies confirm entropy expansion and curriculum design are critical components
- Scaling experiments on internal MoE model show positive trends despite limited training steps

## Why This Works (Mechanism)

### Mechanism 1: Entropy Expansion via Moderate Rollouts on Uniform Distribution
Training on uniformly distributed problems with moderate rollout budgets (8 per prompt) and shorter context (24k tokens) increases output diversity and reduces mode collapse. Broad problem distribution forces exploration across solution templates; limited rollouts prevent overfitting to specific solution modes; shorter context regularizes against repetitive reasoning chains. Break condition: If base model already has high output entropy, Stage 1 may provide diminishing returns.

### Mechanism 2: Large Rollout Budgets Enable Learning of Hard Problems
Challenging competitive programming problems require 64+ rollouts per prompt for stable learning; moderate budgets (8) insufficient for low-initial-accuracy cases. Hard problems have sparse reward landscapes; more samples increase probability of discovering successful solutions within each GRPO group, providing meaningful gradient signal. Break condition: If problem difficulty is misclassified (e.g., "hard" label due to test case errors rather than algorithmic complexity), large rollouts waste compute.

### Mechanism 3: Hard-Focus Curriculum Retains Learning Frontier
Continuously retaining the most difficult instances throughout training extends model's problem-solving boundary. Pre-GRPO filters low-pass-rate cases forward across phases; curriculum prevents optimization from abandoning hard cases once easier ones are solved; progressive filtering ensures compute concentrates on current capability frontier. Break condition: If hard problems are systematically different from evaluation distribution, curriculum will overfit to unrepresentative hard cases.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed here: Core RL algorithm used; differs from PPO in how it computes advantages from group samples rather than value function estimation.
  - Quick check question: Can you explain how GRPO computes advantages differently than PPO, and why this matters for verifiable-reward tasks?

- **Concept: Verifiable Rewards in Code Generation**
  - Why needed here: Entire framework depends on test-case execution providing binary/cost signals; understanding reward sparsity is critical for rollout budget decisions.
  - Quick check question: For a competitive programming problem with 5 test cases, how would you construct a dense vs. sparse reward signal, and what are the tradeoffs?

- **Concept: Mode Collapse and Entropy in LLM Fine-tuning**
  - Why needed here: Paper's central diagnostic (Figure 3) assumes you understand why low entropy during SFT leads to repetitive generation; Stage 1 is designed to counter this.
  - Quick check question: If a model generates syntactically correct but algorithmically identical solutions across different problem types, what does this suggest about its output entropy, and what intervention would you try first?

## Architecture Onboarding

- **Component map:** SFT Phase → RL Stage 1 (Entropy Expansion) → RL Stage 2 (Hard-Focus Curriculum) → Evaluation
- **Critical path:** 1) Difficulty classifier training, 2) SFT data curation via arena learning + twice-hard duplication, 3) Stage 1 entropy expansion (check entropy increases), 4) Stage 2 curriculum (monitor per-phase accuracy), 5) Evaluation on uncontaminated benchmarks
- **Design tradeoffs:** Rollout budget vs. compute (64 rollouts is 8× more expensive than 8); context length varies (24k→32k) for truncation vs. reasoning complexity; dataset size vs. quality (470K→175 high-quality problems)
- **Failure signatures:** Stage 1 insufficient entropy (repetition patterns persist); Stage 2 curriculum collapse (training accuracy plateaus below 50%); reward hacking (pass@1 improves but pass@10 stagnates); distribution shift (LiveCodeV6 improves but Codeforces degrades)
- **First 3 experiments:** 1) Entropy diagnostic: measure output entropy on held-out problems before Stage 1, 2) Rollout budget ablation: compare learning curves with 8 vs. 32 vs. 64 rollouts on 10 hard problems, 3) Curriculum vs. uniform ablation: replicate Table 4 comparison on your own model

## Open Questions the Paper Calls Out

1. Can adaptive curriculum strategies improve upon the fixed three-phase progressive curriculum used in the hard-focus stage? (Future work suggested in conclusion)

2. Can more efficient sampling techniques reduce the computational cost of the large rollout budgets (64+) required for challenging problems? (Efficiency bottleneck noted in conclusion)

3. Does the "hard-focus" curriculum strategy generalize to full convergence in Mixture-of-Experts (MoE) models? (Scaling experiments were limited to 50 steps)

## Limitations
- Statistical significance tests and confidence intervals are not provided for claimed improvements
- Scaling experiments on internal MoE model are preliminary (only 50 steps, not converged)
- Pre-GRPO filtering mechanism's exact implementation details are underspecified
- Paper does not explore whether entropy expansion benefits could be achieved through alternative means

## Confidence

- **High confidence:** Stage 1 entropy expansion improves performance on LiveCode benchmarks and prevents training collapse when Stage 2 includes easy problems (directly supported by ablation Table 4)
- **Medium confidence:** 64-rollout budgets are necessary for learning hard problems (supported by single-case ablation but could reflect model scale or specific problem distribution)
- **Medium confidence:** Hard-focus curriculum outperforms uniform sampling of LiveCodeV6 (supported by large relative improvements but mechanism could be specific to this problem distribution)
- **Low confidence:** Scaling trends on internal MoE model (data is insufficient to establish reliable scaling laws)

## Next Checks

1. **Statistical validation of ablation results:** Replicate Table 4 with proper confidence intervals and statistical tests (e.g., paired t-tests across multiple random seeds) to confirm that the 58% relative improvement on Codeforces is statistically significant.

2. **Alternative entropy expansion validation:** Test whether entropy expansion can be achieved through simpler means (e.g., higher sampling temperature during SFT or GRPO) rather than the full two-stage curriculum.

3. **Cross-distribution curriculum validation:** Evaluate the hard-focus curriculum on a different competitive programming dataset (e.g., problems from a different time period or platform) to test whether the curriculum design generalizes beyond the specific LiveCodeV6 distribution.