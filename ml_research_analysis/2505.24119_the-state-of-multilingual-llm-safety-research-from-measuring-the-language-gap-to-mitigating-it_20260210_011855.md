---
ver: rpa2
title: 'The State of Multilingual LLM Safety Research: From Measuring the Language
  Gap to Mitigating It'
arxiv_id: '2505.24119'
source_url: https://arxiv.org/abs/2505.24119
tags:
- safety
- language
- multilingual
- linguistics
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reveals a significant English-centric bias in multilingual
  LLM safety research, with over 80% of publications focusing solely on English despite
  growing global deployment. Even high-resource non-English languages like Mandarin
  receive only ~10% of the research attention compared to English.
---

# The State of Multilingual LLM Safety Research: From Measuring the Language Gap to Mitigating It

## Quick Facts
- arXiv ID: 2505.24119
- Source URL: https://arxiv.org/abs/2505.24119
- Authors: Zheng-Xin Yong; Beyza Ermis; Marzieh Fadaee; Stephen H. Bach; Julia Kreutzer
- Reference count: 40
- Primary result: Over 80% of multilingual LLM safety research focuses exclusively on English despite growing global deployment

## Executive Summary
This paper reveals a significant English-centric bias in multilingual LLM safety research, with over 80% of publications focusing solely on English despite growing global deployment. Even high-resource non-English languages like Mandarin receive only ~10% of the research attention compared to English. The study finds that non-English languages are rarely studied in depth as standalone topics, instead appearing superficially in broad multilingual evaluations that lack cultural nuance. Additionally, only half of English safety research documents the languages studied, obscuring coverage limitations.

To address these gaps, the paper proposes three research directions: developing culturally-grounded evaluation benchmarks, generating diverse multilingual safety training data, and understanding crosslingual safety generalization through mechanistic interpretability and training data influence analysis. These recommendations aim to create more equitable, inclusive AI safety practices for diverse global populations.

## Method Summary
The authors conducted a comprehensive analysis of 1,093 safety publications to characterize the current state of multilingual LLM safety research. They systematically examined language coverage patterns across publications, identifying which languages were studied and in what depth. The analysis distinguished between English-only research, high-resource non-English languages (like Mandarin), and low-resource language coverage. They also assessed how non-English languages were typically incorporated - whether as standalone research topics or as superficial additions to multilingual evaluations. The study further evaluated documentation practices regarding language specification in safety research.

## Key Results
- Over 80% of multilingual LLM safety research focuses exclusively on English despite global deployment
- High-resource non-English languages like Mandarin receive only ~10% of research attention compared to English
- Only half of English safety research documents which languages were studied, obscuring coverage limitations

## Why This Works (Mechanism)
The proposed research directions address fundamental gaps in current safety research methodology. Culturally-grounded benchmarks would capture safety-relevant nuances specific to different linguistic and cultural contexts, moving beyond surface-level multilingual evaluations. Diverse multilingual safety training data would enable models to learn safety concepts across different cultural frameworks rather than relying on English-centric safety patterns. Understanding crosslingual generalization through mechanistic interpretability would reveal how safety knowledge transfers (or fails to transfer) across languages, informing more effective training strategies.

## Foundational Learning
- Crosslingual generalization: Understanding how safety knowledge transfers between languages is crucial because models trained primarily on English data may not apply safety principles effectively to non-English contexts. Quick check: Compare safety performance across languages for models with varying amounts of non-English safety training data.
- Cultural safety nuances: Different cultures have varying interpretations of what constitutes harmful or unsafe content, making culturally-specific evaluation essential. Quick check: Conduct expert reviews of safety incidents across cultures to identify divergent patterns.
- Mechanistic interpretability: Tools for understanding model internals across languages can reveal how safety concepts are represented and generalized. Quick check: Apply circuit analysis to compare safety-related neuron activation patterns across languages.

## Architecture Onboarding
- Component map: Safety evaluation benchmarks -> Training data generation -> Mechanistic interpretability analysis -> Crosslingual generalization understanding
- Critical path: Benchmark development → Training data creation → Model training → Interpretability analysis → Safety generalization validation
- Design tradeoffs: English-centric research is easier due to abundant resources but creates coverage gaps; culturally-specific research is more resource-intensive but produces more equitable safety outcomes
- Failure signatures: Superficial multilingual coverage without cultural depth, undocumented language coverage, poor safety transfer across languages
- First experiments: 1) Pilot culturally-specific safety scenarios for 3-5 non-English languages, 2) Generate multilingual safety training data with cultural consultation, 3) Apply mechanistic interpretability to compare safety representations across languages

## Open Questions the Paper Calls Out
None

## Limitations
- Focus on academic publications may underrepresent industry safety research with different language priorities
- Analysis relies on publication metadata and abstracts, potentially missing nuanced safety discussions
- Assumes culturally-grounded benchmarks can be created with sufficient cultural authenticity

## Confidence
High confidence for English-centric bias characterization (80%+ English focus documented across 1,093 publications)
Medium confidence for superficial multilingual evaluation coverage patterns (methodological constraints in assessing cultural nuance)
High confidence for proposed research directions as logical responses to documented gaps

## Next Checks
1. Conduct systematic audit of commercial LLM safety documentation to compare industry versus academic language coverage patterns
2. Develop and pilot test culturally-specific safety evaluation scenarios for 3-5 non-English languages to assess feasibility of culturally-grounded benchmarks
3. Implement small-scale crosslingual safety generalization study using mechanistic interpretability tools on models with varying non-English safety data