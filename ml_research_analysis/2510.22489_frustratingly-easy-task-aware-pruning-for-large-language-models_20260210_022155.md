---
ver: rpa2
title: Frustratingly Easy Task-aware Pruning for Large Language Models
arxiv_id: '2510.22489'
source_url: https://arxiv.org/abs/2510.22489
tags:
- pruning
- arxiv
- task-specific
- parameters
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a task-aware pruning framework for large language
  models (LLMs) that preserves task-specific capabilities while reducing model size.
  Traditional pruning methods focus on general language generation and may remove
  important task-specific parameters.
---

# Frustratingly Easy Task-aware Pruning for Large Language Models

## Quick Facts
- arXiv ID: 2510.22489
- Source URL: https://arxiv.org/abs/2510.22489
- Authors: Yuanhe Tian; Junjie Liu; Xican Yang; Haishan Ye; Yan Song
- Reference count: 40
- Primary result: Task-aware pruning method improves accuracy by 1.57-3.58% over baseline Wanda across 50-90% compression ratios

## Executive Summary
This paper introduces a task-aware pruning framework for large language models that preserves task-specific capabilities while reducing model size. Traditional pruning methods focus on general language generation and may remove important task-specific parameters. The proposed method uses both general-domain and task-specific calibration data to compute parameter importance scores, partitioning parameters into shared, general-only, and task-only groups based on activation-norm differences. These group-specific scores are then fused to guide the pruning process. Experiments on Qwen-3 (32B) demonstrate that the approach consistently outperforms the baseline Wanda method across multiple compression ratios and structured pruning patterns, effectively balancing general fluency with task-specific performance for resource-constrained deployment.

## Method Summary
The method computes per-channel activation norms separately using general-domain calibration data (128 samples from English C4) and task-specific calibration data (128 samples from union of MMLU/MedQA/ARC training splits). It partitions channels into general-only, shared, and task-only groups based on the difference in activation norms using threshold α ≈ 0.2. For each group, it computes Wanda importance scores (weight magnitude × activation norm), aggregates these scores across groups, and prunes the lowest scoring parameters globally. The approach maintains task-specific capabilities while achieving significant compression ratios (50%, 75%, 90%) with both unstructured and structured pruning patterns (2:4, 4:8 N:M sparsity).

## Key Results
- At 75% compression, achieves 43.13% accuracy on MMLU compared to 41.56% for baseline Wanda
- At 50% compression, improves ARC accuracy from 52.21% to 52.56%
- Consistently outperforms baseline across all tested compression ratios and structured pruning patterns
- Maintains better perplexity on WikiText-2 compared to baseline at all compression levels

## Why This Works (Mechanism)
The method works by recognizing that different parameters contribute differently to general language understanding versus task-specific performance. By using dual calibration sources and partitioning parameters based on their differential activation patterns, the approach can preserve task-critical weights while still achieving aggressive compression. The fusion of group-specific importance scores ensures that task-specific parameters are protected from pruning even when general parameters in the same layer might be candidates for removal.

## Foundational Learning

**Parameter Importance Scoring**: Why needed - to identify which weights contribute most to model performance; Quick check - verify scores correlate with weight magnitude and activation patterns

**Channel Partitioning**: Why needed - to separate general-purpose from task-specific parameters; Quick check - ensure meaningful distribution across three groups (general-only, shared, task-only)

**Dual Calibration**: Why needed - to capture both general and task-specific behavior; Quick check - confirm calibration datasets adequately represent both domains

**Structured vs Unstructured Pruning**: Why needed - to balance compression ratio with hardware efficiency; Quick check - verify N:M patterns maintain sparsity requirements

## Architecture Onboarding

**Component Map**: Calibration data → Forward passes → Activation norm computation → Channel partitioning → Group-specific importance scoring → Score aggregation → Pruning mask application → Compressed model

**Critical Path**: The forward pass through calibration data is the bottleneck, requiring two full model evaluations (general + task-specific) to compute activation norms

**Design Tradeoffs**: The method trades computational overhead during calibration for better task-specific performance retention. Using α = 0.2 provides good balance but may need adjustment for different domains or model sizes.

**Failure Signatures**: If α is too small, most parameters become shared and task-specific capabilities are not preserved. If α is too large, too few parameters are classified as shared, reducing compression effectiveness.

**3 First Experiments**:
1. Run forward passes on both calibration sets and visualize activation norm distributions to verify partitioning logic
2. Compute group-specific importance scores and examine score distributions to confirm task-only parameters receive high scores
3. Apply pruning at 50% ratio and measure accuracy drop on task-specific benchmarks to establish baseline improvement

## Open Questions the Paper Calls Out
None identified in the paper.

## Limitations
- Assumes task-specific calibration data is available and representative, which may not hold for all deployment scenarios
- Demonstrated primarily on a single 32B-parameter model (Qwen-3), limiting generalizability
- No analysis of inference-time overhead from maintaining task-specific parameter groups

## Confidence

High confidence: Core methodology description, implementation details for parameter grouping and score aggregation

Medium confidence: Reported performance improvements relative to baseline, optimal α selection process

Low confidence: Scalability analysis to other model families, behavior under extreme compression (>90%)

## Next Checks

1. Replicate the calibration data sampling procedure with explicit random seed documentation to verify reported performance margins (±0.1-0.2% accuracy)

2. Conduct ablation study on α parameter sensitivity using the exact per-group parameter count distributions from the original experiments

3. Test the method on at least one additional LLM family (e.g., Llama, Mistral) to validate architecture-agnostic effectiveness claims