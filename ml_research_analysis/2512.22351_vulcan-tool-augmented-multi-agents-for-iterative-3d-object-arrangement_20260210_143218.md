---
ver: rpa2
title: 'VULCAN: Tool-Augmented Multi Agents for Iterative 3D Object Arrangement'
arxiv_id: '2512.22351'
source_url: https://arxiv.org/abs/2512.22351
tags:
- object
- scene
- instruction
- agent
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# VULCAN: Tool-Augmented Multi Agents for Iterative 3D Object Arrangement

## Quick Facts
- **arXiv ID**: 2512.22351
- **Source URL**: https://arxiv.org/abs/2512.22351
- **Reference count**: 40
- **Primary result**: VULCAN achieves 0.000 collision rate and 0.003 floating rate on 111 multi-step 3D arrangement tasks, outperforming end-to-end baselines by 47.9-55.4% in plausibility and 47.1-56.3% in consistency.

## Executive Summary
VULCAN addresses the challenge of iterative 3D object arrangement by introducing a tool-augmented multi-agent framework that combines visual grounding, constraint-based optimization, and adaptive backtracking. The system decomposes the task into specialized roles—Planner, Executor, and Evaluator—each with distinct responsibilities to maintain context coherence and recover from intermediate errors. By leveraging Model Context Protocol (MCP)-based visual APIs and a constraint solver, VULCAN achieves state-of-the-art performance on complex multi-step arrangement tasks while maintaining robustness against planning errors and evaluator hallucinations.

## Method Summary
VULCAN is an inference-only system using Gemini-2.5-pro that solves iterative 3D object arrangement through a multi-agent pipeline. The Planner maintains global context and outputs local instructions with target coordinates. The Executor uses three visual probing tools (LISTOBJECTSINAREA, RAYPROBE, RENDERWITHHIGHLIGHT) to gather spatial information and formulates geometric constraints for a constraint solver. Multiple Evaluator agents vote on solution plausibility via consensus filtering. The system employs adaptive backtracking with anchor reset to recover from dead-end states, and uses Blender modifiers for collision and floating detection. The approach operates on fixed-camera rendered scenes and handles up to 5-6 step tasks.

## Key Results
- Achieves 0.000 collision rate and 0.003 floating rate on benchmark tasks
- Outperforms end-to-end baselines by 47.9-55.4% in plausibility scores
- Reduces human workload from 3 hours to 1 hour per scene through automated reasoning

## Why This Works (Mechanism)

### Mechanism 1: Tool-Augmented Visual Grounding
MCP-based visual APIs enable MLLMs to ground spatial reasoning in 3D scenes without natively parsing 3D data. Three probing tools let the Executor iteratively query object identities, surface planes, and 3D positions, shifting from brittle code generation to function-level interactions that return structured data the MLLM can reason over.

### Mechanism 2: Multi-Agent Role Decomposition
Separating planning, execution, and evaluation across specialized agents prevents context overload and improves multi-step coherence. The Planner maintains global context while the Executor and Evaluator operate only within local scope, preventing any single agent from managing both long-horizon strategy and low-level execution.

### Mechanism 3: Adaptive Backtracking with Anchor Reset
Dynamic anchor repositioning enables efficient recovery from dead-end states in exponential search spaces. The algorithm maintains an "anchor step" as a restart point, moving to half the current depth on failure or advancing when reaching new maximum depth, combining depth-first exploration with strategic rollback.

## Foundational Learning

- **Model Context Protocol (MCP) and tool calling**: Why needed here—VULCAN relies on MCP-based APIs to bridge MLLMs with 3D scene manipulation. Quick check: Can you explain how RAYPROBE converts a 2D pixel coordinate into a 3D surface plane name?

- **Constraint-based optimization for pose solving**: Why needed here—The solver translates symbolic constraints into differentiable losses optimized via AdamW. Quick check: What happens if CLOSETOPIX and NOOVERHANG conflict in their targets?

- **Consensus-based filtering and hallucination mitigation**: Why needed here—MLLM Evaluators can produce false positives. The system polls multiple evaluators and requires positive average scores. Quick check: If 3 Evaluators vote [terrible, excellent, excellent], what is the consensus score and is the solution accepted?

## Architecture Onboarding

- **Component map**: User instruction + initial render → Planner → Executor (calls visual APIs) → Constraint Solver → Collision Detector → Evaluators → Consensus check → Accept/reject → Backtracking controller (if needed)

- **Critical path**: 1) User instruction + initial render → Planner (generates local instruction + coordinate) 2) Executor probes scene via APIs → formulates constraint list → Solver generates pose candidates 3) Collision detector filters candidates → valid pose applied 4) Evaluators vote → consensus check → accept or reject 5) If rejected or failed, backtracking controller resets to anchor depth and retries

- **Design tradeoffs**: Single-camera limitation restricts occluded placements; parallel attempts (N=4) trade compute for robustness; Evaluator consensus threshold is permissive; coordinate guidance reduces ambiguity but requires accurate spatial reasoning.

- **Failure signatures**: Planning error (moving objects without clearing stacked items causes floating), Evaluator hallucination (false positive acceptance), Solver exhaustion (constraint losses remain above threshold), Backtracking loops (repeated failures reset anchor without progress).

- **First 3 experiments**: 1) Validate visual probing accuracy on 10 diverse scenes 2) Stress-test constraint solver with adversarial combinations 3) Characterize Evaluator calibration on 50 arrangements with known ground-truth quality

## Open Questions the Paper Calls Out

- **How can VULCAN support multi-view observations to resolve occlusions?** The conclusion explicitly states the system is limited to single-camera views, restricting performance on tasks requiring novel view observations.

- **Can Evaluator agents be redesigned to eliminate false positives?** The failure analysis section highlights that Evaluator agents can occasionally reach consensus on incorrect verdicts due to hallucinations, suggesting development of more robust evaluation agents as future work.

- **How does adaptive backtracking scale with longer task horizons?** The paper notes exponential search space growth and that backtracking impacts efficiency through trial-and-error, but benchmark tasks appear relatively short (5-6 steps).

## Limitations
- Single-camera limitation prevents handling occluded or non-visible objects
- Evaluator consensus threshold is permissive and may allow false positives
- MCP server implementation details are not fully specified

## Confidence
- **High**: Constraint solver implementation (σ_pix, τ, AdamW parameters), collision detection via Blender modifiers, quantitative metrics
- **Medium**: Tool-augmented visual grounding effectiveness, multi-agent decomposition benefits, backtracking algorithm efficiency
- **Low**: Real-world robustness beyond curated scenes, Evaluator calibration against human judgment, MCP server implementation specifics

## Next Checks
1. **Evaluator Calibration Test**: Run 50 known-good and 50 known-bad arrangements through the full system; measure false positive and false negative rates against ground truth to quantify hallucination impact.

2. **Tool API Accuracy Audit**: Systematically test each visual probing tool (LISTOBJECTSINAREA, RAYPROBE, RENDERWITHHIGHLIGHT) on 20 diverse scenes; measure object name recognition accuracy and plane ID correctness against Blender's ground truth.

3. **Cross-View Generalization**: Create a multi-view variant using 3 camera angles; compare single-view vs. multi-view performance on identical tasks to quantify the single-camera limitation's impact on success rates.