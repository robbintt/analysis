---
ver: rpa2
title: Large Language Models Meet Graph Neural Networks for Text-Numeric Graph Reasoning
arxiv_id: '2501.16361'
source_url: https://arxiv.org/abs/2501.16361
tags:
- graph
- gene
- cell
- embedding
- path
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces a new Text-Numeric Graph (TNG) structure that
  integrates textual annotations and numeric values for graph reasoning in scientific
  discovery. It proposes a joint Large Language Model (LLM) and Graph Neural Network
  (GNN) approach to analyze TNGs, specifically Text-Numeric Omics Signaling Graphs
  (TOSGs) generated from single-cell RNA sequencing data.
---

# Large Language Models Meet Graph Neural Networks for Text-Numeric Graph Reasoning

## Quick Facts
- arXiv ID: 2501.16361
- Source URL: https://arxiv.org/abs/2501.16361
- Reference count: 0
- Introduces Text-Numeric Graph (TNG) structure integrating textual annotations and numeric values for graph reasoning in scientific discovery

## Executive Summary
This paper proposes a novel approach for text-numeric graph reasoning by integrating Large Language Models (LLMs) with Graph Neural Networks (GNNs). The authors introduce Text-Numeric Graphs (TNGs) that combine gene expression data with biomedical text embeddings, then develop a joint LLM-GNN model to analyze these graphs. The model uses PubMedBert to generate sentence embeddings for gene descriptions and pathway texts, which are integrated with numeric gene expression data through a multi-module architecture. Evaluation on cirrhosis, Alzheimer's disease, and PDAC datasets demonstrates significant improvements in classification accuracy (0.88-0.89) and F-score (0.93-0.95) compared to baseline models, showing the effectiveness of combining LLMs with GNNs for biological pathway inference.

## Method Summary
The method consists of a three-module architecture: (1) A gene encoder that uses Graphormer-style transformers with centrality, spatial, and edge encodings to process gene expression values concatenated with PubMedBert sentence embeddings; (2) A path encoder that performs cross-attention between expression-aware path embeddings and path sentence embeddings to compute trainable path importance scores; (3) A graph encoder that applies these importance scores to path embeddings, then uses Jumping Knowledge max-pooling across transformer layers for final classification. The model integrates scRNA-seq data with biomedical text through an "Expander" module that maps gene expression to the same dimension as sentence embeddings before concatenation. Global trainable path weights provide robust pathway selection across samples, and the approach is evaluated on binary classification tasks for three diseases using 0.7/0.1/0.2 train/validation/test splits.

## Key Results
- Classification accuracy: 88.2% on cirrhosis dataset vs. 81.3% for Pathfinder baseline
- F-score improvements: 0.95 on cirrhosis, 0.93 on Alzheimer's, 0.94 on PDAC datasets
- Interpretable pathway discovery: Identified biologically meaningful inter-cell pathways (e.g., CAF to Tumor interactions involving EGFR, ESR1, PDGFRA)
- Robust performance: Consistent improvements across three independent single-cell RNA sequencing datasets

## Why This Works (Mechanism)

### Mechanism 1
Integrating LLM-derived text embeddings with numeric gene expression data improves classification accuracy over numeric-only or text-only approaches. PubMedBert generates 768-dimensional sentence embeddings for gene descriptions and pathway texts; these are concatenated with MLP-expanded expression values via the Expander module, allowing the model to leverage both semantic prior knowledge (gene function, pathway relationships) and sample-specific activation levels. Core assumption: The LLM's pre-training corpus contains biologically meaningful relationships that transfer to downstream graph reasoning tasks. Evidence: Table 3 shows accuracy improving from 0.813 (Pathfinder without LLM embeddings) to 0.882 (current model with PubMedBert embeddings). Break condition: If LLM embeddings are replaced with random vectors of same dimension and performance drops to Pathfinder levels, the mechanism is supported.

### Mechanism 2
Global trainable path importance scores provide more robust pathway selection than sample-specific attention weights. A single trainable weight vector M ∈ R^p is learned across all samples via backpropagation, then passed through sigmoid to produce importance scores I. This contrasts with attention computed per-sample, which the authors claim is "not robust and may vary a lot even given a minor variation." Core assumption: Biologically important pathways are consistent across samples within a condition class. Evidence: Page 11 introduces the trainable path score M ∈ R^p as "identical to all samples and layers" to avoid attribution robustness issues. Break condition: Compare coefficient of variation for path importance scores across repeated runs; if sample-specific attention shows >2x higher variance than global weights while maintaining similar mean performance, mechanism is supported.

### Mechanism 3
Cross-attention between expression-aware path embeddings and LLM path sentence embeddings enables biologically meaningful pathway selection. Path encoder computes expression-aware embeddings via weighted aggregation of gene embeddings using learned importance scores, then uses these as queries with LLM path sentence embeddings as keys/values in cross-attention. This grounds numeric patterns in semantic pathway descriptions. Core assumption: Pathways with matching expression patterns and semantic descriptions are more likely to be biologically relevant. Evidence: Page 10-11 describes the cross-attention module where "the sentence embedding of the path is used and integrated with the expression aware path embedding." Break condition: Ablate cross-attention (use only expression-aware or only sentence embeddings) and measure both accuracy drop and pathway coherence with known biology.

## Foundational Learning

- **Transformer self-attention with graph structural encodings**: Why needed here: The gene encoder modifies standard transformer attention by adding centrality encoding (degree-based), spatial encoding (node ID embeddings), and edge encoding—understanding why these are necessary for graph data vs. sequences. Quick check: Can you explain why standard transformer attention cannot capture graph topology without these additions?

- **Jumping Knowledge Networks**: Why needed here: The graph encoder uses JK networks to combine embeddings from all layers via max-pooling, addressing the over-smoothing problem in deep GNNs where node representations become indistinguishable. Quick check: What happens to node embeddings in a deep GNN without layer combination strategies, and how does max-pooling across layers help?

- **Single-cell RNA-seq expression matrices**: Why needed here: Input data format—sparse matrices where rows are genes, columns are cells, values are expression counts; understanding normalization, gene filtering, and cell-type annotation is assumed. Quick check: Why might a gene expressed in only 2 cells be filtered out before model training?

## Architecture Onboarding

- **Component map**: scRNA expression (n_genes × 1) + Gene sentence embeddings (n_genes × 768) → [Expander: MLP expand → concat → MLP reduce] → n_genes × d → [Gene Encoder: L transformer layers with centrality/spatial/edge encodings] → [Path Encoder: scatter to paths → positional/edge encoding → importance scoring → cross-attention with path sentence embeddings] → [Graph Encoder: sigmoid(M) × path embeddings → Jumping Knowledge max-pool across layers → classification head]

- **Critical path**: Gene sentence embeddings must be pre-computed from PubMedBert before training; pre-defined path list from gene interaction databases is required input; expression data preprocessing (filtering, normalization) directly affects model quality.

- **Design tradeoffs**: PubMedBert frozen vs. fine-tuned: paper uses frozen embeddings (no gradient through LLM), trading domain adaptability for computational efficiency; Global vs. sample-specific path weights: global weights provide stability but may miss sample-specific pathway activations; Number of transformer layers L: deeper captures more complex interactions but increases over-smoothing risk.

- **Failure signatures**: Accuracy near 50% (random): Check if expression values are properly normalized; verify gene ID mapping between expression matrix and sentence embeddings; Loss not decreasing: Check learning rate; verify path indices in scatter operation align correctly with gene indices; Attention weights uniform: May indicate insufficient training epochs or too small dataset for transformer capacity.

- **First 3 experiments**: 1) Reproduce cirrhosis accuracy on one cell type (e.g., T-cells): Start with provided data splits (0.7/0.1/0.2), verify you achieve 86% ± 2% as reported in Table 1; 2) Ablate LLM embeddings: Replace sentence embeddings with zero vectors; expect accuracy drop to Pathfinder levels (~81%); 3) Validate path importance interpretability: Extract top-10 paths by learned importance scores, check overlap with known cirrhosis pathways from literature.

## Open Questions the Paper Calls Out

### Open Question 1
Can the LLM-GNN approach maintain high performance when applied to disease domains or biological contexts beyond cirrhosis, Alzheimer's disease, and pancreatic ductal adenocarcinoma? Basis: The authors test only three specific diseases and conclude that "TNGs and joint LLM-GNN models are important approaches for scientific discovery" without validating on a broader range of conditions. Why unresolved: The three tested diseases represent limited diversity in disease mechanisms and tissue types. What evidence would resolve it: Application and evaluation of the model on additional disease datasets covering different organs and disease mechanisms.

### Open Question 2
How does the choice of text embedding model affect TNG reasoning performance, and would alternative or more recent domain-specific LLMs beyond PubMedBert improve results? Basis: The paper uses PubMedBert exclusively for generating sentence embeddings without ablation studies or comparison to alternative LLMs. Why unresolved: Different LLMs capture different aspects of biomedical knowledge and have varying semantic representation capabilities. What evidence would resolve it: Systematic comparison of model performance using different LLM backbones on identical TNG datasets.

### Open Question 3
How sensitive are the model's predictions and inferred pathways to the pre-defined path list, and can important paths be dynamically discovered rather than pre-specified? Basis: The methodology relies on pre-defined paths from gene-gene interaction databases without analyzing how path selection impacts results. Why unresolved: Pre-defined paths may miss novel pathways specific to certain conditions or introduce bias based on existing knowledge. What evidence would resolve it: Experiments varying path list composition and comparing pre-defined vs. dynamically learned paths.

## Limitations
- Unspecified hyperparameters: Hidden size, number of transformer layers, learning rate, batch size, and MLP dimensions are all critical for reproduction but not provided
- Limited dataset diversity: Only three diseases tested, all from single-cell RNA sequencing without validation on bulk RNA-seq or other omics data
- Pre-defined path dependency: Reliance on gene-gene interaction databases may miss novel biological relationships not captured in existing knowledge bases

## Confidence
- Classification accuracy improvements (88% vs 81%): High - Multiple datasets show consistent gains over Pathfinder baseline
- Global path importance mechanism: Medium - Theoretical justification is strong but lacks ablation studies
- Cross-attention pathway selection: Medium - Biological interpretations are plausible but not rigorously validated
- Integration of LLMs with GNNs for graph reasoning: High - Architectural design is sound and aligns with established Graphormer framework

## Next Checks
1. **Ablation study of global vs. sample-specific path weights**: Replace the trainable global path importance scores with sample-specific attention weights and compare classification accuracy and coefficient of variation across samples. This tests whether the claimed robustness benefit is real.

2. **Random embedding baseline**: Replace PubMedBert sentence embeddings with randomly initialized vectors (same dimension) and measure classification performance. If accuracy drops to Pathfinder levels, this validates that semantic knowledge from LLMs is crucial rather than just model architecture.

3. **Path coherence validation**: For the top-10 pathways identified by the model in each disease dataset, verify overlap with known disease mechanisms from literature and compute pathway coherence scores (e.g., gene set enrichment). This tests whether the model discovers biologically meaningful rather than spurious pathways.