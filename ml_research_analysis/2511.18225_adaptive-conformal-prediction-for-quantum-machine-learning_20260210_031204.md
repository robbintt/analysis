---
ver: rpa2
title: Adaptive Conformal Prediction for Quantum Machine Learning
arxiv_id: '2511.18225'
source_url: https://arxiv.org/abs/2511.18225
tags:
- quantum
- prediction
- score
- conformal
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of providing reliable uncertainty
  quantification for quantum machine learning models in the presence of non-stationary
  hardware noise. The authors formalize how time-varying noise invalidates the exchangeability
  assumption required for standard conformal prediction, even when calibration and
  test data are exchangeable.
---

# Adaptive Conformal Prediction for Quantum Machine Learning

## Quick Facts
- arXiv ID: 2511.18225
- Source URL: https://arxiv.org/abs/2511.18225
- Reference count: 36
- Primary result: Introduces Adaptive Quantum Conformal Prediction (AQCP) that maintains valid uncertainty quantification under non-stationary hardware noise through online recalibration, achieving stable coverage on IBM quantum hardware where standard conformal prediction fails.

## Executive Summary
This paper addresses the critical challenge of providing reliable uncertainty quantification for quantum machine learning models in the presence of non-stationary hardware noise. Standard conformal prediction methods fail when quantum hardware noise varies over time because this breaks the exchangeability assumption required for validity. The authors introduce Adaptive Quantum Conformal Prediction (AQCP), which applies adaptive conformal inference to quantum settings by maintaining an online estimate of the miscoverage level and updating it based on observed prediction accuracy. Experiments on IBM quantum hardware demonstrate that AQCP achieves target coverage levels with greater stability than standard quantum conformal prediction, particularly when using density-estimating score functions like kernel density estimation and high-density region based scores.

## Method Summary
AQCP builds on split conformal prediction but treats the miscoverage level α_t as a control variable that adapts online. After each prediction, it updates α_{t+1} = α_t + γ(α - err_t), where err_t ∈ {0,1} indicates whether the true label was in the prediction set. This maintains asymptotic average coverage guarantees without requiring exchangeability of scores. The method uses a quantum machine learning pipeline with a Hardware-Efficient Ansatz (HEA) consisting of 5 qubits and 5 layers, angle encoding via a neural network (1→10→10→75), and shot-based measurement. Four score functions are evaluated: k-NN, Euclidean distance, kernel density estimation (KDE), and high-density region (HDR) probability mass. The approach is validated on a multimodal regression task using real quantum hardware (ibm_sherbrooke) with 10,000 total points and 100 shots per execution.

## Key Results
- AQCP maintains target coverage of 90% on real quantum hardware where standard QCP coverage degrades due to noise drift
- KDE and HDR score functions produce smaller average prediction sets than k-NN and Euclidean, approaching optimal sizes as shot count increases
- Online adaptation with γ=0.03 provides stable coverage without excessive oscillation across different noise conditions
- The approach works for arbitrary noise conditions without requiring characterization of the noise process

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Time-varying hardware noise breaks exchangeability of conformity scores even when calibration and test data are exchangeable.
- Mechanism: Quantum circuits execute under time-dependent noise channels E_t that affect shot distributions. Since score functions Ŝ(x,y;A_{x,T}) depend on time-stamped sample multisets A_{x,T}, the resulting scores inherit temporal dependence from the noise process, violating the permutation-invariance requirement of exchangeability.
- Core assumption: Noise processes in quantum hardware are non-stationary over the timescale of calibration and testing (supported by empirical observations of hourly/daily recalibration needs and environmental drift).
- Evidence anchors:
  - [abstract] "we formalise how the time-varying noise inherent in quantum processors can undermine conformal guarantees, even when calibration and test data are exchangeable"
  - [Section 3.2] "the induced scores are inherently time-dependent if there the noise of quantum hardware changes across time. This breaks the usual exchangeability assumption"
  - [corpus] Limited direct corpus validation for quantum-specific exchangeability breaking; related work on noise-robust conformal prediction exists but in classical settings (weak cross-domain support)
- Break condition: If hardware noise were provably stationary during the measurement window, standard QCP guarantees would hold without adaptation.

### Mechanism 2
- Claim: Online recalibration via α_t updates maintains asymptotic average coverage under arbitrary noise conditions.
- Mechanism: AQCP treats the miscoverage level α_t as a control variable, updating it after each prediction via α_{t+1} = α_t + γ(α - err_t), where err_t ∈ {0,1} indicates miscoverage. This feedback loop compensates for distribution drift by increasing α_t when under-covering and decreasing it when over-covering, ensuring lim_{N→∞} (1/N) Σ err_j = α almost surely.
- Core assumption: The online testing setting provides sequential access to ground-truth labels for adaptation (labels may be delayed but must arrive eventually).
- Evidence anchors:
  - [abstract] "AQCP maintains validity through online recalibration, adjusting its miscoverage level based on observed prediction accuracy"
  - [Section 4.1] "lim_{N→∞} (1/N) Σ err_j = α a.s." and "These guarantees do not depend on exchangeability"
  - [corpus] "Exploring the Noise Robustness of Online Conformal Prediction" examines robustness of online CP methods to noise, providing supporting evidence for the general framework
- Break condition: If labels are never observed or adaptation step size γ is set to zero, the mechanism collapses to standard online QCP with no drift correction.

### Mechanism 3
- Claim: Density-estimating score functions (KDE, HDR) asymptotically approach optimal prediction set sizes as shot count increases.
- Mechanism: Score functions that estimate conditional density p(y|x)—specifically s_KDE via kernel density estimation and s_HDR via high-density region probability mass—asymptotically belong to the optimal class S₁ (marginal coverage) and S₂ (conditional coverage). As shot count M increases, density estimates converge, yielding prediction sets approaching the theoretical minimum size.
- Core assumption: Quantum samples approximate draws from the true conditional distribution; density estimators are consistent under chosen bandwidth/neighbor parameters.
- Evidence anchors:
  - [Section 4.2] "s_KDE and s_HDR asymptotically lies in S₁" and "s_HDR score will asymptotically approaches S₂ as M→∞"
  - [Section 5.3, Figure 5] "s_KDE and s_HDR produce comparable average set sizes...s_HDR achieves the smallest average set size at M=1,000"
  - [corpus] Weak direct corpus support for QML-specific score function optimality; theoretical foundations from "Theoretical Foundations of Conformal Prediction" provide general CP score function theory
- Break condition: If shot count M is too low (< 10) or density estimator bandwidth is misspecified, scores may poorly approximate true density, yielding inefficient prediction sets.

## Foundational Learning

- Concept: **Exchangeability**
  - Why needed here: Standard conformal prediction guarantees require that calibration and test scores be exchangeable (permutation-invariant). Understanding why noise breaks this property is essential for grasping why AQCP is necessary.
  - Quick check question: Can you explain why two datasets drawn from the same distribution at different times might produce non-exchangeable scores in the presence of time-varying noise?

- Concept: **Quantum Noise Channels (CPTP Maps)**
  - Why needed here: The paper models hardware noise as completely positive trace-preserving maps E_t acting on density matrices. This formalism explains how noise corrupts quantum states and why it varies temporally.
  - Quick check question: If a depolarizing channel with probability p is applied after each gate in a 5-layer circuit, what happens to the cumulative noise effect?

- Concept: **Split Conformal Prediction**
  - Why needed here: AQCP builds on the split conformal framework (separate calibration and test phases). You need to understand the baseline procedure (compute calibration scores → find quantile → construct prediction set) to see what AQCP modifies.
  - Quick check question: For a calibration set of size n=100 and target miscoverage α=0.1, what quantile of the calibration scores determines the prediction set threshold?

## Architecture Onboarding

- Component map:
Input x → Angle Encoder (NN) → Rotation angles θ_W(x)
         ↓
    PQC (HEA, 5 qubits, 5 layers) → Noisy execution E_t
         ↓
    Computational basis measurement → Bitstrings b ∈ {0,1}^Q
         ↓
    Mapping f: {0,1}^Q → Y → Sample multiset A_{x,T} (M shots)
         ↓
    Score function Ŝ(x, y; A_{x,T}) → Compare to quantile λ
         ↓
    Prediction set C(x) + (if online) observe y_true → Update α_t

- Critical path:
  1. **Training phase**: Train angle encoder weights W on cross-entropy loss using noiseless simulator
  2. **Initial calibration**: Collect n calibration points, execute M shots each, compute initial scores S = {s_i}
  3. **Online prediction**: For each test point x_t, collect shots, compute prediction set C(x_t) using current α_t, observe outcome, update α_{t+1}

- Design tradeoffs:
  - **Step size γ**: Higher γ → faster adaptation but more coverage volatility; lower γ → stability but slower response to drift. Paper uses γ=0.03.
  - **Score function choice**: k-NN and Euclidean are simpler but produce larger sets; KDE and HDR are near-optimal at high M but require bandwidth tuning.
  - **Shot count M**: More shots improve density estimation and reduce set size, but increase quantum hardware cost. Diminishing returns above M≈100-500.

- Failure signatures:
  - Coverage oscillation with amplitude > 0.1 around target → γ too high or calibration set too small
  - Systematically large prediction sets → score function misspecified or M too low
  - Coverage drifts below target and doesn't recover → γ too low for noise drift rate
  - Ties in scores causing coverage instability → add tie-breaking noise (σ=10^-4 as in paper)

- First 3 experiments:
  1. **Reproduce coverage stability result**: Implement AQCP with γ=0.03 and k-NN score on a noiseless simulator with artificial time-varying noise injected; verify rolling average coverage stabilizes at target α=0.1.
  2. **Score function comparison**: Run AQCP with all four score functions (Euclidean, k-NN, KDE, HDR) across M ∈ {10, 50, 100, 500, 1000}; plot average set size vs M to confirm KDE/HDR achieve smaller sets at high M.
  3. **Step size sensitivity**: Sweep γ ∈ {0.01, 0.03, 0.05, 0.1} on real quantum hardware data (or realistic noise simulation); quantify tradeoff between adaptation speed and coverage volatility.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can dynamic step-size selection algorithms be adapted to AQCP to improve convergence speed and reduce coverage volatility compared to fixed step sizes?
- Basis in paper: [explicit] The conclusion notes that "a growing body of research investigates optimal step-size selection... and these methods could be explored in quantum settings."
- Why unresolved: The current study uses a fixed adaptation step size ($\gamma=0.03$), which requires a trade-off between volatility and adaptation speed; online optimization of this parameter remains unexplored for quantum noise.
- What evidence would resolve it: Experiments implementing adaptive step-size methods (e.g., strongly adaptive online learning) on quantum hardware, demonstrating tighter adherence to target coverage than fixed-$\gamma$ baselines.

### Open Question 2
- Question: Can the total variation distance of time-varying hardware noise be quantified to provide non-asymptotic (finite-sample) coverage guarantees for Quantum Conformal Prediction?
- Basis in paper: [explicit] The authors state that "if changes in noise over time could be quantified, the theory in Barber et al. (2023) might yield stronger finite-sample guarantees for quantum models."
- Why unresolved: AQCP currently provides asymptotic average coverage; finite-sample guarantees are theoretically possible via the Barber et al. framework but require quantifying the noise distribution shift, which was not performed.
- What evidence would resolve it: A method to estimate the total variation distance of the noise drift between calibration and test shots, resulting in a derived finite-sample lower bound on coverage probability.

### Open Question 3
- Question: Is AQCP robust to "catastrophic" error bursts (e.g., cosmic rays) or does the violation of standard noise models require algorithmic modifications?
- Basis in paper: [inferred] The introduction mentions that cosmic rays can cause catastrophic multi-qubit errors, but the empirical evaluation is limited to a single day on the `ibm_sherbrooke` processor under standard drift conditions.
- Why unresolved: While AQCP handles arbitrary noise theoretically, its practical stability has only been demonstrated under "standard" non-stationary drift, not the extreme, sudden shifts caused by radiation events or cross-device variability.
- What evidence would resolve it: Evaluation of AQCP performance on hardware data containing sudden, high-magnitude error bursts or cross-platform comparison between superconducting and trapped-ion devices.

## Limitations
- Experimental validation is limited to a single quantum processor (ibm_sherbrooke) and a specific regression task
- The paper does not quantify the actual time-varying noise characteristics or measure noise drift rates empirically
- Asymptotic guarantees hold but finite-sample coverage bounds are not provided due to lack of noise characterization
- Density estimation assumptions for KDE and HDR scores are not rigorously tested at low shot counts

## Confidence

- **Mechanism 1 (Exchangeability breaking)**: High confidence - The theoretical argument is sound and the empirical need for hardware recalibration supports the premise of non-stationary noise.
- **Mechanism 2 (Online recalibration)**: Medium confidence - The asymptotic guarantees are proven, but the practical step size γ=0.03 is not systematically tuned or justified beyond empirical observation.
- **Mechanism 3 (Score function optimality)**: Medium confidence - Theoretical alignment with optimal score classes is shown, but experimental evidence is limited to a narrow range of M values and a single task.

## Next Checks

1. **Quantify noise non-stationarity**: Measure or simulate time-varying noise channels on the quantum processor to empirically verify the exchangeability breaking mechanism. Characterize noise drift rate and correlate with required recalibration frequency.
2. **Cross-task generalizability**: Test AQCP on a different quantum machine learning task (e.g., classification or quantum kernel estimation) to assess whether the score function ranking (KDE/HDR vs. k-NN/Euclidean) holds across problem domains.
3. **Step size robustness**: Conduct a systematic ablation study over γ ∈ {0.001, 0.005, 0.01, 0.03, 0.05, 0.1} to quantify the tradeoff between adaptation speed and coverage volatility, and determine if the chosen value is near-optimal.