---
ver: rpa2
title: 'A "watch your replay videos" reflection assignment on comparing programming
  without versus with generative AI: learning about programming, critical AI use and
  limitations, and reflection'
arxiv_id: '2507.17226'
source_url: https://arxiv.org/abs/2507.17226
tags:
- students
- learning
- reflection
- programming
- generative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduced a novel video-based reflection assignment
  for computing students, designed using the DEAL framework, to compare their programming
  processes with and without generative AI. Students recorded two programming sessions,
  then reflected on differences in planning, debugging, help-seeking, and AI use.
---

# A "watch your replay videos" reflection assignment on comparing programming without versus with generative AI: learning about programming, critical AI use and limitations, and reflection

## Quick Facts
- arXiv ID: 2507.17226
- Source URL: https://arxiv.org/abs/2507.17226
- Reference count: 39
- Primary result: Video-based reflection assignment using DEAL framework deepens students' metacognitive awareness about programming processes, AI limitations, and critical AI use

## Executive Summary
This study introduces a novel video-based reflection assignment for computing students to compare their programming processes with and without generative AI. Students recorded two programming sessions and reflected on differences in planning, debugging, help-seeking, and AI use. Analysis of 30 student reflections revealed that the assignment deepened metacognitive awareness: students learned about AI's limitations, benefits, and critical use, as well as general programming skills like planning and collaboration. They also set concrete goals to improve their coding and reflective practices. The approach fostered critical AI literacy and lifelong learning habits. The authors recommend further refinement for novice students and suggest scaling the method through research-practitioner partnerships to integrate AI ethically and effectively in computing education.

## Method Summary
The assignment used the DEAL framework (Describe, Examine, Articulate Learning) where students recorded two programming sessions during a multi-sprint project: first without GenAI tools, then with GenAI tools. Students watched their videos and completed a 4-column reflection table analyzing their process, including time distribution across activities (planning, coding, debugging, documentation, human help, GenAI use). The analysis used qualitative thematic analysis following Braun & Clarke (2006), examining what students learned, how they learned it, why it matters, and what goals they set.

## Key Results
- Students developed insights about programming processes that transcended AI use, including planning, debugging, and help-seeking behaviors
- Students spontaneously set diverse future goals, from improving video-based self-analysis to more intentional AI use
- The assignment fostered critical AI literacy by revealing both benefits and limitations of generative AI in programming workflows

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Comparing a programming session *without* GenAI to one *with* GenAI creates a salient contrast that reveals process changes.
- **Mechanism:** The "delta" between the two sessions makes implicit behaviors (like planning vs. prompting) explicit. By observing the causality between tool use and problem-solving strategy in their own work, students bypass abstract lectures and derive personalized insights.
- **Core assumption:** Students have sufficient memory of the first session or watch the videos closely enough to detect meaningful differences in their workflow.
- **Evidence anchors:**
  - [abstract] "qualitative thematic analysis... finding students developed insights about planning, debugging, and help-seeking behaviors that transcended AI use."
  - [section 3] "Students also compared their two programming sessions... [answering] 'Compare and Evaluate: write about... the differences... and potential causes of those differences.'"
  - [corpus] Paper *73513 ("Demystify, Use, Reflect")* supports the general need for structured reflection to prepare informed users, though it lacks the specific comparative video component.
- **Break condition:** If the two programming tasks differ vastly in complexity (e.g., simple HTML vs. complex backend logic), the comparison confounds tool use with task difficulty, invalidating the reflection.

### Mechanism 2
- **Claim:** Video review acts as an objective "mirror," mitigating the "illusion of competence" often present during live coding.
- **Mechanism:** Students often lack metacognitive awareness while coding. Reviewing a screen recording allows them to observe wasted time, ineffective debugging loops, or "rabbit holes" from a third-person perspective, triggering a realization of inefficiency.
- **Core assumption:** Students actually watch the video rather than skipping through it, and the emotional discomfort of watching oneself struggle does not cause avoidance.
- **Evidence anchors:**
  - [abstract] "Students reported learning to slow down and understand before writing or generating code, recognized patterns in their problem-solving..."
  - [section 7] "Our approach... asks students to record themselves... Critically, we also used the DEAL framework... providing students and guided prompts."
  - [corpus] Corpus neighbors focus heavily on outcomes (e.g., *30976* "Pair Programming Partner"), but there is weak direct corpus evidence for the specific mechanism of "self-video review" in CS education, making this a novel contribution of the paper.
- **Break condition:** If the recording does not capture the relevant context (e.g., off-screen notes or separate browser windows) or if students speed-watch at 4x speed, the "mirror" effect is lost.

### Mechanism 3
- **Claim:** The DEAL framework scaffolds the transition from description to actionable goal-setting.
- **Mechanism:** Without structure, reflection often remains at the "Description" level (what happened). By forcing "Examine" (analysis of causes) and "Articulate Learning" (future application), the assignment converts observation into behavioral intent.
- **Core assumption:** The provided prompts are specific enough to guide novices who may lack the vocabulary to analyze their own cognitive processes.
- **Evidence anchors:**
  - [abstract] "...assignment scaffolded reflection on programming generally and led students to spontaneously set future goals..."
  - [section 5.3] "Students in the reflection generated very diverse goals... [e.g.] 'I will analyze my work in the future through video in time intervals...'"
  - [corpus] Paper *113989* discusses assessment models to ensure learning, but this paper focuses on the *metacognitive* mechanism of self-regulation.
- **Break condition:** If students view the questions as a compliance checklist rather than a heuristic, the depth of analysis degrades into surface-level answers.

## Foundational Learning

- **Concept: Screen Recording & Data Hygiene**
  - **Why needed here:** The mechanism relies on the student capturing a complete, honest dataset of their process.
  - **Quick check question:** "Can you configure a screen recorder to capture the coding window and any browser/documentation references without capturing sensitive notifications?"

- **Concept: Basic Prompt Engineering**
  - **Why needed here:** To ensure the "With AI" session reflects a competent usage of the tool, rather than a failure to use it effectively.
  - **Quick check question:** "Do you know how to structure a prompt that includes necessary context (roles, output format) for a coding task?"

- **Concept: The "Illusion of Competence"**
  - **Why needed here:** Students must understand that *feeling* productive (e.g. generating code fast) does not equate to *being* productive (e.g. understanding the code).
  - **Quick check question:** "Why might generating a working solution faster actually result in weaker long-term learning compared to struggling through a manual implementation?"

## Architecture Onboarding

- **Component map:** Student Project Task (context) -> Recording Software (OBS/Zoom) -> Session 1 (No AI) -> Session 2 (AI) -> Review (Watch & Annotate) -> DEAL Table (Questions) -> Written Reflection (Goals + Analysis)

- **Critical path:** The "Examine" step (Section 3). If the questions asking about "time categories" (planning vs debugging) are skipped or answered vaguely, the subsequent "Articulate Learning" will lack evidence.

- **Design tradeoffs:**
  - **Ecological Validity vs. Control:** The assignment uses real project work (high validity) rather than artificial lab tasks, but this makes comparing student-to-student results difficult because tasks vary in difficulty.
  - **Privacy vs. Honesty:** Requiring screen recordings increases honesty in process but risks capturing personal data; must have strict "edit out sensitive info" protocols.

- **Failure signatures:**
  - **The "Gaming" Signature:** Students reporting "100% planning" or "0% debugging" without variance, suggesting they filled the table without actually watching the video.
  - **The "Task Mismatch" Signature:** Reflections focusing entirely on the difficulty of the specific task rather than the difference in process between AI/No-AI.

- **First 3 experiments:**
  1. **Pilot the Time Categories:** Before full deployment, have TAs code a sample video using the time categories (planning, debugging, etc.) to ensure the categories are mutually exclusive and exhaustive.
  2. **Calibration Session:** Do a "think-aloud" demo in class where the instructor watches a 2-minute clip of themselves coding and models how to fill out the DEAL table.
  3. **Check-in on Goals:** A follow-up survey 2 weeks post-assignment to see if students actually adopted any of the "spontaneous goals" they set (e.g., "Did you actually plan for 30 mins before coding?").

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** To what extent do students retain and act upon the learning and goals set during the reflection assignment in their subsequent courses or professional work?
- **Basis in paper:** [explicit] The authors state, "We recommend follow-up with students with a survey and interviews to see how well they retain and act on their learning and goals they set from reflection."
- **Why unresolved:** The current study analyzed immediate written reflections submitted at the end of a semester and did not track longitudinal behavioral changes.
- **What evidence would resolve it:** Longitudinal survey or interview data collected months or years after the course, along with analysis of future student work artifacts.

### Open Question 2
- **Question:** How must the assignment scaffolding be adapted for lower-level courses (e.g., CS1/CS2) to effectively support novice programmers?
- **Basis in paper:** [explicit] The authors suggest, "Future work and instructors should engage in exploring and refining scaffolded reflection in lower level courses, where results might be different or more scaffolding may be need to be designed."
- **Why unresolved:** The study was conducted in an upper-level introductory software engineering course where students had prior experience; novices may struggle differently with the cognitive load of video analysis.
- **What evidence would resolve it:** Deployment of the assignment in early computing courses with iterative design-based research to identify necessary scaffolding modifications.

### Open Question 3
- **Question:** Does comparative video reflection quantitatively improve metacognitive skills or programming process quality compared to non-video or non-comparative interventions?
- **Basis in paper:** [explicit] The authors propose, "randomized quantitative evaluation of scaffolded comparative video reflection, such as during instead of just the end of a course."
- **Why unresolved:** This study utilized a qualitative thematic analysis without a control group, preventing quantitative claims about efficacy relative to other methods.
- **What evidence would resolve it:** A randomized controlled trial comparing video reflection against standard reflection practices, measuring outcomes via metacognitive inventories or code quality metrics.

## Limitations
- Small sample size (30 out of 36 students) from a single course context limits generalizability
- Comparative mechanism may be confounded if the two programming tasks differ significantly in complexity
- Complete reflection instrument not provided in the paper, making exact replication difficult

## Confidence
- **High confidence**: The DEAL framework effectively scaffolds reflection from description to actionable goals (supported by student goal-setting evidence)
- **Medium confidence**: Video comparison reveals process differences between AI and non-AI sessions (supported by thematic analysis but limited by sample size and potential task confounds)
- **Medium confidence**: Students develop critical AI literacy through this assignment (supported by qualitative themes but could reflect response bias or social desirability)

## Next Checks
1. Pilot the time category coding scheme with TAs coding sample videos to ensure categories are mutually exclusive and exhaustive before full deployment
2. Implement a calibration session where instructors model "thinking aloud" while watching a 2-minute clip of their own coding to demonstrate proper DEAL table completion
3. Conduct a 2-week follow-up survey to verify whether students actually adopted any of the spontaneous goals they set during the reflection (e.g., "Did you actually plan for 30 minutes before coding?")