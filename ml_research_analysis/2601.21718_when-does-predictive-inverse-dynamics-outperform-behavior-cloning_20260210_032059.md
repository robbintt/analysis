---
ver: rpa2
title: When does predictive inverse dynamics outperform behavior cloning?
arxiv_id: '2601.21718'
source_url: https://arxiv.org/abs/2601.21718
tags:
- state
- pidm
- demonstrations
- predictor
- future
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates why predictive inverse dynamics models
  (PIDM) outperform behavior cloning (BC) in offline imitation learning, especially
  with limited expert demonstrations. PIDM decomposes decision-making into a state
  predictor and inverse dynamics model (IDM), conditioning IDM on predicted future
  states.
---

# When does predictive inverse dynamics outperform behavior cloning?

## Quick Facts
- **arXiv ID**: 2601.21718
- **Source URL**: https://arxiv.org/abs/2601.21718
- **Reference count**: 40
- **Primary result**: PIDM achieves up to 5× (3× average) better sample efficiency than BC in 2D navigation; 66% more samples needed for BC in 3D video game.

## Executive Summary
This paper investigates when predictive inverse dynamics models (PIDM) outperform behavior cloning (BC) in offline imitation learning, particularly with limited expert demonstrations. PIDM decomposes decision-making into a state predictor and inverse dynamics model, conditioning the IDM on predicted future states. The theoretical analysis reveals a bias-variance tradeoff: future conditioning reduces variance (boosting sample efficiency), while imperfect state prediction introduces bias that diminishes gains. Empirical validation in 2D navigation tasks shows BC requires up to 5× more demonstrations than PIDM to reach comparable performance. In a complex 3D video game with high-dimensional visual inputs and stochastic transitions, BC needs over 66% more samples than PIDM, confirming PIDM's advantages under real-world conditions.

## Method Summary
The study compares predictive inverse dynamics models (PIDM) against behavior cloning (BC) in offline imitation learning. PIDM combines a state predictor with an inverse dynamics model (IDM), where the IDM conditions on both current state and predicted future state. The 2D navigation tasks use an instance-based state predictor (nearest neighbor lookup) with MLPs for encoding and policy, while the 3D video game uses a learned time-step-conditioned state predictor from a single demonstration. Both methods are evaluated on their ability to reach goals in sequence, with sample efficiency measured as the ratio of demonstrations needed by BC versus PIDM at various performance thresholds (80%, 90%, 95%).

## Key Results
- PIDM achieves up to 5× (3× average) better sample efficiency than BC in 2D navigation tasks
- BC requires over 66% more samples than PIDM in the 3D video game with high-dimensional visual inputs
- Theoretical analysis shows PIDM introduces a bias-variance tradeoff that favors sample efficiency when future state prediction is reasonably accurate
- Additional data sources further widen the performance gap between PIDM and BC

## Why This Works (Mechanism)
PIDM improves sample efficiency by conditioning the inverse dynamics model on predicted future states, which reduces variance in the learning process. This approach decomposes the complex policy learning problem into two simpler subproblems: state prediction and action inference given state transitions. The bias introduced by imperfect state prediction is offset by the variance reduction, particularly when demonstrations are limited. The theoretical analysis shows that under bounded noise conditions, PIDM can achieve better sample complexity than BC when the state prediction error remains controlled.

## Foundational Learning
- **Offline imitation learning**: Learning from fixed datasets of expert demonstrations without online interaction; needed because real-world data collection can be expensive and dangerous
- **Bias-variance tradeoff**: The fundamental tradeoff between model simplicity (bias) and sensitivity to training data (variance); quick check: compare training and test performance
- **Sample efficiency**: The amount of data required to achieve a target performance level; critical in robotics and game environments where data collection is costly
- **State prediction**: Forecasting future states given current state and action; enables counterfactual reasoning about policy decisions
- **Inverse dynamics modeling**: Learning the mapping from state transitions to actions; useful when direct policy learning is unstable

## Architecture Onboarding

**Component map**: State Predictor -> Inverse Dynamics Model -> Policy Output

**Critical path**: Demo trajectories → State predictor → Future state conditioning → IDM training → Policy evaluation

**Design tradeoffs**: Instance-based vs learned state predictors (scalability vs accuracy), conditioning horizon k (bias-variance tradeoff), shared vs separate encoder architectures

**Failure signatures**: 
- BC: High variance across seeds, especially with narrow or deterministic demonstrations
- PIDM: Degradation when state prediction error exceeds threshold, particularly with few or diverse demos

**3 first experiments**:
1. Compare PIDM vs BC sample efficiency on 2D navigation with varying demonstration counts (1, 5, 10, 50)
2. Measure state prediction error vs policy performance correlation in PIDM
3. Test PIDM with different conditioning horizons (k=1, 5, 10) to identify optimal bias-variance tradeoff

## Open Questions the Paper Calls Out
None

## Limitations
- Results rely on hand-designed environments and a single 3D video game task, limiting generalizability
- Theoretical conditions assume bounded noise and specific transition dynamics that may not hold in real-world settings
- Key hyperparameters (learning rates, batch sizes) were tuned but not fully specified, creating reproducibility challenges
- Instance-based state predictor has O(n) query time, limiting scalability to large demonstration sets

## Confidence

**High confidence**: PIDM demonstrates superior sample efficiency compared to BC in tested 2D navigation tasks (4/4 tasks show consistent gains across 20+ seeds)

**Medium confidence**: The 3D video game results are promising but based on a single complex task; the 66% sample efficiency gap requires validation across multiple environments

**Medium confidence**: The theoretical bias-variance tradeoff explanation is mathematically sound but simplified; real-world dynamics often violate bounded noise assumptions

## Next Checks

1. Test PIDM vs BC on additional 3D environments with varying stochasticity and visual complexity to verify the 66% sample efficiency advantage robustness
2. Implement the exact hyperparameter tuning procedure and measure sensitivity to learning rates and batch sizes across task complexities
3. Replace the instance-based state predictor with a learned neural network predictor to evaluate scalability and performance degradation as demonstration count increases