---
ver: rpa2
title: Model selection for behavioral learning data and applications to contextual
  bandits
arxiv_id: '2502.13186'
source_url: https://arxiv.org/abs/2502.13186
tags:
- data
- learning
- contextual
- each
- bandits
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of model selection for behavioral
  learning data, specifically when the data is non-stationary and dependent. The authors
  propose two model selection methods: a general hold-out procedure and an AIC-type
  criterion, both adapted to non-stationary dependent data.'
---

# Model selection for behavioral learning data and applications to contextual bandits

## Quick Facts
- arXiv ID: 2502.13186
- Source URL: https://arxiv.org/abs/2502.13186
- Reference count: 40
- One-line primary result: Two model selection methods for non-stationary dependent data achieve oracle inequalities with error bounds close to i.i.d. case, applied to contextual bandit models for behavioral learning

## Executive Summary
This paper addresses the challenge of model selection for behavioral learning data, where traditional i.i.d. assumptions fail due to sequential dependence. The authors propose two methods - a general hold-out procedure and an AIC-type criterion - both adapted to non-stationary dependent data. They apply these methods to contextual bandit models to identify which behavioral learning strategies best explain human categorization decisions. The theoretical framework provides error bounds that approach those of standard i.i.d. settings while accounting for the temporal dependencies inherent in learning processes.

## Method Summary
The authors develop two model selection methods for non-stationary dependent behavioral data. The hold-out procedure splits data at time N, trains models on early observations and validates on later ones, achieving O((log n + log |M|)/n) error bounds without i.i.d. assumptions. The AIC-type criterion minimizes penalized log-likelihood with O(log(n)/n) bounds under Lipschitz smoothness conditions. Both methods are applied to partition-based contextual bandit models that cluster context features into distinct cells, with each cell running a separate bandit algorithm. The framework is tested on synthetic data generated from known models and applied to human categorization experiments with 176 participants.

## Key Results
- Oracle inequalities achieved with error bounds of O((log n + log |M|)/n) for hold-out procedure and O(log(n)/n) for AIC-type criterion
- AIC-type method successfully identifies true model from synthetic data 66-89% of the time, outperforming hold-out (29-65%)
- Both methods struggle to identify complex "OnePerItem" models when they are the true model
- Real experimental data shows higher prevalence of "OnePerItem" and "ByShape" models compared to simulations

## Why This Works (Mechanism)

### Mechanism 1: Hold-out for Non-Stationary Data
- Claim: A hold-out estimator can select the best model for non-stationary dependent data if split at specific time index N
- Mechanism: Splits time series into training (first N-1 steps) and validation (remaining steps), training on early "learning" phase and validating on later "adapted" phase
- Core assumption: Model probabilities must be predictable with respect to filtration F_{t-1}, meaning past actions define future state
- Evidence anchors: [abstract] "general hold-out procedure... adapted to non-stationary dependent data"; [section 3] "selects the best estimator... regardless of nature of models"
- Break condition: Fails if individual changes learning strategies between training split and validation split

### Mechanism 2: AIC-Type Penalized Likelihood
- Claim: AIC-type penalized log-likelihood criterion approximates model fit for contextual bandits under smoothness condition
- Mechanism: Minimizes -ℓ_n/T_ε + pen(m) using stochastic risk function and dimension-based penalty to balance bias and variance
- Core assumption: Parameterization must satisfy Lipschitz condition (small parameter changes lead to bounded action probability changes)
- Evidence anchors: [abstract] "AIC-type criterion offers an O(log(n)/n) error bound under certain assumptions on the parametrization"
- Break condition: Fails if learner reaches zero error too quickly, causing probabilities to vanish and gradients to disappear

### Mechanism 3: Partition-Based Contextual Bandits
- Claim: Partition-based contextual bandits approximate human categorization by clustering context features into distinct cells
- Mechanism: Context space X is partitioned into disjoint cells (e.g., "ByShape" or "ByPattern") with separate bandit algorithm running within each cell
- Core assumption: Learner perceives objects through static partition and applies consistent policy within those partitions
- Evidence anchors: [section 1.2] "Many traditional cognitive models... can be reinterpreted as contextual bandits"; [section 5] "Each model represents a partition of object space"
- Break condition: Fails if human strategy involves dynamic hierarchy or continuous feature integration not fitting static partition

## Foundational Learning

- **Filtration (Stochastic Processes)**: Needed to define learning process as adapted to F_t (history of actions), replacing i.i.d. assumption with sequential dependence. Quick check: Can you explain why probability of action at time t depends on F_{t-1} rather than just current context?

- **Kullback-Leibler (KL) Divergence**: Used in theoretical error bounds and risk functions as conditional KL divergence between true distribution p* and model p. Quick check: How does "stochastic risk function" K_{N,n} differ from standard KL divergence in i.i.d. settings?

- **Oracle Inequality**: Provides theoretical validity by guaranteeing selected model performs almost as well as best possible model in set. Quick check: What does constant ◇ > 1 represent in oracle inequality for hold-out procedure?

## Architecture Onboarding

- Component map: Input (Context_t, Action_t) pairs -> Models (6 Partition-based CB) -> Estimators (MLE, Hold-out, Penalized MLE) -> Output (Selected Model, Parameters)
- Critical path: Optimization of likelihood for "OnePerItem" model is computational bottleneck due to highest dimensionality (D_m=9)
- Design tradeoffs: Hold-out makes no assumptions but sacrifices data; AIC uses all data but requires strict Lipschitz assumptions and penalty tuning
- Failure signatures: Vanishing gradients when action probabilities hit boundaries; misspecification on complex "OnePerItem" model
- First 3 experiments:
  1. Generate synthetic data from known models and run selection pipeline to tune hyperparameters until recovery accuracy > 65%
  2. Plot average parameter error vs. truncation time N to verify theoretical claim about estimation failure when learner stops making errors
  3. Run calibrated selector on experimental dataset to check if selected model distribution matches paper's findings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is observed preference for OnePerItem model in experimental data linked to specific presentation order of objects in categorization task?
- Basis in paper: [explicit] "It would be interesting for further study to see if this is linked to the presentation order of the objects..."
- Why unresolved: Authors observed higher frequency of OnePerItem model in real data compared to simulations but did not vary presentation sequence to test causal effect
- Evidence needed: Experiments controlling and varying sequence of object presentation to analyze its effect on distribution of selected models

### Open Question 2
- Question: Can theoretical guidelines be established for selecting hyperparameters N (split position) and c (penalty constant) in non-stationary framework?
- Basis in paper: [explicit] "Both model selection methods rely on a hyperparameter... which lack theoretical guidelines and must be empirically tuned"
- Why unresolved: Classical results for cross-validation or slope heuristics assume i.i.d. data, which doesn't hold for dependent learning trajectories
- Evidence needed: Derivation of new oracle inequalities or heuristic constants providing optimal theoretical values without numerical calibration

### Open Question 3
- Question: Can theoretical error bounds for AIC-type criterion be tightened to justify using full dataset (n) rather than theoretical restriction to ≈√n observations?
- Basis in paper: [inferred] "The truncation at √n ≃ 20 required in theoretical results does not seem necessary in practice, and actually looks suboptimal"
- Why unresolved: Assumptions required for theoretical proof force truncation parameter T_ε much smaller than sample size used in successful simulations
- Evidence needed: Proof showing AIC-type criterion satisfies oracle inequality with stopping time T_ε on order of n (or log n) rather than √n

## Limitations

- Core theoretical framework rests on strong assumptions (predictability, Lipschitz smoothness, non-vanishing probabilities) that may be too restrictive for real human behavior
- Method shows model confusion, particularly for complex "OnePerItem" models, suggesting potential systematic bias in selection
- Cannot verify real data application results due to ethics restrictions preventing access to experimental dataset

## Confidence

- **Theoretical Error Bounds**: Medium confidence - bounds derived under strong assumptions that may not hold in practice
- **Synthetic Data Recovery**: Medium confidence - recovers simpler models well but struggles with complex ones indicating systematic bias
- **Real Data Application**: Low confidence - cannot verify results without access to experimental dataset

## Next Checks

1. **Assumption Sensitivity Analysis**: Systematically vary Lipschitz constant and predictability assumptions in synthetic experiments to quantify impact on model selection accuracy

2. **Parameter Recovery Verification**: For synthetic data from known models, measure accuracy of θ estimation across all models, not just selection performance

3. **Model Confusion Mapping**: Create confusion matrix showing which models are most often mistaken for each other, particularly focusing on ByPatternExc/ByShapeExc/OnePerItem cluster