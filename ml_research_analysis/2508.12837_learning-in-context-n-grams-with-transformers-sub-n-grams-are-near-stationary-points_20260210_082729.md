---
ver: rpa2
title: 'Learning In-context n-grams with Transformers: Sub-n-grams Are Near-stationary
  Points'
arxiv_id: '2508.12837'
source_url: https://arxiv.org/abs/2508.12837
tags:
- layer
- learning
- token
- attention
- in-context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies the loss landscape of transformers trained\
  \ on in-context next-token prediction, specifically focusing on learning in-context\
  \ n-gram language models. The authors provide a sufficient condition for parameter\
  \ configurations to be stationary points of the cross-entropy loss, and construct\
  \ parameter configurations for a simplified transformer model that represent k-gram\
  \ estimators for k \u2264 n."
---

# Learning In-context n-grams with Transformers: Sub-n-grams Are Near-stationary Points

## Quick Facts
- **arXiv ID**: 2508.12837
- **Source URL**: https://arxiv.org/abs/2508.12837
- **Reference count**: 40
- **Primary result**: Proves that sub-n-gram solutions are near-stationary points in transformer training for in-context next-token prediction

## Executive Summary
This paper investigates the loss landscape of transformers trained on in-context next-token prediction, focusing on learning in-context n-gram language models. The authors establish a sufficient condition for parameter configurations to be stationary points of the cross-entropy loss and construct parameter configurations for a simplified transformer model that represent k-gram estimators for k ≤ n. They prove that the gradient of the population loss at these solutions vanishes in the limit of infinite sequence length and parameter norm, establishing that sub-n-grams are near-stationary points. This theoretical result provides insight into stage-wise learning dynamics and emergent phase transitions observed during training, where models transition between simpler and more complex dependencies.

## Method Summary
The paper develops a theoretical framework for analyzing transformer training dynamics on in-context next-token prediction tasks. The authors construct parameter configurations representing k-gram estimators within a simplified transformer model and prove that these configurations are near-stationary points under specific conditions. The analysis focuses on the population loss rather than empirical loss, examining the behavior as sequence length approaches infinity and parameter norms grow. The theoretical results are validated through empirical experiments using a simplified transformer architecture that demonstrates stage-wise learning behavior with discrete transitions between near-stationary solutions.

## Key Results
- Established sufficient conditions for parameter configurations to be stationary points of the cross-entropy loss
- Constructed k-gram estimator solutions for k ≤ n in a simplified transformer model
- Proved that gradients of population loss vanish at these solutions in the limit of infinite sequence length and parameter norm
- Demonstrated stage-wise learning behavior in experiments with discrete transitions between near-stationary solutions

## Why This Works (Mechanism)
The theoretical mechanism relies on the structure of the simplified transformer model and the properties of k-gram estimators. When parameters are configured to represent k-gram estimators, the model's predictions become stationary with respect to certain perturbations in the loss landscape. As sequence length increases and parameter norms grow, the gradient of the population loss approaches zero at these configurations, creating near-stationary points. This occurs because the k-gram estimators capture local dependencies that are statistically stable under the infinite sequence length assumption, while higher-order dependencies remain unexplored until the model transitions to more complex configurations.

## Foundational Learning
- **Stationary points in optimization**: Understanding when gradients vanish is crucial for analyzing training dynamics and plateaus. Quick check: Verify that ∇L(w) = 0 at claimed stationary points.
- **Cross-entropy loss for language modeling**: The loss function measures prediction quality for next-token tasks. Quick check: Confirm that cross-entropy properly captures the probabilistic nature of language modeling.
- **In-context learning**: Models learn patterns from prompt sequences without parameter updates. Quick check: Validate that the theoretical framework applies to in-context rather than fine-tuned settings.
- **k-gram language models**: These capture local dependencies of order k in sequences. Quick check: Ensure that k-gram estimators are correctly implemented in the simplified transformer.
- **Population vs empirical loss**: Population loss represents expectations over the true data distribution. Quick check: Verify the relationship between population and empirical losses in finite-sample scenarios.
- **Limit behavior**: Understanding asymptotic properties as sequence length and parameter norms approach infinity. Quick check: Confirm that the limiting behavior is mathematically rigorous and physically meaningful.

## Architecture Onboarding

Component map: Input embeddings -> Self-attention -> Feed-forward layers -> Output logits

Critical path: The self-attention mechanism combined with position encodings enables the transformer to capture sequential dependencies, while the feed-forward layers process these representations to generate next-token predictions.

Design tradeoffs: The simplified transformer sacrifices architectural complexity for theoretical tractability, enabling rigorous mathematical analysis but potentially limiting applicability to standard transformer implementations.

Failure signatures: Stage-wise learning plateaus may indicate the model is stuck at near-stationary points corresponding to sub-optimal k-gram solutions rather than progressing to capture longer-range dependencies.

3 first experiments:
1. Verify gradient vanishing at k-gram estimator configurations by computing ∇L(w) for different k values
2. Test stage-wise learning dynamics by monitoring loss reduction patterns during training
3. Examine transitions between near-stationary points by analyzing parameter trajectory evolution

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis relies on a simplified transformer model that may not capture all complexities of standard architectures
- Assumptions of infinite sequence length and parameter norm may not hold in practical training scenarios
- Focus on population loss rather than empirical loss leaves gaps in understanding finite-sample behavior
- Empirical validation uses simplified architecture rather than standard transformer implementations

## Confidence
- **Theoretical claims for simplified transformers**: Medium
- **Direct applicability to standard transformer architectures**: Low
- **Practical relevance for real-world training scenarios**: Low

## Next Checks
1. Extend the theoretical analysis to standard transformer architectures by examining whether sub-n-gram solutions remain near-stationary points under realistic architectural constraints and finite sequence lengths.

2. Conduct empirical experiments using standard transformer implementations to verify whether stage-wise learning dynamics and transitions between near-stationary solutions are observed in practice.

3. Analyze the gap between population loss and empirical loss in finite-sample settings to quantify how the theoretical results translate to actual training dynamics and assess the robustness of the near-stationary point characterization.