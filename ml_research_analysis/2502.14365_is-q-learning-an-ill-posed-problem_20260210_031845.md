---
ver: rpa2
title: Is Q-learning an Ill-posed Problem?
arxiv_id: '2502.14365'
source_url: https://arxiv.org/abs/2502.14365
tags:
- policy
- learning
- q-learning
- iteration
- values
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the instability of Q-learning in continuous
  environments, a critical challenge in reinforcement learning. Using the cart-pole
  benchmark, the research systematically examines the effects of bootstrapping and
  model inaccuracies by progressively eliminating these potential error sources.
---

# Is Q-learning an Ill-posed Problem?

## Quick Facts
- arXiv ID: 2502.14365
- Source URL: https://arxiv.org/abs/2502.14365
- Authors: Philipp Wissmann; Daniel Hein; Steffen Udluft; Thomas Runkler
- Reference count: 6
- Key result: Q-learning instability persists even with bootstrapping-free targets and true dynamics, suggesting fundamental ill-posedness due to discontinuities in the true Q-function

## Executive Summary
This study systematically investigates the instability of Q-learning in continuous environments by progressively eliminating potential error sources. Using the cart-pole benchmark, the authors first demonstrate high iteration-wise performance variability with standard NFQ, then show that bootstrapping-free methods with learned dynamics reduce but don't eliminate this instability. Finally, using true environment dynamics with rollout-based targets further improves stability but still exhibits significant performance drops between iterations. The authors conclude that discontinuities in the true Q-function - caused by abrupt action switches at decision boundaries - make sample-based function approximation fundamentally unreliable, rendering Q-learning inherently ill-posed in continuous state spaces.

## Method Summary
The study uses the cart-pole benchmark with a four-dimensional state space (position, velocity, angle, angular velocity) and trains Q-functions using neural networks with mean squared error loss. Three experimental conditions are tested: standard NFQ with bootstrapped targets, BSF-NFQ with model-based rollouts using learned dynamics, and BSF-NFQ with true environment dynamics. Each method is trained for 100 iterations on 20,000 offline transitions collected from a random policy. Performance is evaluated by success rate (balancing for 5,000 steps) across 1,000 starting states, with variability assessed by retraining iterations with different random seeds.

## Key Results
- NFQ achieved only 3.8% success rate with significant iteration-wise instability
- BSF-NFQ with learned dynamics improved success to 23.1% but instability persisted
- BSF-NFQ with true dynamics reached 28% success but still exhibited performance drops between iterations
- Discontinuities in true Q-functions persist even with ε-greedy policies, though magnitude is reduced
- Retraining the same iteration with identical targets yields high performance variance across seeds

## Why This Works (Mechanism)

### Mechanism 1: Bootstrapping Elimination via Model-Based Rollouts
Replacing bootstrapped targets with model-based rollouts reduces but doesn't eliminate learning instability by avoiding recursive error accumulation inherent in standard Q-learning's Bellman backup. BSF-NFQ computes targets via multi-step rollouts using learned transition and reward models, but discontinuities in true Q-functions still cause downstream failures.

### Mechanism 2: True Dynamics Substitution
Providing exact environment dynamics further improves success rates but fails to resolve persistent instability. Using ground-truth transition equations eliminates model approximation error as a confounding factor, but the fundamental ill-posed nature of fitting discontinuous targets remains.

### Mechanism 3: Q-Function Discontinuity as Root Cause
The authors argue that discontinuities in true Q-functions render sample-based function approximation fundamentally unreliable. In continuous state spaces, optimal policies can switch actions abruptly at decision boundaries, creating discontinuities that neural networks smooth over, producing averaged approximations that misrepresent the optimal action near boundaries.

## Foundational Learning

- Concept: Bellman Equation and Bootstrapping
  - Why needed here: Understanding how Q-learning propagates value estimates and why this creates error accumulation.
  - Quick check question: Why does bootstrapping amplify small errors across iterations?

- Concept: Function Approximation with Smooth Interpolators
  - Why needed here: Grasping why neural networks fundamentally struggle with discontinuous targets—they approximate local averages rather than sharp boundaries.
  - Quick check question: If true Q-values jump by 20 units between adjacent states, what will a ReLU network likely predict?

- Concept: Deadly Triad (Bootstrapping + Off-Policy + Function Approximation)
  - Why needed here: This paper isolates components of the deadly triad to identify which instability sources remain after eliminating known factors.
  - Quick check question: Which two triad components did BSF-NFQ-real-dyn eliminate, and what remained?

## Architecture Onboarding

- Component map: Dataset (20k tuples) -> Q-network (5-64-1 MLP) -> Target computation (NFQ vs BSF-NFQ) -> Supervised MSE fitting -> Policy extraction -> Evaluation

- Critical path: 1) Generate or load offline dataset 2) Compute Q-targets using chosen method 3) Fit Q-network to targets via supervised learning 4) Extract policy and evaluate 5) Iterate with updated targets

- Design tradeoffs:
  - Bootstrapped targets: Faster computation, but error propagation risk
  - Rollout-based targets: More stable per-iteration estimates, requires model access
  - ε-greedy target policies: Reduce discontinuity magnitude but introduce stochasticity bias
  - Network capacity: Larger networks may overfit noise; smaller networks may underfit structure

- Failure signatures:
  - Policy performance collapses between adjacent iterations despite identical target data
  - High variance across random seeds when retraining on fixed targets
  - Training/validation loss uncorrelated with policy quality
  - Visualized Q-functions show smoothed approximations crossing true discontinuities

- First 3 experiments:
  1. Replicate NFQ baseline on cart-pole; log per-iteration returns and success rate to establish instability baseline
  2. Implement BSF-NFQ with true dynamics; compare iteration-wise variance reduction against NFQ
  3. Visualize Q-function slices (fix 3 state dimensions, vary angle θ) for learned vs true Q-values to identify where smoothing crosses decision boundaries

## Open Questions the Paper Calls Out

### Open Question 1
Can neural network architectures or regularizers be designed to accurately approximate discontinuous Q-functions without sacrificing generalization? The authors conclude that standard NNs "approximate an average over the samples," causing information loss at discontinuities, but do not propose a method to fit these sharp boundaries. What evidence would resolve it: A modified architecture that maintains high-frequency decision boundaries on the cart-pole benchmark without overfitting.

### Open Question 2
Do Q-function discontinuities persist in continuous action spaces, or are they primarily an artifact of discrete argmax operations? The analysis relies on a discrete action space (push left/right), where the "max" operator inherently introduces discontinuities in the value landscape. What evidence would resolve it: Visualizing the true Q-function structure for a continuous-action variant of the benchmark.

### Open Question 3
Does the density of Q-function discontinuities increase with state-space dimensionality in complex environments? The authors demonstrate the effect on a 4-dimensional benchmark but imply that "relevant MDPs" often involve higher dimensions. What evidence would resolve it: Stability analysis and Q-function visualization in high-dimensional control tasks (e.g., MuJoCo locomotion).

## Limitations

- Analysis is confined to a single benchmark (cart-pole), limiting generalization to other continuous control tasks
- The study focuses on the final Q-learning step while holding dataset and target computation fixed across experiments
- Authors demonstrate that discontinuities persist with ε-greedy policies but don't exhaustively explore policy variation effects on discontinuity severity

## Confidence

- **High confidence**: NFQ shows high iteration-wise instability; BSF-NFQ with true dynamics reduces but doesn't eliminate this instability
- **Medium confidence**: Discontinuities in true Q-functions are the fundamental cause of ill-posedness; smoothing by neural networks creates systematic approximation errors
- **Low confidence**: The problem extends universally to all sample-based Q-value evaluation methods; other continuous control tasks will exhibit identical behavior

## Next Checks

1. Test the same methodology on inverted pendulum and mountain car benchmarks to assess generalization across continuous control tasks
2. Compare performance using alternative function approximators (e.g., spline-based, tree-based) that can represent discontinuities more faithfully than neural networks
3. Implement a synthetic MDP with known discontinuous Q-function structure to isolate and quantify the smoothing effect across different network architectures