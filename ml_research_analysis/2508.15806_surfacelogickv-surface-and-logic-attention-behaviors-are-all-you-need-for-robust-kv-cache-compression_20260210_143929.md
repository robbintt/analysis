---
ver: rpa2
title: 'SurfaceLogicKV: Surface and Logic Attention Behaviors are All You Need for
  Robust KV Cache Compression'
arxiv_id: '2508.15806'
source_url: https://arxiv.org/abs/2508.15806
tags:
- head
- layer
- budget
- attention
- surfacelogickv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SurfaceLogicKV, a novel two-stage KV cache
  compression method based on analyzing attention behaviors in large language models.
  By identifying and leveraging Surface Memorization and Logic Construction behaviors,
  the method dynamically allocates KV cache budget at both layer- and head-wise levels.
---

# SurfaceLogicKV: Surface and Logic Attention Behaviors are All You Need for Robust KV Cache Compression

## Quick Facts
- arXiv ID: 2508.15806
- Source URL: https://arxiv.org/abs/2508.15806
- Authors: Mengjie Li; William J. Song
- Reference count: 40
- Key outcome: Two-stage KV cache compression method achieving competitive performance on long-context benchmarks (1K-129K tokens) by leveraging attention behavior analysis

## Executive Summary
This paper introduces SurfaceLogicKV, a novel KV cache compression method that analyzes attention behaviors in large language models to enable robust and efficient memory allocation. By distinguishing between Surface Memorization and Logic Construction behaviors through NIAH (Needle-In-A-Haystack) experiments, the method dynamically allocates KV cache budget at both layer- and head-wise levels. The approach achieves competitive or superior performance compared to existing baselines and even full KV cache in some scenarios, demonstrating improved compression robustness while maintaining inference quality across multiple models and long-context benchmarks.

## Method Summary
SurfaceLogicKV operates in two stages: first, it analyzes attention behaviors using NIAH experiments to compute per-head Inference Scores (INF_sc) that distinguish Surface Memorization from Logic Construction; second, it allocates KV cache budget using a hybrid fixed-dynamic approach that preserves baseline functionality while rewarding critical layer-head pairs. The method combines a minimal guaranteed budget with importance-weighted dynamic allocation, and when KV length exceeds allocated capacity, it selects top-k entries by attention relevance to the current query window while preserving the local context.

## Key Results
- Competitive performance vs. baselines (SnapKV, PyramidKV, HeadKV-R, HeadKV-R2) across Llama-3-8B, Mistral-7B, and Mistral-Large models
- Effective compression on long-context benchmarks (1K-129K tokens) with KV sizes 64-1024
- Robustness achieved through minimum 0.01 layer budget protection, even for layers with extremely low INF_sc
- NIAH analysis reveals 98.5% attention to irrelevant information, 1.5% to logic construction, 0.5% to surface memorization

## Why This Works (Mechanism)

### Mechanism 1: Attention Behavior Taxonomy via NIAH Confusion Matrix
The method maps attention weight distributions from NIAH tests to a confusion matrix analogy, deriving Surface Memorization Score (SF_sc = WO/(WO + WD)) and Logic Construction Score (LG_sc = WO/(WO + WS)) from Weight Correct, Distracted, and Subconscious attention patterns. The harmonic mean of these scores yields Inference Score (INF_sc), which quantifies each layer-head's importance for reasoning. This assumes attention weight patterns on NIAH tasks generalize to diverse downstream tasks.

### Mechanism 2: Hybrid Fixed + Dynamic Budget Allocation
Budget per head = fixed_portion(β) + dynamic_portion(β) × (0.01 + layer_importance) × head_importance. This combines guaranteed baseline functionality with importance-weighted allocation. The 0.01 minimum prevents zero-allocation to low-INF_sc layers, though this is an empirical safeguard rather than theoretically justified.

### Mechanism 3: Top-K KV Selection via Attention Relevance
When KV length exceeds allocated budget, the method selects top-k entries by computing Softmax(Q_window × K_cached^T / √d_k) and retaining top-k indices within budget after preserving the local window. This assumes the most recent query window should always be fully retained while historical relevance can be approximated by attention scores at selection time.

## Foundational Learning

- **Concept: KV Cache mechanics in autoregressive transformers**
  - Why needed here: Understanding that KV cache stores past key/value vectors to avoid recomputation is essential to grasp why compression matters and what's at risk.
  - Quick check question: If a model has 32 layers, 32 heads per layer, and sequence length 10K, what's the approximate memory impact of full KV cache vs. 64-token budget?

- **Concept: Needle-In-A-Haystack (NIAH) testing**
  - Why needed here: The paper's behavioral analysis is built on NIAH variants; understanding what NIAH measures (retrieval from long context) clarifies the experimental setup.
  - Quick check question: How does adding semantically similar distractors (the paper's modification) change what NIAH evaluates compared to the original formulation?

- **Concept: Attention weight interpretation**
  - Why needed here: INF_sc is derived from attention weight distributions; misinterpreting attention as "importance" rather than "relevance under current query" can lead to flawed intuitions.
  - Quick check question: Why might high attention to a needle not guarantee correct reasoning in multi-hop tasks?

## Architecture Onboarding

- **Component map:**
  Behavior Analysis Module (offline) -> Budget Allocator (inference-time) -> KV Selector (per-generation-step)

- **Critical path:**
  Offline: NIAH experiment → Algorithm 1 (SF_sc/LG_sc/INF_sc) → store per-head scores
  Online: Receive prompt → initialize KV → at each step, check capacity → if overflow, run selection (Eq. 7-8) → compress KV → continue generation

- **Design tradeoffs:**
  Higher β → more dynamic allocation (risk: low-INF_sc heads starved); lower β → more uniform (risk: inefficient use of budget)
  Minimum layer budget (0.01) → robustness (cost: wasted budget on potentially unimportant layers)
  Window retention size → coherence vs. historical capacity

- **Failure signatures:**
  Performance collapse on specific task types (e.g., multi-hop reasoning) while NIAH-style retrieval holds → likely INF_sc over-weights surface memorization
  Large variance across runs with same budget → may indicate unstable attention patterns or insufficient NIAH sampling

- **First 3 experiments:**
  1. Reproduce NIAH behavior analysis on your target model to confirm the 98.5%/1.5%/0.5% split holds; if distribution differs significantly, re-derive INF_sc before deployment.
  2. Ablate the 0.01 minimum layer budget by setting it to 0; compare performance on Long Dependency QA tasks to quantify robustness cost.
  3. Stress-test β sensitivity by sweeping β ∈ [1.0, 2.0] on a held-out benchmark; identify if the reported β=1.351 is optimal only for Llama-3-8B/Mistral-7B or generalizes.

## Open Questions the Paper Calls Out

### Open Question 1
How does the interaction between Logic Construction and Surface Memorization attention behaviors evolve as the available KV cache budget increases?
Basis: The authors state in the conclusion of the Ablation Study: "We will consider how Logic Construction attention behavior aids Surface Memorization behavior when KV Size increases as future work."
Why unresolved: The paper observes a performance gap between the two behaviors at low resource levels but lacks a model for how they collaborate when memory constraints are relaxed.
What evidence would resolve it: A study correlating varying KV budgets with the activation frequency and dependency between Logic and Surface heads.

### Open Question 2
How can the "Weight Wide" (wrong focus) attention behavior be effectively analyzed or optimized to improve KV cache compression?
Basis: Page 2 states: "Given the immense size of the broad context region, in-depth research into the wrong focus will be our future work."
Why unresolved: The current method allocates budget based primarily on Surface Memorization and Logic Construction scores, while the large portion of attention attributed to "wrong" or irrelevant focus is largely ignored.
What evidence would resolve it: Experiments that selectively prune or retain "Wide" KV entries to measure the impact on reasoning accuracy and hallucination rates.

### Open Question 3
Is the extremely low Inference Score observed in Layer 1 (and similar layers) indicative of redundancy, or does it serve a critical, unmeasured function in long-context reasoning?
Basis: The paper notes Layer 1 challenges previous oversimplified divisions and assigns a minimum budget "because we have not found rigorous... evidence confirming that such layers can be entirely discarded."
Why unresolved: The authors empirically protect these layers with a 0.01 budget to ensure safety, but lack theoretical proof of their necessity or dispensability.
What evidence would resolve it: Ablation studies showing the degradation curve when strictly removing low-score layers versus random layers.

### Open Question 4
Do the definitions of Surface Memorization and Logic Construction, derived from Needle-In-A-Haystack (NIAH) analysis, generalize to tasks without explicit retrieval targets?
Basis: The methodology relies on NIAH metrics (Weight Correct/Distracted) to define behaviors, which assumes the existence of a specific "needle" or ground truth answer.
Why unresolved: It is unclear if the scoring algorithm effectively distinguishes behaviors in abstract tasks like summarization or creative writing where "Weight Correct" is undefined.
What evidence would resolve it: Correlation analysis between attention heads ranked by NIAH scores and their importance in non-NIAH tasks.

## Limitations
- NIAH-based behavioral analysis may not generalize to non-retrieval tasks like multi-hop reasoning or summarization
- β=1.351 appears empirically tuned for specific models without systematic sensitivity analysis across architectures
- The 0.01 minimum layer budget safeguard lacks theoretical justification and may waste resources

## Confidence

- **High confidence**: NIAH analysis methodology and KV selection algorithm are clearly specified and implementable; hybrid fixed-dynamic budget allocation mechanism is well-defined
- **Medium confidence**: Behavioral taxonomy is theoretically sound but generalizability beyond retrieval tasks requires validation; empirical results show competitive performance but minimum budget safeguard lacks theoretical basis
- **Low confidence**: Assumption that β=1.351 is universally optimal across model families and task types is not supported by systematic sweeps; paper doesn't address domain-specific attention behavior shifts

## Next Checks

1. Apply NIAH-derived INF_sc scores to non-retrieval tasks (multi-hop QA, code generation, summarization) and compare against task-specific attention analysis to measure generalization limits.

2. Systematically sweep β from 1.0 to 2.0 on Llama-3-8B and Mistral-7B across all benchmark tasks to identify optimal values for different task categories.

3. Remove the 0.01 minimum layer budget constraint and test performance degradation on long-range reasoning tasks to validate whether the safeguard is genuinely necessary.