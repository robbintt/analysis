---
ver: rpa2
title: 'SPIRe: Boosting LLM Inference Throughput with Speculative Decoding'
arxiv_id: '2504.06419'
source_url: https://arxiv.org/abs/2504.06419
tags:
- decoding
- draft
- speculative
- cost
- throughput
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of optimizing speculative decoding
  (SD) for high throughput LLM inference, rather than just low latency. The authors
  introduce SPIRe, a draft model that combines static sparse attention, pruned initialization
  from the target model, and a Feedback Transformer architecture with target model
  activation memory to significantly improve throughput.
---

# SPIRe: Boosting LLM Inference Throughput with Speculative Decoding

## Quick Facts
- arXiv ID: 2504.06419
- Source URL: https://arxiv.org/abs/2504.06419
- Authors: Sanjit Neelam; Daniel Heinlein; Vaclav Cvicek; Akshay Mishra; Reiner Pope
- Reference count: 5
- Primary result: SPIRe achieves over 100% increase in modeled throughput compared to vanilla speculative decoding and over 35% compared to MagicDec, particularly when context lengths vary across requests.

## Executive Summary
This paper addresses the challenge of optimizing speculative decoding (SD) for high throughput LLM inference, rather than just low latency. The authors introduce SPIRe, a draft model that combines static sparse attention, pruned initialization from the target model, and a Feedback Transformer architecture with target model activation memory to significantly improve throughput. SPIRe achieves over 100% increase in modeled throughput compared to vanilla speculative decoding and over 35% compared to the strong baseline MagicDec, particularly when context lengths vary across requests. The work includes an implementation-agnostic performance model to evaluate throughput and demonstrates effectiveness when scaling to large batch sizes and medium-to-long contexts.

## Method Summary
SPIRe combines three key techniques: (1) StreamingLLM sparse attention with a sliding window (size 64) and attention sink (size 1) to maintain constant KV cache size regardless of context length, (2) pruned initialization by copying embedding, last N transformer blocks, and unembedding from the target model to improve draft quality, and (3) a Feedback Transformer architecture that substitutes target model activation memory vectors during training to improve draft quality while maintaining parallelism. The draft model is trained with a MixedLoss combining distillation loss and acceptance probability maximization, using target activations for early memory vectors and the draft's own memory vectors autoregressively at inference.

## Key Results
- SPIRe achieves over 100% increase in modeled throughput compared to vanilla speculative decoding
- SPIRe achieves over 35% increase in throughput compared to MagicDec baseline
- Throughput improvements persist or improve as batch size increases, unlike vanilla SD
- Sparse KV cache reduces memory costs while maintaining draft quality when context lengths vary

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sparse KV cache with StreamingLLM attention enables speculative decoding speedups to persist or improve as batch size increases, unlike vanilla SD.
- Mechanism: A sliding window (size 64) + attention sink (size 1) mask keeps KV cache size constant w.r.t. sequence length. In large-batch long-context regimes, KV cache loads dominate memory bandwidth; reducing KV size reduces ∆t more than it reduces acceptance rate τ. The throughput multiplier τ/∆t thus improves with batch size.
- Core assumption: Verification and draft forward passes remain memory-bound; HOI (FLOPs/byte) is such that memory cost exceeds compute cost for relevant batch/context configurations.
- Evidence anchors:
  - [abstract] "Recent work shows that SD can accelerate decoding with large batch sizes too if the context is sufficiently long and the draft model's KV cache is sparse."
  - [section 3.1] "As shown in Figure 4, this results in an increase in the throughput multiplier as the batch size increases, which is never the case for vanilla speculative decoding."
  - [corpus] MagicDec (Chen et al., 2024) demonstrates similar sparse KV benefits; SPIRe builds on this with additional architecture changes.
- Break condition: If batch size is small or context is short, KV cache is not the bottleneck; sparse KV yields no benefit and may harm throughput due to added FLOPs.

### Mechanism 2
- Claim: Pruned initialization from the target model yields higher average generation length τ than random initialization.
- Mechanism: Copy embedding, last N transformer blocks, and unembedding from trained target → draft starts in a region of parameter space already aligned with target's token distribution. Matching embedding dimension enables direct substitution of target activations as memory vectors during training.
- Core assumption: The pruned layers capture sufficient target knowledge; remaining draft layers can be trained efficiently on top.
- Evidence anchors:
  - [section 3.2] "As shown in Figure 5, initializing by pruning the target model produced a significantly higher value of τ than random initialization."
  - [section 5] Training SPIRe costs ~1/4 the FLOPs of training the target model.
  - [corpus] No direct corpus comparison for pruning-based draft initialization; this is a relatively novel contribution.
- Break condition: If draft model depth/width differs substantially from target, simple layer copying may not transfer well.

### Mechanism 3
- Claim: Feedback Transformer with target model activation memory improves draft quality while remaining trainable without full sequential unrolling.
- Mechanism: During training, for tokens t = 1...S-k, replace draft memory vectors m_t with target model activations y_t from corresponding layers. Only compute k forward passes per training step (not S), recovering parallelism. At inference, use draft's own memory vectors autoregressively.
- Core assumption: Target activations provide a high-quality proxy for what the draft's memory vectors should approximate; the draft can learn to approximate this relationship.
- Evidence anchors:
  - [section 3.3] "substituting past memory vectors with target model activations... was better than sharing memory vectors across layers and substituting... with draft model embeddings."
  - [figure 5 ablations] Attending to target activations is the second most important technique after sparse KV.
  - [corpus] Feedback Transformers (Fan et al., 2021) showed representation gains but training costs; SPIRe's activation substitution is a novel adaptation for draft models.
- Break condition: If target model activations are unavailable (e.g., black-box API), this mechanism cannot be applied.

## Foundational Learning

- Concept: **Memory-bound vs compute-bound inference regimes**
  - Why needed here: The entire rationale for SPIRe depends on understanding when KV cache loads (memory) dominate vs weight loads or FLOPs (compute). Small batch = weight-bound; large batch + long context = KV-bound.
  - Quick check question: For batch size 64 and context length 512, is the draft forward pass memory-bound or compute-bound on your hardware? How would you verify?

- Concept: **Speculative decoding acceptance rate and rejection sampling**
  - Why needed here: The throughput multiplier is τ/∆t; τ depends on how many draft tokens the target accepts. Understanding the modified rejection sampling scheme explains why accepted tokens are guaranteed samples from the target distribution.
  - Quick check question: If draft and target distributions are identical, what is the expected acceptance rate for k draft tokens?

- Concept: **Hardware Operational Intensity (HOI)**
  - Why needed here: The performance model converts memory accesses to FLOP-equivalents via HOI to compare compute and memory costs on equal footing. Different hardware has different HOI values.
  - Quick check question: If HOI = 100 FLOPs/byte and KV cache is 1MB, what is the memory cost in FLOP-equivalents?

## Architecture Onboarding

- Component map:
  Target model (8-layer MHA transformer, 67M body params) -> SPIRe draft (2-layer Feedback Transformer with StreamingLLM sparse attention, ~1/4 target body params) -> MixedLoss training with target activation memory substitution

- Critical path:
  1. Training: For each sequence, perform k=4 forward passes on prefixes of length k, 2k, 3k, ...; substitute target activations for positions 1 to S-k
  2. Inference: Autoregressively generate k draft tokens using draft's own sparse KV cache; verify with target; accept/reject via modified rejection sampling
  3. MixedLoss: ω·distillation + (1-ω)·acceptance-probability maximization (ω=0.5)

- Design tradeoffs:
  - Larger draft model → higher τ but higher tdraft; SPIRe chooses 1/4 target size as balance
  - Smaller sparse window → lower KV cost but potentially lower τ; SPIRe uses 64+1
  - Feedback memory → better representations but inhibits training parallelism; mitigated by activation substitution

- Failure signatures:
  - Throughput multiplier < 1.0: Draft model too slow or acceptance rate too low; check ∆t and τ separately
  - Degradation at short contexts: Sparse KV overhead not offset by benefits; consider hybrid approach
  - Training divergence: Target activation dimensions mismatched with draft embedding dimension

- First 3 experiments:
  1. Replicate the throughput multiplier calculation for (B=64, L=512) with vanilla SD baseline; verify ~2.78x speedup vs ~1.0x baseline
  2. Ablate sparse KV: Run SPIRe with dense KV; expect constant throughput multiplier w.r.t. context length (no growth as L increases)
  3. Ablate target activation substitution: Train with draft embeddings instead; measure τ drop

## Open Questions the Paper Calls Out

- Question: How accurately does the proposed implementation-agnostic performance model predict actual wall-clock throughput on production hardware?
  - Basis in paper: [explicit] The Conclusion states, "Future work can empirically validate our performance model."
  - Why unresolved: The authors use a theoretical model based on FLOPs and memory access (HOI) to compare architectures, intentionally isolating results from specific kernel optimizations or hardware nuances.
  - What evidence would resolve it: End-to-end benchmarks on specific hardware (e.g., H100 GPUs) comparing measured throughput against the model's predicted multipliers across varying batch sizes.

- Question: How does the speedup of SPIRe change relative to the maximum speculation depth $k$?
  - Basis in paper: [explicit] The Conclusion lists the need to "analyze the sensitivity of speedup with respect to the maximum speculation depth k."
  - Why unresolved: The evaluation fixes the speculation depth at $k=4$; it is unknown if deeper speculation yields diminishing returns or if the Feedback Transformer architecture supports longer draft sequences efficiently.
  - What evidence would resolve it: A sweep of throughput metrics with $k$ ranging from 1 to 16 to identify the optimal trade-off between verification overhead and acceptance rate.

- Question: How does SPIRe interact with continuous batching systems in a dynamic serving environment?
  - Basis in paper: [explicit] In Related Work, the authors note that "studying their interaction further is an interesting direction for future work."
  - Why unresolved: Continuous batching dynamically preempts and schedules requests, which may conflict with SPIRe's use of static sparse attention and target model activation memory.
  - What evidence would resolve it: Measurements of SPIRe's performance when integrated into a continuous batching scheduler (like vLLM or Orca) under variable load.

- Question: Can more sophisticated pruning methods improve SPIRe's initialization compared to the current layer-selection heuristic?
  - Basis in paper: [explicit] Section 3.2 suggests, "Future work could explore more sophisticated pruning strategies, such as the one described by Muralidharan et al. [2024]."
  - Why unresolved: The current method simply selects the final layers of the target model; advanced pruning might retain a more optimal distribution of weights that increases draft accuracy (acceptance rate).
  - What evidence would resolve it: Comparing the average generation length ($\tau$) of the current SPIRe model against a version initialized using gradient-based or magnitude-based pruning.

## Limitations
- Hardware sensitivity: Performance heavily depends on specific hardware characteristics (HOI, memory bandwidth) not fully specified
- Model size generalization: Results demonstrated only on 67M parameter model, unclear how techniques scale to larger models
- Target activation dependency: Requires access to target model during both training and inference, creating deployment challenges
- Context distribution sensitivity: Performance benefits may be highly sensitive to specific context length distributions in real applications

## Confidence

**High Confidence Claims**:
- The theoretical framework for analyzing throughput in speculative decoding is sound. The throughput multiplier formula τ/Δt and the implementation-agnostic performance model are well-grounded in established principles of memory-bound vs compute-bound computation.
- The observation that sparse KV cache with StreamingLLM attention can improve throughput as batch size increases is supported by both this work and prior research (MagicDec).

**Medium Confidence Claims**:
- The specific architectural choices in SPIRe (Feedback Transformer with target activation memory) provide significant improvements over alternatives. While ablation studies are presented, the comparison is limited to a specific baseline model and hardware configuration.
- The claim of >100% throughput improvement over vanilla SD and >35% over MagicDec is based on the performance model rather than direct empirical measurement across diverse scenarios.

**Low Confidence Claims**:
- The scalability of SPIRe to much larger models (1B+ parameters) and its performance in production environments with varying hardware configurations.
- The long-term stability and generalization of models trained with the MixedLoss objective, particularly regarding potential degradation in generation quality over extended sequences.

## Next Checks

**Validation Check 1**: Replicate the throughput multiplier calculation for a different hardware configuration (e.g., A100 GPU vs TPU) using the provided performance model. Verify that the memory-bound regime assumptions hold and that the claimed improvements persist across hardware with different HOI values and memory bandwidths.

**Validation Check 2**: Conduct ablation studies on model size scaling by implementing SPIRe for target models of 100M, 500M, and 1B parameters. Measure how the throughput multiplier, acceptance rate τ, and training costs scale with target model size. This will reveal whether the 1/4 draft size ratio remains optimal.

**Validation Check 3**: Implement a hybrid approach that dynamically switches between sparse and dense KV cache based on context length and batch size. Compare the average throughput across workloads with varying context length distributions to SPIRe's fixed sparse approach. This will test whether the performance gains are robust to real-world request patterns.