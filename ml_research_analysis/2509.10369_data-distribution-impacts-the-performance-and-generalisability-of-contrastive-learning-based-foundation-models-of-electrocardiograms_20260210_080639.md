---
ver: rpa2
title: Data distribution impacts the performance and generalisability of contrastive
  learning-based foundation models of electrocardiograms
arxiv_id: '2509.10369'
source_url: https://arxiv.org/abs/2509.10369
tags:
- performance
- cohorts
- cohort
- pretraining
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how cohort composition impacts the performance
  and generalisation of contrastive learning-based foundation models for ECG analysis.
  Using over 5.2 million ECGs from four continents, the researchers pretrained CAPE
  foundation models on diverse datasets and systematically evaluated downstream performance
  across six cohorts.
---

# Data distribution impacts the performance and generalisability of contrastive learning-based foundation models of electrocardiograms

## Quick Facts
- arXiv ID: 2509.10369
- Source URL: https://arxiv.org/abs/2509.10369
- Reference count: 40
- Primary result: IDB strategy improves OOD sex AUC from 0.822 to 0.948 and reduces age MAE from 12.00 to 7.35

## Executive Summary
This study investigates how cohort composition impacts the performance and generalisation of contrastive learning-based foundation models for ECG analysis. Using over 5.2 million ECGs from four continents, the researchers pretrained CAPE foundation models on diverse datasets and systematically evaluated downstream performance across six cohorts. They found that while multi-centre pretraining improved in-distribution accuracy, it significantly reduced out-of-distribution generalisation due to encoding cohort-specific artifacts. The authors propose an In-Distribution Batch (IDB) strategy that constrains contrastive learning batches to single cohorts, which improved OOD performance from 0.822 to 0.948 AUC for sex classification and reduced age prediction error from 12.00 to 7.35 MAE. These findings highlight the critical importance of cohort design and evaluation protocols for developing clinically fair and robust foundation models in healthcare.

## Method Summary
The researchers developed CAPE (CArdiac Pre-training with contrAstive sEmantic learning), a contrastive learning-based foundation model for ECG analysis. They pretrained the model on 5.2 million ECGs from four cohorts (BIDMC, CODE, SHZS, VUMC) using 8 leads, 400Hz sampling, and 7-second windows. The encoder used a 4-layer ResNet architecture producing 256-dimensional embeddings. Two training strategies were compared: random batching across cohorts (CAPE-X) and In-Distribution Batch (IDB) with single-cohort batches (CAPE-Z). The model was trained using InfoNCE loss with temperature parameter τ, batch size 1024, and 200 epochs. Downstream evaluation used MLP heads trained on extracted features for age regression and sex classification across six evaluation cohorts.

## Key Results
- CAPE-Z (IDB strategy) improved OOD sex AUC from 0.822 to 0.948 compared to CAPE-X
- Age prediction MAE decreased from 12.00 to 7.35 years using IDB strategy
- CAPE-X (random batching) achieved best in-distribution performance (sex AUC 0.955, age MAE 7.86)
- Secondary care cohorts (BIDMC, VUMC) yielded better representations than primary care cohorts despite smaller size

## Why This Works (Mechanism)

### Mechanism 1: In-Distribution Batch (IDB) Constrains Negative Sampling Distribution
Constraining contrastive learning batches to single cohorts improves out-of-distribution (OOD) generalization by preventing the model from learning spurious cohort-distinguishing features. Standard contrastive learning treats all samples from different patients as negative pairs, regardless of their source distribution. When batches mix multiple cohorts with distinct acquisition hardware or preprocessing pipelines, the model learns to distinguish cohorts via technical artifacts rather than clinically meaningful signals. IDB ensures negative pairs originate from the same distribution, forcing the model to learn within-cohort variability that transfers better.

### Mechanism 2: Multi-Center Pretraining Trades Off In-Distribution Accuracy for OOD Generalization
Pretraining on diverse multi-center data improves in-distribution performance but can degrade OOD generalization when contrastive batches are randomly sampled across cohorts. Multi-center data increases representation diversity, benefiting tasks evaluated on similar distributions. However, random batching introduces inter-cohort distributional noise into the contrastive objective—negative pairs from different cohorts may differ primarily due to acquisition artifacts rather than biological signal, causing the encoder to overfit to these shortcuts.

### Mechanism 3: Secondary Care Cohorts Yield Richer Representations Due to Pathological Diversity
Pretraining on secondary care cohorts (higher cardiovascular disease prevalence) produces more robust representations than healthier population cohorts, independent of cohort size. Secondary care ECGs exhibit greater signal variability due to diverse cardiac pathologies. Contrastive learning exploits this variability—more diverse positive pairs per patient yield richer representations. Healthier populations have reduced waveform variability, limiting the signal available for representation learning.

## Foundational Learning

- **Contrastive Learning (Self-Supervised)**
  - Why needed here: CAPE uses contrastive pretraining to learn ECG representations without labels by pulling together augmented views from the same patient and pushing apart views from different patients.
  - Quick check question: Can you explain why the InfoNCE loss encourages similar representations for positive pairs and dissimilar representations for negative pairs?

- **In-Distribution vs. Out-of-Distribution (OOD) Generalization**
  - Why needed here: The paper's central finding is a trade-off between ID accuracy and OOD robustness; understanding this distinction is essential for interpreting all results.
  - Quick check question: If a model is trained on BIDMC (US secondary care) and tested on CODE (Brazil primary care), is this ID or OOD evaluation?

- **Distribution Shift / Domain Shift**
  - Why needed here: ECG cohorts differ in demographics, health status, recording devices, and institutional practices—all sources of distribution shift that affect model transfer.
  - Quick check question: Name three sources of distribution shift between BIDMC and CODE cohorts mentioned in the paper.

## Architecture Onboarding

- **Component map:**
  - Preprocessing pipeline -> 4-layer ResNet backbone -> 256-dim embeddings -> MLP downstream head
  - 8 leads (I, II, V1–V6) -> Bandpass filter (0.5–100 Hz), notch filter -> Resampling to 400 Hz -> 7-second windows
  - InfoNCE loss -> Same-patient ECG pairs (positives) -> Different-patient ECG pairs (negatives)
  - IDB batch sampler -> Single cohort per batch -> Cycles through cohorts during training

- **Critical path:**
  1. Preprocess all ECGs to standardized format (400 Hz, 8 leads, 7 seconds).
  2. Pretrain CAPE encoder using contrastive loss with either random batching (CAPE-X) or IDB (CAPE-Z) for 200 epochs, batch size 1024 (512 patients × 2 ECGs).
  3. Extract 256-dim features for labeled downstream cohorts using frozen encoder.
  4. Train MLP prediction head on extracted features for age (MAE) or sex (AUC) tasks.
  5. Evaluate ID performance on held-out test splits; evaluate OOD performance on external cohorts.

- **Design tradeoffs:**
  - **Random batching vs. IDB**: Random batching (CAPE-X) maximizes negative diversity per batch but risks learning cohort artifacts; IDB (CAPE-Z) constrains negatives to same distribution, improving OOD robustness at potential cost of slower convergence.
  - **Multi-center vs. single-cohort pretraining**: Multi-center (BCSV) improves ID performance but can degrade OOD; single-cohort may generalize better to similar populations but lacks diversity.
  - **Frozen encoder vs. fine-tuning**: Paper uses frozen backbone with MLP head only; fine-tuning could improve performance but was not evaluated (noted as limitation).
  - **Batch size vs. compute**: InfoNCE benefits from larger batches (more negatives); paper uses 1024 limited by GPU memory (NVIDIA RTX 3090, ~5 min/epoch for BIDMC, ~50 min/epoch for BCSV).

- **Failure signatures:**
  - **Cohort clustering in t-SNE**: If learned features cluster by cohort or device type rather than mixing continuously, the model has encoded distributional artifacts—try IDB.
  - **OOD performance collapse**: Sex AUC dropping to ~0.6 (near random) on external cohorts indicates severe distribution shift sensitivity—verify batch composition strategy.
  - **High variance across OOD runs**: Large confidence intervals on OOD metrics suggest unstable representations—may indicate insufficient pretraining or batch sampling issues.
  - **Secondary care model underperforms on healthy populations**: May indicate overfitting to pathological patterns—consider mixed-cohort pretraining with IDB.

- **First 3 experiments:**
  1. **Reproduce IDB vs. random batching on public data**: Use CODE subset (Zenodo) and PTB-XL; pretrain CAPE-X and CAPE-Z on combined data; evaluate sex/age prediction on PTB-XL held-out set. Verify that CAPE-Z reduces cohort clustering in t-SNE.
  2. **Ablate batch composition**: Test intermediate strategies—e.g., 50% single-cohort batches, 50% mixed; or stratified sampling proportional to cohort size. Measure impact on ID vs. OOD metrics.
  3. **Characterize artifact sources**: Within CODE (which has two device types), train models with device-annotated batches. Test whether device ID can be predicted from CAPE-X vs. CAPE-Z embeddings to quantify artifact encoding.

## Open Questions the Paper Calls Out
The authors note that their analysis of cohort-specific artifacts is limited to comparisons between cohorts and does not definitively rule out other potential factors such as genetic or lifestyle differences between populations. They also highlight that their evaluation focuses on two relatively simple downstream tasks (age and sex), which may not fully capture the generalization challenges for more complex clinical predictions. The paper does not explore fine-tuning the encoder backbone, which could potentially improve results.

## Limitations
- The study cannot definitively rule out biological variability between populations as contributing to OOD generalization failure, focusing primarily on technical artifacts.
- The fixed 4-layer ResNet architecture may limit the model's capacity to learn more robust representations that could better handle distribution shift.
- Evaluation focuses on two relatively simple downstream tasks (age and sex), which may not fully capture generalization challenges for complex clinical predictions.

## Confidence

- **High confidence**: The IDB strategy improves OOD performance (CAPE-Z achieving 0.948 AUC vs 0.822 for sex classification) and the finding that random multi-center pretraining degrades OOD generalization. These results are directly measured and reproducible.
- **Medium confidence**: The mechanism explaining why IDB works (preventing cohort artifact learning) is plausible but not definitively proven. The secondary care vs primary care representation quality difference is supported by the data but could have alternative explanations.
- **Low confidence**: The relative contribution of different artifact sources (device type vs. preprocessing vs. demographic differences) to OOD failure is not quantified. The paper also does not explore fine-tuning the encoder backbone, which could potentially improve results.

## Next Checks

1. **Artifact isolation experiment**: Within CODE (which has two device types), train models with device-annotated batches and measure whether device ID can be predicted from CAPE-X vs CAPE-Z embeddings to quantify artifact encoding.
2. **Intermediate batching strategies**: Test hybrid approaches (e.g., 50% single-cohort batches, 50% mixed) to determine if gradual cohort mixing improves the trade-off between ID and OOD performance.
3. **Fine-tuning evaluation**: Fine-tune the frozen CAPE encoder on downstream tasks and measure whether this recovers some of the ID performance lost when using IDB, while maintaining OOD robustness.