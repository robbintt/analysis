---
ver: rpa2
title: Learning Invariant Graph Representations Through Redundant Information
arxiv_id: '2512.06154'
source_url: https://arxiv.org/abs/2512.06154
tags:
- information
- graph
- invariant
- learning
- spurious
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of out-of-distribution (OOD)
  generalization in graph neural networks by introducing a novel framework called
  Redundancy-guided Invariant Graph learning (RIG). The key insight is that existing
  invariant graph learning methods that rely on classical information-theoretic measures
  are limited in their ability to capture the nuanced relationship between spurious
  and invariant subgraphs.
---

# Learning Invariant Graph Representations Through Redundant Information

## Quick Facts
- arXiv ID: 2512.06154
- Source URL: https://arxiv.org/abs/2512.06154
- Reference count: 29
- Primary result: Introduces RIG framework that leverages Partial Information Decomposition to maximize redundant information between spurious and invariant subgraphs, achieving improved OOD generalization across synthetic and real-world graph datasets

## Executive Summary
This paper addresses out-of-distribution generalization challenges in graph neural networks by introducing a novel framework called Redundancy-guided Invariant Graph learning (RIG). The key insight is that existing invariant graph learning methods relying on classical information-theoretic measures cannot precisely capture the nuanced relationship between spurious and invariant subgraphs. To overcome this, the authors propose leveraging Partial Information Decomposition (PID) to decompose joint information into unique, redundant, and synergistic components. The framework demonstrates improved OOD generalization capabilities across four synthetic and seven real-world graph datasets, outperforming state-of-the-art baselines in most cases.

## Method Summary
RIG employs a three-step alternating optimization strategy to learn invariant graph representations. First, a rationale generator decomposes the input graph into causal (Ĝc) and spurious (Ĝs) subgraphs using soft edge masks. Second, an auxiliary function estimates a lower bound of redundant information between these subgraphs by enforcing similarity between their encodings. Third, the main objective maximizes causal prediction accuracy while maintaining redundancy with the spurious branch and enforcing contrastive constraints on invariant subgraph representations. The framework uses a 3-layer GIN backbone with alternating optimization between redundancy estimation and objective maximization phases.

## Key Results
- On Two-piece dataset with invariant correlation strength 0.8 and spurious correlation strength 0.9, RIG achieves 77.82% accuracy, outperforming GALA (76.51%) and CIGAv2 (49.24%)
- RIG demonstrates improved OOD generalization across seven real-world DrugOOD datasets with ROC-AUC improvements of 1.3% to 8.1% over baselines
- Ablation studies confirm the importance of all three steps, with the complete framework achieving 77.82% accuracy versus 77.00% for Step 0+2 alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Focusing on redundant information rather than total mutual information more precisely separates spurious from invariant subgraphs under both FIIF and PIIF causal models
- Mechanism: PID disentangles I(Y;S) into Uni(Y:S|C) + Red(Y:S,C). Under FIIF, Uni(Y:S|C)=0, so maximizing I(Ŷs;Y) wastefully optimizes a term that should be zero. RIG maximizes only Red(Y:Ĝc,Ŷs), avoiding contamination from unique spurious information
- Core assumption: The true data-generating process follows either the FIIF or PIIF structural causal model where invariant and spurious components have redundant predictive information
- Evidence anchors: [abstract] "motivating the need to precisely focus on redundant information about the target Y shared between spurious subgraphs Gs and invariant subgraphs Gc obtained via PID"; [section 3, Lemma 1] "Under the FIIF assumption, the true spurious variable S does not have any unique information about the target variable Y, i.e., Uni(Y:S|C) = 0, but S and C may have redundant information"

### Mechanism 2
- Claim: Intersection information (I∩ measure) provides a tractable lower bound for redundant information that can be estimated via constrained optimization
- Mechanism: Red(Y:Ŷs,Ĝc) ≥ Red_∩(Y:Ŷs,Ĝc) = max_Q I(Y;Q) s.t. Q = f_θc(Ĝc) = f_θs(Ŷs). The constraint enforces that Q captures only shared information; the maximization ensures Q is maximally predictive of Y
- Core assumption: Deterministic neural networks f_θc, f_θs can approximate the functions that extract shared information; the lower bound is sufficiently tight to guide learning
- Evidence anchors: [page 6, Definition 2] Formal definition of I∩ measure with constrained optimization; [page 6, Eq. 13-14] Practical implementation using cross-entropy loss with constraint regularizer L_c

### Mechanism 3
- Claim: Alternating optimization between redundancy estimation and objective maximization prevents the spurious branch from dominating while ensuring invariant features remain predictive
- Mechanism: Step 1 freezes β,η_c,ϕ_c and optimizes θ_s,θ_c,ϕ_s to estimate redundancy. Step 2 freezes θ_s,θ_c,ϕ_s and optimizes β,η_c,ϕ_c using the estimated redundancy. This prevents gradient interference between the two objectives
- Core assumption: The estimated redundancy from Step 1 is stable enough to guide Step 2 before the next alternation; the assistant model A correctly identifies spurious-dominated vs. invariant-dominated samples
- Evidence anchors: [abstract] "Our approach relies on alternating between estimating a lower bound of redundant information (which itself requires an optimization) and maximizing it along with additional objectives"; [page 6, Algorithm 1] Complete pseudo-code with cycle-based alternation between Step 1 (e_1 epochs) and Step 2 (e_2 epochs)

## Foundational Learning

- Concept: Partial Information Decomposition (PID)
  - Why needed here: Classical mutual information I(Y;A,B) cannot distinguish whether predictive information comes uniquely from A, uniquely from B, or redundantly from both. RIG's theoretical foundation requires understanding this decomposition
  - Quick check question: Given two variables A and B that both predict Y, can you explain why I(Y;A) + I(Y;B) might exceed I(Y;A,B)?

- Concept: Structural Causal Models (FIIF vs. PIIF)
  - Why needed here: The paper's theoretical analysis (Lemmas 1-2) depends on whether the invariant features are fully informative (Y⊥⊥S|C) or partially informative (Y⊁⊥S|C). Different causal structures create different PID decompositions
  - Quick check question: In FIIF, does S causally influence Y, or does C mediate all of S's information about Y?

- Concept: Environment-aware Contrastive Learning for Invariance
  - Why needed here: The constraint I(Ĝp_c;Ĝn_c|Y) uses contrastive learning with samples from different environments (identified by assistant A) to enforce invariant subgraph extraction
  - Quick check question: Why must positive samples G_p share the same label Y but different assistant predictions ŷ_e?

## Architecture Onboarding

- Component map:
  Input Graph G → Rationale Gen h_β (extracts edge masks → Ĝc, Ŷs) → f_ηc, g_ϕc (causal branch → Ŷc) and f_θs, g_ϕs (spurious branch → Ŷs) → Auxiliary f_θc (maps Ĝc to shared Q) → Assistant Model A (identifies G_p, G_n for contrastive sampling)

- Critical path:
  1. Warm-up (Step 0): Train all parameters with L_CE(Y,Ŷc) to initialize representations
  2. Redundancy Estimation (Step 1): Freeze β,η_c,ϕ_c; optimize θ_s,θ_c,ϕ_s to minimize L_r = L_CE(Y,Ŷs) + λ₁L_c(f_θs(Ŷs), f_θc(Ĝc))
  3. Objective Maximization (Step 2): Freeze θ_s,θ_c,ϕ_s; optimize β,η_c,ϕ_c to minimize L = L_CE(Y,Ŷc) + λ₂L_CE(Y,Ŷs) + λ₃L_cont(Ĝp_c,Ĝn_c)
  4. Alternate: Cycle between Step 1 (e₁ epochs) and Step 2 (e₂ epochs)

- Design tradeoffs:
  - λ₁ constraint weight: Too weak → Q not truly shared; too strong → f_θs and f_θc collapse to constant outputs
  - λ₂ redundancy weight: Controls how much spurious branch should predict Y. Higher values increase redundancy but risk over-reliance on spurious features
  - e₁ vs e₂ epoch allocation: e₁ needs enough iterations for redundancy estimation to converge; e₂ needs enough for invariant extraction

- Failure signatures:
  - High spurious accuracy with low causal accuracy: Model relies on spurious branch; check if λ₂ is too high or assistant A is unreliable
  - Zero redundancy in PID decomposition: Step 1 not converging; increase e₁ or check constraint satisfaction
  - High variance across seeds: Assistant model's cluster predictions unstable; try different assistant architectures or increase cluster count

- First 3 experiments:
  1. Reproduce Two-piece {0.8, 0.9} baseline: Use 3-layer GIN (hidden=32), e₁=10, e₂=10, λ₂=2.5, λ₃=128. Verify ~77-78% accuracy and compare PID decomposition with Table 4
  2. Ablate redundancy term: Set λ₂=0 (disable redundancy maximization) and compare against full RIG on {0.8, 0.9}. Expected: significant drop, confirming redundancy mechanism is active
  3. Stress test with stronger spurious correlation: Run on {0.7, 0.9} where spurious correlation (0.9) exceeds invariant (0.7). Compare against GALA and CIGAv2

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the alternating optimization strategy in RIG guarantee theoretical convergence?
- Basis in paper: [explicit] The authors state in Appendix A, "A rigorous proof of the optimization convergence remains an important direction for future research"
- Why unresolved: The framework employs a complex multi-level optimization that alternates between estimating a lower bound of redundant information and maximizing the main objective, making standard convergence proofs difficult to apply directly
- What evidence would resolve it: A formal mathematical proof showing that the alternating updates of θ_s, θ_c (estimation) and β, η_c, ϕ_c (maximization) converge to a stable critical point under specific learning rate conditions

### Open Question 2
- Question: Can reducing the approximation error in redundant information estimation improve performance on datasets where RIG currently underperforms?
- Basis in paper: [explicit] Appendix A notes that RIG "does not consistently achieve superior performance... attributed to the approximation involved in estimating redundant information"
- Why unresolved: RIG currently maximizes a lower bound (intersection information) rather than the exact redundant information term, which may lead to suboptimal graph disentanglement in certain distribution shifts
- What evidence would resolve it: A comparative study where the "Intersection Information" estimator is replaced with tighter or exact PID estimators to measure performance changes on the less challenging datasets mentioned

### Open Question 3
- Question: Can the Redundancy-guided Invariant Learning framework be effectively adapted for non-graph domains?
- Basis in paper: [explicit] The conclusion states, "Future work will study extensions beyond the graph domain"
- Why unresolved: The current implementation relies on a rationale generator specifically designed for graph structures (h: G → G_c), and it is unclear if the PID-based objective translates directly to Euclidean data like images or sequential data
- What evidence would resolve it: A modification of the RIG framework applied to image or text OOD benchmarks (e.g., domain generalization datasets) demonstrating comparable improvements over baseline invariant learning methods

## Limitations
- The I∩ measure lower bound for redundant information is conceptually sound but lacks extensive empirical validation of tightness across diverse graph distributions
- The alternating optimization scheme depends critically on hyperparameter tuning (e₁, e₂, λ weights) with limited ablation on sensitivity
- The assistant model's clustering quality directly impacts contrastive constraint effectiveness but is not extensively evaluated for robustness

## Confidence
- High: Improved OOD generalization over baselines on synthetic and real datasets
- Medium: Theoretical validity of PID decomposition under FIIF/PIIF assumptions
- Low: Scalability and performance guarantees beyond the tested graph sizes and domains

## Next Checks
1. Verify that for FIIF cases, the unique spurious information Uni(Y:Ŷs|Ĝc) approaches zero while Red(Y:Ĝc,Ŷs) remains substantial
2. Test alternative clustering methods (beyond k-means) and evaluate sensitivity to different assistant architectures and cluster counts
3. Apply RIG to non-bio graph datasets (e.g., social networks with known spurious correlations) to test domain generalization beyond molecular graphs