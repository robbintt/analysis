---
ver: rpa2
title: Counterfactual Explanations as Plans
arxiv_id: '2502.09205'
source_url: https://arxiv.org/abs/2502.09205
tags:
- agent
- actions
- knowledge
- such
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a formal account of counterfactual explanations
  for planning problems in the presence of both physical and sensing actions. The
  core method involves using a modal fragment of the situation calculus to formalize
  the distinction between what is true in the real world versus what is known or believed
  by an agent.
---

# Counterfactual Explanations as Plans

## Quick Facts
- **arXiv ID:** 2502.09205
- **Source URL:** https://arxiv.org/abs/2502.09205
- **Reference count:** 40
- **Primary result:** This paper proposes a formal account of counterfactual explanations for planning problems in the presence of both physical and sensing actions using a modal fragment of the situation calculus.

## Executive Summary
This paper introduces a formal framework for counterfactual explanations in planning domains that incorporate both physical actions and sensing actions. The approach uses a modal fragment of the situation calculus to distinguish between what is true in the real world versus what is known or believed by an agent. The framework addresses various settings including agents with partial knowledge, weakened truths, and false beliefs. The paper also introduces diverse counterfactual explanations that can be constrained by specific features, making the approach adaptable to different planning languages.

## Method Summary
The method employs a modal variant of the situation calculus to formalize planning problems with epistemic considerations. It defines counterfactual explanations as alternative action sequences that negate the original goal while being minimally distant from the original plan. The framework handles different knowledge states including partial knowledge, weakened truths, and false beliefs, providing mechanisms to identify missing actions or knowledge. The approach is implemented through a search process that finds minimal explanations and can generate diverse sets of explanations constrained by user-specified features.

## Key Results
- Formal definitions of counterfactual explanations for various knowledge settings (partial truths, weakened truths, false beliefs)
- Introduction of diverse counterfactual explanations constrained by specific features
- Demonstration that the approach is general and adaptable to different planning languages
- Clear distinction between physical actions and sensing actions in explanation generation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Counterfactual explanations for sequential decisions are alternative plans δ' that negate the original goal φ while remaining minimally distant from the original plan δ.
- **Mechanism:** A search process identifies an action sequence δ' such that the background theory Σ entails the negated goal ([δ']¬φ). The mechanism relies on minimizing a distance metric—either length-based or fluent-based—to ensure the explanation is actionable and relevant to the user.
- **Core assumption:** A shorter plan or one affecting fewer fluents correlates with a "better" or more interpretable explanation for the user.
- **Evidence anchors:**
  - [abstract] Mentions providing a formal account of counterfactual explanations based in terms of action sequences.
  - [section] Definition 1 (Page 160) formally defines a CF explanation as a sequence δ' where Σ ⊨ [δ']¬φ and distance is minimal.
  - [corpus] "Counterfactual Explanations for Continuous Action Reinforcement Learning" (ID 31889) supports the application of counterfactuals to sequential action domains.
- **Break condition:** If the planning domain is non-deterministic or the goal φ is a tautology under Σ, no valid counterfactual sequence δ' exists.

### Mechanism 2
- **Claim:** The paper defines a reconciliation mechanism where explanations bridge the gap between an agent's false beliefs and the real world by identifying necessary corrections to the agent's epistemic state.
- **Mechanism:** The system compares the real world theory Σ₀ against the agent's belief theory Σ'₀. If the agent holds a false belief β preventing goal achievement, the explanation identifies β for removal and a truth α for addition, such that the updated belief state entails the goal.
- **Core assumption:** The agent is capable of belief revision (specifically deletion and addition of facts) when presented with the explanation.
- **Evidence anchors:**
  - [abstract] States the need to articulate "what is true versus what is known" and correcting the agent's model.
  - [section] Definition 21 (Page 163) outlines finding α and β to update the agent's theory to entail the goal.
  - [corpus] "What Is a Counterfactual Cause in Action Theories?" (ID 11812) explores causality in action theories, relevant to the logical underpinnings of belief updates.
- **Break condition:** If the required knowledge α is not accessible in the real world theory Σ₀ (i.e., it is not a truth), reconciliation via this specific method fails.

### Mechanism 3
- **Claim:** Explanations can be generated as "diverse" sets of plans to satisfy specific user constraints (features) while still achieving the counterfactual outcome.
- **Mechanism:** Instead of a single minimal plan, the system searches for a set of plans {δ₁, ..., δₙ}. Each plan must satisfy a diversity constraint formula α (e.g., "the object must be made of glass") while toggling the outcome φ, and staying within a distance bound k.
- **Core assumption:** Users have specific features or constraints in mind that define the "context" of the explanation, which must be preserved.
- **Evidence anchors:**
  - [abstract] Mentions the concept of diverse counterfactual explanations allowing for multiple explanations constrained by certain features.
  - [section] Definition 10 (Page 161) formally defines diverse CF explanations as a set of sequences satisfying α ∧ ¬φ.
  - [corpus] "Demystifying Sequential Recommendations" (ID 49049) discusses generating counterfactuals via genetic algorithms, often used for diversity, though direct linkage to this logic is weak in the corpus.
- **Break condition:** If the diversity constraint α is logically incompatible with achieving ¬φ under the dynamics Σ_dyn, the set of explanations will be empty.

## Foundational Learning

- **Concept:** **Situation Calculus & Modal Logic (ES)**
  - **Why needed here:** The paper uses a modal variant of the situation calculus to represent time, action, and knowledge. You must understand how fluents change with actions ([a]α) and how knowledge is represented (Kα) to read the formal definitions.
  - **Quick check question:** If Kφ holds in a situation, does φ necessarily hold in the real world? (Check: No, not unless the real world is in the epistemic state, see Page 156/162).

- **Concept:** **Basic Action Theories (BATs)**
  - **Why needed here:** The architecture relies on distinct components for preconditions (Poss), effects (successor state axioms), and sensing (SF). Understanding how these axioms define the "rules of the game" is critical for implementation.
  - **Quick check question:** Which component of a BAT determines if an action can be legally executed in the current state?

- **Concept:** **Epistemic States vs. Real World**
  - **Why needed here:** The core value proposition is reconciling the difference between what is true (Σ₀) and what the agent believes (Σ'₀).
  - **Quick check question:** In the "False Beliefs" setting (Section 5.3), what is the relationship between Σ₀ and Σ'₀? (Check: Σ₀ ⊭ Σ'₀).

## Architecture Onboarding

- **Component map:**
  - Domain Axiomatizer -> State Manager -> Projection Engine -> Explanation Generator

- **Critical path:**
  1. User submits a goal φ and the system executes a plan δ.
  2. System determines Σ ⊭ [δ]Kφ (Plan fails epistemically).
  3. System initiates Explanation Generation: searches for minimal δ' and potential knowledge updates.
  4. System outputs δ' (Action explanation) or α, β (Knowledge explanation).

- **Design tradeoffs:**
  - **Minimality Metric:** Choosing between length-based (simpler plans) vs. fluent-based (fewer side effects) minimality (Def 2 vs Def 4). Fluent-based is often more intuitive but computationally harder.
  - **Sensing vs. Belief Update:** Deciding if an explanation should be a sensing action (Example 15, p.162) or a knowledge update (Definition 16, p.162). Sensing is "doing"; updating is "telling."

- **Failure signatures:**
  - **Inconsistency:** Σ₀ and Σ'₀ are inconsistent, leading to validity collapse.
  - **Inexecutable Explanations:** Generated plan δ' is valid logically but fails Exec(δ') in the real world (Definition 13 addresses this).
  - **Ignorance:** Agent knows ¬Kφ but cannot find a sequence to toggle this (Explanation generation returns ∅).

- **First 3 experiments:**
  1. **Objective Counterfactuals:** Implement the Blocks World example (Page 157) and verify that δ = pickup(c) · drop(c) yields δ' = δ · repair(c) as the CF explanation for Broken(c).
  2. **Epistemic Explanation:** Create a scenario where the agent misses a rigid property (e.g., Material). Verify the system suggests a sensing action isGlass(d) rather than just assuming the fact.
  3. **False Belief Reconciliation:** Test Section 5.3. Give the agent a false belief (¬Metal(h)). Check if the system correctly identifies β (¬Metal(h)) as the false belief to remove.

## Open Questions the Paper Calls Out
None

## Limitations
- The formalism relies heavily on the situation calculus and modal logic framework, which may limit accessibility for practitioners unfamiliar with these formalisms.
- The assumption that users can interpret counterfactual plans meaningfully is not empirically validated.
- The computational complexity of generating diverse counterfactual explanations under constraints is not addressed, potentially limiting scalability to real-world domains.

## Confidence
- **Mechanism 1 (Plan-based counterfactuals):** High confidence
- **Mechanism 2 (Belief reconciliation):** Medium confidence
- **Mechanism 3 (Diverse explanations):** Low confidence

## Next Checks
1. **Empirical validation:** Implement the Blocks World example (Page 157) and test with human subjects to verify that the generated counterfactual explanations are interpretable and actionable.
2. **Computational complexity analysis:** Measure the runtime performance of generating diverse counterfactual explanations under various constraint settings to identify scalability bottlenecks.
3. **Cross-domain applicability:** Apply the framework to a non-classical planning domain (e.g., robotics with continuous actions) to test the generality of the approach.