---
ver: rpa2
title: 'DEEPQUESTION: Systematic Generation of Real-World Challenges for Evaluating
  LLMs Performance'
arxiv_id: '2505.24532'
source_url: https://arxiv.org/abs/2505.24532
tags:
- question
- questions
- prompt
- reasoning
- original
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces DeepQuestion, a scalable automated framework\
  \ that generates cognitively diverse benchmarks for evaluating large language models\
  \ (LLMs). DeepQuestion leverages Bloom\u2019s taxonomy to create two types of deeper\
  \ questions: scenario-based questions (Q2S) that embed problems in realistic narratives,\
  \ and instruction-based question generation (Q2I) that tests higher-order skills."
---

# DEEPQUESTION: Systematic Generation of Real-World Challenges for Evaluating LLMs Performance

## Quick Facts
- arXiv ID: 2505.24532
- Source URL: https://arxiv.org/abs/2505.24532
- Authors: Ali Khoramfar; Ali Ramezani; Mohammad Mahdi Mohajeri; Mohammad Javad Dousti; Majid Nili Ahmadabadi; Heshaam Faili
- Reference count: 29
- Primary result: LLM performance degrades up to 70% on higher-order reasoning tasks generated via Bloom's taxonomy framework

## Executive Summary
This paper introduces DeepQuestion, a scalable automated framework that generates cognitively diverse benchmarks for evaluating large language models (LLMs). DeepQuestion leverages Bloom's taxonomy to create two types of deeper questions: scenario-based questions (Q2S) that embed problems in realistic narratives, and instruction-based question generation (Q2I) that tests higher-order skills. The framework was applied to GSM8K and physics exam questions, producing datasets where models must solve contextualized problems or generate questions meeting specific instructions. Evaluation across ten models—including reasoning-focused and general-purpose LLMs—revealed substantial performance drops, up to 70% accuracy loss, on higher-order tasks despite strong performance on original benchmarks. These findings highlight persistent reasoning gaps in LLMs and demonstrate the need for cognitively rich benchmarks to advance model evaluation. The DeepQuestion framework and datasets will be released to support further research.

## Method Summary
DeepQuestion uses an iterative dual-LLM prompt generation pipeline to transform existing benchmarks into cognitively deeper variants. The framework applies two methods: Q2S adds realistic scenarios and distractors to preserve original reasoning while testing "apply" level cognition, and Q2I generates questions from instructions to test "evaluate" and "create" levels. The pipeline samples random question batches, generates transformation prompts via Gemini 2.5 Pro, iteratively refines prompts using evaluator feedback (threshold ≥8/10), then applies prompts to generate questions. Q2S evaluation measures solution accuracy; Q2I uses two-step validation (answerability check + LLM-as-Judge comparison on 5 criteria).

## Key Results
- Models showed 70% accuracy loss on Q2I tasks despite >95% performance on original questions
- Q2S caused moderate drops (e.g., Physics: 77%→72% for Gemini-2.0-flash; GSM8K: 88%→83% for Deepseek-R1)
- Generated physics questions lagged originals on physical realism (63% vs 18% win rate) and reasoning demand
- Reasoning models showed 8-30% Q2I drops, while general-purpose models dropped 40-70%

## Why This Works (Mechanism)

### Mechanism 1: Cognitive Hierarchy Degradation
LLM performance degrades progressively as task requirements ascend Bloom's taxonomy levels, with steepest drops at highest levels (evaluate, create). Standard benchmarks test "remember" and "understand" levels through structured questions with clear signal. When tasks require "apply" (Q2S) or "evaluate/create" (Q2I), models must perform multi-step reasoning: identifying relevant concepts, filtering distractors, and synthesizing novel outputs. This exposes shallow pattern matching versus genuine conceptual integration.

### Mechanism 2: Narrative Context Interference
Embedding problems within realistic scenarios with irrelevant details causes performance decline even when core reasoning requirements remain unchanged. Q2S adds "extraneous details and some distractions to simulate real-world complexity." Models must now perform attention filtering—identifying which narrative elements map to problem variables. This tests "apply" level cognition: using knowledge in practical, noisy contexts rather than idealized formulations.

### Mechanism 3: Generative-Derivative Asymmetry
The capacity to solve problems does not imply the capacity to generate valid, instruction-following questions preserving solution paths. Q2I requires three implicit reasoning steps: (1) conceptual understanding of domain/equations, (2) defining appropriate variables and interrelations, (3) selecting values yielding target solutions. This reverses typical inference direction—instead of problem→solution, models must work solution→problem while satisfying instruction constraints.

## Foundational Learning

- **Bloom's Taxonomy (6 cognitive levels)**
  - Why needed here: The entire framework operationalizes this educational theory; understanding the hierarchy (remember→understand→apply→analyze→evaluate→create) is prerequisite to interpreting why Q2S targets level 3 and Q2I targets levels 5-6.
  - Quick check question: Can you explain why "generate a question with solution x=3t-4" is harder than "solve for x given x=3t-4"?

- **LLM-as-Judge Evaluation**
  - Why needed here: Q2I evaluation uses O4-mini as judge for qualitative criteria (reasoning demand, clarity, solution spoiling). Understanding judge biases and comparison methodology is essential for interpreting results.
  - Quick check question: Why might a judge model prefer original questions over generated ones even if generated questions are valid?

- **Iterative Prompt Optimization**
  - Why needed here: The prompt generation pipeline uses generator-evaluator feedback loops. Understanding this dual-LLM pattern helps diagnose where pipeline failures originate.
  - Quick check question: If evaluator LLM assigns score 6/10, what happens next in the pipeline?

## Architecture Onboarding

- **Component map:**
  Source Benchmark (GSM8K/Physics) → [Sample Random Batch] → Prompt Generator LLM (Gemini 2.5 Pro) → Draft Prompt → Prompt Evaluator LLM (Gemini 2.5 Pro) → Score ≥ 8? → Question Generator LLM (with accepted prompt) → Q2S: Scenario Questions / Q2I: Instructions → Target Model Solves / Target Model Generates Q → Verify Answer / Answerability Check + LLM-as-Judge

- **Critical path:** Prompt Generator → Prompt Evaluator loop (determines transformation quality); this is where domain adaptation happens. If prompts are weak, all downstream questions inherit flaws.

- **Design tradeoffs:**
  - Same LLM for all roles (Gemini 2.5 Pro): Ensures consistency but introduces systematic biases. Alternative: different models per role reduces correlation but increases complexity.
  - Score threshold = 8/10: Higher thresholds improve quality but increase iterations; lower thresholds speed generation but risk poor prompts.
  - Q2I answerability check with O4-mini: Uses strong model for validation but creates dependency; if O4-mini can't solve valid questions, false negatives increase.

- **Failure signatures:**
  - Prompt loop never converges: Evaluator criteria may be inconsistent with generator capabilities; check evaluation_criteria in Algorithm 1.
  - Q2S questions solvable but trivial: Scenario generation prompt fails to embed core problem properly.
  - Q2I questions violate instructions: Prompt doesn't enforce constraints (e.g., "final answer must match original"); check instruction-generation prompt in Appendix B.
  - Quality judge prefers originals universally: Judge criteria may favor familiar patterns; review comparison criteria in Appendix B.

- **First 3 experiments:**
  1. Reproduce on held-out GSM8K subset (n=60): Run full Q2S and Q2I pipeline with temperature=0 on 10 models. Verify reported accuracy ranges (Q2S: moderate drops; Q2I: 8-70% drops).
  2. Ablate prompt optimization: Compare iterative generator-evaluator loop (score≥8) against single-pass prompt generation. Measure question quality difference to quantify optimization contribution.
  3. Cross-domain transfer test: Apply DeepQuestion pipeline to a new domain (e.g., chemistry, using corpus-neighbor "Beyond Chemical QA" as reference). Assess whether prompt generator adapts without manual tuning and whether similar taxonomy-based degradation patterns emerge.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the DeepQuestion framework be effectively extended to non-STEM domains such as law or medicine to evaluate higher-order reasoning?
- Basis in paper: [explicit] The authors state in the Limitations section that while the study focused on mathematics and physics, "our framework is not specifically tailored to these benchmarks and can be extended to other domains as well."
- Why unresolved: The current study restricted evaluation to math and physics benchmarks to facilitate expert human verification of specific correct answers, leaving the framework's efficacy in other fields untested.
- What evidence would resolve it: Successful application of DeepQuestion to datasets like MMLU (law/ethics) or MedQA, demonstrating similar performance gaps between standard and "deep" variants.

### Open Question 2
- Question: How can the DeepQuestion framework be adapted to evaluate open-ended questions that lack definite correct answers?
- Basis in paper: [explicit] The authors note in the Limitations that "our proposed framework is capable of handling open-ended questions," but they explicitly "restrict our focus to questions with definite correct answers" in this work.
- Why unresolved: The current evaluation methodology relies on verifiable ground-truth answers (e.g., numerical solutions), which creates a methodological gap for assessing open-ended creative or essayistic tasks.
- What evidence would resolve it: A modified evaluation protocol for DeepQuestion using LLM-as-a-judge or human annotation to score open-ended responses on Bloom's higher levels (Evaluate/Create).

### Open Question 3
- Question: Can LLMs be improved to generate instruction-based questions (Q2I) that match the physical realism and reasoning demand of human-authored problems?
- Basis in paper: [inferred] Results in Figure 3 and Section 4.3 show that generated physics questions significantly lag behind originals in "Physical Realism" (originals win 63% vs 18%) and reasoning demand.
- Why unresolved: The paper demonstrates that while models can follow instructions to generate questions, they struggle with the deep conceptualization required to ensure physical plausibility and rigorous reasoning paths.
- What evidence would resolve it: Experiments where state-of-the-art models achieve a win rate of >45% against original human questions on the "Physical Realism" and "Reasoning Demand" criteria defined in Appendix B.

## Limitations
- Dataset access and reproducibility: The physics benchmark from the Iranian University Entrance Exam is not publicly available, creating a fundamental reproducibility barrier.
- Prompt template completeness: Only physics prompt templates are provided in Appendix B (in Persian). GSM8K prompt templates are referenced but not included.
- Evaluation consistency assumptions: The dual-LLM pipeline assumes evaluator reliability and consistency, creating potential circularity in prompt generation.

## Confidence
- **High confidence:** The taxonomy-based mechanism linking Bloom's levels to LLM performance degradation is well-supported by quantitative results showing consistent accuracy drops across multiple models and domains.
- **Medium confidence:** The narrative context interference mechanism is moderately supported by moderate Q2S performance drops and GSM-Symbolic citations.
- **Low confidence:** The generative-derivative asymmetry mechanism relies heavily on qualitative Q2I evaluation results and lacks direct corpus evidence.

## Next Checks
1. **Cross-domain replication test:** Apply DeepQuestion pipeline to a publicly available dataset (e.g., chemistry questions from Beyond Chemical QA) to verify taxonomy-based degradation patterns emerge without manual prompt tuning.
2. **Ablation of prompt optimization loop:** Compare iterative generator-evaluator loop (score≥8) against single-pass prompt generation on the same random batch. Measure question quality differences and downstream model performance.
3. **Controlled narrative interference test:** Create Q2S variants with systematic distractor variations (zero, minimal, maximal) while keeping core problems identical. Measure performance degradation as a function of narrative complexity.