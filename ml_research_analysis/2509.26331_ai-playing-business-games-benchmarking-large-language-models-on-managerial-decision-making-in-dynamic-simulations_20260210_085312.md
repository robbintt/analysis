---
ver: rpa2
title: 'AI Playing Business Games: Benchmarking Large Language Models on Managerial
  Decision-Making in Dynamic Simulations'
arxiv_id: '2509.26331'
source_url: https://arxiv.org/abs/2509.26331
tags:
- llms
- management
- business
- decisions
- decision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks large language models (LLMs) on long-term
  strategic decision-making using a dynamic 12-month retail business simulation. Five
  leading LLMs (Gemini, ChatGPT, Meta AI, Mistral AI, and Grok) were evaluated in
  identical conditions using a reproducible Excel-based simulator.
---

# AI Playing Business Games: Benchmarking Large Language Models on Managerial Decision-Making in Dynamic Simulations

## Quick Facts
- arXiv ID: 2509.26331
- Source URL: https://arxiv.org/abs/2509.26331
- Authors: Berdymyrat Ovezmyradov
- Reference count: 40
- Primary result: Large language models tested in 12-month retail business simulation; Gemini outperformed others in profitability and revenue, but all struggled with long-term strategic coherence

## Executive Summary
This study evaluates five leading large language models (Gemini, ChatGPT, Meta AI, Mistral AI, and Grok) on their ability to make strategic business decisions in a dynamic retail simulation. Using a reproducible Excel-based framework, the models made monthly decisions on pricing, inventory, marketing, staffing, and forecasting over a 12-month period. The research reveals significant performance variation, with Gemini models showing superior profitability and revenue generation, while all models struggled with long-term coherence, typically collapsing after 3-4 periods. The study introduces a novel open-access benchmark for assessing AI in complex, multi-step business scenarios.

## Method Summary
The research employed a dynamic 12-month retail business simulation where five leading LLMs acted as autonomous "virtual CEOs." Each month, models received structured prompts containing complete business reports from the previous period and generated decisions on ten key variables: pricing, orders, marketing spend, staffing, loans, training, R&D, and sales/profit forecasts. An Excel-based simulator computed the outcomes, which then fed back into the next month's decision-making process. Performance was measured through profit, revenue, market share, and strategic coherence metrics, with all models operating under identical conditions to ensure comparability.

## Key Results
- Gemini models significantly outperformed others, achieving higher profitability and revenue while maintaining better strategic coherence
- All LLMs demonstrated strong early performance but typically collapsed after 3-4 periods, with most showing zero revenue recovery
- Forecast accuracy was poor across all models, with sales forecast errors ranging from 82-150% and profit forecast errors from 108-411%
- Models showed limited adaptability in the second half of the simulation, often reacting belatedly to market changes with counterproductive measures

## Why This Works (Mechanism)

### Mechanism 1: Sequential Context Integration
Performance depends on how well an LLM integrates cumulative feedback across decision periods. Each month, the LLM receives a structured business report and must update its mental model of the business state before making next decisions. Better-performing models maintain coherent state representations across 12 sequential prompts.

### Mechanism 2: Multi-Variable Tradeoff Reasoning
Effective managerial decisions require simultaneous optimization across interdependent variables (pricing, inventory, marketing, staffing). The simulation includes dynamic feedback loops—e.g., pricing affects demand, which affects inventory, which affects cash, which constrains future orders. Models must reason about these interactions rather than optimizing each variable independently.

### Mechanism 3: Forecast Calibration
Performance correlates with forecasting accuracy and appropriate uncertainty acknowledgment. The simulation requires explicit sales and income forecasts. Models must assess their own uncertainty and adjust strategies accordingly. Poor calibration leads to inventory mismatches and cash flow crises.

## Foundational Learning

- **Closed-Loop Decision Dynamics**: The simulation is a closed-loop system where today's decisions affect tomorrow's state, which then affects subsequent decisions. *Why needed here*: LLMs often treat each prompt as independent rather than connected; understanding feedback dynamics is essential for multi-period coherence. *Quick check*: Can you explain why a price increase in month 3 might cause a cash shortage in month 5?

- **Capacity Constraints and Lead Times**: The simulation includes 2-month supplier lead times and capacity constraints. *Why needed here*: Decisions must anticipate future demand with lag; ordering too little creates stockouts, ordering too much ties up cash. *Quick check*: If you need 5,000 units in month 6 and lead time is 2 months, when must you place the order?

- **Financial Statement Linkages**: Income statements, balance sheets, and cash flow statements are interconnected. *Why needed here*: Profitable companies can still fail from cash shortages; models must track liquidity alongside profitability. *Quick check*: Why might a company show positive net income but negative cash flow?

## Architecture Onboarding

- **Component map**: Input prompt (role definition, market conditions, financial data) -> LLM decision generation -> Excel simulation computation -> Results feedback -> Next month prompt

- **Critical path**: Month 1 prompt → LLM decision → Excel calculation → results → Results + Month 2 prompt → repeat for 12 months → Aggregate metrics (profit/revenue ratio, market share, strategic coherence)

- **Design tradeoffs**: Single-turn prompting (each month independent) vs. maintaining conversation context (cumulative token cost); structured output format (easier parsing) vs. free-form reasoning (richer justifications); model selection balancing performance vs. availability

- **Failure signatures**: Collapse pattern (revenue drops to zero around month 4-5 and never recovers); volatility pattern (large month-to-month swings without strategic rationale); neglect pattern (ignoring available decision levers like loans, R&D, training)

- **First 3 experiments**: 1) Baseline replication: Run Gemini Pro and ChatGPT through full 12-month simulation; compare to paper's Figure 1. 2) Prompt variation test: Add explicit "remember you are in month X of 12" framing to test temporal awareness impact. 3) Recovery analysis: After observing collapse, test whether providing simplified summary ("Your cash is critically low; prioritize liquidity") enables recovery

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the effectiveness of LLMs improve when they act as strategic advisors to human managers rather than as autonomous decision-making agents?
- Basis in paper: "Future studies could also explore the effectiveness of an LLM as a strategic advisor to a human manager."
- Why unresolved: This study only evaluated LLMs acting as "virtual CEOs" with full autonomy, finding that most failed to maintain long-term profitability.
- What evidence would resolve it: A comparative experiment measuring business outcomes in scenarios where LLMs provide recommendations which humans accept, modify, or reject versus the fully autonomous setup described.

### Open Question 2
- Question: How does LLM performance vary when introducing specific external disruptions, such as supply chain shocks or sudden HR constraints, into the simulation?
- Basis in paper: "Further research could expand this framework to include more complex simulations with repeated sessions, various scenarios regarding human resources, investment, and disruptions."
- Why unresolved: The current benchmark utilized a standard 12-month simulation, but did not test the models' adaptive capacity against specific, unpredictable market disruptions.
- What evidence would resolve it: Benchmarking the LLMs using the provided open-access simulator modified to include stochastic variables for supply chain failures or labor shortages.

### Open Question 3
- Question: Can the observed "collapse" in decision-making coherence after 3-4 periods be mitigated by altering the prompt structure or context window management?
- Basis in paper: The paper notes that "all LLMs struggled with long-term profitability, with most collapsing after 3-4 periods," but the methodology relied on a consistent prompt structure without testing interventions for memory or coherence.
- Why unresolved: It is unclear if the failure was due to fundamental model limitations or the specific method of presenting historical data in the prompt.
- What evidence would resolve it: Ablation studies testing different context management strategies (e.g., summarization vs. full history) to see if the "point of no return" can be delayed or avoided.

### Open Question 4
- Question: Are the performance differences between Gemini and other models (like ChatGPT and Grok) statistically significant across multiple simulation runs?
- Basis in paper: "This exploratory research used a limited set of experiments... Future work to extend this research should involve more empirical support."
- Why unresolved: The study presents descriptive statistics for a single run (or limited runs) per model, noting "significant performance variation," but lacks the statistical power to confirm if Gemini's superiority is robust.
- What evidence would resolve it: Statistical analysis (e.g., ANOVA) of profit/revenue metrics across a large sample of repeated simulation sessions for each model.

## Limitations
- The closed Excel-based simulation may not reflect real-world business complexity where models could leverage external data sources
- The fixed prompt structure, while ensuring comparability, may not be optimal for any particular model's reasoning style
- The 12-month horizon may be insufficient to fully assess long-term strategic planning capabilities
- Performance assessment relies heavily on financial metrics that may not capture all aspects of managerial competence

## Confidence
**High Confidence**: The comparative performance rankings (Gemini > ChatGPT > others) and the general pattern of early success followed by collapse are well-supported by the data and consistent across multiple metrics.

**Medium Confidence**: The attribution of performance differences to specific mechanisms (sequential context integration, multi-variable tradeoff reasoning) is plausible but not definitively proven.

**Low Confidence**: The interpretation that LLMs "neglect" certain decisions assumes these would improve outcomes, but the simulation structure may make some options suboptimal or the models may be making rational decisions given their limited information.

## Next Checks
1. **Sensitivity Analysis**: Systematically vary the prompt structure (adding explicit temporal markers, decision summaries, or strategic goals) across all models to determine whether performance differences are primarily due to model capability or prompt effectiveness.

2. **Hybrid Approach Testing**: Allow successful models (particularly Gemini) to use external tools (calculator, web search, or a simplified business knowledge base) during decision-making to assess whether tool use bridges the performance gap with human managers.

3. **Decision Attribution Study**: Conduct a fine-grained analysis of which specific decision types (pricing, inventory, marketing spend) most strongly predict success or failure, and whether models show consistent patterns of error across different economic conditions.