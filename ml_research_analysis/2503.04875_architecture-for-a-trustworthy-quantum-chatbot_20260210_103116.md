---
ver: rpa2
title: Architecture for a Trustworthy Quantum Chatbot
arxiv_id: '2503.04875'
source_url: https://arxiv.org/abs/2503.04875
tags:
- quantum
- software
- code
- qiskit
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: C4Q 2.0 is a chatbot designed to provide trustworthy and accurate
  support for quantum computing. It addresses the challenge of unreliable responses
  from generic chatbots in specialized fields by integrating specialized LLMs for
  classification and question answering with a deterministic logical engine for quantum
  computations.
---

# Architecture for a Trustworthy Quantum Chatbot

## Quick Facts
- arXiv ID: 2503.04875
- Source URL: https://arxiv.org/abs/2503.04875
- Reference count: 40
- C4Q 2.0 achieves 97.92% accuracy on Qiskit 1.3.1 while maintaining all unit tests passing

## Executive Summary
C4Q 2.0 is a modular quantum computing chatbot that addresses reliability issues in generic LLMs by separating probabilistic intent classification and parameter extraction from deterministic quantum computation. The architecture uses fine-tuned BERT models to classify user queries into five categories and extract specific parameters, which are then used to populate pre-verified Qiskit code templates. This hybrid neuro-symbolic approach ensures mathematical correctness while maintaining adaptability to library updates. The system demonstrates near-perfect accuracy in classification and high backend reliability through comprehensive testing.

## Method Summary
The architecture implements a hybrid approach where a fine-tuned BERT classifier routes queries to one of five intent categories, and a separate BERT-based QA model extracts specific parameters from the query text. These parameters populate pre-verified Qiskit code templates rather than generating code probabilistically. The system supports gate operations, TSP (via VQE), and KP (via QAOA) through this template-based mechanism. Training involves fine-tuning BERT models on generated context-question-answer triplets, with the logical engine handling deterministic quantum computations. The entire system was developed and tested on a MacBook Pro M2 with 32GB RAM, requiring approximately 9 hours per model.

## Key Results
- 100% classification accuracy across five intent categories
- QA F1 score of 88.3% and exact match of ~87.5%
- 97.92% correct answers on Qiskit 1.3.1 compared to <25% for generic chatbots
- All 189 pytest unit tests passing for backend validation

## Why This Works (Mechanism)

### Mechanism 1: Hybrid Neuro-Symbolic Computation
- Separating probabilistic NLP from deterministic quantum logic reduces hallucination rates
- Queries routed through fine-tuned BERT classifier to determine intent, then parameters extracted via QA model
- Actual quantum computation offloaded to deterministic Qiskit engine rather than LLM generation
- Assumes classification and QA models achieve higher reliability than generative models for intent recognition and parameter extraction

### Mechanism 2: Template-Based Code Generation
- Static templates populated by LLM-extracted parameters maintain higher correctness across library versions
- Uses pre-verified Qiskit code snippets instead of token-by-token generation to reduce syntax errors
- Adapts to library shifts by updating templates rather than retraining entire model
- Assumes problem space is sufficiently constrained to map to finite code templates

### Mechanism 3: User-in-the-Loop Verification
- Mandatory verification step before execution mitigates residual error rate of QA extraction layer
- Visualizes extracted parameters for user confirmation as "human firewall" against ~11.7% extraction error rate
- Assumes users can accurately interpret visualized parameters to catch extraction errors
- Risk of alert fatigue if users blindly confirm without verification

## Foundational Learning

- **QUBO (Quadratic Unconstrained Binary Optimization)**
  - Why needed here: Solves classical problems (TSP, Knapsack) by converting to QUBO format for quantum optimizers like QAOA/VQE
  - Quick check question: Can you formulate a simple Knapsack Problem constraints into a quadratic penalty term?

- **BERT for Extractive QA vs. Generative LLMs**
  - Why needed here: C4Q uses BERT for extracting specific values vs. generative models for free-text
  - Quick check question: What is the difference between a model predicting a span of text (start/end tokens) vs. generating a novel sequence?

- **Qiskit Fundamentals (Gates & Backends)**
  - Why needed here: Deterministic engine relies on Qiskit for circuit construction
  - Quick check question: How do you instantiate a quantum circuit with 2 qubits and apply a Hadamard gate to the first qubit in Qiskit?

## Architecture Onboarding

- Component map:
  Frontend (React) -> API (Django) -> Classification LLM (BERT) -> QA LLM (BERT) -> Logical Engine (Python/Qiskit) -> Database (PostgreSQL)

- Critical path:
  1. User Query received by API
  2. Classification LLM categorizes intent
  3. QA LLM extracts parameters (e.g., rotation angle, city list)
  4. Frontend presents parameters for User Confirmation
  5. Logical Engine fills templates and executes Qiskit code
  6. Result (Diagram + Code Snippet) returned to User

- Design tradeoffs:
  - Accuracy vs. Flexibility: Template approach guarantees accuracy for supported features but lacks flexibility for ad-hoc algorithms
  - Latency vs. Safety: Verification step adds interaction overhead but necessary to catch ~12-30% extraction error rate

- Failure signatures:
  - Misclassification: Query for "TSP" classified as "Gate Application" -> Logical engine receives wrong parameters
  - Extraction Hallucination: QA model extracts non-existent phase value -> Valid but unintended computation
  - Template Drift: Qiskit API updates -> Generated code produces runtime errors

- First 3 experiments:
  1. Parameter Extraction Stress Test: Input queries with ambiguous numeric values to verify QA LLM robustness against phase shift confusion
  2. Version Compatibility Check: Run generated code on both Qiskit 0.46 and 1.3.1 to verify maintainability claims
  3. Intent Boundary Testing: Submit blended queries to evaluate monolithic Classification LLM handling of mixed-intent queries

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can architectural or training improvements reduce QA LLM error rates for complex parameter extraction tasks?
- Basis: Authors note aim to improve QA LLM performance, particularly for phase shift parameters with up to 30% failure rates
- Why unresolved: Current fine-tuned BERT model frequently confuses numeric values, no specific solution proposed
- Evidence needed: Modified C4Q demonstrating statistically significant reduction in extraction errors for phase shift and item weight parameters

### Open Question 2
- Question: Does C4Q 2.0 correctness translate into measurable pedagogical benefits for quantum computing students?
- Basis: Conclusion notes plans to collaborate with educators for user studies
- Why unresolved: Technical accuracy proven but utility in actual educational curriculum not validated
- Evidence needed: Controlled user studies showing students using C4Q achieve better learning outcomes than those using generic LLMs

### Open Question 3
- Question: How does template-based code generation maintenance effort scale with system expansion?
- Basis: Paper highlights manual curation effort for adding features is better than retraining LLMs
- Why unresolved: Unclear if manual curation remains efficient as supported algorithms grow beyond TSP and KP
- Evidence needed: Longitudinal analysis quantifying developer hours for template updates vs. model retraining as quantum libraries expand

## Limitations
- Generalization beyond 5 predefined intent categories remains unverified
- Template-based approach cannot handle novel quantum algorithms without manual template updates
- 100% classification accuracy likely reflects limited test set rather than real-world variability

## Confidence
- **High**: Deterministic backend correctness (189/189 tests passing), template-based code generation mechanism
- **Medium**: Classification accuracy claims (test set specifics not disclosed), comparative performance against generic chatbots
- **Low**: Real-world error rates with diverse user queries, scalability to larger quantum circuits

## Next Checks
1. Test system with adversarial queries mixing multiple intent categories to evaluate classification robustness
2. Measure end-to-end latency including user verification steps with complex TSP instances
3. Verify template maintenance burden by upgrading to Qiskit 2.x and documenting required changes