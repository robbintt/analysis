---
ver: rpa2
title: A Novel Multimodal RUL Framework for Remaining Useful Life Estimation with
  Layer-wise Explanations
arxiv_id: '2512.06708'
source_url: https://arxiv.org/abs/2512.06708
tags:
- data
- bearing
- degradation
- relevance
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of accurately estimating Remaining
  Useful Life (RUL) for rolling-element bearings, a critical aspect of predictive
  maintenance. Existing methods often lack robustness, generalization, and interpretability.
---

# A Novel Multimodal RUL Framework for Remaining Useful Life Estimation with Layer-wise Explanations

## Quick Facts
- **arXiv ID**: 2512.06708
- **Source URL**: https://arxiv.org/abs/2512.06708
- **Reference count**: 29
- **Primary result**: Multimodal framework (ImR + TFR) achieves competitive RUL estimation using 28-48% less training data than baselines while providing interpretable explanations via Layer-wise Relevance Propagation.

## Executive Summary
This paper addresses the challenge of accurately estimating Remaining Useful Life (RUL) for rolling-element bearings, a critical aspect of predictive maintenance. Existing methods often lack robustness, generalization, and interpretability. The authors propose a novel multimodal framework that integrates image representations (ImR) and time-frequency representations (TFR) of vibration signals. The method employs three branches: image and TFR branches for spatial feature extraction using dilated convolutional blocks, and a fusion branch with LSTM and multi-head attention for temporal modeling. A comprehensive feature engineering framework converts vibration signals into ImR via the Bresenham algorithm and TFR via Continuous Wavelet Transform. The approach is validated on XJTU-SY and PRONOSTIA datasets, demonstrating competitive or superior performance compared to baselines while requiring 28% and 48% less training data, respectively. The model shows strong noise resilience and interpretability through multimodal Layer-wise Relevance Propagation (LRP), making it suitable for real-world industrial applications.

## Method Summary
The proposed framework integrates image representations (ImR) and time-frequency representations (TFR) of vibration signals through a three-branch architecture. Horizontal vibration signals are rasterized into 2D images via Bresenham algorithm, while vertical signals are transformed via Continuous Wavelet Transform (CWT) to capture time-frequency energy distributions. Both branches employ dilated convolutional blocks with residual connections to extract spatial degradation features at multiple scales. The fusion branch concatenates features from both branches, passes them through 3-layer LSTM (100→64→64 units) to capture temporal dependencies, applies 8-head attention to emphasize salient features, and outputs RUL through linear layers. The framework is trained using MAE/MSE loss with early stopping and learning rate reduction callbacks.

## Key Results
- Competitive or superior RUL estimation performance on XJTU-SY and PRONOSTIA datasets
- Requires 28% and 48% less training data compared to baseline methods
- Demonstrates strong noise resilience across multiple noise types
- Provides interpretable explanations through multimodal Layer-wise Relevance Propagation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal signal representation (ImR + TFR) provides complementary degradation information that single-modality approaches miss.
- Mechanism: Horizontal vibration signals are rasterized into 2D images via Bresenham algorithm, preserving signal trajectory geometry, while vertical signals are transformed via CWT to capture time-frequency energy distributions. The two representations encode different aspects of bearing degradation—spatial waveform morphology versus frequency-domain energy evolution.
- Core assumption: Bearing degradation manifests differently across signal axes and representation domains; neither alone captures the full degradation signature.
- Evidence anchors:
  - [abstract]: "integrates image representations (ImR) and time-frequency representations (TFR) of vibration signals"
  - [Section 3.1]: Horizontal axis → ImR (Algorithm 1), Vertical axis → TFR (Algorithm 2)
  - [corpus]: Weak direct support; neighbor papers focus on single-modality temporal models (e.g., Bi-LSTM, transformers) without explicit multimodal fusion.
- Break condition: If horizontal and vertical signals are highly correlated (redundant), or if operating conditions cause one modality to dominate with noise in the other, fusion may introduce confusion rather than complementary signal.

### Mechanism 2
- Claim: Dilated convolutions with hierarchical kernel/dilation configurations capture multi-scale degradation patterns without sacrificing receptive field coverage.
- Mechanism: Block 1 uses small kernels (5×5, 3×3) with high dilation (4×4, 3×3) for fine local features; Block 2 uses smaller kernels (3×3, 2×2) with low dilation (2×2, 1×1) for global patterns. Residual connections preserve gradient flow and identity mapping.
- Core assumption: Bearing degradation exhibits both subtle early-stage local cues and large-scale trend patterns that require different receptive field configurations.
- Evidence anchors:
  - [abstract]: "dilated convolutional blocks with residual connections to extract spatial degradation features"
  - [Table 1]: Explicit hyperparameters showing hierarchical dilation rates [4,3,2,1] with kernel sizes decreasing [5→2]
  - [corpus]: No direct comparison; neighbor papers use standard CNNs or attention without explicit dilation hierarchies.
- Break condition: If degradation patterns are primarily high-frequency with minimal multi-scale structure, dilation may introduce irrelevant context; ablation shows MMv1 (no dilation) degrades particularly on unseen conditions.

### Mechanism 3
- Claim: LSTM + Multi-head Attention in the fusion branch filters temporal redundancy and emphasizes informative time steps for RUL regression.
- Mechanism: Concatenated features from both branches feed into 3-layer LSTM (100→64→64 units) capturing long-term temporal dependencies; 8-head attention (key dim=64) then re-weights features to suppress noise and highlight salient degradation transitions before final linear layers.
- Core assumption: RUL prediction depends on identifying critical degradation state changes rather than treating all time steps equally.
- Evidence anchors:
  - [abstract]: "fusion branch with LSTM and multi-head attention for temporal modeling"
  - [Section 3.2.3]: "MHA mechanism subsequently emphasizes salient features"
  - [corpus]: Neighbor papers (e.g., temporal transformer models, Bi-LSTM) validate temporal attention for RUL, supporting this design pattern.
- Break condition: If degradation is smooth and monotonic without abrupt state changes, attention may overfit to noise; ablation shows MMv3 (no attention) has inconsistent performance across conditions.

## Foundational Learning

- **Concept**: Continuous Wavelet Transform (CWT) with Morlet wavelet
  - Why needed here: Converts 1D vibration signals into 2D time-frequency representations; central frequency fc=0.81 balances time/frequency resolution for bearing impulse detection.
  - Quick check question: Given sampling rate 25.6 kHz and operating frequency 35 Hz, can you compute the valid scale range [amin, amax] using Equation 12?

- **Concept**: Layer-wise Relevance Propagation (LRP) rules (LRP-0, LPR-ε, LRP-γ)
  - Why needed here: Backward propagation of prediction relevance through multimodal network requires different rules per layer type (conv: LRP-γ, dense: LRP-ε, pooling: LRP-0).
  - Quick check question: For a MaxPooling layer where 3 inputs share the same maximum value, how should relevance R_j be distributed per Equation 27?

- **Concept**: Bresenham line rasterization
  - Why needed here: Efficiently converts discrete signal samples into connected 2D pixel trajectories without floating-point arithmetic; dummy pixel spacing reduces computational cost.
  - Quick check question: If signal amplitude resolution is 0.015625 per pixel and y-range is [0,1], what image height is required?

## Architecture Onboarding

- **Component map**: Horizontal signal → Normalize → Window (Lw=1000) → Bresenham rasterization → ImR (64×500)
  - Vertical signal → Window → CWT (Morlet) → 7 TF features (E, fd, h, K, Sk, avg, σTF)
  - Image branch: 4 Conv2D (dilated) + residual path → reshape
  - TF branch: 4 Conv1D (dilated) + residual path
  - Fusion branch: Concat → 3 LSTM → Add residual → LayerNorm → 8-head attention → 3 Linear → RUL

- **Critical path**: Input preprocessing (Algorithm 1 & 2) → parallel CNN extraction → concatenation → LSTM temporal modeling → attention filtering → regression. Errors in preprocessing propagate through both branches and cannot be corrected downstream.

- **Design tradeoffs**:
  - Data efficiency vs. model complexity: Uses 28-48% less training data than baselines but has higher inference time due to parallel branches (Figure 8b)
  - Interpretability vs. computational cost: Multimodal-LRP requires storing all activations during forward pass for backward relevance propagation
  - Fixed vs. adaptive: Operating frequency fo determines CWT scale range; requires reconfiguration for different machinery

- **Failure signatures**:
  - If model attends to flat signal regions (LRP heatmap shows uniform relevance): Check normalization—global min-max may compress dynamic range
  - If RUL predictions plateau early: LSTM may be over-smoothing; reduce LSTM units or increase attention heads
  - If unseen condition performance collapses: Dilation configuration may be overfitting to training operating frequencies; verify scale range covers test conditions

- **First 3 experiments**:
  1. Single-modality ablation: Train Image-only and TF-only variants to quantify each modality's contribution and identify dominant branch.
  2. Noise injection sweep: Test uniform/Gaussian/salt-and-pepper noise at varying intensities (beyond paper's fixed levels) to find robustness boundaries.
  3. Operating frequency mismatch: Train at 35 Hz, test at 40 Hz while monitoring CWT scale coverage to validate generalization claims.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can transfer learning techniques be effectively integrated to enhance model robustness in data-limited industrial environments where run-to-failure datasets are scarce?
  - Basis in paper: [explicit] The authors explicitly state in the Discussion that the scarcity of run-to-failure data hinders generalization and suggest exploring transfer learning to address this limitation.
  - Why unresolved: The current study validates the framework using specific benchmark datasets (XJTU-SY and PRONOSTIA) without implementing cross-domain transfer mechanisms.
  - What evidence would resolve it: Demonstrated performance improvements when the model is pre-trained on source domains and fine-tuned on limited target data from different machines.

- **Open Question 2**: Can model compression techniques like knowledge distillation or quantization successfully reduce the computational demands of the multimodal framework for deployment on resource-constrained edge devices?
  - Basis in paper: [explicit] The Discussion section notes the model's relatively large size and computational demands, suggesting compression methods to facilitate efficient inference in resource-constrained environments.
  - Why unresolved: The current architecture prioritizes accuracy using complex parallel branches (Image + TF), resulting in a heavy computational load unsuitable for embedded systems.
  - What evidence would resolve it: Implementation of pruning or quantization showing reduced model size and inference time while maintaining comparable RUL estimation accuracy.

- **Open Question 3**: Do advanced time-frequency techniques, such as the synchrosqueezing transform, provide more informative feature representations than the currently used Morlet wavelet for bearing fault detection?
  - Basis in paper: [explicit] The authors acknowledge the Heisenberg-Gabor uncertainty principle limits wavelet resolution and suggest that advanced transforms may offer more precise feature representations.
  - Why unresolved: The current implementation relies solely on Continuous Wavelet Transform (CWT) with the Morlet wavelet for time-frequency representation.
  - What evidence would resolve it: Comparative experiments showing that synchrosqueezing transforms yield higher resolution features and improved RUL prediction accuracy over the CWT baseline.

## Limitations

- **Dataset Dependency**: The framework relies on specific degradation patterns in XJTU-SY and PRONOSTIA datasets, with limited validation on other bearing types or machinery.
- **Computational Complexity**: The multimodal approach with parallel dilated convolutional branches and attention mechanisms introduces significant computational overhead despite improved data efficiency.
- **Interpretability Method Limitations**: Multimodal Layer-wise Relevance Propagation provides relative relevance scores that require domain expert calibration to translate into actionable maintenance decisions.

## Confidence

- **High Confidence**: The multimodal signal representation mechanism (ImR + TFR) and its contribution to capturing complementary degradation information is well-supported by ablation study results showing MMv2 (no fusion) degrades by 23-42% on unseen conditions.
- **Medium Confidence**: The hierarchical dilated convolution configuration (Block 1: high dilation/low kernel; Block 2: low dilation/high kernel) is theoretically sound but lacks direct comparison with alternative configurations in the paper.
- **Medium Confidence**: The LSTM + attention fusion mechanism effectively filters temporal redundancy, though the specific choice of 8 attention heads and 3 LSTM layers appears somewhat arbitrary without systematic hyperparameter exploration.

## Next Checks

1. **Cross-Platform Validation**: Test the framework on additional bearing datasets (e.g., C-MAPSS turbofan data) to verify generalization beyond the two demonstrated datasets and assess robustness to different degradation patterns.

2. **Computational Efficiency Analysis**: Profile the inference time and memory usage on edge devices to quantify the practical deployment constraints and identify optimization opportunities for the parallel branch architecture.

3. **Ablation Study Extension**: Systematically vary the number of attention heads (1-16) and LSTM layers (1-5) while monitoring both performance and computational cost to identify the optimal configuration trade-off.