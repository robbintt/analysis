---
ver: rpa2
title: What Do Prosody and Text Convey? Characterizing How Meaningful Information
  is Distributed Across Multiple Channels
arxiv_id: '2512.16832'
source_url: https://arxiv.org/abs/2512.16832
tags:
- information
- prosody
- text
- sarcasm
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an information-theoretic framework to quantify
  how much information about a specific linguistic feature (like sarcasm or emotion)
  is conveyed through prosody versus text. The approach estimates mutual information
  between a target feature and different communication channels (audio, text, prosody)
  using fine-tuned speech and language models.
---

# What Do Prosody and Text Convey? Characterizing How Meaningful Information is Distributed Across Multiple Channels

## Quick Facts
- arXiv ID: 2512.16832
- Source URL: https://arxiv.org/abs/2512.16832
- Authors: Aditya Yadavalli, Tiago Pimentel, Tamar I Regev, Ethan Wilcox, Alex Warstadt
- Reference count: 40
- Key result: Audio channels convey over an order of magnitude more information about sarcasm and emotion compared to text alone, while for questionhood the advantage is smaller (2.4x)

## Executive Summary
This paper introduces an information-theoretic framework to quantify how much information about linguistic features (like sarcasm or emotion) is conveyed through prosody versus text. The approach estimates mutual information between target features and different communication channels using fine-tuned speech and language models. Experiments on sarcasm detection, affect classification, and questionhood classification show that audio channels consistently convey more information than text alone, with the advantage being largest for features requiring pragmatic inference.

## Method Summary
The authors estimate mutual information using a classifier-based approach: they train models to predict a target feature from either text, audio, or both, then use the classifier's cross-entropy loss as an upper bound on conditional entropy. This allows computing MI(Y;X) = H(Y) - Hθ(Y|X), where H(Y) is estimated non-parametrically from label frequencies and Hθ(Y|X) is bounded by the classifier's cross-entropy. The audio channel serves as a proxy for prosody by computing MI(Y;A|T), which approximates the unique prosodic contribution to the target feature.

## Key Results
- For sarcasm and affect classification, audio conveys over an order of magnitude more information than text alone
- For questionhood classification, audio advantage is smaller (2.4x), aligning with predictions about syntactic markers
- Local prosodic information often suffices for sarcasm and affect detection even with single-sentence text context
- Whisper-based audio encoders outperform wav2vec2.0 for these tasks, suggesting ASR-pretraining helps capture prosodic features

## Why This Works (Mechanism)

### Mechanism 1
- Classifier cross-entropy loss provides an upper bound on conditional entropy, enabling mutual information estimation without density estimation
- Core assumption: Classifier achieves near-optimal prediction; better models yield tighter bounds
- Evidence: Section 4 discusses avoiding pitfalls of differential entropy estimation
- Break condition: Severe classifier underfitting or noisy/imbalanced labels

### Mechanism 2
- Audio channel proxies for prosody when estimating unique prosodic contributions
- Core assumption: Non-prosodic audio information (noise, accent, speaker identity) minimally impacts semantic feature prediction
- Evidence: Section 3.2 reasons that non-prosodic audio is likely irrelevant to target features
- Break condition: If speaker identity or recording conditions systematically correlate with labels

### Mechanism 3
- Different semantic features show predictable variation in audio vs. text information distribution
- Core assumption: Single-sentence text without broader discourse context is appropriate comparison
- Evidence: Section 6 shows audio advantages are largest for sarcasm and affect
- Break condition: Extended text context substantially increases text-only information contribution

## Foundational Learning

- **Mutual Information as shared uncertainty reduction**
  - Why needed: Framework quantifies how observing one variable reduces uncertainty about another
  - Quick check: If MI(Y; A) = 0.22 bits and H(Y) = 0.98 bits, what fraction of sarcasm uncertainty does audio resolve? (~22%)

- **Cross-entropy bounds entropy for discrete classification**
  - Why needed: Understanding why classifier loss validly estimates conditional entropy
  - Quick check: Why can't we directly compute H(Y|X) without a model? (Requires knowing true conditional distribution)

- **Suprasegmental vs. segmental speech features**
  - Why needed: Framework isolates prosody (pitch, tempo, loudness) from text (lexical content)
  - Quick check: Is a rising pitch contour at utterance end a segmental or suprasegmental cue? (Suprasegmental)

## Architecture Onboarding

- **Component map**: Text encoder (GPT-2) -> Audio encoder (Whisper/wav2vec2.0) -> Classification head (audio+text fusion via concatenation)

- **Critical path**: Prepare dataset → Estimate H(Y) via label frequencies → Train text-only classifier → Train audio-only classifier → Compute MI(Y;T), MI(Y;A), and MI(Y;A|T)

- **Design tradeoffs**:
  - Whisper (ASR-pretrained) vs. wav2vec (self-supervised) for audio encoding
  - GPT-2 size vs. computational tractability
  - Full fine-tuning vs. LoRA for different dataset sizes
  - Sentence-level vs. extended context for text input

- **Failure signatures**:
  - Negative MI estimates indicate poor classifier performance
  - MI(Y;A) ≈ MI(Y;T) suggests prosody uninformative or audio encoder failure
  - Large variance across runs indicates hyperparameter sensitivity
  - Audio+text underperforming audio-only suggests fusion architecture issues

- **First 3 experiments**:
  1. Reproduce sarcasm MI estimates on MUStARD with 5-fold cross-validation
  2. Compare Whisper vs. wav2vec audio encoders on affect classification
  3. Test questionhood classification with extended text context (prior utterances)

## Open Questions the Paper Calls Out

- How does text vs. prosody information contribution change with longer discourse context versus single-sentence context?
- How does information distribution differ across typologically distinct languages (tone languages vs. pitch-accent languages)?
- Can architecture be designed to accept isolated prosodic features directly for MI estimation?

## Limitations
- Proxy assumption that audio approximates prosody may overestimate prosodic contributions
- Sentence-level text context may underestimate text's true information contribution
- Results sensitive to classifier quality and dataset characteristics

## Confidence
- **Medium**: Proxy assumption central limitation without direct validation
- **Medium**: Sentence-level restriction may underestimate text contribution
- **High**: Information-theoretic approach technically sound but sensitive to classifier quality

## Next Checks
1. Test proxy assumption sensitivity by filtering audio to remove speaker identity and background noise
2. Extend text context systematically for sarcasm and emotion detection to measure context effects
3. Apply framework to datasets with different recording conditions and speaker demographics to test generalization