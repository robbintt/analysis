---
ver: rpa2
title: Programmatic Video Prediction Using Large Language Models
arxiv_id: '2505.14948'
source_url: https://arxiv.org/abs/2505.14948
tags:
- video
- frames
- proggen
- prediction
- such
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents ProgGen, a programmatic approach to video
  frame prediction that leverages large language models (LLMs) to synthesize human-interpretable,
  physics-grounded states representing video dynamics. Unlike end-to-end deep learning
  methods, ProgGen generates three programs: a perception program to estimate states
  from frames, a dynamics estimation program to forecast future states, and a rendering
  program to convert predicted states back to visual frames.'
---

# Programmatic Video Prediction Using Large Language Models

## Quick Facts
- arXiv ID: 2505.14948
- Source URL: https://arxiv.org/abs/2505.14948
- Authors: Hao Tang; Kevin Ellis; Suhas Lohit; Michael J. Jones; Moitreya Chatterjee
- Reference count: 14
- Primary result: ProgGen achieves competitive video prediction performance using only 10 training samples versus millions required by diffusion models

## Executive Summary
This paper introduces ProgGen, a programmatic approach to video frame prediction that leverages large language models (LLMs) to synthesize human-interpretable, physics-grounded states representing video dynamics. Unlike end-to-end deep learning methods, ProgGen generates three programs: a perception program to estimate states from frames, a dynamics estimation program to forecast future states, and a rendering program to convert predicted states back to visual frames. The method represents each frame as a set of symbolic attributes (position, velocity, etc.) rather than pixel-level data, enabling better interpretability and counter-factual reasoning.

The authors evaluate ProgGen on two environments: PhyWorld (uniform motion and collision scenarios) and Cart Pole. Remarkably, ProgGen achieves competitive performance with state-of-the-art diffusion models while using only 10 training samples versus millions required by baselines. In PhyWorld, ProgGen outperforms diffusion models (DiT variants) on out-of-distribution velocity predictions, with velocity errors an order of magnitude smaller. On Cart Pole, ProgGen achieves superior performance across all metrics (MAE: 0.003 vs 0.011, PSNR: 30.59 vs 22.01) while also demonstrating effective counter-factual reasoning capabilities.

## Method Summary
ProgGen is a programmatic video prediction framework that uses LLMs to generate three types of programs: perception (P), dynamics (D), and rendering (R). The perception program estimates symbolic states from video frames using segmentation and tracking tools, the dynamics program predicts future states using physics equations, and the rendering program converts predicted states back to visual frames. The approach uses a two-stage training process where LLM-generated programs are first synthesized, then continuous parameters are optimized using a surrogate state-space loss. This enables sample-efficient learning with only 10 training samples while achieving competitive performance with diffusion models that require millions of samples.

## Key Results
- Achieves competitive performance with diffusion models while using only 10 training samples versus millions required by baselines
- Outperforms DiT-large on PhyWorld OOD velocity predictions (error: 0.0150 vs 0.4270, 28x improvement)
- On Cart Pole, achieves superior performance across all metrics (MAE: 0.003 vs 0.011, PSNR: 30.59 vs 22.01)

## Why This Works (Mechanism)

### Mechanism 1: LLM-Driven Modular Program Synthesis
- Claim: Decomposing video prediction into three separate programs (perception, dynamics, rendering) allows LLMs to leverage their code generation capabilities for physics-grounded reasoning.
- Mechanism: A VLM generates Python programs for: (1) P - extracting symbolic states from frames using segmentation/tracking tools, (2) D - forecasting future states via physics equations, (3) R - converting states back to RGB frames via simulation.
- Core assumption: LLMs possess sufficient inductive bias about physics and program structure to generate correct code from only 10 visual examples.
- Evidence anchors:
  - [abstract] "ProgGen utilizes LLM/VLM to synthesize programs: (i) to estimate the states... (ii) to predict the states... (iii) to render the predicted states"
  - [section 4.2] "We seek to leverage the effective program generation capacity of large Language/Large Vision-Language Models... to propose hypotheses for P, D, and R"
  - [corpus] Related work (WorldCoder, DreamCoder) validates program synthesis for world modeling, though corpus evidence for video-specific application is limited
- Break condition: LLM generates syntactically incorrect code, or generated physics equations fundamentally mismatch true dynamics (e.g., non-rigid body deformations).

### Mechanism 2: Symbolic State Representation for OOD Generalization
- Claim: Representing frames as human-interpretable symbolic states (position, velocity, angle) rather than pixels enables orders-of-magnitude better out-of-distribution generalization.
- Mechanism: The perception program P maps RGB frames to physics-grounded attributes; dynamics program D operates on these compact states; this abstraction removes pixel-level spurious correlations.
- Core assumption: The salient dynamics can be captured by a finite set of pre-defined or auto-discovered attributes.
- Evidence anchors:
  - [abstract] "ProgGen achieves competitive performance... while using only 10 training samples versus millions... ProgGen outperforms diffusion models on out-of-distribution velocity predictions, with velocity errors an order of magnitude smaller"
  - [table 1] OOD velocity error: ProgGen 0.0150 vs DiT-large 0.4270 (28x improvement)
  - [corpus] PAN and related world models similarly emphasize symbolic/compact state spaces for long-horizon prediction
- Break condition: Scenes with complex visual phenomena (textures, lighting, non-rigid motion) that cannot be adequately represented by simple physical attributes.

### Mechanism 3: Two-Stage Training with Surrogate Loss
- Claim: Separating discrete program synthesis (stage 1) from continuous parameter optimization (stage 2), combined with state-space surrogate loss, enables sample-efficient learning.
- Mechanism: LLM generates program templates with placeholders; continuous parameters (gravity, mass) are optimized via Powell's method or L-BFGS using a surrogate loss on predicted vs. estimated states (not pixels).
- Core assumption: Program structure is approximately correct after stage 1; only continuous calibration is needed; surrogate state estimates correlate with visual quality.
- Evidence anchors:
  - [section 4.3] "In the first stage, the programs, P, D, and R are estimated, while the trainable continuous parameters θ are estimated in the next"
  - [section 4.3] "Surrogate Loss for Efficient Optimization: The typical pixel-level training loss... is not sensitive to the accuracy of the reconstructed frames"
  - [corpus] No direct corpus validation of two-stage training for video prediction; this appears novel
- Break condition: Surrogate state estimates diverge from true states (e.g., SAM segmentation failures), or program structure requires iteration.

## Foundational Learning

- Concept: Program Synthesis via LLMs
  - Why needed here: Core mechanism - understanding how LLMs generate executable code from examples, including placeholder patterns for continuous parameters.
  - Quick check question: Can you explain why LLMs struggle with continuous parameter estimation but excel at discrete program structure generation?

- Concept: Neuro-Symbolic State Spaces
  - Why needed here: The representation choice that enables sample efficiency - understanding how physical attributes form compact state representations.
  - Quick check question: Given a bouncing ball video, what minimal state attributes would you need to predict future frames? How does this compare to pixel-space representations?

- Concept: Black-Box vs Gradient-Based Optimization
  - Why needed here: Stage 2 training uses Powell's method (non-differentiable programs) or L-BFGS (JAX programs) - understanding when each applies.
  - Quick check question: If your dynamics program D uses a non-differentiable physics engine (Box2D), which optimizer would you use and why?

## Architecture Onboarding

- Component map:
Input Video (F+1 frames) → Grounded-SAM + XMem (segmentation/tracking) → Perception Program P (LLM-generated Python) → Symbolic States {s_0, ..., s_F} → Dynamics Program D (LLM-generated physics equations + θ params) → Predicted States {s_F+1, ..., s_T} → Rendering Program R (LLM-generated + physics simulator) → Output Frames {f_F+1, ..., f_T}

- Critical path:
  1. Prompt engineering for VLM to generate P, D, R programs (determines entire system capability)
  2. Affordance Rules (ARLs) design to constrain program search space
  3. Continuous parameter optimization via surrogate loss (requires accurate state estimation)

- Design tradeoffs:
  - Interpretability vs. expressiveness: Symbolic states are readable but may miss visual nuances
  - Sample efficiency vs. generality: 10 samples suffice for these domains; unknown for complex real-world scenes
  - Surrogate vs. pixel loss: State-space loss is efficient but risks misalignment with visual quality

- Failure signatures:
  - Grounded-SAM mislabeling (Figure 6: pole detected as "black cart") - use GPT-4V verification (noisy)
  - OOD physics not captured by generated programs - requires re-prompting with expanded examples
  - Surrogate loss optimization converges but visual quality degrades - switch to pixel-level loss for refinement

- First 3 experiments:
  1. Replicate PhyWorld uniform motion with 10 samples; verify velocity error < 0.02 (baseline: table 1)
  2. Ablate surrogate loss vs. pixel loss on Cart Pole; expect surrogate to be 10x faster but check PSNR gap
  3. Test counter-factual reasoning: modify pole length in Cart Pole and verify generated frames reflect physics change (Figure 5)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can ProgGen be extended to handle complex real-world attributes such as non-rigid deformations, fluid dynamics, or complex lighting textures?
- Basis in paper: [explicit] The "Limitations" section states that adapting to real-world settings requires a "richer set of attribute descriptions which might be challenging for current LLM/VLM to estimate."
- Why unresolved: The current evaluations (PhyWorld, Cart Pole) rely on rigid-body physics and simple pixel-space rendering, which do not capture the nuance of complex real-world visual phenomena.
- What evidence would resolve it: Successful application and maintained sample efficiency on datasets featuring non-rigid objects (e.g., cloth, water) or natural outdoor scenes.

### Open Question 2
- Question: How can the perception pipeline be made robust to systematic errors in object detection and labeling by the underlying VLM/segmentation models?
- Basis in paper: [explicit] Appendix 7.3 identifies a failure case where Grounded-SAM mislabels a "pole" as a "black cart," and notes that GPT-4v verification is also noisy.
- Why unresolved: The pipeline currently depends heavily on the accuracy of pre-trained models (Grounded-SAM, XMem) without a mechanism to correct semantic errors in the initial state estimation.
- What evidence would resolve it: Ablation studies showing robustness to noisy segmentation inputs, or the integration of a self-correction module that improves state estimation accuracy over the baseline.

### Open Question 3
- Question: How does the computational cost and convergence time of the two-stage optimization scale with the number of interacting objects in a scene?
- Basis in paper: [inferred] The paper notes that "Affordance Rules" are used to reduce candidate hypotheses, suggesting the search space grows significantly with scene complexity.
- Why unresolved: The experiments focus on scenarios with very few objects (1 or 2), leaving the scalability of the black-box optimization (Powell's/L-BFGS) for high-dimensional state spaces unexplored.
- What evidence would resolve it: Performance and timing benchmarks on environments with N > 10 interacting objects, demonstrating that optimization remains tractable.

## Limitations

- The 10-sample training efficiency claim, while impressive on PhyWorld and Cart Pole, has uncertain generalizability to complex, real-world scenes with diverse objects, textures, and occlusions
- The symbolic state representation assumes objects can be adequately described by pre-defined attributes (position, velocity, angle), which may break down for deformable objects, fluid dynamics, or scenes requiring high-frequency visual details
- The pipeline depends heavily on the accuracy of pre-trained models (Grounded-SAM, XMem) without a mechanism to correct semantic errors in the initial state estimation

## Confidence

- **High confidence**: The modular program synthesis mechanism works as described for simple physics domains. The code generation capability of LLMs is well-established, and the decomposition into P, D, R programs is conceptually sound for the evaluated scenarios.
- **Medium confidence**: The 10-sample efficiency claim is validated on the two specific environments but lacks evidence for broader applicability. The OOD generalization advantage over diffusion models is demonstrated but may not scale to more complex visual phenomena.
- **Low confidence**: The robustness of the two-stage training approach with surrogate loss across diverse domains is unproven. The paper does not test failure modes when LLM-generated programs are fundamentally incorrect or when the search space of physics equations is insufficient.

## Next Checks

1. **OOD Complexity Test**: Evaluate ProgGen on the KTH action dataset or similar real-world video datasets with human actions. Measure whether the 10-sample efficiency advantage persists, or if sample requirements scale linearly with visual complexity. Expected outcome: Performance degradation with increased visual diversity, revealing limitations of symbolic state representation.

2. **Program Synthesis Robustness**: Systematically corrupt the training data (e.g., add noise, remove frames, change object appearances) and measure how often LLM-generated programs capture correct physics. Track the percentage of generated programs that fail compilation or produce incorrect dynamics. Expected outcome: Error rate increases significantly with data quality degradation, exposing brittleness in the prompt-response mechanism.

3. **State Representation Capacity**: Test ProgGen on a video dataset with non-rigid objects (e.g., cloth simulation, human body articulation). Measure whether symbolic states can capture the necessary degrees of freedom, or if pixel-level information is lost. Expected outcome: Significant performance drop on deformable object scenarios, revealing fundamental limitations of the physics-grounded attribute representation.