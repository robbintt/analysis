---
ver: rpa2
title: The Representational Alignment between Humans and Language Models is implicitly
  driven by a Concreteness Effect
arxiv_id: '2505.15682'
source_url: https://arxiv.org/abs/2505.15682
tags:
- concreteness
- word
- language
- words
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated how humans and language models represent
  word meaning, focusing on the concreteness dimension. Using Representational Similarity
  Analysis (RSA), the authors compared implicit similarity structures derived from
  human odd-one-out judgments with those from various language models.
---

# The Representational Alignment between Humans and Language Models is implicitly driven by a Concreteness Effect

## Quick Facts
- **arXiv ID:** 2505.15682
- **Source URL:** https://arxiv.org/abs/2505.15682
- **Reference count:** 15
- **Primary result:** Representational alignment between humans and language models is substantially driven by concreteness, with 20.6% average drop in alignment when concreteness is removed

## Executive Summary
This study investigates how humans and language models represent word meaning, focusing on the concreteness dimension. Using Representational Similarity Analysis (RSA), the authors compare implicit similarity structures derived from human odd-one-out judgments with those from various language models. They find significant alignment between human and model representations, particularly for concreteness. Critically, when concreteness is removed from the models' word embeddings, the human-model alignment drops substantially (by 20.6% on average), while removal of other word features like frequency or length has minimal impact. This demonstrates that the representational alignment between humans and language models is substantially driven by concreteness, a dimension that emerges implicitly in both systems without explicit training.

## Method Summary
The study used 40 German nouns selected via spectral clustering to vary concreteness while controlling for frequency, length, and orthographic similarity. Participants completed an odd-one-out task (247 triplets each) to derive behavioral RDMs reflecting implicit semantic distances. The authors extracted word embeddings from fastText, word2vec, BERT (base/large), and GPT2, computing model RDMs using cosine distance. RSA was performed using Spearman correlation between behavioral and model RDMs. Ablation experiments removed concreteness variance from embeddings via Ridge regression, with residuals used to compute new RDMs and test feature importance via Williams test.

## Key Results
- Human-model alignment for concreteness shows ρ > .5 for fastText/word2vec and ρ = .14-.37 for BERT/GPT2 (non-contextual embeddings)
- Ablation of concreteness causes average 20.6% drop in alignment (up to 26% for word2vec), while other features cause <7.6% drops
- Concreteness correlates highly with imageability (r=.93), raising questions about distinct attribution
- Frequency and length ablation show minimal impact on alignment (<7.6% drops)

## Why This Works (Mechanism)

### Mechanism 1: Concreteness as a Shared Organizing Dimension
- Claim: Concreteness serves as a primary organizing dimension that implicitly structures semantic representations in both humans and language models.
- Mechanism: Without explicit training (models) or task instructions (humans), both systems develop representational geometries where concrete and abstract words cluster separately along a detectable axis.
- Core assumption: The alignment captured by RSA reflects genuinely similar representational structure, not incidental correlation with a third variable.
- Evidence anchors: Ablation of concreteness causes average 20.6% drop in alignment (up to 26% for word2vec), while other features cause <7.6% drops.

### Mechanism 2: Implicit Similarity Structure from Triplet Behavioral Tasks
- Claim: Odd-one-out tasks reliably reveal implicit semantic distances that correlate with LM embedding spaces.
- Mechanism: Triplet judgments encode pairwise similarities (selected word is less similar to the other two); aggregating across trials produces a representational dissimilarity matrix (RDM) reflecting human semantic geometry.
- Core assumption: Participants' odd-one-out choices reflect semantic similarity rather than surface features or task strategies.
- Evidence anchors: 9,880 triplets from 40 words; words selected to vary concreteness while controlling frequency, length, orthographic similarity.

### Mechanism 3: Linear Feature Ablation for Causal Attribution
- Claim: Linearly removing feature variance from embeddings tests the causal contribution of that feature to alignment.
- Mechanism: Ridge regression predicts embeddings from the target feature; residuals form ablated embeddings. RSA recomputed on ablated RDMs shows feature importance via alignment drop.
- Core assumption: Features relate linearly to embeddings; ablation removes target variance without destroying other structure.
- Evidence anchors: Concreteness ablation significantly reduces alignment for all models (p<.001); frequency/length/OLD20 show minimal effects.

## Foundational Learning

- **Representational Similarity Analysis (RSA)**
  - Why needed here: RSA enables comparison of representational geometries across fundamentally different systems (human behavior vs. neural network embeddings) by reducing both to pairwise dissimilarity matrices.
  - Quick check question: Why does RSA compare RDMs rather than directly comparing embeddings or behavioral responses?

- **Concreteness Effect in Psycholinguistics**
  - Why needed here: Interpreting the results requires understanding that concreteness is an established cognitive variable affecting word recognition, memory, and neural processing—not just an arbitrary rating.
  - Quick check question: What behavioral and neural differences distinguish concrete from abstract word processing in humans?

- **Static vs. Contextualized Word Embeddings**
  - Why needed here: The study compares fastText/word2vec (static) with BERT/GPT-2 (contextualized), finding higher alignment for static models (ρ>.5) than contextualized (ρ=.14-.37).
  - Quick check question: Why might non-contextual embeddings (before attention layers) show higher alignment with human similarity judgments?

## Architecture Onboarding

- **Component map:**
  - Word selection (40 German nouns) -> Triplet generation (9,880 triplets) -> Behavioral data collection (odd-one-out task) -> Behavioral RDM construction -> Embedding extraction (fastText, word2vec, BERT, GPT2) -> Model RDM construction -> RSA alignment measurement -> Ablation experiments (Ridge regression) -> Feature importance analysis

- **Critical path:**
  1. Word selection controlling confounds (section 3.2)
  2. Behavioral data collection and RDM derivation
  3. Embedding extraction (non-contextual layer for transformers)
  4. Base RSA alignment measurement
  5. Per-feature ablation and alignment comparison (Williams test)

- **Design tradeoffs:**
  - 40-word set enables exhaustive triplet coverage but limits generalizability; authors acknowledge need for replication with larger/different word sets
  - Non-contextual embeddings avoid context effects but may underestimate contextualized model capabilities
  - German-only stimuli; cross-linguistic generalization unclear despite concreteness being "universal"

- **Failure signatures:**
  - Base alignment < 0.2: Likely stimulus-model mismatch or tokenization issues
  - Ablation increases alignment: Feature anti-correlated with human similarity; check for confounds
  - High participant variance: May need more trials or refined stimulus controls

- **First 3 experiments:**
  1. Replicate with larger word set (100+ words) in German and another language to test generalizability
  2. Extract contextualized embeddings with sentence contexts to test whether context modulates concreteness representation
  3. Add neuroimaging (fMRI/EEG) to test whether behavioral alignment predicts brain-model alignment, as authors suggest

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the representational alignment driven by concreteness change when words are processed within a sentence context rather than in isolation?
- **Open Question 2:** Is the shared concreteness representation observed in behavior also reflected in the spatial organization of the human brain?
- **Open Question 3:** Can the observed concreteness effect on alignment be generalized to larger, more balanced datasets and languages other than German?
- **Open Question 4:** Are the observed concreteness effects truly independent of, or driven by, correlated psycholinguistic factors like imageability or contextual availability?

## Limitations
- The 40-word stimulus set limits generalizability to larger vocabularies and different semantic domains
- German-only stimuli raise questions about cross-linguistic validity despite concreteness being described as "universal"
- Behavioral RDM construction assumes odd-one-out choices directly reflect semantic similarity, potentially conflating concreteness with other correlated features
- Ablation method assumes linear relationships between features and embeddings, which may underestimate non-linear or distributed feature representations

## Confidence
- **High confidence:** The observed concreteness-driven alignment pattern (20.6% average drop in RSA when concreteness is ablated) is robust across all tested models and withstands statistical comparison via Williams test
- **Medium confidence:** The interpretation that this alignment reflects genuinely similar representational structure rather than indirect correlation through third variables
- **Low confidence:** Generalizability to larger word sets, other languages, or different semantic dimensions beyond concreteness

## Next Checks
1. Replicate with 100+ German words and conduct cross-linguistic validation in English to test whether concreteness alignment holds across languages
2. Extract contextualized embeddings using actual sentence contexts to determine whether context modulates the concreteness representation and alignment
3. Conduct neuroimaging studies (fMRI/EEG) to test whether behavioral alignment between humans and models predicts neural alignment, as the authors suggest this would be a natural extension