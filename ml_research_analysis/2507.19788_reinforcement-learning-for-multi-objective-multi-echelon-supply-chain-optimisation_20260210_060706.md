---
ver: rpa2
title: Reinforcement Learning for Multi-Objective Multi-Echelon Supply Chain Optimisation
arxiv_id: '2507.19788'
source_url: https://arxiv.org/abs/2507.19788
tags:
- morl
- multi-objective
- inventory
- solutions
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a generalised multi-objective, multi-echelon
  supply chain optimisation model with non-stationary markets based on a Markov decision
  process, incorporating economic, environmental, and social considerations. The model
  is evaluated using a multi-objective reinforcement learning (RL) method, benchmarked
  against an originally single-objective RL algorithm modified with weighted sum using
  predefined weights, and a multi-objective evolutionary algorithm (MOEA)-based approach.
---

# Reinforcement Learning for Multi-Objective Multi-Echelon Supply Chain Optimisation

## Quick Facts
- arXiv ID: 2507.19788
- Source URL: https://arxiv.org/abs/2507.19788
- Reference count: 33
- Multi-objective RL outperforms MOEA and single-objective RL in supply chain optimization

## Executive Summary
This paper introduces a generalized multi-objective, multi-echelon supply chain optimization model using Markov decision processes, addressing economic, environmental, and social objectives in non-stationary markets. The authors develop MORL/D, a multi-objective reinforcement learning approach that decomposes problems into scalarized subproblems solved in parallel, enhanced by shared experience buffers and Pareto Simulated Annealing for weight adaptation. Benchmark experiments show MORL/D achieves superior trade-offs between optimality, diversity, and density compared to modified single-objective RL and MOEA-based methods, particularly in complex supply chain settings.

## Method Summary
The method formulates supply chain optimization as a MOMDP where production and delivery quantities are determined across routes to approximate Pareto fronts for profit, emissions, and service-level inequality. MORL/D decomposes the multi-objective problem into scalarized subproblems, each solved as a single-objective task using MOSAC (an off-policy actor-critic) with parallel processing. A shared experience buffer enables knowledge transfer among policies, while Pareto Simulated Annealing dynamically adapts weight vectors based on neighborhood comparisons. The approach is benchmarked against a weighted-sum PPO modification and NSGA-II MOEA on three network complexities using a customizable simulator.

## Key Results
- MORL/D achieves up to 75% higher hypervolume than MOEA-based methods in complex settings
- MORL/D generates solutions approximately eleven times denser than those from modified single-objective RL methods
- Shared experience buffer implementation enhances hypervolume and EUM values while decreasing sparsity across all supply chain problems

## Why This Works (Mechanism)

### Mechanism 1
Decomposing multi-objective problems into scalarized subproblems enables diverse, near-optimal trade-off solutions in high-dimensional action spaces. MORL/D assigns each policy a weight vector, scalarizes the vector reward into a single objective, and uses MOSAC to optimize each subproblem independently while sharing structural knowledge through neighbor connections. Linear scalarization adequately captures trade-off structure when subproblems are related enough that learning transfers.

### Mechanism 2
Shared experience buffers improve sample efficiency and solution diversity by enabling knowledge transfer across policies. A centralized replay buffer stores state-action-reward tuples from all policies. During training, each policy samples from this shared pool, learning from neighbors' experiences—effectively multiplying the effective sample size per subproblem. Subproblems share meaningful state-action-reward structure, preventing negative transfer from dominating.

### Mechanism 3
Dynamic weight adaptation via Pareto Simulated Annealing prevents premature convergence and promotes front diversity. PSA adjusts each policy's weight vector based on comparison with neighboring solutions—if a policy underperforms in an objective relative to its neighbor, that objective's weight increases, pushing exploration toward underrepresented regions. Local neighborhood comparisons provide sufficient signal for global front coverage.

### Mechanism 4
Sequential decision-making reduces effective dimensionality compared to batch optimization. Instead of optimizing all periods simultaneously (e.g., 5,900 decision variables for complex SC), MOMDP conditions each decision on current state, decomposing the problem into T smaller subproblems with lower per-step action dimensionality. The state representation captures all relevant history; transitions are approximately Markov.

## Foundational Learning

- **Pareto Dominance and Front Approximation**
  - Why needed here: The paper optimizes three conflicting objectives (profit, emissions, service-level inequality); understanding non-dominated solutions is essential to interpret results.
  - Quick check question: Given solutions A=(profit=100, emission=50, SL_ineq=5) and B=(profit=90, emission=40, SL_ineq=5), does A dominate B?

- **Scalarization Functions in Multi-Objective RL**
  - Why needed here: MORL/D and weighted-sum PPO both convert vector rewards to scalars; understanding how weights shape the learned front is critical.
  - Quick check question: If weights are (0.5, 0.3, 0.2) for profit/emissions/SL_ineq and rewards are (100, -50, -10), what is the scalarized return?

- **Off-Policy Actor-Critic (SAC) Basics**
  - Why needed here: MORL/D uses MOSAC; understanding entropy regularization, target networks, and replay buffers clarifies why sample efficiency improves.
  - Quick check question: Why does SAC use a replay buffer while PPO (on-policy) does not?

## Architecture Onboarding

- **Component map:**
  - Environment (Messiah) -> MORL/D Controller (N policies with weight vectors) -> Shared Buffer -> MOSAC updates -> Pareto Archive (non-dominated solutions)
  - PSA Weight Adapter -> adjusts weight vectors based on neighbor comparisons

- **Critical path:**
  1. Initialize SC network in Messiah (define nodes, edges, demand distributions, costs, emissions)
  2. Initialize MORL/D: create N policies with uniform random weights, empty shared buffer, empty archive
  3. For each iteration: each policy interacts with environment for K steps, storing transitions in shared buffer; each policy samples from shared buffer and updates actor-critic via MOSAC; evaluate each policy; update Pareto archive with non-dominated solutions; optionally apply PSA to adapt weights
  4. Return Pareto archive as PF approximation

- **Design tradeoffs:**
  - Population size vs. compute: More policies improve front coverage but scale training cost linearly. Paper uses N=6 as balance.
  - Shared buffer vs. isolation: Shared buffer improves sample efficiency but risks negative transfer if subproblems diverge. Ablation shows net positive.
  - Static vs. adaptive weights: Static weights (PPO) require manual tuning; adaptive (PSA) automate diversity but add hyperparameter (δ).

- **Failure signatures:**
  - Sparse front with gaps: Indicates weight initialization or adaptation failed to cover objective space; check PSA δ and neighbor definition.
  - Unstable EUM over training: Suggests gradient interference between objectives; verify normalization and scalarization stability.
  - Inventory explosion or collapse: Policy learned to ignore demand; check reward penalty for negative inventory and ensure demand signal reaches policy.
  - Dominance by single objective: Hypervolume high but front clustered; likely normalization failure causing one objective to dominate scalarization.

- **First 3 experiments:**
  1. Baseline sanity check: Run single-objective PPO on profit alone; verify it learns reasonable production policy (non-zero inventory, non-trivial profit).
  2. Weight sweep for weighted-sum PPO: Run PPO with 5 hand-picked weight vectors; visualize PF to verify scalarization produces diverse solutions.
  3. MORL/D ablation (buffer only): Run MORL/D with shared buffer enabled but PSA disabled on simple SC; compare hypervolume and sparsity to PPO baseline.

## Open Questions the Paper Calls Out

- **Meta-learning for generalization:** Can meta-learning techniques reduce the training time and improve the generalization of MORL agents for new supply chain tasks? The current study focused on benchmarking standard algorithms and did not implement or evaluate meta-learning strategies.

- **Stochastic state transitions:** How does the introduction of stochastic state transitions, such as lead time fluctuations, impact the performance stability of the proposed MOMDP model? The model formulation excluded stochastic elements to simplify the MDP framework.

- **More intricate supply chain settings:** Does the MORL/D algorithm retain its superior hypervolume and density performance in supply chain settings more intricate than the forward-logistics networks tested? Experiments were restricted to specific network complexities without exploring more complex topologies.

## Limitations
- The proprietary Messiah simulator is not publicly available, limiting independent verification of performance claims.
- Exact parameters for moderate and complex network topologies are referenced to supplementary material not included in the paper.
- MORL/D implementation depends on the MORL Baselines 1.0.0 library, which may not be accessible or may contain implementation details that affect reproducibility.

## Confidence
- **High Confidence**: The core mechanism of MORL/D decomposition and shared experience buffer—supported by ablation studies showing SB improves hypervolume and sparsity across all SC problems.
- **Medium Confidence**: PSA weight adaptation effectiveness—while the paper shows improvements in complex settings, limited ablation testing and lack of direct corpus validation reduce confidence.
- **Medium Confidence**: Performance superiority claims—hypervolume improvements are substantial but depend on proprietary environment; relative rankings may hold but absolute values cannot be verified without Messiah.

## Next Checks
1. Re-implement the Messiah environment from the MOMDP formulation and validate against the simple network case (8-dim action space) to verify reward and state transitions match expected behavior.
2. Run MORL/D with shared buffer disabled on moderate network to isolate the contribution of knowledge transfer versus core decomposition mechanism.
3. Conduct sensitivity analysis on PSA weight adaptation parameters (δ, neighbor definition) to identify stability thresholds and failure modes in the weight update process.