---
ver: rpa2
title: Is In-Context Universality Enough? MLPs are Also Universal In-Context
arxiv_id: '2502.03327'
source_url: https://arxiv.org/abs/2502.03327
tags:
- lemma
- each
- depth
- width
- relu
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether in-context universality is sufficient
  to explain the superior empirical performance of transformers over multilayer perceptrons
  (MLPs). Recent results have shown that transformers are universal in-context learners,
  capable of approximating any real-valued continuous function of a context (a probability
  measure over X) and a query.
---

# Is In-Context Universality Enough? MLPs are Also Universal In-Context

## Quick Facts
- **arXiv ID:** 2502.03327
- **Source URL:** https://arxiv.org/abs/2502.03327
- **Authors:** Anastasis Kratsios; Takashi Furuya
- **Reference count:** 40
- **Primary result:** MLPs with trainable activation functions are universal in-context learners, matching transformers' theoretical expressiveness

## Executive Summary
This paper challenges the prevailing assumption that transformers' in-context learning superiority stems from their universal approximation capability. While recent work established that transformers are universal in-context learners, the authors prove that multilayer perceptrons (MLPs) with trainable activation functions can achieve the same universality. The key insight is that ReLU MLPs can exactly implement the 1-Wasserstein distance for permutation-invariant contexts, enabling them to construct optimal piecewise-constant approximators. This theoretical result suggests that transformers' empirical advantages likely arise from other factors such as inductive bias or training stability rather than universality alone.

## Method Summary
The authors develop a mathematical framework proving that MLPs with trainable activation functions are universal in-context learners. They first establish that ReLU MLPs can exactly compute the 1-Wasserstein distance for permutation-invariant contexts (PICs). Using this foundation, they construct optimal piecewise-constant approximators on the context space, demonstrating that MLPs can approximate any real-valued continuous function of a context and query with arbitrary precision. The proof provides quantitative bounds showing that the depth and width of the approximating MLPs are polynomial in the inverse of the approximation error and the dimension of the context space. The work is complemented by a conversion procedure from MLPs to transformers and a corollary extending the universality result to transformers with multiple attention heads per block.

## Key Results
- MLPs with trainable activation functions are universal in-context learners, matching transformers' theoretical expressiveness
- ReLU MLPs can exactly implement the 1-Wasserstein distance for permutation-invariant contexts
- The depth and width of approximating MLPs scale polynomially with the inverse approximation error and context dimension

## Why This Works (Mechanism)
The mechanism relies on the ability of ReLU MLPs to compute Wasserstein distances, which provide a natural metric for comparing probability measures over contexts. By exactly implementing 1-Wasserstein distance, MLPs can measure similarities between different contexts in the probability measure space. This distance computation enables the construction of piecewise-constant approximators that can adapt to the structure of the context space. The trainable activation functions allow the network to learn optimal representations for computing these distances and constructing the piecewise-constant partitions needed for approximation.

## Foundational Learning

**Permutation-invariant contexts (PICs)** - Why needed: The analysis assumes contexts are probability measures over input spaces that are invariant to input ordering. Quick check: Verify that the target application involves unordered collections or that ordering is irrelevant.

**1-Wasserstein distance** - Why needed: This metric quantifies the difference between probability measures and enables comparison of different contexts. Quick check: Confirm understanding of how Wasserstein distance differs from other probability metrics like KL divergence.

**Universal approximation theorem** - Why needed: The paper extends classical universal approximation to the in-context learning setting. Quick check: Review the original universal approximation theorem for MLPs and understand its limitations.

## Architecture Onboarding

**Component map:** Input context → 1-Wasserstein distance computation → Piecewise-constant partitioning → Function approximation

**Critical path:** The most important components are the exact implementation of Wasserstein distance and the piecewise-constant approximation construction, as these directly enable universality.

**Design tradeoffs:** The main tradeoff is between depth/width (complexity) and approximation accuracy. Polynomial scaling in error and dimension may become prohibitive for high-dimensional contexts.

**Failure signatures:** If the 1-Wasserstein distance cannot be exactly computed or if the context space lacks sufficient structure for piecewise-constant approximation, universality fails.

**First experiments:**
1. Implement the exact 1-Wasserstein distance computation in a ReLU MLP and verify correctness on simple probability measures
2. Construct piecewise-constant approximators for simple continuous functions using the MLP framework
3. Measure the scaling of depth and width requirements as a function of approximation error and context dimension

## Open Questions the Paper Calls Out
The paper does not explicitly identify open questions, but several emerge from the results:
- What specific inductive biases make transformers practically superior despite theoretical equivalence?
- How does the polynomial scaling of MLP complexity compare to transformer complexity in practice?
- Can the conversion procedure from MLPs to transformers be made computationally efficient?

## Limitations
- The universality result is asymptotic and doesn't directly address empirical performance gaps
- Polynomial bounds on depth and width may be impractical for high-dimensional contexts
- The analysis focuses on exact implementability but real-world attention mechanisms may have different computational properties

## Confidence

**High confidence:** The theoretical proofs establishing MLP universality are mathematically rigorous and well-founded.

**Medium confidence:** The practical implications of the theoretical results for real-world tasks, given the polynomial complexity scaling.

**Low confidence:** The extent to which the exact implementability of 1-Wasserstein distance translates to practical computational advantages over attention mechanisms.

## Next Checks

1. Implement the conversion procedure from MLPs to transformers and empirically compare computational complexity for approximating specific functions
2. Conduct controlled experiments comparing transformer and MLP performance on tasks where the theoretical approximation bounds are tight
3. Analyze the empirical generalization gap between theoretically universal MLPs and transformers across varying context dimensions and function complexities