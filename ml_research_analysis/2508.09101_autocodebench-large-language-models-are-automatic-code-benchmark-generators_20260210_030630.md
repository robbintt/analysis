---
ver: rpa2
title: 'AutoCodeBench: Large Language Models are Automatic Code Benchmark Generators'
arxiv_id: '2508.09101'
source_url: https://arxiv.org/abs/2508.09101
tags:
- uni00000003
- uni00000048
- uni00000044
- uni00000055
- uni00000057
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes AutoCodeGen, an automated workflow based on
  LLM-Sandbox interaction for generating high-quality multilingual code generation
  datasets without manual annotations. AutoCodeGen generates test inputs with LLMs
  and obtains test outputs through a multilingual sandbox, ensuring correctness and
  completeness while maintaining high data quality through reverse-order problem generation
  and multiple filtering steps.
---

# AutoCodeBench: Large Language Models are Automatic Code Benchmark Generators

## Quick Facts
- arXiv ID: 2508.09101
- Source URL: https://arxiv.org/abs/2508.09101
- Reference count: 20
- The best model achieved only 64.5% pass@1 on AutoCodeBench-Lite and no model exceeded 53% on the full benchmark.

## Executive Summary
This paper introduces AutoCodeBench, a large-scale multilingual code generation benchmark created through an automated pipeline called AutoCodeGen. The system generates problems by evolving code snippets, creating test cases through sandbox execution, and synthesizing problem descriptionsâ€”all without manual annotations. The resulting benchmark contains 3,920 problems across 20 programming languages, designed to be challenging even for state-of-the-art models. Evaluation shows that even the most advanced open-source and proprietary LLMs struggle with the benchmark's complexity, diversity, and multilingual nature.

## Method Summary
The AutoCodeGen pipeline uses a four-stage automated process: (1) Code Solution Generation using DeepSeek-V3 to refine Stack-Edu code snippets into complete solutions; (2) Test Function Generation via LLM-generated inputs combined with sandbox execution to obtain actual outputs; (3) Problem Generation with strict specifications to create natural language descriptions; and (4) Three-stage filtering including difficulty control (10 samples via DeepSeek-Coder-V2-Lite), quality control (DeepSeek-R1 as critic), and diversity sampling. The system processes 6 high-resource languages directly and uses approximate translation for 14 low-resource languages.

## Key Results
- AutoCodeBench contains 3,920 problems evenly distributed across 20 programming languages
- The best model achieved only 64.5% pass@1 on AutoCodeBench-Lite and no model exceeded 53% on the full benchmark
- Over 30 leading open-source and proprietary LLMs were evaluated, showing consistent difficulty across model families
- Manual verification found 87.6% accuracy for test case generation and 84.7% for problem description generation

## Why This Works (Mechanism)

### Mechanism 1: Execution-Based Verification (Sandboxing)
- **Claim:** Grounding test case generation in sandbox execution, rather than pure LLM inference, appears to reduce hallucination of incorrect expected outputs.
- **Mechanism:** The system generates test inputs via LLM, concatenates them with the generated code solution, and executes the pair in a multilingual sandbox to derive the actual output. This output is then combined with the input to form a verified test case.
- **Core assumption:** The sandbox environment accurately reflects the semantic behavior of the generated code, and the generated code is logically correct relative to the intent of the snippet seed.
- **Evidence anchors:** [abstract] "obtaining test outputs through a multilingual sandbox, ensuring correctness and completeness"; [section 2.2.2] "We concatenate the code solution with test input functions and execute them in the sandbox to obtain the corresponding test outputs."; [corpus] "Can LLMs Generate Reliable Test Case Generators?" (arXiv:2506.06821) highlights the difficulty of reliable test generation, validating the need for this grounding mechanism.
- **Break condition:** If the sandbox environment lacks dependencies or differs from standard production environments, the "ground truth" outputs will be invalid.

### Mechanism 2: Self-Consistency & Reverse Synthesis
- **Claim:** Generating problem descriptions after verifying solutions and test cases (Reverse-Order Generation) may reduce ambiguity and omission errors typical of forward-generation.
- **Mechanism:** Instead of asking an LLM to write a problem and then solve it, the pipeline starts with code snippets, evolves them into solutions, generates tests, and finally asks the LLM to describe the problem based on the working code and tests.
- **Core assumption:** LLMs are better at summarizing existing code logic into natural language than they are at generating self-consistent logic from natural language prompts.
- **Evidence anchors:** [abstract] "achieving high data quality through reverse-order problem generation"; [section 2.2.3] "We find that models often omit key information... when generating programming problems... we prompt [the LLM] to generate... based on the code solution."; [corpus] Weak/missing direct evidence in corpus for this specific reverse-order mechanism.
- **Break condition:** If the generated solution code is convoluted or the test cases are sparse, the LLM may generate a problem description that is too vague or misinterprets the core logic.

### Mechanism 3: Three-Stage Filtering Process
- **Claim:** A three-stage filtering process (Difficulty + Quality + Diversity) is likely necessary to prevent the benchmark from being populated by trivial or repetitive synthetic problems.
- **Mechanism:** First, a "moderately capable" model attempts to solve problems; if it solves one 10/10 times, the problem is discarded as too easy. Second, a reasoning model (DeepSeek-R1) critiques the problem-test alignment. Third, diversity sampling ensures category balance.
- **Core assumption:** The proxy model used for difficulty filtering (DeepSeek-Coder-V2-Lite) accurately represents the threshold between "trivial" and "challenging" for SOTA models.
- **Evidence anchors:** [section 2.2.4] "employ a moderately capable code model... to filter out too easy problems."; [table 4] Shows low pass@1 scores for top models (e.g., Claude Opus 4 at 52.4%), suggesting the filtering successfully retained difficulty; [corpus] "CodeMixBench" (arXiv:2505.05063) suggests existing benchmarks are saturated, reinforcing the need for this filtering.
- **Break condition:** If the filter model is too weak, the benchmark will contain trivially easy problems; if too strong, it may discard diverse but valid problems.

## Foundational Learning

- **Concept:** Execution-Based Verification (Sandboxing)
  - **Why needed here:** The core of AutoCodeBench relies on determining the "ground truth" output of a code snippet. You cannot trust the LLM to predict the output; you must run the code.
  - **Quick check question:** If I give you a Python function that modifies a global variable, can you manually trace the state changes better than an interpreter?

- **Concept:** Self-Consistency & Reverse Synthesis
  - **Why needed here:** The pipeline assumes that if code works (passes tests), the LLM can retroactively describe what it does accurately.
  - **Quick check question:** Given a complex sorting algorithm with no comments, can you write a natural language specification that matches exactly what the code does, including edge cases?

- **Concept:** Instruction Following for Structured Output
  - **Why needed here:** The system relies on prompts (Section 2.2) that force the LLM to adhere to strict specifications (e.g., "Do not include hints," "Use specific function signatures").
  - **Quick check question:** Can you design a prompt that prevents an LLM from adding conversational filler to a JSON object?

## Architecture Onboarding

- **Component map:** Stack-Edu (Code Snippets) -> Evolver (DeepSeek-V3) -> Test Generator (LLM + Sandbox) -> Problem Synthesizer (LLM) -> Filter Pipeline (Difficulty + Quality + Diversity)
- **Critical path:** The Sandbox Execution is the single point of failure. If the sandbox environment (Docker, iptables) is misconfigured or times out, the "Test Output Generation" fails, breaking the verification loop for the test cases.
- **Design tradeoffs:**
  - **Automation vs. Noise:** The pipeline is fully automated but achieves only ~87.6% manual accuracy (Appendix B). This trades the cost of manual labeling for the cost of some noisy data.
  - **Model Bias:** The system uses DeepSeek models for generation and filtering. Section 4.2 acknowledges this may introduce a "push-and-pull" bias where the benchmark is slightly more favorable or unfavorable to the generator's family.
- **Failure signatures:**
  - **Empty Problem Description:** The LLM fails to generate a description from the code.
  - **Signature Mismatch:** The generated problem description refers to a function name not present in the solution code.
  - **Flaky Tests:** Non-deterministic inputs (e.g., random seeds) pass the LLM-critic but fail in the sandbox.
- **First 3 experiments:**
  1. **Sandbox Validation:** Run the pipeline on a small set of known valid Python code. Verify that the generated test inputs actually produce the expected outputs in the sandbox.
  2. **Difficulty Calibration:** Run the "Difficulty Control" filter (Section 2.2.4) on HumanEval problems. Does it correctly classify the easy ones?
  3. **Bias Check:** Evaluate a DeepSeek model vs. a non-DeepSeek model on the generated benchmark to empirically check for the bias discussed in Section 4.2.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** To what extent does the reliance on DeepSeek series models in the AutoCodeGen pipeline introduce a self-preference bias that distorts relative performance rankings?
- **Basis in paper:** [explicit] Section 4.2 explicitly discusses the difficulty of measuring bias, hypothesizing that while a favorable bias toward the DeepSeek family may exist, the impact is minimal.
- **Why unresolved:** The authors note that as mutual distillation between models increases, measuring this bias becomes increasingly complex, and they rely on a "push-and-pull" mechanism rather than eliminating the single-model dependency.
- **What evidence would resolve it:** A comparative analysis where the benchmark is regenerated using a different model family (e.g., GPT-4 or Claude) as the generator, followed by a correlation analysis of the resulting leaderboards.

### Open Question 2
- **Question:** How does the 12.4% noise rate (invalid instances) identified in manual verification impact the statistical reliability of the AutoCodeBench leaderboard?
- **Basis in paper:** [inferred] Section 4.1 and Appendix B report an 87.6% accuracy rate, implying 12.4% of data contains noise (mostly incomplete descriptions), but the authors do not quantify the effect of this noise on the final evaluation metrics.
- **Why unresolved:** While the authors validate the existence of noise, they do not provide an ablation study showing how the removal of these 12.4% of problems alters the pass@1 scores or the relative ordering of models.
- **What evidence would resolve it:** An ablation study re-evaluating a subset of models on a "cleaned" version of the benchmark with the identified invalid instances removed.

### Open Question 3
- **Question:** Does the use of "approximate language translation" for low-resource programming languages introduce semantic drift or artifacts that artificially inflate difficulty?
- **Basis in paper:** [explicit] Section 2.2.5 states that for 14 languages, the authors employ an approximate translation approach due to limited data resources, rather than generating problems natively.
- **Why unresolved:** The paper attributes lower performance in low-resource languages to model capability gaps, but does not isolate whether the translation process itself introduces inconsistencies or errors that increase task difficulty independently of the coding challenge.
- **What evidence would resolve it:** A qualitative human evaluation comparing the semantic consistency and clarity of translated problems versus natively generated problems across the supported languages.

## Limitations
- The pipeline achieves only 87.6% manual accuracy for test case generation and 84.7% for problem description generation, indicating non-trivial error rates
- Reliance on DeepSeek family models for both generation and filtering may introduce bias favoring or disfavoring these models
- The sandbox environment is a critical dependency that could fail if dependencies are missing or execution differs from production environments
- Low-resource language performance depends on translation quality, which is not evaluated and may introduce artifacts

## Confidence

- **High confidence:** The claim that the benchmark is difficult for current models (64.5% pass@1 on AutoCodeBench-Lite, no model exceeding 53% on full benchmark) is well-supported by empirical evaluation of 30+ models across multiple categories.
- **Medium confidence:** The mechanism claims (sandbox grounding reduces hallucinations, reverse-order generation improves quality, three-stage filtering maintains difficulty) are logically sound but lack direct experimental ablation studies comparing alternative approaches.
- **Low confidence:** The assumption that the three-stage filtering process successfully creates a benchmark that is both challenging and diverse, rather than just filtered to match the capabilities of the DeepSeek models used in the pipeline.

## Next Checks

1. **Sandbox Environment Validation:** Execute a random sample of 100 generated test functions against their reference solutions in an independent sandbox environment to verify the 87.6% manual accuracy claim and identify any systematic execution failures.

2. **Model Family Bias Assessment:** Evaluate a diverse set of non-DeepSeek models (e.g., Claude, GPT family) against the benchmark and compare performance distributions to DeepSeek models to empirically quantify the potential bias discussed in Section 4.2.

3. **Low-Resource Language Quality Audit:** Manually audit 20 randomly selected problems from each of the 14 low-resource languages to assess whether translation artifacts or code generation errors compromise problem quality compared to the 6 high-resource languages.