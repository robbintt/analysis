---
ver: rpa2
title: Skip a Layer or Loop it? Test-Time Depth Adaptation of Pretrained LLMs
arxiv_id: '2507.07996'
source_url: https://arxiv.org/abs/2507.07996
tags:
- layers
- depth
- cola
- layer
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Chain-of-Layers (CoLa), a method for dynamically
  adapting the depth of pretrained large language models at test time without finetuning.
  CoLa treats each layer as a modular unit that can be selectively skipped or repeated
  during inference, allowing the model to construct custom execution paths per input.
---

# Skip a Layer or Loop it? Test-Time Depth Adaptation of Pretrained LLMs

## Quick Facts
- arXiv ID: 2507.07996
- Source URL: https://arxiv.org/abs/2507.07996
- Authors: Ziyue Li; Yang Li; Tianyi Zhou
- Reference count: 33
- Primary result: Introduces Chain-of-Layers (CoLa) for test-time depth adaptation, achieving up to 30% depth reduction and accuracy improvements on math and commonsense reasoning tasks

## Executive Summary
This paper introduces Chain-of-Layers (CoLa), a method for dynamically adapting the depth of pretrained large language models at test time without finetuning. CoLa treats each layer as a modular unit that can be selectively skipped or repeated during inference, allowing the model to construct custom execution paths per input. The authors use Monte Carlo Tree Search (MCTS) to efficiently search for optimal layer compositions that maximize prediction accuracy while minimizing depth. Experiments on math and commonsense reasoning tasks show that CoLa consistently finds shallower or more accurate execution paths than the original fixed-depth model, achieving up to 30% depth reduction while improving accuracy. Notably, for over 60% of originally incorrect predictions, CoLa finds shorter paths that yield correct answers, and even for correct predictions, it often identifies more efficient architectures. This demonstrates substantial redundancy in static transformer inference and highlights the potential of test-time depth adaptation for improving both efficiency and generalization.

## Method Summary
Chain-of-Layers (CoLa) treats each layer of a pretrained LLM as an independently reusable module that can be skipped or repeated during inference. The method formulates test-time inference as a sequential decision problem where the model must choose which layers to execute at each step. Monte Carlo Tree Search (MCTS) is employed to efficiently explore the space of possible layer compositions, balancing the trade-off between prediction accuracy and computational depth. During inference, CoLa constructs a custom execution path for each input by dynamically selecting which layers to activate, potentially skipping redundant layers or repeating layers that contribute to better reasoning. The search process is guided by a reward function that penalizes depth while rewarding correct predictions, enabling the discovery of more efficient architectures tailored to each specific input.

## Key Results
- CoLa achieves up to 30% reduction in inference depth while improving or maintaining accuracy on GSM8K and CommonsenseQA tasks
- For over 60% of originally incorrect predictions, CoLa finds shorter execution paths that yield correct answers
- Even for correct predictions, CoLa often identifies more efficient architectures, demonstrating redundancy in fixed-depth inference

## Why This Works (Mechanism)
CoLa exploits the inherent redundancy in fixed-depth transformer inference by treating each layer as a modular unit that can be selectively activated. The MCTS-based search efficiently explores the space of possible layer compositions, discovering that many inputs can be processed correctly with fewer layers than the default full-depth execution. This approach effectively adapts the model's capacity to the complexity of each individual input, allocating computational resources more efficiently.

## Foundational Learning

**Monte Carlo Tree Search (MCTS)**: A heuristic search algorithm that balances exploration and exploitation by building a search tree through random sampling. Why needed: MCTS efficiently explores the exponentially large space of possible layer compositions without exhaustive search. Quick check: Verify that the UCB1 formula is correctly implemented with proper exploration-exploitation balance.

**Layer-wise Modularization**: Treating individual transformer layers as reusable, independently executable modules. Why needed: Enables flexible composition of custom execution paths during inference. Quick check: Confirm that layer outputs are properly cached and can be reused across different execution paths.

**Test-time Adaptation**: Dynamically modifying model behavior during inference without parameter updates. Why needed: Allows efficient inference by adapting computational depth to input complexity. Quick check: Verify that adaptation happens solely through execution path selection, not parameter modification.

**Reward Shaping**: Designing a reward function that balances accuracy and depth reduction. Why needed: Guides the search toward solutions that are both correct and efficient. Quick check: Ensure the reward function properly penalizes depth while rewarding accuracy improvements.

## Architecture Onboarding

**Component Map**: Input -> Layer Selection Policy -> Layer Execution Cache -> Output Prediction
**Critical Path**: MCTS Search -> Layer Selection -> Layer Execution (with caching) -> Reward Evaluation -> Path Selection
**Design Tradeoffs**: The paper trades additional search computation during inference for reduced layer execution, accepting potential latency increases for improved efficiency and accuracy.

**Failure Signatures**: If MCTS fails to find efficient paths, the system defaults to full-depth execution, potentially missing opportunities for depth reduction. Poor reward shaping may lead to suboptimal path selection favoring either accuracy or efficiency at the expense of the other.

**First 3 Experiments to Run**:
1. Compare MCTS-based layer selection against random layer skipping to validate the search strategy's effectiveness
2. Test different reward function formulations to optimize the accuracy-depth trade-off
3. Evaluate layer caching effectiveness by measuring redundant computation across different execution paths

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental evaluation limited to LLaMA-7B on GSM8K and CommonsenseQA, leaving scalability to larger models and broader task domains untested
- MCTS search introduces computational overhead during inference that may offset efficiency gains in latency-sensitive applications
- No wall-clock time comparisons between standard inference and CoLa execution to assess practical deployment costs

## Confidence

**High confidence**: Core technical contribution (layer-skipping mechanism) and its theoretical soundness are well-established

**Medium confidence**: Depth reduction benefits are demonstrated but limited by narrow task diversity; scaling behavior to larger models is unknown

**Medium confidence**: Generalization claims based on specific datasets may not transfer to other domains or task types

## Next Checks

1. Evaluate CoLa across diverse reasoning and generation tasks (including multi-hop QA, code generation, and summarization) to assess generalizability beyond math and commonsense reasoning

2. Measure actual inference latency and computational overhead of MCTS-based search versus fixed-depth execution to determine practical efficiency gains

3. Test CoLa with larger foundation models (LLaMA-13B/30B/70B) to understand scaling behavior and whether depth adaptation benefits persist at increased model capacity