---
ver: rpa2
title: An Evaluation of Large Language Models on Text Summarization Tasks Using Prompt
  Engineering Techniques
arxiv_id: '2507.05123'
source_url: https://arxiv.org/abs/2507.05123
tags:
- summarization
- prompt
- summaries
- text
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a comprehensive evaluation of six large language
  models across four datasets for text summarization using prompt engineering techniques.
  The evaluation employs zero-shot learning and in-context learning with multiple
  prompts to assess model performance across news, dialogue, and scientific domains.
---

# An Evaluation of Large Language Models on Text Summarization Tasks Using Prompt Engineering Techniques

## Quick Facts
- **arXiv ID**: 2507.05123
- **Source URL**: https://arxiv.org/abs/2507.05123
- **Reference count**: 40
- **Primary result**: Comprehensive evaluation of six LLMs across four datasets for text summarization using prompt engineering techniques, revealing performance variations based on dataset characteristics, prompt design, and context length.

## Executive Summary
This study presents a comprehensive evaluation of six large language models across four datasets for text summarization using prompt engineering techniques. The evaluation employs zero-shot learning and in-context learning with multiple prompts to assess model performance across news, dialogue, and scientific domains. Key findings reveal that model performance varies significantly based on dataset characteristics, prompt design, and context length. For long scientific documents, a sentence-based chunking strategy improves summarization quality. Mixtral-8x7B-Instruct-v0.1 and Llama-2-70b-chat demonstrate strong overall performance, with in-context learning generally outperforming zero-shot approaches.

## Method Summary
The study evaluates six large language models (GPT-3.5, GPT-4, Llama-2, Mixtral, Claude-2, and Gemini-Pro) across four summarization datasets using both zero-shot and in-context learning approaches. The evaluation includes news articles from CNN/DailyMail and SAMSum, scientific papers from ArXiv, and dialogue data from SamSum. Multiple prompt engineering strategies are employed, including standard, title-based, and question-answering prompts. For long scientific documents, a sentence-based chunking strategy is implemented to handle context length limitations. Performance is measured using ROUGE metrics, and the study examines how different prompt designs and learning approaches affect summarization quality across various document types and lengths.

## Key Results
- Model performance varies significantly based on dataset characteristics, with news and dialogue domains showing different optimal approaches
- In-context learning generally outperforms zero-shot approaches across most datasets and models
- Mixtral-8x7B-Instruct-v0.1 and Llama-2-70b-chat demonstrate strong overall performance across diverse summarization tasks
- Sentence-based chunking strategy effectively improves summarization quality for long scientific documents

## Why This Works (Mechanism)
This approach works because prompt engineering enables LLMs to better understand the summarization task context and generate more relevant outputs. In-context learning provides few-shot examples that guide the model's behavior, while different prompt designs help frame the task appropriately for different document types. The sentence-based chunking strategy addresses the fundamental limitation of context windows in LLMs when processing long documents, allowing models to maintain coherence while generating summaries of extensive scientific papers.

## Foundational Learning
- **Prompt Engineering**: The practice of designing effective prompts to guide LLM behavior, essential for achieving desired outputs without model fine-tuning. Quick check: Test multiple prompt variations to identify which elicits the best responses for your specific task.
- **Zero-shot vs. In-context Learning**: Zero-shot learning relies solely on model pretraining, while in-context learning uses few-shot examples to guide responses. Quick check: Compare both approaches on your task to determine which yields better results.
- **ROUGE Metrics**: Evaluation metrics that measure overlap between generated and reference summaries, commonly used for summarization quality assessment. Quick check: Calculate multiple ROUGE variants (F1, precision, recall) for comprehensive evaluation.
- **Context Window Management**: The practice of handling documents longer than an LLM's maximum context length through strategies like chunking or summarization. Quick check: Implement chunking for long documents and compare summary quality with full-context approaches.
- **Dataset Domain Specificity**: The observation that model performance varies significantly across different document types and domains. Quick check: Test models on multiple datasets to understand domain-specific performance patterns.
- **Model Architecture Considerations**: Different LLM architectures (decoder-only, encoder-decoder, hybrid) may perform differently on summarization tasks. Quick check: Compare models with different architectures to identify which works best for your specific use case.

## Architecture Onboarding
The component map follows this flow: **Document Input -> Preprocessing/Chunking -> Prompt Engineering -> Model Selection -> Generation -> ROUGE Evaluation**. The critical path is: **Document Input -> Prompt Engineering -> Model Selection -> Generation -> Evaluation**, as this sequence determines the quality of the final summary. Design tradeoffs include choosing between prompt complexity and model capabilities, balancing evaluation comprehensiveness with computational cost, and selecting between zero-shot simplicity and in-context learning effectiveness. Failure signatures include poor ROUGE scores indicating prompt ineffectiveness, model-specific performance drops suggesting architectural limitations, and chunking artifacts revealing context window constraints. Three first experiments: (1) Compare zero-shot vs. in-context learning on a single dataset, (2) Test different prompt designs on the same model, (3) Evaluate chunking strategies for long documents.

## Open Questions the Paper Calls Out
None

## Limitations
- The study relies exclusively on ROUGE metrics, which may not capture semantic coherence, factual accuracy, or user satisfaction
- Results are based on a limited set of six model families without examining newer or domain-specific models
- Prompt engineering strategies represent a limited set of approaches and may not capture the full space of effective designs
- The sentence-based chunking strategy for scientific documents was only validated on one dataset type

## Confidence
- **High**: In-context learning generally outperforms zero-shot approaches
- **Medium**: Specific performance rankings of models across datasets
- **Low**: Generalizability of the sentence-based chunking strategy for scientific documents

## Next Checks
1. Conduct human evaluation studies to validate ROUGE-based findings and assess summary quality dimensions like factual consistency and readability
2. Test the prompt engineering strategies and chunking approach across additional domains and document types to assess generalizability
3. Compare performance against task-specific summarization models and smaller fine-tuned models to establish the practical value proposition of large language models for different use cases