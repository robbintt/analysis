---
ver: rpa2
title: 'HEAD-QA v2: Expanding a Healthcare Benchmark for Reasoning'
arxiv_id: '2511.15355'
source_url: https://arxiv.org/abs/2511.15355
tags:
- questions
- spanish
- arxiv
- question
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HEAD-QA v2 is an expanded multilingual benchmark for healthcare
  reasoning, growing from 6,765 to 12,751 questions across ten years of Spanish medical
  exams. It introduces new English, Italian, Galician, and Russian versions, and evaluates
  multiple open-source LLMs using prompting, retrieval-augmented generation, and probability-based
  answer selection.
---

# HEAD-QA v2: Expanding a Healthcare Benchmark for Reasoning

## Quick Facts
- arXiv ID: 2511.15355
- Source URL: https://arxiv.org/abs/2511.15355
- Reference count: 0
- Primary result: Scale-driven reasoning outperforms complex inference strategies in multilingual healthcare QA

## Executive Summary
HEAD-QA v2 expands the original healthcare benchmark from 6,765 to 12,751 questions across ten years of Spanish medical exams. The benchmark introduces English, Italian, Galician, and Russian translations, enabling multilingual evaluation of medical reasoning capabilities. Testing multiple open-source LLMs reveals that model scale is the primary driver of performance, with Llama-3.1-70B achieving ~84% accuracy in English, while complex inference strategies like Chain-of-Thought and Retrieval-Augmented Generation provide minimal or negative benefits.

## Method Summary
The benchmark evaluates four instruction-tuned LLMs (Llama-3.1-8B/70B, Mistral-7B, Mixtral-8x7B) using zero-shot, few-shot, and Chain-of-Thought prompting, plus Retrieval-Augmented Generation with MedCPT retriever and 18 medical textbooks. Models must output JSON {Answer: N} format. Performance is measured via accuracy, normalized exam score (3 wrong answers cancel 1 correct), and unanswered ratio. Spanish questions are machine-translated to English using Llama-3.1-8B.

## Key Results
- Model scale strongly predicts performance: Llama-3.1-70B achieves ~84% accuracy vs ~45-60% for smaller models
- Complex inference strategies (CoT, RAG) provide limited or negative gains compared to simple zero-shot prompting
- Generation-based answer selection outperforms probability-based selection due to comparative reasoning capability
- Translation quality is high with strong back-translation scores across all language versions

## Why This Works (Mechanism)

### Mechanism 1: Scale-Driven Intrinsic Reasoning
Larger dense models (e.g., 70B parameters) compress vast amounts of factual medical knowledge during pre-training, enabling direct recall and synthesis of complex diagnostic information. This intrinsic capacity reduces reliance on external retrieval or step-by-step decomposition that smaller models struggle to execute effectively.

### Mechanism 2: Inference Strategy Interference
Chain-of-Thought and RAG offer negligible gains because they introduce noise that distracts from models' strong internal priors. Retrieved passages may be tangentially relevant but lack precise context needed for exam logic, while forcing smaller models to generate reasoning steps can increase hallucination rates.

### Mechanism 3: Comparative vs. Independent Probability Selection
Generation-based answer selection (outputting `{"Answer": 3}`) outperforms probability-based selection because it enables comparative reasoning through self-attention across all options simultaneously, while probability-based selection evaluates each option independently, preventing elimination-based reasoning.

## Foundational Learning

- **Concept: In-Context Learning (ICL) vs. Zero-Shot**
  - Why needed here: Understanding ICL is crucial to interpreting why few-shot examples offered limited additional benefitâ€”the models were already instruction-tuned
  - Quick check question: If I add 3 examples to the prompt and performance stays flat, does that imply the model already knew the task format or the examples were poor?

- **Concept: Mixture-of-Experts (MoE)**
  - Why needed here: To interpret Mixtral results, you must distinguish between "total parameters" (47B) and "active parameters" (13B)
  - Quick check question: Why might an MoE model struggle with "comparative reasoning" in probability selection compared to a dense model?

- **Concept: Hallucination in Medical NLP**
  - Why needed here: The failure of RAG implies the model's internal hallucinations were less damaging than the "context noise" introduced by retrieval
  - Quick check question: If RAG lowers accuracy, is the model hallucinating less, or just failing to use the evidence?

## Architecture Onboarding

- **Component map:** HEAD-QA v2 dataset (12,751 questions) -> Preprocessor -> Models (Mistral-7B, Llama-8B/70B, Mixtral-8x7B) -> JSON Parser -> Evaluator (Accuracy, Exam Score, Unanswered Ratio)

- **Critical path:**
  1. Preprocessing: Ensure chemical formulas are in SMILES notation
  2. Translation (if using English): Use provided v2 translations; do not re-translate
  3. Inference: Use Zero-Shot prompting first as the strongest baseline
  4. Parsing: Strictly enforce JSON output parsing `{Answer: int}`

- **Design tradeoffs:**
  - RAG vs. Zero-Shot: RAG adds latency and complexity but decreases performance in this domain. Do not use RAG for HEAD-QA v2 baselines.
  - CoT vs. Direct: CoT increases token usage and latency. It increases "Unanswered Ratio" for smaller models. Use only for largest models (70B+) if interpretability is required.

- **Failure signatures:**
  - High Unanswered Ratio: Using CoT on a small model (<70B) or model with poor instruction following
  - Low Accuracy (~40-50%): Using "Probability-based Selection" method
  - Format Errors: Translation pipeline or CoT prompt causing model to output reasoning text instead of JSON

- **First 3 experiments:**
  1. Establish the Scale Baseline: Run Zero-Shot inference on Llama-3.1-8B vs. 70B to confirm ~20-point accuracy gap
  2. Test the "Comparison" Hypothesis: Implement Probability-based selection on Llama-3.1-8B to confirm accuracy drop to ~45%
  3. Stress Test RAG: Implement RAG pipeline and verify performance drops or stays flat compared to Zero-Shot

## Open Questions the Paper Calls Out

- Why does chain-of-thought prompting reduce accuracy in healthcare multiple-choice QA when it improves performance in other domains? The paper documents this unexpected phenomenon but does not investigate whether it stems from healthcare domain specificity, question format, or prompt design.

- How does in-context example selection influence healthcare QA performance, and can optimized selection close the gap with zero-shot baselines? Only three fixed examples from USMLE were tested; no systematic study of example relevance, diversity, or quantity was conducted.

- Can proprietary frontier LLMs (GPT-4, Claude, Gemini) achieve higher accuracy through RAG or CoT on HEAD-QA v2, or do they exhibit similar strategy insensitivity? Scaling trends from 7B to 70B suggest size matters, but the ceiling for proprietary models remains untested.

## Limitations

- Translation quality uncertainty: Exact BLEU or similar scores are not reported, leaving the translation pipeline's impact on reasoning accuracy across languages unclear.

- RAG implementation ambiguity: The exact prompt template for integrating retrieved passages is not disclosed, limiting reproducibility and making it difficult to distinguish whether poor RAG performance stems from retrieval quality or integration issues.

- Few-shot prompt variability: The paper states few-shot examples were "adapted from USMLE" but provides no examples or ablation studies, making it impossible to determine whether limited benefits reflect true model capability or suboptimal example selection.

## Confidence

- **High Confidence:** The core finding that model scale (Llama-3.1-70B vs. 8B/7B) is the primary driver of accuracy is well-supported by comparative results in Table 2.

- **Medium Confidence:** The conclusion that inference strategies (CoT, RAG) provide limited benefit is reasonable but contingent on specific implementation details not fully disclosed.

- **Low Confidence:** The translation quality assessment lacks quantitative metrics; claims about multilingual performance parity are based on indirect evidence.

## Next Checks

1. Verify scale-driven performance gap: Replicate zero-shot accuracy comparison between Llama-3.1-8B and Llama-3.1-70B on Spanish subset to confirm ~20-point difference.

2. Test probability-based selection failure: Implement log-probability answer selection method on Llama-3.1-8B and verify accuracy drops to ~45%.

3. Validate RAG underperformance: Implement RAG pipeline (MedCPT retriever, top-2 passages) and confirm accuracy either drops or remains statistically indistinguishable from zero-shot.