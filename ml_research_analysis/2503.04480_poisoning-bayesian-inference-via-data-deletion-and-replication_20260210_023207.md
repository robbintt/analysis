---
ver: rpa2
title: Poisoning Bayesian Inference via Data Deletion and Replication
arxiv_id: '2503.04480'
source_url: https://arxiv.org/abs/2503.04480
tags:
- posterior
- data
- attack
- bayesian
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a framework for poisoning attacks on generic
  Bayesian inference by strategically deleting and replicating true observations to
  steer the posterior toward a target distribution. The authors formulate the attacker's
  problem as a stochastic integer program with an intractable objective, then develop
  gradient-based heuristics including SGD-based rounded relaxation and integer-steps
  coordinate descent variants.
---

# Poisoning Bayesian Inference via Data Deletion and Replication

## Quick Facts
- arXiv ID: 2503.04480
- Source URL: https://arxiv.org/abs/2503.04480
- Reference count: 40
- Primary result: Demonstrates that strategically deleting and replicating data points can poison Bayesian inference with minimal data manipulation

## Executive Summary
This paper introduces a framework for poisoning attacks on Bayesian inference by strategically deleting and replicating observations to steer the posterior toward a target distribution. The authors formulate the attacker's problem as a stochastic integer program with an intractable objective, then develop gradient-based heuristics including SGD-based rounded relaxation and integer-steps coordinate descent variants. They prove convexity of the objective and convergence properties under certain conditions. Empirical results on synthetic and real datasets show the attacks are highly effective, requiring manipulation of only a small fraction of data to achieve significant posterior shifts.

## Method Summary
The paper proposes poisoning attacks on Bayesian inference through data deletion and replication. The attacker selects which observations to delete or replicate to minimize the forward Kullback-Leibler divergence between the target posterior and the poisoned posterior. The problem is formulated as a stochastic integer program where the objective involves intractable expectations, which are estimated via sampling from both the current and target posteriors. The authors develop six heuristics including first-order and second-order gradient-based methods (FGSM, SGD-R2, Adam-R2, 2O-R2) and integer-steps coordinate descent variants (1O-ISCD, 2O-ISCD) to solve this optimization problem.

## Key Results
- Attacks are highly effective: manipulating only 0.12% of microcredit data reversed the inferred treatment effect
- In Boston housing data, ~7% data manipulation shifted a key coefficient's posterior mean from 3.2 to 0.8
- The methods are transferable across different priors, with more informative priors offering greater robustness
- High-leverage outliers are preferentially targeted for deletion or replication to maximize posterior shift

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The attacker can compute unbiased gradient estimates to optimize which data points to delete or replicate
- Mechanism: The objective function's gradient with respect to the weight vector w is the difference between two expectations: Eπw(θ|X)[fX(θ)] − EπA(θ)[fX(θ)], where fX(θ) contains log-likelihoods for each data point. By sampling from both the current tainted posterior and the adversarial target, the attacker obtains unbiased gradient estimates without needing a closed-form objective
- Core assumption: The attacker has white-box knowledge of the model (likelihood function) and can sample from both posteriors, typically via MCMC
- Break condition: If MCMC sampling fails to converge or produces highly correlated samples, gradient estimates become biased and optimization degrades

### Mechanism 2
- Claim: The objective function is convex, enabling projected SGD to converge to the continuous relaxation's optimum
- Mechanism: The posterior induced by weights w belongs to an exponential family with natural parameter w. The Hessian of the log-partition function equals the covariance of sufficient statistics, which is positive semi-definite by construction, guaranteeing convexity
- Core assumption: The covariance matrix of log-likelihoods under πw(θ|X) has eigenvalues bounded below by c > 0 for strong convexity
- Break condition: If the log-likelihoods are degenerate (e.g., constant across data points), the covariance matrix becomes singular and strong convexity fails

### Mechanism 3
- Claim: High-leverage outliers are preferentially targeted for deletion or replication to maximize posterior shift
- Mechanism: Data points with extreme covariate values or large residuals contribute disproportionately to the log-likelihood gradients. The gradient-based selection naturally identifies these high-influence points, making noisy datasets easier to attack
- Core assumption: The dataset contains heterogeneity in leverage points; clean, low-variance data limits attack effectiveness
- Break condition: If the dataset has uniform leverage (e.g., balanced experimental design), attacks require more manipulations for equivalent posterior shifts

## Foundational Learning

- Concept: **Kullback-Leibler Divergence (Forward vs Reverse)**
  - Why needed here: The paper uses forward KL(πA || πw) specifically because its gradient requires only sampling from πA, not evaluating its density. Reverse KL would require evaluating πA(θ), which may be unavailable
  - Quick check question: Why can't the attacker use reverse KL divergence if they only have sampling access to the adversarial posterior?

- Concept: **Exponential Families and Natural Parameters**
  - Why needed here: The poisoned posterior πw(θ|X) belongs to an exponential family with weight vector w as the natural parameter. This structure guarantees convexity and provides the gradient/Hessian forms used in optimization
  - Quick check question: What property of exponential families ensures the objective function's convexity in w?

- Concept: **Integer Program Relaxation and Rounding**
  - Why needed here: The R2 family solves a continuous relaxation then rounds to integer weights. The constrained rounding problem has a closed-form solution via the procedure in Proposition 4
  - Quick check question: When rounding a continuous solution w to integer w*, how does the algorithm decide which coordinates to round up versus down?

## Architecture Onboarding

- Component map:
  Gradient Estimator -> MCMC Wrapper -> R2 Optimizer Core -> Rounding Module -> ISCD Search

- Critical path:
  1. Initialize w = 1 (no manipulation)
  2. Draw samples from current πw and target πA
  3. Compute gradient estimate ĝ
  4. Update w via chosen optimizer (SGD-R2, Adam-R2, 2O-R2, or ISCD step)
  5. Repeat until convergence or max iterations
  6. If R2 method: solve constrained rounding to obtain integer w*

- Design tradeoffs:
  - FGSM vs ISCD vs R2: FGSM is fastest (single gradient step) but produces higher KL divergence; ISCD maintains integrality throughout but converges slowly; R2 converges fast but rounding degrades solution quality at large B
  - First-order vs second-order: Second-order (using Hessian estimates) improves convergence but requires larger sample sizes P, Q for stable covariance estimation
  - Sample size (P, Q): Larger samples reduce gradient variance but increase per-iteration MCMC cost; empirical results suggest P=Q≈100–500 depending on model complexity
  - Budget B: Larger B allows more posterior shift but increases rounding error for R2 methods and search time for ISCD

- Failure signatures:
  - KL divergence increases with B (R2 methods): Rounding error dominates; switch to ISCD or reduce B
  - MCMC fails to converge: Posterior becomes degenerate under extreme w; check L∞ constraint and reduce max replications L
  - Gradient estimates have high variance: Increase sample sizes P, Q or use antithetic sampling
  - Attack transfers poorly across priors: Gray-box misspecification; verify prior match between attack construction and target model

- First 3 experiments:
  1. Synthetic linear regression (NIG prior): n=100, B∈{5,55}, target β₁→0. Compare FGSM, ISCD, and R2 on KL divergence and posterior mean shift. Verify convexity and convergence claims
  2. Boston Housing with horseshoe prior: n=404, target β_RM→0. Test gray-box transfer: compute attack assuming NIG prior, evaluate against horseshoe. Measure robustness of informative priors
  3. Microcredit RCT: n=16,560, B=20, target positive treatment effect. Validate scalability and quantify minimal manipulation needed to reverse policy conclusions

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What robust defense mechanisms can effectively detect or mitigate surgical poisoning attacks on Bayesian inference without discarding genuine duplicate observations?
- **Basis in paper:** [explicit] The authors note that while removing duplicates seems like a simple defense, real-world data often contains valid duplicates. They conclude by highlighting "the need for more research on robust defenses against adversarial manipulation in Bayesian inference"
- **Why unresolved:** The paper focuses entirely on formulating and executing attacks (optimization heuristics) rather than developing defensive strategies or robust inference modifications
- **What evidence would resolve it:** A proposed modification to Bayesian update rules or a pre-processing diagnostic tool that identifies malicious replications while retaining a high true-positive rate for valid data

### Open Question 2
- **Question:** Can these poisoning heuristics be successfully adapted to variational inference (VI) to scale attacks against complex, high-dimensional Bayesian models?
- **Basis in paper:** [explicit] The conclusion states, "extending our approach to larger, more complex Bayesian models (e.g., hierarchical, spatial, spatio-temporal) is crucial" and suggests "integrating variational inference methods could enable applications to larger models efficiently"
- **Why unresolved:** The current framework relies on MCMC sampling to estimate gradients, which becomes computationally prohibitive for very large or complex models
- **What evidence would resolve it:** A derivation of the gradient estimator within a VI framework and empirical results showing attack efficacy on hierarchical or spatial models comparable to the MCMC results

### Open Question 3
- **Question:** How can adversarial posteriors be systematically designed using techniques like entropic tilting to optimize specific malicious goals?
- **Basis in paper:** [explicit] The authors state that current attacks "require a fully specified adversarial posterior" and suggest "Future work may thus study how to design adversarial posteriors to achieve specific goals. Ideas such as entropic tilting might be relevant"
- **Why unresolved:** The paper assumes the attacker has already selected a target distribution $\pi_A(\theta)$; it does not explore the meta-problem of optimizing the target itself for maximum impact or stealth
- **What evidence would resolve it:** An algorithmic framework that outputs the optimal target posterior parameters (e.g., for flipping a sign in a specific coefficient) without requiring the attacker to manually specify the full distribution

## Limitations

- The paper assumes white-box access to model likelihood functions and black-box sampling access to the target posterior, which may be overly optimistic in practice
- Convergence guarantees depend on strong convexity of the objective, which may fail in degenerate or high-dimensional settings
- The R2 rounding procedure introduces a hard combinatorial constraint that can degrade solution quality at high manipulation budgets B
- The attack's effectiveness relies heavily on the presence of high-leverage outliers; in balanced or low-variance datasets, much larger budgets may be required

## Confidence

- **High confidence**: Convexity of the objective function and convergence of projected SGD under strong convexity assumptions (supported by analytical proofs and synthetic experiments)
- **Medium confidence**: Effectiveness of gradient-based optimization for identifying high-leverage data points and achieving targeted posterior shifts in real-world datasets (empirical results are compelling but depend on dataset-specific leverage structures)
- **Low confidence**: Robustness of attacks under gray-box conditions (prior misspecification) and scalability to extremely high-dimensional problems (theoretical analysis limited to linear regression)

## Next Checks

1. **Stress-test convexity assumptions**: Systematically vary dataset dimensionality and leverage point distribution to identify conditions where Hessian eigenvalues approach zero, causing optimization failure
2. **Evaluate attack under prior misspecification**: Conduct controlled experiments varying the gap between the attack's assumed prior and the actual target model's prior to quantify robustness degradation
3. **Analyze rounding error scaling**: Characterize how the L2 gap between continuous and integer solutions scales with manipulation budget B, and test adaptive switching between R2 and ISCD methods based on this metric