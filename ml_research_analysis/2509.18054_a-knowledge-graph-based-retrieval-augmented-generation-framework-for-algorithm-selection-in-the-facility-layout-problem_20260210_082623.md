---
ver: rpa2
title: A Knowledge Graph-based Retrieval-Augmented Generation Framework for Algorithm
  Selection in the Facility Layout Problem
arxiv_id: '2509.18054'
source_url: https://arxiv.org/abs/2509.18054
tags:
- problem
- algorithm
- knowledge
- search
- evidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a Knowledge Graph-based Retrieval-Augmented\
  \ Generation (KG-RAG) framework for algorithm selection in the Facility Layout Problem\
  \ (FLP), an NP-hard optimization problem with complex multi-objective trade-offs.\
  \ The method constructs a domain-specific knowledge graph from literature, then\
  \ uses a multifaceted retrieval mechanism\u2014graph-based, vector-based, and cluster-based\u2014\
  to gather evidence, which is processed by a Large Language Model (LLM) to generate\
  \ explainable, data-driven recommendations."
---

# A Knowledge Graph-based Retrieval-Augmented Generation Framework for Algorithm Selection in the Facility Layout Problem

## Quick Facts
- arXiv ID: 2509.18054
- Source URL: https://arxiv.org/abs/2509.18054
- Reference count: 21
- Achieved average reasoning score of 4.7/5, outperforming baseline Gemini 1.5 Flash chatbot (3.3/5)

## Executive Summary
This paper introduces a Knowledge Graph-based Retrieval-Augmented Generation (KG-RAG) framework designed to improve algorithm selection for the Facility Layout Problem (FLP), a complex NP-hard optimization challenge. By constructing a domain-specific knowledge graph from FLP literature and employing a hybrid retrieval mechanism—graph-based, vector-based, and cluster-based—the framework enables a Large Language Model to generate explainable, data-driven recommendations. Evaluated across six FLP cases, the KG-RAG framework significantly outperformed a baseline chatbot, demonstrating enhanced precision, interpretability, and reduced hallucinations. The method supports continuous learning via user feedback, improving long-term accuracy and relevance.

## Method Summary
The KG-RAG framework integrates knowledge graph construction with a multifaceted retrieval strategy to assist algorithm selection for the Facility Layout Problem. A domain-specific knowledge graph is curated from literature, capturing relationships among FLP algorithms, problem types, and objectives. The hybrid retrieval mechanism—comprising graph-based, vector-based, and cluster-based components—gathers comprehensive evidence from this graph. This evidence is processed by a Large Language Model to generate explainable, data-driven recommendations. The framework is designed to reduce LLM hallucinations and improve interpretability, while also supporting continuous learning through user feedback mechanisms.

## Key Results
- KG-RAG achieved an average reasoning score of 4.7/5, outperforming baseline Gemini 1.5 Flash (3.3/5)
- Hybrid retrieval (graph, vector, cluster-based) enabled precise, traceable recommendations in complex FLP scenarios
- Structured KG and multi-retrieval approach reduced hallucinations and improved interpretability

## Why This Works (Mechanism)
The framework leverages a structured knowledge graph to ground LLM reasoning in verifiable domain knowledge, while the hybrid retrieval mechanism ensures comprehensive and context-aware evidence gathering. This combination reduces reliance on LLM internal knowledge, minimizing hallucinations and improving recommendation traceability. The multi-retrieval strategy captures different facets of the problem space, enabling nuanced, data-driven selections tailored to specific FLP characteristics.

## Foundational Learning
- **Knowledge Graph Construction**: Captures algorithmic relationships and domain knowledge from literature; needed to provide structured, traceable context for LLM reasoning; quick check: verify graph completeness and correctness via expert review
- **Hybrid Retrieval (Graph, Vector, Cluster)**: Combines structured, semantic, and similarity-based searches; needed to ensure comprehensive evidence gathering; quick check: ablation study to measure impact of each retrieval type
- **Large Language Model Integration**: Processes retrieved evidence into human-readable recommendations; needed for explainability and user trust; quick check: compare reasoning scores with and without KG context

## Architecture Onboarding

**Component Map**
Knowledge Graph Construction -> Hybrid Retrieval (Graph + Vector + Cluster) -> LLM Processing -> Recommendation Output

**Critical Path**
Literature curation → Knowledge graph building → Hybrid retrieval execution → LLM evidence processing → Recommendation generation

**Design Tradeoffs**
- Manual literature curation ensures high-quality, domain-specific knowledge but risks selection bias and limits scalability
- Hybrid retrieval increases precision and robustness but adds complexity and computational overhead
- Continuous learning via user feedback promises long-term accuracy but requires ongoing data curation and bias monitoring

**Failure Signatures**
- Sparse or biased knowledge graph leads to incomplete or skewed recommendations
- Retrieval failure (e.g., missing relevant algorithms) results in inaccurate LLM outputs
- LLM hallucinations persist if retrieved evidence is insufficient or misinterpreted

**3 First Experiments**
1. Construct and validate a small-scale knowledge graph for a specific FLP variant
2. Test hybrid retrieval accuracy by comparing evidence coverage with and without each retrieval type
3. Evaluate LLM recommendation quality using expert scoring on curated FLP cases

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation sample size limited to six FLP cases, raising generalizability concerns
- Knowledge graph relies on manually curated literature, risking selection bias and restricted coverage
- Relative contribution of each retrieval component not quantified
- Continuous learning mechanism not empirically validated over extended use

## Confidence
- High: Retrieval accuracy and explainability improvements
- Medium: LLM-generated recommendations
- Low: Long-term learning effectiveness

## Next Checks
1. Expand evaluation to 20+ diverse FLP instances and at least two other optimization problem classes to test domain transferability
2. Conduct ablation studies to isolate the impact of each retrieval component (graph, vector, cluster) on recommendation quality
3. Implement and test the continuous learning pipeline with real user feedback over multiple iterations to assess adaptation accuracy and bias mitigation