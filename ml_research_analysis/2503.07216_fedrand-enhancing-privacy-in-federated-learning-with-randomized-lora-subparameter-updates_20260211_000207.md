---
ver: rpa2
title: 'FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA Subparameter
  Updates'
arxiv_id: '2503.07216'
source_url: https://arxiv.org/abs/2503.07216
tags:
- server
- client
- parameters
- lora
- fedrand
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of membership inference attacks
  (MIAs) in federated learning (FL) when training vision-language models (VLMs), where
  attackers can exploit exposed client parameters to detect whether specific data
  instances were used in training. The core method idea is FedRand, a privacy-enhanced
  FL framework where each client randomly selects subparameters of Low-Rank Adaptation
  (LoRA) weights from the server and keeps the remaining counterparts as private parameters.
---

# FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA Subparameter Updates

## Quick Facts
- **arXiv ID:** 2503.07216
- **Source URL:** https://arxiv.org/abs/2503.07216
- **Reference count:** 9
- **Primary result:** FedRand achieves BLEU-4 31.89 on MSCOCO and 80.12% accuracy on ScienceQA while improving MIA robustness (MaxRényi-10% score 66.61% vs 70.22% for FedAvg)

## Executive Summary
This paper addresses membership inference attacks (MIAs) in federated learning (FL) when training vision-language models (VLMs), where attackers can exploit exposed client parameters to detect whether specific data instances were used in training. The core method, FedRand, introduces a privacy-preserving FL framework where each client randomly selects subparameters of Low-Rank Adaptation (LoRA) weights from the server and keeps the remaining counterparts as private parameters. After training both parameters on the client's private dataset, only the non-private parameters are sent back to the server for aggregation, preventing full exposure of client model parameters. The approach achieves performance comparable to FedAvg (an oracle method that communicates full LoRA parameters) while significantly improving robustness against MIAs and reducing communication costs by approximately 25% per round.

## Method Summary
FedRand is a privacy-enhanced FL framework that leverages randomized LoRA subparameter updates to defend against membership inference attacks. The method works by having each client randomly select either the LoRA A or B matrix from the server, initializing the unselected matrix from their local cache, and training both matrices on their private dataset. Only the selected matrix is uploaded to the server for aggregation, while the private matrix remains on the client. The server aggregates received matrices using weighted averaging with specific normalization factors to handle partial participation. This approach maintains task performance comparable to FedAvg while significantly improving privacy by limiting parameter exposure and reducing communication overhead.

## Key Results
- FedRand achieves BLEU-4 score of 31.89 on MSCOCO image captioning task
- ScienceQA accuracy of 80.12% demonstrates strong vision-language understanding
- MaxRényi-10% MIA score of 66.61% shows significant improvement over FedAvg (70.22%)
- Communication costs reduced by approximately 25% per round compared to FedAvg
- Performance comparable to FedAvg while providing stronger privacy guarantees

## Why This Works (Mechanism)
The FedRand mechanism works by creating uncertainty for potential attackers through partial parameter exposure. By randomly selecting which LoRA subparameters to share with the server, clients ensure that no single entity receives complete information about their model updates. This randomization makes it significantly harder for attackers to perform membership inference attacks that rely on analyzing full parameter updates to detect whether specific data instances were used in training. The approach maintains task performance by ensuring that both the shared and private parameters are trained on the client's local data, preventing performance degradation while the server can still aggregate useful information from the partially exposed parameters.

## Foundational Learning
**Federated Learning (FL):** A distributed machine learning paradigm where multiple clients train models collaboratively without sharing their raw data. *Why needed:* Provides the foundation for privacy-preserving collaborative training across distributed clients. *Quick check:* Verify understanding of client-server architecture and data locality principles.

**Membership Inference Attacks (MIAs):** Attacks that attempt to determine whether specific data instances were used in training a machine learning model by analyzing model outputs or parameters. *Why needed:* The primary threat model that FedRand aims to defend against. *Quick check:* Understand the relationship between parameter exposure and privacy vulnerability.

**Low-Rank Adaptation (LoRA):** A parameter-efficient fine-tuning method that decomposes weight updates into low-rank matrices, significantly reducing the number of trainable parameters. *Why needed:* Enables the subparameter selection mechanism central to FedRand's privacy approach. *Quick check:* Verify understanding of how LoRA matrices A and B are used in model updates.

**Rényi Entropy:** A measure of uncertainty or randomness in a probability distribution, used in this paper to quantify the information leakage in MIA evaluation. *Why needed:* Provides the theoretical foundation for the MaxRényi-10% privacy metric. *Quick check:* Understand how entropy relates to membership inference difficulty.

## Architecture Onboarding

**Component Map:** Clients -> Server -> Global Model, with random LoRA subparameter selection at client level and weighted aggregation at server level.

**Critical Path:** Client initialization -> Random LoRA selection -> Local training -> Parameter upload -> Server aggregation -> Global model update.

**Design Tradeoffs:** Privacy vs. performance (FedRand maintains performance while improving privacy), communication efficiency vs. parameter exposure (reduced communication but partial parameter sharing), randomness vs. convergence stability (random selection introduces variance but doesn't compromise final accuracy).

**Failure Signatures:** Aggregation collapse (loss explosion or random accuracy), stale private parameters (poor convergence), high MIA AUROC (privacy defense failure), incorrect normalization factors (weighted averaging errors).

**First Experiments:** 1) Verify that exactly half of LoRA parameters are communicated per round vs FedAvg baseline, 2) Reproduce MaxRényi-10% MIA evaluation with exact member/non-member samples, 3) Run ablation test removing random selection to confirm privacy improvements are from randomization mechanism.

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation focuses on specific attack types and may not capture all privacy vulnerabilities
- Implementation requires careful handling of client-side caching mechanisms for private parameters
- The approach's effectiveness may depend on specific model architectures and task domains
- Technical ambiguities exist around LoRA target modules and entropy order parameters

## Confidence

**High confidence:** Core methodology and overall results are clearly presented and validated through ablation studies
**Medium confidence:** Exact implementation details require assumptions about LoRA target modules and entropy order
**Medium confidence:** Generalizability appears reasonable but results are demonstrated on specific datasets and model architecture

## Next Checks

1. **Implementation verification:** Implement client-side random selection logic and server aggregation with normalization factors, then verify that exactly half of LoRA parameters are communicated per round compared to FedAvg baseline.

2. **Privacy evaluation reproduction:** Reproduce the MaxRényi-10% MIA evaluation using the exact same member (MSCOCO) and non-member (NoCaps) sample sets, and verify the reported AUROC improvements of approximately 4 percentage points over FedAvg.

3. **Ablation testing:** Run controlled experiments removing the random selection component (communicating full LoRA) to confirm that the observed privacy improvements are specifically due to the FedRand randomization mechanism rather than other experimental factors.