---
ver: rpa2
title: 'DocMIA: Document-Level Membership Inference Attacks against DocVQA Models'
arxiv_id: '2502.03692'
source_url: https://arxiv.org/abs/2502.03692
tags:
- docvqa
- attacks
- document
- training
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DocMIA, the first document-level membership
  inference attacks for DocVQA models. The authors propose two novel attacks tailored
  for white-box and black-box settings, without requiring auxiliary datasets.
---

# DocMIA: Document-Level Membership Inference Attacks against DocVQA Models

## Quick Facts
- arXiv ID: 2502.03692
- Source URL: https://arxiv.org/abs/2502.03692
- Authors: Khanh Nguyen; Raouf Kerkouche; Mario Fritz; Dimosthenas Karatzas
- Reference count: 34
- Primary result: First document-level membership inference attacks for DocVQA models achieving up to 82.5% F1 score against Donut and 8.67% TPR at 3% FPR against Pix2Struct-B

## Executive Summary
This paper introduces DocMIA, the first document-level membership inference attacks targeting DocVQA models. The authors propose two novel attacks tailored for white-box and black-box settings, without requiring auxiliary datasets. Their optimization-based features outperform existing state-of-the-art MIAs across multiple DocVQA models and datasets. Specifically, their attacks achieve up to 82.5% F1 score against Donut and 8.67% TPR at 3% FPR against Pix2Struct-B, demonstrating significant privacy vulnerabilities in DocVQA models.

## Method Summary
The DocMIA attack framework extracts membership signals from optimization dynamics during fine-tuning on individual (document, question, answer) triples. For white-box attacks, the method fine-tunes the target model and measures parameter changes (L2-norm distance), optimization steps, and utility scores. Features are aggregated across multiple questions per document using functions like AVG, MIN, MAX, and MED. Black-box attacks distill a proxy model from the target's predictions, then apply white-box methods to the proxy. The attack achieves high effectiveness without auxiliary datasets by exploiting the multi-query nature of DocVQA training.

## Key Results
- Optimization-based features achieve up to 82.5% F1 score against Donut and 8.67% TPR at 3% FPR against Pix2Struct-B
- FL attack variant outperforms baseline methods (LOSS-TA, GRADIENT-UA) across all tested DocVQA models
- Attack effectiveness correlates with train-test accuracy gap, achieving 74.41% F1 when gap > 0.2 and dropping to 47.37% when gap < 0.05
- Black-box attacks remain effective with up to 56.85% F1 score using proxy distillation approach

## Why This Works (Mechanism)

### Mechanism 1: Optimization-Based Distance as Membership Signal
Training documents require smaller parameter changes during fine-tuning than non-training documents because they've already influenced the optimization landscape during initial training. This creates a generalization gap where member documents converge faster with lower final loss.

### Mechanism 2: Multi-Query Aggregation Per Document
Each document appears multiple times with different questions during DocVQA training, creating consistent membership-indicative patterns across questions. Aggregating features across all questions amplifies these signals.

### Mechanism 3: Proxy Model Distillation for Black-Box Transfer
Knowledge distillation transfers membership-indicative patterns from black-box models to controllable proxy models. The proxy inherits behavioral patterns from the target, allowing white-box attack methods to be applied.

## Foundational Learning

- **Concept: Membership Inference Attacks (MIAs)**
  - Why needed here: Core threat model—determining if a data point was in training data by exploiting model behavior differences on seen vs. unseen samples.
  - Quick check question: Can you explain why models typically exhibit different loss/probability distributions on training vs. test data?

- **Concept: DocVQA Model Architectures**
  - Why needed here: Target models have different structures (OCR-based VT5 vs. OCR-free Donut/Pix2Struct) affecting which attack variants are applicable.
  - Quick check question: What are the key differences between OCR-based and OCR-free document understanding models?

- **Concept: Gradient-Based Optimization Dynamics**
  - Why needed here: Attack extracts features from optimization trajectories (distance, steps, utility evolution) during fine-tuning.
  - Quick check question: How does the learning rate affect convergence behavior, and why might training samples converge faster?

## Architecture Onboarding

- **Component map:** Target DocVQA Model -> Feature Extraction Module -> Aggregation Module -> Clustering Module -> Membership Classification
- **Critical path:** For each document: (1) optimize on each Q-A pair, (2) extract Δ/s/u features, (3) aggregate across all questions, (4) normalize features, (5) cluster to predict membership
- **Design tradeoffs:** FL (single layer fine-tuning) balances efficiency and effectiveness; IG (image gradient) works when pixel-differentiation is possible; FLLoRA adds low-rank parameters for stability
- **Failure signatures:** Small train-test accuracy gap → weaker membership signals; high loss variance across questions → no clear separation; rephrased questions → attack performance drops but remains above baseline; DP training → attack F1 drops from ~74% to ~55% with severe utility loss
- **First 3 experiments:**
  1. Run FL attack on Donut model trained on DocVQA dataset; report F1 and TPR@3%FPR; compare against LOSS-TA and GRADIENT-UA baselines
  2. Compare FL, FLLoRA, and IG on same target; measure computational cost vs. attack effectiveness tradeoffs
  3. Distill VT5 proxy from Pix2Struct-B black-box predictions; apply white-box attack to proxy; measure architecture mismatch impact

## Open Questions the Paper Calls Out

### Open Question 1
Can privacy safeguards be developed that mitigate DocMIA without causing the substantial utility degradation observed with Differential Privacy (DP-SGD)? The authors identify the trade-off between privacy and utility as a challenge but do not propose or test alternative defense mechanisms.

### Open Question 2
How does the attack performance degrade when the adversary lacks knowledge of the semantic content of the original training questions? The experiment only covers syntactic rephrasing; it does not test scenarios where the adversary poses entirely different questions or has no prior knowledge of the question type.

### Open Question 3
Do optimization-based features remain effective against modern decoder-only multimodal Large Language Models? The paper evaluates encoder-decoder architectures (Donut, Pix2Struct, VT5), but the reliance on specific gradient dynamics suggests the method may not translate directly to decoder-only models.

## Limitations
- Optimization-distance mechanism lacks ablation studies isolating the effect of convergence-speed signal from other confounding factors
- Critical details about proxy model training for black-box attacks are missing (number of epochs, learning rate, batch size)
- Specific threshold values (τ) for optimization convergence are not reported, affecting reproducibility

## Confidence

- **High confidence** in general attack framework and experimental results showing significant membership leakage in DocVQA models
- **Medium confidence** in optimization-distance mechanism explanation due to insufficient evidence about convergence-speed signal
- **Medium confidence** in black-box proxy attack effectiveness due to missing distillation procedure details

## Next Checks

1. **Ablation on convergence signal**: Compare attack performance when using optimization distance vs. using only the number of optimization steps or utility trajectory to isolate the distance metric's contribution.

2. **Threshold sensitivity analysis**: Systematically vary the optimization stopping threshold τ across multiple orders of magnitude and measure its impact on attack performance.

3. **Architecture mismatch study**: Implement black-box attacks where the proxy model architecture differs substantially from the target to test knowledge transfer limits.