---
ver: rpa2
title: 'Lost in Phonation: Voice Quality Variation as an Evaluation Dimension for
  Speech Foundation Models'
arxiv_id: '2510.25577'
source_url: https://arxiv.org/abs/2510.25577
tags:
- voice
- speech
- quality
- breathy
- creaky
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VQ-Bench, a dataset and evaluation framework
  to test how speech foundation models (SFMs) respond to voice quality variation such
  as breathy, creaky, and end-creak phonation. Using a zero-shot TTS system and voice
  quality conversion, the authors synthesized parallel speech prompts in four phonation
  types across two corpora (Buckeye and VCTK).
---

# Lost in Phonation: Voice Quality Variation as an Evaluation Dimension for Speech Foundation Models

## Quick Facts
- arXiv ID: 2510.25577
- Source URL: https://arxiv.org/abs/2510.25577
- Reference count: 0
- Key outcome: VQ-Bench dataset and framework reveals SFM sensitivity to voice quality variations, with breathy/end-creak voices eliciting more affiliative responses and creaky voice producing more reserved judgments, while female voices receive systematically lower ratings in interview tasks.

## Executive Summary
This paper introduces VQ-Bench, a dataset and evaluation framework to test how speech foundation models (SFMs) respond to voice quality variation such as breathy, creaky, and end-creak phonation. Using a zero-shot TTS system and voice quality conversion, the authors synthesized parallel speech prompts in four phonation types across two corpora (Buckeye and VCTK). They evaluated SFMs in two settings: long-form open-ended generation tasks (career advice, therapy, interviews, storytelling) and speech emotion recognition. Results showed that voice quality significantly influenced SFM outputs and emotion predictions—breathy and end-creak voices tended to elicit more affiliative or calm responses, while creaky voice often led to more reserved or authoritative judgments. Female voices were systematically rated lower in interview tasks. These findings highlight the need to account for paralinguistic variation in SFM evaluation to mitigate biases and improve real-world deployment.

## Method Summary
The authors created VQ-Bench by extracting 12-second reference audio clips from the Buckeye and VCTK corpora, then using F5-TTS to synthesize prompts in four voice qualities (modal, breathy, creaky, end-creak) via VoiceQualityVC parameter manipulation. They evaluated two SFMs (OpenAI speech-to-speech API and LFMAudio2-1.5B) on long-form generation tasks across four categories and speech emotion recognition using an SER model. Outputs were rated by gemini-2.5-flash-lite LLM judge on 12 dimensions, with statistical analysis via CLMM and Bayesian multilevel regression.

## Key Results
- Voice quality significantly affected SFM outputs across 10 of 12 evaluation dimensions in long-form tasks
- Breathy and end-creak voices elicited more affiliative/care-oriented responses; creaky voice produced more reserved judgments
- Female voices received systematically lower ratings than male voices in interview tasks for salary offers and leadership endorsement
- Voice quality also influenced speech emotion recognition predictions across all tested emotion categories

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Controlled acoustic parameter manipulation creates perceptually distinct phonation types while preserving speaker identity and linguistic content.
- Mechanism: Voice quality conversion modifies glottal source characteristics—specifically H1–H2 (spectral tilt indicator), H1–A3 (breathiness measure), and CPPS (creakiness measure)—to shift synthesized speech into modal, breathy, creaky, or end-creak variants. F5-TTS generates base prompts; VoiceQualityVC applies parameter deltas from corpus means.
- Core assumption: Acoustic parameter shifts map reliably to human-perceivable voice quality categories.
- Evidence anchors:
  - [abstract] "controlled dataset with synthesized prompts in four voice qualities, maintaining linguistic content and speaker identity while varying phonation"
  - [section 2.1.3] Table 1 shows explicit parameter modifications: Breathy uses H1–H2 = -1, H1–A3 = +3; Creaky uses VQ = 2, H1–H2 = -1.5
  - [corpus] Weak direct evidence—no corpus papers validate this specific parameter-to-perception mapping
- Break condition: If acoustic parameters do not correspond to perceptually meaningful voice quality distinctions in target population, the controlled variation collapses.

### Mechanism 2
- Claim: SFMs encode paralinguistic voice quality cues that systematically bias downstream generation and classification behavior.
- Mechanism: Raw audio input passes through SFM encoder representations. Voice quality features propagate into latent embeddings, which then condition text/speech generation. Breathy phonation increases affiliative/care-oriented output patterns; creaky phonation shifts toward reserved or authority-linked judgments.
- Core assumption: SFMs learn implicit associations between phonation types and social/affective meaning from training data, similar to human perceptual biases.
- Evidence anchors:
  - [abstract] "breathy and end-creak voices eliciting more affiliative/care-oriented responses while creaky voice produces more reserved judgments"
  - [section 4] Table 2 shows significant voice quality effects across 10 of 12 evaluation dimensions; breathy/end-creak increase STEM ratings (+), creaky increases care orientation (↓)
  - [corpus] "Speak Your Mind" paper corroborates voice-based model bias in speech continuation tasks
- Break condition: If SFM training data lacks voice quality diversity or if model architecture discards paralinguistic features early in processing, voice quality effects disappear.

### Mechanism 3
- Claim: Gender × voice quality interactions produce asymmetric model responses that mirror documented human social biases.
- Mechanism: Female voices systematically receive lower ratings on salary offers and leadership endorsement regardless of phonation. Voice quality effects overlay this baseline—e.g., creaky voice associated with negative attitudes toward female speakers in human studies shows parallel patterns in model judgments.
- Core assumption: SFMs internalize gendered social patterns present in training corpora.
- Evidence anchors:
  - [abstract] "These effects mirror documented human perceptual biases, including gender-based asymmetries"
  - [section 4] "Female voices were systematically rated lower than male voices in the interview task across both corpora, specifically for 'Salary offer' and 'Leadership endorsement'"
  - [corpus] No direct corpus evidence for this specific interaction pattern
- Break condition: If evaluation prompts or judge LLM introduce confounding factors, observed gender effects may not reflect SFM behavior.

## Foundational Learning

- **Phonation types and acoustic correlates**
  - Why needed here: Understanding what H1–H2, H1–A3, and CPPS measure is essential to interpreting how voice quality is being manipulated and whether the synthesis is valid.
  - Quick check question: Would you expect breathy voice to have higher or lower H1–H2 than modal voice? Why?

- **Speech Foundation Model input processing**
  - Why needed here: The entire paper rests on SFMs processing raw audio directly; understanding what information is preserved vs. discarded at each stage determines where voice quality effects could emerge.
  - Quick check question: At what point in an SFM pipeline would paralinguistic information be most vulnerable to loss?

- **LLM-as-judge evaluation methodology**
  - Why needed here: Results depend entirely on gemini-2.5-flash-lite ratings; judge bias or inconsistency could create or obscure real effects.
  - Quick check question: What validity threats exist when using one LLM to evaluate another model's outputs?

## Architecture Onboarding

- **Component map:** Reference audio (12s clips from Buckeye/VCTK) → F5-TTS zero-shot synthesis → VoiceQualityVC parameter modification → 4 phonation variants per prompt → LFMAudio2-1.5B (or OpenAI speech-to-speech API) receives audio prompts, generates responses → LLM judge (gemini-2.5-flash-lite) rates outputs on 12 dimensions; SER model (xlsr-en-speech-emotion-recognition) classifies 8 emotion categories

- **Critical path:**
  1. Voice quality conversion accuracy (Figures 1–2 validate H1–H2/H1–A3 separation)
  2. SFM gender detection reliability (OpenAI API failed this; only LFMAudio2 produced usable results)
  3. Judge consistency across voice quality conditions

- **Design tradeoffs:**
  - Synthetic voices enable controlled variation but may not generalize to natural speech distributions
  - Binary gender structure inherited from source corpora limits broader bias detection
  - Single SER model provides limited view of emotion recognition landscape

- **Failure signatures:**
  - OpenAI API defaulting to male classification for nearly all samples rendered it unusable
  - Non-significant interaction effects suggest underpowered analysis or true null effects
  - LLM judge may have its own voice-quality or gender biases

- **First 3 experiments:**
  1. Replicate with natural (non-synthetic) voice quality variation from corpora containing annotated phonation types to validate generalization.
  2. Ablate SFM encoder layers to identify where voice quality information is most strongly represented.
  3. Test multiple SER and judge models to distinguish SFM behavior from evaluation artifacts.

## Open Questions the Paper Calls Out
None

## Limitations
- Voice quality conversion fidelity uncertain—acoustic parameters show separation but lack perceptual validation
- Synthetic voices may not generalize to natural speech distributions
- Binary gender categorization inherited from source corpora prevents detection of non-binary bias

## Confidence
High confidence: Voice quality variation affects SFM outputs and emotion predictions (replicated across corpora, significant effects in 10/12 dimensions)
Medium confidence: Gender × voice quality interaction patterns (fewer significant interactions, possible confounding factors)
Low confidence: Generalization to natural speech (all stimuli are synthetic, perceptual validation missing)

## Next Checks
1. **Perceptual validation of voice quality synthesis**: Conduct human listening tests to confirm that acoustic parameter manipulations produce distinguishable, category-congruent voice quality perceptions before testing SFM responses.

2. **Cross-model replication with natural speech**: Apply the evaluation protocol to naturally occurring speech from corpora with annotated voice quality (e.g., Saarbruecken Voice Database) to test whether effects persist outside synthetic conditions.

3. **Ablation study of SFM encoder layers**: Identify which encoder layers retain voice quality information by systematically removing or freezing layers and measuring degradation in phonation-sensitive outputs.