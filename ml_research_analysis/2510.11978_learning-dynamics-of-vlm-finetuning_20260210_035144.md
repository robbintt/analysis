---
ver: rpa2
title: Learning Dynamics of VLM Finetuning
arxiv_id: '2510.11978'
source_url: https://arxiv.org/abs/2510.11978
tags:
- cw-dpo
- negatives
- stage
- loss
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a key instability in VLM finetuning called
  the "squeezing effect," where uninformative gradients from easy negatives destabilize
  training. To address this, the authors propose Cooling-Weighted DPO (CW-DPO), a
  two-stage method that first smooths the loss landscape using constrained supervised
  finetuning with gentle negatives, then applies preference optimization with a competence-aware
  cooling weight that suppresses gradients from easy negatives.
---

# Learning Dynamics of VLM Finetuning

## Quick Facts
- arXiv ID: 2510.11978
- Source URL: https://arxiv.org/abs/2510.11978
- Reference count: 40
- Primary result: CW-DPO achieves SOTA CIDEr score of 142.6 on COCO Test

## Executive Summary
This paper identifies a critical instability in vision-language model (VLM) finetuning called the "squeezing effect," where uninformative gradients from easy negatives destabilize training. The authors propose Cooling-Weighted DPO (CW-DPO), a two-stage method that first smooths the loss landscape using constrained supervised finetuning with gentle negatives, then applies preference optimization with a competence-aware cooling weight that suppresses gradients from easy negatives. CW-DPO achieves state-of-the-art performance on multiple vision-language benchmarks and demonstrates improved stability, better calibration, and faster convergence compared to baselines like SFT and vanilla DPO.

## Method Summary
CW-DPO addresses the squeezing effect through a two-stage approach. First, it uses constrained supervised finetuning (SFT) with gentle negatives to smooth the loss landscape and improve stability. Then, it applies preference optimization using a competence-aware cooling weight that dynamically suppresses gradients from easy negatives based on their relative likelihood compared to the ground truth. The cooling weight decreases as the model's competence increases, effectively focusing training on more informative samples and reducing the destabilizing impact of uninformative gradients.

## Key Results
- Achieves state-of-the-art CIDEr score of 142.6 on COCO Test benchmark
- Demonstrates improved stability and better calibration compared to SFT and vanilla DPO
- Shows faster convergence and reduced computational overhead through targeted gradient suppression

## Why This Works (Mechanism)
CW-DPO works by addressing the fundamental instability caused by uninformative gradients from easy negatives during VLM finetuning. The method uses a two-stage approach: first smoothing the loss landscape with constrained SFT using gentle negatives, then applying preference optimization with a competence-aware cooling weight. This cooling weight dynamically suppresses gradients from samples where the model's prediction is already close to the ground truth (easy negatives), preventing these uninformative gradients from destabilizing training. The cooling schedule ensures that as the model becomes more competent, it increasingly focuses on harder, more informative samples that drive meaningful learning.

## Foundational Learning
- **Vision-Language Models (VLMs)**: Neural networks that process both visual and textual inputs, needed to understand the context of VLM finetuning challenges
  - Why needed: The paper addresses specific issues in VLM training that don't exist in single-modality models
  - Quick check: VLMs combine image encoders with language models to generate or reason about visual content

- **Direct Preference Optimization (DPO)**: A preference-based learning method that optimizes models based on pairwise comparisons
  - Why needed: CW-DPO builds upon and modifies DPO to address instability issues
  - Quick check: DPO uses Bradley-Terry model to convert pairwise preferences into a loss function

- **Loss Landscape Smoothing**: Techniques to make optimization more stable by reducing sharp curvature in the loss function
  - Why needed: The first stage of CW-DPO uses this to improve training stability
  - Quick check: Smooth loss landscapes help optimization algorithms navigate more reliably

- **Gradient Suppression**: Dynamically reducing the impact of certain gradients during training
  - Why needed: The cooling weight mechanism specifically targets uninformative gradients for suppression
  - Quick check: This technique is used when certain samples provide little learning signal

## Architecture Onboarding

Component Map: Constrained SFT -> Competence-aware Cooling Weight -> Preference Optimization

Critical Path: Image Encoder → Text Decoder → Loss Computation → Gradient Application → Model Update

Design Tradeoffs:
- Two-stage vs single-stage training: CW-DPO trades additional complexity for improved stability and performance
- Cooling schedule parameters: Balance between early exploration and late exploitation of informative samples
- Computational overhead: Additional computation for competence assessment and cooling weight calculation

Failure Signatures:
- Over-suppression: Model fails to learn from useful negatives if cooling weight is too aggressive
- Under-suppression: Instability persists if cooling weight doesn't sufficiently filter easy negatives
- Schedule misconfiguration: Poor choice of cooling parameters leads to suboptimal convergence

First Experiments:
1. Ablation study on cooling schedule parameters (initial weight, decay rate, minimum threshold)
2. Comparison of gradient norms between CW-DPO and vanilla DPO during training
3. Evaluation of calibration metrics across different competence thresholds

## Open Questions the Paper Calls Out
None

## Limitations
- The "squeezing effect" characterization remains empirically observed rather than mathematically formalized
- CW-DPO introduces additional hyperparameters that may affect reproducibility and robustness
- Evaluation focuses on COCO and RefCOCO benchmarks, potentially limiting generalizability to other VLM applications

## Confidence
High confidence in: Empirical effectiveness on COCO/RefCOCO benchmarks, identification of training instability issues, qualitative improvements in calibration and generation quality.

Medium confidence in: Generalizability across VLM architectures and domains, optimal cooling-weight hyperparameters, long-term stability of trained models.

Low confidence in: Precise mathematical formulation of easy negative destabilization, scalability to extremely large VLMs, performance on non-vision-language tasks.

## Next Checks
1. Test CW-DPO on non-vision-language tasks (medical imaging, remote sensing, multimodal reasoning) to assess cross-domain generalization

2. Systematically vary cooling schedule parameters across wider ranges to identify optimal configurations and sensitivity

3. Quantify wall-clock time, GPU memory usage, and total computational cost of CW-DPO compared to single-stage baselines across different model scales