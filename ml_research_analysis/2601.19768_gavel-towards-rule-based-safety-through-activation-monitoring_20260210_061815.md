---
ver: rpa2
title: 'GAVEL: Towards rule-based safety through activation monitoring'
arxiv_id: '2601.19768'
source_url: https://arxiv.org/abs/2601.19768
tags:
- uni00000048
- uni00000044
- uni00000055
- uni00000052
- uni00000051
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "GAVEL introduces a rule-based framework for activation-based safety\
  \ in LLMs, addressing the limitations of current methods\u2014poor precision, inflexibility,\
  \ and lack of interpretability. It defines Cognitive Elements (CEs) as interpretable\
  \ activation-level primitives, such as \u201Cmaking a threat\u201D or \u201Cpayment\
  \ processing,\u201D and enables rules over CEs to detect nuanced behaviors with\
  \ high precision."
---

# GAVEL: Towards rule-based safety through activation monitoring

## Quick Facts
- arXiv ID: 2601.19768
- Source URL: https://arxiv.org/abs/2601.19768
- Reference count: 36
- Primary result: GAVEL achieves >98% TPR and near-zero FPR on misuse detection via rule-based CE monitoring

## Executive Summary
GAVEL introduces a rule-based framework for activation-based safety in LLMs, addressing the limitations of current methods—poor precision, inflexibility, and lack of interpretability. It defines Cognitive Elements (CEs) as interpretable activation-level primitives, such as "making a threat" or "payment processing," and enables rules over CEs to detect nuanced behaviors with high precision. By decoupling activation engineering from rule configuration, GAVEL supports rapid, customizable safety enforcement without retraining models or detectors. Evaluations show GAVEL achieves over 98% TPR and near-zero FPR across diverse misuse scenarios, outperforming baseline methods including content moderation APIs and activation classifiers. The framework is model-agnostic, supports community sharing of rulesets, and remains robust under adversarial attacks targeting surface text. GAVEL's modular design and lightweight runtime overhead make it practical for real-world deployment, offering a scalable, interpretable, and auditable approach to AI governance.

## Method Summary
GAVEL uses a rule-based system that monitors LLM activations to detect harmful behaviors. It defines Cognitive Elements (CEs) as interpretable activation-level primitives (e.g., "making a threat" or "payment processing") and trains a multi-label classifier on attention outputs from a selected layer range. For each token generated, the system extracts activations, classifies them into CE probabilities, and evaluates user-defined boolean rules over a temporal window. Actions like "stop" or "override" are executed when rules fire. The approach decouples activation engineering from rule configuration, enabling rapid, customizable safety enforcement without retraining models or detectors.

## Key Results
- Achieves >98% TPR and near-zero FPR across diverse misuse scenarios
- Outperforms content moderation APIs and activation classifiers in precision and flexibility
- Demonstrates robustness to surface-text adversarial attacks while maintaining rule-based interpretability

## Why This Works (Mechanism)
GAVEL works by detecting interpretable activation patterns (Cognitive Elements) that correspond to safety-relevant behaviors, then composing these into flexible, user-defined rules. The ERI prompt ("Think about [CE] while revising...") sharpens activation alignment between text and CEs, enabling precise detection. The rule engine aggregates per-token CE probabilities over a temporal window, allowing context-sensitive enforcement. This design enables compositional safety policies (e.g., "stop if threaten AND payment_tools") that are more precise than monolithic classifiers and more adaptable than static filters.

## Foundational Learning
- **Cognitive Elements (CEs)**: Interpretable activation-level primitives that represent specific safety-relevant behaviors. *Why needed*: They provide a granular, composable basis for safety rules. *Quick check*: Can you map a CE to a concrete safety policy (e.g., "payment processing" → block financial fraud)?
- **Activation Monitoring**: Extracting and analyzing model internal states (e.g., attention outputs) to infer behavior. *Why needed*: Surface text alone is brittle to adversarial manipulation. *Quick check*: Does the system extract activations from the same layer range across different models?
- **Rule Engine**: Evaluates boolean expressions over CE presence/absence to trigger safety actions. *Why needed*: Enables flexible, policy-driven enforcement without retraining. *Quick check*: Can a non-technical user define and modify rules without touching code?

## Architecture Onboarding

- Component map:
  - **Excitation Dataset (`D_c`)** -> **Activation Extractor** -> **CE Detector (`g`)** -> **Rule Engine** -> **Enforcement Module**

- Critical path:
  1. **Data Curation**: Define CEs and manually or automatically generate excitation datasets (`D_c`) for each.
  2. **Activation Elicitation**: Use the ERI prompt ("Think about [CE]...") with each dataset `D_c` on the target LLM to collect activation tensors `H_c`.
  3. **Model Training**: Train the multi-label classifier `g` on the aggregated activation data `H = {H_c}`.
  4. **Rule Definition**: Express safety policies as boolean rules over CEs (e.g., `stop if threaten AND payment_tools`).
  5. **Deployment**: For each new token generated by the LLM, extract activations, classify with `g`, update CE presence vector, and evaluate rules. Execute action if a rule fires.

- Design tradeoffs:
  - **CE Granularity**: Finer CEs offer more composability and precision but increase the number of classes for the detector to learn, potentially reducing accuracy on any single CE. Broader CEs are easier to detect but may lead to more false positives.
  - **Window Size (`N`)**: A full-conversation window (`N = conversation length`) captures context but may be computationally expensive to maintain and could lead to "caching" of CEs from irrelevant past topics. Shorter windows are faster but may miss slow-building scams.
  - **Detector Architecture**: A simple RNN is lightweight (~150MB) and fast (<1% overhead) but may be less accurate than a larger Transformer-based classifier.

- Failure signatures:
  - **High False Positive Rate**: The detector is frequently misclassifying benign text as containing a harmful CE. This often stems from poorly defined or overly broad CEs in the excitation dataset.
  - **Rule Never Fires**: The target behavior is observed, but the rule doesn't trigger. This could be because the CE is not being detected (poor classifier accuracy) or the boolean logic of the rule is incorrect.
  - **Language Drift**: The detector, trained on English excitation data, fails to detect CEs in other languages, despite the paper's claims of cross-lingual transfer.

- First 3 experiments:
  1. **Elicitation Method Ablation**: Train three detectors using activations from (a) naive prefilling, (b) a simple revision instruction, and (c) the full Elicited Revision Instruction (ERI). Compare their classification accuracy on a held-out set of CE-labeled data to validate the core elicitation claim.
  2. **Composition vs. Monolith**: Compare the performance of a GAVEL rule (e.g., `phishing`) against a single, monolithic classifier trained end-to-end on "phishing" vs. "not-phishing" data. Measure both true positive rates and false positive rates on challenging, closely-related benign data (e.g., cultural discussions for a "hate speech" detector).
  3. **Window Size Sensitivity**: For a multi-turn scam scenario (e.g., romance baiting), evaluate the detection rate of a rule as a function of the temporal window size `N` (e.g., 1, 5, 20 turns, full conversation) to find the optimal balance between speed and accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Are "CE-level jailbreaks" feasible, where an adversary triggers harmful outputs while explicitly suppressing the activation of specific Cognitive Elements to evade detection?
- Basis in paper: [explicit] The paper identifies a new threat vector: "the CE-level jailbreak, where an adversary attempts to trigger harmful behavior without activating the associated CE signatures."
- Why unresolved: The authors identify this theoretical vulnerability but do not construct or test such attacks in the evaluation; they explicitly call for future work to explore this domain.
- What evidence would resolve it: Empirical results from red-teaming attempts showing successful decoupling of harmful outputs from CE signatures, or theoretical proofs of the robustness of CE detection against such manipulation.

### Open Question 2
- Question: How can adaptive or shorter temporal windows improve the detection of context-sensitive behaviors compared to the full-conversation horizon used in the current GAVEL implementation?
- Basis in paper: [explicit] The authors note that while their experiments typically let the window $N$ span the entire conversation, "shorter or adaptive windows are possible and may better capture context-sensitive behaviors."
- Why unresolved: The current study relies on a static window spanning the entire dialogue; the efficacy of dynamic windowing strategies for memory efficiency or long-range context shifting remains untested.
- What evidence would resolve it: Comparative benchmarks on long-context dialogues showing improved precision or recall when using sliding or attention-based windows versus the full-history baseline.

### Open Question 3
- Question: Can unsupervised methods like Sparse Autoencoders (SAEs) effectively replace the current supervised RNN classifier to discover and detect Cognitive Elements?
- Basis in paper: [inferred] The paper notes SAEs are "computationally expensive" and may not guarantee discovery of safety-critical concepts, but concludes that future work should explore "other CE detection methods (e.g. transformers, SAEs)."
- Why unresolved: GAVEL currently relies on a supervised multi-label RNN trained on specific "excitation datasets"; it is unknown if unsupervised feature dictionaries can provide the necessary precision for compositional rules without manual dataset engineering.
- What evidence would resolve it: A study mapping SAE latents to semantic CEs with high fidelity, demonstrating that rule enforcement can be maintained using unsupervised features without the need for explicit excitation datasets.

## Limitations
- **Model Generalization**: Tested only on Mistral 7B and Llama-3-8B; layer selection for activation extraction may not generalize without recalibration.
- **Dataset Specificity**: Relies on manually curated datasets (150 malicious + 500 benign per category); may not scale to open-domain or noisy real-world data.
- **Real-Time Overhead**: Reported as <1%, but impact on latency in production systems with high request rates is not quantified.

## Confidence
- **High Confidence**: Rule-based activation monitoring works and achieves low FPR with high TPR in controlled evaluations. Modular architecture is practical.
- **Medium Confidence**: Cross-lingual CE detection claims are supported but not thoroughly validated. Community rule-sharing model is conceptually sound but untested at scale.
- **Low Confidence**: Claims about adversarial robustness (surface text attacks) are demonstrated but not stress-tested against advanced evasion techniques.

## Next Checks
1. **Cross-Model Layer Sensitivity**: Test GAVEL on 3+ additional models (e.g., GPT-4, Claude, Phi-3) to determine if layer selection (13–26) is transferable or requires per-model tuning.
2. **Real-World Deployment Benchmark**: Deploy GAVEL in a live chatbot system for 1 week, measuring precision/recall on actual user interactions vs. synthetic test sets.
3. **Adversarial Robustness Stress Test**: Design and execute attacks targeting CE activation patterns (e.g., synonym substitution, prompt injection) to quantify model’s resilience under sustained adversarial pressure.