---
ver: rpa2
title: 'One-Token Rollout: Guiding Supervised Fine-Tuning of LLMs with Policy Gradient'
arxiv_id: '2509.26313'
source_url: https://arxiv.org/abs/2509.26313
tags:
- arxiv
- policy
- learning
- training
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes One-Token Rollout (OTR), a novel fine-tuning
  algorithm that addresses the generalization gap between supervised fine-tuning (SFT)
  and reinforcement learning (RL) in large language models. The key insight is that
  SFT uses static, off-policy data while RL benefits from dynamic, on-policy data.
---

# One-Token Rollout: Guiding Supervised Fine-Tuning of LLMs with Policy Gradient

## Quick Facts
- **arXiv ID**: 2509.26313
- **Source URL**: https://arxiv.org/abs/2509.26313
- **Authors**: Rui Ming; Haoyuan Wu; Shoubo Hu; Zhuolun He; Bei Yu
- **Reference count**: 10
- **Primary result**: One-Token Rollout (OTR) improves generalization in supervised fine-tuning by treating token prediction as single-step RL, achieving 1.24 average accuracy gains on math reasoning tasks over standard SFT.

## Executive Summary
This paper introduces One-Token Rollout (OTR), a novel fine-tuning algorithm that bridges the generalization gap between supervised fine-tuning and reinforcement learning in large language models. The key insight is that SFT uses static, off-policy data while RL benefits from dynamic, on-policy data. OTR transforms static supervised data into token-level on-policy signals by sampling candidate tokens and using the ground-truth token as a reward signal. Extensive experiments demonstrate consistent improvements across mathematical reasoning, code generation, and general reasoning benchmarks, with OTR maintaining computational efficiency comparable to standard SFT.

## Method Summary
OTR treats each token prediction as a single-step reinforcement learning trajectory. For each ground-truth token in the training data, the method samples K candidate tokens from the current policy using a temperature κ > 1 for exploration. The ground-truth token provides a reward signal (1 for match, β = -0.1 for mismatch) to these on-policy samples. The loss function weights ground-truth contributions by N_gt/K (sampling frequency), creating a curriculum effect where tokens within the model's reach are reinforced more strongly. This transforms the learning signal from static memorization to dynamic reinforcement of actions aligned with ground truth.

## Key Results
- OTR achieves average improvements of 1.24 points over SFT on in-domain mathematical reasoning tasks across multiple model families
- Superior out-of-domain generalization on code generation (HumanEval+) and general reasoning (MMLU-Pro) benchmarks
- Ablation studies confirm the importance of negative sampling component (β = -0.1) for preventing training instability
- OTR enables models to converge to higher ground-truth token probabilities during training compared to SFT
- Computational efficiency comparable to standard SFT while providing generalization benefits

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Converting static supervised data into token-level on-policy signals improves generalization compared to off-policy SFT.
- **Mechanism:** At each timestep t, OTR samples K candidate tokens from the current policy π'_θ (with temperature κ > 1 for exploration). The ground-truth token from supervised data provides a reward signal to these on-policy samples. This transforms the learning signal from "memorize this token" to "reinforce actions within current policy reach that align with ground truth."
- **Core assumption:** The generalization gap between SFT and RL stems primarily from the on-policy vs. off-policy nature of training data, not just the loss function.
- **Evidence anchors:**
  - [abstract]: "OTR repurposes static, off-policy supervised data into a dynamic, on-policy signal at the token level"
  - [section 1]: "SFT learns from a fixed, pre-collected dataset, whereas RL utilizes on-policy data sampled from the current policy"
  - [corpus]: Neighbor paper "Retaining by Doing" (arXiv:2510.18874) provides supporting evidence that on-policy data mitigates catastrophic forgetting in LM adaptation.

### Mechanism 2
- **Claim:** Ground-truth token probability dynamically weights the learning signal, preventing over-optimization on low-reachability tokens.
- **Mechanism:** The OTR loss weights the ground-truth token contribution by N_gt/K (sampling frequency). When N_gt = 0 (ground truth never sampled), its loss contribution is zero—signaling the model cannot yet reach that token. The Monte Carlo approximation N_gt/K ≈ π_θ(x_t|s_t) creates a curriculum effect.
- **Core assumption:** Tokens already within the model's reach should be reinforced more strongly than unreachable tokens.
- **Evidence anchors:**
  - [section 3.3]: "If the ground-truth is never sampled, its loss contribution is zero"
  - [section 4.4, Figure 2]: OTR-trained models converge to higher GT token counts than SFT, indicating higher ground-truth probabilities learned
  - [corpus]: Weak direct corpus evidence for this specific weighting mechanism.

### Mechanism 3
- **Claim:** Explicit negative sampling with penalty β < 0 regularizes the distribution by suppressing plausible-but-incorrect alternatives.
- **Mechanism:** The second term in OTR loss (−β/K Σ log π_θ(a'_t,j|s_t) for a'_t,j ≠ x_t) penalizes high probabilities on incorrect tokens the model actually sampled. This creates contrast between ground truth and plausible distractors.
- **Core assumption:** Penalizing incorrect tokens the model generates is more effective than only reinforcing correct ones.
- **Evidence anchors:**
  - [section 4.4]: "A direct comparison between the β = −0.1 and β = 0 rows reveals that the former almost universally outperforms the latter"
  - [section 4.4]: β = 0.1 (positive reward for wrong tokens) causes training instability and collapse
  - [corpus]: Neighbor paper "TMS: Trajectory-Mixed Supervision" (arXiv:2602.03073) explores related on-policy SFT but without explicit negative sampling.

## Foundational Learning

- **Concept**: Policy Gradient Theorem
  - **Why needed here**: OTR derives its loss from the fundamental policy gradient equation ∇_θ J(θ) = E[∇_θ log π_θ(a|s) × r(s,a)], simplified to single-step trajectories.
  - **Quick check question**: Can you explain why multiplying log-probability by reward creates a gradient that increases the probability of high-reward actions?

- **Concept**: On-Policy vs. Off-Policy Data
  - **Why needed here**: The paper's central hypothesis is that on-policy data (sampled from current model) generalizes better than off-policy data (static expert demonstrations).
  - **Quick check question**: Why might training on expert demonstrations that the model cannot yet reproduce lead to memorization rather than generalization?

- **Concept**: Monte Carlo Estimation
  - **Why needed here**: OTR approximates expectations (E[a∼π_θ]) using K samples, and approximates π_θ(x_t|s_t) using N_gt/K.
  - **Quick check question**: How does the variance of Monte Carlo estimation change with K, and what happens if K is too small?

## Architecture Onboarding

- **Component map**: Input (prompt p, ground-truth tokens x_1...x_T) → For each timestep t: Compute logits → Apply temperature κ → Sample K candidates → Compute rewards → Compute loss → Average loss across timesteps

- **Critical path**: The sampling step (wrapped in `torch.no_grad()`) must occur before loss computation. Ground-truth token is only used for reward assignment, not for direct supervision.

- **Design tradeoffs**:
  - Larger K → Better Monte Carlo approximation but higher compute
  - Higher κ → More exploration but noisier gradients
  - β = −0.1 chosen empirically; β = 0 removes regularization, β > 0 causes instability
  - Temperature sampling vs. greedy: exploration is essential for OTR to function

- **Failure signatures**:
  - If GT token counts plateau or collapse during training → likely β > 0 or temperature too low
  - If OTR performs identically to SFT → check that sampling is actually stochastic (κ > 1)
  - If memory spikes → ensure sampling uses `torch.no_grad()` context

- **First 3 experiments**:
  1. **Sanity check**: Train with β = 0, K = 256, κ = 1.3. Verify GT token counts increase during training (Figure 2 baseline).
  2. **Ablation**: Compare β ∈ {−1.0, −0.1, 0, 0.01} on a held-out validation set. Confirm −0.1 yields best average performance.
  3. **Scale test**: Vary K ∈ {64, 128, 256, 512} and measure both accuracy and training time to find efficiency-accuracy tradeoff for your compute budget.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does OTR scale efficiently to models larger than 8 billion parameters and full-sized datasets?
  - Basis in paper: [explicit] The authors explicitly state that scalability to larger models (e.g., 70B+) "remains to be validated" due to computational constraints.
  - Why unresolved: Experiments were limited to models ≤ 8B parameters and subsets of 5k-50k instances.
  - What evidence would resolve it: Benchmarking OTR on 70B+ models trained on full-scale datasets.

- **Open Question 2**: Can more sophisticated reward functions improve upon the binary ground-truth matching reward?
  - Basis in paper: [explicit] The authors note the current mechanism is "relatively simple" and list exploring "more sophisticated reward functions" as future work.
  - Why unresolved: The current reward relies strictly on exact token matching, potentially missing nuanced error signals.
  - What evidence would resolve it: Experiments using dense or model-based rewards within the OTR framework.

- **Open Question 3**: Is there performance gain in extending the method to multi-token rollouts?
  - Basis in paper: [explicit] The authors list investigating the "potential of multi-token rollouts" as a specific direction for future work.
  - Why unresolved: OTR currently treats token generation as a single-step trajectory; longer rollouts may capture dependencies but increase cost.
  - What evidence would resolve it: Ablation studies comparing single-token vs. multi-token rollout performance.

- **Open Question 4**: Is OTR effective for multimodal tasks like vision-language understanding?
  - Basis in paper: [explicit] The authors state the study is confined to text-only and plan to "extend the OTR framework to other modalities."
  - Why unresolved: The current implementation and token-level sampling are designed specifically for text generation.
  - What evidence would resolve it: Application of OTR to vision-language benchmarks showing improvements over SFT.

## Limitations

- **Scalability concerns**: The paper explicitly states that scalability to larger models (70B+) and full-sized datasets remains to be validated, with experiments limited to models ≤ 8B parameters.
- **Theoretical justification gaps**: While empirical success is demonstrated, the theoretical mechanism explaining why on-policy signals specifically improve generalization relies on assumptions that haven't been rigorously proven.
- **Reward signal simplicity**: Using only binary ground-truth matching as reward may oversimplify the learning signal and miss valuable information from semantically similar tokens.

## Confidence

**High Confidence**: The empirical demonstration that OTR outperforms standard SFT across multiple benchmarks (math reasoning, code generation, general reasoning) is well-supported by extensive experiments. The ablation studies clearly show the importance of negative sampling and the dynamic weighting mechanism.

**Medium Confidence**: The theoretical mechanism explaining why on-policy signals improve generalization is plausible but relies on several assumptions about the nature of the generalization gap that haven't been rigorously proven. The connection between token-level on-policy learning and broader generalization benefits is empirically demonstrated but theoretically under-constrained.

**Low Confidence**: The scalability analysis and computational efficiency claims are limited to the specific experimental setup. The paper doesn't extensively explore how OTR performance scales with larger K values, different temperature settings, or varying dataset characteristics.

## Next Checks

1. **Temperature Sensitivity Analysis**: Systematically vary κ across a broader range (1.0 to 2.0) and test on multiple model sizes (3B, 7B, 14B) to determine if the κ = 1.3 finding generalizes or is model/dataset-specific.

2. **Reward Signal Granularity**: Replace the binary reward with a semantic similarity metric (e.g., cosine similarity in embedding space or edit distance) to test whether richer reward signals improve performance beyond the simple binary reward.

3. **Monte Carlo Variance Analysis**: Vary K from 64 to 1024 and measure the trade-off between computational cost and performance improvement, particularly focusing on whether smaller K values (more computationally efficient) can achieve comparable results with optimal temperature and negative sampling settings.