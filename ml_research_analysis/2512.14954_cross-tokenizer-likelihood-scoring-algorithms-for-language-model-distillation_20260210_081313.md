---
ver: rpa2
title: Cross-Tokenizer Likelihood Scoring Algorithms for Language Model Distillation
arxiv_id: '2512.14954'
source_url: https://arxiv.org/abs/2512.14954
tags:
- encoding
- vocabulary
- distillation
- token
- cover
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of cross-tokenizer likelihood scoring
  in language model distillation, where teacher and student models use different vocabularies.
  The core idea leverages the recursive structure of Byte-Pair Encoding (BPE) to create
  a probabilistic framework for computing sequence likelihoods across vocabularies.
---

# Cross-Tokenizer Likelihood Scoring Algorithms for Language Model Distillation

## Quick Facts
- arXiv ID: 2512.14954
- Source URL: https://arxiv.org/abs/2512.14954
- Reference count: 40
- Key outcome: Improves GSM8K accuracy by >2% through cross-tokenizer distillation using recursive BPE structure

## Executive Summary
This work addresses a fundamental challenge in language model distillation where teacher and student models use different vocabularies. The core innovation leverages the recursive structure of Byte-Pair Encoding (BPE) tokenizers to compute sequence likelihoods across vocabularies. By treating tokenization as a tree structure, the method enables exact likelihood computation for subset cases with constant model evaluations per token, and provides both lossless recursive algorithms and fast beam-search approximations for arbitrary vocabularies. Applied to cross-tokenizer distillation for mathematical reasoning, the approach achieves state-of-the-art results on GSM8K while also enabling efficient vocabulary trimming with significant memory savings.

## Method Summary
The method exploits the recursive structure inherent in BPE tokenizers by representing tokenizations as trees where leaves correspond to final tokens and internal nodes represent subword merges. For the subset case where the student vocabulary is contained within the teacher's, the algorithm computes exact sequence likelihoods with O(1) model evaluations per token by recursively applying the chain rule. For arbitrary vocabularies, two approaches are provided: a lossless recursive algorithm that explores all possible alignments, and a beam-search approximation that trades exactness for speed. The framework is validated through cross-tokenizer distillation experiments, demonstrating improved mathematical reasoning performance and enabling efficient vocabulary compression while maintaining or improving model quality.

## Key Results
- Achieves >2% improvement on GSM8K mathematical reasoning benchmark over state-of-the-art cross-tokenizer distillation methods
- Enables up to 12% memory reduction for Qwen2.5-1.5B model through vocabulary trimming while improving baseline performance by up to 4%
- Provides exact likelihood computation with O(1) model evaluations per token for subset cases

## Why This Works (Mechanism)
The approach works by exploiting the hierarchical structure of BPE tokenization. Each BPE token can be decomposed into its constituent subword merges, forming a tree where leaves are the final tokens and internal nodes represent intermediate merge operations. This tree structure allows the computation of cross-vocabulary likelihoods through recursive application of the chain rule. When the student vocabulary is a subset of the teacher's, the algorithm can directly map tokens and compute exact probabilities efficiently. For non-subset cases, the recursive structure enables systematic exploration of all possible token alignments, with beam search providing a tractable approximation when exact computation becomes prohibitive.

## Foundational Learning

1. **Byte-Pair Encoding (BPE) tokenization**
   - Why needed: Understanding the recursive merge operations that create the tree structure
   - Quick check: Verify that each token can be decomposed into merge history

2. **Language model likelihood computation**
   - Why needed: The core task is computing P(sequence) under different vocabularies
   - Quick check: Confirm chain rule application for autoregressive models

3. **Chain rule for probability decomposition**
   - Why needed: Enables recursive computation of joint probabilities across token boundaries
   - Quick check: Validate P(A,B) = P(A)P(B|A) across different tokenizations

4. **Tree traversal algorithms**
   - Why needed: Required for exploring the token decomposition trees systematically
   - Quick check: Ensure correct handling of shared subword merges across tokens

5. **Beam search approximation**
   - Why needed: Provides tractable solution for non-subset vocabulary alignment
   - Quick check: Verify beam search maintains reasonable approximation quality

## Architecture Onboarding

**Component map:** Tokenizer Tree Builder -> Likelihood Calculator -> Beam Search Module -> Distillation Trainer

**Critical path:** Tokenization tree construction → Recursive likelihood computation → Cross-vocabulary probability alignment → Model distillation

**Design tradeoffs:** Exact vs approximate computation (beam search width vs accuracy), memory vs computation (storing full trees vs on-demand computation), subset vs arbitrary vocabulary handling (complexity differences)

**Failure signatures:** 
- Incorrect tree construction leading to invalid probability calculations
- Beam search with insufficient width causing poor alignment quality
- Mismatch between assumed BPE structure and actual tokenizer implementation
- Numerical underflow in probability computations for long sequences

**First experiments:**
1. Verify exact likelihood computation on simple subset cases with known ground truth
2. Compare beam search approximation quality against exact computation on small examples
3. Test vocabulary trimming on a toy model to validate memory reduction claims

## Open Questions the Paper Calls Out

None identified in the source material.

## Limitations

- Assumes BPE-based tokenizers with similar construction principles, limiting applicability to newer tokenization approaches
- Beam search approximation introduces uncharacterized errors that may affect downstream performance
- Experimental validation limited to mathematical reasoning tasks, leaving performance on other domains (code generation, multilingual applications) unexplored
- Memory reduction claims depend on specific trimming strategies that may not generalize across tasks

## Confidence

- **High confidence**: Theoretical framework and recursive algorithm correctness
- **Medium confidence**: O(1) complexity claim and beam search approximation quality
- **Medium confidence**: GSM8K and Qwen2.5-1.5B experimental results
- **Low confidence**: Generalization to non-BPE tokenizers and other domains

## Next Checks

1. Benchmark the method across diverse tokenization schemes beyond BPE, including SentencePiece and WordPiece, to test framework generality
2. Characterize the approximation error of the beam search method through systematic ablation studies varying beam width and depth
3. Evaluate memory and computation trade-offs on larger model scales (e.g., 7B+ parameters) to assess practical scalability