---
ver: rpa2
title: Existing LLMs Are Not Self-Consistent For Simple Tasks
arxiv_id: '2506.18781'
source_url: https://arxiv.org/abs/2506.18781
tags:
- object
- self-consistency
- graph
- reasoning
- edges
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates self-consistency in large language models
  (LLMs) on simple tasks involving binary relations. The authors introduce an inconsistency
  metric based on graph theory, measuring the proportion of edges that must be removed
  to eliminate cycles in the model's relational reasoning.
---

# Existing LLMs Are Not Self-Consistent For Simple Tasks

## Quick Facts
- arXiv ID: 2506.18781
- Source URL: https://arxiv.org/abs/2506.18781
- Reference count: 40
- Primary result: No current LLMs achieve complete self-consistency on simple binary relation tasks, with inconsistency rates up to 89.7% for smaller models

## Executive Summary
This study investigates self-consistency in large language models using graph-theoretic metrics applied to binary relation reasoning tasks. The authors develop a novel inconsistency metric that measures the proportion of edges that must be removed to eliminate cycles in the model's relational reasoning. Testing 12 models ranging from 7B to 16B parameters on temporal, spatial, and kinship reasoning tasks, they find that even state-of-the-art models like GPT-o4-mini and DeepSeek-R1 exhibit non-trivial inconsistencies. The paper introduces two automated methods—graph-based and energy-based—to detect and partially correct these inconsistencies, demonstrating that smaller models can be significantly improved through post-hoc fixing.

## Method Summary
The method constructs directed graphs from LLM predictions of all ordered pairs in N objects, where edges represent binary relations (e.g., A→B means "A is before B"). Inconsistency is measured as the proportion of reverse edges after sorting nodes by in-degree, which approximates the minimum feedback arc set. Two fixing approaches are proposed: graph-based (using Tarjan's algorithm to identify and reverse contradictory edges) and energy-based (gradient descent optimization minimizing an energy function that penalizes relation violations). The framework applies to 1D temporal ordering, 2D spatial positioning, and kinship reasoning tasks, with datasets including US geography, synthetic coordinates, and family trees.

## Key Results
- No models achieve complete self-consistency across all tested tasks
- Smaller models (7B-13B) show inconsistency rates up to 89.7%, while SOTA models (GPT-o4-mini, DeepSeek-R1) achieve better but still non-trivial scores
- Graph-based and energy-based fixing methods show high correlation (r≥0.93) and can partially correct inconsistencies
- Energy-based method proves particularly effective when errors are sparse (<30% reverse edges)
- Models exhibit asymmetric errors, with negative longitude handling being a common failure mode

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cycles in directed relation graphs indicate logical inconsistency in LLM relational reasoning
- Mechanism: The paper encodes LLM predictions as directed graphs where edges represent binary relations (e.g., A→B means "A is before B"). In a perfectly consistent model, all edges would flow in one direction (simply-ordered graph). Cycles (A→B→C→A) reveal contradictions—the model claims mutually impossible relationships.
- Core assumption: Binary relations should be transitive and acyclic for consistency.
- Evidence anchors:
  - [Section 3]: "Loops Indicate Inconsistency. If the model were perfectly consistent, we could order the nodes such that all edges point forward. Inconsistency arises when the graph contains a cycle."
  - [Section 4.5.1]: Tarjan's algorithm identifies strongly connected components; topological sort finds reverse edges.
  - [Corpus]: Related work on self-consistency (Huang et al., Chen et al.) focuses on math reasoning aggregation, not structural graph-based detection.
- Break condition: If relations are genuinely non-transitive (e.g., "prefers" in voting systems), cycle detection would falsely flag valid responses.

### Mechanism 2
- Claim: Energy-based optimization can recover ground-truth relations when errors are sparse
- Mechanism: Each object is assigned a coordinate position. An energy function penalizes relation violations: E(rij) = max(0, 1 + (xi - xj)) for relation "xi precedes xj." Gradient descent minimizes total energy, pushing coordinates toward consistent orderings. When most relations are correct, the optimizer suppresses inconsistent minority edges.
- Core assumption: The majority of LLM-predicted relations are correct (sparsity of errors).
- Evidence anchors:
  - [Section 4.6]: "The method proves particularly effective under the sparsity-of-errors assumption, when most LLM-derived relations are fundamentally correct."
  - [Figure 6b]: Error recovery is near-linear when reverse edges < 30%.
  - [Corpus]: No direct corpus evidence for EBM applied to LLM consistency—this appears novel.
- Break condition: If systematic bias exists (e.g., model always predicts "before" regardless of truth), energy minimization will converge to a consistent but wrong ordering.

### Mechanism 3
- Claim: Next-token prediction architectures inherently lack backward edges for enforcing bidirectional consistency
- Mechanism: The paper applies category theory to argue that LLMs form a "forward-directed language category"—composing morphisms in one direction (A→B→C) but lacking structural mechanisms to enforce A→C consistency when generating C independently. This is an architectural constraint, not just a training issue.
- Core assumption: Category-theoretic composition laws meaningfully describe LLM reasoning structure.
- Evidence anchors:
  - [Section 1]: "The next-token prediction framework naturally forms a forward-directed language category, lacking the necessary backward edges to enforce consistency."
  - [Appendix A.1]: Formal definitions of categories, functors, and self-consistent proxy categories.
  - [Corpus]: Related work does not address architectural causes; this is a theoretical contribution.
- Break condition: Assumption: This remains a hypothesis—the paper does not empirically validate the architectural claim through controlled experiments.

## Foundational Learning

- Concept: **Strongly Connected Components (SCCs) and Tarjan's Algorithm**
  - Why needed here: SCCs identify maximal subgraphs where every node reaches every other node—precisely where inconsistencies (cycles) live. Tarjan's algorithm finds these in O(V+E) time.
  - Quick check question: In a graph with edges A→B, B→C, C→A, how many SCCs exist and what do they indicate about consistency?

- Concept: **Feedback Arc Set and Minimum Edit Distance**
  - Why needed here: The inconsistency metric measures the minimum edges to remove for acyclicity. This is NP-complete (Karp, 1972), so approximations via in-degree sorting are used.
  - Quick check question: Why can't we efficiently compute the exact minimum number of edges to remove for acyclicity?

- Concept: **Category Theory Basics (Objects, Morphisms, Composition)**
  - Why needed here: The paper frames relational reasoning as morphism composition (A→B, B→C should compose to A→C). Self-consistency requires compositional closure.
  - Quick check question: If f: A→B represents "A is father of B" and g: B→C represents "B is mother of C," what should g∘f represent, and why might an LLM violate this?

## Architecture Onboarding

- Component map:
  Data layer (6 datasets) -> Prompting layer (structured templates) -> Graph construction layer (binary relations) -> Inconsistency scoring layer (reverse edge counting) -> Fixing layer (graph-based or EBM)

- Critical path:
  1. Query LLM for all N(N-1) ordered pair relations
  2. Construct directed graph(s) from responses
  3. Compute inconsistency score (proportion of reverse edges)
  4. Optionally apply fixing algorithm and measure improvement

- Design tradeoffs:
  - **Graph vs. EBM fixing**: Graph method directly removes contradictions but may discard correct edges; EBM preserves all information but assumes error sparsity
  - **Computational cost**: Exact minimum feedback arc set is NP-complete; approximation via sorting is O(N log N)
  - **Context provision**: Providing ground-truth coordinates improves accuracy but masks inherent model inconsistency

- Failure signatures:
  - High inconsistency + low error rate: Model is internally coherent but misaligned with reality
  - Low inconsistency + high error rate: Model is consistently wrong (systematic bias)
  - Asymmetric x/y inconsistency in 2D tasks: Indicates specific coordinate-system confusion (e.g., negative longitudes)

- First 3 experiments:
  1. **Baseline consistency check**: Run all 12 models on US-State dataset (51 objects); compare inconsistency scores vs. model size. Expect smaller models >50% inconsistency, SOTA models <5%.
  2. **Fixing method validation**: Apply graph-based and EBM fixing to a model with ~20% inconsistency; measure correlation between methods (paper reports r≥0.93). Verify both methods identify similar reverse edges.
  3. **Context sensitivity test**: Compare inconsistency when providing (a) no context, (b) XY coordinates, (c) ordered relations. Expect ordered-relations to increase error rates due to long-chain composition difficulty (Table 3 shows 72-82% inconsistency for GPT-4o on ordered-relations vs. 0% on XY coordinates).

## Open Questions the Paper Calls Out

- How can LLMs be trained to be inherently self-consistent, rather than relying on post-hoc fixing algorithms?
- Can the proposed self-consistency framework be effectively extended to complex reasoning domains like mathematical proofs, law, or medicine?
- Is the lack of self-consistency an inherent limitation of the decoder-only (next-token prediction) architecture?
- How can models be optimized to simultaneously achieve internal self-consistency and alignment with real-world facts (reality-alignment)?

## Limitations

- The inconsistency metric relies on NP-complete approximations that may overestimate true inconsistency
- Energy-based fixing assumes error sparsity and fails when systematic biases exist
- Category-theoretic architectural claims remain theoretical without empirical validation
- The framework may not generalize to non-transitive relation domains or complex reasoning tasks

## Confidence

**High Confidence**:
- Existence of self-consistency problems in LLMs across tested tasks
- Correlation between model size and inconsistency reduction
- Effectiveness of graph-based inconsistency metric

**Medium Confidence**:
- Theoretical framework connecting category theory to LLM limitations
- Sparsity-of-errors assumption underlying EBM fixing
- Generalizability across different reasoning domains

**Low Confidence**:
- Exact quantitative thresholds for acceptable inconsistency levels
- Long-term stability of consistency improvements from fixing methods
- Transferability to non-binary relational reasoning tasks

## Next Checks

1. **Cross-Domain Validation**: Test the inconsistency metric on non-transitive relation domains (e.g., preference rankings, social network analysis) to verify that cycle detection appropriately handles domains where logical consistency doesn't require acyclicity.

2. **Architectural Intervention Study**: Implement controlled experiments varying LLM architectural components (e.g., bidirectional attention mechanisms, consistency-enforcing loss functions) to empirically validate the category-theoretic claim about forward-directed language categories.

3. **Error Pattern Analysis**: Conduct systematic error analysis across all 12 models and 6 datasets to characterize error types, identify systematic biases, and determine whether inconsistency patterns correlate with specific training characteristics or architectural features.