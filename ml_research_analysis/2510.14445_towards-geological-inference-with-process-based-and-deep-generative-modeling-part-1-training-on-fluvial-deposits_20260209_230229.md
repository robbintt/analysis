---
ver: rpa2
title: 'Towards geological inference with process-based and deep generative modeling,
  part 1: training on fluvial deposits'
arxiv_id: '2510.14445'
source_url: https://arxiv.org/abs/2510.14445
tags:
- training
- samples
- cited
- https
- sample
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Generative modeling has been used for decades to predict subsurface
  physical properties while quantifying uncertainty, but struggles with complex geological
  structures like fluvial deposits due to their continuity. This study investigates
  whether generative adversarial networks (GANs) - a deep learning approach - can
  generate realistic 3D fluvial deposits by training on data from a process-based
  model.
---

# Towards geological inference with process-based and deep generative modeling, part 1: training on fluvial deposits

## Quick Facts
- arXiv ID: 2510.14445
- Source URL: https://arxiv.org/abs/2510.14445
- Reference count: 40
- Primary result: GANs can generate diverse, high-quality 3D fluvial deposits that honor geological laws

## Executive Summary
This study investigates whether generative adversarial networks (GANs) can generate realistic 3D fluvial deposits by training on data from a process-based model. An ablation study demonstrates that architectural elements from the deep learning community for large 2D images transfer directly to 3D fluvial deposits. The trained GAN produces samples that reproduce non-stationarity and detailed features without mode collapse or memorization. Using process-based models provides additional properties like deposition time, enabling validation using geological principles such as the law of superposition. Results suggest GANs are more robust than often assumed for specific geological structures.

## Method Summary
The study trains a 3D GAN on fluvial deposit data generated by the CHILD process-based model. The training set contains 19,000 realizations (128×128×16 cells) with two continuous properties: coarse sediment fraction and deposition time. A DCGAN architecture is extended with residual blocks, spectral normalization, and lazy R1 regularization. Training uses mixed precision on 2× NVIDIA A100 80GB GPUs with batch size 64, running for approximately 40,000 generator iterations. Validation metrics include sliced Wasserstein distance to a validation set and the fraction of cells honoring the law of superposition.

## Key Results
- 2D-to-3D architecture transfer is effective: BigGAN components designed for 512×512 images transfer directly to 128×128×16 3D volumes
- Residual blocks are critical: Adding residual blocks to DCGAN enables stable training while other components provide marginal benefit
- Physics-aware validation works: Including deposition time as a co-generated property enables validation via the law of superposition without requiring separate validation data
- GANs avoid common failures: Generated samples reproduce non-stationarity and detailed features without mode collapse or memorization

## Why This Works (Mechanism)

### Mechanism 1: 2D-to-3D Architecture Transfer via Equivalent Spatial Complexity
The representational challenge scales with total cells rather than dimensionality. A 128×128×16 3D volume (262,144 cells) presents similar complexity to a 512×512 2D image (262,144 cells), so 2D-proven stabilization techniques (residual blocks, spectral normalization) remain effective. The limited structural diversity in fluvial deposits compared to general image datasets reduces optimization difficulty.

### Mechanism 2: Process-Based Auxiliary Properties Enable Physics-Aware Validation
Process-based models naturally produce temporal metadata like deposition time. Training the GAN to generate both physical properties and deposition time creates an internal consistency check: in undeformed strata, younger cells must overlie older ones. The fraction of cells honoring superposition converges to 1.0 as training improves, providing a ground truth validation mechanism without requiring separate validation data.

### Mechanism 3: Residual Blocks + Regularization as Minimum Viable Stabilizers
Residual connections enable gradient flow through deeper networks while spectral normalization constrains discriminator Lipschitz constant, preventing one network from overpowering the other. This combination creates a stable adversarial equilibrium. The ablation study shows that adding residual blocks and either spectral normalization or lazy R1 regularization to DCGAN is sufficient for stable 3D training.

## Foundational Learning

- **Generative Adversarial Network (GAN) minimax game**: Understanding that GAN training is a two-player game seeking Nash equilibrium, not standard optimization. Loss curves don't directly indicate quality. Quick check: Why can't we just monitor generator loss to assess training progress?

- **Residual connections (skip connections within blocks)**: The ablation study shows this is the critical component that enables stable training; without it, DCGAN collapses. Quick check: What problem do residual connections solve in deep networks?

- **Spectral normalization / gradient penalty regularization**: Constrains discriminator capacity to prevent it from overpowering the generator; lazy R1 is a computationally cheaper variant. Quick check: What happens to GAN training if the discriminator becomes too strong?

## Architecture Onboarding

- **Component map**: Latent vector (8-512 dims) → Generator (residual blocks with bottleneck, transposed convolutions) → Generated 3D volume → Discriminator (residual blocks with bottleneck, spectral normalization) → Loss (binary cross entropy with logits) → Generator/ Discriminator updates

- **Critical path**:
  1. Prepare training data: Extract 128×128×16 samples from process-based realizations, scale properties to [-1, 1]
  2. Start with architecture 4 (DCGAN + resblocks + spectral norm + lazy R1)
  3. Monitor: sliced Wasserstein distance to validation set + superposition fraction
  4. Train until both metrics stabilize (~40,000 generator iterations)

- **Design tradeoffs**:
  - Latent vector size (8-512): Paper shows minimal impact on quality; smaller latent spaces may be more entangled, problematic for downstream inversion
  - Batch size: Limited to 64 due to memory (vs. BigGAN's 2048); mixed precision training enables this
  - Random vs. deterministic cropping: Random cropping acts as data augmentation; deterministic enables memorization testing

- **Failure signatures**:
  - Mode collapse: Generated samples cluster in MDS visualization, all similar to each other
  - Training collapse: Sliced Wasserstein distance increases or oscillates without converging
  - Memorization: Generated samples nearly identical to nearest training samples
  - Superposition violation: Fraction of cells honoring law stays below 0.9

- **First 3 experiments**:
  1. Replicate architecture 4 on a small subset (1,900 samples) to verify training stability on your hardware
  2. Test latent interpolation: generate samples along linear paths in latent space to assess entanglement
  3. Generate samples with different latent shapes (spatial dimensions > 1) to test out-of-distribution extrapolation behavior

## Open Questions the Paper Calls Out

### Open Question 1
Does the training stability and sample quality of GANs transfer to larger 3D images and multimodal datasets representing diverse depositional environments? The study was restricted to a specific 3D image size (128×128×16) and a single depositional environment (fluvial deposits).

### Open Question 2
How do GANs compare to diffusion models and variational autoencoders (VAEs) in terms of sample quality, diversity, and computational cost for geological modeling? This study focused exclusively on validating GAN architectures against process-based models.

### Open Question 3
What architectural strategies are required to enable deep generative models to properly extrapolate in non-stationary directions? Current models statistically reproduce geometries without understanding the underlying geological principles, causing them to fail when generating data outside the stationary training distribution.

## Limitations
- Results are limited to fluvial deposits from a single process-based model, potentially not generalizing to other depositional environments
- The ablation study only tests 4 architecture variants on a single dataset, leaving many potential configurations unexplored
- Training relies on specific computational resources (A100 GPUs with 80GB memory), limiting reproducibility on smaller hardware

## Confidence
- **High confidence**: The mechanism for 2D-to-3D architecture transfer via equivalent spatial complexity is well-supported by direct comparison of cell counts and ablation study results
- **Medium confidence**: The claim that GANs are more robust than often assumed requires testing across multiple geological settings and diversity levels not explored in this single-case study
- **Medium confidence**: The assertion that process-based auxiliary properties enable physics-aware validation is supported by superposition testing but hasn't been validated against other geological principles or real-world data

## Next Checks
1. **Scale Test**: Train the same architecture on 3D volumes with 2× dimensions (256×256×32) to identify the complexity threshold where 2D transfer breaks down
2. **Diversity Test**: Generate samples from a multimodal training set containing multiple depositional environments (fluvial, deltaic, marine) to test whether architectural components scale or require modification
3. **Real Data Validation**: Apply the trained model to generate samples conditioned on real well log data from fluvial systems, then validate against independent seismic interpretations to assess real-world applicability