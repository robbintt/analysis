---
ver: rpa2
title: Mind Reading or Misreading? LLMs on the Big Five Personality Test
arxiv_id: '2511.23101'
source_url: https://arxiv.org/abs/2511.23101
tags:
- complex
- essays
- trait
- prompt
- personality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates five large language models (LLMs) for binary\
  \ personality prediction from text using the Five Factor Model (Big Five) under\
  \ zero-shot conditions. The authors test two prompting strategies\u2014minimal and\
  \ enriched with linguistic and psychological cues\u2014across three datasets (Essays,\
  \ MyPersonality, Pandora) and five personality traits."
---

# Mind Reading or Misreading? LLMs on the Big Five Personality Test

## Quick Facts
- **arXiv ID:** 2511.23101
- **Source URL:** https://arxiv.org/abs/2511.23101
- **Reference count:** 40
- **Primary result:** No zero-shot LLM configuration achieves reliable personality prediction from text; enriched prompts improve validity but introduce systematic positive bias.

## Executive Summary
This study evaluates five large language models for binary personality prediction using the Big Five model under zero-shot conditions. Two prompting strategies—minimal and enriched with psychological cues—are tested across three datasets and five personality traits. While enriched prompts reduce invalid outputs and improve class balance, they also introduce a systematic positive bias toward predicting trait presence. The analysis reveals that per-class recall is the most informative metric, exposing significant asymmetries masked by aggregate measures. The study concludes that current out-of-the-box LLMs are not yet suitable for reliable automatic personality prediction from text.

## Method Summary
The study tests zero-shot binary personality prediction from text using five LLMs (GPT-4, Llama3, Mistral, Gemma, Phi3) across three datasets (Essays, MyPersonality, Pandora) and five Big Five traits. Two prompting strategies are employed: minimal prompts requesting a single digit, and enriched prompts with psychological definitions, IPIP facets, and adjective lists. Models are run with temperature=0 and max 20 tokens, with outputs parsed for strict 0/1 format. Per-class recall is emphasized as the primary diagnostic metric, with class-wise F1 ≥0.5 used as a threshold for meaningful performance.

## Key Results
- Enriched prompts reduce invalid outputs and improve class balance but introduce systematic positive bias toward predicting trait presence.
- Per-class recall reveals significant asymmetries masked by aggregate metrics like accuracy and macro-F1.
- Openness and Agreeableness are relatively easier to detect than Extraversion or Neuroticism.
- No configuration yields consistently reliable predictions across all traits and datasets.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Enriching prompts with psychological definitions improves instruction adherence (output validity) but simultaneously shifts the model's decision boundary toward predicting trait presence (positive bias).
- **Mechanism:** The inclusion of detailed trait descriptors, facet-level cues, and adjective lists provides the model with stricter criteria for the task, reducing the probability of generating incoherent or refused outputs. However, the semantic weight of these positive descriptors appears to prime the model, increasing the likelihood of identifying trait-relevant signals even where they are weak or absent.
- **Core assumption:** The observed bias is a function of the prompt's semantic content rather than an inherent property of the model's "belief" about the data.
- **Evidence anchors:** [abstract] "Enriched prompts reduce invalid outputs and improve class balance, but also introduce a systematic bias toward predicting trait presence." [Section 4.3.4] "Complex prompts almost always improve the positive class (Recall_1, F1_1) while degrading the negative class... This systematic positive bias is evident across datasets."

### Mechanism 2
- **Claim:** Aggregate metrics like accuracy conceal asymmetric failure modes in classification, making per-class recall the superior diagnostic for reliability.
- **Mechanism:** In imbalanced or biased binary classification, a model can achieve high accuracy by predicting the majority class or the class favored by prompt bias. Per-class recall forces the evaluator to measure the model's ability to retrieve *both* positive (1) and negative (0) instances independently, exposing whether the model is actually discriminating or merely defaulting.
- **Core assumption:** A reliable personality detector must demonstrate balanced sensitivity to both the presence and absence of a trait.
- **Evidence anchors:** [abstract] "Aggregate metrics such as accuracy and macro-F1 mask significant asymmetries, with per-class recall offering clearer diagnostic value." [Section 4.3.2] "Precision gaps mostly reflect dataset distributions, but recall behavior varies widely... low F1 scores stem from models' inability to recall minority-class instances."

### Mechanism 3
- **Claim:** Zero-shot performance is heavily modulated by the "linguistic visibility" of specific traits, making Openness and Agreeableness easier to detect than Extraversion or Neuroticism.
- **Mechanism:** Traits like Openness correlate strongly with distinct lexical markers (e.g., rich vocabulary, abstract ideas), which LLMs can easily pattern-match. Traits like Extraversion or Neuroticism may rely more on behavioral context or emotional nuance that is sparser in text or harder to distinguish from surface-level sentiment without fine-tuning.
- **Core assumption:** The pre-training data of these LLMs contains stronger correlations between linguistic style and Openness/Agreeableness than for other traits.
- **Evidence anchors:** [abstract] "Openness and Agreeableness are relatively easier to detect, while Extraversion and Neuroticism remain challenging." [Section 4.3.1] Notes that Openness and Agreeableness emerge as relatively easier to capture with balanced F1 values, while others rarely exceed thresholds.

## Foundational Learning

- **Concept:** **Big Five (OCEAN) Personality Model**
  - **Why needed here:** This is the target label schema. Without understanding that traits are orthogonal (e.g., you can be high in Openness but low in Agreeableness), you cannot interpret the independent binary classifications or the "trait salience" findings.
  - **Quick check question:** If a text displays "rich vocabulary" but is "rude to others," which two OCEAN traits are likely being signaled, and are they independent?

- **Concept:** **Zero-Shot Prompting**
  - **Why needed here:** The entire study is framed as a "stress test" of out-of-the-box capabilities. The results (invalid outputs, bias) are specific to the lack of gradient updates or example-based calibration.
  - **Quick check question:** Why would a model produce an "invalid output" (refusal) in a zero-shot setting but not in a few-shot setting?

- **Concept:** **Precision-Recall Trade-off**
  - **Why needed here:** The paper explicitly critiques "Accuracy" and validates "Recall." You must understand that Precision measures "how many selected items are relevant" while Recall measures "how many relevant items are selected" to grasp why high accuracy might still mean the model is useless (e.g., it catches all positive traits but misses all negative ones).
  - **Quick check question:** If a model predicts "Trait Present" for 99 out of 100 texts, and 90 texts actually have the trait, what is likely happening to the Recall for the "Trait Absent" (0) class?

## Architecture Onboarding

- **Component map:** Raw text -> Trait-specific Prompt construction -> 5 Models (GPT-4, Llama3, Mistral, Gemma, Phi3) -> Output Sanitization (parse 0/1) -> Per-Class Metric Calculation
- **Critical path:**
  1. **Input Processing:** Raw text -> Trait-specific Prompt construction.
  2. **Output Sanitization:** Critical step. You must parse the LLM response to extract strictly "0" or "1". Note that models like Phi3 often embed the digit in a sentence ("The answer is 1"), requiring regex parsing, while GPT-4 may refuse entirely.
  3. **Metric Calculation:** Calculate Recall for class 0 and class 1 separately. If Recall_0 is near 0, the system has failed regardless of Accuracy.
- **Design tradeoffs:**
  - **Validity vs. Bias:** Using the "Enriched Prompt" ensures you get valid 0/1 outputs (fixing Phi3's incoherence) but introduces a systematic positive bias. You must choose between a model that refuses/gibberishes (Simple) or one that is biased toward "Yes" (Enriched).
  - **Proprietary vs. Open:** GPT-4 is hampered by safety filters (refusals), while Open Source models (Mistral/Phi3) suffer from task misunderstanding. Open source models can approximate GPT-4 performance *only* when using enriched prompts.
- **Failure signatures:**
  - **Safety Refusals:** GPT-4 returns "I cannot analyze personality..." (High invalid count in Pandora).
  - **Verbose Incoherence:** Phi3 returns "The text suggests... [long explanation]" without the binary digit, or hallucinates a task.
  - **Systematic Bias:** A "Hexagon" shape in the radar charts (Figure 6/7) skewing hard to the right (high Recall_1, near-zero Recall_0) indicates the enriched prompt is merely guessing "1" for everyone.
- **First 3 experiments:**
  1. **Baseline Validity Check:** Run the *Simple Prompt* across all models on a 100-sample subset of "Essays". Measure the rate of invalid outputs (refusals or non-binary answers) to identify which models need the Enriched Prompt for basic instruction following.
  2. **Bias Calibration:** Run the *Enriched Prompt* on the same subset. Compare the ratio of Predicted-1 to Predicted-0 against the ground truth. If the ratio of Predicted-1 is significantly higher than ground truth, confirm the "Positive Bias" mechanism.
  3. **Class-wise Recall Audit:** Identify the "best" result (e.g., Mistral on Agreeableness). Drill down into the False Negatives (texts with trait predicted "0" but labeled "1") to see if the model is missing subtle cues or if the ground truth label is arguably weak.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can calibrated decision rules or cost-sensitive objectives effectively mitigate the systematic positive bias toward predicting trait presence induced by enriched prompts?
- **Basis in paper:** [explicit] The authors explicitly recommend exploring "calibrated decision rules, cost-sensitive objectives, and lightweight adaptation" to address the bias-recall trade-off introduced by prompt enrichment.
- **Why unresolved:** The study demonstrates that enriched prompts fix validity issues but cause positive bias; however, it does not test specific algorithmic debiasing techniques.
- **What evidence would resolve it:** Experiments applying threshold adjustment or cost-sensitive loss functions to enriched prompting strategies to achieve balanced per-class recall.

### Open Question 2
- **Question:** Can lightweight adaptation techniques (e.g., few-shot, PEFT) enable open-source models to consistently surpass pre-LLM benchmarks where zero-shot prompting fails?
- **Basis in paper:** [explicit] The conclusion states that future work should "explore... lightweight adaptation (e.g., few-shot, PEFT)" to improve performance beyond the zero-shot lower bound established in the study.
- **Why unresolved:** The study was strictly limited to zero-shot conditions; it is unknown if parameter-efficient fine-tuning would allow lightweight models to exceed the ~0.6 SOTA benchmark.
- **What evidence would resolve it:** A comparative analysis of few-shot and LoRA-adapted open-source models against the pre-LLM SOTA on the Essays and MyPersonality datasets.

### Open Question 3
- **Question:** How do distinct model architectures (e.g., safety-tuned vs. base) contribute to heterogeneous failure modes in personality prediction tasks?
- **Basis in paper:** [inferred] The paper identifies that failure modes differ significantly (e.g., GPT-4 refuses due to safety filters, while Phi-3 fails due to task misunderstanding), but does not isolate the architectural or tuning causes.
- **Why unresolved:** The study observes the symptoms of these failures (invalid outputs) but does not dissect the internal mechanisms or training data issues leading to them.
- **What evidence would resolve it:** Ablation studies on models with and without safety instruction tuning to measure the causal impact of safety filters on refusal rates in psychological assessment tasks.

## Limitations

- The binarization method for Pandora dataset labels relies on interpolation to match MyPersonality's scale, but exact thresholds per trait are not specified.
- The observed positive bias may be specific to the particular trait descriptions and adjectives used, and different linguistic formulations could yield different bias profiles.
- The analysis focuses on zero-shot conditions only; the mechanisms and limitations may not generalize to few-shot or fine-tuned scenarios.

## Confidence

- **High Confidence:** The finding that per-class recall reveals asymmetric failure modes that aggregate metrics conceal.
- **Medium Confidence:** The claim that enriched prompts introduce systematic positive bias toward predicting trait presence.
- **Medium Confidence:** The observation that Openness and Agreeableness are easier to detect than Extraversion or Neuroticism.

## Next Checks

1. **Prompt Formulation Sensitivity:** Test whether the positive bias persists when using alternative psychological definitions, different adjective sets, or varied prompt structures while keeping the core task instruction constant.
2. **Dataset Generalization:** Validate the per-class recall findings on an independent personality-labeled corpus (e.g., a different social media dataset or controlled writing samples) to confirm that the trait-specific detectability patterns hold.
3. **Few-Shot Calibration:** Replicate the experiments with 3-5 exemplar texts per trait class in the prompt to determine if the systematic bias and validity issues can be mitigated through minimal supervised examples.