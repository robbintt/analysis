---
ver: rpa2
title: 'AraMix: Recycling, Refiltering, and Deduplicating to Deliver the Largest Arabic
  Pretraining Corpus'
arxiv_id: '2512.18834'
source_url: https://arxiv.org/abs/2512.18834
tags:
- quality
- content
- source
- arabic
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors address the redundancy problem in multilingual web
  corpora by leveraging cross-source agreement as a quality signal. Their MixMinMatch
  method combines existing Arabic, Turkish, and Hindi web datasets, performs cross-dataset
  MinHash deduplication, and retains documents recovered by multiple independent sources.
---

# AraMix: Recycling, Refiltering, and Deduplicating to Deliver the Largest Arabic Pretraining Corpus

## Quick Facts
- **arXiv ID**: 2512.18834
- **Source URL**: https://arxiv.org/abs/2512.18834
- **Reference count**: 26
- **Primary result**: AraMix-Matched achieves 4.5% relative improvement over ArabicWeb24 baseline (0.161 vs 0.154 aggregate FineTasks score) using cross-source agreement as quality signal

## Executive Summary
This paper introduces MixMinMatch, a method for improving multilingual pretraining corpora by leveraging cross-source agreement as a quality signal. The approach aggregates existing Arabic web datasets, applies MinHash deduplication, and retains documents that appear in multiple independent sources. Applied to Arabic, this produces AraMix-Matched, which outperforms the best single-source baseline while using fewer total tokens. Crucially, gains persist even after removing content unique to the best baseline, demonstrating that cross-source agreement identifies quality beyond what any single source provides.

## Method Summary
MixMinMatch aggregates multiple existing Arabic web corpora (C4, CulturaX, HPLT 2.0, FineWeb-2, FinePDFs, ArabicWeb24, ClusterLab 101B), applies language-specific quality filtering, and performs MinHash-LSH deduplication. The key innovation is cross-source matching: documents are retained only if their near-duplicate cluster contains representatives from at least two independent sources. This creates a "matched" subset that requires no additional computation beyond standard deduplication. The method produces both a full MinHash corpus and a matched subset, with the latter showing superior downstream performance. Training uses a 1.46B parameter model on 29.36B tokens with the Gemma tokenizer.

## Key Results
- AraMix-Matched achieves 0.161 aggregate FineTasks score vs 0.154 for ArabicWeb24 baseline (4.5% relative improvement)
- Gains persist after removing content unique to best baseline (scores 0.152 without ArabicWeb24)
- Matched subset uses 54B tokens vs ArabicWeb24's 41B tokens
- Cross-source agreement provides quality signal beyond any single source's filtering

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Documents independently retained by multiple curation pipelines are more likely to be high-quality than those retained by a single pipeline.
- **Mechanism**: Each corpus is treated as an independent "vote" on content quality. When multiple pipelines with different crawl schedules, filtering heuristics, and thresholds all retain near-identical content, this agreement functions as an "ensemble filter" that reduces false positives from any single pipeline's blind spots. The method selects documents whose near-duplicate clusters span ≥2 independent sources.
- **Core assumption**: Each pipeline's retention decisions are positively correlated with a latent quality variable.
- **Evidence anchors**: [abstract] "content retained by multiple independent pipelines is more likely to represent high-quality text." [Section 4.3] "This insight connects to a foundational principle in machine learning and statistics: independent agreement reduces uncertainty."
- **Break condition**: Signal degrades if source pipelines are highly correlated or have systematically poor filters.

### Mechanism 2
- **Claim**: MinHash deduplication produces cluster metadata (source provenance) that can be reused as a quality signal at near-zero additional computational cost.
- **Mechanism**: The pipeline aggregates documents while preserving source identity, applies MinHash-LSH deduplication, then counts unique sources per cluster. The "matched" subset retains documents with ≥2 sources. This reuses the Union-Find structure already computed for deduplication.
- **Core assumption**: MinHash deduplication is already a standard preprocessing step in corpus construction.
- **Evidence anchors**: [abstract] "Crucially, this signal requires no additional computation beyond standard deduplication."
- **Break condition**: The "free signal" claim breaks if deduplication is not part of the pipeline.

### Mechanism 3
- **Claim**: Cross-source agreement identifies quality beyond any single source, but is sufficient—not necessary; language-specific curated corpora contribute unique high-quality content.
- **Mechanism**: Ablations show that removing content unique to the best baseline still yields a matched subset that performs on-par or better, demonstrating quality is not merely inherited. However, sources with high survival rates add unique value.
- **Core assumption**: Latent quality exists in content captured by sophisticated language-specific pipelines that generic pipelines miss.
- **Evidence anchors**: [abstract] "gains persist even after removing content unique to the best baseline." [Section 7.2] "AraMix-Matched without ArabicWeb24 scores 0.152, performing on-par with the ArabicWeb24 baseline."
- **Break condition**: Complementarity diminishes if very few sources exist or all sources are derivatives with minimal independent filtering.

## Foundational Learning

### Concept: MinHash and Locality-Sensitive Hashing (LSH) for near-duplicate detection
- **Why needed here**: The entire method builds on MinHash deduplication. Without understanding how shingling, signature generation, banding, and Jaccard similarity estimation work, you cannot debug or tune the deduplication stage.
- **Quick check question**: If you increase the number of bands in LSH while keeping total signature length constant, what happens to recall and precision of near-duplicate detection?

### Concept: Ensemble methods and variance reduction
- **Why needed here**: The core theoretical justification for cross-source agreement relies on ensemble learning principles—independent agreement reduces variance and filters noise.
- **Quick check question**: Why does bagging with highly correlated classifiers provide less variance reduction than with independent classifiers?

### Concept: Corpus deduplication trade-offs in language modeling
- **Why needed here**: The paper positions itself against model-based filtering. Understanding why deduplication improves training efficiency (reduced memorization, better generalization) is essential for evaluating this trade-off.
- **Quick check question**: Name two negative effects of training LLMs on highly duplicated data.

## Architecture Onboarding

### Component map
Source corpora (7 for Arabic) → Language-specific quality filters → MinHash signature generation → LSH banding → Union-Find clustering → Source counting per cluster → Matched subset (≥2 sources) / Full MinHash corpus → Proportional sampling to token budget → Training

### Critical path
(1) Preserving source provenance through all stages (required for cross-source matching), (2) Choosing MinHash parameters (n-gram size, signature length, bands, threshold) that balance precision/recall, (3) Setting the source-count threshold (default: ≥2, but tunable).

### Design tradeoffs
- Higher source-count threshold → smaller, higher-quality corpus but potentially excluding good content from sources with low overlap
- Full MinHash pool vs. Matched subset: MinHash pool offers 3–4× more tokens at slightly lower average quality
- Uniform vs. weighted voting: Paper uses uniform; weighted schemes (e.g., upweighting high-survival sources) are unexplored

### Failure signatures
- Very low matched subset size (<20% of MinHash pool) indicates sources have minimal overlap—may need to lower threshold or add more sources
- Ablation shows >10% degradation when removing best baseline's unique content → suggests cross-source signal is weak
- Survival rates of <50% for most sources suggest high redundancy; a single dominant source may be driving quality

### First 3 experiments
1. Reproduce the pipeline on a single language (start with Arabic) using 2–3 source corpora to validate deduplication, source counting, and matched subset creation
2. Ablation: Train small models (e.g., 150M params, 5B tokens) comparing (a) single best source, (b) matched subset, (c) matched subset with best source removed. Verify that (b) ≥ (a) and (c) ≈ (a)
3. Threshold sweep: Vary source-count threshold (2, 3, 4 sources) and measure corpus size vs. downstream task performance to find the optimal point for your target language

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would a weighted voting scheme that assigns higher weight to sources with demonstrated quality outperform uniform cross-source agreement?
- Basis in paper: Page 8 states: "This complementarity suggests a natural extension: rather than treating all sources equally, a weighted voting scheme could assign higher weight to sources with demonstrated quality."
- Why unresolved: Current method treats all sources equally despite evidence that some sources contribute higher-quality unique content.
- What evidence would resolve it: Ablation experiments comparing uniform weighting against quality-weighted voting schemes.

### Open Question 2
- Question: Does cross-source matching quality transfer to larger model scales (7B+ parameters) and longer training runs?
- Basis in paper: Page 26 (Limitations): "Whether our findings transfer to larger model scales (7B+ parameters) or longer training runs remains an open question."
- Why unresolved: All experiments use 1.46B parameter models on 29B tokens; data quality effects may or may not persist at scale.
- What evidence would resolve it: Training experiments at 7B+ scale comparing MixMinMatch-Matched against single-source baselines.

### Open Question 3
- Question: Does combining cross-source matching as a first-stage filter with model-based quality filtering improve upon either method alone?
- Basis in paper: Page 12 states: "Cross-source matching could serve as a first-stage filter, reducing corpus size before applying more expensive model-based refinement."
- Why unresolved: Model-based filtering underperforms AraMix-Matched alone, but the two approaches may be complementary.
- What evidence would resolve it: Experiments applying model-based scoring to the already-filtered matched subset.

### Open Question 4
- Question: What is the optimal cross-source agreement threshold for different languages and source availability configurations?
- Basis in paper: Page 26: "We use a fixed threshold of 2+ sources for the matched subset. The optimal threshold likely varies by language."
- Why unresolved: Arabic has 7 sources while others have 4-5; requiring 3+ sources may work better for Arabic but exclude too much content for Hindi.
- What evidence would resolve it: Grid search over threshold values (2, 3, 4+ sources) for each language.

## Limitations
- The method requires substantial existing curated datasets and is not applicable to low-resource languages where few quality sources exist
- The core assumption of independent filtering errors between source pipelines is not empirically verified
- The "model-free" and computationally free claim may not hold for languages with fewer than 4-5 independent sources

## Confidence

### High confidence
- The MixMinMatch algorithm works as described and produces measurable downstream improvements
- The MinHash deduplication and source-counting implementation is straightforward and reproducible

### Medium confidence
- The theoretical justification for cross-source agreement as a quality signal is sound, but empirical evidence for independence between sources is limited
- The claim that gains persist after removing best baseline content is supported but could have alternative explanations

### Low confidence
- The assertion that this signal is "model-free" and computationally free beyond standard deduplication may not hold for languages with fewer than 4-5 independent sources

## Next Checks

1. **Independence Analysis**: For each Arabic source, compute pairwise Jaccard similarity of their retained document sets. Quantify actual overlap vs. expected random overlap to empirically verify independence assumptions.

2. **Threshold Sensitivity**: Systematically vary the source-count threshold (≥2, ≥3, ≥4) and measure corpus size reduction vs. downstream performance degradation to find optimal trade-offs for different languages.

3. **Cross-Lingual Generalization**: Apply the same methodology to Turkish and Hindi (the other languages in the MixMinMatch dataset) and compare performance gains. This tests whether the method generalizes beyond Arabic or depends on the specific quality of available sources.