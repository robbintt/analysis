---
ver: rpa2
title: 'REA-RL: Reflection-Aware Online Reinforcement Learning for Efficient Large
  Reasoning Models'
arxiv_id: '2505.19862'
source_url: https://arxiv.org/abs/2505.19862
tags:
- reflection
- arxiv
- reward
- answer
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: REA-RL introduces a reflection-aware online reinforcement learning
  approach to address the overthinking problem in large reasoning models. It employs
  a small reflection model to detect and remove overthinking tokens during sequential
  revision, combined with parallel sampling to improve scaling efficiency.
---

# REA-RL: Reflection-Aware Online Reinforcement Learning for Efficient Large Reasoning Models

## Quick Facts
- arXiv ID: 2505.19862
- Source URL: https://arxiv.org/abs/2505.19862
- Reference count: 40
- Achieves 35% response shortening without compromising accuracy

## Executive Summary
REA-RL introduces a reflection-aware online reinforcement learning approach to address the overthinking problem in large reasoning models. It employs a small reflection model to detect and remove overthinking tokens during sequential revision, combined with parallel sampling to improve scaling efficiency. The method introduces a reflection reward to prevent models from favoring short yet non-reflective responses. Experiments demonstrate that the approach reduces inference costs while preserving performance, with the combined method achieving a 35% reduction in response length without accuracy loss.

## Method Summary
The REA-RL framework combines three key components to combat overthinking in large reasoning models. First, a small reflection model is trained to identify overthinking tokens during the revision process, allowing the system to remove unnecessary thinking steps. Second, parallel sampling is used to improve the efficiency of the reinforcement learning process, enabling better exploration of the solution space. Third, a reflection reward is introduced to the reinforcement learning objective to ensure the model maintains its ability to perform meaningful reflection rather than simply producing shorter responses. The approach operates online, continuously refining the model's reasoning behavior through interaction with the environment.

## Key Results
- Reflection model and reflection reward each reduce inference costs while preserving performance
- Combined approach achieves 35% response shortening without compromising accuracy
- Effectively reduces overthinking on simple problems while maintaining reflection ability on complex ones

## Why This Works (Mechanism)
The overthinking problem occurs when reasoning models generate excessive intermediate steps that don't contribute to the final answer quality. REA-RL addresses this by introducing a learned reflection model that can distinguish between productive thinking and unnecessary elaboration. The reflection model is trained to identify tokens that represent overthinking, allowing the system to prune these steps while preserving essential reasoning. The reflection reward in the reinforcement learning objective ensures that the model doesn't simply learn to produce shorter responses by skipping meaningful reflection steps. By combining these elements with parallel sampling for efficient exploration, the system learns to optimize the trade-off between response length and answer quality.

## Foundational Learning
- **Overthinking in LLMs**: The tendency of large language models to generate excessive intermediate reasoning steps that don't improve answer quality. This is problematic because it increases computational costs without benefits.
  - *Why needed*: Understanding overthinking is crucial as it directly impacts inference efficiency and cost
  - *Quick check*: Compare token counts and answer accuracy between early and late-stage reasoning tokens

- **Reinforcement Learning for Reasoning**: Using RL to fine-tune language models for better reasoning behavior through reward signals. This allows the model to learn optimal reasoning strategies beyond what's possible with supervised learning alone.
  - *Why needed*: RL enables the model to discover efficient reasoning patterns through trial and error
  - *Quick check*: Measure performance improvements over supervised fine-tuning baselines

- **Reflection-aware training**: Training a model to recognize when additional thinking steps are productive versus when they represent overthinking. This creates a meta-level understanding of the reasoning process.
  - *Why needed*: Provides the mechanism to identify and eliminate unnecessary reasoning steps
  - *Quick check*: Test reflection model accuracy on distinguishing useful vs overthinking tokens

## Architecture Onboarding

**Component Map:**
Reflection Model -> Token Filtering -> RL Agent -> Environment -> Reflection Reward

**Critical Path:**
Observation → Reflection Model → Token Pruning → RL Policy → Action → Reward Calculation (with Reflection Reward) → Model Update

**Design Tradeoffs:**
- Small reflection model vs large reflection model: Smaller models are more efficient but may miss nuanced overthinking patterns
- Strict vs lenient reflection thresholds: Stricter thresholds save more computation but risk removing useful reasoning
- Reward shaping complexity: More sophisticated reflection rewards provide better guidance but increase tuning difficulty

**Failure Signatures:**
- Excessive token removal leading to degraded answer quality
- Reward hacking through superficial short responses lacking real reasoning
- Reflection model bias toward certain problem types or reasoning patterns
- Inconsistent performance across different reasoning domains

**3 First Experiments:**
1. Ablation study comparing performance with and without reflection model to measure its individual contribution
2. Varying reflection reward strength to find optimal balance between efficiency and reasoning quality
3. Testing on out-of-distribution problems to assess generalization beyond training data

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness appears tied to specific dataset characteristics and may not generalize well to domains requiring extensive multi-step reasoning
- Method's performance on state-of-the-art large reasoning models remains unverified as experiments use relatively small to medium-sized models
- Reflection model quality is critical - errors in overthinking detection could degrade performance, but the paper doesn't address uncertainty handling

## Confidence
High confidence: Experimental results showing 35% response shortening with preserved accuracy on tested datasets, with sound ablation studies

Medium confidence: Claims about effectiveness across diverse reasoning tasks, as evaluation focuses primarily on mathematical and logical reasoning problems

Medium confidence: Scalability claims to larger models, as experiments were conducted on relatively small to medium-sized models

## Next Checks
1. Test REA-RL on non-mathematical reasoning tasks including commonsense reasoning, code generation, and multi-modal reasoning to assess domain generalizability

2. Evaluate the reflection model's performance on out-of-distribution problems and measure its false positive/negative rates to understand robustness

3. Conduct scaling experiments with larger reasoning models (e.g., GPT-4 level) to verify whether 35% efficiency gains translate to practical improvements at production scale