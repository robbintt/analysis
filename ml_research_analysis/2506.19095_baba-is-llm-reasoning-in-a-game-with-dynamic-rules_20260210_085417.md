---
ver: rpa2
title: 'Baba is LLM: Reasoning in a Game with Dynamic Rules'
arxiv_id: '2506.19095'
source_url: https://arxiv.org/abs/2506.19095
tags:
- object
- baba
- reasoning
- game
- rules
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether LLMs can solve Baba is You, a 2D
  puzzle game where players manipulate game rules by rearranging text blocks. Six
  LLMs are tested with three prompt types, and two models (Mistral 7B, OLMo 7B) are
  finetuned using game data.
---

# Baba is LLM: Reasoning in a Game with Dynamic Rules

## Quick Facts
- **arXiv ID:** 2506.19095
- **Source URL:** https://arxiv.org/abs/2506.19095
- **Reference count:** 0
- **Primary result:** LLMs struggle with dynamic rule reasoning in Baba is You, with larger models performing better but still failing at use-mention distinction.

## Executive Summary
This paper evaluates whether large language models can solve Baba is You, a puzzle game requiring players to manipulate game rules by rearranging text blocks. The study tests six LLMs (GPT-4o, Gemini Flash 1.5, Mistral 7B, OLMo 7B/13B, Mixtral 8x7B) across 14 levels using three prompt types: simple, rule-extended, and action-extended. Results show that larger models like GPT-4o perform better, but all models struggle with understanding dynamic rule changes, particularly the use-mention distinction. Finetuning with LoRA improves level analysis but not solution formulation, highlighting the challenge of reasoning about dynamic rules for LLMs.

## Method Summary
The study evaluates LLMs on 14 Baba is You levels using three prompt types with Plan-and-Solve chain-of-thought scaffolding. Six models are tested, with Mistral 7B and OLMo 7B finetuned using LoRA on a combined dataset (CoT-logic-reasoning, game mechanics questions, solved levels). Performance is manually analyzed across four reasoning steps: level interpretation, problem statement, solution formulation, and action formulation. For GPT-4o and Gemini, 5 runs are executed with majority voting (≥3/5 correct).

## Key Results
- Larger models (GPT-4o) outperform smaller models significantly on level analysis and solution generation.
- Finetuning improves the ability to analyze levels but does not significantly improve solution formulation.
- All models struggle with understanding the use-mention distinction, failing to reason about dynamic rule changes effectively.

## Why This Works (Mechanism)

### Mechanism 1: Prompt-Decomposed Spatial Reasoning
If models are provided with explicit rule definitions and Plan-and-Solve scaffolding, larger LLMs may better map ASCII grids to game states, though this is highly model-dependent. The "Action-extended" prompt decomposes the task into state interpretation and planning, reducing the load on the model to infer game mechanics from scratch.

### Mechanism 2: Finetuning for Syntactic-Semantic Alignment
Supervised finetuning with LoRA appears to improve the parsing of game representation (syntax) but does not necessarily generalize to strategic solution generation (semantics). Training on (Level, Solution) pairs aligns the model with the specific tokens and grid format, but acts more as a pattern matcher than a learner of underlying causal logic.

### Mechanism 3: The Use-Mention Distinction Barrier
Performance degradation suggests LLMs struggle to dynamically toggle between treating text as a physical object ("mention"—pushable blocks) and text as a functional operator ("use"—active rules). LLMs generally optimize for semantic coherence, failing to reflect on how physical manipulation of the grid alters the governing rules of the grid.

## Foundational Learning

- **Concept: The Use-Mention Distinction**
  - Why needed here: To understand why LLMs fail at Baba is You, one must grasp that the game relies on the philosophical distinction between a word as a symbol vs. a word as an object.
  - Quick check question: Can you explain why "Rock" in the sentence "Rock is heavy" is different from a sign reading "ROCK" that you can kick?

- **Concept: Parameter-Efficient Finetuning (PEFT/LoRA)**
  - Why needed here: The study uses LoRA to adapt models. Understanding this helps explain why improvements were limited to "classification" rather than "reasoning."
  - Quick check question: Does LoRA update all weights in a model or just a small adapter matrix? How might this limit learning complex new logic?

- **Concept: Chain-of-Thought (CoT) & Plan-and-Solve**
  - Why needed here: The paper utilizes "Plan-and-Solve" prompting. Evaluating the results requires knowing that this technique forces the model to generate intermediate reasoning steps before an answer.
  - Quick check question: Why might forcing a model to "think step by step" improve answers, and when might it lead to hallucinations?

## Architecture Onboarding

- **Component map:** Input Processor -> Model Layer (6 LLM variants) -> Adapters (LoRA finetuning) -> Evaluation (manual error analysis)
- **Critical path:** Grid Serialization -> Rule Parsing -> Counterfactual Simulation
- **Design tradeoffs:** Prompt Complexity vs. Model Size (complex prompts help large models but hurt smaller ones); Finetuning vs. In-Context Learning (syntax vs. strategy)
- **Failure signatures:** Hallucination (objects/rules not present), Spatial Confusion (geometrically impossible moves), Rule Confusion (misunderstanding rule mechanics)
- **First 3 experiments:**
  1. Baseline Prompt Test: Simple vs. Action-extended prompts on Mistral Level 1
  2. Finetuning Data Ablation: Finetune using only "Questions game mechanics" dataset
  3. Vertical vs. Horizontal Rule Test: Isolate performance on vertical-only rules

## Open Questions the Paper Calls Out

- Would alternative prompting strategies (Reflexion, Tree of Thoughts, analogical prompting) significantly improve LLM performance on dynamic rule manipulation tasks compared to Plan-and-Solve prompting?
- Would pretraining LLMs on substantial game-specific data improve spatial representation and rule-tracking abilities in Baba is You?
- Can automated evaluation tools be developed that reliably assess LLM reasoning quality in Baba is You without manual analysis?

## Limitations

- Manual, subjective evaluation of reasoning chains across only 14 game levels
- Effectiveness of Plan-and-Solve prompting is highly model-dependent
- Finetuning with LoRA shows syntactic improvements but fails to transfer to strategic reasoning

## Confidence

- **High Confidence:** Larger models outperform smaller models; failure modes are consistently observed
- **Medium Confidence:** Finetuning improves level analysis but not solution formulation; vertical rule placement weakness
- **Low Confidence:** Fundamental struggle with use-mention distinction; general reasoning limitations vs. philosophical barrier

## Next Checks

1. **Dataset Size Ablation:** Replicate finetuning experiments with varying dataset sizes (5, 10, 15 levels)
2. **Model Size Threshold Test:** Systematically test prompting effectiveness across model sizes (1B, 3B, 7B, 13B)
3. **Rule Orientation Isolation:** Create controlled test levels with only vertical vs. only horizontal rules