---
ver: rpa2
title: 'dKV-Cache: The Cache for Diffusion Language Models'
arxiv_id: '2505.15781'
source_url: https://arxiv.org/abs/2505.15781
tags:
- diffusion
- cache
- language
- step
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes dKV-Cache, a novel caching mechanism for diffusion
  language models (DLMs) that addresses the inference speed bottleneck caused by non-autoregressive
  architecture and bidirectional attention. The core idea leverages delayed caching
  based on token representation dynamics, where key and value states are cached only
  after tokens are decoded, with one-step delay to ensure stability.
---

# dKV-Cache: The Cache for Diffusion Language Models

## Quick Facts
- arXiv ID: 2505.15781
- Source URL: https://arxiv.org/abs/2505.15781
- Authors: Xinyin Ma; Runpeng Yu; Gongfan Fang; Xinchao Wang
- Reference count: 40
- Primary result: 2-10x inference acceleration for diffusion language models with minimal performance degradation

## Executive Summary
This paper introduces dKV-Cache, a novel caching mechanism specifically designed for diffusion language models (DLMs) that addresses the inference speed bottleneck caused by non-autoregressive architecture and bidirectional attention. The core innovation leverages delayed caching based on token representation dynamics, where key and value states are cached only after tokens are decoded with a one-step delay to ensure stability. Two variants are proposed: dKV-Cache-Decode achieves near-lossless acceleration by caching long-term key-value pairs, while dKV-Cache-Greedy trades some performance for higher speedups by restricting caching to a compact token subset.

Extensive experiments across LLaDA and Dream models demonstrate consistent 2-10x inference acceleration across diverse benchmarks including MMLU, GSM8K, HumanEval, and MBPP, with minimal or negligible performance degradation. The method proves particularly effective for long-context generation where it even improves generation quality. The approach is robust across different decoding lengths, sampling steps, and refresh intervals, representing a significant advancement in making diffusion language models more practical for real-world applications.

## Method Summary
dKV-Cache addresses the computational bottleneck in diffusion language models by implementing a delayed caching mechanism that stores key-value states only after tokens are decoded, using a one-step delay to ensure stability. The method exploits the observation that token representations become more stable after decoding, allowing for efficient caching without compromising generation quality. Two variants are introduced: dKV-Cache-Decode caches long-term key-value pairs for near-lossless acceleration (2-10x speedup), while dKV-Cache-Greedy restricts caching to a compact token subset to achieve higher speedups at the cost of some performance. The approach is specifically tailored to DLMs' non-autoregressive architecture and bidirectional attention, making it incompatible with traditional autoregressive caching methods.

## Key Results
- Achieves 2-10x inference acceleration across LLaDA and Dream models on multiple benchmarks (MMLU, GSM8K, HumanEval, MBPP)
- Near-lossless acceleration with dKV-Cache-Decode variant, showing minimal performance degradation
- Particularly effective for long-context generation, where the method can even improve generation quality
- Robust performance across different decoding lengths, sampling steps, and refresh intervals

## Why This Works (Mechanism)
The mechanism exploits the temporal dynamics of token representations in diffusion language models. During DLM inference, tokens are generated non-autoregressively with bidirectional attention, making traditional autoregressive caching ineffective. The key insight is that token representations stabilize after decoding, allowing for delayed caching without compromising generation quality. By waiting one step before caching key-value states, the method ensures that only stable representations are stored, preventing the propagation of unstable or noisy representations through the cache. This delayed approach leverages the inherent properties of diffusion models' denoising process, where each step refines token representations toward their final values.

## Foundational Learning

**Diffusion Language Models**: A class of non-autoregressive language models that generate text through iterative denoising processes. Why needed: Understanding this architecture is crucial because dKV-Cache exploits specific properties of DLMs that don't exist in autoregressive models. Quick check: Can you explain how bidirectional attention differs from the causal attention in autoregressive models?

**Key-Value Caching**: A technique for storing and reusing attention computations to accelerate inference. Why needed: Traditional KV-caching doesn't work for DLMs due to their non-autoregressive nature, motivating the need for dKV-Cache's delayed approach. Quick check: What's the computational complexity of attention without caching versus with traditional KV-caching?

**Token Representation Dynamics**: The evolution of token embeddings throughout the denoising process in DLMs. Why needed: dKV-Cache relies on the observation that representations stabilize after decoding, which is central to the delayed caching mechanism. Quick check: How do token representations change from early to late denoising steps in typical DLMs?

**Bidirectional Attention**: Attention mechanisms that allow tokens to attend to all other tokens regardless of position. Why needed: This property of DLMs necessitates different caching strategies compared to autoregressive models with causal attention. Quick check: Why can't standard autoregressive KV-caching be directly applied to models with bidirectional attention?

## Architecture Onboarding

**Component Map**: Input tokens → Denoising Network → Bidirectional Attention → Token Representations → Delayed KV Cache → Final Output tokens

**Critical Path**: The denoising network with bidirectional attention is the primary bottleneck that dKV-Cache optimizes by reducing redundant attention computations through delayed caching.

**Design Tradeoffs**: The one-step delay ensures stability but may introduce slight temporal inconsistencies; dKV-Cache-Decode prioritizes accuracy while dKV-Cache-Greedy prioritizes speed over some performance.

**Failure Signatures**: Performance degradation when token representations don't stabilize sufficiently before caching, or when the delay introduces problematic temporal dependencies in sequence generation.

**First Experiments**:
1. Baseline timing comparison of DLM inference with and without dKV-Cache across different model scales
2. Ablation study testing different delay intervals (0, 1, 2 steps) to find optimal stability-performance tradeoff
3. Long-context generation quality comparison to verify the claim that caching can improve quality in extended sequences

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions in the provided content. However, implicit questions include: How does the method generalize to other non-autoregressive architectures beyond DLMs? What are the theoretical guarantees for the temporal consistency when implementing the one-step delay? How does the caching strategy affect generation diversity and creativity?

## Limitations

- Effectiveness highly dependent on specific characteristics of diffusion language models and may not generalize to other non-autoregressive architectures without modification
- The one-step delay mechanism assumes delayed caching doesn't introduce temporal inconsistencies, but lacks theoretical guarantees about this tradeoff
- While described as "minimal or negligible," performance degradation varies across different benchmarks and model scales, with some cases showing measurable impact

## Confidence

**High Confidence**: The empirical speedups (2-10x) and core delayed caching mechanism are well-supported by experimental results across multiple models and benchmarks.

**Medium Confidence**: The claim of "near-lossless acceleration" for dKV-Cache-Decode holds for most cases but shows some performance degradation in specific benchmarks like GSM8K for LLaDA.

**Medium Confidence**: The assertion that the method is particularly effective for long-context generation and may improve quality is supported but requires more systematic investigation across diverse long-context tasks.

## Next Checks

1. Conduct ablation studies isolating the contribution of each component (delayed caching, one-step delay, compact token subset) to quantify their individual impact on speed and accuracy.

2. Test the method's robustness across a wider range of diffusion models with different architectures, scales, and training objectives to establish generalizability.

3. Perform theoretical analysis of the temporal consistency guarantees when implementing the one-step delay caching mechanism, particularly for applications requiring strict coherence.