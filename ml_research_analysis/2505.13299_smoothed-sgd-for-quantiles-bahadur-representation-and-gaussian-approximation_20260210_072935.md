---
ver: rpa2
title: 'Smoothed SGD for quantiles: Bahadur representation and Gaussian approximation'
arxiv_id: '2505.13299'
source_url: https://arxiv.org/abs/2505.13299
tags:
- quantile
- where
- algorithm
- theorem
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses memory and monotonicity issues in quantile
  estimation for streaming data by proposing a smoothed version of the stochastic
  gradient descent (SGD) algorithm. The key innovation is replacing the indicator
  function in the standard SGD quantile estimator with a smooth, piece-wise linear
  approximation, ensuring monotonicity in the quantile level and eliminating quantile
  crossing.
---

# Smoothed SGD for quantiles: Bahadur representation and Gaussian approximation

## Quick Facts
- arXiv ID: 2505.13299
- Source URL: https://arxiv.org/abs/2505.13299
- Reference count: 40
- Primary result: Uniform Bahadur representation and Gaussian approximation for smoothed SGD quantile estimator with exponentially growing dimension

## Executive Summary
This paper addresses memory and monotonicity issues in quantile estimation for streaming data by proposing a smoothed version of the stochastic gradient descent (SGD) algorithm. The key innovation is replacing the indicator function in the standard SGD quantile estimator with a smooth, piece-wise linear approximation, ensuring monotonicity in the quantile level and eliminating quantile crossing. Theoretical contributions include non-asymptotic tail probability bounds for both smoothed SGD with and without Polyak-Ruppert averaging, and a uniform Bahadur representation for the averaged case, enabling Gaussian approximation. The Gaussian approximation result supports uniform confidence bands over both quantile levels and data dimensions, with dimension p allowed to grow exponentially in sample size n. Simulations confirm good finite-sample performance, with empirical coverage close to nominal levels, especially for large n and appropriate learning rate β. The method is shown to work well for both unconditional and conditional quantile estimation, including in high-dimensional settings.

## Method Summary
The method proposes smoothed stochastic gradient descent (SGD) for quantile estimation in streaming data. The key innovation is replacing the non-smooth indicator function in the standard SGD quantile estimator with a smooth, piece-wise linear approximation, ensuring monotonicity in the quantile level and eliminating quantile crossing. The algorithm uses a Polyak-Ruppert averaging scheme and requires specific constraints on the learning rate parameter β. Theoretical analysis provides non-asymptotic tail probability bounds for both smoothed SGD with and without averaging, and establishes a uniform Bahadur representation for the averaged case, enabling Gaussian approximation. The method is applied to both unconditional and conditional quantile estimation, with the conditional case using local constant kernel regression.

## Key Results
- The smoothed SGD algorithm eliminates quantile crossing and ensures monotonicity in the quantile level
- A uniform Bahadur representation is established for the averaged estimator, enabling Gaussian approximation
- Gaussian approximation supports uniform confidence bands over both quantile levels and data dimensions, with dimension p allowed to grow exponentially in sample size n
- Simulation results confirm good finite-sample performance, with empirical coverage close to nominal levels, especially for large n and appropriate learning rate β

## Why This Works (Mechanism)
The smoothed SGD algorithm works by replacing the non-smooth indicator function in the standard SGD quantile estimator with a smooth, piece-wise linear approximation. This smoothing ensures monotonicity in the quantile level and eliminates quantile crossing. The Polyak-Ruppert averaging scheme reduces variance and enables the establishment of a uniform Bahadur representation. The Gaussian approximation result is made possible by the non-asymptotic tail probability bounds and the uniform Bahadur representation, which together provide the necessary conditions for the application of the functional central limit theorem.

## Foundational Learning
- Stochastic gradient descent: A first-order iterative optimization algorithm for finding a local minimum of a differentiable function. Needed for understanding the baseline algorithm being smoothed.
- Bahadur representation: An asymptotic expansion of sample quantiles in terms of empirical processes. Needed for establishing the theoretical foundation of the quantile estimator.
- Gaussian approximation: A method for approximating the distribution of a statistic by a Gaussian process. Needed for constructing uniform confidence bands.
- Quantile crossing: A phenomenon where estimated quantiles fail to preserve their order. Needed for understanding the motivation for smoothing.
- Functional central limit theorem: A generalization of the central limit theorem to infinite-dimensional spaces. Needed for establishing the Gaussian approximation result.
- Kernel regression: A non-parametric method for estimating the conditional expectation of a random variable. Needed for understanding the conditional quantile estimation application.

## Architecture Onboarding

Component map:
- Data stream W_t -> Smoothed SGD algorithm -> Quantile estimates Z_{i,k} -> Averaged estimator Z_{i,n} -> Bahadur representation -> Gaussian approximation -> Uniform confidence bands

Critical path:
The critical path is: Data stream W_t -> Smoothed SGD algorithm -> Averaged estimator Z_{i,n} -> Bahadur representation -> Gaussian approximation. This path is critical because it establishes the theoretical foundation for the method and enables the construction of uniform confidence bands.

Design tradeoffs:
The main design tradeoff is between the smoothness of the approximation function g(x) and the computational efficiency of the algorithm. A smoother g(x) may lead to better theoretical properties but could increase computational cost. The choice of learning rate parameter β also involves a tradeoff between convergence speed and the accuracy of the Gaussian approximation.

Failure signatures:
Potential failure modes include:
- Quantile crossing if the smoothing is insufficient
- Slow convergence or poor coverage if β is too small
- Breakdown of the Gaussian approximation if β is too large or n is too small relative to p
- Bias in conditional quantile estimation if the bandwidth h is not chosen appropriately

First experiments:
1. Implement the smoothed SGD algorithm and verify that it eliminates quantile crossing
2. Test the algorithm on a simple unconditional quantile estimation problem and compare the results to the standard SGD algorithm
3. Apply the method to conditional quantile estimation using the local constant kernel regression approach and assess the coverage of the resulting confidence bands

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the uniform Bahadur representation and Gaussian approximation results be extended to streaming data with temporal dependence?
- Basis in paper: Section 2 explicitly assumes i.i.d. random vectors $W_t$, and the decomposition in Equation (22) relies on $\xi_{i,k+1}$ being martingale differences with respect to the independent history $\mathcal{F}_{i,k}$.
- Why unresolved: The proof techniques rely heavily on the independence of the current observation $X_{i,k+1}$ from the algorithm's past state to establish the linear approximation and concentration bounds.
- What evidence would resolve it: A theoretical extension of Theorem 4.2 that holds under mixing conditions (e.g., $\alpha$-mixing or $\beta$-mixing) for the data stream.

### Open Question 2
- Question: Is the upper bound on the learning rate parameter $\beta < (1+\sqrt{5})/4 \approx 0.809$ in Assumption 2 strictly necessary for the Gaussian approximation?
- Basis in paper: Assumption 2 imposes this specific constraint, whereas standard SGD analysis often allows $\beta < 1$. The paper notes in Remark 2 that the convergence rates change based on $\beta$ regimes.
- Why unresolved: This constraint appears to be a technical requirement for the concentration inequalities used in the proof of Theorem 4.1 rather than a fundamental property of the algorithm.
- What evidence would resolve it: A proof of Theorem 4.2 that relaxes this upper bound, or a simulation study demonstrating performance degradation or success as $\beta$ approaches 1.

### Open Question 3
- Question: How does the choice of smoothing kernel $g(\cdot)$ affect the convergence rates and the remainder term in the Bahadur representation?
- Basis in paper: Section 2 connects the piecewise linear function $g(x)$ to a uniform kernel. The authors note the connection to "conquer" methods but use a specific, non-smooth $g(x)$ (Lipschitz but not differentiable everywhere).
- Why unresolved: The non-differentiability of $g$ at $\pm 1$ impacts the Taylor expansions used in the proofs (e.g., bounds on $\rho_{i,k}$). It is unclear if a smoother kernel would tighten the bounds in Theorem 3.3.
- What evidence would resolve it: A comparative theoretical analysis or simulation evaluating the smoothed SGD algorithm using higher-order smooth kernels (e.g., Epanechnikov or Gaussian) versus the proposed piecewise linear function.

### Open Question 4
- Question: Can the theoretical guarantees for unconditional quantiles be rigorously extended to the nonparametric conditional quantile estimation setting described in Section 5?
- Basis in paper: Section 5 applies the method to conditional quantile estimation via local constant kernel regression (Equation 11) and claims the Gaussian approximation supports it, but all main theorems (3.1–4.2) are stated for unconditional quantiles.
- Why unresolved: The conditional case introduces bias from the kernel bandwidth $h$ and dependency on covariates $X_k$, which are not accounted for in the main theoretical derivations.
- What evidence would resolve it: Derivation of a uniform Bahadur representation specifically for the averaged estimator $\bar{Z}_{i,n}(\tau)$ that explicitly handles the kernel bias and conditional density estimation.

## Limitations
- The theoretical results assume bounded gradients with linear growth, which may not hold in all applications
- The Gaussian approximation result requires n to be sufficiently large relative to p, and the coverage accuracy depends on the choice of β
- The piece-wise linear smoothing scheme introduces approximation error that is not fully quantified in finite samples
- The simulation results are limited in scope and do not explore robustness to heavy-tailed or contaminated data

## Confidence
- Bahadur representation and Gaussian approximation (High): Supported by rigorous non-asymptotic analysis and consistent simulation results
- Monotonicity and quantile crossing elimination (High): Theoretically justified and demonstrated in simulations
- Uniform confidence bands with exponential p growth (Medium): Theoretical results support this, but finite-sample performance is only partially validated

## Next Checks
1. Test the algorithm's robustness to heavy-tailed or contaminated data distributions
2. Conduct simulations in higher dimensions (e.g., p > 100) to assess practical limits of the exponential growth claim
3. Compare the smoothed SGD quantile estimator's performance against non-parametric alternatives in terms of computational efficiency and accuracy