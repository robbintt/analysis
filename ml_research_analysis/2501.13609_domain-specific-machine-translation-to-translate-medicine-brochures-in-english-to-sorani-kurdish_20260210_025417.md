---
ver: rpa2
title: Domain-Specific Machine Translation to Translate Medicine Brochures in English
  to Sorani Kurdish
arxiv_id: '2501.13609'
source_url: https://arxiv.org/abs/2501.13609
tags:
- translation
- brochures
- kurdish
- machine
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the lack of Kurdish language medical resources
  by developing a specialized machine translation model for translating English medicine
  brochures into Sorani Kurdish. A parallel corpus of 22,940 aligned sentence pairs
  from 319 brochures was created and used to train a Statistical Machine Translation
  (SMT) model using the Moses toolkit.
---

# Domain-Specific Machine Translation to Translate Medicine Brochures in English to Sorani Kurdish
## Quick Facts
- arXiv ID: 2501.13609
- Source URL: https://arxiv.org/abs/2501.13609
- Reference count: 23
- This paper addresses the lack of Kurdish language medical resources by developing a specialized machine translation model for translating English medicine brochures into Sorani Kurdish.

## Executive Summary
This paper tackles the scarcity of Kurdish language medical resources by creating a domain-specific machine translation system for English-to-Sorani Kurdish medicine brochures. The authors developed a parallel corpus of 22,940 aligned sentence pairs from 319 pharmaceutical brochures and trained a Statistical Machine Translation model using the Moses toolkit. Through seven experimental corpus configurations, they demonstrated that oversampling shorter brochures within categories achieved the highest BLEU score of 48.93, which improved to 56.87 after post-processing with a medical dictionary.

## Method Summary
The authors created a parallel corpus by extracting text from English and Sorani Kurdish medicine brochures using OCR tools (PDF2Go and i2OCR), manually correcting errors, and aligning sentences using InterText/Hunalign. They trained a phrase-based Statistical Machine Translation model with Moses, testing seven corpus variants including shuffling, XML tagging, categorization, undersampling, and oversampling. Post-processing replaced unknown words using a custom medical dictionary with Google Cloud Translation API fallback. Evaluation combined automatic BLEU scoring with human assessment by native Kurdish-speaking pharmacists, physicians, and medicine users.

## Key Results
- Best SMT model achieved BLEU score of 48.93 using oversampling technique
- Post-processing with medical dictionary improved BLEU scores to 56.87, 31.05, and 40.01 for three new brochures
- Human evaluation showed 50% of professionals found translations consistent, 83.3% rated them accurate, and 66.7% of users felt confident using medications based on translations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Oversampling shorter brochures to match longer ones within categories improves translation quality for low-resource medical domains.
- Mechanism: By duplicating sentences from shorter brochures to equalize lengths across categories (Experiment 7), the model sees more balanced training data. This reduces dominance by longer documents and ensures rare medical terms appear with sufficient frequency for statistical learning.
- Core assumption: Category-level balance correlates with improved phrase-table coverage for medical terminology.
- Evidence anchors:
  - [abstract] "BLEU scores ranging from 22.65 to 48.93" with Experiment 7 achieving the highest (48.93)
  - [section 5.3.1] "oversampling technique in the seventh experiment... achieved the highest score of 48.93" vs undersampling at 30.37
  - [corpus] Weak direct evidence—neighbor papers focus on Kurdish NLP tasks (NER, summarization) but not oversampling strategies for MT.
- Break condition: If categories have highly uneven intrinsic complexity or if duplicated sentences cause overfitting to specific phrasing patterns, oversampling may not generalize.

### Mechanism 2
- Claim: Domain-specific parallel corpora from pharmaceutical sources outperform general-purpose MT for medical brochure translation.
- Mechanism: The 22,940 sentence pairs from 319 medicine brochures contain specialized vocabulary (dosage instructions, side effects, therapeutic uses) that general corpora lack. The SMT phrase-table learns direct mappings for these domain-specific terms rather than compositional translations.
- Core assumption: Medical terminology in Kurdish pharmaceutical brochures follows consistent patterns within this domain.
- Evidence anchors:
  - [abstract] "parallel corpus of 22,940 aligned sentence pairs from 319 brochures" sourced from pharmaceutical companies
  - [section 5.1] Brochures include "therapeutic uses, composition, dosage, safety tips, side effects"
  - [corpus] Neighbor paper "CENTRAL KURDISH MACHINE TRANSLATION" (Amini et al.) shows general NMT achieves only 16.81 BLEU for English-to-Kurdish, compared to this domain-specific approach reaching up to 56.87 after post-editing.
- Break condition: If test documents contain terminology outside the pharmaceutical domain (e.g., diagnoses, hospital procedures), quality may degrade significantly.

### Mechanism 3
- Claim: Post-processing with medical dictionary substitution addresses unknown-word failures in low-resource SMT.
- Mechanism: SMT systems produce "unknown word" tokens when source words lack phrase-table entries. By consulting a manually constructed medical dictionary—and falling back to Google Cloud Translation API—these gaps are filled post-hoc, substantially improving BLEU.
- Core assumption: Unknown words are primarily medical terms with standard Kurdish equivalents, not context-dependent expressions.
- Evidence anchors:
  - [abstract] "post-processing with a medical dictionary, resulting in BLEU scores of 56.87, 31.05, and 40.01"
  - [section 5.6] Pre-post-editing scores: 25.79, 6.46, 12.25 → Post-editing: 56.87, 31.05, 40.01 (improvements of 31.08, 24.59, and 27.76 points respectively)
  - [corpus] Limited corpus evidence for post-editing efficacy in Kurdish specifically; neighbor papers don't address this technique.
- Break condition: If unknown words include polysemous terms requiring context beyond dictionary lookup, simple substitution may introduce errors.

## Foundational Learning

- Concept: **Phrase-based Statistical Machine Translation (PB-SMT)**
  - Why needed here: The Moses toolkit implements PB-SMT, which learns phrase-to-phrase mappings rather than word-to-word. Understanding this explains why corpus quality and alignment directly affect translation quality.
  - Quick check question: Can you explain why "side effects" should be learned as a single phrase unit rather than two separate words?

- Concept: **BLEU Score and n-gram overlap**
  - Why needed here: The paper relies entirely on BLEU for automatic evaluation. BLEU measures precision of n-gram matches against reference translations, with a brevity penalty.
  - Quick check question: Why might a translation with perfect meaning but different word order receive a lower BLEU score?

- Concept: **Word Alignment (Giza++)**
  - Why needed here: Before phrase extraction, Giza++ establishes which source words correspond to which target words. Alignment quality constrains phrase-table quality.
  - Quick check question: What happens to phrase-table quality if alignment systematically mismatches medical terms?

## Architecture Onboarding

- Component map:
  - PDF/AI brochures -> OCR extraction (PDF2Go, i2OCR) -> Manual correction -> Sentence alignment (InterText/Hunalign)
  - Corpus Variants: 7 versions testing shuffling, XML tagging, categorization, undersampling, oversampling
  - Training Pipeline: Tokenization -> Truecasing -> Giza++ alignment -> KenLM language model -> Moses phrase-table -> Decoder
  - Evaluation: BLEU (automatic) + Human review (pharmacists, physicians, users)
  - Post-processing: Medical dictionary lookup -> Google Cloud Translation API fallback

- Critical path:
  1. Corpus quality (OCR accuracy, alignment precision) — garbage in, garbage out
  2. Giza++ word alignment — determines phrase-table coverage
  3. KenLM language model fluency — affects output naturalness
  4. Post-processing dictionary coverage — addresses unknown-word gaps

- Design tradeoffs:
  - **Oversampling vs. Undersampling**: Oversampling (48.93 BLEU) preserves data but risks overfitting; undersampling (30.37 BLEU) reduces overfitting risk but loses rare-term exposure
  - **SMT vs. NMT**: Paper uses SMT for interpretability and lower data requirements; NMT (per neighbor papers) may perform better with larger datasets but requires more compute
  - **Manual vs. Automatic post-editing**: Manual dictionary curation ensures accuracy but doesn't scale; the paper suggests probability-based automation as future work

- Failure signatures:
  - BLEU < 15: Likely severe alignment errors or domain mismatch
  - High unknown-word count: Phrase-table coverage insufficient for test vocabulary
  - Human evaluators rate "consistent" lower than "accurate": Structural/syntactic issues vs. terminology issues

- First 3 experiments:
  1. Reproduce Experiment 1 (baseline): Train on original 22,940-line corpus with 90/10 split. Expected BLEU ~26-27. Diagnose unknown-word patterns.
  2. Reproduce Experiment 7 (best config): Apply oversampling by category. Compare BLEU improvement magnitude. Check for overfitting on duplicated sentences.
  3. Test generalization: Translate a brochure from a third pharmaceutical source not in training data. Measure BLEU drop vs. in-domain test set to quantify domain boundary.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent would state-of-the-art Neural Machine Translation (NMT) approaches improve translation quality over the current Statistical Machine Translation (SMT) baseline?
- Basis in paper: [explicit] The authors state that "leveraging state-of-the-art neural network approaches could further boost performance, especially with larger datasets."
- Why unresolved: The current study utilized the Moses SMT toolkit exclusively, and the authors note that NMT approaches were not tested in this phase of research.
- What evidence would resolve it: A comparative experiment training an NMT model (e.g., Transformer architecture) on the created corpus, evaluated against the SMT baseline using BLEU scores and human evaluation metrics.

### Open Question 2
- Question: Can an automated probability-based post-editing system effectively handle unknown words without the need for manual dictionary intervention?
- Basis in paper: [explicit] The authors suggest that "Automating the post-editing process with probability-based models would assist in handling unknown words."
- Why unresolved: The current methodology relied on manual post-processing where unknown words were replaced using a medical dictionary and the Google Cloud Translation API.
- What evidence would resolve it: A study implementing a probability-based model for unknown word handling, comparing its BLEU scores and terminological accuracy against the manual post-editing results reported in the paper.

### Open Question 3
- Question: How does the domain-specific SMT model compare to general-purpose commercial translation systems like Google Translator and Claude AI in the medical domain?
- Basis in paper: [explicit] The authors identify the need for "evaluating and comparing the current system with existing translation platforms, such as Google Translator and Claude AI."
- Why unresolved: The evaluation was limited to internal metrics (BLEU) and specific human evaluators (pharmacists/users), without benchmarking against external commercial translation engines.
- What evidence would resolve it: A side-by-side evaluation of the proposed model's translations against Google and Claude outputs, judged by medical professionals for accuracy, consistency, and user confidence.

### Open Question 4
- Question: Does integrating rule-based or context-aware systems significantly reduce structural errors such as broken sentences and incorrect translations?
- Basis in paper: [explicit] The authors propose that "Incorporating context-aware and rule-based systems could address challenges like incorrect translations and broken sentences."
- Why unresolved: The current system is purely statistical (SMT), and the authors noted that the initial translations contained structural errors and incorrect translations before manual post-processing.
- What evidence would resolve it: An experiment integrating a hybrid rule-based layer into the translation pipeline, measuring the reduction in structural errors (broken sentences) through human evaluation.

## Limitations
- The parallel corpus and medical dictionary are not publicly available, making direct reproduction difficult
- Detailed Moses configuration parameters and training hyperparameters were not provided
- Lack of statistical significance testing for the reported improvements
- Results are limited to one language pair (English to Sorani Kurdish) and one domain (pharmaceutical brochures)

## Confidence
- **High Confidence**: The methodology for corpus creation (OCR extraction, sentence alignment, oversampling) is clearly described and follows standard NLP practices.
- **Medium Confidence**: The BLEU score improvements (e.g., 48.93 vs 30.37) are plausible given the domain-specific approach and post-processing, but the lack of statistical testing and public data limits certainty.
- **Low Confidence**: The generalizability of the results to other low-resource language pairs or medical domains is not established due to the narrow scope of the study.

## Next Checks
1. Recreate the corpus using publicly available English-Kurdish medical text and test the oversampling strategy to see if similar BLEU gains are achieved.
2. Implement the post-processing pipeline with a new medical dictionary and measure the impact on unknown-word reduction and BLEU scores.
3. Conduct a small-scale human evaluation with Kurdish speakers outside the original group to verify the consistency and accuracy findings.