---
ver: rpa2
title: 'Ontology Learning and Knowledge Graph Construction: A Comparison of Approaches
  and Their Impact on RAG Performance'
arxiv_id: '2511.05991'
source_url: https://arxiv.org/abs/2511.05991
tags:
- ontology
- knowledge
- retrieval
- graph
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study compares Knowledge Graph (KG) construction strategies
  for Retrieval-Augmented Generation (RAG) systems, focusing on ontologies extracted
  from relational databases versus text corpora. It evaluates vector-based RAG, GraphRAG,
  and ontology-guided KGs with and without chunk information.
---

# Ontology Learning and Knowledge Graph Construction: A Comparison of Approaches and Their Impact on RAG Performance

## Quick Facts
- **arXiv ID:** 2511.05991
- **Source URL:** https://arxiv.org/abs/2511.05991
- **Reference count:** 18
- **Key result:** Ontology-guided KGs with textual chunks achieve 90% accuracy, significantly outperforming vector retrieval (60%) and matching GraphRAG performance.

## Executive Summary
This study compares Knowledge Graph (KG) construction strategies for Retrieval-Augmented Generation (RAG) systems, focusing on ontologies extracted from relational databases versus text corpora. It evaluates vector-based RAG, GraphRAG, and ontology-guided KGs with and without chunk information. Results show that ontology-guided KGs incorporating textual chunks achieve competitive performance with state-of-the-art GraphRAG (90% accuracy), significantly outperforming vector retrieval (60% accuracy). Notably, ontology-guided KGs from relational databases match the performance of text-based ones while offering substantial cost savings through a one-time-only ontology learning process and avoiding the complexity of ontology merging.

## Method Summary
The paper evaluates three RAG approaches: Vector RAG using FAISS indexing with nomic-embed-text embeddings, Microsoft GraphRAG with default local search, and custom ontology-guided KGs built using LLMGraphTransformer. Two ontology sources are compared: relational database schemas (using adapted RIGOR method) and text corpora (using adapted Bakker et al. method). Retrieval uses a custom Prize-Collecting Steiner Tree (PCST) optimization that first identifies top-k similar nodes via embedding cosine similarity, then extracts connected subgraphs maximizing relevance while minimizing edge count. Evaluation uses a manually created dataset of 20 domain-specific questions against a single grant application document.

## Key Results
- Vector RAG achieved 60% accuracy, significantly lower than all graph-based approaches
- Ontology-guided KGs with chunk information achieved 90% accuracy, matching GraphRAG performance
- Without chunk integration, ontology-guided KGs performed poorly (15-20% accuracy)
- RDB-derived ontologies performed equivalently to text-derived ontologies (both 90%) while offering substantial cost savings
- GraphRAG achieved 90% accuracy with local search and 85% with global search

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Integrating textual chunks directly into KG nodes dramatically improves retrieval accuracy and answer completeness.
- Mechanism: Ontology-guided KGs with chunks combine symbolic structure (entities, relations) with contextual text segments, enabling both relational reasoning and rich semantic grounding. Without chunks, graphs provide only abstract structure; with chunks, nodes carry the actual source text needed for answer generation.
- Core assumption: LLMs require both structural context (how entities relate) and textual context (what entities contain) to produce complete, factual responses.
- Evidence anchors:
  - [abstract] "ontology-guided KGs incorporating chunk information achieve competitive performance with state-of-the-art GraphRAG (90% accuracy)"
  - [section 5.2] "Text Ontology KG achieved only 15% accuracy... while the RDB Ontology KG reached 20% accuracy... These results indicate that adding textual chunks to graph nodes significantly improves factual grounding"

### Mechanism 2
- Claim: Ontologies extracted from relational databases yield equivalent RAG performance to text-derived ontologies at substantially lower cost.
- Mechanism: RDB schemas are stable, structured, and bounded—ontology learning runs once. Text corpora require continuous ontology extraction, merging, and alignment as new documents arrive. The paper shows both approaches produce functionally equivalent graphs when properly constructed.
- Core assumption: Domain knowledge encoded in RDB schemas (tables, foreign keys, constraints) captures the essential entity-relationship structure that text-derived ontologies would also extract.
- Evidence anchors:
  - [abstract] "ontology-guided KGs from relational databases match the performance of text-based ones while offering substantial cost savings through a one-time-only ontology learning process"
  - [section 5.2] Both RDB and Text ontology KGs with chunks achieved 18/20 correct (90%)

### Mechanism 3
- Claim: Prize-Collecting Steiner Tree (PCST) optimization retrieves coherent, connected subgraphs while balancing relevance and conciseness.
- Mechanism: The retriever first identifies top-k similar nodes via embedding cosine similarity, then assigns "prizes" to nodes proportional to relevance. PCST optimization finds a connected subgraph that maximizes total prize while minimizing edge count—ensuring retrieved context is both relevant and structurally coherent.
- Core assumption: Relevant information is distributed across connected nodes; isolated high-similarity nodes may be insufficient for multi-hop reasoning.
- Evidence anchors:
  - [section 3.3] "Using the Prize-Collecting Steiner Tree (PCST) optimization, a connected subgraph of relevant nodes and edges is extracted, maximizing the number of nodes and minimizing the number of edges retrieved"

## Foundational Learning

- **Concept: Ontology vs. Knowledge Graph**
  - Why needed here: The paper distinguishes ontologies (schema/concept definitions) from KGs (instantiated data). Ontology guides construction; KG is the queryable artifact.
  - Quick check question: Can you explain why extracting an ontology from a schema is fundamentally different from extracting one from unstructured text?

- **Concept: Prize-Collecting Steiner Tree (PCST)**
  - Why needed here: The retriever's subgraph extraction relies on PCST. Understanding this optimization is essential for debugging retrieval quality and tuning parameters.
  - Quick check question: Given a graph with node prizes and edge costs, what does PCST optimize for, and what happens if all edges have uniform cost?

- **Concept: Chunk-Node Binding in KGs**
  - Why needed here: The paper's central finding is that binding text chunks to graph nodes (not just entity labels) is critical for performance.
  - Quick check question: What information would be lost if a KG contained only entity names and relation types, without associated text chunks?

## Architecture Onboarding

- **Component map:** Ontology Extractor (RDB or Text) -> KG Constructor (LLMGraphTransformer) -> Retriever (PCST-based) -> Context Formatter -> LLM Generator
- **Critical path:** If ontology is malformed or incomplete → KG construction produces sparse/noisy graphs → retrieval fails. If chunks are not bound to nodes → accuracy drops from 90% to 15-20%. If PCST parameters are poorly tuned → retrieved subgraph is either too sparse (missing context) or too dense (noise, token waste).
- **Design tradeoffs:** RDB ontology extraction offers lower cost and stable schema but requires existing database; text ontology extraction is more flexible but requires ongoing merging/alignment; chunk integration improves accuracy significantly but increases graph size; PCST retriever provides structured context but adds complexity vs. simple top-k retrieval.
- **Failure signatures:** High "I don't know" rate indicates subgraph not reaching relevant nodes; high "Incomplete" rate suggests chunks not bound to nodes; high "False/Wrong" rate indicates ontology hallucination or chunk misattribution; performance degrades with new documents when using text-based ontology.
- **First 3 experiments:**
  1. Reproduce chunk vs. no-chunk comparison: Build the same KG twice (with and without chunk binding), run the 20-question evaluation, confirm the 15-20% → 90% accuracy jump
  2. Test retriever sensitivity: Vary PCST parameters (node prize scaling, edge costs) and measure impact on subgraph size and answer completeness
  3. Validate RDB ontology quality: Extract ontology from your own database schema using the RIGOR-adapted pipeline, manually inspect TTL output for missing relations or hallucinated constraints before KG construction

## Open Questions the Paper Calls Out
- How does the comparative performance of RDB-derived versus text-derived ontology-guided Knowledge Graphs change when applied to larger, multi-document corpora across diverse domains outside of grant applications?
- Does the performance parity between RDB-based and text-based ontologies persist in scenarios involving highly dynamic, continuously updating text data, or does the "one-time" nature of RDB learning become a limitation?
- To what extent are the reported accuracy levels dependent on the specific Prize-Collecting Steiner Tree (PCST) heuristic used for retrieval, and would the ranking of methods change with a Graph Neural Network (GNN)-based retriever?

## Limitations
- Evaluation based on a single anonymized grant application document, limiting external validity
- No statistical significance testing on the 20-question evaluation results
- Absence of quantitative cost analysis comparing different approaches
- Custom PCST retriever implementation lacks detailed parameter specifications for exact reproduction

## Confidence
- **High confidence:** The fundamental mechanism that chunk integration into KG nodes dramatically improves performance (15-20% → 90% accuracy)
- **Medium confidence:** The claim that RDB ontology-derived KGs match text-derived performance while offering cost savings
- **Low confidence:** The generalizability of the 90% accuracy benchmark to other domains or document types

## Next Checks
1. Evaluate the same KG construction approaches on at least three diverse document corpora (medical literature, legal contracts, technical documentation) using the same 20-question evaluation protocol
2. Conduct significance testing (e.g., McNemar's test) on the 20-question evaluation across multiple runs to determine if performance differences are statistically reliable
3. Measure and report actual resource consumption (GPU hours, LLM API tokens, storage) for each KG construction approach across different corpus sizes to validate claimed cost advantages