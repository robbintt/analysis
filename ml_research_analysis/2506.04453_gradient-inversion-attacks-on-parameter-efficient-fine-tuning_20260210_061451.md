---
ver: rpa2
title: Gradient Inversion Attacks on Parameter-Efficient Fine-Tuning
arxiv_id: '2506.04453'
source_url: https://arxiv.org/abs/2506.04453
tags:
- adapter
- patches
- images
- layer
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work demonstrates the first successful gradient inversion
  attack on parameter-efficient fine-tuning (PEFT) in federated learning, targeting
  adapter-based PEFT mechanisms. By maliciously designing both the pretrained model
  and adapter modules, the attacker can reconstruct local fine-tuning data from adapter
  gradients alone, bypassing the typical privacy assumption that frozen backbone models
  protect sensitive information.
---

# Gradient Inversion Attacks on Parameter-Efficient Fine-Tuning

## Quick Facts
- arXiv ID: 2506.04453
- Source URL: https://arxiv.org/abs/2506.04453
- Reference count: 40
- Primary result: Demonstrates first successful gradient inversion attack on adapter-based PEFT in federated learning, reconstructing up to 85.9% of image patches with high fidelity.

## Executive Summary
This work presents the first successful gradient inversion attack targeting parameter-efficient fine-tuning (PEFT) in federated learning scenarios. The attack exploits the privacy assumption that frozen backbone models protect sensitive information, demonstrating that malicious initialization of both the pretrained model and adapter modules enables reconstruction of local fine-tuning data from adapter gradients alone. Through extensive experiments across CIFAR-10, CIFAR-100, TinyImageNet, and ImageNet, the attack achieves remarkable reconstruction fidelity, recovering up to 85.9% of image patches with LPIPS scores as low as 0.08 and SSIM up to 0.88, even with small adapter dimensions or over multiple training rounds.

## Method Summary
The attack methodology involves maliciously designing both the pretrained model initialization and adapter architecture to enable gradient inversion from adapter updates. The attacker leverages carefully crafted initialization schemes that create correlations between adapter parameters and input features, allowing reconstruction of fine-tuning data from gradient updates. The attack works across various adapter configurations and demonstrates effectiveness even with small adapter dimensions, showing that PEFT does not inherently provide privacy protection in federated learning settings.

## Key Results
- Successfully reconstructed up to 85.9% of image patches from adapter gradients alone
- Achieved LPIPS scores as low as 0.08 and SSIM up to 0.88, indicating high reconstruction fidelity
- Attack remains effective across multiple adapter configurations, small dimensions, and multiple training rounds
- Demonstrated vulnerability across CIFAR-10, CIFAR-100, TinyImageNet, and ImageNet datasets

## Why This Works (Mechanism)
The attack exploits the fundamental vulnerability in PEFT where adapter gradients contain sufficient information to reconstruct input data, despite the backbone model being frozen. By maliciously controlling the initialization of both the pretrained model and adapter modules, the attacker creates specific parameter structures that preserve input-output relationships in the adapter gradients. This allows gradient inversion techniques to recover fine-tuning data from what was assumed to be privacy-preserving parameter-efficient updates.

## Foundational Learning
- Federated Learning Fundamentals: Understanding distributed training where clients update local models and share gradients (needed to grasp the attack context; quick check: understand FedAvg algorithm)
- Parameter-Efficient Fine-Tuning: Knowledge of adapter-based methods that add small trainable modules while freezing the backbone (needed to understand attack surface; quick check: compare full fine-tuning vs adapter-based approaches)
- Gradient Inversion Techniques: Familiarity with methods to reconstruct input data from model gradients (needed to comprehend attack methodology; quick check: understand Deep Leakage from Gradients paper)

## Architecture Onboarding
- Component Map: Pretrained Model -> Adapter Modules -> Gradient Aggregation -> Attacker Reconstruction
- Critical Path: Malicious initialization → Adapter gradient computation → Gradient transmission → Inversion attack → Data reconstruction
- Design Tradeoffs: Attack effectiveness vs. adapter size and complexity; complete control assumption vs. practical feasibility
- Failure Signatures: Poor reconstruction quality when initialization control is limited; reduced effectiveness with noise injection
- First Experiments: 1) Test attack on full fine-tuning baseline, 2) Evaluate with partial initialization control, 3) Assess attack effectiveness with differential privacy noise

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes attacker has full control over both pretrained model initialization and adapter architecture design
- Evaluation focused primarily on image classification tasks; effectiveness on other modalities unclear
- Practical feasibility depends on likelihood of achieving complete adversarial control in real federated learning deployments

## Confidence
- PEFT does not inherently provide privacy protection: High confidence
- Successful reconstruction across multiple adapter configurations: High confidence
- Practical severity in real-world deployments: Medium confidence

## Next Checks
1. Evaluate the attack's effectiveness when the attacker has partial rather than complete control over the model initialization and adapter design
2. Test the attack on larger-scale vision tasks and non-image modalities to assess generalizability
3. Investigate whether the attack remains effective when combined with common federated learning defenses like secure aggregation or differential privacy at reasonable noise levels