---
ver: rpa2
title: 'Dynaword: From One-shot to Continuously Developed Datasets'
arxiv_id: '2508.02271'
source_url: https://arxiv.org/abs/2508.02271
tags:
- danish
- dynaword
- data
- datasets
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Danish Dynaword addresses the need for large-scale, openly licensed
  datasets for Danish language processing by introducing a continuously developed
  corpus that is over four times larger than comparable datasets. The dataset is built
  using a framework that ensures traceable and open licensing, reproducibility, documentation,
  and extensibility.
---

# Dynaword: From One-shot to Continuously Developed Datasets

## Quick Facts
- arXiv ID: 2508.02271
- Source URL: https://arxiv.org/abs/2508.02271
- Reference count: 26
- Primary result: Danish Dynaword is over 4x larger than comparable Danish datasets with 5.9% perplexity improvement over Danish Gigaword

## Executive Summary
Danish Dynaword addresses the critical need for large-scale, openly licensed datasets for Danish language processing through a continuously developed corpus framework. The dataset is over four times larger than comparable resources and built using a rigorous framework ensuring traceable licensing, reproducibility, documentation, and extensibility. Contributions from industry, government, and research institutions enabled growth from ~1B to 4.8B tokens in six months. Training experiments demonstrate that models trained on Dynaword outperform Danish Gigaword by 5.9% in perplexity for continual pre-training and 26% for models trained from scratch.

## Method Summary
The Dynaword framework establishes a community-driven approach to dataset development through lightweight quality tests, reproducible collection scripts, and mandatory datasheets documenting source licenses and collection processes. Contributors identify candidate sources, verify license traceability to original rights holders, write deduplication scripts, run language and coherence checks, and create detailed documentation. The framework excludes Common Crawl-derived data in favor of exclusively openly licensed sources, distinguishing between replicable, open access, and openly licensed tiers. Continuous integration tests validate format, language detection, and documentation compliance before merging contributions.

## Key Results
- Dynaword grew from ~1B to 4.8B tokens in six months through community contributions
- Models trained on Dynaword achieve 5.9% relative perplexity improvement over Danish Gigaword
- Models show 26% improvement on 7 out of 9 downstream Danish tasks compared to Gigaword-trained models

## Why This Works (Mechanism)

### Mechanism 1
An extensible framework with documented contribution processes enables community participation, leading to dataset growth and quality improvement over time. The paper establishes lightweight tests for formatting, quality, and documentation that lower contribution friction. This allowed Danish Dynaword to grow from ~1B to 4.8B tokens in six months through contributions from industry, government, and research institutions.

### Mechanism 2
Requiring traceable, openly licensed sources reduces legal risk and enables sustainable derivative works. The paper distinguishes three licensing tiers (replicable, open access, openly licensed) and requires documentation such as "author died in 1898" rather than unsupported "public domain" claims. This excludes Common Crawl-derived data and sources with ambiguous licensing.

### Mechanism 3
Models pre-trained on Dynaword achieve lower perplexity and improved downstream task performance compared to Danish Gigaword. Controlled experiments with Gemma-3-1B show 5.9% relative perplexity improvement on the full dataset and 2.6% even in size-matched comparisons, suggesting quality—not just scale—drives gains.

## Foundational Learning

- **Open licensing tiers (replicable, open access, openly licensed)**: Why needed - The paper's entire framework hinges on distinguishing these categories; misunderstanding leads to inclusion of legally risky sources. Quick check - Can you explain why CC-0 on a redistributor's page doesn't guarantee the underlying content is openly licensed?

- **Perplexity as a language modeling metric**: Why needed - Primary quality signal in training experiments; you need to interpret the 5.9% improvement meaningfully. Quick check - If Model A has 15% lower perplexity than Model B on held-out text, what does that imply about relative performance?

- **Data contamination and evaluation leakage**: Why needed - The paper explicitly excludes evaluation benchmarks from training; you must understand why this matters. Quick check - Why would training on the Danish Dependency Treebank inflate reported performance on ScaLA?

## Architecture Onboarding

- **Component map**: Data Source Identification → License Review → Collection Scripts → Quality Checks → Datasheet Creation → CI Tests → Repository Merge

- **Critical path**: 1) Identify candidate source with apparent open license, 2) Trace license to original rights holder, 3) Write reproducible collection script with deduplication, 4) Run quality checks (language detection, coherence, alpha ratio for OCR), 5) Create datasheet documenting source, license, collection process, 6) Submit via repository; pass automated tests, 7) Maintainer review; merge or document rejection reason

- **Design tradeoffs**: Scale vs. license clarity (excluding Common Crawl loses ~26B Danish tokens but ensures legal safety), minimal quality filtering vs. downstream flexibility (intentionally light filtering allows task-specific curation later), single-language focus vs. generalizability (Danish as testbed; high-resource languages may need domain-specific dynawords)

- **Failure signatures**: OCR alpha ratio < 0.7 (unreadable text; Common Corpus Danish rejected for this), license traceability broken (redistributor lacks re-licensing rights), non-Danish or incoherent text after collection, evaluation data contamination detected post-hoc

- **First 3 experiments**: 1) Size-matched perplexity comparison (train identical architectures on Dynaword subset vs. baseline corpus to isolate quality effects from scale), 2) Downstream task evaluation (run models on held-out benchmarks with contamination exclusion verified), 3) Ablation by source category (remove legal documents to test domain balance impact on downstream performance)

## Open Questions the Paper Calls Out

### Open Question 1
Can the Dynaword framework be successfully transferred to high-resource languages where licensing complexity and data volume are substantially greater? The authors state it as a blueprint for other languages but acknowledge this remains untested beyond the Danish case.

### Open Question 2
Can multilingual or multimodal data sources effectively close the order-of-magnitude size gap between openly licensed dynawords and Common Crawl-based datasets? The paper proposes this as a potential solution but hasn't tested it experimentally.

### Open Question 3
What mechanisms can effectively detect and prevent dataset poisoning attacks in continuously developed, community-contributed corpora? The paper identifies this vulnerability but offers no implemented solutions, noting that reviewing large-scale changes remains difficult with current tooling.

## Limitations

- The community contribution framework's scalability to low-resource languages or domains without established NLP communities remains untested
- Quality assessment relies heavily on perplexity metrics, which measure in-domain fit rather than genuine language understanding
- Legal compliance claims lack external validation from IP experts or testing against legal challenges

## Confidence

- **High confidence**: Dataset construction methodology and contribution framework are well-documented and reproducible; claim that Dynaword is >4x larger than comparable Danish datasets is verifiable
- **Medium confidence**: 5.9% perplexity improvement over Danish Gigaword is statistically supported but methodology for size-matched comparisons lacks detail; downstream task improvements are promising but based on limited task suite
- **Low confidence**: Claims about legal compliance and risk mitigation are based on internal documentation rather than external legal review; assumption that community contributions will sustain dataset growth long-term hasn't been validated beyond initial six-month period

## Next Checks

1. **Cross-linguistic framework validation**: Attempt to replicate the Dynaword framework for a low-resource language without established NLP communities. Document contribution friction points and success rates compared to the Danish case.

2. **Legal compliance audit**: Engage an IP law expert to review the licensing documentation for 10 randomly selected Dynaword sources, verifying that traceability claims hold up to professional scrutiny and identifying potential legal risks.

3. **Domain-specific quality assessment**: Train models on Dynaword subsets filtered by source category (legal, news, government, etc.) and evaluate on domain-specific downstream tasks. Measure whether the legal document-heavy composition (~2.3B tokens) biases performance toward legal-domain applications.