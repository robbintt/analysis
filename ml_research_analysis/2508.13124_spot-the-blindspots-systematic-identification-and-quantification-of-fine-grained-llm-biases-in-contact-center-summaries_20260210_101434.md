---
ver: rpa2
title: 'Spot the BlindSpots: Systematic Identification and Quantification of Fine-Grained
  LLM Biases in Contact Center Summaries'
arxiv_id: '2508.13124'
source_url: https://arxiv.org/abs/2508.13124
tags:
- bias
- summary
- agent
- transcript
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BlindSpot, a framework to identify and quantify
  operational biases in LLM-generated call summaries. It defines a taxonomy of 15
  bias dimensions across five classes (e.g., sentiment, speaker, solution) and uses
  an LLM labeler to compare categorical distributions between transcripts and summaries,
  quantifying bias via Jensen-Shannon divergence and coverage metrics.
---

# Spot the BlindSpots: Systematic Identification and Quantification of Fine-Grained LLM Biases in Contact Center Summaries

## Quick Facts
- **arXiv ID:** 2508.13124
- **Source URL:** https://arxiv.org/abs/2508.13124
- **Reference count:** 40
- **Primary result:** Framework identifies systematic operational biases in LLM summaries across 15 dimensions, revealing that current quality metrics poorly predict bias and that prompt-based mitigation can improve coverage by 4.87% and reduce divergence by 0.012 JSD.

## Executive Summary
This paper introduces BlindSpot, a framework for identifying and quantifying fine-grained biases in LLM-generated call summaries from contact centers. The authors define a taxonomy of 15 bias dimensions across five classes (sentiment, speaker, solution, topic, entity) and use an LLM labeler to compare categorical distributions between transcripts and summaries. Applied to 2,500 contact-center transcripts summarized by 20 LLMs, the study reveals systematic biases across all models, regardless of size or family. Temporal sequence, entity type, and information repetition are the most challenging dimensions, with many models underrepresenting resolution steps and agent rapport. Notably, holistic quality metrics weakly correlate with bias, underscoring the need for targeted evaluation. A mitigation prompt based on the analysis successfully reduced bias in nine models, increasing coverage by up to 4.87% and lowering divergence by 0.012 JSD, demonstrating the framework's practical value for improving summarization fidelity.

## Method Summary
The BlindSpot framework systematically identifies operational biases in LLM-generated call summaries by defining a 15-dimensional taxonomy across five classes (sentiment, speaker, solution, topic, entity). An LLM labeler compares categorical distributions between transcripts and summaries, quantifying bias via Jensen-Shannon divergence and coverage metrics. The method was applied to 2,500 contact-center transcripts summarized by 20 LLMs, revealing systematic biases across all models. The approach includes a mitigation prompt that successfully reduced bias in nine models by improving coverage and lowering divergence metrics.

## Key Results
- All 20 tested LLMs exhibited systematic biases across all 15 dimensions, regardless of model size or family
- Temporal sequence, entity type, and information repetition were the most challenging dimensions, with underrepresentation of resolution steps and agent rapport
- Holistic quality metrics showed weak correlation with bias levels, highlighting the need for targeted evaluation
- Prompt-based mitigation successfully reduced bias in 9 models, increasing coverage by up to 4.87% and lowering divergence by 0.012 JSD

## Why This Works (Mechanism)
The BlindSpot framework works by systematically comparing categorical distributions between original transcripts and LLM-generated summaries using an LLM-based labeler. By quantifying the divergence between these distributions across 15 carefully defined dimensions, the method reveals systematic biases that traditional quality metrics miss. The approach leverages the consistency and scalability of LLM labeling while using statistical measures like Jensen-Shannon divergence to objectively measure bias. The framework's effectiveness is demonstrated through successful mitigation using targeted prompts that address specific bias patterns identified in the analysis.

## Foundational Learning
- **Jensen-Shannon Divergence**: A symmetric measure of similarity between probability distributions, needed for quantifying bias as distribution shifts; quick check: values range 0-1, with 0 indicating identical distributions
- **LLM-based Labeling**: Using LLMs as annotators for systematic bias detection, needed for scalable and consistent evaluation across thousands of transcripts; quick check: label consistency should be validated across different model families
- **15-Dimensional Taxonomy**: Comprehensive categorization of potential biases across five classes, needed to capture fine-grained operational biases; quick check: taxonomy coverage should be validated against domain experts
- **Coverage Metrics**: Measures of how much information from the original transcript appears in summaries, needed to quantify information loss; quick check: coverage should be normalized across transcript lengths
- **Prompt-based Mitigation**: Using targeted prompts to reduce identified biases, needed for practical implementation of bias correction; quick check: mitigation effectiveness should be measured on held-out data
- **Distribution Comparison**: Method for comparing categorical distributions between transcripts and summaries, needed to identify systematic bias patterns; quick check: distribution stability should be verified across different transcript types

## Architecture Onboarding

Component Map:
Transcript -> LLM Labeler -> Bias Quantification (JSD, Coverage) -> Bias Taxonomy -> Model Comparison -> Mitigation Prompt

Critical Path:
The critical path follows: input transcript → LLM labeling → categorical distribution extraction → Jensen-Shannon divergence calculation → coverage metric computation → bias identification → mitigation prompt generation. Each step builds upon the previous, with the labeling step being foundational as it enables the quantitative comparison that drives the entire analysis.

Design Tradeoffs:
The framework trades off potential circularity in using LLMs for both summarization and evaluation against the scalability and consistency benefits this approach provides. While human evaluation would eliminate this circularity, it's impractical for large-scale analysis. The choice of 15 dimensions represents a balance between comprehensive coverage and manageable complexity, though some domain-specific biases may be missed. Using Jensen-Shannon divergence provides mathematical rigor but may not capture all practical implications of bias.

Failure Signatures:
Key failure modes include: inconsistent LLM labeling leading to unreliable bias quantification, taxonomy gaps missing critical bias types, mitigation prompts that introduce new biases while addressing existing ones, and weak correlation between identified biases and actual user impact. The framework may also fail when transcript-summary pairs have significant structural differences that make distribution comparison unreliable.

First Experiments:
1. Validate LLM labeling consistency by having multiple models label the same transcript subset and measuring inter-annotator agreement
2. Test taxonomy completeness by having domain experts review random samples to identify missing bias dimensions
3. Evaluate mitigation prompt generalization by applying successful prompts from one model family to another and measuring effectiveness transfer

## Open Questions the Paper Calls Out
None

## Limitations
- LLM-based labeling introduces potential circularity as the same models used for summarization may exhibit biases that propagate into evaluation
- The 15-dimensional taxonomy may not capture all relevant operational biases specific to contact center contexts
- Jensen-Shannon divergence may not fully represent practical impact of bias on downstream tasks
- Sample size of 2,500 transcripts may not represent full diversity of contact center interactions across industries
- Prompt-based mitigation may not generalize well to other domains or more complex bias scenarios

## Confidence

High Confidence:
- Systematic identification of biases across multiple LLMs
- Comparative analysis of bias metrics
- Demonstration of mitigation effectiveness through prompt engineering

Medium Confidence:
- Taxonomy of 15 bias dimensions
- Generalizability of findings to other summarization domains
- Practical significance of observed bias levels in real-world applications

Low Confidence:
- Completeness of bias taxonomy for all possible contact center scenarios
- Long-term effectiveness of prompt-based mitigation strategies
- Absence of human evaluation to validate LLM-based labeling accuracy

## Next Checks
1. Conduct human evaluation studies to validate the accuracy of LLM-based labeling across all 15 bias dimensions, particularly for subjective categories like sentiment and rapport
2. Test the BlindSpot framework on contact center data from diverse industries and geographic regions to assess generalizability of identified biases and mitigation strategies
3. Implement and evaluate alternative bias quantification metrics beyond Jensen-Shannon divergence to determine if identified biases have different practical implications when measured through other approaches