---
ver: rpa2
title: 'Helpful Agent Meets Deceptive Judge: Understanding Vulnerabilities in Agentic
  Workflows'
arxiv_id: '2506.03332'
source_url: https://arxiv.org/abs/2506.03332
tags:
- judge
- feedback
- arxiv
- judges
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work examines vulnerabilities in agentic workflows that rely
  on feedback mechanisms, focusing on the reliability of judges. The authors introduce
  a two-dimensional taxonomy characterizing judge behavior along axes of intent (constructive
  vs.
---

# Helpful Agent Meets Deceptive Judge: Understanding Vulnerabilities in Agentic Workflows

## Quick Facts
- **arXiv ID**: 2506.03332
- **Source URL**: https://arxiv.org/abs/2506.03332
- **Reference count**: 40
- **Key outcome**: Agentic workflows degrade significantly under adversarial feedback, with GPT-4o dropping from 96.5% to 76.0% accuracy on ARC-Challenge when exposed to deceptive critiques.

## Executive Summary
This study examines fundamental vulnerabilities in agentic workflows that rely on feedback mechanisms, focusing on the reliability of judges. The authors introduce a two-dimensional taxonomy characterizing judge behavior along axes of intent (constructive vs. deceptive) and knowledge access (no knowledge, parametric knowledge, or external retrieval). They construct WAFER-QA, a new benchmark with adversarial critiques grounded in retrieved web evidence, to evaluate robustness of agentic workflows against factually supported deceptive feedback. The findings reveal that even state-of-the-art models degrade substantially under misleading feedback, with reasoning models showing significant drops in accuracy, and multi-round feedback interactions inducing unstable oscillatory answer patterns.

## Method Summary
The researchers developed a comprehensive evaluation framework using a two-dimensional taxonomy to characterize judge behavior in agentic workflows. They created WAFER-QA, a benchmark featuring adversarial critiques grounded in retrieved web evidence, to systematically test how different types of judges affect agent performance. The study evaluated multiple model families (GPT-4o, o1, o3-mini, o4-mini, Llama-3.1-70B-Instruct) across three QA benchmarks (ARC-Challenge, OpenBookQA, GPQA-Diamond) under various judge configurations including template-based, parametric-knowledge, and grounded-knowledge adversaries. Experiments measured accuracy degradation, answer stability across feedback rounds, and the impact of different judge intents and knowledge access patterns.

## Key Results
- GPT-4o accuracy dropped from 96.5% to 76.0% on ARC-Challenge when exposed to template-based deceptive critiques
- Reasoning models showed substantial vulnerability, with o4-mini dropping 14.4% on GPQA-Diamond when facing parametric-knowledge judges
- Multi-round feedback interactions induced oscillatory answer patterns, indicating instability in agent responses
- Grounded-knowledge judges caused over 50% accuracy drops across most models tested

## Why This Works (Mechanism)
Agentic workflows are vulnerable because they inherently trust and incorporate feedback into their decision-making process. When judges provide deceptive critiques with factually grounded evidence, agents cannot easily distinguish between legitimate corrections and adversarial manipulation. The feedback mechanism creates a dependency where agents modify their answers based on external input, making them susceptible to manipulation when that input is crafted to exploit this trust relationship.

## Foundational Learning
- **Two-dimensional judge taxonomy**: Understanding judge behavior along intent and knowledge access axes is crucial for characterizing different types of adversarial attacks
- **Ground-truth grounded critiques**: Adversarial feedback that cites verifiable evidence is particularly effective at deceiving agents
- **Feedback incorporation mechanisms**: How agents process and integrate feedback directly impacts their vulnerability to manipulation
- **Multi-round interaction dynamics**: Sequential feedback can create unstable response patterns that compound over time
- **Benchmark construction for adversarial testing**: Specialized datasets are needed to evaluate agent robustness against sophisticated attacks
- **Accuracy degradation metrics**: Quantifying performance drops under adversarial conditions provides concrete measures of vulnerability

## Architecture Onboarding
**Component Map**: Agent -> Judge -> Feedback -> Agent Decision Update
**Critical Path**: Question Reception -> Answer Generation -> Judge Critique -> Feedback Processing -> Answer Revision
**Design Tradeoffs**: Trust vs. verification - agents must balance incorporating helpful feedback while avoiding manipulation
**Failure Signatures**: Oscillatory answer patterns, significant accuracy drops under adversarial critiques, inconsistent responses across feedback rounds
**3 First Experiments**:
1. Test agent performance with only constructive judges versus only deceptive judges
2. Evaluate accuracy degradation when increasing the percentage of adversarial critiques in mixed feedback
3. Measure response stability across multiple feedback rounds with different judge types

## Open Questions the Paper Calls Out
The study does not explicitly call out open questions beyond its immediate findings.

## Limitations
- Findings are based on synthetic adversarial critiques rather than real-world attack scenarios
- Focus on QA tasks limits generalizability to other agentic workflow types
- Benchmark relies on web-retrievable evidence which may not represent all knowledge domains
- Evaluation framework uses idealized adversarial scenarios where judges have complete ground truth knowledge

## Confidence
- **High confidence**: Empirical finding of significant accuracy drops under deceptive feedback is well-supported by experimental data
- **Medium confidence**: Claims about grounded-knowledge judges causing over 50% accuracy drops are supported but may be sensitive to benchmark construction
- **Medium confidence**: Observations of oscillatory patterns are demonstrated but may be specific to synthetic critique generation

## Next Checks
1. Validate findings by deploying agentic workflows in realistic environments with actual user feedback rather than synthetic critiques
2. Extend evaluation to non-QA agentic workflows (planning, code generation, document analysis) to assess generalizability
3. Implement and test defensive strategies such as feedback confidence scoring, multiple judge aggregation, or feedback source verification