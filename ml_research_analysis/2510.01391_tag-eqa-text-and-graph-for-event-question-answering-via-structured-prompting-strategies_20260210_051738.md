---
ver: rpa2
title: 'TAG-EQA: Text-And-Graph for Event Question Answering via Structured Prompting
  Strategies'
arxiv_id: '2510.01391'
source_url: https://arxiv.org/abs/2510.01391
tags:
- event
- text
- causal
- graphs
- prompting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models often struggle with event-based questions
  requiring causal or temporal reasoning. TAG-EQA introduces a prompting framework
  that embeds structured causal event graphs into LLM inputs by converting relations
  into natural language statements.
---

# TAG-EQA: Text-And-Graph for Event Question Answering via Structured Prompting Strategies

## Quick Facts
- **arXiv ID:** 2510.01391
- **Source URL:** https://arxiv.org/abs/2510.01391
- **Reference count:** 13
- **Primary result:** TAG-EQA improves event QA accuracy by 5% average over text-only baselines

## Executive Summary
TAG-EQA addresses the challenge of event-based question answering where large language models struggle with causal and temporal reasoning. The framework introduces structured prompting strategies that embed causal event graphs into LLM inputs by converting relations into natural language statements. By evaluating nine different configurations combining zero-shot, few-shot, and chain-of-thought approaches with text-only, graph-only, and combined inputs, TAG-EQA demonstrates consistent improvements in accuracy across multiple question types and model sizes.

## Method Summary
TAG-EQA embeds structured causal event graphs into LLM prompts by converting graph relations into natural language statements. The framework systematically evaluates combinations of zero-shot, few-shot, and chain-of-thought prompting strategies with text-only, graph-only, and hybrid inputs. This structured prompting approach enables LLMs to leverage explicit causal and temporal relationships without requiring model fine-tuning.

## Key Results
- 5% average accuracy improvement over text-only baselines across all configurations
- Up to 12% gains in zero-shot settings when graphs are included
- Up to 18% improvement with graph-augmented chain-of-thought prompting
- Strongest performance gains for causal and temporal question types
- Qwen-32B shows the most benefit from structured input

## Why This Works (Mechanism)
TAG-EQA works by explicitly encoding causal and temporal relationships into LLM prompts through structured graph representations converted to natural language. This externalizes reasoning about event sequences and causal connections that LLMs typically struggle to infer from text alone. The framework leverages the LLM's existing language understanding capabilities while providing explicit structural information about event relationships.

## Foundational Learning

**Causal Event Graphs** - Networks representing cause-effect relationships between events
*Why needed:* LLMs lack inherent understanding of event causality and temporal ordering
*Quick check:* Verify graph construction captures all relevant causal dependencies

**Structured Prompting** - Systematic integration of external knowledge structures into prompts
*Why needed:* Standard prompting fails to convey complex relational information
*Quick check:* Confirm graph statements maintain logical coherence when concatenated with text

**Chain-of-Thought Prompting** - Step-by-step reasoning approaches for complex questions
*Why needed:* Event QA requires multi-step reasoning about relationships
*Quick check:* Validate that reasoning steps align with causal graph structure

## Architecture Onboarding

**Component Map:** Graph Construction -> Natural Language Conversion -> Prompt Assembly -> LLM Inference -> Answer Generation

**Critical Path:** Structured graph statements must be properly aligned with question context to enable effective reasoning. The conversion from graph to natural language is critical for LLM comprehension.

**Design Tradeoffs:** Structured graphs provide explicit reasoning guidance but add prompt complexity and length. The framework trades increased input size for improved reasoning accuracy without model modification.

**Failure Signatures:** Poor graph construction or conversion leads to contradictory statements. Missing causal links result in incomplete reasoning. Overly complex graphs may overwhelm the LLM's context window.

**First Experiments:** 1) Test basic graph-to-text conversion quality with simple event pairs 2) Evaluate zero-shot performance with minimal graph augmentation 3) Compare graph-only vs. text-only baselines on causal questions

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Relies on synthetic graph constructions rather than real-world event graph datasets
- Performance gains measured relative to text-only baselines without isolating causal vs. temporal components
- Limited evaluation to single domain and three LLMs, raising generalization concerns
- Chain-of-thought improvements may conflate graph benefits with general reasoning prompt effects

## Confidence
- **High confidence** in 5% average improvement claim due to controlled experiments
- **Medium confidence** in 12-18% maximum gains as these are configuration-specific
- **Low confidence** in claims about which model benefits most given limited model diversity

## Next Checks
1. Test TAG-EQA on real-world event graph datasets (e.g., Wikidata or temporal knowledge graphs) to assess robustness
2. Conduct ablation studies separating the impact of causal vs. temporal graph embeddings
3. Evaluate across additional domains (e.g., legal, historical QA) and model families (e.g., GPT-4, Claude) to test generalizability