---
ver: rpa2
title: 'Talking Surveys: How Photorealistic Embodied Conversational Agents Shape Response
  Quality, Engagement, and Satisfaction'
arxiv_id: '2508.02376'
source_url: https://arxiv.org/abs/2508.02376
tags:
- agent
- participants
- agents
- https
- embodied
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether embodied conversational agents
  (ECAs) can enhance the quality and engagement of online survey responses compared
  to text-based chatbots. The researchers developed the Virtual Agent Interviewer
  (VAI), an AI-driven survey tool featuring photorealistic video avatars, speech recognition,
  and LLM-powered conversation logic.
---

# Talking Surveys: How Photorealistic Embodied Conversational Agents Shape Response Quality, Engagement, and Satisfaction

## Quick Facts
- **arXiv ID:** 2508.02376
- **Source URL:** https://arxiv.org/abs/2508.02376
- **Reference count:** 40
- **Primary result:** Photorealistic ECAs produce longer, more informative responses and faster per-unit response times in online surveys compared to text-based chatbots.

## Executive Summary
This study evaluates whether photorealistic embodied conversational agents (ECAs) can enhance online survey response quality and engagement compared to text-based chatbots. Using a custom Virtual Agent Interviewer tool with GPT-4o-mini and HeyGen avatars, researchers conducted a between-subjects experiment with 80 UK participants. The ECA prompted significantly more informative responses with more words and characters, and users responded faster per unit of information. While overall satisfaction ratings were similar between agents, qualitative feedback suggested the ECA interaction felt more natural and engaging despite technical issues like latency and Uncanny Valley effects.

## Method Summary
The study employed a between-subjects experiment with 80 UK adults randomly assigned to interact with either a photorealistic ECA or a text-based chatbot. Both agents administered two standardized psychometric questionnaires (BFI-2-S and CFQ) with follow-up questions. The ECA used the HeyGen Interactive Avatar API with GPT-4o-mini and Whisper for speech processing, featuring a push-to-talk mechanism with silence threshold. Responses were evaluated for quality (informativeness, specificity, relevance, clarity), engagement (response time, length, self-disclosure), and satisfaction (naturalness, UX). Data analysis used Mann-Whitney U tests, and response quality scoring employed a labeling prompt with majority voting across three annotators.

## Key Results
- ECA responses were significantly more informative (higher word surprisal sum) and contained more words and characters than chatbot responses
- Users responded faster per unit of information with the ECA compared to the chatbot
- No significant differences were found in specificity, relevance, clarity, or sentiment between agents
- Qualitative feedback indicated the ECA felt more natural and engaging despite technical issues like delays and uncanny valley effects

## Why This Works (Mechanism)
ECAs leverage human-like embodiment and conversational flow to create more engaging survey experiences. The combination of photorealistic video avatars, natural speech processing, and context-aware follow-up questions appears to encourage richer self-disclosure and more detailed responses. The multimodal interaction (voice + visual) may reduce the cognitive distance between traditional human-administered interviews and automated surveys, potentially lowering barriers to expression.

## Foundational Learning
- **Word surprisal calculation** - measures information content of responses using word frequency distributions; needed to quantify informativeness objectively
- **Modular prompt engineering** - separates security, summary, inquiry, and wrapping functions for controlled agent behavior; needed to maintain consistent conversational quality
- **Between-subjects experimental design** - prevents carry-over effects between agent types; needed to isolate ECA impact
- **Mann-Whitney U test** - non-parametric significance testing for ordinal and non-normal data; needed for robust statistical comparison
- **Majority voting for labeling** - ensures reliable quality scoring across multiple annotators; needed to reduce subjective bias
- **SymSpell typo correction** - normalizes text input before analysis; needed to ensure accurate word frequency calculations

## Architecture Onboarding

**Component Map:** User -> React Client -> Django Server -> GPT-4o-mini/HeyGen API -> PostgreSQL/Redis -> Response Analysis

**Critical Path:** User interaction → Speech recognition (Whisper) → LLM processing (GPT-4o-mini) → Avatar response (HeyGen) → User feedback

**Design Tradeoffs:** The system prioritized conversational naturalness over technical perfection, accepting latency and occasional cutoff errors to maintain an embodied interaction experience. This created richer responses but introduced technical friction points.

**Failure Signatures:** Turn-taking delays >1s break immersion and increase frustration; premature cutoff truncates responses; uncanny valley effects reduce perceived naturalness; silence threshold mismanagement causes overlapping speech.

**First Experiments:**
1. Implement basic React client with modular prompt system using GPT-4o-mini for security, summary, and inquiry modules
2. Integrate HeyGen Interactive Avatar API with Whisper STT and configure push-to-talk with silence threshold
3. Deploy Django/PostgreSQL/Redis backend and implement SymSpell preprocessing pipeline for response analysis

## Open Questions the Paper Calls Out
1. **Turn-taking latency mitigation:** Can non-verbal fillers (nodding, gestures) effectively mask system latency to reduce awkwardness of delays in ECA-mediated surveys? The study identified delays >1s as problematic but didn't test behavioral strategies to mitigate perception of latency.

2. **Truthfulness vs. social desirability:** Does voice-based ECA spontaneity result in more truthful disclosures or increased social desirability bias for sensitive data? While human-likeness might amplify socially desirable responses, speech spontaneity "could also be more challenging to fake convincingly," leaving net truthfulness effects unclear.

3. **Generalization to usability testing:** Do ECA benefits extend to complex unmoderated methods like usability testing requiring context-aware interventions? The current prototype was limited to text/speech input and didn't assess reacting to visual user behaviors like clicking errors or hesitation.

## Limitations
- The specific conversational prompt engineering (Security, Summary, Inquiry, Wrapping modules) is referenced but not fully disclosed, making faithful reproduction difficult
- Technical implementation faced real-world issues including uncanny valley effects, latency >1s, and occasional premature cutoff that weren't fully controlled
- Sample size (N=80) lacks power analysis to determine if it was adequate to detect smaller but potentially meaningful differences in specificity and relevance metrics

## Confidence
- **High:** ECAs produce longer, more informative responses with faster per-unit response times - supported by direct quantitative measures across all participants
- **Medium:** Engagement and qualitative satisfaction claims - metrics showed trends but qualitative feedback was mixed with notable technical issues
- **Low:** Self-disclosure variability claims - based on exploratory analysis without clear statistical significance testing

## Next Checks
1. **Prompt Engineering Audit:** Reconstruct and test the modular prompt system using the published labeling prompt as template, measuring how variations in Security/Summary/Inquiry directives affect response quality metrics

2. **Latency Impact Study:** Systematically vary agent response delay (0.5s, 1.5s, 3s) in controlled experiment to quantify threshold at which turn-taking latency degrades user satisfaction and response quality

3. **Sample Size Sensitivity Analysis:** Conduct post-hoc power analysis using reported effect sizes to determine minimum sample size needed to detect differences in specificity and relevance metrics with 80% power