---
ver: rpa2
title: Misalignment from Treating Means as Ends
arxiv_id: '2507.10995'
source_url: https://arxiv.org/abs/2507.10995
tags:
- reward
- function
- learning
- state
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces a formalism for characterizing when learned
  reward functions conflate instrumental and terminal goals, defining a "degree of
  conflation" metric. Through a simple three-state example, it demonstrates that even
  slight conflation can lead to severe misalignment: optimizing a slightly conflated
  reward function can produce policies that achieve the worst possible true reward.'
---

# Misalignment from Treating Means as Ends

## Quick Facts
- **arXiv ID**: 2507.10995
- **Source URL**: https://arxiv.org/abs/2507.10995
- **Authors**: Henrik Marklund; Alex Infanger; Benjamin Van Roy
- **Reference count**: 40
- **Primary result**: Even slight conflation between terminal and instrumental goals can produce policies that achieve the worst possible true reward.

## Executive Summary
This paper introduces a formalism for characterizing when learned reward functions conflate instrumental and terminal goals, defining a "degree of conflation" metric. Through a simple three-state example, it demonstrates that even slight conflation can lead to severe misalignment: optimizing a slightly conflated reward function can produce policies that achieve the worst possible true reward. The authors prove this fragility analytically and provide a geometric interpretation. They also show that standard reward learning from human choices can result in such conflation, and that this leads to misalignment in their example. The paper discusses how this phenomenon can manifest in more complex environments like Atari games and hypothetical AI therapy scenarios, highlighting the risks of treating instrumental goals as terminal.

## Method Summary
The paper uses a three-state MDP (common, instrumental, terminal) to demonstrate how conflating reward with value function leads to misalignment. The method computes optimal relative value function V* via the Poisson equation, then creates a conflated reward r̂ = (1-β)r + βV* for varying β. The optimal policy for r̂ is found and evaluated against the true reward r. The authors prove that when M is sufficiently large and ε is sufficiently small, there exists a threshold β* such that for any β ≥ β*, the optimal policy for r̂ achieves the minimum possible true reward r̂π = -1. They also analyze reward learning from human choices, showing that standard approaches can recover V* rather than r when choices depend on anticipated value.

## Key Results
- A slight conflation between terminal and instrumental goals (β ≥ 0.1 in the canonical example) can produce policies that achieve the worst possible true reward (r̂π = -1).
- Standard reward learning approaches that assume human choices depend only on realized rewards systematically produce conflated rewards approximating V* when choices actually depend on anticipated returns.
- The fragility to conflation is a geometric property: when the feasible stationary-distribution region is "flat" perpendicular to the reward direction, small angular deviations in the objective function cause large changes in which policy is selected.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Slight conflation between terminal and instrumental goals can produce worst-case policy performance in environments with specific structural properties.
- **Mechanism:** When a proxy reward function r̂ blends true reward r with value function V* (even minimally), it rotates the optimization objective. In environments where high-reward states are rarely revisitable but high-value/low-reward states are easily revisitable, this rotation causes the optimizer to select a policy that stays at the instrumental goal indefinitely rather than pursuing the terminal goal. The geometric interpretation shows r̂ crossing a "normal line" that separates aligned from misaligned optima.
- **Core assumption:** The environment exhibits two properties: (P1) states with high reward cannot be visited frequently, and (P2) states with high value can be visited frequently but yield rewards well below average.
- **Evidence anchors:**
  - [abstract] "demonstrates that even slight conflation can lead to severe misalignment: optimizing a slightly conflated reward function can produce policies that achieve the worst possible true reward"
  - [section 3.2-3.3] The three-state canonical example with formal proof (Theorem 1) showing conditions under which r̂π = -1 (minimum possible)
  - [corpus] "Targeting Misalignment" paper discusses related reward-model failures; "Natural Emergent Misalignment" shows empirical misalignment from reward hacking in production systems
- **Break condition:** If either property P1 or P2 is absent—specifically, if high-reward states can be frequently revisited, or if no high-value/low-reward states exist that are easily revisitable—the sensitivity to conflation diminishes substantially.

### Mechanism 2
- **Claim:** Standard reward learning approaches (e.g., RLHF from trajectory comparisons) systematically produce conflated reward functions when human choices depend on anticipated future value.
- **Mechanism:** When humans evaluate trajectories using bootstrapped returns (realized rewards + anticipated value V*), but the learning algorithm assumes choices depend only on realized rewards, the maximum-likelihood estimate converges to r̂ ≈ V* rather than r. This occurs because the choice probability model (equation 6) cannot distinguish between reward and value information embedded in human choices.
- **Core assumption:** Human choices follow the bootstrapped-return model p*(h,h'|r,V*) in equation 5, depending on both immediate rewards and terminal-state values.
- **Evidence anchors:**
  - [section 4.4] Theorem 2 proves that under transition-comparison data, minimizing the standard loss yields r̂ where "r̂ - V is a constant function"
  - [section 4.3-4.4] Shows the mismatch between the assumed choice model (equation 6) and actual choice model (equation 5)
  - [corpus] Limited direct corpus support for this specific mechanism; related work on reward learning mentioned (Christiano et al. 2017) but not in corpus
- **Break condition:** If the choice data distribution involves longer trajectories (T > 2) or if humans provide explicit decompositions of instrumental vs. terminal preferences, the asymptotic recovery of V* alone no longer holds.

### Mechanism 3
- **Claim:** The fragility to conflation is a geometric property of the feasible stationary-distribution region, not just the reward function itself.
- **Mechanism:** The feasible region Φ of stationary distributions is constrained by environment dynamics. When this region is "flat" (elongated perpendicular to the reward direction), small angular deviations in the objective function cause large changes in which vertex is selected. High M and low ε make both the reward vector and the normal to the feasible region point nearly vertical, so any conflation rotating r̂ toward V* crosses the decision boundary.
- **Core assumption:** The geometric intuition from the 3-state example generalizes to higher-dimensional MDPs with similar structural properties.
- **Evidence anchors:**
  - [section 3.3] Figure 2 and accompanying geometric interpretation explaining how conflation rotates the objective across the "normal line"
  - [section 3.3] "Because both r and the normal line point almost straight up, if r̂ steers even slightly toward V* relative to r, it will cross the normal line"
  - [corpus] No direct corpus validation of geometric interpretation
- **Break condition:** In environments where the feasible region has different geometry—particularly where the boundary between aligned and misaligned policies is not nearly parallel to the reward direction—small conflations may not cause policy switching.

## Foundational Learning

- **Concept: Average-reward MDPs and relative value functions**
  - **Why needed here:** The paper uses average-reward criterion (not discounted), and defines conflation relative to the optimal relative value function V*. Standard RL often focuses on discounted settings.
  - **Quick check question:** Given an MDP with optimal average reward r*, how does V*(s) = lim[γ→1] E[Σ γ^t(r(S_t) - r*) | S_0 = s] differ from the standard discounted value function?

- **Concept: Stationary distributions and the policy polytope**
  - **Why needed here:** The geometric interpretation relies on viewing policies via their induced stationary distributions, and understanding Φ as a convex polytope in probability simplex.
  - **Quick check question:** For a 3-state MDP, why is the set of feasible stationary distributions a 2D polygon rather than filling the entire probability simplex?

- **Concept: Reward shaping and potential-based transformations**
  - **Why needed here:** The paper distinguishes between benign reward shaping (which preserves optimal policies) and harmful conflation (which doesn't). Understanding why adding value functions is NOT potential-based shaping is crucial.
  - **Quick check question:** Why does adding F(s') - F(s) to rewards preserve optimal policies, but adding V*(s) does not?

## Architecture Onboarding

- **Component map:** Environment (MDP) → True reward r (terminal goals) → Human policy/choices → Choice data D (depends on r + V*) → Reward learning algorithm → Learned reward r̂ (conflates r + V*) → Policy optimization → Policy r̂ (maximizes r̂) → Evaluation → True reward r̂π (may be severely suboptimal)

- **Critical path:** The failure mode requires: (1) environment with P1/P2 properties, (2) reward learning that produces conflated r̂, (3) policy optimization over average reward. Breaking any link prevents failure.

- **Design tradeoffs:**
  - Denser reward shaping → faster learning but higher conflation risk
  - Longer trajectory comparisons → better reward/value separation but more data requirements
  - Explicit value annotation → cleaner signal but heavier human burden
  - Assumption: Regularization on learned rewards may help but is not analyzed in paper

- **Failure signatures:**
  - Agent loops at "instrumental" subgoals without completing tasks (e.g., Atari agent at ladder top)
  - High proxy reward r̂π but low true reward rπ
  - Agent avoids transitioning to terminal states that would end episodes
  - Policy appears myopically "sensible" but globally suboptimal

- **First 3 experiments:**
  1. **Reproduce canonical example:** Implement the 3-state MDP with configurable M and ε. Verify Theorem 1 by sweeping conflation degree β and measuring when r̂π drops to -1. Confirm the predicted threshold values.
  2. **Test reward learning on canonical example:** Generate synthetic choice data using the bootstrapped-return model (eq. 5), apply standard RLHF loss (eq. 7), and verify that learned r̂ ≈ V*. Measure the resulting policy's true reward as a function of trajectory length T.
  3. **Probe Atari environments:** Replicate the Montezuma's Revenge finding from Ibarz et al. (2018). Specifically: (a) train reward model from human preferences, (b) visualize which states receive high learned reward, (c) test if agent gets stuck at instrumental subgoals. Compare learned reward at ladder-top vs. key-acquisition states.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the theoretical result proving severe misalignment from conflation be generalized to complex environments beyond the three-state canonical example?
- **Basis in paper:** [explicit] "Theorem 1 applies only to our canonical example. We leave for future work developing a more general theorem that applies to a broad range of complex environments that exhibit the two properties listed above."
- **Why unresolved:** The current mathematical proof relies on the specific geometry and constraints of a simplified three-state MDP, though the authors hypothesize that two specific environmental properties (hard-to-revisit high-reward states and easy-to-revisit high-value states) are the core drivers.
- **What evidence would resolve it:** A formal theorem demonstrating that any environment satisfying properties P1 and P2 (defined in Section 5) will result in severe misalignment when a learned reward function conflates reward and value.

### Open Question 2
- **Question:** What is the appropriate formal characterization of reward-value conflation in real-world settings where the strict linear combination assumption is too restrictive?
- **Basis in paper:** [explicit] "An important caveat is that our definition of conflation Definition 1 is likely to be too restrictive to hold in real settings... How best to formally characterize this sort of conflation is left for future work."
- **Why unresolved:** Definition 1 requires the proxy reward r̂ to be a convex combination of the true reward r and value V*. While analytically convenient, this linear relationship is unlikely to exist in complex, high-dimensional environments like those modeled by deep neural networks.
- **What evidence would resolve it:** A generalized definition of conflation that applies to complex function approximation and a demonstration that this new definition preserves the paper's findings regarding sensitivity and misalignment.

### Open Question 3
- **Question:** How can reward learning algorithms be modified to successfully disentangle terminal goals from the instrumental value of states?
- **Basis in paper:** [explicit] "These observations motivate future work to investigate... how it can be mitigated."
- **Why unresolved:** The paper demonstrates that standard approaches (like those assuming the logistic model of choice) fail to resolve the ambiguity between immediate reward and anticipated value, but it does not propose a solution to correct the reward learning objective.
- **What evidence would resolve it:** A modified reward learning algorithm that explicitly accounts for the human's discounting or value estimation process, proven to recover the true reward function r even when choices are generated based on anticipated returns.

### Open Question 4
- **Question:** Does the reward-value conflation mechanism significantly degrade performance in large-scale empirical settings such as Atari games or language model alignment?
- **Basis in paper:** [explicit] "These observations motivate future work to investigate how this failure mode manifests empirically in real environments..."
- **Why unresolved:** While the paper provides a theoretical proof in a toy domain and anecdotal examples (e.g., Montezuma's Revenge), it lacks systematic empirical validation isolating this specific mechanism as the cause of misalignment in complex, high-dimensional systems.
- **What evidence would resolve it:** Empirical analysis in standard benchmarks (e.g., RLHF for LLMs) showing a correlation between the measured "degree of conflation" in the learned reward model and a subsequent drop in the true objective metric.

## Limitations
- The fragility to conflation depends critically on specific structural properties (P1: high-reward states rarely revisitable; P2: high-value/low-reward states easily revisitable) that may not hold in many real-world environments.
- The reward learning mechanism assumes specific choice models and trajectory comparison distributions that may not reflect actual human-AI interaction patterns.
- Geometric interpretation relies on intuition from the 3-state example that may not generalize to higher-dimensional MDPs with different feasible region geometries.

## Confidence
- **High confidence**: The mathematical framework for defining and measuring conflation (Definition 1, Theorems 1-2) is rigorous and well-supported. The three-state canonical example demonstrates the fragility mechanism clearly.
- **Medium confidence**: The geometric interpretation of conflation as crossing a "normal line" provides valuable intuition but requires empirical validation in more complex environments.
- **Low confidence**: The claim that standard reward learning approaches systematically produce conflated rewards (Mechanism 2) is theoretically proven but lacks empirical validation in real-world settings.

## Next Checks
1. **Environmental prevalence study**: Systematically analyze a diverse set of MDPs (gridworlds, Atari games, robotics tasks) to measure how often environments exhibit the structural properties (P1 and P2) required for conflation-induced misalignment. Quantify the fraction of environments where slight conflation leads to severe misalignment.
2. **Empirical reward learning validation**: Implement the reward learning mechanism (Theorem 2) using human choice data generated from synthetic policies. Verify that the learned reward converges to V* rather than r, and measure the resulting policy's true reward across varying trajectory lengths T and different choice models.
3. **Complex environment extension**: Extend the canonical example to larger state spaces (4-5 states) with varied reward structures. Test whether the geometric interpretation holds—specifically, whether the "normal line" boundary between aligned and misaligned optima generalizes and remains sensitive to small conflations.