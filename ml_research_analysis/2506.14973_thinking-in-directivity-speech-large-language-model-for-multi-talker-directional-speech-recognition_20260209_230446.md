---
ver: rpa2
title: 'Thinking in Directivity: Speech Large Language Model for Multi-Talker Directional
  Speech Recognition'
arxiv_id: '2506.14973'
source_url: https://arxiv.org/abs/2506.14973
tags:
- speech
- direction
- audio
- directions
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Directional-SpeechLlama, a speech large language
  model designed to process multi-channel directional audio for smart glasses applications.
  The model enables directional speech recognition, source localization, and bystander
  cross-talk suppression by leveraging microphone arrays.
---

# Thinking in Directivity: Speech Large Language Model for Multi-Talker Directional Speech Recognition

## Quick Facts
- arXiv ID: 2506.14973
- Source URL: https://arxiv.org/abs/2506.14973
- Reference count: 0
- Primary result: Achieves 92% direction prediction accuracy and 3.84% WER on single-talker tasks across 12 directions

## Executive Summary
This paper introduces Directional-SpeechLlama, a speech large language model that leverages microphone arrays to enable directional speech recognition, source localization, and bystander cross-talk suppression for smart glasses applications. The model processes multi-channel audio through a three-stage fine-tuning framework, incorporating spatial information via serialized directional output training (S-DOT) and contrastive direction data augmentation (CDDA). Experimental results demonstrate strong performance on both single-talker and multi-talker directional speech recognition tasks, with the model achieving 98.4% success rate on target-direction ASR while suppressing non-target speakers.

## Method Summary
The approach uses a 7-channel microphone array from smart glasses, processing audio through NLCMV beamforming to create 12 fixed directional channels. A frozen pre-trained audio encoder is bridged to a frozen LLM decoder via a trainable adapter layer. The model undergoes three stages of fine-tuning: first on multi-channel ASR, then with S-DOT incorporating direction labels into transcripts, and finally with CDDA adding distractor speakers for contrastive learning. The linear projection layer is critical for preserving spatial cues by flattening multi-channel features before the encoder. The entire architecture contains 9 billion parameters with 120K fine-tuned parameters.

## Key Results
- Achieves 92% direction prediction accuracy across 12 directions at 30° resolution
- Single-talker ASR reaches 3.84% WER with 98.8% left/right accuracy
- Multi-talker target-direction ASR achieves 98.4% success rate with 3.81% success WER

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multi-channel beamforming preserves spatial cues that enable the LLM to learn direction-dependent representations.
- **Mechanism:** NLCMV beamforming produces multiple fixed beamformed channels, each focused on a distinct direction. The inter-channel amplitude differences encode direction-of-arrival (DOA) information, which the linear projection layer flattens and projects into the encoder's vector space.
- **Core assumption:** The pre-trained audio encoder can learn to associate amplitude patterns across beamformed channels with spatial directions when fine-tuned on labeled data.
- **Evidence anchors:**
  - [abstract]: "leveraging the microphone array of smart glasses to achieve directional speech recognition, source localization, and bystander cross-talk suppression"
  - [Section 2.1.1]: "The inter-channel amplitude differences in the beamformed signals effectively capture the direction-of-arrival (DOA) information"
  - [corpus]: Related work on multi-channel speech processing exists (MIMO-speech, neural beamformers), but corpus evidence for LLM-based spatial reasoning specifically is limited.
- **Break condition:** If beamformed channels are collapsed to mono before the encoder, or if the array geometry differs substantially from training, spatial discrimination degrades.

### Mechanism 2
- **Claim:** Serialized Directional Output Training (S-DOT) creates a unified supervision signal that jointly trains localization and transcription.
- **Mechanism:** By inserting direction tokens (e.g., `<30°>`) directly into the reference transcript sequence before each speaker's text, the model learns to predict direction labels as part of the language modeling objective. This creates implicit supervision for both tasks through a single loss function.
- **Core assumption:** The LLM's next-token prediction can generalize to treat direction labels as structural tokens similar to speaker-change tokens.
- **Evidence anchors:**
  - [abstract]: "S-DOT incorporates spatial information into the model by serializing reference transcripts with direction labels"
  - [Section 2.2]: "This enables the model to jointly learn the DOA information and the speech content"
  - [corpus]: Serialized Output Training (SOT) is established for multi-talker ASR [31], but S-DOT's extension to spatial labels is novel.
- **Break condition:** If speakers move during utterances, or if direction label precision exceeds the beamformer's spatial resolution (30°), the supervision signal becomes noisy.

### Mechanism 3
- **Claim:** Contrastive Direction Data Augmentation (CDDA) teaches the model to suppress non-target directions.
- **Mechanism:** Training examples are augmented with "distractor" speakers from off-target directions. The model must transcribe target-direction speech while ignoring distractor content, creating a contrastive learning pressure to associate each direction label with its corresponding content only.
- **Core assumption:** The model learns a "rejection" behavior for non-target directions rather than simply memorizing specific direction-content pairs.
- **Evidence anchors:**
  - [abstract]: "CDDA improves spatial directivity by augmenting training data with distractor speakers"
  - [Section 2.3]: "This guides the model to transcribe for the desired directions while suppressing output for undesired ones"
  - [Section 4.2]: "CDDA method... achieves the best performance of 3.81% success WER and 98.4% success rate"
  - [corpus]: Contrastive learning for spatial audio is not well-documented in the corpus; this appears to be an underexplored technique.
- **Break condition:** If distractors overlap temporally with target speech (high overlap ratio), the model struggles—Table 4 shows 21.32% sWER at 25% overlap.

## Foundational Learning

- **Concept: Beamforming and Array Processing**
  - **Why needed here:** The entire approach depends on understanding how microphone arrays capture spatial information through time-delay differences and how beamforming extracts directional signals.
  - **Quick check question:** Can you explain why a 7-channel array enables 12-direction discrimination at 30° resolution?

- **Concept: Serialized Output Training (SOT)**
  - **Why needed here:** S-DOT builds directly on SOT for multi-talker ASR. Without understanding the base technique, the spatial extension will be unclear.
  - **Quick check question:** How does inserting a special token between speaker segments enable multi-talker recognition in a single output stream?

- **Concept: Speech LLM Integration via Adapters**
  - **Why needed here:** The architecture uses a frozen audio encoder + trainable adapter to bridge audio and text modalities. This is a standard pattern but critical to understand for implementation.
  - **Quick check question:** Why freeze the audio encoder weights while fine-tuning only the adapter and projection layers?

## Architecture Onboarding

- **Component map:** Multi-channel Audio (7 ch) → NLCMV Beamformer (12 fixed directions) → Linear Projection (flattens multi-channel features → encoder dim) → Audio Encoder [FROZEN] (pre-trained, ~1B params) → Adapter Layer [FINE-TUNED] (conv + attention) → LLaMA-3 Decoder [8B, mostly frozen, 1 layer unfrozen for complex cases]

- **Critical path:** The linear projection layer is the critical innovation—it must correctly flatten and preserve inter-channel amplitude relationships. Incorrect flattening order or dimension handling will break spatial learning.

- **Design tradeoffs:**
  - 12-direction (30° resolution) vs. 5-direction training: 12-direction achieves 92% accuracy (Table 1d) but requires more data; 5-direction is simpler but generalizes poorly to unseen directions
  - CDDA improves seen-direction performance (98.4% SR) but sacrifices unseen-direction generalization (Table 2h)
  - Unfreezing one LLM decoder layer enables handling edge cases (same-direction speakers, empty targets) but increases training cost

- **Failure signatures:**
  - Low L/R accuracy on unseen directions → linear projection may not preserve spatial features correctly
  - High WER with good direction accuracy → adapter not fine-tuned adequately; encoder-audio alignment weak
  - Degradation on overlapping speech (21.32% sWER at 25% overlap) → expected limitation; pretrained model has no overlapping speech exposure

- **First 3 experiments:**
  1. **Sanity check:** Train Stage 1 (multi-channel ASR) only. Verify WER is competitive with Whisper baseline (~6% vs. 4.4% in Table 1). If not, debug linear projection and adapter initialization.
  2. **S-DOT validation:** Train Stage 2 on 5 frontal directions. Confirm direction accuracy >90% on seen directions and L/R accuracy >90% on unseen (Table 1c). Failure indicates serialization format or tokenization issues.
  3. **CDDA ablation:** Compare Stage 3 with and without CDDA on multi-talker target-direction ASR. Expect ~2% SR improvement (Table 2e vs. 2h). No gain suggests distractor augmentation pipeline is broken.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the model's performance on overlapping speech regions be improved, given the substantial degradation from 3.81% to 21.32% sWER at 25% overlap ratio?
- **Basis in paper:** [explicit] Section 4.4 states "performance degrades when handling overlapping speech" and hypothesizes "this shortcoming arises from the pretrained model not being exposed to overlapping speech."
- **Why unresolved:** The current pre-trained LLM backbone lacks exposure to overlapping speech during pre-training, and no augmentation strategy is proposed to address this specific failure mode.
- **What evidence would resolve it:** Experiments with overlap-specific data augmentation or modified pre-training objectives showing improved sWER under overlapping conditions.

### Open Question 2
- **Question:** Can the model generalize to moving speakers who change direction during conversation, relaxing the current static-speaker assumption?
- **Basis in paper:** [explicit] Section 2.2 states "For simplicity, we assume that each speaker does not move and always speaks from a single direction during the conversation."
- **Why unresolved:** The S-DOT serialization scheme assigns one direction label per speaker segment, which cannot capture within-utterance movement.
- **What evidence would resolve it:** Evaluation on datasets with time-varying source positions, potentially requiring frame-level or segment-level direction label modifications.

### Open Question 3
- **Question:** How can CDDA be extended to maintain performance on unseen directions while preserving its benefits for seen directions?
- **Basis in paper:** [explicit] Section 4.2 notes CDDA achieves best performance "albeit at the cost of neglecting unseen directions."
- **Why unresolved:** The contrastive formulation explicitly trains the model to reject distractor directions, which may overfit to the specific direction set used during training.
- **What evidence would resolve it:** Modified CDDA incorporating mixed seen/unseen direction augmentation, evaluated on both seen and unseen direction generalization.

## Limitations
- **Limited generalization to unseen directions:** The model shows 92% accuracy for 12 training directions but drops to 86.9-87.8% on unseen left/right directions, with no evidence of spatial interpolation capability.
- **Sensitivity to speech overlap:** Performance degrades significantly with increased overlap, jumping from 3.81% to 21.32% sWER as overlap ratio increases from 5% to 25%.
- **Fixed beamforming geometry dependency:** The approach requires predetermined NLCMV beamforming coefficients for specific array geometries, limiting adaptability to different microphone configurations.

## Confidence

- **High confidence:** Single-talker directional ASR performance (3.84% WER, 92% direction accuracy) - well-validated through controlled experiments with clear metrics
- **Medium confidence:** Multi-talker target-direction ASR (98.4% success rate, 3.81% sWER) - validated but highly dependent on low overlap conditions
- **Medium confidence:** S-DOT and CDDA effectiveness - novel techniques with positive results but limited ablation studies
- **Low confidence:** Generalization to unseen array geometries and directions - no experiments validate performance outside training conditions

## Next Checks

1. **Direction interpolation test:** Evaluate model performance on intermediate directions (e.g., 15°, 45°) not present in training to assess spatial interpolation capability. Compare against nearest-neighbor baseline to quantify learned spatial generalization.

2. **Array geometry robustness test:** Validate performance using different microphone array configurations (different channel count, spacing) through simulation. Measure degradation in direction accuracy and WER to establish geometry sensitivity bounds.

3. **Real-world deployment validation:** Test on actual Project Aria glasses hardware in real acoustic environments with moving speakers and varying noise conditions. Compare against simulated performance to quantify sim-to-real gap.