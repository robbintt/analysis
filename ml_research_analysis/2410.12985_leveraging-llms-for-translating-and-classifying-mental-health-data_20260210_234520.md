---
ver: rpa2
title: Leveraging LLMs for Translating and Classifying Mental Health Data
arxiv_id: '2410.12985'
source_url: https://arxiv.org/abs/2410.12985
tags:
- health
- mental
- depression
- posts
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the ability of large language models (LLMs)
  to detect depression severity from user-generated posts in English and Greek. We
  use GPT-3.5-turbo to classify posts into four depression severity levels, first
  directly in English and then after translating the posts to Greek.
---

# Leveraging LLMs for Translating and Classifying Mental Health Data

## Quick Facts
- arXiv ID: 2410.12985
- Source URL: https://arxiv.org/abs/2410.12985
- Reference count: 9
- Primary result: GPT-3.5-turbo achieves F1 scores of 0.17-0.25 for English depression severity classification; Greek performance drops except for moderate level

## Executive Summary
This study investigates large language models' ability to detect depression severity from user-generated posts in English and Greek. Using GPT-3.5-turbo in a zero-shot setting, the researchers classify Reddit posts into four depression severity levels, first directly in English and then after machine translation to Greek. Results show consistently low performance across languages, with the model struggling to distinguish between severity levels and often conflating anxiety symptoms with moderate depression. The methodology demonstrates feasibility but highlights significant challenges in using LLMs for mental health assessment across languages, emphasizing the need for human oversight.

## Method Summary
The study uses GPT-3.5-turbo API with temperature=0 for both translation and classification tasks. For English classification, posts from the DEPSEVERITY dataset are directly classified using a prompt that asks the model to categorize text into one of four depression severity levels. For Greek classification, each English post is first translated to Greek using GPT-3.5-turbo, then the translated text is classified using the same prompt. The researchers extract integer labels from the model's free-form responses and compute per-class precision, recall, and F1 scores, with macro-average F1 as the primary metric. The approach is resource-efficient but relies on zero-shot prompting without task-specific training.

## Key Results
- English classification achieves F1 scores ranging from 0.17 to 0.25 across severity levels
- Greek classification performance drops overall except for the moderate level, which improves
- Model frequently misclassifies anxiety-related language as moderate depression
- Edge classes (minimal, severe) show better detection than intermediate severity levels
- Translation accuracy is generally high but doesn't prevent classification errors

## Why This Works (Mechanism)

### Mechanism 1
Zero-shot LLM classification can identify coarse-grained mental health signals but struggles with fine-grained severity distinctions. The LLM retrieves pre-trained associations between linguistic markers (negative emotions, self-criticism, social withdrawal) and mental health concepts, then maps these to severity levels based on pattern intensity—without task-specific training. This works because depression severity manifests in detectable linguistic patterns that generalize across contexts and are represented in the model's pre-training data. However, the overall low performance (F1=0.17) can be attributed to the difficulty of detecting specific levels of depression, which are considered less distinct compared to other conditions. The mechanism breaks when severity levels lack distinct linguistic boundaries or when cultural expressions of distress don't match training distribution.

### Mechanism 2
LLM-based translation preserves semantic content sufficiently for downstream classification, but classification errors compound across the pipeline. The LLM translates English posts to Greek while preserving key emotional terminology; a second LLM call then classifies the translated text using the same prompt-based approach—both steps operating without supervised fine-tuning. This assumes translation quality for mental health language is adequate and semantic loss is minimal compared to classification difficulty. Translation accuracy is generally high, but misclassification often arises from the model's tendency to interpret anxiety-related language as moderate depression. The mechanism breaks when mental health terminology lacks direct equivalents in the target language or when cultural framing differs substantially.

### Mechanism 3
Edge classes (minimal, severe) are more reliably detected than intermediate severity levels due to clearer boundary conditions. Extreme presentations contain more distinctive markers (severe: explicit self-harm ideation; minimal: absence of markers), while moderate levels require nuanced judgment about symptom intensity that overlaps with anxiety and other conditions. This works because severity follows a detectable gradient with qualitatively different linguistic features at extremes. In Greek, overall performance drops except for the moderate level, which improves, while severe (F1=0.22) and minimal (F1=0.25) outperform mild (F1=0.07) in English. The mechanism breaks when comorbid conditions (anxiety + depression) blur severity boundaries or when intermediate classes require contextual judgment beyond surface patterns.

## Foundational Learning

- Concept: Zero-shot vs. fine-tuned classification
  - Why needed here: The study uses GPT-3.5-turbo without task-specific training; understanding this distinction explains both the cost efficiency and performance limitations
  - Quick check question: Can you articulate why zero-shot approaches might underperform on nuanced, multi-class tasks with imbalanced data?

- Concept: Class imbalance effects on evaluation metrics
  - Why needed here: The DEPSEVERITY dataset is highly imbalanced (minimal: 2587, mild: 290, moderate: 394, severe: 282); macro F1 penalizes poor performance on small classes
  - Quick check question: Why would accuracy be misleading here, and what does macro F1 reveal about per-class performance?

- Concept: Error propagation in multi-stage NLP pipelines
  - Why needed here: Translation → classification creates two potential failure points; distinguishing translation errors from classification errors is essential for debugging
  - Quick check question: If Greek F1 drops but translation appears accurate, where should debugging focus?

## Architecture Onboarding

- Component map: Raw text → Translation API → Classification prompt → Label extraction → Metric computation
- Critical path: Text preprocessing → Translation API call → Classification prompt → Label extraction → Metric computation. Translation quality gates downstream classification; prompt design gates model interpretation of severity levels.
- Design tradeoffs:
  - Zero-shot (cheap, fast, low performance) vs. fine-tuned (expensive, higher performance, requires labeled data)
  - Binary classification (easier, less clinically useful) vs. multi-class severity (harder, more actionable for triage)
  - Single-language vs. multilingual (broader reach, compounding errors, lower resource languages underperform)
- Failure signatures:
  - High recall/low precision on severe class: Model over-predicts severity from anxiety keywords
  - Near-zero F1 on mild class: Intermediate severity lacks distinctive markers
  - Cross-language F1 drop without translation errors: Cultural/linguistic mismatch in severity expression
  - Confidence calibration failure: Model appears confident even when wrong
- First 3 experiments:
  1. Baseline replication: Reproduce English classification on DEPSEVERITY subset; verify F1 ≈0.17 macro average with edge-class advantage
  2. Error analysis on anxiety overlap: Manually annotate 50 misclassified posts for anxiety vs. depression markers; quantify conflation rate
  3. Prompt engineering sweep: Test 3 prompt variants (with definitions, with examples, with chain-of-thought) on held-out set; measure F1 delta per class

## Open Questions the Paper Calls Out

### Open Question 1
To what extent does the model's tendency to conflate anxiety symptoms with depression affect the classification accuracy of specific severity levels? The study identifies this error pattern but does not propose or test a mechanism to disentangle distinct clinical concepts (anxiety vs. depression) in the LLM's reasoning process. A follow-up experiment using prompt engineering that explicitly defines boundaries between anxiety and depression symptoms would resolve this by showing whether the F1 score for the "moderate" class stabilizes.

### Open Question 2
Would the detection performance in the target language (Greek) improve if the model were evaluated on native Greek text rather than machine-translated English posts? The limitations section acknowledges that using an LLM for translation introduces "a small loss of information" and relies on translated data because Greek resources for this task do not exist. A comparative study evaluating the model on a newly curated dataset of native Greek social media posts versus the translated English dataset would resolve this by distinguishing model capability from translation artifacts.

### Open Question 3
Can few-shot prompting or in-context learning eliminate the performance volatility observed between the "moderate" severity level and the "mild" or "severe" levels? The authors utilize a resource-efficient 0-shot methodology but observe inconsistent results, specifically a performance increase for the "moderate" class in Greek while other classes drop. Experiments measuring macro F1 scores while incrementally increasing the number of labeled examples provided in the prompt would determine if class-wise variance decreases through better-defined severity boundaries.

## Limitations

- Zero-shot approach yields low performance (F1 0.17-0.25), indicating methodology not ready for clinical deployment
- Class imbalance severely impacts results, with mild and severe classes receiving disproportionately low F1 scores despite clinical importance
- Relies on self-reported labels from Reddit posts, which may contain noise and bias
- Translation quality assessment limited to automated checks rather than human evaluation
- Cross-lingual setup compounds uncertainty as depression expression may vary culturally between English and Greek contexts

## Confidence

**High Confidence**: Translation accuracy preservation - The study shows translations are generally high quality based on automated metrics, and this aligns with GPT-3.5-turbo's established translation capabilities. The mechanism linking translation quality to downstream performance is well-understood in NLP.

**Medium Confidence**: Anxiety-conflation as primary misclassification driver - While the paper identifies anxiety-related language as a major confounder, this conclusion is based on manual inspection of a subset rather than systematic annotation. Other factors (cultural expression differences, prompt ambiguity) may contribute equally.

**Low Confidence**: Severity-level detectability claims - The assertion that edge classes are more detectable than intermediate levels lacks direct empirical validation beyond observed F1 scores. The study doesn't test whether linguistic features actually differ qualitatively between severity levels, nor does it validate that the chosen severity thresholds correspond to clinically meaningful distinctions.

## Next Checks

1. **Controlled error analysis**: Annotate 200 misclassified posts for anxiety markers, symptom intensity, and cultural framing to quantify the relative contribution of each confounder to classification errors.

2. **Prompt variation experiment**: Test three prompt formulations (with severity definitions, with example posts, with chain-of-thought reasoning) across both languages to determine if performance gains are prompt-dependent rather than model/capability-bound.

3. **Cultural validation study**: Recruit bilingual mental health professionals to evaluate 50 translated posts against originals for semantic preservation and cultural appropriateness, measuring inter-rater agreement and identifying systematic translation divergences.