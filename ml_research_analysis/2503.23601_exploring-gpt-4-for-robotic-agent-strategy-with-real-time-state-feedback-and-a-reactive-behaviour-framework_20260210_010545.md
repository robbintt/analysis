---
ver: rpa2
title: Exploring GPT-4 for Robotic Agent Strategy with Real-Time State Feedback and
  a Reactive Behaviour Framework
arxiv_id: '2503.23601'
source_url: https://arxiv.org/abs/2503.23601
tags:
- robot
- goal
- tasks
- language
- ball
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a novel method for using GPT-4 to control a
  humanoid robot in simulation and real-world environments. The approach integrates
  the LLM with a reactive behavior framework and real-time message-passing system
  to generate task plans that achieve user goals.
---

# Exploring GPT-4 for Robotic Agent Strategy with Real-Time State Feedback and a Reactive Behaviour Framework

## Quick Facts
- arXiv ID: 2503.23601
- Source URL: https://arxiv.org/abs/2503.23601
- Reference count: 6
- This work presents a novel method for using GPT-4 to control a humanoid robot in simulation and real-world environments with high success and executability rates.

## Executive Summary
This work presents a novel method for using GPT-4 to control a humanoid robot in simulation and real-world environments. The approach integrates the LLM with a reactive behavior framework and real-time message-passing system to generate task plans that achieve user goals. By leveraging frequent state feedback and a tree-based behavior system, the method produces executable output with smooth transitions between tasks. Experiments show the robot successfully completes goals across varying time horizons in both simulation and on hardware, with high executability rates.

## Method Summary
The method uses GPT-4 via OpenAI API to control a humanoid robot by mapping natural language goals to executable sub-tasks. The system employs a reactive behavior framework with real-time state feedback, prompting the LLM every 2 seconds with current state information and user goals. The LLM outputs structured tasks with priorities that are executed through a Director framework handling resource conflicts and soft transitions. Safety providers (e.g., getup) run in parallel to override LLM decisions when the robot falls. The system was tested on 10 different goals across 3-5 trials each, achieving high success and executability rates in both simulation and hardware.

## Key Results
- 94% success rate in achieving user goals across 10 different tasks
- 94% executability rate for LLM-generated tasks
- Smooth transitions between tasks without execution hiccups
- Effective fall detection and recovery system that overrides LLM when needed

## Why This Works (Mechanism)

### Mechanism 1: Closed-Loop State Synchronization via Rolling Prompts
Frequent re-prompting of the LLM with real-time state data allows the system to function as a reactive closed-loop controller, adapting high-level plans to dynamic environmental changes without complex memory management.

### Mechanism 2: Hierarchical Arbitration via Resource Acquisition
The Director behavior framework acts as a safety governor by enforcing priority-based resource locking, preventing the LLM from issuing conflicting motor commands or bypassing critical safety behaviors.

### Mechanism 3: Syntactic Guardrailing with Programmatic Prompts
Constraining the LLM's output vocabulary to a strict programmatic format (Task + Priority) significantly reduces the rate of non-executable (hallucinated) actions compared to free-form natural language planning.

## Foundational Learning

- **Concept: Publisher-Subscriber Architecture (Middleware)**
  - Why needed: The system relies on NUClear, a real-time message-passing system. Understanding how modular components communicate asynchronously is required to debug timing issues.
  - Quick check: Can you explain how the system ensures the "Vision" provider's data is fresh when the "LLM" provider constructs its prompt?

- **Concept: Behavior Trees / Finite State Machines (FSM)**
  - Why needed: The Director is a tree-based behavior system. You must understand how parent-child nodes execute and how "priority" interrupts work to implement new safety behaviors.
  - Quick check: In a behavior tree, does a higher-priority node pause a lower-priority node or terminate it entirely?

- **Concept: Prompt Engineering & Format Enforcement**
  - Why needed: The system relies on regex to parse LLM output. You need to know how to engineer the prompt to prevent the model from outputting conversational filler that breaks the parser.
  - Quick check: If the LLM outputs "Task: WalkToBall Priority: High" instead of "Priority: 1", will the regex parser fail?

## Architecture Onboarding

- **Component map:** IMU & Camera -> Ball Detection & Fall Detection -> LLM Provider -> Director -> Actuation Layer
- **Critical path:** State Update -> Prompt Construction -> API Call -> Regex Parse -> Director Arbitration must complete within 2-second polling window
- **Design tradeoffs:** Safety vs. Flexibility (hard-coded safety Providers limit LLM's ability to learn novel recovery strategies); Cloud vs. Edge (using OpenAI API enables high reasoning but requires internet connectivity)
- **Failure signatures:** "Phantom Task" Loop (LLM outputs non-existent task); Priority Inversion (Safety Provider has lower priority than LLM); Feedback Drift (Vision module hallucinates ball)
- **First 3 experiments:** Empty World Test (verify LLM outputs "StandStill" with no goal); Parser Stress Test (inject malformed responses to verify safe failure); Interrupt Test (command "Approach ball," push robot over, verify Director switches to "GetUp")

## Open Questions the Paper Calls Out

- Can locally hosted, smaller language models achieve comparable real-time performance to cloud-based GPT-4 within resource-constrained robotic systems?
- Does incorporating localization data into the prompt improve task planning, or does it degrade performance due to increased prompt complexity?
- Can automated prompt optimization techniques mitigate the output volatility caused by minor grammatical changes in user requests?

## Limitations
- Relies on custom NUClear message-passing middleware and Director framework not publicly available
- Safety guarantees limited to explicitly coded scenarios in Director framework
- Limited validation to soccer-related goals using fixed set of seven tasks

## Confidence
- **High Confidence**: Closed-loop state feedback through rolling prompts is well-supported by experimental results
- **Medium Confidence**: Executability and success rates are promising but limited by small sample size and specific task domain
- **Low Confidence**: Computational overhead, network reliability requirements, and behavior under API failures are not addressed

## Next Checks
1. Stress Test Safety Overrides: Systematically test whether the safety provider (GetUp) reliably interrupts LLM commands across all possible failure scenarios
2. Prompt Robustness Validation: Create comprehensive test suite of malformed or ambiguous prompts to verify safe failure
3. Latency Impact Analysis: Measure system performance under realistic network conditions to determine maximum sustainable polling frequency