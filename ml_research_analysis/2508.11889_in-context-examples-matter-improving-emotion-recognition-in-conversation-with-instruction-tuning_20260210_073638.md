---
ver: rpa2
title: 'In-Context Examples Matter: Improving Emotion Recognition in Conversation
  with Instruction Tuning'
arxiv_id: '2508.11889'
source_url: https://arxiv.org/abs/2508.11889
tags:
- in-context
- examples
- tuning
- emotion
- instruction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes InitERC, a one-stage in-context instruction
  tuning framework for emotion recognition in conversation (ERC). Unlike existing
  multi-stage approaches, InitERC jointly captures speaker characteristics, conversational
  context, and emotional states within a unified framework.
---

# In-Context Examples Matter: Improving Emotion Recognition in Conversation with Instruction Tuning

## Quick Facts
- **arXiv ID:** 2508.11889
- **Source URL:** https://arxiv.org/abs/2508.11889
- **Authors:** Hui Ma; Bo Zhang; Jinpeng Hu; Zenglin Shi
- **Reference count:** 22
- **Primary result:** InitERC achieves SOTA performance, outperforming best baseline by 20.25%, 9.44%, and 14.42% in weighted F1 score on IEMOCAP, MELD, and EmoryNLP datasets respectively

## Executive Summary
This paper proposes InitERC, a one-stage in-context instruction tuning framework for emotion recognition in conversation (ERC). Unlike existing multi-stage approaches, InitERC jointly captures speaker characteristics, conversational context, and emotional states within a unified framework. The method constructs a demonstration pool, retrieves relevant in-context examples, designs prompt templates, and performs instruction tuning to align speaker-context-emotion relationships.

Comprehensive experiments on three benchmark datasets (IEMOCAP, MELD, EmoryNLP) demonstrate that InitERC achieves state-of-the-art performance, outperforming the best baseline by 20.25%, 9.44%, and 14.42% in weighted F1 score respectively. The study also investigates the impact of retrieval strategy, example ordering, and number of examples on performance.

## Method Summary
InitERC introduces a one-stage in-context instruction tuning framework that unifies speaker characteristics, conversational context, and emotional states within a single model architecture. The approach constructs a demonstration pool from training data, retrieves relevant in-context examples using text-based similarity metrics, designs specialized prompt templates for ERC tasks, and performs instruction tuning to align the relationships between speakers, context, and emotions. Unlike traditional multi-stage approaches that handle these elements separately, InitERC integrates them into a cohesive framework that leverages in-context learning capabilities of modern language models.

## Key Results
- Achieves state-of-the-art performance on IEMOCAP dataset with 20.25% improvement in weighted F1 score over best baseline
- Outperforms existing methods on MELD dataset by 9.44% in weighted F1 score
- Demonstrates 14.42% improvement on EmoryNLP dataset compared to previous best results

## Why This Works (Mechanism)
The framework's success stems from its ability to leverage in-context examples during inference, allowing the model to adapt to specific conversational patterns and emotional nuances without requiring additional fine-tuning. By retrieving relevant demonstrations that share similar speaker characteristics and conversational contexts, InitERC provides the model with targeted examples that guide emotion prediction for each new conversation. The instruction tuning phase ensures the model learns to properly utilize these in-context examples, creating a synergistic relationship between retrieval and prediction that captures complex speaker-context-emotion dependencies.

## Foundational Learning
- **In-context learning**: Understanding how language models can adapt to new tasks using demonstration examples within prompts. Why needed: Enables model adaptation without additional training. Quick check: Verify model can perform simple tasks with 3-5 examples.
- **Retrieval-based prompting**: Ability to search and select relevant examples from demonstration pools. Why needed: Ensures contextually appropriate examples are provided. Quick check: Test retrieval accuracy on diverse conversational scenarios.
- **Speaker-context-emotion alignment**: Recognizing how speaker identity, conversation history, and emotional states interact. Why needed: Captures complex dependencies in conversational data. Quick check: Validate alignment performance on speaker-specific conversations.

## Architecture Onboarding

**Component Map:**
Demonstration Pool -> Retrieval Engine -> Prompt Template -> Instruction Tuned Model -> ERC Prediction

**Critical Path:**
1. Construct demonstration pool from training data
2. Retrieve relevant in-context examples based on query conversation
3. Generate prompt using retrieved examples and conversation context
4. Feed prompt to instruction-tuned model
5. Produce emotion prediction for target utterance

**Design Tradeoffs:**
- Single-stage vs. multi-stage processing (simplified pipeline vs. specialized components)
- Text-only retrieval vs. multimodal context (computational efficiency vs. contextual richness)
- Fixed vs. adaptive number of in-context examples (consistency vs. flexibility)

**Failure Signatures:**
- Poor retrieval quality leading to irrelevant examples
- Overfitting to demonstration pool patterns
- Prompt template incompatibility with model architecture
- Computational bottleneck during real-time retrieval

**First Experiments:**
1. Baseline ERC performance without in-context examples
2. Retrieval quality assessment on demonstration pool
3. Impact of example ordering on prediction accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- Retrieval strategy generalizability unclear for multimodal conversations
- Fixed number of in-context examples (8) may not be optimal across scenarios
- Evaluation limited to three datasets, potentially missing conversational diversity
- Computational overhead of demonstration pool maintenance could limit practical deployment

## Confidence
- High confidence: Core finding that in-context examples significantly improve ERC performance
- Medium confidence: Specific performance metrics depending on demonstration pool quality
- Low confidence: Claims about generalizability beyond studied datasets

## Next Checks
1. Conduct experiments on additional diverse ERC datasets (including multimodal conversations) to assess generalizability across different conversational domains and speaker demographics.

2. Perform comprehensive ablation study varying the number of in-context examples (1-32) to identify optimal ranges for different conversation lengths and emotional complexity levels.

3. Implement and evaluate computational overhead of demonstration pool maintenance and retrieval system in production-like environment, measuring latency and resource requirements compared to existing approaches.