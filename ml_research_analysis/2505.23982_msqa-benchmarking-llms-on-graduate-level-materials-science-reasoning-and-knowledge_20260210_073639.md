---
ver: rpa2
title: 'MSQA: Benchmarking LLMs on Graduate-Level Materials Science Reasoning and
  Knowledge'
arxiv_id: '2505.23982'
source_url: https://arxiv.org/abs/2505.23982
tags:
- llms
- materials
- question
- science
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MSQA, a graduate-level materials science
  benchmark designed to evaluate large language models (LLMs) on complex reasoning
  and factual knowledge. The benchmark includes 1,757 questions across seven materials
  science subfields, requiring both detailed explanatory responses and binary True/False
  assessments.
---

# MSQA: Benchmarking LLMs on Graduate-Level Materials Science Reasoning and Knowledge

## Quick Facts
- arXiv ID: 2505.23982
- Source URL: https://arxiv.org/abs/2505.23982
- Reference count: 34
- Introduces MSQA, a graduate-level materials science benchmark evaluating LLMs on complex reasoning and factual knowledge across 1,757 questions

## Executive Summary
This paper introduces MSQA, a graduate-level materials science benchmark designed to evaluate large language models (LLMs) on complex reasoning and factual knowledge. The benchmark includes 1,757 questions across seven materials science subfields, requiring both detailed explanatory responses and binary True/False assessments. Through experiments with ten state-of-the-art LLMs, including both general-purpose and domain-specific models, the authors find that proprietary black-box models achieve up to 84.5% accuracy, while open-source models peak around 60.5%. Domain-specific models often underperform due to overfitting and distributional shifts. The study highlights the importance of retrieval augmentation and reveals significant gaps in current LLM performance for advanced materials science reasoning.

## Method Summary
The authors constructed MSQA through expert-driven question design covering seven materials science subfields. Questions require both detailed explanatory responses and binary True/False assessments. Ten state-of-the-art LLMs were evaluated, including general-purpose models (GPT-4, Claude-3, Gemini-1.5) and domain-specific models. The evaluation protocol employed single-expert grading for complex responses, with accuracy measured on the binary True/False format. Retrieval augmentation effectiveness was assessed by comparing performance with and without access to external knowledge sources.

## Key Results
- Proprietary black-box models achieved up to 84.5% accuracy on the MSQA benchmark
- Open-source models peaked around 60.5% accuracy, showing significant performance gaps
- Domain-specific models often underperformed due to overfitting and distributional shifts
- Retrieval augmentation demonstrated importance for improving LLM performance on complex materials science questions

## Why This Works (Mechanism)
The benchmark's design leverages expert knowledge to create questions that require both factual recall and complex reasoning, distinguishing between surface-level understanding and deep domain expertise. The dual-format approach (explanatory and binary) allows for comprehensive evaluation of different reasoning capabilities. Retrieval augmentation provides contextual information that helps bridge knowledge gaps in model training data.

## Foundational Learning
- Materials science subfield distinctions: Understanding the seven distinct areas (thermodynamics, kinetics, etc.) is crucial for interpreting model performance variations across domains
- Retrieval-augmented generation: Why needed - addresses knowledge gaps in specialized domains; Quick check - measure performance improvement with/without retrieval
- Domain-specific model limitations: Why needed - explains underperformance despite specialized training; Quick check - compare performance on in-domain vs out-of-domain questions
- Binary vs explanatory assessment: Why needed - captures different aspects of reasoning capability; Quick check - correlate accuracy with response quality scores

## Architecture Onboarding

Component Map: Question Design -> Model Evaluation -> Expert Grading -> Performance Analysis

Critical Path: Expert question creation → LLM response generation → Binary assessment grading → Accuracy calculation → Retrieval augmentation testing

Design Tradeoffs: Single-expert grading provides consistency but may introduce bias; binary format enables objective assessment but may oversimplify complex reasoning; proprietary vs open-source models trade performance for accessibility

Failure Signatures: Domain-specific models show distributional shift vulnerability; retrieval-augmentation failure indicates knowledge gap beyond available sources; accuracy plateaus suggest fundamental reasoning limitations

First 3 Experiments:
1. Compare single vs multiple expert grading on 100 random responses to establish inter-rater reliability
2. Test LLM performance on held-out questions specifically designed to probe distributional shift vulnerabilities
3. Conduct head-to-head comparisons between LLM responses and expert-generated answers on a subset of questions

## Open Questions the Paper Calls Out
None

## Limitations
- Single-expert evaluation may introduce subjective bias in grading complex materials science responses
- Binary True/False format may not fully capture nuanced understanding required for graduate-level reasoning
- Absence of comparative human expert performance data makes it difficult to establish baseline expectations

## Confidence

High: The benchmark's structural validity and comprehensive coverage of seven materials science subfields

Medium: Accuracy comparisons between proprietary and open-source models

Low: Causal claims about retrieval augmentation effectiveness due to limited ablation studies

## Next Checks

1. Conduct inter-rater reliability testing with multiple domain experts to establish grading consistency

2. Perform head-to-head comparisons between LLM responses and expert-generated answers on a subset of questions

3. Test model performance on held-out questions specifically designed to probe distributional shift vulnerabilities identified in domain-specific models