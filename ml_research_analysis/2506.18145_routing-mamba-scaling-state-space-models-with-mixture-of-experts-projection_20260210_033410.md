---
ver: rpa2
title: 'Routing Mamba: Scaling State Space Models with Mixture-of-Experts Projection'
arxiv_id: '2506.18145'
source_url: https://arxiv.org/abs/2506.18145
tags:
- mamba
- layers
- experts
- arxiv
- scaling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Routing Mamba (RoM), the first framework for
  integrating sparse mixture of experts with State Space Models by leveraging Mamba's
  projection layers as scalable expert components. Unlike naive MoE integration that
  degrades performance, RoM employs selective scaling and shared routing decisions
  across Conv, Gate, and Output projections, enabling effective sparse scaling of
  Mamba layers.
---

# Routing Mamba: Scaling State Space Models with Mixture-of-Experts Projection

## Quick Facts
- arXiv ID: 2506.18145
- Source URL: https://arxiv.org/abs/2506.18145
- Reference count: 40
- Primary result: RoM achieves language modeling performance equivalent to dense Mamba models requiring 2.3× more active parameters

## Executive Summary
This paper introduces Routing Mamba (RoM), the first framework for integrating sparse mixture-of-experts with State Space Models by leveraging Mamba's projection layers as scalable expert components. Unlike naive MoE integration that degrades performance, RoM employs selective scaling and shared routing decisions across Conv, Gate, and Output projections, enabling effective sparse scaling of Mamba layers. The approach demonstrates consistent perplexity improvements across multiple scales and achieves 23% FLOPS savings when applied to hybrid models like Samba.

## Method Summary
RoM expertizes Conv, Gate, and Output projections while sharing lightweight sub-modules (x Proj, dt Proj, Conv1D) across all experts. A single router selects one expert index applied uniformly across all three projection types, fostering synergistic co-adaptation among expert projections. The framework includes jitter noise and SparseMixer for stable routing gradient estimation, and avoids auxiliary load balance losses that can harm performance.

## Key Results
- At 1.3B active parameters (10B total), RoM achieves language modeling performance equivalent to dense Mamba models requiring 2.3× more active parameters
- Consistent perplexity improvements across context lengths (4K/8K/16K) compared to naive MoE integration
- 23% FLOPS savings when applied to hybrid Samba models while maintaining similar performance
- Effective generalization to other SSM architectures including Mamba-2 and Gated DeltaNet

## Why This Works (Mechanism)

### Mechanism 1
Shared routing across Conv, Gate, and Output projections enables coherent expert specialization, whereas independent routing per layer degrades performance. A single router selects a unified "expert pathway" applied to all three projection types simultaneously, aligning expert selection with the functional interdependence of Mamba's projections.

### Mechanism 2
Selective scaling—applying MoE only to large projections while sharing lightweight sub-modules—reduces overhead without sacrificing expressivity. x Proj, dt Proj, and Conv1D have relatively few parameters and are shared across all experts, preserving the SSM's data-dependent parameterization while scaling high-capacity linear projections.

### Mechanism 3
Gating at the Output projection (element-wise multiplication with gate values) preserves the SSM's recurrent dynamics while enabling sparse expert contributions. The final output computes a weighted sum of expert outputs, where the gating weight scales the expert output while the element-wise multiplication applies the SSM output and gate activation.

## Foundational Learning

- **State Space Model (SSM) Recurrence**: Understanding that Y_t = C·h_t, where h_t depends on all prior inputs via linear recurrence, is essential to grasp why experts see "contextualized" representations. Quick check: Given h_t = A·h_{t-1} + B·u_t and Y_t = C·h_t, does Y_t at position t depend only on u_t, or on u_1, ..., u_t?

- **Mixture-of-Experts (MoE) Routing**: RoM uses Top-K routing with shared decisions; understanding how routers compute softmax probabilities and select experts is necessary to implement and debug the routing mechanism. Quick check: If a router outputs softmax scores [0.1, 0.3, 0.4, 0.2] for 4 experts with Top-2 selection, which experts are activated and what are their weights after normalization?

- **Mamba Projection Roles**: RoM selectively expertizes Conv Proj (input expansion + short convolution), Gate Proj (gating pathway), and Output Proj (final projection). Understanding their distinct roles clarifies why shared routing across them enables coherent expert pathways. Quick check: In O = Y ⊙ SiLU(X·W_g)·W_out, which projection controls the gating signal and which produces the final output dimension?

## Architecture Onboarding

- **Component map**: Input X -> Projection Router (Wr ∈ R^{Dm×N}) -> Top-K selection -> shared expert indices -> Conv Proj Experts {Win,i} -> Short Conv + SSM -> Y_t -> Gate Proj Experts {Wg,i} -> G -> Output Proj Experts {Wout,i} -> final O_t; Lightweight shared params: x Proj, dt Proj, Conv1D (one copy for all experts)

- **Critical path**: Implement shared routing: single router produces indices used for Conv, Gate, and Output projections; Ensure SSM (A, B, C, dt) and Conv1D are not expertized; Verify gating at Output Proj uses router weights R_i(X_t) to scale expert outputs; Confirm load balance is not enforced by auxiliary loss.

- **Design tradeoffs**: Expert count vs. Top-K: Paper uses 8 experts, Top-1 for efficiency; Expertize all vs. selective: Original Mamba benefits from selective expertization; Dense last layers: For hybrid RoM + FFN-MoE, keeping last layers dense stabilizes training.

- **Failure signatures**: Independent routing per projection causes PPL degradation; Applying MoE to x_proj, dt_proj, Conv1D slightly hurts performance; Adding load balance loss provides no benefit, slight harm.

- **First 3 experiments**: 1) Baseline sanity check: Replicate Figure 2 on Samba 421M—compare independent MoE vs. RoM; 2) Scaling curve: Train RoM 115M, 353M, 765M, 1.3B on SlimPajama 20B tokens; 3) Ablation on shared components: Compare RoM (Conv, Gate, Out) vs. RoM (Conv, Gate, dt, x, Out).

## Open Questions the Paper Calls Out

- **Broader applicability to attention and SSM variants**: Can the shared routing strategy in RoM be effectively adapted for self-attention and linear attention mechanisms? This remains uncertain as the study focused primarily on Mamba layers and hybrid SSM-Attention models.

- **Optimal configuration design**: What determines the optimal expertization configuration (selective vs. comprehensive) for future SSM architectures? The paper observes different optimal strategies for existing architectures but lacks a unified theoretical framework to predict the best configuration for new SSM variants.

- **Scaling behavior at larger models**: Does the efficiency advantage of RoM (matching dense models with 2.3× active parameters) persist at scales significantly larger than 1.3B active parameters? The paper validates scaling up to 1.3B active parameters, but scaling behaviors at massive scales (7B+ active parameters) remain unverified.

## Limitations

- The effectiveness of shared routing across projections relies on the assumption that Conv, Gate, and Output projections are functionally coupled, which may not hold for future SSM variants
- The ablation studies primarily compare RoM to naive MoE integration rather than to other sparse scaling approaches like LoRA or parameter-efficient fine-tuning methods
- The relative advantage over alternative sparse scaling methods remains unclear due to lack of head-to-head comparisons

## Confidence

**High Confidence:**
- RoM achieves consistent perplexity improvements over naive MoE integration across all tested scales
- The selective scaling approach provides marginal benefits or neutral performance impact
- Gating at the output projection effectively preserves SSM recurrence while enabling sparse expert contributions

**Medium Confidence:**
- RoM matches dense Mamba performance with approximately 2.3× fewer active parameters at 1.3B scale
- The shared routing strategy is superior to independent routing per projection across all tested configurations
- Hybrid RoM + FFN-MoE configurations maintain performance with 23% FLOPS savings

**Low Confidence:**
- Generalization to future SSM architectures beyond Mamba-1, Mamba-2, and Gated DeltaNet
- Performance advantages over alternative sparse scaling approaches like LoRA
- Long-term stability and generalization across diverse downstream tasks beyond evaluated benchmarks

## Next Checks

1. **Architecture Transfer Validation**: Implement RoM on a novel SSM architecture (e.g., RWKV or Hyena) to test whether shared routing across projections remains effective when the functional coupling between Conv, Gate, and Output projections differs from Mamba's architecture.

2. **Alternative Sparse Scaling Comparison**: Conduct a head-to-head comparison between RoM and LoRA-based sparse scaling on the same Mamba architectures, measuring both performance and computational efficiency across multiple scales to determine the relative advantages of each approach.

3. **Component Importance Analysis**: Perform a systematic ablation study varying the parameter counts of dt Proj, x Proj, and Conv1D to identify the threshold where sharing these components across experts begins to bottleneck performance, providing guidance for applying RoM to future SSM variants with different projection parameter distributions.