---
ver: rpa2
title: Discourse Graph Guided Document Translation with Large Language Models
arxiv_id: '2511.07230'
source_url: https://arxiv.org/abs/2511.07230
tags:
- translation
- chunk
- type
- document
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces TransGraph, a discourse-guided document\
  \ translation framework that improves translation quality and terminology consistency\
  \ by constructing structured discourse graphs to capture inter-chunk relationships\
  \ and selectively conditioning each translation segment on relevant graph neighborhoods.\
  \ Unlike memory-intensive agentic systems or generic sequential context, TransGraph\
  \ uses explicit discourse relation labels (e.g., Entity-Coreference, Core\u2192\
  Detail, Motivation\u2192Method) to provide compact, structured context during translation."
---

# Discourse Graph Guided Document Translation with Large Language Models

## Quick Facts
- arXiv ID: 2511.07230
- Source URL: https://arxiv.org/abs/2511.07230
- Reference count: 31
- Key outcome: TransGraph improves document-level translation quality and terminology consistency by using structured discourse graphs to selectively condition each translation segment on relevant graph neighborhoods, outperforming sequential context and agentic baselines while reducing token overhead.

## Executive Summary
TransGraph introduces a novel discourse-guided document translation framework that constructs structured discourse graphs to capture inter-chunk relationships and selectively conditions each translation segment on relevant graph neighborhoods. Unlike memory-intensive agentic systems or generic sequential context, TransGraph uses explicit discourse relation labels (e.g., Entity-Coreference, Core→Detail, Motivation→Method) to provide compact, structured context during translation. Evaluated on three document-level MT benchmarks spanning six languages, TransGraph consistently outperforms sentence-level, single-pass, and agentic baselines in document-level BLEU, COMET, and terminology accuracy while incurring significantly lower token overhead. Analysis shows that discourse relations are crucial for maintaining consistency, and graph-guided retrieval outperforms fixed or sequential context baselines.

## Method Summary
TransGraph is a two-stage document-level MT framework. Stage 1 uses iterative LLM-based chunking with 100-token windows, preserving sentence boundaries, followed by discourse graph construction where LLM labels 10 relation types between chunk pairs within a sliding window. Stage 2 translates each chunk conditioned on up to 5 in-neighbor chunks from the graph, assembling context packages with relation metadata. The approach uses explicit relation labels (Entity-Coreference, Terminology Definition, Core→Detail, etc.) rather than sequential context, enabling selective retrieval of discourse-critical dependencies while reducing token overhead. Translation prompts instruct the LLM to preserve terminology and discourse structure based on these labeled relations.

## Key Results
- TransGraph achieves consistent improvements over baselines (sentence-level, single-pass, and agentic) in document-level BLEU, COMET, and terminology accuracy across three benchmarks
- TransGraph reduces token overhead significantly (59k tokens average vs 104k for TransAgent and 171k for DeLTA)
- Explicit discourse relation labels are crucial for terminology consistency, with ablation showing 10.67 percentage point drop when relations are removed
- Graph-based retrieval captures 65% of meaningful discourse relations beyond the 5-nearest-neighbor window that sequential context misses

## Why This Works (Mechanism)

### Mechanism 1
Selective graph-neighborhood conditioning improves translation quality while reducing token overhead compared to sequential or exhaustive context approaches. TransGraph constructs a directed discourse graph G=(V,E,ℓ) where edges carry semantic relation labels. During translation, chunk j retrieves only its in-neighborhood N⁻(j) capped at k neighbors, rather than all preceding text. This filters irrelevant context while preserving discourse-critical dependencies. The assumption is that discourse relations essential for translation quality are sparse and often non-adjacent; sequential context contains substantial noise. Evidence shows TransGraph averages ≈59k tokens vs. 104k (TransAgent) and 171k (DeLTA) per document.

### Mechanism 2
Explicit discourse relation labels in context packages improve terminology consistency and coreference resolution. Context packages C_j include tuples (i, c_i, ℓ(c_i,c_j)) where ℓ encodes relation types like Entity-Coreference, Terminology Definition, Core→Detail. The translation prompt instructs the LLM to preserve terminology and discourse structure based on these labels. The assumption is that LLMs can exploit structured relation metadata when explicitly instructed; relation-label pairs carry more signal than raw text alone. Evidence shows removing relation labels drops terminology accuracy from 61.72→51.05 on BWB with Qwen3-32B.

### Mechanism 3
Graph-based retrieval captures non-adjacent discourse dependencies that sequential context windows miss. The discourse graph allows retrieval of chunks beyond the immediate k-window. A chunk at position j can receive context from chunk i where |j-i|>>5 if an edge exists. The assumption is that approximately 65% of meaningful discourse relations occur beyond the 5-nearest-neighbor window. Evidence shows "Most of the documents have low ratios around 35%, which means that if we only consider 5-nearest chunks...we are missing out 65% of the relations."

## Foundational Learning

- Concept: Rhetorical Structure Theory (RST)
  - Why needed here: The 10 discourse relation types (Background→Core, Entity-Coreference, etc.) derive from RST; understanding this taxonomy is essential for debugging relation labeling failures.
  - Quick check question: Can you explain why "Motivation→Method" differs from "Problem→Solution" in the paper's schema?

- Concept: Graph in-neighborhood retrieval
  - Why needed here: TransGraph processes chunks in document order but retrieves context from incoming edges only; this directional choice reflects discourse causality (antecedents→anaphors).
  - Quick check question: Why does the paper use in-neighborhood N⁻(j) rather than full neighborhood for context retrieval?

- Concept: Document-level MT evaluation metrics (d-BLEU, d-COMET, Terminology Accuracy)
  - Why needed here: Standard sentence-level BLEU misses discourse phenomena; the paper uses specialized metrics plus LLM-as-Judge cohesion evaluation.
  - Quick check question: What discourse phenomena does d-COMET capture that sentence-level BLEU cannot?

## Architecture Onboarding

- Component map: Document → Chunking Module → V (chunk set) → Graph Constructor → G (discourse graph) → Context Packager → C_j → Translation Engine → ŷ_j → Concatenate ŷ_1...ŷ_N → target document

- Critical path: 1) Document → Chunking Module → V (chunk set) 2) V → Graph Constructor → G (discourse graph) 3) For each j: G → Context Packager → C_j → Translation Engine → ŷ_j 4) Concatenate ŷ_1...ŷ_N → target document

- Design tradeoffs: Window size w: Larger w captures more relations but increases Stage 1 LLM calls; paper uses implicit small window (P constraint). Neighbor cap |N⁻(j)|=5: Limits context length but may truncate relevant relations; ties broken by preferring adjacent chunks. LLM backbone choice: Larger models (Qwen3-32B) achieve higher graph accuracy (94.54%) vs. smaller (Qwen3-8B: 87.27%).

- Failure signatures: Low terminology accuracy despite graph usage: Check relation labeling accuracy; if <80%, noisy labels may mislead translation. High token overhead: Check if |N⁻(j)| cap is exceeded or if chunking produces excessive small chunks. Inconsistent translations across chunks: Verify Context Packager correctly passes relation labels; prompt may need adjustment.

- First 3 experiments: 1) Baseline replication: Run SENT. MT, 1-PASS DOCMT, and TRANSGRAPH on a single document from BWB; compare d-BLEU and terminology accuracy to validate implementation. 2) Ablation on relation types: Remove specific relation types (e.g., Entity-Coreference only) and measure impact on terminology accuracy to identify which relations drive gains. 3) Graph consistency check: Construct discourse graphs from source document and reference translation; measure overlap (using paper's consistency metric) to assess cross-lingual graph stability before full evaluation.

## Open Questions the Paper Calls Out

- Question: How can retrieval-augmented compression or learned relation selection be integrated with TransGraph to improve robustness and efficiency on very long documents with dense relation structures?
- Basis in paper: Limitations section states: "Very long documents with dense relation structure increase retrieval and prompt assembly costs. Future works could combine retrieval-augmented compression and learned relation selection to further improve robustness and efficiency."
- Why unresolved: Current experiments focus on documents averaging 39–143 sentences; scalability to substantially longer documents (e.g., books, legal corpora) with many inter-chunk relations remains untested.
- What evidence would resolve it: Experiments on longer documents (>5000 tokens) comparing baseline TransGraph against variants with compression modules or learned selectors, reporting both translation quality and token cost curves.

- Question: To what extent do residual mislabels in LLM-generated discourse relations harm translation quality, and can error-correcting mechanisms mitigate this?
- Basis in paper: Limitations section notes: "discourse graphs rely on LLM relation labeling. Although our analysis shows high accuracy and reasonable cross–lingual consistency, residual mislabels may inject noisy context."
- Why unresolved: The paper demonstrates high labeling accuracy (up to 94.54% for Qwen3-32B) but does not quantify the downstream translation impact of specific error types or test correction strategies.
- What evidence would resolve it: Controlled experiments injecting synthetic relation mislabels at varying rates and types, measuring BLEU/COMET/terminology degradation, plus comparison of correction mechanisms (e.g., confidence filtering, self-consistency checks).

- Question: What are the optimal hyperparameters (chunk size T, neighborhood size cap, relation window w) across document genres and language pairs?
- Basis in paper: The authors state: "we empirically set T in Stage 1 (Chunking) to be 100 tokens, while the number of in-neighbour chunks N−(j) to retrieve in Stage 2 is set to 5 chunks." No systematic ablation or genre-specific tuning is reported.
- Why unresolved: These values were chosen empirically without sensitivity analysis; optimal settings may differ for technical vs. literary vs. conversational documents, or for morphologically rich vs. analytic languages.
- What evidence would resolve it: Grid-search or Bayesian optimization over T, neighborhood size, and window w across multiple datasets (technical, literary, conversational) and language pairs, reporting quality-cost trade-offs.

- Question: How does the choice and granularity of discourse relation types affect translation quality and terminology consistency?
- Basis in paper: The authors define 10 relation types based on rhetorical structure theory but do not ablate this schema or compare against coarser/finer alternatives. The "No relation" category is used but the cost of false negatives is not analyzed.
- Why unresolved: It is unclear whether all 10 types are equally beneficial, or whether a simplified schema (e.g., 4–5 core relations) could achieve comparable results with lower labeling cost.
- What evidence would resolve it: Ablation experiments comparing full 10-type schema against reduced schemas, measuring translation quality, terminology accuracy, and relation labeling cost/latency.

## Limitations

- Window size ambiguity: The paper specifies relation identification within "small window P" where j-i≤w but never states the actual value, making exact reproduction challenging.
- Evaluation metric consistency: Document-level MT evaluation remains an open challenge with different toolkits producing varying scores, and the paper doesn't specify implementations.
- Cross-lingual graph stability: The paper primarily evaluates English→Chinese direction, and discourse relation quality may vary across language pairs with different discourse structures.

## Confidence

- High confidence: Selective graph-neighborhood conditioning reduces token overhead (59k vs 104k-171k for baselines) while improving translation quality.
- Medium confidence: Explicit discourse relation labels improve terminology consistency, though this depends on accurate relation labeling which varies by LLM size.
- Low confidence: Graph-based retrieval captures non-adjacent dependencies that sequential context misses, as the paper doesn't provide evidence that TransGraph actually retrieves these distant relations in practice.

## Next Checks

1. Reproduce window size sensitivity: Run TransGraph with w=3, w=5, and w=7 on a single document from ACL 60/60. Measure relation identification accuracy, token overhead, and terminology consistency to identify optimal w and verify the paper's claims about sparsity.

2. Ablation on relation type importance: Systematically remove each of the 10 relation types from the context package (one at a time) and measure impact on terminology accuracy and d-BLEU. This will identify which relations drive the performance gains and whether the relation taxonomy is optimal.

3. Cross-lingual graph consistency: Construct discourse graphs from both source documents and reference translations for 10 documents in ACL 60/60. Measure graph overlap using the paper's consistency metric to validate that discourse relations transfer across translation and that TransGraph's cross-lingual performance is justified.