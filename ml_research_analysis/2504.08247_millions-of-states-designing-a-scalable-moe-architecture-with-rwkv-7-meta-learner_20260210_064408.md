---
ver: rpa2
title: 'Millions of States: Designing a Scalable MoE Architecture with RWKV-7 Meta-learner'
arxiv_id: '2504.08247'
source_url: https://arxiv.org/abs/2504.08247
tags:
- state
- meta-state
- rwkv-7
- interactions
- state-based
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Meta-State, a state-based extension to the
  RWKV-7 architecture that replaces the Feed-Forward Network with a novel mechanism
  for token-parameter interactions. The Self-State Encoder module repurposes a portion
  of the Weighted Key-Value state as transformation weights, enabling efficient state-autoregressive
  evolution without introducing new trainable matrices or softmax operations.
---

# Millions of States: Designing a Scalable MoE Architecture with RWKV-7 Meta-learner

## Quick Facts
- arXiv ID: 2504.08247
- Source URL: https://arxiv.org/abs/2504.08247
- Authors: Liu Xiao; Li Zhiyuan; Lin Yueyu
- Reference count: 6
- Primary result: 7.2%–13.1% relative reductions in cross-entropy loss across models from 150M to 1.5B parameters

## Executive Summary
This paper introduces Meta-State, a novel extension to the RWKV-7 architecture that replaces the Feed-Forward Network with a state-based mechanism for token-parameter interactions. The approach repurposes the Weighted Key-Value state as transformation weights through a Self-State Encoder, enabling efficient state-autoregressive evolution without introducing new trainable matrices or softmax operations. Experiments on the Pile dataset demonstrate consistent improvements over Transformer baselines, with performance gains increasing as model size grows.

## Method Summary
Meta-State integrates with RWKV-7 by replacing the FFN with a Self-State Encoder that extracts transformation weights from the existing WKV state. The Meta-State layer computes zt = ReLU(x′t · Wsse) where Wsse is partitioned from the WKV state, then updates via state-autoregressive evolution using decay and gating parameters from WKV. The output projection uses LayerNorm(zt · mstT) · Woms. Scaling is achieved by expanding the WKV state dimension while reusing existing parameters through projection matrices and output projection extension.

## Key Results
- 7.2% to 13.1% relative reductions in cross-entropy loss across all model sizes (150M to 1.5B parameters)
- Performance gains increase with model size, demonstrating scalability
- Linear complexity and constant memory usage maintained throughout

## Why This Works (Mechanism)

### Mechanism 1: Self-State Encoder (SSE) Enables Parameter-Free Token-State Interaction
The SSE repurposes the existing WKV state as transformation weights, eliminating new trainable matrices. It partitions wkvt to extract Wsse and computes zt = ReLU(x′t · Wsse), treating the evolving state itself as a dynamic weight matrix that modulates input tokens based on accumulated context.

### Mechanism 2: State-Autoregressive Meta-State Evolution Preserves Causal Processing
Meta-State updates via mst = mst−1 · [diag(wt) − κ̂tT(at ⊙ κ̂t)] + ztT zt, reusing RWKV-7's decay (wt), removal key (κ̂t), and in-context learning rate (at) to control state persistence while the outer product ztT zt injects new interaction information.

### Mechanism 3: Progressive Scaling via State Expansion and Parameter Reuse
Scaling is achieved by expanding the WKV state dimension while reusing existing Meta-State parameters. When scaling from D/h to D′/h′, a projection matrix Win maps inputs to the expanded dimension, and Woms is extended by appending new columns [Woms, Wnew].

## Foundational Learning

- **Recurrent State-Based Sequence Models (RNN/Lin-RNN)**: Essential for understanding how states compress sequence history in RWKV-7 and Meta-State. Quick check: Can you explain how a recurrent state update differs from attention-based context aggregation, and why this yields O(1) memory at inference?

- **WKV (Weighted Key-Value) Mechanism**: Critical since SSE directly repurposes WKV state as transformation weights. Quick check: Describe how the WKV state evolves over time and what information it encodes at each step.

- **Autoregressive Modeling and Causal Masking**: Fundamental since the paper emphasizes preserving the "autoregressive property." Quick check: Why must state updates depend only on current and past tokens, and what goes wrong if future information leaks into the state?

## Architecture Onboarding

- **Component map**: Input Layer: Token embeddings → LN → TimeMix → LN → Meta-State layer → Output projection
- **Meta-State Layer**: SSE extracts Wsse from wkvt → ReLU(x′t · Wsse) → zt → Meta-State update → LayerNorm(zt · mstT) · Woms → oms
- **Critical path**: Understand RWKV-7 TimeMix block → Implement SSE extraction logic → Implement Meta-State update equations → Verify autoregressive property → Test output projection and gradient flow → For scaling: Implement Win projection and Woms extension
- **Design tradeoffs**: Full wkvt vs. submatrix for Wsse (max information vs. dimension mismatches); Freezing vs. fine-tuning original parameters after scaling (preservation vs. adaptation); Outer product update (efficiency vs. expressiveness)
- **Failure signatures**: Loss NaN (exploding gradients in Meta-State update); No improvement over baseline (degenerate Wsse extraction); Performance degrades after scaling (poor Win initialization); Autoregressive property violation (future information leakage)
- **First 3 experiments**: 1) SSE ablation: Replace SSE with learned projection matrix and compare loss. 2) State size scaling: Train small models with varying WKV state dimensions and plot loss vs. state size. 3) Progressive scaling validation: Train 150M model, expand to 450M, compare against 450M trained from scratch.

## Open Questions the Paper Calls Out
- Can the Meta-State framework be effectively integrated into State Space Models (SSMs) like S4?
- Does the proposed state expansion strategy actually enable progressive model growth without retraining?
- How does the Meta-State architecture perform on sequence-to-sequence generation tasks like translation?
- How does the Meta-State layer interact with standard Mixture of Experts (MoE) routing mechanisms?

## Limitations
- Missing specification of exact WKV state partitioning scheme for Wsse extraction
- Incomplete implementation details for RWKV-7 TimeMix equations being used
- Limited validation of progressive scaling mechanism with empirical scaling experiments
- No comparative analysis with dedicated MoE layers despite title mentioning MoE

## Confidence
- **High Confidence**: Empirical results showing consistent loss improvements across model scales and core mechanism of using WKV state as transformation weights
- **Medium Confidence**: Progressive scaling mechanism's effectiveness and claim of parameter reuse without retraining
- **Low Confidence**: Completeness of reproducibility given missing implementation details for WKV state partitioning and initialization schemes

## Next Checks
1. Implement SSE with different partitioning strategies and measure performance impact compared to learned projection matrix
2. Train a 150M Meta-State model, then scale to 450M using the proposed mechanism and compare against a 450M model trained from scratch
3. Analyze rank and spectral properties of WKV state matrix during training to verify it contains sufficient structure for transformation weights