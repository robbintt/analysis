---
ver: rpa2
title: 'RLSLM: A Hybrid Reinforcement Learning Framework Aligning Rule-Based Social
  Locomotion Model with Human Social Norms'
arxiv_id: '2511.11323'
source_url: https://arxiv.org/abs/2511.11323
tags:
- social
- scene
- agent
- navigation
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RLSLM introduces a hybrid reinforcement learning framework that
  integrates a rule-based social locomotion model into reward functions, enabling
  agents to navigate human-populated environments with minimal training. The framework
  combines psychological insights with RL to optimize mechanical energy, goal progress,
  and social comfort.
---

# RLSLM: A Hybrid Reinforcement Learning Framework Aligning Rule-Based Social Locomotion Model with Human Social Norms

## Quick Facts
- arXiv ID: 2511.11323
- Source URL: https://arxiv.org/abs/2511.11323
- Reference count: 40
- Primary result: Hybrid RL framework combining rule-based social model with reward shaping achieves mean comfort rating of 4.21/5 in VR human-agent interaction experiments

## Executive Summary
RLSLM introduces a hybrid reinforcement learning framework that integrates a rule-based social locomotion model into reward functions, enabling agents to navigate human-populated environments with minimal training. The framework combines psychological insights with RL to optimize mechanical energy, goal progress, and social comfort. Human-agent interaction experiments in immersive VR demonstrate that RLSLM achieves a mean comfort rating of 4.21/5, significantly outperforming rule-based baselines (Δrating = 1.12, Bonferroni-corrected p < 0.001). Ablation and sensitivity analyses confirm improved interpretability over data-driven methods, establishing RLSLM as a scalable, human-centered approach for real-world social navigation.

## Method Summary
RLSLM employs an Actor-Critic reinforcement learning framework where a rule-based Social Locomotion Model (SLM) provides interpretable social discomfort signals embedded in the reward function. The agent learns to navigate using three reward components: mechanical energy penalty (−α per step), goal progress reward based on distance reduction, and social discomfort calculated via asymmetric orientation-sensitive fields derived from psychological experiments. The framework uses fixed SLM parameters (m_a=0.321, n_a=0.856, c=1.430) to compute discomfort fields that account for human facing direction, creating stronger penalties for passing in front of facing individuals. Training converges within 10,000 steps using OpenAI Gym environments with 3D virtual humans.

## Key Results
- RLSLM achieves mean comfort rating of 4.21/5 in immersive VR human-agent interaction experiments
- Outperforms rule-based baselines by Δrating = 1.12 with Bonferroni-corrected p < 0.001
- Ablation studies show removing orientation-sensitive components increases front-passing behavior from 5 to 57.76% of cases
- Sensitivity analysis confirms σ=0.5 weight appropriately balances social comfort against efficiency

## Why This Works (Mechanism)

### Mechanism 1: Psychology-Grounded Reward Shaping
Embedding a rule-based social locomotion model into the RL reward function reduces sample complexity while maintaining behavioral interpretability. The SLM pre-encodes human behavioral patterns from psychological experiments into a continuous comfort field, providing dense, informative reward signals at each timestep without requiring large-scale trajectory datasets.

### Mechanism 2: Orientation-Sensitive Asymmetric Field
Incorporating human facing direction into social discomfort computation enables nuanced avoidance behavior beyond symmetric distance-only models. The social influence field includes a heading-relevant component where discomfort scales with cosine of angle when the agent is in the human's forward field of view, creating stronger penalties for passing in front of facing humans versus behind them.

### Mechanism 3: Multi-Objective Emergent Behavior
Jointly optimizing mechanical energy, goal progress, and social discomfort creates emergent path selection that balances efficiency with social compliance. The reward function combines three signals forcing the agent to find paths that reach goals efficiently while maintaining comfortable distances, with weight σ=0.5 balancing social discomfort against energy/progress.

## Foundational Learning

- **Concept: Proxemics and Personal Space**
  - Why needed here: The framework builds on quantifying how humans perceive spatial intrusion, making comfort decrease with proximity and depend on orientation essential for understanding the social influence model
  - Quick check question: Can you explain why passing in front of a facing person causes more discomfort than passing behind them at the same distance?

- **Concept: Actor-Critic Reinforcement Learning (A2C)**
  - Why needed here: RLSLM uses A2C for policy learning, requiring understanding of actor-critic interactions for debugging training convergence and reward scaling issues
  - Quick check question: What happens to learning if the critic consistently overestimates the value function?

- **Concept: Reward Shaping and Multi-Objective RL**
  - Why needed here: The framework combines three reward components with different scales (α=1, σ=0.5, C=500), making reward normalization and tradeoffs critical for tuning or extending the framework
  - Quick check question: If you doubled the social weight σ from 0.5 to 1.0, what qualitative change would you expect in the agent's paths?

## Architecture Onboarding

- **Component map**: Environment (OpenAI Gymnasium) -> observation vector s_t -> Actor Network (MLP: 64-128-256-128-64) -> action distribution π(a_t|s_t) -> Action Execution -> Environment Transition -> Reward Computation (Mechanical Energy, Goal Progress, Social Influence, Terminal) -> Critic Network (MLP: 64-128-256-128-64) -> value estimate V(s_t) -> A2C Update (via Stable-Baselines3)

- **Critical path**: The Social Influence computation (Equations 3-7) is the most complex and failure-prone component, requiring correct calculation of distances, angles (θ_h, θ), and field composition. A bug here produces plausible-looking but socially wrong behavior.

- **Design tradeoffs**:
  - Interpretability vs. flexibility: Using fixed SLM parameters ensures interpretability but limits adaptation to new cultures/contexts
  - Training speed vs. optimality: A2C with 10K steps converges quickly but may not find globally optimal policies; PPO or SAC could improve at cost of complexity
  - Discrete vs. continuous action: Fixed step length (45cm) with direction selection simplifies learning but produces jagged paths requiring post-processing

- **Failure signatures**:
  - Agent oscillates between two points: Likely reward scaling issue; check if social penalties dominate progress rewards
  - Agent passes directly through people: Social influence not being computed; verify HRSC/HISC/CAC components
  - Agent takes overly long detours: σ set too high; sensitivity analysis shows σ=2.0 produces conservative behavior
  - Training doesn't converge: Check observation normalization; relative positions may have inconsistent scales

- **First 3 experiments**:
  1. Single-human validation: Place one human directly between start and goal, facing the agent's approach path. Verify the agent passes behind rather than in front. Expected: with HRSC enabled, passes behind >90% of time
  2. Social weight sweep: Run scenarios with σ ∈ {0, 0.5, 1.0, 2.0} and measure maximum lateral distance (MLD). Confirm monotonic relationship between σ and MLD per Figure 5(d-e)
  3. Multi-human group avoidance: Create a face-to-face conversation pair blocking the direct path. Verify agent routes around rather than between them. Compare against ablated model without HISC to confirm group-space sensitivity

## Open Questions the Paper Calls Out

### Open Question 1
How does RLSLM perform in environments with dynamically moving agents rather than static individuals? The experimental setup explicitly states that the agent bypasses "one or three static persons" in all evaluation scenarios. Real-world social navigation requires reacting to temporal movement dynamics and predicting future human positions. Evaluation in simulation or VR environments where virtual humans move along independent trajectories would resolve this.

### Open Question 2
Does the comfort alignment observed in immersive VR transfer effectively to physical human-robot interaction? The paper claims applicability for "real-world social navigation" but validates results exclusively using a "VR-based user evaluation pipeline." Visual perception in VR lacks the physical risk and sensory fidelity (e.g., sound, air displacement) of real-world interactions, which may alter user comfort thresholds. Deployment of the RLSLM policy on a physical robot in a real-world user study comparing comfort ratings against the VR baseline would resolve this.

### Open Question 3
Can the fixed social influence parameters generalize across different cultural contexts or social settings? The framework utilizes fixed parameters (m, n, c) fitted to specific behavioral experiments (Zhou et al. 2022) to define the social comfort field. Social norms regarding proxemics and comfort vary significantly by culture and context (e.g., library vs. busy street), which a fixed parameter set may not capture. A cross-cultural user study or an adaptive learning mechanism that adjusts parameters based on observed pedestrian behaviors in diverse environments would resolve this.

## Limitations
- SLM parameters derived from prior psychological experiments may not generalize to all cultural contexts or mobility-impaired populations
- Fixed 45cm step length produces non-smooth trajectories requiring post-processing for real-world deployment
- Current validation in virtual reality environments cannot fully capture physical constraints and uncertainties of real-world navigation

## Confidence

**High Confidence**: The core mechanism of combining rule-based social models with RL reward shaping is well-supported by empirical results (Δrating = 1.12, p < 0.001). Ablation studies convincingly demonstrate necessity of each component, particularly orientation-sensitive field.

**Medium Confidence**: Generalizability of SLM parameters across different cultural contexts and user groups remains uncertain. While framework is interpretable, fixed parameters limit adaptability without retraining or parameter adjustment.

**Low Confidence**: Real-world deployment readiness is questionable. Framework has not been validated with actual physical robots in uncontrolled environments, and step-by-step navigation may prove inadequate for dynamic, high-density scenarios.

## Next Checks
1. Cross-cultural validation: Test RLSLM with participants from diverse cultural backgrounds to verify that fixed SLM parameters (m_a=0.321, n_a=0.856, c=1.430) generalize beyond original psychological study population

2. Physical robot deployment: Implement RLSLM on a mobile robot platform in a real-world environment with dynamic obstacles to evaluate performance under sensor noise, uneven terrain, and non-deterministic human behavior

3. Long-term interaction study: Conduct extended user studies (multiple sessions over weeks) to assess whether RLSLM's behavior remains comfortable and predictable as users become familiar with the agent's navigation patterns