---
ver: rpa2
title: Overview of the TREC 2022 deep learning track
arxiv_id: '2507.10865'
source_url: https://arxiv.org/abs/2507.10865
tags:
- uni00000013
- uni00000015
- uni00000014
- uni00000016
- uni00000019
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The TREC 2022 Deep Learning Track continued benchmarking neural
  ranking methods on the MS MARCO passage and document collections, focusing on constructing
  a reusable test collection. This year's key change was using test queries that never
  contributed to the MS MARCO corpus, along with only judging passages and inferring
  document labels, and deduplicating passages to improve reusability and reduce saturation.
---

# Overview of the TREC 2022 deep learning track

## Quick Facts
- arXiv ID: 2507.10865
- Source URL: https://arxiv.org/abs/2507.10865
- Authors: Nick Craswell; Bhaskar Mitra; Emine Yilmaz; Daniel Campos; Jimmy Lin; Ellen M. Voorhees; Ian Soboroff
- Reference count: 9
- Primary result: Neural methods with large-scale pretraining improved NDCG@10 by 125% over traditional methods for passage ranking and 76% for document ranking in 2022

## Executive Summary
The TREC 2022 Deep Learning Track evaluated neural ranking methods on the MS MARCO v2 corpus, focusing on creating a reusable test collection. The track introduced test queries disconnected from the corpus construction process and concentrated judging efforts on passages while inferring document labels. A key innovation was deduplicating passages to reduce judgment costs and prevent saturation. The results showed neural methods continued to outperform traditional approaches, with the performance gap widening significantly compared to previous years. Notably, single-stage dense retrieval was less competitive than multi-stage approaches combining sparse and dense methods.

## Method Summary
The track used the MS MARCO v2 corpus (138M passages, 12M documents) with 500 test queries, 76 of which were officially judged. The evaluation focused on passage ranking as the primary task, with document relevance inferred from passage judgments. A novel query sampling method selected queries disconnected from the corpus construction to increase difficulty. Continuous Active Learning (CAL) was used to judge passages, with a stopping condition of relevance density below 0.4. Near-duplicate passages were detected and clustered, with judgments propagated across clusters to improve reusability and reduce saturation.

## Key Results
- Neural methods with large-scale pretraining outperformed traditional methods, with the best neural run improving NDCG@10 by 125% over traditional for passage ranking and 76% for document ranking
- Single-stage dense retrieval was significantly less competitive than multi-stage approaches, with the best single-stage dense run 23% worse than the best passage ranking run on NDCG@10
- Deduplication effectively reduced redundant judgments while maintaining evaluation stability, with minimal impact on system ranking scores
- The 2022 test collection successfully addressed saturation issues from 2021, with median Precision@10 dropping from 1.0 to 0.45

## Why This Works (Mechanism)

### Mechanism 1: Semantic Generalization via Pretraining
Pretrained neural language models outperform traditional lexical baselines by a wider margin when query-corpus overlap is reduced. The removal of "easy" queries (those with direct corpus overlap) increases reliance on semantic inference rather than lexical coincidence, where neural models' contextual embeddings bridge vocabulary gaps that exact term matching fails to capture.

### Mechanism 2: Test Collection Reusability via Judging Concentration
Concentrating judgment resources solely on passage-level relevance and inferring document labels improves the discriminative power of the test collection. By allocating the full budget to passage judging rather than splitting it, judges can reach a stopping condition (low relevant density) on more topics, preventing saturation where all top-ranked documents are relevant and making it impossible to distinguish system quality.

### Mechanism 3: Multi-Stage Retrieval Superiority
Multi-stage retrieval pipelines (e.g., first-stage retrieval followed by neural reranking or hybrid fusion) currently outperform single-stage dense retrieval on large-scale, refreshed corpora. Single-stage dense retrieval appears to struggle with the scale or specific characteristics of the MS MARCO v2 corpus, while systems combining multiple signals or stages recover relevant documents that single-stage dense encoders miss.

## Foundational Learning

**Concept: Sparse vs. Dense Retrieval**
Why needed here: To understand why single-stage dense retrieval underperformed and why the best runs used multi-stage approaches (often sparse first-stage).
Quick check question: Does the system retrieve based on exact keyword matches (Sparse/BM25) or vector similarity (Dense/BERT)?

**Concept: Relevance Density & Saturation**
Why needed here: To understand the motivation behind the 2022 track design changes (deduping, query changes) intended to fix the "too many relevants" problem from 2021.
Quick check question: If a test set has a "relevant density" of 0.8 (80% of judged items are relevant), is it useful for detecting small improvements in ranking algorithms?

**Concept: MS MARCO v2 vs. v1**
Why needed here: The track uses a significantly larger corpus (138M passages vs. 8.8M), which changes the difficulty and requires scalable architectures.
Quick check question: Why might a model trained on MS MARCO v1 (smaller corpus) struggle when applied directly to v2?

## Architecture Onboarding

**Component map:**
MS MARCO v2 Corpus (138M passages, 12M docs) + Queries (new sampling method) -> Deduplication (cluster near-duplicate passages) -> Retrieval (Full-ranking or Top-100 Reranking) -> Judging (Depth-10 pooling + CAL with stopping condition) -> Output: Qrels (Relevance Judgments) expanded from deduped clusters

**Critical path:** Establishing the "Relevance Density" curve. If the curve stays above 0.4, the topic is rejected or judged incomplete. The pipeline relies on the CAL loop efficiently finding non-relevant documents to lower density and prove completeness.

**Design tradeoffs:**
- **Query Sampling:** Using queries disconnected from corpus construction increases realism and difficulty but removes the ability to use sparse MS MARCO qrels for evaluation.
- **Label Propagation:** Propagating passage labels to documents saves money but may conflate "passage relevance" with "document relevance" if the document contains mostly irrelevant content.

**Failure signatures:**
- **Saturation:** Queries where median Precision@10 is 1.0 (seen in 2021, reduced in 2022)
- **High Density:** Failure to find < 40% relevant density after budget expenditure (Topic rejected)
- **Single-stage Drop-off:** Significant NDCG loss (23%) if relying solely on single-stage dense retrieval

**First 3 experiments:**
1. **Baseline Verification:** Run the provided Pyserini BM25 baseline on the new query set to establish a trad floor.
2. **Architecture Comparison:** Compare a single-stage dense retriever (e.g., ColBERT/ANCE) against a hybrid approach (BM25 + Dense Rerank) to validate the multi-stage advantage observed in the paper.
3. **Deduping Impact:** Evaluate a run using the "deduped runs" (removing dupes before eval) vs. "expanded qrels" to understand how near-duplicates inflate metric scores (especially P@10).

## Open Questions the Paper Calls Out

**Open Question 1**
Why does training and evaluating models using sparse MS MARCO relevance labels correlate well with results obtained from comprehensive NIST judgments? The paper identifies the correlation and offers a hypothesis regarding the quality of Bing rankings used for label generation, but does not empirically validate the underlying mechanism.

**Open Question 2**
What are the primary factors causing single-stage dense retrieval runs to be significantly less competitive in TREC 2022 compared to previous years? The authors observe the performance drop but can only hypothesize causes, such as the change in query sampling or progress in other retrieval methods, without confirming a specific cause.

**Open Question 3**
To what extent is the widening performance gap between neural ('nnlm') and traditional ('trad') methods attributable to the new, more difficult query sampling method? The experimental setup (new queries) changed simultaneously with model submissions, making it impossible to distinguish if the results reflect a shift in data distribution or genuine algorithmic progress.

## Limitations
- The exact process for selecting queries disconnected from the MS MARCO corpus construction is not fully detailed, making it difficult to assess the magnitude of "difficulty" increase.
- Document-level labels are inferred from passage judgments, but the paper doesn't quantify potential errors from this approach, where a document containing one relevant passage but mostly irrelevant content would be labeled relevant.
- The paper observes a 23% NDCG@10 drop for single-stage dense retrieval but doesn't analyze why (e.g., corpus size effects, deduplication artifacts, or model-specific limitations).

## Confidence

**High:** The observation that multi-stage neural methods outperformed single-stage dense retrieval and traditional methods. This is directly supported by the official TREC 2022 results.

**Medium:** The hypothesis that the increased performance gap between neural and traditional methods is due to more difficult queries. While plausible given the corpus changes, the paper doesn't directly test this claim.

**Medium:** The effectiveness of deduplication in improving test collection reusability. The paper shows reduced judgment costs and stable metrics, but doesn't quantify the long-term reusability gains.

## Next Checks

1. **Query Difficulty Validation:** Manually categorize a sample of the new query set by difficulty (e.g., lexical overlap with corpus, expected answer complexity) to verify the "harder query" hypothesis.

2. **Label Propagation Error Analysis:** Compare inferred document labels against a small set of manually judged documents to quantify propagation accuracy and its impact on document ranking evaluation.

3. **Single-Stage Dense Retrieval Diagnosis:** Analyze the top single-stage dense retrieval runs to identify failure patterns (e.g., specific query types, corpus regions) that explain the performance drop.