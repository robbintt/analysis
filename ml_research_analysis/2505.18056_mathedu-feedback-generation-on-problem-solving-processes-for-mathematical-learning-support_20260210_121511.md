---
ver: rpa2
title: 'MathEDU: Feedback Generation on Problem-Solving Processes for Mathematical
  Learning Support'
arxiv_id: '2505.18056'
source_url: https://arxiv.org/abs/2505.18056
tags:
- student
- feedback
- students
- error
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the reliability of Large Language Models
  (LLMs) in grading authentic student problem-solving processes and generating feedback
  in mathematics education. The authors introduce MathEDU, a dataset consisting of
  real student problem-solving processes and corresponding teacher-written feedback.
---

# MathEDU: Feedback Generation on Problem-Solving Processes for Mathematical Learning Support

## Quick Facts
- arXiv ID: 2505.18056
- Source URL: https://arxiv.org/abs/2505.18056
- Authors: Wei-Ling Hsu; Yu-Chien Tang; An-Zi Yen
- Reference count: 40
- Key outcome: Fine-tuning improves correctness classification and error identification, but LLM-generated feedback remains verbose and often misses underlying misconceptions

## Executive Summary
This study evaluates the reliability of Large Language Models in grading authentic student problem-solving processes and generating feedback for mathematics education. The authors introduce MathEDU, a dataset of real student solutions and teacher-written feedback, to systematically assess model performance across three tasks: answer correctness classification, error identification, and feedback generation. While fine-tuning strategies effectively enhance model performance in classifying correctness and locating errors, the generated feedback consistently falls short of human-written feedback, tending to be verbose and failing to address core misconceptions. The findings highlight the need for more pedagogy-aware AI feedback systems in educational contexts.

## Method Summary
The study introduces MathEDU, a dataset containing 200 authentic student problem-solving processes from elementary and junior high school mathematics, paired with corresponding teacher-written feedback. Three tasks are evaluated: answer correctness classification (binary classification of whether final answers are correct), error identification (locating erroneous steps in the solution process), and feedback generation (producing pedagogically appropriate feedback). Various models including GPT-3.5, GPT-4, and fine-tuned LLaMA-2 are compared using automated metrics such as accuracy, BLEU, and ROUGE scores. The evaluation systematically measures model performance across these tasks, with particular attention to the quality and pedagogical relevance of generated feedback.

## Key Results
- Fine-tuning strategies significantly improve performance in answer correctness classification and error identification tasks
- Generated feedback tends to be verbose and often fails to address underlying student misconceptions
- Automated evaluation metrics show clear gaps between LLM-generated and teacher-written feedback

## Why This Works (Mechanism)
The study demonstrates that while LLMs can be effectively fine-tuned for specific classification tasks in mathematical problem-solving assessment, the generation of pedagogically appropriate feedback remains challenging. The mechanism involves using teacher-written feedback as ground truth to train models, but the complexity of translating pedagogical insights into automated feedback systems reveals limitations in current approaches. The models can identify correct/incorrect answers and locate errors with reasonable accuracy after fine-tuning, but struggle to generate concise, targeted feedback that addresses the conceptual misunderstandings underlying student errors.

## Foundational Learning
- **Fine-tuning**: Why needed - to adapt pre-trained models to specific educational tasks; Quick check - measure performance improvement on classification tasks before and after fine-tuning
- **Automated evaluation metrics**: Why needed - to quantitatively assess model output quality; Quick check - compare BLEU/ROUGE scores against human-written feedback
- **Error identification**: Why needed - to pinpoint specific steps where students make mistakes; Quick check - evaluate accuracy of step-by-step error localization
- **Feedback generation**: Why needed - to provide automated, pedagogically appropriate responses to student work; Quick check - assess whether generated feedback addresses the root cause of errors
- **Pedagogical feedback**: Why needed - to support student learning through targeted guidance; Quick check - evaluate feedback quality against educational best practices
- **Mathematical problem-solving assessment**: Why needed - to evaluate student understanding beyond final answers; Quick check - measure model ability to analyze solution processes

## Architecture Onboarding

**Component Map**: Student solution data -> Error detection model -> Feedback generation model -> Evaluation metrics

**Critical Path**: Input problem-solving process → Error identification → Feedback generation → Quality assessment

**Design Tradeoffs**: Fine-tuning improves classification accuracy but may reduce generalization; verbose feedback generation prioritizes coverage over precision

**Failure Signatures**: Generated feedback misses underlying misconceptions, provides overly general suggestions, or focuses on surface-level errors rather than conceptual misunderstandings

**3 First Experiments**:
1. Compare fine-tuned vs. non-fine-tuned models on error identification accuracy
2. Evaluate feedback quality using both automated metrics and expert pedagogical review
3. Test model performance across different mathematical domains within the dataset

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Dataset limited to elementary and junior high school mathematics topics
- Reliance on teacher-written feedback assumes optimal human feedback without considering pedagogical variations
- Automated evaluation metrics may not correlate with actual pedagogical effectiveness
- Sample size of 200 problems may be insufficient for comprehensive conclusions
- Does not address potential biases in feedback data or generation

## Confidence

**High Confidence**:
- Fine-tuning improves performance in classification and error identification
- Generated feedback tends to be verbose and miss underlying misconceptions

**Medium Confidence**:
- LLM-generated feedback falls short of teacher-written feedback in quality
- Need for pedagogy-aware AI systems is reasonable but not fully validated

**Low Confidence**:
- Broader implications about LLM limitations in educational feedback extend beyond current evidence

## Next Checks
1. Conduct controlled study measuring student learning outcomes with LLM-generated vs. teacher feedback
2. Replicate evaluation framework on more diverse mathematics curriculum including higher-level topics
3. Perform systematic analysis of potential biases in teacher feedback data and LLM-generated feedback