---
ver: rpa2
title: 'Liaozhai through the Looking-Glass: On Paratextual Explicitation of Culture-Bound
  Terms in Machine Translation'
arxiv_id: '2509.23395'
source_url: https://arxiv.org/abs/2509.23395
tags:
- translation
- paratexts
- explicitation
- terms
- term
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper formalizes Genette's (1987) paratext theory for machine
  translation, introducing the task of paratextual explicitation for culture-bound
  terms. The authors construct a dataset of 560 expert-aligned paratexts from four
  English translations of the classical Chinese short story collection Liaozhai and
  evaluate LLMs on identifying and generating paratexts for culture-bound terms.
---

# Liaozhai through the Looking-Glass: On Paratextual Explicitation of Culture-Bound Terms in Machine Translation

## Quick Facts
- arXiv ID: 2509.23395
- Source URL: https://arxiv.org/abs/2509.23395
- Reference count: 18
- This paper introduces paratextual explicitation for culture-bound terms in machine translation, constructing a dataset of 560 expert-aligned paratexts from four English translations of Liaozhai.

## Executive Summary
This paper formalizes Genette's paratext theory for machine translation, introducing the task of paratextual explicitation for culture-bound terms. The authors construct a dataset of 560 expert-aligned paratexts from four English translations of the classical Chinese short story collection Liaozhai and evaluate LLMs on identifying and generating paratexts for culture-bound terms. Using zero-shot prompting and agentic retrieval methods, experiments show that while LLM-generated paratexts improve audience comprehension, they remain less effective than translator-authored ones. Statistical analysis reveals that professional translators vary widely in their use of paratexts, suggesting cultural mediation is inherently open-ended.

## Method Summary
The authors construct a two-step pipeline: (1) identify culture-bound terms in classical Chinese text requiring explanation using theory-grounded prompts, and (2) generate footnote/endnote-style paratexts for those terms. They use zero-shot prompting with QWEN3-8B, testing three prompt variants (Default, Theoretical, Practical) in both non-thinking and thinking modes. An agentic retrieval component extends the best-performing setting by querying bilingual web search (Baidu + Google) to supply cultural context. The dataset contains 560 deduplicated expert-aligned paratexts from four English translations of Liaozhai stories. Evaluation uses automatic metrics (BLEU, ROUGE-L, BERTScore, LLM-as-a-Judge) and human preference testing.

## Key Results
- Theoretical prompt achieves 23.73% F1 for culture-bound term identification vs 16.34% for default prompt
- Agentic retrieval improves paratext generation: BLEU from 1.62 to 2.14, ROUGE-L from 16.82 to 20.59, BERTScore from 85.56 to 86.08
- Human evaluation confirms paratexts improve translation quality but translator versions are still preferred (~60-70% of cases)
- Inter-translator agreement is very low (Krippendorff's α = -0.349), indicating paratextual explicitation is inherently subjective

## Why This Works (Mechanism)

### Mechanism 1: Theory-Grounded Prompting Aligns Model Behavior with Translation Criteria
Prompts informed by translation theory improve culture-bound term identification by providing explicit criteria for recognizing culturally-marked expressions. This constrains the hypothesis space, reducing false positives while maintaining recall. The "thinking" mode further filters candidates through intermediate reasoning.

### Mechanism 2: Bilingual Agentic Retrieval Supplies Long-Tail Cultural Knowledge
Web-based retrieval over both source and target language resources improves paratext quality by accessing source-language documentation where cultural references are richer. Retrieved passages are appended as context for generation.

### Mechanism 3: Task-Specific Prompting Strategy (Identification vs. Explicitation)
Optimal prompting differs by subtask—identification benefits from theoretical framing; explicitation benefits from practical/audience-oriented framing. Term identification requires recognizing cultural salience (classification), while paratext generation requires communicative clarity (generation).

## Foundational Learning

- **Concept: Genette's Paratext Theory**
  - Why needed here: The paper formalizes paratexts (footnotes, endnotes, glossaries) as computational objects for MT, distinguishing them from in-text explicitation.
  - Quick check question: Can you name three types of paratexts and explain why they differ from in-text translation strategies?

- **Concept: Culture-Bound Terms vs. Idioms**
  - Why needed here: Culture-bound terms resist translation because no target-language equivalent exists (e.g., 江湖), unlike idioms with cross-lingual analogues (e.g., 锦上添花 ≈ gild the lily).
  - Quick check question: Why might 江湖 require paratextual explicitation while 锦上添花 might not?

- **Concept: Polysystem Theory & Skopos Theory**
  - Why needed here: These theories inform prompt design—polysystem for identifying culturally-salient terms; skopos for audience-oriented explanation.
  - Quick check question: How would a skopos-based approach differ from a polysystem-based approach when translating for expert vs. general audiences?

## Architecture Onboarding

- **Component map:** Classical Chinese text -> Term identification module (Theoretical prompt + thinking mode) -> Explicitation module (Practical prompt + optional agentic retrieval) -> Translation with linked paratextual apparatus

- **Critical path:** Identification accuracy caps end-to-end performance (missed terms = zero-score paratexts); retrieval latency is ~10x baseline; human evaluation reveals automatic metrics underdetect hallucination

- **Design tradeoffs:** Zero-shot vs. few-shot (zero-shot chosen to reduce bias); thinking vs. non-thinking (thinking improves precision but increases latency/cost); pooled references vs. per-translator (pooling captures all valid explicitations but inflates false negatives)

- **Failure signatures:** Low inter-annotator agreement among translators (Krippendorff's α = -0.349) indicates ground truth is inherently subjective; high false positive rate in non-thinking mode (2351 FP vs. 161 TP for Default); hallucinated historical claims in Table 5

- **First 3 experiments:**
  1. Replicate identification experiment with Theoretical prompt in thinking mode on 10-story subset; measure precision/recall tradeoff vs. Default
  2. Add agentic retrieval to Practical explicitation prompt; compare BLEU and LLM-as-Judge scores against non-retrieval baseline on 20 terms with ground-truth paratexts
  3. Conduct small-scale human evaluation (3 evaluators, 5 stories) comparing LLM-generated vs. translator paratexts; confirm preference gap reported in paper

## Open Questions the Paper Calls Out

### Open Question 1
How can non-expert readers reliably determine whether LLM-generated paratextual explicitations are factually accurate versus hallucinated? Current evaluation metrics assess semantic similarity but cannot verify factual accuracy or cultural authenticity.

### Open Question 2
What evaluation frameworks can adequately capture the context-sensitive, interpretive quality of paratextual explicitations beyond surface-level semantic overlap? BLEU, ROUGE-L, BERTScore, and LLM-as-Judge fail to assess factual accuracy, contextual relevance, reader utility, and stylistic appropriateness simultaneously.

### Open Question 3
To what extent do the patterns of translator disagreement and model difficulty in paratextual explicitation generalize to other language pairs, domains, and cultural contexts? Only one source language, one target language, and one literary genre have been tested.

### Open Question 4
Can paratextual explicitations be effectively personalized to readers' prior knowledge levels and cultural familiarity without requiring human-authored references? Current models generate single explicitations without reader modeling.

## Limitations
- Low inter-annotator agreement among professional translators (Krippendorff's α = -0.349) indicates paratextual explicitation is inherently subjective
- Identification task shows fundamental precision-recall tradeoff that no prompt variant fully resolves (best F1 = 23.73%)
- Agentic retrieval improves quality but introduces significant latency (10x baseline) and potential hallucination risks
- Automatic metrics systematically fail to detect hallucinated content, suggesting current evaluation is incomplete

## Confidence
- **High confidence:** Theoretical prompt superiority for term identification (23.73% F1 vs 16.34% baseline), practical prompt superiority for paratext generation (BLEU 1.57 vs 1.40), human preference for translator paratexts (60-70% preference rate)
- **Medium confidence:** Agentic retrieval benefits (BLEU improvement from 1.62 to 2.14, ROUGE-L from 16.82 to 20.59) due to implementation complexity
- **Low confidence:** End-to-end pipeline performance (BLEU 0.01-0.02) due to compounding errors and inherent task difficulty

## Next Checks
1. Replicate the term identification experiment on a 10-story subset using the Theoretical prompt in thinking mode, measuring the precision-recall tradeoff against the Default prompt baseline to verify the 23.73% F1 result
2. Implement and test the agentic retrieval extension to the Practical explicitation prompt on 20 terms with ground-truth paratexts, comparing automatic metrics (BLEU, ROUGE-L, BERTScore, LLM-as-Judge) against the non-retrieval baseline while monitoring for hallucinated content
3. Conduct a small-scale human evaluation (3 evaluators, 5 stories) comparing LLM-generated paratexts against translator-authored versions to confirm the reported 60-70% preference gap for human translations and assess whether automatic metrics align with human judgment