---
ver: rpa2
title: 'Hide and Seek in Noise Labels: Noise-Robust Collaborative Active Learning
  with LLM-Powered Assistance'
arxiv_id: '2504.02901'
source_url: https://arxiv.org/abs/2504.02901
tags:
- noisy
- noise
- clean
- samples
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of learning from noisy labels
  in text classification by proposing a collaborative framework that combines small
  models (SMs) and large language models (LLMs) with active learning. The method uses
  two SMs to form a co-prediction network and applies a dynamic-enhanced threshold
  strategy to partition noisy data into clean, hard, and purified subsets.
---

# Hide and Seek in Noise Labels: Noise-Robust Collaborative Active Learning with LLM-Powered Assistance

## Quick Facts
- **arXiv ID**: 2504.02901
- **Source URL**: https://arxiv.org/abs/2504.02901
- **Reference count**: 40
- **Primary result**: NoiseAL framework achieves up to 96.4% accuracy under 40% asymmetric noise, outperforming state-of-the-art baselines.

## Executive Summary
This paper addresses the challenge of learning from noisy labels in text classification by proposing a collaborative framework that combines small models (SMs) and large language models (LLMs) with active learning. The method uses two SMs to form a co-prediction network and applies a dynamic-enhanced threshold strategy to partition noisy data into clean, hard, and purified subsets. LLMs are then actively queried to correct labels in the purified set, while different optimization objectives are employed for each subset. Experiments on synthetic and real-world datasets demonstrate that the proposed NoiseAL framework significantly outperforms state-of-the-art baselines, achieving up to 96.4% accuracy under 40% asymmetric noise, and reduces the noise ratio in purified sets from 17.89% to as low as 14.54%. The method shows superior robustness and generalization compared to traditional fixed-threshold approaches.

## Method Summary
The NoiseAL framework employs a co-prediction network consisting of a strong model (BERT) and a weak model (BiLSTM) to identify noisy samples through divergent confidence scores. A dynamic-enhanced threshold strategy partitions data into reliable (R), hard (H), and purified (P) sets. The reliable set R is used to construct demonstrations for in-context learning (ICL) queries to an LLM (GPT-3.5-Turbo), which corrects labels in the purified set P. The framework uses noise-robust reversed cross-entropy loss on the LLM-corrected set P, standard cross-entropy on R, and mixed EmbMix regularization on H. Training occurs over 6 epochs with a 2-epoch warmup, and the best model is selected based on clean validation performance.

## Key Results
- NoiseAL achieves 96.4% accuracy under 40% asymmetric noise, outperforming state-of-the-art baselines.
- The framework reduces noise ratio in purified sets from 17.89% to as low as 14.54% after LLM correction.
- Performance is robust across synthetic noise types (symmetric, asymmetric, instance-dependent) and real-world noisy datasets.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: A co-prediction network (strong + weak model) combined with a dynamic-enhanced threshold improves noisy/clean sample separation.
- **Mechanism**: The weak model (BiLSTM) overfits noisy labels faster than the strong model (BERT). Their divergent confidences expose samples with high "memorization strength" but low clean probability as likely noisy. The dynamic threshold τ(t) = λp(t) + (1−λ)τ(t−1) adapts to each sample's evolving confidence, avoiding the pitfalls of fixed loss thresholds.
- **Core assumption**: Weak models memorize noise earlier; the two models' confidences diverge meaningfully on noisy samples.
- **Evidence anchors**: [abstract]: "we first adopt two SMs to form a co-prediction network and propose a dynamic-enhanced threshold strategy"; [section 4.1-4.2]: "BiLSTM starts to memorize the noisy samples... BERT still maintains high confidence for these clean samples."

### Mechanism 2
- **Claim**: Actively querying an LLM with in-context learning (ICL) corrects noisy labels in the purified set effectively.
- **Mechanism**: Samples identified as "purified" (high confidence but low clean probability) are sent to the LLM. Clean samples from the reliable set are retrieved via feature similarity to construct demonstrations, enabling ICL. The LLM generates corrected labels, which are treated as new supervision.
- **Core assumption**: Samples in the reliable set R are clean enough to provide accurate ICL demonstrations.
- **Evidence anchors**: [abstract]: "LLMs are then actively queried to correct labels in the purified set"; [section 4.3]: "Demonstration Construction... sample top-K nearest neighbors to form demonstration examples."

### Mechanism 3
- **Claim**: Using noise-robust loss (reversed cross-entropy) on the LLM-corrected purified set enables learning despite residual noise.
- **Mechanism**: The LLM does not produce perfect labels. Reversed cross-entropy satisfies the property that the sum over all classes is constant, making its minimizer invariant to label noise below a certain rate. This allows the model to learn from the purified set even with some incorrect corrections.
- **Core assumption**: The noise rate in the LLM-corrected purified set remains below the theoretical tolerance threshold (e.g., <50% for asymmetric noise).
- **Evidence anchors**: [section 4.4]: "we resort to a family of noise-robust loss functions... [reversed cross-entropy] is theoretically noise-tolerant."; [appendix E-F]: Provides theoretical proof for the noise-tolerant property.

## Foundational Learning

- **Concept: Memory Effect in Neural Networks**
  - Why needed here: The entire separation strategy hinges on the empirical observation that models first learn simple/clean patterns before memorizing noise.
  - Quick check question: Can you describe how the loss distributions of clean vs. noisy samples evolve during training, as shown in Figure 2?

- **Concept: In-Context Learning (ICL) for Large Language Models**
  - Why needed here: This is the technique used to make the LLM's zero-shot corrections more accurate by providing it with similar, high-confidence examples.
  - Quick check question: How does the quality of the demonstration examples (from set R) affect the LLM's prediction on a noisy sample?

- **Concept: Co-Teaching for Confirmation Bias Mitigation**
  - Why needed here: Using two models is directly inspired by co-teaching, where models help each other avoid overfitting to their own mistakes.
  - Quick check question: Why is using a single model's own loss to filter its own training data problematic in the context of noisy labels?

## Architecture Onboarding

- **Component map**: Noisy Dataset → Co-prediction Network (BERT + BiLSTM) → Dynamic Selection Module → {Reliable (R), Hard (H), Purified (P)} Sets → Set R (clean) → Feature-Aware Sampling → LLM Prompt (Demonstrations + Query from Set P) → LLM → Corrected Labels for Set P → Training with Combined Losses (LR on R, LP (robust) on P, LH (mixed) on H)

- **Critical path**: The accuracy of the entire system is most sensitive to the initial partitioning (R, P, H). Errors here propagate: bad R harms LLM ICL demonstrations; misclassified P wastes LLM queries or misses real errors.

- **Design tradeoffs**:
  - **LLM Cost vs. Accuracy**: Querying all of P is expensive. The paper samples, but a budget constraint is not explicitly modeled.
  - **Model Diversity vs. Complexity**: A BERT+BiLSTM pair offers high diversity but requires training/maintaining two models. Using two BERTs with different dropouts is simpler but may be less effective.

- **Failure signatures**:
  - **Confirmation bias**: The co-prediction network starts agreeing on wrong labels. Check if the loss distributions of the two models converge too quickly.
  - **ICL failure**: LLM corrections on P are inconsistent or wrong. Check if the retrieval from R is returning noisy samples.
  - **Robust loss failure**: Performance on P degrades. Check the estimated noise rate in P after LLM correction to ensure it's within tolerance.

- **First 3 experiments**:
  1. **Baseline Separation Validation**: On a dataset with known synthetic noise (e.g., 40% asymmetric Trec), visualize the loss and confidence distributions for clean/noisy samples for a single BERT model vs. the co-prediction network. Quantify separation quality (e.g., ROC-AUC for identifying noisy samples).
  2. **LLM Query Cost-Benefit Analysis**: Run the full pipeline with a fixed LLM query budget (e.g., query only 50% of samples in P). Compare final accuracy to the full-query setup to establish the cost-accuracy frontier.
  3. **Ablation on Robust Loss**: Train the final model using standard cross-entropy loss on the LLM-corrected set P instead of reversed cross-entropy. Measure performance degradation, especially on high-noise datasets, to empirically verify the robust loss's contribution.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can the NoiseAL framework be extended to multi-label or hierarchical text classification tasks where label dependencies exist?
- **Basis in paper**: [explicit] The authors explicitly state in the "Limitations" section that generalizing the method to multi-label or hierarchical problems is a "promising direction" but requires handling dependence relations and hierarchical structures, which the current single-label co-prediction network does not support.
- **Why unresolved**: The current methodology relies on partitioning data based on single-label confidence thresholds and cross-entropy losses, which do not inherently model correlations between multiple labels or tree-structured hierarchies.
- **What evidence would resolve it**: A modified version of NoiseAL applied to standard multi-label (e.g., RCV1-v2) or hierarchical datasets, utilizing a multi-label loss function and correlation-aware sampling, demonstrating comparable robustness to the single-label results.

### Open Question 2
- **Question**: Does incorporating graph-based label encoders (e.g., GCN, GAT) into the co-prediction network improve the capture of label dependencies?
- **Basis in paper**: [explicit] The "Limitations" section suggests introducing "powerful label encoders, for example, by applying GCN, GAT, or Graphormer" to capture label features and dependence relations.
- **Why unresolved**: The current implementation uses standard BERT and LSTM encoders that treat labels as independent output classes without modeling their structural relationships.
- **What evidence would resolve it**: An ablation study replacing the current SM architectures with graph-augmented networks, showing improved accuracy on datasets with high label correlation or structured ontologies.

### Open Question 3
- **Question**: To what extent can the framework maintain performance and cost-efficiency when replacing the proprietary GPT-3.5 annotator with smaller, open-source Large Language Models?
- **Basis in paper**: [inferred] While Section 5.6 discusses cost, the method relies entirely on the specific capability of GPT-3.5 to correct labels. The paper does not validate if smaller, cheaper, or open-source models (e.g., Llama, Mistral) can perform the "active annotator" role effectively.
- **Why unresolved**: The denoising capability of the "Purified Set" relies on the LLM's specific zero-shot and in-context learning proficiency; substituting the model might change the noise reduction ratio (17.89% $\to$ 14.54%) and impact the final classification accuracy.
- **What evidence would resolve it**: Benchmarking the NoiseAL framework using various open-source LLMs as the annotator, comparing the resulting noise reduction rates and final accuracy against the GPT-3.5 baseline.

## Limitations
- **LLM Query Dependency**: The method's performance is highly dependent on LLM availability and cost, with no comprehensive budget-constrained analysis provided.
- **Robustness Threshold Sensitivity**: The framework relies on reversed cross-entropy's theoretical noise tolerance, but the exact threshold and noise rate estimation are not fully explored.
- **Demonstration Quality Assumption**: The ICL mechanism assumes the reliable set R is clean enough for accurate demonstrations, which is not rigorously validated.

## Confidence
- **High Confidence**: The co-prediction network with dynamic threshold for noisy/clean separation is well-supported by empirical evidence and aligns with established memory effect literature.
- **Medium Confidence**: The LLM-based ICL correction strategy is innovative and shows strong results, but its effectiveness depends on demonstration quality and LLM robustness.
- **Medium Confidence**: The use of reversed cross-entropy as a noise-robust loss is theoretically sound and validated, but practical performance depends on accurate noise rate estimation.

## Next Checks
1. **Budget-Constrained Performance Analysis**: Run the full pipeline with varying LLM query budgets (e.g., 25%, 50%, 75% of P) to establish the cost-accuracy frontier and identify the minimum viable budget for acceptable performance.
2. **Demonstration Quality Validation**: Systematically corrupt a subset of the reliable set R and measure the impact on LLM correction accuracy to validate the assumption that R must be clean for effective ICL.
3. **Robust Loss Threshold Testing**: Intentionally introduce higher noise rates into the LLM-corrected set P (e.g., 30%, 40%) and measure the performance degradation of reversed cross-entropy vs. standard cross-entropy to empirically verify the theoretical tolerance threshold.