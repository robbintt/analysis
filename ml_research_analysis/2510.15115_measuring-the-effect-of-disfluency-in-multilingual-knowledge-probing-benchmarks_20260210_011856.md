---
ver: rpa2
title: Measuring the Effect of Disfluency in Multilingual Knowledge Probing Benchmarks
arxiv_id: '2510.15115'
source_url: https://arxiv.org/abs/2510.15115
tags:
- language
- languages
- object
- translation
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines how template-based prompts in multilingual
  benchmarks can lead to ungrammatical sentences that degrade LLM factual retrieval
  performance. The authors compare MLAMA's templated prompts with sentence-level translations
  from Google Translate and ChatGPT across four Slavic and five diverse non-Slavic
  languages.
---

# Measuring the Effect of Disfluency in Multilingual Knowledge Probing Benchmarks
## Quick Facts
- arXiv ID: 2510.15115
- Source URL: https://arxiv.org/abs/2510.15115
- Reference count: 35
- Primary result: Templated prompts degrade multilingual LLM factual retrieval; fluent translations improve R@1 by up to 10%

## Executive Summary
This study investigates how template-based prompts in multilingual knowledge probing benchmarks introduce disfluencies that impair large language models' factual retrieval capabilities. The authors compare MLAMA's templated prompts with sentence-level translations from Google Translate and ChatGPT across nine languages spanning Slavic and non-Slavic families. They demonstrate significant performance improvements when using grammatically fluent translations, particularly from Google Translate, suggesting that current template-based datasets may systematically underestimate LLM capabilities in non-English languages.

## Method Summary
The authors created a dataset with 8,688 factual questions across nine languages (four Slavic and five diverse non-Slavic). They compared three prompting strategies: original MLAMA templates, Google Translate translations, and ChatGPT translations. Performance was evaluated using retrieval metrics (R@1, R@5, R@10) across multiple LLM families. The study controlled for entity recognition by using identical entity sets across all prompt variants and conducted ablation analyses to isolate the effects of grammaticality, lexical accuracy, and explicitation.

## Key Results
- Grammatical fluent prompts improved retrieval performance by up to 10% in R@1 compared to templated prompts
- Google Translate consistently outperformed ChatGPT translations, likely due to added contextual clarifications
- Performance gains were most pronounced in morphologically rich languages with complex agreement systems
- Template disfluencies specifically impacted agreement errors and entity inflection accuracy

## Why This Works (Mechanism)
Templated prompts create ungrammatical sentences that force LLMs to rely more heavily on surface-level pattern matching rather than deep linguistic understanding. When prompts are grammatically fluent, models can better leverage their internal representations of syntactic and semantic relationships. The addition of contextual clarifications in translations further aids disambiguation, particularly for languages with rich morphology where entity inflection and agreement are critical for meaning.

## Foundational Learning
- Multilingual knowledge probing: Testing factual retrieval across languages requires careful prompt design to avoid confounding grammatical errors with knowledge gaps
- Template-based vs. sentence-level translation: Different approaches to cross-lingual benchmarking have distinct trade-offs in grammaticality and semantic preservation
- Morphologically rich languages: Agreement systems and inflection patterns significantly impact model performance on factual queries
- Retrieval metrics (R@1, R@5, R@10): Standard evaluation metrics for measuring factual knowledge retrieval accuracy
- Prompt engineering for SVO vs. SOV languages: Word order differences require different prompting strategies to effectively test parametric knowledge

## Architecture Onboarding
- Component map: Template generation -> Translation pipeline -> Prompt injection -> LLM inference -> Retrieval evaluation
- Critical path: Fluent translation generation -> Prompt injection -> Factual retrieval -> Performance measurement
- Design tradeoffs: Grammaticality vs. semantic preservation vs. prompt length; template simplicity vs. translation quality
- Failure signatures: Agreement errors, entity inflection mistakes, and missing relation context in templated prompts
- First experiments: 1) Compare R@1 scores across prompt types, 2) Analyze agreement error frequency, 3) Measure effect of prompt length on retrieval

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How can grammaticality be operationalized into a reliable, scalable metric for multilingual benchmarks that correlates with factual retrieval performance?
- Basis in paper: Section 5.4 states that log-probabilities, surface metrics (chrF), and Grammatical Error Correction (GEC) systems failed to consistently approximate grammaticality.
- Why unresolved: Existing metrics either focus on surface-level overlap or fail to detect agreement errors in morphologically rich languages.
- What evidence would resolve it: The development of an automated evaluation metric that correlates strongly with R@1 improvements across diverse languages without requiring human annotation.

### Open Question 2
- Question: How can factual knowledge probing be effectively adapted for verb-final (SOV/OSV) languages without confounding results with instruction-tuning capabilities?
- Basis in paper: Section 5.4 notes that the standard "object-last" prompting strategy fails for verb-final languages because it excludes the relation context.
- Why unresolved: Alternative approaches, such as masked filling, require instruction-following skills (SFT/RLHF), making it impossible to isolate parametric knowledge.
- What evidence would resolve it: A prompt schema for verb-final languages that retrieves facts at rates comparable to SVO languages without increasing reliance on alignment skills.

### Open Question 3
- Question: To what extent does "pragmatic explicitation" (adding disambiguating context) drive retrieval gains compared to strict grammatical fluency?
- Basis in paper: Section 5.1 notes that Google Translate outperformed ChatGPT, likely due to adding clarifications (e.g., entity definitions), but it is unclear if performance stems from grammar or these added semantic cues.
- Why unresolved: The study compares template filling against full translation but does not disentangle the effect of syntactic correction from the effect of added explanatory tokens.
- What evidence would resolve it: An ablation study isolating retrieval performance on translations that are strictly grammatical but lack explicitation versus those with added context.

## Limitations
- Limited language coverage (only nine languages tested) restricts generalizability across language families
- Performance improvements could stem from increased prompt length rather than grammaticality alone
- Uncertainty about whether fluent translations preserve the exact semantic intent of original templated prompts
- Focus on factual retrieval may not generalize to other types of language understanding tasks

## Confidence
- High confidence: The finding that grammatically fluent prompts yield better retrieval performance than templated prompts across multiple languages and model families
- Medium confidence: The attribution of improvements specifically to grammaticality rather than other factors like prompt length or lexical variation, given the lack of controlled ablation studies
- Low confidence: Claims about the magnitude of performance degradation caused by templated prompts in existing benchmarks, as this was not directly measured against gold-standard evaluations

## Next Checks
1. Test whether templated prompts of equal length to sentence translations produce similar performance, isolating grammaticality effects from length confounds
2. Expand evaluation to additional language families beyond Slavic and the five diverse languages tested
3. Compare retrieval accuracy against human judgments of prompt semantic equivalence to validate that translation improvements preserve intended meaning