---
ver: rpa2
title: Efficient Simple Regret Algorithms for Stochastic Contextual Bandits
arxiv_id: '2601.21167'
source_url: https://arxiv.org/abs/2601.21167
tags:
- lemma
- regret
- where
- line
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies stochastic contextual bandits under the simple\
  \ regret objective for both linear and logistic reward models. The authors develop\
  \ deterministic and randomized algorithms that achieve improved regret bounds compared\
  \ to existing methods, particularly in the logistic case where prior work suffered\
  \ from dependence on a large constant \u03BA = exp(S) related to the unknown parameter\
  \ norm."
---

# Efficient Simple Regret Algorithms for Stochastic Contextual Bandits

## Quick Facts
- arXiv ID: 2601.21167
- Source URL: https://arxiv.org/abs/2601.21167
- Reference count: 40
- One-line primary result: Algorithms achieving $\tilde{O}(d/\sqrt{T})$ simple regret for stochastic contextual bandits with linear and logistic reward models, without dependence on the large constant $\kappa$ in the leading term for logistic bandits.

## Executive Summary
This paper develops algorithms for stochastic contextual bandits under the simple regret objective, focusing on linear and logistic reward models. The key innovation is selecting actions that maximize uncertainty in predicted rewards, enabling faster elimination of suboptimal actions. For linear bandits, the proposed algorithms achieve a simple regret of $O(d/\sqrt{T \log(T/\delta)})$, while for logistic bandits, the approach yields the same bound without dependence on the large constant $\kappa = \exp(S)$ in the leading term. The paper also presents randomized variants based on Thompson sampling that achieve $O(d^{3/2}/\sqrt{T \log(T/\delta)})$ regret. Empirical results validate the theoretical guarantees, demonstrating superior performance compared to uniform exploration and cumulative regret methods.

## Method Summary
The paper proposes deterministic and randomized algorithms for stochastic contextual bandits under simple regret. For linear bandits, MULIN selects actions maximizing the norm $\|\phi(S_t,a)\|_{V_t^{-1}}$ of the feature vector with respect to the inverse design matrix, while SIMPLELINTS samples from a zero-mean Gaussian and selects actions maximizing alignment. For logistic bandits, MULOG and THATS extend this approach using self-concordance analysis and conservative convex optimization to update design matrices, with THATS using Thompson sampling for computational efficiency. The algorithms maintain confidence sets around parameter estimates and select actions to maximize uncertainty, enabling faster convergence to the optimal policy.

## Key Results
- Achieves $\tilde{O}(d/\sqrt{T})$ simple regret for linear contextual bandits, improving upon existing methods
- Extends to logistic bandits with $\tilde{O}(d/\sqrt{T})$ regret without $\kappa$-dependence in the leading term
- Randomized Thompson sampling variants achieve $\tilde{O}(d^{3/2}/\sqrt{T})$ regret with reduced computational cost
- Empirical validation shows superior performance compared to uniform exploration and cumulative regret approaches

## Why This Works (Mechanism)
The algorithms work by strategically exploring actions that maximize uncertainty in predicted rewards, measured by norms like $\|\phi(s,a)\|_{V_t^{-1}}$. This targeted exploration allows faster elimination of suboptimal actions compared to uniform exploration. The elliptical potential lemma bounds how quickly uncertainty can be reduced through strategic data collection. For logistic bandits, the self-concordance property enables conservative updates to the design matrix while maintaining theoretical guarantees. By focusing exploration on actions that provide the most information about distinguishing optimal from suboptimal policies, the algorithms achieve faster convergence to low simple regret.

## Foundational Learning

- **Concept: Stochastic Contextual Bandits**
  - Why needed here: This is the core problem formulation. You must understand that at each round $t$, a context $S_t$ arrives, you pick an action $A_t$, and receive a reward $X_t$. The goal is not to maximize cumulative reward, but to output a single policy $\hat{\pi}$ after $T$ rounds that minimizes simple regret $R(\hat{\pi})$â€”the difference in value between your policy and the optimal one.
  - Quick check question: What is the key difference between the objective of cumulative regret and simple regret?

- **Concept: Regularized Linear and Logistic Regression**
  - Why needed here: These are the supervised learning oracles embedded within the algorithms. MULIN and SIMPLELINTS use regularized least-squares. MULOG and THATS use regularized maximum likelihood estimation for logistic regression. The estimates $\hat{\theta}_t$ and confidence sets $C_t(\cdot), E_t(\cdot)$ derived from these regression problems are what drive the exploration.
  - Quick check question: For logistic regression, why is the regularized negative log-likelihood used, and what does its Hessian $H_t(\theta)$ represent?

- **Concept: Confidence Sets & Elliptical Potential**
  - Why needed here: The entire strategy is built on quantifying uncertainty. The algorithms construct confidence sets (ellipsoids) around their parameter estimates. The "size" of these sets, often measured by norms like $\|\phi(s,a)\|_{V_t^{-1}}$, is what the algorithm maximizes. The elliptical potential lemma is the core theoretical tool that bounds how quickly this uncertainty can be reduced by strategic data collection.
  - Quick check question: What does it mean for an action $\phi(s,a)$ to have a high norm $\|\cdot\|_{V_t^{-1}}$ in terms of the current model's uncertainty?

## Architecture Onboarding

- **Component map:**
  1. Data Store: Accumulates tuples $(S_t, A_t, X_t)$
  2. Regression Module: Computes parameter estimates ($\hat{\theta}_t$, $\bar{\theta}_t$) and design matrices ($V_t$, $L_t$) from the Data Store
  3. Confidence Set Builder: Uses Regression Module outputs to construct confidence ellipsoids ($W_t$, $E_t$, $V_t$)
  4. Action Selector: Selects actions maximizing uncertainty (different strategies for each algorithm variant)
  5. Policy Output: After $T$ rounds, returns greedy policy based on final parameter estimate

- **Critical path:**
  1. Receive context $S_t$
  2. Update design matrix $L_t$ (logistic only) via conservative convex optimization
  3. Run Action Selector using current design matrix and confidence set
  4. Receive reward $X_t$ and add to Data Store
  5. At $T$, compute final parameter estimate and output greedy policy

- **Design tradeoffs:**
  1. MULIN vs. SIMPLELINTS: MULIN achieves better regret bound $\tilde{O}(d/\sqrt{T})$ but requires maximization over actions; SIMPLELINTS has worse bound $\tilde{O}(d^{3/2}/\sqrt{T})$ but is computationally cheaper
  2. Deterministic vs. Randomized (Logistic): MULOG achieves $\tilde{O}(d/\sqrt{T})$ but requires joint maximization over actions and parameters; THATS simplifies action selection using sampled parameters, trading a factor of $\sqrt{d}$ in the regret bound for computational savings

- **Failure signatures:**
  1. $\kappa$-explosion: Large $S$ causes the constant $\kappa$ to become huge, with lower-order terms dominating for practical $T$
  2. Improper Confidence Sets: Incorrect implementation of confidence set constructions breaks the uncertainty reduction argument
  3. Action Set Tractability: Guarantees require finite action sets; continuous spaces make maximization problems intractable

- **First 3 experiments:**
  1. Linear Sanity Check (vs. Uniform): Replicate adversarial linear experiment with $\theta^* = [5, 0, \dots, 0]$, $K=128$, suboptimal arms orthogonal to optimal one
  2. Logistic Noise-Sensitivity (THATS vs. Simple MLE): Vary $M$ with inherently noisy optimal arms (reward mean $\approx 0.5$)
  3. $\kappa$ Scaling Test: Run MULOG and THATS on logistic problems with increasing $S$, measure rounds to reach target simple regret

## Open Questions the Paper Calls Out

- Can the computational cost of our randomized algorithms be further reduced, specifically by avoiding the need to construct confidence sets and solve linear optimization problems in every round?
- How can the proposed methods be adapted to reduce computational costs for large, but structured action sets?
- Can the simple regret analysis paradigm be extended to the general class of stochastic contextual generalized linear bandits?

## Limitations

- Dependence on lower-order terms involving large constant $\kappa = \exp(S)$ in logistic bandit setting
- Analysis assumes finite action sets, limiting applicability to continuous action spaces
- Computational overhead from solving conservative convex optimization problems to update design matrices
- Sensitivity to choice of regularization parameter $\lambda$

## Confidence

- **High:** Linear bandit algorithms (MULIN, SIMPLELINTS) and their regret bounds with clear proofs
- **Medium:** Logistic bandit algorithms (MULOG, THATS) extending analysis using self-concordance properties
- **Medium:** Empirical validation limited to synthetic experiments; real-world data would strengthen claims

## Next Checks

1. **$\kappa$-Scaling Experiment:** Systematically vary $S$ in logistic bandit experiments to quantify practical impact of lower-order terms and validate theoretical predictions about convergence rates

2. **Continuous Action Extension:** Adapt logistic algorithms to handle continuous action spaces by discretizing confidence set intersection, evaluating whether $\tilde{O}(d^{3/2}/\sqrt{T})$ bound can be achieved

3. **Confidence Set Robustness:** Test sensitivity to errors in confidence set construction by deliberately miscalibrating confidence radii $\beta_t(\delta)$ and measuring degradation in simple regret guarantees