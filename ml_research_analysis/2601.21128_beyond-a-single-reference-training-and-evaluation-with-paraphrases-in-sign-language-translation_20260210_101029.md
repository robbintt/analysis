---
ver: rpa2
title: 'Beyond a Single Reference: Training and Evaluation with Paraphrases in Sign
  Language Translation'
arxiv_id: '2601.21128'
source_url: https://arxiv.org/abs/2601.21128
tags:
- paraphrases
- language
- evaluation
- training
- sign
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the use of Large Language Models (LLMs)
  to generate paraphrased references for sign language translation (SLT) evaluation.
  It finds that naively using paraphrases in training does not improve translation
  performance and may even be detrimental.
---

# Beyond a Single Reference: Training and Evaluation with Paraphrases in Sign Language Translation

## Quick Facts
- arXiv ID: 2601.21128
- Source URL: https://arxiv.org/abs/2601.21128
- Reference count: 7
- One-line primary result: Paraphrases in evaluation improve metric-human correlation in sign language translation; naive use in training degrades performance.

## Executive Summary
This paper investigates whether LLM-generated paraphrases can improve sign language translation (SLT) evaluation and training. The authors find that while naively incorporating paraphrases during training degrades performance, using them during evaluation significantly improves automatic metric correlation with human judgments. They introduce BLEUpara, which computes BLEU against multiple paraphrased references and selects the maximum score, showing improved Spearman correlation (0.685 vs 0.657) and better handling of translation outliers. The study releases all generated paraphrases and evaluation code to support reproducible, more reliable SLT evaluation.

## Method Summary
The authors generate K=5 paraphrases per reference using GPT-4o-mini with temperature=0.7 and top-p=0.95, filtering outputs through ParaScore (BERTScore + Normalized Levenshtein Distance weighted by γ=0.35 and ω=0.5, threshold ≥0.7). They evaluate on YouTubeASL (~610K samples) and How2Sign (35K samples) using a pose-based T5 encoder-decoder with SignSpace normalization. BLEUpara computes BLEU against canonical plus paraphrased references, taking the maximum score. Training with paraphrases (random sampling or minimum-loss selection) showed degradation (BLEU-4 dropped from 7.46 to 6.17-6.49), while evaluation with paraphrases improved human correlation.

## Key Results
- BLEUpara improves Spearman correlation with human judgment from 0.657 to 0.685, particularly for extreme cases (0.578 to 0.659 for BLEU<5 or >15)
- Naive paraphrase-based training augmentation degrades performance, dropping BLEU-4 from 7.46 to 6.17-6.49
- GPT-4o-mini outperforms open models (Llama-3.2-3B) in paraphrase quality (ParaScore)
- Sequential prompting yields slightly higher quality paraphrases than iterative prompting
- Context injection during paraphrasing unexpectedly degrades paraphrase quality

## Why This Works (Mechanism)

### Mechanism 1
Multi-reference evaluation via paraphrases improves metric-human correlation, particularly for outlier translations. BLEUpara computes BLEU against multiple paraphrased references and selects the highest-scoring match, partially mitigating synonym penalties and word-order sensitivity inherent to n-gram metrics. Core assumption: LLM-generated paraphrases are semantically equivalent to canonical references. Evidence: Spearman correlation improves from 0.657 to 0.685; for extremes (BLEU<5 or >15), from 0.578 to 0.659. Break condition: If paraphrases introduce semantic drift, BLEUpara may reward incorrect translations.

### Mechanism 2
Paraphrase quality requires balancing semantic preservation against lexical divergence. ParaScore combines BERTScore (semantic similarity) with Normalized Levenshtein Distance (lexical diversity), weighted by hyperparameters γ=0.35 and ω=0.5. Paraphrases scoring below 0.7 threshold are filtered. Core assumption: BERTScore embeddings capture semantic equivalence across surface variations. Evidence: Formal ParaScore definition with 0.7 quality threshold established through manual evaluation at 0.25, 0.50, 0.75 percentiles. Break condition: If semantic drift occurs while BERTScore remains high, low-quality paraphrases may pass filtering.

### Mechanism 3
Naive paraphrase-based training augmentation degrades performance by introducing target ambiguity. When training with randomly sampled paraphrases or minimum-loss selection, the model receives multiple valid targets for identical inputs, potentially increasing optimization ambiguity without clear learning signal. Core assumption: The T5 pose-based model cannot effectively learn from multiple paraphrased targets in the current training setup. Evidence: Baseline achieves BLEU-4 of 7.46; Random sampling drops to 6.17; Min loss drops to 6.49. Break condition: More sophisticated training protocols (hard example mining, curriculum learning) may yield gains—untested.

## Foundational Learning

- **Non-isomorphism in sign-spoken language mapping**: Understanding that one sign sequence can map to multiple valid written translations justifies multi-reference evaluation. Quick check: Can you explain why BLEU penalizes valid alternative wordings?

- **ParaScore components (BERTScore + Levenshtein)**: Interpreting paraphrase quality scores requires understanding the semantic-similarity vs lexical-diversity tradeoff. Quick check: What happens to ParaScore if a paraphrase is semantically correct but lexically identical to the source?

- **Multi-reference BLEU extension**: BLEUpara differs from standard BLEU by computing against K paraphrased references and taking the maximum. Quick check: How does BLEUpara handle a translation that matches a paraphrase but not the canonical reference?

## Architecture Onboarding

- **Component map**: Extract keypoints (MMPose/MediaPipe) -> Normalize poses (SignSpace) -> Train T5 (pose sequences → canonical text) -> Generate paraphrases (GPT-4o-mini) -> Evaluate using BLEUpara (max BLEU across references)

- **Critical path**: 1. Extract keypoints from video 2. Normalize poses via SignSpace 3. Train T5 on pose sequences → canonical text (do NOT use paraphrases for training) 4. Generate paraphrases for test references 5. Evaluate using BLEUpara

- **Design tradeoffs**: GPT-4o-mini: highest ParaScore but proprietary/costly; open models (Llama-3.2-3B) show lower quality. Sequential vs iterative prompting: Sequential is cheaper and slightly higher quality. Adding context to paraphrasing: Actually degraded ParaScore—counterintuitive, requires further investigation.

- **Failure signatures**: Training with paraphrases shows 1–1.3 BLEU drop. Context-augmented paraphrasing shifts ParaScore distribution left (lower quality). Iterative prompting shows no quality gain over sequential.

- **First 3 experiments**: 1. Reproduce BLEUpara correlation improvement: Run human evaluation on 70 sentences (6 annotators, 0–5 scale), compare BLEU vs BLEUpara correlations on extremes subset. 2. Validate paraphrase quality threshold: Manually inspect paraphrases at ParaScore 0.6, 0.7, 0.8 to confirm 0.7 threshold generalizes to your domain. 3. Test alternative training protocols: Instead of random sampling, try curriculum-based paraphrase introduction or hard-example mining.

## Open Questions the Paper Calls Out

- **Cross-lingual generalizability**: Can BLEUpara metric generalize effectively to sign languages other than American Sign Language? The study restricted validation to ASL datasets, leaving efficacy across languages with different grammatical structures unproven. Evidence needed: Correlation analysis between BLEUpara scores and human judgments on diverse sign language datasets (e.g., DGS, BSL, or CSL).

- **Context utilization protocols**: Can specific protocols for utilizing video-level context improve semantic consistency of generated paraphrases, despite naive context injection degrading performance? The current experimental result showed that naive context inclusion caused performance degradation, leaving the optimal method for leveraging discourse information unknown. Evidence needed: Comparative study of context-injection strategies showing positive increase in ParaScore or human evaluation ratings.

- **Selective training strategies**: Can selective training strategies like hard example mining successfully leverage paraphrases for data augmentation, overcoming failures of naive random sampling? The paper found that randomly sampling paraphrases was detrimental to model performance. Evidence needed: Experiments demonstrating that model trained using hard-mined or curriculum-learning paraphrase selection achieves higher BLEU/BLEURT scores than baseline.

## Limitations
- Training degradation findings rely on limited protocols (random sampling, minimum-loss selection); more sophisticated approaches untested
- ParaScore filtering mechanism validated through manual evaluation but lacks external validation for SLT-specific semantic preservation
- Study restricted to American Sign Language datasets; generalizability to other sign languages unproven

## Confidence
- **High confidence**: BLEUpara evaluation improvements and human correlation results (robust evidence from correlation improvement and human evaluation on 70 sentences)
- **Medium confidence**: Paraphrase generation quality findings (GPT-4o-mini best internally but ParaScore applicability to other domains requires validation)
- **Medium confidence**: Training degradation findings (empirical results clear but conclusion about "naive augmentation" may be premature given limited protocol exploration)

## Next Checks
1. Apply ParaScore filtering to paraphrases generated for a different SLT dataset (e.g., PHOENIX-2014) and conduct manual evaluation to verify the 0.7 threshold generalizes across domains.

2. Implement curriculum-based paraphrase introduction where paraphrases are gradually incorporated during training, ordered by ParaScore quality, and compare against the random sampling baseline.

3. Generate paraphrases for references in multiple languages present in multilingual SLT datasets and evaluate whether BLEUpara correlation improvements hold consistently across language pairs.