---
ver: rpa2
title: 'RePo: Language Models with Context Re-Positioning'
arxiv_id: '2512.14391'
source_url: https://arxiv.org/abs/2512.14391
tags:
- context
- position
- tokens
- attention
- rope
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces RePo, a context re-positioning mechanism for
  large language models (LLMs) that dynamically assigns token positions based on their
  relevance rather than using fixed linear indices. This approach is motivated by
  Cognitive Load Theory, which suggests that rigid positional structures increase
  extraneous cognitive load and impair attention allocation.
---

# RePo: Language Models with Context Re-Positioning

## Quick Facts
- arXiv ID: 2512.14391
- Source URL: https://arxiv.org/abs/2512.14391
- Reference count: 27
- Key outcome: RePo dynamically assigns token positions based on relevance, improving performance on noisy, structured, and long-context tasks by up to 13.25 EM points

## Executive Summary
This paper introduces RePo, a differentiable context re-positioning mechanism for large language models that dynamically assigns token positions based on their relevance rather than using fixed linear indices. Motivated by Cognitive Load Theory, RePo employs a module that outputs continuous-valued positions from hidden states, enabling better attention allocation for distant but relevant information. The method is evaluated through continual pre-training on OLMo-2 1B, showing significant improvements on the RULER benchmark for noisy contexts, structured data, and extended context lengths up to 16K tokens.

## Method Summary
RePo replaces fixed linear positional encodings with dynamically learned positions based on token relevance. The method uses a differentiable module f_ϕ that takes hidden states and outputs continuous positions in a shared space. These positions are then used in the attention mechanism to compute attention scores, allowing the model to capture contextual dependencies more effectively. RePo is applied from layer 5 onward in the transformer, with standard RoPE used for initial layers. The method is trained through continual pre-training on OLMo-2 1B with 50B tokens of stage-2 data, and can be extended to longer contexts using the YaRN method.

## Key Results
- On RULER benchmark, RePo outperforms ROPE and NOPE by at least 6.24 points on noisy context tasks and 1.16 points on structured data tasks
- With 16K token extension via YaRN, RePo achieves at least 13.25 EM points improvement on QA and Needle-in-the-Haystack tasks
- Maintains competitive performance on general short-context tasks while being lightweight (0.9% parameter increase)
- Shows 5.48 points improvement on LongBench tasks

## Why This Works (Mechanism)
RePo works by dynamically assigning token positions based on their contextual relevance rather than fixed linear indices. The differentiable module f_ϕ processes hidden states through a SwiGLU sub-layer and linear projection to output scalar positions per head. These learned positions allow the model to allocate more attention to distant but relevant information, particularly beneficial in noisy contexts where standard positional encodings struggle. The method captures the intrinsic structure of input contexts by positioning tokens in a denser, non-linear space that reflects their actual relationships.

## Foundational Learning
- **Cognitive Load Theory**: Explains why rigid positional structures increase extraneous cognitive load and impair attention allocation - needed to understand motivation, check by verifying performance gains on noisy contexts
- **Continuous positional representations**: Understanding real-valued positions in continuous space versus discrete indices - needed for grasping the mathematical formulation, check by plotting position distributions
- **SwiGLU gating mechanism**: Understanding how SwiGLU provides stable gating for position learning - needed for implementation, check by monitoring gradient norms
- **Attention with learned positions**: How positions modify attention computation (A_{i,j} = q_i^T g_θ(z_j - z_i) k_j) - needed for architectural integration, check by comparing attention patterns with RoPE baseline
- **Continual pre-training**: Understanding the process of training on stage-2 data after initial pre-training - needed for reproduction, check by monitoring validation performance
- **Context length extrapolation**: How YaRN enables extension to 16K tokens - needed for long-context evaluation, check by verifying position consistency across scales

## Architecture Onboarding

Component map: Input tokens -> Token embeddings -> Layer 0-4 (RoPE) -> Layer 5+ (RePo) -> Attention with learned positions -> Output

Critical path: Token embeddings → RePo module (f_ϕ) → Position-based attention computation → Transformer layers → Output

Design tradeoffs:
- Applying RePo from layer 5+ vs. all layers: balances stability with effectiveness
- Continuous vs. discrete positions: enables non-linear spacing but requires careful initialization
- Per-head independent position assignment: increases flexibility but adds complexity
- Sharing position representation parameters per layer: reduces parameters while maintaining expressiveness

Failure signatures:
- Position collapse: all z_i converge to similar values, visible as narrow distributions
- No improvement on noisy tasks: indicates RePo not capturing relevant patterns
- Training instability: suggests issues with gradient flow through position module
- Performance drop on short contexts: suggests over-specialization to long/noisy data

First experiments:
1. Verify position distribution: Plot z_i values across different heads and layers for structured vs. noisy inputs
2. Compare attention patterns: Visualize attention mass allocation between RePo and RoPE baselines on needle-in-haystack tasks
3. Ablation study: Train variant with RePo from layer 0 vs. layer 5 to quantify delayed application benefits

## Open Questions the Paper Calls Out
None

## Limitations
- Lack of full training hyperparameter disclosure (learning rate, batch size, initialization) makes faithful reproduction difficult
- Performance improvements could partly reflect general pre-training effects rather than RePo-specific benefits alone
- Reliance on YaRN for long-context extension introduces external dependencies not fully described in the paper

## Confidence
High confidence: Mathematical formulation of RePo module and attention integration are clearly specified with verifiable performance gains
Medium confidence: Qualitative analysis of position allocation supported by visualizations but lacks quantitative metrics for position quality
Low confidence: Generalization claims to arbitrary context lengths rely on external YaRN method not fully described

## Next Checks
1. Position allocation analysis: Plot distribution of learned positions z_i across different heads and layers for varied input types to verify non-linear, dense patterns
2. Ablation on layer application: Train variant applying RePo from layer 0 vs. layer 5 to quantify contribution of delayed application
3. Training stability verification: Monitor gradient norms through RePo module during training to confirm SwiGLU gating prevents position collapse