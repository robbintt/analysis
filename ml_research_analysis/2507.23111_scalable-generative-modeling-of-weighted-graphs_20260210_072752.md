---
ver: rpa2
title: Scalable Generative Modeling of Weighted Graphs
arxiv_id: '2507.23111'
source_url: https://arxiv.org/abs/2507.23111
tags:
- edge
- graphs
- weight
- bigg
- weights
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "BiGG-E is an autoregressive generative model for weighted graphs\
  \ that jointly learns topology and edge weights while maintaining the scalability\
  \ of the BiGG framework. It uses two separate states\u2014topology and weight\u2014\
  merged with Tree-LSTM cells to capture dependencies between structure and weights."
---

# Scalable Generative Modeling of Weighted Graphs

## Quick Facts
- arXiv ID: 2507.23111
- Source URL: https://arxiv.org/abs/2507.23111
- Authors: Richard Williams; Eric Nalisnick; Andrew Holbrook
- Reference count: 40
- Primary result: BiGG-E jointly models weighted graph topology and edge weights via factorized probability and Tree-LSTM state merging, outperforming decoupled baselines on joint distribution quality while maintaining O((n+m) log n) scalability.

## Executive Summary
BiGG-E is an autoregressive generative model for sparse weighted graphs that extends the BiGG framework by maintaining separate topology and weight states, merged via Tree-LSTM cells to capture dependencies between structure and continuous attributes. The model uses a softplus-normal parameterization for edge weights and factorizes the joint probability as p(e,w) = p(e)·p(w|e), enabling stable gradient-based training. Experiments on five datasets demonstrate BiGG-E achieves superior joint modeling performance compared to Adj-LSTM, BiGG-MLP, and BiGG+GCN while scaling to graphs with thousands of nodes.

## Method Summary
BiGG-E generates weighted graphs sequentially by maintaining two Fenwick tree structures—one for topology and one for weights—each summarizing the history of generated edges and weights respectively. At each prediction step, the model merges these states using Tree-LSTM cells before deciding edge existence and sampling weights. Edge weights are modeled as softplus-transformed normal random variables, providing stable gradients while ensuring positivity. The architecture preserves the O((n+m) log n) scalability of BiGG through efficient prefix-sum operations and hierarchical state summaries.

## Key Results
- BiGG-E achieves lowest joint MMD across all five datasets, with weighted Laplacian spectrum MMD of 0.020 on 3D Point Cloud (vs 0.033 for BiGG-MLP)
- Outperforms baselines on weight statistics: MMD of 0.0117 vs 0.0142 (BiGG-MLP) on joint Tree dataset
- Scales to graphs with 15,000+ nodes while maintaining training efficiency, unlike Adj-LSTM which fails beyond ~500 nodes
- Joint modeling provides bidirectional uncertainty flow, preventing error amplification seen in two-stage BiGG+GCN pipeline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint modeling of topology and edge weights via factorized probability enables capturing dependencies between structure and continuous attributes.
- Mechanism: The model factorizes the joint probability p(e,w) = p(e)·p(w|e), where edge existence e is Bernoulli and weight w|e is sampled from a softplus-transformed normal. This allows the model to condition weight generation on topology and vice versa through shared hidden states.
- Core assumption: Edge weights are conditionally independent given the graph history and current topological context.
- Evidence anchors:
  - [Section 3.1]: "As w is only sampled when e exists, we can naturally factor the joint probability pθ(e,w) of observing a weighted edge (e,w) as pθ(e,w) = pθ(e)pθ(w|e)"
  - [Section 3.4.2]: "By leveraging both states during prediction, BiGG-E models the joint interaction between topology and weights"
  - [Corpus]: Weak direct evidence—related work (Weighted Random Dot Product Graphs) addresses weighted graphs but uses latent position models, not autoregressive factorization
- Break condition: If edge weights exhibit strong non-local dependencies that cannot be captured by autoregressive conditioning on prior edges, the factorization may underfit.

### Mechanism 2
- Claim: Maintaining separate topology and weight states that are merged at prediction time preserves scalability while enabling bidirectional information flow.
- Mechanism: BiGG-E maintains two Fenwick tree structures—a topology tree summarizing row generation history and a weight tree summarizing generated edge weights. These are merged via Tree-LSTM cells (h^sum = TreeCell(htop, hwt)) before each prediction, allowing topology to influence weight sampling and weights to influence edge decisions.
- Core assumption: The computational overhead of maintaining and merging two hierarchical states is acceptable given the sparsity assumption m = O(n).
- Evidence anchors:
  - [Abstract]: "A novel softplus-normal parameterization enables stable gradient-based training... The model jointly predicts topology and weights by merging these states using Tree-LSTM cells"
  - [Section 3.4.3]: "The novel architectural design outlined in Section 3.4 allows BiGG-E to extend the efficient training and sampling observed from BiGG to sparse weighted graphs"
  - [Corpus]: HiGen uses hierarchical generation but for unweighted graphs; no direct corpus evidence on dual-state merging
- Break condition: If graphs become dense (m ≈ n²), the O(m log m) weight tree overhead degrades scalability guarantees.

### Mechanism 3
- Claim: Softplus transformation of normal random variables provides stable gradients for positive weight generation while remaining tractable for maximum likelihood training.
- Mechanism: Rather than using gamma (complex likelihood) or log-normal (heavy tails), weights are sampled as w = Softplus(ε) where ε ~ N(μ, σ²). The density p(w|e) ∝ exp[-(log(e^w - 1) - μ)² / 2σ²] is differentiable and bounded.
- Core assumption: The softplus-normal family is sufficiently expressive to model the true weight distribution.
- Evidence anchors:
  - [Section 3.1]: "In our experience, such a transformation of a normal random variable performs best with gradient-based optimization by providing enough flexibility in modeling distributions"
  - [Section 3.1]: "Other candidate distributions, such as the gamma and log-normal distributions, are more challenging to implement because of the complexity of the likelihood in the former and the heavy right-tailedness in the latter"
  - [Corpus]: No corpus papers directly evaluate softplus-normal for graph weights
- Break condition: If true weight distributions are multi-modal or have bounded support far from zero, softplus-normal may be misspecified.

## Foundational Learning

- Concept: **Autoregressive factorization for graphs**
  - Why needed here: BiGG-E generates graphs sequentially, conditioning each edge/weight decision on all prior decisions. Understanding how p(W) = ∏ p(Wij | {Wkl}) decomposes is essential for following the training objective.
  - Quick check question: Given a 4-node graph with edges (1,2), (2,3), what is the conditioning set for edge (3,4)?

- Concept: **Fenwick trees for prefix sums**
  - Why needed here: Both the topology and weight states use Fenwick trees to compute summary representations in O(log n) time. Without this, the model would be O(n²).
  - Quick check question: How many levels does a Fenwick tree have for summarizing 16 rows?

- Concept: **Tree-LSTM cells**
  - Why needed here: Tree-LSTMs merge two child states into a parent state (hj = TreeLSTM(hL, hR)), used throughout BiGG-E for combining topology/weight states and building hierarchical summaries.
  - Quick check question: How does a Tree-LSTM differ from a standard sequential LSTM in terms of input structure?

## Architecture Onboarding

- Component map:
  - Decision trees Tu: Binary trees for each node vu that recursively partition [v1, vu-1] to find edge connections
  - Fenwick topology tree: Hierarchical structure summarizing all previously generated rows; enables O(log n) row conditioning
  - Fenwick weight tree: Parallel structure summarizing all previously generated weights; built via LSTM embeddings merged with Tree-LSTMs
  - Tree-LSTM cells: Three types—row merging (gi_j), summary computation (hrow_u, hwt_k), and state merging (hsum)
  - Prediction heads: MLPs f_p (edge probability), f_μ (weight mean), f_σ² (weight variance)

- Critical path:
  1. For each node vu: Initialize htop_u from hrow_u (Fenwick topology summary)
  2. Traverse Tu: At each node t, merge htop_u(t) with hwt_k → hsum
  3. Predict edge existence in lch(t), recurse if edge found
  4. At leaf (singleton interval): Predict μ, σ² and sample weight
  5. Embed weight via LSTM, update Fenwick weight tree
  6. Continue to rch(t) with conditioned state

- Design tradeoffs:
  - Separate vs. shared states: BiGG-E uses separate topology (dim 256) and weight (dim 32) states, enabling bits compression on topology and preventing weight overfitting—BiGG-MLP's shared state degrades performance
  - Joint vs. two-stage: BiGG+GCN decouples topology and weights, propagating errors; BiGG-E's joint modeling allows bidirectional uncertainty flow
  - Embedding dimensions: Asymmetric dimensions (256 vs. 32) reduce memory by ~20% but may limit weight representation capacity

- Failure signatures:
  - OOM on large graphs: Adj-LSTM fails at ~500 nodes; indicates need for sparse generation approach
  - Degraded topology with BiGG-MLP: Shared state causes orders-of-magnitude worse MMD; confirms need for state separation
  - Inflated weight variance with BiGG+GCN: Two-stage pipeline amplifies topology errors (σ = 0.191 vs. 0.096 on 3D Point Cloud)
  - Overtraining on weights: Paper notes tendency to overfit weights; addressed via loss scaling (10× then 100×) and alternating updates

- First 3 experiments:
  1. Validate softplus-normal expressiveness: Train on tree dataset (hierarchical weights with known Γ distribution); check if global variance (4.0) and per-tree variance (1.0) match theoretical values
  2. Ablate state merging: Compare BiGG-E against BiGG-MLP on joint tree dataset; expect BiGG-E MMD < 1e-4 vs. BiGG-MLP ~1e-2
  3. Stress test scalability: Train on trees with |V| ∈ {100, 500, 1000, 2000, 5000, 10000, 15000}; verify O(log n) training time and O((n+m) log n) sampling, compare against SparseDiff (should fail beyond hundreds of nodes)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can BiGG-E be extended to model joint distributions over graph topologies and high-dimensional edge or node attribute vectors?
- Basis in paper: [explicit] The conclusion states, "we can further explore the benefits of joint modeling edge weights and topologies by learning joint distributions over topologies and vectors of edge and node attributes."
- Why unresolved: The current implementation is restricted to single-dimensional, continuous edge weights ($w \in \mathbb{R}^+$) and does not support multi-modal or vector-based features.
- What evidence would resolve it: Successful integration of a multi-dimensional weight state into the Tree-LSTM architecture and demonstration of performance on datasets with rich edge attributes.

### Open Question 2
- Question: How can the memory consumption of BiGG-E be reduced to facilitate scaling to graphs larger than 15K nodes?
- Basis in paper: [explicit] The authors explicitly list "further improving BiGG-E, especially with respect to memory consumption" as a primary direction for future work.
- Why unresolved: While the model is computationally efficient ($O((n+m)\log n)$), maintaining two separate Fenwick tree states (topology and weight) creates a memory overhead that limits maximum graph size.
- What evidence would resolve it: Architectural modifications (e.g., state compression or parameter sharing) that lower the memory footprint during training without degrading the quality of the joint distribution.

### Open Question 3
- Question: What is the optimal strategy for balancing the trade-off between topological accuracy and edge weight fidelity during joint optimization?
- Basis in paper: [inferred] Appendix A.5 notes that models have a "propensity... to overtrain on the edge weights" and that learning topology is "much more challenging," requiring manual heuristics like loss scaling and decoupled updates.
- Why unresolved: The authors rely on manual regularization techniques (scaling weight loss by factors of 10 or 100) rather than a principled multi-objective optimization scheme.
- What evidence would resolve it: An automated training curriculum or loss-balancing mechanism that converges on both topological MMD and weight statistics without manual intervention.

## Limitations
- The softplus-normal parameterization's expressiveness is not rigorously compared against alternatives like log-normal or gamma distributions
- Performance advantages may be partly dataset-specific, particularly for the jointly constructed Tree dataset
- Scalability guarantees rely on the sparsity assumption m = O(n) without robustness testing on dense graphs
- Memory overhead from maintaining two separate Fenwick tree states limits scaling beyond 15K nodes

## Confidence
- **High Confidence**: Claims about architectural mechanisms (separate topology/weight states, Tree-LSTM merging, Fenwick trees) are well-specified and match implementation details
- **Medium Confidence**: Claims about scalability (O((n+m) log n) sampling) are supported by experiments but rely on the sparsity assumption m = O(n) without robustness testing on dense graphs
- **Medium Confidence**: Claims about joint modeling superiority are supported by ablation studies but may be partly dataset-dependent given the construction of the joint Tree dataset

## Next Checks
1. **Expressiveness Test**: Train BiGG-E and BiGG-MLP on the tree dataset and compare learned weight distributions against the theoretical Γ(μ², μ⁻¹) ground truth, particularly verifying the hierarchical variance structure
2. **Scaling Stress Test**: Run scalability experiments on tree graphs with |V| up to 15,000 nodes, measuring actual runtime against the theoretical O((n+m) log n) bound and comparing against SparseDiff's failure point
3. **Alternative Distribution Ablation**: Replace the softplus-normal with log-normal parameterization in BiGG-E and retrain on the 3D Point Cloud dataset to assess whether the performance advantage is specific to the softplus transformation