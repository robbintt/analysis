---
ver: rpa2
title: Domain Adaptation of Drag Reduction Policy to Partial Measurements
arxiv_id: '2507.04309'
source_url: https://arxiv.org/abs/2507.04309
tags:
- measurements
- policy
- optimal
- control
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of adapting feedback control
  policies trained with full-state measurements to scenarios with only partial measurements,
  motivated by real-world limitations in sensor placement. The authors propose a Policy
  Domain Adaptation (PDA) framework that leverages a Domain-Specific Feature Transfer
  (DSFT) map to reconstruct full measurements from the history of partial measurements.
---

# Domain Adaptation of Drag Reduction Policy to Partial Measurements

## Quick Facts
- arXiv ID: 2507.04309
- Source URL: https://arxiv.org/abs/2507.04309
- Authors: Anton Plaksin; Georgios Rigas
- Reference count: 8
- One-line primary result: Policy Domain Adaptation framework enables optimal control policies trained on full-state measurements to be effectively deployed with only partial measurements in active flow control.

## Executive Summary
This paper addresses the challenge of adapting feedback control policies trained with full-state measurements to scenarios with only partial measurements, motivated by real-world limitations in sensor placement. The authors propose a Policy Domain Adaptation (PDA) framework that leverages a Domain-Specific Feature Transfer (DSFT) map to reconstruct full measurements from the history of partial measurements. They demonstrate this approach in a simulated environment for reducing aerodynamic drag of a simplified road vehicle, where the full measurements are pressure readings in the wake, but only body-base pressure readings are available in deployment.

The core method trains a neural network to map partial measurements and action history to full measurements, then composes this with the optimal policy for full measurements to derive a policy for partial measurements. Experiments show that the PDA-derived policies achieve drag reduction performance close to the optimal policy trained with full measurements, significantly outperforming policies trained directly on partial measurements. The results also reveal that including measurement history is crucial for optimal performance, while action history has minimal impact.

## Method Summary
The method involves training an optimal policy using full-state measurements, then learning a mapping from partial measurements to full measurements using a neural network. This mapping is then composed with the optimal policy to create a policy that operates on partial measurements. The approach is demonstrated in a 2D DNS simulation of flow past a square bluff body, where the goal is to reduce drag by stabilizing vortex shedding using jet actuators.

## Key Results
- PDA-derived policies achieve drag reduction performance close to optimal policy trained with full measurements
- Policies trained directly on partial measurements perform significantly worse than PDA approach
- Including measurement history is crucial for optimal performance, while action history has minimal impact
- Single hidden layer architecture for DSFT map is sufficient, deeper architectures show instability

## Why This Works (Mechanism)
The PDA framework works by learning a reconstruction of the full state from partial observations, allowing the optimal policy trained on full states to be applied. This is particularly effective in flow control where the full state (wake pressure field) contains information about the flow dynamics that is not directly observable from body pressure measurements alone. The DSFT map learns to extract this information from the temporal history of partial measurements.

## Foundational Learning
- Active Flow Control: Why needed? To manipulate flow behavior for performance benefits like drag reduction. Quick check: Policy reduces time-averaged drag coefficient.
- Partial Observability: Why needed? Real sensors provide limited state information. Quick check: PDA uses only body-base pressure vs optimal's wake pressures.
- Domain Adaptation: Why needed? To transfer knowledge from full-measurement to partial-measurement regimes. Quick check: DSFT map composition with optimal policy.
- Temporal State Representation: Why needed? Flow dynamics are inherently temporal. Quick check: History length n=48 optimal, not n=0.

## Architecture Onboarding
- Component map: TQC policy π* (full) <- DSFT map T_θ (partial, history) -> composed policy π*_PDA (partial)
- Critical path: Partial measurements → DSFT → Reconstructed full → Optimal policy → Action
- Design tradeoffs: Simple linear/non-linear DSFT vs complex sequence models; history length n vs performance
- Failure signatures: Linear T_θ sub-optimal; deeper T_θ unstable; short history degrades performance
- First experiments: 1) Vary history length n from 0 to 48 and plot drag vs n; 2) Compare linear vs single hidden layer T_θ; 3) Test m=0 (no action history) vs m>0

## Open Questions the Paper Calls Out
- How robust is the Policy Domain Adaptation (PDA) framework when transferring policies trained in simulation to physical real-world environments (sim-to-real)?
- Is the irrelevance of action history generalizable to other active flow control tasks, or is it specific to the square bluff body geometry?
- Are complex sequence modeling architectures (e.g., Transformers) redundant for all partially observed flow control tasks?

## Limitations
- Limited to 2D DNS simulation, not validated on physical systems
- Specific to bluff body geometry, generalizability to other shapes unknown
- DSFT map trained on data from same domain as target, not tested on out-of-distribution measurements

## Confidence
- High: PDA framework validity and superiority over direct partial-observation RL
- Medium: Empirical findings about history length and action history impact
- Low: Exact numerical performance figures without reproducing environment

## Next Checks
1. Recreate the 2D DNS bluff-body environment with the wake and body-base pressure sensor grid from Fig 1.
2. Train the DSFT map T_θ with history length n varying from 0 to 48; plot drag coefficient vs n to confirm the 48-step optimum.
3. Compare the PDA policy against baselines (optimal full, direct partial RL) across 65 runs to verify reported time-averaged drag reduction performance.