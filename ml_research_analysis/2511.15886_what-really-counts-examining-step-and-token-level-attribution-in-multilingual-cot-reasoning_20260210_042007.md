---
ver: rpa2
title: What Really Counts? Examining Step and Token Level Attribution in Multilingual
  CoT Reasoning
arxiv_id: '2511.15886'
source_url: https://arxiv.org/abs/2511.15886
tags:
- reasoning
- steps
- step
- attribution
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates the attribution patterns underlying Chain-of-Thought
  (CoT) reasoning in multilingual large language models (LLMs). Using both step-level
  and token-level attribution methods, the research analyzes how different reasoning
  steps contribute to final answers across five languages: English, French, German,
  Bengali, and Chinese.'
---

# What Really Counts? Examining Step and Token Level Attribution in Multilingual CoT Reasoning

## Quick Facts
- **arXiv ID**: 2511.15886
- **Source URL**: https://arxiv.org/abs/2511.15886
- **Reference count**: 8
- **Primary result**: Structured CoT improves accuracy for high-resource Latin-script languages but shows minimal benefit for low-resource Bengali

## Executive Summary
This study investigates attribution patterns in Chain-of-Thought reasoning across five languages using step-level and token-level analysis methods. The research focuses on Qwen2.5-1.5B-Instruct model performance on the MGSM benchmark, comparing structured CoT prompting against baseline approaches. Key findings reveal significant performance disparities between high-resource and low-resource languages, with structured CoT showing substantial benefits for English and French but minimal improvement for Bengali. The analysis demonstrates that final reasoning steps consistently receive the highest importance scores, particularly in incorrect predictions, while perturbation experiments reveal decreased accuracy and less coherent attribution patterns when introducing negations or distractors.

## Method Summary
The study employs integrated gradients for token-level attribution and gradient-based approaches for step-level analysis to examine reasoning patterns in multilingual CoT prompting. Experiments use the Qwen2.5 1.5B-Instruct model on the MGSM benchmark across five languages: English, French, German, Bengali, and Chinese. The methodology includes controlled perturbation experiments with negation and distractor conditions to assess attribution robustness. Step-wise attribution analysis quantifies the contribution of individual reasoning steps to final answers, while token-level analysis examines how specific word modifications affect model performance and attribution patterns.

## Key Results
- Structured CoT significantly improves accuracy for high-resource Latin-script languages (English, French) but shows minimal benefit for low-resource Bengali
- Final reasoning step consistently receives highest importance scores, especially in incorrect predictions
- Perturbation experiments show decreased accuracy and less coherent attribution patterns with controlled input modifications

## Why This Works (Mechanism)
Chain-of-Thought reasoning works by decomposing complex problems into sequential reasoning steps that guide the model toward a solution. The mechanism relies on intermediate representations that build upon each other, with each step providing contextual information for subsequent reasoning. In multilingual settings, this process encounters varying levels of support based on training data availability and linguistic complexity. The attribution patterns reveal how models weight different reasoning components, with final steps often dominating due to their direct connection to answer generation. However, the sensitivity to perturbations suggests that the reasoning process may be more fragile than robust, particularly in lower-resource language contexts where the model has less training signal to develop reliable intermediate representations.

## Foundational Learning
**Chain-of-Thought Prompting** - A prompting technique that encourages models to generate intermediate reasoning steps before providing final answers. *Why needed*: Enables complex problem-solving by breaking tasks into manageable sub-steps. *Quick check*: Does the model generate coherent intermediate reasoning steps that logically lead to the answer?

**Attribution Methods** - Techniques for quantifying the contribution of different model components to predictions. *Why needed*: Provides interpretability and helps identify which reasoning elements drive model decisions. *Quick check*: Do attribution scores align with intuitive importance of different reasoning components?

**Integrated Gradients** - A gradient-based attribution method that accumulates gradients along the path from a baseline input to the actual input. *Why needed*: Provides theoretically grounded attribution that satisfies implementation invariance and sensitivity properties. *Quick check*: Are attribution scores stable across multiple runs with the same input?

**Multilingual Model Performance** - The observation that model performance varies significantly across languages based on training data availability and linguistic similarity to high-resource languages. *Why needed*: Explains why CoT benefits may not transfer equally across language boundaries. *Quick check*: Does model performance correlate with training data size and linguistic proximity to English?

## Architecture Onboarding

**Component Map**: Input Text -> Tokenization -> CoT Prompting -> Sequential Reasoning Steps -> Final Answer Generation -> Attribution Analysis

**Critical Path**: The core reasoning pipeline involves prompt formulation, intermediate step generation, and final answer prediction. Attribution analysis occurs post-hoc through gradient computation and integration.

**Design Tradeoffs**: Structured CoT provides better interpretability and reasoning coherence but may introduce computational overhead and sensitivity to prompt formatting. The choice between step-level and token-level attribution involves balancing granularity with computational efficiency.

**Failure Signatures**: Decreased accuracy with perturbation experiments indicates fragile reasoning paths. Overemphasis on final steps in incorrect predictions suggests potential reliance on surface-level patterns rather than robust reasoning. Minimal performance improvement for low-resource languages indicates training data limitations.

**First Experiments**: 1) Baseline accuracy comparison across all five languages without CoT prompting 2) Step-level attribution analysis for correct vs incorrect predictions in English 3) Token-level attribution stability test with minor input modifications

## Open Questions the Paper Calls Out
None

## Limitations
- Study exclusively uses Qwen2.5-1.5B-Instruct, limiting generalizability to larger or different model architectures
- Analysis based solely on MGSM benchmark, which may not capture full spectrum of multilingual reasoning patterns
- Limited language coverage that may not represent full diversity of global writing systems and morphological complexity

## Confidence
- **High Confidence**: Structured CoT improves accuracy for high-resource Latin-script languages while showing minimal benefit for low-resource Bengali
- **Medium Confidence**: Final reasoning steps receive highest importance scores, particularly in incorrect predictions
- **Medium Confidence**: Perturbation experiments show decreased accuracy and less coherent attribution patterns
- **Low Confidence**: Broader claims about CoT prompting's multilingual robustness limitations based on single-model evidence

## Next Checks
1. Replicate step-level and token-level attribution analyses using larger multilingual models (Qwen2.5-7B, LLaMA-3-8B, or GPT-4) to assess generalizability across model capacities

2. Apply same attribution methodology to diverse reasoning benchmarks beyond MGSM, including commonsense reasoning tasks and logical reasoning problems, to evaluate cross-domain generalization

3. Conduct ablation studies systematically removing or modifying individual reasoning steps to quantify step-wise contribution variations across languages and prediction correctness