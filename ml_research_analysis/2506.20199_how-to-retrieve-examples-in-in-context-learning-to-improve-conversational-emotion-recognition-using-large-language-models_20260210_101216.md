---
ver: rpa2
title: How to Retrieve Examples in In-context Learning to Improve Conversational Emotion
  Recognition using Large Language Models?
arxiv_id: '2506.20199'
source_url: https://arxiv.org/abs/2506.20199
tags:
- emotion
- context
- examples
- datasets
- utterances
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates how to improve conversational emotion recognition
  (CER) using large language models (LLMs) through effective in-context learning (ICL).
  The main problem is that CER is subjective and challenging, and previous approaches
  using random ICL examples do not consistently improve performance.
---

# How to Retrieve Examples in In-context Learning to Improve Conversational Emotion Recognition using Large Language Models?

## Quick Facts
- arXiv ID: 2506.20199
- Source URL: https://arxiv.org/abs/2506.20199
- Reference count: 0
- Primary result: AER improves macro F1 by ~1% on average over zero-shot and random ICL baselines

## Executive Summary
This paper addresses the challenge of conversational emotion recognition (CER) using large language models (LLMs) through in-context learning (ICL). The authors find that randomly selected ICL examples fail to consistently improve performance, prompting them to develop an augmented example retrieval (AER) method. AER selects the most semantically coherent reference utterance using sentence similarity and augments it with paraphrased examples. Experiments on IEMOCAP, MELD, and EmoryNLP datasets demonstrate consistent improvements of approximately 1% macro F1 score on average, with gains up to 2% on EmoryNLP. The approach also maintains effectiveness on noisy ASR transcriptions.

## Method Summary
The AER framework enhances ICL for CER by first encoding a combined reference dataset (MELD + EmoryNLP training splits) using SentenceTransformer. For each test utterance, it retrieves the most semantically similar reference example via cosine similarity, then generates four paraphrases using Mistral-7B-Instruct. The system runs five prediction rounds with different paraphrase variants and aggregates results via majority voting. This approach contrasts with zero-shot baselines (no examples) and random ICL (four random examples, one per emotion). The method uses Llama-3.1-8B-Instruct for predictions with minimal temperature settings.

## Key Results
- AER consistently improves macro F1 scores by ~1% on average across all three datasets
- Maximum improvement of 2% achieved on EmoryNLP dataset
- AER outperforms both zero-shot baselines and ICL with random examples
- Performance gains maintained even with noisy ASR transcriptions
- Optimal context sizes vary by dataset: 10 utterances for IEMOCAP, 0 for MELD and EmoryNLP

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieving semantically coherent examples improves ICL effectiveness for emotion recognition over random selection.
- Mechanism: SentenceTransformer encodes both reference and target utterances into dense vectors; cosine similarity identifies the reference utterance most semantically aligned with the target. This coherent example provides the LLM with a reasoning pattern that better matches the current input.
- Core assumption: Semantic similarity in embedding space correlates with emotional reasoning patterns that transfer across utterances.
- Evidence anchors:
  - [abstract] "highlighting the importance of retrieving coherent targeted examples"
  - [section 2.3] "we search for the example utterance based on the cosine similarity"
  - [corpus] Related work (TDR, arXiv:2507.18340) confirms retrieval quality heavily impacts ICL effectiveness, though specific similarity metrics vary.
- Break condition: If target utterances contain domain-specific language not represented in reference data, similarity-based retrieval may surface superficially similar but emotionally irrelevant examples.

### Mechanism 2
- Claim: Paraphrase augmentation of retrieved examples improves prediction robustness through implicit ensemble effects.
- Mechanism: Mistral-7B generates four paraphrases per reference utterance. Each prediction round uses a different paraphrase while keeping other examples fixed; final prediction emerges from majority voting across five rounds.
- Core assumption: Paraphrases preserve emotional semantics while introducing lexical variation that reduces single-prediction noise.
- Evidence anchors:
  - [abstract] "enhancing them through paraphrasing"
  - [section 2.2] "we augment the dataset by generating distinct paraphrases"
  - [corpus] Limited direct corpus evidence on paraphrase-specific augmentation for ICL; primarily supported by this paper's empirical results.
- Break condition: If paraphrasing alters emotional valence or intensity, the augmentation introduces label noise rather than robustness.

### Mechanism 3
- Claim: Conversation context size exhibits diminishing returns and can degrade performance on imbalanced datasets.
- Mechanism: On IEMOCAP, context expansion from 0→10 utterances improves F1 (0.465→0.567); on MELD/EmoryNLP, zero context performs best. Authors speculate LLMs over-predict diverse emotions not in the target set when context is limited, artificially inflating neutral predictions.
- Core assumption: Dataset-specific emotion distributions and annotation conventions interact with how LLMs weight contextual cues.
- Evidence anchors:
  - [section 4.1] "the LLM achieves the highest performance with no conversation context provided on the other two datasets"
  - [section 4.5] "MELD and EmoryNLP datasets have relatively imbalanced emotion distribution"
  - [corpus] Not explicitly tested in neighboring papers; context-length effects appear task-dependent.
- Break condition: If target conversations have implicit emotional arcs requiring longer context, truncation may miss critical signals; the paper does not test beyond 20 utterances.

## Foundational Learning

- Concept: **In-Context Learning (ICL)**
  - Why needed here: The entire framework relies on providing labeled examples in the prompt without weight updates. Understanding why ICL works—and when it fails—is prerequisite to diagnosing retrieval quality issues.
  - Quick check question: Can you explain why random ICL examples failed to consistently improve performance in this study?

- Concept: **Sentence Embeddings and Cosine Similarity**
  - Why needed here: AER uses SentenceTransformer (all-MiniLM-L6-v2) to map utterances to 384-dim vectors. Retrieval depends on understanding embedding space geometry.
  - Quick check question: What would happen if two utterances with opposite emotional valence had high cosine similarity due to shared vocabulary?

- Concept: **Majority Voting for Prediction Stability**
  - Why needed here: The five-round paraphrase voting scheme assumes independent prediction errors cancel out. Understanding ensemble assumptions helps diagnose when voting fails.
  - Quick check question: Under what conditions would five-round voting produce worse results than single-prediction?

## Architecture Onboarding

- Component map:
  - Reference Dataset -> Embedding Encoder -> Retrieval Module -> Paraphrase Generator -> Prompt Constructor -> Inference LLM -> Voting Aggregator

- Critical path:
  1. Pre-compute embeddings for augmented reference dataset (offline)
  2. At inference: encode target utterance → retrieve most similar example → construct prompt with 4 examples (1 coherent + 3 random for other emotions) → run 5 rounds with paraphrase variants → majority vote

- Design tradeoffs:
  - **Context size**: 10 for IEMOCAP, 0 for MELD/EmoryNLP (empirically determined; not generalizable)
  - **Retrieval scope**: Paper tests in-domain vs. out-of-domain retrieval but does not fully report findings
  - **Model scale**: Limited to 8B parameters due to GPU constraints; scaling effects unknown
  - **Emotion granularity**: Collapsed to 4 emotions (happy, sad, neutral, angry); other emotions mapped to neutral

- Failure signatures:
  - Macro F1 similar to or worse than zero-shot baseline → check if retrieved examples are emotionally coherent (not just semantically similar)
  - High variance across paraphrase rounds → paraphrases may be altering emotional semantics
  - Poor performance on ASR transcripts despite AER → verify embedding quality on noisy text

- First 3 experiments:
  1. **Reproduce baseline gap**: Run zero-shot with optimal context sizes vs. random ICL on a held-out subset to confirm random examples don't consistently help.
  2. **Ablate paraphrasing**: Compare AER with single paraphrase vs. 5-round voting to isolate voting benefit from retrieval benefit.
  3. **Test retrieval granularity**: Retrieve top-k similar examples (k=1,3,5) instead of single coherent example + 3 random to measure whether all examples should be similarity-selected.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the Augmented Example Retrieval (AER) method generalize to significantly larger or structurally distinct Large Language Models?
- **Basis in paper:** [explicit] The authors state, "Due to the restriction of GPU capacity, we chose to only experiment on Llama-3.1-8B-Instruct," and explicitly list extending experiments to diverse models like the "Gemma-2 Family" as future work.
- **Why unresolved:** The study's conclusions are currently limited to a single 8B-parameter model architecture due to computational constraints.
- **What evidence would resolve it:** Benchmarking AER performance on larger parameter models or alternative architectures (e.g., Gemma-2) showing consistent macro F1 improvements over baselines.

### Open Question 2
- **Question:** Can the AER framework maintain its efficacy when expanding the classification task to a wider spectrum of emotion categories?
- **Basis in paper:** [explicit] The authors note the current focus is limited to four emotions and aim to "refine the method to enhance the conversational emotion recognition to a wider spectrum of expressed emotions."
- **Why unresolved:** The study mapped all datasets to a simple 4-class schema (happy, sad, neutral, angry), leaving the method's performance on more granular or complex emotion taxonomies unknown.
- **What evidence would resolve it:** Successful application of AER on datasets with 7+ emotion labels (e.g., the full EmoryNLP schema) without performance degradation or retrieval confusion.

### Open Question 3
- **Question:** To what extent does data quality screening and the inclusion of more complete conversation context improve the reliability of AER?
- **Basis in paper:** [explicit] The authors acknowledge limitations in the test data where "some utterances... do not have accurate emotion labels" and propose to "perform data screening... to be more accurate and complete."
- **Why unresolved:** The reported performance metrics may be noisy or suppressed due to inaccurate ground-truth labels and missing context in the current test sets.
- **What evidence would resolve it:** A comparative evaluation on a curated, high-quality dataset with verified labels and full conversational history showing distinct performance gains.

## Limitations

- The study is limited to a single 8B-parameter LLM due to GPU constraints, leaving scaling effects unknown
- Optimal context sizes were empirically determined but not systematically explained for different datasets
- The paraphrase augmentation mechanism lacks direct validation of whether majority voting adds value beyond retrieval quality
- The retrieval strategy uses single-example coherence rather than selecting all examples based on similarity

## Confidence

- **High Confidence**: AER improves macro F1 by ~1% on average over zero-shot and random ICL baselines (consistent results across three datasets)
- **Medium Confidence**: Sentence similarity-based retrieval provides more emotionally coherent examples than random selection (supported by results but lacks direct validation)
- **Medium Confidence**: Paraphrase augmentation improves robustness through implicit ensemble effects (empirical results but lacks ablation studies)
- **Low Confidence**: Zero context works best on MELD/EmoryNLP due to class imbalance causing LLM over-prediction (speculative and untested)

## Next Checks

1. **Ablate the Paraphrasing Component**: Run AER without paraphrase augmentation (single prediction per utterance) and compare performance to the 5-round voting approach to isolate whether the improvement comes from retrieval quality or from the ensemble effect of multiple paraphrases.

2. **Test Full Similarity-Based Retrieval**: Instead of using one coherent example plus three random examples, retrieve the top 4 most similar examples for the four emotion classes and compare this to the current AER approach to determine if all examples should be selected based on semantic similarity rather than just one.

3. **Validate Emotional Coherence of Retrieved Examples**: For a random sample of test utterances, manually inspect the retrieved reference utterance and rate whether it shares similar emotional characteristics to directly test the core assumption that cosine similarity in embedding space correlates with emotional reasoning patterns.