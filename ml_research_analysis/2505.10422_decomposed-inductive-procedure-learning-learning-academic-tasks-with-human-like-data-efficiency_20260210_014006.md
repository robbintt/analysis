---
ver: rpa2
title: 'Decomposed Inductive Procedure Learning: Learning Academic Tasks with Human-Like
  Data Efficiency'
arxiv_id: '2505.10422'
source_url: https://arxiv.org/abs/2505.10422
tags:
- learning
- mechanism
- data
- human
- koedinger
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Decomposed Inductive Procedure Learning: Learning Academic Tasks with Human-Like Data Efficiency

## Quick Facts
- **arXiv ID**: 2505.10422
- **Source URL**: https://arxiv.org/abs/2505.10422
- **Reference count**: 7
- **Key outcome**: DIPL (3-mechanism symbolic rule induction) learns academic procedures 500x more efficiently than DQN/PPO baselines and matches human learning rates

## Executive Summary
DIPL (Decomposed Inductive Procedure Learning) achieves human-like data efficiency on academic tasks by decomposing procedural learning into three distinct mechanisms: how-learning (skill composition), where-learning (spatial generalization), and when-learning (precondition classification). Unlike traditional reinforcement learning that explores through trial-and-error, DIPL leverages worked examples and abductive reasoning to compose primitive functions into executable skills, then generalizes these skills across interface contexts and learns when to apply them. The approach achieves dramatic efficiency gains—learning multi-step procedures in 10-30 examples versus 500K+ timesteps for RL baselines—while maintaining strong generalization across problem sizes.

## Method Summary
The method implements a three-mechanism architecture for learning academic procedures from worked examples. How-learning uses abductive search to compose primitive domain-general functions (Add, Multiply) into skill explanations that reproduce observed actions, selecting the most parsimonious composition and generalizing by replacing constants with variables. Where-learning learns spatial patterns specifying where skills apply using logical conditions over interface elements, enabling transfer across columns and positions. When-learning trains a decision tree classifier to learn symbolic preconditions using relative featurization based on shortest-path adjacency relationships. Skills are represented as production rules assembled from these three components and evaluated on two tutoring domains: fraction arithmetic and 3-digit multi-column addition.

## Key Results
- DIPL achieves 500x efficiency improvement over DQN/PPO baselines, learning procedures in 10-30 examples versus 500K+ timesteps
- Full 3-mechanism DIPL outperforms both 1-mechanism RL and 2-mechanism symbolic approaches across both fraction and multi-column addition domains
- Removing relative featurization from when-learning degrades performance by 13-19 problems, confirming its importance for data efficiency
- DIPL demonstrates strong generalization, successfully applying learned skills to problems of different sizes (2-digit to 4-digit addition)

## Why This Works (Mechanism)

### Mechanism 1: How-Learning
Separating action composition from policy learning reduces the search space by grounding skill acquisition in demonstrated outputs rather than trial-and-error exploration. Abductive search composes primitive domain-general functions to reproduce observed actions from worked examples, selecting the most parsimonious composition and generalizing by replacing constants with variables. This assumes domain-general primitive functions are available as prior knowledge and correct explanations exist within limited search depth (typically 1-3).

### Mechanism 2: Where-Learning
Spatial generalization across interface contexts enables a single skill to transfer across multiple application points without relearning. Learns logical patterns specifying where skills apply using argument variables and selection variables, generalizing from initial examples by relaxing spatial constraints. This assumes spatial relationships are consistent and expressible as logical conditions over interface elements.

### Mechanism 3: When