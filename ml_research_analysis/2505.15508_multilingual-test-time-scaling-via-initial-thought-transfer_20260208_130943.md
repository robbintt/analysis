---
ver: rpa2
title: Multilingual Test-Time Scaling via Initial Thought Transfer
arxiv_id: '2505.15508'
source_url: https://arxiv.org/abs/2505.15508
tags:
- reasoning
- languages
- language
- scaling
- test-time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates multilingual test-time scaling, finding
  that reasoning improvements from longer inference are inconsistent across languages,
  with low-resource languages showing weaker and more variable gains. Models frequently
  switch to English mid-reasoning even under strict monolingual prompts, indicating
  fragile language grounding.
---

# Multilingual Test-Time Scaling via Initial Thought Transfer

## Quick Facts
- **arXiv ID**: 2505.15508
- **Source URL**: https://arxiv.org/abs/2505.15508
- **Reference count**: 17
- **Primary result**: MITT improves test-time scaling for underrepresented languages by transferring high-resource reasoning prefixes via unsupervised prefix-tuning

## Executive Summary
This work investigates multilingual test-time scaling, finding that reasoning improvements from longer inference are inconsistent across languages, with low-resource languages showing weaker and more variable gains. Models frequently switch to English mid-reasoning even under strict monolingual prompts, indicating fragile language grounding. Low-resource languages also exhibit divergent initial reasoning patterns and lower internal consistency compared to high-resource languages. To address these issues, the authors introduce MITT (Multilingual Initial Thought Transfer), an unsupervised prefix-tuning approach that transfers high-resource reasoning prefixes to low-resource languages. MITT significantly improves test-time scaling performance for DeepSeek-R1-Distill-Qwen-7B, especially for underrepresented languages, by providing stable inductive priors across linguistic boundaries.

## Method Summary
The authors evaluate test-time scaling (accuracy vs. generation length) in strictly monolingual settings across high-resource (English, Italian, German, Portuguese) and low-resource (Vietnamese, Tagalog) languages using the AIME 2025 Multilingual dataset (30 numerical math questions). They propose MITT—an unsupervised prefix-tuning approach that fine-tunes LoRA adapters on the first 32 tokens of high-resource reasoning chains to provide stable inductive priors for low-resource languages. The method uses LoRA on query/value projections (r=8, alpha=32, dropout=0.05) with 4-bit quantization, training on 100 sampled prefixes per question for 1-3 epochs.

## Key Results
- Low-resource languages show weaker and more variable test-time scaling gains compared to high-resource languages
- Models frequently switch to English mid-reasoning even under strict monolingual prompts
- MITT significantly improves test-time scaling performance for underrepresented languages by providing stable inductive priors across linguistic boundaries

## Why This Works (Mechanism)

### Mechanism 1
Initial reasoning tokens (~32) establish inductive priors that cascade into downstream reasoning quality. Early tokens shape the structure of subsequent chain-of-thought; unstable prefixes yield variable long-horizon reasoning. Core assumption: The first few tokens encode reasoning strategy, not just linguistic fluency.

### Mechanism 2
Low-resource languages suffer from weaker internal consistency in initial reasoning, leading to erratic test-time scaling. Models lack stable latent representations for low-resource languages; this causes higher variance in early reasoning, which compounds across generation. Core assumption: Pretraining corpus imbalance creates fragile language grounding in reasoning contexts.

### Mechanism 3
High-resource reasoning prefixes transfer stable inductive priors across languages via lightweight prefix tuning. Prefix tuning on English/high-resource initial thoughts (32 tokens) fine-tunes query/value projections, aligning early reasoning structure across languages without requiring supervised cross-lingual data. Core assumption: Reasoning structure is partially language-independent and can be injected via adapter parameters.

## Foundational Learning

- **Test-Time Scaling**: Why needed here: The entire paper investigates how extending inference-time generation improves reasoning—and why this fails for some languages. Quick check: Can you explain why longer chain-of-thought generation might improve answer accuracy without changing model weights?

- **Prefix Tuning / LoRA Adapters**: Why needed here: MITT uses LoRA on query/value projections to inject reasoning priors efficiently. Quick check: What is the difference between full fine-tuning and adapter-based tuning in terms of trainable parameters and computational cost?

- **Language Fidelity in Chain-of-Thought**: Why needed here: The paper measures "language flipping"—models switching to English mid-reasoning—as a diagnostic of multilingual reasoning failure. Quick check: Why might a model generate reasoning in English even when explicitly prompted in another language?

## Architecture Onboarding

- **Component map**: DeepSeek-R1-Distill-Qwen-7B -> LoRA adapters on q_proj and v_proj layers -> 4-bit quantization -> Causal language modeling loss on first 32 tokens -> Inference pipeline with periodic answer extraction

- **Critical path**: 1) Generate 100 initial reasoning prefixes (32 tokens) per question in high-resource language(s) 2) Fine-tune LoRA adapters on these prefixes (1–3 epochs) 3) At inference, prompt in target language; adapter provides cross-lingual reasoning prior 4) Extract answers at increasing token intervals to measure scaling behavior

- **Design tradeoffs**: English-only prefixes vs. multi-high-resource prefixes: English-only is simpler; multi-source may generalize better but adds complexity. LoRA rank/alpha: Higher rank increases expressiveness but reduces parameter efficiency. Token window for prefix: 32 tokens chosen empirically; longer may capture more structure but risks overfitting.

- **Failure signatures**: Flat or declining accuracy curves with increased generation length (indicates failed test-time scaling). High language-flip rate to English in non-English prompts. Low intra-language similarity in initial prefixes (high variance = unstable priors). Prefix tuning showing no improvement over base model.

- **First 3 experiments**: 1) Replicate test-time scaling curves on base model across 2–3 languages (1 high-resource, 1 low-resource) to confirm language-specific scaling gap 2) Implement MITT with English-only prefixes (100 samples per question, 3 epochs, LoRA r=8) and evaluate on held-out low-resource language 3) Ablate prefix length (16 vs. 32 vs. 64 tokens) to test sensitivity of inductive prior to initial reasoning window size

## Open Questions the Paper Calls Out

- **Do the observed inconsistencies in multilingual test-time scaling and the effectiveness of the MITT intervention generalize to non-Latin script languages?** The study is limited to Latin-script languages and does not test how findings transfer to non-Latin scripts like Arabic, Hindi, or Chinese.

- **Can reasoning prefixes from low-resource languages be effectively transferred to improve performance in other contexts, or is the transfer strictly effective from high-resource to low-resource languages?** The authors have not tested whether reasoning patterns from low-resource languages can also be transferred or reused.

- **To what extent are the disparities in test-time scaling consistency between models (e.g., Llama vs. Qwen) caused by pretraining corpus composition versus post-training instruction tuning?** While the paper correlates scaling behavior with model choice, it does not isolate the specific training stage responsible for the fragile language grounding observed.

## Limitations
- Findings rely heavily on a single, relatively small multilingual dataset (AIME 2025, 30 questions), limiting generalizability
- Unsupervised prefix transfer mechanism is not validated on non-Latin scripts or in zero-shot cross-lingual transfer settings
- Internal consistency metrics based on embedding similarity may not fully capture reasoning quality or robustness

## Confidence

**High confidence**: Claims regarding the presence of test-time scaling gaps across languages, and the efficacy of MITT in improving low-resource language performance within the AIME Multilingual dataset.

**Medium confidence**: Claims about the mechanism—that initial reasoning tokens establish inductive priors and that low-resource languages suffer from lower internal consistency—are well-motivated but not definitively proven.

**Low confidence**: Claims about the universal applicability of MITT to non-Latin scripts or more complex multilingual reasoning tasks are speculative, as the current study does not provide evidence in these areas.

## Next Checks

1. **Dataset Generalization Test**: Validate MITT's effectiveness on a larger, more diverse multilingual reasoning dataset (e.g., XSUM or MMLU) to assess whether gains transfer beyond numerical math.

2. **Mechanism Dissection Experiment**: Perform an ablation study isolating the effect of prefix length (e.g., 16 vs. 32 vs. 64 tokens) and prefix source (English-only vs. multi-high-resource) to more precisely characterize the inductive prior mechanism.

3. **Zero-Shot Cross-Lingual Transfer Test**: Evaluate whether MITT can improve reasoning performance in a target language when only prefixes from a different high-resource language (e.g., using German prefixes to improve Vietnamese) are available, to test the robustness of cross-lingual transfer.