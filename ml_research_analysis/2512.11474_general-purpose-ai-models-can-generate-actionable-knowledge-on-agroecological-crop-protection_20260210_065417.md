---
ver: rpa2
title: General-purpose AI models can generate actionable knowledge on agroecological
  crop protection
arxiv_id: '2512.11474'
source_url: https://arxiv.org/abs/2512.11474
tags:
- efficacy
- data
- reported
- management
- pest
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared the knowledge outputs of two AI models on non-chemical
  pest management. Web-grounded DeepSeek consistently screened 4.8-49.7x more literature
  and reported 1.6-2.4x more solutions than non-grounded ChatGPT, yielding 21.6% higher
  efficacy estimates and greater data consistency.
---

# General-purpose AI models can generate actionable knowledge on agroecological crop protection

## Quick Facts
- arXiv ID: 2512.11474
- Source URL: https://arxiv.org/abs/2512.11474
- Reference count: 0
- This study compared the knowledge outputs of two AI models on non-chemical pest management.

## Executive Summary
This study compared web-grounded DeepSeek and non-grounded ChatGPT on their ability to synthesize agroecological pest management literature. DeepSeek consistently screened 4.8-49.7x more literature and reported 1.6-2.4x more biological control agents than ChatGPT, yielding 21.6% higher efficacy estimates and greater data consistency. Despite some inaccuracies and hallucinations, both models captured correct broad trends, suggesting that with human oversight, LLMs can effectively democratize and accelerate agroecological decision support.

## Method Summary
The study submitted identical structured prompts to DeepSeek-R1 (web-grounded) and ChatGPT-4o free-tier to synthesize literature on 9 crop pests and 5 management tactics. Prompts requested efficacy data (mean ± SD, range) for laboratory, greenhouse, and field conditions globally and for China, Indonesia, and Thailand. Models were asked to generate PRISMA diagrams for subset analyses. Outputs were analyzed using ANOVA, linear regression, and paired t-tests in IBM SPSS 30.0 to compare literature coverage, solution reporting, and efficacy consistency.

## Key Results
- DeepSeek screened 4.8-49.7x more literature than ChatGPT across pest-tactic combinations
- DeepSeek reported 1.6-2.4x more biological control agents than ChatGPT
- DeepSeek achieved 21.6% higher efficacy estimates and notably higher laboratory-to-field consistency (R²=0.838 vs 0.219)

## Why This Works (Mechanism)

### Mechanism 1: Web-Grounded Retrieval Expansion
Web-grounded models access a broader evidence base than non-grounded models, leading to higher coverage of biological control agents. Unlike static models limited by training cutoffs, web-grounded models perform real-time retrieval from indexed databases (e.g., CNKI, Wanfang, Web of Science), allowing the system to screen thousands of records rather than relying on pre-encoded knowledge. The core assumption is that the model can successfully parse, filter, and rank live search results according to the strict inclusion criteria provided in the prompt. Evidence shows DeepSeek screened 4,691 publications for *H. armigera* versus ChatGPT's 95 records.

### Mechanism 2: Structured Reasoning for Data Consistency
Models utilizing step-by-step reinforcement learning achieve higher internal consistency between laboratory and field efficacy data than standard generative models. The model establishes a logical chain where field efficacy expectations are modulated by laboratory benchmarks, maintaining a predictable degradation curve (lab > field). Evidence shows DeepSeek showed notably higher laboratory-to-field consistency (R²=0.838) versus ChatGPT's R²=0.219. This is attributed partly to structured, human-like reasoning capabilities.

### Mechanism 3: Hallucination via Confabulation of Scientific Nomenclature
LLMs confabulate specific biological entities and misapply scientific nomenclature when retrieval fails or training data is ambiguous. Instead of returning "unknown," the model generates the most statistically probable continuation of a scientific name or citation, resulting in plausible but non-existent entities (e.g., *B. tabaci* NPV) or conflating taxonomic synonyms as distinct species. Both models hallucinated fictitious agents or references, with DeepSeek also fabricating non-existent agents like *B. tabaci* NPV.

## Foundational Learning

- **Grounding (RAG vs. Static Training)**: Why needed - The core performance differential stems from whether the model can "browse" the web or relies on pre-2023/24 data. Quick check - Can the model cite a paper published last month? If yes, it is likely using Retrieval-Augmented Generation (RAG) or web access.
- **Laboratory-to-Field Translation**: Why needed - A major evaluation metric is "consistency." You must understand that efficacy typically drops from lab to field due to environmental complexity. Quick check - If a biopesticide works 90% of the time in a petri dish, is a 90% efficacy estimate for an open field realistic or suspicious?
- **Hallucination vs. Factuality**: Why needed - The study shows that high "fluency" hides severe factual errors. You must learn to audit the substance, not just the style. Quick check - The model outputs a perfect-looking citation with a volume, page number, and year. Is it definitely real? (Answer: No, verify the DOI).

## Architecture Onboarding

- **Component map**: Input Prompt -> Retrieval/Reasoning Layer (DeepSeek: Web-grounded + R1 reasoning; ChatGPT: Internal knowledge) -> Synthesis Layer (Data extraction & aggregation) -> Verification Layer (Human-in-the-loop)
- **Critical path**: The prompt engineering. The study used "standardized, elaborate and carefully structured instructions." If the prompt does not explicitly define "efficacy," the model's synthesis will be mathematically meaningless.
- **Design tradeoffs**: Breadth vs. Accuracy - DeepSeek maximizes breadth but still hallucinates; ChatGPT minimizes breadth but misses specifics. Cost vs. Access - Web-grounding requires compute for real-time search; static models are cheaper but outdated.
- **Failure signatures**: Impossible Context - Reporting "reduced tillage" efficacy in a "laboratory" setting. Taxonomic Confusion - Treating old and new names as two different agents, doubling the count. Fabricated Citations - References that look real but do not exist.
- **First 3 experiments**:
  1. Run the same prompt on both a web-grounded and non-grounded model to quantify the "coverage gap."
  2. Ask the model for efficacy data on a fictional pest-agent pair to test for confabulation.
  3. Extract lab and field efficacy means for 10 known agents and plot them to test internal logic.

## Open Questions the Paper Calls Out

- **Open Question 1**: How do LLM-generated efficacy estimates and literature syntheses compare quantitatively to formal, human-conducted systematic reviews and meta-analyses? The authors explicitly state that model performance lacks validation against formal knowledge syntheses and human-run reviews. Without this comparison, the absolute accuracy of the AI remains uncertain.

- **Open Question 2**: To what extent does the performance gap between web-grounded and non-grounded models narrow when using paid, subscription-tier versions or specialized academic plugins? The authors note that free-tier GPT outputs hinted at restricted ISI literature database access and suggest that paid versions might be suitable to capture general trends.

- **Open Question 3**: Can LLMs be fine-tuned or prompted to correctly resolve taxonomic synonymies and distinguish old versus new scientific nomenclatures? The results identify that DeepSeek poorly distinguished between old and new nomenclature, treating them as distinct species, which inflates agent counts.

- **Open Question 4**: Does the ability of LLMs to accurately identify "low-resolution efficacy trends" translate into actionable, safe decision-making for farmers without expert intermediaries? It is unclear if "coarse-grained" accuracy is sufficient for farm-level decisions or if the risk of fabricated fictitious agents poses too high a safety/economic risk for non-expert users.

## Limitations
- Model dependency on web-grounded retrieval may inflate data quality if all screened publications are not verified as peer-reviewed
- Hallucination persistence occurs despite higher coverage, with both models fabricating agents and citations
- Prompt sensitivity may significantly alter coverage and accuracy metrics across different runs

## Confidence
- **High Confidence**: General trend that web-grounded models outperform non-grounded models in literature coverage and solution reporting
- **Medium Confidence**: Specific fold-differences in coverage (4.8-49.7x) and efficacy estimates (21.6% higher) are methodologically sound but may vary with prompt engineering
- **Low Confidence**: Claims about the robustness of lab-to-field consistency (R²=0.838 vs 0.219) and the absence of hallucinations in ChatGPT require independent replication

## Next Checks
1. For 10 randomly selected agents reported by DeepSeek, verify whether cited references exist in Web of Science and whether the agents are biologically plausible
2. Run the same query three times with identical prompts on both models; compare variance in literature counts and efficacy estimates
3. For one pest-tactic pair, extract lab and field efficacy data from both models; calculate lab-to-field correlation (R²) to replicate the consistency analysis