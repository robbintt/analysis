---
ver: rpa2
title: Stacked Regression using Off-the-shelf, Stimulus-tuned and Fine-tuned Neural
  Networks for Predicting fMRI Brain Responses to Movies (Algonauts 2025 Report)
arxiv_id: '2510.06235'
source_url: https://arxiv.org/abs/2510.06235
tags:
- tokens
- brain
- vision
- language
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Stacked Regression using Off-the-shelf, Stimulus-tuned and Fine-tuned Neural Networks for Predicting fMRI Brain Responses to Movies (Algonauts 2025 Report)

## Quick Facts
- arXiv ID: 2510.06235
- Source URL: https://arxiv.org/abs/2510.06235
- Reference count: 40
- Primary result: Stacked regression combining pretrained and fine-tuned models achieves state-of-the-art fMRI prediction performance on movie stimuli

## Executive Summary
This work presents a stacked regression approach for predicting fMRI brain responses to movie stimuli by combining multiple neural network representations. The method integrates audio features from Whisper, text features from Llama-3.1-8B, visual features from InternVL3-8B, and a fine-tuned slow r50 vision model. The approach leverages hemodynamic delay modeling and dimensionality reduction through PCA, with final predictions generated through ridge regression with leave-one-out cross-validation. The system was evaluated on both in-distribution (Friends Season 7) and out-of-distribution movie clips, demonstrating robust performance across different stimulus types.

## Method Summary
The approach combines six models through stacked regression to predict fMRI responses to movie stimuli. Feature extraction includes whisper-small audio (2000 PCA dimensions), Llama-3.1-8B text (500 dimensions), three InternVL3-8B representations (250 dimensions each), and a fine-tuned slow r50 vision model (100 dimensions). All models use a 3-TR hemodynamic delay and stimulus windows of 1-3 TRs. Ridge regression with LOOCV is used for both base models and stacking, with dimensionality reduction via PCA (fit on every 5th sample). The slow r50 model is fine-tuned with LoRA on the last two blocks for 3 epochs. Base models are trained on Friends S1-4,6 plus Hidden Figures, while stacking weights are optimized on Friends S5 plus Bourne and Wolf of Wall Street.

## Key Results
- Stacked regression combining multiple modalities outperforms individual models
- Fine-tuned vision model provides additional predictive power beyond pretrained features
- System performs robustly on both in-distribution (Friends S7) and out-of-distribution movie clips
- Individual subject fine-tuning outperforms joint training across subjects

## Why This Works (Mechanism)
The stacked regression approach works by leveraging complementary information from multiple pretrained models while fine-tuning to task-specific features. Each modality captures different aspects of the movie stimuli - audio models capture acoustic patterns and speech, text models capture semantic content, and vision models capture visual features. The fine-tuned slow r50 model learns stimulus-specific visual representations that align with neural responses. By combining these diverse representations through stacked regression, the model can capture both general and stimulus-specific patterns in brain activity.

## Foundational Learning
- **Hemodynamic delay modeling**: fMRI responses lag neural activity by several seconds due to blood flow changes. Why needed: Aligns model predictions with actual neural responses. Quick check: Verify predicted peak correlations occur at 3-TR delay.
- **Dimensionality reduction with PCA**: Reduces high-dimensional neural network features to manageable size while preserving variance. Why needed: Prevents overfitting and computational burden. Quick check: Monitor explained variance ratio for target dimensions.
- **Ridge regression with LOOCV**: Regularized regression with leave-one-out cross-validation for robust parameter estimation. Why needed: Balances model complexity with generalization. Quick check: Verify alpha parameters are selected appropriately for each output.
- **Fine-tuning with LoRA**: Parameter-efficient adaptation of pretrained models using low-rank adaptations. Why needed: Adapts general visual features to fMRI-specific patterns. Quick check: Monitor training and validation loss curves for overfitting.
- **Stacked regression optimization**: Learning optimal linear combinations of base model predictions. Why needed: Combines complementary strengths of different models. Quick check: Verify stacking weights are stable across cross-validation folds.

## Architecture Onboarding
**Component Map**: whisper-small -> PCA(2000) -> ridge -> output; Llama-3.1-8B -> PCA(500) -> ridge -> output; InternVL3-8B variants -> PCA(250) -> ridge -> output; slow r50 -> LoRA fine-tune -> PCA(100) -> ridge -> output; all models -> stacked ridge -> final prediction

**Critical Path**: Feature extraction -> PCA dimensionality reduction -> ridge regression base models -> stacked regression optimization -> final prediction

**Design Tradeoffs**: 
- Stimulus-tuned vs. general models: Stimulus-tuned models capture specific patterns but may overfit; general models provide broader representations
- Fine-tuning vs. feature extraction: Fine-tuning adapts models to task but requires more data; feature extraction is faster but may miss task-specific patterns
- PCA target dimensions: Higher dimensions capture more information but risk overfitting; lower dimensions are more robust but may lose important features

**Failure Signatures**:
- Overfitting in fine-tuned slow r50: Training accuracy improves but validation/held-out accuracy declines
- Poor stacking performance: One model dominates stacking weights, suggesting redundancy or data leakage
- Suboptimal PCA dimensions: Low explained variance or high reconstruction error

**3 First Experiments**:
1. Train and evaluate individual base models to establish baseline performance
2. Test different stimulus windows (1-3 TRs) and hemodynamic delays to find optimal alignment
3. Compare PCA dimensions for each model to find the right balance between information preservation and overfitting prevention

## Open Questions the Paper Calls Out
None

## Limitations
- Several implementation details remain unspecified, including exact PCA fitting procedures and stacking regression algorithms
- Transcript enhancement pipeline lacks precise prompt templates and processing steps
- Fine-tuning schedule and LoRA parameters may not generalize across different hardware configurations
- Cross-subject models may underperform per-subject models, limiting applicability to individual differences

## Confidence
- **High confidence**: Core methodology of stacked regression combining pretrained models, hemodynamic delay modeling, and cross-validation procedures
- **Medium confidence**: General framework for fine-tuning, PCA dimensionality reduction, and ridge regression implementation
- **Low confidence**: Specific parameters for transcript enhancement, exact PCA fitting procedure, and stacking weight optimization algorithm

## Next Checks
1. Validate PCA procedure by confirming whether fitting was performed on training data only or included held-out validation sets
2. Test stacking algorithm sensitivity by implementing multiple approaches (OLS, constrained optimization) to verify result robustness
3. Benchmark transcript generation impact by comparing model performance using enhanced versus raw transcripts