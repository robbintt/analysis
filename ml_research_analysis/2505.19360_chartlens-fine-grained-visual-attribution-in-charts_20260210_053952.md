---
ver: rpa2
title: 'ChartLens: Fine-grained Visual Attribution in Charts'
arxiv_id: '2505.19360'
source_url: https://arxiv.org/abs/2505.19360
tags:
- chart
- charts
- visual
- attribution
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of visual attribution in chart
  understanding by multimodal large language models (MLLMs), where models often generate
  hallucinations that conflict with visual data. The authors introduce ChartLens,
  a novel algorithm that combines heuristic-guided segmentation with SAM (Segment
  Anything Model) and LineFormer to identify chart elements, then uses set-of-marks
  prompting with MLLMs for fine-grained visual attribution.
---

# ChartLens: Fine-grained Visual Attribution in Charts

## Quick Facts
- arXiv ID: 2505.19360
- Source URL: https://arxiv.org/abs/2505.19360
- Authors: Manan Suri; Puneet Mathur; Nedim Lipka; Franck Dernoncourt; Ryan A. Rossi; Dinesh Manocha
- Reference count: 40
- Primary result: ChartLens achieves 26-66% improvements over baselines across bar, line, and pie charts for fine-grained visual attribution

## Executive Summary
This paper addresses the challenge of visual attribution in chart understanding by multimodal large language models (MLLMs), where models often generate hallucinations that conflict with visual data. The authors introduce ChartLens, a novel algorithm that combines heuristic-guided segmentation with SAM (Segment Anything Model) and LineFormer to identify chart elements, then uses set-of-marks prompting with MLLMs for fine-grained visual attribution. They also present ChartVA-Eval, a benchmark containing 1200+ samples with real-world charts from diverse domains like finance and policy, featuring fine-grained attribution annotations. ChartLens achieves 26-66% improvements over baselines across bar, line, and pie charts, demonstrating robust performance in grounding textual responses to specific visual elements in charts.

## Method Summary
ChartLens operates as a post-hoc attribution system that wraps existing chart QA models to verify their responses. The method uses hybrid segmentation (heuristics + SAM/LineFormer) to isolate chart elements, then overlays alphanumeric labels/masks on each element. The marked image is presented to an MLLM (ChatGPT-4o) with a chain-of-thought prompt to validate the QA pair and identify supporting elements. For bar charts, Otsu thresholding with SAM refinement is used; for pie charts, unrolling and edge detection with SAM; for line charts, LineFormer with equally spaced segments. The system is evaluated on ChartVA-Eval benchmark using precision, recall, F1-score (bars/pies) and detection rate/area detected (lines).

## Key Results
- 26-66% improvement in attribution accuracy across bar, line, and pie charts compared to baselines
- Robust performance on real-world charts from diverse domains including finance, policy, and statistics
- Demonstrates effectiveness of set-of-marks prompting in converting free-form grounding to selection-over-marks task

## Why This Works (Mechanism)

### Mechanism 1
Segmentation-based pre-processing coupled with Set-of-Marks (SoM) visual prompting enables fine-grained visual grounding in charts. ChartLens first isolates chart elements via heuristic-guided methods (contour detection, Otsu thresholding) refined by SAM or LineFormer, then overlays alphanumeric labels/masks on each element. The marked image is presented to an MLLM (ChatGPT-4o) with a chain-of-thought prompt to validate the QA pair and identify supporting elements. This converts free-form grounding into a selection-over-marks task, leveraging explicit visual anchors rather than coordinate regression. Core assumption: The MLLM's ability to read and reference explicit marks (e.g., "element A") is more reliable than regressing bounding boxes directly. Segmentation quality critically determines mark availability.

### Mechanism 2
Post-hoc attribution provides a model-agnostic verification layer, decoupling evidence gathering from answer generation. Given (chart, question, answer), ChartLens attempts to ground the answer to specific regions without modifying the underlying QA model. This modular design allows it to wrap any chart understanding system for auditing. Core assumption: Attribution quality is largely independent of answer correctness; missing or mismatched evidence can signal hallucinations to users.

### Mechanism 3
Hybrid segmentation (heuristics + deep models) improves robustness across diverse chart styles and types. Classical methods (thresholding, contour detection) leverage structural priors; SAM refines masks with point prompts, handling noise and low contrast. LineFormer addresses thin, intersecting lines. This combination aims for broader generalization. Core assumption: Combined strengths cover more chart diversity than any single method; SAM and LineFormer generalize to real-world charts.

## Foundational Learning

- **Set-of-Marks (SoM) Prompting**: Transforms grounding from coordinate regression to mark selection, aligning with MLLM capabilities. Quick check: Can the model reliably select "Bar 3" when asked which marked element supports the answer "3"?

- **Instance Segmentation (e.g., SAM)**: Precise masks are foundational for marks; segmentation errors propagate as attribution errors. Quick check: Can you isolate individual bars in a grouped bar chart with touching bars?

- **Post-hoc Attribution vs Joint Generation**: Clarifies that ChartLens adds a verification layer without fixing the base QA model. Quick check: If the QA model gives a wrong answer, should ChartLens return "no attribution" or attempt to find regions?

## Architecture Onboarding

- **Component map**: Input (Chart, Question, Answer) → Chart Type Router → Segmentation Module (Heuristic Preprocessor + SAM/LineFormer Refiner) → Mark Generator → MLLM Attributor → Evaluator

- **Critical path**: Segmentation quality → Mark clarity → MLLM mark reading → Attribution accuracy. Segmentation failures cascade downstream.

- **Design tradeoffs**: SoM vs Direct Bounding Box: SoM simplifies MLLM task but adds segmentation dependency. Heuristics vs Pure Deep Segmentation: Heuristics are fast but brittle; deep models add robustness at compute cost. Line Segmentation Granularity: Fine segments enable precise attribution but may fragment trend evidence.

- **Failure signatures**: Missing marks (undetected elements); spurious marks (noise segmented as elements); hallucinated attribution (MLLM ignores marks); over-segmentation (single element split into multiple marks).

- **First 3 experiments**:
  1. Segmentation Ablation: Replace SAM with heuristic-only segmentation; measure attribution F1 drop.
  2. Prompt Ablation: Remove few-shot examples or CoT; measure precision/recall sensitivity.
  3. Cross-Dataset Generalization: Segment on synthetic (PlotQA), test attribution on real-world (ChartQA); quantify domain gap.

## Open Questions the Paper Calls Out

1. **Can textual chart components (titles, captions, axis labels) be integrated into the attribution framework to improve verification accuracy?**
   - The current approach explicitly filters for visual elements and treats text as background noise to be suppressed by segmentation models. Evidence to resolve: An ablation study measuring attribution performance on charts where textual OCR data is explicitly included as candidate marks versus the current visual-only approach.

2. **To what extent does the Set-of-Marks prompting strategy generalize to complex or unstructured visual data beyond standard charts?**
   - The method is currently validated only on bar, line, and pie charts, which possess clear geometric boundaries. Evidence to resolve: Benchmark results applying ChartLens to datasets of infographics or scientific figures, measuring the model's ability to generate precise attributions in less structured domains.

3. **What is the sensitivity of the final attribution accuracy to errors in the upstream segmentation masks?**
   - It remains unclear if the attribution errors observed are driven by MLLM reasoning failures or by noise introduced during the SAM/LineFormer segmentation phase. Evidence to resolve: A sensitivity analysis plotting attribution F1-scores against varying quality levels (simulated IoU thresholds) of the input segmentation masks.

## Limitations

- The evaluation relies heavily on synthetic charts (846/1200 samples), which may not fully represent real-world chart diversity.
- The method doesn't address multi-chart scenarios or charts with embedded text annotations, which are common in practice.
- The paper assumes MLLMs can reliably read and reference explicit marks, but this assumption is not rigorously tested.

## Confidence

- **ChartLens algorithm effectiveness**: High (well-supported by benchmark results)
- **SoM prompting mechanism**: Medium (mechanistically sound but limited empirical validation)
- **Post-hoc attribution benefits**: Medium (conceptually clear but not fully validated in real-world deployment)
- **Hybrid segmentation robustness**: Medium (claimed but not thoroughly tested across extreme chart variations)

## Next Checks

1. **Cross-model generalization test**: Apply ChartLens to attributions from different MLLM chart QA systems (not just ChatGPT-4o) and measure consistency in hallucination detection rates.

2. **Real-world deployment audit**: Deploy ChartLens on 100+ charts from actual business/financial reports and measure how often it successfully identifies hallucination versus providing false confidence.

3. **Failure mode characterization**: Systematically test ChartLens on charts with known failure patterns (3D charts, gradient fills, complex legends) and quantify performance degradation relative to ideal conditions.