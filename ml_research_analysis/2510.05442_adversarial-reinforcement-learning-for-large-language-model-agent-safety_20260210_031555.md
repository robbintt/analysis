---
ver: rpa2
title: Adversarial Reinforcement Learning for Large Language Model Agent Safety
arxiv_id: '2510.05442'
source_url: https://arxiv.org/abs/2510.05442
tags:
- agent
- arlas
- attacker
- learning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Adversarial Reinforcement Learning for Large Language Model Agent Safety

## Quick Facts
- arXiv ID: 2510.05442
- Source URL: https://arxiv.org/abs/2510.05442
- Reference count: 20
- Primary result: ARLAS reduces Attack Success Rate (ASR) from 6.3% to 5.4% while maintaining Task Success Rate (TSR) at 73.7% on BrowserGym benchmarks

## Executive Summary
This paper introduces ARLAS, an adversarial reinforcement learning framework for enhancing the safety of LLM-based web agents against indirect prompt injections. The method employs co-evolutionary training between an attacker and agent in a zero-sum Markov game, where the attacker generates prompt injections to induce information leakage while the agent learns to defend while completing web tasks. The framework combines imitation learning warmup with population-based adversarial reinforcement learning using Group Relative Policy Optimization (GRPO). Experimental results on BrowserGym and AgentDojo benchmarks demonstrate that ARLAS significantly reduces attack success rates while maintaining task completion capabilities.

## Method Summary
ARLAS implements a two-player zero-sum Markov game where an attacker LLM generates token sequences enclosed in `<action>...</action>` tags that are injected into HTML observations, while an agent LLM must complete web tasks without leaking synthetic user information (passwords, emails, addresses). The training process consists of two stages: (1) Imitation learning warmup using 10K episodes from teacher models (Gemma-3-27B, Qwen-3-32B) filtered for successful episodes, followed by supervised fine-tuning with LoRA adapters (rank=128, α=256); (2) Population-based adversarial reinforcement learning with GRPO where the agent trains against uniformly sampled attackers from all prior iterations. The attacker receives +1 reward for successful information leakage and -1 for task success, while the agent receives opposite rewards. The framework uses LoRA fine-tuning on all linear layers with temperature 1.2 for exploration and KL regularization (β=0.05) to constrain distribution drift.

## Key Results
- ARLAS reduces Attack Success Rate from 6.3% to 5.4% while maintaining Task Success Rate at 73.7%
- Population-based learning improves ASR from 41% (single attacker) to 34% (population sampling)
- Co-evolution generates diverse attacks with cumulative internal diversity increasing from ~0.89 to ~0.95 average pairwise distance
- ARLAS generalizes to AgentDojo benchmarks, reducing ASR from 6.3% to 5.4% without task-specific training

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Co-evolution of attacker and agent in a zero-sum game produces diverse, novel attacks that static datasets cannot capture.
- **Mechanism**: The attacker receives +1 reward when user information is leaked and -1 when the task succeeds; gradient updates push it toward unexplored attack patterns that succeed against the current agent. Diversity emerges because previously successful patterns are gradually learned by the agent, forcing the attacker to explore new strategies.
- **Core assumption**: The attack space is sufficiently high-dimensional that gradient-based exploration discovers meaningfully distinct strategies rather than converging to a single mode.
- **Evidence anchors**:
  - [abstract] "our analysis further confirms that the adversarial process generates a diverse and challenging set of attacks"
  - [section 4.3, Figure 5] Average pairwise distance of attack embeddings increases from ~0.89 (original) to ~0.95 (RL iter 10), measured via cumulative internal diversity
  - [corpus] AdvEvo-MARL (arXiv:2510.01586) reports similar co-evolution producing internalized safety, though in multi-agent rather than single-agent settings
- **Break condition**: If attacker diversity plateaus early (APD curve flattens) or embeddings cluster tightly despite continued training, co-evolution has collapsed into mode-seeking behavior.

### Mechanism 2
- **Claim**: Population-based learning prevents catastrophic forgetting of defenses against earlier attack strategies.
- **Mechanism**: At iteration i, the agent trains against uniformly sampled attackers from {π¹_atk, ..., πⁱ_atk}. This exposes the agent to the full historical distribution of attack patterns rather than only the current attacker's specializations. The objective integrates defense against all previous modes.
- **Core assumption**: Attacks discovered at earlier iterations represent distinct, transferable patterns rather than artifacts of a specific random seed or task subset.
- **Evidence anchors**:
  - [section 3.3] "for each episode, an opponent is uniformly sampled from the existing population of attackers"
  - [section 4.2, Figure 4] ARLAS w/o PBL achieves 41% attack success rate (average) vs. 34% for full ARLAS when evaluated against all attackers
  - [corpus] No direct corpus comparison for population-based training in LLM agents; nearest analog is Fictitious Self-Play (Heinrich & Silver, 2016) in game theory, not LLM safety
- **Break condition**: If agent performance against early attackers degrades significantly in later iterations (off-diagonal performance drops in heatmaps), population sampling is not maintaining historical defenses.

### Mechanism 3
- **Claim**: Imitation learning warmup provides sufficient policy initialization to enable meaningful RL exploration.
- **Mechanism**: Larger teacher models (Gemma-3-27B, Qwen-3-32B) generate demonstration episodes; student models (12B, 14B) are fine-tuned on successful trajectories via supervised loss with KL regularization. This bootstrap produces non-zero success rates, yielding reward variance that GRPO can exploit for credit assignment.
- **Core assumption**: Teacher models' successful strategies are learnable by smaller student architectures and do not induce harmful distribution shift.
- **Evidence anchors**:
  - [section 3.2] "After SFT, both models achieve non-zero success rates, providing a strong starting point for the subsequent adversarial reinforcement learning stage"
  - [section B.1] 10K episodes collected, filtered to retain only successful episodes
  - [corpus] No corpus evidence on warmup necessity for adversarial LLM training specifically
- **Break condition**: If post-SFT attack or task success rates remain near zero (<5%), RL credit assignment will be dominated by noise; re-evaluate teacher capability or increase demonstration scale.

## Foundational Learning

- **Markov Games (Two-Player Zero-Sum)**
  - Why needed here: ARLAS formulates agent-attacker interaction as a Markov game with shared state but opposing rewards. Understanding the tuple (S, A_atk, A_agt, R_atk, R_agt, T) is essential for implementing the co-training loop.
  - Quick check question: Can you explain why R_atk = +1 and R_agt = -1 for the same terminal condition (information leakage)?

- **Group Relative Policy Optimization (GRPO)**
  - Why needed here: ARLAS uses GRPO for credit assignment across multi-turn trajectories with sparse rewards. The algorithm computes group-relative advantages rather than absolute returns.
  - Quick check question: Given G episodes per task with final rewards {r¹_T, ..., r^G_T}, write the advantage formula for the j-th token of step t in episode g.

- **LoRA Fine-Tuning**
  - Why needed here: Training uses LoRA (rank=128, α=256) on all linear layers for efficiency. Understanding adapter mechanics helps debug gradient flow issues.
  - Quick check question: If training loss plateaus but validation attack success rate keeps improving, what might this indicate about LoRA capacity vs. base model freezing?

## Architecture Onboarding

- **Component map**: Teacher models -> BrowserGym environment -> Agent model -> Tool calls -> Reward oracle -> Attacker model -> Prompt injections -> Agent model

- **Critical path**:
  1. Collect imitation data from teacher models → filter for success → SFT warmup (both models)
  2. For each RL iteration: sample attacker from population → collect G episodes per task → compute group-relative advantages → update via GRPO objective with KL penalty
  3. Checkpoint both models; add attacker to population pool
  4. Evaluate on held-out BrowserGym tasks and AgentDojo

- **Design tradeoffs**:
  - Population size vs. compute: Storing all attacker checkpoints increases memory; paper uses all prior checkpoints but truncation could reduce cost at risk of forgetting
  - Temperature (1.2) enables exploration but may reduce precision on structured tool calls
  - KL coefficients (β_SFT = 0.05, β_RL = 0.05) constrain drift from initialization; higher values trade adaptivity for stability

- **Failure signatures**:
  - Attack success rate fails to decrease across iterations → attacker too weak; increase imitation data or teacher model size
  - Task success rate drops while attack success rate drops → agent over-optimizing for safety; adjust reward balance
  - GRPO advantage variance near zero → episode group too homogeneous; increase temperature or group size

- **First 3 experiments**:
  1. Ablate population-based learning: Train with only the latest attacker (ARLAS w/o PBL). Expect higher attack success rate and reduced generalization to AgentDojo.
  2. Scale study: Train Gemma-3-12B vs. Qwen-3-14B on identical tasks. Compare final attack/task success rates to assess sensitivity to base model capability.
  3. Cross-benchmark transfer: Take agent trained on BrowserGym, evaluate zero-shot on AgentDojo without adaptation. Confirm ASR reduction holds (Table 1 shows 6.3% → 5.4% for Gemma).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does the ARLAS framework maintain its safety and capability performance trade-offs when scaled to larger proprietary or open-source LLMs (e.g., 70B+ parameters)?
- **Basis in paper**: [explicit] The authors explicitly state in the conclusion that a limitation is having "only verified the effectiveness of ARLAS on two open-source LLMs" due to computational constraints, identifying evaluation on larger-scale LLMs as a future direction.
- **Why unresolved**: The current study is restricted to Gemma 3 12B and Qwen 3 14B; the dynamics of adversarial RL might change significantly with models possessing greater capacity or different pre-training distributions.
- **What evidence would resolve it**: Evaluation of ARLAS on larger models (e.g., Llama 3 70B) demonstrating consistent reductions in Attack Success Rate (ASR) without degradation in Task Success Rate (TSR).

### Open Question 2
- **Question**: Can the ARLAS framework be extended to defend Vision Language Models (VLMs) against visual prompt injections?
- **Basis in paper**: [explicit] The conclusion identifies the extension of the framework to generate visual prompt injections for VLM-based agents as a "promising future direction" given the rising prominence of VLMs in agent development.
- **Why unresolved**: The current implementation and experiments focus exclusively on text-based indirect prompt injections within Accessibility Trees; visual inputs introduce a different modality of attack surface.
- **What evidence would resolve it**: A modified ARLAS pipeline training a VLM agent against an attacker that generates adversarial visual overlays, showing improved robustness on visual agent benchmarks.

### Open Question 3
- **Question**: Does the defense learned via string-matching leakage rewards generalize to semantic or encoded data exfiltration?
- **Basis in paper**: [inferred] The paper defines information leakage as occurring "if the agent’s response... contains any subset of the user’s private information," implying a reliance on exact string matching.
- **Why unresolved**: Real-world attackers may use encoding (e.g., base64) or paraphrasing to exfiltrate data without triggering an exact string match, a scenario the current reward signal may not optimize against.
- **What evidence would resolve it**: Testing the ARLAS-trained agent against attacks instructing it to obfuscate or paraphrase the leaked data, rather than outputting the secret string verbatim.

## Limitations

- Population-based training lacks rigorous theoretical grounding for maintaining defenses against early attackers, with no analysis of conditions under which historical patterns remain relevant
- Synthetic user information creates an artificial attack surface that may not generalize to real-world agent environments where PII detection is more complex
- Co-evolution mechanism depends on high-dimensional attack space assumptions without analysis of whether gradient exploration discovers orthogonal attack vectors or perturbs existing ones

## Confidence

- **High confidence**: SFT warmup enables RL learning (verified by non-zero post-warmup success rates), population-based learning improves ASR over single-opponent training (demonstrated by ablation in Figure 4)
- **Medium confidence**: Co-evolution produces diverse attacks (supported by APD metrics but lacking per-iter qualitative analysis), ARLAS generalizes to AgentDojo (ASR reduction from 6.3% to 5.4% observed but zero-shot transfer mechanism unclear)
- **Low confidence**: Agent defense mechanism transferability (no analysis of which specific attack patterns the agent learns to counter), scalability claims beyond tested model sizes (12B/14B), real-world applicability given synthetic PII assumptions

## Next Checks

1. **Diversity validation**: Perform t-SNE visualization of attacker embeddings at iterations 1, 5, and 10 to qualitatively verify that new attackers occupy distinct regions rather than perturbing existing clusters. Correlate with ASR trends to confirm diversity drives attack effectiveness.

2. **Historical defense stability**: After training iteration 15, evaluate the agent against each attacker checkpoint individually (not just population sampling). Plot ASR vs. attacker iteration to verify defenses against early attackers persist and identify any catastrophic forgetting patterns.

3. **Zero-shot robustness test**: Train the agent on a subset of BrowserGym tasks, then evaluate on a disjoint subset without further training. Compare ASR reduction to the full-task training results to quantify transfer efficiency and identify task characteristics that enable generalization.