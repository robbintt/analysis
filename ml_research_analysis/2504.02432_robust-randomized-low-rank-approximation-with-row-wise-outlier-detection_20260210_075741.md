---
ver: rpa2
title: Robust Randomized Low-Rank Approximation with Row-Wise Outlier Detection
arxiv_id: '2504.02432'
source_url: https://arxiv.org/abs/2504.02432
tags:
- rows
- outlier
- robust
- error
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a scalable, non-iterative randomized algorithm
  for robust low-rank approximation under row-wise adversarial corruption. The method
  first reduces dimensionality via a Johnson-Lindenstrauss projection, then uses robust
  statistics (median and MAD) to detect and remove corrupted rows based on their projected
  norms.
---

# Robust Randomized Low-Rank Approximation with Row-Wise Outlier Detection

## Quick Facts
- **arXiv ID:** 2504.02432
- **Source URL:** https://arxiv.org/abs/2504.02432
- **Reference count:** 13
- **One-line primary result:** Introduces a scalable, non-iterative randomized algorithm for robust low-rank approximation under row-wise adversarial corruption.

## Executive Summary
This paper presents a one-pass, randomized algorithm for robust low-rank matrix approximation in the presence of row-wise adversarial outliers. The method leverages Johnson-Lindenstrauss dimensionality reduction to preserve the geometry of clean rows, then uses robust statistics (median and MAD) to detect and filter out corrupted rows based on their projected norms. Theoretical analysis shows the algorithm achieves near-optimal approximation error with high probability, provided a sufficient norm gap exists between clean and adversarial rows. Experiments demonstrate effective outlier detection and linear runtime scaling on both synthetic and real datasets.

## Method Summary
The algorithm operates in three stages: (1) Apply a Johnson-Lindenstrauss projection to compress the data matrix, (2) Compute row norms in the projected space and apply median/MAD-based thresholding to identify outliers, (3) Run randomized SVD on the filtered matrix to obtain a low-rank approximation. The sketch dimension scales logarithmically with the number of rows, enabling efficient processing of large datasets. The method requires the outlier fraction to be less than 50% and assumes a sufficient separation (norm gap) between clean and adversarial rows.

## Key Results
- Achieves near-optimal low-rank approximation error with high probability under norm gap conditions
- Runtime scales linearly with the number of rows, enabling efficient processing of large datasets
- Typical relative errors on inliers below 1% with substantial speedups over robust alternatives
- Effective outlier detection confirmed on both synthetic and real datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Dimensionality reduction via random projection preserves the norm hierarchy between clean and adversarial rows, enabling outlier detection in significantly lower dimensions.
- **Mechanism:** A Johnson-Lindenstrauss (JL) transform $\Psi$ projects rows from $R^n$ to $R^s$. With sufficient sketch size $s$, this preserves the Euclidean norms of clean rows within a $(1 \pm \epsilon)$ factor with high probability. Since adversarial rows are assumed to have significantly larger norms, this relative gap is maintained in the projected space.
- **Core assumption:** The sketch dimension $s$ must satisfy $s = O(\frac{1}{\epsilon^2} \log \frac{m}{\delta'})$, and the noise $N$ on clean rows must be bounded.
- **Evidence anchors:**
  - [Abstract] "compressing the data with a Johnson Lindenstrauss projection... preserves the geometry of clean rows"
  - [Page 4, Lemma 3.1] Proves $(1-\epsilon)\|B_{i,:}\|^2 \le \|(A\Psi)_{i,:}\|^2 \le (1+\epsilon)\|B_{i,:}\|^2 + C\delta^2$.
  - [Corpus] Related work "Sketch-and-Precondition" validates sketching for linear algebra speedups, though this paper applies it specifically for norm preservation in outlier detection.

### Mechanism 2
- **Claim:** Robust scalar statistics (Median and MAD) on projected row norms provide a stable threshold for separating inliers from adversarial rows, provided the outlier fraction is $< 50\%$.
- **Mechanism:** Adversarial rows have large norms, skewing the mean but not the median (up to $\alpha < 0.5$). The algorithm computes the median $\hat{\mu}$ and Median Absolute Deviation (MAD) $\hat{\sigma}$ of the projected norms. A threshold $\tau = \hat{\mu} + c\hat{\sigma}$ is set. Rows exceeding this are discarded.
- **Core assumption:** The outlier fraction $\alpha < 0.5$ (breakdown point of median) and the "norm gap" $\gamma$ (separation between max inlier norm and min outlier norm) is sufficiently large.
- **Evidence anchors:**
  - [Page 3] "Since $\alpha < 0.5$, the median-based estimator remains stable in the face of adversarial contamination."
  - [Page 7, Lemma 3.2] Derives the probability bounds for correct classification based on the separation condition.
  - [Corpus] Corpus evidence for this specific scalar thresholding mechanism is weak; neighbors focus on tensor decomposition or deep learning robustness rather than median/MAD norm filtering.

### Mechanism 3
- **Claim:** Filtering outliers prior to decomposition yields a near-optimal low-rank approximation of the clean underlying matrix.
- **Mechanism:** Standard SVD is sensitive to large-magnitude outliers. By removing rows flagged by the threshold $\tau$, the remaining submatrix $\tilde{A}$ consists primarily of clean rows plus bounded noise. A rank-$k$ approximation on this filtered set recovers the clean structure $B$ without the bias introduced by adversarial rows.
- **Core assumption:** The clean matrix $B$ is approximately low-rank ($k$), and the false positive rate (clean rows removed) is small enough that the subspace structure remains intact.
- **Evidence anchors:**
  - [Page 9, Theorem 4.1] Bounds the error $\|B - \hat{B}\|_F \le (1+\epsilon)\|B - B_k\|_F + \eta$.
  - [Page 14, Table 2] Shows Standard PCA error $>0.20$ vs Robust LRA error $0.006$ under $\alpha=0.2$.
  - [Corpus] "Outlier-aware Tensor Robust PCA" similarly argues for separating low-rank structure from outliers, supporting the general decomposition strategy.

## Foundational Learning

- **Concept: Johnson-Lindenstrauss (JL) Lemma**
  - **Why needed here:** This is the engine that makes the algorithm "one-pass" and scalable. Without understanding that random projections preserve vector norms/distances, it is unclear why we can detect high-dimensional outliers in a low-dimensional sketch.
  - **Quick check question:** If you reduce dimension $n \to s$ using a Gaussian matrix, does the ratio $\|Ax\| / \|x\|$ concentrate around 1?

- **Concept: Median Absolute Deviation (MAD)**
  - **Why needed here:** Standard deviation is sensitive to outliers. To find a "robust" threshold for outliers, we need a measure of spread that ignores the outliers themselves. MAD provides this robust scale estimate.
  - **Quick check question:** Why is the MAD multiplied by $\approx 1.4826$ when used as a consistent estimator for the standard deviation of Gaussian data?

- **Concept: Adversarial vs. Random Noise**
  - **Why needed here:** The paper distinguishes between bounded noise $N$ (random) and adversarial corruption (structured, large magnitude). The algorithm is designed to be "robust" to the latter, not just denoise the former.
  - **Quick check question:** In the model $A = B + N$, does the method require $N$ to be sparse (many zeros) or just bounded in $\ell_2$ norm?

## Architecture Onboarding

- **Component map:** Input $A \in \mathbb{R}^{m \times n}$ -> JL Projection $\Psi \in \mathbb{R}^{n \times s}$ -> Row Norms $r_i = \|(A\Psi)_{i,:}\|_2$ -> Threshold Estimator ($\tau = \text{Median}(r) + c \times 1.4826 \times \text{MAD}(r)$) -> Filter ($I = \{i \mid r_i \le \tau\}$) -> Randomized SVD on $A_{I,:}$

- **Critical path:** The sketching step (multiplication $A\Psi$) is the computational bottleneck, scaling as $O(m \cdot s \cdot \text{nnz})$. The subsequent SVD operates on a reduced set of rows but full columns (or a second sketch).

- **Design tradeoffs:**
  - **Sketch dimension $s$:** Larger $s$ improves norm estimation accuracy (lower $\epsilon$) but increases memory and time.
  - **Threshold constant $c$:** Higher $c$ reduces false positives (keeps more data) but risks retaining adversarial rows. Default $c=3$.
  - **Projection Type:** Sparse Rademacher is faster for dense data; Gaussian is more theoretically stable but slower.

- **Failure signatures:**
  - **Norm Camouflage:** Outliers have norms similar to inliers (small $\gamma$). The histogram of $r_i$ will look unimodal, and thresholding fails.
  - **Median Swamping:** If $\alpha > 0.5$, the median shifts to the outlier cluster, and the algorithm deletes the clean data instead.
  - **Dimensionality Collapse:** If $n$ is small but $m$ is huge, sketching adds variance without much computational benefit; direct methods might be better.

- **First 3 experiments:**
  1. **Baseline Validation:** Generate synthetic $A$ with known rank-$k$ and $\alpha=0.2$. Plot histogram of projected norms to visually confirm the "gap" $\gamma$ and verify alignment with threshold $\tau$.
  2. **Scaling Test:** Fix $n, k$, and vary $m$ from $10^3$ to $10^6$. Verify linear runtime scaling of the $A\Psi$ multiplication step.
  3. **Sensitivity Analysis:** Sweep the threshold constant $c \in [2.0, 4.0]$ and outlier fraction $\alpha \in [0.1, 0.4]$. Plot the trade-off curve between Reconstruction Error (on inliers) and Recall (detecting outliers).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can an iterative reweighting scheme provably recover low-rank structure when the norm gap $\gamma$ is negligible?
- **Basis in paper:** [explicit] Section 6.3 proposes "a multi-pass or iterative reweighting approach" to relax the strong separation assumption.
- **Why unresolved:** Current guarantees (Lemma 3.2) strictly require a large norm gap for single-pass thresholding.
- **What evidence would resolve it:** Convergence proofs for a multi-pass variant where $\gamma=0$, or empirical recovery on datasets with indistinguishable outlier norms.

### Open Question 2
- **Question:** Do theoretical robustness guarantees hold when using trimmed estimators for heavy-tailed projected norms?
- **Basis in paper:** [inferred] Section 6.2 lists sub-Gaussian norms as a limitation; Section 6.3 suggests trimmed estimators as a mitigation.
- **Why unresolved:** The probability bounds rely on sub-Gaussian concentration (Lemma 3.2); standard trimmed estimators may lack the necessary concentration properties.
- **What evidence would resolve it:** Derivation of error bounds under finite-variance assumptions using trimmed statistics, or empirical validation on heavy-tailed noise distributions.

### Open Question 3
- **Question:** Can the method detect partial row corruptions (sparse adversarial entries) without losing linear time complexity?
- **Basis in paper:** [explicit] Section 6.3 notes ongoing work on "integrating partial outlier pursuit" for rows that are only partially corrupted.
- **Why unresolved:** Partial corruption may not inflate the row's $\ell_2$ norm sufficiently to trigger the proposed robust threshold.
- **What evidence would resolve it:** Extension of the detection logic to handle entry-wise sparsity within the sketch, or modified theoretical bounds for partial corruption models.

## Limitations
- Theoretical analysis relies on a "norm gap" assumption that may not hold in many real-world scenarios
- Algorithm's breakdown point at 50% outlier fraction is restrictive for extremely corrupted datasets
- Method focuses exclusively on row-wise corruption, leaving column-wise or element-wise corruption unaddressed
- Computational complexity benefits diminish when the clean matrix has many more rows than columns

## Confidence
- **High confidence:** Dimensionality reduction preserves norm hierarchy under JL transform (mechanisms 1 and 3)
- **Medium confidence:** Median/MAD thresholding reliably separates inliers from outliers (mechanism 2)
- **Medium confidence:** Near-optimal low-rank recovery on filtered data (mechanism 3)

## Next Checks
1. **Robustness to camouflage attacks:** Generate synthetic data where adversarial rows have similar norms to inliers but different subspace structure to test detection limits when the norm gap γ is small or absent.
2. **Threshold sensitivity analysis:** Systematically sweep the threshold constant c across multiple α values to quantify the precision-recall trade-off and identify the optimal parameter setting for different corruption levels.
3. **Scalability validation:** Benchmark runtime scaling from m=10³ to m=10⁶ rows while varying the sketch dimension s to empirically verify the claimed linear scaling and identify the point where sketching overhead outweighs benefits.