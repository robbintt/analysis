---
ver: rpa2
title: 'The Forward-Forward Algorithm: Characterizing Training Behavior'
arxiv_id: '2504.11229'
source_url: https://arxiv.org/abs/2504.11229
tags:
- layer
- accuracy
- layers
- which
- forward-forward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the training behavior of Forward-Forward
  networks, an alternative to backpropagation that uses layer-local loss functions.
  The study examines how layer depth affects training dynamics and how individual
  layer accuracy correlates with overall model performance.
---

# The Forward-Forward Algorithm: Characterizing Training Behavior

## Quick Facts
- arXiv ID: 2504.11229
- Source URL: https://arxiv.org/abs/2504.11229
- Reference count: 9
- Primary result: Forward-Forward networks exhibit depth-dependent training delays and weakening correlation between individual layer accuracy and overall model performance

## Executive Summary
This paper investigates the training behavior of Forward-Forward networks, an alternative to backpropagation that uses layer-local loss functions. The study examines how layer depth affects training dynamics and how individual layer accuracy correlates with overall model performance. Experiments vary the number of hidden layers, layer dimensions, and training epochs across MNIST image classification tasks. Results show that deeper layers experience a delay in accuracy improvement compared to shallower layers, with the delay being more pronounced in wider layers. Additionally, shallower layer accuracy strongly correlates with overall model accuracy, while this correlation weakens for deeper layers. The findings suggest that Forward-Forward networks exhibit distinct training characteristics compared to backpropagation-based models, with implications for network design and incremental construction.

## Method Summary
The paper trains Forward-Forward networks on MNIST with labels encoded in border pixels. Each layer independently maximizes a "goodness" function (sum of squared activations) for positive samples and minimizes it for negative samples. The study varies hidden layer count (2-32 layers) and dimensions (100-1000) across 30 seeds. Key metrics include layer accuracy (based on thresholded goodness), overall classification accuracy, and Pearson/Spearman correlations between layer and model accuracy.

## Key Results
- Deeper layers experience delayed accuracy improvement compared to shallower layers
- Delay is more pronounced in wider layers (1000 vs 100 dimensions)
- Shallow layer accuracy strongly correlates with overall model accuracy, while this correlation weakens for deeper layers
- Layer accuracy correlation drops from ~0.9 at layer 0 to ~0.3-0.5 at layer 30

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Layer-local goodness optimization enables training without backward gradient propagation
- Mechanism: Each layer independently maximizes a "goodness" function (sum of squared activations) for positive samples and minimizes it for negative samples, using a threshold-based sigmoid to classify responses. Peer normalization prevents individual nodes from dominating.
- Core assumption: Local optimization signals provide sufficient learning signal without global error propagation
- Evidence anchors:
  - [abstract] "Forward-Forward networks employ layer local loss functions which are optimized based on the layer activation for each forward pass rather than a single global objective function"
  - [Page 1] Describes goodness as "sum of squared activities" with threshold θ and peer normalization at 0.03
  - [corpus] NetworkFF paper confirms conventional FF implementations suffer from "inter-layer isolation, where layers optimize goodness functions independently"

### Mechanism 2
- Claim: Deeper layers experience delayed accuracy improvement due to sequential information cascade
- Mechanism: In feedforward FF networks, each layer receives inputs only from preceding layers. Since layers train independently without backward signals, deeper layers must wait for shallower layers to produce better representations before they can improve.
- Core assumption: Layer training depends primarily on input quality from preceding layers
- Evidence anchors:
  - [abstract] "deeper layers experience a delay in accuracy improvement compared to shallower layers, with the delay being more pronounced in wider layers"
  - [Page 5, Figure 4] Shows epoch at which layers achieve 0.7 target accuracy—positive slope indicates deeper layers reach targets later
  - [corpus] Related work on "Gradient-isolated learning" (Löwe et al.) supports that subsequent layers have no impact on preceding layers

### Mechanism 3
- Claim: Shallow layer accuracy strongly predicts overall model performance; correlation weakens with depth
- Mechanism: Early layers process raw input and their representations propagate through all downstream layers. Their accuracy captures fundamental feature quality that determines final output. Deeper layers depend on this quality and their individual accuracy is less diagnostic of overall performance.
- Core assumption: Classification decisions are more sensitive to early-layer feature quality than deep-layer refinements
- Evidence anchors:
  - [abstract] "shallower layer accuracy strongly correlates with overall model accuracy, while this correlation weakens for deeper layers"
  - [Page 7, Figure 5] Pearson correlation drops from ~0.9 at layer 0 to ~0.3-0.5 at layer 30; Spearman correlation also declines but less dramatically
  - [corpus] Weak direct corpus support—corpus papers focus on algorithm variants rather than layer-depth correlation analysis

## Foundational Learning

- Concept: **Layer-local loss functions**
  - Why needed here: FF networks abandon global objectives; understanding local optimization is prerequisite for debugging training
  - Quick check question: Can you explain why a layer could achieve high local accuracy yet the overall model performs poorly?

- Concept: **Goodness functions (sum of squared activities)**
  - Why needed here: This is the core signal each layer optimizes; choosing alternative goodness functions changes training dynamics
  - Quick check question: What would happen if you used a different goodness function, such as max activation instead of sum of squares?

- Concept: **Positive/negative sample generation**
  - Why needed here: FF learning requires explicit contrastive pairs; incorrect negative sample design breaks the algorithm
  - Quick check question: How does the paper generate negative samples for MNIST classification?

## Architecture Onboarding

- Component map:
  Input layer (MNIST images with label in border) -> Hidden layers (ReLU + peer normalization) -> Goodness computation (sum of squared activations) -> Classification via thresholded goodness

- Critical path:
  1. Encode labels into input pixels (positive sample)
  2. Generate negative sample with incorrect label encoding
  3. Forward pass positive sample → maximize layer goodness
  4. Forward pass negative sample → minimize layer goodness
  5. Measure layer accuracy (correct threshold responses)
  6. Track correlation between layer accuracy and overall model accuracy

- Design tradeoffs:
  - **Layer width vs. delay**: Wider layers (1000 vs 100 activations) show more pronounced delay in deep layers (Figure 4)
  - **Depth vs. correlation**: Deeper networks maintain strong shallow-layer correlation but deep-layer correlation degrades
  - **Training time vs. depth**: Deeper networks require more epochs for deep layers to reach target accuracy

- Failure signatures:
  - Deep layers stuck at ~0.5 accuracy (random threshold response): Input representations from preceding layers insufficient
  - High shallow-layer accuracy but low overall accuracy: Final layer or label encoding failing
  - Excessive variance across seeds (Figure 3): Initialization sensitivity—check peer normalization settings

- First 3 experiments:
  1. Replicate 4-layer, 1000-dimension baseline on MNIST for 30 epochs; verify layer accuracy curves match Figure 2 pattern (shallow layers rise first)
  2. Ablate layer width (100 vs 1000 dimensions) with fixed depth=8; measure epoch at which layer 7 reaches 0.7 accuracy to confirm width-delay interaction
  3. Compute Pearson correlation per layer across training epochs; verify correlation drops with depth as in Figure 5a

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the strong correlation between shallow layer accuracy and overall model accuracy be leveraged to iteratively construct Forward-Forward networks by adding layers only when necessary?
- Basis in paper: [explicit] The author suggests that because shallow layers correlate strongly with output, "a methodological approach to iterative construction of these networks may be possible" rather than fixed architectures.
- Why unresolved: The current work only characterizes the correlation behavior but does not design or test an adaptive architecture algorithm based on these findings.
- What evidence would resolve it: The successful implementation and validation of a training protocol that dynamically adds hidden layers based on shallow layer accuracy thresholds without degrading final performance.

### Open Question 2
- Question: How does the delay in deep layer training and the correlation between layer depth and accuracy generalize to more complex datasets (e.g., CIFAR-10) or non-classification tasks?
- Basis in paper: [explicit] The author notes that experimentation was limited to MNIST and explicitly calls for testing on datasets like CIFAR-10 or Pets to determine if phenomena are attributable to the model or the dataset.
- Why unresolved: The observed "delay" in deeper layers might be an artifact of the simple MNIST task; it is unverified if this behavior persists in more complex data distributions or regression tasks.
- What evidence would resolve it: Replication of the layer-depth timing experiments on diverse datasets (CIFAR-10, audio data) and tasks to see if the cascading delay pattern holds.

### Open Question 3
- Question: To what extent do specific hyperparameters (peer normalization, learning rate, activation function) act as confounding variables for the observed training delays?
- Basis in paper: [explicit] The author admits that experimentation on untreated system characteristics, specifically citing peer normalization and learning rates, is needed to rule out confounding variables preventing causal claims.
- Why unresolved: The study fixed these parameters (e.g., ReLU, specific normalization momentum), making it unclear if the training dynamics are intrinsic to the algorithm or dependent on these specific settings.
- What evidence would resolve it: Ablation studies varying peer normalization momentum and activation functions to observe if the delay in deep layer convergence is reduced, eliminated, or exacerbated.

## Limitations
- Core findings hinge on correct implementation of Forward-Forward algorithm, particularly the goodness function threshold θ and negative sample generation
- Critical parameters not fully specified in main text, creating reproduction challenges
- Correlation analysis between layer and model accuracy is statistically sound, but the claim about weak deep-layer correlation lacks strong supporting evidence in the corpus

## Confidence
- High confidence: Depth-delay relationship and layer-accuracy patterns (Figure 4), as these are directly observable from the training curves
- Medium confidence: Correlation analysis methodology (Pearson/Spearman calculations appear correct), though the interpretation of weak deep-layer correlation needs more validation
- Low confidence: Mechanism explanations for why shallow layers predict overall accuracy better than deep layers—the paper provides limited theoretical justification beyond empirical observation

## Next Checks
1. Verify the threshold θ value used in the goodness function (σ(Σa²_j - θ)) by reproducing layer accuracy curves from Figure 2
2. Test the correlation claim by training additional models with varying architectures to confirm that deep-layer accuracy consistently shows weaker correlation with overall performance
3. Investigate whether the delay in deeper layers is primarily due to sequential information flow or other factors by comparing with models that have skip connections or parallel layer architectures