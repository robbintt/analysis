---
ver: rpa2
title: Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain Perspective
arxiv_id: '2506.14965'
source_url: https://arxiv.org/abs/2506.14965
tags:
- uni000001df
- uni000001ac
- reasoning
- uni000001ae
- uni000001b1
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GURU, a curated RL reasoning corpus of 92K
  examples across six diverse domains (Math, Code, Science, Logic, Simulation, Tabular),
  addressing the challenge of reliable RL reward signals beyond narrow math and code.
  Using domain-specific reward design and filtering, GURU enables systematic RL training
  of 7B and 32B models.
---

# Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain Perspective

## Quick Facts
- arXiv ID: 2506.14965
- Source URL: https://arxiv.org/abs/2506.14965
- Reference count: 40
- Key outcome: GURU-7B/32B models achieve state-of-the-art performance among open RL-trained models, outperforming best baselines by 7.9% and 6.7% on a 17-task evaluation suite

## Executive Summary
This paper addresses the challenge of reliable reinforcement learning (RL) reward signals for large language models (LLMs) by introducing GURU, a curated RL reasoning corpus spanning six diverse domains. The study systematically evaluates how RL training affects reasoning capabilities across domains with varying levels of pretraining coverage. Results reveal domain-dependent RL effects, showing that while pretrained-heavy domains benefit from cross-domain training, less-pretrained domains require in-domain training to effectively acquire reasoning skills. The work demonstrates that RL can expand reasoning boundaries, particularly for complex tasks, though effectiveness varies by domain, model scale, and decoding hyperparameters.

## Method Summary
The authors construct GURU, a 92K-example corpus covering Math, Code, Science, Logic, Simulation, and Tabular domains, with domain-specific reward design and filtering mechanisms. They train 7B and 32B parameter models using RL with these rewards and systematically evaluate performance across the six domains. The study employs Pass@k analysis to measure reasoning effectiveness and compares models trained with in-domain versus cross-domain RL approaches. Experimental results are validated on a 17-task evaluation suite to assess state-of-the-art performance among open RL-trained models.

## Key Results
- Domain-dependent RL effects: pretrained-heavy domains (Math, Code, Science) benefit from cross-domain training, while less-pretrained domains (Logic, Simulation, Tabular) require in-domain training
- GURU-7B/32B models achieve state-of-the-art performance among open RL-trained models, outperforming best baselines by 7.9% and 6.7%
- RL expands reasoning boundaries as shown by Pass@k analysis, particularly for complex tasks across domains

## Why This Works (Mechanism)
The domain-dependent RL effects emerge from the interaction between pretraining coverage and task-specific reward signals. In domains with substantial pretraining data (Math, Code, Science), models can leverage existing knowledge structures and benefit from diverse reward signals across domains, enabling cross-domain transfer. For domains with limited pretraining coverage, the models lack foundational representations, making domain-specific reward signals essential for skill acquisition. This suggests RL serves both as a fine-tuning mechanism for existing skills and as a capability discovery tool for novel reasoning patterns.

## Foundational Learning
- **Reinforcement Learning in NLP**: Needed to understand how reward-based optimization differs from supervised fine-tuning for reasoning tasks; Quick check: Can the model distinguish between reward shaping and policy optimization effects?
- **Domain Adaptation Theory**: Required to explain why cross-domain training works for some domains but not others; Quick check: Does the domain similarity metric predict transfer success?
- **Reasoning Task Taxonomy**: Essential for categorizing the six domains and understanding their pretraining coverage differences; Quick check: Are the domain boundaries empirically justified by performance gaps?

## Architecture Onboarding
**Component Map**: Pretrained LLM -> RL Reward Designer -> GURU Corpus -> RL Trainer -> Domain-specific Model Variants -> Evaluation Suite
**Critical Path**: Domain-specific reward design and filtering → RL training → Pass@k evaluation across 17 tasks
**Design Tradeoffs**: Cross-domain training offers efficiency but may dilute domain-specific skill acquisition; in-domain training ensures specialization but limits generalization
**Failure Signatures**: Poor performance in Logic/Simulation/Tabular indicates insufficient pretraining coverage or inadequate domain-specific reward design
**First Experiments**: 1) Compare 7B vs 32B performance on in-domain vs cross-domain training for each domain; 2) Ablate reward components to identify most influential signals; 3) Test zero-shot transfer from Math/Code to Science/Logic domains

## Open Questions the Paper Calls Out
None

## Limitations
- The GURU corpus, while diverse, may not fully represent real-world reasoning complexity and edge cases
- Domain-specific reward design and filtering introduces potential biases affecting cross-domain generalization
- The study does not fully explore architectural modifications that could improve cross-domain transfer for less-pretrained domains

## Confidence
- **High Confidence**: Domain-dependent RL effects and Pass@k analysis demonstrating RL's impact on reasoning boundaries
- **Medium Confidence**: State-of-the-art claims among open RL-trained models and assertions about RL facilitating skill acquisition in less-pretrained domains

## Next Checks
1. Evaluate GURU models on out-of-distribution reasoning tasks outside the six original domains to assess true generalization capabilities
2. Perform ablation studies varying pretraining distribution and depth across all six domains to understand pretraining-coverage relationships
3. Train and evaluate 70B+ parameter models to determine if domain-dependent patterns persist at larger scales