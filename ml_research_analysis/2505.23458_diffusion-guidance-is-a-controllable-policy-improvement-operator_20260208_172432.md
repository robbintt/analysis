---
ver: rpa2
title: Diffusion Guidance Is a Controllable Policy Improvement Operator
arxiv_id: '2505.23458'
source_url: https://arxiv.org/abs/2505.23458
tags:
- policy
- learning
- cfgrl
- diffusion
- offline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes a theoretical link between diffusion model
  guidance and policy improvement in reinforcement learning. By framing policies as
  products of a reference policy and an optimality function, it shows that sampling
  with guidance on advantage-conditioned diffusion models yields improved policies.
---

# Diffusion Guidance Is a Controllable Policy Improvement Operator

## Quick Facts
- arXiv ID: 2505.23458
- Source URL: https://arxiv.org/abs/2505.23458
- Reference count: 40
- Primary result: CFGRL consistently outperforms standard offline RL methods and goal-conditioned behavioral cloning across state-based and visual domains.

## Executive Summary
This paper establishes a theoretical link between diffusion model guidance and policy improvement in reinforcement learning. By framing policies as products of a reference policy and an optimality function, it shows that sampling with guidance on advantage-conditioned diffusion models yields improved policies. The method, CFGRL, is trained with supervised learning objectives but enables controllable policy improvement during test time. Experiments demonstrate that CFGRL consistently outperforms standard offline RL methods like AWR on ExORL and OGBench tasks, and significantly improves over goal-conditioned behavioral cloning across state-based and visual domains, sometimes doubling success rates. Notably, CFGRL achieves this without requiring value function training, offering a simple yet powerful approach to policy improvement.

## Method Summary
CFGRL trains a single diffusion policy network conditioned on state, action, noise scale, and optimality signal. During training, the network learns to predict flow-matching velocities for both unconditional and optimality-conditioned policies. At test time, classifier-free guidance is applied by combining the conditional and unconditional velocity predictions with a guidance weight w, effectively sampling from a product distribution that prioritizes higher-return actions. The method works for both offline RL (with explicit advantage labeling) and goal-conditioned behavioral cloning (using implicit optimality from goal-reaching probability). CFGRL achieves policy improvement without requiring separate value function training, enabling controllable improvement through the guidance weight.

## Key Results
- CFGRL consistently outperforms standard offline RL methods like AWR on ExORL and OGBench tasks
- CFGRL significantly improves over goal-conditioned behavioral cloning across state-based and visual domains, sometimes doubling success rates
- CFGRL achieves policy improvement without requiring value function training, using only supervised learning objectives

## Why This Works (Mechanism)

### Mechanism 1: Product Policy Factorization via Guidance
- Claim: Classifier-free guidance (CFG) on an optimality-conditioned diffusion policy performs controllable policy improvement by sampling from a product distribution of the reference policy and an optimality function.
- Mechanism:
    1. **Factorization:** A policy is expressed as œÄ(a|s) ‚àù œÄÃÇ(a|s) f(A(s,a)), where œÄÃÇ is the reference policy and f is a non-negative, monotonically increasing function of the advantage A.
    2. **CFG for Sampling:** CFG samples from this product without computing a partition function. The score ‚àáa log œÄ(a|s) ‚âà ‚àáa log œÄÃÇ(a|s) + w ¬∑ (‚àáa log œÄÃÇ(a|s, o) - ‚àáa log œÄÃÇ(a|s)) approximates the product policy score.
    3. **Controllable Improvement:** The guidance weight w acts as an exponent on the optimality term f(A)^w. Increasing w pushes the policy further from the reference toward higher-return actions. Theorem 1 and 2 provide theoretical support.
- Core assumption:
    - The optimality function f is non-negative and monotonically increasing with advantage A.
    - The diffusion model is expressive enough for both unconditional and optimality-conditioned policies.
    - Advantage estimates are reliable for guiding improvement.
- Evidence anchors:
  - [abstract] "By framing policies as products of a reference policy and an optimality function, it shows that sampling with guidance on advantage-conditioned diffusion models yields improved policies."
  - [section 4] "We first define policies as products of two factors ‚Äì a prior reference policy, and an 'optimality' distribution... When the optimality distribution is proportional to a monotonically increasing function of advantage, we prove that the resulting product will be an improvement over the prior."
  - [corpus] Policy Gradient Guidance Enables Test Time Control (arXiv:2510.02148) extends similar guidance principles to policy gradients, supporting the general concept of guidance for controllable improvement.
- Break condition: If the learned optimality function generalizes poorly to out-of-distribution actions, guidance may push toward erroneously high-scoring but poor actions, causing performance collapse.

### Mechanism 2: KL-Regularized Policy Improvement Equivalence
- Claim: CFGRL with controllable guidance weight effectively performs KL-regularized policy improvement.
- Mechanism:
    1. **Equivalence:** Remark 3 shows the optimal KL-regularized policy takes product form œÄ(a|s) ‚àù œÄÃÇ(a|s) exp(A(s,a)/Œ≤), mathematically equivalent to CFGRL's product policy with specific f.
    2. **Guidance as Inverse Temperature:** Guidance weight w acts like 1/Œ≤: higher w (lower Œ≤) emphasizes return maximization over reference adherence.
    3. **Test-Time Control:** Unlike fixed Œ≤ in training, w can be tuned at test time, enabling dynamic adjustment along the reference-adherence to return-maximization spectrum without retraining.
- Core assumption:
    - The KL-regularized product policy is a meaningful and stable improvement operator.
    - Policy-reference divergence can be managed by tuning w.
- Evidence anchors:
  - [abstract] "CFGRL bridges a connection between guidance and traditional RL objectives‚Äîin fact, under certain choices, guided sampling results in a distribution that is equivalent to the solution of a KL-constrained policy improvement objective."
  - [section 4] "Notably, the solutions to the mentioned objective [KL-regularized] naturally form a set of product policies... we will instead develop a framework where the product factors are represented independently, allowing their composition to be freely controlled during evaluation time."
  - [corpus] No direct corpus evidence for this specific theoretical equivalence.
- Break condition: If the offline dataset is extremely poor quality, the reference policy œÄÃÇ will be suboptimal, and KL-regularized improvement may get stuck in a poor local optimum dictated by bad data.

### Mechanism 3: Avoiding Explicit Value Function Training (GCBC case)
- Claim: CFGRL can improve goal-conditioned behavioral cloning (GCBC) without training a separate value or Q-function.
- Mechanism:
    1. **Implicit Optimality from GCBC:** A GCBC policy learns p(g|s,a)‚Äîthe probability of reaching the goal‚Äîwhich acts as a proxy optimality function p(o|s,a).
    2. **Guidance as Free Improvement:** Applying CFG with pre-trained GCBC as conditional model œÄÃÇ(a|s,o) and unconditional policy as œÄÃÇ(a|s) samples from a product distribution prioritizing goal-reaching actions‚Äîa policy improvement step "for free" at test time.
    3. **Leveraging Pre-trained Models:** Repurposes a diffusion imitation learning model for RL-like improvement without additional RL-specific training.
- Core assumption:
    - GCBC's learned p(g|s,a) reliably proxies the optimality function.
    - Base GCBC policy captures meaningful action-goal relationships.
- Evidence anchors:
  - [abstract] "Of particular importance, CFGRL can operate without explicitly learning a value function, allowing us to generalize simple supervised methods (e.g., goal-conditioned behavioral cloning) to further prioritize optimality, gaining performance for 'free' across the board."
  - [section 6] "The key insight is that, since CFGRL enables one step of policy improvement over the base policy, applying CFGRL with guidance on the goal g will produce a policy that is better than the standard GCBC policy."
  - [corpus] Weak direct evidence; mechanism relies on paper's specific GCBC formulation.
- Break condition: If GCBC fails to capture action-goal relationships (e.g., sparse rewards, insufficient coverage), its output won't provide useful optimality signal, and "free" improvement may be nonexistent or harmful.

## Foundational Learning

- Concept: **Classifier-Free Guidance (CFG)**
  - Why needed here: Core sampling technique enabling CFGRL. Combines conditional and unconditional score functions to control generation, repurposed here to steer policy toward higher advantage.
  - Quick check question: How does guidance weight w affect the trade-off between sample diversity and conditioning adherence?

- Concept: **Policy Improvement Operators**
  - Why needed here: Central theoretical contribution‚Äîframing guidance as a policy improvement operator. Understanding how updates guarantee or tend toward higher expected return is essential.
  - Quick check question: What condition must policy update œÄ‚Ä≤ satisfy to be considered an improvement over reference policy œÄÃÇ?

- Concept: **Offline Reinforcement Learning**
  - Why needed here: CFGRL is evaluated for offline RL and goal-conditioned imitation. Understanding offline RL's core challenge‚Äîimproving beyond data without exploration‚Äîis key to appreciating test-time controllable methods.
  - Quick check question: What is the primary risk when an offline RL policy deviates significantly from training-dataset actions?

## Architecture Onboarding

- Component map:
    1. Diffusion Policy Network (vŒ∏) -> Optimality Labeling -> Guidance Sampling Loop

- Critical path:
    1. Start with offline dataset D of state-action pairs.
    2. Implement Optimality Labeling. For GCBC: train standard goal-conditioned diffusion policy. For offline RL: train Q-function first, label by computed advantage.
    3. Train single Diffusion Policy Network using standard conditional flow-matching loss on labeled data. Train both conditional (on o) and unconditional (drop condition with ~0.1 probability).
    4. Evaluate via Guidance Sampling Loop: compute both conditional and unconditional velocity predictions, combine with chosen w.

- Design tradeoffs:
    - **Guidance weight (w)**: Most critical hyperparameter. Low w = safer, closer to reference, possibly suboptimal. High w = higher potential return, but risk of out-of-distribution actions and collapse.
    - **Optimality function choice**: Paper uses simple binary label (A ‚â• 0). Finer-grained functions could provide better signal but may be harder to learn.
    - **Shared vs. separate networks**: Appendix C finds single shared network for conditional/unconditional predictions works better than separate networks, likely due to shared representations.

- Failure signatures:
    - **Performance collapse**: Very high w generates incoherent/physically impossible actions; performance drops sharply.
    - **No improvement**: w ‚âà 1 makes policy behave like standard BC, showing no gain.
    - **Sensitivity to advantage quality**: Poor Q-function (e.g., overestimation) yields noisy/wrong optimality labels, causing detrimental guidance.

- First 3 experiments:
  1. **GCBC Baseline + Guidance:** Train standard diffusion GCBC on offline dataset. Measure success rate. At test time, apply CFG with w ‚àà {1.5, 2.0, 3.0}. Report performance vs. w to validate "free improvement."
  2. **Offline RL Comparison:** On benchmark (D4DL/ExORL), train Q-function via IQL. Train CFGRL conditioned on advantage sign. Compare performance (sweep w) vs. AWR (sweep temperature).
  3. **Ablate Optimality Label:** In offline RL, compare binary (A ‚â• 0) vs. raw scaled advantage as conditioning signal. Does finer-grained optimality improve policy improvement?

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can iterative application of CFGRL achieve multi-step policy improvement, and does guidance-based improvement compose over multiple iterations?
- Basis in paper: [explicit] The authors state "CFGRL does not represent a state-of-the-art RL algorithm" and that "more advanced policy extraction methods and online RL techniques... could provide for stronger extrapolation." The theoretical analysis only proves single-step improvement.
- Why unresolved: The paper demonstrates one step of policy improvement but does not explore whether applying CFGRL iteratively (using the improved policy as the new reference) yields further gains or encounters diminishing returns.
- What evidence would resolve it: Experiments applying CFGRL iteratively in online or semi-online settings, measuring whether performance improves monotonically or plateaus after one step.

### Open Question 2
- Question: What theoretical guarantees or predictive signals exist for selecting the optimal guidance weight w before encountering distribution shift degradation?
- Basis in paper: [inferred] The paper empirically observes that "performance sometimes declines beyond a certain point, likely because the policy deviates too far from the data distribution," but provides no principled method for predicting this divergence point.
- Why unresolved: While Remark 2 proves higher w yields improvement in expected advantage, the practical tradeoff with distribution shift is characterized only empirically through sweeping w at test time.
- What evidence would resolve it: Theoretical bounds relating w to KL divergence from the data distribution, or practical heuristics (e.g., based on dataset coverage statistics) that predict optimal w.

### Open Question 3
- Question: How does the choice of optimality function f affect CFGRL's effectiveness, and are there alternatives to the binary indicator that yield stronger improvement?
- Basis in paper: [explicit] The paper states the framework "supports a range of optimality functions rather than only A = 0" but only experiments with the binary criterion o = ùüô(A ‚â• 0).
- Why unresolved: Different functional forms of f (e.g., exponential, polynomial, or thresholded variants) may provide different tradeoffs between improvement magnitude and stability, but this design space is unexplored.
- What evidence would resolve it: Ablation experiments comparing alternative optimality functions across tasks with varying data quality and reward structures.

## Limitations
- Theoretical guarantees only extend to simplified binary optimality labeling, not more complex optimality functions
- Performance gains may not scale to more complex, long-horizon tasks beyond tested benchmarks
- Limited exploration of sensitivity to guidance weight w across diverse task types

## Confidence
- **High**: Mechanism linking CFG to policy improvement (supported by Theorems 1-2)
- **Medium**: Equivalence to KL-regularized objectives (Remark 3, but limited empirical validation)
- **Medium**: Practical gains in offline RL (consistent results on benchmarks but limited domain diversity)

## Next Checks
1. Test CFGRL on a more diverse set of offline RL tasks with varying dataset quality to assess robustness
2. Conduct an ablation study on optimality labeling granularity (e.g., scaled advantage vs. binary) to quantify impact on improvement
3. Investigate the effect of guidance weight w on sample diversity and policy divergence to better understand controllable improvement boundaries