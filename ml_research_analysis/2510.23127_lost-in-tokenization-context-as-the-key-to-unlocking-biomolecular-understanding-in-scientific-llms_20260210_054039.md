---
ver: rpa2
title: 'Lost in Tokenization: Context as the Key to Unlocking Biomolecular Understanding
  in Scientific LLMs'
arxiv_id: '2510.23127'
source_url: https://arxiv.org/abs/2510.23127
tags:
- protein
- sequence
- context
- function
- biomolecular
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the fundamental challenge of integrating biomolecular
  sequences into Scientific Large Language Models (Sci-LLMs), identifying the "tokenization
  dilemma" where treating sequences as language destroys functional motifs or treating
  them as modality introduces semantic misalignment. The authors propose a context-driven
  approach that bypasses raw sequence interpretation by providing Sci-LLMs with high-level
  structured biological context derived from established bioinformatics tools like
  InterProScan and BLASTp.
---

# Lost in Tokenization: Context as the Key to Unlocking Biomolecular Understanding in Scientific LLMs

## Quick Facts
- arXiv ID: 2510.23127
- Source URL: https://arxiv.org/abs/2510.23127
- Reference count: 40
- Primary result: Context-only input to Sci-LLMs outperforms sequence-only and sequence+context configurations for protein function prediction

## Executive Summary
This paper addresses the fundamental challenge of integrating biomolecular sequences into Scientific Large Language Models (Sci-LLMs), identifying the "tokenization dilemma" where treating sequences as language destroys functional motifs or treating them as modality introduces semantic misalignment. The authors propose a context-driven approach that bypasses raw sequence interpretation by providing Sci-LLMs with high-level structured biological context derived from established bioinformatics tools like InterProScan and BLASTp. Through systematic evaluation on protein function prediction tasks, they demonstrate that context-only input consistently outperforms sequence-only and sequence+context configurations, with context-driven approaches achieving near-perfect functional separation (ARI 0.958) compared to sequence-as-language models (ARI 0.492-0.690) and sequence-as-modality models (ARI 0.809). Remarkably, adding raw sequences to context actually degrades performance, suggesting sequences act as informational noise. The approach generalizes well to novel protein families and shows superior robustness compared to existing paradigms, positioning Sci-LLMs as reasoning engines over expert knowledge rather than sequence decoders.

## Method Summary
The method employs a three-stage pipeline: (1) run InterProScan for Pfam domain identification and intrinsic GO terms, (2) run BLASTp against Swiss-Prot for homolog sequence identification and transfer of GO annotations from top hits, and (3) use ProTrek as conditional fallback when both GO and Pfam are unavailable. The resulting structured context—comprising conserved domain descriptions, functional annotations, and semantic summaries—is formatted into prompts without including raw sequences. The approach is tested across three input modes (sequence-only, context-only, sequence+context) using both specialized Sci-LLMs and general LLMs on protein QA and DNA mutation prediction tasks, with performance measured by LLM-Score and functional clustering accuracy.

## Key Results
- Context-only configuration achieved substantial performance boosts across all evaluated Sci-LLMs (e.g., Intern-S1: 86.15 vs. 84.03 for sequence+context)
- Adding raw sequences to context consistently degraded performance by 10-20 points, confirming sequences act as informational noise
- Context-driven representation achieved near-perfect functional separation (ARI 0.958) versus sequence-as-language (ARI 0.492-0.690) and sequence-as-modality (ARI 0.809)
- The approach showed superior generalization to novel protein families and robustness compared to existing paradigms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Raw biomolecular sequences degrade LLM performance because atomic-level tokenization destroys functional motifs.
- Mechanism: Tokenizers fragment sequences into individual amino acids or nucleotides (e.g., "A", "C", "T", "G" or single residues), breaking apart conserved domains and regulatory elements that constitute the true "words" of biology. The model receives a high-dimensional, low-information-density signal and must relearn biological grammar from disconnected letters—a data-intensive, poorly generalizing process.
- Core assumption: Functional motifs, not atomic residues, are the meaningful semantic units for biological reasoning.
- Evidence anchors:
  - [abstract] "Whether treating sequences as a specialized language, risking the loss of functional motif information"
  - [section 3.2] "the tokenization process is often too granular...it destroys the very structures that carry biological meaning: functional motifs, domains, and regulatory elements"
  - [corpus] Weak corpus evidence; related papers discuss tokenization strategies but don't confirm this specific mechanism.
- Break condition: If a tokenizer could preserve motifs (e.g., via learned subword units that align to domains), this degradation would diminish or disappear.

### Mechanism 2
- Claim: Specialized bio-encoders preserve structure but lose information during semantic alignment with LLMs.
- Mechanism: A pretrained biological encoder (e.g., SaProt) generates well-structured embeddings governed by biophysics and evolution. An alignment module (e.g., Q-Former) must project these into the LLM's linguistic semantic space. This translation process progressively blurs functional distinctions and erases fine-grained signals like point mutations.
- Core assumption: The encoder and LLM semantic spaces are fundamentally misaligned in ways that cannot be fully bridged by current alignment techniques.
- Evidence anchors:
  - [abstract] "or as a separate modality, introducing formidable alignment challenges"
  - [section 5.3 + Figure 3] ARI drops from 0.945 (SaProt encoder) → 0.916 (Q-Former) → 0.809 (final), showing progressive degradation through alignment.
  - [corpus] No direct corpus corroboration; adjacent work on multimodal alignment exists but doesn't test this specific degradation pathway.
- Break condition: If alignment modules could preserve mutation sensitivity and functional separation, the sequence-as-modality approach would match or exceed context-only performance.

### Mechanism 3
- Claim: High-level structured context from bioinformatics tools is both information-dense and natively language-aligned, enabling superior reasoning.
- Mechanism: Instead of forcing LLMs to interpret raw sequences, pre-process sequences through established tools (InterProScan for domains, BLAST for homology-based GO terms, ProTrek as fallback). The output is human-readable text describing conserved domains, functional annotations, and semantic summaries—naturally aligned with LLM training distributions and requiring no cross-modal translation.
- Core assumption: Bioinformatics tools extract the biologically relevant signal that LLMs need, and this extracted signal is sufficient for downstream reasoning tasks.
- Evidence anchors:
  - [abstract] "context-only configuration achieved a substantial performance boost, while adding the raw sequence alongside context consistently led to lower accuracy"
  - [Table 1] Context-only outperforms all other modes across specialized Sci-LLMs and general LLMs (e.g., Intern-S1: 86.15 vs. 84.03 for sequence+context)
  - [Figure 2] Context-driven representation achieves ARI of 0.958, dramatically exceeding all sequence-based methods.
  - [corpus] Adjacent work (ProtTeX, Knowledge-Augmented Long-CoT) supports context/structure-grounded reasoning but doesn't replicate this specific finding.
- Break condition: If the bioinformatics tools fail to extract relevant features (e.g., for truly novel orphan proteins with no homologs or known domains), context quality drops and performance may degrade to sequence-only levels.

## Foundational Learning

- Concept: **Tokenization granularity tradeoffs**
  - Why needed here: Understanding why atomic tokenization destroys biological meaning is central to the paper's thesis. You must grasp that "words" in biology are motifs/domains, not individual residues.
  - Quick check question: If you tokenize "MKTLLV..." as ["M","K","T","L","L","V",...], what biological structures are being destroyed?

- Concept: **Cross-modal semantic alignment**
  - Why needed here: The sequence-as-modality approach relies on aligning embedding spaces. You need to understand that encoder→LLM projection is lossy and can blur functional distinctions.
  - Quick check question: Why would an embedding space trained on evolutionary conservation (biophysics) be hard to align with one trained on human language?

- Concept: **Bioinformatics tool outputs (BLAST, InterProScan, GO terms)**
  - Why needed here: The context-driven approach depends on these tools. You should know what they produce: BLAST finds homologs and transfers annotations; InterProScan identifies conserved domains; GO terms describe molecular function, biological process, cellular component.
  - Quick check question: Given a novel protein with no known homologs, which tool would fail first, and what fallback does the paper propose?

## Architecture Onboarding

- Component map: Raw sequence → InterProScan (Pfam + GO) → BLASTp (homolog GO transfer) → ProTrek (fallback) → Hierarchical context assembly → Structured prompt → LLM reasoning → Answer

- Critical path: Sequence → Tool execution → Hierarchical context assembly → Prompt construction → LLM reasoning → Answer. The context quality determines success; raw sequence is explicitly excluded.

- Design tradeoffs:
  - **Coverage vs. precision**: BLAST/InterProScan provide high-precision context but fail on orphan proteins; ProTrek adds coverage but introduces noise (ablation shows unconditional ProTrek degrades scores).
  - **Speed vs. depth**: Running full toolchain (InterProScan + BLAST) is slower than direct sequence input but yields dramatically better results.
  - **Generalization vs. Memorization**: Context-driven approach generalizes to novel proteins (flat performance across Easy/Medium/Hard splits); sequence-based models collapse on Hard subset.

- Failure signatures:
  - **Orphan proteins**: No BLAST hits, no InterProScan domains → sparse context → lower accuracy.
  - **Mutation sensitivity**: InterProScan and BLAST are insensitive to single/few amino acid changes; wild-type and mutated sequences produce identical context → cannot predict mutation effects.
  - **Noise contamination**: Adding raw sequence to context consistently degrades scores (10-20 point drops observed across models), confirming sequence acts as informational noise.

- First 3 experiments:
  1. **Reproduce Table 1 abridged**: Take 2-3 proteins, run sequence-only vs. context-only through a capable general LLM (e.g., GPT-4 or DeepSeek), measure answer quality with LLM-Score. Expect context-only > sequence-only.
  2. **Test the noise effect**: Take proteins where context-only succeeds, add raw sequence to the prompt, verify performance drops. This confirms the "informational noise" finding.
  3. **Probe failure mode**: Find a protein with <30% identity to Swiss-Prot (orphan-like), verify context sparsity and measure performance gap vs. well-annotated proteins. Optionally test if ProTrek-only context recovers some performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the context-driven framework be adapted to predict the functional consequences of single-point mutations, given that current bioinformatics tools (e.g., InterProScan, BLAST) lack the sensitivity to generate distinct textual contexts for minor sequence variations?
- Basis in paper: [explicit] Appendix J explicitly identifies this limitation, noting that "the context generated for the wild-type and mutated proteins is essentially the same," rendering the current approach ineffective for mutation effect prediction.
- Why unresolved: The study demonstrates that raw sequences act as "informational noise" for LLMs, yet the alternative context-driven approach relies on tools that ignore small variations, creating a blind spot for high-resolution biological tasks.
- What evidence would resolve it: The development of a modified context pipeline that incorporates mutation-specific tools (e.g., stability predictors) which successfully allows the LLM to distinguish between wild-type and mutant functional outcomes.

### Open Question 2
- Question: How can the performance of context-driven models be sustained for "orphan" proteins from unexplored regions of the protein universe that lack detectable homologs or known domains?
- Basis in paper: [explicit] The Conclusion states that "for truly novel orphan proteins... our method's performance may be constrained," and the Ablation Study (Appendix E) reveals that the fallback tool (ProTrek) often introduces noise rather than helpful signal.
- Why unresolved: While the hierarchical context strategy works for proteins with known homologs, the fallback mechanism for novel proteins (ProTrek) was empirically shown to degrade performance when combined with other data, leaving a gap for truly novel sequences.
- What evidence would resolve it: A benchmark evaluation on a dataset of synthetic or extremely novel sequences showing that a new fallback mechanism achieves accuracy comparable to homolog-rich proteins.

### Open Question 3
- Question: Does the "context-over-sequence" paradigm generalize effectively to non-protein biomolecular tasks, such as RNA structure prediction or small molecule property analysis?
- Basis in paper: [explicit] The Conclusion notes that "current analysis has primarily focused on proteins; although we provide some preliminary exploration in Appendix G, a more comprehensive treatment remains for future work."
- Why unresolved: The paper establishes the dominance of context for proteins and preliminary DNA tasks, but it remains unverified if high-level textual context is as readily available or as information-dense for other biomolecular modalities like RNA or metabolites.
- What evidence would resolve it: A systematic comparison of sequence-only vs. context-only modes across RNA-specific (e.g., RNA folding) and small-molecule tasks, confirming that context consistently outperforms raw sequence representation.

## Limitations
- Generalization to orphan proteins with no detectable homologs or known domains may revert to sequence-only performance levels
- Cannot predict functional consequences of single-point mutations due to identical context generation for wild-type and mutant sequences
- Context quality depends on availability of homology information and known domain matches from bioinformatics tools

## Confidence

**High Confidence (4-5)**: The core experimental finding that context-only outperforms sequence-only and sequence+context configurations is robust and well-demonstrated across multiple Sci-LLMs (Intern-S1, BioGPT, BioMedLM, Evo-1, ESM-2) and general LLMs. The performance gap (86.15 vs. 84.03 for Intern-S1) is statistically significant and consistent. The ARI comparisons showing context-driven (0.958) vastly exceeds sequence-as-language (0.492-0.690) and sequence-as-modality (0.809) are compelling.

**Medium Confidence (2-3)**: The explanation for why sequences degrade performance (informational noise vs. tokenization granularity vs. attention interference) is plausible but not definitively proven. The ablation studies showing ProTrek adds noise are convincing, but the underlying cognitive mechanism for sequence-as-noise requires further investigation.

**Low Confidence (0-1)**: Claims about superiority over all existing paradigms are difficult to verify without direct comparison to every relevant approach. The mutation sensitivity limitation is stated but not experimentally validated—the paper doesn't show specific cases where context-only fails but sequence-based methods succeed.

## Next Checks
1. **Orphan Protein Stress Test**: Identify 10-20 truly orphan proteins with no BLAST hits and no InterProScan domains. Compare context-only performance (using only ProTrek-generated context) against sequence-only performance. This will reveal whether the approach collapses to baseline or maintains advantage on novel biology.

2. **Mutation Effect Validation**: Take 5-10 proteins where context-only achieves high accuracy. Generate 3-5 single-point mutations per protein (selected to preserve or disrupt known functional sites based on context). Compare context-only predictions for wild-type vs. mutant—predictions should be identical if context is mutation-insensitive. Then compare sequence-only predictions to see if they detect functional changes.

3. **Cross-Dataset Generalization**: Apply the context generation pipeline to a completely independent dataset (e.g., PDB structures with functional annotations, or a different species' proteome). Measure whether the high performance (ARI 0.958) generalizes beyond the Swiss-Prot-based evaluation set, particularly focusing on proteins with varying levels of annotation completeness.