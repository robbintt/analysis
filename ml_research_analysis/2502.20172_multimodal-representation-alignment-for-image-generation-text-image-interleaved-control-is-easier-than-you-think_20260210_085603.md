---
ver: rpa2
title: 'Multimodal Representation Alignment for Image Generation: Text-Image Interleaved
  Control Is Easier Than You Think'
arxiv_id: '2502.20172'
source_url: https://arxiv.org/abs/2502.20172
tags:
- image
- generation
- training
- diffusion
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'DREAM ENGINE enables arbitrary text-image interleaved control
  in diffusion-based image generation by replacing text-only encoders with a large
  multimodal model (LMM). It uses a two-stage training approach: joint text-image
  alignment and interleaved instruction tuning.'
---

# Multimodal Representation Alignment for Image Generation: Text-Image Interleaved Control Is Easier Than You Think

## Quick Facts
- arXiv ID: 2502.20172
- Source URL: https://arxiv.org/abs/2502.20172
- Authors: Liang Chen; Shuai Bai; Wenhao Chai; Weichu Xie; Haozhe Zhao; Leon Vinci; Junyang Lin; Baobao Chang
- Reference count: 40
- One-line primary result: DREAM ENGINE achieves 0.69 GenEval score, matching SOTA text-to-image models while enabling arbitrary text-image interleaved control

## Executive Summary
DREAM ENGINE enables arbitrary text-image interleaved control in diffusion-based image generation by replacing traditional text encoders with a large multimodal model (LMM). The approach leverages the LMM's pre-aligned representation space through a two-stage training paradigm: first aligning text and images via an adapter layer, then fine-tuning the diffusion backbone for interleaved instruction following. This enables complex compositional generation tasks like object-driven feature mixing and free-form image editing while maintaining strong text-to-image generation performance.

## Method Summary
DREAM ENGINE replaces the text encoders (CLIP+T5) in SD3.5-Large with Qwen2VL-2B-Instruct plus a 2-layer MLP adapter. The adapter projects LMM hidden states to the MM-DiT conditioning space. Stage 1 trains only the adapter on text-to-image (JourneyDB, CC12M, synthetic images) and image-to-image (JourneyDB) alignment tasks. Stage 2 unfreezes DiT with LoRA rank 32 on attention layers, training on UltraEdit for editing and internal object detection data for object-driven generation. Visual features are blended via skip connections with ratio r sampled uniformly during training.

## Key Results
- Achieves 0.69 GenEval score, matching or exceeding state-of-the-art text-to-image models
- Successfully performs object-driven feature mixing and free-form image editing tasks
- Demonstrates zero-shot capability in image-to-image tasks when trained only on text-to-image alignment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LMMs provide pre-aligned representation space that substitutes for dedicated text encoders without architectural complexity
- Mechanism: Adapter maps LMM hidden states to MM-DiT conditioning space using two-layer MLP; LMM pretraining already aligns text and images
- Core assumption: LMM internal representation space is sufficiently well-aligned that simple projection preserves semantic structure
- Evidence anchors: Abstract states LMMs offer effective shared representation space; Section 2.2 describes adapter architecture; UniFusion corpus supports unified encoder benefits
- Break condition: Adapter dimensionality too low or training data lacks coverage, collapsing multimodal distinctions

### Mechanism 2
- Claim: Two-stage training enables stable alignment by freezing pretrained components and progressively unfreezing based on task complexity
- Mechanism: Stage 1 trains only adapter on text-to-image and image-to-image alignment, preserving LMM and DiT; Stage 2 unfreezes DiT for interleaved instruction tasks
- Core assumption: Stage 1 adapter provides good conditioning so Stage 2 DiT fine-tuning focuses on instruction-following rather than basic alignment
- Evidence anchors: Abstract describes two-stage training paradigm; Section 2.3 notes mutual reinforcement between tasks; ARMOR corpus supports staged training stability
- Break condition: Stage 1 data insufficient or misaligned, propagating noisy conditioning to Stage 2

### Mechanism 3
- Claim: Visual feature blending via skip connections preserves low-level visual fidelity that LLM processing may attenuate
- Mechanism: Final image patch representation is weighted sum: hI = (1-r)·hLLM + r·hViT; r sampled uniformly during training
- Core assumption: ViT features retain spatial detail lost through LLM autoregressive processing; blending doesn't introduce artifacts
- Evidence anchors: Section 2.2 describes skip connection to avoid visual information loss; Figure 9 shows ablation results; no direct corpus evidence for this specific mechanism
- Break condition: r too low for editing (loses identity preservation) or too high for generation (over-constrains creative variation)

## Foundational Learning

- Concept: Rectified Flow Matching
  - Why needed here: DREAM ENGINE uses rectified flows (not standard DDPM) for diffusion objective; understanding velocity parameterization vθ(zt, c, t) and straight-path assumption is necessary to debug training loss behavior
  - Quick check question: Can you explain why rectified flow uses v = x0 − ε rather than predicting noise ε directly?

- Concept: MM-DiT Modality-Specific Processing
  - Why needed here: Backbone uses separate LayerNorm and MLP per modality with shared attention; this design affects how interleaved conditioning propagates and must be understood for architectural modifications
  - Quick check question: In MM-DiT, why keep separate MLPs while sharing attention weights across modalities?

- Concept: LoRA Fine-Tuning in Transformers
  - Why needed here: Stage 2 trains DiT via LoRA (rank 32) on attention layers; understanding low-rank adaptation is critical for debugging why certain capabilities emerge or fail during instruction tuning
  - Quick check question: What is the memory and compute tradeoff of LoRA vs. full fine-tuning for the DiT attention layers?

## Architecture Onboarding

- Component map: Interleaved input sequence → Qwen2VL-2B-Instruct ViT+LLM processing → Visual feature blender (skip connection with ratio r) → 2-layer MLP adapter (4096 hidden dim) → MM-DiT conditioning space → Denoising trajectory

- Critical path: Input interleaved sequence → LMM ViT + LLM processing → blended visual features → adapter projection → MM-DiT modulation embedding (avg pooled + timestep) → denoising trajectory

- Design tradeoffs:
  - Freezing LMM preserves understanding capabilities but limits generation quality gains from joint training
  - Removing token length limit enables arbitrary interleaved inputs but increases memory pressure
  - Uniform sampling of r during training adds flexibility but requires inference-time tuning per task

- Failure signatures:
  - Low CLIP score on reconstruction (>0.3 L2 distance): Check adapter convergence, increase Stage 1 data quality
  - Instruction following fails but reconstruction works: Stage 2 LoRA may be undertrained or data lacks instruction diversity
  - Object mixing produces copy-paste artifacts: Blending ratio r may be too high at inference

- First 3 experiments:
  1. **Adapter-only baseline**: Train Stage 1 on text-to-image only (no image-to-image), evaluate GenEval and reconstruction CLIP. Verify claim that image-to-image task is partially zero-shot from text-to-image training.
  2. **Blending ratio ablation**: Fix r ∈ {0.0, 0.25, 0.5, 0.75, 1.0} at inference for reconstruction and editing tasks. Quantify consistency-creativity tradeoff curve.
  3. **Interleaved control stress test**: Construct multi-image prompts (3+ images with conflicting attributes) and compare against Emu2-Gen baseline. Identify failure modes not covered in paper's qualitative examples.

## Open Questions the Paper Calls Out

- **End-to-end LMM fine-tuning**: The paper suggests unfreezing the LMM during training has large potential in further improving generation performance, though it was frozen to preserve understanding capabilities. Comparative evaluation between frozen-LMM and fully fine-tuned LMM setups would resolve this.

- **Extension to other modalities**: The conclusion explicitly lists extending this framework to other modalities, such as video or 3D content, as a primary direction for future research. Successful application to video generation architectures would demonstrate generalizability.

- **Robustness of feature mixing**: The paper highlights emergent feature mixing capabilities but acknowledges these were not explicitly present in training data, suggesting lack of controlled analysis on failure cases or robustness. Quantitative analysis on datasets specifically designed for conflicting multi-image compositions would provide evidence.

## Limitations

- Internal object detection dataset construction remains opaque, creating uncertainty about whether strong performance stems from robust instruction understanding or dataset-specific memorization
- 2-layer MLP adapter capacity lacks ablation studies to verify sufficiency for semantic gap between LMM representations and MM-DiT conditioning space
- Visual feature blending mechanism efficacy not comprehensively quantified across different task types and consistency-creativity tradeoffs

## Confidence

**High confidence** in:
- Core feasibility of replacing text encoders with LMMs for image generation
- Stage 1 adapter training stabilizing basic text-image alignment
- Two-stage training approach providing stable convergence

**Medium confidence** in:
- Sufficiency of 2-layer MLP adapter architecture
- Effectiveness of visual feature blending mechanism
- Generalization of GenEval performance to out-of-distribution prompts

**Low confidence** in:
- Specific internal object detection dataset construction and impact on instruction-following capabilities
- Claim that image-to-image alignment is "partially zero-shot" from text-to-image training
- Long-term stability of interleaved generation beyond evaluated tasks

## Next Checks

1. **Adapter architecture ablation**: Train DREAM ENGINE variants with 1-layer, 3-layer, and 4096-dim vs 8192-dim adapters on Stage 1 tasks only. Measure reconstruction CLIP scores and text-to-image GenEval performance to identify optimal capacity.

2. **Blending ratio systematic evaluation**: Create test suite with tasks requiring different consistency-creativity tradeoffs (e.g., portrait editing vs. creative composition). Fix r at discrete values and measure identity preservation, visual quality, and creative diversity metrics.

3. **Out-of-distribution instruction stress test**: Construct prompts with novel object combinations, contradictory attributes, and complex compositional requirements not present in training data. Compare DREAM ENGINE against baselines on these challenging cases to identify fundamental limitations of the interleaved approach.