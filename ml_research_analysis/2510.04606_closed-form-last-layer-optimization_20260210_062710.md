---
ver: rpa2
title: Closed-Form Last Layer Optimization
arxiv_id: '2510.04606'
source_url: https://arxiv.org/abs/2510.04606
tags:
- batch
- last
- layer
- size
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses optimizing neural networks with a squared loss
  by leveraging the closed-form solution for the linear last layer weights. The key
  idea is to treat the last layer as a deterministic function of the backbone parameters
  and optimize solely for these parameters, effectively alternating between gradient
  descent steps on the backbone and closed-form updates on the last layer.
---

# Closed-Form Last Layer Optimization

## Quick Facts
- **arXiv ID:** 2510.04606
- **Source URL:** https://arxiv.org/abs/2510.04606
- **Reference count:** 40
- **Primary result:** Closed-form solution for the last layer accelerates squared loss optimization; adding proximal regularization stabilizes training with small batches.

## Executive Summary
This paper introduces a method to optimize neural networks with squared loss by treating the last linear layer as a closed-form function of the backbone parameters. Instead of backpropagating through the last layer, the method computes its optimal solution via ridge regression at each step and only updates the backbone with gradient descent. To handle stochastic minibatch training, proximal regularization stabilizes the last layer update by balancing current batch information with accumulated knowledge. Theoretical analysis in the NTK regime proves convergence, and empirical results show improvements over standard SGD on several supervised tasks.

## Method Summary
The method alternates between two updates: (1) compute the optimal last layer weights W* in closed form using ridge regression (or a proximal variant for minibatches), and (2) update the backbone parameters θ via standard SGD on the batch loss with W fixed at W*. This eliminates wasteful gradient steps on W and speeds convergence for squared loss. The proximal variant adds a regularization term to W's update, preventing overfitting to individual minibatches and stabilizing training.

## Key Results
- **CIFAR-100:** ℓ2 c.f. proximal (λ=0.1) achieves 76.8% accuracy vs. 76.4% for ℓ2-SGD.
- **ImageNet:** ℓ2 c.f. proximal (λ=10) achieves 76.8% top-1 accuracy with batch size 4096.
- **FNO regression:** ℓ2 c.f. proximal matches or exceeds ℓ2-SGD on 1D Burgers equation prediction.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Treating the last layer as a closed-form function of the backbone eliminates wasteful gradient steps on W and speeds convergence.
- **Mechanism:** The squared loss has an optimal solution for the last layer (W) in closed-form via ridge regression. By computing W* explicitly at each step, you optimize only the backbone parameters (θ) while keeping W optimal. Theorem 1 shows this is equivalent to alternating between closed-form W updates and gradient steps on θ.
- **Core assumption:** The loss is squared error; the backbone features are differentiable; W is re-solved optimally at each step.
- **Evidence anchors:** [abstract] "We show this is equivalent to alternating between gradient descent steps on the backbone and closed-form updates on the last layer." [section 3, Theorem 1] Formal proof that ∇θL* = ∇θL(W*, θ).
- **Break condition:** If the loss is not squared (e.g., cross-entropy), closed-form solution no longer applies; method fails.

### Mechanism 2
- **Claim:** Regularizing the last layer update against previous estimates stabilizes training in the stochastic (minibatch) setting.
- **Mechanism:** In SGD, computing W* only on the current batch overfits that batch, causing unstable W estimates. By adding a proximal term λ||W - W_prev||² to the loss, the new W* stays close to the previous estimate while fitting the batch. This balances current batch information with accumulated knowledge.
- **Core assumption:** Minibatch training; λ > 0 is a tunable hyperparameter; batches are sampled i.i.d.
- **Evidence anchors:** [section 4, Eq. 12-17] Defines proximal loss and updates explicitly to address stochasticity. [experiments, Fig. 2, 3, 4, 5] ℓ2 c.f. proximal outperforms ℓ2 c.f. ridge for small batches.
- **Break condition:** If λ is too small, instability reappears; if λ is too large, W cannot adapt quickly to changing features.

### Mechanism 3
- **Claim:** The backbone gradient can be computed without backpropagating through the closed-form last layer solution.
- **Mechanism:** By the envelope theorem, ∇θL*(θ) = ∇θL(W*, θ). The gradient w.r.t. θ when W is held fixed at its optimal value is identical to the gradient of the composite loss. No need to differentiate through the inverse matrix in W*(θ), avoiding O(d³) complexity per backprop step.
- **Core assumption:** W is exactly optimal for the current θ; the loss is differentiable.
- **Evidence anchors:** [section 3, Theorem 1] Formal proof using chain rule and optimality condition.
- **Break condition:** If W is not optimal (e.g., due to approximations or early stopping), the gradient approximation degrades.

## Foundational Learning

- **Concept: Ridge Regression**
  - Why needed here: The closed-form last layer solution is ridge regression. You must understand its formula (W* = Y Φᵀ(ΦΦᵀ + βI)⁻¹), the role of β (regularization), and how it prevents overfitting.
  - Quick check question: If β=0 and Φ is singular, what happens to W*?

- **Concept: Stochastic Gradient Descent with Minibatches**
  - Why needed here: The method addresses core issues of SGD (overfitting to batches) via proximal regularization. You need to understand batch-wise loss computation and parameter updates.
  - Quick check question: Why might a closed-form solution computed on a single batch be unreliable?

- **Concept: Neural Tangent Kernel (NTK) Regime**
  - Why needed here: Theoretical convergence guarantees are provided in the infinite-width NTK regime. It helps to know this is a limiting case where training dynamics simplify to kernel gradient descent.
  - Quick check question: What property of the NTK ensures convergence in this regime?

## Architecture Onboarding

- **Component map:**
  - Backbone (ϕ_θ) -> Last Layer (W) -> Proximal Last Layer Solver -> Backbone Optimizer -> Training Loop

- **Critical path:**
  1. Forward pass through ϕ_θ to get features for the batch.
  2. Compute W* using Eq. 13 (proximal ridge solve, O(d²B + d³) per batch, B=batch size).
  3. Compute loss and ∇θL_batch with W fixed at W* (standard backprop).
  4. Update θ via SGD step (or other optimizer).
  5. Update W ← W*.
  6. Repeat until convergence.

- **Design tradeoffs:**
  - **Batch size:** Larger batches allow the ridge variant (λ=0) to work; small batches require proximal term (λ>0).
  - **λ vs. β:** β regularizes W in the overall loss; λ regularizes W against its previous estimate. Tune λ for stability, especially with small batches.
  - **Optimizer for backbone:** SGD with momentum works well; Adam underperforms in experiments (Fig. F.3).
  - **Last layer initialization:** Zero initialization for W performs best (Fig. F.2).

- **Failure signatures:**
  - **Unstable W:** W oscillates wildly → λ too small.
  - **Slow convergence:** W barely changes → λ too large.
  - **Poor final performance:** Underfitting → β or λ too large; check feature quality.
  - **Adam failure:** Using Adam for backbone leads to instability; gradient rescaling may require adaptive λ (not yet solved).

- **First 3 experiments:**
  1. **Baseline comparison:** On CIFAR-10 with batch size 32, compare standard ℓ2-SGD, ℓ2 c.f. ridge (β), and ℓ2 c.f. proximal (λ). Measure test accuracy and track W stability (e.g., ||W_t - W_{t-1}||) over time.
  2. **Ablation on λ:** On CIFAR-100 with batch size 128, sweep λ ∈ {0.001, 0.1, 10, 1000} while fixing other hyperparameters. Observe test accuracy and variance of W updates.
  3. **Batch size sensitivity:** On FNO regression task (Sec. 6), vary batch size ∈ {8, 32, 128}. Compare proximal vs. ridge variants. Verify proximal maintains performance at small batches where ridge fails.

## Open Questions the Paper Calls Out

- **Can a similar closed-form optimization strategy be effectively adapted for the cross-entropy loss in classification tasks?**
  - Basis in paper: [Explicit] The conclusion states, "In future work, we will focus on adapting a similar closed-form strategy to the cross entropy loss in the classification setting."
  - Why unresolved: The current method relies on the analytical solution available for ridge regression (squared loss). Cross-entropy involves a softmax non-linearity and log-likelihood, for which no simple closed-form solution exists for the last layer weights.
  - What evidence would resolve it: A derivation of a tractable, iterative, or approximate closed-form update for the last layer under cross-entropy loss that converges faster or more stably than standard gradient descent.

- **Does defining the proximal parameter $\lambda$ per dimension or adaptively over time restore compatibility with adaptive optimizers like Adam?**
  - Basis in paper: [Explicit] The conclusion notes, "Understanding how to define parameters $\lambda$ per last layer dimension and adapt these over the course of the training is of interest, since it could lead to a better performance with the Adam algorithm."
  - Why unresolved: Experiments (Figure F.3) show the method fails when using Adam, likely because Adam's adaptive gradient rescaling conflicts with the current scalar, fixed regularization of the last layer.
  - What evidence would resolve it: An extension of the algorithm featuring a vector-valued or time-dependent $\lambda$ that achieves performance parity with Adam on standard benchmarks compared to the current SGD-only version.

- **Is the under-performance on ImageNet compared to cross-entropy caused by the loss function or by training practices?**
  - Basis in paper: [Inferred] On ImageNet, the method under-performs Cross Entropy. The authors note, "The under-performance... could be due to advantageous properties of the cross entropy loss in classification, or simply that the training practices with cross entropy have been greatly perfected over the years."
  - Why unresolved: It is unclear if the performance gap is a fundamental limitation of using squared loss for classification or a practical issue of hyperparameter tuning and optimization schedules.
  - What evidence would resolve it: A comprehensive ablation study on large-scale datasets showing that, with equivalent tuning and architecture, the squared loss approach can match the peak accuracy of modern cross-entropy training pipelines.

## Limitations
- **Loss Function Restriction:** The closed-form solution only works for squared loss; classification requires a squared loss workaround which may be suboptimal.
- **Hyperparameter Sensitivity:** The method introduces two additional hyperparameters (β for ridge, λ for proximal) that require careful tuning.
- **Computational Overhead:** Computing the closed-form W update requires matrix inversions, adding O(d³) complexity per batch.

## Confidence
- **High Confidence:** The equivalence of the closed-form and alternating optimization (Theorem 1) and the basic proximal regularization mechanism (Mechanism 2) are well-established and empirically validated.
- **Medium Confidence:** The theoretical convergence guarantees in the NTK regime are rigorous but apply to a limiting case. Generalization to finite-width networks is not proven.
- **Low Confidence:** The poor performance of Adam for backbone updates is reported but not fully explained. It may be a dataset-specific artifact or a more fundamental limitation.

## Next Checks
1. **Robustness to Optimizer:** Repeat CIFAR-100 experiments with Adam, RMSprop, and SGD with different momentum values to confirm the claimed sensitivity and identify any exceptions.
2. **Scaling with Feature Width:** Measure training time and test accuracy as a function of feature dimension d on a synthetic regression task to quantify the O(d³) overhead and its impact on performance.
3. **Loss Function Generalization:** On a small classification dataset (e.g., Fashion-MNIST), compare ℓ2 c.f. proximal (squared loss + argmax) against standard cross-entropy training and a hybrid approach (cross-entropy loss, but W updated via ridge regression).