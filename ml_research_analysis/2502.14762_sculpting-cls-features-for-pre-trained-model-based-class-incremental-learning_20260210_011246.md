---
ver: rpa2
title: Sculpting [CLS] Features for Pre-Trained Model-Based Class-Incremental Learning
arxiv_id: '2502.14762'
source_url: https://arxiv.org/abs/2502.14762
tags:
- learning
- methods
- pre-trained
- tosca
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TOSCA, a class-incremental learning method
  that addresses catastrophic forgetting in pre-trained models by deploying a single
  lightweight Learn and Calibrate (LuCA) module on the final [CLS] token. The LuCA
  module combines an adapter for task-specific feature transformation with a calibrator
  for discriminative feature enhancement, achieving a balance between stability and
  plasticity.
---

# Sculpting [CLS] Features for Pre-Trained Model-Based Class-Incremental Learning

## Quick Facts
- arXiv ID: 2502.14762
- Source URL: https://arxiv.org/abs/2502.14762
- Reference count: 40
- Method achieves 4-12% higher accuracy than adapter-based methods and 7-21% higher than prompt-based methods on out-of-distribution datasets

## Executive Summary
TOSCA introduces a novel approach for class-incremental learning with pre-trained models by deploying a single lightweight Learn and Calibrate (LuCA) module on the final [CLS] token of a frozen Vision Transformer backbone. The method achieves a harmonious balance between stability and plasticity by localizing adaptations to the final semantic aggregation point while preserving the pre-trained model's feature hierarchy. Through entropy-based task inference and parameter-efficient fine-tuning, TOSCA demonstrates superior performance across multiple out-of-distribution benchmarks while using significantly fewer parameters than traditional adapter-based approaches.

## Method Summary
TOSCA addresses catastrophic forgetting in class-incremental learning by freezing the entire Vision Transformer backbone and applying a single lightweight LuCA module exclusively on the final [CLS] token. The LuCA module combines an adapter for task-specific feature transformation with a calibrator for discriminative feature enhancement. For each new task, a new LuCA module is instantiated and trained with cross-entropy loss plus L1 regularization to encourage sparse, orthogonal configurations. During inference, the frozen [CLS] feature is routed through all stored LuCA modules, and predictions are selected by minimizing Shannon entropy across the union of task-specific outputs. This approach achieves parameter efficiency by using ~8× fewer parameters than layer-wise adapters while maintaining strong performance on out-of-distribution datasets.

## Key Results
- Achieves 4-12% higher accuracy than adapter-based methods on out-of-distribution datasets
- Demonstrates 7-21% higher accuracy than prompt-based methods across multiple benchmarks
- Uses ~8× fewer parameters than layer-wise adapter approaches in ViT-B/16
- Shows strong stability-plasticity balance through [CLS]-only localization strategy

## Why This Works (Mechanism)

### Mechanism 1: Semantic Aggregation Localization
TOSCA restricts adaptation to the final [CLS] token to maximize stability by preserving the pre-trained feature hierarchy while allowing plasticity at the final decision point. The frozen backbone serves as a robust feature extractor, and the LuCA module acts as an adaptable decision layer, treating the backbone as a "ventral visual stream" and the module as a "prefrontal cortex."

### Mechanism 2: Orthogonal Specialization via Sparsity
The method applies L1-regularization to LuCA module parameters, encouraging sparse configurations across tasks. This sparsity theoretically promotes orthogonal subspaces that minimize interference during inference, with experiments showing accuracy peaking at moderate L1 strength before degrading.

### Mechanism 3: Entropy-Based Task Inference
TOSCA uses entropy minimization to identify the correct task-specific module during inference. The specialized module produces lower-entropy predictions for in-distribution data compared to mismatched modules, enabling task identification without explicit labels.

## Foundational Learning

- **Stability-Plasticity Dilemma**: The core problem TOSCA solves—stability preserves old knowledge while plasticity enables learning new knowledge. Quick check: If you freeze 100% of a model, which capability do you lose: learning new classes or remembering old ones?
- **Vision Transformer ([CLS] Token)**: TOSCA relies on the [CLS] token's role in aggregating global information for classification. Quick check: In a ViT, is the [CLS] token processed independently, or does it attend to patch tokens via self-attention?
- **Parameter-Efficient Fine-Tuning (Adapters)**: Understanding standard adapter architecture (bottleneck layers) is essential to grasp TOSCA's modifications. Quick check: Why do standard adapters typically reduce computational cost compared to full fine-tuning?

## Architecture Onboarding

- **Component map:** Frozen ViT backbone → [CLS] token → LuCA module (adapter → calibrator) → prototypical classifier → entropy minimization selector
- **Critical path:** 1) Freeze backbone 2) Extract [CLS] token 3) Initialize new LuCA module 4) Train with CE + L1 loss 5) Inference via entropy minimization over all modules
- **Design tradeoffs:** Bottleneck dimension r (48 optimal), sparsity λ (moderate levels best), adapter vs calibrator order (proposed order superior)
- **Failure signatures:** Task confusion when entropy gaps shrink, over-regularization causing degraded expressiveness
- **First 3 experiments:** 1) Component ablation (adapter only vs calibrator only) 2) Projection dimension sweep (r ∈ {16, 32, 48, 64}) 3) Orthogonality visualization of weight matrices with/without L1 regularization

## Open Questions the Paper Calls Out

- **Few-shot CIL performance**: How does TOSCA perform when each class has only 1-5 training examples? The authors aim to explore few-shot scenarios in future work.
- **Pre-trained model robustness**: How robust is TOSCA when deployed on pre-trained models with weaker generalization or substantial domain gaps? The method relies heavily on pre-trained model generalization.
- **Task selection scalability**: How does entropy-based task selection degrade as task count grows to 50+ tasks? The method's ability to discriminate among many modules is untested.

## Limitations
- No ablation studies comparing [CLS]-only placement versus layer-wise adapters
- Entropy-based task inference assumes clean separation between task-specific modules without quantifying task similarity
- Orthogonality mechanism via L1 regularization asserted without rigorous mathematical proof

## Confidence

- **High confidence**: Parameter-efficiency claims supported by explicit comparison (~8× fewer parameters)
- **Medium confidence**: Stability-plasticity balance claim lacks direct empirical validation against layer-wise baselines
- **Low confidence**: Orthogonality mechanism asserted without rigorous mathematical proof or empirical verification

## Next Checks

1. **Adapter placement ablation**: Implement and test TOSCA with adapters at multiple transformer layers versus only final [CLS] to measure stability-plasticity tradeoffs
2. **Task similarity analysis**: Measure entropy gaps between correct and incorrect task modules for semantically similar classes to quantify task confusion rates
3. **Orthogonality verification**: Compute pairwise cosine similarity between LuCA weight matrices across tasks to empirically verify orthogonal subspace formation