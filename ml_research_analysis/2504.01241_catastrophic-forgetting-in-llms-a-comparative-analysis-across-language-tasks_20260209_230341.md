---
ver: rpa2
title: 'Catastrophic Forgetting in LLMs: A Comparative Analysis Across Language Tasks'
arxiv_id: '2504.01241'
source_url: https://arxiv.org/abs/2504.01241
tags:
- tasks
- forgetting
- learning
- performance
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses catastrophic forgetting in Large Language
  Models (LLMs) during sequential fine-tuning on multiple Natural Language Understanding
  (NLU) tasks. The research evaluates various open-source LLMs under 10 billion parameters
  on GLUE benchmark tasks (SST-2, MRPC, CoLA, MNLI) using continual fine-tuning with
  prompt engineering.
---

# Catastrophic Forgetting in LLMs: A Comparative Analysis Across Language Tasks

## Quick Facts
- **arXiv ID:** 2504.01241
- **Source URL:** https://arxiv.org/abs/2504.01241
- **Authors:** Naimul Haque
- **Reference count:** 2
- **Primary result:** Sequential fine-tuning on GLUE tasks shows smaller models like Phi-3.5-mini exhibit minimal forgetting (0.02) while larger models show higher learning rates at the cost of increased forgetting.

## Executive Summary
This study investigates catastrophic forgetting in Large Language Models during sequential fine-tuning on multiple Natural Language Understanding tasks. Using a diverse set of open-source LLMs under 10 billion parameters, the research evaluates performance across GLUE benchmark tasks (SST-2, MRPC, CoLA, MNLI) through continual fine-tuning with prompt engineering. The results demonstrate a clear trade-off between learning capability and retention, with Phi-3.5-mini achieving the best balance of minimal forgetting while maintaining strong learning performance, and Orca-2-7b achieving the highest overall accuracy (0.81 average). The study provides empirical evidence that smaller models can effectively balance plasticity and stability, challenging assumptions about model size and continual learning performance.

## Method Summary
The study employs continual instruction fine-tuning, where base LLMs are sequentially adapted to GLUE NLU tasks using prompt engineering. Datasets are transformed into specific instructional prompts (Table 1), and models are fine-tuned in sequence (SST-2 → MRPC → CoLA → MNLI). After each fine-tuning episode, models are evaluated on all previously trained tasks to quantify performance degradation. The methodology measures two key metrics: "Forgetting" (maximum accuracy during training minus final accuracy) and "Learning" (maximum accuracy minus base accuracy). The approach uses accuracy as the primary metric and focuses exclusively on models under 10 billion parameters to ensure applicability to agentic workflows.

## Key Results
- Phi-3.5-mini exhibits minimal forgetting (0.02) while maintaining strong learning capabilities across sequential tasks
- Orca-2-7b achieves the best overall performance with 0.81 average accuracy after fine-tuning
- Larger models (>7B parameters) demonstrate higher learning rates but suffer from increased forgetting, confirming the plasticity-stability trade-off
- Sequential fine-tuning without regularization techniques results in measurable performance degradation on earlier tasks

## Why This Works (Mechanism)
The study demonstrates that catastrophic forgetting occurs when neural networks overwrite prior knowledge during sequential training on new tasks. The use of instruction fine-tuning with prompt engineering helps stabilize learning by providing explicit task instructions, though this alone cannot prevent forgetting. The observed trade-off between plasticity (ability to learn new tasks) and stability (ability to retain old knowledge) follows established principles in continual learning, where models optimized for rapid adaptation tend to be more susceptible to interference from subsequent training.

## Foundational Learning

- **Concept: Catastrophic Forgetting**
  - **Why needed here:** This is the central problem the paper addresses. Understanding that neural networks tend to overwrite prior knowledge when sequentially trained on new tasks is essential.
  - **Quick check question:** What would happen to model accuracy on Task A immediately after it is fine-tuned on Task B?

- **Concept: Instruction Fine-Tuning & Prompt Engineering**
  - **Why needed here:** The study's methodology relies on transforming datasets into specific instructional prompts. Understanding how prompts guide model behavior is key to replicating the experiment.
  - **Quick check question:** How does formatting a dataset sample as an instruction (e.g., "Analyze the sentiment...") change the model's learning objective compared to raw text completion?

- **Concept: Trade-off: Plasticity vs. Stability**
  - **Why needed here:** The paper identifies a core trade-off where models good at learning new tasks (plasticity) are bad at remembering old ones (stability). This frame is needed to interpret the results.
  - **Quick check question:** In the context of this paper, does a "high learning rate" metric typically correspond to low or high forgetting for larger models?

## Architecture Onboarding

- **Component map:**
  - Base Model (M0) -> Prompt Engine (PE(X)) -> Fine-tuning Loop (M0 -> M1 -> ... -> Mn) -> Evaluation Harness -> Metrics

- **Critical path:**
  1. Select base model under 10B parameters.
  2. Prepare GLUE tasks (SST-2, MRPC, CoLA, MNLI) using the defined prompt templates (Table 1).
  3. Execute fine-tuning sequentially.
  4. **Crucial Step:** After training on Task Ti, immediately evaluate on *all* prior tasks {T1...Ti} to quantify degradation.

- **Design tradeoffs:**
  - **Model Selection:** Choosing a smaller model (e.g., Phi-3.5-mini) for stability (min forgetting) vs. a larger one (e.g., Qwen2.5-7B) for peak performance (max learning).
  - **Metrics:** Using accuracy as the sole metric, which may not capture nuances in model confidence or calibration, as noted in the paper's limitations.

- **Failure signatures:**
  - **Runaway Forgetting:** Observing a significant drop in accuracy on Task T1 after fine-tuning on Task T2 (e.g., >10-20% drop), particularly in larger models (>7B).
  - **Zero-shot Failure:** The base model (M0) performs very poorly on a task, meaning the "learning" metric has little room to improve or may be learning from noise.

- **First 3 experiments:**
  1. **Baseline Run:** Take a stable model identified in the paper (Phi-3.5-mini) and replicate the 4-task GLUE sequence. Verify that observed forgetting is near the reported ~0.02.
  2. **Ablation on Scale:** Run the same 4-task sequence with a larger model (Llama-3.1-8B) and a smaller one (Llama-3.2-1B) to directly compare the stability-plasticity trade-off curve.
  3. **Prompt Robustness Check:** Alter the prompt templates (e.g., make them less explicit) and re-run the fine-tuning on a single task pair to test the assumption that prompt engineering is a key driver of retention.

## Open Questions the Paper Calls Out

- **Open Question 1:** How do regularization-based or memory-replay strategies compare to standard sequential fine-tuning in mitigating forgetting for sub-10B parameter models?
  - **Basis in paper:** [explicit] The conclusion states, "Future work should explore more advanced continual learning techniques to mitigate catastrophic forgetting."
  - **Why unresolved:** The study isolated sequential fine-tuning to establish a baseline, deliberately excluding methods like Elastic Weight Consolidation (EWC) or Gradient Episodic Memory (GEM) mentioned in the literature review.
  - **What evidence would resolve it:** A comparative study applying EWC or replay mechanisms to the specific model set (e.g., Phi-3.5-mini, Orca-2-7b) used in this paper.

- **Open Question 2:** Does the observed trade-off—where higher learning rates correlate with increased forgetting—persist in models exceeding 10 billion parameters?
  - **Basis in paper:** [inferred] The limitations section notes the study "focused on models under 10 billion parameters," acknowledging that results "may not generalize to larger models."
  - **Why unresolved:** The experimental scope was restricted to smaller architectures suitable for agentic workflows, leaving the scaling laws of forgetting in larger models untested.
  - **What evidence would resolve it:** Applying the same continual fine-tuning protocol to larger variants (e.g., 70B parameter models) to check if the forgetting metrics scale linearly or change qualitatively.

- **Open Question 3:** To what extent does the specific phrasing of task instructions (prompt engineering) mask or exaggerate the true degree of catastrophic forgetting?
  - **Basis in paper:** [inferred] The authors note that "Relying on prompt engineering may introduce biases affecting performance comparisons."
  - **Why unresolved:** The study relied on fixed prompt templates (Table 1) to stabilize training, without testing how sensitive the "forgetting" metric is to variations in these prompts.
  - **What evidence would resolve it:** Ablation studies measuring performance variance when the prompt format for Task A is altered after the model has already been fine-tuned on Task B.

## Limitations

- The study is limited to 4 GLUE tasks without evaluation on diverse task types (e.g., multi-step reasoning or generation tasks), which may limit generalizability to broader LLM applications.
- Using accuracy as the sole metric may not capture nuanced performance changes in model confidence or calibration.
- The lack of specification regarding task ordering sequence and fine-tuning hyperparameters creates uncertainty in reproducing exact experimental conditions.

## Confidence

- **High Confidence:** The core finding that smaller models like Phi-3.5-mini exhibit lower forgetting rates (0.02) while maintaining learning capability is well-supported by the experimental results.
- **Medium Confidence:** The claim that Orca-2-7b achieves the best overall performance (0.81 average accuracy) is supported by the data, though specific conditions require clarification.
- **Low Confidence:** The generalizability of findings to non-GLUE tasks or to different prompt engineering approaches remains uncertain.

## Next Checks

1. **Task Order Sensitivity:** Replicate the experiment with multiple different task sequences (e.g., reverse order, random order) to determine whether the observed forgetting patterns are consistent or dependent on specific task progression.

2. **Hyperparameter Impact Study:** Systematically vary learning rates and batch sizes across the same model comparisons to quantify how optimization choices affect the plasticity-stability trade-off curve.

3. **Cross-Domain Generalization:** Apply the same sequential fine-tuning methodology to a non-GLUE benchmark (e.g., SuperGLUE or a reasoning dataset) to test whether smaller models maintain their forgetting advantage across different task types.