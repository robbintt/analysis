---
ver: rpa2
title: 'MINT: Memory-Infused Prompt Tuning at Test-time for CLIP'
arxiv_id: '2506.03190'
source_url: https://arxiv.org/abs/2506.03190
tags:
- prompt
- mint
- prompts
- test-time
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'MINT: Memory-Infused Prompt Tuning at Test-time for CLIP MINT
  addresses the challenge of improving Vision-Language Pre-trained Models'' (VLMs)
  generalization under test-time distribution shifts. The core method introduces a
  Memory Prompt Bank (MPB) that stores learnable key-value prompt pairs, functioning
  as a memory of previously seen samples.'
---

# MINT: Memory-Infused Prompt Tuning at Test-time for CLIP

## Quick Facts
- arXiv ID: 2506.03190
- Source URL: https://arxiv.org/abs/2506.03190
- Authors: Jiaming Yi; Ruirui Pan; Jishen Yang; Xiulong Yang
- Reference count: 30
- Primary result: Achieves 63.12% average Top-1 accuracy across four ImageNet benchmarks, with 78.68% on ImageNet-R

## Executive Summary
MINT introduces Memory-Infused Prompt Tuning at Test-time for CLIP, a method that dynamically adapts vision-language models during inference without source data or retraining. The approach leverages a learnable Memory Prompt Bank (MPB) that stores key-value prompt pairs, which are retrieved and composed using hierarchical visual features from incoming images to create Associative Prompts. These prompts are injected into the image encoder to improve generalization under distribution shifts. MINT combines this with learnable text prompts and confidence-filtered entropy minimization, achieving state-of-the-art test-time adaptation performance across multiple challenging benchmarks.

## Method Summary
MINT operates by maintaining a learnable Memory Prompt Bank (MPB) of key-value pairs during test-time. For each incoming image, hierarchical visual features extracted from multiple encoder layers serve as queries to retrieve the most relevant memory prompts via cosine similarity. The retrieved prompts are averaged to form an Associative Prompt, which is prepended to the image encoder input alongside the original patches. Simultaneously, learnable text prompts are prepended to class embeddings. The model generates 64 augmented views per image, applies confidence-based filtering to select the most reliable predictions (top 10% lowest entropy), and optimizes both text prompts and MPB parameters via entropy minimization plus similarity regularization using AdamW. This enables continuous adaptation while preserving pre-trained knowledge and avoiding source data requirements.

## Key Results
- Achieves 63.12% average Top-1 accuracy across four ImageNet benchmarks (ImageNet-R, A, V2, Sketch)
- Breakthrough performance of 78.68% on ImageNet-R
- Outperforms existing test-time adaptation methods by significant margins
- Ablation studies confirm the effectiveness of hierarchical feature querying, confidence filtering, and memory-based prompt retrieval

## Why This Works (Mechanism)

### Mechanism 1: Memory-Based Associative Prompt Retrieval
MINT's MPB enables sample-specific visual context generation by retrieving and composing stored prompt components based on input similarity. The MPB stores N key-value pairs where keys serve as retrieval indices and values are prompt tokens. For each test image, hierarchical visual features extracted from multiple encoder layers serve as queries; cosine similarity matching retrieves the top-k most relevant memory prompts, which are then averaged to form a single Associative Prompt injected at the image encoder input. This works because visual features from test images share exploitable similarity structure with previously seen patterns, allowing meaningful retrieval from accumulated memory.

### Mechanism 2: Hierarchical Multi-Layer Feature Querying
Querying the MPB with features from multiple encoder layers captures both low-level and high-level semantics, enabling more precise prompt retrieval than single-layer approaches. The [CLS] token is extracted from N_layers distinct transformer layers. Earlier layers capture texture/edge information; deeper layers capture semantic concepts. Each layer's query retrieves its own top-k prompts; the union across layers forms the final associative prompt. This design leverages the complementary information from different visual abstraction levels to achieve more complete coverage of relevant memory entries.

### Mechanism 3: Confidence-Filtered Entropy Minimization
Selecting only high-confidence augmented views for loss computation stabilizes test-time adaptation by filtering noisy or ambiguous samples. Each test image generates 64 augmented views via random cropping. Predictions with lowest self-entropy (top 10%) contribute to the loss. The optimization objective combines entropy minimization with a similarity regularization term encouraging query-key alignment. AdamW updates text prompts and MPB parameters jointly. This approach assumes low prediction entropy correlates with correct pseudo-labels, filtering out unreliable high-entropy predictions that would propagate errors.

## Foundational Learning

- **Vision-Language Model Contrastive Pre-training (CLIP)**: Understanding how CLIP's dual-encoder architecture aligns images and text in shared embedding space is essential, as MINT builds on this pre-aligned structure. Quick check: Can you explain why cosine similarity between image and text embeddings enables zero-shot classification?

- **Prompt Tuning vs. Fine-tuning**: MINT modifies prompts (input-level continuous vectors) rather than model weights, making adaptation lightweight and preserving pre-trained knowledge. Quick check: What is the difference between prepending learnable tokens to encoder input versus updating encoder weights directly?

- **Test-Time Adaptation (TTA) Constraints**: MINT operates without source data, labels, or backpropagation through the full model; recognizing these constraints clarifies why entropy minimization and memory mechanisms are necessary substitutes for supervised learning. Quick check: Why can't standard supervised loss be used during test-time, and what self-supervised signal does MINT use instead?

## Architecture Onboarding

- **Component map:** Test Image → Augmentation → 64 Views → Image Encoder → Hierarchical Features → MPB Retrieval → Associative Prompt → Prompt Injection → Adapted Features → Prediction → Confidence Filter → Loss → Gradient Update on Text Prompts + MPB

- **Critical path:** Image → hierarchical feature extraction → MPB query → associative prompt assembly → encoder injection → prediction → confidence filtering → entropy+similarity loss → parameter update. Any failure in retrieval quality or injection mechanism propagates directly to adaptation quality.

- **Design tradeoffs:**
  - MPB size (N_MPB): Larger bank stores more patterns but increases retrieval cost and requires more data to populate meaningfully. Paper finds 512 optimal.
  - Prompt length (L_m): Longer prompts are more expressive but increase parameter count; L_m=2 balances expressiveness against test-data scarcity.
  - Injection layer: First-layer injection outperforms deeper layers in ablations—likely because early visual features are more malleable for guiding subsequent processing.

- **Failure signatures:**
  - Prompt collapse: MPB keys converge to similar vectors, reducing retrieval diversity.
  - Confidence miscalibration: If entropy consistently fails to identify correct predictions, filtered gradients reinforce errors.
  - Overfitting to recent samples: MPB continuously updates; without regularization, earlier-domain knowledge may be overwritten.

- **First 3 experiments:**
  1. Reproduce ablation comparing MINT against static visual prompts and text-only baselines on ImageNet-A to verify dynamic associative prompt mechanism's contribution.
  2. Test MPB size sweep across {128, 256, 512, 1024} on ImageNet-R to confirm 512 is optimal and understand sensitivity curves.
  3. Vary confidence threshold κ over {5%, 10%, 20%, 50%} to characterize how filtering aggressiveness affects accuracy and stability.

## Open Questions the Paper Calls Out

- Can the memory retrieval and composition mechanisms be optimized to reduce computational costs and improve inference speed for large-scale applications? The authors explicitly state exploring more efficient memory retrieval and composition is key for future work, as current overhead may hinder deployment in resource-constrained environments.

- Does the MINT framework generalize effectively to other architectures and complex multimodal tasks beyond image classification? The authors propose extending MINT to video understanding and VQA to validate its versatility, as current validation is limited to image classification benchmarks.

- Is MINT robust to hyperparameter settings and initialization without requiring scenario-specific tuning? The authors note that initialization and hyperparameters of MPB components may require scenario-specific tuning, and it's unclear if optimal settings transfer universally across different dataset scales.

## Limitations
- The optimal MPB size of 512 and prompt length of 2 are dataset-specific and may not generalize to all domain shift scenarios
- Claims about "long-tail robustness" lack supporting quantitative analysis beyond the four benchmark datasets
- The framework introduces computational overhead that may limit deployment in high-frequency or resource-constrained environments

## Confidence
- **High confidence:** The core mechanism of hierarchical feature-based memory prompt retrieval is technically sound and well-justified by CLIP's architecture
- **Medium confidence:** The claimed performance breakthrough depends on multiple interacting design choices whose individual contributions are not fully isolated
- **Low confidence:** Generalization claims to arbitrary unseen distributions lack supporting quantitative analysis

## Next Checks
1. Reproduce Table 2 ablations comparing MINT against static visual prompts and text-only baselines on ImageNet-A to verify the dynamic associative prompt mechanism's contribution
2. Sweep MPB size across {128, 256, 512, 1024} on ImageNet-R to confirm the claimed optimal of 512 and characterize performance sensitivity to memory capacity
3. Vary confidence threshold κ over {5%, 10%, 20%, 50%} to quantify the trade-off between stability and information retention in the confidence filtering mechanism