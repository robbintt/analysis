---
ver: rpa2
title: 'Have ASkotch: A Neat Solution for Large-scale Kernel Ridge Regression'
arxiv_id: '2407.10070'
source_url: https://arxiv.org/abs/2407.10070
tags:
- nystr
- lemma
- sampling
- uniform
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ASkotch, a new solver for large-scale kernel
  ridge regression (KRR). The authors address the scalability challenges of full KRR
  by combining sketch-and-project iterative methods with Nystrom approximations, enabling
  linear convergence without expensive per-iteration costs.
---

# Have ASkotch: A Neat Solution for Large-scale Kernel Ridge Regression

## Quick Facts
- arXiv ID: 2407.10070
- Source URL: https://arxiv.org/abs/2407.10070
- Reference count: 40
- One-line primary result: ASkotch achieves state-of-the-art performance on large-scale kernel ridge regression by combining sketch-and-project methods with Nystrom approximations

## Executive Summary
This paper introduces ASkotch, a new solver for large-scale kernel ridge regression (KRR) that addresses the scalability challenges of full KRR. By combining sketch-and-project iterative methods with Nystrom approximations, ASkotch enables linear convergence without expensive per-iteration costs. The method outperforms state-of-the-art approaches including EigenPro, PCG, and Falkon on 23 diverse KRR tasks, demonstrating that full KRR can achieve better predictive performance than inducing points KRR while handling massive datasets up to 10⁸ samples.

## Method Summary
ASkotch is an iterative solver for large-scale KRR that replaces exact block projections with Nystrom approximations to achieve linear-time iterations. The method uses Approximate Ridge Leverage Score (ARLS) sampling for condition-number-free convergence and automatically computes stepsizes via preconditioned smoothness constants. It combines randomized projection, Nystrom low-rank factorization, and Woodbury identity updates with Nesterov acceleration to solve the linear system $(K + \lambda I)w = y$ efficiently at scale.

## Key Results
- ASkotch outperforms EigenPro, PCG, and Falkon on 23 diverse KRR tasks across multiple domains
- Achieves condition-number-free linear convergence under appropriate assumptions with near-optimal O(n²) complexity
- Successfully scales to massive datasets up to 10⁸ samples while maintaining superior predictive performance
- Demonstrates that full KRR with ASkotch is a viable and superior alternative to inducing points methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Replacing the exact block projection with a Nyström approximation enables linear-time iterations without breaking convergence guarantees.
- **Mechanism:** Standard Sketch-and-Project (SAP) requires inverting a block matrix $(K_{BB} + \lambda I)$ at O(b³) cost. ASkotch approximates this matrix using a rank-r Nyström sketch ($\hat{K}_{BB}$). By using the Woodbury identity on $(\hat{K}_{BB} + \rho I)$, the inversion cost drops to O(br), allowing for massive block sizes ($b \approx 10^4$) that exploit GPU parallelism.
- **Core assumption:** The damping parameter $\rho$ is set such that $\hat{K}_{BB} + \rho I$ spectrally approximates $K_{BB} + \lambda I$ sufficiently well to act as a preconditioner.
- **Evidence anchors:**
  - [Page 9, Algo 1] Details the Nyström update step replacing exact inversion
  - [Page 14, Lemma 3] Proves the Nyström projector approximates the SAP projector if the stepsize is set via the preconditioned smoothness constant
  - [Corpus] The paper "Joker: Joint Optimization Framework for Lightweight Kernel Machines" corroborates the general need for lightweight kernel approximations

### Mechanism 2
- **Claim:** Approximate Ridge Leverage Score (ARLS) sampling enables condition-number-free convergence rates.
- **Mechanism:** Standard iterative methods suffer convergence slowdowns based on the condition number $\kappa(K)$. ASkotch uses ARLS sampling to bias block selection toward "high-leverage" coordinates. The paper proves a novel reduction from ARLS sampling to Determinantal Point Processes (DPPs), showing that this sampling strategy effectively preconditions the problem, making the convergence rate depend on the effective dimension $d_\lambda(K)$ rather than $\kappa(K)$.
- **Core assumption:** The kernel matrix has a relatively small effective dimension $d_\lambda(K)$ (i.e., fast spectral decay), which is standard for Laplacian and Matérn kernels.
- **Evidence anchors:**
  - [Page 5] "Technical Contribution 1" describes the ARLS-to-DPP reduction
  - [Page 24, Corollary 18] Explicitly states the condition-number-free convergence when $d_\lambda(K) = O(\sqrt{n})$
  - [Corpus] "Uniform convergence for Gaussian kernel ridge regression" provides background on convergence challenges

### Mechanism 3
- **Claim:** Automatic stepsize computation via preconditioned smoothness constants stabilizes the approximate projection.
- **Mechanism:** Because the Nyström update is not a true mathematical projection (it lacks idempotency), a fixed stepsize of 1 (used in exact SAP) can cause divergence. ASkotch estimates a dynamic stepsize $\eta_B = 1/L_{P_B}$ using randomized powering. This ensures the "Nyström projector" $\hat{\Pi}_{B,\rho}$ remains bounded by the identity matrix in the Loewner order.
- **Core assumption:** The randomized powering algorithm converges rapidly enough (within ~10 iterations) to provide a useful bound for the stepsize.
- **Evidence anchors:**
  - [Page 10, Section 2.4] Describes the `get_L` subroutine for automatic stepsize computation
  - [Page 14, Eq 9] Shows the inequality $\sigma_{P_B}/\hat{L}_{P_B} \Pi_B \preceq \hat{\Pi}_{B,\rho}$ which relies on this stepsize
  - [Corpus] Evidence weak/no direct corpus support for this specific automatic stepsize technique in KRR

## Foundational Learning

- **Concept: Kernel Ridge Regression (KRR)**
  - **Why needed here:** ASkotch is a solver for the specific linear system $(K + \lambda I)w = y$ derived from KRR. You cannot understand the "approximate Hessian" mechanism without understanding that the Hessian is the kernel matrix itself.
  - **Quick check question:** Can you explain why solving the linear system $(K + \lambda I)w = y$ is computationally prohibitive for $n=10^8$ using standard Cholesky decomposition?

- **Concept: Sketch-and-Project (SAP)**
  - **Why needed here:** This is the iterative framework ASkotch modifies. Understanding that SAP works by randomly projecting the current solution onto the solution space of a subset of constraints is necessary to understand the "block update" logic.
  - **Quick check question:** How does the convergence rate of SAP depend on the spectrum of the expected projection matrix $E[\Pi_B]$?

- **Concept: Nyström Approximation**
  - **Why needed here:** This is the core approximation technique. You must understand how a low-rank approximation $\hat{K} = (K\Omega)(\Omega^T K \Omega)^+ (K\Omega)^T$ allows for cheap matrix inversion via the Woodbury identity.
  - **Quick check question:** Why does the Nyström method reduce the complexity of inverting a matrix from $O(b^3)$ to $O(br)$, where $r$ is the rank?

## Architecture Onboarding

- **Component map:** Sampler -> Kernel Oracle -> Nyström Engine -> Step Estimator -> Solver -> Accelerator
- **Critical path:** The sequential dependency is Sampling → Kernel Eval → Nyström Factorization → Powering → Woodbury Solve. The kernel evaluation and Nyström factorization are typically the bottlenecks.
- **Design tradeoffs:**
  - **Block size ($b$) vs. Memory:** Larger $b$ improves convergence (better preconditioning) but increases memory for $K_{BB}$. The paper uses $b=n/100$.
  - **Rank ($r$) vs. Precision:** Increasing $r$ improves the Nyström approximation quality (faster convergence) but linearly increases the cost of the Woodbury solve and factorization.
  - **ARLS vs. Uniform:** ARLS offers better theoretical guarantees but requires the BLESS preprocessing step ($O(n^2)$ setup sometimes). Uniform is free to start but may converge slower on non-uniform data.
- **Failure signatures:**
  - **Divergence (NaNs):** Often caused by a damping parameter $\rho$ that is too small or a faulty stepsize estimate $L_{P_B}$ (Page 10).
  - **OOM (Out of Memory):** Block size $b$ is too large for the GPU when materializing $K_{BB}$.
  - **Slow Convergence:** The rank $r$ is too small ($r < d_\lambda(K)$), causing the preconditioner to be ineffective.
- **First 3 experiments:**
  1. **Sanity Check:** Run ASkotch on a small synthetic dataset ($n=1000$) with a known solution (e.g., $y = K w_{true} + \epsilon$) to verify that the relative residual $\|Kw - y\| / \|y\|$ decreases linearly to machine precision.
  2. **Scaling Benchmark:** Replicate the "Taxi" experiment (Page 28, Fig 1) on a subsample of $n=10^5$ or $10^6$. Compare wall-clock time against a standard PCG baseline to validate the empirical speedup.
  3. **Ablation Study:** Run the solver with acceleration disabled (`use_accel=False`, effectively "Skotch") vs. enabled to determine the contribution of Nesterov acceleration on your specific dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the acceleration parameters ˆµ and ˆν in ASkotch be automatically selected without manual tuning?
- **Basis in paper:** [explicit] "We believe it is possible to make the acceleration in ASkotch parameter-free, as done in Dereziński et al. (2025b), but we leave this extension to future work."
- **Why unresolved:** Current implementation requires users to set ˆµ = λ and ˆν = n/b manually, with constraints ˆµ ≤ ˆν and ˆµˆν ≤ 1.
- **What evidence would resolve it:** An adaptive scheme that dynamically adjusts acceleration parameters during optimization while maintaining theoretical convergence guarantees.

### Open Question 2
- **Question:** Can ASkotch be extended to distributed implementations that scale to n ≳ 10⁹ training points?
- **Basis in paper:** [explicit] The conclusion states the goal to "create a distributed implementation of ASkotch that scales to datasets with n ≳ 10⁹ training points (such as the full taxi dataset)."
- **Why unresolved:** The current single-GPU implementation handled n = 10⁸ on taxi, but the full dataset has n ≈ 10⁹ and requires distributed computing across multiple devices.
- **What evidence would resolve it:** A multi-GPU or multi-node implementation maintaining near-linear speedup while preserving convergence properties.

### Open Question 3
- **Question:** Can ASkotch deliver high-quality solutions using mixed precision (single/half) to reduce memory and compute requirements?
- **Basis in paper:** [explicit] The conclusion lists the goal to "develop a mixed-precision version of ASkotch that delivers high-quality results with less memory and compute."
- **Why unresolved:** While ASkotch is stable in single precision, further precision reduction (half precision) may introduce numerical instabilities that require careful algorithmic modifications.
- **What evidence would resolve it:** Demonstrating comparable predictive accuracy with reduced precision while achieving significant memory savings and computational speedups.

## Limitations
- Theoretical guarantees depend on the assumption that the effective dimension $d_\lambda(K)$ is small (typically $O(\sqrt{n})$), which may not hold for all kernel and data distributions
- The BLESS algorithm for ARLS sampling can be sensitive to its own parameters and may fail on extremely large or high-dimensional datasets
- The adaptive damping strategy assumes that the smallest eigenvalue of the Nystrom approximation reliably bounds the kernel matrix's spectral properties

## Confidence
- **High confidence:** The empirical scaling results (linear convergence on datasets up to $n=10^8$) and the basic correctness of the algorithmic pipeline (Woodbury application, randomized powering)
- **Medium confidence:** The theoretical reduction from ARLS to DPP sampling and the condition-number-free convergence claims, as these rely on unproven assumptions about the quality of the leverage score estimates
- **Medium confidence:** The automatic stepsize computation, as the randomized powering procedure is not extensively validated for robustness to numerical precision issues in the appendix

## Next Checks
1. **Small-scale correctness check:** Run ASkotch on a $n=1000$ synthetic dataset with a known ground truth $w_{true}$ and verify the relative residual $\|K_\lambda w - y\| / \|y\|$ decreases linearly to machine precision
2. **Ablation of Nystrom rank:** Run the Taxi experiment (or a $n=10^6$ subsample) with different Nystrom ranks $r \in \{50, 100, 200, 500\}$. Plot the convergence curves to empirically verify the predicted improvement in convergence rate with higher $r$
3. **Comparison to exact SAP:** Implement a small-scale exact Sketch-and-Project solver for $n \leq 2000$ (using Cholesky on $K_{BB}$). Compare its convergence curve to ASkotch with $b=100$ and $r=50$ to validate that the approximation does not fundamentally break the linear convergence property