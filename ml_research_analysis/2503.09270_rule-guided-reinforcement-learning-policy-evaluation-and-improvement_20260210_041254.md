---
ver: rpa2
title: Rule-Guided Reinforcement Learning Policy Evaluation and Improvement
arxiv_id: '2503.09270'
source_url: https://arxiv.org/abs/2503.09270
tags:
- rules
- policy
- rule
- policies
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents LEGIBLE, a framework for evaluating and improving
  deep reinforcement learning (RL) policies using domain knowledge expressed as rules.
  The approach mines rules from trained RL policies, generalizes them using metamorphic
  relations to express symmetries and domain knowledge, and guides policy execution
  with these rules to detect weaknesses and improve performance.
---

# Rule-Guided Reinforcement Learning Policy Evaluation and Improvement

## Quick Facts
- arXiv ID: 2503.09270
- Source URL: https://arxiv.org/abs/2503.09270
- Reference count: 17
- Key outcome: LEGIBLE framework improves DQN policy performance by up to 273% through rule-guided execution that reveals generalization failures

## Executive Summary
This paper presents LEGIBLE, a framework for evaluating and improving deep reinforcement learning (RL) policies using domain knowledge expressed as rules. The approach mines rules from trained RL policies, generalizes them using metamorphic relations to express symmetries and domain knowledge, and guides policy execution with these rules to detect weaknesses and improve performance. Experiments with DQN policies trained in six PAC-Man and five highway driving environments show that rule-guided execution consistently improves performance, with average cumulative rewards increasing by up to 273% compared to non-guided execution. The method also outperforms random testing baselines in detecting policy weaknesses.

## Method Summary
LEGIBLE combines rule mining from trained policies with metamorphic relations to create generalized rule sets that guide execution. Rules are mined using LIME for feature importance and RIPPER for rule learning, with feature importance integrated into the rule-growing optimization. Domain knowledge is encoded as metamorphic relations that specify how decisions should transform under state changes (e.g., symmetries). During execution, positive rules are enforced directly while negative rules block actions by setting Q-values to negative infinity. Weaknesses are identified through Welch tests comparing cumulative rewards with/without rule guidance, and greedy selection composes rule sets that collectively improve performance.

## Key Results
- Rule-guided execution improved cumulative rewards by up to 273% in highway driving environments
- The framework consistently outperformed random testing baselines in detecting policy weaknesses
- Greedy selection of rule sets produced policies with better performance than extended training alone
- Welch tests confirmed statistical significance (p < 0.05) for detected weaknesses across environments

## Why This Works (Mechanism)

### Mechanism 1: Rule Mining Captures High-Value Policy Decisions
Mining rules from a trained policy creates a partial symbolic representation that captures the most important decisions while filtering noise. The system uses LIME to identify feature importance for action selection, then applies RIPPER to generate Horn clause rules with feature importance weights integrated into the rule-growing optimization. Rules are filtered by accuracy (≥0.9) and coverage (≥0.01) thresholds to ensure they represent genuine policy strategy.

### Mechanism 2: Metamorphic Relations Transfer Learned Behavior to Symmetric Situations
Domain knowledge expressed as metamorphic relations enables systematic generalization of rules to related situations where the policy may have failed to learn equivalent behaviors. MRs are defined as sets of feature relations connecting actions and features across state-action pairs, enabling the generation of rule sets that should be equivalent under domain symmetries. For example, a 90-degree clockwise rotation MR maps northward-movement rules to eastward-movement rules by relating corresponding directional features.

### Mechanism 3: Rule-Guided Execution Reveals Generalization Failures and Improves Performance
Enforcing generalized rules during execution identifies policy weaknesses (situations where the policy makes suboptimal decisions it wouldn't make in symmetric states) and produces improved policies through rule composition. Algorithm 1 monitors policy execution: if a single positive rule triggers, its action is enforced; otherwise, actions blocked by negative rules have their Q-values set to negative infinity before selecting the maximum. Weaknesses are identified via Welch tests comparing cumulative rewards with/without rule guidance.

## Foundational Learning

- **Concept: Q-Learning and State-Action Value Functions**
  - Why needed here: The framework specifically targets Q-learning-based policies, using Q-values to identify avoided actions for negative rule mining and to select actions when rules don't uniquely determine behavior.
  - Quick check question: Given a Q-function Q(s,a), how would you identify which action a policy avoids in state s?

- **Concept: Metamorphic Testing**
  - Why needed here: LEGIBLE adapts metamorphic relations from software testing to RL, using them to specify how agent decisions should change in response to state changes (e.g., symmetries).
  - Quick check question: What is the key difference between metamorphic testing and traditional testing with explicit correctness criteria?

- **Concept: LIME (Local Interpretable Model-Agnostic Explanations)**
  - Why needed here: LIME provides feature importance scores that weight the rule mining process, ensuring extracted rules reflect what the policy actually considers important.
  - Quick check question: How does LIME generate explanations for a prediction—does it require access to the model's internal parameters?

## Architecture Onboarding

- **Component map:**
  Trained Policy π → [Rule Mining: LIME + RIPPER] → Raw Rules R → [Rule Generalization] → Generalized Rule Sets {R_g} → [Rule-Guided Execution] → Weakness Detection + Improved Policy

- **Critical path:** The evaluation loop (Algorithm 1) is the core runtime component. It must efficiently check rule triggers, handle rule conflicts, and fall back gracefully. The greedy selection loop (Algorithm 2) depends on successful weakness detection across all candidate rule sets.

- **Design tradeoffs:**
  - Manual vs. automated MRs: MRs must be manually specified based on domain knowledge, providing interpretability and control but requiring domain expertise
  - Rule filtering thresholds: Accuracy ≥0.9 and coverage ≥0.01 filter aggressive but may discard useful rare-case rules
  - Conflict resolution: When multiple positive rules trigger, the system falls back to Q-values with negative rule blocking, which is conservative but may miss opportunities for more sophisticated resolution

- **Failure signatures:**
  - No weaknesses detected: May indicate MRs don't match actual environment symmetries, or the policy has already generalized well
  - Performance degrades under rule guidance: Suggests generalized rules are invalid or overconstrain the policy
  - Random action selection frequent: Indicates rule coverage is too sparse or conflicts are common

- **First 3 experiments:**
  1. Baseline validation: Run the trained policy without rule guidance for n=100-250 episodes to establish baseline cumulative reward
  2. Single rule set evaluation: For each mined rule ρ, generate R_g using your MRs, run Algorithm 1, and compare cumulative reward to baseline via Welch test
  3. Composed rule improvement: Apply Algorithm 2's greedy selection to compose multiple weakness-revealing rule sets and measure final cumulative reward

## Open Questions the Paper Calls Out

### Open Question 1
Can the enforcement of generalized rules be integrated directly into the reinforcement learning training process, rather than applied post-hoc, to guide the policy toward optimal behavior during learning? The conclusion states the authors will "work on a more seamless integration of generalized rules into RL policies through rule-guided training."

### Open Question 2
How can metamorphic testing (MT) for RL policies be effectively combined with environment-focused testing methods, such as search-based testing? The conclusion proposes to "study how to combine MT with existing work that focuses on the environment, like search-based testing."

### Open Question 3
Can the generation of Metamorphic Relations (MRs) be automated to reduce the reliance on manual domain knowledge? The paper notes that "MRs generally need to be created manually and they reflect some domain knowledge," identifying manual definition as a requirement and potential bottleneck.

### Open Question 4
Does the efficacy of LEGIBLE extend to on-policy algorithms (e.g., PPO) given that the current method relies on Q-value manipulation? Appendix C states that "rule-guided execution does not work well for on-policy algorithms, like PPO," and the primary experiments focus exclusively on DQN.

## Limitations

- Manual specification of metamorphic relations requires substantial domain expertise and may not generalize to environments without clear exploitable symmetries
- The framework depends heavily on the quality of the initial RL policy - if the policy hasn't learned meaningful behaviors to generalize, rule-guided execution may provide little benefit
- The claimed 273% performance improvements need further validation across different environment variants and statistical analysis of improvement distributions

## Confidence

- **High Confidence**: The core mechanism of using metamorphic relations to generalize learned behaviors to symmetric situations is theoretically sound
- **Medium Confidence**: The effectiveness of rule-guided execution for policy improvement is supported by experimental results, though the magnitude of improvements warrants further validation
- **Low Confidence**: The claim that this approach outperforms random testing baselines in detecting policy weaknesses lacks sufficient comparative analysis

## Next Checks

1. **Ablation Study on MR Specification**: Systematically vary the metamorphic relations to quantify how much performance improvement depends on correctly specified MRs versus the rule-guided execution framework itself.

2. **Cross-Environment Generalization**: Apply the framework to environments without clear exploitable symmetries to test whether rule-guided execution still provides meaningful improvements or if performance degrades significantly.

3. **Statistical Power Analysis**: Conduct power analysis on the Welch tests used for weakness detection to determine if the sample sizes provide adequate statistical power to reliably detect true weaknesses versus random variation.