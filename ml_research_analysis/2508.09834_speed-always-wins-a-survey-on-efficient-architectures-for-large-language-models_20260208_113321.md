---
ver: rpa2
title: 'Speed Always Wins: A Survey on Efficient Architectures for Large Language
  Models'
arxiv_id: '2508.09834'
source_url: https://arxiv.org/abs/2508.09834
tags:
- arxiv
- attention
- preprint
- efficient
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey systematically reviews efficient architectures for
  large language models, addressing the challenge of high computational costs associated
  with traditional transformer models. It covers a wide range of approaches including
  linear sequence modeling, sparse sequence modeling, efficient full attention mechanisms,
  sparse mixture-of-experts, hybrid architectures, and diffusion large language models.
---

# Speed Always Wins: A Survey on Efficient Architectures for Large Language Models

## Quick Facts
- arXiv ID: 2508.09834
- Source URL: https://arxiv.org/abs/2508.09834
- Authors: Weigao Sun; Jiaxi Hu; Yucheng Zhou; Jusen Du; Disen Lan; Kexin Wang; Tong Zhu; Xiaoye Qu; Yu Zhang; Xiaoyu Mo; Daizong Liu; Yuxuan Liang; Wenliang Chen; Guoqi Li; Yu Cheng
- Reference count: 40
- Primary result: Comprehensive survey of efficient LLM architectures addressing quadratic complexity and memory bottlenecks through linear sequence modeling, sparse attention, MoE, hybrid approaches, and diffusion methods

## Executive Summary
This survey systematically reviews efficient architectures for large language models, addressing the challenge of high computational costs associated with traditional transformer models. It covers a wide range of approaches including linear sequence modeling, sparse sequence modeling, efficient full attention mechanisms, sparse mixture-of-experts, hybrid architectures, and diffusion large language models. Each category is examined for its core principles, representative methods, and practical implementations. The survey highlights how these innovations enable more scalable, memory-efficient, and performant models across text, vision, audio, and multimodal domains.

## Method Summary
This survey systematically reviews and categorizes efficient Large Language Model (LLM) architectures across seven categories (Linear, Sparse, MoE, Hybrid, Diffusion, etc.) to address the quadratic complexity and memory bottlenecks of standard Transformers. The methodology involves analyzing published papers and implementations to extract core principles, representative methods, and practical considerations for each architectural category. The survey provides a comprehensive taxonomy of approaches, their theoretical foundations, implementation challenges, and applications across different domains.

## Key Results
- Significant improvements in inference speed and memory usage through linear sequence modeling and hardware-aware optimizations
- Sparse Mixture-of-Experts architectures enable massive parameter scaling without proportional computational cost
- Hybrid architectures combining different efficiency mechanisms show state-of-the-art performance across multiple domains
- Applications demonstrated in specialized fields including medical imaging, autonomous driving, and remote sensing

## Why This Works (Mechanism)

### Mechanism 1: Linear Sequence Complexity Reduction
- **Claim:** Replacing softmax attention with linear recurrence or state space models reduces computational complexity from quadratic $O(N^2)$ to linear $O(N)$.
- **Mechanism:** Reformulates attention as a recurrent process using kernel-based approximations or state matrices that update a compressed memory state at each step, avoiding pairwise interactions.
- **Core assumption:** Context for next token prediction can be captured in fixed-size hidden state without storing all past KV pairs.
- **Evidence anchors:** Abstract mentions addressing high computational costs; Section 2.1 shows complexity reduction from $O(N^2d)$ to $O(Nd^2)$.
- **Break condition:** Performance degrades on tasks requiring precise recall of specific tokens from very long contexts.

### Mechanism 2: Hardware-IO-Aware Memory Optimization
- **Claim:** Optimizing memory access patterns between HBM and GPU SRAM yields speedups equivalent to algorithmic complexity reduction.
- **Mechanism:** FlashAttention tiles computations to load blocks of Q, K, and V into fast SRAM, computing attention locally and minimizing memory round-trips.
- **Core assumption:** Memory bandwidth is the primary bottleneck during training and inference, more so than raw FLOPs.
- **Evidence anchors:** Section 4.1 describes FlashAttention's memory transfer optimization; abstract mentions improvements in inference speed and memory usage.
- **Break condition:** Gains diminish when memory bandwidth is no longer the bottleneck or overhead exceeds memory access cost for small sequences.

### Mechanism 3: Conditional Compute via Sparse Mixture-of-Experts
- **Claim:** Decoupling model size from computational cost by activating only subset of parameters per token enables massive scale without proportional inference latency.
- **Mechanism:** MoE replaces monolithic FFN with many smaller expert networks and a Router that selects top-k experts per token.
- **Core assumption:** Different tokens require different processing, making specialized sub-networks more efficient than dense processing.
- **Evidence anchors:** Section 5 introduces conditional computation approach; abstract mentions improvements in model scalability.
- **Break condition:** "Routing collapse" where only fraction of experts utilized, or high routing overhead negates latency gains.

## Foundational Learning

- **Concept: Computational Complexity ($O(N)$ vs $O(N^2)$)**
  - **Why needed here:** The entire paper responds to quadratic complexity of Transformer's self-attention mechanism.
  - **Quick check question:** If I double input sequence length, does computation time quadruple (quadratic) or double (linear)?

- **Concept: The KV-Cache Bottleneck**
  - **Why needed here:** Autoregressive generation requires storing KV pairs for all previous tokens, memory footprint grows linearly with sequence length.
  - **Quick check question:** Why does memory requirement for inference increase as model generates more tokens?

- **Concept: Autoregressive vs. Non-Autoregressive Generation**
  - **Why needed here:** Survey covers Diffusion LLMs which break standard "one token at a time" paradigm.
  - **Quick check question:** In standard LLM, can 10th token be generated before 9th is finalized?

## Architecture Onboarding

- **Component map:** Input Tokens -> Efficient Sequence Modeling Layer -> Conditional Compute Layer -> Hybridization -> Output Tokens

- **Critical path:** Start with Section 4 (Efficient Full Attention) and Section 5 (Sparse MoE). These are most practical for optimizing existing LLMs without re-architecting core stack. Move to Section 2 (Linear) and Section 6 (Hybrid) only if specific memory/throughput constraints demand architectural replacement.

- **Design tradeoffs:**
  - **Throughput vs. Recall:** Linear models offer high throughput but may struggle with precise recall of specific long-context details compared to full attention.
  - **Capacity vs. Routing Overhead:** MoE offers high capacity but introduces complexity in load balancing and distributed communication.

- **Failure signatures:**
  - **"Recall Failure":** Linear model hallucinates or misses specific facts provided early in long prompt. (Fix: Hybridize with attention layers)
  - **"Load Imbalance":** MoE training diverges or slows down because only 1-2 experts being trained. (Fix: Check Auxiliary Loss coefficients or use expert-choice routing)
  - **"OOM during Context Extension":** Even with optimizations, activation memory spikes. (Fix: Implement checkpointing or use more aggressive quantization)

- **First 3 experiments:**
  1. **Benchmark FlashAttention:** Measure inference latency and memory footprint of standard Transformer (Llama-3) with and without FlashAttention-2 enabled on sequence lengths 4k, 16k, 32k.
  2. **Validate MoE Scaling:** Convert dense checkpoint to sparse MoE (upcycling) and compare perplexity against dense model of equivalent active parameters.
  3. **Hybrid Ablation:** Build hybrid model (4 Linear layers + 1 Attention layer) and evaluate on "Needle in a Haystack" retrieval task to identify breaking point of linear layers.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can linear recurrent models match reasoning performance of Transformers when scaling test-time computation through long Chain-of-Thought?
- **Basis in paper:** Section 2.6 explicitly poses: "Can we boost linear recurrent models' reasoning performance by scaling test-time computation through long CoT?"
- **Why unresolved:** Linear models rely on fixed-size hidden states which may limit capacity to store intermediate reasoning steps compared to explicit KV cache of Transformers.
- **What evidence would resolve it:** Empirical studies showing linearized models achieving performance parity with AR models on complex reasoning benchmarks (MATH) using identical test-time scaling strategies.

### Open Question 2
- **Question:** Can Non-Autoregressive Diffusion LLMs achieve reasoning capabilities on par with Reinforcement Learning-enhanced Autoregressive models?
- **Basis in paper:** Section 7.1 states: "Whether Diffusion LLMs can match the reasoning prowess of Reinforcement Learning (RL)-enhanced AR models has been a significant open problem."
- **Why unresolved:** While Diffusion LLMs offer parallel decoding, uncertainty exists whether non-autoregressive nature can support sequential multi-step dependencies required for high-level logical deduction.
- **What evidence would resolve it:** Benchmarks demonstrating that Diffusion LLMs, trained with specialized RL algorithms (like diffu-GRPO), can solve complex multi-step reasoning problems with high fidelity.

### Open Question 3
- **Question:** How can efficiency be maximized through joint co-design of algorithms, systems, and hardware?
- **Basis in paper:** Section 9 identifies "Algorithm-System-Hardware Co-Design" as key future direction to improve efficiency on edge devices and specialized chips.
- **Why unresolved:** Current research often treats these layers separately; interaction between efficient kernels and diverse hardware memory hierarchies remains under-optimized.
- **What evidence would resolve it:** Holistic framework where specific efficient architecture achieves superior latency and throughput on target hardware specs compared to separately optimized components.

### Open Question 4
- **Question:** How can efficient architectures be adapted to handle infinite or unbounded contexts effectively for RAG and agentic workflows?
- **Basis in paper:** Section 9 lists "Infinite Long Context" as future direction to enhance RAG and agents.
- **Why unresolved:** While linear models have fixed memory, "infinite" context requires resolving trade-off between compressing history and retaining precise details for exact recall in unbounded streams.
- **What evidence would resolve it:** Architectures successfully processing sequences of millions of tokens beyond current limits without memory overflow or quality degradation in retrieval tasks.

## Limitations
- Survey nature means no original experimental validation provided
- Hardware dependency issues not fully characterized across different deployment scenarios
- Quantitative comparisons across domains (medical imaging, autonomous driving, remote sensing) lacking
- Implementation complexity for methods like "upcycling" not detailed

## Confidence
- **High Confidence:** Quadratic complexity of standard Transformer attention; hardware-aware optimizations consistently reduce bottlenecks; sparse MoE can decouple parameter count from computational cost
- **Medium Confidence:** Linear sequence models achieve comparable performance to Transformers; hybrid architectures show state-of-the-art performance; diffusion-based LLMs offer viable alternatives
- **Low Confidence:** Specific numerical performance comparisons between linear approaches; domain-specific claims without empirical validation; universal applicability across all modalities

## Next Checks
1. **Implement and benchmark FlashAttention-2:** Measure actual latency and memory savings on Llama-3 across sequence lengths 4k, 16k, and 32k, comparing against baseline PyTorch attention implementation.

2. **Validate MoE upscaling protocol:** Convert a dense Llama-2 checkpoint to sparse MoE variant using "upcycling" approach, measuring perplexity against dense model with equivalent active parameters.

3. **Test linear model recall limitations:** Construct "Needle in a Haystack" experiment where specific fact is placed at position 1 and position 8000 in long prompt, measuring retrieval accuracy for both linear (Mamba) and full attention models.