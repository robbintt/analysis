---
ver: rpa2
title: Learning with Monotone Adversarial Corruptions
arxiv_id: '2601.02193'
source_url: https://arxiv.org/abs/2601.02193
tags:
- learning
- points
- algorithm
- adversary
- monotone
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies learning under monotone adversarial corruptions,
  a model where an adversary can add labeled points to a clean i.i.d. dataset while
  respecting the true labeling function.
---

# Learning with Monotone Adversarial Corruptions

## Quick Facts
- arXiv ID: 2601.02193
- Source URL: https://arxiv.org/abs/2601.02193
- Authors: Kasper Green Larsen; Chirag Pabbaraja; Abhishek Shetty
- Reference count: 11
- Key outcome: Optimal learning algorithms (OIG, majority voting) can suffer constant error under monotone adversarial corruptions, while ERM remains robust with O(d log(n/d)/n) error rate.

## Executive Summary
This paper studies binary classification under monotone adversarial corruptions, where an adversary can add labeled points to a clean i.i.d. dataset while respecting the true labeling function. The authors demonstrate that optimal learning algorithms like the One-inclusion Graph (OIG) algorithm and majority voting-based approaches can suffer constant or suboptimal error rates in this setting, despite the corrupted points being honestly labeled. In contrast, they show that Empirical Risk Minimization (ERM) remains robust, achieving an expected error rate of O(d log(n/d)/n) where d is the VC dimension and n is the number of clean samples. The results highlight the fragility of optimal learning algorithms to violations of exchangeability, while ERM's uniform convergence-based guarantees provide inherent robustness.

## Method Summary
The paper analyzes three learning algorithms (ERM, OIG, and majority voting) under monotone adversarial corruptions where an adversary adds honestly-labeled but potentially non-representative points to clean i.i.d. data. The core analysis involves constructing specific hypothesis classes and adversarial strategies to demonstrate failure modes. For the OIG lower bound, the authors create a graph structure where the adversary's points trap the orientation strategy. For majority voting, the adversary coordinates errors across subsamples. ERM's robustness is established through standard uniform convergence arguments applied to the clean sample subset.

## Key Results
- Optimal learning algorithms (OIG, majority voting) can be forced to achieve constant error (e.g., 1/4) under monotone adversarial corruptions with VC dimension 1
- ERM maintains error rate of O(d log(n/d)/n) regardless of the number of corrupted points added
- An oblivious monotone adversary (adding points without seeing clean data) can be handled by OIG to achieve optimal O(d/n) error
- The gap between optimal i.i.d. rates and robust rates under monotone corruptions remains an open question

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Empirical Risk Minimization (ERM) maintains error rates of O(d log(n/d)/n) even when an adversary adds arbitrary amounts of correctly labeled, non-representative data.
- **Mechanism:** The adversary is constrained to "monotone corruptions"—adding points labeled by the ground-truth h*. Since an ERM selector chooses a hypothesis consistent with all training points, and the true hypothesis h* remains consistent, the added points do not increase the empirical risk of the optimal hypothesis. The generalization error relies on uniform convergence over the clean sample size n, making the algorithm indifferent to the volume of honest but biased additions.
- **Core assumption:** The uniform convergence bound must depend primarily on the complexity of the hypothesis class (VC dimension d) and the number of i.i.d. samples (n), effectively filtering out the influence of the adversarial subset on the complexity term.
- **Evidence anchors:** [abstract] "uniform convergence-based algorithms do not degrade in their guarantees." [section 3] Theorem 3.1 shows that every ERM attains expected error O(d log(n/d)/n) by conditioning on the permutation and applying standard bounds to the clean subset.

### Mechanism 2
- **Claim:** Optimal learning algorithms like the One-inclusion Graph (OIG) can suffer constant error (e.g., 1/4) even with VC dimension 1, specifically when the adversary adaptively selects points based on the clean sample.
- **Mechanism:** The OIG algorithm relies on the leave-one-out principle and exchangeability. An adaptive adversary observes the clean sample S and injects points that "trap" the OIG orientation strategy. By specifically targeting the graph structure (e.g., creating specific edges between vertices), the adversary forces the orientation to point away from the true hypothesis for a test point, breaking the symmetry usually guaranteed by i.i.d. assumptions.
- **Core assumption:** The adversary knows the clean sample S and the specific deterministic or randomized orientation strategy of the learner (non-uniform algorithm selection).
- **Evidence anchors:** [abstract] "all known optimal learning algorithms... can be made to achieve suboptimal expected error." [section 4.2] Theorem 4.4 constructs a specific hypothesis class where the adversary adds points y_i for every x_i ∈ S, creating a graph where the true hypothesis is isolated or mis-oriented with probability 1/2.

### Mechanism 3
- **Claim:** Majority voting algorithms (like Bagging) fail to achieve optimal rates because the adversary can correlate the error of disjoint subsets.
- **Mechanism:** Majority voting relies on the independence of errors across subsamples. An adaptive adversary, seeing the clean data, can insert a small number of "trap" points (e.g., specific y_T points) that appear in a majority of the subsamples. This forces the base ERMs in those subsets to converge to a "bad" hypothesis that performs well on the trap but fails on the clean distribution, coordinating the failure of the vote.
- **Core assumption:** The subsampling strategy of the voter is fixed and known, allowing the adversary to calculate which corrupted points will appear in >50% of subsets.
- **Evidence anchors:** [abstract] "...algorithms... can be made to achieve suboptimal expected error... exposing their overreliance on exchangeability." [section 4.1] The proof of Theorem 4.2/4.3 shows that with m = 2n/t corrupted points, the probability of a subsample containing no adversarial point is small, ensuring the majority vote is poisoned.

## Foundational Learning

- **Concept: Exchangeability (vs. I.I.D.)**
  - **Why needed here:** This is the central assumption violated by the monotone adversary. The paper proves that while algorithms like OIG work under exchangeability (permutation invariance), the adaptive selection of new points breaks this symmetry, causing failure. ERM works because it does not strictly require exchangeability for its uniform convergence proofs, only that the clean points are representative.
  - **Quick check question:** If I shuffle the combined dataset of clean and corrupted points, does the distribution of the clean points conditional on the corrupted points remain i.i.d. from the original distribution D? (Answer: No, if the adversary is adaptive.)

- **Concept: VC Dimension and Sample Complexity**
  - **Why needed here:** The bounds are expressed strictly in terms of d (VC dimension) and n. Understanding that Θ(d/n) is the "gold standard" for optimal learning is necessary to see why O(d log(n/d)/n) is considered "suboptimal" yet robust, and why constant error is a catastrophic failure.
  - **Quick check question:** Why does the ERM bound in this paper scale as O(d log(n/d)/n) rather than just O(d/n)? (Answer: It inherits the standard uniform convergence bound which includes the log factor, often tight for ERM specifically.)

- **Concept: The Leave-One-Out Principle**
  - **Why needed here:** This is the theoretical engine behind the One-inclusion Graph (OIG). The paper demonstrates a setting where leave-one-out reasoning fails because the "left out" point is no longer exchangeable with the training points (the adversary rigged the training set based on what would have been the test set or other points).
  - **Quick check question:** In the standard leave-one-out analysis, why do we assume the error on the left-out point is representative of the test error? (Answer: Because the training set and test point are drawn i.i.d.; this paper breaks that draw mechanism.)

## Architecture Onboarding

- **Component map:**
  - Generator: Draws n i.i.d. samples from distribution D
  - Adversary Module: Adaptive (reads S_clean, outputs S_corr) OR Oblivious (outputs S_corr independent of S_clean). Constraint: S_corr must be labeled by h*.
  - Learner Core:
    - *Strategy A (ERM):* Finds any h ∈ H consistent with S_clean ∪ S_corr
    - *Strategy B (OIG):* Constructs graph on S ∪ {x_test}; orients edges to minimize max out-degree; predicts based on edge orientation
    - *Strategy C (Voter):* Partitions S into subsets; runs ERM on each; takes majority vote

- **Critical path:** The degradation of OIG and Majority voting relies entirely on the Adaptive Adversary module having read access to S_clean. If you block this access (switch to Oblivious), the "constant error" failure mode disappears.

- **Design tradeoffs:**
  - **Optimality vs. Robustness:** OIG provides optimal O(d/n) rates in i.i.d. settings but crashes to constant error under adaptive corruption. ERM provides slightly worse O(d log(n/d)/n) rates but is robust to the corruption.
  - **Knowledge Assumption:** The lower bounds assume the adversary knows the learner's specific random seed or orientation strategy. In practice, if the learner uses cryptographic randomness unknown to the adversary, some attacks might be mitigated, though the theoretical fragility remains.

- **Failure signatures:**
  - **OIG Failure:** The algorithm constructs a graph with isolated vertices or specific edge orientations where the true hypothesis h* is on the losing end of a "dangling" edge connected to a corrupted point.
  - **Majority Failure:** A high percentage of subsamples all contain a specific "trap" point y_T, causing them to all vote unanimously for a hypothesis that is incorrect on the true distribution.

- **First 3 experiments:**
  1. **Reproduce the VC-1 Lower Bound:** Implement the hypothesis class (intervals/thresholds or the specific set construction from Theorem 4.4) and the adaptive adversary. Plot OIG error vs. n to verify it stays flat (constant error) rather than decreasing as 1/n.
  2. **Stress Test ERM:** Use the same setup as Experiment 1 but run ERM. Verify that the error decreases as O(log(n)/n) regardless of the adversary's adaptivity.
  3. **Oblivious vs. Adaptive Ablation:** Run the OIG algorithm with an adversary who adds the same number of points but chooses them independently of the clean sample (Oblivious). Verify that the error rate returns to the optimal O(d/n) trajectory.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the optimal expected error for learning a binary hypothesis class of VC dimension d with n clean samples under a monotone adversary?
- **Basis in paper:** [explicit] The authors explicitly pose this as "Open Question 1," noting the gap between the O(d log(n/d)/n) ERM upper bound and the optimal i.i.d. rate of O(d/n).
- **Why unresolved:** Current technical tools cannot handle the lack of exchangeability; standard reductions (e.g., subsampling) incur sample size blowups, and stable sample compression schemes fail when conditioning on compression sets makes remaining samples non-i.i.d.
- **What evidence would resolve it:** An algorithm achieving the optimal O(d/n) rate or a general lower bound proving that Ω(d log(n/d)/n) is the best possible rate for any learner in this setting.

### Open Question 2
- **Question:** Is it possible to achieve an expected error of O(d_L/n) for learning a binary hypothesis class with Littlestone dimension d_L under a monotone adversary?
- **Basis in paper:** [explicit] Listed as "Open Question 2," asking if online-to-batch conversions can yield better rates than ERM for classes with small Littlestone dimension.
- **Why unresolved:** Standard online-to-batch analyses fail to provide guarantees in the presence of monotone corrupted samples, creating a gap between online learnability and robustness to this specific distribution shift.
- **What evidence would resolve it:** A modified online-to-batch analysis that remains robust to monotone corruptions or a lower bound construction showing that the O(d log(n/d)/n) rate of ERM is unavoidable even for classes with small d_L.

### Open Question 3
- **Question:** Can any vanishing expected error rate be achieved for learnable partial binary classes or multiclass classes in the monotone adversary model?
- **Basis in paper:** [explicit] The paper highlights this as "Open Question 3," noting that while OIG works for these classes in i.i.d. settings, its analysis breaks down for adaptive monotone adversaries.
- **Why unresolved:** Uniform convergence does not hold for these class types, and the authors have been unable to analyze the One-inclusion Graph (OIG) algorithm under the non-exchangeable data distributions created by the adversary.
- **What evidence would resolve it:** A proof showing the OIG algorithm (or a novel alternative) achieves vanishing error in this setting, or a separation result proving that learning is impossible.

## Limitations
- The analysis assumes clean data is i.i.d. and that the adversary is constrained to honest labeling, limiting generalizability to settings where label corruption is possible.
- Lower bound constructions rely on specific hypothesis classes and may not extend to all VC classes uniformly.
- The gap between optimal i.i.d. rates and robust rates under monotone corruptions remains unresolved, with current bounds potentially not tight.

## Confidence
- **High**: ERM achieving O(d log(n/d)/n) under monotone corruptions (Theorem 3.1)
- **Medium**: OIG suffering constant error under adaptive monotone adversary (Theorem 4.4) - proof is constructive but relies on specific orientation strategy
- **Medium**: Majority voting failure due to correlated subset errors (Theorem 4.2) - proof assumes specific adversarial ERM tie-breaking

## Next Checks
1. **Empirical validation**: Implement the OIG lower bound construction and verify constant error across different n values
2. **Robustness check**: Test whether cryptographic randomness in orientation selection mitigates the OIG attack
3. **Tightness verification**: Compare ERM's O(d log(n/d)/n) empirical rate against theoretical prediction across multiple hypothesis classes