---
ver: rpa2
title: 'DaKultur: Evaluating the Cultural Awareness of Language Models for Danish
  with Native Speakers'
arxiv_id: '2504.02403'
source_url: https://arxiv.org/abs/2504.02403
tags:
- cultural
- danish
- language
- data
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DAKULTUR, the first native Danish cultural
  awareness dataset for evaluating Large Language Models (LLMs). The dataset is created
  through a native-speaker-driven evaluation study with 63 demographically diverse
  participants who prompt three Danish-adapted LLMs (SNAKMODEL, LLAMA 2-7B-chat+INSTda,
  LLAMA 2-7B-base+INSTda) with cultural tasks and rate their responses.
---

# DaKultur: Evaluating the Cultural Awareness of Language Models for Danish with Native Speakers

## Quick Facts
- arXiv ID: 2504.02403
- Source URL: https://arxiv.org/abs/2504.02403
- Reference count: 12
- Primary result: Training on native Danish data more than doubles LLM cultural acceptance rates (14%→42%) compared to translated data

## Executive Summary
This paper introduces DAKULTUR, the first native Danish cultural awareness dataset for evaluating Large Language Models. Through a native-speaker-driven study with 63 participants, the authors prompt three Danish-adapted LLMs with cultural tasks and rate their responses. The dataset contains 1,038 high-quality input-response pairs across 12 cultural topics. Results show that models trained on native Danish data (SNAKMODEL) achieve significantly higher acceptance rates than those trained on translated data, with particular improvements in culturally-specific topics like food and traditions. The study demonstrates that native-speaker data is critical for cultural alignment in LLMs, as translated data yields limited cultural awareness despite linguistic competence.

## Method Summary
The study evaluates cultural awareness of Danish LLMs through a native-speaker-driven approach. Three LLaMA 2-7B-based models were compared: LLaMA 2-7B-base + INSTda, LLaMA 2-7B-chat + INSTda, and SNAKMODEL (LLaMA 2-7B-base + LMTda + INSTda). Sixty-three demographically diverse Danish participants prompted these models with cultural tasks and rated responses on a binary accept/reject scale. The evaluation covered 12 cultural topics and generated 1,038 high-quality input-response pairs. Models were also evaluated on the ScandEval benchmark, including cultural tasks (proverbs and citizenship test). The study specifically compared models trained on native Danish data versus translated instruction data to assess cultural alignment effectiveness.

## Key Results
- Models trained on native Danish data (SNAKMODEL) more than double acceptance rates compared to models trained on translated data (14%→42%)
- Cultural alignment varies significantly by demographics, with male-identity participants and those from the capital region showing slightly higher acceptance rates
- Automatic benchmarks (ScandEval) poorly correlate with human cultural acceptance when tasks are translated, but show better alignment for native cultural tasks
- The maximum 42% acceptance rate indicates current adaptation techniques remain insufficient for full cultural alignment

## Why This Works (Mechanism)

### Mechanism 1
Native-language training data is superior to automatically translated data for cultural adaptation of language models. Cultural context is implicit in authentic linguistic expression, and models trained on native data can internalize these cultural priors while those trained on translated data may achieve linguistic fluency without cultural depth.

### Mechanism 2
Evaluating cultural awareness requires non-translated, culturally-native benchmark tasks. Translated evaluation benchmarks cannot assess cultural understanding because they do not query culturally-specific knowledge. Only tasks grounded in native contexts reveal if a model has internalized local culture.

### Mechanism 3
Cultural alignment is demographically conditioned. A "culture" is not monolithic; age, gender, and region create different expectations for culturally appropriate responses. A model may align well with some subgroups and poorly with others.

## Foundational Learning

- **Cultural Alignment vs. Linguistic Competence**
  - Why needed here: This is the central distinction. A model can be grammatically fluent in Danish but still provide culturally inappropriate responses.
  - Quick check question: Give an example where a model could pass a linguistic test (e.g., named entity recognition) but fail a cultural evaluation.

- **Native Data vs. Translated Data**
  - Why needed here: The primary actionable finding. Understanding why translation fails for culture is critical for data strategy.
  - Quick check question: Why might translating a high-quality English instruction-tuning dataset be insufficient for teaching a model about a specific culture's traditions?

- **Intra-Cultural Variation**
  - Why needed here: "Danish culture" is not a monolith. Engineers must recognize that cultural alignment is subjective and varies by user demographics.
  - Quick check question: If your model is rated highly by users in Copenhagen, what additional validation step should you take before claiming it is culturally aligned for all of Denmark?

## Architecture Onboarding

- **Component map:** Base Model (English-centric LLaMA 2-7B) → Adaptation Paths (+INSTda: translated instruction tuning, +LMTda: native Danish pre-training) → Evaluation Suite (ScandEval automatic benchmark + DaKultur human evaluation) → Cultural Topics (12 categories for granular analysis)

- **Critical path:** Acquire native Danish corpus → Continued pre-training (+LMTda) → Instruction tuning (+INSTda) → Evaluate on culturally-native tasks (both automatic and human). Skipping the native pre-training step is the primary failure mode.

- **Design tradeoffs:**
  - Native vs. Translated Data: Native data is effective but scarce; translated data is scalable but yields poor cultural results
  - Automatic vs. Human Evaluation: Human evaluation is the gold standard but costly; small native benchmarks can be viable proxies
  - Demographic Alignment: Tradeoff between general cultural model vs. optimizing for specific subgroups

- **Failure signatures:**
  1. Anglocentric Output: Model provides U.S.-centric answers to Danish cultural queries
  2. Fluent but Incorrect: Model generates perfect Danish but with factually wrong or shallow cultural content
  3. Topic Asymmetry: High performance on factual topics but failure on implicit norms

- **First 3 experiments:**
  1. Ablate Training Data: Compare models with only translated instruction tuning vs. native pre-training then instruction tuning
  2. Benchmark Correlation Analysis: Measure model performance on standard linguistic tasks and cultural tasks to check correlation with human acceptance
  3. Demographic Slice Analysis: Segment human evaluation data by participant region, age, and gender to identify alignment gaps

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the efficacy of native data over translated data for cultural adaptation generalize to model architectures beyond Llama 2-7B and to languages typologically distinct from Danish?
- **Open Question 2:** What methodologies are required to overcome the 42% acceptance rate ceiling observed even in models trained on native data?
- **Open Question 3:** How does the geographic and demographic skew of the evaluators impact the generalizability of the "cultural alignment" definitions used in the study?

## Limitations
- Sample size of 63 participants limits statistical power and generalizability of demographic findings
- Capital region overrepresentation potentially skews results toward urban cultural norms
- Binary accept/reject judgments cannot capture nuanced cultural acceptability or specific failure modes
- Study focuses on single language (Danish), limiting broader claims about cultural adaptation mechanisms

## Confidence
- **High Confidence:** Native Danish training data significantly outperforms translated data (14%→42% acceptance) is robust and well-supported
- **Medium Confidence:** Demographic variation findings are suggestive but limited by sample size
- **Low Confidence:** Broader claims about cultural alignment mechanisms across languages cannot be supported from this single-language study

## Next Checks
1. Replicate human evaluation study with 200+ demographically balanced Danish participants to validate demographic effects
2. Apply same methodology to another low-resource language with distinct cultural norms to test generalizability
3. Conduct qualitative analysis of rejected responses to categorize failure modes and assess systematic cultural blind spots in translated-data models