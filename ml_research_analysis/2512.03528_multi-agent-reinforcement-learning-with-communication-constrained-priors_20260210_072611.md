---
ver: rpa2
title: Multi-Agent Reinforcement Learning with Communication-Constrained Priors
arxiv_id: '2512.03528'
source_url: https://arxiv.org/abs/2512.03528
tags:
- communication
- learning
- messages
- multi-agent
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of multi-agent reinforcement
  learning (MARL) in communication-constrained environments, specifically focusing
  on scenarios with lossy communication. The authors propose a novel framework that
  models communication-constrained priors to distinguish between lossy and lossless
  messages and uses a dual mutual information estimator (Du-MIE) to quantify their
  impact on agent behavior.
---

# Multi-Agent Reinforcement Learning with Communication-Constrained Priors

## Quick Facts
- arXiv ID: 2512.03528
- Source URL: https://arxiv.org/abs/2512.03528
- Reference count: 38
- Key outcome: CC-MADDPG achieves average rewards of up to 138.0 in challenging communication-constrained scenarios, significantly outperforming baselines like FC-MADDPG (1.5) and Dropout-MADDPG (68.7) under heavy communication constraints.

## Executive Summary
This paper addresses the challenge of multi-agent reinforcement learning (MARL) in communication-constrained environments, specifically focusing on scenarios with lossy communication. The authors propose a novel framework that models communication-constrained priors to distinguish between lossy and lossless messages and uses a dual mutual information estimator (Du-MIE) to quantify their impact on agent behavior. The method incorporates this into the reward function to enhance the positive influence of reliable messages and mitigate the negative effects of corrupted messages. Experimental results across multiple benchmark environments demonstrate that the proposed CC-MADDPG algorithm outperforms existing baselines, maintaining robust performance even under severe communication constraints.

## Method Summary
The paper proposes CC-MADDPG, which extends MADDPG with communication-constrained priors and a dual mutual information estimator (Du-MIE). The framework models communication reliability through binary parameters ι_ij ∈ {0,1} that distinguish lossy from lossless messages. During training, a Du-MIE module uses JSD and CLUB estimators to separately quantify the information content of reliable and unreliable messages. This mutual information is then integrated into the reward function, shaping policies to leverage reliable communication while becoming robust to message loss. The approach is algorithm-agnostic and can be combined with different MARL algorithms through reward shaping.

## Key Results
- CC-MADDPG achieves average rewards of up to 138.0 in challenging scenarios under heavy communication constraints
- The method outperforms FC-MADDPG (1.5) and Dropout-MADDPG (68.7) by significant margins under severe communication constraints
- Ablation studies show the full model with both JSD and CLUB components outperforms either component alone
- Test-matched priors outperform generic dropout-0.2 priors in most environments

## Why This Works (Mechanism)

### Mechanism 1: Communication-Constrained Prior as Training Signal
Incorporating communication loss patterns during training produces policies robust to real-world message degradation. A binary link parameter ι_ij ∈ {0,1} characterizes whether agent i's message to agent j is reliable. During training, this prior distinguishes lossy (ι=0) from lossless (ι=1) messages, allowing the learning process to weight them differently rather than treating all communication uniformly.

### Mechanism 2: Dual Mutual Information Estimation for Message Decoupling
Separately maximizing information from reliable messages while minimizing information from unreliable messages improves decision quality under mixed communication conditions. The Dual Mutual Information Estimator (Du-MIE) uses two complementary estimators: JSD estimator to maximize lower bound of I(m_ji; a_i) for lossless messages, and CLUB estimator to minimize upper bound for lossy messages.

### Mechanism 3: Information-Theoretic Reward Shaping
Augmenting the environment reward with mutual information terms guides policy optimization toward communication-aware cooperation. The shaped reward becomes r̃_t = r_t + Σ[α·ι_ji·I_JSD - β·(1-ι_ji)·I_CLUB], which integrates directly into standard MARL TD-loss computations.

## Foundational Learning

- **Concept: Decentralized Partially Observable MDP (Dec-POMDP) with Communication**
  - Why needed here: The paper formalizes multi-agent coordination under partial observability as a 9-tuple G = ⟨N, S, O, A, T, R, Z, M, I, γ⟩, extending standard Dec-POMDP with message space M and communication link status I.
  - Quick check question: Given a 3-agent system where ι_12=1 and ι_13=0, which agent receives reliable communication from agent 1?

- **Concept: Mutual Information Estimation via Neural Networks**
  - Why needed here: Exact MI computation requires joint and marginal distributions that are intractable for high-dimensional message-action spaces. The paper uses neural estimators to approximate bounds.
  - Quick check question: Why does the JSD estimator use samples from joint distribution P_MA for the first term but marginal product P_M ⊗ P_A for the second term?

- **Concept: Centralized Training, Decentralized Execution (CTDE)**
  - Why needed here: The framework integrates with CTDE algorithms like MADDPG, where critics access global state information during training but actors execute using only local observations and received messages.
  - Quick check question: During execution, can agent i access ι_ji values to weight incoming messages, or must it rely solely on the trained policy's implicit handling?

## Architecture Onboarding

- **Component map:** Environment → Communication Link Predictor → Replay Buffer → Du-MIE Module (JSD network, CLUB network) → Reward Shaping Layer → MARL Optimizer

- **Critical path:**
  1. Collect trajectories with communication constraints active
  2. Every k steps, update Du-MIE networks using Equation 5
  3. Compute shaped rewards for all transitions in batch
  4. Update MARL networks using Equations 7-8
  5. Repeat until convergence

- **Design tradeoffs:**
  - Generic vs. Matched Priors: Test-matched priors outperform generic dropout-0.2 but require environment-specific knowledge
  - Dropout Rate Selection: Higher rates increase robustness but reduce exploitability of reliable communication
  - Coefficient Tuning: α, β ∈ {0.001, 0.01}; larger values amplify MI signal but risk reward hacking

- **Failure signatures:**
  - MI estimator divergence: JSD or CLUB losses fail to decrease → check sample diversity, reduce learning rate
  - Policy collapse under heavy constraints: Performance matches no-communication baseline → increase α or reduce dropout rate
  - Over-reliance on shaped reward: Agents achieve high shaped reward but low task reward → reduce α, β coefficients

- **First 3 experiments:**
  1. Baseline comparison under light constraints: Run CC-MADDPG vs. FC-MADDPG and Dropout-MADDPG in Simple_Tag with light DBC (threshold=5)
  2. Ablation on Du-MIE components: Test three variants (JSD-only, CLUB-only, full) in Simple_Spread under medium DBC
  3. Prior sensitivity test: Compare dropout-0.2 vs. test-matched priors across all three DBC levels in Simple_Reference

## Open Questions the Paper Calls Out

- Can the framework be extended to value-based learning algorithms (e.g., QMIX, VDN) while maintaining the dual mutual information optimization benefits?
- Can agents adaptively learn robust policies when communication constraints are highly dynamic and non-stationary within a single episode?
- How can communication-constrained priors be automatically learned or adapted when the underlying communication model is completely unknown?
- Does the binary lossy/lossless message distinction limit performance in environments with partial or probabilistic message degradation?

## Limitations
- The framework assumes communication constraints are stationary and can be characterized by priors, which may not hold in dynamic real-world networks
- Scalability may be limited by quadratic growth in agent pairs for Du-MIE computation and increased memory requirements
- Computational overhead from dual MI estimators is not quantified relative to baseline methods

## Confidence

**High Confidence (9/10):** Core algorithmic claims regarding Du-MIE and reward shaping are well-supported by ablation studies and consistent with information-theoretic principles.

**Medium Confidence (7/10):** Transferability claims to real-world systems rely on reasonable but untested assumptions about prior matching.

**Low Confidence (4/10):** Claims about robustness to unknown constraint patterns are extrapolated from limited test cases.

## Next Checks

1. **Prior Mismatch Robustness:** Train CC-MADDPG with dropout-0.2 priors but test under correlated burst losses. Measure performance degradation compared to matched priors.

2. **Dynamic Constraint Adaptation:** Implement a meta-learning approach where ι_ij values are periodically updated during training based on recent communication statistics. Compare final performance against static priors.

3. **Real-World Deployment Test:** Deploy CC-MADDPG on a multi-robot system with actual wireless communication constraints. Compare performance against a baseline trained without communication priors under identical network conditions.