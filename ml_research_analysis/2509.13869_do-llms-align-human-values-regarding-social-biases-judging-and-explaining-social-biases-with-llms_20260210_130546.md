---
ver: rpa2
title: Do LLMs Align Human Values Regarding Social Biases? Judging and Explaining
  Social Biases with LLMs
arxiv_id: '2509.13869'
source_url: https://arxiv.org/abs/2509.13869
tags:
- llms
- qwen2
- rate
- bias
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the alignment of large language models (LLMs)
  with human values regarding social biases (HVSB). The authors investigate how LLMs
  judge biased scenarios and their ability to explain HVSB, using four datasets and
  twelve models from four model families.
---

# Do LLMs Align Human Values Regarding Social Biases? Judging and Explaining Social Biases with LLMs

## Quick Facts
- **arXiv ID**: 2509.13869
- **Source URL**: https://arxiv.org/abs/2509.13869
- **Reference count**: 40
- **Primary result**: LLMs perform better on biased scenarios with negative questions or stereotype answers, and larger models don't necessarily align better with human values regarding social biases.

## Executive Summary
This paper investigates whether large language models (LLMs) align with human values regarding social biases (HVSB) through two tasks: judging biased scenarios and generating explanations for biases. The authors evaluate twelve models from four families across four datasets, finding that LLMs show better alignment with HVSB when scenarios contain negative questions or harmful stereotype answers. Interestingly, increasing parameter scales doesn't guarantee better alignment, and judgmental consistency is higher within the same model family. The study also reveals that LLMs prefer their own generated explanations, and smaller LMs fine-tuned on HVSB explanations achieve higher human readability but lower model agreeability.

## Method Summary
The study evaluates LLM alignment with HVSB through judgment and explanation tasks using four datasets (BBQ, BiasDPO, StereoSet, CrowS-Pairs). For judgment, models classify scenarios as biased or unbiased, with misalignment rates calculated. Adversarial system prompts test robustness. For explanations, models generate bias explanations which are evaluated for human readability (FKGL, GFI, CLI) and model agreeability (whether other models agree with the explanations). The authors also fine-tune smaller LMs (GPT-2, OPT, Phi, Flan-T5) using QLoRA on 2,000 explanations generated by GPT-4o to assess the trade-off between readability and model agreeability.

## Key Results
- LLMs show better HVSB alignment on scenarios with negative questions or stereotype answers compared to non-negative contexts
- Increasing parameter scales does not guarantee better alignment with human values regarding social biases
- Judgmental consistency is higher within the same model family
- LLMs prefer their own generated explanations over those from other models
- Fine-tuned smaller LMs produce more readable explanations but have lower model agreeability

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs align better with HVSB when scenarios contain negative questions or harmful stereotype answers.
- **Mechanism:** Safety alignment training (e.g., RLHF) creates sensitivity to "negative" linguistic markers, acting as a heuristic for identifying harmful content more readily than subtle, non-negative stereotyping.
- **Core assumption:** Alignment training data disproportionately penalizes negative valence, creating a detection bias rather than deep understanding of bias mechanics.
- **Evidence anchors:**
  - [Section 4.1] "LLMs are more 'sensitive' to contexts containing negative content..."
  - [Figure 1] Shows higher misalignment rates for non-negative questions vs. negative questions across bias categories.
  - [Corpus] *More Women, Same Stereotypes* confirms LLMs struggle with embedded biases in neutral contexts.

### Mechanism 2
- **Claim:** LLMs exhibit a "self-preference" bias, judging explanations generated by themselves as more agreeable than those generated by other models.
- **Mechanism:** Self-preference operates through distributional similarity - explanations generated by Model A create internal representations that align closely with Model A's activation patterns when acting as a validator.
- **Core assumption:** Model agreeability is partially a function of stylistic familiarity rather than pure logical validity.
- **Evidence anchors:**
  - [Abstract] "...LLMs prefer their own generated explanations."
  - [Section 5.3] "Each target model $T_i$ has a higher preference ranking when it is the same as the explanation-generating model $E_i$."

### Mechanism 3
- **Claim:** Fine-tuning smaller LMs on HVSB explanations decouples readability from "model agreeability."
- **Mechanism:** Smaller models can learn syntactic and semantic features of explanations (improving readability scores) but lack capacity to replicate the complex reasoning depth of the teacher model, resulting in text that humans find easy to read but large models find logically "shallow" or unconvincing.
- **Core assumption:** Readability metrics capture surface-level syntax, while model agreeability captures reasoning depth.
- **Evidence anchors:**
  - [Abstract] "...explanations generated by the fine-tuned smaller LMs are more readable, but have a relatively lower model agreeability."
  - [Section 5.4] "...explanations generated by the fine-tuned smaller LMs achieve higher readability..."

## Foundational Learning

- **Concept: Human Values regarding Social Biases (HVSB)**
  - **Why needed here:** This is the target variable of the entire study, distinguishing general "harm" from specifically human-centric judgments on social stereotypes.
  - **Quick check question:** If a model flags a non-stereotypical statement as biased, is it misaligned or just over-sensitive?

- **Concept: Misalignment Rate vs. Attack Success Rate**
  - **Why needed here:** These are the two distinct failure modes measured - misalignment measures passive failure, while Attack Success Rate measures active vulnerability under adversarial prompting.
  - **Quick check question:** Does a model with a low misalignment rate always have a low attack success rate?

- **Concept: Model Agreeability**
  - **Why needed here:** This serves as a proxy for "understanding" in the explanation task, measuring if a "Target" model accepts the logic of an explanation generated by an "Explanation" model.
  - **Quick check question:** If Model A agrees with its own explanation 90% of the time, but Model B agrees with Model A's explanation only 50% of the time, is Model A a good explainer?

## Architecture Onboarding

- **Component map:**
  - Scenario Construction (BBQ/BiasDPO/StereoSet) -> Judge Model (J) -> Misalignment Rate
  - Scenario Construction -> Adversarial Prompt (p^A) -> Attack Success Rate
  - Scenario Construction -> Explanation Model (E) -> Readability Metrics
  - Scenario + Explanation -> Target Model (T) -> Model Agreeability

- **Critical path:**
  1. Scenario Construction (BBQ/BiasDPO/StereoSet)
  2. Zero-shot Judgment (Calculate Misalignment Rate)
  3. Adversarial Injection (Calculate Attack Success Rate)
  4. Explanation Generation (Calculate Readability)
  5. Cross-Model Validation (Calculate Agreeability)

- **Design tradeoffs:**
  - **LLM-as-Judge vs. Human Eval:** The paper relies on LLMs as Target Models to evaluate explanations for efficiency, acknowledging the "self-preference" bias as a limitation.
  - **Readability vs. Logic:** Optimizing explanations for human readability resulted in lower logical validity (model agreeability).

- **Failure signatures:**
  - **Inverse Scaling:** Larger models (e.g., Qwen2.5-72B) performing worse than smaller ones (e.g., Qwen2.5-3B) on misalignment rates.
  - **High Rejection Rates:** Llama3.1-8B refusing to answer under targeted attacks, which counts as a defense but signals brittleness.

- **First 3 experiments:**
  1. **Negative vs. Non-Negative Sensitivity:** Run the Judgment pipeline on BBQ dataset comparing negative vs. non-negative question types to confirm safety alignment sensitivity hypothesis.
  2. **Self-Preference Test:** Generate explanations for 50 bias scenarios using Model A, then measure agreeability rate when Model A acts as Target, and compare with Model B as Target.
  3. **Small LM Distillation:** Fine-tune a small causal LM (e.g., OPT-1.3B) on GPT-4o-generated HVSB explanations and measure the BLEU/ROUGE scores vs. the drop in model agreeability.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the alignment of LLMs with human values regarding social biases generalize to non-English linguistic and cultural contexts?
- **Basis in paper:** [explicit] The authors state in the Limitations section: "First, we only studied social biases in the English context."
- **Why unresolved:** The study is limited to English datasets and models, leaving cross-linguistic and cross-cultural alignment unexamined.
- **What evidence would resolve it:** Experiments using bias benchmarks translated into or natively constructed for multiple languages, evaluated on multilingual or non-English LLMs, with analysis comparing alignment patterns across cultures.

### Open Question 2
- **Question:** How does human evaluation of LLM-generated explanations for HVSB compare to the automatic "model agreeability" metric used in this study?
- **Basis in paper:** [explicit] The authors note as a limitation: "we only considered automatic evaluation methods (human readability and LLMs) to evaluate the explanations and did not consider human evaluations."
- **Why unresolved:** While model agreeability measures inter-LLM consensus, it does not directly measure how well explanations serve human understanding of bias.
- **What evidence would resolve it:** A human subject study where annotators rate the clarity, helpfulness, and accuracy of LLM-generated explanations, compared against model agreeability scores.

### Open Question 3
- **Question:** How robust is LLM alignment with HVSB under a wider range of adversarial attacks beyond untargeted and targeted system prompts?
- **Basis in paper:** [explicit] The authors limit their adversarial testing: "we only studied two attack types: untargeted system prompt and targeted system prompt."
- **Why unresolved:** Real-world attacks are more diverse. The study's conclusion on alignment robustness is based on a narrow attack spectrum.
- **What evidence would resolve it:** Evaluating the same LLMs against a broader suite of attacks (e.g., jailbreaks, prompt injection, role-play exploits) to measure consistency in judgment and explanation tasks.

### Open Question 4
- **Question:** Does the source of explanation data (from different LLMs) significantly impact the performance and alignment of fine-tuned smaller language models?
- **Basis in paper:** [explicit] The authors state a limitation regarding their fine-tuning data: "we only used the explanations generated by GPT-4o... did not use the explanations generated by the other LLMs."
- **Why unresolved:** The observed properties of fine-tuned smaller models may be artifacts of using GPT-4o's explanation style.
- **What evidence would resolve it:** Fine-tuning smaller LMs using explanations generated by each LLM from the studied model families and comparing their resulting faithfulness, readability, and model agreeability.

## Limitations
- The reliance on GPT-4o to convert SS/CP samples into realistic scenarios introduces potential model-specific bias not fully controlled for.
- Model agreeability as a proxy for explanation quality conflates logical validity with stylistic familiarity, potentially masking genuine reasoning gaps.
- Results are primarily based on English-language datasets; cross-lingual generalization remains untested.

## Confidence
- **High Confidence:** The observed sensitivity of LLMs to negative linguistic markers in bias detection is robust across multiple datasets and model families.
- **Medium Confidence:** The self-preference bias in explanation validation is consistently observed but may be partially explained by shared training data rather than pure "self-recognition."
- **Low Confidence:** The claim that parameter scaling doesn't guarantee better HVSB alignment needs broader model family testing beyond the four families studied.

## Next Checks
1. **Cross-Validation with Human Judges:** Re-run the explanation agreeability experiments with human evaluators rather than LLM targets to disentangle model-specific bias from genuine logical validity.
2. **Adversarial Prompt Ablation:** Systematically remove individual components from the adversarial prompts to identify which linguistic patterns most effectively bypass safety alignment.
3. **Cross-Lingual Transfer:** Test the judgment and explanation models on non-English bias datasets to assess whether negative-content sensitivity is language-dependent.