---
ver: rpa2
title: Agentic Policy Optimization via Instruction-Policy Co-Evolution
arxiv_id: '2512.01945'
source_url: https://arxiv.org/abs/2512.01945
tags:
- instruction
- policy
- instructions
- answer
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces INSPO, a novel Instruction-Policy co-evolution
  framework that integrates instruction optimization as a dynamic component of the
  reinforcement learning (RL) loop for training LLM-based agents. Unlike existing
  RLVR approaches that rely on static, manually-designed instructions, INSPO maintains
  a dynamic population of instruction candidates and employs an experience-driven
  instruction generation mechanism, automating the discovery of more effective reasoning
  strategies via reflecting on online feedback along the policy learning process.
---

# Agentic Policy Optimization via Instruction-Policy Co-Evolution

## Quick Facts
- arXiv ID: 2512.01945
- Source URL: https://arxiv.org/abs/2512.01945
- Authors: Han Zhou; Xingchen Wan; Ivan VuliÄ‡; Anna Korhonen
- Reference count: 40
- Primary result: INSPO achieves 38.2% EM score on multi-turn retrieval tasks, surpassing Search-R1 by 6%

## Executive Summary
This paper introduces INSPO, an Instruction-Policy co-evolution framework that integrates instruction optimization as a dynamic component of the reinforcement learning loop for training LLM-based agents. Unlike existing approaches that rely on static, manually-designed instructions, INSPO maintains a dynamic population of instruction candidates and employs an experience-driven instruction generation mechanism. The method periodically prunes low-performing instructions and generates new ones through an on-policy reflection mechanism that analyzes past trajectories to evolve more effective reasoning strategies.

The approach addresses a fundamental limitation in LLM agent training where static instructions may not capture the evolving reasoning needs during policy optimization. By treating instruction optimization as an integral part of the RL loop rather than a pre-processing step, INSPO can discover more effective and innovative reasoning strategies through online feedback. Experiments on multi-turn retrieval and reasoning tasks demonstrate substantial performance improvements over strong baselines, achieving state-of-the-art results with only marginal computational overhead.

## Method Summary
INSPO implements a co-evolutionary framework where instruction candidates evolve alongside the policy through an iterative cycle of selection, pruning, and generation. The system maintains a population of NP instruction candidates, periodically pruning the lowest-performing ones based on policy outcomes and generating new candidates through on-policy reflection. The reflection mechanism analyzes successful and failed trajectories from the experience replay buffer to extract reasoning patterns and generate improved instructions. During training, the policy samples from the instruction population to guide its reasoning process, creating a feedback loop where both policy and instructions improve through mutual interaction. The framework integrates seamlessly with standard RLVR pipelines while adding the instruction optimization component as a complementary evolutionary process.

## Key Results
- Achieves 38.2% average EM score on all benchmarks with Qwen-2.5-3B, surpassing Search-R1 by 6%
- Demonstrates consistent improvement across multiple tasks including RE-Talk and HotpotQA
- Shows that evolved instructions discover more strategic reasoning paths and avoid erroneous patterns
- Maintains only marginal computational overhead compared to baseline RLVR approaches

## Why This Works (Mechanism)
INSPO works by treating instruction optimization as a dynamic evolutionary process rather than a static design choice. The key insight is that effective reasoning instructions for LLM agents are not universal but depend on the current policy capabilities and task requirements. By maintaining a population of instructions that evolve based on policy performance feedback, the system can discover task-specific reasoning strategies that static instructions might miss. The on-policy reflection mechanism leverages the policy's own experiences to generate new instructions, creating a self-improving cycle where successful reasoning patterns are captured and propagated while ineffective ones are eliminated.

## Foundational Learning

**Reinforcement Learning from Verifiable Rewards (RLVR)**
- Why needed: Provides the base policy optimization framework that INSPO enhances
- Quick check: Verify that the policy improves on standard RLVR benchmarks before adding instruction evolution

**Experience Replay Buffers**
- Why needed: Stores trajectory data for retrospective analysis during instruction generation
- Quick check: Confirm the buffer captures both successful and failed trajectories with sufficient detail

**Evolutionary Algorithm Principles**
- Why needed: Guides the population-based approach to instruction optimization
- Quick check: Validate that selection pressure appropriately balances exploration and exploitation

**On-policy Reflection**
- Why needed: Enables the system to learn from its own experiences rather than external supervision
- Quick check: Test that reflection quality correlates with instruction improvement rates

**Population-based Optimization**
- Why needed: Maintains diversity of reasoning strategies while allowing focused improvement
- Quick check: Monitor instruction population diversity metrics over training iterations

## Architecture Onboarding

**Component Map**
Experience Replay Buffer -> Reflection Engine -> Instruction Population -> Policy Sampler -> Policy Executor -> Reward Collector -> Experience Replay Buffer

**Critical Path**
Policy Executor -> Reward Collector -> Instruction Population Update -> Policy Sampler -> Policy Executor

**Design Tradeoffs**
- Population size vs. computational overhead: Larger populations provide more diversity but increase reflection computation
- Pruning frequency vs. stability: More frequent pruning enables faster adaptation but risks premature convergence
- Reflection depth vs. quality: Deeper analysis produces better instructions but requires more computational resources

**Failure Signatures**
- Population collapse: All instructions converge to similar patterns, indicating insufficient diversity maintenance
- Oscillation: Instruction performance fluctuates without sustained improvement, suggesting unstable selection pressure
- Stagnation: No new instructions generated over extended periods, indicating reflection mechanism issues

**First Experiments**
1. Test instruction evolution on a simple synthetic reasoning task to verify basic functionality
2. Compare INSPO performance against static instruction baselines on RE-Talk
3. Analyze evolved instruction patterns to understand what reasoning strategies emerge

## Open Questions the Paper Calls Out
- **Open Question 1**: Does INSPO's effectiveness transfer to other tool types beyond search-based retrieval?
- **Open Question 2**: Can INSPO work effectively with weaker or open-source LLM-based optimizers?
- **Open Question 3**: Does INSPO scale effectively to larger model families beyond Qwen 2.5?
- **Open Question 4**: How sensitive is INSPO to the choice of hyperparameters governing the population dynamics?

## Limitations
- Experimental validation is limited to two specific tasks (RE-Talk and HotpotQA) using Qwen-2.5-3B
- Computational overhead claims of "marginal" impact are not quantified with specific metrics
- The effectiveness of individual components (pruning, reflection, generation) lacks systematic ablation studies
- Claims about discovering "innovative" strategies are supported by qualitative examples but lack systematic analysis

## Confidence
- **High**: The core conceptual framework of integrating instruction optimization into the RL loop is technically sound and well-articulated
- **Medium**: The empirical improvements over static instruction baselines appear substantial but require broader validation
- **Low**: Claims about instruction evolution discovering "innovative" strategies are supported by qualitative examples but lack systematic analysis of what makes evolved instructions effective

## Next Checks
1. Test INSPO across multiple model sizes (including >10B parameters) and architectures to verify scalability and robustness of the co-evolution approach
2. Conduct ablation studies isolating the contributions of the pruning mechanism, experience replay buffer, and on-policy reflection to quantify their individual impact on performance
3. Measure and report concrete computational overhead metrics (training time, memory usage, inference latency) to validate the "marginal" overhead claim with quantitative evidence