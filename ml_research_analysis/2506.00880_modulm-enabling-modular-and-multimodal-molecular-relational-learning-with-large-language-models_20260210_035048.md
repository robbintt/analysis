---
ver: rpa2
title: 'ModuLM: Enabling Modular and Multimodal Molecular Relational Learning with
  Large Language Models'
arxiv_id: '2506.00880'
source_url: https://arxiv.org/abs/2506.00880
tags:
- molecular
- modulm
- learning
- interaction
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ModuLM introduces a modular framework for benchmarking LLM-based
  molecular relational learning, supporting diverse molecular inputs (1D SMILES, 2D
  graphs, 3D conformations), 50+ model configurations, and systematic evaluation.
  On DDI tasks, it achieves AUC-ROC up to 0.982 and accuracy up to 0.964; on SSI tasks,
  MAE as low as 0.340 and RMSE down to 0.601; on CSI tasks, MAE down to 15.42.
---

# ModuLM: Enabling Modular and Multimodal Molecular Relational Learning with Large Language Models

## Quick Facts
- **arXiv ID**: 2506.00880
- **Source URL**: https://arxiv.org/abs/2506.00880
- **Reference count**: 40
- **Primary result**: Achieves AUC-ROC up to 0.982 on DDI tasks, MAE as low as 0.340 on SSI tasks, and MAE down to 15.42 on CSI tasks

## Executive Summary
ModuLM introduces a modular framework for benchmarking LLM-based molecular relational learning, supporting diverse molecular inputs (1D SMILES, 2D graphs, 3D conformations), 50+ model configurations, and systematic evaluation. The framework enables dynamic model assembly and ablation studies, highlighting the importance of multimodal data and interaction layers over model scale. On DDI tasks, it achieves AUC-ROC up to 0.982 and accuracy up to 0.964; on SSI tasks, MAE as low as 0.340 and RMSE down to 0.601; on CSI tasks, MAE down to 15.42. ModuLM addresses the lack of standardized LLM-MRL benchmarking by providing extensible, reproducible experiments.

## Method Summary
ModuLM provides a modular framework that integrates diverse molecular representations (1D SMILES, 2D molecular graphs, and 3D conformations) with large language models for molecular relational learning tasks. The framework supports systematic evaluation across three task types: drug-drug interaction (DDI), structure similarity index (SSI), and cell signaling index (CSI). It incorporates specialized encoders (GCN, GAT, Uni-Mol, etc.) for different molecular representations, interaction layers for modeling molecular pair relationships, and alignment mechanisms to bridge encoder outputs with LLM embeddings. The framework allows for comprehensive ablation studies and dynamic model assembly through its modular design.

## Key Results
- DDI tasks: AUC-ROC up to 0.982 and accuracy up to 0.964
- SSI tasks: MAE as low as 0.340 and RMSE down to 0.601
- CSI tasks: MAE down to 15.42
- Framework supports 50+ model configurations and systematic evaluation
- Multimodal data integration and interaction layers show greater impact than model scale

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal molecular representations improve LLM-based relational learning performance.
- Mechanism: The framework integrates 1D SMILES strings, 2D molecular graphs, and 3D molecular conformations through specialized encoders (GCN, GAT, Uni-Mol, etc.). This allows the model to capture structural information at multiple levels of abstraction—sequential, topological, and spatial—which are then aligned and fed into the LLM backbone. The richer feature set allows the LLM to learn more robust representations of molecular interactions.
- Core assumption: Distinct molecular representations (1D, 2D, 3D) capture non-redundant structural information crucial for predicting interactions, and this information can be effectively fused within an LLM's context window.
- Evidence anchors:
  - [abstract] "supporting diverse molecular inputs (1D SMILES, 2D graphs, 3D conformations)... highlighting the importance of multimodal data."
  - [section] "integrating multimodal information, such as 2D molecular graphs and 3D molecular conformations, can indeed enhance model performance. Notably, models that incorporate 3D molecular conformation information achieve the best results."
  - [corpus] Neighbor paper "ProtoMol" also supports this, stating multimodal representation learning "enhances predictive accuracy... through the integration of structural and textual" information.
- Break condition: Performance gains diminish or disappear if the encoders for different modalities produce highly correlated embeddings, or if the alignment process (MLP/Q-Former) fails to project them into a compatible semantic space for the LLM.

### Mechanism 2
- Claim: Explicit interaction layers significantly improve the modeling of molecular pair relationships.
- Mechanism: Beyond encoding individual molecules, the framework introduces dedicated interaction layers (e.g., Bilinear Attention, Cross Attention) that operate on the combined embeddings of the molecular pair. These layers are designed to explicitly capture and model the relationships and dependencies between the two entities before the final LLM reasoning stage.
- Core assumption: The relationship between molecular pairs is not merely a function of their individual properties but requires explicit modeling of their joint feature space.
- Evidence anchors:
  - [abstract] "highlighting the importance of multimodal data and interaction layers over model scale."
  - [section] "adding interaction layers consistently improves model performance to some extent. This confirms the importance of interaction layers in LLM-based MRL models."
  - [corpus] Corpus evidence is limited; the paper 'Representational Alignment with Chemical Induced Fit for Molecular Relational Learning' focuses on substructure alignment, which is related but not direct evidence for general interaction layers.
- Break condition: If the interaction layer design is too simplistic (e.g., simple concatenation) or too complex (causing overfitting on small datasets), it will fail to capture meaningful relational features, negating the benefit.

### Mechanism 3
- Claim: Smaller, fine-tuned LLMs can outperform larger models on specialized MRL tasks due to better task-specific adaptation.
- Mechanism: The framework allows for systematic comparison of various LLM backbones. Smaller models, when subjected to fine-tuning (e.g., via LoRA), may adapt their parameters more significantly to the specific MRL dataset than larger models, which tend to retain stronger generalization capabilities that might hinder task-specific optimization.
- Core assumption: The pre-training knowledge of LLMs is broadly sufficient, and the bottleneck for MRL tasks lies in the model's ability to adapt to the specific data distribution and interaction patterns rather than in its raw scale.
- Evidence anchors:
  - [abstract] "highlighting the importance of multimodal data and interaction layers over model scale."
  - [section] "larger model sizes do not necessarily lead to better performance in MRL tasks... smaller LLMs adapt better during fine-tuning, leading to stronger task specialization in MRL scenarios."
  - [corpus] No direct corpus evidence found comparing LLM scale in MRL.
- Break condition: The trend may reverse on larger or more complex datasets where the reasoning capacity of larger models becomes critical, or if the smaller model lacks sufficient capacity to integrate the multimodal inputs effectively.

## Foundational Learning

**Molecular Relational Learning (MRL)**
- Why needed here: This is the core problem the framework addresses. MRL involves predicting properties or relationships (like interaction type, binding affinity) between a *pair* of molecules, not just a single one.
- Quick check question: Can you explain why predicting a drug-drug interaction is a relational learning task rather than a simple property prediction task?

**Molecular Representations (1D, 2D, 3D)**
- Why needed here: ModuLM's primary contribution is its ability to ingest and process molecules in three different formats. Understanding what information each captures (sequence, graph topology, spatial geometry) is essential for using the framework.
- Quick check question: Which molecular representation would you expect to be most important for predicting a property that depends on the precise fit of a small molecule into a protein's binding pocket?

**Encoder-LLM Alignment (Q-Former/MLP)**
- Why needed here: Molecular encoders (like GNNs) output vectors in one space, while LLMs operate on token embeddings. An alignment mechanism is a prerequisite for feeding one into the other.
- Quick check question: If you were using a pre-trained graph encoder and a frozen LLM, why would you need a trainable projection layer between them?

## Architecture Onboarding

**Component map:**
Data Preprocessing -> Encoders (Uni-Mol for 3D, GIN for 2D) -> Interaction Layers (Cross Attention, Bilinear) -> Alignment Projector (MLP/Q-Former) -> LLM Backbone (LLaMA, DeepSeek) -> Prediction

**Critical path:** Data -> Encoder -> Interaction Layer -> Alignment Projector -> LLM Backbone -> Prediction. The accuracy of the entire system hinges on the quality of the encoder outputs and the effectiveness of the alignment step.

**Design tradeoffs:**
- Encoder Choice: 3D encoders (Uni-Mol) are more expressive but computationally expensive and require 3D data generation (via RDKit). 2D encoders are faster but may miss stereochemical details.
- Alignment Method: Q-Former allows for more complex text-molecule alignment during pre-training but adds complexity. A simple MLP is faster but may provide a less rich bridge.
- Interaction Layer: Simple mean pooling is fast but uninformative. Bilinear attention is powerful but can overfit on small datasets.

**Failure signatures:**
- Catastrophic Forgetting: The LLM loses its general language understanding after fine-tuning if the learning rate is too high.
- Modality Collapse: The LLM ignores the molecular embeddings and bases its prediction solely on the text prompt, often seen when alignment is poor.
- Interaction Overfitting: The model learns spurious correlations in the interaction layer, performing well on validation data but failing on test data with new molecule combinations.

**First 3 experiments:**
1. Baseline Establishment: Run a model with a single modality (e.g., 1D SMILES only) on a standard DDI dataset to establish a performance floor.
2. Modality Ablation: Add one additional modality (e.g., 2D graphs) and compare performance against the baseline to quantify the added value of structural information.
3. Interaction Layer Impact: Using the best-performing modality setup, compare a model with a simple "Mean" interaction layer versus a "Cross Attention" layer to validate the importance of explicit interaction modeling.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the performance landscape and optimal configuration within ModuLM's full space of over 50,000 potential model combinations?
- Basis in paper: [explicit] The authors state in the Limitations section that "a comprehensive exploration of all possible model combinations is beyond the scope of this study" and identify this as a "key direction for future research."
- Why unresolved: The current study only selects 15 specific configurations to demonstrate the framework's capability, leaving the vast majority of the architectural search space unexplored.
- What evidence would resolve it: A systematic benchmark study using Neural Architecture Search (NAS) or grid search across the full suite of 2D/3D encoders and interaction layers.

### Open Question 2
- Question: Why does increasing the LLM backbone scale (e.g., from 1.5B to 14B parameters) often result in reduced performance on specific MRL tasks?
- Basis in paper: [explicit] In Section 4.2, the authors observe that "larger model sizes do not necessarily lead to better performance" and hypothesize that strong generalization in larger LLMs may limit task-specific adaptation.
- Why unresolved: The paper identifies the phenomenon and offers a hypothesis (over-generalization) but does not isolate the mechanism or test it via varying fine-tuning depths or regularization strategies.
- What evidence would resolve it: Ablation studies comparing full-parameter fine-tuning against PEFT (LoRA) across different model scales to determine if the issue is optimization difficulty or capacity saturation.

### Open Question 3
- Question: How does the integration of newer generation LLMs and geometric encoders impact the performance ceiling of the ModuLM framework?
- Basis in paper: [explicit] The Future Work section commits to "introducing additional types of encoders... and expanding support for a broader range of LLMs to improve generalizability."
- Why unresolved: The current study restricts experiments to a fixed set of 7 LLM backbones and specific 2D/3D encoders, leaving the behavior of state-of-the-art or larger architectures unknown.
- What evidence would resolve it: Benchmark results from integrating the ModuLM interface with newer open-weights models (e.g., Llama 3) and advanced 3D encoders not included in the initial suite.

## Limitations
- Dataset Generalization Gap: Effectiveness on out-of-distribution molecular structures remains uncertain
- Computational Resource Requirements: Integration creates significant overhead without reported efficiency metrics
- Alignment Method Sensitivity: Performance gains depend heavily on alignment hyperparameter choices

## Confidence

**High Confidence**: The framework's modular architecture and its ability to systematically benchmark different model configurations are well-established. The core claim that multimodal data integration and interaction layers improve performance over single-modality approaches is supported by consistent experimental results across all three task types.

**Medium Confidence**: The claim that smaller LLMs can outperform larger ones in MRL tasks due to better fine-tuning adaptation is supported by the reported results but lacks broader validation. The optimal model size appears to be dataset-dependent, and the general trend requires further testing across diverse molecular prediction tasks.

**Low Confidence**: The assertion that ModuLM's performance represents a significant advance over existing specialized molecular prediction methods is difficult to evaluate without direct comparisons to state-of-the-art non-LLM approaches on the same benchmarks.

## Next Checks
1. **Cross-Domain Transferability Test**: Evaluate ModuLM on a completely different molecular prediction task (e.g., protein-ligand binding affinity prediction) using molecules structurally distinct from the training datasets to assess true generalization capability.

2. **Computational Efficiency Profiling**: Measure end-to-end inference time, GPU memory usage, and throughput for different model configurations (varying LLM sizes, encoder combinations) to identify practical deployment constraints.

3. **Ablation on Alignment Mechanisms**: Systematically compare the MLP and Q-Former alignment approaches across multiple LLM architectures to determine whether the performance gains from multimodal integration are robust to the choice of alignment method.