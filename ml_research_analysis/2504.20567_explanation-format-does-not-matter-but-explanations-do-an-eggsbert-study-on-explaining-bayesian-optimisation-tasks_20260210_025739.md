---
ver: rpa2
title: Explanation format does not matter; but explanations do -- An Eggsbert study
  on explaining Bayesian Optimisation tasks
arxiv_id: '2504.20567'
source_url: https://arxiv.org/abs/2504.20567
tags:
- explanation
- explanations
- participants
- task
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the impact of explanation formats on user
  performance in Bayesian Optimization (BO) tasks. Researchers created an accessible
  egg cooking scenario where participants tuned six parameters to achieve a perfect
  soft-boiled egg.
---

# Explanation format does not matter; but explanations do -- An Eggsbert study on explaining Bayesian Optimisation tasks

## Quick Facts
- **arXiv ID:** 2504.20567
- **Source URL:** https://arxiv.org/abs/2504.20567
- **Reference count:** 40
- **Primary result:** Any explanation format significantly increases Bayesian Optimization task success rate (29% to 54%) without affecting mental load, but format choice doesn't impact performance.

## Executive Summary
This study investigates how different explanation formats affect user performance in Bayesian Optimization (BO) tasks. Using an accessible egg cooking scenario, researchers compared three explanation formats—visual bar charts, rule-based lists, and natural language descriptions—against a no-explanation baseline. Results show that explanations significantly improve task success rates and reduce trials needed, but the specific format doesn't matter. Users benefit from understanding which parameters to tune and their optimal ranges, regardless of whether this information is presented visually, as rules, or in natural language.

## Method Summary
The study used a between-subjects online experiment with 213 participants tuning six parameters to achieve a perfect soft-boiled egg using Bayesian Optimization. Three explanation formats were tested: Visual (bar charts showing parameter importance), Rules (list of "Tune" and "No-Tune" parameters with ranges), and Language (GPT-4 generated text descriptions). The TNTRules algorithm generated consistent explanation content across formats by sampling the Gaussian Process surrogate model. Participants were randomly assigned to one of the four conditions (including control) and completed the egg cooking task while their success rate, trials, understanding, trust, and task load were measured.

## Key Results
- Explanations significantly increased task success rate from 29% (control) to 54% (any explanation)
- All explanation formats reduced the number of trials needed to achieve success
- No significant differences were found between Visual, Rules, and Language formats
- Explanations improved understanding and trust without increasing perceived task load

## Why This Works (Mechanism)

### Mechanism 1: Actionable Search Space Reduction
- **Claim:** Providing explanations that categorize parameters into "Tune" vs. "No-Tune" significantly improves task success rates by reducing the effective dimensionality of the optimization problem for the user.
- **Mechanism:** The TNTRules algorithm aggregates the Gaussian Process surrogate model to identify insensitive parameters. By explicitly labeling these as "No Tune," the system lowers the cognitive load required to explore the parameter space, allowing users to focus adjustment efforts on high-sensitivity variables.
- **Core assumption:** Users struggle primarily with identifying *which* parameters to adjust rather than *how* to adjust them.
- **Evidence anchors:** Results showed that any explanation format significantly increased task success rate (from 29% to 54%) and reduces trials needed.

### Mechanism 2: Information Equivalence (Content Over Format)
- **Claim:** If the informational content (parameter importance and ranges) is high-fidelity and consistent across conditions, the specific visual format has negligible impact on immediate task performance.
- **Mechanism:** The study standardized the *content* across all three formats using the TNTRules output. Because the "Tune" ranges and "No-Tune" stability information were identical in substance, the cognitive benefit derived from this guidance remained constant regardless of whether it was read as text or viewed as a bar chart.
- **Core assumption:** Users can effectively parse and utilize the guidance equally well in visual, textual, or rule-based formats for low-dimensional tasks (6 parameters).
- **Evidence anchors:** Kruskal-Wallis test confirmed that there was neither a significant difference in success rate nor trials across the three explanation formats.

### Mechanism 3: Trust-Based Efficiency
- **Claim:** Explanations increase user trust and perceived understanding, which correlates with reduced mental demand and faster task completion.
- **Mechanism:** Transparency acts as a cognitive buffer. By revealing the system's reasoning (even abstractly), the "black box" anxiety is reduced. This is evidenced by increased Trust in Automation (TiA) scores and lower NASA-TLX mental demand scores in the explanation conditions.
- **Core assumption:** The explanations provided were "truthful" (high fidelity to the underlying optimization function).
- **Evidence anchors:** Adding explanations significantly reduce mental demand and participants experienced their performance as significantly better.

## Foundational Learning

- **Concept: Bayesian Optimization (BO) Loop**
  - **Why needed here:** The study frames the "Eggsbert" system as a BO agent. You must understand that BO is a sequential model-based approach (using a Gaussian Process) designed to minimize a black-box function.
  - **Quick check question:** Does the optimizer require the true function form to be known, or does it learn it via sampling?

- **Concept: Surrogate Model Explainability**
  - **Why needed here:** The paper uses TNTRules, which explains the *surrogate* (the GP model), not the raw data. Understanding that the explanation is an approximation of an approximation is critical for interpreting the results.
  - **Quick check question:** If the GP posterior uncertainty is high in a region, should the "explanation" for that region be high or low confidence?

- **Concept: Task Load vs. Performance**
  - **Why needed here:** A key finding is that explanations improved performance *without* increasing task load. Distinguishing between objective performance (success rate) and subjective experience (NASA-TLX) is necessary to interpret the utility of the XBO methods.
  - **Quick check question:** If an explanation doubles the reading time but halves the number of trials, does the total task load necessarily increase?

## Architecture Onboarding

- **Component map:** Bayesian Optimizer (GPyTorch/GP) -> Explanation Generator (TNTRules) -> Format Renderer (Bar Chart/Rule List/GPT-4 Text Verbalizer) -> User Interface (Sliders & Feedback)

- **Critical path:**
  1. GP Update: The BO loop updates the Gaussian Process with new evaluation data (user trials)
  2. Rule Extraction: TNTRules samples the GP to generate the Explanation Dataset
  3. Discretization: The continuous GP space is binned into "Tune" ranges vs. "No-Tune" constants
  4. Verbalization/Visualization: The abstract rules are piped into the specific modality (e.g., GPT-4 for the LANGUAGE condition)

- **Design tradeoffs:**
  - Truthfulness vs. Simplicity: The paper notes explanations were high-fidelity. In production, simplifying complex multi-modal distributions into single "Tune" ranges risks losing valid optima
  - Format Selection: Since the paper found no performance difference, selection should be based on integration cost (e.g., text is easier to log; charts are better for dashboards)

- **Failure signatures:**
  - Static Recommendations: If users blindly follow recommendations without fine-tuning within ranges (over-reliance), the optimization may stall in local optima
  - Sensitivity Mismatch: If the surrogate model misidentifies a highly sensitive parameter as "No-Tune," the user will fail to correct the outcome

- **First 3 experiments:**
  1. Baseline Replication: Implement the "Egg" environment and TNTRules logic; verify that the "Tune" ranges actually correspond to the mathematical optimum of the cooking equation
  2. Format Stress Test: Increase parameter dimensionality (e.g., from 6 to 12) to see if the "Format doesn't matter" result holds or if Visual/Rule formats degrade compared to structured text
  3. Noise Injection: Deliberately perturb the BO recommendations to verify that the "Tune" ranges are wide enough to guide users back to the optimum despite noisy starting points

## Open Questions the Paper Calls Out
- Do explanation formats differentially impact users' mental models and performance in prolonged usage scenarios?
- Do these findings generalize to high-dimensional, real-world industrial tasks involving domain experts?
- How do explanation formats affect objective measures of understanding compared to self-reported perceptions?

## Limitations
- The "format doesn't matter" finding is limited to low-dimensional (6-parameter) tasks with expert-designed physics functions
- No validation of explanation quality for tasks outside controlled laboratory conditions
- Unknown generalizability to non-expert users or high-dimensional spaces
- GPT-4 text generation introduces potential variability not controlled for

## Confidence
- **High confidence**: Explanations improve success rates and reduce trials (p < 0.001, large effect sizes)
- **Medium confidence**: No significant format differences (p = 0.177, small sample sizes per condition)
- **Low confidence**: Generalizability to real-world BO tasks or different user populations

## Next Checks
1. Test explanation effectiveness with non-expert users on more complex parameter spaces
2. Evaluate performance degradation when explanations contain errors or are unfaithful to the surrogate model
3. Measure long-term retention and transfer of explanation-based knowledge to new tasks