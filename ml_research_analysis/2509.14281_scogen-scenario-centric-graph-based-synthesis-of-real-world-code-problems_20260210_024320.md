---
ver: rpa2
title: 'SCoGen: Scenario-Centric Graph-Based Synthesis of Real-World Code Problems'
arxiv_id: '2509.14281'
source_url: https://arxiv.org/abs/2509.14281
tags:
- domain
- knowledge
- code
- coding
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the scarcity of real-world coding problems
  that constrains the advancement of code large language models. The authors propose
  SCoGen, a framework that synthesizes realistic code problems by extracting application
  scenarios, domain knowledge, domain skills, and coding skills from real-world datasets
  like Stack Overflow and Kaggle.
---

# SCoGen: Scenario-Centric Graph-Based Synthesis of Real-World Code Problems

## Quick Facts
- **arXiv ID**: 2509.14281
- **Source URL**: https://arxiv.org/abs/2509.14281
- **Reference count**: 40
- **Key outcome**: SCoGen synthesizes realistic code problems by extracting application scenarios, domain knowledge, domain skills, and coding skills from real-world datasets, resulting in fine-tuned models that significantly outperform state-of-the-art on real-world benchmarks (24.32% accuracy on BigCodeBench Instruct Hard vs. 18.24% baseline) while maintaining competitive performance on algorithm-level tasks.

## Executive Summary
This paper addresses the scarcity of real-world coding problems that constrains the advancement of code large language models. The authors propose SCoGen, a framework that synthesizes realistic code problems by extracting application scenarios, domain knowledge, domain skills, and coding skills from real-world datasets like Stack Overflow and Kaggle. These elements are integrated into a scenario-centric graph, and a sampling strategy controls the complexity and diversity of generated problems. Experiments show that fine-tuned models using SCoGen significantly outperform state-of-the-art open-source models on real-world benchmarks while maintaining competitive performance on algorithm-level tasks.

## Method Summary
SCoGen synthesizes realistic code problems through a multi-stage pipeline. First, it curates and filters Stack Overflow (3M documents) and Kaggle notebooks (0.5M documents) based on length and language criteria. Next, an LLM extracts four fundamental elements—application scenarios, domain knowledge, domain skills, and coding skills—from each document. These elements are connected in a scenario-centric graph where edges represent co-occurrence within the same document. A sampling strategy (controlled by complexity and temperature parameters) selects feature combinations, which are then synthesized into problems using an LLM. Finally, answers are generated and the resulting 500K question-answer pairs are used to fine-tune code LLMs via supervised learning.

## Key Results
- Fine-tuned models using SCoGen significantly outperform state-of-the-art open-source models on real-world benchmarks (24.32% accuracy on BigCodeBench Instruct Hard vs. 18.24% baseline)
- The framework maintains competitive performance on algorithm-level benchmarks (19.00% accuracy on LiveCodeBench vs. 16.13% baseline)
- Intermediate complexity and temperature settings (C1T2, C2T3) yield optimal performance, while high complexity with high temperature (C3T3) degrades results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structuring problem elements in a scenario-centric graph improves task realism by capturing how domain knowledge, domain skills, and coding skills co-occur in actual programming contexts.
- Mechanism: Edges connect nodes that co-occur in the same seed document; traversing from an application scenario node retrieves knowledge and skill combinations that have actually been used together, preserving contextual dependencies that random combination would break.
- Core assumption: Real-world programming tasks are defined by stable co-occurrence patterns among domain concepts and implementation techniques.
- Evidence anchors:
  - [abstract] "a scenario-centric graph that interconnects domain knowledge, domain skills, and coding skills"
  - [section 3.3] "G = (V, E), where V is the set of all entities... and E is the set of edges representing the relationships between these entities... An edge (u, v) ∈ E exists if the corresponding entities u and v co-occur within the same document"
  - [corpus] Weak direct corpus support for this specific graph mechanism; neighbor papers focus on multimodal code generation and benchmarking, not graph-based synthesis.
- Break condition: If seed documents are noisy or sparse, co-occurrence edges may reflect incidental rather than meaningful relationships, degrading problem coherence.

### Mechanism 2
- Claim: Controlling sampling temperature and problem complexity enables a tunable tradeoff between diversity and coherence in generated problems.
- Mechanism: Temperature T flattens the transition probability distribution over graph neighbors; higher T increases sampling of lower-probability nodes, expanding diversity. Complexity (number of features) controls how many knowledge–skill combinations are stacked into a single problem. Empirically, intermediate settings (C1–C2, T1–T2) perform best; high complexity with high temperature (C3T3) degrades performance.
- Core assumption: Diversity and coherence are inversely related; too much diversity produces incoherent problem specifications.
- Evidence anchors:
  - [section 3.4.1] "As T increases, the distribution becomes flatter, giving higher weight to nodes with lower normalized probabilities"
  - [section 4.3.2] "under the combination of complexity C3 and temperature setting T3... the synthesized questions may consist of knowledge and skill components that lack coherence"
  - [corpus] No direct external validation; corpus papers do not examine temperature/complexity tradeoffs in synthesis.
- Break condition: If the graph has weak connectivity, high temperature may sample nearly unrelated nodes, yielding unrealistic or unsolvable problems.

### Mechanism 3
- Claim: Training on scenario-grounded synthetic problems improves real-world benchmark performance while maintaining algorithm-level capability through transfer.
- Mechanism: Synthetic problems require models to integrate domain reasoning with code implementation, exercising skills that align with tasks in BigCodeBench and NaturalCodeBench. Because these problems embed algorithmic elements within broader contexts, models retain algorithm-level performance without explicit algorithmic training data.
- Core assumption: Real-world coding tasks share underlying reasoning patterns with synthesized scenario-centric problems.
- Evidence anchors:
  - [abstract] "fine-tuned models using SCoGen significantly outperform state-of-the-art open-source models on real-world benchmarks (e.g., 24.32% accuracy on BigCodeBench Instruct Hard vs. 18.24% baseline)"
  - [section 4.2] "our model also exhibits strong performance on algorithm-level benchmarks... On the LiveCodeBench benchmark, our fine-tuned Qwen2.5-Coder-7B achieves an accuracy of 19.00%, significantly outperforming the Qwen2.5-Coder-7B-Instruct, which attains 16.13%"
  - [corpus] Indirect support from HackerRank-ASTRA (arXiv 2502.00226), which emphasizes evaluating LLMs on cross-domain, multi-file project problems—aligned with SCoGen's scenario-centric focus.
- Break condition: If synthetic problems diverge in distribution from target benchmarks, improvements will not transfer.

## Foundational Learning

- Concept: Knowledge Graph Construction and Traversal
  - Why needed here: Understanding how nodes and edges represent entities and co-occurrence relations is prerequisite to following the sampling and synthesis pipeline.
  - Quick check question: Given nodes A, B, C with co-occurrence frequencies f(A,B)=10, f(A,C)=5, what is the first-step transition probability P(A→B)?

- Concept: Softmax Temperature Scaling
  - Why needed here: The temperature parameter T reshapes probability distributions over graph neighbors; grasping this is essential to tuning diversity vs. coherence.
  - Quick check question: If T→∞ in a softmax over three probabilities [0.7, 0.2, 0.1], what does the resulting distribution approach?

- Concept: Supervised Fine-Tuning (SFT) for Code LLMs
  - Why needed here: The method applies SFT on 500K synthetic question-answer pairs; understanding SFT mechanics clarifies how synthetic data translates into benchmark gains.
  - Quick check question: What is the risk of fine-tuning on synthetic data without answer verification?

## Architecture Onboarding

- Component map: Seed Curation -> Fundamental Elements Extraction -> Scenario-Centric Graph Construction -> Sampling Strategy (Random or LLM-based) -> Problem Synthesis (LLM) -> Answer Generation (LLM) -> SFT Training Data

- Critical path:
  1. Ingest and deduplicate Stack Overflow and Kaggle documents (3M + 0.5M final)
  2. Extract AS, DK, DS, CS per document using prompted LLM
  3. Build graph G with typed edges based on co-occurrence
  4. Compute transition probabilities (first- and second-step); apply temperature
  5. Sample features for target complexity; prompt LLM to synthesize problems
  6. Generate answers; assemble 500K pairs for SFT

- Design tradeoffs:
  - Random vs. LLM-based sampling: Random preserves graph distribution; LLM-based may introduce selection bias and reduce diversity (Table 2 shows random averaging 33.42% vs. LLM 32.39%)
  - Complexity vs. temperature: C3T3 yields lowest average (32.05%); intermediate settings (C1T3, C2T3) perform best (34.61%, 34.00%)
  - No answer verification: Faster iteration but risks noisy supervision; deferred to future work

- Failure signatures:
  - Incoherent problems: Typically arise at C3T3—too many diverse elements break scenario unity
  - Low benchmark lift: May indicate temperature too low (insufficient diversity) or graph edges too sparse (limited reachable nodes)
  - Domain mismatch: If seed sources lack coverage of target benchmark domains, synthetic problems will not transfer

- First 3 experiments:
  1. Reproduce C1T2 on Qwen2.5-Coder-7B with 100K samples; confirm BCB Instruct Hard lift vs. baseline
  2. Ablate graph connectivity: randomly shuffle edges and measure performance drop to validate co-occurrence mechanism
  3. Add sandbox answer verification on a 10K subset; compare error rates and downstream benchmark deltas to quantify supervision noise impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can a formal answer verification mechanism be integrated to ensure the functional correctness of synthesized solutions?
- Basis in paper: [explicit] The authors state they "defer the implementation of a formal answer verification mechanism to future work" and currently lack sandbox validation.
- Why unresolved: Current synthesis relies solely on LLM generation without execution-based verification, risking logical errors in the training data.
- What evidence would resolve it: Integration of a sandbox environment yielding higher execution pass rates for generated samples.

### Open Question 2
- Question: Can the framework be extended to generate repository-level challenges rather than isolated code generation tasks?
- Basis in paper: [explicit] The Limitations section notes the framework "do[es] not yet fully encompass repository-level challenges" and focuses on code generation.
- Why unresolved: Current questions fail to capture the complexities of cross-file dependencies and architectural decisions inherent in real-world software repositories.
- What evidence would resolve it: Successful synthesis of multi-file problems requiring inter-file dependency resolution for solution.

### Open Question 3
- Question: Does the efficacy of SCoGen scale effectively to models significantly larger than 8B parameters?
- Basis in paper: [explicit] The Conclusion states the intent to "investigate the scalability of our framework by applying it to larger models with 32B or more."
- Why unresolved: Experiments were restricted to 1.5B–8B models; it is unclear if this data synthesis strategy saturates or improves capabilities in larger base models.
- What evidence would resolve it: Fine-tuning a 32B+ model demonstrating consistent performance gains over baselines on BigCodeBench.

## Limitations

- The paper lacks formal answer verification, relying solely on LLM generation without execution-based validation, which introduces potential noise in the training data
- The framework does not yet encompass repository-level challenges, focusing only on isolated code generation tasks rather than multi-file dependencies and architectural decisions
- Critical implementation details for MinHash deduplication, Qwen3-32B "non-thinking mode" configuration, and stratified subsampling strategy are unspecified, limiting reproducibility

## Confidence

- **High Confidence**: The core claim that SCoGen significantly improves real-world benchmark performance (24.32% vs 18.24% on BigCodeBench Instruct Hard) is well-supported by the experimental results presented.
- **Medium Confidence**: The mechanism explanation for how the scenario-centric graph preserves contextual dependencies through co-occurrence edges is plausible but lacks direct external validation in the corpus.
- **Low Confidence**: The assertion that training on synthetic problems maintains algorithm-level capability through transfer is supported only by single-benchmark comparisons and could reflect other factors.

## Next Checks

1. Reproduce the C1T2 configuration on Qwen2.5-Coder-7B with 100K samples to verify the BCB Instruct Hard performance lift against baseline.
2. Conduct an ablation study by randomly shuffling graph edges and measuring performance degradation to validate the co-occurrence mechanism's contribution.
3. Implement sandbox answer verification on a 10K subset and compare error rates and benchmark deltas against the unverified approach to quantify supervision noise impact.