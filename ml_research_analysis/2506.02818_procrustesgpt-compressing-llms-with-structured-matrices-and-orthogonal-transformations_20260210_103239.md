---
ver: rpa2
title: 'ProcrustesGPT: Compressing LLMs with Structured Matrices and Orthogonal Transformations'
arxiv_id: '2506.02818'
source_url: https://arxiv.org/abs/2506.02818
tags:
- matrices
- matrix
- arxiv
- orthogonal
- compression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ProcrustesGPT, a fine-tuning-free framework
  for compressing large language models (LLMs) using structured matrices and orthogonal
  transformations. The key insight is that LLM outputs are invariant under certain
  orthogonal transformations of weight matrices, which can be leveraged to improve
  compressibility within structured matrix classes.
---

# ProcrustesGPT: Compressing LLMs with Structured Matrices and Orthogonal Transformations

## Quick Facts
- arXiv ID: 2506.02818
- Source URL: https://arxiv.org/abs/2506.02818
- Reference count: 17
- Primary result: 25% compression of OPT and Llama2 models while maintaining perplexity and zero-shot performance

## Executive Summary
ProcrustesGPT introduces a fine-tuning-free framework for compressing large language models using structured matrices and orthogonal transformations. The method leverages the theoretical insight that LLM outputs remain invariant under certain orthogonal transformations of weight matrices, enabling improved compressibility within structured matrix classes. By formulating compression as an optimization problem to minimize output differences on calibration data, ProcrustesGPT achieves state-of-the-art results compared to other fine-tuning-free compression methods.

## Method Summary
The approach formulates LLM compression as an optimization problem where orthogonal transformations are found to minimize the difference between original and compressed network outputs on calibration data. The method exploits the theoretical property that LLM outputs are invariant under certain orthogonal transformations of weight matrices, which can be leveraged to improve compressibility within structured matrix classes. ProcrustesGPT is evaluated using Kronecker product and GS matrix representations on OPT and Llama2 models, achieving 25% compression of weight matrices while maintaining model performance across various tasks and model sizes.

## Key Results
- Achieves 25% compression of weight matrices in OPT and Llama2 models
- Outperforms SliceGPT and other fine-tuning-free baselines with perplexity of 36.08 vs 38.65 on WikiText2
- Maintains zero-shot task performance while achieving compression
- Kronecker product decompositions show slightly better accuracy than GS matrices

## Why This Works (Mechanism)
The framework works by exploiting the mathematical property that certain orthogonal transformations of weight matrices do not change the output of neural networks. This allows the method to search for optimal orthogonal transformations that make weight matrices more compressible within structured matrix representations while preserving the original network behavior. The optimization process finds transformations that minimize the difference between original and compressed outputs on calibration data, effectively making the structured approximation better fit the original weights.

## Foundational Learning
- **Orthogonal transformations**: Linear transformations that preserve inner products and vector norms; needed to understand the mathematical foundation of output invariance; quick check: verify that Q^T Q = I for orthogonal matrix Q
- **Structured matrix representations**: Matrices with specific patterns (Kronecker products, GS matrices) that enable efficient storage and computation; needed to understand compression targets; quick check: confirm that structured matrices reduce storage complexity
- **Calibration data**: Representative input data used to optimize orthogonal transformations; needed to ensure the compressed model maintains performance; quick check: verify that calibration data distribution matches deployment data
- **Output invariance**: Property where certain transformations preserve network outputs; needed to understand the theoretical basis for compression; quick check: test that transformed weights produce similar outputs
- **Optimization objectives**: Loss functions that measure difference between original and compressed outputs; needed to guide the transformation search; quick check: monitor convergence of optimization process

## Architecture Onboarding
**Component map**: Input data -> Original model -> Calibration outputs -> Orthogonal transformation optimization -> Structured matrix approximation -> Compressed model

**Critical path**: The optimization of orthogonal transformations is the critical path, as it directly determines the quality of the compressed model. This process requires careful selection of calibration data and optimization hyperparameters to achieve optimal compression without performance degradation.

**Design tradeoffs**: The method trades computational resources during compression (for finding optimal transformations) against inference efficiency and storage savings. Kronecker products offer better accuracy but may be less computationally efficient than GS matrices during inference, creating a fundamental tradeoff between compression quality and acceleration potential.

**Failure signatures**: Poor calibration data quality or insufficient quantity can lead to suboptimal orthogonal transformations and degraded model performance. Over-compression or inappropriate structured matrix choices may cause significant accuracy loss. Computational constraints during optimization can result in premature convergence to suboptimal solutions.

**3 first experiments**:
1. Test output invariance property on simple linear layers with random orthogonal transformations
2. Compare compression quality using different structured matrix representations on a small model
3. Evaluate sensitivity to calibration data size and quality on a medium-sized model

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the limitations section suggests several areas for future investigation regarding generalization to non-autoregressive tasks and the impact of calibration data requirements.

## Limitations
- Effectiveness depends on having sufficient quality calibration data
- Orthogonal transformation optimization can be computationally expensive for large models
- Current evaluation focuses primarily on autoregressive language modeling tasks
- Fine-tuning-free claim may oversimplify practical deployment considerations

## Confidence
**High confidence**: The theoretical foundation regarding output invariance under orthogonal transformations is sound, and empirical results showing improved perplexity and task performance over baselines are well-supported.

**Medium confidence**: The relative performance comparison between Kronecker product and GS matrix representations suggests Kronecker products yield slightly better accuracy, but computational efficiency trade-offs need more systematic evaluation across different hardware architectures.

**Medium confidence**: The claim of being "fine-tuning-free" is technically accurate but may oversimplify practical deployment considerations, as the orthogonal transformation optimization still requires significant computational resources and calibration data.

## Next Checks
1. Evaluate ProcrustesGPT's performance on non-autoregressive LLM tasks such as summarization and question answering to assess broader applicability
2. Test the method's sensitivity to calibration dataset size and quality to establish minimum requirements for effective compression
3. Conduct end-to-end inference latency measurements on different hardware platforms to quantify the practical benefits of GS matrix representations for acceleration