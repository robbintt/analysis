---
ver: rpa2
title: Analyzing Information Sharing and Coordination in Multi-Agent Planning
arxiv_id: '2508.12981'
source_url: https://arxiv.org/abs/2508.12981
tags:
- plan
- system
- agents
- notebook
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates multi-agent systems (MAS) for long-horizon
  planning tasks by using a travel planning benchmark. The authors propose two mechanisms
  to improve MAS performance: (1) a shared notebook that allows agents to record and
  access structured tool call results to avoid hallucinations, and (2) an orchestrator
  agent that dynamically directs the conversation and focus of different specialist
  agents.'
---

# Analyzing Information Sharing and Coordination in Multi-Agent Planning

## Quick Facts
- **arXiv ID:** 2508.12981
- **Source URL:** https://arxiv.org/abs/2508.12981
- **Authors:** Tianyue Ou; Saujas Vaduguru; Daniel Fried
- **Reference count:** 7
- **Primary result:** Shared notebook reduces hallucinations by 18% and orchestrator boosts hard constraint satisfaction by 13.5% in multi-agent travel planning.

## Executive Summary
This paper addresses the challenge of long-horizon planning in multi-agent systems (MAS) by proposing two mechanisms to improve reliability and coordination. A shared notebook allows agents to record and access structured tool call results, reducing hallucinations by grounding plan compilation in verified data. An orchestrator agent dynamically directs the conversation and focus of specialist agents, enabling iterative refinement and backtracking to resolve interdependent constraints. Evaluated on a travel planning benchmark, these mechanisms together improve the final pass rate from 7.5% to 25% (17.5% absolute gain) over single-agent baselines.

## Method Summary
The authors implement a multi-agent system using the AutoGen framework to solve long-horizon travel planning tasks on the TravelPlanner benchmark. The system comprises expert agents (transportation, hotel, restaurant, attraction) that call tools and write structured results to a shared notebook, an orchestrator that dynamically selects the next agent based on conversation state and self-reflection, and plan compiler/critic modules that synthesize final itineraries from the notebook and conversation. The notebook grounds plan compilation in tool outputs to avoid hallucinations, while the orchestrator enables non-linear problem solving through iterative revisits. The system is evaluated with both GPT-4o and Claude 4 Sonnet models.

## Key Results
- Shared notebook reduces hallucinated details by 18% and improves final pass rate by 3.75% over conversation alone.
- Orchestrator dynamically directs MAS to focus on and further reduce errors by up to 13.5% in focused sub-areas.
- Combined mechanisms boost final pass rate from 7.5% to 25% (17.5% absolute improvement) over single-agent baseline.

## Why This Works (Mechanism)

### Mechanism 1: Shared Notebook
- **Claim:** A shared notebook reduces hallucination rates by grounding final planning in retrieved tool outputs rather than conversational memory.
- **Mechanism:** Expert agents write structured tool results to a central notebook N. During plan compilation, the system conditions on this structured evidence alongside the goal, bypassing the need to recall specific details from a long dialogue history.
- **Core assumption:** LLMs are more reliable at synthesizing plans from explicitly provided structured context than from unstructured, multi-turn conversation history where details degrade.
- **Evidence anchors:** Abstract shows 18% reduction in hallucinated details; section 4 explains agents verify exact names/numbers via notebook to avoid hallucinations.

### Mechanism 2: Dynamic Orchestrator
- **Claim:** Dynamic routing by an orchestrator agent improves resolution of interdependent constraints compared to fixed workflows.
- **Mechanism:** An orchestrator agent D observes conversation state and dynamically selects next expert agent, allowing system to revisit earlier decisions rather than following linear path.
- **Core assumption:** Complex planning requires iterative refinement and backtracking that fixed sequential workflows cannot support.
- **Evidence anchors:** Abstract shows up to 13.5% error reduction; section 5 Table 3 shows 0.66 transportation and 0.63 hotel revisits per task correlating with error drops.

### Mechanism 3: Self-Reflection
- **Claim:** Self-reflection capabilities in orchestrator enable better prioritization of "hard" constraints over "commonsense" ones.
- **Mechanism:** Orchestrator reasons about current state before selecting next agent, creating feedback loop to identify specific failure modes and activate agents to fix them.
- **Core assumption:** LLM can accurately diagnose "weakest link" in partial plan given conversation history.
- **Evidence anchors:** Section 5 shows self-reflection variants outperform fixed order in Hard Macro Pass Rate (25% vs 13.75% for Claude).

## Foundational Learning

- **Concept:** Context Window Degradation
  - **Why needed here:** Paper addresses long-horizon tasks where details from early turns are lost or hallucinated in later turns.
  - **Quick check question:** Can you explain why increasing dialogue turns typically degrades LLM's ability to recall specific numeric details without external memory?

- **Concept:** Constraint Satisfaction / Backtracking
  - **Why needed here:** Orchestrator's core improvement is enabling non-linear problem solving (revisiting previous steps), standard in CSP but novel in linear LLM flows.
  - **Quick check question:** How does "fixed workflow" architecture fundamentally fail when facing mutually exclusive constraints (e.g., budget vs. dining preference)?

- **Concept:** Tool Use Grounding
  - **Why needed here:** Notebook mechanism relies on distinction between "raw tool output" (truth) and "agent summary" (potential hallucination).
  - **Quick check question:** Why must final planner reference raw tool output rather than agent's textual summary of that output to ensure validity?

## Architecture Onboarding

- **Component map:** Orchestrator D -> Experts Ek -> Notebook N -> Plan Compiler PC/Critic PR
- **Critical path:**
  1. Orchestrator analyzes goal
  2. Orchestrator activates Expert (e.g., Hotel)
  3. Expert calls tool -> Writes to Notebook -> Messages summary
  4. Orchestrator reviews summary -> Activates next Expert (or loops back)
  5. Planner reads Notebook (skipping chat history for details) -> Generates plan

- **Design tradeoffs:**
  - Quality vs. Delivery Rate: Orchestrator system trades ~7-10% drop in Delivery Rate for ~10-15% gain in Hard Macro Pass Rate.
  - Complexity vs. Hallucination: Notebook adds I/O overhead but is primary block for 18% reduction in hallucinated details.

- **Failure signatures:**
  - "Context amnesia": Agents reference non-existent entities. Fix: Check Notebook integration.
  - "Planning loops": System revisits same expert >3 times without progress. Fix: Tune Orchestrator reflection prompts or max-turn limit.
  - "Silent tools": Experts message c but fail to write to N. Fix: Enforce strict tool-to-notebook binding.

- **First 3 experiments:**
  1. Baseline Reproduction: Run single agent on TravelPlanner benchmark to verify 7.5% pass rate and identify primary hallucination vectors.
  2. Ablation on Memory: Run Expert system without Notebook (chat-only). Measure specific delta in entity hallucination (expect ~18% increase).
  3. Stress Test on Constraints: Introduce contradictory constraints and verify if Orchestrator successfully initiates "revisit" loop rather than failing silently.

## Open Questions the Paper Calls Out
- How can multi-agent systems integrate explicit conflict detection mechanisms to better resolve coupled constraints that iterative conversation currently fails to address?
- Can dynamic step limits or more efficient coordination strategies mitigate the drop in delivery rates caused by the overhead of flexible multi-agent orchestration?
- Would granting the orchestrator access to the shared notebook (ground truth data) improve its ability to direct agents compared to relying solely on conversation history?

## Limitations
- Step limit ambiguity: The paper references a "step limit" causing delivery rate drops but does not specify the exact threshold.
- Self-reflection prompt specificity: The exact implementation details of the orchestrator's self-reflection mechanism are not provided.
- Limited domain generalization: The evaluation is confined to travel planning and may not generalize to domains with different coordination patterns.

## Confidence
- Notebook Reduces Hallucinations (18%): Medium confidence
- Orchestrator Improves Constraint Satisfaction (13.5% gain): Medium confidence
- Structured Information Sharing + Adaptive Coordination is Effective: High confidence

## Next Checks
1. Ablation of Self-Reflection: Run orchestrator system with and without self-reflection capability to isolate its contribution to constraint satisfaction gains.
2. Cross-Domain Transfer: Apply notebook and orchestrator architecture to non-travel planning task (e.g., software development) to assess generalization.
3. Stress Test with Contradictory Constraints: Systematically introduce conflicting constraints and measure if orchestrator successfully initiates revisits and backtracking.