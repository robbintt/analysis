---
ver: rpa2
title: 'LeanConjecturer: Automatic Generation of Mathematical Conjectures for Theorem
  Proving'
arxiv_id: '2506.22005'
source_url: https://arxiv.org/abs/2506.22005
tags:
- closure
- interior
- theorem
- subset
- conjectures
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents LeanConjecturer, a pipeline for automatically
  generating university-level mathematical conjectures in Lean 4 using Large Language
  Models (LLMs). The system addresses data scarcity in formal theorem proving by combining
  rule-based context extraction with LLM-based theorem statement generation.
---

# LeanConjecturer: Automatic Generation of Mathematical Conjectures for Theorem Proving

## Quick Facts
- arXiv ID: 2506.22005
- Source URL: https://arxiv.org/abs/2506.22005
- Reference count: 10
- Key outcome: Generated 12,289 conjectures from 40 Mathlib seed files, with 3,776 identified as non-trivial (not provable by aesop), validated through reinforcement learning showing improved theorem proving capabilities in topology.

## Executive Summary
LeanConjecturer addresses the data scarcity problem in formal theorem proving by automatically generating university-level mathematical conjectures in Lean 4. The system combines rule-based context extraction with LLM-based theorem statement generation to produce syntactically valid, novel, and potentially non-trivial conjectures. Through iterative generation and evaluation, the pipeline generates thousands of conjectures that serve as training data for reinforcement learning, demonstrating improved proof capabilities for topology-related problems including semi-open, alpha-open, and pre-open sets.

## Method Summary
The method employs a hybrid pipeline: rule-based extraction of imports, global variables, and namespaces from Mathlib seed files, followed by LLM (OpenAI o3) generation of theorem statements constrained to Lean syntax. Generated conjectures undergo three-stage evaluation: syntactic validity via `sorry` replacement, novelty checking via `exact?` against Mathlib and previously generated conjectures, and non-triviality assessment via `aesop` tactic failure. Valid novel conjectures are collected and fed back as context for iterative generation (up to 15 iterations). The resulting conjectures train DeepSeek Prover V2 via Group Relative Policy Optimization (GRPO) with binary rewards, improving proof success rates on topology problems.

## Key Results
- Produced 12,289 conjectures from 40 Mathlib seed files with average 103.25 novel conjectures per file
- Identified 3,776 syntactically valid, non-trivial conjectures (cannot be proven by aesop tactic)
- GRPO training improved proof success rates from 2285 to 2306/24576 on topology problems
- Verified several non-trivial theorems related to semi-open, alpha-open, and pre-open sets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid rule-based context extraction combined with LLM statement generation reduces syntactic errors and hallucinated imports.
- Mechanism: Rule-based methods extract imports, global variable declarations, and namespace opens from seed files. The LLM generates only theorem statements (starting with `theorem`, ending with `:= by`), preventing invention of non-existent libraries. Post-processing strips modifiers and annotations. This leverages LLM pattern variation while using deterministic code for structural correctness.
- Core assumption: LLM can produce meaningful variations when relieved of context-management burden, and rule-based extraction captures sufficient context.
- Evidence anchors: [abstract] hybrid approach combines rule-based context extraction with LLM-based theorem statement generation; [section 3.1] LLM generates only theorem statements, contextual information extracted separately using rule-based methods.

### Mechanism 2
- Claim: Iterative generation with context-aware novelty checking prevents duplicates and enables exploration of related mathematical spaces.
- Mechanism: Valid novel conjectures are collected into a new file that becomes inspiration for the next iteration. The `exact?` command checks novelty against both Mathlib and previously generated conjectures in context. Iteration terminates when no new novel conjectures emerge or max iterations reached.
- Core assumption: LLM can explore variations of variations without mode collapse, and `exact?` provides reliable novelty detection.
- Evidence anchors: [abstract] Through iterative generation and evaluation, LeanConjecturer produced 12,289 conjectures; [section 3.3] novelty assessed not only against Mathlib but also against previously generated conjectures; [section 4.2] 25 out of 40 files reached maximum iteration limit of 15.

### Mechanism 3
- Claim: Non-triviality filtering via automated tactics (aesop) identifies conjectures that provide genuine learning value for theorem provers.
- Mechanism: Conjectures are tested against `aesop` tactic. If aesop succeeds, the conjecture is considered trivial (mechanically provable). Failure suggests the conjecture may require non-trivial proof strategies, making it valuable for training. Both trivial and non-trivial conjectures are retained for GRPO training.
- Core assumption: aesop failure correlates with genuine mathematical difficulty rather than simply being outside aesop's tactic coverage.
- Evidence anchors: [abstract] 3,776 identified as syntactically valid and non-trivial, that is, cannot be proven by aesop tactic; [section 3.2] If aesop fails, the conjecture is considered potentially non-trivial.

## Foundational Learning

- Concept: **Lean 4 syntax and type theory basics**
  - Why needed here: The entire pipeline depends on generating Lean 4 theorem statements. Understanding `theorem`, `:= by`, `sorry`, declarations, and type dependencies is essential for debugging generation failures and evaluating syntactic validity.
  - Quick check question: Can you explain why replacing a proof with `sorry` and checking for "declaration uses 'sorry'" warning is a valid syntax test?

- Concept: **Automated theorem proving tactics (aesop, exact?)**
  - Why needed here: These tactics form the evaluation backbone—syntactic validity via `sorry`, novelty via `exact?`, non-triviality via `aesop`. Understanding their capabilities and limitations is critical for interpreting results.
  - Quick check question: What does it mean when `exact?` succeeds versus when `aesop` succeeds, and why might a true conjecture fail both?

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: The paper uses GRPO for reinforcement learning with generated conjectures as training data. Understanding reward signals, policy updates, and overfitting risks helps interpret the modest problem-rate improvements (47→50/192).
  - Quick check question: Why might proof rate improve (2285→5307/24576) while problem rate barely changes (47→50/192), and what does this suggest about generalization?

## Architecture Onboarding

- Component map: Seed files -> Context Extractor -> LLM Generator -> Post-processor -> Evaluator (Syntax check -> Novelty check -> aesop test) -> Collection -> Iterator -> GRPO Trainer

- Critical path: Seed file → Context extraction → LLM generation → Post-processing → Syntax check → Novelty check → (if novel) aesop test → Collection → Iteration → GRPO training. The novelty check against previous iterations is the key feedback mechanism.

- Design tradeoffs:
  - "As many as possible" prompt ensures volume but may reduce per-conjecture quality; without it, single-theorem files produce few conjectures
  - Retaining aesop-provable conjectures for GRPO maximizes training data but dilutes challenge difficulty
  - Using o3 for generation, DeepSeek for proving separates concerns but requires two models
  - 15-iteration cap: 25/40 files hit this limit, suggesting under-exploration vs. computational budget

- Failure signatures:
  - Hallucinated imports/lemmas: Mitigated by rule-based extraction; if LLM generates undefined terms, syntax check fails
  - Mode collapse/repetition: Detected by novelty check against previous iterations; `exact?` with context catches duplicates
  - False conjectures: Not detected until proving stage; aesop failure doesn't indicate falsity, just difficulty
  - Redundant proof steps: Observed in verified conjectures ("proofs continued with additional steps even after the theorem was already proven")

- First 3 experiments:
  1. **Single-file trace**: Run pipeline on `Mathlib.Algebra.Group.Commutator` (1 theorem) with and without "as many as possible" prompt; compare conjecture counts, syntax validity rates, and iteration behavior. Target: replicate Table 2 results locally.
  2. **Novelty check ablation**: Disable the context-based novelty check (only check against Mathlib, not previous iterations); measure duplicate rate across 5 iterations on a topology file. Target: quantify mode-collapse effect.
  3. **GRPO minimal repro**: Train DeepSeek Prover V2 7B for 1 epoch on 192 topology conjectures using provided repo (https://github.com/auto-res/open-r1.git); compare baseline vs. post-training proof rate on held-out semi-open set conjectures. Target: verify 2285→2306/24576 improvement is reproducible.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can reinforcement learning strategies be adapted to improve generalization to novel mathematical concepts beyond the training distribution?
- Basis in paper: Section 4.3 shows GRPO training improved proof success rates for interior/closure problems but did not significantly improve performance on alpha-open set conjectures.
- Why unresolved: Current training optimization appears to specialize the model on specific problem types rather than enhancing general reasoning capabilities.
- What evidence would resolve it: Demonstrating that models trained on specific topological properties show statistically significant improvements on structurally distinct, unseen mathematical domains.

### Open Question 2
- Question: Can integrating LeanConjecturer with self-play systems (e.g., STP) create a more robust hybrid learning environment?
- Basis in paper: Section 5.3 states that "The potential for combining our conjecture generation approach with self-play systems such as STP represents a direction for future research."
- Why unresolved: The current pipeline separates generation and proving, whereas self-play requires a dynamic feedback loop between the conjecturer and prover.
- What evidence would resolve it: Implementation of a hybrid system that leverages LeanConjecturer's generation within a self-play loop, showing improved data efficiency or discovery rates.

### Open Question 3
- Question: How can the conjecture generation process be refined to produce deeper mathematical insights rather than simple variations?
- Basis in paper: Section 5.3 notes that future work should focus on "refining the conjecture generation process to produce deeper and more insightful mathematical statements."
- Why unresolved: Current LLM-based generation relies on existing seed files, which often results in valid but shallow variations of known theorems.
- What evidence would resolve it: Development of guiding techniques or heuristics that result in generated theorems rated as "insightful" or "non-trivial" by mathematical experts.

## Limitations
- Reliance on automated evaluation tools (aesop, exact?) whose coverage and limitations are not fully characterized, potentially conflating genuine difficulty with tactic coverage gaps
- GRPO training showed only marginal improvement (47→50/192 problem rate) and did not generalize well to structurally distinct conjecture types
- "Proofs continued with additional steps even after the theorem was already proven" suggests potential overfitting to generation patterns rather than genuine reasoning

## Confidence
- **High confidence**: Hybrid context extraction + LLM generation mechanism is clearly specified and directly reproducible; three-stage evaluation pipeline (syntax → novelty → non-triviality) is well-defined and verifiable
- **Medium confidence**: 103.25 average conjectures per seed file and 3,776 non-trivial count depend on undocumented o3 sampling settings and seed file selection; 2285→2306/24576 proof rate improvement is marginal and may not generalize
- **Low confidence**: "Mathematical discovery" claim is supported by verified conjectures but lacks systematic comparison to existing theorems; overfitting concerns from redundant proof steps

## Next Checks
1. **Single-file trace reproducibility**: Run the pipeline on `Mathlib.Algebra.Group.Commutator` (1 theorem) with and without "as many as possible" prompt; compare conjecture counts, syntax validity rates, and iteration behavior to replicate Table 2 results.
2. **Novelty check ablation**: Disable the context-based novelty check (only check against Mathlib, not previous iterations); measure duplicate rate across 5 iterations on a topology file to quantify mode-collapse effect.
3. **GRPO minimal repro**: Train DeepSeek Prover V2 7B for 1 epoch on 192 topology conjectures using the provided repo (https://github.com/auto-res/open-r1.git); compare baseline vs. post-training proof rate on held-out semi-open set conjectures to verify the 2285→2306/24576 improvement is reproducible.