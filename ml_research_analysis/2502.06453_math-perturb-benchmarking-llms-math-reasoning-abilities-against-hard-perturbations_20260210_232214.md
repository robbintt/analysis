---
ver: rpa2
title: 'MATH-Perturb: Benchmarking LLMs'' Math Reasoning Abilities against Hard Perturbations'
arxiv_id: '2502.06453'
source_url: https://arxiv.org/abs/2502.06453
tags:
- original
- problems
- problem
- arxiv
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper constructs MATH-P-Simple and MATH-P-Hard, benchmarks
  for assessing large language models' math reasoning under simple and hard perturbations.
  MATH-P-Hard modifies problems so original solution paths no longer apply, while
  keeping minimal lexical changes.
---

# MATH-Perturb: Benchmarking LLMs' Math Reasoning Abilities against Hard Perturbations

## Quick Facts
- arXiv ID: 2502.06453
- Source URL: https://arxiv.org/abs/2502.06453
- Authors: Kaixuan Huang, Jiacheng Guo, Zihao Li, Xiang Ji, Jiawei Ge, Wenzhe Li, Yingqing Guo, Tianle Cai, Hui Yuan, Runzhe Wang, Yue Wu, Ming Yin, Shange Tang, Yangsibo Huang, Chi Jin, Xinyun Chen, Chiyuan Zhang, Mengdi Wang
- Reference count: 40
- Key outcome: MATH-P-Hard perturbations cause 10%-25% performance drops across 18 models, exposing models' over-reliance on memorized problem-solving patterns

## Executive Summary
MATH-Perturb introduces two new benchmarks - MATH-P-Simple and MATH-P-Hard - to evaluate large language models' math reasoning abilities when problems are modified with minimal lexical changes but significant reasoning alterations. MATH-P-Hard problems require different solution paths than their originals while maintaining high textual similarity. Across 18 models, performance drops of 10%-25% on MATH-Hard demonstrate that models often apply memorized solution patterns without verifying their applicability to modified contexts. The study reveals that in-context learning with original problems can actually mislead models, worsening performance on hard perturbations.

## Method Summary
The paper constructs MATH-P-Simple and MATH-P-Hard by perturbing 279 MATH Level-5 problems with minimal edits while requiring modified answers. Simple perturbations maintain the same solution method but with changed parameters; hard perturbations require fundamentally different reasoning approaches. A team of 12 PhD-level annotators created perturbations with constraints on edit distance and semantic similarity. Evaluation uses zero-shot chain-of-thought prompting without tools, validated by SymPy equivalence checking. Error analysis categorizes failures into memorization behaviors versus capability limitations.

## Key Results
- MATH-P-Hard causes 10%-25% performance drops across 18 evaluated models
- Manual inspection shows memorization causes 40% of o1-mini errors and 25% of Claude-3.5-Sonnet errors on hard perturbations
- In-context learning with original problems often misleads large models, causing correct→wrong transitions in 18-40% of cases
- Models exhibit "habitual reasoning" - applying memorized solution patterns without assessing applicability to modified contexts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exhibit "habitual reasoning" - applying memorized solution patterns without verifying applicability to modified problem contexts
- Mechanism: During training, models learn associations between problem surface features and solution strategies. When test problems share these surface features but require different reasoning paths, the learned association triggers automatically. This bypasses any verification step that would assess whether the memorized strategy still applies.
- Core assumption: Models optimize for pattern completion rather than first-principles derivation; training on specific problem distributions creates exploitable shortcuts
- Evidence anchors: [abstract] "models blindly apply learned problem-solving skills without assessing their applicability to modified contexts"; [section 3.2] "models may ignore the modified assumptions and presume that the original assumptions still hold"; manual inspection estimates memorization causes 40% of o1-mini errors, 25% of Claude-3.5-Sonnet errors

### Mechanism 2
- Claim: Minimal lexical perturbations with maximal reasoning changes create a "reasoning gap" that exposes shallow pattern-matching vs. deep comprehension
- Mechanism: Hard perturbations maintain low edit distance (normalized ~0.2-0.4) and high semantic similarity (cosine ~0.8-0.9) while altering essential mathematical structure. Models relying on surface similarity retrieve and apply irrelevant solution templates. The disconnect between lexical proximity and structural divergence is the failure trigger.
- Core assumption: Model attention and retrieval mechanisms weight surface textual similarity over structural/mathematical properties
- Evidence anchors: [section 2] "minimal modifications... the modified problems stay close to the original problems in the text form"; [figure 4] Normalized edit distance distributions peak around 0.2-0.4 for both Simple and Hard; cosine similarities cluster at 0.7-0.9

### Mechanism 3
- Claim: In-context learning with original examples creates competing effects: helpful knowledge transfer vs. misleading pattern priming
- Mechanism: ICL provides two signals: (1) relevant mathematical knowledge that could help solve the modified problem; (2) solution template that may not apply. For MATH-P-Hard, the misleading effect partially cancels the ICL benefit. Models primed with original solutions are more likely to force-fit those approaches rather than derive new ones.
- Core assumption: ICL demonstrations bias the model's solution space toward the demonstrated approach, reducing exploration of alternative strategies
- Evidence anchors: [section 3.4] "n(correct→wrong)" rates of 18-40% for large models; ICL helps some problems but hurts others, leaving "only marginal improvements (less than 5%)"; [table 7] Claude-3.5-Sonnet: 27 correct→wrong vs. 57 wrong→correct on MATH-P-Hard with ICL

## Foundational Learning

- **Concept: Distribution Shift vs. Out-of-Distribution Generalization**
  - Why needed here: The paper evaluates whether models trained on one problem distribution can generalize to "hard perturbations" that require different reasoning. Understanding distribution shift clarifies why performance drops 10-25% despite models scoring >90% on original problems.
  - Quick check question: If a model achieves 95% accuracy on training-distribution problems but 70% on perturbed variants with minimal lexical changes, what does this suggest about memorization vs. reasoning?

- **Concept: Counterfactual Testing for Memorization Detection**
  - Why needed here: MATH-Perturb uses counterfactual perturbations (modifying problems while keeping surface similarity) to detect whether models rely on memorized solutions. This is a methodological foundation for the entire benchmark.
  - Quick check question: Why is it insufficient to measure accuracy alone when evaluating reasoning? What additional signal do counterfactual perturbations provide?

- **Concept: Chain-of-Thought as Evaluation Protocol**
  - Why needed here: The paper adopts zero-shot CoT as the standard evaluation method. CoT exposes intermediate reasoning steps, making memorization behaviors (like applying irrelevant solution templates) more detectable in model outputs.
  - Quick check question: How might CoT responses reveal memorization that would be hidden in direct answer-only evaluation?

## Architecture Onboarding

- **Component map:** Original MATH Level-5 Problems (279) → [Perturbation Module] → Simple Perturbation → MATH-P-Simple (279) and Hard Perturbation → MATH-P-Hard (279) → [Evaluation Pipeline] → Zero-shot CoT prompting → SymPy equivalence checker → Error categorization → [Analysis Layer] → Performance comparison → ICL ablation → Failure mode annotation

- **Critical path:**
  1. **Problem selection:** Level-5 MATH problems only (hardest difficulty; modern models already solve easier problems)
  2. **Perturbation design:** Annotators apply "minimal edits" with "changed answers" constraint; Hard perturbations require different solution methods
  3. **Quality control:** 12 PhD-level annotators; cross-validation; manual verification against o1-mini outputs
  4. **Evaluation:** Zero-shot CoT (no tools/code interpreter); SymPy equivalence checking
  5. **Analysis:** Categorize responses into 4 cases (both correct, both wrong, easy-only, hard-only); quantify memorization vs. capability errors

- **Design tradeoffs:**
  - **Dataset size (279) vs. annotation depth:** Smaller than full MATH but rigorously annotated; trades breadth for controlled perturbation quality
  - **Minimal edits vs. answer change requirement:** Constrains perturbation space; some valid hard perturbations may be excluded
  - **Zero-shot vs. few-shot evaluation:** Zero-shot better isolates memorization from training; few-shot would confound with ICL effects
  - **Tool use disabled:** Prevents brute-force solutions that would bypass reasoning evaluation

- **Failure signatures:**
  1. **Solution-path copying:** Model applies original solution steps verbatim to perturbed problem (Figure 1: simplifying cancellation when denominator changed from x+1 to x+0)
  2. **Assumption ignoring:** Model reasons as if original constraints still hold (Figure 5: ignoring "specific order" requirement in probability problem)
  3. **Answer hallucination:** Model outputs original problem's answer instead of modified answer (Figure 6: outputting all values instead of minimum value)
  4. **ICL misleading:** Performance degrades when original problem shown as demonstration (Table 7: correct→wrong transitions)

- **First 3 experiments:**
  1. **Baseline triad evaluation:** Run target model on Original, MATH-P-Simple, MATH-P-Hard with zero-shot CoT. Compute per-category accuracy drops. Expected pattern: Original ≈ Simple > Hard (10-25% gap indicates memorization vulnerability).
  2. **ICL ablation study:** Evaluate MATH-P-Hard with (a) no demonstration, (b) original problem demonstration, (c) random problem demonstration. Quantify n(wrong→correct) vs. n(correct→wrong) transitions. Large models should show 20-40% misleading effect.
  3. **Failure mode annotation:** On problems where model solves Original/Simple but fails Hard, manually classify errors as: (a) memorization (blindly applies original approach), (b) capability limitation (insufficient math knowledge), (c) computation error. Expect memorization fraction to scale with model capability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can models be trained to detect when previously learned solution patterns are no longer applicable to a modified problem?
- Basis in paper: [explicit] Authors identify a "new form of memorization where models blindly apply learned problem-solving skills without assessing their applicability to modified contexts" and explicitly "call for research efforts to address this challenge."
- Why unresolved: Current models show 10-25% performance drops on MATH-P-Hard, and even advanced models like o1-mini exhibit this memorization behavior in 40% of errors.
- What evidence would resolve it: Training interventions that improve performance specifically on hard perturbations without sacrificing original problem accuracy, coupled with analysis showing models explicitly assess applicability before applying solution methods.

### Open Question 2
- Question: Does training on systematically perturbed mathematical problems improve generalization to hard perturbations, or does it simply create new memorization patterns?
- Basis in paper: [inferred] The paper hypothesizes that "any naive fine-tuning technique with a limited distribution of problem settings will hurt the generalization of the language models against hard perturbations" based on ICL results.
- Why unresolved: While ICL with original problems shows misleading effects, the paper does not test whether training on diverse perturbations could build more robust reasoning.
- What evidence would resolve it: Experiments fine-tuning models on systematically varied perturbation types (simple and hard), then evaluating on held-out MATH-P-Hard problems to measure generalization versus overfitting.

### Open Question 3
- Question: Do longer reasoning chains at inference time help models recognize when to abandon memorized solution patterns?
- Basis in paper: [inferred] Appendix C.5 shows inference-time scaling improves pass rates, but the paper does not analyze whether additional compute specifically helps with pattern-recognition failures on MATH-P-Hard.
- Why unresolved: The scaling experiments are preliminary and do not distinguish between computational errors versus memorization-induced errors.
- What evidence would resolve it: Analysis categorizing whether pass@k improvements on MATH-P-Hard come from correcting memorization behaviors or from fixing computational mistakes.

### Open Question 4
- Question: What retrieval-augmented approaches could provide relevant but non-misleading context for perturbed problems?
- Basis in paper: [explicit] The paper shows MRR of 0.986-0.995 for retrieving original problems, and notes that original problems in context can mislead models on hard perturbations.
- Why unresolved: High semantic similarity between original and perturbed problems means standard retrieval will surface misleading examples, but the paper does not explore alternative retrieval strategies.
- What evidence would resolve it: Development of retrieval methods that identify when a retrieved example applies to the query problem versus when it could be misleading, with evaluation on MATH-P-Hard.

## Limitations
- Manual perturbation process introduces potential bias in what types of reasoning changes are selected
- Performance drop attribution (40% memorization for o1-mini vs. 25% for Claude-3.5-Sonnet) relies on subjective manual inspection without systematic validation
- Zero-shot CoT evaluation may not fully capture reasoning capabilities that could emerge with different prompting strategies or tool use

## Confidence
- **High confidence:** The existence of 10-25% performance drops on MATH-P-Hard (measured performance difference is robust)
- **Medium confidence:** The mechanism of "habitual reasoning" (inferred from error patterns but not directly measured)
- **Medium confidence:** The quantitative attribution of errors to memorization vs. capability (manual annotation introduces subjectivity)

## Next Checks
1. **Automated perturbation analysis:** Develop a computational metric to quantify the reasoning gap between original and perturbed problems, validating that Hard perturbations consistently require different solution methods beyond surface-level changes.

2. **Cross-model error consistency:** Run the same perturbation pipeline on a fixed set of problems across all 18 models and analyze whether failure patterns (solution-path copying, assumption ignoring) are consistent across architectures or model-specific.

3. **ICL intervention study:** Test whether explicit disclaimers ("this is a modified version of the original problem") in ICL demonstrations can mitigate the correct→wrong transition rate observed in large models.