---
ver: rpa2
title: 'COMET-SG1: Lightweight Autoregressive Regressor for Edge and Embedded AI'
arxiv_id: '2601.20772'
source_url: https://arxiv.org/abs/2601.20772
tags:
- comet-sg1
- memory
- behavior
- autoregressive
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: COMET-SG1 is a lightweight autoregressive regressor designed for
  edge AI, prioritizing long-horizon stability over short-horizon accuracy. It encodes
  temporal behavior using linear projections on multiple time scales and advances
  predictions through memory-anchored transitions, avoiding recurrence and attention.
---

# COMET-SG1: Lightweight Autoregressive Regressor for Edge and Embedded AI

## Quick Facts
- **arXiv ID:** 2601.20772
- **Source URL:** https://arxiv.org/abs/2601.20772
- **Reference count:** 16
- **Key outcome:** A lightweight autoregressive regressor prioritizing long-horizon stability over short-horizon accuracy, achieving 1-step MAE of 0.0061 with ~2.9 KB parameters on synthetic time-series.

## Executive Summary
COMET-SG1 is a lightweight autoregressive regressor designed for edge AI, prioritizing long-horizon stability over short-horizon accuracy. It encodes temporal behavior using linear projections on multiple time scales and advances predictions through memory-anchored transitions, avoiding recurrence and attention. Experiments on non-stationary synthetic time-series show that while feedforward and recurrent baselines suffer from drift or instability during long-horizon rollout, COMET-SG1 maintains bounded deviation across multiple random seeds. The model achieves competitive short-horizon accuracy (e.g., 1-step MAE of 0.0061) with a compact footprint (~2.9 KB parameters), making it suitable for deployment on microcontroller-class hardware.

## Method Summary
COMET-SG1 uses a memory-anchored autoregressive architecture that encodes temporal patterns through linear projections across multiple time scales. Unlike recurrent or attention-based models, it advances predictions by referencing a fixed memory state, which helps maintain stability during long-horizon rollout. The design is explicitly optimized for edge deployment, trading some short-horizon accuracy for robustness to drift in non-stationary time-series. The model is trained and evaluated on synthetic non-stationary datasets, with performance measured in terms of mean absolute error (MAE) for short horizons and deviation bounds for long-horizon predictions.

## Key Results
- 1-step MAE of 0.0061 on synthetic non-stationary time-series
- Maintains bounded deviation during long-horizon rollout, unlike feedforward and recurrent baselines
- Model size of approximately 2.9 KB, suitable for microcontroller-class hardware

## Why This Works (Mechanism)
COMET-SG1 maintains stability in long-horizon predictions by anchoring its autoregressive updates to a fixed memory state, rather than relying on recurrence or attention. This memory-anchored transition avoids the compounding errors typical of standard autoregressive models, especially in non-stationary environments. The use of linear projections across multiple time scales allows the model to capture both short- and long-term temporal dependencies without the computational overhead of recurrence or attention mechanisms.

## Foundational Learning
- **Autoregressive modeling**: Predicting future values based on previous predictions; needed to generate long sequences step-by-step; quick check: can the model generate predictions indefinitely given an initial state?
- **Non-stationary time-series**: Data whose statistical properties change over time; needed because real-world sensor data often exhibits such behavior; quick check: does the model maintain performance as data distribution shifts?
- **Memory-anchored transitions**: Using a fixed memory state to stabilize autoregressive updates; needed to prevent drift in long-horizon predictions; quick check: does the model's deviation remain bounded as horizon increases?
- **Linear projections on multiple time scales**: Encoding temporal dependencies at different granularities; needed to capture both short- and long-term patterns efficiently; quick check: can the model adapt to both fast and slow changes in the data?
- **Edge deployment constraints**: Limited memory and compute on microcontrollers; needed to justify the model's compact size and efficiency focus; quick check: does the model fit within the memory budget of typical edge devices?
- **Deviation bounds**: Quantitative measure of prediction stability over long horizons; needed to compare robustness across models; quick check: is the model's deviation significantly lower than baselines at long horizons?

## Architecture Onboarding
- **Component map:** Input time-series → Linear projections (multiple time scales) → Memory-anchored transition → Autoregressive output
- **Critical path:** The memory-anchored transition is the core innovation, as it stabilizes long-horizon predictions by avoiding recurrence and attention.
- **Design tradeoffs:** Prioritizes long-horizon stability over short-horizon accuracy; sacrifices some precision for robustness and compactness.
- **Failure signatures:** Drift and instability in long-horizon rollout for standard autoregressive models; overfitting to short-term patterns without memory anchoring.
- **3 first experiments:**
  1. Compare short-horizon MAE on synthetic non-stationary data against a lightweight LSTM baseline.
  2. Measure long-horizon prediction deviation across multiple random seeds.
  3. Evaluate model size and inference speed on a representative microcontroller.

## Open Questions the Paper Calls Out
None

## Limitations
- Performance gains demonstrated only on synthetic non-stationary time-series; real-world sensor data generalization unverified.
- No empirical memory, latency, or energy measurements provided for edge deployment claims.
- No ablation comparing memory-anchored transitions to lightweight recurrence mechanisms.

## Confidence
- **Short-horizon accuracy claims (High):** MAE values are precise and reproducible on synthetic data.
- **Long-horizon stability claims (Medium):** Demonstrated on synthetic data; real-world validation needed.
- **Edge deployment feasibility (Low):** No empirical latency, memory, or energy measurements provided.
- **Architectural novelty (Medium):** Clear description of linear-projection encoding, but lack of comparison to lightweight recurrent alternatives limits novelty claims.

## Next Checks
1. Evaluate COMET-SG1 on at least two real-world edge sensor datasets (e.g., IMU gesture recognition, ECG anomaly detection) and compare long-horizon rollout stability against a lightweight LSTM baseline.
2. Measure actual memory footprint, inference latency, and energy consumption on a representative microcontroller (e.g., ARM Cortex-M4) under realistic prediction workloads.
3. Perform ablation studies removing the memory-anchored transition mechanism to quantify its contribution to long-horizon stability versus standard autoregressive decoding.