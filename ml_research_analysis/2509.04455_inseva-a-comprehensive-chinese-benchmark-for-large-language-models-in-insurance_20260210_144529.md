---
ver: rpa2
title: 'INSEva: A Comprehensive Chinese Benchmark for Large Language Models in Insurance'
arxiv_id: '2509.04455'
source_url: https://arxiv.org/abs/2509.04455
tags:
- insurance
- knowledge
- text
- areas
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: INSEva is a comprehensive Chinese benchmark for evaluating large
  language models in the insurance domain, addressing the gap in domain-specific AI
  evaluation. It features 38,704 high-quality evaluation examples from authoritative
  sources across eight business areas, four task formats, and multiple difficulty
  levels.
---

# INSEva: A Comprehensive Chinese Benchmark for Large Language Models in Insurance

## Quick Facts
- arXiv ID: 2509.04455
- Source URL: https://arxiv.org/abs/2509.04455
- Reference count: 40
- Primary result: Chinese benchmark with 38,704 insurance domain examples revealing model weaknesses in logical reasoning and safety compliance despite >80 average scores

## Executive Summary
INSEva addresses the critical gap in domain-specific evaluation of large language models for insurance applications. The benchmark features 38,704 high-quality evaluation examples sourced from authoritative Chinese materials across eight business areas, four task formats, and multiple difficulty levels. Through comprehensive evaluation of eight state-of-the-art LLMs, the study reveals that while models demonstrate foundational insurance competency with average scores above 80, significant gaps persist in complex reasoning tasks. Notably, models struggle with insurance logical reasoning (ILR) and safety compliance (ISC), and exhibit a trade-off between faithfulness and completeness in open-ended responses.

## Method Summary
The benchmark employs a multi-stage data pipeline: authoritative sources (certification exams, regulatory standards, business data) undergo augmentation (paraphrasing, colloquial rewrites, option randomization) and rigorous quality control (rule-based validation, expert review with Cohen's Kappa = 0.87, multi-LLM voting). Evaluation uses accuracy for deterministic questions (MC/TF) and an LLM-based framework for open-ended responses, decomposing outputs into statements to assess faithfulness (supported by context) and completeness (matches ground truth). The 38,704 examples are stratified across eight business areas, four task formats, two difficulty levels, and cognitive/knowledge dimensions.

## Key Results
- State-of-the-art LLMs achieve average scores above 80, indicating foundational insurance competency
- Models perform poorly on insurance logical reasoning (ILR: 56.68-73.66) and safety compliance (ISC: 70.19-80.45)
- Domain-specific financial models underperform general-purpose LLMs, suggesting financial training doesn't transfer to insurance
- Reasoning models show trade-off: high completeness but lower faithfulness versus non-reasoning models

## Why This Works (Mechanism)

### Mechanism 1
Multi-dimensional taxonomy enables fine-grained capability differentiation across insurance-specific dimensions. Four-axis categorization (business areas × task formats × difficulty × cognition/knowledge) creates a structured pressure test that isolates where models fail—e.g., general LLMs scored >80 on average but dropped to ~70.9 on Insurance Logical Reasoning (ILR) and ~73 on Insurance Safety Compliance (ISC), revealing specific weaknesses not visible in aggregate scores. Core assumption: The taxonomy dimensions are orthogonal and cover the skill space relevant to real insurance work.

### Mechanism 2
Dual evaluation metrics (faithfulness + completeness) reveal a trade-off in open-ended generation that accuracy alone would obscure. Decompose model responses and ground truth into statements; use an LLM-based judge to score supported statements (faithfulness) and recalled ground-truth statements (completeness). Reasoning models (e.g., DeepSeek-R1) achieve higher completeness but lower faithfulness; non-reasoning models show the inverse. Core assumption: The LLM-based judge approximates human expert assessment reliably.

### Mechanism 3
Data sourced from authoritative professional materials ensures domain authenticity and exposes gaps that synthetic or web-scraped data would miss. Collect from certification exams, regulatory standards, and internal business resources; apply augmentation (paraphrasing, colloquial rewrites) for diversity; enforce quality via rule-based, expert-based, and multi-LLM voting modules. Core assumption: Augmentation preserves difficulty and domain specificity.

## Foundational Learning

- Concept: Insurance domain specificity vs. finance
  - Why needed here: Insurance focuses on risk assessment, pricing, and claims, not capital appreciation; using finance benchmarks would miss critical reasoning patterns (e.g., clause constraints, liability analysis).
  - Quick check question: Can you articulate three key differences between insurance and finance tasks that would affect LLM evaluation?

- Concept: Cognitive vs. knowledge dimensions (Bloom's taxonomy adapted)
  - Why needed here: The benchmark maps tasks to Remembering → Evaluating (cognitive) and Factual → Procedural (knowledge); understanding this helps interpret why models excel at low-cognition tasks (Rem. >85%) but struggle with high-cognition (Eva. <75%).
  - Quick check question: Classify "interpret policy exclusion clauses for a specific disease" into cognitive and knowledge dimensions.

- Concept: LLM-as-judge reliability
  - Why needed here: Open-ended evaluation relies on an LLM to decompose and score statements; you must understand its failure modes (length bias, verbosity reward, self-consistency issues).
  - Quick check question: What three safeguards would you add to an LLM-based judge to reduce false positives in faithfulness scoring?

## Architecture Onboarding

- Component map: Data layer (38,704 examples) -> Taxonomy engine (4 dimensions) -> Evaluation controller (accuracy vs. LLM-judge) -> LLM-judge module (statement decomposition)

- Critical path: Ingest example → retrieve taxonomy tags → route to appropriate evaluator (accuracy vs. LLM-judge) → compute metrics → aggregate by dimension → output diagnostic heatmaps

- Design tradeoffs:
  1. Faithfulness vs. completeness: Optimizing for one degrades the other; in production, prioritize faithfulness to avoid hallucinations in high-risk insurance advice
  2. General LLMs vs. domain-specific: General models outperform finance-specific ones on INSEva, suggesting over-specialization to finance benchmarks harms insurance transfer
  3. Deterministic vs. open-ended coverage: 92% of examples are MC/TF (deterministic); only 8% are QA/MD (open-ended). This balances scalability with nuanced assessment

- Failure signatures:
  1. ILR scores consistently low across all models → indicates systematic difficulty with multi-step numerical reasoning in insurance contexts
  2. Faithfulness-completeness trade-off → if deployment over-indexes on completeness, users may receive confident but unsupported claims
  3. Domain-specific models underperforming general LLMs → suggests current finance-domain training does not transfer to insurance; watch for similar gaps in other verticals

- First 3 experiments:
  1. Ablate the taxonomy: Evaluate the same model on shuffled dimension labels to test whether performance drops are due to genuine skill gaps or taxonomy artifacts
  2. Judge calibration: Compare LLM-based faithfulness scores against human expert annotations on a 100-example subset to measure systematic bias
  3. Cross-lingual consistency: Translate 10% of the benchmark to English, evaluate the same models, and verify that ILR/ISC gaps persist

## Open Questions the Paper Calls Out

- Question: How can future model architectures resolve the trade-off between faithfulness and completeness in high-risk insurance scenarios?
  - Basis in paper: [explicit] The authors observe that reasoning models (e.g., Deepseek-R1) achieve high completeness but suffer from lower faithfulness, whereas non-reasoning models show the inverse, posing a dilemma for insurance applications
  - Why unresolved: The paper identifies this trade-off as a significant challenge but does not propose a solution for balancing information coverage with strict factual accuracy
  - What evidence would resolve it: A training methodology that simultaneously increases completeness scores while maintaining or improving faithfulness metrics on the ISD (Insurance Service Dialogues) task

- Question: What specific architectural or data improvements are required for domain-specific financial LLMs to outperform general-purpose models in insurance?
  - Basis in paper: [explicit] The evaluation reveals that financial domain-specific models (Fin-R1, DianJin-R1) underperform compared to general LLMs like GPT-4o and Doubao, suggesting existing financial training does not transfer effectively to insurance
  - Why unresolved: It is unclear if this underperformance is due to insufficient insurance-specific training data, model scale limitations, or fundamental differences between financial reasoning and insurance logic
  - What evidence would resolve it: Demonstration of a specialized financial model fine-tuned on insurance data achieving superior performance on the INSEva benchmark compared to leading general-purpose models

- Question: To what extent does the Chinese-centric design of INSEva limit its transferability to English-centric or multilingual insurance applications?
  - Basis in paper: [explicit] The authors explicitly list the exclusive focus on Chinese as a limitation, noting it "may limit the direct applicability of our benchmark to models trained primarily on English"
  - Why unresolved: While a small-scale cross-lingual test showed similar trends, the paper notes the full benchmark relies on Chinese regulatory standards and linguistic nuances not present in translation
  - What evidence would resolve it: The construction and validation of a native English version of the dataset showing high correlation with the Chinese benchmark's evaluation of model capabilities

## Limitations
- Benchmark is exclusively Chinese, limiting applicability to models trained on English data
- Performance gaps in ILR/ISC may reflect benchmark difficulty rather than universal model limitations
- Claims about domain-specific models underperforming general LLMs based only on financial models, not insurance-specialized ones

## Confidence
- **High Confidence**: Data collection methodology and quality control procedures (authoritative sources, expert validation, multi-stage filtering)
- **Medium Confidence**: The ILR/ISC performance gaps and faithfulness-completeness trade-off (supported by internal results but lacking external validation)
- **Low Confidence**: Claims about domain-specific models underperforming general LLMs (only finance-specific models tested, not insurance-specialized ones)

## Next Checks
1. Judge Calibration Study: Compare LLM-based faithfulness/completeness scores against human expert annotations on 200 randomly selected responses to measure systematic bias and false positive rates
2. Taxonomy Independence Test: Evaluate models on dimension labels scrambled between examples to verify that performance differences reflect genuine skill gaps rather than taxonomy artifacts
3. Cross-Lingual Consistency: Translate 15% of benchmark examples to English, evaluate same models, and confirm that ILR/ISC gaps persist across languages