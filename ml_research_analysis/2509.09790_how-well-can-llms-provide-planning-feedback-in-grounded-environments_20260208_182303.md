---
ver: rpa2
title: How well can LLMs provide planning feedback in grounded environments?
arxiv_id: '2509.09790'
source_url: https://arxiv.org/abs/2509.09790
tags:
- feedback
- action
- reasoning
- actions
- gripper
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates how well large language models (LLMs) and
  vision-language models (VLMs) can provide planning feedback across diverse environments
  and feedback types. The authors systematically compare feedback quality across five
  types: binary feedback, preference feedback, action advising, goal advising, and
  delta action feedback, using domains ranging from symbolic games to continuous control.'
---

# How well can LLMs provide planning feedback in grounded environments?

## Quick Facts
- arXiv ID: 2509.09790
- Source URL: https://arxiv.org/abs/2509.09790
- Reference count: 40
- Key outcome: LLMs and VLMs can provide planning feedback across diverse environments, with larger reasoning models showing superior accuracy and less bias, though performance degrades significantly in domains with complex dynamics or continuous state and action spaces.

## Executive Summary
This paper systematically evaluates how well large language models (LLMs) and vision-language models (VLMs) can provide planning feedback across diverse environments and feedback types. The authors compare five feedback types—binary feedback, preference feedback, action advising, goal advising, and delta action feedback—across symbolic games, text-based environments, and continuous control domains. They find that larger and reasoning models consistently provide more accurate feedback, exhibit less bias, and benefit more from inference methods like in-context learning and chain-of-thought reasoning, though performance degrades significantly in domains with complex dynamics or continuous state and action spaces.

## Method Summary
The authors evaluate FM performance by sampling snapshots from expert, random, and half-expert policies across six environments, then generate ground truth labels using optimal policies (e.g., BFS for grids, built-in planners for text, behavior cloning experts for robotics). They test five feedback types using zero-shot inference with Chain-of-Thought prompts requiring JSON output, evaluating conditions including In-Context Learning (ICL), Thinking Guides, and Dynamics Hints. Models tested include Llama 3.1 (8B, 70B), DeepSeek R1, Qwen VL, and others across both text and vision modalities.

## Key Results
- Larger and reasoning models consistently provide more accurate feedback and exhibit less bias than smaller non-reasoning models
- VLMs outperform LLMs in continuous control domains when image observations are used instead of text descriptions
- Inference methods like ICL and dynamics hints improve accuracy for larger models but can degrade performance for smaller models due to prompt overload

## Why This Works (Mechanism)

### Mechanism 1: Scaling Law and Reasoning Capacity Mitigates Cognitive Biases
Larger reasoning models maintain objective estimation of state-action values over input context, reducing positional and class bias compared to smaller non-reasoning models.

### Mechanism 2: Grounding Fidelity via Observation Modality Alignment
VLMs provide higher quality feedback in continuous or visually complex domains only when observation modality matches domain's native structure (images), bypassing verbalization bottlenecks that affect LLMs.

### Mechanism 3: Conditional Utility of Inference-Time Compute
Inference techniques improve feedback accuracy primarily for models with capacity to integrate complex instructions, while degrading performance for smaller models due to "prompt overload" that dilutes attention mechanisms.

## Foundational Learning

- **Concept: Value Functions ($Q^\pi$, $V^\pi$)**
  - **Why needed here:** The evaluation framework compares FM output to ground truth derived from optimal policy; understanding $Q^*$ is crucial for defining binary and preference feedback.
  - **Quick check question:** Can you explain why "Preference Feedback" is defined as comparing $Q(s, a_1)$ vs $Q(s, a_2)$ rather than just checking immediate reward?

- **Concept: Markov Decision Process (MDP) & State-Spaces**
  - **Why needed here:** The paper distinguishes between symbolic, text-based, and continuous state spaces; understanding environment representation is crucial for diagnosing LLM failures.
  - **Quick check question:** Why does a "continuous state space" (like a robot arm position) present a harder verbalization challenge for an LLM than a "discrete state space" (like a grid coordinate)?

- **Concept: In-Context Learning (ICL) vs. Fine-Tuning**
  - **Why needed here:** The paper evaluates ICL as a primary method for improving feedback; distinguishing between prompt examples and weight updates is essential.
  - **Quick check question:** According to the paper, why might adding more ICL examples hurt a small model (like Llama 8B) while helping a large one?

## Architecture Onboarding

- **Component map:** Environment Sampler -> Verbalizer -> Feedback Model (FM) -> Evaluator
- **Critical path:** The Verbalizer -> FM interface; if state representation doesn't match model's pre-training distribution or lacks crucial info, FM reasons from false premises.
- **Design tradeoffs:**
  - Binary feedback is easier (80%+ accuracy) but provides less signal than Action Advising
  - Text is cheaper and works for grid worlds; Vision is required for robotics but demands larger VLMs
  - Rich dynamics hints help large reasoning models but confuse smaller ones; default to simple prompts for <10B models
- **Failure signatures:**
  - Format Errors: Small models outputting invalid JSON or inventing options
  - Bias Loops: Small models stuck outputting "NO" for binary feedback regardless of action quality
  - State Hallucination: VLMs misidentifying object positions in MiniGrid
- **First 3 experiments:**
  1. **Sanity Check (Binary Feedback):** Run Llama 3.1 8B vs. 70B on Cliff Walking; expect 8B to have high precision/low recall (bias toward "NO") and 70B to be balanced.
  2. **Modality Stress Test (Continuous Control):** Compare VLM vs. LLM on Robomimic; expect VLM preference accuracy (>70%) to significantly beat LLM action advising (~0%).
  3. **Prompt Overload Test:** Provide small model (8B) full environment dynamics in prompt; observe if accuracy drops compared to zero-shot baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLM feedback quality be improved in grounded environments with complex dynamics or continuous state and action spaces?
- Basis in paper: The authors state future study is required in how to improve LLM feedback in domains with complex dynamics and with continuous state and action spaces.
- Why unresolved: Current performance degrades significantly in these settings, and standard inference methods fail to bridge this gap.
- What evidence would resolve it: New methods achieving high feedback accuracy (>80%) in continuous control tasks comparable to symbolic domains.

### Open Question 2
- Question: How can agents efficiently learn from LLM-provided feedback to improve policy performance?
- Basis in paper: The authors explicitly call for future research on how to learn from such feedback efficiently.
- Why unresolved: This study focuses on evaluating feedback quality itself rather than training agents with it.
- What evidence would resolve it: Experiments showing agents trained using LLM feedback achieve higher reward or faster convergence than baselines.

### Open Question 3
- Question: How can the benefits of inference-time techniques like ICL and CoT be extended to smaller, non-reasoning models?
- Basis in paper: The authors note that smaller FMs generally do not benefit from these techniques and are occasionally confused by additional complexities.
- Why unresolved: The paper establishes a performance gap where complex prompts help large models but hurt small ones.
- What evidence would resolve it: Prompting strategies or model adaptations allowing smaller models to utilize ICL/CoT without degradation in accuracy.

### Open Question 4
- Question: How can the provided feedback be systematically leveraged to improve planning sample efficiency?
- Basis in paper: The Conclusion advocates for future research in how to leverage such feedback to systematically improve planning sample efficiency and quality.
- Why unresolved: While the paper confirms LLMs can provide high-quality feedback, it stops short of validating whether this feedback reduces trials needed for an agent to learn a task.
- What evidence would resolve it: Benchmarks demonstrating agents require fewer environment interactions when guided by LLM feedback compared to standard RL or IL methods.

## Limitations
- Reliance on ground truth labels derived from expert policies may not capture real-world planning complexity
- VLM superiority assumes static images contain sufficient state information, which may not hold for tasks requiring temporal reasoning
- Inference-time methods were tested within controlled prompt engineering frameworks, and effectiveness may vary with different prompt formulations

## Confidence
- **High confidence:** Comparative analysis showing larger reasoning models outperform smaller models in accuracy and bias mitigation, and general trend that VLMs excel in continuous control with visual observations
- **Medium confidence:** Effectiveness of inference-time methods varies significantly by model size, with larger models benefiting while smaller ones may degrade
- **Low confidence:** The assertion that feedback quality degradation in continuous domains is primarily due to verbalization challenges rather than fundamental limitations in LLM reasoning capabilities

## Next Checks
1. **Temporal reasoning test:** Evaluate FM performance on Robomimic using multi-frame image sequences versus single images to determine if VLM superiority persists when temporal information is explicitly provided.

2. **Cross-domain transfer validation:** Test the same FM across multiple domains with identical prompt structures to verify performance differences stem from model capabilities rather than prompt engineering variations.

3. **Human evaluation benchmark:** Compare FM outputs against human expert feedback on the same task instances to establish whether ground truth labels from expert policies represent true optimality or algorithmic preferences that may differ from human judgment.