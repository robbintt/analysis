---
ver: rpa2
title: 'Evaluating Subword Tokenization Techniques for Bengali: A Benchmark Study
  with BengaliBPE'
arxiv_id: '2511.05324'
source_url: https://arxiv.org/abs/2511.05324
tags:
- bengali
- bengalibpe
- tokenization
- tokenizers
- subword
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of subword tokenization for morphologically
  rich languages like Bengali, where existing tokenizers (SentencePiece, HuggingFace
  BPE) perform poorly due to their multilingual, Latin-script-focused design. The
  authors propose BengaliBPE, a Byte Pair Encoding tokenizer customized for the Bengali
  script, featuring Unicode normalization, grapheme-level initialization, and morphology-aware
  merge rules to preserve linguistic coherence.
---

# Evaluating Subword Tokenization Techniques for Bengali: A Benchmark Study with BengaliBPE

## Quick Facts
- arXiv ID: 2511.05324
- Source URL: https://arxiv.org/abs/2511.05324
- Reference count: 23
- Primary result: BengaliBPE achieves 0.36 tokens per character with interpretable morphological segmentation while maintaining high classification accuracy (macro-F1 ≥ 0.88)

## Executive Summary
This paper addresses the problem of subword tokenization for morphologically rich languages like Bengali, where existing tokenizers (SentencePiece, HuggingFace BPE) perform poorly due to their multilingual, Latin-script-focused design. The authors propose BengaliBPE, a Byte Pair Encoding tokenizer customized for the Bengali script, featuring Unicode normalization, grapheme-level initialization, and morphology-aware merge rules to preserve linguistic coherence. BengaliBPE is evaluated against three baselines (Whitespace, SentencePiece BPE, HuggingFace BPE) on a large Bengali news classification dataset. All methods achieve high classification accuracy (macro-F1 ≥ 0.88), but BengaliBPE provides the finest segmentation (0.36 tokens per character) and the most interpretable morphological splits, at a modest computational cost (365 samples/sec vs. 14,910 for whitespace). These results highlight the value of language-aware tokenization for morphologically complex scripts and establish BengaliBPE as a foundation for future Bengali NLP systems, including contextual language model pretraining.

## Method Summary
The study benchmarks four tokenization approaches on a Bengali news classification task using the Potrika dataset (~327,126 articles, 8 categories). Methods include Whitespace tokenization, SentencePiece BPE, HuggingFace BPE, and the proposed BengaliBPE. BengaliBPE implements grapheme-level initialization from normalized Bengali text, morphology-aware merge constraints, and is trained with 24K vocabulary size. All tokenizers are evaluated using TF-IDF features (unigrams+bigrams, min_df=2) with Logistic Regression classification. Metrics include tokens per character, encoding throughput, and classification performance (accuracy, macro-F1). The Python implementation uses NFKC Unicode normalization, filters to Bengali Unicode (U+0980–U+09FF), and applies morphology-aware constraints during BPE training.

## Key Results
- All four methods achieve high classification accuracy with macro-F1 ≥ 0.88
- BengaliBPE provides finest segmentation at 0.36 tokens per character vs. 0.15-0.17 for baselines
- BengaliBPE produces most interpretable morphological splits (e.g., ["বাংলা", "ভাষা", "ভাষী", "রা"] vs. ["বাংলা", "ভাষাভা", "ষীরা"])
- Computational cost: BengaliBPE processes 365 samples/sec vs. 14,910 for whitespace baseline

## Why This Works (Mechanism)

### Mechanism 1: Grapheme-Level Initialization Prevents Diacritic Fragmentation
- Claim: Initializing BPE vocabulary from normalized Bengali graphemes rather than raw bytes reduces linguistically invalid token boundaries.
- Mechanism: Standard BPE starts from individual bytes or characters, which can split consonant-diacritic combinations (e.g., consonant + "kar" vowel signs) that function as single phonetic units. BengaliBPE pre-composes these into grapheme clusters before merge learning, so subsequent merges never create tokens that sever diacritic-base relationships.
- Core assumption: Bengali orthographic units (graphemes) align better with morphological boundaries than raw Unicode codepoints.
- Evidence anchors:
  - [abstract]: "grapheme-level initialization...to maintain linguistic consistency and preserve subword integrity"
  - [section 3.3]: "prevents the accidental splitting of consonant clusters or dependent vowel signs, which are frequent in Bengali"
  - [corpus]: MorphTok paper confirms BPE's greedy merging "often" disregards morpheme boundaries in Indian languages

### Mechanism 2: Morphology-Aware Merge Constraints Preserve Semantic Units
- Claim: Restricting BPE merges to linguistically plausible boundaries improves morphological interpretability without requiring explicit morphological parsing.
- Mechanism: Standard BPE merges purely by frequency co-occurrence, which can combine unrelated subwords that happen to appear together often. BengaliBPE applies heuristics that block merges across root-suffix boundaries that would create semantically opaque units. This produces segmentations like ["ভাষা", "ভাষী", "রা"] (language-speaker-plural) rather than arbitrary fragments like ["ভাষাভা", "ষীরা"].
- Core assumption: Hand-crafted linguistic constraints generalize across the morphological variation in the training corpus.
- Evidence anchors:
  - [abstract]: "morphology-aware merge rules to preserve linguistic coherence"
  - [section 3.3]: "Merge operations are restricted to linguistically plausible boundaries (e.g., between roots and suffixes)"
  - [table 1]: Shows BengaliBPE producing ["বাংলা", "ভাষা", "ভাষী", "রা"] vs. HuggingFace's ["বাংলা", "ভাষাভা", "ষীরা"]
  - [corpus]: MorphBPE paper similarly argues morpheme-aware tokenization bridges "linguistic complexity" for efficient LLM training

### Mechanism 3: Finer Granularity Captures Morphological Signal for Neural Models (Not Proven)
- Claim: Higher tokens-per-character ratio may benefit neural embedding models by exposing morpheme-level patterns, though this was not demonstrated in the paper's TF-IDF experiments.
- Mechanism: Fine-grained tokenization increases the vocabulary's coverage of morphological variants as compositional combinations of smaller units. This should theoretically improve generalization to unseen word forms in neural models that learn distributed subword embeddings. However, TF-IDF + Logistic Regression does not benefit from this—it relies on exact token matching.
- Core assumption: Neural contextual models will leverage morpheme-level tokens better than word-level or coarse subword tokens.
- Evidence anchors:
  - [abstract]: Claims BengaliBPE is "a foundation for future Bengali NLP systems, including contextual language model pretraining"
  - [section 5]: "finer, linguistically grounded segmentation is crucial for contextual models that depend on subword embeddings"
  - [table 3]: BengaliBPE achieves 0.363 tokens/char vs. 0.152 for whitespace, but lower TF-IDF F1 (0.885 vs. 0.909)
  - [corpus]: Corpus signals on morphological tokenization for neural models are present but not empirically validated in this paper

## Foundational Learning

- Concept: **Byte Pair Encoding (BPE) Vocabulary Learning**
  - Why needed here: BengaliBPE modifies the standard BPE algorithm; you must understand the baseline to evaluate the modifications.
  - Quick check question: Given corpus ["low", "lower", "newest", "widest"], what are the first two merges standard BPE would learn?

- Concept: **Unicode Normalization Forms (NFC, NFD, NFKC, NFKD)**
  - Why needed here: BengaliBPE relies on NFKC normalization to canonicalize visually identical characters with different byte representations.
  - Quick check question: Why might NFKC be preferred over NFC for machine learning preprocessing pipelines?

- Concept: **Tokens-per-Character as a Granularity Metric**
  - Why needed here: The paper uses this metric to quantify segmentation fineness; interpreting results requires understanding what it measures.
  - Quick check question: A tokenizer produces 0.36 tokens/char on Bengali text and 0.20 tokens/char on English text. What might this indicate about the relative morphological complexity of the two corpora?

## Architecture Onboarding

- Component map: Raw Bengali text → NFKC normalization → character filtering (U+0980-U+09FF + punctuation/whitespace) → whitespace collapse → grapheme segmentation → constrained BPE merges → vocabulary file → subword token sequence

- Critical path:
  1. Raw Bengali text → NFKC normalization (inconsistent Unicode → canonical form)
  2. Normalized text → grapheme segmentation (base + diacritic combinations)
  3. Grapheme stream → constrained BPE merges → vocabulary file
  4. New text → same pipeline → token IDs for downstream model

- Design tradeoffs:
  - **Granularity vs. TF-IDF performance**: BengaliBPE's 0.36 tokens/char captures more morphology but reduces F1 from 0.909 (whitespace) to 0.885 on TF-IDF + LogReg—acceptable for neural pipelines, suboptimal for sparse bag-of-words
  - **Speed vs. interpretability**: Python implementation runs 365 samples/sec vs. SentencePiece's 1,708 (C++)—acceptable for research/preprocessing, bottleneck for real-time serving
  - **Language-specificity vs. portability**: Bengali-only design optimizes for one script; extending to other Indic languages requires re-engineering grapheme rules

- Failure signatures:
  - **High OOV rate on domain shift**: If vocabulary trained only on news, social media or literary Bengali may produce excessive unknown tokens
  - **Inconsistent normalization**: Legacy-encoded Bengali text may bypass NFKC properly, producing fragmented graphemes
  - **Merge constraint violations**: Overly aggressive morphology rules may block productive compounds, inflating vocabulary size

- First 3 experiments:
  1. **Reproduce baseline comparison**: Train all four tokenizers on Potrika subset, verify tokens/char, encoding speed, and TF-IDF F1 match reported values (±0.01)
  2. **Domain transfer test**: Train BengaliBPE on news, evaluate tokens/char and OOV rate on held-out Bengali Wikipedia or social media corpus—assess generalization
  3. **Neural downstream pilot**: Train a simple Bengali text classifier (e.g., fastText or small Transformer) using BengaliBPE vs. SentencePiece tokens on same data; compare embedding quality via probing task—this tests the paper's hypothesized mechanism 3

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the fine-grained segmentation of BengaliBPE yield superior performance in Transformer-based pretraining compared to standard multilingual tokenizers?
- Basis in paper: [explicit] Section 7 (Future Work) states the intent to integrate BengaliBPE into Transformer pretraining to evaluate effects on contextual embedding quality.
- Why unresolved: The current study evaluates downstream performance using TF-IDF and Logistic Regression, a setup that favors coarser tokenization and may mask the benefits of morphological awareness in deep neural architectures.
- What evidence would resolve it: Benchmark comparisons (e.g., NER, sentiment analysis) using a BengaliBERT model initialized with BengaliBPE versus standard SentencePiece or HuggingFace tokenizers.

### Open Question 2
- Question: Can BengaliBPE be optimized via compiled backends to achieve encoding speeds competitive with C++ implementations like SentencePiece?
- Basis in paper: [explicit] Section 6 (Limitations) identifies the pure Python implementation as a bottleneck for large-scale pipelines, and Section 7 proposes investigating compiled backends (C++ or Rust) for optimization.
- Why unresolved: The current Python implementation processes only 365 samples/sec compared to SentencePiece's 1,707 samples/sec, making efficiency a critical hurdle for adoption in large-scale pretraining.
- What evidence would resolve it: Runtime benchmarks showing that a compiled version of BengaliBPE matches or exceeds the throughput of existing optimized tokenizers while retaining linguistic rules.

### Open Question 3
- Question: Can the linguistically informed heuristics used in BengaliBPE be effectively adapted for other morphologically rich Indic languages?
- Basis in paper: [explicit] Section 7 (Future Work) explicitly proposes expanding BengaliBPE to Hindi, Assamese, and Odia by modifying preprocessing and merging rules.
- Why unresolved: The current algorithm relies on Bengali-specific constraints (e.g., Unicode normalization for Bengali script), and it is unproven whether this specific morphological awareness transfers effectively to the distinct orthographic structures of other Indic scripts.
- What evidence would resolve it: Successful application of the framework to Hindi or Odia corpora, demonstrating high morphological interpretability and consistent segmentation without extensive re-engineering.

## Limitations
- Morphology-aware merge constraints are described qualitatively but not fully specified, making exact replication challenging
- Evaluation is confined to a single Bengali news domain, leaving domain generalization performance uncertain
- The paper does not address potential vocabulary inflation when encountering rare or out-of-domain morphological patterns

## Confidence
**High Confidence**: The claim that BengaliBPE achieves superior morphological interpretability is well-supported by direct empirical evidence (tokenization examples in Table 1) and measurable outcomes (finer granularity at 0.36 tokens/char vs. 0.15-0.17 for baselines).

**Medium Confidence**: The assertion that BengaliBPE is "a foundation for future Bengali NLP systems, including contextual language model pretraining" is reasonable given the results, but remains untested in this work.

**Low Confidence**: The mechanism by which morphology-aware merge constraints improve downstream performance for neural models is entirely hypothetical at this stage. The paper's TF-IDF + Logistic Regression experiments actually show slightly worse performance with BengaliBPE (0.885 F1 vs. 0.909 for whitespace).

## Next Checks
1. **Neural Model Pilot Study**: Train a Bengali text classifier (e.g., fastText or small Transformer) using BengaliBPE vs. SentencePiece tokens on the same Potrika dataset. Compare embedding quality via probing tasks (e.g., morphological tagging accuracy) to test whether BengaliBPE's fine-grained tokens provide measurable benefits for neural models, as claimed.

2. **Domain Transfer Analysis**: Evaluate BengaliBPE's OOV rate and tokens/char on held-out Bengali Wikipedia and social media corpora. This will assess whether the vocabulary trained on news generalizes to other Bengali text domains, addressing the limitation of single-domain evaluation.

3. **Merge Constraint Specification**: Request or reconstruct the exact morphology-aware merge heuristics used by BengaliBPE. Without these rules, faithful replication and extension to other morphologically rich languages is impossible. This could involve contacting the authors or reverse-engineering from the tokenizer's behavior on test cases.