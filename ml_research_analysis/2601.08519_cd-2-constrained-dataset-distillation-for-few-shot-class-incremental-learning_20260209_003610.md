---
ver: rpa2
title: 'CD^2: Constrained Dataset Distillation for Few-Shot Class-Incremental Learning'
arxiv_id: '2601.08519'
source_url: https://arxiv.org/abs/2601.08519
tags:
- knowledge
- learning
- which
- incremental
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses catastrophic forgetting in few-shot class-incremental\
  \ learning (FSCIL) by introducing a Constrained Dataset Distillation (CD\xB2) framework\
  \ that more effectively preserves critical knowledge from previous classes. The\
  \ core idea combines dataset distillation to synthesize highly condensed, class-related\
  \ samples (Dataset Distillation Module, DDM) with a distillation constraint module\
  \ (DCM) that maintains feature and structure consistency across incremental sessions."
---

# CD²: Constrained Dataset Distillation for Few-Shot Class-Incremental Learning

## Quick Facts
- arXiv ID: 2601.08519
- Source URL: https://arxiv.org/abs/2601.08519
- Reference count: 18
- Achieves state-of-the-art average accuracy of 68.67% on CIFAR100

## Executive Summary
This paper introduces CD², a Constrained Dataset Distillation framework designed to address catastrophic forgetting in Few-Shot Class-Incremental Learning (FSCIL). The method combines dataset distillation to synthesize highly condensed class-related samples with a distillation constraint module that maintains feature and structure consistency across incremental sessions. The approach effectively preserves critical knowledge from previous classes while enabling the model to learn new ones, achieving significant performance improvements over existing FSCIL methods.

## Method Summary
CD² integrates two key components: a Dataset Distillation Module (DDM) that generates synthetic samples capturing essential class information through MMD loss optimization, and a Distillation Constraint Module (DCM) that employs feature retention and structure retention losses to reduce covariate shift and preserve knowledge. The framework trains a ResNet12 backbone with an MLP classifier, generating memory via DDM during the base session and applying DCM losses during incremental sessions to mitigate catastrophic forgetting.

## Key Results
- Achieves state-of-the-art average accuracy of 68.67% on CIFAR100, outperforming NC-FSCIL by 1.17%
- Demonstrates slower accuracy degradation across incremental sessions compared to baseline methods
- Ablation study confirms contributions from both DDM and DCM components to overall performance gains

## Why This Works (Mechanism)
The framework addresses catastrophic forgetting by creating a compact, informative memory set through dataset distillation that captures the essence of each class. The distillation constraint module then enforces consistency in both feature representations and structural relationships between old and new data, preventing significant shifts in the learned representations that typically cause forgetting.

## Foundational Learning
- **Dataset Distillation**: Synthesizes informative samples that represent entire classes in compact form; needed to create efficient memory that doesn't grow unboundedly
- **MMD Loss**: Measures distributional similarity between synthetic and real features; needed to ensure synthetic samples capture real data distribution
- **Feature Retention Loss**: Maintains feature consistency between old and new sessions; needed to prevent catastrophic forgetting
- **Structure Retention Loss**: Preserves class relationship structures; needed to maintain discriminative boundaries
- **Adaptive Loss Scaling**: Adjusts constraint strength based on class distribution; needed to balance stability and plasticity
- **Incremental Learning**: Adds new classes without retraining on old data; needed to simulate real-world continuous learning scenarios

## Architecture Onboarding

**Component Map:** Base Training -> DDM Synthesis -> Incremental Loop (Classifier Training + DDM Update + Memory Update)

**Critical Path:** Base session training → DDM memory generation → Each incremental session (frozen backbone, classifier training with DCM losses, DDM synthesis, memory update)

**Design Tradeoffs:** Compact memory via DDM vs. potential information loss; strong constraints vs. flexibility to learn new classes

**Failure Signatures:** Poor early session performance suggests backbone not properly frozen; synthetic samples appearing as noise indicates DDM optimization issues

**First Experiments:**
1. Verify base training produces reasonable baseline accuracy before DDM synthesis
2. Test synthetic sample quality by visualizing generated images and checking feature distributions
3. Validate incremental session performance with and without DCM constraints

## Open Questions the Paper Calls Out
None

## Limitations
- Performance still degrades across many incremental sessions despite improvements
- Memory set, while compact, grows with each session and may become unwieldy
- The method requires careful tuning of multiple hyperparameters (DDM iterations, loss weights, etc.)

## Confidence

**High Confidence:** The core methodology combining dataset distillation with distillation constraints is clearly specified and reproducible. The use of MMD loss for sample synthesis and the dual constraint approach (feature and structure retention) are well-documented.

**Medium Confidence:** The adaptive scaling factor formula for the structure retention loss is specified, but its practical implementation and sensitivity to parameter choices are unclear without code.

**Low Confidence:** Exact hyperparameter values for optimizer settings, batch sizes, and network architecture details are missing, which may impact exact reproduction of reported results.

## Next Checks
1. Verify the backbone freezing mechanism during incremental sessions and DDM synthesis - this is critical for preventing catastrophic forgetting
2. Validate the MMD-based sample synthesis by checking synthetic sample quality and feature distribution alignment with real data
3. Test the adaptive scaling factor implementation to ensure the structure retention loss properly balances between stability and plasticity across sessions