---
ver: rpa2
title: An Empirical Study on Noisy Data and LLM Pretraining Loss Divergence
arxiv_id: '2602.02400'
source_url: https://arxiv.org/abs/2602.02400
tags:
- noise
- noisy
- data
- divergence
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically investigates whether noisy data causes
  large language model pretraining loss divergence and how it does so. The authors
  inject controlled synthetic uniform random noise into clean datasets and analyze
  training dynamics across model sizes from 480M to 5.2B parameters.
---

# An Empirical Study on Noisy Data and LLM Pretraining Loss Divergence

## Quick Facts
- arXiv ID: 2602.02400
- Source URL: https://arxiv.org/abs/2602.02400
- Reference count: 24
- Primary result: Noisy data induces LLM pretraining loss divergence with probability dependent on noise type, amount, and model scale

## Executive Summary
This paper systematically investigates whether noisy data causes large language model pretraining loss divergence and how it does so. The authors inject controlled synthetic uniform random noise into clean datasets and analyze training dynamics across model sizes from 480M to 5.2B parameters. They find that noisy data does induce training loss divergence, with divergence patterns differing from those caused by high learning rates, enabling diagnostic separation. The study provides empirical evidence that uniform random noise can destabilize LLM pretraining and offers practical guidance for identifying and mitigating noise-induced failures.

## Method Summary
The authors systematically inject synthetic uniform random noise into clean pretraining corpora and train decoder-only transformer models across a range of scales. Noise injection is controlled by two parameters: vocabulary size (how many tokens the noise can draw from) and injection ratio (what percentage of tokens are noise). They train models from 480M to 5.2B parameters, monitor training stability through loss divergence metrics, and track attention logits and parameter norms to develop diagnostic criteria. Two noise injection methods are compared: inserting noise tokens between existing tokens versus overwriting existing tokens with noise.

## Key Results
- Noisy data induces training loss divergence, with divergence probability depending strongly on noise type, amount, and model scale
- Divergence patterns differ from high learning rate divergence, enabling diagnostic separation based on attention logit thresholds (~1800 for noise vs ~4000 for high LR)
- Dense and MoE models show similar sensitivity to noisy data
- Restricted noise vocabularies (e.g., 5 tokens) are significantly more destabilizing than full-vocabulary noise
- Model depth is the primary architectural factor determining noise sensitivity, not width

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Restricting noise tokens to a smaller vocabulary subset increases divergence probability.
- **Mechanism:** When noise tokens are drawn from a restricted vocabulary, the model encounters higher-frequency repetitions of the same noisy tokens, which amplifies gradient conflicts and destabilizes the optimization landscape compared to noise spread across the full vocabulary.
- **Core assumption:** Uniform random noise from restricted vocabularies approximates real-world noise patterns (e.g., hash codes using limited character sets).
- **Evidence anchors:** Section 4.1, Figure 2 shows restricted vocabularies increase divergence probability; Section 3.1 motivates restricted noise vocabularies by noting real-world noise typically occupies limited vocabulary subsets.
- **Break condition:** If noise is spread uniformly across the full tokenizer vocabulary (|Vn| ≈ 200K), divergence probability drops substantially even at high noise ratios.

### Mechanism 2
- **Claim:** Model depth, not width, is the primary architectural factor determining noise sensitivity.
- **Mechanism:** Deeper networks propagate noise-induced gradient perturbations through more layers, compounding activation instabilities. Width scaling primarily increases capacity without proportionally extending the gradient propagation path.
- **Core assumption:** The relationship between depth and noise sensitivity generalizes beyond the 480M–5.2B parameter range studied.
- **Evidence anchors:** Section 4.2, Figure 6 shows width has limited effect on stability while depth substantially degrades stability; 35-layer, 2.5B model diverges in 15% of runs even at 5% noise ratio.
- **Break condition:** Shallow architectures (≤10 layers) may remain stable even at elevated noise ratios.

### Mechanism 3
- **Claim:** Noise-induced divergence exhibits a distinct activation signature (maximum attention logit threshold ~1800) compared to high learning-rate divergence (threshold ~4000).
- **Mechanism:** Noisy data causes moderate attention logit growth through repeated exposure to unpredictable tokens, whereas high learning rates drive rapid, unbounded parameter norm growth that cascades into attention collapse.
- **Core assumption:** These thresholds are model-size-invariant, as suggested by authors' replication across architectures.
- **Evidence anchors:** Section 4.3.1, Figure 7 shows high LR divergences produce significantly larger maximum attention logits (threshold ~4000) than noisy data (threshold ~1800); Section 4.3.2, Figure 8 shows noise-induced divergence exhibits significantly smaller parameter norms than high-LR divergence.
- **Break condition:** If maximum attention logits exceed ~4000, divergence is more likely due to learning rate; if between ~1800–4000 with elevated noise, data quality is the probable cause.

## Foundational Learning

- **Concept: Attention logits and softmax collapse**
  - **Why needed here:** The paper's diagnostic method relies on interpreting maximum attention logit values as early warning signals for divergence.
  - **Quick check question:** Can you explain why a maximum attention logit of 4000 would cause softmax to approach a one-hot distribution?

- **Concept: AdamW optimizer dynamics with gradient clipping**
  - **Why needed here:** The experimental setup uses specific AdamW hyperparameters (β1=0.9, β2=0.95, gradient clipping=1.0), and understanding these helps interpret why noise destabilizes training.
  - **Quick check question:** How does gradient clipping at norm 1.0 interact with noisy gradients from random token sequences?

- **Concept: Mixture-of-Experts routing and load balancing**
  - **Why needed here:** Section 4.5 compares dense and MoE models; understanding token routing is essential to interpret why MoEs show comparable noise sensitivity.
  - **Quick check question:** Why might one hypothesize that MoE routing could amplify noise sensitivity, and what does the paper find instead?

## Architecture Onboarding

- **Component map:** Noise injection module -> Model backbone (decoder-only transformer with RoPE, SwiGLU, Group Query Attention) -> Stability monitors (attention logits, parameter norms) -> Optional mitigations (QK-layernorm, router z-loss)

- **Critical path:**
  1. Define noise vocabulary size |Vn| and injection method (insert vs. overwrite)
  2. Set noise ratio α (start ≤30% for initial experiments)
  3. Monitor maximum attention logits at step 1000
  4. If logits exceed ~1800, consider data cleaning or QK-layernorm intervention

- **Design tradeoffs:**
  - QK-layernorm: Eliminates divergence but modifies architecture (may affect downstream performance comparisons)
  - Data cleaning: Most effective but requires computational investment
  - Shallower models: Reduce noise sensitivity but may trade off model capability

- **Failure signatures:**
  - Noise-induced divergence: Max attention logits ~1800–4000, moderate parameter norm growth, loss diverges gradually
  - High-LR divergence: Max attention logits >4000, large parameter norm growth, loss spikes sharply
  - Stable spike: Temporary loss increase with quick recovery (categorize as stable)

- **First 3 experiments:**
  1. **Baseline noise sensitivity:** Train 540M dense model with |Vn|=5, α=35% insertion noise, 20 seeds; measure divergence rate and max attention logits at step 1000
  2. **Diagnostic validation:** Compare high-LR run (lr=4.5e-2, clean data) vs. noisy data run (lr=1.85e-2, α=50%); verify distinct attention logit thresholds
  3. **Intervention test:** Add QK-layernorm to configuration from experiment 1; confirm divergence elimination and measure impact on clean-token validation loss

## Open Questions the Paper Calls Out

- **Question:** Why does reducing the noise vocabulary size increase the probability of divergence?
  - **Basis in paper:** Section 4.1 establishes that restricted vocabularies are significantly more destabilizing than full-vocabulary noise, but the paper offers no theoretical explanation for this phenomenon.
  - **Why unresolved:** The authors focus on characterizing the empirical correlation and diagnostic thresholds rather than deriving the underlying cause of the gradient instability.
  - **What evidence would resolve it:** A mechanistic interpretability study analyzing gradient norms or loss landscape geometry when models are exposed to repetitive, restricted-token noise.

- **Question:** How do non-uniform, "natural" noise types affect divergence compared to the synthetic uniform noise studied?
  - **Basis in paper:** The paper acknowledges that real data contains "unregulated web content" but relies entirely on synthetic uniform random noise for controlled experimentation.
  - **Why unresolved:** Synthetic noise ensures experimental control but may not replicate the token distributions or semantic structures of actual web-scraped garbage data.
  - **What evidence would resolve it:** Experiments injecting real, low-quality web data segments (e.g., bot-generated text) and comparing the resulting activation signatures to the synthetic baselines.

- **Question:** Do noise sensitivity trends persist at scales significantly larger than 5.2B parameters?
  - **Basis in paper:** The methodology is restricted to models ranging from 480M to 5.2B parameters, while state-of-the-art frontier models are orders of magnitude larger.
  - **Why unresolved:** Computational constraints limit the feasibility of running controlled ablation studies on noisy data at the frontier scale.
  - **What evidence would resolve it:** Extrapolating the divergence probability curves from the 480M-5.2B range to predict and verify stability in a 70B+ parameter run.

## Limitations
- Study focuses exclusively on synthetic uniform random noise rather than real-world noise patterns in pretraining corpora
- Experimental scope limited to decoder-only transformers within 480M–5.2B parameter range
- Diagnostic thresholds for distinguishing noise-induced from high-LR divergence require validation across diverse model scales and architectures

## Confidence
- **High confidence**: The empirical demonstration that noisy data induces training loss divergence, supported by controlled experiments across multiple model sizes and noise configurations
- **Medium confidence**: The architectural sensitivity findings (depth vs width scaling) and the effectiveness of QK-layernorm as a mitigation technique
- **Low confidence**: The hypothesis that restricted noise vocabularies (e.g., |Vn|=5) accurately model real-world noise patterns

## Next Checks
1. **Real-world noise validation**: Test the diagnostic framework on actual noisy pretraining datasets (e.g., datasets known to contain corrupted tokens, hash codes, or out-of-distribution content) to verify that the 1800/4000 attention logit thresholds generalize beyond synthetic uniform random noise.

2. **Scale extension study**: Replicate the depth-width sensitivity experiments on models outside the 480M–5.2B range, particularly examining very deep (100+ layer) architectures and extremely wide models (>4096 dimensions) to test whether the depth scaling relationship holds at extreme scales.

3. **Cross-architecture diagnostic robustness**: Evaluate whether the attention logit-based diagnostic method works for encoder-decoder transformers and other model families, as the current validation is limited to decoder-only architectures. This would establish whether the divergence signatures are architecture-dependent or general properties of transformer training dynamics.