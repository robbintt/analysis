---
ver: rpa2
title: 'Don''t Pay Attention, PLANT It: Pretraining Attention via Learning-to-Rank'
arxiv_id: '2410.23066'
source_url: https://arxiv.org/abs/2410.23066
tags:
- attention
- stage
- plant
- label
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of extreme multi-label text
  classification (XMTC) where models must assign relevant labels from a large label
  set, often including rare labels. The core method, PLANT (Pretrained and Leveraged
  Attention), introduces a two-stage training strategy that pretrains the attention
  layer as a learning-to-rank module using mutual information gain to guide attention
  weights, then leverages these pretrained weights in full end-to-end training.
---

# Don't Pay Attention, PLANT It: Pretraining Attention via Learning-to-Rank

## Quick Facts
- arXiv ID: 2410.23066
- Source URL: https://arxiv.org/abs/2410.23066
- Authors: Debjyoti Saha Roy; Byron C. Wallace; Javed A. Aslam
- Reference count: 40
- Key outcome: PLANT achieves consistent gains across multiple LLM backbones in extreme multi-label text classification, with particularly strong performance on rare labels in few-shot settings

## Executive Summary
PLANT addresses the challenge of extreme multi-label text classification (XMTC) by pretraining the attention layer as a learning-to-rank module guided by mutual information gain (MIG). The method consists of two stages: first pretraining the attention weights to rank tokens based on their MIG relevance to each label, then leveraging these pretrained weights in end-to-end fine-tuning. This approach is architecture-agnostic and integrates seamlessly with various LLM backbones, showing consistent improvements across ICD coding, legal topic classification, and content recommendation tasks. Gains are especially pronounced in few-shot settings and for rare labels, with improvements of +0.7 macro-F1 and +1.4 precision@8 on MIMIC-IV-full compared to state-of-the-art baselines.

## Method Summary
PLANT is a two-stage training framework for extreme multi-label text classification. Stage 1 pretrains a multi-head attention module using a pairwise ranking loss optimized by nDCG, where the target rankings are derived from mutual information gain (MIG) scores computed between tokens and labels across the training corpus. This creates attention weights that prioritize tokens most informative for each label, even for rare labels. Stage 2 performs standard end-to-end fine-tuning using HNM-augmented asymmetric focal loss with gradual unfreezing to preserve the pretrained attention patterns. The method is architecture-agnostic and can be applied to various LLM backbones including Mistral-7B, LLaMA3-8B, DeepSeek-V3, and Phi-3.

## Key Results
- Consistent gains across multiple LLM backbones (Mistral-7B, LLaMA3-8B, DeepSeek-V3, Phi-3) in extreme multi-label text classification
- Substantial improvements on rare labels, with gains of +0.7 macro-F1 and +1.4 precision@8 on MIMIC-IV-full compared to SOTA baselines
- Particularly effective in few-shot settings, where pretraining provides a strong initialization for the attention mechanism
- Ablation studies confirm that attention initialization is the key driver of these gains, with performance collapsing when Stage 1 is skipped

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** MIG provides a principled, corpus-derived signal for identifying which tokens are most relevant to each label, overcoming data scarcity for rare labels
- **Mechanism:** MIG computes the reduction in entropy about a label's presence given a token's presence, identifying tokens with strong conditional dependence on labels even if they co-occur in only a few documents
- **Core assumption:** Statistical relationships between tokens and labels in the training corpus reliably proxy for semantic relevance
- **Evidence anchors:** MIG-guided pretraining outperforms frequency-based pretraining (ablation 6, Table 6); top-k MIG tokens serve as ground truth for attention pretraining
- **Break condition:** Fails if label-token relationships are non-statistical or if spurious correlations exist in the corpus

### Mechanism 2
- **Claim:** Pretraining the multi-label attention module as a Learning-to-Rank model creates a high-quality initialization that directly improves downstream performance on rare labels and few-shot settings
- **Mechanism:** Attention weights are trained to correctly rank MIG-identified relevant tokens at the top, "planting" informative attention patterns before end-to-end fine-tuning
- **Core assumption:** Well-initialized attention is more critical for handling label imbalance than downstream fine-tuning alone
- **Evidence anchors:** Better Stage 1 nDCG leads to higher Stage 2 macro-F1; random initialization yields only 5.5% rare-F1 (Figure 4)
- **Break condition:** Compromised if Stage 1 overfits to MIG signals or if downstream attention patterns differ fundamentally

### Mechanism 3
- **Claim:** MIG-guided attention pretraining enables zero-shot generalization to unseen rare labels by learning semantically coherent token patterns that transfer across related labels
- **Mechanism:** Model learns to associate semantically meaningful token clusters with their corresponding labels during pretraining on common labels, enabling recognition of relevant tokens for unseen rare labels
- **Core assumption:** Semantically related labels share common informative tokens that the model can learn from common labels
- **Evidence anchors:** PLANT trained only on common labels retains substantial performance (7.3-8.1%) versus random init collapse (0.5-1.1% rare-F1) (Figure 3)
- **Break condition:** Fails if rare labels have token distributions significantly different from any common label

## Foundational Learning

- **Extreme Multi-Label Text Classification (XMTC)**
  - **Why needed here:** PLANT is designed specifically for XMTC, characterized by thousands to hundreds of thousands of labels with highly skewed, long-tailed distributions
  - **Quick check question:** Given a dataset with 50,000 labels where 80% appear in fewer than 5 documents, what would be the expected performance of a standard classifier on the labels in the tail of the distribution?

- **Multi-Label Attention**
  - **Why needed here:** PLANT's central contribution is a method for initializing the multi-label attention layer that computes separate attention distributions for each label
  - **Quick check question:** How does a multi-label attention layer's output for a single label differ from the output of a standard self-attention layer?

- **Learning-to-Rank (L2R) & Pairwise Ranking Loss**
  - **Why needed here:** Stage 1 reformulates attention learning as a ranking problem using pairwise ranking loss to train attention to place MIG-identified tokens at the top
  - **Quick check question:** In a ranking task with a pairwise loss, how does the model penalize itself for incorrectly ordering two items, j and h?

## Architecture Onboarding

- **Component map:**
  - LLM Backbone (M_adapt) -> Hidden states (H_i) -> Multi-Head Attention (MultiHead) -> Label-specific attention scores (S) -> Classification head
  - Label Embeddings (E) -> Queries for attention mechanism
  - MIG Scores (r_l) -> Static relevance targets for pretraining

- **Critical path:**
  1. Data Preparation: Compute MIG scores for all token-label pairs (CPU-based, one-time step)
  2. Stage 1 Training: Initialize E and W_attn, train MultiHead using pairwise ranking loss against MIG rankings for 10 epochs
  3. Stage 2 Training: Initialize full model with Stage 1 weights, fine-tune using HNM-augmented Focal Loss with gradual unfreezing

- **Design tradeoffs:**
  - MIG Top-k (k): Smaller k focuses on most informative tokens but may ignore context; larger k is more robust but adds noise (k=1000 is robust)
  - Training Regimen: Two-stage approach adds complexity but yields significant gains; single-stage is simpler but worse on rare labels
  - Backbone Scale: Effective across different model sizes; relative overhead of Stage 1 is smaller for larger models

- **Failure signatures:**
  - Random-Init Collapse: Skipping Stage 1 causes rare label performance to collapse to baseline levels
  - MIG Failure: Replacing MIG with term frequency degrades performance, indicating ranking quality matters
  - Overfitting in Stage 1: Too many epochs may cause overfitting to corpus statistics

- **First 3 experiments:**
  1. MIG Validation: Compute MIG scores for a few labels on sample data and manually inspect top-ranked tokens for semantic sense
  2. Ablation on Rare Labels: Train with and without PLANT attention pretraining, compare macro-F1 specifically on held-out rare labels
  3. k-Sensitivity Sweep: Run training with different k values (500, 1000, 2000) and plot final macro-F1 to determine optimal token set size

## Open Questions the Paper Calls Out

- Can PLANT pretraining be adapted for multimodal tasks where cross-signal attention is critical? (The paper suggests it could extend naturally to multimodal tasks)
- How does computational cost scale with industrial-sized label spaces containing millions of labels? (Current experiments use thousands of labels)
- Does initializing label embeddings using semantic descriptions provide additional performance gains? (Paper uses random initialization for E)

## Limitations
- Evaluation limited to three domains (ICD coding, legal topic classification, content recommendation) may not generalize to all XMTC tasks
- Computational overhead from MIG precomputation and two-stage training may be prohibitive for production deployments
- Method is designed specifically for transformer-based architectures and may not apply to other model types

## Confidence
- **High Confidence**: MIG-guided attention pretraining improves rare label performance is well-supported by ablation studies
- **Medium Confidence**: Zero-shot generalization claims are supported by controlled experiments but rely on limited demonstration
- **Low Confidence**: Claims about applicability to emerging domains like genomics or social media are speculative without empirical validation

## Next Checks
1. Apply PLANT to a fundamentally different XMTC task (e.g., e-commerce product categorization or multi-label sentiment analysis) to assess cross-domain generalizability
2. Conduct systematic ablation varying Stage 1 training duration (0 to 20 epochs) to quantify tradeoff between pretraining investment and rare label gains
3. Replace MIG with alternative corpus-derived ranking signals (PMI, chi-squared, or learned attention) to determine if MIG's specific properties are essential or if ranking-based pretraining is the key innovation