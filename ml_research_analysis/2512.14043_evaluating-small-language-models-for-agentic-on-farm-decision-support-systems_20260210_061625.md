---
ver: rpa2
title: Evaluating Small Language Models for Agentic On-Farm Decision Support Systems
arxiv_id: '2512.14043'
source_url: https://arxiv.org/abs/2512.14043
tags:
- dairy
- data
- system
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks 20 open-source Small Language Models (SLMs)
  for deployment in on-farm decision support systems under realistic compute constraints
  (NVIDIA T4 GPU, 16 GB VRAM). Building on prior work, a multi-agent system with five
  specialized agents (literature/web search, SQL/NoSQL database interaction, predictive
  modeling, and integrity validation) was evaluated.
---

# Evaluating Small Language Models for Agentic On-Farm Decision Support Systems

## Quick Facts
- arXiv ID: 2512.14043
- Source URL: https://arxiv.org/abs/2512.14043
- Authors: Enhong Liu; Haiyu Yang; Miel Hostens
- Reference count: 17
- Primary result: Qwen-4B (27/30 correct) outperformed quantized larger models for farm decision support on NVIDIA T4 GPU

## Executive Summary
This study benchmarks 20 open-source Small Language Models (SLMs) for deployment in on-farm decision support systems under realistic compute constraints (NVIDIA T4 GPU, 16 GB VRAM). Building on prior work, a multi-agent system with five specialized agents (literature/web search, SQL/NoSQL database interaction, predictive modeling, and integrity validation) was evaluated. After initial screening of five test questions, only three models advanced to comprehensive testing with 30 questions. Qwen-4B achieved the highest performance (27/30 correct), outperforming 8-bit quantized Mistral-7B (19/30) and 4-bit quantized Qwen-8B (12/30). Qwen-4B demonstrated superior capability across most tasks but showed instability in NoSQL database interactions via PySpark. The study confirms SLMs can effectively support farm data integration and decision-making while preserving privacy and minimizing resource requirements, though further fine-tuning for dairy-specific terminology is needed.

## Method Summary
The study evaluated SLMs using a two-phase approach: Phase 1 screened 20 models with 5 questions (2 literature, 2 web, 1 SQL), advancing only 3 to Phase 2 which tested 30 questions across 6 categories (literature/web search, SQL, NoSQL, predictive modeling, intention validation, and integrity validation). All agents operated under one shared SLM to minimize computational requirements. The system used a supervisor agent to classify queries and route them to specialized subagents. Evaluation was conducted on an NVIDIA T4 GPU with 16 GB VRAM using LangGraph for agent orchestration. Prompts were customized per model in Phase 2, with specific focus on output formats and tool constraints.

## Key Results
- Qwen-4B achieved the highest accuracy (27/30 correct) across all task categories
- Unquantized Qwen-4B outperformed both 8-bit quantized Mistral-7B (19/30) and 4-bit quantized Qwen-8B (12/30)
- NoSQL/PySpark database interaction remained the weakest task category, with Qwen-4B scoring only 3/5
- Fine-tuning is needed for dairy-specific terminology comprehension and acronym resolution

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A single supervisory agent can classify farmer queries and route them to specialized subagents, enabling unified access to heterogeneous data sources through natural language.
- Mechanism: The supervisor interprets query intent, classifies it into one of five discrete categories (literature/web search, SQL, NoSQL, predictive modeling, integrity validation), and activates the corresponding subagent with appropriate tool access.
- Core assumption: Farm queries map cleanly to discrete task types; users phrase questions in ways the classifier can disambiguate.
- Evidence anchors:
  - [abstract]: "coordinated by a single supervisory agent"
  - [section]: "The system is governed by a supervisor agent, whose primary role is to interpret incoming farmers' questions, classify questions, and select the appropriate subagent. The possible routing options are restricted to the following sub-agents..."
  - [corpus]: "Small Language Models are the Future of Agentic AI" (arxiv 2506.02153) supports specialization patterns for efficient task execution.
- Break condition: Queries requiring multi-step reasoning across agents; terminology variations ("milking cow" vs. "milk cow") that bypass correct routing.

### Mechanism 2
- Claim: A single shared SLM can drive multiple specialized agents through task-specific prompt engineering rather than requiring separate fine-tuned models per agent.
- Mechanism: Each subagent receives tailored system prompts specifying output formats, rules, and tool constraints. The shared SLM generates tool calls (SQL, PySpark, search queries) based on these prompts without model-level specialization.
- Core assumption: The SLM has sufficient instruction-following capability to be guided by prompts alone across diverse task types.
- Evidence anchors:
  - [abstract]: "All agents operated under one shared SLM to minimize computational requirement"
  - [section]: "prompts were carefully customized to the specific requirements of each remaining model. For instance, prompts for Qwen models explicitly outlined the required rules and output formats."
  - [corpus]: Limited direct corpus evidence for shared-backbone multi-agent architectures in agricultural AI; related work focuses on single-task SLMs.
- Break condition: Models below ~2B parameters cannot reliably follow instructions; "thinking" models (e.g., Qwen-8B) interleave reasoning tokens with code, breaking execution pipelines.

### Mechanism 3
- Claim: Quantization enables deployment of larger models in memory-constrained environments, but performance degrades on hardware lacking optimized low-bit arithmetic support.
- Mechanism: Quantization reduces memory footprint (4-bit or 8-bit weights) but introduces numerical precision loss, slower kernel execution, and dequantization overhead—particularly harmful for reasoning tasks requiring high-fidelity internal representations.
- Core assumption: The inference hardware provides tensor cores or equivalent acceleration for quantized operations.
- Evidence anchors:
  - [section]: "experiments in this work were conducted on an NVIDIA T4 GPU...with limited memory bandwidth and no dedicated hardware support for 4-bit arithmetic. Under these conditions, quantized models are more susceptible to numerical precision loss, slower kernel execution, and frequent dequantization overhead."
  - [section]: Qwen-4B (unquantized) achieved 27/30 vs. 4-bit quantized Qwen-8B at 12/30.
  - [corpus]: No directly comparable corpus evidence on quantization effects in agricultural SLM deployment.
- Break condition: GPUs without tensor cores (T4 vs. A100); tasks requiring precise multi-step reasoning.

## Foundational Learning

- Concept: **Agentic AI architecture (supervisor-subagent pattern)**
  - Why needed here: The paper's core contribution is demonstrating this pattern works for farm decision support under compute constraints.
  - Quick check question: Can you explain why a supervisor agent is preferable to requiring farmers to manually select which tool to use?

- Concept: **Model quantization and hardware co-design**
  - Why needed here: The counter-intuitive finding that a smaller unquantized model (Qwen-4B) outperformed larger quantized models highlights hardware dependencies.
  - Quick check question: Why does 4-bit quantization harm "thinking" models more than conversational models on a T4 GPU?

- Concept: **Retrieval-Augmented Generation (RAG) limitations**
  - Why needed here: The paper notes native RAG struggles with quantitative synthesis; understanding this guides when to use alternative approaches.
  - Quick check question: Why might a RAG system successfully retrieve relevant documents but still fail to answer "How many computer vision frameworks have been applied in dairy science?"

## Architecture Onboarding

- Component map: User query → Supervisor classification → Subagent selection → Tool invocation → Code generation → Execution → Response synthesis
- Critical path: User query → Supervisor classification → Subagent selection → Tool invocation → Code generation → Execution → Response synthesis
- Design tradeoffs:
  - Shared SLM vs. per-agent models: Shared backbone reduces VRAM but requires careful prompt engineering per model
  - Quantization: Enables larger models on T4 (16GB VRAM) but degrades reasoning; unquantized Qwen-4B outperformed 8-bit Mistral-7B and 4-bit Qwen-8B
  - Thinking vs. conversational models: Reasoning-enabled models (Qwen-8B) produced intertwined code/reasoning tokens, breaking execution
- Failure signatures:
  - NoSQL/PySpark: Consistently lowest scores (Qwen-4B: 3/5, others: 1/5)—likely due to limited PySpark training data in SLMs
  - Terminology sensitivity: "milking cow" vs. "milk cow" produced different routing/results; acronym expansion (RFI vs. "residual feed intake") failed even with correct retrieval
  - Quantized thinking models: 4-bit Qwen-8B showed worst overall performance (12/30) and highest latency
- First 3 experiments:
  1. **Routing accuracy baseline**: Run 30 benchmark questions through supervisor; measure classification accuracy per category. Target: >90% correct routing.
  2. **Code execution success rate**: Compare SQL vs. PySpark code generation and execution success. Hypothesis: SQL will significantly outperform PySpark based on paper findings.
  3. **Latency profiling by subagent**: Measure end-to-end response times per subagent type on T4 hardware. Establish SLA thresholds for real-time farm use.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can fine-tuning SLMs with dairy-specific corpora (government reports, extension publications, farm management documentation) significantly improve semantic comprehension of domain terminology and acronym resolution?
- Basis in paper: [explicit] "Fine-tuning is still needed to further refine SLM performance in dairy-specific questions" and "the SLMs still requires fine-tuning with dairy-specific data to fully understand the specialized language, terminology, and practices in the field."
- Why unresolved: The study demonstrated that subtle terminology variations (e.g., "milking cow" vs. "milk cow") and acronym associations (RFI ↔ residual feed intake) caused failures even when retrieval provided correct context. The semantic comprehension layer remains limited without domain adaptation.
- What evidence would resolve it: Pre/post fine-tuning comparison showing improved accuracy on dairy-specific terminology tests, particularly acronym resolution and synonym recognition tasks.

### Open Question 2
- Question: What architectural modifications or specialized training would enable SLMs to reliably generate syntactically correct and semantically meaningful PySpark code for NoSQL database operations?
- Basis in paper: [explicit] "PySpark-based code generation and execution process, when interacting with JSON-format NoSQL databases, remains limited and unstable. This instability likely stems from the scarcity of training data involving PySpark programming and NoSQL database operations."
- Why unresolved: Qwen-4B achieved only 3/5 on NoSQL tasks (lowest category), and LLaMA-70B succeeded with identical prompts, suggesting the limitation is model-scale related rather than prompt-engineering solvable.
- What evidence would resolve it: A fine-tuned SLM achieving ≥80% accuracy on NoSQL query tasks, or identification of minimum parameter count/architecture features required for stable PySpark code generation.

### Open Question 3
- Question: Can lightweight video and audio models be integrated into edge-deployable SLM-based agentic systems without exceeding the computational constraints of typical on-farm hardware?
- Basis in paper: [inferred] "Integrating such functionality would substantially raise hardware and energy costs, which may limit affordability and adoption by farmers. The trade-off between video-processing capability and cost-effectiveness therefore remains a key design consideration for future iterations."
- Why unresolved: Video processing pipelines demand significantly greater computational resources, yet are increasingly essential for welfare monitoring, locomotion scoring, and health detection. Model pruning, quantization, and knowledge distillation are proposed but untested in this context.
- What evidence would resolve it: A working prototype demonstrating real-time video/audio analysis integrated with the multi-agent system, operating within comparable hardware constraints (≤16GB VRAM) with acceptable latency.

## Limitations
- Sample size of evaluated models limited to 3 after screening, reducing generalizability across SLM landscape
- Benchmark questions may not capture full complexity of real-world farm decision scenarios
- Study focuses exclusively on dairy farming context, limiting applicability to other agricultural domains
- Hardware constraints (NVIDIA T4 GPU) may not represent modern edge deployment scenarios with more efficient hardware
- Does not address long-term model drift or adaptation to evolving farm practices and terminology

## Confidence
- **High Confidence**: Qwen-4B's superior performance (27/30 correct) over quantized alternatives on the tested benchmark is well-supported by direct experimental evidence
- **Medium Confidence**: The broader claim that SLMs can effectively support farm data integration and decision-making while preserving privacy is supported but relies on the specific architecture and benchmark used
- **Low Confidence**: Claims about real-time farm deployment feasibility and long-term maintenance requirements extend beyond the study's scope

## Next Checks
1. **Generalization Testing**: Evaluate the same SLM models on benchmark questions from other agricultural domains (e.g., crop management, poultry farming) to assess domain transferability of the architecture and performance patterns observed in dairy contexts.

2. **Hardware Benchmarking**: Repeat the evaluation on modern edge hardware (e.g., NVIDIA Jetson Orin, Apple Silicon) to determine if quantization benefits emerge on hardware with native low-bit arithmetic support, and establish performance-per-watt baselines for farm deployment.

3. **Fine-tuning Impact Assessment**: Conduct controlled experiments fine-tuning the top-performing SLM (Qwen-4B) on dairy-specific terminology and NoSQL/PySpark patterns, then re-evaluate across all task categories to quantify performance gains and identify remaining bottlenecks.