---
ver: rpa2
title: 'Shared Path: Unraveling Memorization in Multilingual LLMs through Language
  Similarities'
arxiv_id: '2505.15722'
source_url: https://arxiv.org/abs/2505.15722
tags:
- memorization
- language
- extra
- languages
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces the first large-scale study of memorization\
  \ in multilingual large language models (MLLMs), examining 95 languages across diverse\
  \ model scales, architectures, and memorization definitions. The study challenges\
  \ the prevailing assumption that memorization is highly correlated with training\
  \ data availability, showing that among similar languages, those with fewer training\
  \ tokens often exhibit higher memorization\u2014a trend only evident when cross-lingual\
  \ relationships are explicitly modeled."
---

# Shared Path: Unraveling Memorization in Multilingual LLMs through Language Similarities

## Quick Facts
- **arXiv ID:** 2505.15722
- **Source URL:** https://arxiv.org/abs/2505.15722
- **Reference count:** 33
- **Primary result:** Cross-lingual relationships, not training data volume, best predict memorization patterns in multilingual LLMs

## Executive Summary
This work presents the first large-scale empirical study of memorization across 95 languages in multilingual language models. The authors challenge the prevailing assumption that memorization correlates strongly with training data availability by demonstrating that among similar languages, those with fewer training tokens often exhibit higher memorization rates. This counterintuitive pattern only emerges when cross-lingual relationships are explicitly modeled, revealing that memorization behaviors in multilingual models are deeply interconnected through linguistic similarity rather than isolated to individual languages. The study spans diverse model scales and architectures, showing consistent patterns that suggest cross-lingual transferability fundamentally shapes memorization vulnerabilities.

The research introduces a novel graph-based correlation metric that incorporates language similarity to capture these interconnected memorization behaviors. This approach reveals that memorization patterns form a "shared path" across related languages, where high memorization in one language often predicts similar patterns in linguistically proximate languages regardless of their individual training volumes. These findings have significant implications for how we evaluate and mitigate memorization risks in multilingual models, suggesting that language-aware approaches are essential rather than focusing solely on data quantity metrics.

## Method Summary
The study conducts a large-scale empirical analysis of memorization across 95 languages using multiple multilingual language models of varying scales and architectures. The authors systematically evaluate memorization by examining the overlap between model outputs and training data across different language pairs. They introduce a novel graph-based correlation metric that explicitly incorporates language similarity measures to capture cross-lingual memorization patterns. The methodology involves computing memorization scores for each language, then analyzing how these scores correlate across language families while accounting for linguistic distances. This approach enables the identification of interconnected memorization behaviors that traditional monolingual analyses would miss. The study examines both decoder-only and encoder-decoder architectures to assess whether the observed patterns are architecture-specific or represent fundamental properties of multilingual model training.

## Key Results
- Memorization is more strongly predicted by cross-lingual similarity than by training data quantity
- Languages with fewer training tokens often show higher memorization when they are linguistically similar to high-memorization languages
- The graph-based correlation metric reveals interconnected memorization patterns across related languages that traditional analyses miss
- Findings are consistent across both decoder-only and encoder-decoder model architectures

## Why This Works (Mechanism)
The cross-lingual transferability of learned representations in multilingual models creates pathways for memorization patterns to propagate across similar languages. When a model memorizes content in one language, the shared linguistic structures and semantic mappings between related languages enable this memorization to influence outputs in other languages, regardless of their individual training data volumes. This mechanism suggests that memorization vulnerabilities in multilingual models are fundamentally interconnected rather than language-specific.

## Foundational Learning
- **Language similarity metrics** - Quantifying linguistic relationships between languages is essential for understanding cross-lingual transfer; quick check: verify similarity measures capture typological and genealogical relationships
- **Graph-based correlation analysis** - Modeling memorization as interconnected patterns across a language similarity graph; quick check: validate that graph structure meaningfully captures observed memorization correlations
- **Cross-lingual transferability** - Understanding how knowledge transfers between languages in multilingual models; quick check: measure zero-shot performance across language pairs to confirm transferability exists
- **Memorization detection methodologies** - Accurately identifying when models reproduce training data; quick check: compare multiple memorization detection approaches for consistency
- **Multilingual corpus composition** - Understanding how training data distribution affects model behavior; quick check: analyze token counts and data quality across languages
- **Architectural differences in multilingual models** - How decoder-only vs encoder-decoder designs affect cross-lingual phenomena; quick check: compare memorization patterns across architectures with similar training data

## Architecture Onboarding

**Component Map:** Input languages -> Embedding layer -> Transformer blocks -> Output layer -> Token prediction -> Memorization detection

**Critical Path:** Training data → Language embeddings → Cross-lingual attention patterns → Output generation → Memorization vulnerability

**Design Tradeoffs:** The study uses existing pretrained models rather than training from scratch, prioritizing breadth across languages and architectures over fine-grained control over training dynamics. This enables large-scale analysis but limits the ability to isolate specific training factors.

**Failure Signatures:** Memorization patterns that contradict training data volume expectations, particularly high memorization in low-resource languages that are linguistically similar to high-memorization languages. Models showing isolated memorization patterns rather than interconnected ones across language families.

**First Experiments:**
1. Compute baseline memorization scores for all 95 languages independently
2. Calculate pairwise language similarity metrics using established linguistic databases
3. Apply the graph-based correlation metric to identify interconnected memorization patterns

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Findings are limited to memorization and may not generalize to other linguistic capabilities
- The graph-based correlation metric requires validation beyond memorization detection
- Code-mixed training data is not quantified or explicitly analyzed for its impact on cross-lingual patterns
- Analysis is limited to decoder-only and encoder-decoder architectures, excluding other model types

## Confidence
**High confidence** in the core finding that cross-lingual similarity better predicts memorization than training volume, supported by large-scale empirical evidence across multiple architectures.

**Medium confidence** in the generalizability of the graph-based correlation approach beyond memorization detection.

**Low confidence** in conclusions about other model architectures not tested in the study.

## Next Checks
1. Conduct controlled experiments isolating the impact of code-mixed training data on cross-lingual memorization patterns, particularly examining whether mixed-language content strengthens or weakens the observed similarity-based correlations.

2. Extend the graph-based correlation methodology to predict other cross-lingual phenomena (e.g., zero-shot translation quality, cross-lingual alignment) to validate whether the approach generalizes beyond memorization.

3. Test the memorization-similarity relationship on additional model architectures including encoder-only models and specialized multilingual architectures to establish whether the observed patterns are architecture-agnostic.