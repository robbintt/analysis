---
ver: rpa2
title: Large Scale Retrieval for the LinkedIn Feed using Causal Language Models
arxiv_id: '2510.14223'
source_url: https://arxiv.org/abs/2510.14223
tags:
- member
- retrieval
- item
- embeddings
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper describes a retrieval system for LinkedIn\u2019s feed\
  \ that uses a fine-tuned causal language model to generate high-quality embeddings\
  \ for both members and content, replacing a multi-source retrieval pipeline. The\
  \ system uses a dual-encoder architecture with LLaMA 3 and optimizes for engagement-based\
  \ relevance."
---

# Large Scale Retrieval for the LinkedIn Feed using Causal Language Models

## Quick Facts
- arXiv ID: 2510.14223
- Source URL: https://arxiv.org/abs/2510.14223
- Reference count: 9
- Primary result: +0.8% revenue and +0.2% daily unique professional interactions in online A/B test

## Executive Summary
This paper describes a unified retrieval system for LinkedIn's feed that replaces a complex multi-source pipeline with a single dual-encoder architecture using a fine-tuned causal language model. The system generates high-quality embeddings for both members and content using LLaMA-3 3B, optimized for engagement-based relevance. By consolidating retrieval into one index and incorporating quantized numerical features in prompts, the system achieves sub-50ms latency while improving recall and engagement metrics. An online A/B test demonstrated significant improvements in revenue and daily unique professional interactions, particularly among newer members.

## Method Summary
The system uses LLaMA-3 3B as a shared dual-encoder to generate member and post embeddings from concatenated feature vectors in prompts. Member prompts include profile information and truncated positive engagement history, while post prompts contain metadata and text content. The model is trained with InfoNCE loss using in-batch negatives plus two hard negatives per member, with mean pooling over all tokens. A key innovation was quantizing numerical features (popularity counts) to percentage buckets (1-100) to improve feature encoding. The system employs matryoshka representation learning for flexible dimensionality and achieves 2000-candidate retrieval from hundreds of millions of items in sub-50ms at several thousand QPS.

## Key Results
- Improved Recall@10 with InfoNCE loss and matryoshka embeddings
- Minimal performance drop when reducing embedding dimensionality
- +0.8% increase in revenue and +0.2% increase in daily unique professional interactions in online A/B test
- Larger gains observed among newer members

## Why This Works (Mechanism)
The system works by creating dense, semantically meaningful embeddings that capture both member preferences and content characteristics in a unified space. The fine-tuned causal language model learns to map heterogeneous features (profile data, engagement history, content metadata) into a shared embedding space where similarity indicates relevance. Quantizing numerical features to percentages enables the model to better encode ordinal relationships and relative popularity, improving the correlation between popularity and similarity scores. The InfoNCE loss with hard negatives forces the model to distinguish between genuinely relevant items and those that are merely similar in surface features.

## Foundational Learning

**Dual-Encoder Architecture**: Two separate neural networks that encode queries and documents into the same embedding space. Needed because it enables efficient similarity search at scale. Quick check: Cosine similarity between embeddings should correlate with relevance.

**InfoNCE Loss**: Noise-Contrastive Estimation loss that distinguishes positive pairs from negative samples. Needed to train embeddings where similar items are close and dissimilar items are far. Quick check: Loss should decrease as positive pairs become more similar than negatives.

**Matryoshka Representation Learning**: Technique for training embeddings at multiple dimensionalities simultaneously. Needed to enable flexible deployment with different computational budgets. Quick check: Performance should degrade gracefully as dimensionality decreases.

**Prompt Engineering for Retrieval**: Designing structured prompts that combine heterogeneous features into text format. Needed to leverage pre-trained LMs for structured data. Quick check: Feature correlations in embedding space should match domain knowledge.

## Architecture Onboarding

**Component Map**: Member features -> Prompt template -> Member encoder -> Member embedding; Post features -> Prompt template -> Post encoder -> Post embedding; Embeddings -> FAISS index -> Top-k retrieval -> Ranking model

**Critical Path**: Prompt generation → Dual-encoder inference → FAISS similarity search → Top-k candidate retrieval → Final ranking

**Design Tradeoffs**: Unified dual-encoder vs. separate models (simpler deployment vs. specialized optimization), quantized features vs. raw counts (better encoding vs. information loss), mean pooling vs. last-token pooling (robust aggregation vs. context sensitivity)

**Failure Signatures**: Low recall with raw count features, performance drops with last-token pooling, poor cold-start performance for new members

**First Experiments**:
1. Compare quantized vs. raw numerical feature encoding in prompts
2. Evaluate mean pooling vs. last-token pooling impact on recall
3. Measure retrieval performance stratified by member tenure

## Open Questions the Paper Calls Out
None

## Limitations
- Relies on proprietary engagement data and member features, limiting reproducibility
- Evaluation methodology using ranking model as "oracle" may overestimate true retrieval effectiveness
- Performance on cold-start scenarios lacks detailed analysis of specific feature importance

## Confidence

**High Confidence**: General architectural approach and reported offline improvements in Recall@10 are well-established and plausible.

**Medium Confidence**: Specific quantitative results (0.8% revenue increase, 0.2% DPU increase) are difficult to fully validate without exact implementation details.

**Low Confidence**: Claims about quantized numerical features' mechanism are based on observed correlations rather than causal analysis.

## Next Checks

1. Implement and test quantized vs. raw numerical feature encoding in prompts using a public dataset to verify claimed improvements.

2. Conduct ablation studies comparing mean pooling vs. last-token pooling on LinkedIn dataset to confirm 5-12% Recall@10 improvement.

3. Perform stratified offline evaluation focusing on new member cohorts to validate performance gains and identify feature importance.