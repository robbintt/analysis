---
ver: rpa2
title: Empirical Likelihood for Random Forests and Ensembles
arxiv_id: '2511.13934'
source_url: https://arxiv.org/abs/2511.13934
tags:
- random
- forests
- jackknife
- empirical
- likelihood
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops an empirical likelihood (EL) framework for
  random forests and related ensemble methods, enabling likelihood-based inference
  for statistical uncertainty. The core idea is to construct an EL statistic using
  jackknife pseudo-values derived from the incomplete U-statistic structure inherent
  in ensemble predictions.
---

# Empirical Likelihood for Random Forests and Ensembles

## Quick Facts
- **arXiv ID:** 2511.13934
- **Source URL:** https://arxiv.org/abs/2511.13934
- **Reference count:** 11
- **Primary result:** Empirical likelihood (EL) framework for random forests using jackknife pseudo-values, with modified EL restoring asymptotic pivotality under sparse subsampling.

## Executive Summary
This paper develops an empirical likelihood framework for random forests and related ensemble methods, enabling likelihood-based inference for statistical uncertainty. The core idea is to construct an EL statistic using jackknife pseudo-values derived from the incomplete U-statistic structure inherent in ensemble predictions. Under dense-subsampling asymptotics, the proposed EL procedure yields asymptotically pivotal inference, but tends to over-cover under sparser regimes due to loss of pivotality. To address this, a modified EL statistic is introduced that restores asymptotic pivotality in both dense- and sparse-subsampling settings. Theoretical results are established under general assumptions and specialized to honest random forests. Simulations demonstrate that the modified EL achieves accurate coverage and practical reliability relative to existing methods like the infinitesimal jackknife.

## Method Summary
The method treats random forest predictions as incomplete generalized U-statistics and constructs jackknife pseudo-values via jackknife-after-subsampling. These pseudo-values form estimating equations for the empirical likelihood function. A modified version adjusts for Efron-Stein bias in the jackknife-after-subsampling variance, restoring asymptotic pivotality under both dense and sparse subsampling regimes. The approach is computationally efficient and retains key properties of conventional EL, such as range preservation and data-driven confidence region shape.

## Key Results
- The EL statistic converges to chi-squared under dense-subsampling but over-covers under sparse regimes
- Modified EL statistic restores asymptotic pivotality in both subsampling regimes
- Simulation studies show mEL achieves ~95% coverage while basic EL over-covers in sparse settings
- mEL outperforms infinitesimal jackknife in coverage accuracy across tested scenarios

## Why This Works (Mechanism)

### Mechanism 1
Random forest predictions can be treated as incomplete generalized U-statistics, enabling jackknife-based empirical likelihood inference. The ensemble prediction is an average over a randomly selected subset of all possible subsample-based trees rather than the full combinatorial collection. By constructing jackknife pseudo-values via jackknife-after-subsampling—computing leave-one-out analogues using only the already-selected trees—one can form an EL function without computing the complete U-statistic. This incompleteness is essential to the computational scheme.

### Mechanism 2
Under dense-subsampling asymptotics (N_n grows sufficiently fast relative to n·ζ_{n,s}/(s·ζ_{n,1})), the EL statistic ℓ(θ_n) converges to χ²_1. Dense-subsampling ensures that the incompleteness term Ũ_n in Hoeffding's decomposition is asymptotically negligible. The jackknife pseudo-values then satisfy a CLT and their second moment converges, yielding the standard EL asymptotic result. This pivotality is lost if N_n grows slowly (sparse-subsampling).

### Mechanism 3
The modified EL statistic ̃ℓ(θ) restores asymptotic pivotality under both dense- and sparse-subsampling regimes. The modification adjusts jackknife pseudo-values to correct for Efron-Stein-type bias in the jackknife-after-subsampling variance, which overestimates the highest-order term by factor (s−1). Under sparse-subsampling, this inflation becomes non-negligible. The modified pseudo-values incorporate an additional variance component V̂_2 = n(s−1)/B_n · ζ̂_{n,s} to internalize the incompleteness contribution.

## Foundational Learning

- **U-statistics and Hoeffding decomposition**
  - Why needed here: Random forests are recast as generalized incomplete U-statistics; the decomposition into projection terms is central to the asymptotic analysis.
  - Quick check question: Can you explain why an incomplete U-statistic differs from a complete one in terms of both computation and variance?

- **Empirical likelihood (Owen, 1988)**
  - Why needed here: The paper builds a likelihood-based inference framework by constructing an EL function from jackknife pseudo-values as estimating equations.
  - Quick check question: What constraint defines the EL optimization problem, and why does the dual form involve only a scalar Lagrange multiplier?

- **Jackknife pseudo-values and bias**
  - Why needed here: Pseudo-values are the building blocks for the EL function; understanding their variance properties (and Efron-Stein bias) is essential for the modification.
  - Quick check question: How does the jackknife-after-subsampling scheme differ from standard leave-one-out jackknife, and what computational advantage does it offer?

## Architecture Onboarding

- **Component map:**
  Tree training -> Index tracking (which observations each tree uses) -> Pseudo-value construction -> Lagrange multiplier solving -> Confidence region via χ²_1 quantile

- **Critical path:**
  1. Tree training → 2. Index tracking (which observations each tree uses) → 3. Pseudo-value construction → 4. Lagrange multiplier solving → 5. Confidence region via χ²_1 quantile

- **Design tradeoffs:**
  - Dense vs. sparse subsampling: More trees (larger N_n) enables standard EL but increases computation; sparse regimes require modification
  - Honesty condition: Enables theoretical guarantees but may reduce predictive flexibility; double-sample trees are one implementation
  - EL vs. Wald-type intervals: EL provides range preservation and data-driven shapes but requires solving nonlinear optimization per candidate θ

- **Failure signatures:**
  - Over-coverage: Using unmodified EL under sparse-subsampling; solution is mEL
  - Non-convergence of λ solver: Pseudo-values too extreme or collinear; check moment bounds and subsample size
  - Bias-dominated predictions: If s grows too slowly relative to n, bias may not vanish; verify s/n → 0

- **First 3 experiments:**
  1. Reproduce simulation coverage plots: Implement EL and mEL on the MLR and MARS DGPs with n∈{200,400,800}, varying α∈{1.1,1.2}; verify mEL achieves ~95% coverage while EL over-covers in sparse regime
  2. Sensitivity to N_n: Fix n=400, vary expected tree count N_n from 500 to 5000; plot coverage vs. N_n to observe the dense/sparse transition
  3. Honest vs. standard forests: Compare coverage using honest trees (honesty=TRUE) vs. Breiman's original RF; quantify the theoretical-empirical gap when honesty is violated

## Open Questions the Paper Calls Out

### Open Question 1
Can the asymptotic pivotality of the modified empirical likelihood (mEL) statistic be established for standard adaptive random forests that violate the honesty condition? The theoretical guarantees rely on specific bias bounds and variance ratios derived for honest trees to satisfy Assumptions 1-3; adaptive trees utilize response variables for splitting, potentially altering the variance ratio ζ_{n,s}/ζ_{n,1} and invalidating the current proofs.

### Open Question 2
How robust is the coverage of the modified EL confidence region when the random forest prediction bias is large relative to its variance? The paper constructs confidence regions for the expected tree prediction θ_n, but in finite samples with complex data structures, the gap between θ_n (estimand) and θ_0 (true conditional expectation) may be non-negligible, potentially leading to under-coverage for the true parameter.

### Open Question 3
Can this EL framework be extended to provide uniform confidence bands for the entire regression function rather than pointwise inference? The theoretical results are derived for a scalar parameter θ_n at a fixed evaluation point x_0, without addressing simultaneous inference across the feature space. Extending pointwise empirical likelihood to uniform bands requires establishing the weak convergence of the entire empirical likelihood process as a function of x.

## Limitations
- Theoretical guarantees rely heavily on dense-subsampling asymptotics and kernel moment conditions
- Empirical validation is limited to two synthetic DGPs and a single real dataset
- Computational burden scales linearly with N_n, potentially limiting scalability
- The modification requires accurate variance estimation, which can be unstable with small B_n

## Confidence

- Dense-subsampling pivotality (Mechanism 2): High — rigorous proof under stated assumptions with simulation support
- Modified EL pivotality (Mechanism 3): Medium — theoretical proof given, but empirical validation is limited and variance estimation is a potential weak point
- Honest RF necessity: Medium — theoretical assumptions require honesty, but practical necessity for mEL performance is not fully demonstrated

## Next Checks
1. Test mEL on high-dimensional (d > 20) and heavy-tailed noise DGPs to assess robustness beyond the current six-dimensional, Gaussian-noise setting
2. Evaluate coverage under non-honest forests (honesty=FALSE) with the same n and N_n to quantify the impact of assumption violations
3. Implement a real-time variant of mEL that updates confidence regions incrementally as new trees are added, to assess practical scalability