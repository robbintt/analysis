---
ver: rpa2
title: MOSLIM:Align with diverse preferences in prompts through reward classification
arxiv_id: '2505.20336'
source_url: https://arxiv.org/abs/2505.20336
tags:
- preference
- reward
- policy
- moslim
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MOSLIM addresses the challenge of multi-objective alignment in
  large language models by introducing a novel framework that uses a single reward
  model and policy model to achieve diverse preference alignment through prompting.
  The core idea involves a multi-head reward model that classifies question-answer
  pairs into preference categories and intensities, combined with a reward mapping
  function that converts classification results into scalar rewards for policy optimization.
---

# MOSLIM:Align with diverse preferences in prompts through reward classification

## Quick Facts
- arXiv ID: 2505.20336
- Source URL: https://arxiv.org/abs/2505.20336
- Authors: Yu Zhang; Wanli Jiang; Zhengyu Yang
- Reference count: 14
- Key outcome: Single reward model + single policy model achieves multi-objective preference alignment via prompting, eliminating preference-specific SFT and enabling flexible control.

## Executive Summary
MOSLIM addresses the challenge of multi-objective alignment in large language models by introducing a novel framework that uses a single reward model and policy model to achieve diverse preference alignment through prompting. The core idea involves a multi-head reward model that classifies question-answer pairs into preference categories and intensities, combined with a reward mapping function that converts classification results into scalar rewards for policy optimization. This approach eliminates the need for preference-specific supervised fine-tuning and enables flexible control of different preference combinations during inference. Experiments demonstrate that MOSLIM outperforms existing methods across multiple benchmarks, achieving up to 57% improvement in helpfulness scores compared to MORLHF on the most challenging dataset, while requiring significantly fewer GPU computing resources (164 GPU hours vs 196-400 hours for baselines).

## Method Summary
MOSLIM trains a multi-head reward model that classifies question-answer pairs into discrete preference categories and intensities using cross-entropy loss per head. During training, the model records moving averages and standard deviations per preference dimension. A Gaussian-normalized reward mapping function converts classification outputs into scalar rewards by z-score normalizing each dimension and averaging with masking for unspecified dimensions. The policy model, initialized from SFT, is optimized using preference prefixes in prompts (e.g., "<helpfulness 5>") and learns to respond to these prefixes through RL optimization without requiring preference-specific SFT. At inference, users can control multiple preference dimensions and intensities by simply modifying prompt prefixes.

## Key Results
- MOSLIM achieves 57% improvement in helpfulness scores over MORLHF on the most challenging dataset
- Requires only 164 GPU hours for training versus 196-400 GPU hours for baseline methods
- Demonstrates effective controllability over preference intensities and dimensions with performance gains scaling with reward model size

## Why This Works (Mechanism)

### Mechanism 1: Classification-Based Reward Modeling
The reward model outputs discrete class probabilities per preference dimension rather than a single scalar, using cross-entropy loss per head. This preserves nuanced preference information that majority-voting approaches lose, as scalar regression can obscure minority preferences through averaging.

### Mechanism 2: Gaussian-Normalized Reward Mapping
During training, moving average and standard deviation are recorded per preference dimension. At inference, each dimension's target intensity is normalized via z-score transformation, then averaged with masking for unspecified dimensions. This enables meaningful aggregation across heterogeneous preference dimensions with different intensity scales.

### Mechanism 3: Prompt-Conditioned Policy Learning Without Preference-Specific SFT
The policy learns to respond to preference prefixes through RL optimization alone. During training, prompts carry preference prefixes, but these are stripped before reward model evaluation. The policy learns to map prefix context to aligned behavior via reward signal, enabling generalization to novel preference combinations at inference.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: MOSLIM builds on PPO, RLOO, and Online-DPO frameworks; understanding KL-penalized policy optimization and reward modeling is prerequisite.
  - Quick check question: Why does the KL divergence penalty prevent reward hacking in policy optimization?

- Concept: Multi-Objective Optimization
  - Why needed here: MOSLIM addresses trade-offs among conflicting preferences (helpfulness, honesty, harmlessness).
  - Quick check question: Why can't all preference objectives be simultaneously maximized in multi-objective alignment?

- Concept: Reward Modeling Paradigms (Scalar vs. Distributional vs. Classification)
  - Why needed here: Understanding the information loss in scalar scoring versus classification-based approaches is central to MOSLIM's design rationale.
  - Quick check question: What information does a scalar reward score obscure compared to a classification distribution over intensities?

## Architecture Onboarding

- Component map: Q&A pairs -> Multi-head Reward Model (cross-entropy loss) -> Reward Mapping (z-score normalization) -> Policy Model (PPO/RLOO/Online-DPO) -> Preference-conditioned outputs
- Critical path: 1) Train multi-head reward model on labeled preference data using cross-entropy loss per head; 2) Record running statistics per dimension during training; 3) Initialize policy from SFT model; 4) Run policy optimization using reward mapping; 5) At inference, inject preference prefix to control output
- Design tradeoffs: DataType granularity (DataType 4 offers more control but lower accuracy ~38%; DataType 1 achieves ~97% accuracy); reward model size (larger models improve accuracy but increase GPU hours); policy optimization algorithm (PPO stable, RLOO faster, Online-DPO best empirical performance)
- Failure signatures: Reward model intensity accuracy <50% indicates unreliable reward signal; KL divergence spiking during training suggests excessive policy deviation; controllability gap where increasing intensity doesn't produce higher preference scores indicates failed conditioning
- First 3 experiments: 1) Validate reward model classification accuracy across DataTypes (1-4) and model sizes (7B, 57B, 72B); 2) Compare MOSLIM vs. baselines (MORLHF, RSoups, RiC) on MT-Bench, HaluEval 2.0, and Hackaprompt benchmarks; 3) Ablate controllability by sweeping intensity n (1-5) per dimension and verifying monotonic score increases

## Open Questions the Paper Calls Out

- Open Question 1: How does MOSLIM's performance scale when applied to a significantly larger number of preference dimensions beyond the three tested (helpfulness, harmlessness, honesty)?
- Open Question 2: Can the reward model's accuracy on fine-grained intensity classifications (DataType 4) be improved without sacrificing the computational efficiency advantages of MOSLIM?
- Open Question 3: Is the Gaussian-based reward mapping function optimal, or would alternative mapping formulations yield better policy optimization performance?
- Open Question 4: How does MOSLIM handle inherently conflicting preference specifications where satisfying one dimension necessarily degrades another?

## Limitations
- Limited ablation of reward mapping function robustness across preference dimensions, with no validation for non-linear or incompatible preference trade-offs
- Claims about information preservation through classification versus scalar scoring lack direct experimental validation against distributional reward modeling approaches
- Efficiency claims based on single experimental comparison without systematic ablation across different dataset sizes or model scales

## Confidence

- High confidence: Core framework design and implementation details are clearly specified with well-defined components (multi-head RM, reward mapping, prompt-conditioned policy)
- Medium confidence: Empirical results show consistent improvements over baselines, but limited cross-dataset validation and ablation studies reduce generalizability claims
- Low confidence: Claims about information preservation through classification versus scalar scoring lack direct experimental validation

## Next Checks

1. Validate reward mapping robustness by testing non-additive preference combinations (e.g., extreme helpfulness vs. extreme harmlessness) to confirm normalization doesn't mask incompatibility
2. Conduct systematic resource consumption analysis across different dataset sizes and model scales to confirm efficiency claims hold beyond the reported comparison
3. Implement ablation studies comparing MOSLIM's classification-based approach against distributional reward modeling to empirically validate information preservation claims