---
ver: rpa2
title: Extracting Actionable Insights from Building Energy Data using Vision LLMs
  on Wavelet and 3D Recurrence Representations
arxiv_id: '2509.21934'
source_url: https://arxiv.org/abs/2509.21934
tags:
- energy
- data
- loss
- time
- building
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework that transforms 1D building energy
  time series into 3D visual representations (continuous wavelet transforms and recurrence
  plots) for interpretation by vision-language large models (VLLMs). By converting
  temporal energy data into visual encodings, the framework enables VLLMs to visually
  detect anomalies, monitor consumption patterns, and generate natural language optimization
  recommendations.
---

# Extracting Actionable Insights from Building Energy Data using Vision LLMs on Wavelet and 3D Recurrence Representations

## Quick Facts
- arXiv ID: 2509.21934
- Source URL: https://arxiv.org/abs/2509.21934
- Reference count: 34
- Transforms 1D building energy time series into 3D visual representations for VLLM interpretation

## Executive Summary
This paper introduces a framework that transforms 1D building energy time series into 3D visual representations (continuous wavelet transforms and recurrence plots) for interpretation by vision-language large models (VLLMs). By converting temporal energy data into visual encodings, the framework enables VLLMs to visually detect anomalies, monitor consumption patterns, and generate natural language optimization recommendations. The approach is demonstrated on real-world building energy datasets, where the Idefics-7B VLLM achieves validation losses of 0.0952 with CWTs and 0.1064 with RPs, outperforming direct fine-tuning on raw time-series data (0.1176) for anomaly detection. This work bridges time-series analysis and visualization, providing a scalable and interpretable framework for actionable energy analytics in building management.

## Method Summary
The framework transforms normalized 1D energy time series into 512×512 CWT scalograms or recurrence plots, then fine-tunes a frozen vision encoder (Idefics-7B) with updated language decoder and cross-attention parameters. The VLLM processes these visual representations to generate natural language recommendations for energy optimization. Training uses AdamW optimization with gradient accumulation, achieving lower validation losses than direct time-series fine-tuning through the visual encoding approach.

## Key Results
- Idefics-7B achieves validation loss of 0.0952 with CWT representations, outperforming raw time-series baseline (0.1176)
- Recurrence plot approach yields validation loss of 0.1064, also surpassing direct time-series fine-tuning
- CWT representations enable better frequency-localized anomaly detection than raw time series

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Converting 1D time series into 3D CWT scalograms enables VLLMs to detect frequency-localized anomalies more effectively than raw time series.
- Mechanism: CWTs use scalable wavelet kernels to capture both high-frequency transient events and low-frequency trends simultaneously, producing time-frequency maps that VLLM vision encoders process as spatial patterns rather than sequential data.
- Core assumption: VLLMs pre-trained on general visual data can transfer spatial pattern recognition to domain-specific scientific visualizations without extensive domain pre-training.
- Evidence anchors: [abstract] reports 0.0952 validation loss with CWTs; [Section II.A] describes multi-resolution analysis; corpus validation limited.

### Mechanism 2
- Claim: Recurrence plots encode temporal dynamics as spatial textures that VLLMs can interpret for pattern recognition tasks.
- Mechanism: RPs reconstruct phase space from 1D time series and compute distance matrices visualizing temporal recurrences, converting temporal dependencies into 2D textures where repeating patterns manifest as diagonal structures.
- Core assumption: Visual texture patterns in RPs correspond to meaningful temporal patterns that VLLMs can learn to associate with specific energy behaviors through fine-tuning.
- Evidence anchors: [abstract] notes RPs capture temporal dynamics; [Table II] shows 0.1064 validation loss; corpus validation limited.

### Mechanism 3
- Claim: Freezing the vision encoder while fine-tuning only the language decoder and cross-attention weights preserves pre-trained visual representations while enabling domain-specific outputs.
- Mechanism: Supervised fine-tuning with frozen Θ_v and updated Θ_d/Θ_c aligns visual features with energy-domain terminology via cross-attention gradient flow, preventing catastrophic forgetting of general visual capabilities.
- Core assumption: Pre-trained vision encoders contain sufficient feature extraction capabilities for CWT/RP scientific visualizations; only linguistic mapping requires domain adaptation.
- Evidence anchors: [Section II.B.4] specifies frozen encoder strategy; [Section III.D] reports lower training-validation loss differential; corpus validation limited.

## Foundational Learning

- **Concept: Continuous Wavelet Transform (CWT)**
  - Why needed here: CWT is the primary transformation converting 1D energy signals into interpretable 3D time-frequency representations. Understanding equation (1) is essential for debugging transformation artifacts.
  - Quick check question: Can you explain why the complex Morlet wavelet (equation 2) provides optimal balance between time and frequency localization for energy anomaly detection?

- **Concept: Vision Transformer (ViT) Patch Embeddings**
  - Why needed here: The VLLM vision encoder uses ViT architecture to process CWT/RP images as patch sequences. Understanding equation (4) clarifies how 512×512 scalograms become tokenized inputs.
  - Quick check question: Given a 512×512 CWT image with patch size 16×16, how many patch embeddings does the encoder produce (N = HW/P²)?

- **Concept: Cross-Modal Fusion via Cross-Attention**
  - Why needed here: Cross-attention mechanisms bridge visual features from scalograms with language tokens to generate domain-specific recommendations. Equation (6) is the fusion bottleneck.
  - Quick check question: In cross-attention, why are queries derived from language decoder states (H^l-1) while keys and values come from visual features (V)?

## Architecture Onboarding

- **Component map**: Data Transformation Layer -> Vision Encoder -> Cross-Modal Fusion -> Language Decoder -> Training Controller

- **Critical path**:
  1. Normalize 1D energy time series to [0,1] using min-max scaling (24-hour windows)
  2. Generate CWT scalograms or RPs at 512×512 resolution
  3. Construct prompt: `<ANALYSIS_TYPE>` ⊕ "Query: " Q ⊕ Image (equation 7)
  4. Forward through frozen vision encoder → patch embeddings V
  5. Cross-attention fusion with language decoder hidden states
  6. Compute cross-entropy loss (equation 9), backpropagate to Θ_d and Θ_c only
  7. Validate on 25% held-out split; monitor perplexity, ROUGE-L, BLEU

- **Design tradeoffs**:
  - CWT vs. RP: CWT excels at frequency-localized anomalies (validation loss 0.0952 vs. 0.1064); RPs better for temporal recurrence patterns
  - Frozen encoder vs. full fine-tuning: Freezing prevents catastrophic forgetting but may limit adaptation to scientific plot distributions
  - Model scale: Idefics-7B generalizes best but requires 8×A100 GPUs; smaller models (Florence-2-Large) show overfitting
  - Batch size vs. memory: Gradient accumulation (Γ=8) simulates batch size 48 within single-GPU memory constraints

- **Failure signatures**:
  - High train/val loss gap: Overfitting (Florence-2-Large: train 0.152 vs. val 0.187 for temperature-CWT)
  - Gradient norm spikes: Training instability (Florence-2-Large shows periodic spikes in Figure 4)
  - Early plateau: Capacity mismatch (Pixtral-7B plateaus at ~480 steps)
  - High perplexity on specific conditions: Weak feature-target correlation (wind speed PPL: 10.312 vs. humidity PPL: 7.845)

- **First 3 experiments**:
  1. **Representation ablation**: Fine-tune Idefics-7B on (a) raw time series, (b) CWT scalograms, (c) RPs using identical train/val splits; compare validation loss to confirm visual encoding advantage per abstract claims.
  2. **Freezing strategy validation**: Compare (a) full fine-tuning, (b) frozen vision encoder (paper approach), (c) frozen decoder on humidity-CWT data; monitor train/val gap and ROUGE-L scores.
  3. **Weather condition robustness test**: Evaluate fine-tuned model across humidity, temperature, and wind speed conditions; expect strong humidity performance (BLEU 0.671), moderate temperature (BLEU 0.612), weak wind speed (BLEU 0.543) per Table III.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can integrating agentic AI systems enable real-time, closed-loop energy optimization by directly enacting VLLM recommendations?
- Basis in paper: [explicit] The conclusion states, "Future work will integrate agentic AI systems to perform recommended actions... allowing closed-loop optimization of energy savings in real time without human intervention."
- Why unresolved: The current framework operates in an inference-only mode, generating natural language reports without executing control actions or integrating with building management system actuators.
- What evidence would resolve it: A deployed system where VLLM outputs directly trigger HVAC or lighting adjustments, demonstrating autonomous control and verified energy reduction.

### Open Question 2
- Question: Does fusing Continuous Wavelet Transform (CWT) and Recurrence Plot (RP) representations into a single visual input improve anomaly detection robustness?
- Basis in paper: [explicit] The discussion notes, "Future work should explore hybrid representations (CWT + RP fusion)... to further enhance robustness."
- Why unresolved: The study evaluates CWT and RP encodings separately; the potential synergies between time-frequency localization (CWT) and temporal pattern recurrence (RP) within the VLLM remain untested.
- What evidence would resolve it: Comparative benchmarks showing that a fused input format outperforms individual CWT or RP inputs on validation loss and anomaly detection metrics.

### Open Question 3
- Question: Do high text-generation scores (BLEU/ROUGE) correlate with actual improvements in energy efficiency when recommendations are deployed?
- Basis in paper: [inferred] The paper validates the framework using validation loss and text similarity metrics, but does not quantify the physical energy savings resulting from the generated recommendations.
- Why unresolved: High linguistic similarity to reference texts does not guarantee that the insights are physically optimal or that the suggested control strategies effectively reduce consumption in practice.
- What evidence would resolve it: Operational field trials measuring the change in kilowatt-hour (kWh) consumption before and after implementing the VLLM's optimization recommendations.

## Limitations

- Extremely small dataset size (27 samples per weather class) raises concerns about generalization to larger building energy datasets
- No qualitative assessment of recommendation quality or user acceptance in real building management contexts
- High computational requirements (8×A100 GPUs) may limit practical deployment for many organizations

## Confidence

**High Confidence:** The core claim that VLLMs can process 3D visual representations of time series data to generate natural language recommendations is well-supported by the experimental results and theoretical framework.

**Medium Confidence:** The assertion that CWT representations specifically enable better frequency-localized anomaly detection has strong evidence but depends heavily on the specific dataset characteristics.

**Low Confidence:** Claims about the framework's scalability and real-world applicability are weakly supported by the extremely limited dataset and lack of practical deployment evidence.

## Next Checks

1. **Dataset Generalization Test:** Reproduce the experiment on a larger, publicly available building energy dataset (e.g., ASHRAE Great Energy Predictor III) with at least 1000 samples across multiple building types to verify whether the 20% improvement over raw time-series fine-tuning persists at scale.

2. **Human Evaluation Study:** Conduct a blind study where building energy experts evaluate recommendations generated by the VLLM system versus traditional energy management software on the same anomalies, measuring practical utility and actionability scores.

3. **Computational Efficiency Analysis:** Profile memory usage and inference latency across different VLLM scales (Idefics-7B vs Florence-2-Large) on commodity hardware (single A100) to determine the practical deployment threshold and identify optimization opportunities for real-time building monitoring.