---
ver: rpa2
title: 'B-score: Detecting biases in large language models using response history'
arxiv_id: '2505.18545'
source_url: https://arxiv.org/abs/2505.18545
tags:
- b-score
- multi-turn
- single-turn
- random
- biden
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether allowing large language models (LLMs)
  to observe their own prior responses reduces biases in multi-turn conversations.
  A new metric, B-score, is proposed to detect biases by comparing the probability
  of an answer appearing in single-turn versus multi-turn settings.
---

# B-score: Detecting biases in large language models using response history

## Quick Facts
- arXiv ID: 2505.18545
- Source URL: https://arxiv.org/abs/2505.18545
- Reference count: 40
- Key outcome: Multi-turn conversations significantly reduce LLM biases in Random questions and partially in Subjective ones; B-score improves answer verification accuracy by +9.3% on proposed questions and +2.9% on standard benchmarks.

## Executive Summary
This paper introduces B-score, a metric that detects biases in LLM responses by comparing answer probabilities between single-turn and multi-turn conversations. The key insight is that allowing models to observe their own response history in multi-turn settings enables them to implicitly correct distributional biases, particularly for questions requiring randomness or factual correctness. The method achieves significant improvements in bias detection and answer verification accuracy across 8 tested LLMs.

## Method Summary
The method involves running each question N=30 times in both single-turn (independent queries with context reset) and multi-turn (consecutive turns with history) settings. For each answer option, B-score is computed as the difference between single-turn and multi-turn probabilities. A two-step verification process uses B-score thresholds to accept or reject answers flagged as potentially biased. The approach includes randomizing option order per query and enforcing strict output formatting ({{choice}}) for reliable parsing.

## Key Results
- Multi-turn conversations significantly reduce bias in Random questions (+0.41 improvement) and partially in Subjective questions (+0.27)
- B-score effectively identifies biased answers, outperforming verbalized confidence scores
- Two-step verification using B-score improves accuracy by +9.3% on proposed questions and +2.9% on standard benchmarks (MMLU, HLE, CSQA)

## Why This Works (Mechanism)

### Mechanism 1
LLMs reduce bias in multi-turn settings by implicitly detecting and correcting imbalances in their output history. When a model observes skewed distributions (e.g., repeatedly picking "7"), it lowers the probability of selecting that option again to align with the prompt's request for randomness. This works for Random questions but not Subjective ones where consistency is interpreted as preference rather than error.

### Mechanism 2
Comparing single-turn ($P_{single}$) against multi-turn ($P_{multi}$) probabilities isolates stubborn biases from correctable ones. The difference (B-score = $P_{single} - P_{multi}$) quantifies how much of the initial high probability was an artifact of single-shot decoding versus genuine knowledge. High-frequency answers that drop in multi-turn are likely biases; those that persist are likely genuine capabilities.

### Mechanism 3
Verbalized confidence scores are unreliable for bias detection because they reflect perceived question difficulty rather than distributional bias of specific answers. LLMs tend to output high confidence for any generated answer, even statistically biased ones. Confidence is tuned to token plausibility, not the uniformity required by the prompt.

## Foundational Learning

- **Concept: Greedy Decoding vs. Sampling Bias** - Why needed: Biases appear at temperature=0 because the highest probability token dominates. Understanding context shifts in logits is crucial. Quick check: Why does multi-turn succeed where temperature fails? (Hint: Multi-turn adds directional context, not just noise).

- **Concept: Context Window Accumulation** - Why needed: B-score relies on multi-turn setup where history is fed back into the model. Quick check: Does the model change weights or just output based on prompt context in multi-turn?

- **Concept: Distribution Matching (Uniform vs. Skewed)** - Why needed: You must understand the ground truth for Random questions is uniform distribution to interpret B-score. Quick check: If top-choice single-turn probability is 0.80 for a 4-choice question, what should multi-turn probability approach?

## Architecture Onboarding

- **Component map**: Randomizer -> Single-Turn Sampler (N=30) -> Multi-Turn Runner (N=30) -> B-Score Calculator -> Verifier

- **Critical path**: 1) Randomize option order per query 2) Maintain exact same prompt phrasing across turns 3) Calculate empirical probabilities accurately from N samples

- **Design tradeoffs**: Cost vs. Stability (N=30 vs N=10), Strict format adherence for parsing, Context overflow risk for very long runs

- **Failure signatures**: Format drift (model starts chatting about answers), Subjective conflation (misinterpreting preferences as bias), Context overflow for large N

- **First 3 experiments**: 1) Random Number Sanity Check (verify 7 bias drops to ~10%) 2) BBQ Benchmark replication (~89% verification accuracy) 3) Threshold tuning grid search on validation set

## Open Questions the Paper Calls Out

- Can B-score insights be used to create automated debiasing methods during model training? The paper suggests this would be beneficial but hasn't experimented with incorporating B-score into training processes.

- How effectively does B-score generalize to established external benchmarks for bias and hallucination detection? The authors validate on their 36 questions and standard QA benchmarks but haven't tested on standard bias datasets like CrowS-Pairs or TruthfulQA.

- Can computational overhead be reduced for real-time applications? The method requires multiple generation passes (N=30), and while fewer samples may suffice, the authors suggest this needs further investigation.

## Limitations

- Sampling stability varies across question types; N=10 may suffice for binary choices but this isn't validated for all 4 types
- Subjective question classification boundary between "preference" and "bias" is philosophically ambiguous
- Results primarily reported for GPT-4o; performance on smaller or different model architectures remains unclear

## Confidence

**High Confidence**: B-score mechanism works as described for Random questions; 9.3% verification improvement is well-supported

**Medium Confidence**: Partial effectiveness on Subjective questions; generalization to standard benchmarks shows improvement but smaller effect sizes

**Low Confidence**: Claims about B-score "outperforming" confidence scores without exploring alternative confidence formulations; robustness across model families

## Next Checks

1. **Sample Size Sensitivity Analysis**: Replicate B-score with N=10, N=20, and N=30 samples per condition to quantify variance and determine minimum viable sample size

2. **Model Family Generalization**: Test B-score methodology on diverse LLMs including smaller models and non-transformer architectures to verify mechanism isn't specific to GPT-4o

3. **Confidence Score Variants**: Compare B-score against log-probability scores, ensemble voting confidence, and calibrated confidence scores to establish whether advantage is specific to B-score or general to distributional analysis