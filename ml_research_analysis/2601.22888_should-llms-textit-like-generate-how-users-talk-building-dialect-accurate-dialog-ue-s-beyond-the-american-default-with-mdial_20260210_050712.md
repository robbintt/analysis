---
ver: rpa2
title: Should LLMs, $\textit{like}$, Generate How Users Talk? Building Dialect-Accurate
  Dialog[ue]s Beyond the American Default with MDial
arxiv_id: '2601.22888'
source_url: https://arxiv.org/abs/2601.22888
tags:
- dialect
- english
- llms
- mdial
- like
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MDial, a large-scale framework for generating
  multi-dialectal conversational data across nine English dialects, addressing the
  problem that over 80% of English speakers do not use Standard American English (SAE)
  yet experience higher failure rates and stereotyped responses from LLMs. The method
  involves creating a parallel dataset using rule-based transformations (RBTs) with
  native linguist annotations across lexical, orthographic, and morphosyntactic dimensions,
  while distinguishing between features appropriate for model generation versus user
  speech.
---

# Should LLMs, $\textit{like}$, Generate How Users Talk? Building Dialect-Accurate Dialog[ue]s Beyond the American Default with MDial

## Quick Facts
- arXiv ID: 2601.22888
- Source URL: https://arxiv.org/abs/2601.22888
- Authors: Jio Oh; Paul Vicinanza; Thomas Butler; Steven Euijong Whang; Dezhi Hong; Amani Namboori
- Reference count: 40
- Primary result: Over 80% of English speakers use non-Standard American English dialects, yet LLMs show higher failure rates and stereotyped responses for these dialects

## Executive Summary
This paper introduces MDial, a large-scale framework for generating multi-dialectal conversational data across nine English dialects, addressing the problem that over 80% of English speakers do not use Standard American English (SAE) yet experience higher failure rates and stereotyped responses from LLMs. The method involves creating a parallel dataset using rule-based transformations (RBTs) with native linguist annotations across lexical, orthographic, and morphosyntactic dimensions, while distinguishing between features appropriate for model generation versus user speech. Independent evaluations show MDial outputs are preferred over prior methods in 98% of pairwise comparisons for dialect naturalness. The MDialBenchmark dataset with 50k+ dialogs (97k+ QA pairs) is used to evaluate 17 LLMs on dialect identification and response generation tasks, revealing that even frontier models achieve under 70% accuracy, fail to reach 50% for Canadian English, and systematically misclassify non-SAE dialects as American or British. Post-training on MDial data substantially improves dialectal capabilities, with turn 1 accuracy reaching 96.7% after fine-tuning.

## Method Summary
The MDial framework creates a parallel dataset using rule-based transformations (RBTs) with native linguist annotations across nine English dialects. The approach systematically converts Standard American English conversations into dialect-appropriate forms by applying transformation rules at lexical, orthographic, and morphosyntactic levels. The framework distinguishes between dialect features suitable for model generation (e.g., vocabulary variations) versus user speech (e.g., phonetic spellings), ensuring realistic conversational dynamics. The resulting MDialBenchmark dataset contains over 50,000 dialogs with 97,000+ question-answer pairs across nine dialects. The methodology emphasizes native linguist involvement to ensure dialect authenticity and prevent stereotyping, with transformations guided by linguistic expertise rather than purely automated approaches.

## Key Results
- MDial outputs were preferred over prior methods in 98% of pairwise comparisons for dialect naturalness
- 17 evaluated LLMs achieved under 70% accuracy on dialect identification tasks using MDialBenchmark
- Canadian English was the most challenging dialect, with models failing to reach 50% accuracy
- Non-SAE dialects were systematically misclassified as American or British
- Post-training on MDial data improved dialect identification accuracy to 96.7% at turn 1

## Why This Works (Mechanism)
MDial works by addressing the fundamental mismatch between the SAE-centric training data of most LLMs and the linguistic diversity of actual English speakers. The rule-based transformation approach with native linguist oversight ensures that dialect features are applied systematically and authentically rather than through ad-hoc generation. By creating parallel datasets with explicit dialect annotations, the framework enables targeted evaluation and fine-tuning of dialectal capabilities. The distinction between generation-appropriate and user-appropriate dialect features prevents the introduction of stereotypes while maintaining conversational authenticity. The large-scale nature of the dataset (50k+ dialogs) provides sufficient coverage for effective model adaptation, addressing the data scarcity problem that typically affects non-SAE dialect representation in language technology.

## Foundational Learning
- Rule-based transformations (RBTs): Systematic application of linguistic rules to convert text between dialects, needed to ensure consistency and coverage across multiple dialect features; quick check: verify transformation rules capture known dialectal variations
- Dialect annotation schemes: Structured labeling of linguistic features at lexical, orthographic, and morphosyntactic levels, needed to enable precise evaluation and model training; quick check: validate annotations against native speaker intuitions
- Native linguist involvement: Expert linguistic knowledge applied throughout the transformation process, needed to prevent stereotyping and ensure authenticity; quick check: conduct inter-rater reliability analysis among linguists
- Parallel dataset construction: Creation of matched SAE-dialect pairs, needed for controlled evaluation of dialectal capabilities; quick check: verify semantic equivalence between SAE and dialect versions
- Dialect classification metrics: Accuracy measurements for identifying dialectal variants, needed to quantify model performance across different English varieties; quick check: test on known dialect samples
- Fine-tuning methodology: Post-training adaptation using dialect-specific data, needed to improve model performance on underrepresented dialects; quick check: measure improvement on held-out dialect samples

## Architecture Onboarding
Component map: Original SAE dialogs -> Rule-based transformations (lexical, orthographic, morphosyntactic) -> Native linguist validation -> Dialect-specific parallel dataset -> MDialBenchmark evaluation suite -> LLM fine-tuning -> Improved dialectal responses

Critical path: The core workflow involves starting with Standard American English dialogs, applying systematic rule-based transformations to generate dialect-appropriate variants, having native linguists validate the authenticity and appropriateness of these transformations, compiling the validated data into the MDialBenchmark dataset, and using this dataset to evaluate and fine-tune LLMs for improved dialectal performance.

Design tradeoffs: The rule-based approach trades flexibility for consistency, ensuring systematic coverage of dialect features but potentially missing nuanced variations. Native linguist involvement adds authenticity but limits scalability. The distinction between generation and user features prevents stereotyping but requires careful linguistic judgment. The focus on nine major dialects provides breadth but may miss regional variations within those dialects.

Failure signatures: Models may over-generalize dialect features, apply transformations inappropriately across contexts, or introduce stereotypes despite linguist oversight. Evaluation may be biased if judges are aware of the generation method. Fine-tuning may lead to catastrophic forgetting of SAE capabilities. The system may struggle with code-switching or mixed-dialect conversations.

First experiments:
1. Test dialect transformation rules on controlled SAE sentences to verify systematic application
2. Evaluate model dialect identification accuracy before and after fine-tuning on MDial data
3. Conduct human preference studies comparing MDial outputs to naturally occurring dialect conversations

## Open Questions the Paper Calls Out
None

## Limitations
- Human evaluation methodology may be biased since judges evaluated MDial outputs against baselines they knew were MDial-generated, potentially priming favorable ratings
- Only 16 linguists were involved in developing transformation rules across nine dialects, raising concerns about adequate coverage of dialect diversity within regions
- The approach focuses on surface-level linguistic features (lexical, orthographic, morphosyntactic) without deeply examining pragmatic or cultural aspects of dialect use
- Preference for MDial outputs over baselines doesn't necessarily indicate genuine naturalness, only relative improvement

## Confidence
- High confidence: Empirical findings regarding LLM dialect identification performance (under 70% accuracy, systematic misclassification patterns)
- Medium confidence: Effectiveness of MDial for generating dialect-appropriate conversational data
- Low confidence: Claims about improved user experience and reduced stereotyping without direct user studies

## Next Checks
1. Conduct truly blind human evaluations where MDial outputs are compared against naturally occurring dialectal conversations rather than just other synthetic baselines
2. Expand the linguist team and conduct inter-rater reliability analysis to ensure transformation rules adequately capture dialect diversity within regions
3. Perform user studies with speakers of target dialects to measure whether MDial-generated responses actually improve comprehension, satisfaction, and reduce stereotyping in real-world applications