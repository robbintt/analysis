---
ver: rpa2
title: Curriculum Guided Massive Multi Agent System Solving For Robust Long Horizon
  Tasks
arxiv_id: '2512.08545'
source_url: https://arxiv.org/abs/2512.08545
tags:
- curriculum
- agents
- agent
- reasoning
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a hierarchical multi-agent architecture for
  long-horizon reasoning tasks, distributing decision-making across a 64x64 grid of
  lightweight agents. The system employs a spatial curriculum that progressively expands
  the operational region of the grid, ensuring agents master simpler central tasks
  before tackling more complex peripheral ones.
---

# Curriculum Guided Massive Multi Agent System Solving For Robust Long Horizon Tasks

## Quick Facts
- arXiv ID: 2512.08545
- Source URL: https://arxiv.org/abs/2512.08545
- Reference count: 28
- Primary result: Hierarchical 64×64 agent grid with spatial curriculum solves Tower of Hanoi with improved stability and reduced oracle dependence

## Executive Summary
This paper introduces a hierarchical multi-agent architecture for long-horizon reasoning tasks, distributing decision-making across a 64×64 grid of lightweight agents. The system employs a spatial curriculum that progressively expands the operational region of the grid, ensuring agents master simpler central tasks before tackling more complex peripheral ones. Negative Log-Likelihood (NLL) is integrated as a confidence measure, enabling the curriculum to prioritize regions where agents are both accurate and well-calibrated. A Thompson Sampling curriculum manager adaptively selects training zones based on competence and NLL-driven reward signals. Evaluated on a spatially-grounded Tower of Hanoi benchmark, the approach demonstrates improved stability, reduced oracle usage, and stronger long-range reasoning through distributed agent cooperation.

## Method Summary
The architecture distributes Tower of Hanoi moves across a 64×64 grid using spiral mapping, where each cell hosts an independent agent that decides locally or escalates to an oracle. A four-stage spatial curriculum expands from central to peripheral regions, with each stage having increasing difficulty bands (radius 0.18 to 0.99). Agents maintain competence scores updated via learning rate η, and a verifier uses NLL-based confidence to decide when to escalate. Thompson Sampling manages the curriculum by sampling competence-based reward distributions for each region, prioritizing zones with high competence and low NLL. The overall reward combines competence, NLL, and oracle usage with weights α, β, and γ.

## Key Results
- 64×64 agent grid with spatial curriculum solves Tower of Hanoi with reduced oracle escalations
- Thompson Sampling-based curriculum manager achieves faster stage progression than ε-greedy or UCB baselines
- NLL-integrated reward shaping improves calibration and reduces premature stage advancement
- Distributed agent cooperation demonstrates improved stability for long-horizon reasoning tasks

## Why This Works (Mechanism)
The system leverages hierarchical decomposition to convert a long-horizon task into manageable sub-problems, each handled by a lightweight agent. The spatial curriculum ensures gradual difficulty scaling, preventing catastrophic forgetting and enabling agents to build robust foundations before tackling complex regions. NLL serves as a dual-purpose metric—both measuring uncertainty and shaping rewards to prioritize well-calibrated performance. Thompson Sampling introduces principled exploration-exploitation in curriculum management, balancing between exploiting high-performing regions and exploring under-sampled areas.

## Foundational Learning

**Spiral Mapping**: Converts linear move sequences to 2D coordinates for grid-based distribution
*Why needed*: Enables spatial decomposition of sequential tasks across agents
*Quick check*: Verify mapping preserves move order and covers full grid uniformly

**Thompson Sampling**: Bandit algorithm using Beta posterior sampling for action selection
*Why needed*: Balances exploration of new regions with exploitation of competent ones
*Quick check*: Arm selection should diversify initially, then concentrate on high-reward regions

**Negative Log-Likelihood (NLL)**: -ln(p) where p is model confidence in decision
*Why needed*: Quantifies calibration quality beyond simple accuracy
*Quick check*: High NLL should correlate with escalation to oracle

**Verifier Threshold**: Decision boundary for escalating uncertain moves to oracle
*Why needed*: Prevents propagation of errors from poorly calibrated agents
*Quick check*: Escalation rate should decrease as competence increases

**Competence Score**: [0,1] metric tracking agent's success rate in its region
*Why needed*: Enables curriculum progression based on demonstrated ability
*Quick check*: Scores should monotonically increase with training

## Architecture Onboarding

**Component Map**: Thompson Sampling -> Curriculum Manager -> Stage Progression -> Agent Grid -> NLL + Competence -> Thompson Sampling

**Critical Path**: Spiral mapping → Agent decision → Verifier → Oracle escalation → NLL update → Competence update → Thompson Sampling reward → Curriculum stage advancement

**Design Tradeoffs**: 
- Lightweight agents vs. centralized reasoning: enables scalability but sacrifices coordination
- Spatial curriculum vs. flat training: improves learning efficiency but requires careful difficulty calibration
- NLL-based calibration vs. accuracy-only: better uncertainty handling but computationally heavier

**Failure Signatures**:
- High NLL with high competence: reward weights misaligned, curriculum advancing too quickly
- Oracle overuse (>50%): verifier threshold too high, agents insufficiently trained
- Oracle underuse (<5% on hard regions): verifier threshold too low, missing learning opportunities

**First Experiments**:
1. Run Tower of Hanoi with 64×64 grid, log per-agent competence and NLL over 1000 ticks
2. Compare Thompson Sampling vs. ε-greedy curriculum management on stage progression speed
3. Vary α, β, γ weights to find optimal balance between competence, calibration, and oracle usage

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to single synthetic Tower of Hanoi benchmark, lacking cross-domain validation
- Heavy reliance on proprietary API access (Mistral, DeepSeek) creates reproducibility barriers
- Claims of "distributed reasoning through agent cooperation" lack evidence of inter-agent communication or coordination

## Confidence

**High Confidence**: Hierarchical curriculum framework and Thompson Sampling-based region selection are well-grounded in established multi-armed bandit theory and curriculum learning principles.

**Medium Confidence**: Reported performance gains are plausible but lack statistical significance tests or multiple seed runs to demonstrate robustness.

**Low Confidence**: Claims about distributed reasoning through agent cooperation are overstated given the fully decentralized nature of the current setup.

## Next Checks

1. **Ablation Study on Reward Weights**: Systematically vary α, β, and γ across multiple seeds to quantify their impact on stage progression speed and cumulative regret.

2. **Cross-Domain Transfer Test**: Apply the 64×64 agent grid to a non-synthetic task (e.g., block stacking or sequential decision-making in a simulated environment) to assess generalizability.

3. **Inter-Agent Interference Analysis**: Introduce metrics to measure conflicting actions between neighboring agents and evaluate state boundary maintenance under high load.