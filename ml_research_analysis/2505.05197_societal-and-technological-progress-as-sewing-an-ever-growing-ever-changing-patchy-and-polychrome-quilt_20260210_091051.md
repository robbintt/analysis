---
ver: rpa2
title: Societal and technological progress as sewing an ever-growing, ever-changing,
  patchy, and polychrome quilt
arxiv_id: '2505.05197'
source_url: https://arxiv.org/abs/2505.05197
tags:
- social
- systems
- quilt
- human
- norms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper challenges the \"Axiom of Rational Convergence\" in\
  \ AI alignment\u2014the assumption that rational agents will converge on a single\
  \ set of values under ideal conditions. Instead, the authors propose an \"appropriateness\
  \ framework\" that treats persistent disagreement as normal and designs for it through\
  \ four principles: contextual grounding, community customization, continual adaptation,\
  \ and polycentric governance."
---

# Societal and technological progress as sewing an ever-growing, ever-changing, patchy, and polychrome quilt

## Quick Facts
- arXiv ID: 2505.05197
- Source URL: https://arxiv.org/abs/2505.05197
- Reference count: 11
- Challenges the "Axiom of Rational Convergence" in AI alignment

## Executive Summary
This paper proposes an "appropriateness framework" for AI alignment that treats persistent disagreement as normal rather than a problem to be solved. Instead of seeking universal values, the authors advocate for contextual grounding, community customization, continual adaptation, and polycentric governance to create an ecosystem of specialized AI systems. The framework draws on conflict theory, cultural evolution, and institutional economics to manage pluralism through distributed decision-making rather than value unification.

## Method Summary
The paper presents a theoretical position challenging universal alignment approaches, proposing instead a framework based on four design principles: contextual grounding (providing AI with situational data), community customization (allowing local norm-shaping), continual adaptation (ongoing feedback-based learning), and polycentric governance (distributed decision-making authority). Rather than specifying technical implementations, the authors provide conceptual foundations and critique the assumption that rational agents will converge on shared values.

## Key Results
- Identifies the "Axiom of Rational Convergence" as a flawed assumption underlying current AI alignment approaches
- Proposes four principles for managing persistent disagreement in pluralistic societies
- Frames AI safety as a problem of conflict management rather than value unification
- Suggests decentralized ecosystem of specialized AI systems rather than monolithic universal AI

## Why This Works (Mechanism)

### Mechanism 1: Polycentric Governance Distributes Conflict Resolution
Stability emerges from distributed decision-making authority across multiple governance centers. When one fails, others compensate, preventing single points of failure. Power concentration, not value disagreement, is the primary threat to social stability. Break condition: if governance centers capture each other through regulatory capture or monopoly.

### Mechanism 2: Contextual Grounding Enables Local Appropriateness
AI systems behave inappropriately primarily due to missing context rather than misaligned objectives. Providing situational data (geography, social roles, cultural practices) allows systems to infer which norms apply. Shifts problem from "encode universal ethics" to "detect which patch of the quilt you're in." Break condition: if contextual data enables surveillance or manipulation outweighing benefits.

### Mechanism 3: Sanction-Driven Adaptation Creates Norm Convergence
AI systems learn community-specific norms through feedback signals without explicit encoding. Continuous feedback serves as universal interface for norm learning, mirroring how humans internalize culturally opaque norms. Break condition: if powerful AI can manipulate or ignore sanction mechanisms, feedback becomes unreliable.

## Foundational Learning

- **Thick vs. Thin Morality (Walzer)**: The paper's core distinction between universal "thin" principles and culturally-embedded "thick" norms underpins why universal alignment fails. Quick check: Can you name an example where the same action is virtuous in one cultural context and violating in another?

- **Mistake Theory vs. Conflict Theory (Alexander)**: The paper explicitly frames its critique as shifting from Mistake Theory (disagreement = error) to Conflict Theory (disagreement = fundamental). Quick check: When you see persistent disagreement, do you instinctively assume one side is wrong?

- **Polycentric Governance (Ostrom)**: The proposed solution relies on distributed authority. Understanding Ostrom's empirical work provides institutional-economics grounding for why this should work. Quick check: Can you explain why multiple overlapping authorities might be more stable than a single centralized one?

## Architecture Onboarding

- Component map: Foundation model -> Context-specialized variants -> Context capture layer -> Customization interface -> Feedback/sanction pipeline -> Polycentric governance overlay

- Critical path: 1. Define context schema, 2. Build privacy-preserving context injection, 3. Implement community customization hooks, 4. Deploy sanction collection and aggregation, 5. Establish governance boundaries

- Design tradeoffs:
  - Context richness vs. privacy: More context improves appropriateness but creates surveillance risk
  - Specialization vs. capability: Single system serving all contexts defaults to "bland LLM-speak"
  - Local adaptation vs. global constraints: Community customization must operate within broader societal constraints

- Failure signatures:
  - Bland uniformity: System gives corporate-safe responses everywhere
  - Context-inappropriate behavior: Correct factual output that violates situational norms
  - Governance capture: Single stakeholder dominates customization
  - Sanction manipulation: Bad actors flood feedback to shift norms adversarially

- First 3 experiments:
  1. Context ablation study: Measure appropriateness ratings for identical model with/without contextual signals across distinct domains
  2. Community customization pilot: Give two distinct communities customization tools; measure whether AI instances diverge meaningfully
  3. Sanction learning robustness test: Simulate adversarial sanction patterns; measure whether system can distinguish legitimate feedback from manipulation

## Open Questions the Paper Calls Out

- **Question 1**: How can decentralized feedback mechanisms be designed to remain robust when advanced AI systems become capable of manipulating or ignoring the sanctions intended to train them?
  - The paper asks what happens when AI systems become powerful enough to shape, manipulate, or ignore feedback mechanisms themselves, and posits the solution is designing mechanisms that become stronger, not weaker, in the face of attempts to manipulate them.

- **Question 2**: How can AI systems achieve necessary contextual grounding without violating privacy constraints defined by contextual integrity?
  - The authors note that pursuing context-aware AI must go hand-in-hand with developing strong technical and governance solutions because the necessary data sources produce personally-identifiable information.

- **Question 3**: How can global AI safety initiatives overcome the "start-up problem" to mobilize collective action against existential risk despite deep-seated value heterogeneity?
  - The paper highlights the "start-up problem," arguing that mobilizing action for X-risk mitigation is difficult because defining the goal "sufficiently clearly... immediately forces choices that reflect particular values."

## Limitations
- No technical specification provided for implementing the appropriateness framework
- Sparse empirical grounding despite theoretical foundations
- Tension between context capture requirements and privacy preservation not resolved

## Confidence
- High confidence: Philosophical critique of universal alignment is well-supported
- Medium confidence: Four design principles are logically coherent and draw on established institutional economics
- Low confidence: Mechanism by which sanctioning creates norm convergence is under-specified

## Next Checks
1. Context-Behavior Mapping Validation: Implement controlled experiments testing whether identical AI capabilities produce different appropriateness judgments when given different contextual signals
2. Community Norm Divergence Measurement: Deploy customization tools to two distinct communities and measure whether their AI instances develop measurably different norm-enforcement patterns
3. Sanction Learning Robustness Testing: Simulate adversarial sanction patterns to test whether the system can distinguish legitimate feedback from manipulation attempts