---
ver: rpa2
title: A Survey on Prompt Tuning
arxiv_id: '2507.06085'
source_url: https://arxiv.org/abs/2507.06085
tags:
- prompt
- tuning
- learning
- prompts
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey provides a comprehensive overview of prompt tuning,
  a parameter-efficient approach for adapting large language models by prepending
  trainable continuous vectors while keeping the model frozen. The paper categorizes
  existing methods into direct prompt learning and transfer learning approaches.
---

# A Survey on Prompt Tuning

## Quick Facts
- arXiv ID: 2507.06085
- Source URL: https://arxiv.org/abs/2507.06085
- Reference count: 33
- Primary result: Comprehensive survey of prompt tuning methods, categorizing them into direct learning and transfer learning approaches with analysis of frameworks, innovations, and limitations

## Executive Summary
This survey provides a comprehensive overview of prompt tuning, a parameter-efficient approach for adapting large language models by prepending trainable continuous vectors while keeping the model frozen. The paper categorizes existing methods into direct prompt learning and transfer learning approaches. Direct learning includes general optimization methods, encoder-based techniques, decomposition strategies, and mixture-of-experts frameworks. Transfer learning encompasses general transfer approaches, encoder-based methods, and decomposition strategies. The survey analyzes each method's framework, innovations, advantages, and limitations, providing visualizations for comparison.

## Method Summary
The survey systematically reviews prompt tuning methods, which adapt frozen LLMs by optimizing continuous prompt vectors prepended to input embeddings. The approach is categorized into direct learning (general optimization, encoder-based, decomposition, and MoE methods) and transfer learning (general transfer, encoder-based, and decomposition approaches). The core mechanism involves initializing soft prompts, prepending them to input embeddings, forward passing through the frozen model, computing task-specific loss, and backpropagating gradients to update only the prompt parameters.

## Key Results
- Prompt tuning effectiveness correlates with model scale, performing well on large models (>1B parameters) but less effectively on smaller ones
- Training instability and computational inefficiency due to extended input sequences remain significant challenges
- Transfer learning approaches show promise for few-shot learning but require careful source task selection
- Encoder-based reparameterization methods provide stability but add parameters and complexity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prepending trainable continuous vectors to input embeddings can adapt frozen language models to downstream tasks with minimal parameter updates.
- Mechanism: Soft prompts $P \in \mathbb{R}^{m \times d}$ are prepended to create a lengthened input sequence $[P; E(x)]$, where $E(\cdot)$ is the embedding layer. These vectors are optimized directly in continuous space, unrestricted by vocabulary constraints, encoding task-specific information through gradient updates while the pretrained model parameters remain frozen.
- Core assumption: Task-relevant adaptations can be sufficiently captured in the input embedding space without modifying internal model weights.
- Evidence anchors:
  - [abstract] "parameter-efficient approach for adapting language models by prepending trainable continuous vectors while keeping the model frozen"
  - [section 2] "soft prompts are continuous vectors in the embedding space... unrestricted by vocabulary constraints, enabling optimization in a continuous space"
  - [corpus] Limited direct corpus evidence; "Cross-Prompt Encoder for Low-Performing Languages" mentions soft prompts for PEFT but doesn't validate the prepending mechanism specifically.
- Break condition: Performance substantially degrades on smaller models (<1B parameters), as prompt tuning effectiveness correlates with model scale.

### Mechanism 2
- Claim: Injecting trainable prompts across multiple transformer layers provides deeper adaptation capacity than input-only prompting, approaching full fine-tuning performance.
- Mechanism: Methods like P-Tuning v2 and Prefix Tuning add prefix tokens or key-value pairs at every transformer layer rather than only at the input. This allows prompts to influence intermediate representations directly, with P-Tuning v2 using a classification head instead of verbalizers.
- Core assumption: Intervening at intermediate layers enables more expressive task adaptation than input-space modifications alone.
- Evidence anchors:
  - [section 3] "P-Tuning v2 incorporates prefix trainable tokens as soft prompts in every transformer layer... deep prompt design eliminates the need for verbalizers"
  - [section 3] "prompt depth influences model performance and adding soft prompts to deeper layers can match the performance of full-layer prompting"
  - [corpus] Weak corpus evidence; no neighbors directly address multi-layer prompt injection.
- Break condition: Requires task-specific hyperparameter tuning; may underperform full fine-tuning on complex sequence labeling tasks, particularly with smaller models.

### Mechanism 3
- Claim: Transferring prompts from source tasks to initialize or compose target task prompts improves few-shot performance through knowledge reuse.
- Mechanism: Transfer methods like SPoT pre-train prompts on source tasks and use them for initialization, while ATTEMPT computes attention-weighted combinations of source prompts. The framework assumes task similarity correlates with transfer effectiveness, with early checkpoints serving as task embeddings for similarity measurement.
- Core assumption: Cross-task knowledge can be compressed into prompt parameters and effectively transferred to related tasks.
- Evidence anchors:
  - [section 4] "SPoT introduces transfer learning into prompt tuning through two transfer strategies... generic transfer trains a general soft prompt on multiple source tasks"
  - [section 4] "task similarity strongly correlates with transfer effectiveness, with high-quality source tasks being either large-scale datasets, complex reasoning tasks, or tasks similar to the target"
  - [corpus] "Transferable Modeling Strategies for Low-Resource LLM Tasks" supports knowledge transfer via soft prompts but doesn't validate the specific transfer mechanisms.
- Break condition: Effectiveness depends heavily on source task selection quality; incomplete capture of transfer characteristics through task embeddings; additional computational overhead for computing task embeddings.

## Foundational Learning

- Concept: **Parameter-Efficient Fine-Tuning (PEFT)**
  - Why needed here: Prompt tuning is a subset of PEFT; understanding the broader category helps contextualize why updating only small parameter groups can achieve comparable performance to full fine-tuning.
  - Quick check question: Can you explain why updating 0.01% of parameters might achieve similar task performance as updating all parameters?

- Concept: **Continuous vs Discrete Prompts**
  - Why needed here: Soft prompts operate in continuous embedding space, not as natural language tokens; this distinction is critical for understanding why they can be optimized via gradient descent.
  - Quick check question: What constraints do discrete prompts face that continuous soft prompts do not?

- Concept: **Transfer Learning and Task Similarity**
  - Why needed here: Many advanced prompt tuning methods leverage knowledge from source tasks; understanding how task similarity affects transfer effectiveness is essential for method selection.
  - Quick check question: Would you expect prompts trained on sentiment analysis to transfer better to emotion classification or to mathematical reasoning?

## Architecture Onboarding

- Component map:
  - Frozen PLM backbone -> Soft prompt tensor -> Optional encoder -> Optional multi-layer prefix -> Task head

- Critical path:
  1. Initialize soft prompts (random or from vocabulary embeddings)
  2. Prepend prompts to input: $[P; E(x)]$
  3. Forward through frozen PLM
  4. Compute task-specific loss
  5. Backpropagate gradients to prompt parameters only
  6. Update prompts via optimizer (typically Adam with careful learning rate tuning)

- Design tradeoffs:
  - **Prompt length**: Longer prompts encode more information but increase memory/compute; XPrompt shows not all tokens contribute equally
  - **Depth**: Input-only (Lester et al.) vs multi-layer (P-Tuning v2); deeper is more expressive but adds parameters
  - **Reparameterization**: Direct optimization is simpler but unstable; encoders add stability but increase parameters
  - **Initialization**: Random vs vocabulary-based; paper indicates initialization strongly affects convergence

- Failure signatures:
  - Training fails to converge → Check learning rate (often needs values like 0.3 for prompts)
  - Large performance gap vs fine-tuning on small models → Prompt tuning scales with model size; consider P-Tuning v2 or Prefix Tuning
  - High variance across runs → Use encoder-based reparameterization (RPT, P-Tuning) or vocabulary initialization
  - Poor few-shot performance → Consider transfer learning methods (SPoT, ATTEMPT, MPT)

- First 3 experiments:
  1. **Baseline comparison**: Implement vanilla Prompt Tuning with random vs vocabulary initialization on a classification task; measure convergence speed and final performance
  2. **Depth ablation**: Compare input-only prompting vs P-Tuning v2 (multi-layer) on sequence labeling; vary prompt length per task complexity
  3. **Transfer validation**: Pre-train prompts on a high-resource source task (e.g., MNLI), transfer to a low-resource target task; compare against training from scratch

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the optimal compression ratio for soft prompt pruning be determined a priori without conducting computationally expensive trial trainings?
- Basis in paper: [explicit] In the analysis of XPrompt, the authors explicitly state: "However, one question left is how to find the optimal compression ratio without trial training."
- Why unresolved: While XPrompt demonstrates that prompt tokens can be pruned to improve efficiency, identifying the specific subset of "positive" tokens currently requires a hierarchical pruning process dependent on iterative training and weight rewinding.
- What evidence would resolve it: A theoretical framework or proxy metric that correlates initial prompt statistics or task complexity with the optimal sparsity level, eliminating the need for empirical search.

### Open Question 2
- Question: What are the semantic meanings and interaction mechanisms of learned continuous prompts within the frozen pretrained model?
- Basis in paper: [explicit] The challenges section lists "Explainability" and states: "The semantic meaning of learned prompts and their interaction mechanisms with pretrained models remain poorly understood, limiting their improvements."
- Why unresolved: Continuous vectors optimize in an unconstrained embedding space distinct from discrete natural language tokens, making it difficult to map specific prompt dimensions to human-understandable linguistic functions or specific model behaviors.
- What evidence would resolve it: Causal tracing or probing studies that successfully correlate specific perturbations in the soft prompt space with predictable, interpretable changes in the model's internal representations or outputs.

### Open Question 3
- Question: Can prompt tuning methods be effectively adapted to bridge the performance gap with full fine-tuning on smaller, resource-constrained model scales?
- Basis in paper: [explicit] The paper identifies "Model scale dependency" as a key challenge, noting prompt tuning "performing well on large models but decreasing on smaller ones, limiting its applicability in resource-constrained settings."
- Why unresolved: The effectiveness of current prompt tuning methods appears intrinsically linked to the high capacity and redundancy of Large Language Models (LLMs), creating a performance gap on smaller models where the frozen parameters lack the flexibility to be steered effectively by soft prompts alone.
- What evidence would resolve it: A modified prompt tuning framework (e.g., utilizing deep prompting or advanced reparameterization) that achieves performance parity with full fine-tuning on models with fewer than 1 billion parameters.

## Limitations
- The survey lacks direct empirical validation of claimed mechanisms across diverse model scales
- Performance comparisons across different prompt tuning categories are not empirically established
- Claims about transfer learning effectiveness depend on non-standardized task similarity metrics
- The assertion that deep prompting matches full fine-tuning performance lacks systematic quantitative validation

## Confidence

- **High Confidence**: The fundamental mechanism of prompt tuning (prepending trainable continuous vectors to frozen model inputs) is well-established in the literature and represents the core innovation of the approach. The categorization framework (direct vs transfer learning, encoder-based vs decomposition strategies) provides a logical organizational structure for understanding method diversity.

- **Medium Confidence**: The identified challenges around computational efficiency and training instability are supported by community experience, though specific failure rates and conditions are not quantified. The correlation between prompt tuning effectiveness and model scale is theoretically sound but requires empirical validation across the full spectrum of model sizes.

- **Low Confidence**: The transferability claims for knowledge transfer methods depend heavily on task similarity metrics that are not standardized across studies. The assertion that specific source tasks (large-scale datasets, complex reasoning tasks) provide superior transfer quality lacks systematic evaluation across diverse task pairs.

## Next Checks

1. **Scale-Aware Performance Validation**: Implement the same prompt tuning method (e.g., vanilla Prompt Tuning) across models of varying scales (125M to 11B parameters) on identical downstream tasks, measuring both performance and parameter efficiency ratios to empirically verify the claimed scale dependency.

2. **Transfer Quality Quantification**: Design a controlled experiment comparing prompt transfer effectiveness across task similarity metrics - measure performance when transferring between similar tasks (sentiment→emotion classification) versus dissimilar tasks (sentiment→mathematical reasoning), using standardized similarity measures.

3. **Encoder Stability Analysis**: Compare training stability and convergence rates between direct prompt optimization and encoder-based reparameterization methods (P-Tuning, RPT) across multiple random seeds and learning rate schedules, quantifying variance reduction and convergence speed improvements.