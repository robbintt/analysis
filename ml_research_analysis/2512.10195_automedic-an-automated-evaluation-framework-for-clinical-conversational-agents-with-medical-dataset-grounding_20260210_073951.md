---
ver: rpa2
title: 'AutoMedic: An Automated Evaluation Framework for Clinical Conversational Agents
  with Medical Dataset Grounding'
arxiv_id: '2512.10195'
source_url: https://arxiv.org/abs/2512.10195
tags:
- clinical
- medical
- patient
- conversational
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AutoMedic is a fully automated multi-agent framework for evaluating
  large language models (LLMs) as clinical conversational agents. It converts static
  medical QA datasets into interactive virtual patient profiles and simulates multi-turn
  clinical dialogues between LLM agents.
---

# AutoMedic: An Automated Evaluation Framework for Clinical Conversational Agents with Medical Dataset Grounding

## Quick Facts
- arXiv ID: 2512.10195
- Source URL: https://arxiv.org/abs/2512.10195
- Authors: Gyutaek Oh; Sangjoon Park; Byung-Hoon Kim
- Reference count: 15
- Primary result: AutoMedic is a fully automated multi-agent framework for evaluating large language models (LLMs) as clinical conversational agents.

## Executive Summary
AutoMedic introduces a novel framework for evaluating LLMs as clinical conversational agents by transforming static medical QA datasets into interactive virtual patient profiles. The framework simulates multi-turn clinical dialogues between specialized LLM agents with bounded information access, using a CARE metric to assess performance across accuracy, conversational efficiency, empathy, and robustness. Human expert evaluations validated the framework's reliability and demonstrated strong alignment between CARE scores and clinical judgment. The study reveals that LLMs perform significantly worse in conversational settings compared to static QA, with proprietary models like Claude Sonnet 4 and GPT-4o showing balanced, high-level performance while most medical-tuned models underperformed.

## Method Summary
AutoMedic transforms static medical QA datasets into virtual patient profiles through a profile generator agent that filters unsuitable items and extracts three information tiers: demographics (shared with doctor), basic information (patient-held), and optional clinical data (clinical staff-held). The framework simulates multi-turn clinical dialogues using four specialized agents: doctor (subject of evaluation), patient, clinical staff, and CARE evaluator. The doctor agent must actively inquire using tagged queries to gather information rather than passively receive complete context. Performance is quantified using the CARE metric, which evaluates accuracy with conversational penalty, conversational efficiency, empathy rated by patient agent, and robustness (proportion of successful conversations). Supporting agents use GPT-4o to ensure stability.

## Key Results
- LLMs perform significantly worse in conversational settings compared to static QA benchmarks
- Proprietary models (Claude Sonnet 4, GPT-4o) showed balanced, high-level performance across all CARE dimensions
- Medical-tuned models generally underperformed general models in conversational settings
- Strong correlation (r=0.7547) between static and conversational accuracy, but all models showed degradation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Static medical QA datasets can be systematically transformed into interactive virtual patient profiles for conversational evaluation.
- Mechanism: A profile generator agent applies filtering criteria (excluding research scenarios, abstract concepts, image-dependent queries, etc.) then extracts three information tiers—demographics (shared with doctor), basic information (patient-held), and optional clinical data (clinical staff-held). This information partitioning forces the doctor agent to actively inquire rather than passively receive.
- Core assumption: The original QA context contains sufficient patient-specific details to construct a coherent persona; imputed data does not alter the ground-truth answer.
- Evidence anchors:
  - [abstract] "AutoMedic transforms off-the-shelf static QA datasets into virtual patient profiles, enabling realistic and clinically grounded multi-turn clinical dialogues between LLM agents."
  - [section 3.2] "This process transforms a static medical QA query into a structured virtual patient profile... organized into three distinct categories: Demographics, Basic Information, Optional Information."
  - [corpus] CP-Env (arXiv:2512.10206) similarly addresses gaps in static benchmarks by simulating clinical pathways, reinforcing that isolated QA inadequately captures dynamic decision-making.
- Break condition: Source QA items lacking patient context (e.g., mechanism questions, classification tasks) fail filtering; datasets like MedMCQA show only 10–15% appropriate samples.

### Mechanism 2
- Claim: Role-specialized multi-agent simulation creates ecologically valid clinical dialogue through constrained information access and structured communication protocols.
- Mechanism: Four agents operate with bounded knowledge—doctor sees only demographics, patient holds symptoms/history, clinical staff controls test results. Doctor must use `<patient>` and `<clinical>` tags to request information; simulation terminates on `</end>` or max turns (20). This enforces information-seeking behavior rather than answer generation from complete context.
- Core assumption: Agent role adherence remains stable across multi-turn exchanges; supporting agents (patient, clinical staff) respond consistently using GPT-4o as infrastructure.
- Evidence anchors:
  - [section 3.1] "The primary agent is the doctor agent, which emulates a physician and serves as the sole subject of our evaluation. The other three agents... act in supporting roles."
  - [section 3.3] "By isolating technical data until specifically requested, this design maintains the simulation's integrity and ensures the proper evaluation of the doctor agent's information-gathering skills."
  - [corpus] AgentClinic (Schmidgall et al., 2024, cited in paper) pioneered similar multi-agent clinical simulation but lacks automated quantitative evaluation beyond accuracy.
- Break condition: Role-breaking (doctor self-simulates other agents), abrupt termination (≤3 turns), or invalid final answers trigger robustness failures.

### Mechanism 3
- Claim: The CARE metric quantifies multi-dimensional clinical conversational competence and correlates with human expert judgment.
- Mechanism: Four sub-scores—S_ACC (accuracy with conversational penalty), S_CES (inverse words-per-turn for efficiency), S_EMP (patient-agent empathy rating), S_ROB (proportion of successful conversations)—are computed automatically. Human validation (Cohen's Kappa 0.82 for profile filtering; Gwet's AC2 0.60–0.95 across CARE dimensions) confirms alignment.
- Core assumption: Patient-agent empathy proxy reflects genuine empathetic communication; word-per-turn inversely correlates with strategic questioning quality.
- Evidence anchors:
  - [section 3.4] "To conduct this assessment automatically and without human intervention, we introduce the CARE metric... quantitatively assesses the agent's performance across four distinct aspects."
  - [section 5.3, Table 3] Human-CARE alignment shows Gwet's AC2 of 0.9460 (accuracy), 0.6002 (efficiency), 0.8571 (empathy), 0.6209 (robustness); trends in expert scores mirror CARE rankings.
  - [corpus] No corpus papers directly validate multi-faceted automated metrics for clinical dialogue; this appears novel.
- Break condition: Empathy scoring depends on patient-agent reliability; if patient-agent sycophantically rates all doctors highly, discrimination degrades.

## Foundational Learning

- Concept: **Information asymmetry in multi-agent simulation**
  - Why needed here: The framework's core innovation is partitioning patient data across agents; understanding why this matters is essential for designing simulations that test information-gathering rather than pattern-matching.
  - Quick check question: If the doctor agent received the full patient profile upfront, what capability would no longer be evaluated?

- Concept: **Static vs. conversational accuracy gap**
  - Why needed here: Paper finds strong correlation (r=0.7547) between static and conversational accuracy, but all models show degradation; medical-tuned models often underperform general models conversationally.
  - Quick check question: Why might a model with high USMLE-style QA accuracy fail in multi-turn clinical dialogue?

- Concept: **Metric reliability validation**
  - Why needed here: Automated metrics require correlation with human judgment; understanding Fleiss' Kappa, Cohen's Kappa, and Gwet's AC2 interpretation is necessary to assess whether CARE scores are meaningful.
  - Quick check question: A Gwet's AC2 of 0.60 indicates what level of inter-rater agreement, and why might this be acceptable for conversational efficiency but concerning for accuracy?

## Architecture Onboarding

- Component map:
  - Profile Generator Agent -> Information distribution to agents -> Multi-turn simulation -> Final question presentation -> CARE scoring

- Critical path: Medical QA dataset → Profile Generator (filter + extract) → Information distribution to agents → Multi-turn simulation → Final question presentation → CARE scoring

- Design tradeoffs:
  - Using GPT-4o for all supporting agents ensures stability but masks how patient/staff agent choice affects doctor performance
  - Min-max normalization per dataset enables cross-dataset comparison but obscures absolute performance differences
  - 20-turn maximum balances information-gathering opportunity against computational cost; may truncate complex cases

- Failure signatures:
  - **Role-breaking**: Doctor generates both questions and answers (observed in DeepSeek-R1-70B)
  - **Checklist behavior**: High words-per-turn indicates unnatural multi-question dumps (observed in gpt-oss-120B)
  - **Premature termination**: ≤3 turns with missing tags
  - **Hallucinated tests**: Clinical staff returns "unavailable" if result not in profile; doctor requesting unavailable data indicates poor strategy

- First 3 experiments:
  1. Replicate human validation on 30 samples using different annotators to confirm inter-rater reliability holds
  2. Test doctor agent with alternative patient agents (e.g., Claude Sonnet 4) to measure sensitivity of S_EMP to patient-agent choice
  3. Run AutoMedic on a held-out QA dataset (not in original six) to validate framework generalization claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a weighted composite score be developed for the CARE metric that aligns with holistic human expert preferences?
- Basis in paper: [explicit] The authors note that they "did not define a single, aggregated score" because the four dimensions have different scales and relative importance varies by application.
- Why unresolved: A simple sum or average is insufficient to capture the complex trade-offs between accuracy, empathy, and efficiency.
- What evidence would resolve it: Calibration of a weighted formula against human expert rankings from head-to-head model comparisons.

### Open Question 2
- Question: How does the variability of the patient and clinical staff agents influence the evaluation of the doctor agent?
- Basis in paper: [explicit] The study utilized GPT-4o exclusively for supporting roles to ensure stability, identifying the analysis of "different LLMs in these roles" as a necessary future direction.
- Why unresolved: The behavior of the doctor agent is reactive; changing the persona or intelligence of the interlocutors may alter the difficulty or trajectory of the dialogue.
- What evidence would resolve it: Ablation studies swapping the underlying models for the patient and staff agents while keeping the doctor agent constant.

### Open Question 3
- Question: Can the AutoMedic framework be effectively extended to evaluate vision-language models (VLMs) using multimodal inputs?
- Basis in paper: [explicit] The limitations section states the current evaluation is limited to text and does not incorporate other modalities like medical imaging.
- Why unresolved: Real-world clinical scenarios are inherently multimodal, and it is unclear if the current profile generation and simulation logic transfers to visual data.
- What evidence would resolve it: Adapting the profile generator to include image findings and successfully benchmarking VLMs on the CARE metric.

### Open Question 4
- Question: Does domain-specific fine-tuning on static medical QA data cause a degradation in the conversational capabilities of LLMs?
- Basis in paper: [inferred] Results showed medical-tuned models generally underperformed compared to general proprietary models, suggesting fine-tuning may erode communication skills.
- Why unresolved: While the performance drop is observed, the specific mechanism (e.g., catastrophic forgetting of social skills) is hypothesized but not proven.
- What evidence would resolve it: Comparing CARE metric scores of base models against their medically fine-tuned counterparts across different tuning strategies.

## Limitations

- Framework depends on quality and representativeness of static medical QA datasets with significant filtering requirements
- Automated empathy scoring assumes patient-agent consistency without validation for different patient-agent choices
- Information partitioning design may oversimplify complex clinical scenarios where patient knowledge overlaps with clinical data domains

## Confidence

- **High confidence**: The framework's architecture and multi-agent simulation mechanism are technically sound and validated through human expert correlation (Gwet's AC2 0.60-0.95 across CARE dimensions)
- **Medium confidence**: The empirical finding that conversational performance lags static QA performance is robust, but specific degradation patterns require larger-scale validation
- **Medium confidence**: The CARE metric's validity as comprehensive evaluation tool is supported by human validation, but sensitivity to agent choice remains unexplored

## Next Checks

1. Conduct inter-annotator reliability study with different expert teams on the same 30 samples to confirm human validation results are not team-specific
2. Test the framework with alternative supporting agents (Claude Sonnet 4 as patient agent) to measure sensitivity of S_EMP and determine if results are GPT-4o-dependent
3. Apply AutoMedic to a held-out medical QA dataset not included in the original six to validate framework generalization beyond the specific datasets used in development