---
ver: rpa2
title: Neural Optimal Transport Meets Multivariate Conformal Prediction
arxiv_id: '2509.25444'
source_url: https://arxiv.org/abs/2509.25444
tags:
- wasserstein
- index
- quantile
- prediction
- sliced
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a neural optimal transport framework for
  conditional vector quantile regression (CVQR) and applies it to multivariate conformal
  prediction. The method parameterizes the conditional vector quantile function as
  the gradient of a convex potential using input-convex neural networks, enabling
  efficient estimation of multivariate ranks and quantiles.
---

# Neural Optimal Transport Meets Multivariate Conformal Prediction

## Quick Facts
- arXiv ID: 2509.25444
- Source URL: https://arxiv.org/abs/2509.25444
- Reference count: 40
- Primary result: Neural OT-based CVQR achieves S-W2 distances of 0.072 on Banana dataset with competitive conformal coverage-efficiency trade-offs

## Executive Summary
This paper introduces a neural optimal transport framework for conditional vector quantile regression (CVQR) and applies it to multivariate conformal prediction. The method parameterizes the conditional vector quantile function as the gradient of a convex potential using input-convex neural networks, enabling efficient estimation of multivariate ranks and quantiles. To accelerate training and inference, amortized optimization of the dual potentials is employed, reducing the cost of solving high-dimensional variational problems. The induced multivariate ranks are then leveraged for conformal prediction, constructing distribution-free predictive regions with finite-sample validity. Unlike coordinatewise methods, the approach adapts to the geometry of the conditional distribution, producing tighter and more informative regions. Experiments on benchmark datasets show improved coverage-efficiency trade-offs compared to baselines, highlighting the benefits of integrating neural optimal transport with conformal prediction.

## Method Summary
The framework learns conditional vector quantiles by parameterizing them as gradients of convex potentials implemented via input-convex neural networks (PICNNs). The training objective minimizes the Wasserstein-2 distance between the reference distribution and the conditional distribution of the target given covariates. To accelerate computation, an amortized optimization approach learns a predictor that provides warm starts for the inner optimization problem. For conformal prediction, vector ranks derived from the learned quantiles are used as conformity scores, with split conformal prediction applied to construct predictive regions. The method handles multivariate targets while adapting to conditional distribution geometry, and includes a re-ranking variant to improve conditional coverage at the cost of increased set volume.

## Key Results
- Neural OT-based CVQR achieves S-W2 distances of 0.072 for Banana dataset and competitive distances across other synthetic datasets
- Amortized optimization reduces training time by ~40% while maintaining quantile estimation accuracy
- Pullback conformal prediction achieves competitive or better worst-slab coverage with smaller log-volume than OT-CP and ellipsoidal baselines on real datasets
- The method demonstrates improved coverage-efficiency trade-offs compared to coordinatewise and standard multivariate conformal approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Input-convex neural networks (PICNNs) guarantee monotonicity of the conditional vector quantile function.
- Mechanism: The conditional vector quantile Q_Y|X(u, x) is parameterized as ∇_u φ_θ(u, x) where φ_θ is a PICNN. Since gradients of convex functions are cyclically monotone, this enforces the Brenier map structure from optimal transport theory, ensuring valid push-forward from reference F_U to conditional F_Y|X.
- Core assumption: Assumption 1 (reference distribution has convex support with density) and Assumption 2 (conditional distributions have densities).
- Evidence anchors:
  - [Section 4]: "Our method parameterizes the conditional vector quantile function as the gradient of a convex potential implemented by an input-convex neural network, ensuring monotonicity and uniform ranks."
  - [Theorem 2, Section 3]: "The conditional vector quantile is Q_Y|X(u, x) = ∇_u φ(u, x) for F_U-a.e. u."
  - [corpus]: Thurin et al. (2025) and Klein et al. (2025) use discrete OT for ranks but lack the continuous neural parameterization for conditional adaptation.
- Break condition: If the reference distribution support is non-convex or the PICNN architecture fails to maintain convexity (e.g., due to improper weight constraints), the monotonicity guarantee fails.

### Mechanism 2
- Claim: Amortized optimization of the conjugate problem reduces computational cost by ~40% while maintaining quantile estimation accuracy.
- Mechanism: Instead of solving the inner argmax ū_φ(y, x) = argmax_u {u^T y - φ_θ(u, x)} with L-BFGS at each iteration, a predictor ũ_ϑ(y, x) is learned jointly to provide warm starts. The two-time-scale update rule ensures ũ_ϑ tracks φ_θ's dynamics without gradient propagation through the solver.
- Core assumption: The amortization model evolves on a faster timescale than the potential φ_θ (two-time-scale stochastic approximation).
- Evidence anchors:
  - [Section 4, AC-NQR]: "We extend this approach to the conditional case by introducing an amortization model ũ_ϑ(y, x) parameterized by ϑ that maps (y, x) to a point that should ideally be close to the true solution."
  - [Table 1]: Training time reduced from 15.08s (C-NQR) to 8.89s (AC-NQR_U) per epoch; inference from 1.76s to 1.12s.
  - [corpus]: Amos (2023) demonstrated amortization for non-conditional OT; this paper extends it to conditional VQR. Corpus lacks direct baselines for amortized conditional OT.
- Break condition: If learning rates between φ_θ and ũ_ϑ are not properly balanced, or if ũ_ϑ fails to generalize to out-of-distribution (y, x) pairs, warm starts degrade and may increase total iterations.

### Mechanism 3
- Claim: Conformal prediction using OT-based vector ranks achieves tighter predictive regions than coordinatewise methods while maintaining finite-sample coverage.
- Mechanism: The vector rank Q^{-1}_Y|X(y, x) maps predictions to a reference space U. Conformity scores S_i = ||Q^{-1}_Y|X(Y_i, X_i)|| quantify centrality in U. Split conformal prediction on these scores yields pullback sets Ĉ^pb_α(x) = {y : ||Q^{-1}_Y|X(y, x)|| ≤ ρ_{1-α}} that adapt to conditional distribution geometry.
- Core assumption: Exchangeability of (X_1, Y_1), ..., (X_n, Y_n), (X_test, Y_test) and correct estimation of Q^{-1}_Y|X.
- Evidence anchors:
  - [Section 5]: "P(Y_test ∈ Ĉ^pb_α(X_test)) ≥ 1 - α under the assumption that (X_1, Y_1), ..., (X_test, Y_test) are exchangeable."
  - [Theorem 3]: "C^pb_α(x) minimizes Lebesgue volume among all sets with x-conditional coverage of at least 1-α" under radiality assumptions.
  - [Figure 3-4]: PB/RPB methods achieve competitive or better worst-slab coverage with smaller log-volume than OT-CP and ellipsoidal baselines on scm20d, sgemm, bio, blog datasets.
- Break condition: If Ũ = Q^{-1}_Y|X(Y, X) is anisotropic (not radially symmetric) due to model misspecification, Euclidean norms in U become unreliable. Re-ranking (RPB) partially mitigates this.

## Foundational Learning

- Concept: Brenier Maps and Optimal Transport
  - Why needed here: The paper builds on Brenier's theorem that the optimal transport map between two distributions with quadratic cost is the gradient of a convex potential (Monge-Ampère equation). Understanding this explains why PICNNs are the natural architecture choice.
  - Quick check question: Given a convex potential φ, what is the push-forward property of its gradient ∇φ?

- Concept: Fenchel-Legendre Conjugacy and c-Transforms
  - Why needed here: The dual formulation (Eq. 2-6) relies on conjugate potentials φ and ψ = φ* where φ*(y) = sup_u {u^T y - φ(u)}. The quantile and rank maps are gradients of these conjugates. Computing the conjugate is the computational bottleneck addressed by amortization.
  - Quick check question: If φ(u) = (1/2)||u||^2, what is φ*(y) and what transport map does ∇φ represent?

- Concept: Split Conformal Prediction
  - Why needed here: The framework uses split CP where a calibration set D_cal determines the threshold ρ_{1-α} via order statistics of conformity scores. Marginal coverage P(Y ∈ Ĉ(X)) ≥ 1 - α is guaranteed under exchangeability, but conditional coverage is not.
  - Quick check question: For a calibration set of size n = 100 and target coverage 1 - α = 0.9, which order statistic determines the threshold?

## Architecture Onboarding

- Component map:
  - **PICNN (Partially Input-Convex Neural Network)**: φ_θ(u, x) - convex in u, conditioned on x. Uses Softplus activations and nonnegative weight constraints (Eq. 16-17).
  - **Amortization Network**: ũ_ϑ(y, x) - MLP with residual connection. Trained with L2 loss ||ũ_ϑ(y, x) - ū_φ(y, x)||².
  - **Inner Solver (L-BFGS)**: Computes exact conjugate ū_φ(y, x) = argmax_u {u^T y - φ_θ(u, x)}. Warm-started by ũ_ϑ.
  - **Conformal Calibration**: Computes S_i = ||Q^{-1}_Y|X(Y_i, X_i)|| on D_cal, determines ρ_{1-α} via order statistics.

- Critical path:
  1. Sample mini-batch {(x_i, y_i)} from training data
  2. For each (y_i, x_i): compute ũ_i = ũ_ϑ(y_i, x_i), then refine with L-BFGS to get ū_i
  3. Sample u_i ~ F_U (e.g., N(0, I))
  4. Compute loss V(θ) = (1/|B|) Σ [φ_θ(u_i, x_i) + (u_i^T y_i - φ_θ(ū_i, x_i))]
  5. Backprop through φ_θ only (not through ū_i - Danskin's theorem)
  6. Update θ and ϑ with AdamW

- Design tradeoffs:
  - **C-NQR vs AC-NQR**: Exact L-BFGS (C-NQR) is accurate but slow (100 inner iterations); amortized (AC-NQR) is faster (50 iterations with warm start) but requires tuning two learning rates.
  - **φ vs ψ parameterization**: Estimating φ (linked to F_U) vs ψ (linked to F_Y|X) have comparable performance; paper experiments with both (U/Y variants).
  - **PB vs RPB**: Pullback sets (PB) assume radial symmetry; re-ranked pullback (RPB) adds an OT re-ranking step on calibration data for robustness to misspecification but increases set volume.

- Failure signatures:
  - **Non-convexity collapse**: If PICNN weights violate nonnegativity constraints, φ_θ may not be convex → gradients no longer form valid transport map → rank estimates become non-uniform.
  - **Inner solver divergence**: If α (strong convexity parameter) is too small or PISCNN not used, L-BFGS may oscillate or fail to converge, especially for high-dim U.
  - **Conformal over-coverage**: If Ũ distribution is highly anisotropic, Euclidean ball thresholds over-cover, producing overly large prediction sets.
  - **Amortizer drift**: If ũ_ϑ learning rate is too high relative to φ_θ, amortizer chases a moving target without converging.

- First 3 experiments:
  1. **PICNN convexity validation**: On Banana/Star synthetic datasets, verify that φ_θ(·, x) is convex by checking Hessian eigenvalues at sampled points. Compare quantile recovery (L2-UV metric) with and without Softplus weight constraints.
  2. **Amortization ablation**: Measure training time and S-W2 distance for AC-NQR vs C-NQR across dimensions d_y ∈ {2, 4, 8, 16} on Neal's Funnel. Plot iterations to convergence and final quantile accuracy.
  3. **Conformal coverage-volume tradeoff**: On bio/blog/sgemm real datasets, compare PB, RPB, OT-CP, and ellipsoidal baselines. Report marginal coverage, worst-slab coverage, and log-volume at α ∈ {0.1, 0.2, 0.3}. Verify finite-sample coverage holds across all splits.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can this framework be extended to broader classes of generative models to provide tighter efficiency guarantees in high-dimensional regimes?
- Basis in paper: [explicit] The conclusion states: "Future work includes extending the framework to broader classes of generative models and exploring tighter efficiency guarantees in high-dimensional regimes."
- Why unresolved: The current method relies on input-convex neural networks (PICNNs) to ensure monotonicity; it is unclear if other generative architectures can maintain these guarantees while scaling.
- What evidence would resolve it: Theoretical analysis or empirical results showing that non-convex generative models can produce valid prediction sets with smaller volumes in dimensions significantly higher than 16.

### Open Question 2
- Question: How can the re-ranking procedure be modified to maintain sharp conditional coverage without the associated increase in prediction set volume?
- Basis in paper: [inferred] In Section 7.2, the authors note that the re-ranking step (RPB) improves conditional coverage but increases set volume, calling it a "questionable trade-off."
- Why unresolved: The paper demonstrates the trade-off empirically but does not propose a mechanism to decouple the correction of rank misspecification from the expansion of the prediction region.
- What evidence would resolve it: A modified re-ranking algorithm that achieves the same "worst-slab coverage" as RPB but with an average log-volume comparable to the standard pullback (PB) method.

### Open Question 3
- Question: How does the volume-optimality of the pullback balls degrade when the conditional distribution violates the radiality assumption?
- Basis in paper: [inferred] Theorem 3 proves the prediction sets are Lebesgue volume-optimal, but relies on the assumption that the Jacobian determinant is a function of the radius (radiality).
- Why unresolved: Real-world data is rarely perfectly elliptical or radial, yet the paper does not quantify the loss in efficiency (volume minimality) when this structural assumption is relaxed.
- What evidence would resolve it: Theoretical bounds or empirical measurements comparing the volume of pullback sets against true HPD regions for multimodal or heavy-tailed synthetic distributions.

## Limitations

- The framework assumes reference distribution F_U has convex support with density and conditional distributions have densities, which may not hold for all real-world data.
- The amortized optimization approach introduces approximation error that may accumulate in high-dimensional settings, and requires careful tuning of learning rates.
- The finite-sample coverage guarantees rely on exchangeability, which may not hold for dependent data or under distribution shift scenarios.
- The Euclidean norm-based conformity scores assume some degree of isotropy in the transformed space; severe anisotropy due to model misspecification could lead to over-coverage and overly conservative prediction sets.

## Confidence

- **High Confidence**: The monotonicity guarantee from PICNN parameterization and the Brenier map structure; the two-time-scale update rule for amortized optimization; the finite-sample coverage guarantee from split conformal prediction.
- **Medium Confidence**: The computational speedup claims from amortization (requires precise learning rate balancing); the volume-efficiency improvements in conformal regions (depends on data geometry and potential misspecification).
- **Low Confidence**: The robustness of re-ranking (RPB) to severe anisotropy in Ũ; performance guarantees under non-exchangeable data or distribution shift.

## Next Checks

1. **Convexity Validation**: On synthetic datasets with known conditional quantiles, verify that the learned φ_θ(u, x) is convex in u by checking Hessian eigenvalues at sampled points, and compare quantile recovery accuracy with and without the Softplus weight constraints.

2. **Amortization Ablation**: Measure training time and quantile estimation accuracy (S-W2 distance) for AC-NQR vs C-NQR across varying dimensions (d_y ∈ {2, 4, 8, 16}) on Neal's Funnel, plotting iterations to convergence and sensitivity to learning rate ratios.

3. **Coverage-Volume Tradeoff**: On real datasets (bio, blog, sgemm), compare PB, RPB, OT-CP, and ellipsoidal baselines reporting marginal coverage, worst-slab coverage, and log-volume at multiple α levels, verifying finite-sample coverage holds across all calibration splits and checking sensitivity to anisotropy in Ũ.