---
ver: rpa2
title: 'VERITAS: Leveraging Vision Priors and Expert Fusion to Improve Multimodal
  Data'
arxiv_id: '2510.15317'
source_url: https://arxiv.org/abs/2510.15317
tags:
- data
- arxiv
- critic
- zhang
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VERITAS addresses the problem of low-quality multimodal training
  data, which often contains factual errors and hallucinations due to inadequate visual
  perception by current large multimodal models. The proposed method integrates vision
  priors from specialized models (RAM++ and PP-OCRv4) with assessments from three
  state-of-the-art LMMs (GPT-4o, Gemini-2.5-Pro, Doubao-1.5-pro) using domain-aware
  statistical fusion with James-Stein shrinkage.
---

# VERITAS: Leveraging Vision Priors and Expert Fusion to Improve Multimodal Data

## Quick Facts
- arXiv ID: 2510.15317
- Source URL: https://arxiv.org/abs/2510.15317
- Reference count: 16
- Primary result: VERITAS consistently improves model performance across six multimodal benchmarks, particularly in text-rich and fine-grained reasoning tasks.

## Executive Summary
VERITAS addresses the problem of low-quality multimodal training data, which often contains factual errors and hallucinations due to inadequate visual perception by current large multimodal models. The proposed method integrates vision priors from specialized models (RAM++ and PP-OCRv4) with assessments from three state-of-the-art LMMs (GPT-4o, Gemini-2.5-Pro, Doubao-1.5-pro) using domain-aware statistical fusion with James-Stein shrinkage. A lightweight GRPO-trained critic model distills this ensemble's judgments at lower cost, while self-refinement generates and selects improved answers. VERITAS consistently improves model performance across six multimodal benchmarks, particularly in text-rich and fine-grained reasoning tasks. The GRPO critic achieves near-GPT-4o ranking fidelity (Kendall's tau 0.71) while being two orders of magnitude more efficient.

## Method Summary
VERITAS is a four-stage pipeline that enhances multimodal training data quality. First, it extracts vision priors using RAM++ (object tags) and PP-OCRv4 (text) from images. Second, three state-of-the-art LMMs (GPT-4o, Gemini-2.5-Pro, Doubao-1.5-pro) provide structured critiques and scores, which are fused using domain-aware James-Stein shrinkage to produce ground-truth labels. Third, a lightweight GRPO-trained critic model learns to replicate the expert fusion judgments at lower computational cost. Fourth, self-refinement generates multiple answer candidates, with the GRPO critic selecting the best one. The method was evaluated on 95,955 samples from seven public datasets and downstream fine-tuning on six benchmarks.

## Key Results
- +6.1% accuracy improvement on MME benchmark
- +7.7% accuracy improvement on OCR-VQA benchmark
- GRPO critic achieves Kendall's tau 0.71 ranking fidelity vs GPT-4o while being two orders of magnitude more efficient

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Injecting explicit vision priors from specialized models improves critique accuracy by grounding evaluation in observable evidence rather than relying solely on LMM perception.
- **Mechanism:** RAM++ extracts object tags; PP-OCRv4 extracts text from images. These structured priors are appended to all subsequent prompts, anchoring critiques to verifiable visual facts.
- **Core assumption:** Specialized vision models remain more reliable than general LMMs for fine-grained perception tasks.
- **Evidence anchors:**
  - [abstract] "VERITAS leverages visual recognition models (RAM++) and OCR systems (PP-OCRv4) to extract structured vision priors"
  - [Section 5.2] "The prior raises OCR-VQA by +10.1 points and MME by +10.9"
  - [corpus] Related work confirms LMMs struggle with fine-grained visual perception (FMR=0.66 max correlation)
- **Break condition:** If specialized vision models produce noisy or contradictory outputs on your target domain, prior injection may introduce noise rather than clarity.

### Mechanism 2
- **Claim:** Domain-aware James-Stein shrinkage fusion of multi-expert scores produces more reliable ground truth labels than single-expert or simple averaging.
- **Mechanism:** (1) Z-normalize each critic within its domain; (2) compute signal-to-noise weights; (3) shrink toward corpus prior via empirical Bayes factor; (4) weighted average with percentile rescaling. This reduces variance in low-data domains while preserving unbiasedness.
- **Core assumption:** Individual critics have biased but complementary error distributions; shrinkage improves calibration.
- **Evidence anchors:**
  - [abstract] "assessments from three state-of-the-art LMMs... using domain-aware statistical fusion with James-Stein shrinkage"
  - [Appendix B.4] "The James-Stein shrinkage estimator does not increase the risk and typically reduces it"
  - [corpus] Corpus lacks direct comparative evidence on shrinkage fusion methods
- **Break condition:** If experts are highly correlated or systematically wrong in the same direction, fusion cannot correct shared biases.

### Mechanism 3
- **Claim:** GRPO-trained critics achieve near-GPT-4o ranking fidelity while being two orders of magnitude cheaper by learning relative comparisons rather than absolute scores.
- **Mechanism:** GRPO eliminates value-function estimation by comparing candidate answers against a group baseline from the old policy. Group-relative advantages provide low-variance, self-normalizing training signals.
- **Core assumption:** Relative ranking ability transfers better to out-of-domain data than absolute score regression.
- **Evidence anchors:**
  - [Section 5.3] "GRPO pushes the figure to 0.724 and Kendall's τ to 0.711, i.e. 89% of GPT-4o's fidelity"
  - [Section 5.3] On OOD data: "Lightweight GRPO Critic maintains Pearson r=0.628 vs. SFT critic's 0.312"
  - [corpus] Related work on R1-Reward confirms RL-trained reward models improve multimodal evaluation
- **Break condition:** If your downstream task requires absolute quality thresholds rather than relative ranking, GRPO's advantages may not transfer.

## Foundational Learning

- **Concept:** James-Stein Shrinkage Estimation
  - **Why needed here:** Core to the fusion mechanism that produces ground-truth scores from noisy expert judgments.
  - **Quick check question:** Can you explain why shrinking domain-specific weights toward a global mean reduces expected squared error?

- **Concept:** Group Relative Policy Optimization (GRPO)
  - **Why needed here:** Training objective for the lightweight critic; differs from standard PPO by using group baselines instead of learned value functions.
  - **Quick check question:** What advantage does GRPO's group-relative advantage computation provide over absolute reward prediction?

- **Concept:** Vision-Language Model Hallucination Patterns
  - **Why needed here:** Understanding why LMMs produce factual errors in visual contexts motivates the entire pipeline design.
  - **Quick check question:** Why might an LMM correctly describe an image's general scene while hallucinating specific text or object counts?

## Architecture Onboarding

- **Component map:** Stage 1 (Vision Prior Extraction) → Stage 2 (Tri-Expert Assessment + Shrinkage Fusion) → Stage 3 (GRPO Critic Training) → Stage 4 (Self-Refinement + Selection)

- **Critical path:** Stage 2 produces fused scores (Ŝ) that serve as ground truth for Stage 3 critic training. Errors in fusion propagate to critic; poor critic corrupts Stage 4 selection.

- **Design tradeoffs:**
  - Closed-source experts (GPT-4o, Gemini, Doubao) vs. open-source alternatives: +7.7 OCR-VQA gap reported
  - Filtering vs. rewriting: Filtering loses 46% of data; rewriting preserves coverage but adds API costs
  - Long prompts vs. efficiency: Comprehensive prompts improve quality but increase token costs

- **Failure signatures:**
  - Critic overfitting to in-domain data (SFT critic drops to r=0.312 OOD; GRPO maintains r=0.628)
  - Vision prior noise on out-of-distribution images
  - Expert consensus amplification of shared biases

- **First 3 experiments:**
  1. Ablate vision priors on your target domain: compare 1-Expert vs. 1-Expert(w/o prior) to measure prior contribution
  2. Validate fusion quality: compute correlation between fused scores and human annotations on 100-200 samples
  3. Test OOD critic generalization: evaluate trained critic on held-out domain using CLEVR-style error injection

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the comprehensive critique and rewrite prompts be optimized to reduce computational overhead while preserving the high-quality semantic and syntactic feedback necessary for effective multimodal data refinement?
- **Basis in paper:** [explicit] The authors state in the Limitations section: "Future work could consider how to optimize prompt design to maintain or enhance performance without significantly increasing computational costs."
- **Why unresolved:** The current implementation relies on long, context-heavy prompts to ensure accuracy, but this creates a trade-off with efficiency that has not been explored or benchmarked in the current study.
- **What evidence would resolve it:** Experiments comparing the current prompt structure against compressed or streamlined versions, measuring the trade-off between inference cost (FLOPs/latency) and the resulting improvement in downstream benchmark scores (e.g., OCR-VQA, MME).

### Open Question 2
- **Question:** To what extent does the reliability of the initial Vision-Prior Extraction (RAM++ and PP-OCRv4) act as a bottleneck, and can the Tri-Expert Assessment stage correct for hallucinated or incorrect vision priors rather than propagating them?
- **Basis in paper:** [inferred] The method assumes in Stage 1 that specialized vision experts provide "trustworthy" and "grounded evidence." However, OCR and tagging models are prone to specific errors (e.g., handwriting), and the paper does not analyze if the LMM critics can override these noisy priors.
- **Why unresolved:** The pipeline injects priors as textual context for the critics. If the priors contain errors, it is unclear if the experts will hallucinate based on that text or use the raw image to correct the context, risking error propagation.
- **What evidence would resolve it:** A robustness analysis where synthetic noise is injected into the OCR/Tag inputs (e.g., swapping characters or labels) to measure the failure rate of the final fused score and refined answer quality.

### Open Question 3
- **Question:** Can the VERITAS pipeline be adapted to close the performance gap between open-source expert trios and proprietary models (GPT-4o, Gemini) in reasoning-heavy tasks without relying on closed-source APIs?
- **Basis in paper:** [explicit] The authors identify the dependency on restricted proprietary models as a limitation. Additionally, the "VERITAS(open-source)" ablation shows a performance drop in reasoning tasks, indicating the "ceiling remains constrained by the strength of the... expert models."
- **Why unresolved:** While open-source models provide accessibility, the paper demonstrates they currently lack the reasoning fidelity to match the proprietary ensemble, particularly for complex tasks like MathVista.
- **What evidence would resolve it:** Research into iterative self-correction or specialized fine-tuning of the open-source experts (e.g., Qwen, InternVL) specifically for the critique task, aiming to match the Kendall's Tau correlation (0.71) achieved by the proprietary GRPO critic.

## Limitations

- **Vision Prior Bottleneck:** Performance heavily depends on RAM++ and PP-OCRv4 reliability, which may fail on unusual text layouts or non-Latin scripts.
- **Expert Fusion Assumptions:** James-Stein shrinkage cannot correct for shared biases among LMM critics if they have common blind spots.
- **Proprietary Dependencies:** Reliance on GPT-4o, Gemini-2.5-Pro, and Doubao-1.5-pro creates accessibility barriers and performance gaps with open-source alternatives.

## Confidence

**High Confidence** (Empirical support, reproducible):
- The 4-stage pipeline architecture and training procedures (vision priors, tri-expert assessment, GRPO training, self-refinement)
- Quantitative improvements on six benchmarks (+6.1% MME, +7.7% OCR-VQA, etc.)
- GRPO critic efficiency claims (2 orders of magnitude cheaper than GPT-4o)

**Medium Confidence** (Some assumptions, limited validation):
- The mechanism by which vision priors improve critique accuracy (relies on LMMs having consistent vision perception weaknesses)
- Domain-aware James-Stein shrinkage superiority over simpler fusion methods (no ablation comparison)
- The trade-off between data preservation (rewriting vs filtering) is justified by the +7.7% OCR-VQA gain

**Low Confidence** (Complex claims, minimal empirical backing):
- Long-term generalization to domains outside the 7 source datasets
- Performance on real-world data with significant domain shift
- The claim that GRPO's group-relative advantages transfer to all multimodal evaluation tasks

## Next Checks

1. **Ablation of Vision Priors:** Run VERITAS without RAM++ and PP-OCRv4 priors on a held-out domain to measure the true contribution of vision grounding versus expert consensus alone.

2. **Fusion Method Comparison:** Implement and compare alternative fusion strategies (simple averaging, weighted voting, Bayesian model averaging) against James-Stein shrinkage to quantify the specific benefit of shrinkage estimation.

3. **Stress Testing on OOD Data:** Systematically degrade vision quality in CLEVR-500 (add noise, occlusion, lighting changes) and measure how vision prior reliability affects the entire pipeline's performance degradation curve.