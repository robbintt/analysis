---
ver: rpa2
title: 'DiffCSS: Diverse and Expressive Conversational Speech Synthesis with Diffusion
  Models'
arxiv_id: '2502.19924'
source_url: https://arxiv.org/abs/2502.19924
tags:
- prosody
- speech
- context
- conversational
- proposed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating diverse and expressive
  conversational speech by proposing DiffCSS, a novel framework that leverages diffusion
  models for prosody prediction and an LM-based TTS backbone for high-quality speech
  synthesis. The key innovation is a diffusion-based context-aware prosody predictor
  that samples diverse prosody embeddings conditioned on multimodal conversational
  context (text and speech).
---

# DiffCSS: Diverse and Expressive Conversational Speech Synthesis with Diffusion Models

## Quick Facts
- arXiv ID: 2502.19924
- Source URL: https://arxiv.org/abs/2502.19924
- Reference count: 36
- One-line result: DiffCSS achieves MOS 3.602 expressiveness and NDB 4 prosody diversity, outperforming deterministic baselines on conversational speech synthesis.

## Executive Summary
This paper introduces DiffCSS, a novel framework for generating diverse and expressive conversational speech using diffusion models. The key innovation is a diffusion-based context-aware prosody predictor that samples varied prosody embeddings conditioned on multimodal conversational context (text and acoustic prosody). Combined with a prosody-enhanced ParlerTTS backbone, the system synthesizes speech with significantly improved expressiveness, coherence, and diversity compared to deterministic approaches. Experiments on the DailyTalk dataset demonstrate substantial improvements across multiple objective and subjective metrics.

## Method Summary
DiffCSS employs a two-stage training approach. First, a prosody-enhanced ParlerTTS backbone is pre-trained on LibriTTS-R (585h, 2,456 speakers) and fine-tuned on DailyTalk (2,541 conversations, ~20h). The backbone uses FACodec to extract frame-level prosody features, compresses them via cross-attention with learnable queries, and conditions the LM-based TTS decoder. Second, a diffusion-based prosody predictor is trained on DailyTalk using a Transformer encoder with cross-attention to multimodal context (text and acoustic prosody). The predictor samples diverse prosody embeddings through iterative denoising, which guide the TTS backbone to generate expressive speech. Audio is encoded/decoded using DAC codec (24 codebooks).

## Key Results
- MOS expressiveness: DiffCSS achieves 3.602 vs. 3.347 for deterministic baselines
- Prosody diversity: NDB reduced from 13 to 4 and JSD from 0.156 to 0.036 compared to deterministic approaches
- Contextual coherence: MOS coherence 3.574 vs. 3.362 for deterministic baselines
- Ground truth alignment: Generated prosody distribution aligns more closely with ground truth than deterministic methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diffusion models enable stochastic sampling of diverse prosody embeddings from identical conversational contexts, addressing the one-to-many mapping problem in CSS.
- Mechanism: The diffusion process learns to model the conditional distribution p(prosody | context) rather than predicting a single point estimate. During inference, sampling from Gaussian noise and iteratively denoising with the context-conditioned denoiser produces varied prosody embeddings that remain contextually valid.
- Core assumption: The conditional prosody distribution given conversational context is multimodal and learnable via score matching on diffused samples.
- Evidence anchors:
  - [abstract] "A diffusion-based context-aware prosody predictor is proposed to sample diverse prosody embeddings conditioned on multimodal conversational context."
  - [Section III.D] NDB reduced from 13 to 4 and JSD from 0.156 to 0.036 compared to deterministic baselines, showing generated prosody distribution aligns closer to ground truth.
  - [corpus] No direct corpus evidence on diffusion for CSS prosody; neighboring papers focus on retrieval and graph-based context modeling, not stochastic sampling.
- Break condition: If prosody diversity metrics (NDB, JSD) fail to improve over deterministic baselines during validation, the diffusion schedule or denoiser capacity may be insufficient.

### Mechanism 2
- Claim: Multimodal context fusion (text + acoustic prosody) through cross-attention enables the denoiser to generate contextually appropriate prosody.
- Mechanism: Context representation c = [s₁, p₁, ..., sₙ, pₙ] concatenates T5 sentence embeddings (textual) with prosody embeddings (acoustic) from prior turns. Cross-attention layers in the denoiser attend to this fused context while predicting noise.
- Core assumption: Prior conversational turns contain signal sufficient to condition appropriate prosody for the current utterance.
- Evidence anchors:
  - [Section II.B] "Multimodal context information c is incorporated through cross-attention."
  - [Section III.E] Ablation shows removing acoustic context degrades performance more than removing textual context (NDB: 10 vs 5, JSD: 0.129 vs 0.041), indicating acoustic prosody carries stronger signal.
  - [corpus] Multimodal Fine-grained Context Interaction Graph Modeling paper confirms multimodal context benefits CSS but uses graph networks, not diffusion.
- Break condition: If ablation shows neither modality improves over no-context baseline, context encoding (T5 or prosody extractor) is failing to capture relevant signal.

### Mechanism 3
- Claim: Prosody embeddings extracted via FACodec and compressed through cross-attention provide disentangled, tractable conditioning for the TTS backbone.
- Mechanism: FACodec extracts frame-level prosody features; learnable query tokens attend via cross-attention to produce fixed-length embeddings (m ≪ n). These guide the LM-based TTS through cross-attention in the decoder.
- Core assumption: FACodec features capture prosody information sufficiently disentangled from speaker and content.
- Evidence anchors:
  - [Section II.A] "To extract prosody features disentangled from other information, we employ the pre-trained FACodec."
  - [Section II.A] Cross-attention with learnable queries reduces computation by deriving fixed-length embeddings from variable-length frame features.
  - [corpus] No corpus evidence on FACodec specifically; neighboring papers use various codec approaches but don't evaluate prosody disentanglement quality.
- Break condition: If TTS quality degrades when conditioned on extracted prosody vs. reference speech, the disentanglement is insufficient or information is lost in compression.

## Foundational Learning

- Concept: **Denoising Diffusion Probabilistic Models (DDPM)**
  - Why needed here: The prosody predictor uses the standard diffusion training objective (predicting added noise) and DDPM sampling. Understanding the forward/reverse process is essential for debugging sampling quality.
  - Quick check question: Can you explain why the denoiser predicts noise ε rather than directly predicting z₀, and how the noise schedule αₜ affects sample diversity?

- Concept: **Cross-attention conditioning in Transformers**
  - Why needed here: Two critical components use cross-attention—prosody compression (queries attend to frame features) and context conditioning (denoiser attends to multimodal context).
  - Quick check question: Given query dimension d_q and key/value dimension d_kv, what is the computational complexity of cross-attention and how does sequence length affect it?

- Concept: **Neural audio codecs (DAC, FACodec)**
  - Why needed here: The system uses DAC for speech tokenization and FACodec for prosody extraction. Understanding codec bottlenecks and residual vector quantization is necessary for interpreting token quality.
  - Quick check question: What is the difference between DAC's role (speech tokenization for TTS) and FACodec's role (prosody feature extraction), and why can't one codec serve both purposes?

## Architecture Onboarding

- Component map: Input: Conversational history (text + speech) + current text → [Context Encoder] T5 (text) + FACodec + Cross-Attn (prosody embeddings) → [Diffusion Predictor] Transformer denoiser with cross-attention to context → [Prosody Embedding] Sampled from diffusion process → [Prosody-enhanced ParlerTTS] Decoder-only LM with cross-attention to prosody + speaker embeddings → Output: Synthesized speech via DAC decoder

- Critical path: Context encoding → Diffusion sampling (T iterations) → TTS generation → Audio decoding. The diffusion sampling adds T forward passes at inference; this is the latency bottleneck.

- Design tradeoffs:
  - Diffusion steps T: More steps improve sample quality but increase latency. Paper doesn't specify T value—check implementation.
  - Number of query tokens m: More tokens capture finer prosody but increase TTS computation. Paper uses m ≪ n without specifying exact values.
  - Two-stage training: Separating prosody predictor from TTS enables modular improvement but assumes frozen TTS prosody representation during predictor training.

- Failure signatures:
  - Mode collapse: Generated prosody embeddings cluster in few bins (high NDB) → denoiser underfitting or insufficient diffusion steps.
  - Context incoherence: High MOS variance across raters → cross-attention failing to integrate context, or context encoder producing weak signal.
  - Audio artifacts: MCD degradation → DAC reconstruction issues or TTS token prediction errors.

- First 3 experiments:
  1. **Reproduce baseline comparison**: Train the Transformer encoder baseline (deterministic) vs. diffusion predictor with identical context encoding. Verify NDB/JSD improvements before debugging anything else.
  2. **Ablate diffusion steps**: Sweep T ∈ {10, 50, 100, 500} and plot NDB/JSD vs. latency to find the Pareto frontier. This determines practical deployment constraints.
  3. **Visualize prosody embedding space**: t-SNE plot of ground truth, deterministic, and diffusion-generated embeddings. Confirm diffusion samples cover the ground truth distribution rather than just being different from deterministic outputs.

## Open Questions the Paper Calls Out

- Question: Can the iterative denoising process of the diffusion model be accelerated to meet the latency requirements of real-time conversational agents?
  - Basis in paper: [inferred] The methodology (Section II.B) describes an iterative denoising process (Equations 5 and 6) that requires $T$ steps to predict prosody embeddings, a computational characteristic of diffusion models that typically hinders real-time streaming applications.
  - Why unresolved: The paper does not report inference speed or Real-Time Factor (RTF) metrics, focusing instead on synthesis quality and diversity; thus, the feasibility of deploying this approach in low-latency interactive settings remains unverified.
  - What evidence would resolve it: A comprehensive latency analysis reporting RTF under different diffusion steps ($T$), or a comparative study integrating acceleration techniques (e.g., DDIM, consistency distillation) without degrading the MOS for coherence.

- Question: Does the two-stage training strategy, which freezes the TTS backbone, limit the model's ability to optimally adapt to the diffusion-predicted prosody embeddings?
  - Basis in paper: [inferred] Section II.C explicitly states that the TTS backbone parameters are frozen before training the diffusion-based prosody predictor to enhance controllability, preventing gradient flow between the prosody predictor and the acoustic generator.
  - Why unresolved: While this strategy decouples the modules, it creates a "feed-forward" bottleneck where the TTS backbone cannot learn to correct systematic errors or ambiguities in the prosody embeddings generated by the predictor.
  - What evidence would resolve it: An ablation study comparing the current frozen-backbone approach against an end-to-end fine-tuning strategy (where both modules are jointly optimized) using subjective coherence (C-MOS) and spectral distortion (MCD) metrics.

- Question: To what extent does the unsupervised prosody embedding space correspond to interpretable, human-perceivable prosodic attributes (e.g., emphasis, question intonation)?
  - Basis in paper: [inferred] The paper notes in Section II.A that the prosody extractor learns embeddings in an "unsupervised manner" from reference speech, and while diversity is demonstrated, the semantic meaning of the latent space dimensions is not analyzed.
  - Why unresolved: High NDB/JSD scores confirm diversity, but they do not guarantee that the diffusion model controls specific, meaningful prosodic functions rather than simply modeling random variance or noise.
  - What evidence would resolve it: An interpretability study using probing classifiers to determine if specific dimensions of the generated embedding $\hat{z}_0$ correlate strongly with annotated prosodic labels (e.g., pitch accent, duration, energy).

- Question: How does the model's performance scale with longer conversational histories beyond the fixed 4-turn context window used in experiments?
  - Basis in paper: [inferred] Section III.A states that conversations are divided into chunks containing 5 utterances, with the first 4 serving as context; this arbitrary truncation may discard relevant long-term dependencies present in extended dialogues.
  - Why unresolved: The cross-attention mechanism in the prosody predictor may struggle with fixed-length context optimization or computational complexity if the context length $N$ increases significantly.
  - What evidence would resolve it: Evaluation of C-MOS and NDB scores on test sets with variable, untruncated context lengths (e.g., 8, 16, or 32 turns) to determine if additional context improves coherence or introduces noise.

## Limitations
- Evaluation relies on subjective MOS ratings without standardized benchmarks for CSS diversity tasks
- Diffusion sampling adds significant computational overhead without performance scaling analysis
- FACodec disentanglement claim lacks quantitative validation
- DailyTalk dataset may not generalize to real-world scenarios with more speakers, topics, or turn structures

## Confidence
- **High confidence** in the core technical contributions: diffusion-based prosody sampling combined with multimodal context conditioning is novel and well-justified by the architectural description
- **Medium confidence** in the quantitative results: MOS, NDB, and JSD improvements are statistically significant but baselines are relatively simple deterministic approaches
- **Low confidence** in the generalization claims: effectiveness demonstrated only on DailyTalk conversations without validation on out-of-domain data or different turn counts

## Next Checks
1. **Diffusion sampling efficiency analysis**: Systematically sweep diffusion steps T from 10 to 500 and plot NDB/JSD versus inference latency to identify the practical T value for deployment
2. **Out-of-domain generalization test**: Evaluate DiffCSS on conversations from other datasets (e.g., VCTK, TED talks) or on DailyTalk conversations with different turn counts (3, 7, 10 utterances)
3. **FACodec feature quality validation**: Conduct a controlled ablation where prosody embeddings are replaced with random noise or speaker identity embeddings to quantify the TTS quality dependence on FACodec's disentanglement ability