---
ver: rpa2
title: 'FinAuditing: A Financial Taxonomy-Structured Multi-Document Benchmark for
  Evaluating LLMs'
arxiv_id: '2510.08886'
source_url: https://arxiv.org/abs/2510.08886
tags:
- financial
- llms
- value
- xbrl
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces FinAuditing, the first taxonomy-structured,
  multi-document benchmark for evaluating LLMs on financial auditing tasks. Built
  from real US-GAAP-compliant XBRL filings, it defines three tasks: FinSM for semantic
  consistency, FinRE for relational consistency, and FinMR for numerical consistency.'
---

# FinAuditing: A Financial Taxonomy-Structured Multi-Document Benchmark for Evaluating LLMs

## Quick Facts
- arXiv ID: 2510.08886
- Source URL: https://arxiv.org/abs/2510.08886
- Reference count: 40
- Primary result: Even top LLMs show 60-90% accuracy drops on hierarchical multi-document financial reasoning tasks

## Executive Summary
This paper introduces FinAuditing, the first taxonomy-structured multi-document benchmark for evaluating LLMs on financial auditing tasks. Built from real US-GAAP-compliant XBRL filings, it defines three tasks: FinSM for semantic consistency, FinRE for relational consistency, and FinMR for numerical consistency. The benchmark evaluates models on structured semantic retrieval, hierarchical relation understanding, and multi-step numerical reasoning. Zero-shot experiments on 13 state-of-the-art LLMs reveal significant performance gaps, with even top models like DeepSeek-V3, GPT-4o, and Fin-o1-14B showing substantial accuracy drops on hierarchical multi-document reasoning, exposing the limitations of current LLMs in taxonomy-grounded financial auditing.

## Method Summary
The benchmark is constructed from 372 real US-GAAP-compliant XBRL filings, with 1,102 annotated instances across three tasks. FinSM tests semantic consistency through retrieval augmentation, FinRE evaluates hierarchical relation understanding using XML linkbases, and FinMR assesses numerical consistency through schema-aware arithmetic reasoning. The evaluation uses zero-shot prompting with a maximum context window of 81,920 tokens, and employs an LLM-as-a-judge framework for FinMR scoring. Thirteen models including GPT-4o, DeepSeek-V3, and domain-specific models are evaluated using standardized metrics like Hit Rate, Macro-F1, and error rate decomposition.

## Key Results
- DeepSeek-V3 achieves highest FinSM Hit Rate at 11.89%, but overall performance remains low
- GPT-4o excels at FinRE with F1 scores of 72.36% (Reversal) and 83.16% (Inappropriateness)
- Fin-o1-14B achieves best FinMR accuracy at 52.65% but suffers 71% Structural Error Rate
- Financial domain models (Fin-o1-14B, Fin-R1) underperform general-purpose models on structured auditing tasks
- Open-source models show near-zero performance on CombinationErr relation classification

## Why This Works (Mechanism)

### Mechanism 1: Taxonomy-Grounded Semantic Retrieval (FinSM)
- **Claim:** Accurate error detection depends on aligning unstructured filing text with structured taxonomy definitions via retrieval augmentation
- **Core assumption:** LLMs possess sufficient internal knowledge of financial semantics to distinguish correct and incorrect tags once relevant taxonomy context is retrieved
- **Evidence:** DeepSeek-V3 achieves highest Hit Rate (11.89%), but overall performance remains low with best Recall at 8.83%
- **Break condition:** Retrieval mechanism fails to surface correct taxonomy definition due to chunking granularity, causing semantic alignment breakdown

### Mechanism 2: Hierarchical Relation Verification (FinRE)
- **Claim:** Detecting structural errors requires models to function as schema validators parsing hierarchical dependencies in XBRL linkbases
- **Core assumption:** Models can interpret XML-like structured data and maintain hierarchical relationships across long contexts
- **Evidence:** GPT-4o achieves high F1 on "Reversal" and "Inappropriateness" but struggles with "CombinationErr"
- **Break condition:** Performance degrades when models treat XML tags as text tokens rather than structural nodes

### Mechanism 3: Schema-Aware Arithmetic Reasoning (FinMR)
- **Claim:** Numerical verification requires dual-path process: extracting reported value then recomputing using calculation linkbases
- **Core assumption:** Models can handle multi-step logic: locate data, identify calculation rules, execute arithmetic without hallucination
- **Evidence:** High Structural Error Rate (71%) for Fin-o1-14B suggests schema interpretation failures despite calculation ability
- **Break condition:** Models cannot parse Calculation Linkbase to determine which elements sum to parent, leading to high Calculation Error Rates

## Foundational Learning

- **Concept: XBRL Filing Structure (Instance vs. Linkbases)**
  - **Why needed:** Benchmark treats filing as collection of six interconnected documents; understanding modular architecture is required to debug structural vs semantic errors
  - **Quick check:** Can you distinguish between value in Instance Document and calculation rule in Calculation Linkbase?

- **Concept: US-GAAP Taxonomy as a Knowledge Graph**
  - **Why needed:** Paper chunks taxonomy into "core" (attributes) and "relation" (edges) chunks; visualizing as graph is essential for FinRE
  - **Quick check:** If LLM identifies "Reversal" error, is it detecting typo or misplacement of node in hierarchy?

- **Concept: Zero-Shot Evaluation in Domain Contexts**
  - **Why needed:** Paper evaluates 13 models in zero-shot setting to establish baseline without fine-tuning; understanding difference from fine-tuned performance helps interpret "60-90% accuracy drop"
  - **Quick check:** Why might general-purpose model (GPT-4o) outperform domain-specific model (Fin-R1) on structural classification tasks?

## Architecture Onboarding

- **Component map:** Raw XBRL filings (372 filings) + US-GAAP Taxonomy (Excel/JSON) -> Preprocessor (splits into 6 document types, chunks taxonomy) -> Evaluator (LLM with prompt + relevant chunks) -> Judge (LLM-as-a-judge for FinMR, exact match for FinRE/FinSM)

- **Critical path:** Most brittle component is Data Segmentation. System must correctly map roleType IDs to specific financial statements to segment filing. If segmentation fails, LLM receives irrelevant context, causing low Recall in FinSM

- **Design tradeoffs:**
  - Chunking Strategy: Creates concept-centric chunks to fit long contexts. Trade-off: Smaller chunks may lose cross-document dependencies; larger chunks exceed context windows (81,920 token limit)
  - Metric Selection: LLM-as-a-judge for FinMR allows nuanced evaluation but introduces model-based bias compared to deterministic code execution

- **Failure signatures:**
  - Semantic Hallucination: Model tags concept as erroneous not because it violates taxonomy but because it misinterprets "core chunk" definition
  - Structural Blindness: Model correctly performs arithmetic but selects wrong numbers because it failed to traverse Presentation Linkbase

- **First 3 experiments:**
  1. Replicate FinSM Retrieval: Implement taxonomy chunking pipeline and test if embedding model (BGE-M3) can retrieve correct "core chunk" for XBRL tag to establish non-LLM baseline
  2. Isolate Linkbase Dependency: Run FinRE tasks by stripping away all linkbases except Presentation Linkbase to determine if hierarchical errors are primarily visual/structural or semantic
  3. Error Type Analysis: Focus specifically on "CombinationErr" type in FinRE (where most models collapsed to 0%) to analyze if failure is due to tokenization of XML attributes or lack of reasoning capability

## Open Questions the Paper Calls Out

- **Can retrieval-augmented generation improve performance on FinSM and FinRE tasks?** The authors note current LLMs struggle with both coverage and balance between precision and recall across all FinSM metrics, with average token lengths exceeding 33k per instance, suggesting potential value from retrieval or context optimization approaches. This remains unresolved as paper evaluates only zero-shot prompting without exploring architectural modifications.

- **Why do financial domain-specific models underperform general-purpose models on structured auditing tasks?** The authors observe that financial domain models (Fin-o1-14B and Fin-R1) perform poorly, with Fin-o1-14B yielding zero recall, indicating a mismatch between domain-specific training objectives and retrieval-oriented evaluation. The paper documents the performance gap but does not analyze whether domain fine-tuning on generative tasks harms retrieval and structural reasoning capabilities.

- **What specific structural reasoning capabilities are most deficient in current LLMs for hierarchical XBRL relationships?** The authors report that most open-source models struggle on CombinationErr, with many collapsing to near zero, indicating that axis-member validation across linkbases poses the greatest challenge. The paper identifies CombinationErr as hardest category but does not decompose whether failures stem from axis-member constraint understanding, cross-linkbase reasoning, or dimensional logic interpretation.

## Limitations

- Dataset availability: Paper states data is "available at Hugging Face" but does not provide direct link or repository name, making retrieval a search task
- Prompt formatting details: Paper provides core instruction prompt but does not detail how raw XML data from XBRL filings and taxonomy chunks are formatted and concatenated into final prompt context
- Input truncation: With average input lengths over 33k tokens, critical information may be cut off if not properly prioritized within 81k token limit

## Confidence

- **High Confidence:** Experimental design, evaluation metrics, and overall finding that current LLMs struggle with hierarchical multi-document reasoning in financial contexts
- **Medium Confidence:** Attribution of performance drops to specific mechanisms (taxonomy alignment, XML structure interpretation, calculation linkbase parsing) requires further validation through ablation studies
- **Low Confidence:** Direct comparison of model performance may be influenced by factors not fully controlled for, such as models' internal tokenization of XML and training data exposure to financial concepts

## Next Checks

1. Replicate Taxonomy Retrieval Baseline: Implement taxonomy chunking pipeline and test standard embedding model (BGE-M3) to establish non-LLM baseline for FinSM retrieval accuracy
2. Ablate Linkbase Dependencies: Run FinRE tasks by progressively stripping away linkbases (starting with Presentation Linkbase) to determine if hierarchical errors are primarily structural or semantic
3. Analyze CombinationErr Failure Mode: Focus specifically on "CombinationErr" type in FinRE, where most models failed, to analyze raw XML and model outputs for tokenization vs reasoning failure