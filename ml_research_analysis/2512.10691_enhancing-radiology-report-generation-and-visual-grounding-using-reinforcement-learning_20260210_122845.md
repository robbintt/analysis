---
ver: rpa2
title: Enhancing Radiology Report Generation and Visual Grounding using Reinforcement
  Learning
arxiv_id: '2512.10691'
source_url: https://arxiv.org/abs/2512.10691
tags:
- report
- radvlm
- arxiv
- thinking
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates using reinforcement learning (RL) with
  Group Relative Policy Optimization (GRPO) to improve a vision-language model (VLM)
  for chest X-ray (CXR) report generation and visual grounding. The authors start
  from a Qwen3-VL-based model, perform supervised fine-tuning (SFT) on CXR data to
  build RadVLM, then train a "thinking" variant, and finally apply GRPO with clinically
  grounded rewards (RadCliQ for reports, IoU-based soft-F1 for grounding).
---

# Enhancing Radiology Report Generation and Visual Grounding using Reinforcement Learning

## Quick Facts
- arXiv ID: 2512.10691
- Source URL: https://arxiv.org/abs/2512.10691
- Authors: Benjamin Gundersen; Nicolas Deperrois; Samuel Ruiperez-Campillo; Thomas M. Sutter; Julia E. Vogt; Michael Moor; Farhad Nooralahzadeh; Michael Krauthammer
- Reference count: 34
- Key outcome: GRPO with clinically grounded rewards improves radiology report generation and visual grounding beyond SFT-only training, with RadCliQ and soft-F1 rewards performing best.

## Executive Summary
This paper investigates using reinforcement learning with Group Relative Policy Optimizer (GRPO) to improve a vision-language model (VLM) for chest X-ray report generation and visual grounding. Starting from a Qwen3-VL-based model, the authors perform supervised fine-tuning (SFT) on CXR data to build RadVLM, then train a "thinking" variant, and finally apply GRPO with clinically grounded rewards (RadCliQ for reports, IoU-based soft-F1 for grounding). Experiments compare RadVLM variants with and without thinking, as well as general-domain Qwen3-VL models. Results show that GRPO consistently improves both tasks, with explicit thinking providing marginal gains at most. RadVLM+RL variants achieve state-of-the-art performance on most metrics, demonstrating that clinically aligned RL is a powerful complement to SFT for medical VLMs.

## Method Summary
The authors start with Qwen3-VL-8B-Instruct and perform two-stage training: first SFT on 1M+ RadVLM image-text pairs to create RadVLM, then cold-start SFT on synthetic thinking examples to create RadVLM-Thinking. They then apply GRPO with clinically grounded rewards - RadCliQ (composite of BERTScore, CheXbert similarity, RadGraph-F1) for report generation and Hungarian-matched soft-F1 for visual grounding. Training uses 8 rollouts per prompt, KL penalty of 0.01, and asymmetric clipping. They compare RadVLM variants with/without thinking against general-domain Qwen3-VL models.

## Key Results
- GRPO with RadCliQ reward consistently improves report generation metrics beyond SFT-only training
- GRPO with soft-F1 reward improves visual grounding mAP (84.5 vs 82.1 for SFT-only)
- Explicit thinking provides minimal or no improvement on these tasks
- RadVLM+RL variants achieve state-of-the-art performance on most metrics
- RadGraph-F1 reward alone triggers reward hacking, producing extremely short reports (93 chars vs 207 baseline)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GRPO with clinically grounded rewards improves report generation beyond SFT-only training
- Mechanism: SFT optimizes next-token likelihood, which is agnostic to clinical correctness. GRPO samples N outputs per image-prompt pair, computes task-specific rewards (RadCliQ), and uses group-relative advantages to update the policy. This directly optimizes for clinical alignment rather than token probability.
- Core assumption: RadCliQ composite metric correlates with clinical utility; reward signal is sufficiently differentiable to guide policy improvement.
- Evidence anchors:
  - [abstract] "RL provides additional gains on both tasks... highlighting clinically aligned RL as a powerful complement to SFT for medical VLMs"
  - [section 3.2] "For report generation, we use as a reward RadCliQ... which has been shown to align better with experts than any of these alone"
  - [corpus] Related work (MRG-R1, arXiv:2512.16145) reports similar findings: token-level training fails to guarantee clinical correctness, RL with clinical rewards helps.
- Break condition: If RadCliQ reward becomes gameable (e.g., models generate clinically plausible but incorrect text that scores high), performance gains would not transfer to real clinical utility.

### Mechanism 2
- Claim: Hungarian-matched soft-F1 reward provides stable, differentiable grounding supervision for bounding box prediction
- Mechanism: Standard mAP uses binary IoU thresholds, causing discontinuities. Soft-F1 assigns partial credit proportional to IoU for matched predictions via Hungarian algorithm. This smooth reward signal enables more stable gradient estimates during RL updates.
- Core assumption: Smooth reward landscape improves policy optimization compared to threshold-based metrics; partial credit for close matches is meaningful for medical grounding.
- Evidence anchors:
  - [section 3.2] "In contrast to mAP, which counts a prediction as correct only if its IoU exceeds a fixed threshold, this reward gives partial credit for close matches and avoids discontinuities"
  - [Table 2] RadVLM+RL achieves 84.5 mAP vs 82.1 for RadVLM SFT-only (+2.4 points)
  - [corpus] Weak corpus evidence—related work focuses on report generation, limited grounding-specific RL comparisons.
- Break condition: If partial credit enables "reward hacking" (e.g., generating many low-IoU boxes that collectively maximize reward without precise localization), grounding precision could degrade.

### Mechanism 3
- Claim: Explicit chain-of-thought ("thinking") provides marginal or no improvement over direct-answer models in this medical VLM setting
- Mechanism: Cold-start SFT teaches the model to generate intermediate reasoning traces before final answers. However, GRPO computes rewards only on final answers. If thinking traces don't provide informative gradients or if cold-start degrades base performance, thinking variants underperform.
- Core assumption: The synthesized thinking data quality is sufficient; thinking traces are causally linked to answer quality in medical imaging tasks.
- Evidence anchors:
  - [abstract] "explicit thinking does not appear to further improve results"
  - [section 4.4.1] "thinking models generally perform slightly worse than their instruct counterparts"
  - [section 5] "Many factors could be at play here: initial performance degradation due to cold-start fine-tuning, the thinking traces could be non-informative for the tasks at hand"
  - [corpus] DiagCoT (arXiv:2509.06409) shows chain-of-thought benefits for diagnostic reasoning—but uses different task framing.
- Break condition: If higher-quality thinking data (e.g., expert-annotated radiologist reasoning) were available, thinking could potentially help.

## Foundational Learning

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: Core RL algorithm replacing PPO's critic network with group-based advantage estimation
  - Quick check question: Can you explain why GRPO uses group mean/std normalization instead of a learned value function?

- Concept: **Reward hacking**
  - Why needed here: Table 3 shows RadGraph-F1 reward causes models to generate very short reports (93 avg characters vs 207 baseline), optimizing metric without improving quality
  - Quick check question: What monitoring would detect if RadCliQ reward is being gamed rather than genuinely improving clinical correctness?

- Concept: **Cold-start SFT for reasoning**
  - Why needed here: Required before GRPO to equip models with thinking capability; quality of this data affects downstream RL performance
  - Quick check question: Why might providing ground truth during thinking generation (Appendix I) bias the model?

## Architecture Onboarding

- Component map: Qwen3-VL-8B-Instruct -> SFT on RadVLM dataset -> RadVLM -> Cold-start SFT on thinking examples -> RadVLM-Thinking -> GRPO with RadCliQ/soft-F1 rewards -> RadVLM+RL
- Critical path: SFT quality -> base performance -> RL gain magnitude. Strong SFT (RadVLM) provides higher starting rewards and larger absolute gains than RL on general-domain Qwen3-VL.
- Design tradeoffs:
  - Thinking vs direct: Thinking adds inference cost (~2x tokens) without performance gains in this setting
  - Reward choice: RadCliQ balances lexical/clinical metrics; single metrics (RadGraph-F1) risk reward hacking
  - Batch size vs reward latency: RadCliQ requires 32 parallel copies to avoid bottleneck
- Failure signatures:
  - RadGraph-F1 reward -> very short reports (93 chars) = reward hacking detected
  - Thinking models lower reward early in training = cold-start quality issue
  - Qwen3-VL+RL generates long reports initially then shortens = RadCliQ discouraging noise
- First 3 experiments:
  1. Replicate Table 3 ablation: train RadVLM+RL with different rewards (RadCliQ, BERTScore, RadGraph-F1, GLEU), monitor response length and all metrics to detect reward hacking
  2. Ablate cold-start data quality: compare ground-truth-conditioned thinking generation vs. distillation-from-32B approach (Appendix J), measure downstream RL performance
  3. Test reward hacking robustness: add held-out clinical metric not in training reward (e.g., expert radiologist evaluation) to verify gains generalize beyond optimized metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does explicit thinking not improve performance on radiology tasks, and under what conditions might it become beneficial?
- Basis in paper: [explicit] Authors state: "explicit thinking does not appear to further improve results" and speculate about possible causes including "initial performance degradation due to cold-start fine-tuning, the thinking traces could be non-informative for the tasks at hand, a lack of vision modality in the thinking process, or the pre-training and post-training processes of VLMs such as Qwen3-VL lack radiology data compared to math and code."
- Why unresolved: The paper experiments with thinking models but does not isolate which factor causes the lack of improvement; only observes the phenomenon.
- What evidence would resolve it: Ablation studies varying thinking data quality, incorporating visual grounding in reasoning traces, and testing with expert-curated radiologist reasoning examples.

### Open Question 2
- Question: Can LLM-as-Judge rewards (e.g., GREEN) be designed to avoid reward hacking while enabling optimization of more complex clinical objectives like multi-turn conversations?
- Basis in paper: [explicit] Authors state: "LLM-as-Judge approaches require additional care to prevent reward hacking, to design robust prompts, and to select appropriate models. Yet, LLM-as-Judge is a powerful tool and could potentially enable optimization of multi-turn conversations."
- Why unresolved: The paper uses RadCliQ rather than LLM-as-Judge; the challenges of reward hacking with generative evaluators remain unaddressed.
- What evidence would resolve it: Experiments comparing LLM-as-Judge rewards against current metrics, measuring reward hacking incidence, and evaluating multi-turn conversational performance.

### Open Question 3
- Question: What is the optimal cold-start data generation strategy for equipping medical VLMs with reasoning capabilities?
- Basis in paper: [explicit] Authors identify "the choice of cold-start generation strategy" as a limitation and note that providing ground truth "potentially biasing the model toward generating CoT that directly leads to that specific answer."
- Why unresolved: The paper tries multiple strategies (with/without ground truth, one-stage vs. two-stage) but finds all underperform compared to non-thinking variants.
- What evidence would resolve it: Systematic comparison of cold-start strategies with human expert evaluation of reasoning quality, measuring transfer to unseen pathologies.

### Open Question 4
- Question: Can joint RL optimization of report generation and visual grounding yield mutual improvements compared to task-specific training?
- Basis in paper: [inferred] The paper applies GRPO "individually for each task and model variant" but does not explore whether simultaneously optimizing both tasks with a combined reward could leverage their complementary nature.
- Why unresolved: No experiments with multi-task RL or combined reward functions were conducted.
- What evidence would resolve it: Experiments using weighted combinations of RadCliQ and soft-F1 as a joint reward, comparing against separately trained models.

## Limitations

- Reward design sensitivity: RL outcomes are highly sensitive to metric design, as shown by RadGraph-F1 triggering reward hacking (extremely short reports)
- Single modality: All experiments use chest X-ray data only; generalization to other imaging modalities (CT, MRI, ultrasound) is unknown
- Thinking data quality: Cold-start SFT uses synthetic thinking data with undisclosed generation quality controls, potentially explaining why thinking variants underperform

## Confidence

- High confidence: GRPO with RadCliQ reward improves report generation metrics beyond SFT-only training
- Medium confidence: GRPO with soft-F1 reward improves visual grounding mAP
- Low confidence: Explicit thinking provides no benefit in this setting

## Next Checks

1. **Reward robustness test**: Train RadVLM+RL with multiple reward variants (RadCliQ, BERTScore, RadGraph-F1, GLEU) while monitoring response length, all report metrics, and a held-out clinical evaluation metric not in training rewards.

2. **Thinking data quality ablation**: Compare downstream RL performance when cold-start thinking data is generated via ground-truth-conditioned generation versus the distillation-from-235B approach.

3. **Cross-modal generalization**: Apply the same SFT+RL pipeline to a different medical imaging modality (e.g., radiology CT or dermatology images) using modality-specific datasets and metrics.