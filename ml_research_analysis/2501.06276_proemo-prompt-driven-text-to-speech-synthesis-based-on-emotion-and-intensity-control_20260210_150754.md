---
ver: rpa2
title: 'PROEMO: Prompt-Driven Text-to-Speech Synthesis Based on Emotion and Intensity
  Control'
arxiv_id: '2501.06276'
source_url: https://arxiv.org/abs/2501.06276
tags:
- speech
- emotion
- intensity
- control
- synthesis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating expressive speech
  synthesis with controllable emotion and intensity. The proposed method extends FastSpeech
  2 by incorporating emotion and intensity encoders trained with HuBERT features,
  enabling multi-speaker expressive speech generation.
---

# PROEMO: Prompt-Driven Text-to-Speech Synthesis Based on Emotion and Intensity Control

## Quick Facts
- arXiv ID: 2501.06276
- Source URL: https://arxiv.org/abs/2501.06276
- Reference count: 0
- Primary result: Multi-speaker expressive TTS with emotion/intensity control via HuBERT encoders and LLM-driven prosody scaling

## Executive Summary
This paper addresses the challenge of generating expressive speech synthesis with controllable emotion and intensity. The proposed method extends FastSpeech 2 by incorporating emotion and intensity encoders trained with HuBERT features, enabling multi-speaker expressive speech generation. During inference, large language models (GPT-4) predict prosody adjustments at both utterance and word levels to match target emotions. Objective evaluations show emotion classification accuracy improvements from 74.8% to 79.7% with local-level prompt control, while maintaining low word error rates (0.125-0.130) and character error rates (0.038-0.043). Human subjective tests indicate that the system with both global and local prompt controls achieves higher Mean Opinion Scores (3.728) for expressiveness and naturalness.

## Method Summary
The PROEMO system combines FastSpeech 2 with emotion and intensity encoders trained on HuBERT features, plus LLM-driven prosody control. The method involves pretraining a multispeaker FS2 on LibriTTS (900K steps), then fine-tuning on ESD (50K steps) with added HuBERT-based emotion and intensity encoders. Intensity labels are derived via a learned ranking function on acoustic features. At inference, GPT-4 predicts global and local scaling factors for pitch, energy, and duration, which are mapped to constrained ranges and applied to FS2's variance adaptor outputs before HiFiGAN vocoding.

## Key Results
- Emotion classification accuracy improves from 74.8% to 79.7% with local-level prompt control
- Word error rates remain low at 0.125-0.130 across systems
- Character error rates stay minimal at 0.038-0.043
- System with global+local controls achieves highest MOS (3.728) for expressiveness and naturalness
- Perceptual intensity ranking accuracy reaches approximately 72%

## Why This Works (Mechanism)

### Mechanism 1: Learned Emotion Embeddings via HuBERT Features
Emotion representations extracted from self-supervised speech features improve expressive synthesis when fused with the TTS backbone. HuBERT's transformer layers encode acoustic-phonetic information; a classification head trained on labeled emotional speech forces these representations to cluster by emotion category. The resulting embeddings are concatenated with speaker embeddings to condition the FS2 decoder.

### Mechanism 2: LLM-Scaled Prosody Adjustment at Two Granularities
GPT-4 can predict scaling factors for pitch, energy, and duration that produce perceptually distinct emotional prosody without degrading intelligibility. FS2's variance adaptor predicts per-phoneme duration, energy, and pitch. GPT-4 is prompted with text and target emotion to output global (utterance-level) and local (word-level) scaling factors, which are mapped via quadratic functions to constrained ranges before application.

### Mechanism 3: Relative Intensity Ranking from Acoustic Features
Continuous emotion intensity can be estimated without explicit labels by learning a relative ranking function on acoustic features. OpenSMILE extracts 384-dimensional acoustic features; a linear ranking function r(x) = Wx is learned via SVM-style optimization to order emotional vs. neutral utterances. This produces normalized intensity scores (0-1) for training the intensity encoder's regression head.

## Foundational Learning

- **FastSpeech 2 variance adaptor**
  - Why needed here: The variance adaptor is the intervention point for prosody control; understanding its outputs (duration, pitch, energy per phoneme) is prerequisite to modifying them via LLM.
  - Quick check question: Can you explain why FS2 allows non-autoregressive prosody modification compared to autoregressive TTS?

- **Self-supervised speech representations (HuBERT)**
  - Why needed here: HuBERT provides the feature backbone for emotion/intensity encoders; understanding what its layers capture informs whether to freeze convolutions vs. fine-tune transformers.
  - Quick check question: What linguistic/acoustic information is concentrated in HuBERT's later transformer layers versus early convolutional layers?

- **Prosody parameters (pitch, energy, duration)**
  - Why needed here: The LLM output directly manipulates these three parameters; intuition about their perceptual effects is essential for debugging scaling ranges.
  - Quick check question: Which parameter is most sensitive to over-scaling in terms of speech naturalness, and what does the paper conclude about its safe range?

## Architecture Onboarding

- **Component map:** Waveform -> HuBERT (frozen conv + fine-tuned transformer) -> Emotion/Intensity encoders -> FS2 (phoneme encoder -> Variance adaptor -> Mel decoder) -> HiFiGAN vocoder

- **Critical path:**
  1. Pre-train FS2 on LibriTTS (900K steps)
  2. Train intensity ranking function on ESD acoustic features
  3. Fine-tune FS2 + HuBERT encoders on ESD (50K steps)
  4. Inference: FS2 predicts prosody -> GPT-4 scales prosody -> HiFiGAN generates waveform

- **Design tradeoffs:**
  - Local-only vs. global+local control: Local-only achieves higher ECA (79.7% vs. 77.1%) but lower MOS (3.408 vs. 3.728); the paper suggests global+local is better for perceived naturalness.
  - Duration scaling range: Narrowed to [0.74, 1.34] after observing that duration is more sensitive than energy to over-scaling.

- **Failure signatures:**
  - High WER/CER with prompt control: Scaling ranges may be too aggressive; check if GPT-4 outputs are being mapped correctly.
  - Low emotion classification accuracy on generated speech: Emotion encoder may not have converged; verify HuBERT transformer layers were unfrozen during fine-tuning.
  - Inconsistent GPT-4 outputs: Prompt template instability; paper notes this required redesign with explicit reasoning steps.

- **First 3 experiments:**
  1. Ablate emotion encoder: Run FS2w/Emo&Int with and without emotion embeddings; expect ECA drop of ~5-10% if encoder is effective.
  2. Test scaling sensitivity: Systematically vary one parameter's range (e.g., duration [0.5, 2] vs. [0.74, 1.34]) while holding others constant; measure WER and MOS.
  3. Cross-dataset intensity generalization: Apply the learned ranking function to a different emotional speech dataset; check if intensity scores correlate with human perception.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the emotion and intensity encoders transfer effectively to typologically diverse languages (e.g., tonal languages like Mandarin) without requiring extensive retraining on labeled emotional speech corpora?
- Basis in paper: The conclusion explicitly identifies "Multilingual and Cross-cultural contexts" as a future research direction.
- Why unresolved: All experiments were conducted exclusively on English datasets (LibriTTS, ESD), and the emotion encoder was trained on English emotional speech patterns that may not generalize across languages with different prosodic conventions.
- What evidence would resolve it: Cross-lingual transfer experiments measuring ECA and MOS on non-English emotional speech datasets with minimal fine-tuning.

### Open Question 2
- Question: Can prosody prediction be achieved with comparable expressiveness using smaller, locally-deployed language models instead of GPT-4 API calls, thereby enabling real-time applications?
- Basis in paper: The conclusion mentions "exploring Real-time Applications" while the methodology notes "instability in GPT-4 output" requiring prompt engineering to mitigate noise.
- Why unresolved: The system depends on external GPT-4 API calls during inference, introducing latency and dependency that may preclude real-time conversational use cases.
- What evidence would resolve it: Benchmarks comparing prosody prediction quality and inference latency between GPT-4 and smaller fine-tuned models (e.g., Llama-7B) on the same control tasks.

### Open Question 3
- Question: What factors contribute to the 28% error rate in perceptual intensity ranking, and can the learned ranking function be improved to better align with human perception?
- Basis in paper: The PIR test shows approximately 72% accuracy, indicating a substantial gap between predicted intensity levels and human perception.
- Why unresolved: Intensity labels are derived algorithmically via relative ranking rather than human annotation, and the paper notes that "annotating intensity labels poses challenges, even for humans."
- What evidence would resolve it: Ablation studies comparing algorithmically-derived intensity labels against human-annotated ground truth, and analysis of systematic misperception patterns across emotion categories.

### Open Question 4
- Question: How does the naturalness-expressiveness trade-off manifest as emotional intensity increases, and are there optimal scaling bounds that preserve speech quality?
- Basis in paper: Table 1 shows MCD increases (worsens) with prompt control while ECA improves, suggesting a trade-off; the paper empirically narrowed duration scaling bounds to [0.74, 1.34] to preserve naturalness.
- Why unresolved: The bounds were determined empirically without systematic exploration of the quality-expressiveness frontier across different intensity levels.
- What evidence would resolve it: Systematic MOS and MCD evaluation across the full range of scaling factors for each prosodic dimension (pitch, energy, duration) at multiple intensity levels.

## Limitations

- Reliance on GPT-4 API calls introduces variability and potential reproducibility challenges
- Limited human subjective testing with only 10 subjects may not provide sufficient statistical power
- Intensity labels derived algorithmically lack ground truth validation on datasets beyond ESD
- No systematic exploration of the naturalness-expressiveness trade-off across different intensity levels

## Confidence

- **High Confidence:** The technical feasibility of combining HuBERT-based emotion encoders with FastSpeech 2 architecture. The general framework for intensity ranking using acoustic features is sound.
- **Medium Confidence:** The effectiveness of the two-level (global+local) LLM scaling approach compared to single-level alternatives. The specific parameter ranges chosen for duration, pitch, and energy scaling.
- **Low Confidence:** The reproducibility of GPT-4 prompt engineering and its consistent performance across different text inputs and emotional targets. The claim that the learned intensity ranking function produces perceptually meaningful scores without ground truth labels.

## Next Checks

1. **Scaling Function Sensitivity Analysis:** Systematically vary the quadratic mapping functions' coefficients and measure the impact on WER, MOS, and ECA to empirically justify the chosen parameter ranges.

2. **LLM Output Stability Test:** For a fixed set of 50 diverse text-emotion pairs, collect 10 independent GPT-4 responses each and compute the coefficient of variation for scaling factors. Correlate this stability metric with downstream synthesis quality metrics.

3. **Cross-Dataset Intensity Transfer:** Apply the learned ranking function to emotional speech from a different dataset (e.g., IEMOCAP) and conduct a perceptual study comparing the predicted intensity scores against human ratings to validate generalization beyond ESD.