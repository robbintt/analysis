---
ver: rpa2
title: Classification EM-PCA for clustering and embedding
arxiv_id: '2511.18992'
source_url: https://arxiv.org/abs/2511.18992
tags:
- data
- clustering
- embedding
- algorithm
- mixture
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of simultaneous data embedding
  and clustering, where traditional sequential approaches like PCA followed by K-means
  often fail due to misaligned objectives. The authors propose CEM-PCA, an algorithm
  that combines PCA-based dimensionality reduction with CEM clustering in a unified
  framework.
---

# Classification EM-PCA for clustering and embedding

## Quick Facts
- arXiv ID: 2511.18992
- Source URL: https://arxiv.org/abs/2511.18992
- Authors: Zineddine Tighidet; Lazhar Labiod; Mohamed Nadif
- Reference count: 40
- Primary result: CEM-PCA simultaneously optimizes PCA-based embedding and CEM clustering, achieving superior performance to sequential baselines and deep learning methods across diverse datasets.

## Executive Summary
The paper introduces CEM-PCA, an algorithm that unifies dimensionality reduction and clustering by jointly optimizing a PCA-based reconstruction objective with a CEM (Classification EM) clustering term. This simultaneous optimization addresses the misalignment between embedding axes and cluster separation that plagues sequential PCA-then-K-means approaches. The method leverages a regularization parameter to balance data reconstruction with clustering structure, achieving state-of-the-art performance on synthetic, image, biomedical, and text datasets.

## Method Summary
CEM-PCA extends PCA by incorporating a clustering regularization term into the objective function. The algorithm alternates between updating cluster means, cluster assignments via CEM, the embedding matrix via SVD, and the projection matrix. A key innovation is the coupling of the low-dimensional embedding B with cluster means M through a regularization parameter δ, ensuring the latent space geometry aligns with cluster boundaries. The method can also incorporate graph Laplacian smoothing to capture non-linear manifold structures.

## Key Results
- CEM-PCA significantly outperforms sequential PCA-K-means, reduced K-means, and deep learning approaches on benchmark datasets
- The method achieves near-perfect clustering performance (Acc, NMI, ARI) on USPS, COIL20, and other image datasets
- Experiments demonstrate robustness across diverse data types including synthetic, image, biomedical, and text datasets
- The approach offers improved interpretability for high-dimensional data by providing both meaningful clusters and low-dimensional embeddings

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Simultaneous optimization prevents misalignment between embedding axes and cluster separation found in sequential PCA-K-means.
- **Mechanism:** The algorithm couples PCA reconstruction with clustering using a regularization term δ||B-M||². This forces the low-dimensional embedding B to approximate the cluster means M, ensuring the latent space geometry aligns with the cluster boundaries.
- **Core assumption:** The structure of the classes is partially recoverable in a low-dimensional linear subspace, or can be smoothed into one via graph Laplacian preprocessing.
- **Evidence anchors:**
  - [abstract] "combining simultaneously and non-sequentially the two tasks –Data embedding and Clustering–"
  - [section III.C] "The second term ||B-M||²... is the key to achieve good embedding while taking the clustering structure into account."
- **Break condition:** If the regularization parameter δ is set to 0, the mechanism fails, reverting to standard PCA without clustering constraints.

### Mechanism 2
- **Claim:** Using Classification EM (CEM) instead of standard EM accelerates convergence while estimating Gaussian mixture parameters.
- **Mechanism:** CEM inserts a hard classification step (C-step) between the Expectation and Maximization steps. This converts posterior probabilities to hard assignments (z_ik ∈ {0,1}) before updating parameters, reducing the computational overhead of soft assignments typical in standard EM.
- **Core assumption:** The data distribution within clusters can be approximated by Gaussian components, justifying the GMM framework.
- **Evidence anchors:**
  - [abstract] "Classification EM (CEM)... offers a fast convergence solution"
  - [section III.B] "This leads for maximizing the complete-data log-likelihood... [with] fewer iterations."
- **Break condition:** If clusters are heavily overlapping with non-Gaussian shapes, the hard assignments of CEM may converge to poor local optima compared to soft EM.

### Mechanism 3
- **Claim:** The optimization admits a closed-form solution for the embedding matrix B via Singular Value Decomposition (SVD).
- **Mechanism:** By fixing other variables, the objective reduces to maximizing Tr((XQ + δM)Bᵀ). The solution is derived analytically as B* = UVᵀ, where UΔVᵀ is the SVD of (XQ + δM), avoiding gradient-based instability.
- **Core assumption:** The orthogonality constraint BᵀB = I holds, ensuring the latent dimensions remain uncorrelated.
- **Evidence anchors:**
  - [section IV.A.2] "The solution of Eq. (5) comes from the singular value decomposition (SVD) of (XQ + δM)."
- **Break condition:** If the matrix (XQ + δM) is ill-conditioned, numerical instability in the SVD could degrade the embedding quality.

## Foundational Learning

- **Concept: Gaussian Mixture Models (GMM) & EM Algorithm**
  - **Why needed here:** CEM-PCA replaces K-means with a probabilistic clustering backbone. Understanding how EM estimates means and covariances is required to interpret the "M-step" in the paper's loop.
  - **Quick check question:** Can you explain the difference between the E-step (expectation) and the C-step (classification) in the CEM algorithm?

- **Concept: Trace Maximization & Orthogonality**
  - **Why needed here:** The paper frames the PCA and embedding optimization as a trace maximization problem subject to orthogonality constraints (BᵀB = I).
  - **Quick check question:** Why does maximizing the trace of BᵀXᵀXB relate to finding the principal components of X?

- **Concept: Graph Laplacian Regularization**
  - **Why needed here:** The paper uses a Laplacian matrix W to smooth data (X ← WX) before processing, enabling the linear method to capture non-linear manifold structures.
  - **Quick check question:** How does multiplying the data by the adjacency/weight matrix W preserve local neighborhood structure?

## Architecture Onboarding

- **Component map:** Input X (optionally smoothed by W) → Latent Space (orthonormal B, projection Q) → Clustering Layer (means M, assignments Z, covariances Σ) → Output (embedded clusters)
- **Critical path:** The loop relies on the mutual update of M (clustering centroids) and B (embedding).
  1. Initialize B via PCA.
  2. Update M: Pull M towards B (Equation 9).
  3. Update B: Compute SVD of (XQ + δM) and set B = UVᵀ. This step fuses reconstruction (XQ) and clustering structure (δM).

- **Design tradeoffs:**
  - Parameter δ: High δ prioritizes clustering compactness over data reconstruction fidelity (risk: distorted embedding). Low δ acts like standard PCA (risk: missed cluster structure).
  - Dimensionality p: Must be large enough to capture cluster variance but small enough for noise reduction.

- **Failure signatures:**
  - Initialization Sensitivity: The paper notes using 20 initializations; single runs may converge to local optima.
  - Dimension Mismatch: If p (latent dimensions) is lower than the intrinsic dimension of the clusters, the projection Q will merge distinct classes.

- **First 3 experiments:**
  1. Baseline Replication (Chang Data): Run K-means on first 2 PCs vs. CEM-PCA to verify the "Chang" phenomenon where early PCs fail to separate classes.
  2. Sensitivity Analysis (δ): Sweep δ from 10⁻⁶ to 1 on a validation set to find the "slight regularization" sweet spot mentioned in Section V.C.
  3. Ablation (Graph Laplacian): Run CEM-PCA with and without the X ← WX smoothing on a non-linear dataset (e.g., two concentric rings) to test manifold adherence.

## Open Questions the Paper Calls Out

- **Question:** Can CEM-PCA be effectively adapted to uncover meaningful clusters within the intermediate layers of Large Language Models (LLMs) to enhance interpretability?
  - **Basis in paper:** [explicit] The conclusion states the method "could also offers promise for interpretability in large language models by uncovering meaningful clusters in embeddings and intermediate transformer layers."
  - **Why unresolved:** The paper focuses on benchmark datasets (images, text corpora) and does not implement or validate the method on actual LLM transformer architectures.
  - **What evidence would resolve it:** Experiments applying CEM-PCA to internal states of models like GPT or BERT, demonstrating semantic or functional clustering of neurons/layers.

- **Question:** Is there a theoretically grounded method for automatically determining the optimal latent dimensionality p without manual tuning?
  - **Basis in paper:** [inferred] The authors state in Section V.C that "In our experiments, p=10 appears to be a good choice," suggesting the parameter is currently set heuristically rather than derived from the data structure.
  - **Why unresolved:** The paper provides no objective function or statistical test (like the elbow method or eigen-gap) to automate the selection of p.
  - **What evidence would resolve it:** A robust criterion or algorithm that adapts p to the intrinsic dimensionality of specific datasets.

- **Question:** How sensitive is the convergence of CEM-PCA to the choice of the regularization parameter δ, and can this weighting be dynamically adjusted?
  - **Basis in paper:** [inferred] Section V.C notes that δ "has a significant impact" and requires grid searching values between 10⁻⁶ and 1.
  - **Why unresolved:** The paper treats δ as a hyperparameter to be tuned, rather than integrating it into the optimization as a learnable variable.
  - **What evidence would resolve it:** A sensitivity analysis showing performance variance across δ, or a modified algorithm that updates δ iteratively.

## Limitations

- The method's performance heavily depends on careful tuning of the regularization parameter δ, with suggested values requiring empirical validation
- Initialization sensitivity requires multiple runs (20 suggested), raising concerns about reproducibility with single executions
- The method assumes linear separability in the reduced space, which may not hold for highly non-linear manifolds even with Laplacian preprocessing

## Confidence

- **High Confidence:** The theoretical framework linking CEM-PCA to graph Laplacian regularization and the closed-form SVD solution for the embedding matrix
- **Medium Confidence:** The empirical superiority claims, given the extensive comparisons across diverse datasets, though reproducibility depends on hyperparameter tuning
- **Low Confidence:** The generalization to extremely high-dimensional, low-sample scenarios (e.g., genomics) where covariance estimation may become unstable

## Next Checks

1. **Hyperparameter Sensitivity Analysis:** Systematically vary δ across multiple orders of magnitude on benchmark datasets to map the performance landscape and identify the stability region

2. **Non-linear Manifold Test:** Apply CEM-PCA with Laplacian smoothing to a synthetic dataset with known non-linear structure (e.g., Swiss roll) and compare against non-linear methods like t-SNE+K-means to validate manifold adherence

3. **Covariance Robustness Test:** Implement CEM-PCA with diagonal or spherical covariance constraints and test on high-dimensional, low-sample datasets to assess stability against singular covariance matrices