---
ver: rpa2
title: 'BengaliFig: A Low-Resource Challenge for Figurative and Culturally Grounded
  Reasoning in Bengali'
arxiv_id: '2511.20399'
source_url: https://arxiv.org/abs/2511.20399
tags:
- reasoning
- cultural
- bengali
- answer
- riddles
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BengaliFig is a 435-item challenge set of Bengali riddles designed
  to evaluate figurative and culturally grounded reasoning in low-resource languages.
  Riddles were curated from oral and literary traditions, deduplicated, normalized,
  and converted to multiple-choice format via an AI-assisted pipeline.
---

# BengaliFig: A Low-Resource Challenge for Figurative and Culturally Grounded Reasoning in Bengali

## Quick Facts
- arXiv ID: 2511.20399
- Source URL: https://arxiv.org/abs/2511.20399
- Reference count: 40
- Key outcome: 435-item Bengali riddle set exposes persistent gaps in culturally grounded reasoning even in frontier LLMs.

## Executive Summary
BengaliFig is a deliberately curated 435-item challenge set of Bengali riddles designed to evaluate figurative and culturally grounded reasoning in low-resource languages. The dataset emphasizes riddles from oral and literary traditions that require resolving metaphors or cultural references, rather than factual recall. Each item is annotated across five orthogonal dimensions: reasoning type, trap type, cultural depth, answer category, and difficulty. Eight frontier LLMs were evaluated under zero-shot and few-shot chain-of-thought prompting. Results show that even top models struggle with metaphorical and culturally specific riddles, with accuracy dropping notably on culturally embedded and grapheme-constrained items. Few-shot CoT yielded limited gains, confirming that explicit reasoning prompts do not overcome deep cultural grounding gaps. BengaliFig thus serves as a culturally focused diagnostic probe for future inclusive NLP evaluation.

## Method Summary
Bengali riddles were scraped from web sources, deduplicated, and normalized. An AI-assisted pipeline converted them to multiple-choice format with four options each. Distractors were generated via constraint-aware AI, with grapheme counts enforced where applicable. Each item was annotated across five orthogonal dimensions using LLM assistance and human validation. Eight frontier LLMs (GPT-4.1, GPT-5, Claude variants, DeepSeek-V3, LLaMA-4, Qwen3) were evaluated under zero-shot and few-shot chain-of-thought prompting on the full set and a curated 30-item subset.

## Key Results
- Zero-shot accuracy ranged from 49.2% to 82.3%, with top models still struggling on culturally specific riddles.
- Culturally specific items scored ~10pp lower than universal items, highlighting a persistent reasoning gap.
- Grapheme-constrained riddles (28 items) saw sharp accuracy drops, exposing script-aware phonological weaknesses.
- Few-shot CoT on the hardest solvable subset showed limited improvement, suggesting reasoning prompts cannot bridge cultural grounding gaps.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** High-signal, low-volume challenge sets isolate specific reasoning failures better than broad benchmarks.
- **Mechanism:** By curating items requiring metaphor and cultural reference resolution, the dataset forces "System 2" reasoning. Orthogonal annotation allows failure attribution to specific cognitive gaps.
- **Core assumption:** Model failures reflect deep semantic or cultural representation gaps, not surface vocabulary.
- **Evidence anchors:**
  - [abstract] Targets "figurative and culturally grounded reasoning" to expose "consistent weaknesses."
  - [section 1] Argues "fewer deliberately curated examples can reveal failure modes that large corpora... do not reveal."
  - [corpus] Neighbor paper validates efficacy of traditional riddles as stress-tests.

### Mechanism 2
- **Claim:** Orthogonal multi-axis annotation enables precise failure attribution by disentangling linguistic complexity from cultural depth.
- **Mechanism:** Each item labeled across five independent dimensions, allowing isolation of cultural knowledge as a load factor.
- **Core assumption:** Annotated dimensions are independent and correctly identified.
- **Evidence anchors:**
  - [section 3.2] Describes "LLM-assisted framework" labeling across five orthogonal dimensions.
  - [section 4.1] Reports 10.0pp gap between universal and cultural-specific riddles.
  - [corpus] Neighbor "CRaFT" supports trend toward explanation-based evaluation.

### Mechanism 3
- **Claim:** Script-aware constraint generation exposes phonological weaknesses in non-Latin script processing.
- **Mechanism:** Pipeline extracts grapheme constraints (e.g., syllable counts) and generates distractors; LLM failure to meet constraints reveals phonological grounding gaps.
- **Core assumption:** Inability to generate grapheme-compliant distractors reflects fundamental phonological representation gaps.
- **Evidence anchors:**
  - [section 3.4] Describes "constraint-aware, AI-assisted pipeline" for grapheme counts.
  - [section 4.1] Reports no distractors met grapheme constraints, causing "sharp degradation."
  - [corpus] No direct corpus evidence; appears to be a unique diagnostic contribution.

## Foundational Learning

- **Concept:** **Bengali Orthography (Graphemes vs. Characters)**
  - **Why needed here:** Paper highlights LLMs fail on "grapheme count" constraints; graphemes represent syllables distinct from Unicode characters.
  - **Quick check question:** Does "কাজ" count as 2 characters or 2 graphemes? (Answer: 2 graphemes, ka-ja).

- **Concept:** **Figurative vs. Literal NLP**
  - **Why needed here:** Dataset punishes "surface literal" interpretation; must distinguish semantic similarity from pragmatic/metaphorical inference.
  - **Quick check question:** If a riddle describes "a horse running over a clock," does a literal model predict "time" or "broken clock"?

- **Concept:** **Chain-of-Thought (CoT) Prompting**
  - **Why needed here:** Paper evaluates "Few-shot CoT" with limited gains; understanding CoT is necessary to interpret why reasoning examples fail without cultural priors.
  - **Quick check question:** Why might providing reasoning examples fail to help if the model lacks underlying cultural priors to validate those steps?

## Architecture Onboarding

- **Component map:**
  Ingestion (web scraper -> raw riddles) -> Processing (deduplication -> normalization) -> Annotation (heuristic priors + DeepSeek-V3 -> human validators) -> MCQ Factory (constraint extractor -> DeepSeek-V3 -> GPT-4 -> final MCQ) -> Evaluation (API harness -> zero-shot/few-shot CoT).

- **Critical path:** Constraint Extraction & MCQ Generation stage is highest risk; distractor generation failed to meet grapheme constraints, invalidating "Trap Type" evaluation if options are too easy or obviously wrong.

- **Design tradeoffs:**
  - Size vs. Depth: Compact set (435 items) with rich 5-dimension annotation over larger, shallowly annotated set.
  - Cost vs. Heterogeneity: Used DeepSeek for generation and GPT-4 for selection to reduce self-endorsement bias.

- **Failure signatures:**
  - Surface-Literal Trap Susceptibility: Models scoring high on standard benchmarks may score near random (25-50%) here.
  - Graphemic Blindness: 28-item grapheme-constrained subset shows models selecting answers with incorrect syllable counts.
  - Cultural Gap: Consistent ~10% accuracy drop between universal and cultural-specific items.

- **First 3 experiments:**
  1. **Baseline Zero-Shot:** Run full 435-item set on target model to establish "Surface-Literal" vs. "Metaphorical" accuracy delta.
  2. **Constraint Ablation:** Isolate 28 grapheme-constrained riddles; evaluate if model ignores syllable constraint.
  3. **CoT Mismatch:** Apply Few-Shot CoT to "hardest yet solvable" subset; verify if reasoning trace improves selection or hallucinates grapheme counts.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can multimodal models grounded in vision and sound better solve culturally embedded riddles that implicitly reference perceptual and sensory attributes?
- **Basis in paper:** [explicit] Conclusion states: "many riddles evoke inherently multimodal reasoning... Future extensions could therefore explore how multimodal models grounded in language, vision, and sound handle such culturally embedded reasoning tasks in low-resource contexts."
- **Why unresolved:** BengaliFig is text-only; no multimodal evaluation was conducted despite many riddles encoding visual, auditory, or tactile cues.
- **What evidence would resolve it:** Evaluating vision-language models on a multimodal extension of BengaliFig, comparing performance against text-only baselines on riddles with perceptual content.

### Open Question 2
- **Question:** What is the human performance baseline on BengaliFig, and how does it compare to frontier LLMs across difficulty and cultural depth dimensions?
- **Basis in paper:** [explicit] Limitations state: "we did not obtain a human performance baseline... participation relied on voluntary sign-ups and we did not receive enough responses to draw meaningful conclusions."
- **Why unresolved:** Insufficient volunteer participation prevented meaningful human benchmarking; the 82.3% top LLM accuracy lacks contextualization against native speaker performance.
- **What evidence would resolve it:** A controlled user study with native Bengali speakers solving the full 435-item set, stratified by self-reported cultural knowledge, with accuracy reported per annotation dimension.

### Open Question 3
- **Question:** What architectural or training interventions enable LLMs to respect grapheme-count constraints in non-Latin scripts like Bengali?
- **Basis in paper:** [explicit] Paper reports: "Not a single riddle with a grapheme constraint received a complete set of options meeting that limit... The failure of LLMs to generate distractor options conforming to the grapheme count reveals their graphemic and phonological weakness in non Latin scripts."
- **Why unresolved:** Even frontier models failed orthographic constraint tasks (28-item subset showed sharp degradation), suggesting fundamental gaps in script-aware phonological representation.
- **What evidence would resolve it:** Controlled experiments testing whether explicit grapheme tokenization, fine-tuning on syllable-counting tasks, or incorporating phonological features improves constraint compliance in Bengali and similar scripts.

### Open Question 4
- **Question:** Would running few-shot chain-of-thought prompting on the full BengaliFig set reveal different patterns than the 30-item "hardest yet solvable" subset?
- **Basis in paper:** [explicit] Limitations state: "our evaluation of few-shot chain-of-thought (CoT) prompting was restricted to a curated subset of the hardest but solvable riddles... running few-shot CoT across the entire probe set could yield additional insights."
- **Why unresolved:** CoT showed ceiling effects and heterogeneous gains on the 30-item subset; broader evaluation could clarify whether patterns generalize or mask category-specific effects.
- **What evidence would resolve it:** Full-dataset few-shot CoT evaluation with per-dimension accuracy breakdowns, comparing against zero-shot to identify which annotation categories benefit most from explicit reasoning guidance.

## Limitations
- Dataset size (435 items) is relatively small for robust statistical analysis of model capabilities.
- Annotation process relies on LLM assistance, raising concerns about self-endorsement bias and accuracy of fine-grained dimension labeling.
- Evaluation focuses on a single language family (Bengali), limiting claims about cross-linguistic generalizability of observed reasoning gaps.

## Confidence
- **High confidence:** Observation that even frontier models struggle with metaphorical and culturally specific riddles is well-supported by reported performance gaps.
- **Medium confidence:** Claim that explicit reasoning prompts fail to overcome cultural grounding gaps is supported but limited by small sample size of curated subset.
- **Low confidence:** Diagnostic value of grapheme constraint failure is plausible but not definitively proven, as distractor generation failure may reflect pipeline errors versus model phonological representation gaps.

## Next Checks
1. Replicate zero-shot evaluation with full 435-item set across multiple model versions and providers to verify consistency of ~10pp cultural-specific accuracy gap.
2. Conduct human validation study of annotation dimensions on stratified sample of 50 riddles to assess inter-annotator agreement and annotation reliability.
3. Extend evaluation to a related low-resource language (e.g., Marathi or Tamil) to test whether observed reasoning failures are language-specific or reflect broader limitations in non-Latin script processing.