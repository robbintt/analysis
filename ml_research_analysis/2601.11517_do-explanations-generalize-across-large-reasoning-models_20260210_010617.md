---
ver: rpa2
title: Do explanations generalize across large reasoning models?
arxiv_id: '2601.11517'
source_url: https://arxiv.org/abs/2601.11517
tags:
- arxiv
- dapo
- reasoning
- transfer
- consistency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large reasoning model (LRM) chain-of-thought
  (CoT) explanations generalize across different models. The core method evaluates
  whether explanations from one LRM lead other LRMs to the same answers, measuring
  this via cross-model consistency.
---

# Do explanations generalize across large reasoning models?

## Quick Facts
- arXiv ID: 2601.11517
- Source URL: https://arxiv.org/abs/2601.11517
- Reference count: 40
- Primary result: Chain-of-thought explanations from large reasoning models improve cross-model consistency from ~25% to ~66% on MedCalc-Bench and from ~54% to ~62% on Instruction Induction.

## Executive Summary
This paper investigates whether chain-of-thought (CoT) explanations from large reasoning models (LRMs) generalize across different models. The core method evaluates whether explanations from one LRM lead other LRMs to the same answers, measuring this via cross-model consistency. Results show that CoT explanations do generalize—shared explanations increase answer consistency between models from ~25% to ~66% on MedCalc-Bench and from ~54% to ~62% on Instruction Induction. A sentence-level ensembling strategy further improves consistency, and more consistent explanations also correlate with higher human preference ratings and better performance after reinforcement learning post-training. These findings suggest that generalizable explanations exist but caution against over-relying on LRM explanations for novel insights, as they can still lead models to the same wrong answers.

## Method Summary
The study evaluates CoT generalization by transferring explanations between five LRMs (NRR, OpenT, OSS, QwQ, DAPO) across two benchmarks: MedCalc-Bench (medical calculations, exact match) and Instruction Induction (logic tasks, BERTScore). Four CoT methods are tested: Empty CoT (baseline), Default CoT (greedy/sampled), Transfer CoT (using another model's CoT), and Ensembled CoT (generator-evaluator loop with perplexity selection). Pairwise consistency and accuracy metrics are computed, with explicit answer removal via GPT-4o-mini before transfer evaluation. The ensembled approach generates 3 candidate sentences per step, selecting the lowest perplexity option until end-of-thought.

## Key Results
- Shared CoT explanations increase cross-model consistency from ~25% to ~66% on MedCalc-Bench and ~54% to ~62% on Instruction Induction
- Sentence-level ensembling strategy consistently improves consistency across all model pairs
- Higher consistency correlates with better human preference ratings and improved RL post-training performance
- Cross-model transfer accuracy improves significantly when models share consistent explanations

## Why This Works (Mechanism)
The paper demonstrates that LRMs can leverage consistent reasoning patterns from other models to improve their own performance. When explanations generalize well, they provide reliable intermediate reasoning steps that different models can follow to reach similar conclusions. The ensembling strategy further enhances this by selecting the most coherent and fluent reasoning paths across multiple generators, creating explanations that are both internally consistent and transferable between model architectures.

## Foundational Learning
- Cross-model consistency metrics: Measures how often two models give the same answer when given the same CoT
  - Why needed: Primary evaluation metric for generalization across models
  - Quick check: Compute pairwise answer matches between models given shared CoTs
- Perplexity-based ensembling: Selects sentences with lowest perplexity from multiple candidates
  - Why needed: Ensures selected explanations are both fluent and likely
  - Quick check: Compare perplexity scores of candidate sentences before selection
- Answer removal protocols: Strips explicit answers from CoTs before transfer
  - Why needed: Prevents direct answer leakage while preserving reasoning
  - Quick check: Verify transferred CoTs contain reasoning but no numerical conclusions

## Architecture Onboarding
**Component map:** LRM generator -> CoT extraction -> Answer removal -> Transfer to different LRM -> Consistency evaluation

**Critical path:** Problem input → CoT generation (per model) → Answer removal → Cross-model transfer → Pairwise consistency computation

**Design tradeoffs:** Sentence-level ensembling favors fluency over logical coherence, potentially sacrificing reasoning quality for transfer success. Using GPT-4o-mini for answer removal introduces dependency on proprietary models.

**Failure signatures:** Inconsistent CoTs lead to random answer patterns across models; poor answer removal results in direct answer transfer rather than reasoning generalization; ensembling may select grammatically correct but logically flawed sentences.

**First experiments:** 1) Run pairwise consistency baseline without any CoT sharing, 2) Test answer removal effectiveness by checking for numerical keywords in transferred CoTs, 3) Compare greedy vs. sampled decoding for Default CoT consistency

## Open Questions the Paper Calls Out
1. Can reinforcement learning post-training pipelines be explicitly modified to optimize for both reasoning accuracy and explanation generalizability simultaneously? Current RL objectives don't inherently reward transferable explanations.
2. Do more sophisticated ensembling strategies significantly improve explanation validity beyond the sentence-level perplexity method? Current approach may sacrifice logical coherence for fluency.
3. Does high cross-model consistency actually correlate with human generalization or merely reflect shared model biases? Current user study may not capture true knowledge transfer.
4. Can the evaluation framework identify concept-level explanations that generalize across diverse examples rather than just instance-level reasoning? Current study focuses on per-instance transfer.

## Limitations
- Several methodological parameters unspecified (nucleus sampling p-value, ensembled CoT termination criteria, RLVR hyperparameters)
- Reliance on proprietary GPT-4o-mini for answer removal introduces external dependency
- Human preference study limited to 15 participants across specific domains
- Current benchmarks may not capture true concept-level generalization needed for scientific discovery

## Confidence
- High: CoT explanations improve cross-model consistency in both benchmarks
- Medium: Correlation between consistency and human preference/RL post-training performance
- Medium: Ensembled CoT consistently outperforms individual generators

## Next Checks
1. Re-run full consistency analysis with multiple p-values for nucleus sampling to bound sensitivity
2. Implement open-source answer-removal method (e.g., Llama-3.1-8B-Instruct) to confirm no reliance on GPT-4o-mini
3. Perform ablation on ensembled CoT termination criteria (max sentences, stop token) to verify stability of consistency gains