---
ver: rpa2
title: How Order-Sensitive Are LLMs? OrderProbe for Deterministic Structural Reconstruction
arxiv_id: '2601.08626'
source_url: https://arxiv.org/abs/2601.08626
tags:
- semantic
- structural
- across
- recovery
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OrderProbe introduces a deterministic benchmark for evaluating
  large language models' structural reconstruction ability using fixed four-character
  expressions in Chinese, Japanese, and Korean. Unlike sentence-level restoration,
  these expressions have unique canonical orders enabling exact-match scoring.
---

# How Order-Sensitive Are LLMs? OrderProbe for Deterministic Structural Reconstruction

## Quick Facts
- **arXiv ID:** 2601.08626
- **Source URL:** https://arxiv.org/abs/2601.08626
- **Reference count:** 40
- **Key outcome:** Exact reconstruction of fixed four-character expressions remains difficult for LLMs, with zero-shot recovery frequently below 35%, revealing a clear dissociation between semantic recall and structural planning.

## Executive Summary
OrderProbe introduces a deterministic benchmark for evaluating large language models' structural reconstruction ability using fixed four-character expressions in Chinese, Japanese, and Korean. Unlike sentence-level restoration, these expressions have unique canonical orders enabling exact-match scoring. The benchmark includes 3,543 curated samples and a diagnostic framework evaluating semantic fidelity, logical validity, consistency, robustness, and information density. Experiments on twelve LLMs show that exact reconstruction remains difficult, with zero-shot recovery frequently below 35%, revealing a clear dissociation between semantic recall and structural planning. The framework enables fine-grained analysis of failure modes, demonstrating that structural robustness is not an automatic byproduct of semantic competence.

## Method Summary
OrderProbe uses 3,543 canonical four-character expressions from Chinese, Japanese, and Korean, with 23 non-identity permutations per expression (81,489 total samples). The task requires models to reconstruct the canonical order and provide a semantic explanation. Evaluation uses exact-match scoring for structural accuracy plus a diagnostic framework with six metrics: Semantic Fidelity, Logical Validity, Structural Consistency, Robustness, and Information Density. The benchmark was tested across twelve LLMs using three prompting strategies (zero-shot, chain-of-thought, 3-shot) at temperature=0, with no fine-tuning.

## Key Results
- Exact reconstruction rates remain low (<35%) across all models and prompting strategies
- Clear dissociation between semantic recall (often >80%) and structural reconstruction ability
- Korean Hangul performance collapses due to loss of local semantic anchors when characters are scrambled
- Zero-shot prompting yields the most reliable exact-match results despite lower overall accuracy

## Why This Works (Mechanism)

### Mechanism 1: Deterministic Proxy for Structural Planning
- **Claim:** Fixed four-character expressions with unique canonical orders act as deterministic proxies for evaluating structural reconstruction
- **Mechanism:** Exact-match scoring isolates structural planning from semantic generation by removing ambiguity present in sentence-level tasks
- **Core assumption:** Failure to reconstruct canonical order stems from structural planning deficits rather than knowledge gaps
- **Evidence anchors:** Abstract states expressions "have a unique canonical order and thus support exact-match scoring"; sentence-level restoration is "ill-posed for automated evaluation"

### Mechanism 2: Semantic-Structure Dissociation
- **Claim:** Models may retrieve correct semantic meaning while failing to reconstruct structural form
- **Mechanism:** Models treat scrambled characters as "bag of tokens" for semantic retrieval but fail to reassemble specific sequence required for canonical form
- **Core assumption:** High semantic scores verify model "knows" idiom, proving low reconstruction scores are structural failures
- **Evidence anchors:** Abstract mentions "consistent dissociation between semantic recall and structural planning"; models generate "fluent, meaning-aligned explanations while failing to reconstruct the canonical order"

### Mechanism 3: Local Anchor Preservation in Script Typology
- **Claim:** Reconstruction success is conditional on script's ability to preserve local semantic anchors when scrambled
- **Mechanism:** Logographic characters retain standalone meaning when position-shifted, providing "hooks" for reassembly; phonogrammatic scripts lose these anchors
- **Core assumption:** Tokenizer/model processes logographic characters as distinct semantic units
- **Evidence anchors:** Korean Hangul used as "phonogrammatic negative control"; logographic scripts provide "stronger local anchors"

## Foundational Learning

- **Concept: Exact-Match vs. Semantic Evaluation**
  - **Why needed here:** Standard benchmarks rely on semantic similarity which can be "gamed" by fluent but hallucinated explanations
  - **Quick check question:** Does "Yi-Jing-Ming-Ren" count as correct if meaning "famous overnight" is right but characters are "Yi-Ming-Jing-Ren"?

- **Concept: Permutation Sensitivity (Robustness)**
  - **Why needed here:** Paper evaluates all 23 non-identity permutations; robust model should handle any scramble
  - **Quick check question:** If I swap first and last characters of an idiom, does accuracy drop significantly compared to swapping middle two?

- **Concept: Chain-of-Thought (CoT) Format Drift**
  - **Why needed here:** CoT improves reasoning but can introduce "format drift" in weaker models, lowering exact-match scores
  - **Quick check question:** When asking model to "think step-by-step," does it output reasoning in final answer line, breaking exact-match parser?

## Architecture Onboarding

- **Component map:** Input Layer (scrambled string + prompt) -> LLM -> Parser (exact-match evaluator) -> Output
- **Critical path:** Prompt Template -> LLM -> Parser interface; prompt must enforce strict output formatting (e.g., "Output strictly 2 lines")
- **Design tradeoffs:** CoT vs. Zero-shot (CoT improves reasoning but risks format violations; Zero-shot cleaner but yields lower recovery); Metric Complexity (6 metrics offer granular analysis but are heavier to implement)
- **Failure signatures:** Literal Hallucination (high consistency, low logic score); Format Drift (valid reasoning but fails to output 4-char string on designated line); Generic Fallback (high consistency with low recovery, common in Korean)
- **First 3 experiments:** 1) Canonical Baseline (unscrambled inputs to verify model "knows" idioms, expect >85% semantic); 2) Script Comparison (ZH-CN vs KO to reproduce "local anchor" collapse, expect ~25% vs <6% recovery); 3) Permutation Stress Test (visualize recovery across 23 permutations to identify reliance on specific positions)

## Open Questions the Paper Calls Out

- **Open Question 1:** Does semantic-structure dissociation persist in alphabetic languages or morphologically rich systems where character scrambling affects surface forms differently?
  - **Basis:** Limitations section states influence of different script typologies on model performance "has not been thoroughly analyzed for alphabetic languages"
  - **Why unresolved:** Current study focuses exclusively on CJK languages, using Korean as phonogrammatic control rather than extending to alphabetic scripts
  - **What evidence would resolve it:** Application to fixed expressions in alphabetic languages (e.g., English idioms) or morphologically rich languages (e.g., Finnish)

- **Open Question 2:** Can LLMs be trained or prompted to apply structural reasoning to novel four-character combinations not in pretraining corpora?
  - **Basis:** Limitations section notes future developments may require models to "handle novel combinations beyond memorized content"
  - **Why unresolved:** Paper demonstrates frequency-stratified analysis but does not test generalization to synthetic or unseen expressions
  - **What evidence would resolve it:** Experiments using synthetic four-character strings with artificial grammatical rules

- **Open Question 3:** Which specific architectural modifications to positional encodings or attention mechanisms can mitigate structural reconstruction deficits?
  - **Basis:** Related Work connects findings to "positional limitations of Transformer architectures" and "Reversal Curse"
  - **Why unresolved:** Paper diagnoses failure mode but focuses on evaluation rather than proposing architectural solutions
  - **What evidence would resolve it:** Comparative study evaluating Transformer variants on OrderProbe benchmark

## Limitations

- Deterministic scoring creates artificial conditions that may not reflect real-world structural competence requirements
- Corpus construction relied on expert filtering without fully specified criteria thresholds or inter-rater reliability metrics
- Diagnostic framework metric weights (w1=0.5, w2=0.3, w3=0.2) appear heuristic rather than systematically validated

## Confidence

- **High Confidence:** Dissociation between semantic recall and structural planning is well-supported by consistent patterns across twelve models
- **Medium Confidence:** Logographic vs. phonogrammatic distinction shows clear effects, but mechanism explanation is somewhat speculative
- **Low Confidence:** Claims about fundamental limitations in LLMs' structural planning abilities extend beyond what benchmark can demonstrate

## Next Checks

1. **Tokenization Impact Study:** Run benchmark using different tokenization schemes (character-level, word-level, sub-word) on same expressions to determine if performance gaps stem from tokenization artifacts
2. **Variable-Length Generalization Test:** Extend methodology to five-character and three-character expressions to determine if dissociation scales with expression length
3. **Cross-Lingual Transfer Analysis:** Evaluate models using expressions from one script to predict canonical forms in another script (e.g., ZHâ†’JA transfer) to determine if structural competence transfers across writing systems