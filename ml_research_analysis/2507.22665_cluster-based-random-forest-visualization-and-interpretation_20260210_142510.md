---
ver: rpa2
title: Cluster-Based Random Forest Visualization and Interpretation
arxiv_id: '2507.22665'
source_url: https://arxiv.org/abs/2507.22665
tags:
- trees
- decision
- rules
- feature
- visualization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach for visualizing and interpreting
  random forests by clustering decision trees based on both their decision rules and
  prediction outcomes. The authors develop a new distance metric that incorporates
  interval overlap in decision rules, enabling semantically meaningful clustering
  of trees.
---

# Cluster-Based Random Forest Visualization and Interpretation

## Quick Facts
- **arXiv ID:** 2507.22665
- **Source URL:** https://arxiv.org/abs/2507.22665
- **Reference count:** 40
- **Primary result:** Introduces clustering-based RF visualization with new distance metric and rule-based aggregation plots, evaluated on Glass dataset (84% accuracy) and user study (good SUS/ICE-T scores).

## Executive Summary
This paper presents a novel approach to interpreting random forests by clustering decision trees based on both their decision rules and prediction outcomes. The authors introduce a new distance metric that incorporates interval overlap in decision rules, enabling semantically meaningful clustering of trees. Two new visualizations—the Feature Plot (feature usage frequency by depth) and the Rule Plot (aggregated decision rules via medoid mapping)—allow users to explore model behavior at multiple levels of aggregation. The system was evaluated using the Glass dataset and a user study with domain experts, demonstrating its utility for understanding feature importance, identifying misclassifications, and exploring model behavior.

## Method Summary
The method involves training a random forest using scikit-learn, extracting all decision rules (paths) from each tree, and computing a pairwise distance matrix using a novel interval-overlap metric that measures rule similarity. Trees are then clustered hierarchically using complete linkage, with a dynamic hybrid cut determining cluster boundaries. A medoid tree is selected for each cluster to serve as a structural scaffold, and rules from all trees in the cluster are mapped onto this medoid for visualization. The Feature Plot displays feature usage frequency stratified by tree depth, while the Rule Plot visualizes aggregated decision rules using opacity to indicate rule density.

## Key Results
- New distance metric based on interval overlap enables semantically meaningful clustering of decision trees
- Feature Plot and Rule Plot visualizations effectively communicate feature importance and decision rule patterns
- User study with domain experts showed good usability (SUS/ICE-T scores) and identified system as useful for understanding model behavior
- Successfully applied to Glass dataset (84% accuracy) and Penguin dataset

## Why This Works (Mechanism)

### Mechanism 1: Interval-Overlapping Distance Metric
The system calculates a distance matrix between trees using a custom metric that computes overlap of feature intervals defined by split points for rules leading to the same class. This ensures trees are grouped together only if they use similar ranges of feature values to reach the same conclusions, regardless of the exact topology of the tree.

### Mechanism 2: Representative-Tree Rule Projection
The algorithm selects a medoid tree as a visual anchor and maps every rule from every tree in the cluster to the closest rule in the medoid. The Rule Plot visualizes this density using opacity—darker regions indicate feature ranges used by many rules in the cluster.

### Mechanism 3: Depth-Segregated Feature Frequency (Feature Plot)
The Feature Plot divides visualization into rows corresponding to tree depths, with bar widths proportional to how often features are used as split points at each depth. This encodes feature importance (top rows) vs. specialization (bottom rows).

## Foundational Learning

**Concept: Random Forest Ensemble Logic**
- *Why needed:* To understand that the system visualizes a collection of classifiers, not a single model. Users must grasp that clusters represent subsets of the ensemble that vote similarly.
- *Quick check:* If Tree A uses `feature_x > 5` and Tree B uses `feature_x > 5.1` to classify the same class, should they be in the same cluster? (Answer: Ideally yes, if using the proposed interval metric).

**Concept: Hierarchical Clustering & Medoids**
- *Why needed:* The core navigation depends on users selecting the level of aggregation by cutting a dendrogram. Understanding this helps interpret cluster stability.
- *Quick check:* What does the "representative tree" (medoid) represent in terms of distance to other trees in the cluster?

**Concept: Interval Arithmetic (Overlap)**
- *Why needed:* To interpret the distance metric. Users must understand that similarity is defined by shared decision boundaries (ranges), not just shared features.
- *Quick check:* Does an interval distance of 0 mean the trees are identical, or just that their decision boundaries for a specific rule are identical?

## Architecture Onboarding

**Component map:** Sidebar (global stats, projection, cluster control) -> Cluster View (Feature Plot + Rule Plot) -> Instance View (individual node-link diagrams)

**Critical path:**
1. Ingest dataset → Train RF (scikit-learn)
2. Extract rules (paths) from all trees
3. Compute pairwise distance matrix using interval-overlap metric
4. Cluster using hierarchical linkage → Dynamic tree cut
5. Select Medoids for each cluster
6. Render aggregation plots (map rules to medoid)

**Design tradeoffs:**
- Color Scalability: Uses sequential color scales for features/classes because categorical colors become indistinguishable past ~10-12 categories
- Aggregation vs. Loss: Rule Plot keeps medoid structure but overlays opacity, trading structural fidelity for density information

**Failure signatures:**
- "Washout" in Rule Plot: Uniformly light heatmap indicates too heterogeneous cluster
- Color Collision: Features and classes sharing similar hues due to sequential scaling limits

**First 3 experiments:**
1. Cluster Stability Test: Vary "minimum cluster size" slider. Do representative trees change drastically, or do clusters merge/split cleanly?
2. Rule Projection Verification: Select a specific rule in Rule Plot. Filter data to match that rule. Do individual trees confirm this logic?
3. Feature Importance Correlation: Compare top features in Feature Plot against standard global feature importance metrics. Are they consistent?

## Open Questions the Paper Calls Out

**Open Question 1:** How does the proposed rule-based distance metric incorporating interval overlap perform compared to semantic and structural metrics across diverse real-world datasets? (The paper notes there is no in-depth comparison of different semantic, structural, and rule-based approaches.)

**Open Question 2:** Can the Feature Plot and Rule Plot visualizations be effectively adapted to other tree-based ensemble methods, such as Gradient Boosted Trees? (The authors note it would be interesting to explore how their approach can be adapted to other rule-based and tree-based models.)

**Open Question 3:** What alternative visual encoding strategies can overcome the system's limitation regarding the number of distinguishable features and classes? (The paper notes it is limited by using color to represent features and classes, becoming difficult as categories increase.)

## Limitations

- Color encoding becomes ineffective for datasets with many features or classes (>10-12 categories)
- Scalability challenges with very large forests due to pairwise distance computation
- Interval overlap metric assumes continuous features; categorical features require different handling
- User study sample size is small (6-8 experts), limiting generalizability of usability findings

## Confidence

The core claims about interval-overlap distance metrics and medoid-based aggregation are **High** confidence for tested datasets, but generalization to categorical features or very large forests is **Low** due to untested assumptions. The user study results (SUS/ICE-T scores) support usability but sample size is small (6-8 experts), making generalizability **Medium**. The absence of comparative evaluations against existing RF visualization tools is a notable gap.

## Next Checks

1. **Scalability Stress Test:** Apply the system to a larger dataset (e.g., 10+ features, 1000+ trees) and measure MDS projection quality and clustering stability.
2. **Metric Comparison:** Compare the interval-overlap distance to simpler alternatives (e.g., Jaccard on feature splits) on a held-out test set to validate semantic meaningfulness.
3. **Categorical Feature Handling:** Test the distance metric on a dataset with categorical features (e.g., UCI Voting Records) to assess robustness.