---
ver: rpa2
title: 'MedChat: A Multi-Agent Framework for Multimodal Diagnosis with Large Language
  Models'
arxiv_id: '2506.07400'
source_url: https://arxiv.org/abs/2506.07400
tags:
- clinical
- diagnostic
- medical
- glaucoma
- report
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MedChat is a multi-agent framework for automated glaucoma diagnosis
  from retinal fundus images that combines specialized vision models with role-specific
  LLM agents coordinated by a director agent. The system addresses the limitations
  of single-agent LLM approaches in medical imaging, such as hallucinations, limited
  interpretability, and insufficient domain-specific knowledge, by emulating multidisciplinary
  clinical workflows.
---

# MedChat: A Multi-Agent Framework for Multimodal Diagnosis with Large Language Models

## Quick Facts
- arXiv ID: 2506.07400
- Source URL: https://arxiv.org/abs/2506.07400
- Reference count: 40
- Primary result: Multi-agent framework for automated glaucoma diagnosis from retinal fundus images that improves reliability and reduces hallucination risk compared to single-agent LLM approaches

## Executive Summary
MedChat is a multi-agent framework for automated glaucoma diagnosis from retinal fundus images that combines specialized vision models with role-specific LLM agents coordinated by a director agent. The system addresses the limitations of single-agent LLM approaches in medical imaging, such as hallucinations, limited interpretability, and insufficient domain-specific knowledge, by emulating multidisciplinary clinical workflows. MedChat processes fundus images through a glaucoma classifier and optic disc/cup segmentor, then distributes the findings to specialized agents (ophthalmologist, optometrist, pharmacist, glaucoma specialist) that generate domain-specific sub-reports, which are synthesized into a comprehensive diagnostic report by the director agent.

## Method Summary
The framework processes retinal fundus images through a SwinV2 classifier for glaucoma probability and a SegFormer segmentor for optic disc/cup segmentation. These outputs are verbalized into clinical language (probability grades and cup-to-disc ratio) and combined with optional clinical notes to form a core prompt. A GPT-4.1 query identifies relevant clinical roles, then parallel GPT-4.1 agents generate sub-reports with role-specific constraints to minimize redundancy. A director agent synthesizes these sub-reports into a final diagnostic report, identifying consensus and resolving minor contradictions.

## Key Results
- Demonstrates improved reliability and reduced hallucination risk compared to single-agent systems
- Produces clinically-grounded reports that integrate diverse medical perspectives
- Provides actionable recommendations based on image-derived features like cup-to-disc ratio and glaucoma probability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Verbalizing vision model outputs into structured natural language improves LLM reasoning fidelity over medical images.
- Mechanism: Discretizing classifier probabilities into diagnostic grades (e.g., "possible glaucoma" for 0.2 ≤ p < 0.5) and converting segmentation masks into CDR values grounds LLM reasoning in quantitatively-derived, clinician-interpretable terms rather than raw model outputs.
- Core assumption: LLMs reason more reliably over qualitative clinical language than over raw probabilities or pixel maps.
- Evidence anchors:
  - [abstract]: "MedChat processes fundus images through a glaucoma classifier and optic disc/cup segmentor, then distributes the findings to specialized agents"
  - [Section III-B]: "Verbalizing the model's output parallels qualitative phrasing in clinical reports and helps LLMs reason more effectively over diagnostic content [4]"
  - [corpus]: Citrus-V paper similarly emphasizes "Unified Medical Image Grounding for Clinical Reasoning" as a mechanism for aligning visual evidence with language outputs.
- Break condition: If verbalization thresholds (e.g., p < 0.2 → "no glaucoma") do not align with clinical decision boundaries, the linguistic framing may mislead downstream agents.

### Mechanism 2
- Claim: Role-specific prompting with scope constraints reduces redundancy and encourages complementary perspectives across agents.
- Mechanism: Each agent receives the same core prompt augmented with role-specific instructions that constrain responses to domain-relevant observations, explicitly directing agents to "avoid repeating what is not within your scope."
- Core assumption: Agents will comply with scope instructions and possess sufficient implicit domain knowledge to generate meaningful specialist perspectives.
- Evidence anchors:
  - [Section III-C]: "These instructions are designed to maintain a professional clinical voice, avoid artificial references to model components, and prevent overlap across agent outputs"
  - [Section III-C]: "By encouraging each agent to contribute findings unique to its specialty, MedChat promotes breadth and minimizes redundancy"
  - [corpus]: Tree-of-Reasoning paper addresses multi-agent reasoning depth but does not explicitly test scope-constrained prompting for role differentiation.
- Break condition: If agents lack domain-specific fine-tuning (noted as a limitation in Section VI), role outputs may converge superficially despite prompt constraints.

### Mechanism 3
- Claim: Director-agent synthesis improves report coherence and can correct minor inconsistencies across sub-reports.
- Mechanism: The director agent receives concatenated sub-reports and is instructed to "identify areas of consensus, resolve minor contradictions, and produce a summary" while omitting explicit references to source sub-reports.
- Core assumption: A single synthesis agent can reliably detect and resolve contradictions without introducing new hallucinations.
- Evidence anchors:
  - [Section III-D]: "It can correct minor inaccuracies or inconsistencies that may appear in individual sub-reports and synthesize novel insights across roles into a unified and clinically appropriate plan"
  - [Section VI]: "Since all agents receive the same shared prompt, their outputs often show a high degree of consensus, limiting the diversity of reasoning" (acknowledged limitation)
  - [corpus]: MedCoAct paper proposes "Confidence-Aware Multi-Agent Collaboration" but does not directly validate director-level correction mechanisms.
- Break condition: If sub-reports contain substantive (not minor) contradictions reflecting genuine clinical ambiguity, forced consensus may obscure uncertainty.

## Foundational Learning

- Concept: **Vision-Language Grounding**
  - Why needed here: Understanding how to bridge image-derived features (segmentation masks, probability scores) with language model inputs is essential for debugging the prompt construction pipeline.
  - Quick check question: Can you explain why the CDR is computed as √(|Mcup| / (|Mcup| + |Mdisc|)) rather than a direct area ratio?

- Concept: **Multi-Agent Coordination Patterns**
  - Why needed here: MedChat uses a hierarchical pattern (parallel specialists → sequential synthesis); knowing alternative patterns (debate, voting, iterative refinement) helps evaluate tradeoffs.
  - Quick check question: What failure mode might occur if all agents receive identical prompts without role-specific augmentation?

- Concept: **Hallucination Mitigation Strategies**
  - Why needed here: The framework claims reduced hallucination risk; understanding grounding techniques (evidence citation, constrained decoding) is necessary to assess whether claims hold.
  - Quick check question: Does MedChat's design prevent hallucinations, or does it shift responsibility for detection to the director agent?

## Architecture Onboarding

- Component map:
  Vision Backend -> Prompt Constructor -> Role Generator -> Parallel Specialist Agents -> Director Agent -> Output

- Critical path: Image input → classifier + segmentor → prompt construction → role generation → parallel sub-report generation → director synthesis → output. The vision-to-prompt stage is the primary grounding bottleneck.

- Design tradeoffs:
  - All agents share identical core prompts (ensures consistency but limits reasoning diversity, per Section VI)
  - No domain-specific fine-tuning (faster deployment but reduced clinical precision)
  - Director agent resolves contradictions post-hoc rather than surfacing uncertainty explicitly

- Failure signatures:
  - High consensus across sub-reports with minimal role differentiation (indicates prompt constraints insufficient)
  - Director report introduces claims not present in any sub-report (synthesis hallucination)
  - Generic responses when clinical notes absent (Section VI acknowledges this limitation)

- First 3 experiments:
  1. **Ablation on verbalization thresholds**: Vary Grade(p) boundaries and measure impact on downstream agent agreement and clinical plausibility.
  2. **Role diversity quantification**: Compute lexical/semantic overlap between sub-reports with and without role-specific constraints to validate scope enforcement.
  3. **Director consistency check**: Inject controlled contradictions into synthetic sub-reports and measure director's resolution accuracy vs. hallucination rate.

## Open Questions the Paper Calls Out
None

## Limitations
- Clinical utility depends critically on vision model accuracy, which is not fully described
- Shared prompt across agents may artificially constrain reasoning diversity
- Director agent could potentially mask genuine clinical uncertainty rather than surfacing it
- Only validated on glaucoma diagnosis from fundus images, performance on other conditions unknown

## Confidence
- **High confidence**: Verbalization of vision model outputs into clinical language is a sound approach for grounding LLM reasoning
- **Medium confidence**: Role-specific prompting effectively reduces redundancy and encourages complementary perspectives
- **Low confidence**: Director agent reliably improves report coherence and corrects minor inconsistencies without introducing new hallucinations

## Next Checks
1. **Vision model robustness validation**: Test the glaucoma classifier and segmentation models across diverse patient populations and imaging conditions to quantify their accuracy and failure modes.
2. **Role differentiation measurement**: Conduct a controlled study comparing sub-report overlap with and without role-specific constraints, measuring both semantic diversity and clinical relevance.
3. **Uncertainty surfacing evaluation**: Systematically inject clinically ambiguous cases with conflicting evidence into the pipeline and assess whether the director agent appropriately surfaces uncertainty versus forcing artificial consensus.