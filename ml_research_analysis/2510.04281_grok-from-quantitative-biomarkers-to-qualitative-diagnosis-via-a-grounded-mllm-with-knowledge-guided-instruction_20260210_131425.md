---
ver: rpa2
title: 'GROK: From Quantitative Biomarkers to Qualitative Diagnosis via a Grounded
  MLLM with Knowledge-Guided Instruction'
arxiv_id: '2510.04281'
source_url: https://arxiv.org/abs/2510.04281
tags:
- grok
- retinal
- biomarkers
- clinical
- instruction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# GROK: From Quantitative Biomarkers to Qualitative Diagnosis via a Grounded MLLM with Knowledge-Guided Instruction

## Quick Facts
- arXiv ID: 2510.04281
- Source URL: https://arxiv.org/abs/2510.04281
- Reference count: 27
- Primary result: GROK achieves Macro F1 0.25 (max 0.37) on 6-category fundus disease classification via quantitative-to-qualitative reasoning chain.

## Executive Summary
This paper presents GROK, a Multimodal Large Language Model that translates quantitative retinal biomarkers into qualitative clinical diagnoses by integrating Color Fundus Photography (CFP) and Optical Coherence Tomography (OCT) with knowledge-guided instruction tuning. The model employs a three-stage pipeline: (1) expert-crafted prompts and OpenAI o3 generate synthetic instruction data linking biomarkers to diagnoses; (2) OCT slices are contrastively aligned with 3D biomarker vectors to improve precision; and (3) the LLM backbone is fine-tuned using LoRA to learn the quantitative-to-qualitative reasoning chain. GROK outperforms baselines in clinical quality metrics and demonstrates the potential of grounded MLLMs for automated retinal disease diagnosis.

## Method Summary
GROK is trained in three stages: (1) Knowledge-Guided Instruction Generation uses extracted OCT/CFP biomarkers and images with expert-crafted prompts to generate synthetic chain-of-thought instruction data via OpenAI o3; (2) OCT-Biomarker Alignment employs contrastive learning to align 2D OCT B-scans with 3D biomarker vectors, improving quantitative estimation; (3) Supervised Fine-Tuning trains a Qwen2-7B-Instruct backbone with frozen visual encoders and LoRA adapters on the generated instruction data. The model jointly processes CFP and OCT through independent encoders, projecting their embeddings to the LLM for multimodal reasoning.

## Key Results
- GROK achieves Macro F1 0.25 on 6-category fundus disease classification, outperforming baselines.
- Evidence Grounding score improves from 36.31 (OCT-only) to 68.70 with dual-modality input.
- OCT-Biomarker Alignment reduces biomarker regression MAE from 5.0785 to 3.6069 compared to non-aligned RETFound.

## Why This Works (Mechanism)

### Mechanism 1: Quantitative-to-Qualitative Chain-of-Thought Distillation
Distilling reasoning from a high-capacity teacher model (OpenAI o3) using expert-verified prompts enables a smaller model (Qwen2-7B) to learn a structured diagnostic workflow that links numeric biomarkers to clinical text. The "Knowledge-Guided Instruction Generation" stage forces GROK to learn the process of inference—connecting numbers to qualitative descriptors—rather than just pattern matching.

### Mechanism 2: OCT-to-Biomarker Contrastive Alignment
Pre-training the visual encoder to align 2D OCT slices with 3D biomarker vectors improves the precision of quantitative estimation compared to generic visual pre-training. The contrastive loss (InfoNCE) explicitly aligns the embedding of an OCT B-scan with a vector of its corresponding numerical clinical biomarkers, forcing the encoder to preserve metric information.

### Mechanism 3: Dual-Modality Complementary Fusion
Jointly processing CFP and OCT is required to maximize "Evidence Grounding" and "Coverage Completeness." The architecture uses independent encoders for each modality, whose embeddings are projected into a shared token space for the LLM. CFP provides surface/vascular context while OCT provides structural/depth context, and the LLM synthesizes these to form a coherent "sub-inference" chain.

## Foundational Learning

- **Concept: Contrastive Learning (Image-Vector)**
  - Why needed here: Standard VLP aligns images to text. This paper aligns images to *numbers* (biomarkers). You must understand how to modify the contrastive loss to treat a dense vector of clinical metrics as the "positive" pair target instead of a text token sequence.
  - Quick check question: How does the loss function in Eq. (2) differ if $z_t$ (biomarker embedding) is a continuous vector rather than a discrete text embedding?

- **Concept: Instruction Tuning & Knowledge Distillation**
  - Why needed here: GROK's performance relies on "Stage I" data generation. You need to grasp that the model isn't just learning from raw images/diagnoses, but from a *generated reasoning process* provided by a teacher model.
  - Quick check question: Why is training on the teacher's "Chain-of-Thought" (intermediate reasoning steps) potentially more effective for clinical interpretability than training only on the final diagnosis label?

- **Concept: Parameter-Efficient Fine-Tuning (LoRA)**
  - Why needed here: The paper uses LoRA to adapt a 7B parameter model on a single GPU. Understanding LoRA is critical to replicating this architecture without requiring massive compute resources.
  - Quick check question: In the GROK architecture, which specific components are frozen and which are updated during the Supervised Instruction Fine-Tuning stage?

## Architecture Onboarding

- **Component map:** CFP Image + OCT B-scan + Clinical Biomarkers → RetiZero (CFP) + RETFound+Alignment (OCT) → MLP Projectors → Qwen2-7B-Instruct → Diagnosis Report

- **Critical path:** (1) Extract 37 biomarkers → Feed to OpenAI o3 with "Eye-Guideline" → Generate grounded instruction pairs. (2) Train OCT Encoder via contrastive loss against 3D biomarker vectors. (3) Freeze encoders → Train Projectors + LoRA adapters on LLM using generated instructions.

- **Design tradeoffs:** Synthetic vs. Gold Labels: The model relies on OpenAI o3 to generate its training "truth." This trades the authenticity of human-written reports for the structured reasoning and scalability of synthetic data. 2D vs. 3D OCT: The architecture uses a 2D central B-scan for efficiency, trading off the volumetric context available in full 3D OCT scans.

- **Failure signatures:** Metric Drop: "Quantitative Accuracy" falling below 60 likely indicates the OCT-Biomarker alignment failed or the projector is misaligned. Hallucination: Generating "redundant sentences" without linking to evidence suggests the instruction tuning data lacked sufficient grounding constraints. Modality Ignorance: If "Evidence Grounding" is low despite multimodal input, check if the gradient flows through both projectors.

- **First 3 experiments:** (1) Alignment Verification: Validate the OCT encoder by running a linear regression on the extracted embeddings vs. ground truth biomarkers. Target the MAE/RMSE metrics in Table III. (2) Ablation on Modality: Run inference on the test set using *only* the OCT branch to quantify the specific information gain provided by the CFP branch. (3) Rubric Evaluation: Generate reports for a small validation set and manually score "Reasoning Consistency" to verify if the model links sub-inferences to the final diagnosis correctly.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does integrating full volumetric OCT data, rather than single central-foveal 2D B-scans, significantly improve biomarker alignment accuracy and diagnostic performance?
- Basis in paper: The authors state in the Limitations section that "our CLIP-style pretraining aligns only central-foveal 2D B-scans with 3D biomarkers, omitting volumetric context and thus limiting spatial feature capture."
- Why unresolved: The current computational framework reduces 3D scans to 2D slices to facilitate CLIP-style alignment, leaving the potential performance gain from full volumetric processing untested.
- What evidence would resolve it: A comparative ablation study training the OCT encoder on full 3D volumes versus 2D slices and measuring the change in quantitative biomarker estimation error (MAE/RMSE).

### Open Question 2
- Question: Can full-parameter fine-tuning of larger backbone models (e.g., >7B parameters) overcome the current "far below clinically deployable" macro-level diagnostic classification performance?
- Basis in paper: The conclusion notes that "limited computational resources prevented use of larger backbones or full-model fine-tuning," and lists exploring "full-model fine-tuning on larger backbones" as future work.
- Why unresolved: The current study relied exclusively on LoRA fine-tuning on a 7B-parameter Qwen2 backbone due to hardware constraints, capping the model's reasoning capacity and classification accuracy (Macro F1 ≤ 0.25).
- What evidence would resolve it: Benchmarking GROK's performance when scaled to larger models (e.g., 70B parameters) with full fine-tuning to observe if Macro F1-scores reach clinically acceptable thresholds.

### Open Question 3
- Question: Is the "Quantitative-to-Qualitative" reasoning chain robust across external, multi-ethnic datasets with balanced disease distributions?
- Basis in paper: The authors highlight the reliance on UKBiobank data which "result[ed] in pronounced class imbalance and a long-tail disease distribution" and note the "absence of publicly available paired CFP–OCT datasets."
- Why unresolved: The model was trained and evaluated on a single dataset with significant class imbalance, making its generalizability to other populations or data distributions uncertain.
- What evidence would resolve it: Evaluation of the model on an external, paired CFP-OCT dataset with a balanced distribution of the six target disease categories to assess generalization without distribution bias.

## Limitations
- Heavy reliance on a single proprietary teacher model (OpenAI o3) for generating the entire instruction dataset creates a single point of failure.
- Evaluation framework uses the same model type (OpenAI o3) that generated training data to judge student model outputs, creating circular validation.
- Architecture decisions rest on unproven assumptions about 2D OCT B-scans containing sufficient information to approximate 3D biomarkers and LLM's ability to fuse distinct visual modalities without explicit cross-attention.

## Confidence
**High Confidence Claims:**
- The three-stage pipeline architecture (instruction generation → biomarker alignment → fine-tuning) is technically sound and reproducible
- The ablation results showing the importance of multimodal input (CFP + OCT) are reliable given the controlled experimental design
- The contrastive learning approach for OCT-biosignal alignment follows established principles and produces measurable improvements in biomarker regression tasks

**Medium Confidence Claims:**
- The quantitative-to-qualitative diagnostic chain-of-thought reasoning genuinely mirrors clinical decision-making processes
- The synthetic instruction data quality is sufficient to teach GROK clinically meaningful reasoning patterns
- The LLM judge provides reliable assessment of clinical quality metrics

**Low Confidence Claims:**
- GROK's outputs would be considered "clinician-grade" by human medical professionals
- The model generalizes to OCT and CFP data from different clinical centers or imaging protocols
- The reasoning chains are diagnostically sound rather than superficially plausible

## Next Checks
1. **Human Expert Validation:** Recruit 3-5 ophthalmology specialists to manually evaluate GROK's diagnostic reports on a held-out test set, comparing against both ground truth labels and the LLM judge's assessments.

2. **Cross-Center Generalization:** Fine-tune GROK on a small subset of OCT/CFP data from a different clinical center and evaluate performance degradation to test robustness to domain shifts in medical imaging.

3. **Failure Mode Analysis:** Systematically generate inputs designed to trigger known failure modes (e.g., contradictory biomarkers, rare disease presentations, poor image quality) and analyze whether GROK's reasoning chains correctly identify uncertainty or provide appropriate hedging rather than overconfident incorrect diagnoses.