---
ver: rpa2
title: Towards Understanding Visual Grounding in Visual Language Models
arxiv_id: '2509.10345'
source_url: https://arxiv.org/abs/2509.10345
tags:
- visual
- arxiv
- grounding
- language
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey comprehensively reviews visual grounding in vision-language
  models (VLMs), examining how these models establish precise correspondences between
  textual descriptions and specific regions within visual scenes. The paper covers
  the evolution from early CNN-RNN approaches to modern transformer-based VLMs, analyzing
  architectural components including vision encoders, multimodal connectors, and training
  strategies.
---

# Towards Understanding Visual Grounding in Visual Language Models

## Quick Facts
- arXiv ID: 2509.10345
- Source URL: https://arxiv.org/abs/2509.10345
- Authors: Georgios Pantazopoulos; Eda B. Özyiğit
- Reference count: 40
- Key outcome: This survey comprehensively reviews visual grounding in vision-language models (VLMs), examining how these models establish precise correspondences between textual descriptions and specific regions within visual scenes.

## Executive Summary
This survey provides a comprehensive review of visual grounding in vision-language models, tracing the evolution from early CNN-RNN approaches to modern transformer-based architectures. The authors examine how VLMs establish precise correspondences between textual descriptions and specific regions within visual scenes, covering architectural components, training strategies, and diverse applications. The paper identifies key challenges including the balance between mapping and compression strategies, the impact of image resolution, and the importance of maintaining grounding objectives throughout training. Current limitations include benchmark saturation and reliance on pseudo-labeled data, while suggesting promising research directions such as improved pre-training data, ecological validity in evaluations, and integration of grounding with multimodal reasoning capabilities.

## Method Summary
The survey synthesizes visual grounding research across multiple domains including Referring Expression Comprehension, Grounded Visual Question Answering, Grounded Captioning, and GUI Agents. It examines architectural evolution from CNN-RNN to transformer-based VLMs, analyzing vision encoders (primarily SigLIP/ViT), multimodal connectors (MLP, CNN, Q-Former), and training strategies. The paper evaluates various representation approaches including pixel-level versus object-centric visual features and raw versus discretized coordinate outputs. Key evaluation metrics include Intersection over Union (IoU) for REC tasks and precision metrics for grounding evidence in GVQA applications.

## Key Results
- Feature-preserving connector approaches (MLP/CNN) outperform compression-based resamplers (Q-Former) for spatial reasoning tasks
- Raw numerical coordinate outputs leverage LLM numerical priors more effectively than discretized tokens
- High-resolution processing via dynamic sub-image encoding significantly improves fine-grained grounding capabilities
- Grounding objectives must be maintained throughout multi-stage training to prevent catastrophic forgetting
- Benchmark saturation on established datasets limits meaningful progress measurement in visual grounding research

## Why This Works (Mechanism)

### Mechanism 1: Feature-Preserving Mapping for Spatial Fidelity
Mapping visual embeddings directly to LLM input space via MLPs preserves fine-grained spatial relationships better than compression via attention-based resamplers. The connector acts as a translation layer that avoids aggressive downsampling, maintaining spatial coordinates inherent in patch indices for arithmetic and compositional reasoning over visual regions. This requires sufficient LLM context length capacity to handle longer visual token sequences.

### Mechanism 2: Numerical Prior Activation via Raw Coordinates
Representing bounding boxes as raw numeric token sequences leverages pre-existing numerical understanding in the LLM backbone, whereas discretizing coordinates requires learning new tokens from scratch. The LLM's text-only pre-training provides robust numerical ordering and representation circuits that can be recycled for coordinate interpretation, though modality gap ambiguity may cause confusion between semantic numbers and visual coordinates.

### Mechanism 3: High-Resolution Scaling via Dynamic Sub-Image Encoding
Processing high-resolution images by dividing them into sub-images while retaining downscaled global views enables fine-grained grounding capabilities that low-resolution encoders cannot achieve. This strategy provides high-fidelity local information alongside global context essential for small object detection and GUI element identification, though computational costs increase substantially and attention mechanisms must effectively correlate objects across sub-image boundaries.

## Foundational Learning

- **Concept: Intersection over Union (IoU)**
  - Why needed: Primary evaluation metric for Referring Expression Comprehension that quantifies overlap between predicted and ground truth bounding boxes
  - Quick check: If a model predicts a box that perfectly contains the object but is twice the size of ground truth, will IoU be high or low?

- **Concept: Vision Encoder Alignment (CLIP vs. SigLIP)**
  - Why needed: Choice of vision encoder pre-training objective impacts grounding quality, with SigLIP's sigmoid loss handling batch efficiency differently than CLIP's contrastive loss
  - Quick check: Does the survey suggest language model or vision encoder quality has more significant impact on final VLM performance?

- **Concept: Catastrophic Forgetting in Multi-Stage Training**
  - Why needed: VLMs trained in stages may forget grounding capabilities when learning new tasks unless grounding data is mixed throughout training
  - Quick check: Why does the survey recommend including earlier development stage data during fine-tuning?

## Architecture Onboarding

- **Component map:** Image + Text Query -> Vision Encoder (SigLIP/ViT) -> Visual Patches -> Connector (MLP/CNN/Q-Former) -> Visual Tokens + Text Tokens -> LLM Backbone (Vicuna/Qwen) -> Text Output + Coordinates

- **Critical path:** Connector choice is most nuanced architectural decision. Simple 2-layer MLP preserves spatial features but increases token count, while Q-Former compresses tokens but risks losing spatial locality. For grounding tasks, survey suggests favoring feature-preserving or CNN-based connectors over attention-based resamplers.

- **Design tradeoffs:**
  - Resolution vs. Compute: High-res inputs improve accuracy but drastically increase visual token count
  - Raw vs. Discrete Coordinates: Raw coordinates leverage LLM numerical priors but may suffer modality gap; discrete tokens are learned from scratch but offer cleaner vocabularies
  - Object-Centric vs. Pixel-Level: Pixel-level is modern standard for general VLMs, while Object-Centric is cleaner but constrained by predefined categories

- **Failure signatures:**
  - Hallucination: Model describes object not present in image (language priors override visual features)
  - Spatial Drift: Model identifies correct object category but places box on wrong instance (weak spatial attention)
  - Modality Gap: Model confuses numerical tokens in text with visual coordinates

- **First 3 experiments:**
  1. Connector Ablation: Train identical VLMs using MLP vs. Q-Former, evaluate on RefCOCO/Ref-L4 to measure spatial fidelity drop
  2. Coordinate Format Test: Fine-tune baseline VLM using raw numeric vs. discretized bin tokens, compare convergence speed and final IoU
  3. Resolution Scaling: Evaluate grounding task at standard resolution vs. dynamic resolution via sub-image splitting, measure performance on small UI elements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does inclusion of grounding objectives during training causally reduce object hallucinations in VLMs?
- Basis: Section 2.2 notes conflicting evidence about whether grounding objectives exhibit causal relationship with hallucination reductions
- Why unresolved: Conflict exists between grounding providing interpretable links versus actively reducing language prior reliance
- What evidence would resolve: Controlled intervention studies using causal mediation analysis on VLMs with identical backbones but differing only in grounding loss functions

### Open Question 2
- Question: What are specific performance trade-offs between discretised coordinate representations and raw numeric coordinates?
- Basis: Section 2.3 notes raw-coordinate formats may leverage numerical knowledge but lacks in-depth comparison
- Why unresolved: Current research hasn't isolated impact of modality gap on numerical reasoning and spatial accuracy
- What evidence would resolve: Rigorous ablation study comparing bounding box regression accuracy using both formats across varying model scales

### Open Question 3
- Question: How does balance between feature-preserving mapping and compression affect grounding at high resolutions?
- Basis: Sections 3.2 and 5.2 highlight tension between mapping and compression, noting feature-preserving advantage diminishes with increased resolution
- Why unresolved: Unclear whether compression efficiency gains justify loss of fine-grained spatial details for precise grounding
- What evidence would resolve: Comparative analysis of connector modules fixed on same backbones evaluated on grounding benchmarks across resolution spectrum

### Open Question 4
- Question: Can non-Transformer language backbones match Transformer performance on grounding when scaled and instruction-tuned?
- Basis: Section 5.1 explicitly questions suitability of transformer alternatives, noting current evidence based on relatively small, non-instruction-tuned models
- Why unresolved: Efficiency of state-space models attractive but capacity for cross-modal alignment remains unproven at required scale
- What evidence would resolve: Evaluation of large-scale, instruction-tuned VLMs with Mamba backbones on standard grounding benchmarks against comparable Transformer baselines

## Limitations

- Reliance on pseudo-labeled data for training may introduce noise and systematic errors
- Benchmark saturation on established datasets like RefCOCO limits meaningful progress measurement
- Disconnect exists between evaluation metrics and practical utility for real-world applications

## Confidence

**High Confidence:**
- Superiority of feature-preserving connector architectures over compression-based approaches for spatial reasoning
- Importance of maintaining grounding objectives throughout multi-stage training to prevent capability degradation
- Fundamental challenge of benchmark saturation limiting progress measurement

**Medium Confidence:**
- Specific advantage of raw coordinate outputs over discretized tokens based on numerical prior activation
- Recommendation for high-resolution scaling via dynamic sub-image encoding as general solution
- Relative importance of vision encoder quality versus LLM backbone quality

**Low Confidence:**
- Generalizability of grounding architecture recommendations across all VLM applications
- Optimal balance between computational cost and grounding fidelity in high-resolution processing
- Long-term stability of grounding capabilities under extended fine-tuning on non-grounding tasks

## Next Checks

1. **Connector Architecture Ablation Study:** Systematically compare MLP, CNN, and Q-Former connector architectures across multiple grounding benchmarks while controlling for vision encoder quality, training data, and hyperparameters to quantify spatial fidelity impact.

2. **Coordinate Output Format Experiment:** Train identical VLMs using raw numerical coordinates versus discretized bin tokens across different languages to test numerical prior activation hypothesis and assess cross-linguistic generalization.

3. **Resolution Scaling Impact Analysis:** Evaluate grounding performance on fine-grained tasks at multiple resolution levels while measuring computational overhead and spatial reasoning accuracy to validate claimed trade-offs between resolution and grounding capability.