---
ver: rpa2
title: Early Detection and Reduction of Memorisation for Domain Adaptation and Instruction
  Tuning
arxiv_id: '2510.11372'
source_url: https://arxiv.org/abs/2510.11372
tags:
- memorisation
- epoch
- fine-tuning
- instruction
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates memorisation during domain adaptation and
  instruction tuning of large language models (LLMs), focusing on both Pythia, Llama3,
  and Mistral model families across 1.4B-70B parameters. The key finding is that a
  simple n-gram partial memorisation score reliably predicts verbatim memorisation,
  with high-scoring n-grams often memorised in early epochs before validation perplexity
  or evaluation performance is optimised.
---

# Early Detection and Reduction of Memorisation for Domain Adaptation and Instruction Tuning

## Quick Facts
- arXiv ID: 2510.11372
- Source URL: https://arxiv.org/abs/2510.11372
- Reference count: 17
- Primary result: N-gram partial memorisation scores reliably predict verbatim memorisation, enabling early stopping and regularisation techniques that reduce memorisation by up to 40% with minimal performance loss.

## Executive Summary
This paper investigates memorisation during domain adaptation and instruction tuning of large language models (LLMs), focusing on Pythia, Llama3, and Mistral model families across 1.4B-70B parameters. The key finding is that a simple n-gram partial memorisation score reliably predicts verbatim memorisation, with high-scoring n-grams often memorised in early epochs before validation perplexity or evaluation performance is optimised. Using this n-gram score as an early stopping criterion significantly reduces memorisation with minimal performance loss. Additionally, a novel n-gram-aware loss regularisation technique reduces memorisation by up to 40% across all model families tested, outperforming existing mitigation strategies while minimising evaluation performance trade-offs.

## Method Summary
The method introduces two practical techniques for detecting and mitigating memorisation during LLM fine-tuning. First, a partial n-gram memorisation score computes the proportion of matching n-grams between model output and target, enabling early detection of samples likely to become verbatim memorised. Second, an n-gram-aware regularisation loss adds a penalty term to the standard cross-entropy loss, discouraging the model from overconfidently learning training-specific n-grams beyond pre-trained baseline probabilities. The approach tracks k-extractable memorisation using greedy decoding with prefix lengths {12, 16, 20} and 20-token suffixes, requiring a frozen pre-trained model for baseline probability computation.

## Key Results
- N-gram partial memorisation scores reliably predict which samples will become verbatim memorised, with median scores for memorised samples markedly higher than non-memorised baseline across all dataset categories.
- Early stopping using n-gram scores (threshold ~20) reduces memorisation by halting training before the steepest memorisation phase, achieving lower memorisation with smaller performance gaps compared to validation perplexity-based stopping.
- N-gram regularisation reduces memorisation by up to 40% across all tested model families while minimising evaluation performance trade-offs, outperforming Goldfish loss baseline.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: N-gram partial memorisation scores rise measurably before samples become verbatim-memorised, providing an early warning signal.
- Mechanism: As fine-tuning progresses, models first assign higher probability to local n-gram patterns within a sample before they can reproduce longer sequences verbatim. This creates a detectable gap between samples that will be memorised (higher partial scores) and those that will not (baseline-level scores).
- Core assumption: Verbatim memorisation is a progressive process where local pattern confidence increases before global sequence reproduction capability.
- Evidence anchors:
  - [abstract] "We use a simple but effective n-gram memorisation score which reliably precedes verbatim memorisation."
  - [section 4.1, Figure 2] Shows median partial memorisation scores for samples about to transition to verbatim memorisation are markedly higher than the non-memorised baseline across all dataset categories.

### Mechanism 2
- Claim: Threshold-based early stopping using n-gram memorisation scores reduces verbatim memorisation by halting training before the steepest memorisation phase.
- Mechanism: Since memorisation increases rapidly in early epochs—often before optimal validation perplexity or task performance—stopping when the average partial memorisation score exceeds a threshold (empirically found to be ~20) captures the "sweet spot" where the model has learned task-relevant patterns but not yet encoded training data verbatim.
- Core assumption: Task performance and memorisation are not perfectly coupled; there exists a training window where generalisation is achieved without full memorisation.
- Evidence anchors:
  - [abstract] "Memorisation increases dramatically in the first few epochs, often significantly before either validation perplexity or evaluation performance is optimised."
  - [section 4.2, Figure 5] Using n-gram selection criterion provides lower memorisation with smaller performance gaps compared to validation perplexity or accuracy-based stopping.

### Mechanism 3
- Claim: Penalising n-gram confidence exceeding pre-trained model baselines in the loss function discourages memorisation while preserving generalisation.
- Mechanism: The regularisation term $L_{reg} = \lambda \sum_{g \in G} [\max\{0, p_\theta(g) - p_{\theta_0}(g) - \tau\}]^2$ adds a quadratic penalty only when the fine-tuned model's confidence in an n-gram exceeds the pre-trained model's by margin $\tau$. This discourages the model from "over-confidently" learning training-specific n-grams while allowing it to leverage pre-trained knowledge.
- Core assumption: Memorisation manifests as inflated confidence on training-specific n-grams beyond what the pre-trained model reasonably assigns; generalisation does not require such inflation.
- Evidence anchors:
  - [abstract] "We introduce an n-gram-aware loss regulariser and show that it reduces memorisation across all model families tested by up to 40% while minimising evaluation performance trade-offs."
  - [section 4.3, Table 2] N-gram regularisation achieves best memorisation-performance trade-off across models (avg 5.45% memorisation, 4.95% eval gap vs baseline).

## Foundational Learning

- Concept: **k-extractable memorisation** (Carlini et al., 2023)
  - Why needed here: This is the ground-truth metric for verbatim memorisation used throughout the paper. A suffix is k-extractable if the model generates it exactly when prompted with a k-token prefix using greedy decoding.
  - Quick check question: Given a training sample with prefix "The patient was diagnosed with" and suffix "acute lymphoblastic leukemia," how would you test if this suffix is 16-extractable?

- Concept: **N-gram overlap metrics**
  - Why needed here: The partial memorisation score computes the proportion of matching n-grams between model output and target, enabling fine-grained tracking before verbatim reproduction occurs.
  - Quick check question: If a model outputs "The quick brown fox" and the target is "The quick brown dog," what is the 3-gram overlap score?

- Concept: **Regularisation as penalty terms in loss**
  - Why needed here: The n-gram regulariser adds a penalty to the standard cross-entropy loss. Understanding how auxiliary loss terms work is essential for implementing and tuning this approach.
  - Quick check question: In L2 regularisation, what happens to the penalty when weight values approach zero? How does this differ from the n-gram penalty when fine-tuned confidence approaches pre-trained confidence?

## Architecture Onboarding

- Component map: Memorisation Evaluator -> N-gram Scorer -> Early Stopping Controller -> Regularised Loss Function -> Pre-trained Reference Model

- Critical path:
  1. Load pre-trained model; create frozen reference copy.
  2. Fine-tune on target dataset (≤5000 samples recommended for fast iteration).
  3. At each epoch: (a) compute memorisation metrics on training data, (b) compute n-gram scores, (c) evaluate on held-out validation set.
  4. If using early stopping: check if average partial memorisation exceeds threshold.
  5. If using regularisation: compute n-gram penalty alongside standard loss at each batch.

- Design tradeoffs:
  - **Early stopping vs. regularisation**: Early stopping is simpler (no code changes to training loop) but less effective (~8% vs ~5.5% memorisation). Regularisation requires pre-trained model inference (memory/compute overhead) but achieves better trade-offs.
  - **N-gram sizes**: Smaller n-grams (3-4) are noisier; larger n-grams (7+) are computationally expensive. Paper finds {4,5,6} effective.
  - **Prefix lengths for k-extractable**: Shorter prefixes (<12) collide across corpora; longer prefixes (>20) reduce usable samples.
  - **Threshold selection**: Lower thresholds reduce memorisation more aggressively but risk under-training; threshold of 20 empirically balanced.

- Failure signatures:
  - **Early stopping too aggressive**: Validation perplexity significantly worse than baseline; model underfits. Solution: raise threshold or use regularisation instead.
  - **Regularisation too strong (high λ)**: Model fails to learn task; evaluation accuracy drops sharply. Solution: reduce λ or increase margin τ.
  - **No memorisation reduction observed**: Check that n-gram sizes and prefix lengths are appropriate for dataset; verify pre-trained reference model is correctly frozen.
  - **Memory overflow with regularisation**: Running pre-trained model alongside fine-tuning doubles memory. Solution: use gradient checkpointing or compute baseline probabilities offline.

- First 3 experiments:
  1. **Baseline memorisation dynamics**: Fine-tune a small model (Pythia 1.4B or 2.8B) on a single dataset (e.g., XSum) for 8 epochs; log memorisation, n-gram scores, validation perplexity, and task accuracy at each epoch. Reproduce Figure 1/2 patterns to validate setup.
  2. **Early stopping threshold sweep**: Using same setup, test early stopping with thresholds {15, 20, 25, 30}. Plot memorisation vs. evaluation performance trade-off curve to identify optimal threshold for your data.
  3. **Regularisation ablation**: Implement n-gram regularisation with λ ∈ {0.1, 0.5, 1.0} and τ ∈ {0.0, 0.1, 0.2}. Compare memorisation reduction and performance gap against Goldfish loss baseline and early stopping.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does the n-gram regularisation technique effectively reduce memorisation in code generation, mathematical reasoning, and multimodal tasks? The current study only evaluates text-based NLP tasks (classification, QA, summarisation, instruction following). Code and mathematical reasoning involve different structural patterns, and multimodal tasks introduce additional complexity.

- **Open Question 2**: How do alternative decoding strategies such as beam search affect memorisation leakage patterns compared to the greedy decoding tested in this study? Greedy decoding represents only one generation strategy. Beam search, nucleus sampling, and temperature-based sampling may surface different memorisation behaviours or alter the effectiveness of n-gram based early stopping.

- **Open Question 3**: Does the early memorisation phenomenon generalise to larger fine-tuning datasets (beyond 5,000 samples)? The methodology uses "a maximum of 5,000 samples" for fine-tuning "to rapidly experiment over model scales." Real-world domain adaptation often involves much larger datasets, but whether early memorisation dynamics scale similarly remains untested.

## Limitations
- The tested model families (Pythia, Llama3, Mistral) may not be representative of general LLM behaviour during fine-tuning across diverse architectures.
- The n-gram regularisation approach requires inference from a frozen pre-trained model during fine-tuning, potentially limiting practical deployment due to memory and computational overhead.
- The evaluation focuses primarily on classification and summarization tasks, leaving open questions about effectiveness for reasoning, code generation, or structured output prediction.

## Confidence

**High confidence**: The predictive relationship between n-gram partial memorisation scores and subsequent verbatim memorisation is well-supported. The paper demonstrates consistent patterns across multiple datasets and model families showing that samples with higher n-gram scores in early epochs are significantly more likely to become verbatim memorised later.

**Medium confidence**: The effectiveness of the n-gram regularisation technique in reducing memorisation by up to 40% while maintaining evaluation performance is supported by the presented results, but the absolute magnitude may be dataset and model-dependent. The specific hyperparameters (λ, τ) were tuned for the tested configurations.

**Medium confidence**: The practical utility of n-gram-based early stopping as a simple intervention is empirically validated, but its effectiveness compared to more sophisticated stopping criteria or adaptive learning rate schedules is not explored.

## Next Checks

1. **Architecture Generalisation Test**: Apply the n-gram regularisation technique to a model family not tested in the original study (e.g., a decoder-only transformer with global attention or a mixture-of-experts architecture) to verify the approach's effectiveness across diverse architectural designs.

2. **Dataset Size Scaling**: Systematically vary training set sizes from 100 to 10,000 samples (with multiple seeds) to determine how memorisation dynamics and the effectiveness of both early stopping and regularisation scale with dataset size, particularly examining whether the 20-token threshold remains appropriate.

3. **Alternative Task Types**: Test the methodology on non-classification/non-summarization tasks such as code generation (e.g., HumanEval), mathematical reasoning, or multi-turn dialogue to verify whether the n-gram-based memorisation detection and mitigation strategies transfer to tasks with different output characteristics and evaluation metrics.