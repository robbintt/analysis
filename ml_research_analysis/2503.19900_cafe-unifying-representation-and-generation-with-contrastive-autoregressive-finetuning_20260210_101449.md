---
ver: rpa2
title: 'CAFe: Unifying Representation and Generation with Contrastive-Autoregressive
  Finetuning'
arxiv_id: '2503.19900'
source_url: https://arxiv.org/abs/2503.19900
tags:
- multimodal
- arxiv
- language
- zhang
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CAFe is a fine-tuning framework that enables large vision-language
  models (LVLMs) to perform both multimodal retrieval and text generation without
  compromising either capability. It introduces a contrastive-autoregressive objective
  that combines InfoNCE contrastive loss for representation learning with autoregressive
  language modeling loss, trained jointly on paired image-text data.
---

# CAFe: Unifying Representation and Generation with Contrastive-Autoregressive Finetuning

## Quick Facts
- arXiv ID: 2503.19900
- Source URL: https://arxiv.org/abs/2503.19900
- Reference count: 14
- CAFe is a fine-tuning framework enabling large vision-language models to perform both multimodal retrieval and text generation without compromising either capability

## Executive Summary
CAFe introduces a novel fine-tuning framework that unifies multimodal retrieval and text generation capabilities in large vision-language models (LVLMs). The framework addresses a fundamental limitation where previous LVLMs struggle to perform both tasks effectively, as they were typically optimized for either representation learning or text generation. By combining contrastive and autoregressive objectives during training, CAFe enables models to generate aligned image and text embeddings while maintaining strong text generation capabilities.

The framework demonstrates significant improvements across multiple benchmarks, achieving state-of-the-art performance in zero-shot image-text retrieval while reducing object hallucination. CAFe-7B outperforms previous LVLMs and CLIP on Flickr30K with 87.5% R@1 for image→text retrieval, and shows a 10.2 percentage point improvement on the MMEB multimodal retrieval benchmark. The approach is particularly notable for maintaining competitive multimodal understanding performance while adding robust retrieval capabilities.

## Method Summary
CAFe employs a dual-objective training approach that combines InfoNCE contrastive loss for representation learning with autoregressive language modeling loss. The framework uses a specialized embedding instruction that enables the model to generate aligned image and text embeddings while preserving its text generation capabilities. During training, paired image-text data is processed through both objectives simultaneously, allowing the model to learn representations that are both discriminative for retrieval tasks and coherent for generation tasks.

The key innovation lies in the joint optimization of these two seemingly conflicting objectives. The contrastive component ensures that image and text embeddings of the same pair are brought closer in representation space, effectively removing the modality gap that existed in previous models. Meanwhile, the autoregressive component maintains the model's ability to generate free-form text. This dual training approach is implemented through a specialized embedding instruction that can toggle between generative and contrastive modes based on the task requirements.

## Key Results
- CAFe-7B achieves 87.5% R@1 for image→text retrieval and 75.3% R@1 for text→image retrieval on Flickr30K
- Demonstrates 10.2 percentage point improvement over state-of-the-art on MMEB multimodal retrieval benchmark
- Reduces object hallucination by 1.5 percentage points compared to baselines, achieving 89.1% accuracy on POPE and 88.9% F1 on THRONE

## Why This Works (Mechanism)
CAFe works by addressing the fundamental trade-off between representation learning and text generation in LVLMs. The mechanism operates through simultaneous optimization of two complementary objectives: the InfoNCE contrastive loss ensures that semantically similar image-text pairs are mapped to nearby points in embedding space, while the autoregressive loss maintains the model's ability to generate coherent text sequences. This dual optimization creates a unified representation space where different modalities are no longer separated by a modality gap.

The specialized embedding instruction serves as a critical component that allows the model to switch between generative and contrastive modes seamlessly. When generating text, the model operates in its standard autoregressive mode, producing coherent responses. When performing retrieval tasks, the same instruction can be used to generate embeddings that capture semantic similarity across modalities. This architectural flexibility enables CAFe to handle both tasks without requiring separate model instances or compromising performance on either task.

## Foundational Learning

**Contrastive Learning**: Learning representations by bringing similar samples closer and pushing dissimilar ones apart in embedding space. Needed to enable effective cross-modal retrieval; check by verifying embedding similarity scores for matched pairs.

**Autoregressive Language Modeling**: Predicting the next token in a sequence given previous tokens. Essential for maintaining text generation capabilities; verify through perplexity measurements on text-only tasks.

**Multimodal Embeddings**: Representing different data modalities (images, text) in a shared embedding space. Required to bridge the modality gap; test by measuring cross-modal retrieval accuracy.

**InfoNCE Loss**: A contrastive loss function that maximizes the mutual information between positive pairs while minimizing it for negative pairs. Critical for effective contrastive learning; validate through retrieval performance metrics.

**Embedding Instructions**: Specialized prompts that guide the model to produce specific types of outputs (embeddings vs. text). Enables task-specific behavior; test by examining output format consistency.

## Architecture Onboarding

Component Map: Input -> Image Encoder + Text Encoder -> Joint Embedding Space -> Contrastive Loss + Autoregressive Loss -> CAFe Model

Critical Path: Image/Text Input → Specialized Embedding Instruction → Joint Embedding Generation → InfoNCE Loss Computation → Parameter Updates

Design Tradeoffs: The framework balances between discriminative power for retrieval (favoring contrastive learning) and generative quality (favoring autoregressive learning). The dual-objective training introduces computational overhead but eliminates the need for separate models for each task.

Failure Signatures: Poor retrieval performance indicates insufficient contrastive learning; degraded text generation suggests the autoregressive component is being overshadowed; modality gap persistence indicates embedding instruction issues.

First Experiments:
1. Test zero-shot retrieval on Flickr30K to verify basic functionality
2. Evaluate text generation quality on standard benchmarks to ensure capability preservation
3. Measure embedding similarity for matched vs. mismatched image-text pairs to validate contrastive learning

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on specialized embeddings may limit flexibility compared to dedicated retrieval models
- Computational overhead from dual-objective training not extensively explored
- Limited evaluation of real-world deployment scenarios with latency and resource constraints

## Confidence

**High confidence**: Retrieval performance claims on Flickr30K and MMEB benchmarks showing clear improvements over previous LVLMs and CLIP. Hallucination robustness claims with substantial improvements on POPE and THRONE metrics.

**Medium confidence**: Claims about maintaining strong multimodal understanding performance comparable to LLaVA-OneVision, as comparative evidence is limited.

## Next Checks

1. Conduct ablation studies removing the contrastive objective to quantify its specific contribution to retrieval performance versus generation capability

2. Evaluate CAFe on real-world multimodal retrieval scenarios with longer text queries and diverse image types not covered in standard benchmarks

3. Measure and report inference latency and computational overhead when switching between generative and retrieval modes to assess practical deployment viability