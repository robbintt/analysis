---
ver: rpa2
title: 'FedAVOT: Exact Distribution Alignment in Federated Learning via Masked Optimal
  Transport'
arxiv_id: '2509.14444'
source_url: https://arxiv.org/abs/2509.14444
tags:
- learning
- transport
- distribution
- optimal
- users
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses distribution misalignment in federated learning
  (FL), where the availability distribution of participating users (q) differs from
  the importance distribution defining the optimization objective (p). This misalignment
  causes bias and instability in standard FedAvg.
---

# FedAVOT: Exact Distribution Alignment in Federated Learning via Masked Optimal Transport
## Quick Facts
- arXiv ID: 2509.14444
- Source URL: https://arxiv.org/abs/2509.14444
- Authors: Herlock; Rahimi; Dionysis Kalogerias
- Reference count: 0
- The paper addresses distribution misalignment in federated learning (FL), where the availability distribution of participating users (q) differs from the importance distribution defining the optimization objective (p).

## Executive Summary
This paper tackles a fundamental challenge in federated learning: distribution misalignment between user availability and optimization objectives. When participating users' availability distribution (q) differs from the importance distribution (p) defining the optimization objective, standard FedAvg methods suffer from bias and instability. The proposed FedAVOT method reformulates client aggregation as a masked optimal transport problem to align these distributions using Sinkhorn scaling, achieving theoretically guaranteed convergence rates that are independent of the number of participating users per round.

## Method Summary
FedAVOT addresses distribution misalignment in federated learning by reformulating the client aggregation step as a masked optimal transport (MOT) problem. The method uses Sinkhorn scaling to align the availability distribution (q) with the importance distribution (p), ensuring unbiased updates. Under a nonsmooth convex FL setting, FedAVOT achieves an O(1/√T) convergence rate that is independent of the number of participating users per round, with the remarkable property that only two clients per round are needed to achieve the same rate as full participation.

## Key Results
- FedAVOT achieves O(1/√T) convergence rate under nonsmooth convex FL settings, independent of the number of participating users per round
- The method requires only two clients per round to achieve the same rate as full participation
- Experiments on linear regression and MNIST classification show FedAVOT significantly outperforms FedAvg in both restricted availability and coordinated sampling settings

## Why This Works (Mechanism)
The key insight is that standard FedAvg implicitly assumes the availability distribution of participating users matches the importance distribution defining the optimization objective. When this assumption fails, the weighted aggregation of client updates becomes biased. FedAVOT explicitly aligns these distributions through masked optimal transport, ensuring that each client's contribution to the global model is weighted according to its true importance rather than just its availability.

## Foundational Learning
- **Optimal Transport Theory**: Provides mathematical framework for measuring and aligning probability distributions
  - Why needed: To formulate the distribution alignment problem as a transport problem
  - Quick check: Verify Sinkhorn-Knopp algorithm implementation for matrix scaling
- **Sinkhorn Scaling**: Iterative algorithm for matrix scaling that converges to doubly stochastic matrices
  - Why needed: To efficiently solve the optimal transport problem in each aggregation step
  - Quick check: Confirm convergence of scaling iterations and numerical stability
- **Federated Learning Convergence Analysis**: Understanding of how participation patterns affect convergence rates
  - Why needed: To establish theoretical guarantees for the proposed method
  - Quick check: Validate assumptions about bounded gradients and Lipschitz continuity

## Architecture Onboarding
- **Component Map**: Local Updates -> Masked OT Alignment -> Global Aggregation -> Model Update
- **Critical Path**: The alignment step between local updates and global aggregation is the core innovation
- **Design Tradeoffs**: Computational overhead of MOT vs. convergence speed and communication efficiency
- **Failure Signatures**: If distributions cannot be accurately estimated, the alignment may introduce additional noise
- **First Experiments**: 1) Verify MOT implementation on synthetic distributions 2) Test convergence on simple convex problems 3) Compare communication efficiency with FedAvg on MNIST

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis only covers nonsmooth convex objectives, not strongly convex or non-convex settings
- Assumes availability distributions q and importance distributions p are known or estimable
- Experimental evaluation uses relatively simple tasks that may not represent real-world complexity

## Confidence
- Theoretical convergence rate (O(1/√T) independent of participation): Medium
- Masked optimal transport formulation: High
- Empirical performance improvements: High
- Practical applicability with minimal clients: Medium

## Next Checks
1. Test FedAVOT on more complex, real-world federated learning tasks with heterogeneous data distributions
2. Verify the method's performance when availability distributions q are not known a priori but must be estimated
3. Extend the theoretical analysis to strongly convex and non-convex settings common in modern federated learning