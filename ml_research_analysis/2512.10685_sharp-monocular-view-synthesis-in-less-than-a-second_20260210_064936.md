---
ver: rpa2
title: Sharp Monocular View Synthesis in Less Than a Second
arxiv_id: '2512.10685'
source_url: https://arxiv.org/abs/2512.10685
tags:
- depth
- view
- image
- synthesis
- sharp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SHARP is a feed-forward approach to photorealistic view synthesis
  from a single image. It produces a 3D Gaussian representation in under a second
  via a single forward pass through a neural network, enabling real-time rendering
  of nearby views.
---

# Sharp Monocular View Synthesis in Less Than a Second

## Quick Facts
- arXiv ID: 2512.10685
- Source URL: https://arxiv.org/abs/2512.10685
- Reference count: 40
- Produces photorealistic view synthesis in under a second via 3D Gaussian splatting

## Executive Summary
SHARP introduces a feed-forward approach to photorealistic view synthesis from a single image, producing a 3D Gaussian representation in under a second via a single forward pass through a neural network. The method uses a modified Depth Pro backbone, depth adjustment, and Gaussian refinement modules, trained end-to-end with a carefully designed loss function. Experiments on multiple datasets show SHARP achieves state-of-the-art image fidelity, reducing LPIPS by 25-34% and DISTS by 21-43% compared to the best prior model, while being three orders of magnitude faster. The approach excels at synthesizing sharp, high-resolution images from nearby viewpoints, supporting interactive browsing of personal photo collections.

## Method Summary
SHARP is a feed-forward approach to photorealistic view synthesis from a single image. It produces a 3D Gaussian representation in under a second via a single forward pass through a neural network, enabling real-time rendering of nearby views. The method uses a modified Depth Pro backbone, depth adjustment, and Gaussian refinement modules, trained end-to-end with a carefully designed loss function. Experiments on multiple datasets show SHARP achieves state-of-the-art image fidelity, reducing LPIPS by 25-34% and DISTS by 21-43% compared to the best prior model, while being three orders of magnitude faster. The approach excels at synthesizing sharp, high-resolution images from nearby viewpoints, supporting interactive browsing of personal photo collections.

## Key Results
- Achieves state-of-the-art image fidelity with 25-34% reduction in LPIPS and 21-43% reduction in DISTS compared to best prior model
- Generates a 3D Gaussian representation in under a second via single forward pass
- Enables real-time rendering of nearby views for interactive browsing

## Why This Works (Mechanism)
The method works by training a neural network to predict a 3D Gaussian representation from a single image in a single forward pass, enabling real-time rendering. The architecture combines a depth estimation backbone with Gaussian refinement modules, and uses a multi-component loss function that balances perceptual quality, geometric accuracy, and regularization to prevent degenerate Gaussians.

## Foundational Learning
- **3D Gaussian Splatting**: Point-based representation where each Gaussian has position, color, opacity, and covariance. Why needed: Enables efficient real-time rendering while maintaining high visual quality. Quick check: Verify that rendered images maintain sharp details and avoid floaters.
- **Depth Estimation**: Monocular depth prediction from RGB input. Why needed: Provides geometric constraints for 3D reconstruction. Quick check: Compare predicted depth maps against ground truth on validation set.
- **Perceptual Loss**: Feature-based similarity metrics (LPIPS, DISTS) that correlate with human perception. Why needed: Better captures image quality than pixel-wise metrics. Quick check: Ensure perceptual metrics improve during training.
- **Differentiable Rendering**: Enables gradient flow from rendered images back to 3D representation. Why needed: Allows end-to-end training of the entire pipeline. Quick check: Verify gradients flow correctly through rendering during backpropagation.

## Architecture Onboarding

**Component Map**: Depth Pro ViT encoder -> DPT depth decoder -> Depth adjustment U-Net -> Gaussian decoder -> Differentiable renderer

**Critical Path**: The forward pass from encoder through all decoders to produce Gaussians, followed by differentiable splatting, is the critical path for inference speed. Any bottleneck here directly impacts the sub-second goal.

**Design Tradeoffs**: The architecture trades off between expressiveness (many Gaussians) and efficiency (single forward pass). The two-stage training balances synthetic data scale with real-world adaptation, while frozen/unfrozen components optimize for both stability and fine-tuning capability.

**Failure Signatures**: 
- Blurry outputs indicate insufficient perceptual loss weighting or missing Gram loss
- Floaters/large Gaussians suggest inadequate regularization (L_grad, L_splat)
- Boundary artifacts on reflective surfaces indicate frozen monodepth backbone limitations
- OOM errors during training suggest memory optimization issues with perceptual loss

**3 First Experiments**:
1. Verify Gaussian renderer produces correct images from synthetic 3D Gaussian data
2. Test depth estimation accuracy on validation set with frozen encoder
3. Validate single forward pass inference time meets sub-second requirement

## Open Questions the Paper Calls Out
None

## Limitations
- Relies on proprietary training datasets (synthetic multi-view images and real image collections) that are not publicly available
- Uses in-house differentiable renderer implementation not specified in detail
- Computation graph surgery for perceptual loss memory optimization described conceptually without implementation details

## Confidence

**High**: Runtime and image fidelity improvements (LPIPS/DISTS gains) given extensive evaluation across multiple benchmarks and comparison to prior work

**Medium**: Architectural details due to some unspecified components (exact backbone unfreezing schedule, perceptual loss optimization)

**Low**: Exact reproduction without access to proprietary training data and in-house renderer

## Next Checks

1. Validate architectural fidelity by reproducing ablation results for depth adjustment and Gaussian refinement modules using public synthetic dataset with known multi-view ground truth
2. Test memory efficiency claims by profiling peak GPU memory usage during training with and without reported computation graph optimizations
3. Evaluate runtime claims by benchmarking end-to-end inference time on representative dataset, measuring both network forward pass and rendering latency