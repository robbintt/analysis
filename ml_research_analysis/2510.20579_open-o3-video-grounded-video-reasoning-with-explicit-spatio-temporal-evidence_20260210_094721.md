---
ver: rpa2
title: 'Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence'
arxiv_id: '2510.20579'
source_url: https://arxiv.org/abs/2510.20579
tags:
- video
- temporal
- reasoning
- arxiv
- grounding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Open-o3 Video, a framework that enhances
  video reasoning by integrating explicit spatio-temporal evidence. While most models
  generate only textual rationales, Open-o3 Video grounds its reasoning in concrete
  visual observations by highlighting key timestamps, objects, and bounding boxes.
---

# Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence

## Quick Facts
- **arXiv ID:** 2510.20579
- **Source URL:** https://arxiv.org/abs/2510.20579
- **Reference count:** 40
- **Primary result:** State-of-the-art performance on V-STAR benchmark with 14.4% improvement in mAM and 24.2% in mLGM over Qwen2.5-VL

## Executive Summary
Open-o3 Video introduces a framework for video reasoning that grounds its outputs in explicit spatio-temporal evidence. Unlike traditional models that generate only textual rationales, Open-o3 Video produces structured outputs including bounding boxes, timestamps, and object labels, making reasoning verifiable and interpretable. The approach combines cold-start supervised fine-tuning with reinforcement learning using a composite reward function that balances accuracy, temporal alignment, and spatial precision. On the V-STAR benchmark, the model achieves state-of-the-art performance, demonstrating the effectiveness of explicit evidence traces for video understanding tasks.

## Method Summary
The framework employs a two-stage training strategy: cold-start supervised fine-tuning (SFT) on 30k spatio-temporal reasoning samples, followed by reinforcement learning with Group Sequence Policy Optimization (GSPO) on 36k samples. The model generates a specialized syntax with `<obj>`, `<box>`, and `<t>` tags during Chain-of-Thought reasoning, forcing explicit links between textual reasoning and visual evidence. The RL stage uses a composite reward incorporating accuracy, temporal proximity (with adaptive Gaussian kernels), and spatial IoU, with temporal gating preventing noisy spatial feedback. The approach addresses the dependency between spatial and temporal rewards by using curriculum-style relaxation of temporal constraints.

## Key Results
- Achieves state-of-the-art performance on V-STAR benchmark: 14.4% improvement in mAM and 24.2% in mLGM over Qwen2.5-VL
- Consistently outperforms baselines on other video understanding tasks including VideoMME, WorldSense, VideoMMMU, and TVGBench
- Test-time scaling with confidence-aware voting improves reliability by 1-2% over standard inference
- Ablation studies confirm importance of both cold-start initialization and temporal gating mechanisms

## Why This Works (Mechanism)

### Mechanism 1: Explicit Spatio-Temporal Evidence Traces
The model generates structured outputs with `<obj>...</obj><box>...</box>at<t>...</t>s` syntax during reasoning, forcing explicit linkage between textual tokens and visual evidence. This grounding approach improves verifiability compared to text-only rationales by anchoring semantic concepts to specific pixel data. The visual encoder must have sufficient localization capacity to generate accurate boxes and timestamps, with the framework enforcing spatio-temporal alignment through composite rewards.

### Mechanism 2: Adaptive Temporal Proximity and Gating
To solve the dependency where spatial rewards require accurate temporal predictions, the framework employs adaptive temporal proximity (Gaussian reward kernel with decaying width σ) and temporal gating (spatial rewards zeroed if temporal error exceeds τ=3s). This curriculum-based relaxation allows the policy to escape local minima where early temporal inaccuracy blocks spatial learning signals, providing dense yet reliable feedback during training.

### Mechanism 3: GSPO for Sequence-Level Stability
Group Sequence Policy Optimization calculates importance ratios and clipping at the sequence level rather than token level, offering more stable convergence for long-horizon grounded reasoning. This alignment with episode-level rewards reduces variance in long Chain-of-Thought sequences where token-level credit assignment suffers from high variance, particularly beneficial for coherent long sequences of interleaved text and coordinates.

## Foundational Learning

- **Concept: Visual Grounding & IoU (Intersection over Union)**
  - **Why needed here:** The RL feedback loop relies on calculating visual IoU between predicted and ground truth bounding boxes
  - **Quick check question:** If a predicted box perfectly contains the ground truth box but is twice as large, is the IoU high or low?

- **Concept: Policy Optimization (RL)**
  - **Why needed here:** The model optimizes a reward function (accuracy + temporal + spatial) rather than supervised labels
  - **Quick check question:** In GSPO, if a sequence receives a high reward, how does that affect the probability of generating that specific sequence again?

- **Concept: Cold-Start Initialization**
  - **Why needed here:** Direct RL failed; SFT was required to prime the model with the complex syntax of grounded reasoning
  - **Quick check question:** Why might an RL model struggle to discover the specific syntax `<box>[...]</box>` from scratch without supervised examples?

## Architecture Onboarding

- **Component map:** Qwen2.5-VL-7B (Base Model) -> STGR-CoT-30k (SFT) -> STGR-RL-36k (RL) -> GSPO (Trainer) -> Composite Reward Calculator -> vLLM Server (Inference) -> Confidence-Aware Voting (Test-time Scaling)

- **Critical path:**
  1. Data Prep: Verify STGR data has strictly aligned JSON (timestamp + boxes)
  2. SFT: Train on CoT-30k to install the `<obj>/<box>/<t>` syntax
  3. RL: Train on RL-36k using GSPO, ensuring adaptive σ schedule is active

- **Design tradeoffs:**
  - Strictness vs. Learning Speed: Tight temporal gating (τ=3s) ensures precision but may starve model of rewards early in training
  - Format Complexity: Highly verbose structured output increases inference latency and token cost compared to standard VideoLLMs

- **Failure signatures:**
  - Spatial Collapse: Model outputs correct answers but generic/hallucinated boxes because temporal gating is disabled or τ is too loose
  - Reward Hacking: Model generates valid boxes but for wrong object class to maximize format reward without semantic grounding

- **First 3 experiments:**
  1. Baseline SFT vs. RL: Train using only STGR-CoT-30k (SFT) and compare grounding accuracy against full SFT+GSPO pipeline
  2. Ablate Temporal Gating: Set τ=∞ during RL training to reproduce "noisy spatial feedback" issue
  3. Test-Time Scaling Check: Run inference with N=1 vs N=8 on VideoMME subset to verify +1-2% gain

## Open Questions the Paper Calls Out
- How can audio and speech modalities be effectively integrated into the spatio-temporal grounding framework to enhance reasoning for videos where auditory cues carry critical semantic information?
- How does the framework's performance degrade on longer videos (>10 minutes) with complex multi-object interactions, and what architectural modifications are needed to maintain spatial precision for small or occluded objects?
- What is the optimal trade-off between the number of test-time samples (N) and computational cost in confidence-aware voting, and does this relationship vary across different video reasoning task types?

## Limitations
- Current design does not integrate audio or speech information, which often carries crucial cues for understanding video content
- Handling longer videos with complex scenes and smaller objects remains challenging due to limited high-quality spatio-temporal data
- The framework's performance on temporally fast-paced videos and videos with rapid object movement has not been thoroughly evaluated

## Confidence
- **High Confidence**: V-STAR benchmark results are well-documented with ablation studies
- **Medium Confidence**: Adaptive temporal proximity mechanism is theoretically sound but sensitivity to annealing schedules unexplored
- **Medium Confidence**: Two-stage training approach is justified but alternative cold-start strategies not explored

## Next Checks
1. **Temporal gating ablation study**: Systematically vary τ (3s, 1s, 5s) during RL training and measure trade-off between spatial precision and learning stability
2. **GSPO mechanism isolation**: Implement GSPO variant that keeps sequence-level clipping but removes group importance ratio calculation
3. **Cross-dataset generalization**: Evaluate on temporally fast-paced dataset (Something-Something) to test τ=3s threshold and σ schedule generalizability