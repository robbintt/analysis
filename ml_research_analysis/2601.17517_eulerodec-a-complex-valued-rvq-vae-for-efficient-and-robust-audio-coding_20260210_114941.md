---
ver: rpa2
title: 'EuleroDec: A Complex-Valued RVQ-VAE for Efficient and Robust Audio Coding'
arxiv_id: '2601.17517'
source_url: https://arxiv.org/abs/2601.17517
tags:
- complex
- phase
- audio
- complex-valued
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EuleroDec, the first end-to-end complex-valued
  RVQ-VAE neural audio codec that preserves magnitude-phase coupling across the entire
  analysis-quantization-synthesis pipeline. Unlike existing frequency-domain codecs
  that split complex spectra into real-valued components or employ adversarial discriminators,
  EuleroDec uses complex convolutions, attention, and normalization to model phase
  coherently without GANs or diffusion post-filters.
---

# EuleroDec: A Complex-Valued RVQ-VAE for Efficient and Robust Audio Coding

## Quick Facts
- arXiv ID: 2601.17517
- Source URL: https://arxiv.org/abs/2601.17517
- Reference count: 0
- Primary result: First end-to-end complex-valued RVQ-VAE audio codec preserving magnitude-phase coupling across the entire pipeline, achieving 95% faster convergence while matching or exceeding state-of-the-art baselines in-domain and best-in-class out-of-domain performance.

## Executive Summary
EuleroDec introduces the first end-to-end complex-valued RVQ-VAE neural audio codec that preserves magnitude-phase coupling throughout the analysis-quantization-synthesis pipeline. Unlike existing frequency-domain codecs that split complex spectra into real-valued components or employ adversarial discriminators, EuleroDec uses complex convolutions, attention, and normalization to model phase coherently without GANs or diffusion post-filters. Trained on LibriTTS for 24 kHz speech at 6 and 12 kbps, it matches or exceeds state-of-the-art baselines in-domain and achieves best-in-class out-of-domain performance on LibriTTS-other, particularly in phase-aware metrics like SI-SDR and GDD. Training requires less than 50k steps, achieving 95% faster convergence while maintaining high perceptual quality.

## Method Summary
EuleroDec is a complex-valued variational autoencoder that operates directly on complex-valued STFT spectrograms without splitting them into real and imaginary components. The architecture uses complex convolutions with modReLU activation and 2×2 whitening normalization to preserve phase-magnitude coupling throughout encoding. A residual vector quantizer with 12 codebooks (2048 entries each) compresses the latent representation, followed by a symmetric decoder with complex operations. The model achieves 6 kbps at stride-8 and 12 kbps at stride-4 temporal compression. Training uses reconstruction losses (multi-resolution mel L1, spectrogram L1, complex spectral convergence) plus a quantization commitment term, without adversarial discriminators or diffusion post-filters.

## Key Results
- Matches or exceeds state-of-the-art baselines in-domain at both 6 and 12 kbps
- Achieves best-in-class out-of-domain performance on LibriTTS-other, particularly in phase-aware metrics (SI-SDR, GDD)
- Requires less than 50k training steps (95% faster convergence than baselines)
- Maintains high perceptual quality (PESQ) while improving phase accuracy

## Why This Works (Mechanism)

### Mechanism 1: Complex-valued operations preserve magnitude-phase coupling
Complex convolutions are linear over ℂ, coupling real and imaginary parts and allowing the model to learn amplitude-phase interactions rather than treating them as independent channels. The modReLU activation applies thresholding to modulus while preserving phase, and 2×2 whitening normalization models cross-channel dependence jointly. This coupling lets the model learn amplitude-phase interactions rather than treating x and y as independent channels. Phase and magnitude in audio signals have irreducible statistical dependencies that cannot be captured by treating them as separate real-valued channels.

### Mechanism 2: Removing adversarial discriminators improves training stability
Existing spectral codecs split complex spectra into real-valued components, inadequately representing the signal and requiring adversarial discriminators to compensate. EuleroDec's complex-valued pipeline provides sufficient representation power intrinsically, eliminating the need for GANs. The inadequacy that adversarial training compensates for stems from real-valued factorization of complex signals, not from fundamental limitations of neural codecs.

### Mechanism 3: Complex-domain consistency constrains representations for generalization
Complex operations maintain phase equivariance (rotational symmetry), constraining the learned representation space to geometrically meaningful regions and preventing overfitting to training distribution statistics. On LibriTTS-other (mismatched microphones, reverberation, noise), EuleroDec achieves best SI-SDR and GDD while baseline APCodec's performance collapses. Phase geometry is more invariant across recording conditions than magnitude statistics, so preserving it improves robustness.

## Foundational Learning

- **Complex numbers and Euler's formula (z = reiθ)**: Every operation in EuleroDec works on complex64 tensors. Understanding that phase rotates in the complex plane (eiφ multiplication) is essential for grasping why modReLU preserves phase and why 2×2 whitening matters. Given z₁ = 3+4i and z₂ = 1+i, what is |z₁|, ∠z₁, and z₁·z₂? If you multiply z₁ by eiπ/4, what happens to its phase?

- **STFT (Short-Time Fourier Transform) fundamentals**: EuleroDec operates on complex STFT spectrograms (NFFT=512, hop=64, Hann window). Understanding the time-frequency trade-off and why phase matters for transient localization (GDD metric) is critical. Why does a hop size of 64 samples at 24 kHz give ~46.9 tokens/s after stride-8 compression? What happens to phase if you reconstruct from magnitude-only STFT?

- **Residual Vector Quantization (RVQ)**: The quantizer uses 12 codebooks with 2048 entries each. Understanding how residuals are iteratively approximated (r(m) = z − Σe(j)) and why codebook perplexity matters is essential for debugging quantization issues. With K=2048 entries (11 bits) and M=12 codebooks, why is the bitrate ~6 kbps at stride-8? What does "100% code utilization" indicate vs. a collapsed codebook?

- **Wirtinger calculus (optional but recommended)**: The paper states optimization uses "Wirtinger calculus" for complex gradients. Standard backprop doesn't apply directly to holomorphic functions. Why can't we treat complex parameters as two independent real parameters for gradient computation? What does ∂f/∂z* mean?

## Architecture Onboarding

- **Component map**: Waveform -> STFT (512-point, 64-hop, Hann) -> complex64 spectrogram [B, 1, 257, T] -> Encoder (5 residual blocks, attention, downsampling) -> Flatten -> 12-stage RVQ -> Decoder (mirrored encoder) -> iSTFT reconstruction

- **Critical path**: 1) Waveform → STFT (preserving complex structure), 2) Encoder downsampling (anisotropic: stages 1,3,4 reduce both F and T; stage 2 reduces F only), 3) Frequency attention before quantization (critical for phase coherence across bins), 4) RVQ commitment loss (β=0.05) anchors encoder outputs to codebook, 5) Codebook EMA with warm-up (0.98→0.999) prevents early collapse, 6) Decoder mirrors encoder without pooling branches

- **Design tradeoffs**: Non-streaming (bidirectional attention) → Better phase coherence but precludes live use (RTF=0.344 on RTX 3090); Linear STFT (not log-magnitude) → Preserves phase but requires careful normalization; 12 codebooks vs. more → Sufficient for 6-12 kbps; Stride-8 (6 kbps) vs. stride-4 (12 kbps) → Temporal resolution vs. bitrate trade-off

- **Failure signatures**: Codebook collapse: Perplexity drops below ~50%; Phase artifacts (pre-echo, smearing): High GDD; Out-of-domain collapse: Large gap between test-clean and test-other; Slow convergence: If >50k steps needed

- **First 3 experiments**: 1) Sanity check: Train autoencoder-only (no quantizer) on 1-hour subset. Verify complex reconstruction with L2 on complex STFT + spectral convergence. Target: LSD <0.5, PESQ >3.0, 2) Quantizer integration: Add 4-codebook RVQ (reduced from 12) at 12 kbps. Monitor code usage per epoch. Target: 100% utilization by step 5k, perplexity >1500, 3) Domain robustness test: Train on LibriTTS clean-100, evaluate on both test-clean and test-other at 6 kbps. Target: SI-SDR gap <3 dB between domains

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several key limitations remain unaddressed in the current work.

## Limitations
- The non-causal architecture precludes real-time streaming applications despite achieving good phase coherence
- Performance on music and non-speech audio remains untested, limiting generalizability claims
- The phase-magnitude trade-off in perceptual quality is not fully characterized, with APCodec achieving higher PESQ in-domain

## Confidence

- **High confidence**: Complex convolution mechanics, RVQ implementation details, and ablation results for architectural components
- **Medium confidence**: The mechanism that removing GANs improves training stability specifically for complex-valued representations
- **Low confidence**: The claim that complex-domain consistency is the primary driver of out-of-domain generalization

## Next Checks

1. **Ablation on adversarial training**: Train EuleroDec with and without GAN discriminators while keeping the complex-valued architecture constant. Measure convergence speed, stability, and perceptual quality to isolate the effect of GAN removal.

2. **Cross-dataset generalization**: Evaluate on non-speech datasets (e.g., music, environmental sounds, MUSDB18) to test whether complex-domain consistency provides benefits beyond the LibriTTS-other domain shift.

3. **Phase-magnitude trade-off analysis**: Conduct a perceptual listening test comparing EuleroDec and APCodec outputs, explicitly assessing whether phase accuracy improvements come at unacceptable costs to magnitude fidelity or overall naturalness.