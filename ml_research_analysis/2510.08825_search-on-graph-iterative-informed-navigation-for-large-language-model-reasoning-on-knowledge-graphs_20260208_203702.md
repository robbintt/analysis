---
ver: rpa2
title: 'Search-on-Graph: Iterative Informed Navigation for Large Language Model Reasoning
  on Knowledge Graphs'
arxiv_id: '2510.08825'
source_url: https://arxiv.org/abs/2510.08825
tags:
- arxiv
- language
- entity
- knowledge
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Search-on-Graph (SoG), a simple yet effective
  framework for knowledge graph question answering that uses a single LLM with an
  iterative 1-hop search function. Instead of complex planning or large subgraph retrieval,
  SoG follows an "observe-then-navigate" approach where the LLM examines available
  relations at each entity before deciding the next hop.
---

# Search-on-Graph: Iterative Informed Navigation for Large Language Model Reasoning on Knowledge Graphs

## Quick Facts
- arXiv ID: 2510.08825
- Source URL: https://arxiv.org/abs/2510.08825
- Reference count: 40
- Primary result: State-of-the-art KGQA performance across six benchmarks using single LLM with iterative 1-hop search, no fine-tuning required

## Executive Summary
Search-on-Graph (SoG) introduces a simple yet effective framework for knowledge graph question answering that uses a single large language model with an iterative 1-hop search function. Unlike complex planning or large subgraph retrieval methods, SoG follows an "observe-then-navigate" approach where the LLM examines available relations at each entity before deciding the next hop. The method adapts seamlessly to different KG schemas and handles high-degree nodes through adaptive filtering. Across six KGQA benchmarks spanning Freebase and Wikidata, SoG achieves state-of-the-art performance without fine-tuning, with particularly strong gains on Wikidata benchmarks (+16% improvement over previous best methods).

## Method Summary
SoG uses a single LLM equipped with a SEARCH function that returns 1-hop neighbors in markdown table format. The LLM iteratively calls this function, examining available relations at each entity before deciding the next hop. The framework employs adaptive filtering for high-degree nodes (threshold k=50) and uses few-shot prompting with 5 exemplars per dataset. The method works across different KG schemas without fine-tuning, using a simple "observe-then-navigate" principle that reduces hallucination risk by conditioning decisions on observed graph structure rather than parametric knowledge.

## Key Results
- State-of-the-art performance on both Freebase (WebQSP 91.3%, GrailQA 86.9%) and Wikidata benchmarks (QALD-9 82.5%, QALD-10 79.8%)
- +16% improvement on Wikidata benchmarks over previous best methods
- Consistent improvements across all six KGQA benchmarks without fine-tuning
- 78.0% accuracy with lowest total tokens (5,622.5) using adaptive filtering strategy

## Why This Works (Mechanism)

### Mechanism 1: Observation-Grounded Navigation Reduces Hallucination Risk
Providing LLMs with actual available relations at each entity reduces reasoning errors compared to pre-planning approaches. At each step, the SEARCH function returns concrete 1-hop neighbors in markdown format, and the LLM conditions its next action on observed graph structure rather than parametric knowledge. This reduces hallucination propagation through the reasoning chain. The gold reasoning path requires relation types exposed in the initial property-only view, and single-path navigation does not sacrifice answer recall versus beam search.

### Mechanism 2: Adaptive Filtering Preserves Context Window Under High-Degree Nodes
The two-stage filtering strategy (property discovery → targeted retrieval) enables navigation of entities with thousands of neighbors without context overflow. When |R| > k (threshold=50) and no properties are specified, the method returns only unique property types, then makes a second targeted call with properties_to_filter_for. This transforms potentially intractable retrieval into two bounded-context operations. The relevant relation for the next reasoning hop appears in the property-only view, and LLMs can correctly identify and filter for that property based on question semantics.

### Mechanism 3: Schema-Agnostic Single-Function Design Enables Cross-KG Transfer
The architectural minimalism (one SEARCH function, no fine-tuning) facilitates transfer across heterogeneous KG schemas. The SEARCH function accepts only entity ID, direction, and optional properties—no schema-specific parameters. The markdown output includes both machine-readable IDs and human-readable labels, allowing the LLM to reason over relation semantics without hardcoded schema knowledge. Relation labels provide sufficient semantic signal for the LLM to map question intent to graph traversal decisions, regardless of underlying schema conventions.

## Foundational Learning

- **Concept: Knowledge Graph Topology (1-hop neighborhoods, directionality)**
  - Why needed here: SoG's core operation is iteratively expanding 1-hop neighborhoods. Understanding that entities have both incoming and outgoing relations—and that these define traversable paths—is essential for debugging navigation traces.
  - Quick check question: Given entity "Amsterdam" and relation "capital" (outgoing from Netherlands), what direction would you search to find Netherlands from Amsterdam?

- **Concept: LLM Tool-Calling / Function-Calling APIs**
  - Why needed here: The framework relies on structured tool outputs (not free-form text) that the LLM parses to make sequential decisions. Understanding the tool-call loop (model generates call → executor returns result → model reasons over result) is prerequisite.
  - Quick check question: In the SoG workflow, what happens when the SEARCH function returns 594 rows for a high-degree entity?

- **Concept: Few-Shot Prompting for Structured Reasoning**
  - Why needed here: SoG uses 5 manually constructed exemplars per dataset to demonstrate navigation patterns. The method's performance ceiling depends on the LLM's ability to generalize from these examples.
  - Quick check question: According to Figure 2, how many exemplars are needed before performance plateaus, and what does this suggest about the learning curve?

## Architecture Onboarding

- **Component map:** Question + Topic Entities → Few-Shot Prompt Construction → LLM (with SEARCH tool) → Tool Call: SEARCH(entity, direction, properties?) → Adaptive Neighbor Retrieval (Algorithm 1) → Markdown Table Output → LLM Reasoning → [Loop until answer found] → Final Answer

- **Critical path:** The prompt construction (system instructions + 5 exemplars) → SEARCH function implementation → adaptive filtering thresholds (k=50, p=1000). Errors in any component break the navigation loop.

- **Design tradeoffs:**
  - Single-path vs. beam search: SoG explores one relation per hop, reducing noise but potentially missing alternative paths
  - Property-first filtering: Saves tokens but requires additional turns for high-degree nodes
  - No fine-tuning: Enables plug-and-play deployment but limits optimization for specific KG schemas
  - Markdown vs. JSON output: Markdown reduces tokens (7,981.8 vs. 12,047.3) but may affect parsing reliability on some LLMs

- **Failure signatures:**
  - Infinite loops: LLM repeatedly searches the same entity without progressing toward answer
  - Premature termination: LLM outputs "Final answer" without sufficient evidence in tool outputs
  - Property misidentification: LLM filters for irrelevant properties when disambiguation requires seeing actual values
  - Schema confusion: LLM misinterprets relation labels due to schema-specific conventions (e.g., Freebase compound value types)

- **First 3 experiments:**
  1. **Baseline sanity check:** Run SoG (0-shot, no tool definitions) on 50 samples from WebQSP to establish IO Prompting baseline, then add tool definitions only. Expect jump similar to Figure 2 (e.g., 61.1% → ~74% for GPT-4o equivalent).
  2. **High-degree node stress test:** Identify entities with >500 neighbors (e.g., country entities), run with and without property filtering enabled. Measure token usage and accuracy to validate Table 2 findings.
  3. **Cross-schema transfer probe:** Run identical prompts on both Freebase and Wikidata versions of QALD questions. Compare navigation traces to identify schema-specific failure modes (e.g., qualifier handling in Wikidata).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific model capabilities determine whether an LLM can effectively leverage the iterative observation-decision framework for KG navigation?
- Basis in paper: The authors state: "The performance difference reveals that model architecture and inherent reasoning capabilities are critical for SoG's effectiveness... the Thinking variant extracts more value from navigation demonstrations, indicating that SoG's performance ceiling depends on the model's underlying capacity for structured reasoning over KGs."
- Why unresolved: The paper demonstrates the gap between Thinking and Instruct variants but does not isolate which architectural features (attention patterns, reasoning traces, training objectives) drive this difference.
- What evidence would resolve it: A systematic comparison controlling for model size, comparing models with and without explicit reasoning training, and analysis of attention patterns during KG traversal steps.

### Open Question 2
- Question: How does SoG perform when entity linking errors are introduced, rather than using gold entity annotations?
- Basis in paper: The methodology section states: "we use the gold entity annotations provided in the datasets, where entity mentions in questions are already linked to their KG identifiers, thus bypassing the need for entity linking." This is a methodological simplification not addressed in experiments.
- Why unresolved: Real-world KGQA systems must handle entity linking; performance degradation from linking errors could substantially impact SoG's practical applicability.
- What evidence would resolve it: Experiments using automatic entity linking (e.g., TAGME, DBpedia Spotlight) instead of gold annotations, reporting accuracy as a function of linking error rates.

### Open Question 3
- Question: What are the optimal thresholds for high-degree node filtering (k) and maximum result truncation (p) across different KG characteristics?
- Basis in paper: The authors set "high-degree threshold k=50 and maximum result size p=1000, balancing information completeness with context window constraints" without systematic ablation of these parameters.
- Why unresolved: These values may be suboptimal for KGs with different degree distributions (e.g., Wikidata's denser connectivity vs. Freebase's more hierarchical structure).
- What evidence would resolve it: Grid search over k ∈ {25, 50, 100, 200} and p ∈ {500, 1000, 2000} on held-out validation sets for each KG, measuring accuracy and token efficiency trade-offs.

## Limitations

- Manual few-shot exemplars not fully specified in paper, creating ambiguity about exact prompt construction
- Performance relies on gold entity annotations, bypassing real-world entity linking challenges
- Single-path navigation may miss alternative reasoning paths compared to beam search approaches
- Adaptive filtering thresholds (k=50, p=1000) may require tuning for different KG schemas or LLM capabilities

## Confidence

- **High confidence:** The adaptive filtering mechanism for high-degree nodes is well-specified and directly validated through controlled ablation showing reduced token usage with maintained accuracy
- **Medium confidence:** Cross-KG schema transfer claims are supported by empirical results but rely on the LLM's ability to interpret heterogeneous relation labels without explicit schema mapping
- **Medium confidence:** The "observe-then-navigate" principle reducing hallucination risk is plausible but lacks direct empirical isolation from other factors

## Next Checks

1. **Few-shot exemplar impact test:** Systematically vary the number and quality of exemplars (0, 1, 3, 5) on a single benchmark to measure performance sensitivity and identify the learning curve plateau
2. **Property disambiguation validation:** For entities where multiple relation types are semantically plausible, test whether the property-only view leads to incorrect filtering compared to seeing actual values first
3. **Beam search comparison:** Implement a multi-path variant that explores top-3 relations per hop and compare answer recall against single-path SoG on multi-hop benchmarks to validate the single-path design choice