---
ver: rpa2
title: 'AutoMeet: a proof-of-concept study of genAI to automate meetings in automotive
  engineering'
arxiv_id: '2507.16054'
source_url: https://arxiv.org/abs/2507.16054
tags:
- meeting
- meetings
- information
- minutes
- stddev
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AutoMeet presents a proof-of-concept pipeline using generative
  AI to automate meeting documentation in automotive engineering. The system records
  meetings, generates transcripts via Whisper, and creates summaries using GPT-4o,
  which are then made searchable through a chatbot interface.
---

# AutoMeet: a proof-of-concept study of genAI to automate meetings in automotive engineering

## Quick Facts
- arXiv ID: 2507.16054
- Source URL: https://arxiv.org/abs/2507.16054
- Reference count: 0
- Primary result: Proof-of-concept pipeline using Whisper + GPT-4o achieves 2.58/6 summary quality; users estimate 10.5% time savings if adopted

## Executive Summary
AutoMeet is a proof-of-concept pipeline that automates meeting documentation in automotive engineering by transcribing meetings with Whisper, summarizing with GPT-4o, and making content searchable via a chatbot interface. A survey of 42 developers found that 51% of work time is spent in meetings, with potential 10.5% time savings from reduced meeting needs and improved information transparency. Users reported high willingness to record meetings when privacy features like easy recording controls and data deletion are provided, but summary quality (2.58/6) and domain-specific vocabulary errors remain challenges. The study identifies user acceptance and organizational factors—not technical ones—as the main barriers to implementation.

## Method Summary
The system processes meeting recordings through a local Whisper transcription pipeline, then uses GPT-4o via LangChain for summarization with iterative refinement for long transcripts. A manual privacy filter reviews summaries for personal data and technical accuracy before database integration. The RAG-based chatbot enables asynchronous query access to meeting content. The proof-of-concept used local storage and a custom OpenAI API endpoint to address privacy concerns, with both short and long summary versions generated for different use cases.

## Key Results
- Average summary quality rated 2.58/6 (1=very good, 6=very bad) across 12 users
- Users spend approximately 51% of work time in meetings
- Estimated 10.5% working time savings possible through reduced meeting needs and improved information transparency
- High user willingness to record meetings (median rating 1/4) when privacy features are present
- Technical vocabulary errors (e.g., "centering" vs "sintering") significantly impact summary quality

## Why This Works (Mechanism)

### Mechanism 1: Asynchronous Knowledge Retrieval via RAG
The pipeline converts ephemeral speech into structured text (transcripts) and then into vector embeddings. When users query the system, a RAG architecture retrieves relevant past meeting segments and feeds them to an LLM to synthesize answers, potentially reducing the total number of meetings required. The core assumption is that users will trust the system enough to rely on it for critical information rather than attending live meetings.

### Mechanism 2: Privacy-Preserving Incentive Alignment
User acceptance of surveillance-based AI tools depends more on granularity of user control over data lifecycle than on technical summary quality. The system proposes user-controlled toggles (start/stop recording) and guarantees deletion of raw audio after processing, lowering perceived risk of permanent records of casual speech. Users are rational actors who will trade privacy for efficiency if they retain agency.

### Mechanism 3: Domain-Specific Hallucination Mitigation via Human-in-the-Loop
LLMs cannot currently distinguish domain-specific jargon without supervision, so the mechanism relies on manual review to transform AI-drafts into trusted minutes. The pipeline outputs a "pre-summary" which a human must validate to correct transcription errors and LLM hallucinations before data enters the central database. The core assumption is that users have enough time and motivation to review AI output.

## Foundational Learning

- **Word Error Rate (WER) in Speech-to-Text**: Critical because transcription quality varies significantly (0.23 to 0.65 error rate) based on audio quality and speaker overlap. Quick check: How does the system handle a transcript where "sintering" is mistranscribed as "centering," and which module is responsible for catching this?

- **Retrieval-Augmented Generation (RAG)**: The core architecture for the chatbot that prevents the LLM from answering from general training data and forces it to use only the meeting minutes provided. Quick check: In the AutoMeet architecture, what happens if the retriever fails to fetch a relevant meeting document—does the LLM refuse to answer or hallucinate?

- **Iterative Refinement (Summarization)**: Needed because meeting transcripts often exceed the context window of standard LLM prompts. Quick check: Why is a single prompt insufficient for summarizing a 60-minute engineering meeting, and how does iterative refinement solve this?

## Architecture Onboarding

- **Component map**: Meeting Recording (Audio) + Presentation Slides (PDF) -> Whisper (Transcription) -> GPT-4o (Summarization via LangChain) -> Human-in-the-loop (Privacy Filter / Technical Correction) -> Local folders (PoC) -> Central Database (Target) -> RAG Chatbot (GPT-4o + Vector Store)

- **Critical path**: 1. Recording quality (Audio must be clear enough for Whisper to lower WER) 2. Summary validation (Human must correct technical terms) 3. Indexing (Corrected text must be ingested into the Vector DB for the Chatbot to find it)

- **Design tradeoffs**: Local vs. Cloud (PoC uses local Whisper and custom API to address privacy, but may limit processing speed); Short vs. Long Summaries (Long preserve detail for RAG, short reduce reading time); Automation vs. Control (Fully automating saves time but risks errors, manual review ensures accuracy but costs human time)

- **Failure signatures**: "Centering" instead of "Sintering" (STT model lacks domain vocabulary); Hallucinated Action Items (LLM invents tasks never agreed upon); Privacy Leakage (Names appearing in summaries despite prompts to omit them)

- **First 3 experiments**: 1. WER Benchmarking: Run transcription on 5 different meeting types to measure correlation between crosstalk and transcription failure rates 2. RAG Retrieval Accuracy: Ingest 10 corrected meeting minutes and query chatbot with specific technical questions to see if it retrieves correct source document 3. Summary Acceptor Test: Measure time for engineer to edit AI-generated summary vs writing minutes from scratch to validate "10.5% time saved" hypothesis

## Open Questions the Paper Calls Out

### Open Question 1
How do organizational implementation strategies (vs. technical improvements) affect user acceptance and sustained adoption of AI meeting tools in engineering contexts? The study concludes that "organizational aspects are crucial for a successful ethical usage of such a system" and identifies user acceptance, not technical limitations, as the main barrier to implementation. This proof-of-concept collected initial user feedback but did not test long-term deployment or compare different organizational rollout strategies.

### Open Question 2
What is the actual time savings and productivity impact when AutoMeet is deployed at organizational scale, compared to the estimated 10.5% working time savings? The paper estimates "savings can be estimated to be around 10.5% of saved working time per employee" based on survey projections, but notes the full pipeline implementation is "a potential mid- to long-term target." The estimate is based on self-reported survey data about hypothetical usage, not actual deployment metrics.

### Open Question 3
How effectively can domain-specific fine-tuning or vocabulary adaptation address the transcription errors (e.g., "centering" vs. "sintering") that currently limit summary quality in technical engineering contexts? The proof-of-concept used off-the-shelf Whisper without domain adaptation; the authors recommend shifting to "more stable tooling" but do not test improved transcription approaches. The paper reports summary quality ratings of only 2.58/6, with users noting that "special vocabulary is not recognized by the current transcription model."

## Limitations

- Domain-specific accuracy gap: Technical vocabulary errors significantly impact summary quality, and the manual review step required for correction contradicts promised efficiency gains
- Privacy implementation assumptions: High user willingness to record meetings was reported in survey context, but actual implementation may reveal stronger resistance
- Organizational adoption barriers: Paper identifies user acceptance as main barrier but provides limited empirical evidence about how these factors manifest in actual automotive engineering workflows

## Confidence

- **High Confidence**: Core observation that meeting time represents ~51% of developer work time is well-supported by survey data (n=42); basic feasibility of using Whisper + GPT-4o for transcription and summarization is demonstrated
- **Medium Confidence**: Estimated 10.5% time savings from reduced meeting needs and improved information transparency is plausible but relies on assumptions about user behavior change not directly tested
- **Low Confidence**: Assertion that privacy features alone will drive adoption is based on survey ratings rather than observed behavior; technical architecture described but key implementation details remain unspecified

## Next Checks

1. **Domain Vocabulary Benchmarking**: Conduct controlled test where Whisper transcribes 20 automotive engineering meetings with high technical content, then measure frequency and impact of domain-specific transcription errors on downstream summary quality

2. **RAG Retrieval Effectiveness**: With database of 50 corrected meeting summaries, perform 100 randomized technical queries to measure retrieval accuracy and assess whether users can reliably find specific information without attending meetings

3. **Time Savings Validation**: Track cohort of engineers using the system for one month, measuring actual time spent on meeting documentation versus traditional methods, and documenting whether human review burden negates projected efficiency gains