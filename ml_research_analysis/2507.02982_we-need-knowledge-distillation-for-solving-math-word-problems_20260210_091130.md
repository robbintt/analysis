---
ver: rpa2
title: We Need Knowledge Distillation for Solving Math Word Problems
arxiv_id: '2507.02982'
source_url: https://arxiv.org/abs/2507.02982
tags:
- vectors
- accuracy
- bert
- distillation
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of knowledge distillation to compress
  large language models for solving math word problems, addressing the high computational
  costs of deploying LLMs in educational settings. The approach involves compressing
  the embedded vectors from BERT and transferring this knowledge to a smaller student
  model.
---

# We Need Knowledge Distillation for Solving Math Word Problems

## Quick Facts
- arXiv ID: 2507.02982
- Source URL: https://arxiv.org/abs/2507.02982
- Authors: Zhenquan Shen; Xinguo Yu; Xiaotian Cheng; Rao Peng; Hao Ming
- Reference count: 40
- One-line primary result: Knowledge distillation enables a student model with 1/12 parameters to achieve ~90% of BERT-base's performance on math word problems while reducing computational costs.

## Executive Summary
This paper addresses the high computational costs of deploying large language models (LLMs) for solving math word problems (MWPs) by introducing a knowledge distillation approach. The method compresses BERT-encoded vectors from 768 to 256 dimensions and transfers this knowledge to a smaller student model with 1/12 the parameters. The student model achieves nearly 90% of the teacher model's performance across three MWP tasks: relation extraction, equation formulation, and answer retrieval. The study also investigates why this compression works, finding that part-of-speech information is more critical than entity recognition for MWP solving.

## Method Summary
The approach uses a two-stage knowledge distillation process: first distilling from pre-trained BERT to obtain a preliminary student, then from fine-tuned BERT to the final student. A linear compressor reduces BERT's 768-dimensional vectors to 256 dimensions. The student model consists of an embedding layer and three transformer layers with 256 hidden size and 16 attention heads. Training minimizes MSE loss between teacher and student encoded vectors, with task-specific decoders (QRAN for relation extraction, GTS decoder for equation formulation and answer retrieval) used for evaluation.

## Key Results
- The student model achieves ~90% of BERT-base's performance while using only 1/12 of its parameters.
- Compressing BERT vectors to 256 dimensions maintains task performance across relation extraction, equation formulation, and answer retrieval.
- Part-of-speech information, rather than entity recognition, is crucial for MWP compressibility.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Compressing BERT-encoded vectors to 256 dimensions preserves task performance for MWP-related tasks.
- **Mechanism:** The 768-dimensional BERT vectors contain redundancy for mathematical tasks. A linear layer compressor projects these vectors to a lower-dimensional space while retaining task-critical information. PCA analysis shows linear properties dominate BERT's representations, making linear compression effective.
- **Core assumption:** Redundant dimensions in BERT embeddings do not encode information critical for MWP solving.
- **Evidence anchors:** Compression to 256 dimensions maintains task performance; further compression to 128 dimensions causes 38.55% relative drop in accuracy.

### Mechanism 2
- **Claim:** Part-of-speech information, not entity recognition, is the critical signal for MWP solving.
- **Mechanism:** Mathematical reasoning relies on identifying logical and quantitative relations rather than understanding specific entities. Nouns function as placeholders. This means embeddings can discard entity-specific details while preserving syntactic/POS patterns.
- **Core assumption:** MWP solving is fundamentally a syntactic-template-matching problem where logical structure outweighs entity semantics.
- **Evidence anchors:** POS tagging accuracy remains nearly unchanged after compression to 64 dimensions; models pre-trained on POS tagging show significantly higher initial and final accuracy on equation/answer tasks.

### Mechanism 3
- **Claim:** Two-stage knowledge distillation transfers encoding capability to a student model with 1/12 parameters while retaining ~90% performance.
- **Mechanism:** First, distill from pre-trained BERT to obtain a preliminary student. Second, distill from fine-tuned BERT to the final student. MSE loss between teacher and student vectors guides training. The student learns to approximate the teacher's encoding function.
- **Core assumption:** The encoding function can be approximated by a much smaller network if task-relevant information is preserved.
- **Evidence anchors:** Final answer accuracy: 32.87% (undistilled) → 33.37% (distilled); relative to BERT-base's ~33.27%, student achieves ~90%+ of teacher performance.

## Foundational Learning

- **Concept: Knowledge Distillation (Hinton et al., 2015)**
  - Why needed here: The core technique for transferring knowledge from BERT (teacher) to smaller student model.
  - Quick check question: Can you explain why soft targets from the teacher provide more information than hard labels?

- **Concept: Transformer Architecture (Attention, Multi-Head, FFN)**
  - Why needed here: Both teacher (BERT-base: 12 layers, 768 hidden) and student (3 layers, 256 hidden) use transformer layers; understanding attention is essential.
  - Quick check question: What is the role of Q, K, V in scaled dot-product attention?

- **Concept: Dimensionality Reduction (PCA, Linear Projection)**
  - Why needed here: The compressor uses linear layers and PCA analysis to reduce 768-dim vectors to 256-dim.
  - Quick check question: Why does PCA preserve maximum variance, and why might that matter for downstream task performance?

## Architecture Onboarding

- **Component map:** Teacher (BERT-base 768-dim, 12 layers, 12 heads) -> Compressor (linear 768→256) -> Student (Embedding + 3 layers, 256 hidden, 16 heads) -> Decoders (QRAN for relation extraction, GTS for equation/answer)

- **Critical path:** 1) Extract 768-dim vectors from BERT's final layer; 2) Compress to 256-dim via linear layer; 3) Train student to minimize MSE vs. compressed teacher vectors; 4) Evaluate on three MWP tasks using task-specific decoders

- **Design tradeoffs:** 256 dims provides best balance (128 dims causes ~40% relative accuracy drop); linear compressor vs. PCA (linear is simpler and trainable); two-stage distillation adds training cost but improves initialization

- **Failure signatures:** Accuracy plateaus well below teacher (compressor may discard task-critical dimensions); student overfits quickly (distillation loss may not capture meaningful structure); task-specific performance varies widely (encoder may not generalize)

- **First 3 experiments:**
  1. **Baseline compression test:** Freeze BERT, train compressor (linear 768→256) + decoder on Math23K; measure accuracy drop vs. full BERT. Confirms 256 dims is viable.
  2. **Ablation on compression method:** Compare linear layer vs. PCA vs. LLE vs. MDS on relation extraction task at 256/128/64 dims. Expect PCA and linear to outperform nonlinear methods.
  3. **Distillation effectiveness:** Train student from scratch vs. two-stage distillation; compare initial and final accuracy on equation/answer tasks. Expect distilled student to converge faster and reach higher accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the compressibility of embedded vectors and the sufficiency of part-of-speech (POS) information generalize to decoder-only Large Language Models (e.g., GPT, LLaMA)?
- Basis in paper: The study focuses exclusively on BERT (an encoder-only model), while the related work acknowledges that decoder-only LLMs are becoming prominent for math reasoning.
- Why unresolved: The distillation method relies on vector regression from BERT's specific architecture; decoder models function differently, generating text auto-regressively rather than encoding static vectors.
- What evidence would resolve it: Applying the specific vector compression and distillation pipeline to a decoder-only architecture and evaluating if the student model retains 90% performance.

### Open Question 2
- Question: Can the relationship between part-of-speech reliance and vector compressibility be formalized as a causal mechanism rather than a correlation?
- Basis in paper: The authors state: "To position this [POS sufficiency] as one of the intrinsic reasons for the compressibility of vectors in MWP tasks, we must clarify the relationship between the two."
- Why unresolved: The paper demonstrates that POS tagging remains accurate with compressed vectors and aids MWP solving, but this establishes utility, not necessarily the theoretical cause of compressibility.
- What evidence would resolve it: An ablation study demonstrating that enforcing POS retention during compression directly prevents performance degradation, effectively proving causality.

### Open Question 3
- Question: Can advanced non-linear compression techniques recover the performance drop observed when reducing vector dimensions below 256?
- Basis in paper: The authors found that reducing dimensions to 128 caused a significant accuracy drop (approx. 38-40%), and linear/PCA methods outperformed tested non-linear ones (LLE, MDS) at maintaining similarity.
- Why unresolved: The study concluded linear compression was superior for the tested methods, but did not explore if more sophisticated non-linear autoencoders could better preserve the manifold structure at extreme compression rates.
- What evidence would resolve it: Testing learned non-linear compressors (e.g., variational autoencoders) to determine if they maintain higher accuracy than linear layers at 128 or 64 dimensions.

## Limitations

- The claim of "90% of teacher performance" lacks external benchmarking against published MWP solving baselines.
- The mechanism analysis relies on correlation rather than direct causal evidence for why POS information enables compressibility.
- The two-stage distillation process adds significant complexity without clear quantification of its marginal benefit versus single-stage approaches.

## Confidence

**High Confidence:** The empirical observation that linear compression to 256 dimensions preserves substantial task performance is well-supported by controlled experiments comparing different dimensionalities with measurable accuracy drops.

**Medium Confidence:** The claim that the student model achieves "nearly 90% of teacher performance" is credible given the reported accuracies, but lacks comparison to published MWP solving baselines.

**Low Confidence:** The theoretical claims about why compression works (POS importance over entity recognition) and the broader implication that MWPs are primarily syntactic problems lack direct experimental validation.

## Next Checks

1. **External Baseline Comparison:** Evaluate the distilled student model against published MWP solving approaches on Math23K (e.g., GTS, MWPDiff) to determine whether the compression achieves state-of-the-art performance or merely competitive results within the distillation framework.

2. **Direct POS vs. Entity Ablation:** Design an experiment that explicitly controls for POS information versus entity recognition by creating modified versions of the Math23K dataset where either POS tags or entity labels are removed or corrupted, then measure the differential impact on MWP solving performance across compressed and uncompressed models.

3. **Single-Stage Distillation Validation:** Implement and evaluate a single-stage distillation approach (directly from pre-trained BERT to student without the intermediate fine-tuning step) to quantify the marginal benefit of the two-stage process and determine whether the additional complexity is justified by measurable performance gains.