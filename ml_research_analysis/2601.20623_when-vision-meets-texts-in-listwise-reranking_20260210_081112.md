---
ver: rpa2
title: When Vision Meets Texts in Listwise Reranking
arxiv_id: '2601.20623'
source_url: https://arxiv.org/abs/2601.20623
tags:
- reranking
- training
- text
- multimodal
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Rank-Nexus is a lightweight multimodal reranker that processes
  image-text documents through progressive training, starting with text reranking,
  then image pairwise ranking, and finally joint listwise ranking. The model uses
  knowledge distillation from large language models to generate training data, employs
  diversity-based coreset selection to minimize data requirements, and operates on
  a compact 2B parameter vision-language model.
---

# When Vision Meets Texts in Listwise Reranking

## Quick Facts
- **arXiv ID:** 2601.20623
- **Source URL:** https://arxiv.org/abs/2601.20623
- **Reference count:** 40
- **Key outcome:** Rank-Nexus achieves state-of-the-art performance on text (DL19, DL20, BEIR) and multimodal benchmarks (INQUIRE, MMDocIR) while maintaining 2-8× faster inference than reasoning-based rerankers through progressive training and diversity-based coreset selection.

## Executive Summary
Rank-Nexus introduces a lightweight multimodal reranker that achieves state-of-the-art performance on both text and multimodal benchmarks while operating with only 2B parameters and 7.5% of typical training data. The system uses progressive training across modalities—starting with text reranking, then image pairwise ranking, and finally joint listwise ranking—to build ranking capabilities that transfer across domains. By employing knowledge distillation from large language models and diversity-based coreset selection, Rank-Nexus demonstrates that effective multimodal reranking can be achieved without the computational overhead of reasoning-based approaches.

## Method Summary
Rank-Nexus processes image-text documents through a three-stage progressive training pipeline: (1) text listwise reranking using 4k MS MARCO samples, (2) image pairwise ranking using 1k distilled samples, and (3) image listwise ranking using 2.1k MMDocIR samples. The model employs knowledge distillation from Claude-4.5/GPT-4 to generate training data, uses CLIP embeddings for quality filtering and diversity-based coreset selection via a greedy maximum marginal diversity algorithm, and applies QLoRA fine-tuning (rank=8, α=8) to a 2B parameter vision-language model. Inference uses direct listwise ranking computation via Plackett-Luce probability without chain-of-thought reasoning.

## Key Results
- Achieves 74.6 nDCG@10 on DL19, outperforming 7B RankZephyr (73.9) and approaching GPT-4 (75.6)
- Maintains 2.16s/query inference speed vs 8.92s for RankZephyr and 16.0s for Rank-R1
- Text-only training improves image ranking by +2.8 nDCG@50 on INQUIRE without any image exposure
- Performance peaks at 4k training samples, degrading at larger sizes due to noise accumulation
- Diversity-based selection (74.60 DL19) substantially outperforms random (71.90) and K-means (73.23) sampling

## Why This Works (Mechanism)

### Mechanism 1: Progressive Cross-Modal Training Enables Knowledge Transfer
Sequential training across modalities builds ranking capabilities that transfer from text to images because ranking logic is modality-agnostic. Text reranking teaches fundamental principles like relevance identification and candidate comparison, which the visual encoder bridges into shared reasoning space.

### Mechanism 2: Diversity-Based Coreset Selection Outperforms Volume Scaling
Strategic selection of diverse training samples achieves superior performance with 7.5% of typical data volume. Greedy maximum marginal diversity algorithm maximizes semantic coverage while filtering redundant patterns, avoiding noise accumulation from homogeneous large datasets.

### Mechanism 3: Direct Listwise Computation Avoids Reasoning Overhead
Direct score computation without chain-of-thought reasoning achieves comparable or better accuracy at 8-16× lower latency. The model outputs relevance scores for all documents simultaneously via Plackett-Luce probability model, optimizing global ranking directly.

## Foundational Learning

- **Listwise vs Pairwise Reranking**: Understanding this distinction explains why the model can capture global ordering context. Quick check: Given Equation 3 (Plackett-Luce), why would processing all candidates together yield different results than aggregating pairwise comparisons via Equation 1?
- **Knowledge Distillation Pipeline**: Training data is generated by querying Claude-4.5/GPT-4 for rankings. Understanding distillation helps debug data quality issues. Quick check: What are the failure modes if teacher model rankings contain systematic biases or errors?
- **CLIP Embedding Space Alignment**: Quality filtering and diversity selection both rely on CLIP embeddings. The shared embedding space enables cross-modal similarity computation. Quick check: How does cosine similarity between image and text embeddings enable filtering of weakly-related query-image pairs?

## Architecture Onboarding

- **Component map:** Base VLM (InternVL-3-2B/Qwen3-VL-2B) → CLIP Encoder → Quality Filtering (similarity threshold) → Greedy Diversity Selection → Three-Stage Training → QLoRA Fine-tuning → Inference via Direct Listwise Ranking
- **Critical path:** Teacher model distillation → CLIP-based quality filtering → Diversity coreset selection → Stage 1 text training → Stage 2+3 image training → Direct forward pass inference
- **Design tradeoffs:** 2B vs 7B+ (faster inference but may struggle with fine-grained visual details), Direct computation vs reasoning (8-16× faster but may miss edge cases), 4k samples vs 100k (93% reduction requires high-quality curation), Temperature τ=0.1 (low temperature increases confidence but may overfit)
- **Failure signatures:** Training data >4k samples degrades performance, skipping Stage 1 text training yields worse image ranking, low CLIP similarity pairs cause noisy gradients, random sampling instead of diversity selection causes 3-5 point nDCG drop
- **First 3 experiments:** (1) Data scaling replication: train on {1k, 4k, 8k, 10k} samples to verify diminishing returns pattern, (2) Progressive training ablation: compare full 3-stage vs direct joint vs text-only initialization, (3) Selection strategy comparison: implement diversity-based vs random vs K-means centroid sampling on new multimodal corpus

## Open Questions the Paper Calls Out

- **Cross-modal transfer mechanism:** Why does text-only reranking training improve image retrieval performance? The paper observes +2.8 INQUIRE improvement but treats it as empirical finding rather than investigating underlying mechanisms.
- **Data scaling degradation:** What causes performance degradation beyond 4,000 samples? The paper attributes this to redundancy, noise, and overfitting but doesn't isolate specific factors.
- **Retriever dependency:** How dependent is Rank-Nexus on the choice of first-stage retriever? The reranker is evaluated exclusively on ColQwen outputs; sensitivity to retrieval quality remains unquantified.
- **Fine-grained visual reasoning:** Can lightweight models match proprietary models on fine-grained visual reasoning without increasing parameters? The 2B model occasionally struggles with complex technical diagrams compared to larger models.

## Limitations

- Heavy reliance on knowledge distillation from proprietary LLMs (Claude-4.5, GPT-4) creates licensing and API dependency issues
- Cross-modal transfer mechanism remains theoretically under-explained despite empirical success
- Performance may degrade on highly technical or domain-specific imagery requiring fine-grained visual analysis
- Diversity-based coreset selection depends critically on CLIP embedding quality and unspecified similarity thresholds

## Confidence

- **High Confidence:** Progressive training efficacy, direct listwise computation efficiency gains, diversity-based selection superiority, data scaling diminishing returns
- **Medium Confidence:** Cross-modal transfer from text to image ranking, CLIP-based quality filtering effectiveness, QLoRA configuration impact
- **Low Confidence:** Generalization to specialized domains, long-term stability of distilled knowledge, exact CLIP variant selection impact

## Next Checks

1. **Mechanism validation:** Conduct ablation studies comparing progressive training against direct joint training and text-only initialization on INQUIRE to quantify each stage's contribution and test cross-modal transfer boundaries.
2. **Data selection replication:** Implement diversity-based vs random vs K-means centroid sampling strategies on a new multimodal corpus to verify that coreset selection provides consistent 3-5 point nDCG improvements across domains.
3. **Model scaling experiment:** Test whether 4B-8B parameter vision-language models improve fine-grained visual reasoning performance on technical diagram datasets while measuring inference latency tradeoffs.