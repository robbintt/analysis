---
ver: rpa2
title: 'SIRI: Scaling Iterative Reinforcement Learning with Interleaved Compression'
arxiv_id: '2509.25176'
source_url: https://arxiv.org/abs/2509.25176
tags:
- length
- training
- arxiv
- compression
- siri
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of repetitive thinking patterns
  in Large Reasoning Models (LRMs), which leads to increased token usage without proportional
  gains in performance. The proposed method, SIRI, introduces an iterative reinforcement
  learning framework that alternates between compressing and expanding the reasoning
  budget by dynamically adjusting the maximum rollout length during training.
---

# SIRI: Scaling Iterative Reinforcement Learning with Interleaved Compression

## Quick Facts
- arXiv ID: 2509.25176
- Source URL: https://arxiv.org/abs/2509.25176
- Reference count: 26
- Primary result: SIRI improves AIME24 performance by 43.2% while reducing token usage by 46.9% on a 1.5B model

## Executive Summary
SIRI addresses the problem of repetitive reasoning patterns in Large Reasoning Models by introducing an iterative reinforcement learning framework that alternates between compressing and expanding the reasoning budget. The method forces models to produce more concise reasoning during compression phases while allowing exploration during expansion phases. Experiments on DeepSeek-R1-Distill-Qwen-1.5B show that SIRI-low improves AIME24 performance by 43.2% while reducing token usage by 46.9% after three iterations, pushing the efficiency-performance Pareto frontier forward.

## Method Summary
SIRI implements iterative reinforcement learning with a cosine-length scheduler that oscillates between L_min=8192 and L_max=16384 tokens over 640-step cycles. The method uses GRPO with decoupled clip thresholds (0.28 high, 0.2 low) and a length-capping reward that returns 1 only if a correct answer can be extracted from the truncated response. Training proceeds through at least three full compression-expansion cycles on 40K math questions, with both compressed-end (SIRI-low) and expanded-end (SIRI-high) checkpoints extracted for evaluation.

## Key Results
- SIRI-low achieves 43.2% accuracy improvement on AIME24 while reducing token usage by 46.9%
- SIRI-high achieves highest accuracy among all methods tested
- The method consistently pushes the Pareto frontier forward across different model sizes
- Compression-expansion cycles influence verification behavior, suppressing "wait" tokens during compression

## Why This Works (Mechanism)

### Mechanism 1: Compression-Forced Reasoning Densification
Constraining maximum rollout length during compression phases forces the model to prioritize high-value reasoning steps, pruning redundant patterns while preserving critical inference chains. The reward function only assigns positive gradients to responses achieving correctness within the reduced budget, selecting for "reasoning density." This works with smooth schedulers (640-cycle cosine) but not with abrupt ones, suggesting the model needs time to identify and preserve essential reasoning.

### Mechanism 2: Backtracking Token Modulation via Budget Oscillation
The compression-expansion cycle specifically targets verification and backtracking behavior ("wait" tokens), which are primary sources of token inefficiency. Under length pressure, the model suppresses these tokens that signal verification loops. When the budget relaxes, these tokens re-emerge in a more targeted manner—the model has learned when backtracking is actually necessary versus when it was purely habitual. This creates a refined verification policy that activates only on genuinely uncertain branches.

### Mechanism 3: Entropy-Bounded Exploration via Cyclical Constraints
The oscillating length schedule maintains policy entropy within a bounded range, preventing both entropy collapse (over-convergence to short, incorrect patterns) and entropy explosion (unconstrained exploration that wastes tokens). Compression phases reduce entropy as the model commits to shorter, higher-confidence paths. Expansion phases allow entropy to rise again as the model explores variations on the now-condensed reasoning core. Critically, entropy stabilizes within a range, indicating the model retains exploration capacity even as it becomes more efficient.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**
  - Why needed: SIRI builds directly on GRPO/DAPO for its RL training loop; understanding how advantages are computed from group-level rewards is essential for debugging reward shaping and length-capping interactions
  - Quick check: Given 8 sampled responses with rewards [1, 1, 0, 0, 0, 0, 0, 0], what is the advantage of the first response? (Answer: (1 - 0.25) / std ≈ positive)

- **Test-Time Scaling / Chain-of-Thought Length Budgets**
  - Why needed: The entire SIRI framework operates by manipulating maximum generation length during training; understanding why longer CoT helps—and when it becomes wasteful—is prerequisite to appreciating what the scheduler is optimizing
  - Quick check: If a model achieves 90% accuracy with 10K tokens and 85% accuracy with 4K tokens, which point is on the Pareto frontier? (Answer: Depends on your efficiency-performance tradeoff)

- **KL Divergence and Policy Divergence in RLHF**
  - Why needed: SIRI adopts DAPO's removal of KL penalty, allowing the policy to diverge further from the reference; this is a deliberate design choice that affects how aggressively the model can change its behavior during compression phases
  - Quick check: What failure mode does removing the KL constraint risk, and what does SIRI rely on to prevent it? (Answer: Reward hacking or excessive drift; SIRI relies on the length cap and outcome-based reward)

## Architecture Onboarding

- **Component map:** Base model (DeepSeek-R1-Distill-Qwen) -> VeRL framework -> GRPO/DAPO optimizer -> Cosine length scheduler -> Length-capping reward function -> 40K math dataset

- **Critical path:**
  1. Initialize from distilled reasoning model (not raw base model—needs prior reasoning capability)
  2. Configure cosine scheduler with cycle length ≥480 steps (640 preferred) to ensure smooth compression
  3. Run at least 3 full compression-expansion cycles (1920+ steps) before evaluating final checkpoints
  4. Extract both compressed-end (SIRI-low) and expanded-end (SIRI-high) checkpoints for deployment

- **Design tradeoffs:**
  - **Stair vs. cosine scheduler:** Stair gives stronger expansion-phase gains but sharper compression-phase drops; cosine provides more stable improvement per cycle
  - **Cycle length:** Shorter cycles (320) train faster but risk performance collapse during compression; longer cycles (640) are more robust but require more compute
  - **L_min/L_max ratio:** Paper uses 8K/16K (2:1); tighter ratios may not provide enough compression signal, wider ratios may cause unrecoverable performance drops

- **Failure signatures:**
  - Accuracy drops >5% during compression phase and does not recover → scheduler too aggressive; increase cycle length or raise L_min
  - Response length does not track scheduler (stays flat) → reward signal not differentiating by length; check that groups contain mixed-length correct/incorrect responses
  - Entropy collapses toward zero → policy over-converging; consider reducing clip-low threshold or adding entropy bonus during expansion

- **First 3 experiments:**
  1. Reproduce the 640-cycle cosine schedule on DeepSeek-R1-Distill-Qwen-1.5B with provided hyperparameters; validate accuracy-length dynamics match Figure 4
  2. Ablate scheduler shape: compare cosine vs. stair vs. stair-cosine with fixed 480-cycle length on validation data; measure both final accuracy and minimum accuracy during compression
  3. Test generalization to different base models: apply best scheduler configuration to DeepSeek-R1-Distill-Qwen-7B and compare efficiency gains

## Open Questions the Paper Calls Out

1. What determines the upper performance threshold of SIRI, and do factors like dataset size or algorithm efficiency fundamentally limit it? (The paper demonstrates consistent gains across three iterations but does not establish when or why improvements would plateau.)

2. Does SIRI generalize to reasoning-intensive domains beyond mathematics, such as code generation? (All experiments are conducted on mathematical reasoning benchmarks.)

3. Why does performance continue improving across iterations even after the model learns to be token-efficient? (The token frequency analysis explains what changes behaviorally but not why iterative cycles yield compounding benefits rather than diminishing returns.)

## Limitations
- Results rely on proprietary answer extraction logic and dataset composition that are not fully specified
- The method shows promise across different model scales but has not been tested on larger frontier models where overthinking patterns may differ fundamentally
- Mechanistic explanations are largely correlational rather than causally proven

## Confidence
- **Medium** for core efficiency-performance claims (43.2% accuracy gain, 46.9% token reduction) due to reliance on specific dataset configuration and distillation base model
- **Low** for mechanistic explanations as evidence is largely correlational without controlled ablations
- **Medium** for generalization across model scales (1.5B, 7B) but untested on larger models

## Next Checks
1. **Ablate Reward Design**: Replace the length-capping reward with a simple length penalty while keeping the cosine scheduler to isolate whether the binary correct/incorrect signal is essential

2. **Cross-Domain Generalization**: Apply SIRI to multi-step code generation (e.g., HumanEval) to test whether the same "wait" token suppression and reasoning densification occur in non-mathematical domains

3. **Scaling to Larger Models**: Train SIRI on a 34B or 70B parameter model using the same 8K/16K scheduler configuration to validate effectiveness at scale where token costs are highest