---
ver: rpa2
title: 'Dual-Force: Enhanced Offline Diversity Maximization under Imitation Constraints'
arxiv_id: '2501.04426'
source_url: https://arxiv.org/abs/2501.04426
tags:
- offline
- learning
- reward
- skills
- diversity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Dual-Force, an offline algorithm for diversity
  maximization under imitation constraints. The method enhances diversity using the
  Van der Waals force objective and successor features, eliminating the need for a
  skill discriminator.
---

# Dual-Force: Enhanced Offline Diversity Maximization under Imitation Constraints

## Quick Facts
- arXiv ID: 2501.04426
- Source URL: https://arxiv.org/abs/2501.04426
- Reference count: 28
- Primary result: Dual-Force recovers diverse quadruped behaviors offline, including walking with different base heights and angular velocities

## Executive Summary
Dual-Force introduces an offline algorithm for diversity maximization under imitation constraints using Van der Waals force objectives and successor features. The method eliminates the need for skill discriminators by providing closed-form diversity gradients, and conditions value functions and policies on pre-trained Functional Reward Encodings (FRE) to handle non-stationary rewards. Experiments on a quadruped robot demonstrate efficient recovery of diverse behaviors including obstacle navigation and walking with varying base heights and angular velocities.

## Method Summary
Dual-Force maximizes diversity under imitation constraints using a Van der Waals force objective with successor features, eliminating skill discriminators. The method employs DICE importance sampling for off-policy estimation and conditions value functions and policies on pre-trained FRE embeddings to handle non-stationary rewards. During training, the algorithm iteratively computes VdW rewards, encodes them via FRE, updates value functions using DICE, and adjusts policies via weighted behavioral cloning while maintaining KL constraints through Lagrange multipliers.

## Key Results
- Recovers diverse quadruped locomotion behaviors including walking with different base heights and angular velocities
- Successfully performs obstacle navigation reaching target positions in offline datasets
- Enables zero-shot recall of all skills encountered during training through FRE conditioning
- Eliminates need for skill discriminators while maintaining stable training

## Why This Works (Mechanism)

### Mechanism 1: Van der Waals Force Replaces Skill Discriminator
The VdW objective computes diversity via successor feature distances ψᵢ = E_{dᵢ(s)}[φ(s)]. When skills are close (ℓᵢ < ℓ₀), repulsion pushes them apart; when far (ℓᵢ > ℓ₀), attraction prevents excessive dispersion. The gradient βᵢ(s,a) = (1 - (ℓᵢ/ℓ₀)³)⟨φ(s), ψᵢ - ψⱼ*⟩ is analytically computable, removing the need to train a discriminator that would otherwise struggle with single-step offline updates and non-stationary rewards.

### Mechanism 2: DICE Importance Sampling Enables Off-Policy VdW Estimation
The dual formulation V* = argmin_V [(1-γ)E[V(s₀)] + log E_{dO}[exp{R + γTV - V}]] yields importance ratios ηᵢ(s,a) = softmax(δᵢ) from TD errors. These ratios enable offline expectation estimation: E_{dᵢ}[f(s,a)] ≈ E_{dO}[ηᵢ(s,a)f(s,a)], recovering successor features and policy gradients without environment interaction.

### Mechanism 3: Functional Reward Encoding Stabilizes Non-Stationary Training
FRE maps reward functions to latent embeddings zᵣ via a transformer-based VAE trained on diverse reward classes. During Dual-Force iteration, each non-stationary reward Rᵢ gets encoded to zᵢ, and V(·,zᵢ), π(·|·,zᵢ) condition on this embedding. Stored (Rᵢ, zᵢ) pairs enable zero-shot skill recall by conditioning on zᵢ at test time.

## Foundational Learning

- **Successor Features (Dayan, 1993; Barreto et al., 2016)**
  - Why needed: Core representation for VdW diversity measurement. Must understand ψ = E[φ(s)] as expected cumulative feature visitation.
  - Quick check: Given a feature map φ(s) = [base_height, angular_velocity], what does ψᵢ - ψⱼ represent behaviorally?

- **Fenchel Duality in RL (DICE Framework)**
  - Why needed: Foundation for offline estimation. Must grasp how conjugate duals yield importance ratios without environment samples.
  - Quick check: Why does the dual formulation require only a value function V(s) rather than explicit policy rollouts?

- **KL-Constrained Markov Decision Process**
  - Why needed: Dual-Force optimizes diversity subject to D_{KL}(dᵢ || d_E) ≤ ε. Must understand how Lagrange multipliers balance constraint satisfaction vs. objective maximization.
  - Quick check: If Lagrange multiplier σ(μᵢ) → 1, does the policy prioritize imitation or diversity?

## Architecture Onboarding

- **Component map:** State discriminator c*(s) → Van der Waals reward βᵢ(s,a) → FRE encoder F: (S×R)ᵐ → Z → Value function Vᵢ(·, zᵢ) → Policy πᵢ(·|·, zᵢ) → Importance weights wᵢ → Successor features ψᵢ

- **Critical path:**
  1. Pre-train c* distinguishing D_E from D_O states
  2. Pre-train FRE on random linear/MLP/human-engineered rewards
  3. Loop until convergence:
     - Compute ψᵢ from current wᵢ via Eq. 16
     - Compute VdW reward βᵢ(s,a) using closest-skill distances
     - Form total reward Rᵢ = (1-σ(μᵢ))βᵢ + σ(μᵢ)log(c*/(1-c*))
     - Encode Rᵢ → zᵢ via FRE
     - Update Vᵢ(·, zᵢ) minimizing Eq. 14
     - Compute new importance ratios wᵢ from TD errors
     - Update πᵢ via weighted behavior cloning
     - Adjust μᵢ based on constraint violation estimator φᵢ

- **Design tradeoffs:**
  - n (number of parallel occupancies): Higher n → more diverse skill set, but linear scaling in compute/memory. Paper uses n=3.
  - ℓ₀ (VdW threshold): Controls diversity pressure. Small ℓ₀ → strong repulsion, potentially unstable; large ℓ₀ → recovers pure max-min distance.
  - FRE pre-training distribution: Broader coverage increases robustness but may reduce embedding quality for specific VdW reward structures.

- **Failure signatures:**
  - Skill collapse: All ψᵢ converge to similar values → φ(s) lacks discriminative power or ℓ₀ misconfigured
  - Constraint violation: Policies diverge from expert → c* discriminator poor, or Lagrange multipliers not adapting
  - Value instability: V loss oscillates → FRE embeddings zᵢ inconsistent across iterations
  - Poor zero-shot recall: Stored zᵢ don't reproduce skills → FRE not generalizing to VdW rewards

- **First 3 experiments:**
  1. Sanity check: Run Dual-Force with n=1 on locomotion data. Verify policy recovers expert behavior and c* discriminator accuracy > 95%.
  2. Successor feature validation: Compute ψ for known behavioral modes in D_O. Check if φ(s) yields separable clusters via UMAP projection.
  3. Ablation: FRE vs. no-FRE: Compare training stability (V loss variance) and final skill diversity (pairwise ψ distance) when conditioning on zᵢ vs. directly on Rᵢ.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided text.

## Limitations
- Performance degrades when offline dataset lacks sufficient coverage of expert state distribution
- VdW reward complexity may exceed FRE pre-training distribution, limiting generalization
- Method relies on accurate state-discriminators and precise successor features, potentially brittle with sensor noise
- Zero-shot recall performance not quantitatively measured beyond visual inspection

## Confidence
- **High:** Dual-Force recovers diverse locomotion behaviors in quadruped experiments
- **Medium:** VdW objective eliminates discriminator instability (theoretical derivation solid, empirical comparison limited)
- **Medium:** FRE conditioning improves stability with non-stationary rewards (ablation shows benefit, pre-training distribution coverage unclear)
- **Low:** Zero-shot skill recall works robustly across all trained skills (only visually inspected; quantitative recall metrics not reported)

## Next Checks
1. Compute and visualize pairwise distances between learned successor features ψᵢ across skills; verify they form distinct clusters corresponding to different behavioral modes
2. Measure constraint violation (D_KL bounds) throughout training; confirm Lagrange multipliers μᵢ adapt to maintain imitation while enabling diversity
3. For zero-shot recall, measure normalized return achieved when conditioning policy on stored FRE embeddings zᵢ at test time, comparing against optimal performance