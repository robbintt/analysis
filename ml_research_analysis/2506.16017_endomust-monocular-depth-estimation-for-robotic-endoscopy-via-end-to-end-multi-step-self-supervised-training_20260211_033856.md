---
ver: rpa2
title: 'EndoMUST: Monocular Depth Estimation for Robotic Endoscopy via End-to-end
  Multi-step Self-supervised Training'
arxiv_id: '2506.16017'
source_url: https://arxiv.org/abs/2506.16017
tags:
- depth
- estimation
- image
- flow
- proposed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EndoMUST, a self-supervised monocular depth
  estimation framework for robotic endoscopy. It addresses illumination inconsistencies
  and sparse textures by integrating optical flow, appearance flow, and intrinsic
  image decomposition.
---

# EndoMUST: Monocular Depth Estimation for Robotic Endoscopy via End-to-end Multi-step Self-supervised Training

## Quick Facts
- arXiv ID: 2506.16017
- Source URL: https://arxiv.org/abs/2506.16017
- Reference count: 30
- This paper introduces EndoMUST, a self-supervised monocular depth estimation framework for robotic endoscopy. It addresses illumination inconsistencies and sparse textures by integrating optical flow, appearance flow, and intrinsic image decomposition. The core innovation is a three-step training strategy that isolates training of different networks to prevent interference. The method also employs multiscale self-supervised training for intrinsic image decomposition and full-module parameter-efficient finetuning via DV-LoRA. Evaluated on SCARED and Hamlyn datasets, EndoMUST achieves state-of-the-art performance, with 4-10% lower error than existing methods and superior zero-shot generalization.

## Executive Summary
EndoMUST is a self-supervised monocular depth estimation framework specifically designed for robotic endoscopy. The method addresses two key challenges in endoscopic imaging: illumination inconsistencies and sparse textures. It introduces a three-step end-to-end training strategy that sequentially isolates different supervision signals to prevent gradient interference. The framework integrates optical flow, appearance flow, and intrinsic image decomposition with multiscale supervision and employs DV-LoRA for parameter-efficient finetuning of the foundation depth model. Experimental results demonstrate state-of-the-art performance on SCARED and Hamlyn datasets, with 4-10% lower error than existing methods and superior zero-shot generalization capabilities.

## Method Summary
EndoMUST operates on sequential endoscopic frames, predicting depth maps, ego-motion, and intrinsic images through a three-step training strategy. In Step I, optical flow and appearance flow networks are trained to register frames while handling illumination changes. Step II focuses on intrinsic image decomposition using multiscale supervision to separate reflectance and shading components. Step III trains the depth network (Depth Anything with DV-LoRA) and pose networks using rigid transformation alignment and illumination-free alignment. The sequential isolation prevents interference between different supervision signals. The method employs a photometric reconstruction loss combined with SSIM, smoothness constraints, and rigid transformation alignment to supervise the networks without ground truth depth labels.

## Key Results
- Achieves state-of-the-art performance on SCARED dataset with RelAbs of 0.046 compared to 0.050-0.051 for existing methods
- Demonstrates superior zero-shot generalization on Hamlyn dataset with RelAbs of 0.117
- Outperforms baselines by 4-10% in depth estimation error metrics
- Successfully handles illumination inconsistencies and sparse textures in endoscopic scenes

## Why This Works (Mechanism)

### Mechanism 1
Sequential isolation of training steps reduces gradient interference between different supervision signals. Each epoch splits into three steps: (I) optical flow/appearance flow registration with smoothness losses, (II) multiscale intrinsic image decomposition with reconstruction loss, (III) depth/ego-motion training with rigid transformation alignment. At each step, only the relevant subnetworks receive gradient updates while others are frozen. This isolates L1 (flow registration), L2 (decomposition), and L3 (transformation alignment) into separate optimization phases rather than simultaneous joint training. The core assumption is that joint optimization creates conflicting gradient directions that degrade convergence.

### Mechanism 2
Multiscale supervision improves intrinsic image decomposition detail capture. Images are scaled to three resolutions (×1, ×0.75, ×0.5), decomposed into reflectance R and shading S at each scale, and reconstruction loss is computed across all scales. This enforces consistent decomposition across spatial frequencies. The core assumption is that single-scale decomposition loses fine-grained texture detail needed for accurate reflectance/shading separation. The method assumes the decomposition network has sufficient capacity at all scales.

### Mechanism 3
DV-LoRA parameter-efficient finetuning enables foundation model adaptation with minimal trainable parameters while preserving transferability. For each Transformer block, DV-LoRA adds low-rank adaptation to the feed-forward network and QKV projection layers. Only ~1.8M parameters are trained (vs 27M in some baselines), reducing overfitting risk on limited endoscopic data. The core assumption is that the Depth Anything foundation model has learned transferable monocular depth representations, and low-rank adaptation is sufficient to shift the domain without catastrophic forgetting.

## Foundational Learning

- **Concept: Photometric reconstruction loss (Eq. 1)**
  - Why needed here: Self-supervised depth relies on warping source images to target view and measuring appearance difference; understanding SSIM + L1 combination is essential for debugging alignment failures
  - Quick check question: Can you explain why SSIM is preferred over pure L1 for measuring perceptual similarity?

- **Concept: Intrinsic image decomposition (albedo × shading)**
  - Why needed here: Separating reflectance (material color) from shading (illumination) allows illumination-invariant alignment, critical for endoscopy's non-Lambertian reflections
  - Quick check question: Given an endoscopic image with specular highlights, which component (R or S) should contain them?

- **Concept: Inverse warping with depth + ego-motion**
  - Why needed here: Rigid transformation projects source pixels to target view using predicted depth, pose P, and intrinsics K; errors here propagate to all downstream losses
  - Quick check question: If ego-motion rotation is incorrect by 2°, which regions of the warped image will show largest misalignment?

## Architecture Onboarding

- **Component map:**
  Depth Network (Depth Anything + DV-LoRA) <- Pose Network <- Intrinsic Decomposition Network <- Appearance Flow Network <- Optical Flow Network

- **Critical path:**
  1. Implement spatial transformer for inverse warping (flow-based and depth-based)
  2. Implement Eq. 1 photometric loss with visibility mask M from backward flow
  3. Build three-step training loop with proper gradient freezing/unfreezing
  4. Integrate DV-LoRA modules into Depth Anything Transformer blocks

- **Design tradeoffs:**
  - Sequential training (I→II→III) vs parallel: sequential may slow convergence but reduces interference (Table V supports sequential)
  - DV-LoRA rank=4: lower rank reduces parameters but may limit adaptation capacity (Table VI shows warm-up sensitivity)
  - Multiscale IID adds memory overhead for 3× decomposition branches

- **Failure signatures:**
  - Blurry depth maps at tissue boundaries → check smoothness loss weighting (0.001 Lsm)
  - Specular highlights causing depth errors → verify IID is separating shading correctly
  - Pose drift in long sequences → check ego-motion supervision from L3 transformation alignment
  - GPU OOM → note Table V shows "II→{I,III}" causes OOM, indicating memory constraints when fusing steps

- **First 3 experiments:**
  1. Reproduce ablation I→II→III vs I→{II,III} (Table V) on SCARED subset to validate isolation benefit before full training
  2. Visualize intrinsic decomposition outputs (R, S) on frames with strong specular highlights to verify shading capture
  3. Run zero-shot inference on Hamlyn with pretrained SCARED weights to establish baseline before any modifications

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the framework's generalization capability be further improved to handle the feature diversity found in a wider variety of endoscopic scenes?
  - Basis in paper: The conclusion states, "Feature diversity in various endoscopic scenes is still a challenge for generalization of the model."
  - Why unresolved: While the model demonstrates strong zero-shot performance on the Hamlyn dataset, the authors acknowledge that significant variations in anatomical structures or surgical environments may still degrade performance.
  - What evidence would resolve it: Evaluation results on diverse organ datasets (e.g., gastrointestinal, sinus, or transoral) or pathology-specific datasets showing comparable error rates to the SCARED abdominal data without retraining.

- **Open Question 2:** Does the reliance on optical-flow-based masking ($M$) suffice for handling dynamic occlusions caused by surgical instruments, or is explicit segmentation required?
  - Basis in paper: The introduction identifies "occlusion of the end effector" as a potential challenge, but the method relies on a visibility mask $M$ derived from backward optical flow consistency rather than explicit instrument detection.
  - Why unresolved: Optical flow can struggle to distinguish between independent tool motion and background tissue deformation, potentially violating the static scene assumption required for the rigid transformation alignment in Step III.
  - What evidence would resolve it: A comparative ablation study on video sequences with significant tool manipulation, measuring depth error specifically in regions occluded by instruments using flow-based masks versus semantic instrument masks.

- **Open Question 3:** To what extent does the rigid transformation assumption in the depth supervision limit accuracy in scenes with significant non-rigid tissue deformation?
  - Basis in paper: The method supervises depth using ego-motion $P$ and rigid projection, which assumes the scene is static or the camera is moving relative to a static object, whereas endoscopic scenes often contain soft, deforming tissues.
  - Why unresolved: The use of appearance flow and optical flow helps align appearance, but the core depth loss relies on rigid geometry; large tissue deformations could force the network to predict erroneous depth to minimize photometric loss.
  - What evidence would resolve it: Quantitative analysis of depth accuracy on ex-vivo tissue datasets with controlled non-rigid deformations compared to methods specifically designed for non-rigid structure from motion.

## Limitations

- The framework relies on cited sub-modules (optical flow, appearance flow, IID) without detailing their specific architectures, creating potential reproducibility gaps
- The sequential training strategy's benefits are demonstrated empirically but lack theoretical justification for why gradient interference is the dominant failure mode
- Zero-shot generalization on Hamlyn is impressive but only tested on one external dataset, limiting generalization claims

## Confidence

- **High Confidence:** Depth performance metrics on SCARED (RelAbs 0.046, δ<1.25 0.955) - these are direct quantitative comparisons with clear baselines
- **Medium Confidence:** Zero-shot Hamlyn performance (RelAbs 0.117) - while reported, single-dataset testing limits robustness claims
- **Medium Confidence:** Sequential training superiority - ablation supports it, but the mechanism (gradient interference vs other factors) remains speculative

## Next Checks

1. **Ablation Replication:** Run I→II→III vs I→{II,III} on SCARED subset (500 frames) to verify the 3-4% RelAbs improvement from sequential isolation before full training
2. **Cross-dataset Testing:** Evaluate EndoMUST on at least one additional endoscopic dataset (e.g., MICCAI Endoscopic Challenge) to validate zero-shot generalization claims beyond Hamlyn
3. **DV-LoRA Sensitivity:** Test rank=2, rank=4, and rank=8 configurations to establish the tradeoff between parameter efficiency and performance, particularly for the Hamlyn zero-shot case where overfitting risk is highest