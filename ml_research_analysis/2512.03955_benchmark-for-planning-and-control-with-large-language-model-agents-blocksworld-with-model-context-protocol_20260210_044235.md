---
ver: rpa2
title: 'Benchmark for Planning and Control with Large Language Model Agents: Blocksworld
  with Model Context Protocol'
arxiv_id: '2512.03955'
source_url: https://arxiv.org/abs/2512.03955
tags:
- planning
- agent
- agents
- benchmark
- block
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a benchmark for evaluating LLM-based agents
  in planning and control tasks using the Blocksworld domain. The benchmark provides
  a simulation environment with five complexity categories and uses the Model Context
  Protocol (MCP) for standardized agent-environment interaction.
---

# Benchmark for Planning and Control with Large Language Model Agents: Blocksworld with Model Context Protocol

## Quick Facts
- arXiv ID: 2512.03955
- Source URL: https://arxiv.org/abs/2512.03955
- Authors: Niklas Jobs; Luis Miguel Vieira da Silva; Jayanth Somashekaraiah; Maximilian Weigand; David Kube; Felix Gehlhoff
- Reference count: 3
- Primary result: LLM-based agents achieve 60-80% success on solvable Blocksworld tasks, with 100% accuracy identifying impossible scenarios

## Executive Summary
This paper introduces a benchmark for evaluating LLM-based agents in planning and control tasks using the Blocksworld domain. The benchmark provides a simulation environment with five complexity categories and uses the Model Context Protocol (MCP) for standardized agent-environment interaction. A single-agent implementation was evaluated across 50 scenarios, achieving 60-80% success rates on solvable tasks and 100% accuracy in identifying impossible scenarios. Performance degraded with increasing complexity, especially under partial observability and additional constraints. The benchmark enables systematic comparison of LLM agents with classical planning methods and supports future research into hybrid approaches, dynamic events, and multi-robot coordination.

## Method Summary
The benchmark consists of 50 predefined JSON scenarios across 5 categories (10 each): Basic, Non-constructive actions, Impossible, Additional constraints (block sizes), and Partial observability. A ReAct agent in LangGraph uses OpenAI o3 model (200K context window) with a 7-step workflow: get rules → get status → plan → verify → correct → execute → confirm. The simulation provides REST API endpoints wrapped by an MCP server, exposing tools like `verify_plan`, `get_status`, and primitive actions (`pick_up`, `stack`). Performance is measured by success rate, execution time, planning attempts, token consumption, and cost per scenario.

## Key Results
- Single-agent ReAct implementation achieved 60-80% success on solvable tasks
- 100% accuracy in identifying impossible scenarios with 1.8 attempts on average
- Performance degraded significantly under partial observability (Category 5) and additional constraints
- Token consumption reached up to 192,000 tokens for complex scenarios
- Cost per scenario ranged from $0.004 to $0.016 depending on complexity

## Why This Works (Mechanism)

### Mechanism 1: Decoupled Interface Standardization
The Model Context Protocol (MCP) allows agent logic to be separated from simulation implementation, enabling modular evaluation. The architecture wraps a domain-specific REST API (Blocksworld simulation) inside a generic MCP server. The LLM agent interacts solely with standard MCP tools (discovery, invocation) rather than hardcoded API endpoints, abstracting the underlying state management.

### Mechanism 2: Verification-Guided Replanning
Decoupling plan verification from execution allows the agent to correct reasoning errors before irreversibly modifying the environment state. The system employs a `verify_plan` tool that simulates an action sequence against constraints without executing it. If verification fails, the agent receives natural language error feedback, triggering an internal correction loop (ReAct) before physical execution.

### Mechanism 3: Complexity Scaling via Observability Constraints
Performance degradation is driven primarily by the need for internal world-model maintenance under partial observability, distinct from simple planning depth. The benchmark introduces "Partial Observability" (Category 5), where only top blocks are visible. The agent must execute information-gathering actions or maintain a probabilistic internal state, straining the context window and reasoning coherence.

## Foundational Learning

- **ReAct Pattern (Reasoning + Acting)**: The agent uses LangGraph to interleave "Thought" steps with "Tool" calls. Understanding this loop is critical to debugging why the agent might verify a plan but fail to execute it. Quick check: Can you trace a single loop where the agent generates a thought, calls a tool, and processes the observation?

- **Symbolic Planning (PDDL/Blocksworld)**: The benchmark relies on classical preconditions (e.g., "hand must be empty to pick up"). Without grasping these symbolic rules, one cannot distinguish between an LLM reasoning error and a valid plan blocked by a constraint. Quick check: What are the preconditions for the `stack(block_x, block_y)` action in this domain?

- **Context Window Management**: Results show token usage up to 192,000 in complex scenarios. Efficient prompting or memory management is required to prevent context overflow. Quick check: How does the system handle the state history when the token count approaches the model's limit (e.g., 200k for o3)?

## Architecture Onboarding

- **Component map**: Simulation Core (Python/Pygame) -> REST API (Flask) -> MCP Server -> MCP Host (LangGraph) -> LLM Agent (o3) with Memory

- **Critical path**:
  1. Initialization: Agent calls `get_rules` and `get_status` to ground itself
  2. Verification: Agent proposes a plan → `verify_plan` tool returns success/failure
  3. Execution: If verified, Agent calls primitive tools (`pick_up`, `stack`)

- **Design tradeoffs**:
  - Single Agent vs. Multi-Agent: The paper uses a single ReAct agent for simplicity, but notes that multi-agent systems might handle "reasoning vs. execution" better. Tradeoff is complexity vs. robustness
  - Internal vs. External Verification: The agent relies on an external tool (`verify_plan`). An alternative is to rely on the LLM's internal "thought chain" for verification, which is faster but less reliable

- **Failure signatures**:
  - Hallucinated Objects: Agent referencing block names that do not exist (common in Category 5)
  - Premature Termination: Agent claiming the goal is met when the simulation state says otherwise (false positive in `get_status` interpretation)
  - Looping: Repeatedly attempting the same invalid action despite error feedback

- **First 3 experiments**:
  1. Baseline Validation: Run the provided single-agent on Category 1 (Basic) scenarios to reproduce the ~80% success rate and calibrate token costs
  2. Constraint Stress Test: Run the agent on Category 4 (Additional Constraints) specifically to observe how it handles "unstacking" errors when block size rules are active
  3. Observability Limit: Test Category 5 (Partial Observability) with a context window limit reduced by 50% to quantify the correlation between memory capacity and success rate degradation

## Open Questions the Paper Calls Out

- **Agent Architectural Patterns**: Which agent architectural patterns (single-agent, hierarchical multi-agent, hybrid LLM-symbolic) best address reliability, multi-step reasoning, and error recovery in planning tasks? Only a single-agent ReAct architecture was evaluated; no multi-agent or hybrid approaches were tested.

- **Dynamic Events Performance**: How do LLM agents perform when dynamic events (runtime errors, changing goals, physical disturbances) occur during plan execution? Current benchmark uses static scenarios with fixed initial and goal states; no mid-execution changes occur.

- **LLM vs Classical Planners**: How do LLM-based agents compare to classical symbolic planners on this benchmark under identical conditions? Despite the REST API enabling classical planner integration, only LLM agent results are presented.

## Limitations
- Evaluation relies on a single LLM agent (OpenAI o3) without comparing alternative models or prompting strategies
- Benchmark's generalizability to other domains remains unproven despite MCP's domain-agnostic claims
- Performance degradation under partial observability suggests fundamental limitations that may not be overcome by architectural refinements alone

## Confidence

- **High Confidence**: The basic mechanism of MCP-based decoupling and verification-guided planning is sound and well-supported by the results
- **Medium Confidence**: The explanation for partial observability challenges is plausible but not definitively proven
- **Low Confidence**: Claims about cost-effectiveness compared to classical planners are speculative, as no direct comparison is provided

## Next Checks
1. Test the benchmark with alternative LLMs (e.g., Claude, Gemini) and different prompting strategies to isolate whether performance limitations are model-specific or architectural
2. Apply the MCP framework to a different planning domain (e.g., navigation or logistics) to validate the protocol's domain-agnostic claims
3. Implement a hybrid agent that combines LLM reasoning with classical planner subroutines for verification, measuring whether this improves performance on complex scenarios