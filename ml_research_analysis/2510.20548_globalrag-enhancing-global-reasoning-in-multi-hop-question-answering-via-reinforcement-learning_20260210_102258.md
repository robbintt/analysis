---
ver: rpa2
title: 'GlobalRAG: Enhancing Global Reasoning in Multi-hop Question Answering via
  Reinforcement Learning'
arxiv_id: '2510.20548'
source_url: https://arxiv.org/abs/2510.20548
tags:
- reasoning
- think
- globalrag
- training
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GlobalRAG is a reinforcement learning framework for multi-hop question
  answering that addresses the limitations of existing approaches by introducing explicit
  global planning and faithful execution. The method decomposes questions into structured
  subgoals, coordinates retrieval with reasoning, and refines evidence iteratively,
  guided by Planning Quality and SubGoal Completion rewards.
---

# GlobalRAG: Enhancing Global Reasoning in Multi-hop Question Answering via Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.20548
- Source URL: https://arxiv.org/abs/2510.20548
- Reference count: 30
- GlobalRAG achieves 14.2% average improvement in EM and F1 on multi-hop QA benchmarks using only 8k training examples

## Executive Summary
GlobalRAG introduces a reinforcement learning framework for multi-hop question answering that addresses critical limitations in existing retrieval-augmented generation approaches. The method explicitly decomposes questions into structured subgoals, coordinates retrieval with reasoning through faithful execution, and iteratively refines evidence using Planning Quality and SubGoal Completion rewards. Extensive experiments on five multi-hop QA benchmarks demonstrate that GlobalRAG significantly outperforms strong baselines, achieving average improvements of 14.2% in both EM and F1, while using only 42% of the training data used by baseline approaches.

## Method Summary
GlobalRAG is a reinforcement learning framework that enhances global reasoning in multi-hop question answering through explicit global planning and faithful execution. The method decomposes complex questions into structured subgoals, coordinates retrieval operations with reasoning steps, and iteratively refines evidence based on planning quality and subgoal completion rewards. Using GRPO training with Qwen2.5-3B-Instruct, the framework processes 8,394 training trajectories from multiple datasets, achieving 14.2% average improvements in EM and F1 while using only 8k training examples compared to baseline approaches that use significantly more data.

## Key Results
- Achieves 14.2% average improvement in EM and F1 across five multi-hop QA benchmarks
- Uses only 8,394 training trajectories (42% of data used by baseline approaches)
- Demonstrates significant performance gains on HotpotQA, 2WikiMultiHopQA, and MuSiQue datasets

## Why This Works (Mechanism)
The framework works by addressing two fundamental limitations in existing multi-hop QA approaches: the absence of global planning and unfaithful execution. Global planning decomposes questions into structured subgoals that guide the entire reasoning process, while faithful execution ensures that retrieval queries align with planned subgoals. The iterative refinement process, guided by Planning Quality and SubGoal Completion rewards, allows the model to progressively improve its reasoning quality by adjusting both the plan structure and the execution fidelity.

## Foundational Learning
- **Multi-hop QA**: Reasoning across multiple documents to answer complex questions - needed to understand the complexity of the task and why single-hop approaches fail
- **Reinforcement Learning for NLP**: Using reward-based training for text generation tasks - needed to optimize the planning and execution coordination without explicit supervision
- **Retrieval-Augmented Generation**: Integrating external knowledge retrieval with language model generation - needed to provide evidence for reasoning steps
- **Graph Edit Distance**: Measuring similarity between structured plans - needed for computing Planning Quality rewards that compare generated plans against golden trajectories
- **FAISS-GPU Indexing**: Efficient similarity search over large document collections - needed for scalable retrieval operations during both training and inference

## Architecture Onboarding

Component Map: Question -> Global Planning -> Faithful Execution -> Evidence Refinement -> Answer Generation

Critical Path: The core execution path involves decomposing the question into subgoals, generating retrieval queries aligned with each subgoal, performing evidence retrieval, and iteratively refining the reasoning process based on reward signals. The GRPO training loop optimizes this entire pipeline end-to-end.

Design Tradeoffs: The framework trades increased computational complexity (iterative refinement, multiple retrieval operations) for improved reasoning quality. The use of only 8k training examples versus baseline approaches using significantly more data represents a data efficiency tradeoff that comes at the cost of more sophisticated reward engineering.

Failure Signatures: Common failure modes include incomplete or cyclic plans (indicating global planning absence), retrieval queries that drift from planned subgoals (unfaithful execution), and training instability when reward weights are too high. These can be diagnosed by validating DAG structure, comparing search queries against plans, and monitoring validation curves.

First Experiments:
1. Implement the Planning Quality reward computation independently and test with synthetic examples to verify structural and semantic consistency metrics
2. Train the Qwen2.5-3B-Instruct model on HotpotQA using the specified GRPO configuration and compare EM/F1 scores after each epoch
3. Conduct ablation studies removing each major component (global planning, faithful execution coordination, progressive weight annealing) to quantify individual contributions

## Open Questions the Paper Calls Out
None

## Limitations
- The exact prompt templates for golden trajectory generation are not fully specified, referencing external PER-PSE work
- The framework relies heavily on GPT-4o for trajectory generation, creating deployment dependencies
- No sensitivity analysis provided for critical hyperparameters like reward weights and temperature settings

## Confidence
- High: Experimental results showing 14.2% average improvement in EM and F1 on five benchmarks
- Medium: Architectural design choices for global planning and faithful execution coordination
- Low: Scalability and generalization beyond tested benchmarks with limited training data

## Next Checks
1. Implement the Planning Quality reward computation independently and verify graph edit distance and maximum common subgraph algorithms against specifications
2. Train the Qwen2.5-3B-Instruct model on HotpotQA using the specified GRPO configuration and compare performance metrics
3. Conduct controlled ablation experiments to quantify individual contributions of global planning, faithful execution coordination, and progressive weight annealing