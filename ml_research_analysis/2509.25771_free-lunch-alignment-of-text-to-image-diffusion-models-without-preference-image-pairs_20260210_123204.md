---
ver: rpa2
title: Free Lunch Alignment of Text-to-Image Diffusion Models without Preference Image
  Pairs
arxiv_id: '2509.25771'
source_url: https://arxiv.org/abs/2509.25771
tags:
- preference
- prompt
- image
- human
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Text Preference Optimization (TPO), a novel
  method for aligning text-to-image diffusion models without requiring human preference
  image pairs. The key idea is to leverage large language models to automatically
  generate mismatched text prompts that describe different images, enabling preference
  optimization over text pairs instead of image pairs.
---

# Free Lunch Alignment of Text-to-Image Diffusion Models without Preference Image Pairs

## Quick Facts
- arXiv ID: 2509.25771
- Source URL: https://arxiv.org/abs/2509.25771
- Authors: Jia Jun Cheng Xian; Muchen Li; Haotian Yang; Xin Tao; Pengfei Wan; Leonid Sigal; Renjie Liao
- Reference count: 31
- Primary result: Achieves state-of-the-art alignment on T2I models without requiring human preference image pairs

## Executive Summary
This paper introduces Text Preference Optimization (TPO), a novel method for aligning text-to-image diffusion models without requiring human preference image pairs. The key insight is to leverage large language models to automatically generate mismatched text prompts that describe different images, enabling preference optimization over text pairs instead of image pairs. This approach achieves state-of-the-art alignment performance across multiple benchmarks, outperforming existing diffusion-based preference optimization methods while eliminating the need for costly human preference annotations.

## Method Summary
TPO works by training the model to prefer matched prompts over mismatched prompts, which are constructed by perturbing original captions using a large language model. The framework reformulates direct preference optimization (DPO) and Kullback-Leibler optimization (KTO) to operate on input text conditions rather than output images, deriving TDPO and TKTO loss functions. The method uses a two-stage approach: first fine-tuning the diffusion model on high-quality paired data, then applying TPO with LLM-generated negative prompts to improve alignment between text and generated images.

## Key Results
- Achieves state-of-the-art alignment performance across multiple benchmarks without human preference image pairs
- Outperforms existing diffusion-based preference optimization methods on alignment metrics
- Demonstrates strong correlation between implicit preference scores and human preference metrics
- Shows that removing spatial modifications improves TKTO performance, suggesting spatial prompts inject high-variance supervision

## Why This Works (Mechanism)

### Mechanism 1: LLM-Generated Text Preference Pairs
Creating mismatched text prompts is significantly easier and more scalable than collecting human preference image pairs. LLMs generate negative prompts by perturbing original captions using four modification principles—content, attribute, spatial, and contextual changes—while original captions serve as matched prompts. This works because the original dataset has high-quality captions that accurately describe images.

### Mechanism 2: Preference Optimization over Input Conditions
Optimizing over text pairs rather than image pairs maintains alignment effectiveness while eliminating annotation costs. The method reformulates DPO/KTO to operate on input text conditions, deriving TDPO and TKTO where loss compares likelihood ratios between matched and mismatched prompts. This works under the assumption that text condition is sampled from dataset and independent of model parameters.

### Mechanism 3: Implicit Text Preference Correlates with Human Preference
Models that better distinguish matched vs mismatched prompts receive higher human preference scores. The implicit preference score—the difference in diffusion loss between positive and negative pairs—shows positive correlation with human preference metrics in regression analysis. This works because a meaningful portion of human preference judgment is directly attributable to text-image alignment quality.

## Foundational Learning

- **Concept: Diffusion Models and Denoising Score Matching**
  - Why needed here: Understanding how diffusion models learn p_θ(x|c) via noise prediction ε_θ(x_t, t, c) is essential for interpreting the TDPO/TKTO loss formulations
  - Quick check question: Why does the diffusion loss ||ε - ε_θ(x_t, t, c)||² relate to the likelihood p_θ(x|c)?

- **Concept: Bradley-Terry Preference Model**
  - Why needed here: TDPO builds on the BT model to formulate preferences as σ(r(c_w, x) - r(c_l, x)), which is then reparameterized using model likelihoods
  - Quick check question: How does the Bradley-Terry model convert pairwise preferences into a probabilistic formulation?

- **Concept: Direct Preference Optimization (DPO)**
  - Why needed here: TPO extends DPO's key insight of implicit reward representation to the T2I setting with input condition optimization
  - Quick check question: How does DPO avoid explicit reward modeling while still optimizing for preferences?

## Architecture Onboarding

- **Component map**: LLM Prompt Editor (Gemini 2.0 Flash) → SFT Stage → TDPO/TKTO Stage → Reference Model (frozen copy)
- **Critical path**: Prepare HPSD image-caption pairs → Generate c_l using LLM with random strategy selection from four principles → Run SFT until convergence (~17,500 steps, batch_size=256, lr=1e-5) → Apply TDPO/TKTO with clipping (β=5000, batch_size=16, lr=1e-6) → Select checkpoint via 500-prompt validation sweep
- **Design tradeoffs**: More modification principles → richer negatives but higher LLM cost and potential noise; Higher clipping threshold → more stable training but weaker preference signal; Only U-Net fine-tuning → efficient but limits text encoder discrimination power
- **Failure signatures**: Sample quality degrades after several hundred steps → increase clipping strength; Model fails to distinguish c_w from c_l → check LLM prompt quality; Training diverges → reduce β or increase batch size
- **First 3 experiments**: 1) Replicate SFT stage on HPSD subset (10K pairs) to verify baseline convergence behavior; 2) Ablate each modification principle separately to identify highest-impact categories; 3) Compare TDPO vs TKTO on held-out prompts, correlating implicit preference scores with human preference metrics

## Open Questions the Paper Calls Out

- **Question 1**: How does the modification budget (number of prompt edits) systematically impact alignment performance and stability?
  - Basis: The authors observe non-monotonic results where k=1,2,3 do not follow a clear trend, suggesting an interaction between edit complexity and model capacity
  - Evidence needed: Comprehensive ablation study correlating specific edit budgets with convergence speed and final preference scores across different model scales

- **Question 2**: Why does removing "Spatial and Dynamic" modifications improve performance for TKTO, and does this indicate inherent ambiguity in spatial supervision?
  - Basis: Section 4.3 and Table 3 show that removing spatial modifications improves TKTO results; authors hypothesize spatial prompts inject high-variance, low-signal supervision
  - Evidence needed: Qualitative and quantitative analysis of attention maps generated from spatial mismatch prompts compared to other modification types

- **Question 3**: Can TPO effectiveness be improved by fine-tuning the text encoder alongside the U-Net?
  - Basis: Section D states that keeping the pre-trained text encoder fixed may limit the framework's capacity to discriminate between closely related prompts
  - Evidence needed: Comparison experiment where the text encoder is unfrozen during TPO training, measuring changes in cosine similarity distance between positive and negative prompt embeddings

## Limitations

- The independence assumption (p_θ(c) = p_ref(c)) underlying the TDPO/TKTO reformulation may break down as text and image distributions become coupled during fine-tuning
- The correlation between implicit preference scores and human preferences relies on a specific interpretation of what drives human judgment in T2I generation
- The method's effectiveness may vary depending on the quality and characteristics of the LLM used for prompt generation

## Confidence

- **High Confidence**: The practical effectiveness of TPO demonstrated through benchmark comparisons and state-of-the-art performance
- **Medium Confidence**: The theoretical foundation of reformulating DPO/KTO for text conditions, particularly the independence assumption
- **Medium Confidence**: The scalability and cost-effectiveness claims, as computational overhead and quality variance across different LLM implementations require further characterization

## Next Checks

1. **Independence Assumption Stress Test**: Systematically evaluate how p_θ(c) diverges from p_ref(c) during TPO training across different dataset sizes and fine-tuning durations. Measure the correlation between this divergence and alignment performance degradation.

2. **Ablation of Correlation Assumptions**: Design controlled human studies to quantify the relative contribution of prompt adherence versus aesthetic quality to overall preference scores. Test whether models optimized solely for text-image alignment maintain human preference advantages when aesthetic factors dominate.

3. **Cross-LLM Robustness Evaluation**: Repeat the TPO pipeline using different LLM providers and model sizes for prompt generation. Characterize the variance in alignment performance and identify failure modes specific to certain prompt editing strategies or LLM capabilities.