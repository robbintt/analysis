---
ver: rpa2
title: 'FloorplanQA: A Benchmark for Spatial Reasoning in LLMs using Structured Representations'
arxiv_id: '2507.07644'
source_url: https://arxiv.org/abs/2507.07644
tags:
- reasoning
- hssd
- layouts
- room
- spatial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FloorplanQA introduces a benchmark to assess spatial reasoning
  in large language models using structured 2D floorplan layouts. The dataset includes
  2,000 layouts (1,800 synthetic, 200 from real scenes) and 16,000 questions covering
  geometric computations, placement feasibility, visibility, and path planning.
---

# FloorplanQA: A Benchmark for Spatial Reasoning in LLMs using Structured Representations

## Quick Facts
- arXiv ID: 2507.07644
- Source URL: https://arxiv.org/abs/2507.07644
- Reference count: 40
- Primary result: Models reliably compute distances and angles but struggle with complex spatial reasoning, especially when handling overlapping objects or multi-step planning.

## Executive Summary
FloorplanQA introduces a benchmark to assess spatial reasoning in large language models using structured 2D floorplan layouts. The dataset includes 2,000 layouts (1,800 synthetic, 200 from real scenes) and 16,000 questions covering geometric computations, placement feasibility, visibility, and path planning. Models were evaluated on both symbolic and visual input modes, with accuracy measured per task and layout type. Results show models reliably compute distances and angles but struggle with complex spatial reasoning, especially when handling overlapping objects or multi-step planning. Reasoning-focused models outperform general models on geometric unions but still fail on pathfinding and free-space estimation. Tool augmentation (Python interpreter) improves metric calculations but not planning. Symbolic layouts remain as effective as visual renderings for these tasks.

## Method Summary
The benchmark uses 2,000 2D floorplan layouts encoded in JSON/XML, with 16,000 questions covering eight task types. Synthetic layouts (1,800) were generated via Gemini 2.5 Pro; 200 real layouts came from HSSD. Questions span metric computation, topology, and dynamic reasoning, evaluated zero-shot at temperature=0. Ground truth is computed using Shapely for geometric operations. Accuracy is measured with strict tolerances (2% for scalars, exact for booleans, Fréchet distance for paths), and truncation is tracked via API stop reasons.

## Key Results
- Models reliably compute distances and angles but struggle with complex spatial reasoning.
- Reasoning-focused models outperform general models on geometric unions but still fail on pathfinding and free-space estimation.
- Tool augmentation improves metric calculations but not planning; symbolic layouts remain as effective as visual renderings.

## Why This Works (Mechanism)

### Mechanism 1: Task Complexity Stratification
- **Claim**: Performance degrades predictably from single-step metric computation to multi-step constraint optimization.
- **Mechanism**: Three tiers emerge from the task design—Metric tasks (distance, angles) require one geometric operation; Topology tasks (placement, visibility) require constraint checking; Dynamic tasks (pathfinding, free-space) require search over continuous spaces. Models with chain-of-thought capabilities allocate more inference compute to harder tiers, yielding relative gains.
- **Core assumption**: Task difficulty is driven primarily by reasoning depth (number of dependent operations), not input complexity.
- **Evidence anchors**:
  - [Section 4.1]: "Three distinct tiers emerge... Metric tasks achieve 75–95% averaged accuracy... Optimization and planning tasks achieve only 5–45% averaged accuracy."
  - [Section 4.1]: "Reasoning-focused models show notable improvements on geometric union tasks, gaining +10–40% on Free Space and Max Box."
  - [Corpus]: Limited direct corpus support; neighbor papers focus on scene generation rather than diagnostic evaluation.
- **Break condition**: If models were simply failing on arithmetic rather than reasoning, tool augmentation would uniformly improve all tasks—but it doesn't (Section 4.2).

### Mechanism 2: Symbolic Representation Sufficiency
- **Claim**: Structured text encodings (JSON/XML) provide sufficient geometric grounding for spatial tasks, matching visual input utility.
- **Mechanism**: Coordinate-based representations expose explicit metric relations (distances, angles) without requiring perception-to-symbol translation. Models trained on code and structured data can parse bounding boxes and polygons as native tokens, bypassing vision-language alignment overhead.
- **Core assumption**: Models have internalized geometric primitives (centroids, intersections) from training on code/math corpora.
- **Evidence anchors**:
  - [Section 4.2]: "Accuracy remained stable across formats (±3 percentage points)... suggesting models encode layout semantics independently of serialization syntax."
  - [Appendix F, Table 8]: VLM inputs provide "selective gains" but "do not consistently outperform symbolic input."
  - [Corpus]: Neighbor paper "Direct Numerical Layout Generation" similarly uses numerical coordinates for spatial reasoning.
- **Break condition**: If visual grounding were essential, VLM+image conditions would dominate—but they show inconsistent gains.

### Mechanism 3: Tool-Augmentation Asymmetry
- **Claim**: External computation resolves arithmetic errors but not algorithmic reasoning failures.
- **Mechanism**: Python interpreters execute formulas correctly (centroid, distance) but cannot fix incorrect spatial algorithms (collision detection, path search). Models must still generate correct solution strategies; tools only improve execution fidelity.
- **Core assumption**: The bottleneck for complex spatial tasks is choosing the right algorithm, not computing it accurately.
- **Evidence anchors**:
  - [Section 4.2, Table 2]: Tools improve Pair Distance (+43 points) but decrease Max Box (−4.5) and Shortest Path (−12.5).
  - [Section 5]: "Tool augmentation yields strong gains on arithmetic-heavy tasks but limited benefit on planning tasks, indicating failures stem from spatial reasoning rather than calculation errors."
  - [Appendix F, Figure 8]: Failure cases show models generate "incorrect algorithms—treating boundary contact as collision, or performing incomplete search."
- **Break condition**: If planning failures were due to numerical precision, tools would help—they don't.

## Foundational Learning

- **Concept: Polygon Centroid via Shoelace Formula**
  - **Why needed here**: Most metric tasks (distance, angle) require centroid computation first. Errors on HSSD layouts (complex polygons) cluster around this step.
  - **Quick check question**: Given vertices (0,0), (2,0), (2,1), (0,1), compute the centroid. (Answer: (1, 0.5))

- **Concept: Geometric Union vs. Summation**
  - **Why needed here**: Free-space calculation fails when models sum object areas instead of computing union. Overlapping objects (e.g., TV on TV stand) cause double-counting errors.
  - **Quick check question**: Two unit squares overlap by 0.25 units. What's the union area? (Answer: 1.75, not 2.0)

- **Concept: Clearance Buffering and Configuration Space**
  - **Why needed here**: Path planning requires shrinking free space by clearance radius. Models often ignore this step, producing paths that clip obstacle corners.
  - **Quick check question**: A 0.5m-wide agent navigates among obstacles. How much must obstacles be expanded before running A*? (Answer: 0.25m radius)

## Architecture Onboarding

- **Component map**: Layout encoder -> Question renderer -> Answer extractor -> Scorer
- **Critical path**: Layout → question template → model response → answer extraction → tolerance comparison. Truncation (especially for reasoning models) occurs at response stage; track via `stop_reason`.
- **Design tradeoffs**:
  - Axis-aligned bounding boxes (synthetic) vs. arbitrary polygons (HSSD): synthetic isolates reasoning from geometry complexity
  - Zero-shot prompting vs. few-shot: paper uses zero-shot to measure unaided capability
  - Token budgets: 12,288 (large), 8,192 (mid-size), 4,096 (GPT-5). Insufficient for HSSD layouts with many objects (50%+ truncation on some models)
- **Failure signatures**:
  - **Free Space**: Model returns sum of object areas instead of union (systematic underestimation)
  - **Shortest Path**: Path violates clearance or passes through obstacles (collision-checking skipped)
  - **Max Box**: Returns axis-aligned rectangle when rotated rectangle is larger (optimization incomplete)
  - **HSSD Metric Tasks**: Centroid errors cascade to distance/angle failures
- **First 3 experiments**:
  1. **Baseline diagnostic**: Run 3 models (1 general, 1 reasoning, 1 small) on 50 kitchens only. Confirm metric > constraint > planning tier ordering. Time: ~2 hours.
  2. **Union error isolation**: On 20 living rooms with known overlaps, log whether model output matches `room_area - sum(objects)` vs. `room_area - union(objects)`. Confirms mechanism 3.
  3. **Tool augmentation probe**: Enable Python interpreter for GPT-4.1 on Max Box task. Inspect generated code for search strategy correctness vs. numeric precision. Expected: algorithm errors persist despite tools.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can hybrid architectures combining LLMs with external geometric solvers effectively resolve failures in collision avoidance and clearance reasoning?
- **Basis in paper**: [explicit] The authors propose "Near term" work to "hybridize with external geometric solvers... to compensate for models’ weaknesses."
- **Why unresolved**: Current models fail on planning tasks (Shortest Path, Free Space) even with tool use due to flawed internal spatial logic.
- **What evidence would resolve it**: Improved accuracy on planning tasks when models are coupled with verified geometric libraries compared to standalone baselines.

### Open Question 2
- **Question**: Does fine-tuning on irregular, overlap-heavy layouts with explicit spatial constraints improve model coherence under rotation and union operations?
- **Basis in paper**: [explicit] The paper suggests "Longer term" efforts to "train with explicit spatial constraints and harder distributions... so models learn to maintain coherence."
- **Why unresolved**: Models currently treat object areas as independent rather than unioned and struggle with non-axis-aligned polygons (HSSD).
- **What evidence would resolve it**: Increased accuracy on HSSD layouts and Max Box tasks following fine-tuning on the proposed distributions.

### Open Question 3
- **Question**: How can tool-augmentation be improved to ensure models generate algorithmically correct geometric code rather than merely precise arithmetic?
- **Basis in paper**: [inferred] Tool augmentation improved arithmetic tasks but failed planning tasks because models wrote code with "incorrect algorithms—treating boundary contact as collision."
- **Why unresolved**: The bottleneck is spatial reasoning logic, not numerical calculation; current tools do not verify the algorithmic validity of generated scripts.
- **What evidence would resolve it**: Performance gains on planning tasks in tool-augmented settings where generated code correctly distinguishes between valid contact and collision.

## Limitations
- Performance gaps may reflect training data distribution shifts rather than intrinsic reasoning deficits.
- The 2% tolerance on scalar tasks is stricter than typical VQA benchmarks, potentially penalizing models for minor numerical imprecision.
- Tool augmentation results are constrained by API availability—only select models support Python execution.

## Confidence
- **High**: Task difficulty stratification (Metric > Topology > Dynamic tiers); symbolic representation sufficiency for geometric grounding; tool-augmentation asymmetry (arithmetic vs. algorithmic errors).
- **Medium**: Generalization to real layouts (HSSD subset); symbolic vs. visual input parity; impact of reasoning model truncation.
- **Low**: Training data distribution effects; true upper bounds of current models on spatial reasoning; transferability to 3D or noisy visual inputs.

## Next Checks
1. **Training data probing**: Analyze model pretraining corpora for frequency of synthetic indoor layouts and polygon coordinates to quantify potential memorization effects.
2. **Tolerance sensitivity**: Re-score scalar tasks with 5% tolerance (as used for Free Space) to isolate precision vs. reasoning failures.
3. **Visual input augmentation**: Generate simplified renderings of symbolic layouts and evaluate whether VLM+image conditions improve systematically on tasks with shape complexity (HSSD layouts).