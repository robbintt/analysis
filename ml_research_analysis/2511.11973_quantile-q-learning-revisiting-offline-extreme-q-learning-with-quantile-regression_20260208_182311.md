---
ver: rpa2
title: 'Quantile Q-Learning: Revisiting Offline Extreme Q-Learning with Quantile Regression'
arxiv_id: '2511.11973'
source_url: https://arxiv.org/abs/2511.11973
tags:
- learning
- offline
- value
- uni00000013
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses instability and hyperparameter sensitivity\
  \ in Extreme Q-Learning (XQL) for offline reinforcement learning. The authors propose\
  \ estimating the temperature coefficient \u03B2 via quantile regression under mild\
  \ assumptions, eliminating the need for dataset-specific tuning."
---

# Quantile Q-Learning: Revisiting Offline Extreme Q-Learning with Quantile Regression

## Quick Facts
- arXiv ID: 2511.11973
- Source URL: https://arxiv.org/abs/2511.11973
- Authors: Xinming Gao; Shangzhe Li; Yujin Cai; Wenwu Yu
- Reference count: 12
- One-line primary result: Proposes a principled method to estimate the temperature coefficient β via quantile regression, achieving competitive performance with stable training dynamics using consistent hyperparameters across all tasks.

## Executive Summary
This paper addresses instability and hyperparameter sensitivity in Extreme Q-Learning (XQL) for offline reinforcement learning. The authors propose estimating the temperature coefficient β via quantile regression under mild assumptions, eliminating the need for dataset-specific tuning. They further introduce value regularization with mild generalization to improve training stability. Experiments on D4RL and NeoRL2 benchmarks show that their method achieves competitive or superior performance compared to state-of-the-art baselines, while maintaining stable training dynamics and using a consistent set of hyperparameters across all tasks and domains.

## Method Summary
Quantile Q-Learning (QQL) learns two value networks (V_ψ1, V_ψ2) via quantile regression at quantiles α₁=1-exp(-1) and α₂=1-exp(-exp(ω)) where ω≈0.57721. The temperature coefficient β(s) is derived as (V_ψ2(s)-V_ψ1(s))/ω, eliminating manual tuning. Value regularization with mild generalization (Eq. 7) adds stability, controlled by λ. The Q-function is updated via MSE Bellman error (Eq. 5), and the policy via an AWR-style objective (Eq. 6). Training uses consistent hyperparameters (λ=1.0, ζ=1.0) across all tasks.

## Key Results
- Achieves strong performance on D4RL and NeoRL2 benchmarks while using consistent hyperparameters across all tasks
- Demonstrates improved training stability with lower variance compared to XQL baselines
- Eliminates dataset-specific tuning of the critical temperature coefficient β through principled quantile regression estimation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Estimating the temperature coefficient β via quantile regression eliminates the need for dataset-specific hyperparameter tuning.
- Mechanism: The method reframes β as a state-dependent function β(s) derived from the difference between two value function estimates: V(s) (the α₁-quantile of Q(s,a)) and V̂(s) (the α₂-quantile), under the assumption that the Bellman error follows a Gumbel distribution. This allows β to be learned directly from data properties rather than set manually.
- Core assumption: The error between the optimal and estimated Q-functions follows a state-dependent Gumbel distribution: ε(s,a) ~ G(0, β(s)). Further, (Q(s,a) - V(s)) follows a negative Gumbel distribution with the same scale.
- Evidence anchors:
  - [abstract]: The authors propose "a principled method to estimate the temperature coefficient β via quantile regression under mild assumptions, eliminating the need for dataset-specific tuning."
  - [section]: Proposition 2 and 3 establish the theoretical link between quantiles of the Q-function and the Gumbel distribution's properties, leading to the estimators in Eqs. 3 and 4.
  - [corpus]: A related paper, PIQL, also addresses the limitations of fixed hyperparameters in expectile regression, a related distributional RL technique, though it uses a support constraint rather than quantile regression for estimation.
- Break condition: The mechanism would likely fail if the Bellman error distribution deviates significantly from a Gumbel distribution, violating Assumption 1 and 2.

### Mechanism 2
- Claim: Value regularization with mild generalization improves training stability and mitigates over-conservatism.
- Mechanism: The method augments the quantile regression loss for value functions by including a term from next-state value estimates sampled from the current policy. A conservative estimate is used for V(s') to manage extrapolation error, and the process is controlled by a hyperparameter λ.
- Core assumption: Mild generalization, by sampling actions from the current policy π(·|s'), allows the value function to generalize beyond the offline dataset without incurring catastrophic extrapolation error, particularly when paired with conservative estimation.
- Evidence anchors:
  - [abstract]: To further improve training stability, the authors "introduce a value regularization technique with mild generalization, inspired by recent advances in constrained value learning."
  - [section]: Section 4.3 details the updated objective (Eq. 7) that combines in-sample learning with a generalized term, and the ablation study (Figure 3) shows performance drops without it.
  - [corpus]: The paper "Doubly Mild Generalization for Offline Reinforcement Learning" (Mao et al., 2024) is cited as the inspiration for this technique.
- Break condition: The mechanism may fail if the generalization coefficient λ is set too high, causing the learned policy to diverge significantly from the behavior policy and incur large extrapolation error, or if the conservative estimation is insufficient to penalize out-of-distribution actions.

### Mechanism 3
- Claim: A unified set of hyperparameters (λ=1.0, ζ=1.0) can achieve strong performance across diverse domains and datasets.
- Mechanism: By learning the critical temperature parameter β adaptively, the method reduces its dependence on other fixed hyperparameters. The ablation study (Table 4) shows that performance is robust to changes in λ and ζ within a wide range.
- Core assumption: The primary source of hyperparameter sensitivity in the original XQL was the manually tuned temperature β. Once β is automated, other hyperparameters can remain fixed.
- Evidence anchors:
  - [abstract]: Experimental results demonstrate the algorithm achieves strong performance "while maintaining stable training dynamics and using a consistent set of hyperparameters across all datasets and domains."
  - [section]: Table 3 directly compares the proposed method with consistent hyperparameters against XQL variants with different tuning levels, showing XQL's performance drops significantly without tuning while QQL's remains high.
  - [corpus]: Corpus evidence on hyperparameter robustness is weak. The related papers do not explicitly focus on this property as a primary contribution.
- Break condition: This claim would not hold if the chosen fixed hyperparameters (λ=1.0, ζ=1.0) were found to be suboptimal for a novel, unseen domain, suggesting some residual sensitivity.

## Foundational Learning

- Concept: Extreme Value Theory (EVT) in Reinforcement Learning
  - Why needed here: The entire algorithm is built on the XQL framework, which models the Bellman error using EVT, specifically assuming a Gumbel distribution. Understanding this assumption is critical.
  - Quick check question: Can you explain why the Gumbel distribution is a suitable choice for modeling the distribution of the maximum Q-value or the Bellman error in the context of the Extreme Value Theorem?

- Concept: Quantile Regression
  - Why needed here: This is the core technique used to estimate the value functions V(s) and V̂(s), which are then used to derive the adaptive temperature β(s).
  - Quick check question: How does the pinball loss function (quantile regression loss) differ from standard mean squared error, and what property does it enforce on the learned value function?

- Concept: Conservative Estimation in Offline RL
  - Why needed here: A key challenge in offline RL is avoiding overestimation of Q-values for out-of-distribution actions. This paper introduces a specific form of conservative estimation within its value regularization framework.
  - Quick check question: Why is it crucial for an offline RL algorithm to learn a pessimistic or conservative value function, especially for actions not well-represented in the offline dataset?

## Architecture Onboarding

- Component map: Q_θ (Q-function) -> V_ψ1, V_ψ2 (value networks) -> π_φ (policy) -> β(s) (derived temperature)
- Critical path:
  1. Sample batch of transitions (s, a, r, s') from offline dataset
  2. Sample actions a' from current policy π for states s'
  3. Compute losses for V_ψ1 and V_ψ2 using quantile regression with mild generalization
  4. Compute Q-function loss using derived Bellman error incorporating learned β(s)
  5. Update policy using AWR-style objective
- Design tradeoffs: Adds computational overhead from two value networks and policy sampling for value regularization, traded for eliminating manual β tuning and improving stability. Conservative estimation in regularization trades some generalization potential for safety against extrapolation error.
- Failure signatures:
  - Instability or NaNs during training: May indicate numerical instability in β(s) computation when (V_ψ2 - V_ψ1) becomes very small. Implementation mitigates with lower bound clipping (β_low = 0.1).
  - Suboptimal or overly conservative policy: Could result from insufficient mild generalization (low λ) or violated Gumbel distribution assumption leading to poor β(s) estimation.
- First 3 experiments:
  1. Core Verification: Reproduce Figure 2a comparison, training both QQL and MXQL on Hopper-medium-replay, plotting training curves to confirm QQL's lower variance and stable learning.
  2. Ablation on Core Mechanism: Remove learned β(s), replace with fixed constant, compare performance to full QQL to isolate adaptive temperature estimation contribution.
  3. Sensitivity Analysis: Test robustness claim by training QQL on AntMaze-umaze without changing hyperparameters (λ=1.0, ζ=1.0), compare final performance to baselines requiring per-dataset tuning.

## Open Questions the Paper Calls Out
- Can the quantile-based temperature estimation and value regularization techniques be successfully adapted to the online reinforcement learning setting to improve training stability in interactive environments?
- Does the consistency of the β(s) scale between Assumption 1 and Assumption 2 hold in complex, high-dimensional environments beyond the simplified toy example provided?
- What factors cause Quantile Q-Learning to underperform the baseline XQL on specific safety-constrained tasks like SafetyHalfCheetah?

## Limitations
- Performance claims rely on the assumption that Bellman errors follow a Gumbel distribution, which may not hold for all offline datasets
- Computational overhead of maintaining two value networks and additional policy sampling may limit scalability to larger problems
- Mild generalization mechanism introduces hyperparameter λ that, while claimed robust, could still affect performance in novel domains

## Confidence

- **High confidence** in core mechanism (quantile regression for β estimation) due to clear theoretical grounding in Propositions 2-3 and ablation support showing significant performance drops without it
- **Medium confidence** in generalization claims, as robustness to hyperparameters is demonstrated across known benchmarks but not extensively validated on truly novel domains
- **Medium confidence** in stability improvements, as training curves show reduced variance but comparison with XQL (which also uses quantile regression) could be more comprehensive

## Next Checks

1. Test QQL on a dataset with known non-Gumbel error distributions (e.g., synthetic environments with heavy-tailed rewards) to evaluate robustness to the core distributional assumption
2. Perform systematic sensitivity analysis on mild generalization coefficient λ across a wider range (0.1 to 10) on multiple domains to quantify claimed robustness
3. Implement memory-efficient variant sharing representations between Q-function and value networks to assess whether computational overhead is necessary for performance gains