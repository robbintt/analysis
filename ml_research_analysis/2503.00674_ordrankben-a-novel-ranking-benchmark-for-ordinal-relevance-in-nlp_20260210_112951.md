---
ver: rpa2
title: 'OrdRankBen: A Novel Ranking Benchmark for Ordinal Relevance in NLP'
arxiv_id: '2503.00674'
source_url: https://arxiv.org/abs/2503.00674
tags:
- ranking
- relevance
- ordinal
- tasks
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces OrdRankBen, a novel NLP ranking benchmark
  that addresses the lack of ordinal relevance labels in existing datasets. By incorporating
  structured ordinal labels (1-5) instead of binary or continuous relevance scores,
  OrdRankBen enables more precise evaluation of ranking models' ability to capture
  fine-grained relevance distinctions.
---

# OrdRankBen: A Novel Ranking Benchmark for Ordinal Relevance in NLP

## Quick Facts
- **arXiv ID:** 2503.00674
- **Source URL:** https://arxiv.org/abs/2503.00674
- **Reference count:** 40
- **Primary result:** General-purpose LLMs (GPT-mini, LInstruct) outperform ranking-specific models on ordinal relevance ranking tasks.

## Executive Summary
OrdRankBen introduces a novel NLP ranking benchmark that addresses the lack of ordinal relevance labels in existing datasets. By incorporating structured ordinal labels (1-5) instead of binary or continuous relevance scores, the benchmark enables more precise evaluation of ranking models' ability to capture fine-grained relevance distinctions. The benchmark includes two datasets constructed from MSMARCO with distinct ordinal label distributions, and evaluation of nine models across three categories shows that general LLMs outperform ranking-specific models in ordinal ranking tasks.

## Method Summary
The benchmark constructs two datasets from MSMARCO: a document ranking task with 579,300 query-document pairs using uniform selection for label assignment, and a passage ranking task with 678,100 query-passage pairs using GPT-4 judgment for binary-0 passages. Labels are assigned on an ordinal scale of 1-5 representing 0%-100% relevance. Nine models are evaluated using semantic similarity scores to generate ranked lists, with performance measured by ERR@k and nDCG@k metrics across cutoffs k=[5,10,15,20,30,60].

## Key Results
- General-purpose LLMs (GPT-mini, LInstruct) outperform ranking-specific LLMs on ordinal relevance tasks
- nDCG metric demonstrates higher sensitivity to cutoff values compared to ERR, imposing heavier penalties on model performance
- Incorporation of ordinal relevance labels significantly enhances ability to discern nuanced differences among ranked items
- Document ranking task shows lower performance metrics compared to passage ranking task, likely due to different label distribution strategies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured ordinal labels (1-5) enable more precise evaluation of ranking models by capturing multi-granularity relevance distinctions.
- Mechanism: Binary labels collapse relevance into two states, losing fine-grained distinctions. Continuous scores lack explicit ordinal structure, making relative importance ambiguous. Ordinal labels preserve ordered relevance levels (0%, 25%, 50%, 75%, 100%) while maintaining interpretable boundaries.
- Core assumption: Real-world relevance exists on a graded spectrum where "slightly relevant" and "highly relevant" are meaningfully distinct states that models should discriminate.
- Evidence anchors:
  - [abstract]: "binary labels oversimplify relevance distinctions, while continuous scores lack a clear ordinal structure"
  - [section 1]: "binary labels oversimplify the ranking problem by failing to capture fine-grained differences between items"
  - [corpus]: Limited direct corpus evidence on ordinal vs binary comparison; related work (Zhuang 2024 cited) explores fine-grained relevance labels for zero-shot rankers.

### Mechanism 2
- Claim: nDCG is more sensitive to cutoff values and imposes heavier performance penalties than ERR on ordinal ranking tasks.
- Mechanism: nDCG applies graded relevance weighting with logarithmic discounting that varies significantly by position and cutoff. ERR uses probabilistic relevance that saturates when highly relevant items appear early, making it less responsive to variations in lower-ranked items.
- Core assumption: The choice of evaluation metric should reflect whether the application prioritizes top-heavy precision (ERR) or full-list quality assessment (nDCG).
- Evidence anchors:
  - [section 4]: "nDCG is sensitive to the cutoff, making it more responsive than ERR in detecting differences in top-ranking results"
  - [section 4]: Table 6 shows nDCG std=0.061 (DR) vs ERR std=0.0003, with nDCG average scores substantially lower
  - [corpus]: No direct corpus comparison of ERR vs nDCG on ordinal benchmarks found.

### Mechanism 3
- Claim: General-purpose LLMs (GPT-mini, LInstruct) outperform ranking-specific models on ordinal relevance tasks, suggesting general instruction-following capability transfers to fine-grained relevance judgment.
- Mechanism: Ranking-focused LMs fine-tuned on binary/continuous MSMARCO labels may overfit to coarse relevance distinctions. General LLMs, trained on diverse instruction-following tasks, can interpret nuanced relevance criteria without task-specific fine-tuning.
- Core assumption: Ordinal relevance judgment is fundamentally a semantic understanding task that benefits from broad language modeling capability rather than ranking-specific optimization.
- Evidence anchors:
  - [section 4]: "GPTmini obtains the best performance on both tasks, according to the average nDCG score"
  - [section 5]: "general LLMs, particularly GPTmini and LInstruct, demonstrate superior performance across both tasks"
  - [corpus]: "Likert or Not" paper (2025) finds LLMs achieve state-of-the-art zero-shot relevance ranking, supporting generalization claim.

## Foundational Learning

- **Learning-to-Rank (LTR) approaches (pointwise, pairwise, listwise)**: Why needed here - OrdRankBen evaluates models trained under different LTR paradigms; understanding these distinctions explains why ranking-specific models may struggle with ordinal labels they weren't optimized for. Quick check question: Can you explain why a model trained with pairwise loss on binary labels might fail to distinguish between "moderately relevant" and "highly relevant" items?

- **Graded relevance metrics (nDCG, ERR)**: Why needed here - Interpreting results requires understanding how each metric weights relevance grades and positions. ERR saturates; nDCG penalizes throughout the list. Quick check question: Given a ranked list where the top result has relevance=5, how does doubling the cutoff from k=10 to k=20 affect ERR vs nDCG differently?

- **Ordinal vs nominal vs continuous variables**: Why needed here - The benchmark's core contribution is treating relevance as ordinal (ordered categories with meaningful distances) rather than nominal (binary) or continuous (unstructured scores). Quick check question: Why is converting a continuous relevance score of 0.73 to "label 4" not simply rounding?

## Architecture Onboarding

- **Component map:** Query selection → Label assignment → Model inference → Ranking → Evaluation
- **Critical path:** 1. Query selection → 6,781 queries with 100+ candidates (passage) or all queries with 100 candidates (document) → 2. Label assignment → ordinal scale via defined strategy → 3. Model inference → semantic similarity scores per query-text pair → 4. Ranking → sort by score, compute metrics against ground-truth ordinal labels
- **Design tradeoffs:** Uniform selection (document task) ensures balanced label distribution but may mislabel true relevance; GPT judgment (passage task) may introduce annotation bias but captures semantic nuance. ERR provides stable rankings across cutoffs but may miss fine-grained differences; nDCG is sensitive but noisy.
- **Failure signatures:** ERR scores showing near-zero variance across cutoffs → model placing all relevant items at top or metric saturation; Large performance gap between document and passage tasks → model sensitive to label distribution imbalance (passage task is 59% label-5); nDCG decreasing with increasing k → model struggles to rank lower-relevance items correctly
- **First 3 experiments:** 1. Baseline reproduction: Run all 9 evaluated models on OrdRankBen passage task; verify ERR@5 and nDCG@5 match reported values within ±0.01; 2. Metric sensitivity analysis: Plot ERR@k and nDCG@k for k ∈ {5, 10, ..., 60} for 3 representative models (one per category); confirm nDCG shows higher variance; 3. Label perturbation test: Collapse ordinal labels to binary (4-5 → relevant, 1-3 → not relevant); compare model rankings to quantify information loss from binarization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do general-purpose LLMs outperform ranking-specific LLMs on ordinal relevance tasks?
- Basis in paper: [explicit] The results section notes that general LLMs like GPT-mini and LInstruct "demonstrate superior performance" compared to ranking-focused LLMs, a finding that warrants further investigation into the underlying mechanisms.
- Why unresolved: The paper reports the phenomenon but does not provide an analysis of whether this is due to model architecture, training data scale, or inherent prompt adherence capabilities.
- What evidence would resolve it: Ablation studies comparing the zero-shot ordinal reasoning of general vs. ranking-specific models on controlled subsets of data.

### Open Question 2
- Question: To what extent does the instruction to "balance label distribution" during annotation skew the ground truth relevance?
- Basis in paper: [inferred] The prompt template in Appendix C explicitly instructs the model to ensure labels are predicted "as evenly as possible," potentially forcing a distribution that contradicts the semantic reality of the query-passage pairs.
- Why unresolved: Artificial balancing may introduce noise by forcing the model to assign incorrect relevance scores to satisfy a statistical quota, degrading benchmark validity.
- What evidence would resolve it: A comparison between the "balanced" GPT annotations and human annotations where no distribution constraints are applied.

### Open Question 3
- Question: Does the "Uniform Selection" method for document ranking introduce label noise by conflating rank position with ordinal relevance?
- Basis in paper: [inferred] The authors assign labels based strictly on position (e.g., top 20 get label 5) derived from continuous scores, assuming the initial ranking perfectly correlates with multi-level relevance.
- Why unresolved: This heuristic may mislabel highly relevant documents in the "lower" buckets or vice-versa if the original ranking was imperfect.
- What evidence would resolve it: A correlation analysis between the continuous scores of the boundary documents (e.g., rank 20 vs. rank 21) and their assigned ordinal labels.

## Limitations
- The document ranking task uses uniform selection which may not reflect true relevance distributions, while the passage ranking task depends on GPT-4 judgment for binary-0 passages
- Significant performance gaps between document and passage tasks could reflect annotation artifacts rather than model capability differences
- Superior performance of general LLMs may be partially attributed to using the same model family (GPT-mini) for evaluation as was used in annotation, raising fairness concerns

## Confidence
- **High confidence**: The ordinal label mechanism (Mechanism 1) and metric sensitivity findings (Mechanism 2) are well-supported by the evaluation data and mathematical properties of nDCG/ERR
- **Medium confidence**: The general LLM superiority claim (Mechanism 3) requires careful interpretation given potential annotation bias and the lack of ablation studies controlling for model family overlap
- **Low confidence**: The assumption that ordinal labels provide meaningful information gain over binary labels in real-world applications lacks direct empirical validation in the paper

## Next Checks
1. Conduct cross-validation using models from different families than those used for annotation to isolate model capability from annotation bias effects
2. Perform ablation studies comparing model performance when using ordinal vs binary labels on the same datasets to quantify the practical benefit of ordinal granularity
3. Test whether the nDCG sensitivity advantage persists when using alternative positional discounting functions or when evaluating on datasets with different label distributions