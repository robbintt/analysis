---
ver: rpa2
title: 'Understanding the Thinking Process of Reasoning Models: A Perspective from
  Schoenfeld''s Episode Theory'
arxiv_id: '2509.14662'
source_url: https://arxiv.org/abs/2509.14662
tags:
- reasoning
- episode
- verify
- explore
- annotation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work applies Schoenfeld\u2019s Episode Theory\u2014a cognitive\
  \ framework for human mathematical problem-solving\u2014to analyze reasoning traces\
  \ generated by Large Reasoning Models (LRMs). By annotating thousands of sentences\
  \ and paragraphs from model-generated solutions with seven cognitive labels (Read,\
  \ Analyze, Plan, Implement, Explore, Verify, Monitor), the authors establish the\
  \ first publicly available benchmark for fine-grained machine reasoning analysis."
---

# Understanding the Thinking Process of Reasoning Models: A Perspective from Schoenfeld's Episode Theory

## Quick Facts
- **arXiv ID:** 2509.14662
- **Source URL:** https://arxiv.org/abs/2509.14662
- **Reference count:** 17
- **Key outcome:** The study applies Schoenfeld's Episode Theory to LRM reasoning traces, creating a 7-label sentence-level annotation framework (Read, Analyze, Plan, Implement, Explore, Verify, Monitor) that reveals cognitive-like transition patterns in model reasoning.

## Executive Summary
This paper introduces the first cognitive framework for analyzing Large Reasoning Model (LRM) thinking processes, adapting Schoenfeld's Episode Theory from human mathematical problem-solving to machine reasoning traces. By annotating thousands of sentences from DeepSeek-R1's SAT math solutions with seven cognitive episode labels, the authors create a publicly available benchmark and demonstrate that LRM reasoning exhibits structured transition patterns resembling human cognition. The framework enables fine-grained analysis of how models approach problems and provides a foundation for developing more interpretable and controllable reasoning systems.

## Method Summary
The researchers constructed a benchmark by collecting 1,385 SAT Math items and generating responses using DeepSeek-R1. They manually annotated 38 responses (915 paragraphs, 3,087 sentences) with hierarchical labels: three paragraph-level categories (General, Explore, Verify) and seven sentence-level categories (Read, Analyze, Plan, Implement, Explore, Verify, Monitor). The annotation followed detailed guidebooks derived from Schoenfeld's Episode Theory. They evaluated three baseline approaches for automated annotation: LLM-based zero-shot classification using GPT-4.1 with in-context examples, and training-based methods using BERT/RoBERTa fine-tuning and embeddings with SVM/MLP/KNN classifiers. The dataset and guidebooks are publicly available at https://github.com/MingLiiii/Schoenfeld_Reasoning.

## Key Results
- The framework successfully captured structured reasoning patterns in LRM traces, with "Plan" and "Implement" being the most frequent episode categories
- Sentence-level automated annotation achieved best performance of 0.805 accuracy and 0.764 Cohen's κ using GPT-4.1 with guidebook and examples
- Transition analysis revealed that LRMs follow specific cognitive-like patterns, with Plan-to-Implement being the most frequent transition (probability 0.40)
- The annotation revealed distinct episode patterns between paragraph and sentence levels, with paragraph-level showing more Explore and Verify activities

## Why This Works (Mechanism)
The framework works by providing a theoretically grounded lens through which to interpret LRM reasoning traces. Schoenfeld's Episode Theory, originally developed to understand human mathematical problem-solving, maps naturally onto the sequential decision-making processes observed in LRM outputs. The seven-episode framework captures the full spectrum of reasoning activities from initial problem comprehension (Read) through analysis (Analyze), strategy formulation (Plan), execution (Implement), exploration of alternatives (Explore), and validation (Verify/Monitor). By treating model outputs as sequences of cognitive episodes rather than mere text generation, the framework reveals underlying reasoning structures that are otherwise obscured by the surface form of the outputs.

## Foundational Learning
- **Schoenfeld's Episode Theory**: A cognitive framework categorizing human mathematical problem-solving into distinct phases (Read, Analyze, Plan, Implement, Explore, Verify, Monitor). Why needed: Provides theoretical foundation for interpreting LRM reasoning as cognitive processes rather than just text generation. Quick check: Can you explain how each episode maps to observable behaviors in problem-solving?
- **Hierarchical annotation schema**: Two-level labeling system with paragraph-level (3 labels) and sentence-level (7 labels) categories. Why needed: Captures both coarse-grained reasoning strategies and fine-grained cognitive activities. Quick check: Can you distinguish when to use paragraph vs. sentence labels?
- **Cognitive episode transition analysis**: Examining the probabilities of moving between different episode categories. Why needed: Reveals the sequential structure and reasoning patterns in model outputs. Quick check: What does a high probability of Plan-to-Implement transitions indicate about model behavior?
- **Cohen's kappa statistic**: Measures inter-annotator agreement accounting for chance agreement. Why needed: Provides reliable assessment of annotation consistency for subjective cognitive labels. Quick check: What κ value indicates substantial agreement?
- **Zero-shot vs. training-based classification**: Different approaches to automated annotation with distinct trade-offs. Why needed: Determines the most effective way to scale the annotation framework. Quick check: Why did LLM-based methods outperform training-based methods despite less data?

## Architecture Onboarding

**Component Map**
Data (SAT items) → DeepSeek-R1 (response generation) → Human annotation (episode labeling) → Benchmark dataset → Automated classifiers (LLM or training-based) → Performance evaluation

**Critical Path**
1. Problem generation and response collection (DeepSeek-R1)
2. Human annotation following guidebooks (paragraph and sentence levels)
3. Automated classifier training/evaluation
4. Transition pattern analysis and interpretation

**Design Tradeoffs**
- Theoretical rigor (Schoenfeld framework) vs. practical applicability to machine reasoning
- Granularity (7 vs. fewer labels) vs. annotation complexity and agreement
- Zero-shot (no training data needed) vs. training-based (potentially better performance) approaches
- SAT domain specificity vs. generalizability to other problem types

**Failure Signatures**
- Low inter-annotator agreement indicates ambiguous episode boundaries
- Confusion between Analyze/Verify and Implement/Verify pairs suggests need for clearer distinction criteria
- Poor classifier performance indicates mismatch between guidebook specifications and observable patterns
- Inconsistent transition patterns across different LRMs suggests domain-specific rather than universal reasoning structures

**Three First Experiments**
1. Apply the annotation framework to reasoning traces from a different LRM (e.g., OpenAI o1) to test generalizability of observed patterns
2. Conduct ablation studies varying guidebook specificity to quantify the impact on annotation agreement and classifier performance
3. Compare transition patterns between machine and human reasoning traces on identical problems to validate cognitive alignment claims

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the Schoenfeld framework generalize to high-complexity mathematical reasoning, such as Olympiad-level problems, or does the increased difficulty require new episodic categories?
- **Basis in paper:** The authors explicitly state in the Limitations section that "SAT is designed as a college admission test... the overall difficulty level is relatively moderate" and plan to include more challenging datasets.
- **Why unresolved:** The current study is restricted to SAT problems, which may not trigger the full range of complex problem-solving behaviors found in harder mathematical domains.
- **What evidence would resolve it:** Annotation of reasoning traces from advanced datasets (e.g., AIME, IMO) using the current guidebook, followed by analysis of inter-annotator agreement and transition matrices.

### Open Question 2
- **Question:** How can the performance of automated annotation agents be improved to reliably replace labor-intensive human annotation?
- **Basis in paper:** The Limitations section notes that "the accuracy for automatic annotation is not very high, further efforts are needed to improve the accuracy."
- **Why unresolved:** While the best zero-shot method achieved 0.805 accuracy, this is insufficient for fully automated high-fidelity analysis.
- **What evidence would resolve it:** Development of specialized fine-tuned models or advanced prompting strategies achieving accuracy and Cohen's κ scores comparable to human inter-annotator reliability.

### Open Question 3
- **Question:** Can this framework be operationalized to actively control or guide the generation of Large Reasoning Models, rather than solely serving as a post-hoc analysis tool?
- **Basis in paper:** The abstract claims the framework "enables future work on more controllable and transparent reasoning systems," yet the paper only demonstrates post-hoc annotation.
- **Why unresolved:** The paper establishes a method for understanding thinking processes but does not demonstrate steering the model during inference.
- **What evidence would resolve it:** An intervention study where the model is constrained to follow specific episodic transitions and the impact on problem-solving accuracy is measured.

### Open Question 4
- **Question:** Do the distinct episodic transition dynamics observed in the study generalize across different Large Reasoning Model architectures?
- **Basis in paper:** The paper relies exclusively on DeepSeek-R1 reasoning traces.
- **Why unresolved:** It is unclear if observed probabilities are specific to DeepSeek-R1's training data or represent universal properties of large reasoning models.
- **What evidence would resolve it:** A comparative study applying the same annotation protocol to traces from other LRMs (e.g., GPT-o1, Claude) and comparing resulting state transition matrices.

## Limitations
- The framework's validity depends on the assumption that Schoenfeld's human cognition theory appropriately maps onto LRM reasoning traces, without independent validation
- Moderate classifier performance (best κ=0.764) indicates significant ambiguity in label definitions, particularly for boundary cases
- Small corpus size (38 annotated responses) limits generalizability of findings
- All experiments use a single LRM (DeepSeek-R1) without comparison across different model architectures or prompting strategies

## Confidence

- **High confidence**: The methodology for applying Schoenfeld's framework to LRM traces is clearly specified and reproducible; the hierarchical annotation schema is well-defined; the benchmark dataset creation process is transparent
- **Medium confidence**: The claim that identified transition patterns "align with human cognitive processes" is supported by the annotation scheme but lacks direct comparison to human problem-solving data; the moderate classifier performance suggests the episode categories capture real structure but with substantial ambiguity
- **Low confidence**: The assertion that this framework provides meaningful insight into "LRM cognition" rather than just text generation patterns; the generalizability of findings across different LRMs, problem domains, or reasoning strategies

## Next Checks

1. **Cross-model validation**: Apply the annotation framework to reasoning traces from multiple LRMs (e.g., OpenAI o1, Claude 3.5, Gemini) to test whether the episode patterns and transition probabilities are consistent across architectures or specific to DeepSeek-R1's reasoning style.

2. **Human comparison study**: Collect and annotate reasoning traces from humans solving the same SAT problems, then compare the frequency distributions and transition patterns between human and machine reasoning to empirically test the "alignment with human cognitive processes" claim.

3. **Ablation on guidebook specificity**: Systematically vary the level of detail in the annotation guidebook (full version vs. simplified rules) and measure changes in inter-annotator agreement and classifier performance to quantify how much of the observed structure depends on the specific theoretical framing versus observable text patterns.