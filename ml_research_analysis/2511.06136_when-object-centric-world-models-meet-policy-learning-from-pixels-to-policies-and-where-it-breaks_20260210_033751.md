---
ver: rpa2
title: 'When Object-Centric World Models Meet Policy Learning: From Pixels to Policies,
  and Where It Breaks'
arxiv_id: '2511.06136'
source_url: https://arxiv.org/abs/2511.06136
tags:
- learning
- world
- object-centric
- dlpwm
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces DLPWM, an object-centric world model that
  disentangles visual scenes into object-level latents. While DLPWM achieves strong
  reconstruction and prediction performance, including robustness to out-of-distribution
  visual variations, policies trained on its latents underperform compared to DreamerV3
  in downstream model-based control tasks.
---

# When Object-Centric World Models Meet Policy Learning: From Pixels to Policies, and Where It Breaks

## Quick Facts
- arXiv ID: 2511.06136
- Source URL: https://arxiv.org/abs/2511.06136
- Reference count: 9
- Primary result: DLPWM achieves strong visual representation learning but policies trained on its latents underperform DreamerV3 in downstream control tasks

## Executive Summary
This paper introduces DLPWM, an object-centric world model that disentangles visual scenes into object-level latents, achieving strong reconstruction and prediction performance with robustness to visual variations. Despite these strengths, policies trained on DLPWM's latents underperform compared to DreamerV3 in model-based control tasks. The authors identify that representation shifts during multi-object interactions, particularly near contact points, lead to unstable policy learning. They propose using exponential moving averages of latent slots as a mitigation strategy for future work.

## Method Summary
DLPWM is an object-centric world model that disentangles visual scenes into object-level latents, enabling strong reconstruction and prediction performance. The model is trained on pixel inputs and learns to represent scenes as compositions of individual objects, providing robustness to out-of-distribution visual variations. However, when policies are trained on these object-centric latents for downstream control tasks, they underperform compared to DreamerV3, suggesting a gap between representation quality and policy learning effectiveness.

## Key Results
- DLPWM achieves strong reconstruction and prediction performance with robustness to out-of-distribution visual variations
- Policies trained on DLPWM latents underperform compared to DreamerV3 in downstream model-based control tasks
- Latent trajectory analysis reveals representation shifts during multi-object interactions, particularly near contact points

## Why This Works (Mechanism)
DLPWM's object-centric approach successfully disentangles visual scenes into meaningful object representations, enabling robust visual reconstruction and prediction. However, the policy learning component fails because the learned representations undergo significant shifts during complex multi-object interactions. These shifts, particularly pronounced near contact points between objects, corrupt the per-slot inputs used by the policy, leading to unstable learning. The mechanism suggests that while object-centric representations are excellent for visual understanding, they may introduce instability in the temporal consistency needed for policy learning.

## Foundational Learning
- Object-centric representations: Why needed - enables compositional understanding of scenes; Quick check - evaluate scene decomposition quality
- Latent space dynamics: Why needed - captures temporal evolution of objects; Quick check - analyze latent trajectory smoothness
- Policy learning from latent representations: Why needed - enables planning and control from abstract state spaces; Quick check - measure policy performance on benchmark tasks
- Representation stability: Why needed - ensures consistent inputs for policy learning; Quick check - quantify representation shift magnitude during interactions

## Architecture Onboarding

Component Map:
Pixel Input -> Object-Centric Encoder -> Latent Slots -> Latent Dynamics Model -> Policy Network -> Action Output

Critical Path:
Pixel Input -> Object-Centric Encoder -> Latent Slots -> Policy Network -> Action Output (for control), with Latent Dynamics Model providing predictions for planning

Design Tradeoffs:
- Object-centric vs. monolithic representations: Object-centric provides better compositional understanding but may introduce instability during interactions
- Latent slot consistency: Maintaining stable slot assignments across timesteps vs. allowing flexible re-assignment
- Temporal abstraction: Balancing between fine-grained temporal modeling and computational efficiency

Failure Signatures:
- Policy performance degradation despite strong visual reconstruction
- Latent trajectory instability during multi-object interactions
- Representation shifts specifically near contact points between objects

First 3 Experiments:
1. Compare policy performance using EMA-smoothed latents vs. raw latents across varying interaction complexities
2. Test policy learning in environments with controlled contact vs. non-contact interaction modes
3. Quantify correlation between latent trajectory stability metrics and policy performance across different scene compositions

## Open Questions the Paper Calls Out
The paper proposes using exponential moving averages of latent slots to mitigate the instability issues but leaves this as future work. The effectiveness of this approach remains untested, and the paper calls for systematic validation of whether this solution addresses the core problem of representation shifts during multi-object interactions.

## Limitations
- The proposed solution of using EMA-smoothed latents remains untested and unverified
- The explanation for performance gaps relies on qualitative trajectory analysis rather than systematic ablation studies
- The connection between representation shifts and policy learning instability needs more rigorous quantitative validation

## Confidence
- Visual representation learning effectiveness: High
- Identification of representation shift problem: Medium
- Explanation of mechanism linking shifts to policy failure: Medium
- Proposed solution effectiveness: Low (untested)

## Next Checks
1. Conduct systematic ablation study comparing policy performance when using EMA-smoothed latents versus raw latents across varying levels of object interaction complexity
2. Perform controlled experiments isolating whether representation shifts occur specifically near contact points by using environments with contact and non-contact interaction modes
3. Execute quantitative analysis of correlation between latent trajectory stability metrics and policy performance across different scene compositions to establish relationship strength