---
ver: rpa2
title: 'KDPE: A Kernel Density Estimation Strategy for Diffusion Policy Trajectory
  Selection'
arxiv_id: '2508.10511'
source_url: https://arxiv.org/abs/2508.10511
tags:
- kdpe
- robot
- trajectories
- tasks
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of trajectory selection in Diffusion
  Policy (DP), where the stochastic denoising process can produce outlier trajectories
  leading robots out of the data distribution. The authors propose KDPE, a Kernel
  Density Estimation strategy that samples multiple trajectories from DP and selects
  the most representative one based on action density.
---

# KDPE: A Kernel Density Estimation Strategy for Diffusion Policy Trajectory Selection

## Quick Facts
- **arXiv ID**: 2508.10511
- **Source URL**: https://arxiv.org/abs/2508.10511
- **Reference count**: 40
- **Primary result**: KDPE improves Diffusion Policy success rates from 79.3% to 88.2% average across 7 simulated tasks

## Executive Summary
Diffusion Policy generates diverse trajectories through stochastic denoising, but this can produce outlier trajectories that lead robots out of the data distribution. KDPE addresses this by sampling multiple trajectories and selecting the most representative one based on action density estimation. Using a manifold-aware kernel that handles position, orientation, and gripper state with appropriate geometric distances, KDPE achieves better performance than standard DP on both simulated and real robot tasks while adding only ~2.24Hz computational overhead.

## Method Summary
KDPE is a post-processing module for Diffusion Policy that filters outlier trajectories at inference time. The method generates N=100 trajectories from DP given an observation, then estimates the probability density function of the final action (8th timestep) in each trajectory using Kernel Density Estimation with a manifold-aware kernel. This kernel computes Euclidean distances for position and gripper components, and geodesic distances (via Lie group logarithm) for SO(3) orientations. The trajectory with the highest density is selected for execution. The approach adds minimal computational overhead (~13ms for population generation, ~3ms for KDE) while significantly improving success rates by filtering low-probability outlier trajectories.

## Key Results
- KDPE achieves 88.2% average success rate vs 79.3% for standard DP across 7 simulated tasks
- Particularly effective on precision tasks (ToolHang +10.3%, CoffeeMaking +8.3%)
- Shows larger improvements on lower-quality datasets (MimicGen: 80.6% vs 72.6% vs RoboMimic: 91.6% vs 89.3%)
- Maintains performance advantage under visual perturbations, with KDPE-C performing 1.4% worse vs 5.9% for DP-C under unseen backgrounds
- Adds only ~2.24Hz computational overhead, suitable for real-time control

## Why This Works (Mechanism)

### Mechanism 1: Outlier Filtering via Density Selection
Selecting the highest-density trajectory from a population filters outliers while preserving multimodal modes learned by Diffusion Policy. The stochastic denoising process produces a distribution of trajectories conditioned on the same observation. By estimating the PDF via KDE and selecting the trajectory with highest density, KDPE discards low-probability samples that fall outside the training distribution's dominant modes. This operates at inference time without retraining.

### Mechanism 2: Manifold-Aware Kernel for Heterogeneous Action Spaces
A manifold-aware kernel combining Euclidean distances for position/gripper and geodesic distances for SO(3) orientations enables meaningful density estimation across heterogeneous action components. Standard Gaussian kernels assume Euclidean structure, but robotic actions span different manifolds. The proposed kernel computes differences using appropriate geometry: Euclidean subtraction for position and gripper, Lie group logarithm (axis-angle via log map) for rotations, yielding proper Riemannian distance metrics.

### Mechanism 3: Efficient Single-Action Density Estimation
Estimating density on only the final action provides sufficient signal for outlier rejection with minimal computation. Full trajectory density estimation requires high-dimensional kernels and T-1 conditional KDE computations. Using only the final action reduces this to a single KDE evaluation, avoiding curse of dimensionality while maintaining effectiveness.

## Foundational Learning

- **Diffusion Policy (DDPM for robot control)**: KDPE is an add-on module that requires understanding how DP generates trajectories via iterative denoising from random noise. Can you explain why the same observation can yield different trajectories from Diffusion Policy?

- **Kernel Density Estimation (KDE)**: The core selection mechanism; understanding how kernels estimate PDFs from finite samples is essential for debugging bandwidth choices. How does changing kernel bandwidth affect the smoothness and mode-sensitivity of the estimated density?

- **Lie groups and SO(3) geometry**: The manifold-aware kernel uses log maps to convert rotation differences to tangent space vectors. Implementing this incorrectly will break orientation handling. What is the geometric interpretation of log(R_j^T R_i) and why can't we simply subtract rotation matrices?

## Architecture Onboarding

- **Component map**: Observation -> DP backbone -> N trajectories -> KDE module (manifold-aware kernel) -> Selection module (argmax) -> Receding horizon controller

- **Critical path**: 1) Observation → DP backbone → N trajectories (each T×D) 2) Extract final actions {a_i} from each trajectory 3) Compute pairwise distances ∆_ij using manifold-appropriate metrics 4) Evaluate kernel k(a_i, a_j) for all pairs 5) Sum kernels to get density ρ(a_i) for each action 6) Select trajectory with max ρ, execute first action(s), repeat

- **Design tradeoffs**: Population size N: larger N improves density estimation but increases latency (paper uses N=100; generating population adds ~13ms, KDE adds ~3ms). Bandwidth parameters (σ_pos=0.05, σ_rot=0.25, σ_grip=1.0): control sensitivity to outliers. Action horizon position: paper uses 8th action (execution horizon).

- **Failure signatures**: KDPE-OOD (minimum density selection) crashes performance (71.6% vs 88.2%), confirming directionality matters. If KDPE provides no improvement, check if policy generates diverse enough trajectories or if bandwidths are correctly tuned.

- **First 3 experiments**: 1) Baseline replication: Train DP-C on RoboMimic Can task, compare DP vs KDPE with N=100, verify 1-3% improvement. 2) Ablation on kernel components: Disable rotation or gripper components, measure impact on orientation precision tasks. 3) Population size sweep: Test N ∈ {16, 32, 64, 100, 200} on CubeSort, plot success rate vs inference time.

## Open Questions the Paper Calls Out

- **Can KDPE guide the denoising process?** Using KDPE to guide the denoising process of DP rather than solely for post-hoc trajectory selection. The current implementation acts as a filter on the final output, not as an intermediate step within the diffusion process.

- **How to overcome curse of dimensionality?** Applying KDPE to higher-dimensional problems like bimanual manipulation while avoiding curse of dimensionality. Current validation is limited to single-arm (10-DOF) actions, and kernel-based methods struggle with sparse data in high-dimensional spaces.

- **Can PDF detect irrecoverable OOD states?** Detecting when the robot enters an OOD state via the estimated PDF and switching to alternative trajectory selection methods. KDPE currently maximizes density but lacks a mechanism to detect when the generated population is fundamentally flawed.

- **Does KDPE transfer to other generative models?** Applying KDPE to other generative models like flow matching, or to generalist policies. The evaluation is restricted to the specific Diffusion Policy architecture, leaving transferability unproven.

## Limitations
- KDPE's effectiveness is fundamentally bounded by the quality of the underlying Diffusion Policy—it cannot recover from a poorly trained policy.
- Performance depends on carefully tuned bandwidth parameters that may not generalize across all tasks or action scales.
- Current approach is limited to single-arm manipulation; scaling to bimanual or dexterous manipulation faces curse of dimensionality challenges.
- Selecting based on final action assumes it represents trajectory quality, which may fail if intermediate actions are anomalous.

## Confidence
- **High**: Core claim of 88.2% vs 79.3% success rate improvement is well-supported by simulation and real-robot experiments across multiple tasks.
- **Medium**: Computational overhead claim (~2.24Hz) is reported but implementation details are not fully specified.
- **Medium**: Bandwidth parameters and population size (N=100) are presented as effective but lack systematic tuning procedure.

## Next Checks
1. **Systematic parameter sensitivity analysis**: Conduct sweeps of bandwidth parameters (σ_pos, σ_rot, σ_grip) and population size N on CubeSort to identify sensitivity and optimal configurations.

2. **Intermediate-action quality evaluation**: Design experiment to test whether filtering based on intermediate actions (e.g., 4th action) provides better outlier rejection than final action for tasks requiring precise intermediate steps.

3. **High-dimensional scaling validation**: Test KDPE on bimanual manipulation task to empirically assess curse of dimensionality impact and determine if current N=100 population size remains sufficient.