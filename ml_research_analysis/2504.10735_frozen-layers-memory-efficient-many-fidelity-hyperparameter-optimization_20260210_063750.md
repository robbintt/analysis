---
ver: rpa2
title: 'Frozen Layers: Memory-efficient Many-fidelity Hyperparameter Optimization'
arxiv_id: '2504.10735'
source_url: https://arxiv.org/abs/2504.10735
tags:
- fidelity
- layers
- training
- layer
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the challenge of efficient hyperparameter\
  \ optimization (HPO) for large-scale deep learning models, where full training is\
  \ computationally expensive. The authors propose a novel fidelity source based on\
  \ freezing a variable number of layers during training\u2014only training the last\
  \ z layers while keeping the first n-z frozen."
---

# Frozen Layers: Memory-efficient Many-fidelity Hyperparameter Optimization

## Quick Facts
- **arXiv ID:** 2504.10735
- **Source URL:** https://arxiv.org/abs/2504.10735
- **Reference count:** 40
- **Primary result:** Layer freezing provides up to 3x memory savings and 4x runtime reduction while maintaining strong rank correlations (>0.85) for hyperparameter optimization

## Executive Summary
This paper introduces a novel fidelity source for multi-fidelity hyperparameter optimization (HPO) based on freezing a variable number of layers during training. By training only the last z layers while keeping the first n-z frozen, the method achieves significant memory and computational savings compared to full model training. The key insight is that even with most layers frozen, the relative ranking of hyperparameter configurations remains strongly correlated with full training results, enabling effective HPO at low fidelities. Experiments across ResNet and Transformer architectures demonstrate high rank correlations even when training only 40% of layers, while substantially reducing resource requirements.

## Method Summary
The method freezes the first n-z layers of a neural network, training only the last z layers. This reduces memory by eliminating the need to store intermediate activations for frozen layers and shortens the backward pass. The approach works by leveraging the inductive biases of randomly-initialized deep networks, where even frozen layers provide "random features" that preserve the relative ranking of hyperparameter configurations. The layer-splitting algorithm recursively traverses the model to identify sequential layers, which can then be frozen or trained based on the fidelity parameter z. The method can be combined with other fidelity sources like epochs or training tokens.

## Key Results
- Memory savings up to 3x and runtime reductions up to 4x compared to full model training
- Spearman's rank correlation >0.85 even when training only 40% of layers
- High correlation preservation across ResNet-18 (CIFAR-100) and GPT-2 style Transformers (SlimPajama)
- The approach works effectively as a standalone fidelity source or when combined with other fidelities

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Selectively training only the last z layers reduces GPU memory consumption by eliminating the need to store intermediate activations for frozen layers.
- **Mechanism:** During standard backward pass, activations from forward pass must be retained in memory for gradient computation. By freezing first n-z layers, their intermediate activations are not required for backward pass as no gradients are computed for these layers.
- **Core assumption:** Network's forward pass is sequential enough that layers can be grouped and frozen without creating unresolvable dependencies for trainable layers.
- **Evidence anchors:** [abstract] "...offering significant compute and memory savings..."; [Section 3.2, Fig. 1] "lower memory because the activations of the first z layers do not require to be kept in memory for the backward pass"; [Section 4.2, Fig. 2] monotonic decrease in runtime and memory as trainable layers decrease.

### Mechanism 2
- **Claim:** Even with significant portion of layers frozen at random initialization, relative ranking of hyperparameter configurations remains strongly correlated with their performance when training full model.
- **Mechanism:** Frozen layers provide "random features" for later trainable layers. Randomly-initialized deep networks possess powerful inductive bias sufficient to distinguish good HPs from bad ones, even if absolute performance is lower.
- **Core assumption:** Task's underlying structure is compatible with inductive biases of random features, meaning ranking of HP quality is preserved, not just absolute loss.
- **Evidence anchors:** [abstract] "...preserving rank correlations between hyperparameters at low fidelities..."; [Section 3.2] cites prior work showing "randomly-initialized DL models can extract powerful features even without training"; [Section 4.3, Fig. 4] Spearman's rank correlation approaching 1.0 with as few as 40% of layers trainable.

### Mechanism 3
- **Claim:** Freezing layers reduces computational cost by shortening backward pass and eliminating optimizer updates for frozen parameters.
- **Mechanism:** Gradient computation and application are restricted to last z layers. No gradients are backpropagated through frozen layers, and no optimizer steps are performed on their weights.
- **Core assumption:** Overhead of managing frozen/trainable split is negligible compared to compute saved by avoiding backward pass and optimizer step for frozen layers.
- **Evidence anchors:** [Section 3.2, Fig. 1] "lower compute due to a shorter backward path... no optimizer state needs to be kept in memory..."; [Section 4.2, Fig. 2] empirical results show monotonic decrease in runtime and memory as trainable layers decrease.

## Foundational Learning

- **Concept: Multi-Fidelity Hyperparameter Optimization (MF-HPO)**
  - **Why needed here:** The entire paper is framed as introducing new fidelity source for MF-HPO. Must understand that MF-HPO uses cheaper, lower-fidelity evaluations to quickly screen HP configurations, promoting only promising ones to expensive, high-fidelity evaluations.
  - **Quick check question:** What is the primary goal of a "fidelity source" in MF-HPO?

- **Concept: Spearman's Rank Correlation Coefficient**
  - **Why needed here:** Key metric used to validate proposed method. Paper argues method is valid because rank correlation between low-fidelity (partially frozen) and high-fidelity (fully trained) performance is high.
  - **Quick check question:** If low-fidelity evaluation has Spearman's correlation of 0.95 with full-fidelity result, what does that imply about its usefulness for HPO?

- **Concept: Fidelity Formalism (Cost & Mutual Information Monotonicity)**
  - **Why needed here:** Paper formally defines what constitutes valid fidelity. Need to understand these properties (cost should decrease with fidelity, information should increase) to see how authors justify their approach.
  - **Quick check question:** According to paper's formalism (Section 3.1), what two properties must a variable satisfy to be valid fidelity parameter?

## Architecture Onboarding

- **Component map:** Algorithm 1 (Layer-Splitting) -> RecursiveTraversal function -> Fidelity parameter z (1 to n) -> HPO Loop (uses z as fidelity dimension)
- **Critical path:** Implementing RecursiveTraversal function from Algorithm 1 on own model. This function must correctly identify all parameter-containing layers and freeze first n-z of them. U argument (optional unwrap class types) is crucial for correctly navigating non-sequential sub-modules.
- **Design tradeoffs:**
  - Granularity of layer splitting: Finer-grained control allows for smoother cost trade-offs but is more complex to implement and manage
  - Fidelity combination: Layer freezing can be used alone or combined with other fidelities like epochs
  - No checkpoint resumption: Cannot easily "continue" training from lower layer-fidelity to higher one, as would require complex unfreezing schedule
- **Failure signatures:**
  - Non-sequential architecture: Splitting algorithm will fail or be incorrect for models not fundamentally sequential
  - Rank correlation collapse: If task's rank correlation is very poor at low fidelities (<0.6), using layer freezing as fidelity will mislead HPO process
  - Low GPU utilization: Extremely low fidelity (very few trainable layers) might underutilize GPU parallelism
- **First 3 experiments:**
  1. Hardware Profiling: Run target model with different values of fidelity parameter z and measure memory usage and runtime per step
  2. Rank Correlation Check: Perform small grid search over key hyperparameters at low fidelity (e.g., z=0.4n) and full fidelity (z=n), compute Spearman's rank correlation
  3. Integrated MF-HPO Run: Integrate layer freezing fidelity into MF-HPO algorithm and run full optimization, comparing final performance and total wall-clock time against baseline

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can a principled continuation mechanism be developed to enable freeze-thaw MF-HPO with layer freezing?
- **Basis in paper:** [explicit] Page 4 states authors "consciously do not pursue this direction... and dedicate it to its own focused future work," and Page 9 lists lack of such mechanism as limitation
- **Why unresolved:** Currently, increasing fidelity (unfreezing layers) changes task dynamics, requiring training from scratch; unfreezing schedules likely interact unpredictably with other hyperparameters like learning rate
- **Evidence:** An algorithm that allows training to resume seamlessly after unfreezing additional layers without resetting optimizer states or distorting hyperparameter performance rankings

### Open Question 2
- **Question:** What is the optimal algorithm for automatically discretizing architectures into layers suitable for use as fidelity parameters?
- **Basis in paper:** [explicit] Page 5 notes that while heuristics exist, "the optimal algorithm for layer splitting is left for future work"
- **Why unresolved:** Paper relies on domain knowledge and heuristics to define layer boundaries, which may not generalize to new or specialized architectures without manual tuning
- **Evidence:** An automated splitting algorithm that maximizes rank correlation or cost-efficiency trade-offs on diverse set of novel neural architectures without human intervention

### Open Question 3
- **Question:** How does frozen layer fidelity affect optimization of layer-specific hyperparameters?
- **Basis in paper:** [explicit] Page 9 states in Limitations that "The effect of layer-specific HPs on frozen layers as fidelity should be studied along with a wider benchmarking"
- **Why unresolved:** Paper demonstrates results primarily on global hyperparameters, leaving behavior of layer-wise specific configurations unknown
- **Evidence:** Comprehensive study showing whether rank correlation is preserved when tuning layer-specific hyperparameters across different fidelity levels

## Limitations
- Cannot easily continue training from lower to higher layer-fidelity due to complex unfreezing dynamics
- Performance depends on rank correlation preservation, which may not hold for tasks requiring precise early-layer feature extraction
- Limited validation to ResNet and Transformer architectures, generalizability to other architectures unknown

## Confidence

**High Confidence:** Memory and runtime savings claims are directly supported by hardware profiling experiments showing up to 3x memory reduction and 4x runtime improvement.

**Medium Confidence:** Rank correlation results show strong correlations (>0.85) for ResNet and Transformers, but evidence is limited to two architecture families and relies on assumption about random initialization preserving HP rankings.

**Low Confidence:** Paper doesn't address performance on architectures with complex skip connections or non-sequential structures, and impact on optimization convergence when unfreezing layers is unexplored.

## Next Checks

1. **Cross-architecture validation:** Test layer freezing on architectures with complex connectivity (DenseNet, U-Net) to verify mechanism holds beyond sequential models and identify failure conditions.

2. **Task diversity assessment:** Evaluate rank correlation preservation across tasks requiring different levels of early-layer feature extraction (image classification vs. fine-grained detection vs. language modeling) to determine method's scope limits.

3. **Unfreezing dynamics study:** Implement and evaluate protocol for gradually unfreezing layers during training to understand how frozenâ†’trainable transition affects convergence and whether intermediate states provide useful fidelity signals.