---
ver: rpa2
title: Enhancing Hallucination Detection through Noise Injection
arxiv_id: '2502.03799'
source_url: https://arxiv.org/abs/2502.03799
tags:
- noise
- uncertainty
- detection
- hallucination
- injection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of detecting hallucinations
  in Large Language Models (LLMs), where models generate plausible but incorrect responses.
  The authors propose a method that combines epistemic uncertainty (model uncertainty)
  with aleatoric uncertainty (data uncertainty) to improve hallucination detection.
---

# Enhancing Hallucination Detection through Noise Injection

## Quick Facts
- arXiv ID: 2502.03799
- Source URL: https://arxiv.org/abs/2502.03799
- Reference count: 17
- Primary result: Noise injection into MLP activations improves hallucination detection AUROC by 4-7% across multiple datasets and model architectures

## Executive Summary
This paper addresses the challenge of detecting hallucinations in Large Language Models by combining epistemic uncertainty (model uncertainty) with aleatoric uncertainty (data uncertainty). The authors propose a noise injection technique that perturbs hidden unit activations in intermediate layers during sampling, effectively approximating a distribution over plausible models. This approach is evaluated across multiple datasets and model architectures, showing consistent improvements in hallucination detection while maintaining model accuracy. The method demonstrates robustness across different numbers of samples, noise magnitudes, and uncertainty metrics.

## Method Summary
The method injects uniform noise into MLP activations of intermediate layers during model sampling to capture epistemic uncertainty. For each question, K samples are generated by perturbing hidden unit activations with noise magnitude α in a specified layer range [L1, L2]. The final answers are extracted from each sample, and answer entropy is computed over the K answers to serve as the hallucination detection score. The noise magnitude is selected based on validation sets, with typical values ranging from 0.01 to 0.11 depending on model and dataset. This approach complements standard sampling-based aleatoric uncertainty estimation, providing a more comprehensive uncertainty quantification for hallucination detection.

## Key Results
- Noise injection improves hallucination detection AUROC by 4-7% compared to prediction layer sampling alone
- Method maintains model accuracy without degradation across all tested configurations
- Answer entropy (entropy over final answer frequencies) proves effective for reasoning tasks
- Performance improvements are robust across different numbers of samples, noise magnitudes, and uncertainty metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Injecting uniform noise into MLP activations of intermediate layers approximates sampling from a distribution over plausible models, capturing epistemic uncertainty that standard sampling misses.
- Mechanism: Perturbing hidden activations during generation creates variability across samples that reflects model-level uncertainty (whether the model "knows" the answer) rather than just data-level randomness. Higher dispersion under perturbation indicates hallucination.
- Core assumption: The noise magnitude and layer selection create a valid surrogate distribution q(ω) that approximates the Bayesian posterior over model weights given training data.
- Evidence anchors:
  - [abstract] "By perturbing hidden unit activations during sampling, the method approximates a distribution over plausible models, allowing better estimation of model confidence."
  - [section 3.2] "We capture epistemic uncertainty through noise injection... responses exhibit greater variability when hallucinating (grey), as evidenced by higher entropy values."
  - [corpus] Weak direct support; related papers focus on alternative uncertainty metrics (effective rank, semantic energy) rather than noise injection specifically.
- Break condition: If noise magnitude is too high, accuracy degrades; if too low, no epistemic signal emerges. Lower layers require smaller magnitudes due to error propagation sensitivity (Table 5).

### Mechanism 2
- Claim: Aleatoric uncertainty (from standard sampling) and epistemic uncertainty (from noise injection) are complementary signals that together improve hallucination detection beyond either alone.
- Mechanism: Aleatoric uncertainty captures inherent data ambiguity; epistemic uncertainty captures model knowledge gaps. Their moderate correlation (Pearson r=0.58) indicates they provide non-redundant information.
- Core assumption: Hallucinations correlate with both high data uncertainty AND high model uncertainty, but these uncertainties manifest independently.
- Evidence anchors:
  - [abstract] "noise injection captures epistemic uncertainty, complementing the aleatoric uncertainty captured by standard sampling"
  - [section 3.3, Figure 3] "model performance under the two types of uncertainty is only weakly correlated, with a Pearson correlation of 0.58. This suggests that there is a positive but complementary relationship."
  - [corpus] Related papers (Semantic Energy, Effective Rank) explore alternative uncertainty decompositions but do not directly validate this complementarity claim.
- Break condition: When baseline aleatoric detection is already saturated (e.g., TriviaQA with Phi-3 at 82% AUROC), epistemic addition yields diminishing returns.

### Mechanism 3
- Claim: Answer entropy—the entropy over final answer frequencies across samples—serves as an effective hallucination detection score for reasoning tasks.
- Mechanism: For tasks with lengthy reasoning chains (GSM8K, CSQA), token-level entropy is noisy. Counting distinct final answers and computing frequency-based entropy focuses signal on what matters: answer convergence vs. divergence.
- Core assumption: Correct answers cluster around a single value; hallucinated responses scatter across wrong answers.
- Evidence anchors:
  - [section 3.1, Equation 4] "Hans(Y) = −Σ p(aj) log p(aj) where p(aj) is the empirical probability of each unique answer aj over the K final answers"
  - [Table 1] Demonstrates computation: correct answer "3" appears 67%, wrong answer "4" appears 33%, yielding entropy that reflects uncertainty.
  - [corpus] Alternative metrics (semantic entropy, lexical similarity, predictive entropy) exist; this paper shows noise injection improves all of them (Table 6), suggesting the mechanism is metric-agnostic.
- Break condition: If models produce invalid formats or if reasoning varies while answers converge spuriously, answer entropy may mislead.

## Foundational Learning

- **Aleatoric vs. Epistemic Uncertainty**
  - Why needed here: The entire premise rests on distinguishing data randomness (which sampling captures) from model ignorance (which noise injection reveals). Without this distinction, the method appears unmotivated.
  - Quick check question: Given a model trained on limited medical data, would you expect higher aleatoric or epistemic uncertainty on rare diseases?

- **Bayesian Posterior Approximation**
  - Why needed here: The paper frames noise injection as approximating p(ω|D), the posterior over model parameters. Understanding this Bayesian interpretation explains why perturbation is theoretically justified, not just a hack.
  - Quick check question: Why might uniform noise centered at pre-trained weights be a reasonable surrogate for the true posterior in large models?

- **Entropy as Uncertainty Quantification**
  - Why needed here: The detection threshold operates on entropy values. Understanding what entropy measures—and why high entropy signals low confidence—is essential for interpreting results.
  - Quick check question: If you observe answer entropy of 0.0 across 10 samples, what does this imply about model confidence?

## Architecture Onboarding

- **Component map:**
  Input prompt → Model forward pass → [Noise injection at MLP layers L1-L2] → Token sampling (temperature T) → Answer extraction → Frequency counting (K samples) → Entropy computation → Threshold comparison

- **Critical path:**
  1. Noise magnitude selection (α ∈ {0.01, 0.03, 0.05, 0.07, 0.09, 0.11}) per model/dataset via validation set
  2. Layer selection (upper third of layers is default; exact ranges in Appendix A.2)
  3. Number of samples K (10 is default; more samples improve both detection and accuracy)

- **Design tradeoffs:**
  - Noise magnitude: Higher α increases epistemic signal but risks accuracy drop. Lower layers tolerate less noise.
  - Number of samples: More samples improve AUROC but increase latency linearly (K=20 vs K=10)
  - Per-dataset vs. per-model tuning: Per-dataset optimal but expensive; per-model (e.g., α=0.05 for Llama-2-13B across all datasets) still improves over baseline

- **Failure signatures:**
  - Accuracy degradation: α too high for model/layer combination
  - Detection AUROC near 50%: Noise injection not capturing epistemic signal (check layer range, magnitude)
  - High answer extraction failures: Prompt format mismatch; verify chat templates
  - Saturated baseline (>80% AUROC without noise): Limited room for improvement; focus on other tasks

- **First 3 experiments:**
  1. **Reproduce GSM8K case study** with Llama-2-7B-chat, α=0.07, layers 20-32, K=10, T=0.5. Verify AUROC improvement from ~71.5% to ~76% per Table 2.
  2. **Ablate noise magnitude** on validation set: sweep α ∈ {0.01, 0.03, 0.05, 0.07, 0.09} and plot AUROC vs. accuracy to find Pareto frontier.
  3. **Test layer sensitivity** by injecting noise into lower (0-10), middle (10-20), and upper (20-32) layers separately, holding α constant. Confirm robustness per Table 5.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the uniform noise distribution U(0, α) optimal, or would alternative distributions (e.g., Gaussian, Laplacian) better approximate the Bayesian posterior over model parameters?
- Basis in paper: [inferred] The paper uses uniform noise without comparing against other distributions or providing theoretical justification for this choice.
- Why unresolved: No ablation study compares different noise distributions; the choice appears arbitrary rather than theoretically motivated.
- What evidence would resolve it: Systematic comparison of different noise distributions across datasets, measuring both detection AUROC and approximation quality to true Bayesian uncertainty.

### Open Question 2
- Question: Can the noise magnitude α be determined theoretically rather than through validation set tuning?
- Basis in paper: [explicit] "In practice, the noise magnitude can be selected based on the validation set" (Section 3.4); Table 4 shows varying optimal α across temperatures, suggesting context-dependence.
- Why unresolved: No principled method for setting α without empirical tuning is provided, creating deployment overhead.
- What evidence would resolve it: A theoretical framework connecting α to model scale, dataset size, or temperature; or a self-calibrating procedure that determines α without held-out data.

### Open Question 3
- Question: How does noise injection compare theoretically and empirically to established Bayesian approximation methods like MC Dropout or deep ensembles?
- Basis in paper: [inferred] The paper references MC Dropout (Gal & Ghahramani, 2016b) in Related Work but does not compare against it empirically, despite both methods perturbing model activations to capture epistemic uncertainty.
- Why unresolved: No direct comparison establishes whether noise injection is superior, equivalent, or complementary to existing Bayesian approximation techniques.
- What evidence would resolve it: Head-to-head experiments on the same benchmarks comparing noise injection, MC Dropout, and ensemble-based uncertainty estimation for hallucination detection.

## Limitations
- Method's efficacy diminishes when baseline hallucination detection performance is already high (>80% AUROC)
- Optimal noise magnitude α requires dataset-specific validation set tuning, creating computational overhead
- Layer selection strategy (upper third) lacks theoretical justification for why this range captures epistemic uncertainty most effectively

## Confidence
- **High Confidence:** The core empirical finding that noise injection improves AUROC scores by 4-7% compared to prediction layer sampling alone is well-supported by the presented results across multiple datasets and model architectures.
- **Medium Confidence:** The claim that aleatoric and epistemic uncertainties are complementary (Pearson r=0.58) is supported by correlation analysis, though the practical significance of this moderate correlation in real-world applications remains unclear.
- **Medium Confidence:** The mechanism that noise injection approximates Bayesian posterior sampling is theoretically plausible but lacks direct empirical validation showing the noise distribution actually approximates the true posterior over model weights.

## Next Checks
1. **Ablation study on noise magnitude tuning cost:** Systematically evaluate whether per-dataset optimal α values (determined via validation) are necessary, or if a single α per model (e.g., α=0.05 for Llama-2-13B across all datasets) provides comparable performance while reducing computational overhead.

2. **Generalization to domain-specific knowledge:** Test noise injection on domain-specific datasets (e.g., biomedical or legal QA) where epistemic uncertainty from limited training data is expected to be high, to validate whether the method generalizes beyond general knowledge tasks.

3. **Computational overhead analysis:** Measure and report the wall-clock time increase per query when using K=10 vs K=20 samples with noise injection, and calculate the AUROC improvement per unit time to assess practical deployment tradeoffs.