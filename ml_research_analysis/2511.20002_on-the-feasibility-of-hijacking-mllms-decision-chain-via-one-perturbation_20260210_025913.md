---
ver: rpa2
title: On the Feasibility of Hijacking MLLMs' Decision Chain via One Perturbation
arxiv_id: '2511.20002'
source_url: https://arxiv.org/abs/2511.20002
tags:
- perturbation
- semantic
- targets
- adversarial
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that a single universal perturbation can
  hijack multiple stateless decisions of multimodal large language models by acting
  as a "semantic router," directing inputs with distinct visual semantics toward predefined
  textual targets. The authors introduce the Semantic-Aware Universal Perturbation
  (SAUP) and develop the Semantic-Oriented (SORT) optimization algorithm, which leverages
  normalized space optimization and a semantic separation strategy to overcome geometric
  constraints in the latent space.
---

# On the Feasibility of Hijacking MLLMs' Decision Chain via One Perturbation

## Quick Facts
- arXiv ID: 2511.20002
- Source URL: https://arxiv.org/abs/2511.20002
- Reference count: 39
- A single adversarial perturbation can hijack multiple decisions of multimodal large language models by routing inputs with distinct visual semantics toward predefined textual targets.

## Executive Summary
This paper introduces Semantic-Aware Universal Perturbations (SAUPs) that can hijack multiple decisions of multimodal large language models (MLLMs) through a single adversarial perturbation. Unlike traditional universal adversarial perturbations that map all inputs to one target, SAUPs act as a "semantic router" that directs inputs with different visual semantics to distinct predefined textual targets. The authors develop the SORT optimization algorithm, which combines normalized space optimization and semantic separation to overcome geometric constraints in the latent space. The work demonstrates that a single adversarial frame can trigger cascading errors across an MLLM's decision chain, revealing a fundamental vulnerability in how these models process multimodal inputs.

## Method Summary
The authors introduce the Semantic-Aware Universal Perturbation (SAUP) framework to hijack multiple stateless MLLM decisions. The SORT algorithm optimizes perturbations in normalized space (δ = Ψ⁻¹(Δ)) to stabilize gradient updates and uses a hybrid loss combining cross-entropy with a margin-based separation term. The perturbation is applied as a frame or corner patch to input images, and the optimized δ produces a dominant shift in latent space that routes semantically distinct inputs to their respective target outputs. The method is evaluated on three MLLMs (Llava, Qwen, InternVL3) using the newly created RIST dataset with fine-grained semantic annotations for autonomous driving and robotics scenarios.

## Key Results
- A single adversarial frame achieves attack success rates of 93%, 77%, 61%, and 66% for controlling 2, 3, 4, and 5 distinct targets respectively
- Normalized space optimization (NSO) is critical, with ablation showing 49.5% ASR drop without it
- Frame perturbations (7,056 pixels) can handle up to 7 targets, while corner perturbations (1,600 pixels) collapse to 0% ASR at 8 targets
- Performance varies with semantic granularity and perturbation capacity constraints

## Why This Works (Mechanism)

### Mechanism 1
A single perturbation shifts all input representations toward a shared adversarial subspace through dominant shift μδ = ϕ(δ) in latent space. The first-order Taylor expansion z ≈ ϕ(δ) + Jδ·x approximates encoder behavior near δ, where the perturbation-induced shift dominates the representation. Evidence: Figure 4a shows perturbed clusters separating from clean clusters and centering around the "pure perturbation" point. Break condition: If ρmax > L (Maximum Required Expansion Ratio exceeds Lipschitz constant), no δ can simultaneously map all input pairs to their targets.

### Mechanism 2
The same perturbation exploits subtle semantic differences to route inputs to distinct targets by amplifying input differences ∥x(i) − x(j)∥₂ into larger latent separations ∥ẑ(i) − ẑ(j)∥₂. The Jacobian Jδ preserves and expands class-specific information within the adversarial subspace. Evidence: Figure 4b shows perturbed features form distinct sub-clusters corresponding to original semantic categories, and Figure 4c demonstrates alignment through diagonal dominance in confidence heatmap. Break condition: If input semantics are too similar (∥x(i) − x(j)∥₂ → 0), the required expansion ratio ρij → ∞, exceeding any finite Lipschitz bound.

### Mechanism 3
Normalized space optimization stabilizes gradient updates by reparameterizing perturbation via δ = Ψ⁻¹(Δ), avoiding pixel-space plateaus. The margin loss LMargin = E[max(0, m − ΔPcj)] explicitly separates target confidence gaps, pushing perturbed features toward target cluster centers. Evidence: Figure 10 shows "w/o NSO" loss remains high and flat while default SORT converges, and Table 1 shows 49.5% ASR drop without NSO. Break condition: If perturbation capacity is insufficient, multi-target encoding fails.

## Foundational Learning

- Concept: First-order Taylor expansion for neural network approximations
  - Why needed here: The paper decomposes encoder behavior z(c) ≈ ϕ(δ) + Jδ·x(c) using Taylor expansion around δ. Understanding local linearity assumptions is critical for interpreting why semantic deflection works.
  - Quick check question: If the encoder were highly nonlinear around δ, would the semantic deflection approximation over- or under-estimate the actual separation?

- Concept: Spectral norm and Lipschitz continuity
  - Why needed here: Theorem 3.4 bounds feasibility via the Lipschitz constant L = sup_z ∥Jz∥₂. The spectral norm of Jδ quantifies amplification capacity.
  - Quick check question: Given two vision encoders with Lipschitz constants L₁ = 10 and L₂ = 50, which would more likely permit a 5-target SAUP with fine-grained semantics?

- Concept: Universal Adversarial Perturbations (UAPs) vs. targeted attacks
  - Why needed here: SAUP extends UAP from many-to-one (all inputs → one target) to many-to-many (semantic routing). Prior art context helps distinguish the novelty.
  - Quick check question: Why does a standard targeted UAP fail to achieve semantic routing, even if it achieves high ASR?

## Architecture Onboarding

- Component map:
Input Image x(c) + Perturbation δ → Vision Encoder ϕ (e.g., CLIP ViT) → Latent Embedding z(c) ≈ ϕ(δ) + Jδ·x(c) → Backbone Decoder D (LLM) → Output Text y = D(z(c), p)

- Critical path:
1. Initialize Δ = 0 in normalized space
2. Sample batch of (image, target) pairs
3. Construct adversarial input: x_adv = Ψ(x) + Δ
4. Compute hybrid loss and backprop to Δ
5. Clip Δ within [Ψ(0), Ψ(1)] to maintain valid pixel range
6. Iterate until convergence; return δ = Ψ⁻¹(Δ)

- Design tradeoffs:
  - Frame vs. Corner constraint: Frame (7,056 pixels) offers higher capacity for multi-target encoding; Corner (1,600 pixels) is stealthier but collapses beyond 7 targets
  - Training set size: <50 images causes overfitting (100% train, ~10% test on Llava). 50+ images improves generalization significantly (95% test)
  - Number of targets: Each additional target reduces ASR (93% → 31% on Qwen from 2 → 9 targets) due to information capacity limits

- Failure signatures:
  - Loss plateau early in training: NSO not applied; switch to normalized space optimization
  - High train ASR, low test ASR: Insufficient training diversity; increase training set size and semantic coverage
  - ASR collapses as targets increase: Perturbation region too small; enlarge frame width or corner patch size
  - All targets map to same output: SSO not applied; add margin loss to enforce separation

- First 3 experiments:
1. Replicate the 2-target attack on ImageNet with Qwen using frame constraint (6px width). Verify ~93% ASR on test set with 50 training images
2. Ablate NSO: Compare optimization in [0,1] pixel space vs. normalized space. Plot loss curves and measure ASR gap
3. Test capacity limits: Systematically increase targets from 2 to 9 on Llava with both frame and corner constraints. Identify the target count where corner ASR collapses to 0%

## Open Questions the Paper Calls Out

### Open Question 1
How can Semantic-Aware Universal Perturbations (SAUPs) be adapted to remain effective under physical-world conditions, such as variable lighting, brightness changes, and print distortions? The current study is limited to a digital white-box setting to verify latent space behavior, explicitly prioritizing mechanism understanding over the noise and transformations inherent in physical deployment.

### Open Question 2
To what extent do SAUPs transfer to unseen MLLM architectures or visual encoders in a black-box setting where the adversary lacks access to model gradients? The proposed SORT algorithm relies on full model access (white-box) for gradient-based optimization; the paper does not evaluate if the "semantic routing" capability is an intrinsic property that transfers across models without this access.

### Open Question 3
Does the specific method of visual-textual alignment (e.g., full-parameter supervised fine-tuning vs. frozen encoder projection) causally determine the vulnerability of an MLLM to semantic-aware hijacking? The paper observes a correlation where "deeper alignment... may amplify the influence of adversarial perturbations," but it does not isolate the specific architectural component responsible through controlled ablation studies.

## Limitations

- Geometric feasibility constraints: The perturbation cannot achieve desired routing when Maximum Required Expansion Ratio exceeds the vision encoder's Lipschitz constant, but these bounds are not empirically validated across diverse architectures
- Evaluation scope: All experiments focus on three specific MLLM architectures, limiting generalizability to other models with different vision backbones or latent space geometries
- Capacity constraints: Frame perturbations can handle up to 7 targets while corner perturbations collapse to 0% ASR at 8 targets, suggesting fundamental information-theoretic limits not fully explored

## Confidence

**High Confidence**: Basic feasibility of single-frame universal perturbations for MLLMs is well-supported through consistent experimental results across multiple models and datasets.
**Medium Confidence**: The semantic routing mechanism's theoretical foundation is sound, but practical bounds are not fully characterized and the geometric constraints require more systematic exploration.
**Low Confidence**: Generalizability claims to arbitrary MLLM architectures and theoretical feasibility bounds are not empirically validated, with limited testing across different vision backbone designs.

## Next Checks

1. **Lipschitz Constant Validation**: Systematically measure the spectral norm ∥Jδ∥₂ for different perturbation magnitudes and semantic input pairs across all three evaluated models. Compare these measurements against the ρmax values computed from input distance distributions to verify Theorem 3.4's feasibility conditions in practice.

2. **Architecture Transferability Test**: Apply the same SAUP generation methodology to at least two additional MLLM architectures with different vision backbones (e.g., SigLIP-based and ConvNeXt-based models). Measure whether the semantic routing capability transfers or breaks down, and identify architectural features that enable/disable this vulnerability.

3. **Fine-Grained Semantic Capacity Analysis**: Create a controlled experiment with systematically varying semantic granularity (e.g., using ImageNet subclasses or synthetic semantic hierarchies) to identify the exact threshold where semantic routing fails. Measure input distance distributions and compute the corresponding ρmax values to empirically validate the geometric constraints.