---
ver: rpa2
title: 'ACT: Bridging the Gap in Code Translation through Synthetic Data Generation
  & Adaptive Training'
arxiv_id: '2507.16478'
source_url: https://arxiv.org/abs/2507.16478
tags:
- code
- numbers
- data
- test
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of improving code translation
  across programming languages by enhancing open-source large language models (LLMs)
  for automated translation. Traditional rule-based methods are inflexible and labor-intensive,
  while proprietary LLM APIs raise security and dependency concerns.
---

# ACT: Bridging the Gap in Code Translation through Synthetic Data Generation & Adaptive Training

## Quick Facts
- arXiv ID: 2507.16478
- Source URL: https://arxiv.org/abs/2507.16478
- Authors: Shreya Saxena; Siva Prasad; Zishan Ahmad; Vishal Vaddina
- Reference count: 5
- Primary result: ACT-finetuned models significantly outperform base models in Java-to-Go and C++-to-Rust translation tasks

## Executive Summary
The paper addresses the challenge of improving code translation across programming languages by enhancing open-source large language models (LLMs) for automated translation. Traditional rule-based methods are inflexible and labor-intensive, while proprietary LLM APIs raise security and dependency concerns. The authors propose ACT (Auto-Train for Code Translation), a framework that enables in-house finetuning of open-source LLMs using automatically generated synthetic data. The key innovation is ACT's synthetic data generation module, which expands seed code samples through breadth- and depth-based expansion strategies, incorporates mutation-based unit testing for functional accuracy, and validates code execution in Docker containers.

## Method Summary
The ACT framework enables in-house finetuning of open-source LLMs using automatically generated synthetic data. The core innovation is a synthetic data generation module that expands seed code samples through breadth- and depth-based expansion strategies, incorporates mutation-based unit testing for functional accuracy, and validates code execution in Docker containers. The framework includes an adaptive controller that dynamically adjusts finetuning hyperparameters and generates targeted training data based on real-time evaluation. This approach addresses the limitations of traditional rule-based methods and proprietary LLM APIs by providing a secure, scalable, and accurate code translation solution.

## Key Results
- ACT-finetuned DeepSeek-Coder-V2-Instruct model achieved pass@1 score of 0.6248 (vs 0.5348 base) for Java-to-Go translation
- Pass@5 score reached 0.7019 (vs 0.6048 base) for Java-to-Go translation
- Similar performance improvements observed for C++-to-Rust translation tasks

## Why This Works (Mechanism)
The framework's effectiveness stems from its ability to generate high-quality synthetic training data that captures the semantic and syntactic nuances of code translation. By combining breadth and depth expansion strategies with mutation-based testing and Docker validation, ACT creates a diverse and functionally accurate training corpus. The adaptive controller ensures optimal finetuning by dynamically adjusting hyperparameters based on real-time evaluation metrics, preventing overfitting and maximizing translation accuracy.

## Foundational Learning
- Synthetic data generation: Creating artificial training data that mimics real-world scenarios; needed for scaling code translation without manual annotation; quick check: verify generated code maintains functional equivalence
- Breadth/depth expansion: Two strategies for code sample augmentation; breadth adds variations, depth increases complexity; quick check: measure diversity and coverage of expanded samples
- Mutation-based testing: Introducing controlled changes to test code robustness; needed for ensuring translation accuracy; quick check: confirm mutation doesn't break core functionality
- Docker container validation: Executing code in isolated environments; needed for security and consistency; quick check: verify container isolation and resource limits
- Adaptive hyperparameter tuning: Dynamic adjustment of training parameters; needed for optimal finetuning; quick check: monitor training stability and convergence

## Architecture Onboarding
Component map: Seed Code -> Synthetic Data Generator -> Docker Validator -> Adaptive Controller -> Finetuned Model

Critical path: The synthetic data generation and validation pipeline (Seed Code → Synthetic Data Generator → Docker Validator) forms the core of ACT's effectiveness, as high-quality training data directly impacts model performance.

Design tradeoffs: The framework prioritizes security and accuracy over speed, using Docker containers for validation which adds overhead but ensures functional correctness. The adaptive controller adds complexity but optimizes training efficiency.

Failure signatures: Poor synthetic data quality manifests as translation errors or semantic drift. Container validation failures indicate code execution issues. Adaptive controller failures result in unstable training or suboptimal convergence.

First experiments:
1. Test synthetic data generation on simple code snippets to verify expansion strategies
2. Validate Docker container execution with basic test cases
3. Run adaptive controller with pre-finetuned models to assess hyperparameter adjustment

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic data generation pipeline generalizability to non-CS domain translations remains untested
- Adaptive controller effectiveness in long-term iterative training scenarios lacks validation
- Docker container validation security claims require detailed attack vector analysis

## Confidence
- **High confidence**: Synthetic data generation methodology and core components are well-documented with consistent performance improvements
- **Medium confidence**: Comparative performance claims against proprietary APIs need validation with larger, diverse codebases
- **Medium confidence**: Security benefits of in-house finetuning lack comprehensive threat modeling and real-world deployment case studies

## Next Checks
1. Test ACT framework performance on additional language pairs including Python-to-JavaScript and Go-to-Rust to assess generalizability across different programming paradigms
2. Conduct a comprehensive security audit of the Docker container validation process, including penetration testing for container escape vulnerabilities
3. Implement a long-term iterative training experiment tracking model performance degradation or improvement over multiple training cycles to validate the adaptive controller's effectiveness