---
ver: rpa2
title: Learning Model Representations Using Publicly Available Model Hubs
arxiv_id: '2510.02096'
source_url: https://arxiv.org/abs/2510.02096
tags:
- training
- scratch
- performance
- backbone
- weights
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to learn weight space representations
  directly from unstructured, publicly available neural network models on Hugging
  Face, bypassing the need for curated, homogeneous model zoos. The approach uses
  a modified encoder-decoder transformer architecture with masked loss normalization
  to handle heterogeneous architectures, datasets, and scales.
---

# Learning Model Representations Using Publicly Available Model Hubs

## Quick Facts
- arXiv ID: 2510.02096
- Source URL: https://arxiv.org/abs/2510.02096
- Reference count: 40
- Primary result: A method to learn weight space representations directly from unstructured, publicly available neural network models on Hugging Face, bypassing curated model zoos.

## Executive Summary
This paper introduces a method to learn weight space representations directly from unstructured, publicly available neural network models on Hugging Face, bypassing the need for curated, homogeneous model zoos. The approach uses a modified encoder-decoder transformer architecture with masked loss normalization to handle heterogeneous architectures, datasets, and scales. The method enables training a single, architecture-agnostic representation from diverse models, including over 171 billion parameters. The resulting representation generalizes well across architectures (e.g., ResNets, ConvNeXTs, ViTs, transformers) and datasets, often outperforming baselines trained on model zoos. It also generalizes to unseen modalities, such as generating weights for GPT-2 models. The results demonstrate that high-quality weight space representations can be learned directly from real-world, heterogeneous model collections without requiring expensive, curated datasets.

## Method Summary
The method trains a weight space representation using a modified autoencoder on 2,000 heterogeneous models from Hugging Face, totaling 171 billion parameters. The backbone is a transformer encoder-decoder (adapted from SANE) with three key innovations: Masked Loss Normalization (MLN) to handle scale variance across layers, dense tokenization to reduce padding overhead, and sinusoidal positional encodings to scale to arbitrary model depths. The training objective combines contrastive learning with MSE reconstruction. The representation is evaluated by generating weights for new architectures and measuring finetuning performance across datasets like ImageNet, CIFAR, and TinyImageNet.

## Key Results
- Training on heterogeneous HF models yields representations that outperform baselines trained on curated model zoos
- The method generalizes to unseen architectures (ResNets, ConvNeXTs, ViTs, transformers) and even unseen modalities (GPT-2 generation from vision-only training)
- Dense tokenization reduces padding from ~20% to ~0.01% without degrading reconstruction quality
- Larger backbones (16-layer transformer) significantly improve performance on larger target architectures

## Why This Works (Mechanism)

### Mechanism 1
Per-token loss normalization enables training across heterogeneous architectures without global preprocessing. Masked Loss Normalization (MLN) re-centers and rescales each token using batch-level statistics (mean/standard deviation computed only over unmasked elements) before computing reconstruction error. This removes scale-related biases across layers with vastly different weight distributions, allowing the same backbone to process architectures of varying depth and width.

### Mechanism 2
Dense tokenization reduces padding sparsity from ~20% to ~0.01%, improving memory efficiency without degrading reconstruction quality. Rather than slicing weights per outgoing channel (sparse approach), dense tokenization flattens each layer's weights entirely before partitioning into fixed-size tokens. Fewer boundary misalignments mean less zero-padding.

### Mechanism 3
Training on heterogeneous HF models yields representations that generalize across architectures and modalities, including out-of-distribution tasks (e.g., GPT-2 from vision-only training). Diversity of training samples (171B parameters across 2000 models spanning ResNets, ViTs, ConvNeXts, etc.) exposes the backbone to a broader weight-space manifold. The encoder learns structure common across architectures, as hypothesized by Dravid et al. (2023) and Huh et al. (2024), enabling transfer to unseen architectures/modalities.

## Foundational Learning

- **Concept:** Weight Space Learning (WSL)
  - **Why needed here:** The entire method treats neural network parameters as data tokens to be encoded/decoded. Understanding WSL framing explains why tokenization, normalization, and reconstruction matter.
  - **Quick check question:** Can you explain why predicting model accuracy from weights requires a learned representation rather than raw weight statistics?

- **Concept:** Transformer Autoencoders with Contrastive Learning
  - **Why needed here:** The backbone (adapted from SANE) uses encoder-decoder transformers with contrastive loss. Understanding attention over weight tokens and contrastive objectives clarifies how representations are learned.
  - **Quick check question:** How does masking (augmentation) in the contrastive loss encourage the encoder to learn robust representations?

- **Concept:** Positional Encodings for Variable-Length Sequences
  - **Why needed here:** Replacing learned embeddings with sinusoidal encodings is critical for scaling to arbitrary model sizes without exploding parameter counts.
  - **Quick check question:** Why would learned positional embeddings become impractical when processing models with 1.3B parameters?

## Architecture Onboarding

- **Component map:** Input Pipeline (HF model download → sanity checks → tokenization (dense) → positional encoding (sinusoidal, 3D: token/layer/intra-layer position)) → Encoder (Transformer (8-16 layers, 1536 dim, 8 heads) → latent Z (128 dim)) → Training Objective (Contrastive loss (augmentations: masking, noise) + MSE reconstruction with MLN) → Decoder (Symmetric transformer → reconstructed tokens → detokenize to weights) → Sampling (KDE over anchor latent vectors → sample Z → decode → new weights)

- **Critical path:** Set up HF model filtering (tags: image-classification, segmentation, etc.) → Implement dense tokenization with configurable token size (dt=230 used) → Implement MLN ensuring masked elements excluded from statistics → Train encoder-decoder with windowing (512 tokens/window from 4096-token sequences) → Validate reconstruction R² on held-out models before downstream experiments

- **Design tradeoffs:**
  - Small vs. Large backbone: ~450M params (8 layers) trains ~54h; ~900M params (16 layers) trains ~198h. Large backbone significantly better for larger target architectures (e.g., Swin-Base, ResNet-152)
  - Sparse vs. Dense tokenization: Dense 20% more memory-efficient, similar reconstruction quality. Sparse may preserve more architectural inductive bias
  - Sinusoidal vs. Learned positional encodings: Sinusoidal scales to arbitrary sequence lengths; learned requires knowing max sequence length a priori

- **Failure signatures:**
  - Generated models at random accuracy after finetuning: Likely MLN not correctly masking padding, or insufficient training tokens (<47M tokens shows near-random performance per Table 12)
  - Training instability/NaN losses: Check MLN implementation—batch statistics over all-masked tokens will cause division by zero
  - Cannot load/tokenize HF models: Sanity check pipeline failing; verify auto-class instantiation and weight extraction
  - Poor generalization to new architectures: Training set may be too homogeneous; increase diversity even if it means including lower-quality models

- **First 3 experiments:**
  1. Validate MLN: Train backbone on ResNet-18 model zoo with MLN vs. layer-wise loss normalization; compare generated model accuracy on CIFAR-10/100/TinyImageNet (Table 1 baseline)
  2. Tokenization ablation: Compare dense vs. sparse tokenization on reconstruction R² and downstream generation quality (Figure 7, Figure 8)
  3. Scale test: Generate weights for progressively larger architectures (ResNet-18 → ResNet-50 → ResNet-152) using HF-Large backbone; compare epochs-to-convergence vs. scratch training (Tables 7-8)

## Open Questions the Paper Calls Out
- To what extent does selection bias in uncurated model repositories affect the robustness and performance of learned weight space representations?
- Can a single weight space backbone effectively learn a unified representation when trained on a truly multi-modal dataset containing text, audio, and vision models?
- What are the structural causes of the "small impurities" in generated weights that result in near-random initial performance before fine-tuning?

## Limitations
- The method's reliance on Hugging Face's ecosystem introduces reproducibility challenges due to unspecified model IDs and potential API changes
- The dense tokenization approach may discard architectural inductive biases that could be important for certain downstream tasks
- Claims about modality generalization (training on vision models, generating GPT-2 weights) are demonstrated but not deeply explained mechanistically

## Confidence

- **High Confidence:** The core autoencoder architecture with MLN and dense tokenization is well-specified and the ablation studies provide strong empirical support for these design choices
- **Medium Confidence:** Claims about training on heterogeneous model zoos being superior to curated model zoos are supported by comparisons to baselines, but the paper doesn't test whether the same method applied to curated zoos would underperform
- **Low Confidence:** The assertion that shared weight-space structure enables cross-modal generalization (vision → language) lacks mechanistic explanation beyond showing it works empirically

## Next Checks

1. **MLN Implementation Verification:** Reproduce the ablation from Table 1 by training with and without MLN on a small ResNet-18 model zoo, measuring whether small layers collapse without normalization

2. **Architectural Prior Sensitivity:** Test whether sparse tokenization (preserving channel-wise structure) performs better than dense tokenization when generating architectures with strong spatial priors, such as U-Nets for segmentation tasks

3. **Generalization Boundary Test:** Attempt to generate weights for architectures with fundamentally different parameter sharing (e.g., RNNs or graph neural networks) from the same HF-Large backbone to determine the limits of cross-architecture transfer