---
ver: rpa2
title: 'StarBench: A Turn-Based RPG Benchmark for Agentic Multimodal Decision-Making
  and Information Seeking'
arxiv_id: '2510.18483'
source_url: https://arxiv.org/abs/2510.18483
tags:
- arxiv
- control
- preprint
- information
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "StarBench introduces a benchmark for testing whether vision-language\
  \ models can play real video games like humans\u2014by perceiving raw screens, deciding,\
  \ and issuing precise keyboard-mouse actions\u2014without relying on scripted APIs.\
  \ The benchmark uses Honkai: Star Rail, a complex turn-based RPG, to evaluate agents\
  \ across eight combat tasks under two regimes: (1) direct control, where agents\
  \ output low-level click-and-keyboard primitives from screenshots with no semantic\
  \ aids; and (2) tool-assisted control, where structured action tuples and optional\
  \ textualized UI observations ease grounding."
---

# StarBench: A Turn-Based RPG Benchmark for Agentic Multimodal Decision-Making and Information Seeking

## Quick Facts
- **arXiv ID:** 2510.18483
- **Source URL:** https://arxiv.org/abs/2510.18483
- **Reference count:** 40
- **Primary result:** VLMs fail at direct pixel-based game control but succeed with semantic abstractions and OCR support

## Executive Summary
StarBench introduces a benchmark for testing whether vision-language models can play real video games like humans—by perceiving raw screens, deciding, and issuing precise keyboard-mouse actions—without relying on scripted APIs. The benchmark uses Honkai: Star Rail, a complex turn-based RPG, to evaluate agents across eight combat tasks under two regimes: (1) direct control, where agents output low-level click-and-keyboard primitives from screenshots with no semantic aids; and (2) tool-assisted control, where structured action tuples and optional textualized UI observations ease grounding. A diagnostic experiment further measures when agents choose to seek brief guidance before acting. Results show that VLMs fail almost entirely in direct control, exposing a fundamental gap in pixel-to-primitive localization, but achieve high success rates in tool-assisted mode, especially with OCR support. Judicious information-seeking correlates with improved performance, establishing StarBench as a reproducible yardstick for multimodal decision-making and agentic information seeking in real-client play.

## Method Summary
StarBench evaluates VLMs playing Honkai: Star Rail combat in a real 1920×1080 client across 8 tasks under two regimes. Direct Control (DC) requires agents to output absolute screen coordinates and keypresses from raw screenshots with no semantic aids. Tool-Assisted (TA) mode provides structured action tuples (character, move, target) and optional YOLOv8 bounding boxes plus PaddleOCR text extraction. A third Ask-or-Act experiment lets agents decide whether to retrieve guidance from a LightRAG corpus before each episode. Success metrics include completion steps, reward, remaining cycles, and scores per task. Models tested include GPT-4o-mini, Claude 3.5 Sonnet, and Gemini 1.5 Flash with ReAct/Reflexion variants. DC performance is near zero, while TA with OCR often achieves 100% success; asking before play improves performance when calibrated.

## Key Results
- VLMs fail almost entirely in direct control, exposing a fundamental gap in pixel-to-primitive localization
- Structured action abstractions and OCR support enable high success rates in tool-assisted mode
- Judicious information-seeking correlates with improved performance, especially for implicit-goal tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Providing structured action abstractions (tool-assisted mode) bridges the perception-to-control gap that causes VLMs to fail in raw pixel-based control.
- **Mechanism:** In Direct Control (DC), the model must map raw pixels to absolute screen coordinates (e.g., `click (960, 540)`), requiring high-precision spatial grounding. In Tool-Assisted (TA) mode, the action space is abstracted to semantic tuples $a_t = (c, m, t)$ (character, move, target). This removes the burden of pixel-level localization, allowing the VLM to operate on semantic logic (e.g., "select Character 0") rather than spatial coordinates.
- **Core assumption:** The failure in DC is primarily driven by localization errors (clicking non-UI elements) rather than an inability to understand the game rules.
- **Evidence anchors:**
  - [abstract] "VLMs fail almost entirely in direct control... but achieve high success rates in tool-assisted mode."
  - [section 5.3] "Most cases failed because VLMs selected incorrect space for targets... results[ing] in bad performance (e.g., keep using the basic attacks)."
  - [corpus] *ManagerBench* evaluates autonomous LLM actions, but StarBench specifically isolates the *spatial grounding* of those actions.
- **Break condition:** If a VLM succeeds in DC mode, or fails in TA mode despite correct tuple generation, the assumption that "grounding is the primary bottleneck" breaks.

### Mechanism 2
- **Claim:** Textualizing UI elements via OCR significantly reduces decision friction regarding action legality and timing, which bounding boxes alone cannot resolve.
- **Mechanism:** Game UIs contain dense, stylized text (HP values, skill points) that VLMs often misread natively. Injecting explicit OCR text (e.g., "Skill Points: 0") into the prompt allows the VLM to verify preconditions before emitting an action tuple, preventing illegal moves (e.g., casting a Skill with 0 points) and improving efficiency.
- **Core assumption:** The VLM can reason over the appended text to maintain internal state (e.g., tracking resource economy) better than it can infer state from raw pixels.
- **Evidence anchors:**
  - [abstract] "High success rates in tool-assisted mode, especially with OCR support."
  - [section 5.3] "Without OCR, models misread or miss small texts... leading to illegal or mistimed actions... SR drops from 100% to 62.5%."
  - [corpus] *Octopus* suggests multimodal orchestration, but StarBench isolates the specific utility of text-extraction for state validation.
- **Break condition:** If performance in TA mode remains unchanged when OCR is disabled (YOLO-only), then spatial boxes are sufficient and the text mechanism is redundant.

### Mechanism 3
- **Claim:** Strategic retrieval of external guidance ("asking") before an episode improves downstream performance by injecting missing priors about mechanics or target priorities.
- **Mechanism:** The agent enters a pre-episode state where it decides to *Act* or *Ask*. If it asks, a targeted query (via LightRAG) retrieves textual hints (mechanics, priorities) which are persisted in the context window for the entire battle. This compensates for stale or missing pre-trained knowledge in a live-service game.
- **Core assumption:** The VLM can effectively formulate a query based on uncertainty and successfully integrate the retrieved hint into its action selection policy.
- **Evidence anchors:**
  - [abstract] "Judicious information-seeking correlates with improved performance."
  - [section 5.3] "Brief guidance produces measurable uplifts... GPT-4o-mini attains the largest Effect... models ask more proactively [for] implicit-goal tasks."
  - [corpus] *Searching in Space and Time* discusses retrieval loops, supporting the general viability of memory-retrieval architectures for agents.
- **Break condition:** If asking yields generic or misleading advice that degrades performance, or if the model asks indiscriminately without calibrating to need, the mechanism introduces noise rather than value.

## Foundational Learning

- **Concept: Partially Observable Markov Decision Process (POMDP)**
  - **Why needed here:** The paper models combat as a POMDP because the VLM only sees the UI (observation), not the internal game state (latent variables like exact enemy cooldowns). Understanding POMDPs explains why the agent must infer hidden state from visual cues.
  - **Quick check question:** Can you distinguish between the *observation* (screenshot) and the *latent state* (internal HP/damage values) in the HSR environment?

- **Concept: UI Grounding & Localization**
  - **Why needed here:** The core finding is that VLMs lack "pixel-to-primitive localization." You must understand that grounding refers to mapping a semantic concept ("Attack Button") to a spatial coordinate $(x,y)$ to comprehend why the DC regime fails.
  - **Quick check question:** Why does outputting a semantic label (e.g., "Skill") succeed where outputting a coordinate $(x=100, y=200)$ fails?

- **Concept: Skill Point Economy**
  - **Why needed here:** HSR combat relies on a shared resource pool (skill points) where skills consume and basic attacks replenish. The OCR mechanism specifically aids in reading this counter to prevent illegal actions.
  - **Quick check question:** If the OCR reads "Skill Points: 0", which action tuple $(c, m, t)$ becomes invalid?

## Architecture Onboarding

- **Component map:** Environment (HSR client) -> Perception (YOLOv8 + PaddleOCR) -> Agent (VLM) -> Executor (pyautogui)

- **Critical path:**
  1. **Input:** Capture screenshot.
  2. **Perception (TA only):** Run YOLO/OCR to textify UI.
  3. **Decision:** VLM processes prompt (Screenshot + optional Text) -> Output Action (Coords or Tuple).
  4. **Execution:** Map output to OS-level click/keypress.

- **Design tradeoffs:**
  - **DC vs. TA:** DC evaluates "human-like" control but is brittle; TA evaluates strategy but relies on brittle detectors.
  - **OCR vs. No-OCR:** OCR adds latency and pipeline complexity but is proven necessary for tracking numeric resources (HP/SP).

- **Failure signatures:**
  - **DC Mode:** "Infinite steps" or SR=0% caused by clicking non-interactive areas (sky/background).
  - **TA Mode (No-OCR):** Getting stuck in loops (using Basic Attack when Skill is needed) or generating illegal actions (Skill with 0 SP).

- **First 3 experiments:**
  1. **DC Baseline:** Run a strong VLM in DC mode on "Echo of War" to confirm SR=0% and diagnose localization errors (where is it clicking?).
  2. **TA Ablation:** Run TA mode with YOLO enabled but OCR *disabled* to quantify the drop in Step count and Success Rate.
  3. **Ask Calibration:** Trigger the "Ask" function on a novel task vs. a known task to see if the model calibrates its question quality based on task familiarity.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can future VLMs bridge the grounding gap to perform non-trivially in the Direct Control regime without external semantic aids?
- **Basis in paper:** [explicit] The authors conclude that "Current VLMs fail almost entirely in direct control, revealing a core deficiency in pixel-to-primitive localization."
- **Why unresolved:** The paper serves as a benchmark establishing a baseline failure; it does not propose architectural or training solutions to fix the localization deficit.
- **Evidence:** A model achieving >20% success rate on Echo of War tasks using only raw screenshots and OS primitives.

### Open Question 2
- **Question:** How can agents learn to dynamically calibrate the "ask-or-act" decision to maximize efficiency rather than asking indiscriminately?
- **Basis in paper:** [inferred] While Gemini 1.5 Flash asked frequently (97.5% Ask Rate), it achieved very low Efficiency (6.1), suggesting models currently lack a mechanism to weigh the cost of asking against the benefit.
- **Why unresolved:** The benchmark measures the *outcome* of asking, but the paper does not explore methods for agents to learn an optimal policy for *when* to seek help.
- **Evidence:** An agent that iteratively improves its Efficiency score by modulating its Ask Rate based on decision uncertainty or past failure logs.

### Open Question 3
- **Question:** Will the proposed companion emulator alter the relative performance ranking of VLMs by removing OS-level noise?
- **Basis in paper:** [explicit] The limitations section notes the live client has "OS-level input flakiness" and states, "We are developing a companion emulator... [to] allow frozen UI states."
- **Why unresolved:** The emulator is currently under development; it is unknown if deterministic inputs will significantly boost success rates or simply reduce variance.
- **Evidence:** A comparative study of baseline models run on the live client versus the deterministic emulator showing changes in Success Rate and Steps.

## Limitations

- Reliance on YOLOv8 and PaddleOCR introduces potential brittleness in the TA pipeline
- The 1.14MB knowledge corpus may not capture all necessary game mechanics
- The controlled HSR environment may not generalize to other game genres with different UI layouts

## Confidence

- **High:** Core finding that VLMs cannot reliably map pixels to coordinates in DC mode
- **Medium:** Conclusion that TA abstractions effectively bridge the grounding gap
- **Medium:** Effectiveness of OCR for state tracking and action legality

## Next Checks

1. Implement DC baseline with ground truth UI coordinates logged to quantify the spatial precision gap
2. Run TA ablation study with YOLO-only (no OCR) to measure the specific contribution of text extraction to state tracking
3. Test Ask-or-Act on tasks with known vs. novel mechanics to validate whether the model truly calibrates information needs