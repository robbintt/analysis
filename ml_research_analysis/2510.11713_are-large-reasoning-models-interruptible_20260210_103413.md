---
ver: rpa2
title: Are Large Reasoning Models Interruptible?
arxiv_id: '2510.11713'
source_url: https://arxiv.org/abs/2510.11713
tags:
- reasoning
- interrupt
- road
- problem
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper examines how large reasoning models (LRMs) behave when
  their inference process is interrupted mid-thought, either to obtain a quick answer
  or to incorporate new information. The authors introduce two types of interruptions:
  time-constrained (hard: immediate answer; soft: speedup) and update-driven (new
  context mid-inference).'
---

# Are Large Reasoning Models Interruptible?

## Quick Facts
- arXiv ID: 2510.11713
- Source URL: https://arxiv.org/abs/2510.11713
- Reference count: 40
- Large reasoning models (LRMs) lose up to 60% accuracy when interrupted mid-inference; novel failure modes identified.

## Executive Summary
This paper examines how large reasoning models (LRMs) behave when their inference process is interrupted mid-thought, either to obtain a quick answer or to incorporate new information. The authors introduce two types of interruptions: time-constrained (hard: immediate answer; soft: speedup) and update-driven (new context mid-inference). Across math and coding benchmarks, static evaluations overestimate LRM robustness—accuracy can drop by up to 60% with late updates. Novel failure modes are identified: reasoning leakage (continuing to think in the answer), panic (premature termination under speedup), and self-doubt (ignoring updates). Prompt guidance mitigates self-doubt on simpler tasks but harder domains remain fragile. The work demonstrates that current LRMs are not inherently interruptible and require targeted evaluation and design for dynamic environments.

## Method Summary
The authors evaluate LRMs under two interruption scenarios: time-constrained (hard interrupt forcing immediate answer; soft interrupt requesting speedup) and update-driven (injecting new information mid-reasoning that changes the task). They use math benchmarks (GSM8K, MATH500, AIME) and LiveCodeBench-v6, evaluating at interruption positions X ∈ {0.1, 0.3, 0.5, 0.7, 0.9} relative to full reasoning trace length. For update-driven interrupts, they augment datasets with generated updates. Models tested include Qwen3 (1.7B, 8B, 32B), GPT-OSS-20B, and Magistral-S-1.2 (24B). They measure interruption-conditioned accuracy A_i(X) and output length L_i(X), classifying failures via LLM-based classifier.

## Key Results
- LRMs can lose up to 60% accuracy when interrupted at late stages (X=0.9) during reasoning.
- Reasoning leakage occurs when hard interrupts force models to relocate unfinished thinking into answer sections.
- Update-driven interrupts trigger self-doubt in 80% of update errors, with models ignoring new information.
- Prompt guidance reduces self-doubt but doesn't eliminate it, especially on harder tasks.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hard interrupts cause reasoning leakage because the model's training objective prioritizes chain-of-thought completion over instruction following.
- Mechanism: When forced to terminate via `<end-thinking>` tokens, the model relocates unfinished reasoning into the answer segment rather than abandoning it. Smaller models (1.7B) exhibit more leakage, with some generating reasoning *after* the answer before emitting an EOS token.
- Core assumption: RL training for reasoning creates a strong prior that "thinking must complete," which overrides user instructions.
- Evidence anchors:
  - [Section 4] "We refer to this phenomenon as reasoning leakage, where the model continues internal reasoning within the answer region instead of halting its thought process as instructed."
  - [Section 6.1] "This suggests that during training, the RL objective encouraging chain-of-thought thinking takes precedence even after the final answer has been generated."
  - [Corpus] Related work on "overthinking" (ConMax paper) describes similar compulsions toward extended reasoning.
- Break condition: Leakage severity may decrease if models are explicitly trained with forced-termination signals during RL.

### Mechanism 2
- Claim: Soft interrupts (speedup requests) trigger panic behavior where models terminate reasoning prematurely, mistaking urgency for a signal to answer immediately.
- Mechanism: Speedup instructions are interpreted as "answer now" rather than "compress reasoning." Over 90% of new errors under soft interrupt are classified as panic, with models closing thinking using <1% of remaining context.
- Core assumption: Models lack meta-cognitive awareness of their own reasoning state to distinguish "compress" from "abort."
- Evidence anchors:
  - [Section 4] "GPT-OSS and Qwen3 sometimes abandon their reasoning within a few tokens after the update, with up to 80% of performance loss attributable to this panic behavior."
  - [Section 1] "Panic, where under time pressure models abandon reasoning entirely and return incorrect answers."
  - [Corpus] "The Illusion of Thinking" discussion hints at reasoning cliffs where models fail to recognize task boundaries—similar failure mode, different trigger.
- Break condition: Panic rates may be reduced by training models with explicit "partial reasoning is acceptable" examples.

### Mechanism 3
- Claim: Update-driven interrupts fail due to self-doubt—models question new information rather than integrating it.
- Mechanism: When updates arrive mid-reasoning, models often second-guess whether the update is authoritative, continuing with original (now incorrect) reasoning paths. Roughly 80% of update errors stem from self-doubt.
- Core assumption: Models trained on static problems develop a prior that "problem statements are stable," causing skepticism toward in-flight changes.
- Evidence anchors:
  - [Section 5] "Models are prone to doubting whether the update is correct and continue with their original thinking process without taking into account the new updated information."
  - [Section 5] "The pathology is more pronounced in math tasks, and GPT-OSS exhibits minimal doubt behavior compared to the other models."
  - [Corpus] Weak direct corpus evidence; related work on capability boundaries (self-awareness paper) discusses models' poor calibration about when to trust vs. doubt.
- Break condition: Prompt guidance (framing updates as "verified by user") reduces but does not eliminate self-doubt, suggesting training-time exposure is also needed.

## Foundational Learning

- **Concept: Anytime Algorithms**
  - Why needed here: The paper frames interruptibility as an anytime property—models should produce monotonically better answers with more compute. Understanding anytime behavior helps interpret the "approximate anytime behavior" findings.
  - Quick check question: If you interrupt an anytime algorithm at 30% of its budget, should you expect 30% quality? What assumptions does this require?

- **Concept: Test-Time Compute Scaling**
  - Why needed here: LRMs extend reasoning at inference time. The paper's interruption experiments manipulate this compute budget dynamically, which requires understanding how models allocate compute across reasoning vs. answering.
  - Quick check question: In a standard LRM, how is the boundary between "reasoning tokens" and "answer tokens" enforced? What happens if that boundary is violated?

- **Concept: Context Drift in Interactive Systems**
  - Why needed here: The "frozen world" assumption fails when environments change mid-inference. Understanding context drift helps frame why update-driven interruptions are qualitatively different from time-constrained ones.
  - Quick check question: In a collaborative coding session, what types of changes might invalidate a model's partial reasoning? Which changes are model-responsible vs. user-responsible to communicate?

## Architecture Onboarding

- **Component map:**
  - Interruption injection layer -> Tracing wrapper -> Evaluation harness -> Dataset augmentation pipeline

- **Critical path:**
  1. Run model to completion to get full reasoning trace length R_T
  2. Determine interruption point X ∈ {0.1, 0.3, ..., 0.9}
  3. Inject interruption tokens at position ⌊X × R_T⌋
  4. Continue generation, measure accuracy and output length
  5. Classify failures (leakage/panic/doubt) via LLM-based classifier

- **Design tradeoffs:**
  - Assistant-turn vs. user-turn interruption: Assistant-turn better for models with single-thinking-block constraints (Qwen3); user-turn more natural but breaks formatting
  - Relative vs. absolute interruption positions: Relative (X% of R_T) handles variable trace lengths; absolute would require per-model calibration
  - Prompt guidance vs. no guidance: Guidance recovers 30-50% accuracy on updates but adds ~10-20% token overhead

- **Failure signatures:**
  - Reasoning leakage: Answer length 3-10× longer than oracle; thinking tokens appear in code comments or after `\boxed{}`
  - Panic: Thinking block closes within 1% of remaining context after soft interrupt; immediate wrong answer
  - Self-doubt: Output contains phrases like "Wait, but the initial problem said..." or explicit comparisons between update and original

- **First 3 experiments:**
  1. Replicate hard interrupt on GSM8K at X=0.3 and X=0.7 with Qwen3-8B; measure both accuracy and answer length ratio to verify leakage
  2. Test prompt guidance ablation on AIME: compare "no guidance" vs. "guidance" vs. "guidance + speedup" at late-stage (X=0.9) interruptions
  3. Scale sweep: run Qwen3-1.7B/8B/32B on update-driven interrupts for GSM8K; confirm that smaller models show higher doubt rates and fail to recover even on easy tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can training-based methods (as opposed to inference-time interventions) improve LRM robustness to interruptions and context updates?
- Basis in paper: [explicit] The paper concludes that "robust interruptibility is not an inherent property of most models, but rather a capability that requires dedicated evaluation and design," and the authors focus only on inference-time interventions.
- Why unresolved: The authors do not explore whether fine-tuning or specialized training objectives (e.g., reinforcement learning with interruption-aware rewards) could reduce pathologies like reasoning leakage, panic, and self-doubt at the source.
- What evidence would resolve it: Train LRMs with explicit interruption scenarios in the training distribution and compare panic, leakage, and doubt rates against inference-time-only baselines on the same benchmarks.

### Open Question 2
- Question: How do LRMs behave under noisy, adversarial, or multi-turn interruptions?
- Basis in paper: [explicit] The limitations section states: "In practice, interruptions may be noisy, adversarial, or multi-turn."
- Why unresolved: The study evaluates only single, well-defined interruptions with clean update messages. Real-world deployments may involve confusing or malicious interruption content and sequential interventions.
- What evidence would resolve it: Evaluate the same models under noisy update messages (typos, irrelevant info) and adversarial interruptions (misleading corrections), measuring accuracy degradation and failure mode incidence.

### Open Question 3
- Question: Do interruptibility pathologies generalize to domains beyond mathematics and programming?
- Basis in paper: [explicit] The authors state: "our evaluation relies primarily on math and programming tasks, which may not capture the ample diversity of real-world scenarios such as collaborative writing, planning, or open-ended dialogue."
- Why unresolved: The structural properties of math/code tasks (verifiable outputs, structured reasoning) may elicit different behaviors than open-ended or creative domains.
- What evidence would resolve it: Apply the same interruption protocols to benchmarks in planning, dialogue, or multi-document summarization and compare panic, leakage, and doubt rates.

## Limitations

- The evaluation framework assumes oracle accuracy from full reasoning traces, which may not hold for models exhibiting post-answer overfitting.
- LLM-based failure classification introduces potential bias and may be subject to the same reasoning pathologies being measured.
- Update-driven dataset augmentation via GPT-5 may not reflect realistic real-world interruptions where humans provide contradictory or noisy information.
- The study doesn't investigate whether models can recover accuracy if given additional compute after an initial interruption.

## Confidence

**High confidence**: The core empirical findings about reasoning leakage and panic behavior are well-supported by quantitative measurements and multiple independent model evaluations.

**Medium confidence**: The mechanism explanations for why models exhibit these pathologies (RL training prioritizing chain-of-thought completion, lack of meta-cognitive awareness) are plausible but not definitively proven.

**Low confidence**: The effectiveness of prompt guidance for mitigating self-doubt is based on limited experimental variation and may not generalize to more complex update scenarios.

## Next Checks

1. **Cross-task generalization test**: Evaluate interruption effects on non-math/coding domains (e.g., multi-turn dialogue, document QA) to determine if the identified failure modes are specific to reasoning-heavy tasks or represent broader interruptibility challenges.

2. **Recovery dynamics measurement**: After soft interrupts, measure whether models that initially panic can recover accuracy if given additional reasoning budget, or whether the panic state is irreversible within the same inference.

3. **Adversarial update generation**: Create updates specifically designed to trigger self-doubt (e.g., ambiguous or self-contradictory information) to establish causal links between update characteristics and failure mode prevalence, rather than correlational observations from GPT-5-generated updates.