---
ver: rpa2
title: 'MORSE-500: A Programmatically Controllable Video Benchmark to Stress-Test
  Multimodal Reasoning'
arxiv_id: '2506.05523'
source_url: https://arxiv.org/abs/2506.05523
tags:
- reasoning
- self
- video
- multimodal
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# MORSE-500: A Programmatically Controllable Video Benchmark to Stress-Test Multimodal Reasoning

## Quick Facts
- arXiv ID: 2506.05523
- Source URL: https://arxiv.org/abs/2506.05523
- Reference count: 40
- Key outcome: Current models achieve 0-5% on planning tasks while reaching 23.6% overall, revealing fundamental reasoning gaps

## Executive Summary
MORSE-500 is a programmatically generated video benchmark designed to stress-test multimodal reasoning across six cognitive categories: abstract, mathematical, physical, planning, spatial, and temporal. The benchmark's core innovation is embedding questions directly within videos, requiring models to visually extract and reason over information rather than relying on text-only shortcuts. With 500 videos spanning 3.1 hours, it provides controllable difficulty through parameterized generation, enabling systematic stress-testing of model capabilities as they improve.

## Method Summary
MORSE-500 uses Python scripts (Manim, Matplotlib, MoviePy, generative video models) to create deterministic video instances with embedded questions. The benchmark covers six reasoning categories with category-specific templates and parameterized difficulty controls. Videos are generated with controllable parameters like entity count, reasoning depth, and visual complexity. Models receive only the prompt "Answer the question in this video" and must extract the question visually before reasoning. Evaluation uses frame sampling (2 FPS, 32 max frames default) and string matching for accuracy, with human baseline at 55.4% and best model (o3) at 23.6%.

## Key Results
- SOTA models show dramatic performance variation across categories: 36.9% on mathematical vs. 3.0% on planning
- Planning tasks show near-random performance (0-5% accuracy) across all models
- Best model (o3) achieves 23.6% overall accuracy vs. human baseline of 55.4%
- Temporal processing degrades performance by 5-10% when moving from image+text to video inputs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Programmatic video generation enables scalable, controllable difficulty that avoids benchmark saturation.
- **Mechanism**: Each video instance is generated via deterministic Python scripts with parameterized controls for entity count, reasoning depth, distractor density, temporal dynamics, and visual complexity. By adjusting these parameters, new instances can be generated on-demand with precisely specified difficulty profiles.
- **Core assumption**: Models that fail on systematically varied instances of the same reasoning type share a fundamental capability gap rather than surface-level memorization issues.
- **Evidence anchors**:
  - [abstract]: "This script-driven design allows fine-grained control over visual complexity, distractor density, and temporal dynamics—enabling difficulty to be scaled systematically as models improve."
  - [section 2.1]: "Difficulty scaling operates across multiple orthogonal dimensions: structural complexity (number of entities, interaction patterns), cognitive demands (reasoning depth, abstraction level), environmental challenges (visual noise, occlusion, temporal irregularity)..."
- **Break condition**: If model performance improves rapidly across all difficulty scalings without architectural changes, the benchmark may be measuring surface pattern-matching rather than reasoning depth.

### Mechanism 2
- **Claim**: Embedding questions directly within video content forces genuine visual extraction and eliminates text-only shortcuts.
- **Mechanism**: Rather than providing questions as separate textual prompts, MORSE-500 embeds question text within the video frames themselves. Models receive only the prompt "Answer the question in this video" with no additional context, requiring them to locate, read, and interpret the question from the visual stream before reasoning.
- **Core assumption**: Models cannot rely on linguistic priors or text-only reasoning shortcuts when questions must be visually extracted.
- **Evidence anchors**:
  - [abstract]: "Questions are embedded directly within the video content, rather than provided as separate textual prompts. This ensures models must extract and reason over information grounded in visual input alone, eliminating shortcut cues."
  - [section 2.1]: "...truly vision-centric assessment where questions are embedded directly within visual content rather than provided as separate text..."
- **Break condition**: If models develop robust OCR that extracts embedded questions and then applies text-only reasoning without visual grounding, the mechanism's diagnostic value degrades.

### Mechanism 3
- **Claim**: Spanning six cognitively distinct reasoning categories reveals category-specific architectural limitations.
- **Mechanism**: The benchmark covers mathematical, abstract, spatial, temporal, physical, and planning reasoning, each grounded in cognitive science frameworks (CHC theory, dual-process theory). Category-specific performance gaps (e.g., 0-5% on planning vs. 25-35% on physical) suggest different underlying capabilities rather than a monolithic "reasoning" skill.
- **Core assumption**: Different reasoning categories require separable cognitive or architectural mechanisms; poor performance in one category does not predict performance in others.
- **Evidence anchors**:
  - [abstract]: "...neglecting the broader spectrum of reasoning skills—including abstract, physical, planning, spatial, and temporal capabilities—required for robust multimodal intelligence."
  - [section 3.2]: "Performance varies dramatically across reasoning categories, revealing systematic weaknesses in current models...Planning tasks show near-random performance across all models (0-5% accuracy), indicating a critical gap in multi-step reasoning and goal-directed behavior."
- **Break condition**: If architectural improvements yield correlated gains across all six categories simultaneously, the taxonomy may not reflect truly separable capabilities.

## Foundational Learning

- **Concept**: **Temporal memory and state tracking in video transformers**
  - **Why needed here**: The ablation study shows performance degrades when shifting from static image-text to temporal sequences, and FPS/frame count experiments reveal non-monotonic scaling—suggesting current architectures struggle to maintain coherent state representations over time.
  - **Quick check question**: Can you explain why increasing FPS from 2 to 8 and max frames from 32 to 128 did not improve overall accuracy, and what this implies about temporal information processing?

- **Concept**: **Programmatic benchmark design and parameter space exploration**
  - **Why needed here**: MORSE-500's core innovation is difficulty controllability through script parameters. Understanding how to systematically vary complexity dimensions (entity count, distractor density, temporal irregularity) is essential for creating diagnostic variants.
  - **Quick check question**: Given the difficulty presets (easy/medium/hard with maze sizes 6/10/15 and visibility ranges 3/2/1), what parameter combination would target the 50-60% difficulty range?

- **Concept**: **Multi-step compositional reasoning chains**
  - **Why needed here**: Planning tasks (0-5% accuracy) and the ARC-AGI2 examples demonstrate that even frontier models fail at chaining multiple reasoning steps while maintaining intermediate state—a prerequisite for goal-directed behavior.
  - **Quick check question**: In the Frozen Lake path selection task, what cognitive operations must a model perform to correctly identify a valid path among 5-7 options with fog-of-war visibility constraints?

## Architecture Onboarding

- **Component map**: Python scripts (Manim, Matplotlib, MoviePy) -> Task templates with difficulty parameters -> Automated validation -> Expert human evaluation -> Evaluation harness with frame sampling -> Answer extraction via LLM

- **Critical path**:
  1. Define reasoning category and task type (e.g., Planning → Maze navigation)
  2. Set difficulty parameters within category-specific bounds
  3. Generate video via deterministic script with embedded question
  4. Run automated validation (rendering artifacts, label-visual alignment)
  5. Optional: expert review for ambiguous cases
  6. Evaluate model with minimal prompt ("Answer the question in this video")
  7. Extract and verify answer

- **Design tradeoffs**:
  - **Synthetic vs. real footage**: Programmatic generation ensures reproducibility and difficulty control but may lack naturalistic complexity; MORSE-500 uses both (scripted animations + curated real footage + generative video models for physical tasks)
  - **Frame sampling strategy**: Higher FPS and more frames increase temporal information but may introduce noise; Table 4 shows FPS=2, max_frames=32 is optimal for Gemini 2.5 Flash
  - **Question embedding vs. separate prompt**: Embedded questions eliminate shortcuts but conflate OCR/reading with reasoning; separate prompts isolate reasoning but allow text-only strategies

- **Failure signatures**:
  - **Near-random planning performance (0-5%)**: Indicates inability to maintain goal state across multi-step sequences
  - **High variance across categories for same model**: Gemini 2.5 Pro scores 36.9% on mathematical but 3.0% on planning—suggests category-specific architectural gaps
  - **Temporal degradation pattern**: Performance drops from 62.4% (image+text) to 57.3% (video) on MathVista ablation—indicates temporal processing brittleness
  - **FPS saturation without gains**: Increasing from 2 FPS / 32 frames to 8 FPS / 128 frames yields no improvement—suggests context window and temporal integration as bottlenecks

- **First 3 experiments**:
  1. **Difficulty scaling validation**: Generate maze tasks with identical structure but varying maze_size (6→10→15) and visibility_range (3→2→1). Confirm that model accuracy decreases monotonically with difficulty score. This validates the parameter-to-difficulty mapping.
  2. **Category isolation test**: For a model that shows 20%+ gap between mathematical and planning performance, test whether planning-specific training (maze navigation data) closes the gap without affecting mathematical performance. This tests category separability.
  3. **Temporal memory probe**: Compare model performance on the same task presented as (a) single image with question overlay, (b) multi-image sequence, (c) full video. Map the degradation curve to identify where temporal integration fails—is it frame accumulation, temporal ordering, or state maintenance?

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What architectural mechanisms (e.g., hierarchical planning, temporal memory) are required to elevate model performance on multi-step planning tasks above the current near-random baseline?
- Basis in paper: [explicit] The authors note that "perhaps most concerning, planning tasks show near-random performance across all models (0-5% accuracy)" and that this "raises questions about the suitability of current models for autonomous decision-making."
- Why unresolved: Current SOTA models (o3, Gemini 2.5 Pro) fail specifically in the "Planning" category (e.g., mazes, rope knots) despite high performance in other areas like math, suggesting current architectures lack necessary goal-directed planning modules.
- What evidence would resolve it: Demonstrating significant accuracy gains (e.g., >20%) on MORSE-500 planning tasks using new architectures that explicitly incorporate hierarchical planning or extended temporal memory.

### Open Question 2
- Question: Does the integration of synchronized audio cues improve model performance on temporal reasoning and cause-effect identification tasks?
- Basis in paper: [explicit] The limitations section states, "Future extensions could incorporate audio cues, spoken questions, or audio-visual reasoning tasks to provide more comprehensive evaluation," noting the current focus is exclusively visual.
- Why unresolved: The current benchmark version omits audio, leaving the contribution of auditory temporal signals (e.g., "sound of a collision" vs "visual of collision") to causal inference untested in this stress-test environment.
- What evidence would resolve it: A comparative study evaluating audio-visual models (e.g., Qwen2.5-Omni) on MORSE-500 tasks with and without the newly added audio tracks.

### Open Question 3
- Question: How can evaluation frameworks effectively disentangle genuine reasoning failures from perceptual limitations in dynamic, cluttered video environments?
- Basis in paper: [explicit] The authors acknowledge that "tasks involving complex visual scenes, occlusion, or visual clutter may conflate reasoning failures with perceptual limitations, making it difficult to attribute poor performance to specific cognitive deficits."
- Why unresolved: Without a mechanism to isolate perception errors (e.g., failing to see the knot) from reasoning errors (e.g., failing to understand the knot sequence), diagnostic signals from the benchmark remain ambiguous.
- What evidence would resolve it: Ablation studies varying visual clutter (distractor density) while keeping reasoning depth constant to see if performance drops are correlated with perception thresholds rather than reasoning complexity.

## Limitations
- Synthetic generation may miss naturalistic visual complexity present in real-world video
- Focus on short videos (under 30 seconds) with limited entity interactions may underestimate sustained temporal reasoning complexity
- Evaluation relies on string matching which may not capture partial credit for near-correct reasoning chains
- Unclear whether embedded questions truly prevent text-only reasoning or simply shift challenge to OCR + text reasoning

## Confidence
- **High confidence**: Programmatic generation mechanism enabling controllable difficulty scaling is well-specified and reproducible
- **Medium confidence**: Embedded questions eliminate shortcut cues assumes models cannot robustly extract and reason over embedded text without visual grounding
- **Medium confidence**: Six-category taxonomy's claim of capturing "the full spectrum" of reasoning skills is grounded in cognitive science frameworks but may not fully represent real-world multimodal reasoning diversity

## Next Checks
1. **OCR robustness validation**: Test whether current frontier models can reliably extract embedded questions from video frames using state-of-the-art OCR, then evaluate if they can answer correctly using text-only reasoning. This validates whether visual grounding remains necessary beyond text extraction.

2. **Real-world transfer test**: Generate a small subset of MORSE-500 tasks using real-world footage (e.g., planning tasks with actual maze navigation videos) while maintaining identical difficulty parameters. Compare performance drops to identify whether synthetic generation misses critical visual complexity dimensions.

3. **Temporal integration probe**: Systematically vary frame sampling parameters (FPS from 1-8, max frames from 8-128) across all task categories and measure performance degradation curves. This will identify whether temporal integration failures are uniform across reasoning types or specific to certain cognitive operations.