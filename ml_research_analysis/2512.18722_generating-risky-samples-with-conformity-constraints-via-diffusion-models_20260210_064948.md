---
ver: rpa2
title: Generating Risky Samples with Conformity Constraints via Diffusion Models
arxiv_id: '2512.18722'
source_url: https://arxiv.org/abs/2512.18722
tags:
- samples
- generated
- risky
- conformity
- category
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating risky samples
  that deceive a target model while still conforming to the conditioned category.
  Previous methods using diffusion models for this task often struggle to maintain
  category conformity, introducing label noise and limiting effectiveness.
---

# Generating Risky Samples with Conformity Constraints via Diffusion Models

## Quick Facts
- arXiv ID: 2512.18722
- Source URL: https://arxiv.org/abs/2512.18722
- Reference count: 7
- Generates risky samples that deceive target models while maintaining category conformity

## Executive Summary
This paper addresses the challenge of generating adversarial samples that fool target classifiers while preserving category semantics. Previous diffusion-based methods often produce label noise by violating category conformity. The authors propose RiskyDiff, which incorporates both text and image embeddings as implicit constraints and adds an explicit conformity score. The method also introduces embedding screening and risky gradient guidance to increase the risk of generated samples. Extensive experiments demonstrate that RiskyDiff significantly outperforms existing methods in risk level, generation quality, and category conformity, while also improving target model generalization when used for data augmentation.

## Method Summary
RiskyDiff uses Stable-unCLIP as its backbone, taking both text and image embeddings as conditions. It estimates Gaussian statistics from validation data CLIP embeddings, screens embeddings using an error predictor MLP, and applies risky gradient guidance during DDIM sampling. The method combines a risk score (target model loss) with a conformity score (CLIP similarity) to guide the generation process. Key hyperparameters include gradient scale s=10 (ImageNet, CIFAR-100, NICO++) or s=20 (PACS) and conformity coefficient 位=0.0001.

## Key Results
- RiskyDiff generates samples with higher error rates than baseline methods while maintaining better category conformity
- Generated samples achieve lower FID scores compared to baselines when measuring against original error samples
- Models trained with RiskyDiff-generated samples show improved generalization on OOD datasets (PACS, NICO++)

## Why This Works (Mechanism)

### Mechanism 1: Dual-Stream Implicit Conditioning
Injecting both text and image embeddings as input conditions preserves category semantics better than text-only conditioning during risky generation. The architecture uses Stable-unCLIP, which accepts a CLIP image embedding alongside the text embedding. By sampling from a Gaussian distribution estimated from validation data of the target class, the generation is anchored to the class's visual manifold before risky perturbations are applied.

### Mechanism 2: Explicit Conformity Gradient Guidance
An explicit loss term based on CLIP similarity enforces semantic alignment during the diffusion denoising steps. During sampling, a conformity score (dot product of image and text embeddings) is added to the risk score. The gradient of this combined score guides the noise prediction, ensuring generated samples maintain semantic meaning of the category.

### Mechanism 3: Embedding Screening
Pre-filtering image embeddings via an error predictor increases the probability of generating risky samples without altering the diffusion model. Before generation, candidate embeddings are sampled from the class Gaussian and passed through an MLP trained on validation data to predict if the embedding corresponds to a model error. Only embeddings predicted as "risky" are passed to the diffusion model.

## Foundational Learning

- **Concept: Classifier Guidance in Diffusion** - Why needed: The core modification involves modifying noise prediction using gradients of external scores. Quick check: How does the gradient scale affect the trade-off between image quality and guidance strength?

- **Concept: CLIP Latent Space** - Why needed: The method relies on text and image embeddings in the same space serving as mutual constraints. Quick check: Does the method assume a frozen CLIP encoder or is it fine-tuned? (Assumption: Frozen)

- **Concept: Label Noise in Data Augmentation** - Why needed: The paper frames the problem as avoiding "label noise" from non-conforming adversarial examples. Quick check: Why would training on a risky sample that violates conformity degrade generalization more than standard adversarial examples?

## Architecture Onboarding

- **Component map:** CLIP Text Encoder -> CLIP Image Encoder -> Stable-unCLIP Diffusion Model -> VAE Decoder -> Target Classifier

- **Critical path:** 1) Estimate Gaussian statistics of CLIP embeddings for target class using validation data. 2) Sample and screen embeddings using Error Predictor. 3) Initiate DDIM sampling loop with gradient-guided noise prediction combining risk and conformity scores.

- **Design tradeoffs:** Gradient Scale (s) vs. Conformity Coefficient (位): Increasing s raises error rate but may degrade image quality; increasing 位 ensures semantic correctness but may lower attack success rate.

- **Failure signatures:** Mode Collapse (identical images across seeds), Semantic Drift (samples fool model but look like different class), Zero Risk (samples too simple/standard).

- **First 3 experiments:**
  1. Hyperparameter Sweep: Run generation with s=0 and 位=0 to establish baseline, then vary s to observe trade-off curve on FID and Error Rate.
  2. Conformity Validation: Human evaluation of generated samples for "cat" class to verify they are not "tigers," comparing RiskyDiff against AdvDiffuser.
  3. Generalization Test: Retrain target model on original dataset + generated risky samples and measure OOD accuracy on PACS/NICO++.

## Open Questions the Paper Calls Out
None

## Limitations
- Embedding screening mechanism relies heavily on validation data quality and may fail with small or unrepresentative datasets
- CLIP similarity as conformity proxy may not perfectly align with human perception, especially for visually similar categories
- Method effectiveness on complex domains beyond standard benchmarks remains untested

## Confidence
- **High confidence:** Dual conditioning mechanism works as described; FID score comparisons against baselines are standard and reproducible
- **Medium confidence:** Explicit conformity score via CLIP similarity improves semantic alignment, but exact contribution relative to implicit conditioning is unclear
- **Low confidence:** Embedding screening significantly boosts risk without degrading quality, as evidence for this claim is weakest

## Next Checks
1. Ablation on screening quality: Compare RiskyDiff with and without error predictor, measuring trade-off between risk and conformity to quantify screening's contribution.

2. Cross-dataset embedding generalization: Train error predictor on ImageNet validation data and test performance on CIFAR-100 or PACS embeddings to validate generalizability.

3. Human evaluation of semantic validity: Conduct user study where participants label generated "cat" images as "cat," "tiger," or "neither" to validate high CLIP similarity correlates with human-perceived category conformity.