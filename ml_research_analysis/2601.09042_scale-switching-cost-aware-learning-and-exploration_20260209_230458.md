---
ver: rpa2
title: 'SCaLE: Switching Cost aware Learning and Exploration'
arxiv_id: '2601.09042'
source_url: https://arxiv.org/abs/2601.09042
tags:
- regret
- online
- control
- cost
- scale
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work tackles the challenge of unbounded metric movement costs
  in bandit online convex optimization by proposing SCaLE, the first algorithm to
  achieve sub-linear dynamic regret in high-dimensional environments with noisy bandit
  feedback. SCaLE addresses the fundamental trade-off between exploration (necessary
  for learning the unknown cost matrix) and exploitation (necessary for minimizing
  switching costs), which is particularly challenging in rank-deficient settings where
  bandit feedback may provide no useful signal.
---

# SCaLE: Switching Cost aware Learning and Exploration

## Quick Facts
- arXiv ID: 2601.09042
- Source URL: https://arxiv.org/abs/2601.09042
- Authors: Neelkamal Bhuyan; Debankur Mukherjee; Adam Wierman
- Reference count: 40
- Primary result: First algorithm achieving sub-linear dynamic regret in high-dimensional bandit OCO with unbounded metric movement costs

## Executive Summary
This paper addresses the fundamental challenge of learning and minimizing switching costs in online convex optimization when the underlying metric matrix is unknown. The proposed SCaLE algorithm combines robust statistical estimation via trace-norm minimization with distribution-agnostic online optimal control to achieve sub-linear dynamic regret in high-dimensional environments. By decomposing regret into eigenvalue-error and eigenbasis-perturbation components, SCaLE overcomes the degeneracy of rank-deficient settings where standard bandit feedback provides no useful signal. The algorithm requires no prior knowledge of the hitting cost structure and demonstrates robustness to heavy-tailed stochastic disturbances where gradient-based methods fail.

## Method Summary
SCaLE operates in two phases: an exploration phase where carefully scaled Gaussian noise is injected to generate diverse rank-1 measurements, followed by a convex trace-norm minimization problem to estimate the unknown cost matrix $\hat{A}$; and an exploitation phase where control matrices are computed via dynamic programming based on $\hat{A}$ and applied using Lazy Aware Interpolation (LAI) structure. The algorithm achieves statistical consistency for the cost matrix estimate through sufficient exploration (requiring $m = c_1 \cdot rd$ rounds) and maintains stability through distribution-agnostic control design that avoids gradient-based methods vulnerable to heavy-tailed noise.

## Key Results
- Achieves $O(T^{2/3})$ regret in rank-deficient cases and $O(T^{1/2})$ regret in full-rank cases
- First algorithm to achieve sub-linear dynamic regret in high-dimensional bandit OCO with unbounded metric movement costs
- Extensive experiments validate robustness in heavy-tailed stochastic environments where gradient-based methods fail
- Introduces HySCaLE heuristic variant that leverages exploitation-phase feedback for improved practical performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Dedicated exploration phase with scaled Gaussian noise achieves statistical consistency for unknown cost matrix $\hat{A}$
- **Mechanism:** Injects noise $z_t$ around minimizer $v_t$ for $m$ rounds, then solves trace-norm minimization to estimate $\hat{A$, separating learning from control phases
- **Core assumption:** Cost matrix $A$ is positive semi-definite, noise $\eta_t$ is bounded, but disturbance process can be heavy-tailed
- **Evidence anchors:** Abstract describes combination of trace-norm minimization with online optimal control; Section 3.1 details exploration loop and $m = c_1 \cdot rd$ requirement
- **Break condition:** Insufficient exploration length $m$ leads to inconsistent estimation and unbounded regret

### Mechanism 2
- **Claim:** Spectral regret decomposition enables tight $O(T^{2/3})$ regret characterization in rank-deficient settings
- **Mechanism:** Separates estimation error into eigenvalue and eigenbasis components, showing eigenbasis perturbation contributes $\epsilon^2$ rather than $\epsilon$ scaling
- **Core assumption:** Estimation error $\epsilon = \|\hat{A} - A\|_*$ is small relative to spectral gap of $A$
- **Evidence anchors:** Abstract mentions novel spectral regret analysis; Section 5.2 derives $\Delta_{basis}$ terms showing $\epsilon^2$ scaling
- **Break condition:** Small spectral gap relative to noise degrades perturbation bounds and $T^{2/3}$ guarantee

### Mechanism 3
- **Claim:** LAI structure with estimated matrix remains robust to heavy-tailed disturbances without sub-Gaussian assumptions
- **Mechanism:** Computes control matrices $\hat{C}_k$ via dynamic programming based on $\hat{A$, avoiding gradient-based methods that diverge under heavy-tailed noise
- **Core assumption:** Disturbance sequence is martingale with bounded covariance but no specific distributional shape required
- **Evidence anchors:** Abstract highlights distribution-agnostic regret; Section 4.2 shows SCaLE succeeding under Cauchy noise while POL/OAL fail
- **Break condition:** Unbounded disturbance covariance or violated martingale assumption breaks LAI stability

## Foundational Learning

- **Concept: Martingale Difference Sequences**
  - **Why needed here:** Theoretical guarantees rely on environment minimizers evolving as martingale; understanding $E[v_t | \mathcal{F}_{t-1}] = v_{t-1}$ shows algorithm doesn't need future knowledge
  - **Quick check question:** Does the algorithm require the distribution of $v_t$ to be Gaussian?

- **Concept: Matrix Perturbation Theory (Davis-Kahan/Weyl)**
  - **Why needed here:** Core theoretical novelty uses bounds on eigenvector rotation when matrix is noisy; understanding gap between $\|\hat{A} - A\|_F$ and $\|\sin \Theta(P, Q)\|$ is essential
  - **Quick check question:** Why does regret scale with $\epsilon^2$ for eigenbasis errors instead of just $\epsilon$?

- **Concept: Trace-Norm Minimization (Matrix Completion)**
  - **Why needed here:** Statistical engine of paper; estimation step is nuclear-norm regularized convex program designed to recover low-rank structure from limited measurements
  - **Quick check question:** Why is trace-norm (nuclear norm) used in Equation 3.1 instead of Frobenius norm?

## Architecture Onboarding

- **Component map:** Exploration Module -> Estimator -> Planner (RecurGen) -> Controller
- **Critical path:** The Estimator; if matrix recovery fails due to insufficient exploration time $m$ or excessive noise $\bar{\eta}$, control matrices become defective leading to instability
- **Design tradeoffs:**
  - Exploration Length ($m$) vs. Horizon ($T$): Larger $m$ reduces estimation error but increases linear exploration cost; paper suggests $m \approx c_1 \cdot rd$
  - Noise Scaling ($\gamma$): High $\gamma$ improves signal-to-noise ratio for estimation but increases switching costs during exploration
- **Failure signatures:**
  - Gradient Divergence: HySCaLE or baselines in heavy-tailed noise show unbounded instantaneous regret (Figure 4)
  - Linear Regret Growth: Insufficient exploration ($m \ll rd$) causes regret to grow linearly with $T$ due to inconsistent estimation
- **First 3 experiments:**
  1. Sensitivity Analysis of Hyperparameter $c_1$: Line-search for $c_1$ against varying noise levels $\bar{\eta} \in [1, 10^4]$ to verify universality claim (Figure 6)
  2. Rank-Deficient Scaling: Generate synthetic data with $r=1, d=4$ and vary Horizon $T \in [200, 2000]$; plot Log-Regret vs. Log-T to verify $T^{2/3}$ slope (Figure 5)
  3. Heavy-Tail Robustness: Run SCaLE vs. Passive Online Learner (POL) with Cauchy-distributed disturbances to demonstrate failure of gradient-based methods vs. stability of trace-norm estimation (Figure 4)

## Open Questions the Paper Calls Out
None

## Limitations
- Trace-norm minimization sensitivity to hyper-parameter tuning in high-noise regimes requires broader validation beyond Figure 6
- Practical feasibility of required exploration length $m = c_1 \cdot rd$ for large $d$ and $r$ remains uncertain
- No analysis for non-PSD cost matrices, limiting applicability to certain real-world scenarios

## Confidence
- Theoretical regret bounds: **High** (rigorously proven)
- Experimental validation: **Medium** (limited benchmark diversity)
- Scalability claims: **Low** (experiments focus on moderate-dimensional settings)

## Next Checks
1. **Robustness to Model Misspecification:** Test SCaLE when true cost matrix $A$ is not exactly low-rank but has slowly decaying spectrum; measure how quickly regret degrades as effective rank increases
2. **Online Adaptation Without Restart:** Implement variant interleaving exploration and exploitation without full restart; evaluate if partial feedback reduces required exploration length $m$
3. **Computational Scaling:** Benchmark runtime of trace-norm estimator and LAI planner for $d > 100$ against theoretical $O(d^3)$ scaling; identify bottlenecks for practical deployment