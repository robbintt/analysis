---
ver: rpa2
title: Thinking-Free Policy Initialization Makes Distilled Reasoning Models More Effective
  and Efficient Reasoners
arxiv_id: '2509.26226'
source_url: https://arxiv.org/abs/2509.26226
tags:
- tfpi
- arxiv
- training
- reasoning
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Thinking-Free Policy Initialization (TFPI) addresses the high computational
  cost of RLVR training for reasoning models, which demands extremely long context
  lengths and leads to substantial resource usage. The core idea is to introduce a
  simple "ThinkFree" operation that explicitly discards thinking content during inference
  and training, enabling more efficient reasoning without specialized rewards or complex
  training designs.
---

# Thinking-Free Policy Initialization Makes Distilled Reasoning Models More Effective and Efficient Reasoners

## Quick Facts
- arXiv ID: 2509.26226
- Source URL: https://arxiv.org/abs/2509.26226
- Reference count: 33
- A 4B model trained with TFPI reached 89.0% accuracy on AIME24 and 65.5% on LiveCodeBench using less than 4K H20 hours

## Executive Summary
Thinking-Free Policy Initialization (TFPI) addresses the computational inefficiency of Reinforcement Learning from Verified Responses (RLVR) for reasoning models, which requires extremely long context lengths and substantial resources. The approach introduces a "ThinkFree" operation that explicitly discards thinking content during both inference and training, enabling more efficient reasoning without specialized rewards or complex training designs. TFPI serves as an initialization stage before standard RLVR, bridging long Chain-of-Thought distillation and RLVR.

The method accelerates RL convergence, achieves higher performance ceilings, and produces more token-efficient models. Experiments demonstrate that TFPI outperforms direct RL training and other efficient reasoning baselines across multiple benchmarks while reducing output token usage by 70% or more. A 4B model trained with TFPI reached 89.0% accuracy on AIME24 and 65.5% on LiveCodeBench using less than 4K H20 hours, validating the approach's effectiveness.

## Method Summary
TFPI introduces a simple "ThinkFree" operation that discards thinking content during both inference and training phases. The method serves as an initialization stage before standard RLVR, effectively bridging long Chain-of-Thought distillation and RLVR. By removing the need to process and store lengthy reasoning traces, TFPI significantly reduces computational requirements while maintaining reasoning performance. The approach eliminates the need for specialized rewards or complex training designs typically required for efficient reasoning model training.

## Key Results
- 4B model trained with TFPI reached 89.0% accuracy on AIME24 and 65.5% on LiveCodeBench
- Achieved results using less than 4K H20 hours of compute
- Reduced output token usage by 70% or more compared to baseline approaches
- Consistently outperformed direct RL training and other efficient reasoning baselines across multiple benchmarks

## Why This Works (Mechanism)
TFPI works by fundamentally changing how reasoning models process and store intermediate reasoning steps. Traditional approaches require models to generate and maintain lengthy Chain-of-Thought traces, which creates substantial computational overhead during both training and inference. By introducing the "ThinkFree" operation that explicitly discards thinking content, TFPI reduces the context length requirements and eliminates the need to process unnecessary intermediate tokens. This architectural simplification allows the model to focus computational resources on the final answer generation rather than intermediate reasoning steps, leading to faster convergence and improved token efficiency.

## Foundational Learning
- **Reinforcement Learning from Verified Responses (RLVR)**: Why needed - provides the reward signal for training reasoning models based on verified answers. Quick check - verify that the reward function properly handles the ThinkFree operation and doesn't penalize models for discarding intermediate reasoning.
- **Chain-of-Thought (CoT) Distillation**: Why needed - transfers reasoning capabilities from larger models to smaller ones. Quick check - ensure the distilled CoT patterns remain effective when intermediate steps are discarded.
- **Context Window Management**: Why needed - determines how much information the model can process simultaneously. Quick check - verify that reduced context requirements don't limit the model's ability to handle complex reasoning chains.
- **Token Efficiency Metrics**: Why needed - measures the computational and economic efficiency of the model. Quick check - confirm that token reduction correlates with actual performance improvements rather than just superficial compression.
- **Model Convergence Analysis**: Why needed - evaluates how quickly models reach optimal performance. Quick check - compare convergence rates between TFPI-initialized and directly-trained models under identical conditions.
- **Benchmark Performance Evaluation**: Why needed - quantifies model effectiveness on standardized tasks. Quick check - validate results across diverse benchmark types to ensure generalizability.

## Architecture Onboarding
- **Component Map**: Base model -> ThinkFree Operation -> RLVR Training -> Performance Evaluation
- **Critical Path**: Data preprocessing → ThinkFree application → RLVR reward computation → Parameter updates → Inference
- **Design Tradeoffs**: TFPI trades reasoning transparency for computational efficiency by discarding intermediate steps. This improves speed and reduces resource usage but may limit interpretability and could potentially impact performance on tasks requiring visible reasoning chains.
- **Failure Signatures**: Models may fail to capture complex multi-step reasoning patterns, struggle with problems requiring detailed intermediate justification, or show degraded performance on open-ended reasoning tasks where intermediate steps would normally be visible.
- **3 First Experiments**: 1) Compare token efficiency and accuracy between TFPI and standard RLVR on AIME24 benchmark. 2) Test model robustness on out-of-distribution problems to identify potential blind spots. 3) Evaluate performance degradation when forced to generate visible reasoning chains on tasks where TFPI would normally discard them.

## Open Questions the Paper Calls Out
None

## Limitations
- The long-term impact on model generalization and reasoning quality remains unclear when reasoning traces are discarded
- Insufficient evidence about performance on open-ended reasoning tasks requiring detailed intermediate steps
- The comparison baseline isn't entirely clear regarding model specifications and training duration
- Token efficiency improvements don't necessarily correlate with reasoning quality

## Confidence
- **High confidence**: Computational efficiency improvements and basic performance gains on reported benchmarks
- **Medium confidence**: TFPI as a general-purpose initialization stage due to task-specific dependencies
- **Medium confidence**: TFPI being more "effective" than alternatives given limited scope of evaluated tasks

## Next Checks
1. Test TFPI-trained models on open-ended reasoning tasks where intermediate steps would normally be visible, to assess whether performance degrades when reasoning transparency is required
2. Compare final performance of TFPI-initialized models against models trained with full Chain-of-Thought throughout, controlling for total training compute and time
3. Evaluate model robustness to adversarial or out-of-distribution problems to determine whether the thinking-free approach creates blind spots in reasoning capabilities