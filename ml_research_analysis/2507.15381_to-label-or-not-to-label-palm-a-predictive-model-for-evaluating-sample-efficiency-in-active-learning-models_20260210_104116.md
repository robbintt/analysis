---
ver: rpa2
title: 'To Label or Not to Label: PALM -- A Predictive Model for Evaluating Sample
  Efficiency in Active Learning Models'
arxiv_id: '2507.15381'
source_url: https://arxiv.org/abs/2507.15381
tags:
- learning
- accuracy
- amax
- palm
- coverage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "PALM is a mathematical model that characterizes active learning\
  \ (AL) trajectories through four interpretable parameters: achievable accuracy (Amax),\
  \ coverage efficiency (\u03B4), early-stage performance (\u03B1), and scalability\
  \ (\u03B2). It provides a predictive description of AL behavior from partial observations,\
  \ enabling estimation of future performance and principled comparisons across different\
  \ strategies."
---

# To Label or Not to Label: PALM -- A Predictive Model for Evaluating Sample Efficiency in Active Learning Models

## Quick Facts
- arXiv ID: 2507.15381
- Source URL: https://arxiv.org/abs/2507.15381
- Reference count: 40
- Primary result: PALM is a mathematical model that characterizes active learning (AL) trajectories through four interpretable parameters: achievable accuracy (Amax), coverage efficiency (δ), early-stage performance (α), and scalability (β).

## Executive Summary
PALM is a parametric mathematical model that characterizes active learning trajectories through four interpretable parameters: achievable accuracy (Amax), coverage efficiency (δ), early-stage performance (α), and scalability (β). The model provides a predictive description of AL behavior from partial observations, enabling estimation of future performance and principled comparisons across different strategies. PALM was validated through extensive experiments on CIFAR-10/100 and ImageNet-50/100/200, covering a wide range of AL methods and self-supervised embeddings, demonstrating effective generalization across datasets, budgets, and strategies.

## Method Summary
PALM models AL learning curves using the equation A = Amax[1-(1-δ)^((B/b+α)^β)], where B is cumulative labeled samples and b is batch size. The model treats labeled samples as randomly placed objects covering fraction δ of the data space, with accuracy determined by coverage probability. Four parameters capture the shape: Amax (asymptotic ceiling), δ (per-sample coverage), α (effective starting point shift), and β (growth scaling). Parameters are estimated via nonlinear least squares from sparse early observations, enabling curve extrapolation and strategy comparison. The model was validated across CIFAR-10/100 with ResNet-18 and ImageNet-50/100/200 with ResNet-50 using various AL strategies including Random, Uncertainty, Margin, Entropy, TypiClust, and DBAL, with and without self-supervised embeddings.

## Key Results
- PALM accurately predicts full AL learning curves from as few as 2% of labeled samples (1,000 samples on CIFAR-10)
- Self-supervised embeddings significantly increase coverage efficiency δ (from 0.094 to 0.535 for Margin sampling on CIFAR-10) and reduce early-stage parameter α
- MoCov3 consistently achieves the highest Amax and δ values across all strategies, indicating superior representation quality and faster learning progression

## Why This Works (Mechanism)

### Mechanism 1
Labeled samples can be modeled as randomly placed objects in feature space, where cumulative coverage probability follows a compact parametric form. The random covering problem yields EC = 1 − (1 − δ)^s for s objects covering fraction δ. This coverage maps to accuracy through contributions from covered (A_C) and uncovered regions (A_UC). Core assumption: Samples are placed independently; each covers approximately the same volume fraction δ.

### Mechanism 2
Four interpretable parameters jointly capture AL learning curve shape and can be estimated from sparse early observations. The normalized accuracy function A = A_max[1-(1-δ)^(B/b+α)^β] captures asymptotic ceiling (A_max), per-sample coverage (δ), effective starting point shift (α), and growth scaling (β). Given ≥4 accuracy observations at different budgets, nonlinear least squares yields parameter estimates enabling curve extrapolation. Core assumption: The parametric form approximates true dynamics across datasets and strategies.

### Mechanism 3
Self-supervised embeddings increase δ (coverage efficiency) and reduce α (delayed onset), accelerating learning under tight budgets. SSL methods provide semantically structured representations where each labeled sample contributes more effective coverage. Higher δ means fewer samples needed for same accuracy gain; lower α indicates better early generalization from uncovered regions. Core assumption: Embedding quality directly translates to per-sample coverage efficiency.

## Foundational Learning

**Random covering problem and coverage probability**: Why needed here - PALM's core equation derives from modeling labeled samples as objects covering a space; understanding this probabilistic foundation is necessary to interpret δ and the (1-δ)^B term. Quick check: Given δ=0.1, what is the probability a random point remains uncovered after 10 labeled samples?

**Nonlinear least squares parameter estimation**: Why needed here - PALM requires fitting 4-5 parameters from sparse accuracy observations; understanding convergence behavior and numerical stability issues with exponentiation is critical for implementation. Quick check: Why does normalizing budget B by batch size b improve numerical stability?

**Self-supervised representation learning (contrastive methods)**: Why needed here - The paper extensively evaluates how SSL embeddings affect PALM parameters; practitioners must understand what SimCLR/MoCo/BYOL provide to interpret δ differences. Quick check: Why might MoCov3 yield higher δ than BYOL on the same AL task?

## Architecture Onboarding

**Component map**: Data module (labeled pool L, unlabeled pool U, feature extractor) -> AL strategy module (query function) -> PALM estimator (nonlinear regression) -> Predictor/extrapolator (forward simulation)

**Critical path**: 1) Run AL loop for k iterations, recording (cumulative budget B_i, validation accuracy A_i) 2) Once ≥4 data points collected, invoke nonlinear least squares with bounds (δ∈[0,1], β>0, α>-B, A_max≤100) 3) Validate fit by comparing predicted vs. actual on held-out iterations 4) Use fitted model to estimate budget required for target accuracy or compare strategies

**Design tradeoffs**: Early termination vs. fit stability (fewer samples enable faster decisions but increase parameter variance); normalized vs. unnormalized budget (normalization reduces numerical overflow but requires consistent batch sizes); single-strategy vs. comparative evaluation (PALM enables direct strategy comparison but requires running multiple AL loops upfront)

**Failure signatures**: Exploding (B+α)^β term (floating-point overflow when B is large and β>1; mitigate with budget normalization); flat or near-zero δ estimates (occurs when early accuracy gains are minimal; collect more data points); oscillating parameter estimates across repetitions (high variance in AL trajectories; increase repetitions); systematic underestimation of A_max (pool exhaustion reduces late-stage gains)

**First 3 experiments**: 1) Baseline validation on CIFAR-10: Run Random sampling with ResNet-18, collect 20 accuracy points, fit PALM at 6, 10, 20 points; verify predicted curve matches full trajectory within 2% accuracy 2) Embedding ablation: Repeat experiment 1 with SimCLR embeddings; confirm δ increases (target: 0.3→0.5) and α decreases 3) Cross-dataset generalization: Fit PALM on first 10% of ImageNet-50 budget, predict performance at 4% budget across 3 AL strategies

## Open Questions the Paper Calls Out

**Open Question 1**: How can the PALM formulation be modified to account for finite unlabeled pools and the depletion of informative samples in later AL cycles? The current model assumes an effectively infinite unlabeled pool, causing estimation drift once the pool is nearly depleted.

**Open Question 2**: Does PALM generalize to dense prediction tasks such as semantic segmentation or object detection? The experimental validation is strictly limited to image classification datasets, leaving unclear if coverage efficiency (δ) and normalized accuracy function translate to tasks where label space is structural.

**Open Question 3**: Is PALM robust to the choice of model architecture, specifically Vision Transformers (ViT)? All reported experiments utilize ResNet backbones, leaving the interaction between PALM parameters and attention-based architectures unexplored.

## Limitations

- Model assumes random coverage of feature space, but real AL strategies often introduce correlations between selected samples that violate independence assumptions
- Relationship between SSL embedding quality and coverage efficiency (δ) remains empirically observed rather than theoretically grounded
- Model's extrapolation capability beyond observed budget ranges requires further validation, particularly for very slow-learning methods

## Confidence

- **High Confidence**: The four-parameter formulation's ability to capture AL trajectories when sufficient early observations are available; empirical validation across CIFAR-10/100 and ImageNet-50/100/200; effectiveness of PALM for comparing strategies via interpretable parameters
- **Medium Confidence**: The random covering assumption's validity across all AL strategies; SSL embedding's consistent impact on δ across different tasks; parameter estimation stability with very limited data (≤200 samples)
- **Low Confidence**: Long-range extrapolation accuracy beyond observed budget ranges; model behavior under severe pool exhaustion conditions; theoretical justification for coverage-probability mapping to accuracy

## Next Checks

1. **Independence Verification**: Systematically test PALM's accuracy when applied to AL strategies known to produce correlated sample selections (e.g., uncertainty sampling on imbalanced datasets) and quantify prediction degradation compared to uncorrelated sampling methods.

2. **Early-Budget Robustness**: Evaluate PALM's parameter estimation stability and predictive accuracy when fitting with only 100-200 labeled samples across all evaluated strategies, identifying which methods consistently yield unreliable fits under severe budget constraints.

3. **Theoretical Grounding**: Develop analytical bounds on the approximation error between PALM's coverage-based accuracy predictions and true AL performance under various data distribution assumptions, particularly for non-uniform feature space densities.