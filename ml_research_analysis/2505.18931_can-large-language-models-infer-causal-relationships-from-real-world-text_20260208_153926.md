---
ver: rpa2
title: Can Large Language Models Infer Causal Relationships from Real-World Text?
arxiv_id: '2505.18931'
source_url: https://arxiv.org/abs/2505.18931
tags:
- causal
- graph
- text
- node
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ReCITE, the first benchmark to evaluate causal
  reasoning from real-world text. The benchmark consists of diverse academic literature
  with complex causal graphs, varying in length, explicitness, and domain.
---

# Can Large Language Models Infer Causal Relationships from Real-World Text?

## Quick Facts
- arXiv ID: 2505.18931
- Source URL: https://arxiv.org/abs/2505.18931
- Reference count: 40
- Even best-performing model achieves only 0.535 F1 score on causal graph construction from real-world text

## Executive Summary
This paper introduces ReCITE, the first benchmark to evaluate causal reasoning from real-world text. The benchmark consists of diverse academic literature with complex causal graphs, varying in length, explicitness, and domain. Experiments with ten state-of-the-art LLMs show that even the best-performing model achieves only 0.535 F1 score, struggling particularly with implicit causal relationships. Performance degrades substantially as causal relationships become less explicit, and remains poor even when node names are provided, indicating that the primary challenge is causal reasoning itself, not entity recognition.

## Method Summary
ReCITE evaluates LLMs' ability to construct causal graphs from real-world academic text. The benchmark uses 292 academic papers containing causal loop diagrams as ground truth. Models receive preprocessed text and an expected node count, then generate JSON-formatted causal graphs. Evaluation uses an LLM-as-a-Judge (DeepSeek v3.2) to assess semantic similarity, abstraction alignment, and importance weighting rather than exact string matching. The task requires zero-shot inference on state-of-the-art models like Claude Opus 4.5 and GPT-5.2.

## Key Results
- Best model achieves only 0.535 F1 score on causal graph construction
- Performance drops by roughly half as causal relationships become less explicit
- Providing ground-truth node names improves edge F1 by only 0.037-0.042
- 85-90% of generated edges have textual support, but only 17-33% match ground truth

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model performance degrades proportionally as causal relationships become less explicit in source text.
- Mechanism: When nodes are explicitly mentioned (direct naming or synonyms), LLMs leverage pattern matching from pretraining on explicit causal language ("leads to", "causes"). When relationships are implicit or absent, this pattern-matching pathway breaks, forcing genuine inference that current architectures struggle to perform over long contexts.
- Core assumption: Explicitness score (proportion of nodes mentioned in text) captures a meaningful difficulty gradient rather than noise.
- Evidence anchors:
  - [abstract] "Performance degrades substantially as causal relationships become less explicit"
  - [section 4.3] "F1 dropping by around half for all models between the samples with lowest and highest levels of explicitness"
  - [corpus] Related work (Lee et al., CLEAR-3K) confirms causal explanatory capability remains limited, supporting the inference-gap hypothesis.
- Break condition: If models were provided retrieval access to external causal knowledge bases, explicitness dependence might diminish.

### Mechanism 2
- Claim: The primary bottleneck is causal reasoning itself, not entity recognition.
- Mechanism: Providing ground-truth node names removes the identification task entirely, yet edge-level F1 improves only marginally (+0.037–0.042 for top models) or even decreases for weaker models. This suggests that knowing *what* the causal variables are does not substantially help models determine *how* they connect.
- Core assumption: Name-assisted setting cleanly isolates reasoning from identification.
- Evidence anchors:
  - [abstract] "Performance remains poor even when node names are provided, indicating that the primary challenge is causal reasoning itself, not entity recognition"
  - [section 4.2, Table 4] Edge F1 improvements are minimal; Qwen 2.5 32B and Llama 3.1 8B actually decline.
  - [corpus] Corpus evidence on this specific decomposition is weak; related work focuses on single-task evaluation.
- Break condition: If models had explicit training on chain-of-thought causal inference patterns (not just general reasoning), the gap might narrow.

### Mechanism 3
- Claim: Models generate textually plausible but graph-incorrect causal edges, indicating they can extract "some causality" but not the "correct causality."
- Mechanism: 85–90% of generated edges have textual support, yet only 17–33% match ground-truth edges. Models successfully identify that A causally relates to B somewhere in a lengthy document, but fail to verify the specific directed relationship encoded in the target graph—suggesting a verification/alignment failure rather than complete reasoning failure.
- Core assumption: Ground-truth graphs represent authoritative causal structure; textual support without graph match reflects imprecision rather than valid alternative interpretations.
- Evidence anchors:
  - [section 4.2, Appendix O] "While 85–90% of generated edges have some textual support, only 17–33% match ground-truth edges"
  - [abstract] "Even the best-performing model achieves only 0.535 F1 score, struggling particularly with implicit causal relationships"
  - [corpus] Related benchmarks (CausalGraphBench, CLadder) use synthetic/explicit inputs, avoiding this text-graph alignment problem.
- Break condition: If evaluation allowed multiple valid causal graphs (different abstraction levels), precision might appear higher.

## Foundational Learning

- Concept: **Causal Loop Diagrams (CLDs)**
  - Why needed here: ReCITE's ground-truth graphs are drawn from system dynamics papers using CLDs—directed graphs with feedback loops where nodes represent causal variables and edges represent causal relationships.
  - Quick check question: Can you explain why a CLD with feedback cycles requires different reasoning than a directed acyclic graph (DAG)?

- Concept: **Explicitness Gradient**
  - Why needed here: The paper operationalizes explicitness as whether nodes are explicitly mentioned, implicitly inferable, or absent. This determines whether models can use extraction vs. inference.
  - Quick check question: Given text stating "Training increased. Performance improved," what explicitness level would you assign to a node labeled "skill development"?

- Concept: **LLM-as-a-Judge Evaluation**
  - Why needed here: ReCITE uses DeepSeek v3.2 to evaluate semantic similarity, abstraction alignment, and importance weighting—standard metrics fail on graph construction from text.
  - Quick check question: Why can't simple F1 against ground-truth node/edge strings work for this task?

## Architecture Onboarding

- Component map:
  - PDF -> PyMuPDF extraction -> Mistral Small (markdown conversion) -> o3-mini (normalization, graph-reference removal)
  - Academic papers with CLDs -> Human annotation (JSON edges) -> Post-processing validation (second annotator, Cohen's κ=0.987)
  - LLM response -> DeepSeek v3.2 judge -> Node/edge precision/recall with semantic + abstraction scoring -> F1, SHD

- Critical path: Text preprocessing quality determines whether models see realistic inputs; judge calibration determines whether scores are meaningful. Both require human validation.

- Design tradeoffs:
  - Providing expected node count simplifies evaluation but removes a real-world ambiguity; authors argue real tasks typically specify granularity.
  - Using LLM-as-a-Judge enables scale but introduces model-specific biases (mitigated via cross-judge agreement analysis, r=0.78–0.91).
  - Academic literature as source ensures ground-truth validity but limits domain generality (mostly STEM/management, not narrative text).

- Failure signatures:
  - **Repeating outputs**: Base models may output tokens endlessly (Qwen-2.5-7B base shown in Appendix N).
  - **High precision, low recall**: Models generate valid-looking edges that don't match ground truth (textually plausible but graph-incorrect).
  - **Direction reversals**: Rare (<1.1%) but detectable; indicates correct relationship identification but inverted causality.

- First 3 experiments:
  1. Run Claude Opus 4.5 on a 5-node explicit sample vs. a 50-node implicit sample; compare F1 gap to quantify explicitness sensitivity.
  2. Ablate node-count specification; measure variance in generated graph size and whether scores remain interpretable.
  3. Swap DeepSeek v3.2 judge for Gemini 3 Flash; verify ranking stability across 20 samples using Spearman's ρ.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can retrieval-augmented generation (RAG) or few-shot prompting significantly improve LLM performance on inferring causal relationships from real-world text?
  - Basis in paper: [explicit] The authors state: "techniques like few-shot prompting or retrieval-augmented generation may further improve performance" but only evaluate zero-shot settings.
  - Why unresolved: The paper deliberately excludes these techniques to establish baseline capabilities; their effectiveness remains unknown.
  - What evidence would resolve it: Experiments comparing zero-shot vs. few-shot vs. RAG-augmented approaches on ReCITE, with statistically significant F1 score improvements.

- **Open Question 2**: How can LLMs be trained or prompted to better handle implicit causal relationships that require genuine reasoning rather than extraction?
  - Basis in paper: [explicit] Performance degrades substantially as explicitness decreases (F1 drops by roughly half between most and least explicit samples), and this is identified as a primary challenge.
  - Why unresolved: The paper characterizes the problem but does not propose or test interventions for improving implicit causal inference.
  - What evidence would resolve it: Development of methods (architectural changes, training objectives, or prompting strategies) that narrow the performance gap between explicit and implicit samples on ReCITE.

- **Open Question 3**: Why do LLMs generate plausible causal relationships supported by text but fail to recover the specific ground-truth relationships encoded in causal graphs?
  - Basis in paper: [explicit] "85–90% of generated edges have some textual support, only 17–33% match ground-truth edges, suggesting models generate plausible causal relationships from the text but fail to recover the specific relationships encoded in the ground-truth graph."
  - Why unresolved: The paper identifies this gap but does not investigate whether it stems from ambiguity in texts, multiple valid interpretations, or genuine reasoning failures.
  - What evidence would resolve it: Analysis correlating edge prediction accuracy with textual ambiguity measures, or human studies validating whether ground-truth graphs represent uniquely correct interpretations.

- **Open Question 4**: Do the causal reasoning limitations identified on academic literature with causal loop diagrams generalize to other text genres, domains, or causal modeling paradigms?
  - Basis in paper: [explicit] The limitations section states: "Our corpus is drawn from open-access academic literature... performance on ReCITE may not fully generalize to other text genres or domains not represented in the benchmark" and "Model performance on this benchmark may not generalize to causal graphs constructed under different paradigms."
  - Why unresolved: ReCITE focuses specifically on system dynamics-style causal loop diagrams from academic papers; no evaluation on other causal modeling approaches or text sources exists.
  - What evidence would resolve it: Construction and evaluation of analogous benchmarks using other causal frameworks (e.g., Bayesian networks, structural equation models) and text sources (e.g., news, policy documents, scientific papers without explicit diagrams).

## Limitations

- **Domain Specificity**: The corpus consists primarily of academic literature (STEM/management), which may not generalize to narrative text or other real-world domains where causal reasoning is equally important.

- **LLM-as-a-Judge Validity**: The evaluation relies entirely on DeepSeek v3.2's ability to judge causal graph quality, introducing potential model-specific biases that may affect the interpretation of results.

- **Explicitness Operationalization**: The explicitness score (proportion of nodes mentioned) is a proxy measure that may not capture all relevant aspects of causal relationship clarity.

## Confidence

- **High Confidence**: Performance degradation with decreasing explicitness; causal reasoning is harder than entity recognition; models generate textually plausible but graph-incorrect edges.
- **Medium Confidence**: The 0.535 F1 score represents the true difficulty of the task; the explicitness gradient is the primary driver of performance differences.
- **Low Confidence**: The LLM-as-a-Judge evaluation is free from systematic biases; results generalize to non-academic text domains.

## Next Checks

1. **Judge Calibration Test**: Run the same 20 samples through three different LLM judges (DeepSeek v3.2, Gemini 3 Flash, Claude Opus 4.5) and measure rank correlation of F1 scores to quantify judge-specific bias.

2. **Abstraction-Level Variation**: Test whether providing different expected node counts for the same text (e.g., 10 vs 30 nodes) produces meaningfully different F1 scores, revealing if models can adapt their reasoning granularity.

3. **Cross-Domain Generalization**: Apply the benchmark to narrative text (news articles, fiction) with known causal structures and compare performance degradation to academic literature to assess domain-specific limitations.