---
ver: rpa2
title: 'Vote-in-Context: Turning VLMs into Zero-Shot Rank Fusers'
arxiv_id: '2511.01617'
source_url: https://arxiv.org/abs/2511.01617
tags:
- video
- retrieval
- fusion
- zero-shot
- such
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Vote-in-Context (ViC) is a training-free framework that uses a
  Vision-Language Model (VLM) to jointly rerank and fuse video retrieval results by
  serializing both visual content (via S-Grid) and retriever metadata into the VLM
  prompt. This enables adaptive, list-wise reasoning over heterogeneous retrievers
  without fixed fusion formulas.
---

# Vote-in-Context: Turning VLMs into Zero-Shot Rank Fusers
## Quick Facts
- **arXiv ID**: 2511.01617
- **Source URL**: https://arxiv.org/abs/2511.01617
- **Reference count**: 29
- **Primary result**: ViC achieves up to +40 points improvement in Recall@1 over prior SOTA in zero-shot video retrieval settings

## Executive Summary
Vote-in-Context (ViC) is a training-free framework that leverages Vision-Language Models (VLMs) to jointly rerank and fuse video retrieval results. By serializing both visual content (via S-Grid) and retriever metadata into the VLM prompt, ViC enables adaptive, list-wise reasoning over heterogeneous retrievers without relying on fixed fusion formulas. Evaluated across multiple datasets including MSR-VTT, DiDeMo, ActivityNet, and VATEX, ViC demonstrates dramatic improvements in single-list reranking and consistently outperforms traditional fusion baselines like CombSUM and RRF.

## Method Summary
ViC transforms VLMs into zero-shot rank fusers by serializing both visual content (via S-Grid) and retriever metadata into the VLM prompt. The framework operates by first extracting visual features from videos using S-Grid, then combining these with metadata from multiple retrievers into a structured prompt. The VLM processes this serialized information to perform list-wise ranking, effectively learning to weigh and fuse different retrieval sources without explicit training. This approach enables adaptive fusion that can handle heterogeneous retrievers and outperforms traditional score-based fusion methods.

## Key Results
- Achieves Recall@1 scores of 87.1% (t2v) / 89.0% (v2t) on MSR-VTT
- Reaches 99.6% Recall@1 (v2t) on VATEX
- Outperforms traditional fusion baselines like CombSUM and RRF by up to +40 points

## Why This Works (Mechanism)
ViC works by leveraging the inherent reasoning capabilities of VLMs to perform joint reranking and fusion. The VLM can process both visual information (via S-Grid) and metadata from multiple retrievers in context, allowing it to learn adaptive fusion weights without explicit training. This list-wise approach captures complex relationships between retrieval sources that fixed formulas cannot, while the VLM's language understanding helps resolve semantic ambiguities in video-text matching.

## Foundational Learning
- **Vision-Language Models (VLMs)**: Pre-trained models that understand both visual and textual information
  - *Why needed*: Core reasoning engine for fusion and reranking
  - *Quick check*: Verify VLM supports both image and text input in same context
- **S-Grid**: Visual feature extraction method for videos
  - *Why needed*: Converts video content into VLM-compatible format
  - *Quick check*: Confirm S-Grid produces consistent feature maps across videos
- **List-wise ranking**: Joint ranking of all candidates rather than pairwise comparisons
  - *Why needed*: Enables global optimization and context-aware decisions
  - *Quick check*: Ensure candidate list is complete before VLM processing
- **Zero-shot learning**: Performance without task-specific training
  - *Why needed*: Enables rapid deployment without costly fine-tuning
  - *Quick check*: Validate VLM prompt structure works across different datasets
- **Rank fusion**: Combining multiple ranked lists into single consensus ranking
  - *Why needed*: Leverages complementary strengths of different retrievers
  - *Quick check*: Verify all retriever outputs are properly normalized
- **Prompt engineering**: Structured input design for VLMs
  - *Why needed*: Controls how VLMs process and reason about video-text pairs
  - *Quick check*: Test prompt variations to optimize VLM performance

## Architecture Onboarding
**Component map**: Video -> S-Grid -> Visual Features -> VLM Prompt Builder -> VLM -> Ranked List

**Critical path**: Video → S-Grid extraction → Metadata collection → Prompt serialization → VLM reasoning → Final ranking

**Design tradeoffs**: ViC trades computational efficiency (VLM inference) for higher accuracy and adaptability compared to traditional fusion methods. The approach avoids expensive training but requires careful prompt engineering and VLM selection.

**Failure signatures**: Poor performance when token limits are exceeded, metadata quality is low, or VLMs cannot effectively process the serialized visual-text combination. May struggle with very long videos or highly specialized domains.

**Three first experiments**:
1. Test single retriever reranking performance to isolate VLM contribution
2. Compare different VLM models (GPT-4V, Gemini, Claude) for fusion quality
3. Evaluate prompt structure variations (metadata ordering, visual feature presentation)

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability concerns with longer video content due to token limits
- Heavy reliance on VLM's ability to process concatenated visual and metadata information
- Limited validation on diverse video domains beyond benchmark datasets

## Confidence
- **Core claim (novel zero-shot fusion mechanism)**: High - clear methodological distinction and substantial empirical improvements
- **Reproducibility and comparability**: Medium - evaluation setup provided but requires specific VLM implementations
- **Long-term robustness and efficiency at scale**: Low - lacks stress tests on very long videos or larger candidate pools

## Next Checks
1. Test ViC's performance and token efficiency on datasets with longer videos (e.g., instructional videos, movies) to assess scalability limits
2. Conduct ablation studies isolating the impact of S-Grid versus retriever metadata in the VLM prompt to quantify each component's contribution
3. Evaluate ViC's robustness on out-of-distribution video domains (e.g., user-generated content, surveillance footage) to establish generalization beyond curated benchmarks