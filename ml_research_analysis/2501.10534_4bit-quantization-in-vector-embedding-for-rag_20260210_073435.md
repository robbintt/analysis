---
ver: rpa2
title: 4bit-Quantization in Vector-Embedding for RAG
arxiv_id: '2501.10534'
source_url: https://arxiv.org/abs/2501.10534
tags:
- quantization
- vectors
- similarity
- vector
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the memory limitations of Retrieval-Augmented
  Generation (RAG) systems, which require large amounts of memory to store high-dimensional
  embedding vectors. The proposed solution is to use 4-bit quantization to reduce
  the memory requirements of these vectors while maintaining retrieval accuracy.
---

# 4bit-Quantization in Vector-Embedding for RAG

## Quick Facts
- **arXiv ID:** 2501.10534
- **Source URL:** https://arxiv.org/abs/2501.10534
- **Reference count:** 32
- **Primary result:** 4-bit quantization with group size ≤128 achieves higher retrieval accuracy than HNSW while using 8× less memory

## Executive Summary
This paper addresses the memory limitations of RAG systems by proposing 4-bit quantization for embedding vectors. The approach uses symmetric linear quantization with group-wise scaling to compress 32-bit floating point embeddings into 4-bit integers while preserving retrieval accuracy. Experiments show that 8-bit quantization maintains high accuracy with only slight degradation, and 4-bit quantization with appropriate group sizes (≤128) outperforms the state-of-the-art HNSW algorithm in retrieval accuracy. The method also demonstrates superior performance compared to Product Quantization for semantic similarity tasks.

## Method Summary
The paper employs symmetric linear quantization to compress embedding vectors from 32-bit floats to 4-bit integers. The quantization formula is `xq = Clamp(Round[x/s])` where `s = max(|x|)/(2^(b-1))`, with group-wise scaling applied for 4-bit quantization. Vectors are partitioned into groups (typically 32-128 dimensions), with each group computing its own scale factor based on local maximum values. This preserves relative distances within groups better than global scaling. The method includes dequantization via `xdq = s × xq` before similarity computation. Experiments were conducted on the dbpedia-openai-1M-1536 dataset and semantic textual similarity benchmarks, comparing against HNSW and Product Quantization baselines.

## Key Results
- 8-bit quantization maintains retrieval accuracy with only slight degradation compared to FP32
- 4-bit quantization with group size ≤128 achieves higher accuracy than HNSW (M=64, ef=50) baseline
- Pearson correlation coefficients for semantic similarity drop only ~4% for INT4 vs ~26-51% for Product Quantization

## Why This Works (Mechanism)

### Mechanism 1
Reducing embedding precision from 32-bit floats to 4-8 bit integers preserves retrieval accuracy while cutting memory by 4-8x. Symmetric linear quantization maps continuous FP32 values to discrete integer ranges using a scaling factor derived from the maximum absolute value. The dequantization process multiplies back by the same scale, introducing bounded quantization error proportional to precision reduction. Embedding vector magnitudes and directions are robust to small perturbations, preserving cosine similarity rankings even when individual component precision degrades.

### Mechanism 2
Group-wise quantization mitigates INT4 accuracy loss by allowing local scaling factors per vector segment rather than one global scale. High-dimensional vectors are partitioned into groups (e.g., size 32-128), with each group computing its own scale factor based on local maximum. This preserves relative distances within that region better than a single global scale that must accommodate the entire vector's range. Important semantic information distributed across dimensions benefits more from local precision than global precision.

### Mechanism 3
Quantized KNN can outperform approximate methods (HNSW) in retrieval accuracy while using less memory than uncompressed indices. Unlike HNSW which trades accuracy for speed via graph-based approximation, quantized exact search maintains deterministic nearest-neighbor identification. The 8x memory reduction (FP32→INT4) enables larger datasets in memory, potentially improving recall by expanding the searchable corpus. The retrieval task tolerates quantization-induced ranking perturbations better than HNSW's approximation errors.

## Foundational Learning

- **Concept:** Cosine similarity and distance preservation under quantization
  - **Why needed here:** The entire evaluation rests on whether relative angular distances between vectors survive precision reduction
  - **Quick check question:** Given vectors A=[0.1, 0.9], B=[0.11, 0.89], can a 4-bit quantizer (16 levels) distinguish their cosine similarities from each other vs. from a third vector C=[0.5, 0.5]?

- **Concept:** RMSE as proxy for retrieval quality
  - **Why needed here:** Paper uses RMSE between FP32 and quantized cosine similarities to predict downstream retrieval performance
  - **Quick check question:** If RMSE between quantized and FP32 cosine scores is 0.016 (INT4, group=128), does this guarantee top-10 retrieval overlap remains high, or could systematic biases still corrupt rankings?

- **Concept:** Memory vs. accuracy Pareto frontier in vector databases
  - **Why needed here:** Choosing between HNSW, Product Quantization, and scalar quantization requires understanding their distinct trade-off curves
  - **Quick check question:** For a 10M vector corpus with 1024-d embeddings on a 16GB RAM constraint, which method would you evaluate first: HNSW with ef=100, PQ with M=32/K=256, or INT4 with group=64?

## Architecture Onboarding

- **Component map:** Raw Documents → Embedding Model → FP32 Vectors → Quantizer (INT8/INT4 + group-wise scaling) → Quantized Vector Store → Query → Embed → Quantize (same scale params) → Cosine Similarity Search → Top-K → RAG Context

- **Critical path:**
  1. Determine acceptable accuracy degradation threshold from downstream RAG evaluation (e.g., retrieval recall@10 ≥ 0.80)
  2. Back-calculate maximum RMSE tolerance from accuracy curves
  3. Select minimum viable bit-width and group size meeting tolerance
  4. Validate group scales are stored efficiently (group_scale × num_groups overhead)

- **Design tradeoffs:**
  | Choice | Memory | Accuracy | Speed (Assumption: hardware support) |
  |--------|--------|----------|--------------------------------------|
  | FP32 + KNN | 1.0× baseline | 1.0 | Slowest (no SIMD advantage) |
  | INT8 | 0.25× | 0.98 overlap | Fast (native support) |
  | INT4 + group=32 | 0.125× | 0.67 overlap | Theoretically fastest, requires custom kernels |
  | HNSW (M=64) | ~0.3-0.5× (index overhead) | ~0.40-0.50 overlap | Very fast, approximate |
  | Product Quantization | ~0.05-0.10× | <0.10 overlap | Fast, lossy |

- **Failure signatures:**
  - Retrieval returns semantically unrelated documents: quantization collapsed distinct embeddings to same quantized representation
  - Asymmetric query-document performance: query-side quantization using different scales than index-side
  - Group-size too large for sparse distributions: RMSE spikes (Table I shows 6.7× increase from group=32 to group=256)

- **First 3 experiments:**
  1. **Baseline calibration:** Reproduce Table II results on your embedding model and document corpus. Measure FP32→INT8→INT4 retrieval overlap at group sizes [32, 64, 128, 256].
  2. **End-to-end RAG impact:** Compare RAG answer quality using FP32 vs. INT4 group=64 retrieval. Quantify hallucination rate and answer correctness.
  3. **Scale storage audit:** Implement the quantization pipeline and measure actual memory usage including scale factor overhead.

## Open Questions the Paper Calls Out

- **Question:** What is the actual speedup in search latency when using INT4 quantization compared to FP32, and how does it scale with database size?
  - **Basis in paper:** Section VI states: "To confirm the expected performance benefits, it would be essential to measure the actual impact on searching speed, which was not done in this work."
  - **Why unresolved:** The paper claims quantization "speeds up the searching process" but provides no empirical timing measurements to validate this.

- **Question:** Can INT4 quantization maintain retrieval accuracy when combined with approximate nearest neighbor algorithms like HNSW?
  - **Basis in paper:** The paper compares quantized KNN against HNSW separately but does not explore whether INT4 vectors can be indexed with HNSW graphs for further acceleration.
  - **Why unresolved:** Combining quantization with graph-based ANN could yield compound benefits, but the interaction between low-precision vectors and HNSW's navigable structure is untested.

- **Question:** How does INT4 quantization performance vary across different embedding models with varying dimensionality?
  - **Basis in paper:** Experiments used only one dataset (dbpedia with 1536-dim OpenAI embeddings) and one alternative embedding (bge-large-en-v1.5 with 1024-dim).
  - **Why unresolved:** Embedding distributions differ across models; INT4 may perform worse on models with tighter value distributions or higher dimensions.

## Limitations
- Results rely heavily on controlled synthetic datasets and a single embedding model, with generalization to diverse embedding architectures and real-world corpora untested
- Practical viability of INT4 search is constrained by current hardware limitations - without native INT4 support in mainstream frameworks like PyTorch, theoretical benefits may be offset by software overhead
- The RMSE-to-accuracy relationship established for dbpedia vectors may not generalize when embedding distributions differ significantly from the tested dataset

## Confidence

**High Confidence:** The fundamental quantization mechanism (symmetric linear quantization with group-wise scaling) is mathematically sound and the observed FP32→INT8 accuracy preservation aligns with established literature on embedding compression.

**Medium Confidence:** The claim that INT4 with group size ≤128 outperforms HNSW in retrieval accuracy is supported by the controlled dbpedia experiments, but requires validation across diverse datasets and embedding models to confirm it's not dataset-specific.

**Low Confidence:** The assertion that this approach will maintain accuracy in production RAG systems without extensive tuning, and that the RMSE thresholds translate directly to downstream task performance, needs empirical validation with real user queries and end-to-end evaluation.

## Next Checks

1. **Cross-dataset generalization test:** Reproduce the INT4 group-wise quantization on at least two additional embedding models and diverse corpora to verify the RMSE-to-accuracy relationship holds beyond dbpedia-openai-1M-1536.

2. **End-to-end RAG performance validation:** Implement the full RAG pipeline using INT4-compressed embeddings and measure not just retrieval overlap but actual answer quality through human evaluation or LLM-based judgment, comparing hallucination rates and answer correctness against FP32 baseline across multiple query types.

3. **Hardware implementation feasibility study:** Benchmark the actual runtime performance and memory usage of INT4 search on available hardware, including software decompression overhead, to determine whether the theoretical 8× memory reduction translates to practical speed improvements or if framework limitations negate the benefits.