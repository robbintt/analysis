---
ver: rpa2
title: Few-Shot Prompting for Extractive Quranic QA with Instruction-Tuned LLMs
arxiv_id: '2508.06103'
source_url: https://arxiv.org/abs/2508.06103
tags:
- answer
- question
- arabic
- quranic
- fine-tuned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the use of instruction-tuned large language
  models (LLMs) for extractive Quranic Question Answering (QA), addressing the unique
  challenges posed by the complex, semantically rich, and low-resource nature of Quranic
  Arabic. Traditional fine-tuned transformer models were compared with few-shot prompting
  approaches using LLMs like Gemini and DeepSeek, employing specialized Arabic prompt
  templates and structured post-processing pipelines to refine answer spans.
---

# Few-Shot Prompting for Extractive Quranic QA with Instruction-Tuned LLMs

## Quick Facts
- arXiv ID: 2508.06103
- Source URL: https://arxiv.org/abs/2508.06103
- Reference count: 24
- Key outcome: Instruction-tuned LLMs with few-shot prompting significantly outperform fine-tuned transformers for extractive Quranic QA, achieving pAP@10 of 0.637 with Gemini

## Executive Summary
This study investigates the application of instruction-tuned large language models (LLMs) for extractive Quranic Question Answering (QA), addressing the unique challenges of complex, semantically rich, and low-resource Quranic Arabic. The research compares traditional fine-tuned transformer models with few-shot prompting approaches using LLMs like Gemini and DeepSeek, employing specialized Arabic prompt templates and structured post-processing pipelines. Results demonstrate that instruction-tuned LLMs with Arabic instructions and few-shot prompts significantly outperform fine-tuned transformers, achieving state-of-the-art performance in this domain.

## Method Summary
The study employs a comparative approach between traditional fine-tuned transformers and few-shot prompting using instruction-tuned LLMs. The methodology involves developing specialized Arabic prompt templates tailored for Quranic text, implementing structured post-processing pipelines to refine answer spans, and evaluating performance across different LLM architectures. The experiments specifically focus on extractive QA tasks using the Quran as the target corpus, leveraging the unique linguistic characteristics of Quranic Arabic to test the effectiveness of few-shot learning approaches.

## Key Results
- Instruction-tuned LLMs with few-shot prompting achieved pAP@10 of 0.637 with Gemini
- Few-shot prompting approaches significantly outperformed traditional fine-tuned transformers
- Arabic-specific prompt templates and post-processing pipelines improved answer quality
- Results represent state-of-the-art performance for extractive Quranic QA systems

## Why This Works (Mechanism)
The success of instruction-tuned LLMs with few-shot prompting stems from their ability to leverage pre-trained knowledge while adapting to domain-specific requirements through targeted instructions. The Arabic prompt templates help bridge the gap between general language understanding and the specific linguistic features of Quranic Arabic, including its unique vocabulary, grammatical structures, and semantic nuances. The post-processing pipeline effectively refines raw model outputs into precise answer spans, addressing the extractive nature of the QA task.

## Foundational Learning

1. Quranic Arabic Linguistics
   - Why needed: Understanding unique vocabulary, grammar, and semantic structures
   - Quick check: Familiarity with classical Arabic morphological patterns

2. Few-Shot Learning Principles
   - Why needed: Enabling model adaptation with minimal examples
   - Quick check: Understanding prompt engineering and in-context learning

3. Extractive QA Task Definition
   - Why needed: Distinguishing from generative QA approaches
   - Quick check: Knowledge of answer span identification methods

4. Instruction-Tuning Concepts
   - Why needed: Preparing models for task-specific instructions
   - Quick check: Understanding alignment techniques for LLMs

5. Post-Processing Pipeline Design
   - Why needed: Refining model outputs for precise answer extraction
   - Quick check: Experience with text processing and span refinement

## Architecture Onboarding

Component Map: Prompt Templates -> LLM Processing -> Post-Processing Pipeline -> Answer Extraction

Critical Path: Question Input → Arabic Prompt Template → Instruction-Tuned LLM → Raw Answer Span → Post-Processing → Final Answer

Design Tradeoffs:
- Model selection vs. computational resources
- Prompt complexity vs. response accuracy
- Post-processing sophistication vs. latency

Failure Signatures:
- Incorrect answer spans due to ambiguous Quranic context
- Model hallucination when faced with complex theological questions
- Performance degradation with out-of-distribution Quranic passages

First Experiments:
1. Baseline evaluation with fine-tuned transformers
2. Simple few-shot prompting without specialized templates
3. Ablation study on post-processing pipeline components

## Open Questions the Paper Calls Out
None

## Limitations
- Results may not generalize to other Arabic text genres beyond Quranic Arabic
- Focus on extractive QA may not capture complex interpretive questions
- Post-processing pipeline validation and impact assessment is limited
- Comparison limited to small set of instruction-tuned LLMs (Gemini and DeepSeek)

## Confidence

High confidence:
- Superior performance of instruction-tuned LLMs with few-shot prompting vs. fine-tuned transformers
- Effectiveness of Arabic-specific prompt templates and post-processing pipelines

Medium confidence:
- Generalizability to other low-resource Arabic QA tasks
- Robustness across different LLM architectures

Low confidence:
- Ability to handle complex interpretive questions requiring deep religious understanding

## Next Checks

1. Cross-domain evaluation: Apply few-shot prompting approach to other Arabic religious texts or classical Arabic literature to assess generalizability

2. Post-processing ablation studies: Quantify individual contributions of pipeline components to answer quality and identify improvement areas

3. Expanded LLM testing: Evaluate approach with larger, more diverse set of instruction-tuned LLMs including open-source models to verify consistent performance gains