---
ver: rpa2
title: 'Chain-of-Descriptions: Improving Code LLMs for VHDL Code Generation and Summarization'
arxiv_id: '2507.12308'
source_url: https://arxiv.org/abs/2507.12308
tags:
- code
- vhdl
- generation
- llms
- summarization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Chain-of-Descriptions (CoDes), a framework
  to improve Large Language Models (LLMs) for VHDL code generation and summarization
  tasks. The authors evaluate existing code LLMs on two datasets (VHDL-Eval and VHDL-Xform)
  and find they underperform across metrics like Pass@1, self-consistency, and ROUGE-L.
---

# Chain-of-Descriptions: Improving Code LLMs for VHDL Code Generation and Summarization

## Quick Facts
- arXiv ID: 2507.12308
- Source URL: https://arxiv.org/abs/2507.12308
- Reference count: 36
- Primary result: Chain-of-Descriptions framework significantly improves VHDL code generation and summarization by LLMs

## Executive Summary
This paper addresses the challenge of using Large Language Models (LLMs) for VHDL code generation and summarization tasks, where existing models underperform. The authors introduce Chain-of-Descriptions (CoDes), a framework that generates intermediate descriptive steps based on problem statements or code, which are then integrated with the original prompt to enhance LLM outputs. Through comprehensive experiments on two newly constructed datasets (VHDL-Eval and VHDL-Xform), the framework demonstrates substantial improvements across multiple metrics including Pass@1, self-consistency, and ROUGE-L for both code generation and summarization tasks.

## Method Summary
The Chain-of-Descriptions framework works by decomposing complex VHDL tasks into intermediate descriptive steps. For code generation, it first generates descriptive steps from problem statements, then integrates these with the original prompt to guide the LLM. For code summarization, it generates descriptive steps from VHDL code itself. The framework employs multi-step execution where intermediate results inform subsequent steps, and tests both single-step and multi-step approaches. The descriptive prompts are systematically varied in length to optimize performance. The approach is evaluated against baseline LLMs using automated metrics across two specialized datasets constructed specifically for VHDL tasks.

## Key Results
- CoDes significantly improves performance across all metrics for both VHDL code generation and summarization
- Multi-step execution consistently outperforms single-step execution
- Longer descriptive prompts improve code generation results more than summarization tasks

## Why This Works (Mechanism)
The Chain-of-Descriptions framework improves LLM performance by bridging the gap between natural language problem statements and formal VHDL code through intermediate descriptive reasoning. By generating step-by-step descriptions that break down complex hardware design requirements into manageable components, the framework provides LLMs with clearer semantic context and structured guidance. This approach addresses the challenge that LLMs often struggle with domain-specific languages like VHDL due to their highly structured syntax and specialized semantics. The multi-step execution allows for iterative refinement where intermediate descriptive outputs inform subsequent generations, creating a feedback loop that progressively improves accuracy. The descriptive prompts serve as semantic scaffolding that helps LLMs better understand the intent behind hardware design specifications, leading to more accurate code generation and more faithful summarization of existing VHDL implementations.

## Foundational Learning

**VHDL (VHSIC Hardware Description Language)**: A hardware description language used to model digital systems. *Why needed*: The entire paper focuses on improving LLM performance specifically for this domain-specific language. *Quick check*: Can you identify the basic structure of a VHDL entity-architecture pair?

**Code Generation Metrics (Pass@1)**: Measures whether the first generated code sample passes all test cases. *Why needed*: Primary evaluation metric for assessing code correctness. *Quick check*: Do you understand how Pass@1 differs from metrics that evaluate multiple generated samples?

**Self-consistency**: A decoding strategy that samples multiple outputs and selects the most consistent answer through majority voting. *Why needed*: Used to evaluate and improve the reliability of LLM outputs. *Quick check*: Can you explain how self-consistency reduces hallucination in LLM outputs?

**ROUGE-L**: A metric for evaluating text summarization quality by measuring the longest common subsequence between generated and reference summaries. *Why needed*: Primary evaluation metric for summarization tasks. *Quick check*: Do you understand the difference between ROUGE-L and other ROUGE variants?

## Architecture Onboarding

**Component Map**: Problem Statement -> Descriptive Step Generator -> Integrated Prompt -> LLM -> VHDL Code/Summary

**Critical Path**: The most time-consuming path is the Descriptive Step Generator, as it must process the entire problem statement or code before LLM inference can begin. This sequential dependency means that optimizing the descriptive generation step has the largest impact on overall latency.

**Design Tradeoffs**: The framework trades computational overhead (generating intermediate descriptions) for improved accuracy. Longer descriptive prompts improve code generation but increase token costs and latency. Multi-step execution provides better results but requires more inference passes compared to single-step approaches.

**Failure Signatures**: Poor performance occurs when descriptive steps are too generic (missing domain-specific semantics), when the problem statement lacks sufficient detail, or when the LLM fails to properly integrate descriptive guidance with the original prompt. The framework may also struggle with highly complex designs that require deep hierarchical decomposition.

**First Experiments**:
1. Compare single-step vs multi-step execution performance on a simple VHDL counter design
2. Test the impact of descriptive prompt length on code generation accuracy using a basic state machine example
3. Evaluate whether descriptive steps generated from code improve summarization quality compared to direct code-to-text approaches

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions beyond the scope of the proposed framework and experiments.

## Limitations

The evaluation relies heavily on automated metrics without extensive human evaluation to validate semantic correctness and functional equivalence of generated VHDL code. The specialized datasets used for evaluation lack detailed documentation about their size, diversity, and representativeness of real-world hardware design tasks. The framework's computational overhead from generating intermediate descriptions is not quantified, which could be significant for practical deployment.

## Confidence

- **Medium**: Claims about overall performance improvements across all metrics
- **Medium**: Assertions about multi-step execution consistently outperforming single-step
- **Medium**: Conclusions about descriptive prompt length effects on code generation

## Next Checks

1. Conduct human expert evaluation to verify semantic correctness and functional equivalence of generated VHDL code beyond automated metrics
2. Perform ablation studies varying the number of intermediate description steps and prompt lengths to establish optimal configurations
3. Measure and report computational overhead (latency, token costs) of the CoDes framework compared to baseline approaches