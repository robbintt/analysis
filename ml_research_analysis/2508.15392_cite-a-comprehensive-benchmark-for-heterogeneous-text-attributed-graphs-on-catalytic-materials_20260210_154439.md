---
ver: rpa2
title: 'CITE: A Comprehensive Benchmark for Heterogeneous Text-Attributed Graphs on
  Catalytic Materials'
arxiv_id: '2508.15392'
source_url: https://arxiv.org/abs/2508.15392
tags:
- graph
- heterogeneous
- cite
- nodes
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CITE is a large heterogeneous text-attributed citation graph benchmark
  for catalytic materials, containing 438K nodes and 1.2M edges. The dataset integrates
  four node types (Papers, Authors, Journals, Keywords) and supports node classification
  tasks.
---

# CITE: A Comprehensive Benchmark for Heterogeneous Text-Attributed Graphs on Catalytic Materials

## Quick Facts
- **arXiv ID**: 2508.15392
- **Source URL**: https://arxiv.org/abs/2508.15392
- **Reference count**: 40
- **Primary result**: CITE contains 438K nodes and 1.2M edges, with heterogeneous graph models achieving Micro-F1 up to 0.9936 and Macro-F1 of 0.5655

## Executive Summary
CITE is a large-scale heterogeneous text-attributed citation graph benchmark for catalytic materials research, featuring 438,304 nodes across four types (Papers, Authors, Journals, Keywords) and 1,220,373 edges. The dataset introduces node classification tasks with 85 SCI discipline categories, revealing significant challenges in structural complexity, textual richness, and label imbalance. Experiments demonstrate that heterogeneous graph neural networks significantly outperform homogeneous alternatives, with TAPE (LLM+Graph) achieving state-of-the-art performance. The dataset exposes critical limitations in current graph learning architectures, particularly their inability to handle long-tail label distributions and effectively integrate textual and structural information.

## Method Summary
CITE integrates heterogeneous graph structures with text attributes from catalytic materials research papers. The benchmark uses BERT-base to encode paper titles and abstracts (128-token truncation) as node features, while maintaining four distinct node types and four edge types. The dataset is split 60/20/20 for training, validation, and testing. Four model paradigms are evaluated: homogeneous GNNs (GCN, GAT, GraphSAGE), heterogeneous GNNs (SimpleHGN, HGT, RGCN, etc.), LLM-centric approaches (LLaMA-7B), and LLM+Graph hybrids (TAPE, GraphGPT). Performance is measured using Micro-F1 and Macro-F1 metrics, with the latter exposing severe class imbalance issues across the 85-discipline taxonomy.

## Key Results
- Heterogeneous graph models significantly outperform homogeneous models: SimpleHGN achieves 0.9791 Micro-F1 vs GCN's 0.5057
- TAPE (LLM+Graph) achieves highest performance: 0.9936 Micro-F1 and 0.5655 Macro-F1
- Macro-F1 scores remain low across all models (0.0239-0.5655), revealing severe label imbalance
- Journal nodes are critical: their removal causes >40% performance drop in heterogeneous models
- Standalone LLMs perform poorly (0.0614 Micro-F1) but improve significantly when integrated with graph architectures

## Why This Works (Mechanism)

### Mechanism 1: Type-Aware Relational Aggregation Enables Discipline-Specific Signal Preservation
Heterogeneous architectures maintain separate transformation weights per edge type, allowing journal-paper edges to encode venue-specific disciplinary priors while author-paper edges capture collaboration patterns. When journal nodes are removed, HGT Micro-F1 drops from 0.9791 to 0.5389—a 45% degradation compared to only 0.09% drop when removing keywords. This asymmetric sensitivity indicates journal connectivity forms latent topic clusters that homogeneous isotropic aggregation cannot isolate.

### Mechanism 2: LLM-to-GNN Cascaded Decoupling Reduces Output Stochasticity
TAPE's two-stage pipeline (LLM pseudo-labels → GNN refinement) achieves robustness by treating LLM outputs as noisy priors rather than final predictions. LLaMA-7B generates probabilistic pseudo-labels from text prompts; these are injected as node features for SimpleHGN. The GNN then enforces structural consensus—correcting ambiguous "MATERIALS_SCI" predictions by reinforcing journal-specific patterns via neighborhood aggregation. This explains why TAPE achieves 0.9936 Micro-F1 while standalone LLaMA only reaches 0.0614.

### Mechanism 3: Long-Tail Label Imbalance Requires Distribution-Sensitive Architectures
The Micro-F1/Macro-F1 gap (0.9936 vs 0.5655 for TAPE) reveals that current architectures overfit majority classes while failing to model rare disciplines like "Crystallography" (0.6% of papers). Homogeneous models collapse to near-zero Macro-F1 (GCN: 0.0239) because flattening heterogeneous signals removes minority-class features. Even TAPE's 0.5655 Macro-F1 indicates only ~56% of minority classes are correctly captured.

## Foundational Learning

- **Heterogeneous Graph Neural Networks (HGT, RGCN, SimpleHGN)**:
  - Why needed here: CITE has 4 node types with distinct semantic roles. Homogeneous GNNs cannot differentiate authorship edges from citation edges, leading to semantic confusion.
  - Quick check question: Can you explain why applying the same weight matrix to author-paper and paper-journal edges would lose disciplinary signals?

- **Text-Attributed Graphs (TAGs) and LLM Integration Strategies**:
  - Why needed here: Nodes carry BERT-encoded abstracts (128 tokens). Understanding whether LLM should preprocess text (TAPE), be the final classifier (GraphGPT), or work standalone determines architecture choice.
  - Quick check question: What are the tradeoffs between using LLM for feature extraction vs using LLM for final classification?

- **Long-Tail Classification Metrics (Micro-F1 vs Macro-F1)**:
  - Why needed here: The 0.9936/0.5655 gap signals overfitting to majority classes. Macro-F1 weights each class equally, exposing minority-class failures.
  - Quick check question: If a model achieves 0.99 Micro-F1 but 0.50 Macro-F1 on a 100-class problem, what does this imply about its per-class performance distribution?

## Architecture Onboarding

- **Component map**:
  Paper nodes (127,690) → Author nodes (221,097) via Paper-Author edges (506K)
  Paper nodes (127,690) → Journal nodes (2,553) via Paper-Journal edges (126K)
  Paper nodes (127,690) → Keyword nodes (86,964) via Paper-Keywords edges (253K)
  Paper nodes (127,690) → Paper nodes (127,690) via Paper-Paper edges (335K citations)

- **Critical path**:
  1. Load CITE from HuggingFace (JSON metadata → CSV processing)
  2. Build heterogeneous graph: assign unique IDs, construct node sets VP/VA/VJ/VK
  3. Generate edges: Paper-Paper (citations), Paper-Author, Paper-Journal, Paper-Keywords
  4. Tokenize text with BERT, truncate to 128 tokens
  5. Train with 60/20/20 split, evaluate Micro-F1 and Macro-F1

- **Design tradeoffs**:
  - Homogeneous vs Heterogeneous: Homogeneous (GCN, GAT) fuses all nodes into paper features but loses journal disciplinary signals. Heterogeneous (HGT, SimpleHGN) preserves type-specific edges but adds complexity (HGT: 28M params vs GCN: 8M).
  - TAPE vs GraphGPT: TAPE (cascaded, robust, 0.9936 Micro-F1) vs GraphGPT (end-to-end, unstable outputs, 0.0214 Micro-F1). Choose TAPE for stability, GraphGPT only if tight text-graph integration is required.
  - Handling label imbalance: Standard cross-entropy overfits majority. Consider focal loss, class-weighted loss, or oversampling—but none tested in paper.

- **Failure signatures**:
  - Homogeneous models: Micro-F1 ~0.50, Macro-F1 <0.05 → indicates semantic dilution from node-type collapse
  - LLaMA standalone: Micro-F1 <0.10, generates labels outside predefined set → LLM lacks structural awareness
  - GraphGPT: Produces null responses or class name variants (e.g., "Engineering Studies" not in label set) → tight coupling compounds ambiguity
  - Journal node ablation: >40% Micro-F1 drop for HGT → confirms high-degree heterogeneous nodes are critical

- **First 3 experiments**:
  1. **Baseline establishment**: Run SimpleHGN on CITE with default hyperparameters (hidden_dim=256, num_layers=3, num_heads=8). Expect Micro-F1 ~0.99, Macro-F1 ~0.50. If Macro-F1 <0.40, check label distribution in data split.
  2. **Ablation for journal importance**: Remove Paper-Journal edges from SimpleHGN. If Micro-F1 drops >30%, confirms journal nodes are critical for this domain. Compare to removing Paper-Author edges (expect smaller drop).
  3. **LLM integration comparison**: Run TAPE (LLaMA-7B pseudo-labels → SimpleHGN) vs LLaMA-7B standalone on a 1K-paper subset. Measure output consistency across 3 runs (TAPE should show σ<0.01, LLaMA σ>0.003). If TAPE Macro-F1 <0.55, inspect pseudo-label quality for minority classes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can architectures be designed to natively unify textual and structural signals without sacrificing relational granularity in heterogeneous graphs?
- Basis in paper: The discussion explicitly states there is an "urgent need for architectures that natively unify textual and structural signals without sacrificing relational granularity" because current flattenings distort semantics.
- Why unresolved: Current models either collapse heterogeneous nodes into monolithic features (losing granularity) or use rigid message-passing that cannot fully exploit unstructured text.
- What evidence would resolve it: A model that maintains distinct edge-type semantics while processing raw text end-to-end, achieving high Macro-F1 on CITE without ad-hoc homogenization.

### Open Question 2
- Question: What specific hybrid sampling strategies or loss reweighting techniques can effectively mitigate the extreme long-tail label distribution found in datasets like CITE?
- Basis in paper: The paper observes that the divergence between Micro-F1 and Macro-F1 reveals pervasive class imbalance, noting that "addressing this imbalance necessitates novel techniques, such as hybrid sampling strategies or loss reweighting."
- Why unresolved: Standard evaluation frameworks fail to account for this complexity, causing models to overfit majority classes while systemically neglecting tail classes (e.g., "Crystallography").
- What evidence would resolve it: A training paradigm that significantly improves Macro-F1 scores for minority classes on CITE compared to standard cross-entropy loss.

### Open Question 3
- Question: How can the consistency of Large Language Models (LLMs) be stabilized to prevent misaligned predictions in extreme multi-class scenarios?
- Basis in paper: The paper highlights "substandard consistency of LLM models' output," noting they struggle with accurate label alignment and produce non-deterministic outputs or invalid class names.
- Why unresolved: LLMs treat nodes as isolated text snippets and suffer from answer stochasticity, resulting in near-zero performance for some models (e.g., GraphGPT) on specific taxonomies.
- What evidence would resolve it: A prompt engineering or constrained decoding method that eliminates invalid label generation and raises LLM-centric Micro-F1 on CITE to competitive levels.

## Limitations
- Limited generalization to other domains beyond catalytic materials research
- Performance depends heavily on specific LLM choice (LLaMA-7B) and GNN architecture (SimpleHGN)
- Degree effects may confound node type importance in ablation studies

## Confidence
- **High confidence**: Heterogeneous graph models outperform homogeneous ones (0.9791 vs 0.5057 Micro-F1 for SimpleHGN vs GCN)
- **Medium confidence**: The interpretation that journal nodes are more critical than keyword nodes due to disciplinary clustering
- **Medium confidence**: The claim that TAPE's cascaded design is superior to GraphGPT's tight coupling

## Next Checks
1. **Cross-domain validation**: Apply the best-performing TAPE pipeline to a different citation graph domain (e.g., computer science papers from arXiv) to test whether the heterogeneous architecture and journal node importance generalize beyond catalytic materials.

2. **Degree vs type ablation**: Systematically vary node degrees across node types in CITE (e.g., reduce journal degrees to match author degrees) while keeping the same number of nodes, to distinguish whether performance depends on node type or degree distribution.

3. **Alternative imbalance handling**: Implement focal loss or class-weighted cross-entropy in the baseline GCN model to determine whether the Macro-F1 gap (0.9936 vs 0.5655) can be closed through loss function modification rather than requiring heterogeneous architectures.