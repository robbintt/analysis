---
ver: rpa2
title: 'LingLanMiDian: Systematic Evaluation of LLMs on TCM Knowledge and Clinical
  Reasoning'
arxiv_id: '2602.01779'
source_url: https://arxiv.org/abs/2602.01779
tags:
- chinese
- reasoning
- clinical
- evaluation
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "LingLanMiDian is a large-scale, expert-curated benchmark designed\
  \ to evaluate Large Language Models on Traditional Chinese Medicine knowledge and\
  \ clinical reasoning. It unifies 13 tasks across five domains\u2014TCM licensing\
  \ exams, fundamental knowledge, Chinese patent medicine, information extraction,\
  \ and diagnostic-therapeutic decision-making\u2014using standardized metrics including\
  \ accuracy, F1, cosine similarity, and MAE."
---

# LingLanMiDian: Systematic Evaluation of LLMs on TCM Knowledge and Clinical Reasoning

## Quick Facts
- **arXiv ID**: 2602.01779
- **Source URL**: https://arxiv.org/abs/2602.01779
- **Reference count**: 40
- **Primary result**: Large-scale, expert-curated benchmark unifying 13 TCM tasks across five domains to evaluate LLM clinical reasoning capabilities

## Executive Summary
LingLanMiDian presents a comprehensive benchmark for evaluating Large Language Models on Traditional Chinese Medicine knowledge and clinical reasoning. The benchmark unifies 13 tasks across five domains—TCM licensing exams, fundamental knowledge, Chinese patent medicine, information extraction, and diagnostic-therapeutic decision-making—using standardized metrics including accuracy, F1, cosine similarity, and MAE. Evaluated on 14 state-of-the-art LLMs under zero-shot protocol, models achieved high accuracy on recall tasks but struggled with multi-hop reasoning, information extraction from classical texts, and dosage prediction. The benchmark includes a 400-item hard subset per task to assess advanced reasoning capabilities, revealing substantial gaps in expert-level TCM reasoning despite strong factual knowledge retention.

## Method Summary
The benchmark development involved a two-stage expert-driven process where TCM specialists first curated datasets across 13 distinct tasks spanning five domains, then standardized evaluation metrics and protocols. Each task includes a 400-item hard subset designed to challenge advanced reasoning capabilities. The evaluation protocol employed zero-shot testing across 14 state-of-the-art LLMs including both dense and MoE architectures, with results measured against expert-verified ground truth answers using multiple metrics: accuracy for categorical decisions, F1 for information extraction, cosine similarity for continuous predictions, and MAE for dosage estimation. The unified framework enables systematic comparison across diverse TCM competencies while maintaining reproducibility through standardized input-output formats.

## Key Results
- LLMs achieved high accuracy on factual recall tasks but showed significant performance degradation on multi-hop reasoning and information extraction from classical texts
- MoE models demonstrated architectural advantages for discrete categorical reasoning but failed to extend these gains to compositional clinical tasks like prescription formulation
- Models exhibited particular difficulty with dosage prediction and syndrome differentiation tasks, highlighting gaps in clinically faithful decision-making
- The hard subset consistently exposed performance limitations across all models, with average accuracy dropping 15-25% compared to standard subsets

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its systematic unification of diverse TCM competencies under standardized evaluation protocols. By incorporating both knowledge recall and complex reasoning tasks across multiple domains, it captures the full spectrum of clinical expertise required in TCM practice. The expert-curated datasets ensure ground truth quality, while the multi-metric evaluation framework accommodates different task types from categorical classification to continuous prediction. The inclusion of hard subsets specifically designed to challenge advanced reasoning creates meaningful discrimination between model capabilities beyond simple memorization.

## Foundational Learning
- **TCM licensing exam format**: Standardized multiple-choice and short-answer assessments covering core TCM principles, essential for establishing baseline knowledge evaluation
- **Syndrome differentiation patterns**: Recognition and classification of TCM diagnostic patterns based on symptom clusters, critical for clinical reasoning assessment
- **Herb properties and interactions**: Understanding of individual herb characteristics and synergistic/antagonistic relationships in formulations, fundamental for prescription evaluation
- **Classical text interpretation**: Extraction and reasoning from classical TCM literature, necessary for assessing deep domain expertise
- **Dosage calculation principles**: Application of TCM-specific dosing rules considering patient factors and herb properties, vital for therapeutic safety assessment
- **Patent medicine categorization**: Knowledge of standardized TCM formulations and their clinical applications, important for regulatory and commercial context evaluation

Quick check: Verify that evaluation metrics appropriately weight clinical relevance versus technical accuracy for each domain.

## Architecture Onboarding

**Component Map**: Task Curation -> Dataset Standardization -> Metric Definition -> Model Evaluation -> Result Analysis

**Critical Path**: Expert Curation → Dataset Assembly → Metric Specification → Zero-shot Evaluation → Performance Analysis

**Design Tradeoffs**: The zero-shot protocol prioritizes reproducibility and fair comparison but may underestimate model potential compared to fine-tuning; the hard subset inclusion increases benchmark difficulty but may reduce overall model performance scores.

**Failure Signatures**: Performance degradation on classical text extraction indicates limitations in handling domain-specific linguistic patterns; dosage prediction errors suggest difficulties with continuous value estimation; syndrome differentiation failures reveal challenges in multi-step clinical reasoning chains.

**3 First Experiments**:
1. Compare zero-shot vs. few-shot performance across all 13 tasks to establish performance ceiling
2. Evaluate model sensitivity to lexical variations in TCM terminology using synonym substitution
3. Test compositional reasoning by chaining syndrome diagnosis to prescription generation tasks

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: To what extent can incorporating multimodal cues (tongue images, pulse waveforms) bridge the gap between text-based inference and perception-to-reasoning competence in TCM?
- **Basis in paper**: [explicit] Section 5.6 (Future Work) states that LingLan is text-only and excludes multimodal cues, limiting assessment; the authors plan a multimodal extension to evaluate end-to-end reasoning.
- **Why unresolved**: Current TCM benchmarks rely solely on text, failing to capture the diagnostic weight of physical observations essential to the clinical workflow.
- **What evidence would resolve it**: A multimodal benchmark extension showing that models utilizing visual/signal data significantly outperform text-only baselines on diagnostic and therapeutic tasks.

### Open Question 2
- **Question**: How can a comprehensive synonym and normalization resource be constructed to reliably handle the pervasive lexical variability in TCM syndrome labels and herb names?
- **Basis in paper**: [explicit] Section 5.5 identifies the lack of a broad synonym and normalization resource as a "central impediment" to reliable evaluation, noting that current methods only partially mitigate lexical variability.
- **Why unresolved**: TCM terminology is heterogeneous with high synonymy; existing metrics like character-level F1 cannot fully capture semantic equivalence or handle variant canonical forms.
- **What evidence would resolve it**: The development of a standardized ontology or resource that, when applied to evaluation metrics, reduces scoring discrepancies caused by surface-form variations without manual adjudication.

### Open Question 3
- **Question**: Why do architectural optimizations (like MoE routing) that improve discrete categorical reasoning fail to benefit compositional clinical tasks such as prescription formulation?
- **Basis in paper**: [inferred] Section 5.1 notes that architectural gains from MoE models apply to discrete category decisions but "do not extend to strongly compositional... tasks such as prescription composition," suggesting a bottleneck in chain consistency.
- **Why unresolved**: It is unclear if this limitation is due to the "parameterized memory" nature of dense models being better suited for list-based recall or if routing mechanisms in MoE disrupt coherent multi-step generation.
- **What evidence would resolve it**: Ablation studies on MoE vs. dense models using specific compositional constraints (e.g., enforcing syndrome-prescription compatibility) to determine if structural consistency improves with scale or architecture.

## Limitations
- Zero-shot evaluation protocol may underestimate model performance compared to fine-tuning approaches
- Expert consensus-based ground truth introduces potential subjectivity in complex clinical scenarios
- Benchmark coverage may not fully represent real-world TCM practice complexity and integration with modern medicine

## Confidence
- **High confidence**: Claims regarding model performance on factual recall tasks and standardized exam-style questions
- **Medium confidence**: Claims about model limitations in multi-hop reasoning and information extraction from classical texts
- **Medium confidence**: Benchmark construction methodology and task categorization

## Next Checks
1. Replicate benchmark evaluation using few-shot and fine-tuning protocols to establish upper performance bounds for state-of-the-art LLMs
2. Conduct expert validation study comparing LLM-generated clinical recommendations against actual treatment outcomes in real patient cases
3. Perform ablation studies to identify which specific benchmark components most effectively discriminate between model capabilities in advanced TCM reasoning tasks