---
ver: rpa2
title: Unlocking LLM Safeguards for Low-Resource Languages via Reasoning and Alignment
  with Minimal Training Data
arxiv_id: '2510.10677'
source_url: https://arxiv.org/abs/2510.10677
tags:
- reasoning
- alignment
- harmful
- languages
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of detecting malicious requests
  in low-resource languages using large language models (LLMs). The authors propose
  ConsistentGuard, a reasoning-based multilingual safeguard that improves explainability
  and knowledge transfer between languages through alignment.
---

# Unlocking LLM Safeguards for Low-Resource Languages via Reasoning and Alignment with Minimal Training Data

## Quick Facts
- arXiv ID: 2510.10677
- Source URL: https://arxiv.org/abs/2510.10677
- Reference count: 23
- Primary result: A reasoning-based multilingual safeguard improves safety classification for low-resource languages using only 1,000 training samples

## Executive Summary
This paper addresses the challenge of detecting malicious requests in low-resource languages using large language models (LLMs). The authors propose ConsistentGuard, a reasoning-based multilingual safeguard that improves explainability and knowledge transfer between languages through alignment. The method employs a three-stage training framework: knowledge distillation via supervised fine-tuning (SFT), reasoning training using Group Relative Policy Optimization (GRPO) with novel length and diversity rewards, and cross-lingual alignment using Constrained Alignment Optimization (CAO). Evaluated across three datasets in six languages (English, French, Chinese, Japanese, Bengali, and Hindi) with only 1,000 training samples, ConsistentGuard outperforms larger models trained on significantly more data, achieving strong classification performance and interpretability.

## Method Summary
The ConsistentGuard framework uses a three-stage training process to enable multilingual safety classification. First, a cold-start phase distills reasoning traces from a 671B parameter teacher model (DeepSeek V3) to a 3B parameter student (Qwen2.5) via SFT, providing explicit reasoning structures (Understanding → Rule Matching → Judging). Second, reasoning training employs GRPO with composite rewards: a length reward (sine function) penalizing reasoning sequences deviating from target length $L_{best}$, and a diversity reward penalizing trigram repetition to encourage high-density conditional information. Third, cross-lingual alignment uses Constrained Alignment Optimization (CAO) to transfer knowledge from high-resource languages to low-resource languages while preventing catastrophic forgetting through a global regularization term. The system processes inputs with 14 predefined judge principles and outputs both reasoning traces and verdicts.

## Key Results
- ConsistentGuard outperforms larger models trained on significantly more data across three safety datasets in six languages
- The three-stage training framework achieves strong classification performance with only 1,000 training samples
- The model demonstrates improved explainability through generated reasoning traces compared to non-reasoning baselines

## Why This Works (Mechanism)

### Mechanism 1: Constrained Reinforcement Learning for Reasoning Density
The system uses Group Relative Policy Optimization (GRPO) with a composite reward function to control reasoning length and diversity. A length reward (sine function) penalizes reasoning sequences that deviate from a target length $L_{best}$, preventing efficiency loss from "overthinking." A diversity reward penalizes trigram repetition, forcing the model to utilize fixed token budgets for higher-density conditional information. This improves classification density and generalization compared to unconstrained reasoning or standard supervised fine-tuning. The assumption is that trigram repetition correlates with low-quality reasoning and an optimal fixed length exists balancing efficiency and performance across languages.

### Mechanism 2: Cross-Lingual Alignment via Constrained Optimization (CAO)
CAO aligns model outputs between low-resource languages and high-resource languages using a constrained objective that bridges performance gaps without catastrophic forgetting. The method constructs data pairs where rejected outputs from low-resource languages are paired with successful outputs from high-resource languages. Unlike standard Direct Preference Optimization (DPO), CAO adds a global regularization term ($L_c$) using KL-divergence to constrain optimization direction and reduce deviation of the anchor sample's representation. The assumption is that failures in low-resource languages stem from reasoning inconsistency rather than fundamental lexical knowledge gaps, and high-resource reasoning is valid and transferable.

### Mechanism 3: Cold-Start Distillation for Small Model Capacity
Distilling explicit reasoning structures from a large model enables a small 3B parameter model to succeed where self-evolution RL fails. Smaller models lack the latent capacity to discover complex reasoning chains via RL "self-evolution" from scratch. The framework first uses SFT to distill a specific problem-solving plan from DeepSeek V3 (671B), initializing the policy with valid reasoning structures before RL refinement. The assumption is that the 3B student model has sufficient capacity to replicate the reasoning steps of the 671B teacher model.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**: This core algorithm drives reasoning capability by estimating baselines from group scores rather than separate value functions, crucial for training reasoning chains. Quick check: How does GRPO estimate baseline advantage without a separate critic model, and why does this matter for memory usage?

- **Direct Preference Optimization (DPO)**: The proposed CAO method modifies DPO. Understanding standard DPO (optimizing preferred vs. dispreferred outputs) is necessary to understand why the authors added a global constraint to prevent collapse. Quick check: In standard DPO, what does the $\beta$ parameter control, and what happens to the reference model during training?

- **Knowledge Distillation**: The "Cold Start" phase relies on distilling reasoning traces from a 671B model to a 3B model. Quick check: Why is "white-box" distillation (matching logits/activations) often preferred over "black-box" (matching text outputs) for reasoning tasks, and which does this paper appear to use?

## Architecture Onboarding

- **Component map**: Input (safety query + 14 judge principles) → Base Model (Qwen2.5-3B) → Teacher Model (DeepSeek V3 671B for SFT) → Output (reasoning trace + verdict)

- **Critical path**: The data pair construction for CAO (Section 2.3.1). Correctly identifying failure sets in low-resource languages and mapping them to success sets in English is essential. If this mapping is noisy, the alignment optimizes the wrong vector.

- **Design tradeoffs**: 
  - Data Efficiency vs. Robustness: Using only 1,000 seed samples forces extreme dependence on generalization and distillation quality, potentially lacking robustness to adversarial attacks not covered in seed diversity
  - Length vs. Complexity: The length reward ($L_{best}=512$) trades off deep reasoning capability for latency/efficiency

- **Failure signatures**:
  - Repetitive Reasoning: If the diversity reward is misconfigured, the model will output repetitive phrases to satisfy the length reward
  - Catastrophic Forgetting: Without the $L_c$ term in CAO, the model's performance on English will drop as it overfits to align with low-resource language failures

- **First 3 experiments**:
  1. Reasoning Ablation: Train with SFT only vs. SFT+GRPO (without CAO) to validate reasoning gain on English
  2. Reward Sensitivity: Vary $L_{best}$ (256 vs. 512 vs. 1024) to find reasoning length inflection point where accuracy plateaus but latency spikes
  3. Alignment Baselines: Compare DPO vs. CAO on specific failure sets in Bengali/Hindi to confirm global constraint is necessary to prevent English performance collapse

## Open Questions the Paper Calls Out

None

## Limitations
- Reward function sensitivity: The GRPO training relies on precise calibration of length and diversity rewards, with the assumption that a single optimal length works universally across six languages untested
- Alignment generalization: The CAO method's effectiveness depends on finding valid failure-success pairs, but for culturally specific harms, English reasoning may not be applicable
- Teacher distillation quality: The cold-start SFT depends entirely on the quality of reasoning traces from the 671B teacher, creating potential blind spots if teacher reasoning is biased or incomplete

## Confidence

- **High Confidence**: Experimental results demonstrating strong performance (AUC-ROC, accuracy) compared to baselines on three datasets are directly reported and verifiable
- **Medium Confidence**: Proposed mechanisms (length/diversity rewards in GRPO, CAO vs. DPO) are theoretically sound with significant reported gains, but limited ablation studies and lack of extensive hyperparameter sensitivity analysis reduce confidence
- **Low Confidence**: The claim of achieving "interpretability" is weak, as the paper shows reasoning traces but does not perform systematic evaluation of their correctness, faithfulness, or usefulness for human auditors

## Next Checks

1. **Reward Ablation and Sensitivity**: Conduct comprehensive ablation study varying $L_{best}$ (256, 512, 1024 tokens) and weights of length/diversity rewards. Report trade-off between reasoning length, latency, and classification accuracy for each safety category in each language to identify true inflection point.

2. **Alignment Robustness Test**: Create "conflicting anchor" test set where English reasoning is demonstrably incorrect for a given low-resource context. Measure whether CAO correctly avoids aligning to this bad anchor, or if it still forces the low-resource model to follow flawed English logic.

3. **Reasoning Faithfulness Audit**: Perform human evaluation study where expert reviewers assess (query, reasoning trace, verdict) triplets for logical soundness, relevance, and verdict agreement. Measure inter-annotator agreement and correlation between reasoning quality scores and classification accuracy.