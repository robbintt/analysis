---
ver: rpa2
title: A Mathematical Framework for Custom Reward Functions in Job Application Evaluation
  using Reinforcement Learning
arxiv_id: '2511.16073'
source_url: https://arxiv.org/abs/2511.16073
tags:
- reward
- grpo
- training
- policy
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study addresses the inflexibility of traditional Applicant
  Tracking Systems (ATS), which rely on rigid keyword matching and often overlook
  qualified candidates due to semantic mismatches. A two-stage methodology is proposed:
  Supervised Fine-Tuning (SFT) to establish a baseline model, followed by Generative
  Reward Policy Optimization (GRPO) using a custom multi-component reward function.'
---

# A Mathematical Framework for Custom Reward Functions in Job Application Evaluation using Reinforcement Learning

## Quick Facts
- arXiv ID: 2511.16073
- Source URL: https://arxiv.org/abs/2511.16073
- Reference count: 12
- Primary result: 91% accuracy on unseen test data, with 0.85 recall and 1.0 precision for SELECTED class

## Executive Summary
This paper addresses the inflexibility of traditional Applicant Tracking Systems by proposing a two-stage methodology that combines Supervised Fine-Tuning (SFT) with Generative Reward Policy Optimization (GRPO) using a custom multi-component reward function. The approach enables more holistic candidate evaluation by aligning model outputs with human recruiter judgment rather than relying on rigid keyword matching. Experimental results demonstrate significant improvements over SFT-only baselines, with the GRPO-refined model achieving 91% accuracy on unseen test data and maintaining high precision while avoiding reward hacking through the multi-component reward structure.

## Method Summary
The methodology employs a two-stage pipeline: first, SFT on 2,700 synthetic resume-job description pairs to establish baseline format compliance and structure, then GRPO optimization using a custom four-component reward function (Status Correctness 40%, Score Accuracy 20%, Skills Matching 20%, Experience Evaluation 20%) with KL-divergence regularization. The model uses Qwen2-0.5B-Instruct with 4-bit quantization and LoRA adapters (rank=16, alpha=32), training for 2 epochs in SFT (LR=2×10⁻⁴) followed by 337 steps of GRPO (LR=2×10⁻⁶, KL beta=0.1). The multi-component reward prevents reward hacking by requiring simultaneous satisfaction of competing objectives, while KL regularization constrains policy drift to preserve SFT-acquired knowledge.

## Key Results
- Achieved 91% accuracy on held-out test set of 104 samples
- SELECTED class recall of 0.85 with perfect precision (1.0)
- Overall F1 score of 0.92 for binary classification
- Demonstrated significant improvement over SFT-only baseline through GRPO optimization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The multi-component reward function prevents reward hacking by requiring simultaneous satisfaction of multiple, partially competing objectives.
- Mechanism: Four weighted criteria (Status Correctness 40%, Score Accuracy 20%, Skills Matching 20%, Experience Evaluation 20%) are combined into a single scalar via weighted linear combination, forcing holistic reasoning rather than shortcut optimization.
- Core assumption: The four criteria capture sufficient signal for human-aligned evaluation; their weighted combination meaningfully trades off competing objectives.
- Evidence anchors: Abstract states reward function "helps mitigate reward hacking" through holistic assessment; Section 3.2.1 explains interdisciplinary nature combats reward hacking by requiring satisfaction of many conflicting goals.

### Mechanism 2
- Claim: KL-divergence regularization constrains policy drift, preserving SFT-acquired knowledge while allowing targeted refinement.
- Mechanism: Beta parameter (0.1) penalizes divergence from SFT reference policy, creating a trust region where the model can adjust reasoning patterns but cannot wander into implausible output distributions.
- Core assumption: SFT policy represents a valid behavioral prior; some divergence is beneficial but excessive divergence correlates with reward hacking.
- Evidence anchors: Section 3.3 states regularization "functionally restrains the model to a space of realistic and sensible solutions"; KL divergence reached 0.34767 demonstrating controlled optimization.

### Mechanism 3
- Claim: The two-stage pipeline (SFT → GRPO) decomposes learning into distinct phases: format/structure acquisition, then reasoning refinement.
- Mechanism: SFT on 2,700 samples teaches the model to output valid JSON with score and status fields; GRPO then optimizes decision logic within that format, with 97.3% loss reduction (4.94 → 0.13) indicating successful reasoning refinement.
- Core assumption: SFT provides sufficient behavioral grounding that GRPO can focus on policy quality rather than output validity.
- Evidence anchors: Section 3.1 describes SFT format training; Section 4 reports training loss reduction from 4.94 to 0.13 demonstrating reasoning optimization after format establishment.

## Foundational Learning

- Concept: **Group-Relative Policy Optimization (GRPO)**
  - Why needed here: GRPO is the RL algorithm optimizing the policy; understanding it is prerequisite to diagnosing training dynamics and hyperparameter choices.
  - Quick check question: How does GRPO differ from PPO in how it estimates advantages? (Assumption: GRPO uses group-relative comparisons rather than value function estimation.)

- Concept: **Reward Hacking**
  - Why needed here: The paper explicitly identifies reward hacking as the critical failure mode in early experiments; recognizing its signatures is essential for debugging.
  - Quick check question: A model achieves near-perfect reward but outputs nonsensical evaluations. Is this likely reward hacking, and what would you check first?

- Concept: **KL-Divergence as Regularization**
  - Why needed here: The beta parameter controls the trade-off between reward optimization and behavioral drift; misunderstanding this leads to either stagnant or unstable training.
  - Quick check question: If KL divergence drops to near-zero early in GRPO training, what does this imply about the learning dynamics?

## Architecture Onboarding

- Component map: Qwen2-0.5B-Instruct -> 4-bit quantization -> LoRA adapter (rank=16, alpha=32) -> SFT stage -> GRPO stage -> reward function computation -> policy update with KL penalty -> evaluation

- Critical path: SFT training convergence → SFT adapter checkpoint → GRPO initialization from SFT adapter → reward function computation per sample → policy update with KL penalty → evaluation on held-out test set

- Design tradeoffs:
  - Small model (600M) vs. larger: Chosen for efficiency and reduced hallucination; trades off raw capability. Assumption: Domain-specific fine-tuning compensates for scale.
  - Single GRPO epoch vs. more: Intentionally limited to 337 steps to reduce over-optimization risk. Trades off potential further gains for stability.
  - No GRPO validation set: Validation metrics deemed uninformative for policy optimization. Trades off monitoring granularity for simplified pipeline.

- Failure signatures:
  - Excessive negative outputs: Indicates overly aggressive penalties in reward function; the paper reports this in initial experiments.
  - High KL divergence with low reward improvement: Policy drifting without learning; increase beta or reduce learning rate.
  - Perfect precision but low recall (1.0 / <0.5): Model too conservative; check if false negative penalties are too high.

- First 3 experiments:
  1. Replicate SFT-only baseline and verify format compliance on 20-30 samples before proceeding to GRPO.
  2. Ablate one reward component (e.g., remove Skills Matching, reweight others) and measure impact on test accuracy and false positive rate.
  3. Vary KL beta (0.05, 0.1, 0.2) on a held-out validation slice to characterize the stability frontier before full GRPO training.

## Open Questions the Paper Calls Out

- Question: Can the model be extended to identify and flag ambiguous "grey area" candidates for human review rather than forcing binary SELECTED/REJECTED decisions?
  - Basis in paper: [explicit] The conclusion states the model can be improved to "identify candidacies of ambiguous grey areas and mark them as subject to manual review."
  - Why unresolved: Current framework produces only binary classifications with numerical scores, lacking explicit uncertainty quantification or triage mechanisms for borderline cases.
  - What evidence would resolve it: Implementation of uncertainty-aware output layer and evaluation of calibration metrics on borderline test cases to validate reliable deferral behavior.

- Question: How does model performance transfer to real-world, non-synthetic resume data with inherent noise, formatting variability, and privacy-sensitive content?
  - Basis in paper: [inferred] Methodology states data was artificially created using templates and logical rules to eliminate privacy concerns in real resumes; no validation on actual resume data reported.
  - Why unresolved: Synthetic data ensures internal consistency but may not capture linguistic diversity, formatting inconsistencies, and edge cases present in authentic recruitment pipelines.
  - What evidence would resolve it: Benchmark evaluations on de-identified real resume datasets from actual hiring processes, comparing performance metrics against synthetic data results.

## Limitations
- Evaluation constrained by synthetic data that may not capture real-world complexity and ambiguity of resume evaluation
- Multi-component reward function weights (40/20/20/20) set without systematic ablation studies to justify optimality
- Single epoch GRPO training and lack of validation set during policy optimization introduce overfitting risks without detection
- 600M parameter model size may limit depth of reasoning possible compared to larger models
- Claim of "significant improvement" over ATS systems lacks direct comparison to existing ATS baselines

## Confidence
- **High Confidence**: SFT training effectiveness (format compliance, convergence metrics reported), KL regularization mechanism (KL values measured and reported), basic GRPO optimization (loss reduction documented)
- **Medium Confidence**: Multi-component reward function effectiveness (component weights not ablated), overall performance gains (91% accuracy vs baseline not directly compared), real-world applicability (synthetic data limitation)
- **Low Confidence**: Anti-reward hacking claims (no direct test of reward hacking susceptibility), robustness to adversarial resumes (not evaluated), generalization to different industries or evaluation criteria (single synthetic dataset used)

## Next Checks
1. **Ablation Study of Reward Components**: Systematically remove each reward component individually and measure impact on test accuracy, false positive rate, and KL divergence to validate the anti-reward hacking mechanism.

2. **Direct ATS Baseline Comparison**: Implement a simple keyword-based ATS system on the same synthetic dataset and compare performance metrics to quantify the claimed improvement over traditional approaches.

3. **Real-World Data Validation**: Apply the trained model to a small set of real resume-job description pairs (with expert annotations) to assess performance degradation and identify gaps between synthetic and actual evaluation scenarios.