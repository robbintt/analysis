---
ver: rpa2
title: 'Distill Not Only Data but Also Rewards: Can Smaller Language Models Surpass
  Larger Ones?'
arxiv_id: '2502.19557'
source_url: https://arxiv.org/abs/2502.19557
tags:
- student
- teacher
- reward
- responses
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel knowledge distillation method that
  transfers both responses and rewards from large language models (LLMs) to smaller
  student models. The key innovation is a self-supervised reward learning mechanism
  that generates pseudo-rewards by comparing the inherent structure of teacher and
  student responses, eliminating the need for external reward signals.
---

# Distill Not Only Data but Also Rewards: Can Smaller Language Models Surpass Larger Ones?

## Quick Facts
- arXiv ID: 2502.19557
- Source URL: https://arxiv.org/abs/2502.19557
- Reference count: 19
- Student models (3B) surpassed teachers (8B) on GSM8K (83.02% vs 80.97%) and MMLU-PRO (40.02% vs 39.88%)

## Executive Summary
This paper presents a novel knowledge distillation framework that transfers both responses and rewards from large language models to smaller student models. The key innovation is a self-supervised reward learning mechanism that generates pseudo-rewards by comparing teacher and student responses, eliminating the need for external reward signals. By combining supervised fine-tuning with reinforcement learning guided by the learned reward model, the approach achieves consistent performance improvements on GSM8K and MMLU-PRO benchmarks. The method is particularly effective when teacher supervision is weaker, with smaller models achieving up to 9.38% improvement on MMLU-PRO.

## Method Summary
The framework introduces a self-supervised reward learning mechanism that compares inherent structures between teacher and student responses to generate pseudo-rewards. This eliminates the dependency on external reward signals while maintaining effective guidance for student training. The approach combines supervised fine-tuning (SFT) with reinforcement learning (RL) where the reward model learned through self-supervision guides the RL process. This dual-transfer of both responses and rewards enables more effective knowledge distillation compared to traditional SFT-only methods, allowing smaller models to not only match but in some cases surpass their larger teachers on benchmark tasks.

## Key Results
- Llama3-3B students achieved 83.02% accuracy on GSM8K, surpassing 8B teacher's 80.97%
- MMLU-PRO results showed 3B students at 40.02% vs teacher's 39.88%
- Up to 9.38% improvement on MMLU-PRO achieved with 1B student model when teacher supervision was weaker
- Consistent performance improvements across both benchmarks compared to SFT-only baselines

## Why This Works (Mechanism)
The self-supervised reward learning mechanism works by generating pseudo-rewards through structural comparison between teacher and student responses, eliminating the need for costly external reward modeling. This approach captures the quality differential between responses without requiring human annotation or predefined reward functions. The combination of SFT for initial alignment followed by RL guided by the learned reward model allows students to refine their outputs beyond simple imitation. The framework effectively transfers both the surface-level responses and the underlying reasoning quality from teachers to students, enabling smaller models to achieve or exceed teacher performance through more efficient knowledge extraction.

## Foundational Learning
- **Knowledge Distillation**: Why needed: Enables smaller models to learn from larger models' capabilities. Quick check: Can students match or exceed teacher performance on benchmarks?
- **Reinforcement Learning**: Why needed: Allows models to optimize for reward-based objectives rather than just supervised targets. Quick check: Does RL improve over SFT-only training?
- **Self-Supervised Learning**: Why needed: Eliminates dependency on external labeled data or reward signals. Quick check: Can pseudo-rewards generated internally guide effective training?
- **Supervised Fine-Tuning**: Why needed: Provides initial alignment and baseline capability before RL refinement. Quick check: Does SFT provide sufficient starting point for RL?
- **Reward Modeling**: Why needed: Guides model behavior toward desired outputs through optimization. Quick check: Do learned rewards correlate with actual output quality?
- **Response Structure Analysis**: Why needed: Enables comparison of reasoning quality beyond surface-level text matching. Quick check: Can structural differences predict response quality?

## Architecture Onboarding

**Component Map**: Data -> SFT Module -> Reward Model -> RL Module -> Distilled Student

**Critical Path**: The most critical execution path flows through data preprocessing, SFT fine-tuning of the student, reward model training through self-supervision, and RL optimization guided by the learned rewards.

**Design Tradeoffs**: The framework trades computational complexity during training (additional reward model training and RL phases) for improved final model performance and reduced model size. This increases training time and resource requirements but enables deployment of smaller, more efficient models.

**Failure Signatures**: Poor teacher quality leads to weak reward signals and suboptimal student performance. Insufficient diversity in training data can cause reward models to overfit to specific patterns. Computational constraints during RL may prevent adequate exploration of the response space.

**Three First Experiments**:
1. Run SFT-only baseline to establish performance floor without reward learning
2. Evaluate reward model quality by testing correlation between pseudo-rewards and human judgments
3. Compare student performance against teacher across multiple random seeds to assess statistical significance

## Open Questions the Paper Calls Out
The paper does not explicitly identify open questions or limitations in its discussion, focusing instead on demonstrating the effectiveness of the proposed approach on the studied benchmarks.

## Limitations
- Limited generalizability to non-mathematical and non-knowledge-based tasks like creative writing or code generation
- Computational overhead of reward learning phase not quantified, making deployment cost assessment difficult
- Ablation studies insufficient to isolate contributions of individual components (SFT vs RL vs reward model)

## Confidence

High confidence: Student models consistently outperformed teachers on GSM8K and MMLU-PRO benchmarks with specific numerical comparisons and statistical reporting supporting the findings.

Medium confidence: Self-supervised reward learning drives improvements, though the paper demonstrates effectiveness without fully isolating component contributions or comparing against alternative reward learning approaches.

Low confidence: The approach generalizes to all reasoning tasks and model sizes, given the limited experimental scope focusing primarily on mathematical reasoning and professional knowledge tasks.

## Next Checks
1. Evaluate the method on diverse task categories including creative writing, code generation, and multimodal understanding to assess generalizability beyond mathematical and knowledge-based tasks.
2. Conduct controlled ablation studies isolating SFT, RL, and reward model contributions with statistical significance testing to determine which components drive performance gains.
3. Measure and report wall-clock time, GPU memory usage, and total computational cost for the complete distillation pipeline to assess practical deployment feasibility compared to baseline methods.