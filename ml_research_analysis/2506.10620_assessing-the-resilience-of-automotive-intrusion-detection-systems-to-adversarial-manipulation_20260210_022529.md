---
ver: rpa2
title: Assessing the Resilience of Automotive Intrusion Detection Systems to Adversarial
  Manipulation
arxiv_id: '2506.10620'
source_url: https://arxiv.org/abs/2506.10620
tags:
- attacks
- attack
- adversarial
- detection
- decay
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the resilience of automotive intrusion
  detection systems (IDSs) to adversarial manipulation, focusing on evasion attacks
  that aim to bypass detection while maintaining malicious intent. The authors extend
  their prior work by evaluating gradient-based adversarial attacks under white-box,
  grey-box, and black-box scenarios against state-of-the-art IDSs on two real-world
  datasets (ReCAN and CarHacking).
---

# Assessing the Resilience of Automotive Intrusion Detection Systems to Adversarial Manipulation

## Quick Facts
- **arXiv ID:** 2506.10620
- **Source URL:** https://arxiv.org/abs/2506.10620
- **Reference count:** 40
- **Primary result:** Gradient-based evasion attacks can degrade automotive IDS performance by up to 60% under white-box scenarios, with effectiveness decreasing in grey-box (39%) and black-box (43%) scenarios.

## Executive Summary
This paper investigates the resilience of automotive intrusion detection systems to adversarial manipulation through gradient-based evasion attacks. The authors adapt computer vision techniques to the automotive domain by incorporating CAN-specific constraints like valid signal ranges and real-time execution requirements. They evaluate attacks across white-box, grey-box, and black-box scenarios on two real-world datasets (ReCAN and CarHacking) using state-of-the-art IDSs. The findings reveal that while evasion attacks are feasible, their success depends heavily on attacker knowledge, dataset quality, and target IDS architecture, with autoencoder-based systems showing greater resilience than predictor-based ones.

## Method Summary
The study employs gradient-based adversarial attacks (FGSM, BIM, DeepFool) adapted for CAN bus data by adding clipping and rounding steps to respect discrete signal ranges. Six IDS architectures are evaluated: FFNN, CANdito autoencoder, and LSTM/GRU predictors. Attacks are tested under three knowledge scenarios: white-box (full model access), grey-box (partial knowledge), and black-box (oracle-based transferability). The primary metric is TPR degradation, with AP measuring perturbation magnitude. Data preprocessing uses the READ heuristic to extract Physval and Binary signals from raw CAN logs, normalized to [0,1].

## Key Results
- White-box attacks achieved up to 60% TPR degradation using DeepFool
- Grey-box and black-box attacks achieved 39% and 43% degradation respectively
- Autoencoder-based IDSs (CANdito) showed greater resilience than predictor-based models
- In grey- and black-box scenarios, perturbations sometimes increased detection rates due to negative transfer
- DeepFool consistently outperformed other algorithms across all scenarios

## Why This Works (Mechanism)

### Mechanism 1: Gradient-Based Perturbation with Domain Constraints
Gradient-based attacks calculate loss gradients w.r.t. input signals, then apply perturbations while clipping to [0,1] and rounding to ensure valid CAN bit representations. This domain adaptation preserves attack effectiveness while maintaining protocol compliance.

### Mechanism 2: Oracle-Based Transferability
Adversarial samples crafted against surrogate models can transfer to unknown target IDSs, though effectiveness varies. This black-box approach relies on similar decision boundaries between oracle and target systems.

### Mechanism 3: Autoencoder Robustness via Temporal Reconstruction
Autoencoders reconstruct entire windows of packets rather than predicting single frames, making them less sensitive to local perturbations. This window-to-window approach requires attackers to disrupt larger temporal contexts.

## Foundational Learning

- **Controller Area Network (CAN) Bus**
  - **Why needed here:** Understanding CAN's broadcast architecture and lack of authentication is crucial for grasping attack vectors and why signal validity constraints matter.
  - **Quick check question:** How does the lack of authentication in the CAN protocol facilitate the threat model described in the paper?

- **Adversarial Machine Learning (Evasion)**
  - **Why needed here:** The paper applies gradient-based attacks that optimize minimal input changes to maximize model error.
  - **Quick check question:** Why is minimizing the perturbation ($\delta$) critical in an automotive context compared to a generic computer vision context?

- **Time-Series Anomaly Detection (Prediction vs. Reconstruction)**
  - **Why needed here:** Different IDS architectures (predictors vs autoencoders) have varying vulnerabilities to temporal perturbations.
  - **Quick check question:** Why might a predictor model be more vulnerable to a perturbation in a single packet than a window-based autoencoder?

## Architecture Onboarding

- **Component map:** Raw CAN logs -> READ preprocessing -> Normalize [0,1] -> IDS models (FFNN, CANdito, LSTM/GRU) -> Attack module (oracle with clipping/rounding) -> Evaluation (TPR degradation, AP)

- **Critical path:**
  1. Feature extraction: Identifying Physval/Binary signals from 64-bit CAN payloads
  2. Perturbation loop: Oracle query → Gradient computation → Clipping/Rounding → Evasion check
  3. Injection validation: Semantic validity and real-time feasibility verification

- **Design tradeoffs:**
  - Attack strength vs. semantic preservation: Stronger perturbations risk destroying malicious intent
  - Knowledge vs. realism: White-box attacks are effective but unrealistic; black-box is realistic but less effective

- **Failure signatures:**
  - Negative transfer: Perturbations increase detection in black-box scenarios
  - Semantic loss: Continuous Change attacks collapse to normal traffic patterns

- **First 3 experiments:**
  1. Baseline validation: Run 6 IDS models on raw datasets without attacks to establish TPR/Precision
  2. White-box DeepFool: Implement DeepFool with clipping/rounding against Short LSTM oracle
  3. Semantic check: Visualize Continuous Change attack to verify malicious intent preservation

## Open Questions the Paper Calls Out

- **Open Question 1:** To what extent do perturbed attack sequences preserve the original attack intent and achieve the desired physical impact on the vehicle?
  - **Basis:** Authors state future work will evaluate how well perturbed sequences preserve attack intent.
  - **Evidence needed:** Functional safety tests on real vehicles or hardware-in-the-loop simulators.

- **Open Question 2:** How does the injection of adversarial packets influence the real-time behavior of the CAN bus and the vehicle's state?
  - **Basis:** Authors identify lack of real test vehicle as significant limitation.
  - **Evidence needed:** Live testing monitoring bus load, error frames, and ECU state changes during adversarial injection.

- **Open Question 3:** Are score-based adversarial attack methods applicable and effective in the tabular and temporal domains of automotive systems?
  - **Basis:** Conclusion suggests investigating alternative approaches like score-based methods.
  - **Evidence needed:** Comparative analysis of score-based vs gradient-based attacks in automotive context.

## Limitations

- Domain adaptation from computer vision may not capture all CAN protocol nuances
- Semantic validation limited to single-step verification rather than end-to-end effect validation
- Black-box scenarios rely on transferability assumptions that may not hold for different architectures

## Confidence

- **High Confidence:** White-box attack effectiveness (60% TPR degradation) and DeepFool algorithm superiority
- **Medium Confidence:** Grey-box and black-box attack results (39% and 43% degradation) - dependent on surrogate model quality
- **Low Confidence:** Architectural resilience claims - weak supporting evidence in corpus

## Next Checks

1. Cross-Platform Transferability: Test attack effectiveness against IDSs trained on different vehicle models
2. Semantic Effect Validation: Implement end-to-end validation verifying perturbed signals achieve intended malicious effects
3. Real-Time Injection Testing: Validate adversarial sequences can be successfully injected without CAN bus errors