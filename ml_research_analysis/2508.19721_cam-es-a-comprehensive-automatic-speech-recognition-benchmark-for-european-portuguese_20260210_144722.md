---
ver: rpa2
title: "CAM\xD5ES: A Comprehensive Automatic Speech Recognition Benchmark for European\
  \ Portuguese"
arxiv_id: '2508.19721'
source_url: https://arxiv.org/abs/2508.19721
tags:
- speech
- portuguese
- proc
- data
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces CAM\xD5ES, the first comprehensive evaluation\
  \ benchmark for European Portuguese (EP) and other Portuguese varieties, addressing\
  \ the under-representation of EP and African/Asian Portuguese in ASR research. The\
  \ benchmark includes 46h of EP test data spanning multiple domains (read speech,\
  \ broadcast news, talks/lectures, conversational speech, and sociolinguistic interviews),\
  \ along with smaller datasets for Brazilian Portuguese (BP) and African/Asian Portuguese\
  \ (AAP)."
---

# CAMÕES: A Comprehensive Automatic Speech Recognition Benchmark for European Portuguese

## Quick Facts
- arXiv ID: 2508.19721
- Source URL: https://arxiv.org/abs/2508.19721
- Reference count: 40
- First comprehensive EP ASR benchmark with 46h test data across 5 domains, achieving 35% relative WER improvement over zero-shot models

## Executive Summary
This paper introduces CAMÕES, the first comprehensive evaluation benchmark for European Portuguese (EP) and other Portuguese varieties, addressing the under-representation of EP and African/Asian Portuguese in ASR research. The benchmark includes 46h of EP test data spanning multiple domains (read speech, broadcast news, talks/lectures, conversational speech, and sociolinguistic interviews), along with smaller datasets for Brazilian Portuguese (BP) and African/Asian Portuguese (AAP). The authors evaluate a range of foundation models including Whisper Large v3, SeamlessM4T-v2, MMS-all, and Phi-4-Multimodal, both in zero-shot and fine-tuned settings, as well as E-Branchformer models trained from scratch. Using 425h of curated EP training data, they achieve state-of-the-art results across all Portuguese varieties.

## Method Summary
The authors curate a comprehensive 425h training dataset for European Portuguese and evaluate it using both zero-shot foundation models (Whisper Large v3, SeamlessM4T-v2, MMS-all, Phi-4-Multimodal) and fine-tuned models including E-Branchformer architectures. The benchmark includes 46h of EP test data across five distinct domains, plus smaller datasets for Brazilian Portuguese and African/Asian Portuguese varieties. Models are evaluated on Word Error Rate (WER) across all varieties, with particular attention to cross-dialect generalization through joint training approaches.

## Key Results
- CAMÕES benchmark established with 46h EP test data across 5 domains
- 35% relative WER improvement over strongest zero-shot foundation model
- Joint training on multi-varietal data yields robust single models that generalize across Portuguese dialects
- E-Branchformer models trained from scratch achieve state-of-the-art results across all Portuguese varieties

## Why This Works (Mechanism)
The success stems from addressing the chronic under-representation of European Portuguese in ASR research through comprehensive data curation and evaluation. By creating a multi-domain benchmark that includes not just EP but also Brazilian and African/Asian Portuguese varieties, the authors enable proper evaluation of model generalization across dialectal variation. The combination of foundation model fine-tuning with architectures specifically designed for ASR (E-Branchformer) leverages both the broad linguistic coverage of large models and the efficiency of task-specific architectures.

## Foundational Learning
- **Benchmark construction** - Why needed: Provides standardized evaluation framework for previously neglected language varieties. Quick check: Verify domain coverage matches real-world use cases.
- **Multi-dialect training** - Why needed: Portuguese has significant regional variation requiring models to handle EP, BP, and AAP. Quick check: Test cross-dialect performance on held-out varieties.
- **Foundation model adaptation** - Why needed: Leverages large-scale pretraining while adapting to specific language characteristics. Quick check: Compare fine-tuned vs zero-shot performance.
- **E-Branchformer architecture** - Why needed: Specialized ASR architecture optimized for sequence modeling efficiency. Quick check: Validate architectural improvements over standard transformers.
- **WER evaluation** - Why needed: Standard metric for ASR performance assessment. Quick check: Ensure consistent tokenization and alignment procedures.

## Architecture Onboarding

**Component Map:**
Training data (425h EP) -> Model architectures (E-Branchformer, foundation models) -> Benchmark datasets (46h EP, BP, AAP) -> WER evaluation

**Critical Path:**
Data curation → Model training/fine-tuning → Multi-domain evaluation → Cross-variety testing

**Design Tradeoffs:**
The work balances between leveraging large foundation models versus training specialized architectures from scratch, ultimately finding that E-Branchformer models achieve superior performance while requiring less computational resources than fine-tuning massive models.

**Failure Signatures:**
- Poor performance on conversational speech indicates insufficient training data for spontaneous speech
- Cross-dialect degradation suggests inadequate multi-variety exposure during training
- High WER on broadcast news may indicate domain mismatch with training data

**3 First Experiments:**
1. Fine-tune Whisper Large v3 on 425h EP data and evaluate on all benchmark domains
2. Train E-Branchformer from scratch using same training data for direct architecture comparison
3. Jointly train single model on EP + BP + AAP data to test cross-dialect generalization

## Open Questions the Paper Calls Out
None identified in source material.

## Limitations
- Evaluation focuses primarily on WER without reporting other metrics like CER or human assessment of transcription quality
- The 425h training dataset, while larger than previous efforts, remains relatively modest compared to major language benchmarks
- Comparison pool is small—only four foundation models evaluated, leaving open questions about performance against older but widely-used models

## Confidence

**Major Claim Clusters Confidence:**
- **High Confidence**: The establishment of CAMÕES as a multi-domain benchmark with diverse Portuguese varieties is well-supported by the dataset composition and evaluation protocol.
- **Medium Confidence**: Claims about "state-of-the-art" performance are substantiated for the tested model pool, but broader claims about general ASR advancement require testing against a wider range of models and languages.
- **Medium Confidence**: The assertion that joint training improves generalization across Portuguese varieties is plausible but based on limited ablation studies.

## Next Checks
1. Evaluate CAMÕES models against a broader range of ASR baselines including older Whisper versions and monolingual models to better contextualize the claimed improvements.
2. Conduct ablation studies isolating the contributions of dataset size, domain diversity, and model architecture to quantify their relative impact on WER reductions.
3. Test model robustness on out-of-domain or noisy speech data not included in the benchmark to assess real-world generalization beyond the curated test sets.