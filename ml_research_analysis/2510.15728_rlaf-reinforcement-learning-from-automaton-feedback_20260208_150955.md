---
ver: rpa2
title: 'RLAF: Reinforcement Learning from Automaton Feedback'
arxiv_id: '2510.15728'
source_url: https://arxiv.org/abs/2510.15728
tags:
- reward
- learning
- state
- function
- automaton
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents RLAF, a reinforcement learning framework that
  uses deterministic finite automaton (DFA) feedback to guide policy learning in environments
  with complex, history-dependent reward structures. Instead of manual reward engineering,
  the method generates preferences over trajectory segments based on DFA structure
  and uses these to learn a reward function via pairwise ranking loss.
---

# RLAF: Reinforcement Learning from Automaton Feedback

## Quick Facts
- arXiv ID: 2510.15728
- Source URL: https://arxiv.org/abs/2510.15728
- Reference count: 40
- Outperforms traditional reward engineering and automaton-based baselines across six environments including 12D continuous Warehouse Robotics

## Executive Summary
RLAF presents a reinforcement learning framework that replaces manual reward engineering with preferences derived from deterministic finite automata (DFA). The method generates preference orderings over trajectory segments based on DFA structure and learns a reward function via pairwise ranking loss. Two variants are proposed: a static approach that learns the reward function once, and a dynamic approach that iteratively refines both reward and policy. The framework is evaluated across discrete and continuous domains, showing better sample efficiency and task completion rates than traditional reward machines and LTL-guided approaches.

## Method Summary
RLAF uses DFA-generated preferences to learn reward functions without manual reward engineering. The method constructs a Product MDP by augmenting environment states with DFA states, then generates trajectory preferences using either subtask-based scoring (counting completed subgoals minus distance to next) or transition-value scoring (aggregating teacher Q-values). A reward model is trained via pairwise ranking loss on these preferences, and policies are optimized using standard RL algorithms (Q-learning for discrete, TD3 for continuous). The static variant learns reward once from random trajectories, while the dynamic variant iteratively refines reward and policy through multiple rounds.

## Key Results
- Static and dynamic RLAF variants outperform reward machines and LTL-guided approaches on Dungeon Quest and Mountain Car Collection
- Dynamic variant achieves better asymptotic performance through iterative refinement on discrete tasks
- Static variant performs better on continuous control tasks like Warehouse Robotics (12D state space)
- Sample efficiency improves significantly compared to baselines across all tested environments

## Why This Works (Mechanism)

### Mechanism 1: DFA-Generated Preference Orderings
The DFA encodes temporal dependencies as a state machine. Two scoring approaches generate preferences: (1) subtask-based scoring using `score(τ) = ws·Ns(τ) - wd·d(τ)` where Ns counts completed subgoals in order and d measures distance to next subgoal; (2) transition value-based scoring using `score(τ) = Σ Qdfa(qt, σt)` aggregating desirability of automaton transitions. Higher scores indicate preferred trajectories.

### Mechanism 2: Reward Learning from Pairwise Preferences
A reward function learned via pairwise ranking loss on DFA-generated preferences captures temporal structure more effectively than hand-engineered rewards. The learned reward function r̂θ((s,q),a) is trained by minimizing pairwise ranking loss: `L(θ) = Σ max(0, m - (R̂θ(τp) - R̂θ(τn)))` where τp is preferred over τn.

### Mechanism 3: Product MDP State Augmentation
Augmenting environment states with DFA states converts non-Markovian reward dependencies into Markovian form, enabling standard RL algorithms. The Product MDP Mprod = (S×Q, A, Tprod, (s0,q0), Rprod) combines environment states S with DFA states Q, making rewards depend only on current product state.

## Foundational Learning

- **Non-Markovian Reward Decision Processes (NMRDPs)**: Tasks where rewards depend on state-action history, not just current state. Why needed: RLAF addresses environments where standard Q-learning fails due to history-dependent rewards.
- **Deterministic Finite Automata (DFAs)**: State machines that encode temporal task specifications. Why needed: DFAs provide the formal structure for generating preferences and tracking subgoal completion.
- **Preference-Based Reward Learning**: Learning rewards from pairwise trajectory comparisons rather than direct reward signals. Why needed: RLAF replaces human preferences with DFA-generated ones to eliminate manual reward engineering.

## Architecture Onboarding

- **Component map**: DFA Specification -> Labeling Function L -> Preference Generator -> Reward Model r̂θ -> Policy Optimizer -> Learning Mode Controller
- **Critical path**: 1) Define DFA from task specification, 2) Implement labeling function that detects subgoal achievement, 3) Collect initial trajectories, 4) Generate preference pairs using scoring function, 5) Train reward model via ranking loss, 6) Optimize policy using learned reward, 7) (Dynamic only) Return to step 3 with updated policy
- **Design tradeoffs**: Static vs Dynamic (computational cost vs asymptotic performance), Subtask-based vs Transition value scoring (simplicity vs transfer learning), Distance metric choice (task-dependent)
- **Failure signatures**: Reward model fails to converge (check preference distribution), Policy doesn't improve (verify labeling function correctness), Good training but poor test performance (overfitting, use dynamic mode)
- **First 3 experiments**: 1) Validate preference quality in simple gridworld by computing scores manually, 2) Implement static RLAF on Dungeon Quest comparing against reward machine baseline, 3) Test transfer learning by training teacher on 5×5 Minecraft, distilling to DFA, transferring to 10×10 student environment

## Open Questions the Paper Calls Out

- **Can DFAs be learned or refined automatically from demonstrations or natural language specifications?** Current work assumes DFAs are given; manual construction becomes prohibitive for complex or large-scale environments.
- **Does the belief-state preference elicitation approach for partially observable environments empirically match its theoretical promise?** Appendix E provides theoretical formulation but no experiments validate the belief-state scoring and intrinsic motivation bonuses in practice.
- **Do automaton-guided attention and factorized representations provide practical scaling benefits in very high-dimensional spaces?** Theoretical complexity bounds are derived but not tested; the 12D Warehouse Robotics experiment is moderate-dimensional.
- **How robust is RLAF to approximate or noisy DFA specifications?** No experiments examine performance when DFA transitions are incorrect or missing; theoretical guarantees assume preference consistency.

## Limitations
- Manual construction of DFAs becomes prohibitive for complex or large-scale environments
- Critical implementation details like labeling function design and reward model architecture are not specified
- Theoretical guarantees assume Lipschitz continuity and preference consistency without empirical validation
- Transfer learning approach requires a "teacher" environment, limiting applicability

## Confidence
- **High Confidence**: DFA-based preference generation mechanism and Product MDP construction
- **Medium Confidence**: Static and dynamic learning variants' relative performance
- **Low Confidence**: Reward model architecture specifications and hyperparameter settings

## Next Checks
1. Implement and test labeling functions for high-dimensional continuous environments to validate Product MDP construction scalability
2. Conduct ablation studies comparing subtask-based vs transition-value scoring across domains to quantify transfer learning benefits
3. Validate theoretical assumptions empirically by testing preference consistency and Lipschitz continuity in environments with varying reward complexity