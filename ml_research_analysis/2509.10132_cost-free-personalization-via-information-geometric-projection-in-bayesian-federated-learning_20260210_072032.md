---
ver: rpa2
title: Cost-Free Personalization via Information-Geometric Projection in Bayesian
  Federated Learning
arxiv_id: '2509.10132'
source_url: https://arxiv.org/abs/2509.10132
tags:
- learning
- global
- local
- data
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a cost-free personalization framework for Bayesian
  Federated Learning using information-geometric projection. The method projects the
  global posterior onto a local neighborhood defined by a sphere centered at the client's
  local posterior, enabling tunable trade-offs between global generalization and local
  specialization.
---

# Cost-Free Personalization via Information-Geometric Projection in Bayesian Federated Learning

## Quick Facts
- arXiv ID: 2509.10132
- Source URL: https://arxiv.org/abs/2509.10132
- Reference count: 40
- Method achieves competitive local performance while significantly outperforming baselines on global data (84.61% FMNIST, 79.87% SVHN, 57.75% CIFAR-10)

## Executive Summary
This paper introduces a novel cost-free personalization framework for Bayesian Federated Learning (BFL) based on information-geometric projection. The method projects the global posterior onto a sphere centered at each client's local posterior, enabling tunable trade-offs between global generalization and local specialization. Under mild assumptions, this projection is mathematically equivalent to computing a barycenter, yielding closed-form solutions for specific divergences (Reverse KL, Wasserstein-2) with minimal computational overhead. The approach achieves competitive performance on local data while significantly outperforming state-of-the-art methods on global data across multiple benchmark datasets.

## Method Summary
The proposed method personalizes BFL models by computing the barycenter between global and local posteriors, controlled by a parameter λ. For Gaussian mean-field posteriors, closed-form barycenter solutions exist for both Wasserstein-2 and Reverse KL divergences. The framework requires minimal additional computation beyond standard BFL aggregation, making it truly "cost-free." During personalization, each client computes pg,k = barycenter(pg, pk; wg, wk) where wg=1/(λ+1) and wk=λ/(λ+1), allowing fine-grained control over the global-local trade-off. Experiments demonstrate the method's effectiveness across FashionMNIST, SVHN, and CIFAR-10 datasets with Dirichlet-based label shift partitioning.

## Key Results
- Achieves 84.61% accuracy on FashionMNIST, 79.87% on SVHN, and 57.75% on CIFAR-10 in global settings
- Outperforms state-of-the-art personalization methods on global data while maintaining competitive local performance
- Produces well-calibrated uncertainty estimates with favorable ECE and NLL metrics
- Demonstrates effective trade-offs between local and global performance across different λ values

## Why This Works (Mechanism)
The method works by leveraging the geometric structure of probability distributions in the information space. By projecting the global posterior onto a sphere centered at the local posterior, the framework ensures that the personalized model remains close to both the global consensus and the client's local knowledge. The equivalence to barycenter computation provides a principled way to interpolate between these distributions. This geometric interpretation naturally balances the competing objectives of leveraging global knowledge while preserving local specificity, without requiring additional optimization or communication overhead.

## Foundational Learning
- Bayesian Federated Learning: A framework where clients maintain distributions over model parameters rather than point estimates, enabling uncertainty quantification. Why needed: Provides the probabilistic foundation for personalization and uncertainty-aware decision making.
- Information Geometry: The study of statistical manifolds and the geometric structure of probability distributions. Why needed: Enables the geometric interpretation of posterior operations and the projection framework.
- Barycenter Computation: The generalization of weighted averages to probability distributions, minimizing a divergence measure. Why needed: Provides the mathematical foundation for the cost-free personalization mechanism.
- Dirichlet Distribution Partitioning: A method to simulate label distribution shift across clients by sampling label proportions from a Dirichlet distribution. Why needed: Creates realistic heterogeneity scenarios for evaluating personalization methods.
- Gaussian Mean-Field Approximation: Assuming diagonal covariance structure for posterior distributions to enable tractable computations. Why needed: Makes closed-form barycenter solutions computationally feasible.

## Architecture Onboarding

**Component Map**
IVON Optimizer -> Local Posterior (μ_k, Σ_k) -> Global Aggregation (WB/BKB) -> Barycenter Personalization (pg,k) -> Evaluation

**Critical Path**
1. Local training with IVON to obtain posterior parameters
2. Server-side aggregation using barycenter formulas
3. Client-side personalization through barycenter computation
4. Evaluation on local and global test sets

**Design Tradeoffs**
- Gaussian mean-field assumption vs. full covariance: Computational efficiency vs. expressiveness
- Closed-form solutions vs. iterative optimization: Speed and determinism vs. potential for more complex posteriors
- Single λ parameter vs. client-specific tuning: Simplicity vs. optimal personalization

**Failure Signatures**
- Poor global performance: λ too large, over-emphasizing local posteriors
- Poor local performance: λ too small, over-emphasizing global posterior
- Numerical instability: Covariance matrix singularity during barycenter computation
- Slow convergence: Inappropriate Hessian initialization for IVON optimizer

**First Experiments**
1. Verify barycenter computation by testing that pg,k reduces to pk when λ→∞ and pg when λ=0
2. Test sensitivity to λ by sweeping values and measuring local/global performance trade-offs
3. Validate closed-form solutions by comparing with numerical optimization on small examples

## Open Questions the Paper Calls Out
1. **Automatic λ adaptation**: How to dynamically adjust the personalization parameter per client based on local data characteristics to improve trade-offs over fixed λ values.
2. **Non-parametric extension**: How to extend the geometric projection framework to non-parametric Bayesian FL settings where closed-form barycenter solutions are unavailable.
3. **Feature/quantity shift evaluation**: How the method performs under feature and quantity heterogeneity beyond the label shift tested, which are common in real-world FL scenarios.
4. **Cross-device scalability**: How the framework scales to cross-device FL with thousands of clients and naturally occurring partitions, beyond the cross-silo simulations tested.

## Limitations
- Limited to diagonal covariance matrices, restricting expressiveness compared to full-covariance alternatives
- CIFAR-10 results (57.75% accuracy) suggest the shallow architecture may not be optimal for more complex datasets
- Experimental evaluation limited to label shift heterogeneity, not testing feature or quantity shift scenarios
- Unspecified communication protocol details (rounds, epochs, sampling) that could affect reproducibility

## Confidence
- **High**: Theoretical equivalence between geometric projection and barycenter computation, rigorous mathematical derivation
- **Medium**: Empirical claims about state-of-the-art performance, limited comparison set and unspecified experimental details
- **Medium**: Calibration results, limited to ECE and NLL metrics without comprehensive uncertainty evaluation

## Next Checks
1. Verify the equivalence between geometric projection and barycenter computation through ablation studies varying λ and comparing with direct optimization
2. Test the sensitivity of results to different client weighting schemes (uniform vs data-proportional) and sampling strategies
3. Evaluate scalability and performance on larger models (ResNet-18/34) and datasets (ImageNet, EMNIST) to assess real-world applicability