---
ver: rpa2
title: LLMs to Support a Domain Specific Knowledge Assistant
arxiv_id: '2502.04095'
source_url: https://arxiv.org/abs/2502.04095
tags:
- question
- questions
- industry
- answer
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work develops a domain-specific knowledge assistant for IFRS
  sustainability reporting using synthetic data generation and evaluation. The project
  creates a novel dataset of 1,063 diverse question-answer pairs using advanced LLM
  techniques, achieving average quality scores of 8.16/10.
---

# LLMs to Support a Domain Specific Knowledge Assistant

## Quick Facts
- **arXiv ID**: 2502.04095
- **Source URL**: https://arxiv.org/abs/2502.04095
- **Reference count**: 0
- **Primary result**: Novel synthetic dataset of 1,063 IFRS sustainability QA pairs with 8.16/10 quality scores; LLM-based pipeline achieves 93.45% accuracy on single-industry MCQs

## Executive Summary
This work develops a domain-specific knowledge assistant for IFRS sustainability reporting using synthetic data generation and evaluation. The project creates a novel dataset of 1,063 diverse question-answer pairs using advanced LLM techniques, achieving average quality scores of 8.16/10. Two custom architectures are developed: a RAG pipeline achieving 85.32% accuracy on single-industry and 72.15% on cross-industry questions, and an LLM-based pipeline achieving 93.45% and 80.30% accuracy respectively. The LLM-based approach outperforms the baseline by 12.80 and 27.36 percentage points. A custom industry classifier and fine-tuned Llama 3.1 8B model are integrated to improve handling of complex queries across multiple industries.

## Method Summary
The system uses a novel synthetic QA generation approach combining Chain-of-Thought reasoning and Few-shot prompting to create 1,063 IFRS-specific questions from the 1520 Sustainability Disclosure Standards document. Two custom architectures are developed: a RAG pipeline with industry classification and vector retrieval, and an LLM-based pipeline that uses industry classification with full document extraction. Both pipelines route queries through a GPT-4o-mini industry classifier and employ a fine-tuned Llama 3.1 8B model for answer generation.

## Key Results
- Synthetic dataset achieves average quality score of 8.16/10 across 1,063 questions
- LLM-based pipeline outperforms baseline by 12.80 percentage points (93.45% vs 80.65%) on single-industry MCQs
- LLM-based pipeline outperforms baseline by 27.36 percentage points (80.30% vs 52.94%) on cross-industry MCQs
- Industry classification improves handling of complex queries by reducing cross-industry retrieval noise

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Synthetic QA generation using Chain-of-Thought (CoT) and Few-shot prompting creates high-quality evaluation datasets for low-resource domains.
- **Mechanism**: The system instructs a strong LLM to first select a reference text snippet from the document before generating a question. This grounds the question. Few-shot examples then guide the style of the question to match user intent.
- **Core assumption**: The generator LLM's internal representation of "question quality" aligns with downstream task requirements.
- **Evidence anchors**: Abstract mentions synthetic QA dataset creation using CoT reasoning and few-shot prompting; Section 3.2.2 details the three-step generation process.

### Mechanism 2
- **Claim**: Pre-retrieval classification of industry sectors significantly improves answer accuracy by constraining the search space.
- **Mechanism**: An "Industry Classification" step using GPT-4o-mini labels queries with relevant industry tags before retrieval, filtering the vector database metadata to prevent cross-contamination.
- **Core assumption**: Queries in this domain are inherently siloed by industry.
- **Evidence anchors**: Abstract mentions integration of industry classification; Section 4.4.1 explains how classification prevents confusing chunks from multiple industries.

### Mechanism 3
- **Claim**: An "LLM-as-Retriever" pattern outperforms traditional vector-similarity retrieval for complex domain logic.
- **Mechanism**: The LLM-based pipeline bypasses vector embeddings, passing the entire relevant industry markdown to an LLM which extracts relevant chunks dynamically for each query.
- **Core assumption**: Semantic loss from embedding models outweighs the cost of using an LLM to read documents directly.
- **Evidence anchors**: Abstract shows LLM-based pipeline achieving 93.45% accuracy; Section 4.2.4 explains the flexibility of dynamic chunk extraction.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: Understanding how RAG splits documents into chunks and retrieves them via similarity is required to appreciate why the LLM-based pipeline outperformed it.
  - Quick check question: What is the primary advantage of RAG over pure fine-tuning for knowledge-intensive tasks?

- **Concept: LoRA (Low-Rank Adaptation)**
  - Why needed here: The paper utilizes LoRA to fine-tune Llama 3.1 8B, allowing the model to internalize domain knowledge without prohibitive cost.
  - Quick check question: How does LoRA reduce the computational cost of fine-tuning a Large Language Model?

- **Concept: Prompt Engineering (Chain-of-Thought)**
  - Why needed here: The quality of the synthetic dataset relies entirely on prompt engineering techniques like CoT and Few-shot prompting.
  - Quick check question: Why is Few-shot prompting particularly useful for generating domain-specific questions?

## Architecture Onboarding

- **Component map**: User Query -> Domain Relevance Check (LLM) -> Industry Classifier (GPT-4o-mini) -> Filter Vector DB/Load Full Markdown -> KNN Retrieval/LLM Context Extractor -> Context Construction -> Fine-tuned Llama 3.1 8B -> Answer
- **Critical path**: The Industry Classifier is the single point of failure for both pipelines. If it misclassifies the industry, the RAG pipeline filters out the answer, and the LLM-pipeline loads the wrong document.
- **Design tradeoffs**: The LLM-based pipeline achieves ~8% higher accuracy but requires multiple sequential LLM calls, significantly increasing latency compared to vector search.
- **Failure signatures**: Baseline RAG drops to ~52% accuracy on cross-industry questions due to retrieval of noisy chunks from wrong industries; without CoT generation, dataset contains vague questions that do not test domain-specific IFRS knowledge.
- **First 3 experiments**:
  1. Run the baseline RAG pipeline with no industry filtering vs. perfect industry filtering to measure upper bound of retrieval improvement.
  2. Test "Custom Markdown" chunking vs. "Semantic" chunking to verify if preserving table structures improves metric retrieval.
  3. Compare Base Llama 3.1 8B vs. Fine-tuned Llama 3.1 8B on the synthetic MCQ dataset to isolate the contribution of domain adaptation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the synthetic QA dataset retain high quality when validated by human domain experts compared to the automated evaluation framework?
- Basis in paper: Section 5.2 lists "Human QA evaluation" as future work to systematically verify the quality of generated questions and answers.
- Why unresolved: The project relied on LLM-based evaluation metrics due to cost, which may fail to evaluate questions at the human level.
- What evidence would resolve it: A systematic study where domain experts grade the 1,063 questions on the defined quality metrics.

### Open Question 2
- Question: How do the proposed architectures perform in real-world scenarios regarding user experience, usefulness, and context awareness?
- Basis in paper: Section 5.2 states the chatbot has not been evaluated through the lens of user experience or long-context dialogue.
- Why unresolved: Evaluation relied solely on MCQ accuracy and BLEU/ROUGE scores, which do not capture subjective user utility.
- What evidence would resolve it: User studies measuring usefulness and factual accuracy on Likert scales.

### Open Question 3
- Question: Can a hybrid RAG system optimize performance by selectively applying multi-query transforms only to cross-industry questions?
- Basis in paper: Section 5.2 proposes a "Cross-industry multi-query" pipeline to leverage the technique only where it displays advantages.
- Why unresolved: Experiments showed multi-query transforms significantly hurt local factual accuracy while aiding complex cross-industry queries.
- What evidence would resolve it: Performance results from a pipeline that conditionally applies query expansion based on industry scope detection.

## Limitations
- The system's performance is heavily dependent on the quality of the synthetic QA dataset, which was evaluated primarily through LLM metrics rather than human expert validation.
- The 13-percentage-point drop in cross-industry accuracy reveals fundamental brittleness in the industry classification mechanism.
- The system is evaluated only on multiple-choice questions with fixed answer options, leaving open-ended IFRS compliance advice untested.

## Confidence
- **High Confidence**: The comparative advantage of the LLM-based pipeline over baseline RAG is well-supported with statistical separation.
- **Medium Confidence**: The industry classification mechanism's impact is demonstrated but relies on a single document collection.
- **Low Confidence**: The synthetic dataset generation quality is asserted through LLM evaluation rather than human-expert validation across diverse IFRS scenarios.

## Next Checks
1. Have IFRS practitioners evaluate a stratified sample of 100 synthetic questions across all quality dimensions to validate the LLM evaluation scores.
2. Test the complete pipeline on a second IFRS document (e.g., IFRS 16 Leases) to verify generalization beyond the 1520 standard.
3. Replace the multiple-choice format with open-ended IFRS compliance questions requiring synthesis across multiple sections, measuring both accuracy and hallucination rates.