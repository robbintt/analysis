---
ver: rpa2
title: 'Fairy2i: Training Complex LLMs from Real LLMs with All Parameters in $\{\pm
  1, \pm i\}$'
arxiv_id: '2512.02901'
source_url: https://arxiv.org/abs/2512.02901
tags:
- quantization
- arxiv
- complex
- real
- real-valued
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Fairy2i, a universal framework that enables
  the transformation of pre-trained real-valued large language models (LLMs) into
  complex-valued representations, allowing for extreme low-bit quantization while
  reusing existing checkpoints. The core innovation lies in a mathematical reparameterization
  that expresses any real-valued linear layer as an equivalent widely-linear complex
  form, preserving exact equivalence in forward computation.
---

# Fairy2i: Training Complex LLMs from Real LLMs with All Parameters in $\{\pm 1, \pm i\}$

## Quick Facts
- arXiv ID: 2512.02901
- Source URL: https://arxiv.org/abs/2512.02901
- Reference count: 40
- Primary result: Achieves C4 perplexity of 7.85 on LLaMA-2 7B with effective 2-bit precision

## Executive Summary
Fairy2i introduces a universal framework for transforming pre-trained real-valued large language models into complex-valued representations using only parameters from the set $\{\pm 1, \pm i\}$. The method leverages a mathematical reparameterization that expresses any real-valued linear layer as an equivalent widely-linear complex form, enabling exact forward equivalence without information loss. By employing phase-aware quantization with fourth roots of unity and recursive residual quantization, Fairy2i achieves extreme low-bit compression (2-bit effective precision) while maintaining near-full-precision performance on LLaMA-2 7B.

## Method Summary
Fairy2i transforms real-valued LLMs through three key innovations: (1) a widely-linear complex representation that exactly captures any real linear layer with even dimensions, (2) phase-aware quantization using the four fourth roots of unity $\{\pm 1, \pm i\}$ with axis-wise scaling, and (3) recursive residual quantization that iteratively reduces approximation error. The method applies quantization-aware training with straight-through estimators to existing checkpoints, avoiding training from scratch. Applied to LLaMA-2 7B with 30B training tokens, Fairy2i achieves C4 perplexity of 7.85 with T=2 recursive stages (2-bit effective precision).

## Key Results
- C4 perplexity of 7.85 on LLaMA-2 7B with T=2 recursive stages (2-bit effective precision)
- Outperforms state-of-the-art real-valued binary and ternary quantization methods at equivalent bit budgets
- Approaches full-precision baseline performance while enabling extreme low-bit compression
- Recursive quantization improves PPL from 11.03 (T=1) to 8.74 (T=2) with only 50% additional storage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Any real-valued linear layer with even dimensions can be exactly represented as a widely-linear complex form without loss of information.
- Mechanism: Real weight matrix R ∈ ℝ^(2n×2m) decomposes into complex matrices U, W ∈ ℂ^(n×m) via Re(U) = ½(R₁₁ + R₂₂), Im(U) = ½(R₂₁ - R₁₂), Re(W) = ½(R₁₁ - R₂₂), Im(W) = ½(R₁₂ + R₂₁). Output computed as y = Ux + Wx̄.
- Core assumption: Real linear layer has even input and output dimensions.
- Evidence anchors: [abstract] "lossless mathematical equivalence between real and widely-linear maps"; [Section 3.1] Theorem 1 with uniqueness proof.
- Break condition: Odd dimensions requiring padding, or conjugate operations cannot be efficiently implemented.

### Mechanism 2
- Claim: Phase-aware quantization to fourth roots of unity provides more efficient bit utilization than real-valued ternary quantization.
- Mechanism: Project each complex weight w to nearest codeword by angle θ = Arg(w), selecting from {±1,±i} based on quadrant. Apply separate per-tensor scaling factors s_re and s_im along each axis.
- Core assumption: Weight distribution has sufficient angular spread to benefit from four directional choices.
- Evidence anchors: [abstract] "phase-aware complex quantization with highly efficient codebook of fourth roots of unity"; [Section 3.2] Equations 10-12 defining deterministic projection.
- Break condition: Weights cluster predominantly along one axis, degrading to binary representation.

### Mechanism 3
- Claim: Recursive quantization of residuals reduces approximation error with linear storage increase but constant inference latency.
- Mechanism: After initial quantization, compute residual R^(t+1) = R^(t) - Ŵ^(t) and quantize using same codebook. Deployed weight W_q ≈ Σₜ Ŵ^(t). Parallel execution enables O(1) latency.
- Core assumption: Sufficient parallel compute units exist for simultaneous stage execution.
- Evidence anchors: [abstract] "recursive residual quantization mechanism that iteratively minimizes quantization error"; [Section 3.3] Algorithm 1 and Equation 13.
- Break condition: Hardware cannot parallelize stage execution, causing linear latency scaling.

## Foundational Learning

- Concept: **Widely-linear (augmented) complex filtering**
  - Why needed here: Standard complex linear maps cannot represent all real linear maps due to missing conjugate term; widely-linear form y = Ux + Wx̄ is necessary for lossless conversion.
  - Quick check question: Can a standard complex matrix U ∈ ℂ^(n×m) represent any real matrix R ∈ ℝ^(2n×2m)? (Answer: No—the conjugate term is required for completeness.)

- Concept: **Straight-through estimator (STE) for quantization-aware training**
  - Why needed here: Quantization projection is non-differentiable; STE treats discretization as identity in backward pass, allowing gradients to flow to full-precision master weights.
  - Quick check question: Why must master weights be maintained in full precision during QAT? (Answer: To preserve gradient information and allow continuous optimization despite discrete forward passes.)

- Concept: **Fourth roots of unity as a 2-bit codebook**
  - Why needed here: Set {±1,±i} has exactly 4 elements encoding cleanly in 2 bits. Unlike ternary {+1,0,-1} which underutilizes 2-bit encoding space, this codebook fully utilizes available representational capacity.
  - Quick check question: Why might {±1,±i} outperform ternary quantization at same nominal bit-width? (Answer: Four uniformly distributed directions on unit circle versus three values including zero provides better angular coverage for complex weight distributions.)

## Architecture Onboarding

- Component map: Input transformation -> Parameter transformation -> Phase quantizer -> Recursive stages -> Inference kernel
- Critical path:
  1. Verify even dimensions in all linear layers (Q, K, V, O projections; up/gate/down projections)
  2. Transform R → (U, W) for each layer using Equation 3
  3. Initialize QAT with FP16 master weights; apply PhaseQuant in forward pass
  4. Train on sufficient tokens (paper uses 30B) with WSD learning rate schedule
  5. Export quantized codes {B^(t)_re, B^(t)_im} and scales {s^(t)_re, s^(t)_im} for inference

- Design tradeoffs:
  - T=1 (1-bit): Minimal storage, higher PPL (~11.0); suitable for extreme compression
  - T=2 (2-bit): Recommended; best accuracy-storage balance (PPL ~7.85)
  - T=3 (3-bit): Diminishing returns; 50% more storage for marginal accuracy gain
  - Per-tensor vs. per-group scaling: Paper uses per-tensor; group scaling may improve accuracy at cost of metadata

- Failure signatures:
  - Dimension mismatch errors during transformation indicate odd-dimension layers requiring padding
  - Exploding loss early in QAT suggests learning rate too high or insufficient warmup
  - PPL plateau above expected suggests insufficient training tokens or suboptimal learning rate schedule
  - Slow inference despite quantization indicates lack of parallel stage implementation

- First 3 experiments:
  1. **Unit test**: Transform a small real matrix (e.g., 4×4) to widely-linear form, verify R = R_lin(U) + R_conj(W) and that forward pass produces identical outputs
  2. **Overfit test**: Apply Fairy2i-W1 to a single batch; confirm training loss decreases despite hard quantization (validates STE implementation)
  3. **Scaling ablation**: Train Fairy2i-W2 with T=1, T=2, T=3 on held-out validation set to reproduce Table 2 and verify T=2 is optimal for target hardware

## Open Questions the Paper Calls Out

- Can Fairy2i surpass the accuracy of the original full-precision baseline with extended training on larger datasets?
- How does Fairy2i perform when scaled to larger models (e.g., LLaMA-3 70B) and multimodal architectures?
- What is the impact of different grouping strategies for scaling factors in phase-aware complex quantization?
- What theoretical properties of the complex-valued loss landscape explain the robustness observed in low-bit regimes?

## Limitations

- Dimension constraints requiring even input/output dimensions for all linear layers, with unclear handling of odd dimensions
- Dataset reproducibility challenges due to vague specification of RedPajama subset used for training
- Training dynamics uncertainty from unspecified STE implementation details and precise WSD schedule parameters

## Confidence

**High confidence**: Mathematical proof of lossless real-to-complex transformation via widely-linear maps (Theorem 1) is rigorous and verifiable; phase-aware quantization mechanism is clearly defined; hardware efficiency claims for parallel execution are logically sound.

**Medium confidence**: Empirical results showing PPL of 7.85 on C4 are reproducible in principle but depend on unspecified training dataset details and STE implementation; claim of outperforming state-of-the-art real-valued quantization methods is supported but comparison methodology could benefit from more transparency.

**Low confidence**: Claim about minimal additional storage cost for recursive stages (50% increase from T=1 to T=2) lacks detailed analysis of memory bandwidth implications; assertion that this approach "reuses existing checkpoints" doesn't fully address potential compatibility issues with different model architectures.

## Next Checks

1. **Mathematical verification test**: Implement the widely-linear transformation on a small random real matrix (4×4 or 6×6) and verify that forward passes with the original real matrix and the transformed complex pair produce numerically identical outputs.

2. **Scale computation validation**: For a single layer from a pre-trained checkpoint, implement the phase-aware quantization with exact scale computation per Equation 11, verify that the quantized forward pass with STE training can recover training loss on a single batch, and compare computed scales against manually calculated reference values.

3. **Dimensionality audit**: Systematically analyze all linear layers in LLaMA-2 7B architecture to identify which layers have odd dimensions, document the padding strategy used for these layers, and quantify the impact on parameter count and potential performance degradation through ablation studies.