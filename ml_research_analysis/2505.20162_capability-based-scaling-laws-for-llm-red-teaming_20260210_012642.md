---
ver: rpa2
title: Capability-Based Scaling Laws for LLM Red-Teaming
arxiv_id: '2505.20162'
source_url: https://arxiv.org/abs/2505.20162
tags:
- uni00000014
- uni00000012
- uni00000016
- uni00000018
- uni0000001c
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) are becoming increasingly capable
  and agentic, raising the importance of red-teaming for safe deployment. Traditional
  prompt-engineering approaches may become ineffective as models surpass red-teamers
  in capabilities, turning red-teaming into a weak-to-strong problem.
---

# Capability-Based Scaling Laws for LLM Red-Teaming

## Quick Facts
- arXiv ID: 2505.20162
- Source URL: https://arxiv.org/abs/2505.20162
- Reference count: 40
- Key outcome: Jailbreaking scaling law predicts attack success based on attacker-target capability gap

## Executive Summary
This paper establishes that the effectiveness of red-teaming LLM safety measures fundamentally depends on the capability gap between attacker and target models. Through systematic evaluation of over 600 attacker-target pairs using LLM-based jailbreak attacks, the authors demonstrate that attack success drops sharply when targets surpass attackers in capability. The findings suggest that as models become more capable, traditional fixed-capability red-teaming approaches (including human red-teamers) may become ineffective, necessitating new safety evaluation paradigms that account for capability asymmetries.

## Method Summary
The researchers evaluated 612 attacker-target model pairs across diverse model families, sizes, and capability levels using LLM-based jailbreak attacks designed to mimic human red-teamers. They measured attack success rates and correlated these with performance on MMLU-Pro social science benchmark splits. From this empirical data, they derived a jailbreaking scaling law that predicts attack success rates for fixed targets based on the capability gap between attacker and target models. The study frames red-teaming as a weak-to-strong problem where traditional approaches may fail as target models surpass attacker capabilities.

## Key Results
- More capable models consistently perform better as attackers
- Attack success rates drop sharply once target capability exceeds attacker capability
- Attack success correlates with high performance on MMLU-Pro social science benchmark splits

## Why This Works (Mechanism)
The scaling law emerges from fundamental capability asymmetries between attackers and targets. When attackers have lower capability than targets, they lack the persuasive and manipulative abilities needed to circumvent safety measures. The correlation with social science benchmarks suggests that models excelling at understanding human behavior and social dynamics become more effective at jailbreaking. This creates a natural barrier where increasingly capable models become resistant to attacks from less capable adversaries.

## Foundational Learning
- Capability gap dynamics: Understanding how asymmetric capabilities affect attack success
- Scaling law formulation: Mathematical relationship between capability differences and jailbreak rates
- Benchmark correlation analysis: Linking specific capability dimensions to attack effectiveness
- Red-teaming paradigm shift: Recognizing limitations of fixed-capability evaluation approaches
- Model capability measurement: Accurately assessing and comparing different models' abilities

## Architecture Onboarding
**Component Map**: Attacker models -> Jailbreak generation -> Target models -> Safety evaluation -> Success measurement
**Critical Path**: Capability assessment → Attack generation → Success evaluation → Scaling law derivation
**Design Tradeoffs**: Fixed capability evaluation vs. dynamic capability development, benchmark specificity vs. generalizability
**Failure Signatures**: Attack success plateaus when targets exceed attacker capabilities, correlation breakdown with non-social benchmarks
**First Experiments**: 
1. Test scaling law predictions against human red-teamers with documented expertise levels
2. Evaluate alternative safety benchmarks beyond MMLU-Pro social science
3. Conduct longitudinal studies tracking attack success as capabilities evolve

## Open Questions the Paper Calls Out
None

## Limitations
- Generalizability beyond specific attack methodologies employed
- Static framing may underestimate adaptive nature of real-world red-teaming
- Narrow focus on MMLU-Pro social science benchmark correlation

## Confidence
- Fixed-capability attackers becoming ineffective: Medium
- Open-source models amplifying risks: Medium
- Scaling law predictive accuracy: High within studied parameters, Low for extreme differentials

## Next Checks
1. Test the scaling law's predictions against human red-teamers with documented expertise levels to verify if the capability gap framework holds for human attackers
2. Evaluate attack success rates using alternative safety benchmarks beyond MMLU-Pro social science to determine if the correlation generalizes to other capability dimensions
3. Conduct longitudinal studies tracking attack success rates as both attacker and target models undergo capability improvements to validate the static capability gap assumption against dynamic capability development scenarios