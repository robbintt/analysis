---
ver: rpa2
title: Tensor Generalized Approximate Message Passing
arxiv_id: '2504.00008'
source_url: https://arxiv.org/abs/2504.00008
tags:
- tensor
- teg-amp
- where
- algorithm
- decomposition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Tensor Generalized Approximate Message Passing
  (TeG-AMP), a novel algorithm for low-rank tensor inference problems including tensor
  completion and decomposition. The method is derived as an approximation of the sum-product
  belief propagation algorithm in high dimensions, leveraging central limit theorem
  and Taylor series approximations.
---

# Tensor Generalized Approximate Message Passing

## Quick Facts
- arXiv ID: 2504.00008
- Source URL: https://arxiv.org/abs/2504.00008
- Reference count: 40
- Key outcome: TeG-AMP achieves up to 80% error reduction compared to state-of-the-art methods in MNIST digit recovery tasks

## Executive Summary
This paper introduces Tensor Generalized Approximate Message Passing (TeG-AMP), a novel algorithm for low-rank tensor inference problems including completion and decomposition. The method is derived as an approximation of sum-product belief propagation in high dimensions, leveraging central limit theorem and Taylor series approximations. TeG-AMP is developed based on the general Tensor Ring (TR) decomposition model, making it applicable to various low-rank tensor types including CP, TT, and Tucker decompositions. The algorithm demonstrates significant performance improvements over state-of-the-art methods like BiG-AMP and Alternating Minimization, particularly in handling noise and achieving higher recovery accuracy.

## Method Summary
TeG-AMP is developed by approximating the sum-product belief propagation algorithm for high-dimensional tensor inference problems. The method treats tensor entries as independent random variables and uses central limit theorem approximations to derive iterative mean and variance update rules. The algorithm is based on Tensor Ring (TR) decomposition, which provides a unified framework for various low-rank tensor formats. A simplified version called TeS-AMP is proposed specifically for low CP-rank tensor inference problems, reducing computational complexity while maintaining performance.

## Key Results
- Achieves up to 80% error reduction in MNIST digit recovery compared to BiG-AMP and Alternating Minimization
- Demonstrates improved robustness to noise in tensor completion tasks
- Shows better recovery performance for various low-rank tensor decompositions including CP, TT, and Tucker formats

## Why This Works (Mechanism)
TeG-AMP leverages the high-dimensional structure of tensor problems to apply central limit theorem approximations, enabling efficient iterative updates for mean and variance estimates. The Tensor Ring decomposition provides a flexible framework that unifies different low-rank tensor representations, allowing the algorithm to adapt to various problem structures. By treating tensor entries as independent variables and using Gaussian approximations for message passing, the algorithm achieves computational efficiency while maintaining good recovery performance.

## Foundational Learning
1. **Tensor Ring Decomposition** - Why needed: Provides unified framework for various low-rank tensor formats
   Quick check: Verify tensor can be represented in TR format with appropriate bond dimensions

2. **Sum-Product Belief Propagation** - Why needed: Forms theoretical foundation for message passing updates
   Quick check: Confirm convergence conditions for BP on loopy factor graphs

3. **Central Limit Theorem Approximations** - Why needed: Enables efficient mean and variance calculations in high dimensions
   Quick check: Validate approximation quality for specific tensor dimensions and ranks

## Architecture Onboarding

Component Map:
TR decomposition -> Mean/Variance updates -> Damping mechanism -> Convergence check

Critical Path:
1. Initialize tensor factors and parameters
2. Iteratively update mean and variance estimates using CLT approximations
3. Apply damping to ensure convergence
4. Check stopping criteria and output recovered tensor

Design Tradeoffs:
- Computational complexity vs. approximation accuracy
- TR decomposition flexibility vs. specialized format efficiency
- Damping strength vs. convergence speed

Failure Signatures:
- Divergence due to insufficient damping
- Poor recovery when tensor entries are highly correlated
- Computational bottlenecks in high-dimensional settings

First Experiments:
1. Test on synthetic tensors with known CP/TT/Tucker ranks
2. Compare convergence behavior with varying damping parameters
3. Evaluate performance on tensors with different noise levels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the computational complexity of calculating the mean and variance updates in the TeG-AMP algorithm be further simplified?
- Basis in paper: Section 6.1 explicitly states: "Future works will be to find ways to further simplify the computation of the mean and variance in the algorithm."
- Why unresolved: The authors note that the complexity of TeG-AMP ($O(d^4 \prod r_i)$) is higher than alternating minimization. While a simplified version (TeS-AMP) was proposed for CP decomposition, the general TR-decomposition algorithm remains computationally intensive.
- What evidence would resolve it: A modified iteration scheme or closed-form approximation that reduces the asymptotic complexity of the variance calculations (Equation 25) without significantly sacrificing recovery accuracy.

### Open Question 2
- Question: How robust is TeG-AMP when the assumption of statistically independent tensor entries is violated in real-world data?
- Basis in paper: The derivation in Section 2.2 (Equation 4) assumes "independent priors of each element," and Section 3.3 relies on this independence for Central Limit Theorem approximations.
- Why unresolved: The authors apply the method to MNIST digits, which possess strong spatial correlations among pixels, yet the paper does not quantify how dependent data structures affect the validity of the approximations or convergence.
- What evidence would resolve it: An analysis of recovery error specifically comparing performance on synthetic tensors with independent entries versus those with correlated (e.g., Markovian) structures.

### Open Question 3
- Question: What are the theoretical performance guarantees for TeG-AMP in finite-dimensional settings where large-system limit approximations fail?
- Basis in paper: Section 3.3 notes that derivations assume large system limits ($r, d, N \to \infty$), and Section 3.6 admits that "in practical applications... these dimensions are finite, and hence the algorithm may diverge."
- Why unresolved: While adaptive damping is used as a heuristic fix, the paper does not provide theoretical bounds or guarantees for convergence and estimation error when the tensor size is small.
- What evidence would resolve it: A theoretical derivation of the state evolution for finite-sized tensors, or empirical error bounds that hold for small $N$ and $d$.

## Limitations
- Theoretical foundations rely heavily on asymptotic approximations that may not hold in finite-dimensional settings
- Empirical validation limited to single dataset type (MNMNIST digits), raising questions about generalizability
- Computational complexity analysis incomplete, particularly regarding scalability to very high-dimensional tensors

## Confidence
- **High confidence**: Theoretical derivation of TeG-AMP from sum-product belief propagation is well-established
- **Medium confidence**: Performance improvements over existing methods are demonstrated but require broader validation
- **Low confidence**: Claims about robustness to noise and scalability to high-dimensional tensors need further empirical support

## Next Checks
1. Conduct experiments on diverse tensor datasets beyond MNIST, including both synthetic and real-world applications
2. Perform systematic ablation studies to evaluate the impact of hyperparameters and approximation assumptions
3. Compare computational efficiency and scalability with additional state-of-the-art tensor inference methods on large-scale problems