---
ver: rpa2
title: The Best Instruction-Tuning Data are Those That Fit
arxiv_id: '2502.04194'
source_url: https://arxiv.org/abs/2502.04194
tags:
- data
- arxiv
- training
- grape
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "GRAPE improves instruction-tuning by selecting responses aligned\
  \ with the base model\u2019s pretrained distribution. It gathers multiple candidate\
  \ responses per instruction, ranks them by normalized probability under the target\
  \ model, and selects the highest-scoring one."
---

# The Best Instruction-Tuning Data are Those That Fit

## Quick Facts
- **arXiv ID:** 2502.04194
- **Source URL:** https://arxiv.org/abs/2502.04194
- **Authors:** Dylan Zhang; Qirun Dai; Hao Peng
- **Reference count:** 40
- **Primary result:** GRAPE improves instruction-tuning by selecting responses aligned with the base model's pretrained distribution.

## Executive Summary
GRAPE introduces a novel approach to instruction-tuning that selects responses based on alignment with the base model's pretrained distribution rather than relying on external rewards or preference models. By ranking candidate responses using normalized probability under the target model and selecting the highest-scoring ones, GRAPE avoids distribution shift and improves training efficiency. The method demonstrates significant performance gains across multiple benchmarks, using less data and fewer epochs than traditional approaches while outperforming stronger generators.

## Method Summary
GRAPE is a response-centric data selection framework for instruction-tuning that operates by gathering multiple candidate responses per instruction from a reference model, then ranking them using the target model's normalized log-probability. The method selects the highest-scoring response for training, ensuring that fine-tuning data stays aligned with the model's pretrained distribution. This approach differs from traditional instance-level selection by focusing on the quality of individual responses rather than filtering entire instructions. GRAPE is simple, scalable, and effective across diverse model families, requiring only the ability to generate multiple candidate responses per instruction.

## Key Results
- GRAPE achieves up to 13.8% improvement over the strongest generator and 17.3% over training with 3× more data on UltraInteract-SFT
- On Tulu3/Olmo-2 mixtures, GRAPE outperforms strong baselines with 4.5× more data by up to 6.1%
- Llama3.1-8B using GRAPE surpasses Tulu3-SFT with only 1/3 of the data and half the epochs

## Why This Works (Mechanism)
The core insight is that instruction-tuning data should be selected to maintain alignment with the base model's pretrained distribution. By using the target model's own probability estimates to rank candidate responses, GRAPE ensures that fine-tuning samples come from a distribution the model is already optimized to handle. This avoids the distribution shift that occurs when using externally generated or reward-model-selected responses, leading to more stable and efficient training.

## Foundational Learning
- **Instruction-tuning:** Fine-tuning language models on instruction-response pairs to improve task-following capabilities. Why needed: Core problem being addressed. Quick check: Model can follow novel instructions accurately.
- **Distribution alignment:** Ensuring training data matches the statistical properties of the model's pretrained distribution. Why needed: Prevents catastrophic forgetting and instability. Quick check: KL divergence between pretrained and fine-tuned distributions remains low.
- **Response selection:** Choosing the best response from multiple candidates for training. Why needed: Improves data quality without requiring additional human annotation. Quick check: Selected responses have higher quality scores than alternatives.
- **Probability-based ranking:** Using model likelihood as a quality signal for response selection. Why needed: Provides an intrinsic, model-specific measure of response quality. Quick check: Higher probability responses correlate with better downstream performance.

## Architecture Onboarding

### Component Map
Reference Model -> Candidate Generation -> Target Model -> Probability Ranking -> Response Selection -> Fine-tuning Pipeline

### Critical Path
1. Generate multiple candidate responses per instruction using reference model
2. Compute normalized log-probabilities of each response under target model
3. Select highest-scoring response for training
4. Fine-tune target model on selected response-instruction pairs

### Design Tradeoffs
- **Single vs. multiple reference models:** Using one reference model simplifies the pipeline but may limit diversity. Using multiple could improve coverage but increases complexity.
- **Sequence-level vs. token-level probability:** Sequence-level is computationally efficient but may miss local quality issues. Token-level is more granular but computationally expensive.
- **Fixed vs. iterative selection:** Fixed selection is simpler but iterative could compound improvements. Iterative requires more compute and risks overfitting.

### Failure Signatures
- **Overfitting to reference model:** If GRAPE consistently selects responses similar to the reference model, the target model may not learn new capabilities.
- **Loss of diversity:** If probability ranking favors conservative responses, the model may become less creative or capable of handling edge cases.
- **Computational bottleneck:** If candidate generation becomes too expensive, the efficiency gains may be negated.

### Exactly 3 First Experiments
1. Compare GRAPE-selected responses against human-annotated responses on the same instructions to validate quality.
2. Test GRAPE with different reference model qualities (strong vs. weak) to assess robustness.
3. Apply GRAPE to a completely different task domain (e.g., code generation) to test generalizability.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can GRAPE's response-centric selection be effectively combined with instance-level (instruction) data selection methods to simultaneously optimize curriculum and supervision?
- Basis in paper: [explicit] The "Discussion and Limitations" section states that GRAPE differs from instance-level selection but "this response-centric perspective is complementary... both might be combined to enhance overall data quality."
- Why unresolved: The authors maintained a fixed set of instructions across experiments to isolate the effect of response selection, leaving the interaction between selecting optimal instructions and optimal responses untested.
- What evidence would resolve it: A study evaluating a pipeline that applies instruction filtering (e.g., difficulty or diversity-based) prior to applying GRAPE for response selection, compared to either method in isolation.

### Open Question 2
- Question: How robust is GRAPE when applied to candidate pools with high rates of incorrect or low-quality responses, particularly for non-verifiable tasks?
- Basis in paper: [explicit] The "Limitations" section notes that "GRAPE assumes a quality-controlled candidate pool from which to select samples" and that effectiveness may be influenced by the model's capabilities.
- Why unresolved: The controlled experiments used verified reasoning datasets (e.g., UltraInteract), and the real-world experiments used curated preference datasets (Tulu3/Olmo-2), effectively ensuring a baseline of quality that may not exist in raw synthetic data generation.
- What evidence would resolve it: Experiments applying GRAPE to noisy candidate pools (e.g., unfiltered outputs from weaker models) in open-ended generation tasks to see if the probability metric filters out noise or erroneously selects fluent but incorrect responses.

### Open Question 3
- Question: Does incorporating token-level likelihood weighting into the loss function yield consistent improvements over the sequence-level probability ranking used for data selection?
- Basis in paper: [explicit] Appendix B.4 ("Token-level GRAPE") describes an alternative approach where loss is weighted by token likelihood, noting it "yields consistent improvements" and suggesting further investigation of the insight.
- Why unresolved: The main GRAPE framework relies on selecting whole responses based on sequence-level normalized log-probability, while the token-level variant was tested only briefly on the OpenThoughts dataset.
- What evidence would resolve it: A comparative analysis of standard SFT, sequence-level GRAPE, and token-level weighted SFT across diverse benchmarks and model architectures to determine if the granular approach offers superior distribution alignment.

### Open Question 4
- Question: What are the convergence properties and performance limits of iteratively applying GRAPE (Multi-Round GRAPE)?
- Basis in paper: [explicit] Appendix G ("Multi-Round GRAPE") demonstrates that a second round of fine-tuning with re-selected responses improves performance, noting the method "remains effective and even compounding."
- Why unresolved: The paper only evaluates two rounds of training; it does not identify the point of diminishing returns where the model might suffer from overfitting to the narrowed distribution or lack of external diversity.
- What evidence would resolve it: An extended experiment running iterative GRAPE selection and training for 5 or more rounds while monitoring for performance plateaus or degradation on generalization benchmarks.

## Limitations
- GRAPE's effectiveness depends on the quality of the reference model used for candidate generation and ranking, which may not generalize across all model families.
- The method requires generating multiple candidate responses per instruction, introducing computational overhead that may offset efficiency gains in some scenarios.
- Performance improvements are demonstrated primarily on instruction-tuning tasks and specific benchmarks, with limited evaluation of generalizability to other fine-tuning scenarios.

## Confidence
- **High**: The core finding that aligning instruction-tuning data to the base model's distribution improves performance, supported by multiple experiments and ablation studies.
- **Medium**: The claimed efficiency gains (e.g., "3× less data") and scalability across model families, as these depend on specific experimental conditions and reference model choices.
- **Low**: The generalizability of GRAPE to non-instruction-tuning fine-tuning scenarios and its robustness to different reference model qualities.

## Next Checks
1. Evaluate GRAPE's performance using multiple reference models (not just Tulu3) to assess robustness to reference quality and model family differences.
2. Test GRAPE on a broader set of datasets and evaluation benchmarks, including non-instruction-tuning tasks, to establish generalizability.
3. Quantify and compare the computational overhead of candidate generation and ranking against the training efficiency gains.