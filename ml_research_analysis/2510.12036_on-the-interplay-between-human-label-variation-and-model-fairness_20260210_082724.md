---
ver: rpa2
title: On the Interplay between Human Label Variation and Model Fairness
arxiv_id: '2510.12036'
source_url: https://arxiv.org/abs/2510.12036
tags:
- fairness
- training
- each
- section
- sbic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the interplay between human label variation
  (HLV) and model fairness, a previously unexplored area. HLV has been shown to improve
  model generalization by preserving minority views, but its impact on fairness is
  unknown.
---

# On the Interplay between Human Label Variation and Model Fairness

## Quick Facts
- arXiv ID: 2510.12036
- Source URL: https://arxiv.org/abs/2510.12036
- Reference count: 28
- Key finding: HLV training methods consistently improve performance without harming fairness across most configurations, and can even improve fairness when groups/classes are weighted equally

## Executive Summary
This paper investigates the previously unexplored relationship between human label variation (HLV) and model fairness. While HLV has been shown to improve model generalization by preserving minority views, its impact on fairness remains unknown. The study compares training on majority-vote labels with four HLV methods across two tasks: offensive classification (SBIC) and legal area classification (TAG). The findings reveal that HLV training methods consistently improve overall performance without harming fairness across most configurations. Notably, HLV can even improve fairness for a substantial number of configurations, particularly when all groups/classes are weighted equally. The analysis demonstrates that fairness improvements stem from minority annotations, as shown through temperature scaling experiments that weight minority annotations more heavily.

## Method Summary
The study employs a comprehensive experimental design comparing standard majority-vote training with four HLV-based training approaches: repeated labelling, soft labelling, minimizing Jensen-Shannon divergence, and maximizing soft micro F1 score. The experiments are conducted on two datasets - SBIC for offensive classification and TAG for legal area classification. For each task, models are trained using both standard cross-entropy loss and the four HLV methods, with evaluations conducted across multiple fairness metrics including demographic parity, equal opportunity, and equalized odds. The study systematically varies weighting schemes, including uniform weighting across groups/classes and balanced weighting, to assess how different configurations impact the trade-off between performance and fairness.

## Key Results
- HLV training methods consistently improve overall performance across all configurations
- Fairness is not harmed in most configurations when using HLV methods
- HLV can improve fairness for a substantial number of configurations, particularly when all groups/classes are weighted equally
- Fairness improvements are linked to minority annotations, as demonstrated by temperature scaling experiments

## Why This Works (Mechanism)
HLV preserves the diversity of human perspectives, including minority viewpoints that might be lost in majority voting. By incorporating this variation into the training process, models gain exposure to a broader range of perspectives, which can help them better handle edge cases and minority groups. The temperature scaling experiments reveal that when minority annotations are weighted more heavily, fairness improvements become more pronounced, suggesting that HLV's strength in capturing diverse perspectives directly contributes to fairer outcomes.

## Foundational Learning
- Human Label Variation (HLV): Understanding how multiple annotators label the same data differently - needed to appreciate why single majority votes may lose valuable information
- Demographic Parity: A fairness metric requiring equal positive prediction rates across groups - needed to evaluate group-level fairness
- Equal Opportunity: Fairness metric requiring equal true positive rates across groups - needed to assess fairness in positive class predictions
- Temperature Scaling: A technique for adjusting model confidence - needed to understand how weighting minority annotations affects fairness
- Cross-Entropy Loss: Standard training objective - needed as baseline for comparison with HLV methods
- Soft Labelling: Using probability distributions instead of hard labels - needed to understand one HLV approach

## Architecture Onboarding

Component Map: Data -> Preprocessing -> Model Training -> Evaluation -> Fairness Metrics Analysis

Critical Path: The experimental pipeline processes raw data through preprocessing, applies one of five training methods (majority vote baseline + four HLV methods), then evaluates both performance and fairness metrics across multiple configurations.

Design Tradeoffs: The study prioritizes methodological rigor by testing multiple HLV approaches and fairness definitions, but this comes at the cost of computational resources. The choice to use established fairness metrics provides comparability but may miss domain-specific fairness concerns.

Failure Signatures: Models trained with HLV might show increased variance in predictions due to the incorporation of conflicting annotations. Performance degradation could occur if the HLV methods amplify noise rather than useful variation.

First Experiments:
1. Train a baseline model using majority-vote labels and establish performance/fairness benchmarks
2. Apply temperature scaling to the baseline model to test the impact of weighting minority annotations
3. Implement one HLV method (e.g., soft labelling) and compare its fairness metrics against the baseline

## Open Questions the Paper Calls Out
None

## Limitations
- Results are based on two specific datasets and may not generalize to other domains
- The analysis focuses on particular fairness metrics that may not capture all aspects of real-world fairness
- Temperature scaling experiments represent a simplified approach to weighting minority annotations

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| HLV improves performance across configurations | High |
| HLV does not harm fairness in most configurations | Medium |
| HLV can improve fairness when groups are equally weighted | Medium |
| Minority annotations drive fairness improvements | Medium |

## Next Checks

1. Test the proposed HLV methods on additional datasets and tasks to assess generalizability
2. Conduct ablation studies to isolate the effects of different HLV components on fairness outcomes
3. Explore alternative weighting schemes and fairness definitions to better understand when HLV improves fairness