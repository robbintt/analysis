---
ver: rpa2
title: 'Knowing What''s Missing: Assessing Information Sufficiency in Question Answering'
arxiv_id: '2512.06476'
source_url: https://arxiv.org/abs/2512.06476
tags:
- information
- context
- sufficiency
- question
- missing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of determining whether a context
  contains sufficient information to answer a question, especially for inferential
  questions that require reasoning beyond direct text extraction. The authors propose
  an "Identify-then-Verify" framework that reformulates sufficiency assessment as
  explicitly identifying what information is missing.
---

# Knowing What's Missing: Assessing Information Sufficiency in Question Answering

## Quick Facts
- arXiv ID: 2512.06476
- Source URL: https://arxiv.org/abs/2512.06476
- Reference count: 7
- Key outcome: "Identify-then-Verify" framework substantially improves sufficiency assessment for inferential questions via self-consistency and verification, with tunable strictness for different answerability criteria.

## Executive Summary
This paper addresses the challenge of determining whether a context contains sufficient information to answer a question, particularly for complex inferential questions requiring reasoning beyond direct text extraction. The authors propose a novel "Identify-then-Verify" framework that reformulates sufficiency assessment as explicitly identifying missing information. By generating multiple hypotheses about what's missing via self-consistency, establishing semantic consensus, and performing a final verification step, the method achieves substantial improvements over strong baselines across diverse multi-hop and answerability benchmarks.

## Method Summary
The framework operates in two stages: Identify and Verify. First, it generates N=5 hypotheses about missing information using a smaller LLM (Llama-3.1-8B-Instruct) with non-zero temperature sampling to encourage diverse outputs. These hypotheses are then encoded using a sentence transformer (all-MiniLM-L6-v2) and a semantic consensus is established by selecting the hypothesis with highest average cosine similarity above a threshold (0.3). Finally, a verification step using a larger model (GPT-4o) checks whether the consensus gap claim is actually absent from the context, providing a guard against hallucination. The framework adapts to different sufficiency criteria through configurable verification strictness.

## Key Results
- Substantial accuracy improvements over strong baselines across multi-hop benchmarks (MuSiQue, HotpotQA, 2WikiMultiHopQA)
- Outperforms specialized missing information detection methods on answerability benchmarks (SQuAD v2, FaithEval)
- Verification step reduces hallucination-induced errors by up to 37% on multi-hop datasets
- Framework adapts to different sufficiency definitions (pragmatic vs. strict) through tunable verification strictness

## Why This Works (Mechanism)

### Mechanism 1: Self-Consistency Reduces Single-Pass Instability
Multiple sampled hypotheses about missing information yield more reliable sufficiency signals than single-pass binary classification. Non-zero temperature sampling explores diverse token sequences, producing a distribution of gap hypotheses rather than one deterministic output. Semantic consensus extracts the stable "center of gravity" from this distribution. Disagreement across runs reflects epistemic uncertainty about the question-context pair. Evidence shows hypothesis disagreement rate spikes from 1→2 runs and stabilizes after ~4 runs, motivating N=5. Break condition: If disagreement remains high even after N=5 (no clear consensus above similarity threshold), the framework conservatively defaults to "insufficient"—this may over-reject genuinely sufficient contexts.

### Mechanism 2: Generative Reformulation Creates Falsifiable Claims
Asking "what is missing?" rather than "is it sufficient?" produces explicit, verifiable claims that improve judgment accuracy. Binary classification elicits shallow pattern-matching. Generative prompting forces the model to articulate a concrete hypothesis about the gap, which can then be tested against the source text. Models are better at verifying a specific claim than at global sufficiency assessment. Break condition: If the model hallucinates plausible-sounding gaps that pass verification superficially, falsifiability breaks down. The verification step is the guard but is itself prompt-dependent.

### Mechanism 3: Verification as Anti-Hallucination Guard
A separate verification step that checks whether the consensus gap claim is truly absent reduces false insufficiency predictions. Decoupling identification from verification prevents the model from committing to a hallucinated gap. If the verifier finds the claimed missing information in the context, the original insufficiency judgment is overturned. Verification is an easier task than identification and can be performed reliably by smaller or different models. Break condition: If verification prompt is too lenient (accepts loose inferences) or too strict (requires literal matches), accuracy degrades on benchmarks with mismatched sufficiency definitions.

## Foundational Learning

- **Self-Consistency in LLMs**: Why needed here: The framework relies on aggregating multiple stochastic outputs to reduce variance. Understanding why sampling with temperature > 0 produces meaningful diversity is essential. Quick check question: Why would greedy decoding (temperature=0) fail to capture model uncertainty in this task?

- **Semantic Similarity via Embeddings**: Why needed here: Consensus is established by computing cosine similarity between hypothesis embeddings, not string matching. Quick check question: Given hypotheses "the date isn't mentioned" and "it lacks a specific year," would naive majority voting work? Why not?

- **Sufficiency as a Spectrum (Inference vs. Literal Matching)**: Why needed here: The paper shows SQuAD v2 and FaithEval penalize pragmatic inference, while CouldAsk rewards it. Verification strictness must be tuned per use case. Quick check question: A context states "The CEO announced he would step down next year." Is this sufficient to answer "When did the CEO resign?" under strict vs. pragmatic criteria?

## Architecture Onboarding

- **Component map**: Question + context → 5 parallel "what's missing?" prompts → all-MiniLM-L6-v2 embeddings → cosine similarity matrix → consensus claim → GPT-4o verification → final sufficiency label

- **Critical path**: 1. Question + context → 5 parallel "what's missing?" prompts 2. Encode each hypothesis → similarity matrix → consensus claim 3. Consensus claim + original context → verification prompt 4. Verification result → final sufficiency label

- **Design tradeoffs**: Latency vs. accuracy: N=5 parallel calls add inference time but reduce error; consider N=3 for latency-sensitive deployments. Model size split: Smaller model for identification (benefits from diversity), larger/smaller for verification (task is simpler). Strictness tuning: One prompt change adapts from pragmatic QA to strict fact-checking; requires domain-specific calibration.

- **Failure signatures**: High disagreement with no consensus (low max similarity) → conservative "insufficient" may reject valid contexts. Verification too lenient on inferential benchmarks → over-predicts "sufficient" (observed on FaithEval default). Verification too strict on pragmatic benchmarks → over-predicts "insufficient" (observed on CouldAsk with strict config).

- **First 3 experiments**: 1. Reproduce the disagreement curve: Run the Identify step with N=1,2,3,4,5 on a held-out subset of MuSiQue. Confirm disagreement stabilizes around N=4–5 before full pipeline deployment. 2. Ablate verification strictness: Test the same pipeline on SQuAD v2 with both "pragmatic inference allowed" and "strict literal match" verification prompts. Reproduce the 15%+ accuracy swing shown in Table 6. 3. Model swap stress test: Replace Llama-8B in Identify with a different open model (e.g., Mistral-7B) while keeping GPT-4o for Verify. Measure accuracy delta to confirm the identification stage is model-agnostic.

## Open Questions the Paper Calls Out

- Can methods be developed to automatically determine the optimal verification strictness setting for a given task or domain? Currently, strictness is manually configured; the framework requires human judgment to decide between pragmatic inference and strict literal matching modes.

- What more efficient sampling strategies could reduce the computational cost of the self-consistency step while maintaining accuracy? The framework requires N=5 LLM calls per input; this multiplies inference cost and latency compared to single-pass approaches.

- How robust is the semantic consensus threshold (0.3) across different question types, domains, and embedding models? The paper fixes the minimum similarity threshold at 0.3 without ablation or justification; sensitivity to this hyperparameter is not analyzed.

- What benchmarks better capture the complexity of real-world open-ended, inferential questions with multiple potential information gaps? Existing benchmarks often test either extractive multi-hop reasoning or strict literal faithfulness, not open-ended inferential sufficiency with multiple plausible gaps.

## Limitations

- Prompt sensitivity is the dominant uncertainty: The entire framework's accuracy hinges on the identification prompt wording, verification prompt strictness, and LLM-judge filtering criteria. Exact prompt strings are not disclosed, making faithful reproduction challenging.

- Consensus threshold tuning lacks principled guidance: The 0.3 similarity threshold for consensus selection is empirical but not justified. In cases of genuinely ambiguous contexts or complex inferential gaps, similarity distributions may be flatter, causing premature "insufficient" judgments.

- Verification strictness is dataset-dependent and poorly characterized: The framework achieves high accuracy on SQuAD v2 and FaithEval by tuning verification to be strict (rejecting inferential answers), but this same strictness causes failures on CouldAsk. The paper demonstrates the problem but doesn't provide a principled method for calibrating strictness per dataset or use case beyond manual prompt adjustment.

## Confidence

- High confidence: The core mechanism (Identify-then-Verify reformulation) is novel and the experimental results show consistent accuracy gains over baselines across multiple benchmarks.
- Medium confidence: While the framework works on curated benchmarks, real-world deployment requires careful prompt engineering and dataset-specific tuning. The paper doesn't address scaling to noisy, open-domain contexts.
- Low confidence: The framework was tested primarily with Llama-3.1-8B for identification and GPT-4o for verification. Performance with other model combinations, especially fully open-source stacks, is not characterized.

## Next Checks

1. **Prompt ablation study**: Systematically vary identification and verification prompt wording on a held-out MuSiQue subset. Measure how sufficiency accuracy changes with phrasing to establish sensitivity bounds and identify prompt templates that are robust across different question types.

2. **Consensus threshold sensitivity**: For each benchmark, plot sufficiency accuracy against varying similarity thresholds (0.2 to 0.5). Identify the optimal threshold per dataset and quantify how often no hypothesis meets threshold. This would reveal whether the 0.3 default is universally appropriate or needs dataset-specific tuning.

3. **Cross-model robustness test**: Replace Llama-3.1-8B with three different open models (e.g., Mistral-7B, Qwen-7B, DeepSeek-Coder-6.7B) while keeping GPT-4o verification. Measure accuracy degradation to determine if the identification stage truly benefits from model diversity or is constrained by model capability.