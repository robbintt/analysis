---
ver: rpa2
title: 'Safety Game: Balancing Safe and Informative Conversations with Blackbox Agentic
  AI using LP Solvers'
arxiv_id: '2510.09330'
source_url: https://arxiv.org/abs/2510.09330
tags:
- safety
- safe
- arxiv
- game
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Safety Game, a model-independent framework
  for balancing safety and helpfulness in LLM responses using game-theoretic principles
  and linear programming solvers. The core idea is to frame the safety-helpfulness
  trade-off as a two-player zero-sum game, where the equilibrium strategy maximizes
  helpfulness subject to a safety risk cap relative to a designated safe fallback.
---

# Safety Game: Balancing Safe and Informative Conversations with Blackbox Agentic AI using LP Solvers

## Quick Facts
- arXiv ID: 2510.09330
- Source URL: https://arxiv.org/abs/2510.09330
- Reference count: 15
- Safety Game achieves up to 2× improvement in accuracy over state-of-the-art ranking baselines while balancing safety and helpfulness in LLM responses.

## Executive Summary
This paper presents Safety Game, a model-independent framework for balancing safety and helpfulness in LLM responses using game-theoretic principles and linear programming solvers. The core idea is to frame the safety-helpfulness trade-off as a two-player zero-sum game, where the equilibrium strategy maximizes helpfulness subject to a safety risk cap relative to a designated safe fallback. LLM agents compute this strategy at inference time using a linear program built from per-response estimates of helpfulness and risk. Evaluated on three major benchmarks (HHH, TruthfulQA, and SafetyBench), Safety Game outperforms state-of-the-art ranking baselines in 11 of 15 test cases, achieving up to 2× improvement in accuracy. On the largest benchmark, SafetyBench, it delivers the best performance in 4 out of 5 test cases, and consistently excels with more advanced models like GPT-OSS-20B. The method requires no retraining or access to model internals, making it a scalable, black-box solution for safe LLM deployment.

## Method Summary
Safety Game formulates the safety-helpfulness trade-off as a two-player zero-sum game where the LLM commits to a mixed strategy over candidate responses before observing user intent. The minimax equilibrium maximizes expected helpfulness while ensuring expected extra risk never exceeds the safe fallback baseline, enforced through a Lagrangian penalty with bounded multiplier μ ∈ [0, β]. At inference time, the framework queries a frozen LLM for log-likelihoods of YES/NO completions to helpfulness and safety probes, normalizes these via log-sum-exp, computes margins relative to the safe fallback, and solves a linear program to find the optimal probability distribution over candidates. If the LP is infeasible, the safe fallback response is returned; otherwise, the candidate with highest probability in the equilibrium distribution is selected.

## Key Results
- Outperforms state-of-the-art ranking baselines in 11 of 15 test cases across three benchmarks
- Achieves up to 2× improvement in accuracy compared to baselines
- On SafetyBench, delivers best performance in 4 out of 5 test cases
- Linear penalty achieves higher BLEU-Acc on TruthfulQA (+3.66 @1B, +2.70 @8B) but triggers more safety fallbacks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Framing safety-helpfulness trade-offs as a two-player zero-sum game yields a principled equilibrium strategy that can be computed via linear programming.
- Mechanism: The LLM (Player 1) commits to a mixed strategy π over candidate responses before observing user intent (Player 2). The minimax equilibrium maximizes expected helpfulness while ensuring expected extra risk never exceeds the safe fallback baseline, enforced through a Lagrangian penalty with bounded multiplier μ ∈ [0, β].
- Core assumption: User intent can be modeled as a chance event switching between helpfulness-only evaluation (S1) and safety-enforcement evaluation (S2) with fixed probability weights.
- Evidence anchors:
  - [abstract] "We formulate this dilemma as a two-player zero-sum game whose minimax equilibrium captures the optimal balance between safety and helpfulness."
  - [Section 3.3-3.4] Derives the bounded-multiplier reformulation (Eq. 3.2) and branch decomposition showing how λ = μ/β acts as Player 1's safety control.
  - [corpus] Related work "Equilibrate RLHF" similarly frames help-safety tradeoffs game-theoretically but operates at training time; no direct corpus validation of inference-time LP approach.
- Break condition: If candidate risks are uniformly high or helpfulness scores are unreliable, the equilibrium collapses to the safe fallback rs; LP feasibility determines this.

### Mechanism 2
- Claim: Probe-based log-likelihood scoring provides model-agnostic estimates of helpfulness and risk without requiring access to internal weights.
- Mechanism: Two binary probes (ϕH, ϕS) query a frozen LLM for log-likelihoods of YES/NO completions to questions like "Is this answer helpful?" and "Is this answer risky?". Log-sum-exp normalization converts these to scores hi, si ∈ (-∞, 0]. Margins Mi = hi - hs and Δi = si - ss measure improvement over the safe fallback.
- Core assumption: The frozen LLM's log-likelihoods reliably discriminate helpful vs. unhelpful and safe vs. risky responses.
- Evidence anchors:
  - [Section 3.1] Defines probe scoring: "hi := logP(yH_i, nH_i), si := logP(yS_i, nS_i)" with LSE normalization.
  - [Section 5] Notes limitation: "relax the assumption on discrete and known action space" for open-ended questions remains future work.
  - [corpus] No corpus validation found for probe-based safety scoring; neighboring papers assume access to model internals or fine-tuned classifiers.
- Break condition: If probes systematically misrank candidates (e.g., assign high helpfulness to unsafe answers), the LP will optimize toward incorrect equilibria.

### Mechanism 3
- Claim: A sigmoid penalty function softens the hard safety cap while discouraging "cap-hugging" behavior near the risk threshold.
- Mechanism: The linear penalty μ(ΣπiΔi - T) jumps discontinuously at the cap, causing boundary sensitivity. The sigmoid Psigmoid(R) = μ/(1 + exp[-κ(R-T)]) applies gradual pressure near T, with positive boundary slope κβ/4 nudging mixtures slightly inward when helpfulness allows.
- Core assumption: Small controlled violations of the risk cap are acceptable when they yield substantial helpfulness gains.
- Evidence anchors:
  - [Section 3.3] "The sigmoid introduces a small, nonzero cost in a narrow band around T... nudges solutions to remain slightly below the threshold when possible."
  - [Section 4.5 Ablation] Linear penalty achieves higher BLEU-Acc on TruthfulQA (+3.66 @1B, +2.70 @8B) but triggers more safety fallbacks (38.1% @1B vs 16.8% @8B); sigmoid degrades at 1B.
  - [corpus] No corpus comparison; this penalty design appears novel to this work.
- Break condition: When κ is poorly tuned, sigmoid may allow excessive risk violations or be too permissive; linear penalty provides stricter enforcement.

## Foundational Learning

- **Minimax equilibrium in zero-sum games**
  - Why needed here: The core formulation requires understanding how equilibrium strategies protect against worst-case opponents (adversarial users).
  - Quick check question: If Player 2 can choose between two evaluation modes (helpfulness-only vs. safety-enforcement), what mixed strategy by Player 1 guarantees at least the fallback payoff?

- **Linear programming with inequality constraints**
  - Why needed here: The inference-time solver must efficiently handle a constrained optimization problem; understanding dual variables (μ, λ) clarifies how the risk cap is enforced.
  - Quick check question: In max ΣπiMi subject to ΣπiΔi ≤ T, what does an optimal dual variable μ* > 0 indicate about the constraint?

- **Log-sum-exp normalization for binary probes**
  - Why needed here: Raw log-likelihoods are not directly comparable; LSE normalization ensures scores are well-calibrated probabilities over {YES, NO}.
  - Quick check question: Given log-likelihoods y = log p(YES) and n = log p(NO), compute the normalized log-probability of YES.

## Architecture Onboarding

- **Component map**:
  Prompt x → Candidate Generation (k options) → Probe Scoring (ϕH, ϕS for each) → Margin Computation (Mi, Δi vs. fallback rs) → LP Solver (Eq. 3.3) → Feasibility Check → If infeasible: return rs; else: return argmax_i πi

- **Critical path**: The LP solve (Eq. 3.3) is the gating operation; feasibility determines whether any response beyond rs is permissible. The probe scoring phase scales O(k) in LLM calls; LP solve is O(k) variables with simple constraints.

- **Design tradeoffs**:
  - Linear vs. sigmoid penalty: Linear is stricter (harder cap enforcement) but shows better accuracy in ablations; sigmoid is smoother but underperforms on smaller models.
  - Risk cap T: Lower T is more conservative (more fallbacks); T = 1.0 (100) performed best in ablations. Tune on dev set once.
  - Dual bound β: Largely inert for accuracy in experiments; controls probability weight of safety-enforcement branch (β/(β+1)).
  - Include explicit safe fallback candidate: Ablation shows dual activity (μ/β) collapses to 0 without it, even if fallback rate stays 0%.

- **Failure signatures**:
  - LP always infeasible: All candidates flagged as high-risk; check probe calibration or relax T.
  - Always selecting fallback: Candidates may have Δi >> T or Mi ≤ 0; inspect score distributions.
  - Sudden accuracy drop: Probe model changed or prompt format drifted; revalidate probe outputs.
  - Excessive cap violations (sigmoid): κ too low; increase steepness or switch to linear.

- **First 3 experiments**:
  1. **Probe validation**: On a held-out set of 50-100 prompts with human-annotated helpfulness/risk labels, measure rank correlation between probe scores and human judgments. Target: Spearman ρ > 0.6.
  2. **Ablation on T and penalty type**: Sweep T ∈ {0.1, 1.0, 10, 100} with both linear and sigmoid penalties on TruthfulQA. Replicate paper's finding that linear + T=1.0 is best for smaller models.
  3. **End-to-end comparison on SafetyBench**: Implement the full pipeline and compare against baseline G (generative ranking). Run McNemar's test to confirm n10 >> n01 as reported in Appendix C.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Safety Game framework be extended to handle sequential dependencies in multi-turn dialogues or debates?
- Basis in paper: [explicit] The conclusion states, "A potential future work is to further extend our approach to other types of safety alignment settings, for example, sequential dialogues/debates."
- Why unresolved: The authors note that sequential dependencies introduce significant complexity due to the combinatorial nature of the setting, which the current single-shot LP formulation does not address.
- What evidence would resolve it: A modified game formulation that incorporates conversation history and demonstrates maintained safety guarantees across multi-turn interactions.

### Open Question 2
- Question: How can the reliance on a discrete, finite set of candidate responses be relaxed to accommodate open-ended questions?
- Basis in paper: [explicit] The conclusion identifies the need to "relax the assumption on discrete and known action space (i.e., the set of possible answers) and allow the agent to engage with more generic QA settings (e.g., open-ended questions)."
- Why unresolved: The current Linear Programming solver requires a finite candidate set $R$ to function; infinite or continuous action spaces require fundamentally different optimization machinery.
- What evidence would resolve it: A method for applying the safety cap constraint to continuous token generation or infinite candidate pools without tractability issues.

### Open Question 3
- Question: How can the framework be generalized to multi-player settings involving distinct agents with competing utility components?
- Basis in paper: [explicit] The conclusion suggests, "a natural extension is a multi-player Safety Game where distinct agents (e.g., user, developer, regulator) optimize different utility components."
- Why unresolved: The current model is a two-player zero-sum game; adding players like regulators with distinct utility functions complicates the equilibrium calculation and the definition of the safety cap.
- What evidence would resolve it: A theoretical expansion of the game-theoretic model and a corresponding solver that balances the conflicting safety and helpfulness definitions of multiple stakeholders.

## Limitations
- Probe-based safety scoring lacks validation for open-ended responses and non-MCQ domains
- Sigmoid penalty parameterization (α vs κ) is unclear, affecting reproducibility
- LP solver's numerical tolerance for feasibility is unspecified, impacting fallback rates

## Confidence

- **High confidence**: The game-theoretic framing as a two-player zero-sum game and the minimax equilibrium derivation are mathematically sound and well-established. The core LP formulation (Eq. 3.3) is clearly specified.
- **Medium confidence**: The probe-based scoring mechanism is reasonable and follows established practices, but lacks independent validation for safety assessment. The performance improvements (up to 2× accuracy) are reported but depend heavily on probe reliability.
- **Low confidence**: The generalization to open-ended questions and non-MCQ tasks remains theoretical ("future work"). The claim that no model access is required may not hold if probe calibration requires extensive domain-specific tuning.

## Next Checks

1. **Probe validation study**: Implement the binary probe scoring mechanism and validate it against human-annotated safety and helpfulness labels on a held-out set of 50-100 prompts. Measure rank correlation (Spearman ρ) between probe scores and human judgments, targeting ρ > 0.6 for both safety and helpfulness probes.

2. **T and penalty type ablation**: Systematically sweep the risk cap T ∈ {0.1, 1.0, 10, 100} with both linear and sigmoid penalties on the TruthfulQA benchmark. Verify the paper's finding that linear penalty with T=1.0 outperforms other configurations, particularly for smaller models (1B, 8B).

3. **SafetyBench end-to-end replication**: Implement the complete pipeline and run comprehensive evaluation on SafetyBench English test. Use McNemar's test to confirm statistically significant improvements over baseline methods, specifically checking that n10 >> n01 in pairwise comparisons as reported in Appendix C.