---
ver: rpa2
title: A Two-Stage GPU Kernel Tuner Combining Semantic Refactoring and Search-Based
  Optimization
arxiv_id: '2601.12698'
source_url: https://arxiv.org/abs/2601.12698
tags:
- performance
- optimization
- shapes
- code
- kernel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of achieving near-expert performance
  in GPU kernel optimization, a critical bottleneck for high-performance computing
  and large-model training. It introduces a two-stage GPU kernel tuner that combines
  semantic refactoring with search-based optimization, moving beyond purely generative
  rewriting approaches.
---

# A Two-Stage GPU Kernel Tuner Combining Semantic Refactoring and Search-Based Optimization

## Quick Facts
- arXiv ID: 2601.12698
- Source URL: https://arxiv.org/abs/2601.12698
- Reference count: 39
- Primary result: Speedups exceeding 3x on SGLang kernels via two-stage semantic refactoring plus search-based autotuning

## Executive Summary
This paper addresses the challenge of achieving near-expert performance in GPU kernel optimization through a novel two-stage approach. The method combines semantic refactoring with search-based optimization, moving beyond purely generative rewriting approaches. By parameterizing kernels into explicitly tunable templates and then searching within hardware-constrained spaces, the system achieves stable performance improvements with better interpretability. Experiments on real-world kernels from SGLang demonstrate significant speedups while maintaining correctness.

## Method Summary
The method employs a four-agent system using DeepSeek-V3.2: Planning, Generation, Tuning, and Testing agents. It operates in two levels: Level 1 performs semantics-preserving semantic refactoring to create parameterizable kernel templates; Level 2 searches template parameters within hardware resource constraints. The Generation Agent parameterizes execution strategies including thread granularity, loop scheduling, vectorization, and shared memory organization. The Tuning Agent derives feasible parameter spaces from hardware limits and performs search-based optimization. The process iterates with feedback from measurement signals guiding subsequent refinements, ultimately producing both general configurations for average performance and specialized configurations for specific input shapes.

## Key Results
- Speedups exceeding 3x compared to native SGLang implementations
- Template-plus-search design reduces iteration randomness and improves interpretability
- Method successfully extended to other backends like OpenCL and HIP
- Outperforms agent-only direct rewriting approaches in stability and systematic configuration discovery

## Why This Works (Mechanism)

### Mechanism 1: Template-Based Parameterization Converts Implicit Optimization into Explicit Search Space
Refactoring kernels into explicitly parameterizable templates reduces LLM-driven optimization randomness by exposing tunable dimensions as discrete, searchable parameters rather than implicit code choices. The Generation Agent transforms kernels into templates with explicit parameters (thread granularity, loop unrolling, vectorization, shared memory organization), creating a bounded discrete search space that can be systematically explored.

### Mechanism 2: Two-Level Hierarchical Optimization Separates Structural and Parametric Search
Decoupling semantic-level refactoring (structure) from parameter-level tuning (configuration) enables more stable convergence and higher performance ceilings. Level 1 performs semantics-preserving rewriting to improve structural fitness; Level 2 searches parameter configurations within hardware constraints. Feedback from Level 2 measurements guides Level 1 rewriting decisions in subsequent iterations.

### Mechanism 3: Hardware-Constrained Feasibility Pruning Prevents Invalid Configurations
Deriving parameter ranges from hardware resource limits (registers, shared memory, thread bounds) before search eliminates compile failures and invalid launches, improving search efficiency. The Tuning Agent computes feasible parameter spaces by constraining configurations to satisfy hardware resource upper-bound vectors, pruning obviously infeasible or inefficient combinations before empirical evaluation.

## Foundational Learning

- **GPU execution model (thread blocks, warps, occupancy, memory hierarchy)**: Understanding how thread granularity, shared memory usage, and register pressure interact with hardware is essential to interpret tuning results. *Quick check: Given a kernel with 256 threads per block and 48KB shared memory limit, what happens if you double the per-thread register usage?*

- **Template metaprogramming and compile-time parameters**: The method exposes optimization dimensions as template parameters; understanding how CUDA templates work is prerequisite to reading generated code. *Quick check: How does a `constexpr` or preprocessor-based tile size differ from a runtime variable in terms of compiler optimization?*

- **Autotuning search strategies (grid search, Bayesian optimization, evolutionary search)**: The Tuning Agent performs search-based optimization; knowing tradeoffs between exploration and exploitation helps diagnose slow or suboptimal convergence. *Quick check: When would random sampling outperform grid search in a high-dimensional discrete parameter space?*

## Architecture Onboarding

- **Component map**: Planning Agent -> Generation Agent -> Tuning Agent -> Testing Agent -> (feedback to Planning Agent)
- **Critical path**: Baseline kernel → TestingAgent.GenerateTests → GenerationAgent.Templatize → TuningAgent.DeriveFeasibleSpace → TuningAgent.Search → TestingAgent.EvaluatePerformance → PlanningAgent.Score → (iterate or terminate)
- **Design tradeoffs**: General vs. specialized configurations (general maximizes average performance, specialized is shape-specific); search budget vs. performance ceiling (more parameters increase exponential search space); agent model choice (same base model reduces confounds but may limit specialist capabilities)
- **Failure signatures**: All configurations fail correctness gating (semantic reference corrupted); performance plateau after early iterations (parameter space exhausted or structural bottleneck dominates); high variance across runs (profiling noise or non-deterministic behavior); empty feasible space (over-pruning from resource constraints)
- **First 3 experiments**: 1) Reproduce speedup claim on one SGLang kernel with same shapes and data types; 2) Ablate template layer by running multi-agent loop without explicit parameterization and compare performance stability; 3) Cross-shape analysis comparing general configuration against per-shape specialized configurations

## Open Questions the Paper Calls Out

- **Cross-backend generalization**: Can the two-stage tuner maintain performance advantages when applied to non-CUDA backends like OpenCL and HIP? The paper states it "can be further extended" but provides no evidence beyond CUDA.

- **LLM dependency**: How sensitive is semantic refactoring quality to the choice of underlying Large Language Model? The setup uses DeepSeek-V3.2 to reduce confounds, leaving the dependency on model capability unexplored.

- **Computational overhead**: What is the computational overhead of the search-based autotuning stage relative to inference speedup gained? The method prioritizes stable gains but doesn't quantify the wall-clock time required for the tuning process itself.

## Limitations

- **Hardware constraint modeling**: Exact derivation formula for resource bounds is unspecified, potentially affecting search efficiency and validity of performance claims
- **Generalization boundaries**: Performance demonstrated only on three SGLang kernels, representing a narrow slice of GPU programming patterns
- **Search strategy details**: Specific search algorithm (grid, random, Bayesian) is not specified, impacting reproducibility and scalability claims

## Confidence

- **High Confidence**: Core architectural insight of decoupling semantic refactoring from parameter search provides systematic and interpretable approach
- **Medium Confidence**: 3× speedup claim based on clear experimental setup but lacking specification for search strategy and hardware constraint derivation
- **Low Confidence**: Cross-domain generalization claims lack supporting evidence beyond CUDA/SGLang domain

## Next Checks

1. **Cross-Shape Performance Gap Analysis**: Compare general configuration against per-shape specialized configurations across all test shapes to quantify performance penalty from shape-insensitive optimization.

2. **Ablation of Template Layer**: Run multi-agent loop without explicit template parameterization and compare final performance, iteration variance, and convergence stability.

3. **Hardware Constraint Sensitivity Test**: Systematically relax or tighten resource constraint bounds by 10-20% and observe changes in search space size, compilation success rate, and final performance.