---
ver: rpa2
title: Comparing BFGS and OGR for Second-Order Optimization
arxiv_id: '2512.06969'
source_url: https://arxiv.org/abs/2512.06969
tags:
- bfgs
- line
- hessian
- search
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper compares the classical BFGS quasi-Newton method with\
  \ a novel Online Gradient Regression (OGR) approach for second-order optimization\
  \ in high-dimensional problems like neural network training. BFGS maintains a positive-definite\
  \ Hessian approximation via the Sherman-Morrison update but is limited in non-convex\
  \ settings and requires O(D\xB2) memory."
---

# Comparing BFGS and OGR for Second-Order Optimization

## Quick Facts
- arXiv ID: 2512.06969
- Source URL: https://arxiv.org/abs/2512.06969
- Reference count: 15
- One-line primary result: OGR outperforms BFGS on standard test functions, especially in non-convex landscapes, while maintaining near-first-order computational cost

## Executive Summary
This paper compares the classical BFGS quasi-Newton method with a novel Online Gradient Regression (OGR) approach for second-order optimization in high-dimensional problems like neural network training. BFGS maintains a positive-definite Hessian approximation via the Sherman-Morrison update but is limited in non-convex settings and requires O(D²) memory. OGR instead estimates curvature online by performing exponential moving average regression of gradients against positions, allowing it to capture both positive and negative eigenvalues without explicit Hessian computation or inversion. Across standard test functions, OGR consistently outperformed BFGS in both final objective value and convergence speed, particularly in non-convex landscapes. For example, on the Rosenbrock function, OGR with fixed step size outperformed its line-searched version, suggesting line search can be overly conservative in narrow valleys.

## Method Summary
OGR estimates local curvature by treating the gradient-position relationship as a linear regression problem, maintaining exponentially weighted statistics of (θ_t, g_t) pairs and solving a weighted least-squares problem to estimate Hessian H and extremum location p simultaneously. The method uses eigendecomposition with eigenvalue clipping for numerical stability and can capture negative curvature through sign-based step direction adjustment. BFGS maintains positive-definite Hessian approximation via Sherman-Morrison updates and secant condition. Both methods were tested with and without backtracking line search on 9 standard test functions using 200 random starting points per function.

## Key Results
- OGR consistently outperformed BFGS on all test functions in both final objective value and convergence speed
- On Rosenbrock function, OGR with fixed step size (α=0.5) outperformed line-searched version, suggesting line search can be overly conservative in narrow valleys
- OGR showed particular advantage in non-convex landscapes (Rastrigin, Ackley, Schwefel) where it can estimate negative curvature and navigate saddle points
- OGR makes more effective use of curvature information at near-first-order computational cost

## Why This Works (Mechanism)

### Mechanism 1: Online Gradient-Position Regression for Curvature Estimation
OGR estimates local curvature by treating the gradient-position relationship as a linear regression problem, avoiding explicit Hessian construction. The method models the local quadratic approximation g(θ) ≈ H(θ - p), where H is the Hessian and p is the estimated extremum/saddle location. It maintains exponentially weighted statistics of (θ_t, g_t) pairs and solves a weighted least-squares problem to estimate H and p simultaneously. The update rule θ_{t+1} = θ_t - αH^{-1}g uses eigendecomposition with eigenvalue clipping for numerical stability.

### Mechanism 2: Symmetrization via Eigendecomposition of Covariance Structure
The direct regression formula produces an asymmetric matrix; symmetrization improves numerical properties while preserving curvature information. The symmetrized estimator decomposes the position covariance θθ^T = ODO^T and solves for H in a transformed coordinate system where the symmetry constraint becomes tractable. This leverages the eigendecomposition structure for efficient computation.

### Mechanism 3: Signed Step Direction for Saddle-Aware Navigation
Unlike BFGS which enforces positive definiteness, OGR can estimate negative curvature and adjust step direction accordingly. The 1D variant uses θ_{t+1} = θ_t + α·sign(λ̂)[p̂ - θ_t], where the sign factor inverts direction for negative curvature (concave regions). In the multivariate case, eigenvalue clipping by absolute value preserves negative curvature information while preventing division by near-zero values.

## Foundational Learning

- **Quasi-Newton Methods and Secant Condition**
  - Why needed: BFGS is the baseline comparator; understanding how it approximates curvature through successive gradient differences clarifies why OGR's direct regression approach differs fundamentally
  - Quick check: Given two successive gradient observations g_k and g_{k+1} at positions x_k and x_{k+1}, what constraint does the secant condition impose on the Hessian approximation?

- **Weighted Least Squares with Exponential Decay**
  - Why needed: OGR's core operation is solving a WLS problem where weights decay exponentially; this balances stale vs. recent information and determines effective sample size
  - Quick check: If β = 0.2 as used in the paper, approximately how many recent iterations contribute meaningfully to the weighted statistics?

- **Eigendecomposition for Matrix Inversion Stability**
  - Why needed: OGR computes H^{-1} via eigendecomposition with eigenvalue clipping; understanding eigenvalue sign and magnitude effects on inversion stability is essential for debugging
  - Quick check: If H has eigenvalues {-100, 0.001, 50} and you clip by absolute value to minimum ε = 10^{-6}, what are the post-clipping eigenvalues before inversion?

## Architecture Onboarding

- **Component map:**
  [Gradient Stream] → [EMA Statistics Update] → [Covariance Computation] → [Symmetrized H Estimator] → [Eigendecomposition + Clip] → [Step Direction: H^{-1}g] → [Step Size Control: α, τ]

- **Critical path:** The eigendecomposition of the position covariance (for symmetrization) and the Hessian estimate (for inversion) represents the computational bottleneck. Both are O(d³) operations where d is problem dimension—acceptable for the paper's test functions but potentially prohibitive for high-dimensional neural networks without subspace restriction.

- **Design tradeoffs:**
  1. EMA decay β: Low values (0.2 in paper) adapt quickly but provide noisier estimates; high values smooth noise but lag behind curvature changes
  2. Full multivariate vs. subspace restriction: Paper uses full Hessian estimation for test functions; subspace approach described but not benchmarked
  3. Line search vs. fixed step: Rosenbrock experiments show line search can be counterproductive in narrow valleys

- **Failure signatures:**
  1. Exploding step sizes: ||∆θ|| >> typical gradient norms due to near-zero eigenvalues in H after clipping
  2. Stagnation in flat regions: Loss plateaus despite non-zero gradients when eigenvalue clipping washes out curvature information
  3. Covariance singularity: Numerical errors in matrix inversion when insufficient parameter movement makes covariance ill-conditioned

- **First 3 experiments:**
  1. Reproduce Rosenbrock comparison (2D, no line search): Verify trajectory plots show faster valley traversal
  2. Ablate EMA decay β: Test β ∈ {0.1, 0.2, 0.5, 0.9} on Rastrigin and Sphere to measure optimal ranges
  3. Scale test on subspace variant: Implement multidimensional OGR with d=10 subspace dimensions on 100D quadratic to validate curvature estimation advantage

## Open Questions the Paper Calls Out

- Does OGR maintain its convergence advantages over BFGS when training large-scale neural networks with stochastic gradients?
- Why does fixed-step OGR outperform line-searched OGR on the Rosenbrock function, and does this pattern generalize to other narrow-valley landscapes?
- Can automated or adaptive schedules for the EMA decay parameter β and subspace dimension d eliminate the manual hyperparameter sensitivity of OGR?
- Does combining OGR with momentum or Adam-style adaptive learning rates yield cumulative benefits, or do their curvature estimates conflict?

## Limitations

- Lack of empirical validation on high-dimensional problems where full Hessian estimation becomes computationally prohibitive
- Hyperparameter sensitivity to EMA decay β not thoroughly explored - β=0.2 was chosen without systematic ablation
- No comparison to modern second-order methods like K-FAC or natural gradient approaches that approximate curvature in neural network-friendly ways

## Confidence

- **High Confidence:** The core mechanism of online gradient-position regression for curvature estimation is well-specified and mathematically sound
- **Medium Confidence:** The claim that OGR can estimate negative curvature and navigate non-convex landscapes is supported by test function results but requires further validation on problems where saddle points are known to be problematic
- **Low Confidence:** The scalability claims for high-dimensional problems are currently theoretical without experiments on subspace-restricted OGR

## Next Checks

1. **Subspace OGR Scalability Test:** Implement the multidimensional OGR from Section II.E with d=10 subspace dimensions on a 100D quadratic with known Hessian to validate whether subspace restriction preserves curvature estimation advantage while reducing computational cost.

2. **Neural Network Validation:** Apply OGR (using the subspace variant) to train a small but non-convex neural network (e.g., a 2-layer MLP on MNIST) from multiple random initializations to compare convergence speed and final accuracy against Adam and L-BFGS baselines.

3. **Hyperparameter Sensitivity Analysis:** Systematically vary the EMA decay parameter β ∈ {0.1, 0.2, 0.5, 0.9} on a representative set of test functions (Sphere for convex, Rastrigin for non-convex, Rosenbrock for ill-conditioned) to determine optimal β ranges for different landscape characteristics.