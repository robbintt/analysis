---
ver: rpa2
title: 'Soft Filtering: Guiding Zero-shot Composed Image Retrieval with Prescriptive
  and Proscriptive Constraints'
arxiv_id: '2512.20781'
source_url: https://arxiv.org/abs/2512.20781
tags:
- soft
- cirevl
- searle
- image
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Soft Filtering (SoFT) is a training-free re-ranking module for
  zero-shot composed image retrieval (ZS-CIR) that addresses the limitations of single
  fused queries by explicitly modeling both prescriptive (must-have) and proscriptive
  (must-avoid) user intent. It uses multimodal LLMs to extract dual textual constraints
  from reference-modification pairs and reweights candidate similarity scores without
  modifying the base retrieval model.
---

# Soft Filtering: Guiding Zero-shot Composed Image Retrieval with Prescriptive and Proscriptive Constraints

## Quick Facts
- arXiv ID: 2512.20781
- Source URL: https://arxiv.org/abs/2512.20781
- Reference count: 18
- Primary result: SoFT achieves 65.25 R@5 on CIRR (+12.94), 27.93 mAP@50 on CIRCO (+6.13), and 58.44 R@50 on FashionIQ (+4.59)

## Executive Summary
Soft Filtering (SoFT) is a training-free re-ranking module for zero-shot composed image retrieval that addresses limitations of single fused queries by explicitly modeling both prescriptive (must-have) and proscriptive (must-avoid) user intent. The method leverages multimodal LLMs to extract dual textual constraints from reference-modification pairs and reweights candidate similarity scores without modifying the base retrieval model. Applied on top of CIReVL, SoFT improves performance across multiple benchmarks while providing a two-stage dataset pipeline that constructs multi-target triplets and refines them into single-target variants for more comprehensive evaluation.

## Method Summary
SoFT extracts dual constraints (prescriptive and proscriptive) from reference-modification pairs using a multimodal LLM, then reweights base retrieval scores through a combination of reward and penalty terms. The method operates as a plug-in module that computes CLIP-based similarities between candidates and constraint text, combining them with the base retriever's scores using a convex combination parameter λ. The framework includes a two-stage dataset construction pipeline that first identifies multiple plausible targets per query to capture ambiguous user intent, then refines these into single-target variants for standard evaluation.

## Key Results
- CIRR benchmark: 65.25 R@5 (+12.94) when applied to CIReVL
- CIRCO benchmark: 27.93 mAP@50 (+6.13) when applied to CIReVL
- FashionIQ benchmark: 58.44 R@50 (+4.59) when applied to CIReVL

## Why This Works (Mechanism)

### Mechanism 1: Dual Constraint Decomposition Prevents Information Dilution
Explicitly separating prescriptive and proscriptive constraints from reference-modification pairs improves retrieval accuracy over single fused query representations. The LLM performs two-step reasoning: Attribute Classification identifies keep/add/remove attribute-value pairs, then Constraint Generation converts these into prescriptive text (combining keep+add) and proscriptive text (based on remove). These constraints guide separate reward and penalty scoring against candidates. Break condition occurs when modification text lacks sufficient attribute specificity or reference image features cannot be reliably extracted by the LLM.

### Mechanism 2: Soft Score Reweighting Enables Constraint-Aware Reranking
Combining reward and penalty signals as continuous score adjustment outperforms hard filtering while maintaining differentiability. For each candidate, compute three CLIP-based cosine similarities: s_base (original CIR model score), s_reward (similarity to prescriptive constraint), s_penalty (similarity to proscriptive constraint). The SoFT score combines these as: s_SoFT = s_base ⊙ s_reward + (1 - s_penalty)/2. Final ranking uses convex combination: s_final = (1-λ)s_base + λ·s_SoFT. Break condition occurs when CLIP embeddings fail to distinguish fine-grained visual attributes or when λ is set inappropriately.

### Mechanism 3: Multi-Target Evaluation Captures Ambiguous User Intent
Single-target benchmarks underestimate true retrieval capability by penalizing semantically valid alternatives. The two-stage pipeline: (1) Retrieve diverse candidates via three criteria, then LLM assigns confidence scores (threshold τ=0.85) to identify valid multi-targets; (2) For single-target evaluation, rewrite modification text using contrastive distractors to create discriminative queries. Break condition occurs when original modification text is already highly specific or when LLM confidence threshold is set too high/low.

## Foundational Learning

- **CLIP Vision-Language Alignment**: SoFT depends entirely on computing CLIP cosine similarities between candidate images and LLM-generated textual constraints. Understanding CLIP's joint embedding space behavior is essential for debugging reward/penalty signals. Quick check: Can you explain why CLIP might fail to distinguish between "black t-shirt" and "navy t-shirt" in embedding space, and how this would affect penalty scoring?

- **Zero-Shot Composed Image Retrieval (ZS-CIR) Paradigm**: SoFT is a training-free module specifically designed to plug into existing ZS-CIR systems. Understanding baseline methods (CIReVL, SEARLE) clarifies what s_base represents and why it can be improved. Quick check: How does CIReVL generate its retrieval query differently from SEARLE, and why might this affect optimal λ selection?

- **Large Language Model Prompt Engineering for Structured Output**: SoFT's constraint extraction relies on a specific prompt structure (Attribute Classification → Constraint Generation). Understanding prompt design tradeoffs affects reproducibility and adaptation to new domains. Quick check: Why does the paper use a two-step prompt (classification then generation) rather than directly generating constraints, and what failure modes might each approach exhibit?

## Architecture Onboarding

- **Component map**: Input Layer (I_ref + T_mod) → Constraint Extraction Module (GPT-4o) → Base Retrieval System (CIReVL/SEARLE) → Soft Filtering Module (CLIP ViT-L/14) → Output (Re-ranked candidates)

- **Critical path**: LLM API call for constraint extraction → CLIP feature extraction for all candidates and constraints → Similarity matrix computation → Score recombination and sorting

- **Design tradeoffs**: LLM choice (GPT-4o used exclusively), CLIP backbone (ViT-L/14 outperforms ViT-B/32), λ parameter (CIReVL optimal at λ=1.0, SEARLE optimal at λ=0.2), penalty term instability in fine-grained domains

- **Failure signatures**: Constraint extraction failure (LLM returns generic descriptions), penalty over-suppression (Recall drops dramatically on FashionIQ), base model incompatibility (mixed results with SEARLE on single-target FashionIQ)

- **First 3 experiments**:
  1. Reproduce CIReVL + SoFT on CIRR subset with ViT-B/32, λ=1.0, verify R@1 improvement from ~60% to ~70%
  2. Ablate reward vs. penalty components on CIRCO; expect reward-only (~22 mAP@5) to outperform penalty-only
  3. Test λ sensitivity curve on FashionIQ with CIReVL; expect monotonic improvement to λ=1.0

## Open Questions the Paper Calls Out

### Open Question 1
Can the proscriptive constraint be stabilized for fine-grained retrieval domains where pretrained CLIP embeddings fail to capture subtle visual distinctions? The penalty term causes severe performance drops on FashionIQ because CLIP embeddings struggle with subtle visual differences. What evidence would resolve it: Experiments replacing CLIP visual encoder with fine-grained alternatives (e.g., DINOv2) within SoFT scoring mechanism.

### Open Question 2
How does SoFT perform when implemented with smaller, open-source multimodal models compared to the currently required GPT-4o? The framework's reliance on a large proprietary model limits reproducibility and deployment. What evidence would resolve it: Comparative ablation study measuring constraint extraction quality and final retrieval accuracy using open models like LLaVA or Idefics2.

### Open Question 3
Does the performance variance on standard FashionIQ stem from evaluation underspecification or a fundamental misalignment of SoFT's re-weighting logic? The authors attribute instability to the benchmark's single-target limitation, but the interaction between SoFT's dual constraints and benchmark's ambiguity remains unquantified. What evidence would resolve it: Correlating magnitude of SoFT's score adjustments with human judgments of semantic validity for non-annotated targets.

## Limitations

- **LLM Dependency**: Performance tightly coupled to GPT-4o's ability to extract meaningful attributes, making it vulnerable to model variations and domain-specific failures
- **Cost Considerations**: Substantial API costs ($2.69-$17.68 per dataset) due to dual constraint extraction may limit practical deployment
- **Domain Sensitivity**: Penalty term instability on FashionIQ suggests approach requires careful tuning per domain, particularly for fine-grained attribute changes

## Confidence

- **High Confidence**: Core mechanism (dual constraint decomposition + soft reweighting) and main empirical results on CIRR and CIRCO benchmarks
- **Medium Confidence**: Generalization across base retrievers (mixed results with SEARLE on FashionIQ suggest base model compatibility is non-trivial)
- **Medium Confidence**: Multi-target dataset construction pipeline, though validation against CIRCO's ground truth provides external corroboration

## Next Checks

1. **Domain Transfer Test**: Apply SoFT to a new fine-grained domain (e.g., product catalogs) to verify penalty term stability and optimal λ tuning requirements

2. **Base Model Compatibility Study**: Systematically test SoFT with additional base retrievers (e.g., CoCa, BLIP) to map the parameter space of λ values needed for different architectures

3. **Cost-Performance Trade-off Analysis**: Compare GPT-4o against smaller multimodal models (e.g., LLaVA-1.5) for constraint extraction to quantify the practical deployment threshold