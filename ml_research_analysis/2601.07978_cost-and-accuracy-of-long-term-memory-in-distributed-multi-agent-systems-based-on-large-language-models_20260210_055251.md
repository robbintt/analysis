---
ver: rpa2
title: Cost and accuracy of long-term memory in Distributed Multi-Agent Systems based
  on Large Language Models
arxiv_id: '2601.07978'
source_url: https://arxiv.org/abs/2601.07978
tags:
- arxiv
- available
- online
- memory
- mem0
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study introduces a testbed to evaluate long-term memory frameworks\
  \ in distributed multi-agent systems under varying network conditions. Two memory\
  \ systems\u2014mem0 (vector-based) and Graphiti (graph-based)\u2014were compared\
  \ using the LoCoMo benchmark in unconstrained and constrained network scenarios."
---

# Cost and accuracy of long-term memory in Distributed Multi-Agent Systems based on Large Language Models

## Quick Facts
- arXiv ID: 2601.07978
- Source URL: https://arxiv.org/abs/2601.07978
- Reference count: 40
- Primary result: mem0 significantly outperforms Graphiti in computational and financial efficiency while maintaining equivalent accuracy on the LoCoMo benchmark

## Executive Summary
This study introduces a testbed to evaluate long-term memory frameworks in distributed multi-agent systems under varying network conditions. Two memory systems—mem0 (vector-based) and Graphiti (graph-based)—were compared using the LoCoMo benchmark in unconstrained and constrained network scenarios. Results show mem0 significantly outperformed Graphiti in computational and financial efficiency, with 86.5% faster loading and 14.7% faster query response times, while maintaining equivalent accuracy. Network constraints had minimal impact on performance. A statistical Pareto efficiency framework identified mem0 as the optimal choice, balancing cost and accuracy in DMAS environments.

## Method Summary
The study compared mem0 and Graphiti memory systems in a distributed multi-agent system using the LoCoMo benchmark. Three Docker-based agents (Coordinator using Qwen2.5:3b, Responder using GPT-4o-mini, and Memory Agent using either mem0 or Graphiti) processed 199 questions across 19 sessions. Network conditions were varied using Toxiproxy (200ms latency, 8Mbps bandwidth). Performance metrics included cost (AWS Fargate compute + OpenAI tokens), latency (loading/query time), and accuracy (string/semantic similarity). A statistical Pareto efficiency framework was applied to determine dominance when accuracy differences were not statistically significant.

## Key Results
- mem0 was 86.5% faster during the loading phase and 14.7% faster during the Q&A phase than Graphiti
- mem0 consumed fewer computational resources and had lower financial costs than Graphiti
- Accuracy differences between mem0 and Graphiti were not statistically significant (p > 0.05)
- Network constraints had minimal impact on overall performance metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Vector-based memory (mem0) achieves lower computational cost than graph-based memory (Graphiti) for DMAS long-term memory.
- Mechanism: mem0's architecture, which includes LLM compression, reduces token consumption and processing overhead compared to Graphiti's graph database approach.
- Core assumption: Cost differences are primarily due to the underlying memory architecture (vector vs. graph) and not confounded by unmeasured implementation details.
- Evidence anchors: Results indicate mem0 significantly outperforms Graphiti in efficiency, featuring faster loading times, lower resource consumption, and minimal network overhead. mem0 was 86.5% faster during the loading phase and 14.7% faster during the Q&A phase than Graphiti.

### Mechanism 2
- Claim: Long-term memory accuracy is not statistically different between mem0 and Graphiti on the LoCoMo benchmark for DMAS.
- Mechanism: Both systems provide retrieved context to a language model (gpt-4o-mini) for answer generation, and statistical analysis shows retrieval differences don't translate to significant accuracy differences.
- Core assumption: The LoCoMo benchmark and evaluation methodology provide a reliable measure of "knowledge retention" and the sample size (N=199) is sufficient to detect meaningful differences.
- Evidence anchors: Crucially, accuracy differences were not statistically significant. A statistical Pareto efficiency framework identified mem0 as the optimal choice, balancing cost and accuracy.

### Mechanism 3
- Claim: A statistical Pareto efficiency framework provides a more robust method for comparing memory systems than simple cost-accuracy ratios when accuracy differences are not statistically significant.
- Mechanism: When performance differences fall within the margin of random variance, a system with statistically equivalent accuracy but lower cost is declared "Pareto-optimal."
- Core assumption: The choice of α=0.05 for the z-test is an appropriate threshold for determining "statistical equivalence."
- Evidence anchors: A statistical Pareto efficiency framework identified mem0 as the optimal choice, balancing cost and accuracy. A system S_A (mem0) is defined as strictly dominant over S_B (Graphiti) if: 1. C(S_A) < C(S_B) (Financial Dominance), and 2. The difference A(S_B) - A(S_A) was not statistically significant (Statistical Equivalence).

## Foundational Learning

- Concept: **Vector Embeddings & Databases** vs. **Knowledge Graphs**
  - Why needed here: This is the fundamental architectural difference between the two systems being compared.
  - Quick check question: If your data has deep, complex relationships (e.g., "A is a parent of B, who is employed by C"), which architecture might have a theoretical advantage?

- Concept: **Statistical Significance (p-value)**
  - Why needed here: The paper's core conclusion rests on the lack of a statistically significant difference in accuracy.
  - Quick check question: If a p-value is greater than 0.05, does that mean there is definitively no difference between two groups, or that the evidence isn't strong enough to prove a difference?

- Concept: **Pareto Efficiency**
  - Why needed here: This is the decision-making framework used to select the "optimal" system.
  - Quick check question: If one option is cheaper and at least as good as another in every measured aspect, is it Pareto-efficient?

## Architecture Onboarding

- Component map: Coordinator Agent (qwen2.5:3b) -> Memory Agent (mem0/Graphiti) -> Responder Agent (gpt-4o-mini)
- Critical path:
    1. Loading Phase: LoCoMo service provides data → Coordinator decomposes it → Memory Agent stores via mem0 or Graphiti
    2. Q&A Phase: Question sent to Coordinator's API → Coordinator forwards to Memory Agent for retrieval → Coordinator sends question + memories to Responder → Responder produces final answer
- Design tradeoffs: The paper demonstrates a clear tradeoff: cost & efficiency (mem0) vs. relational structure (Graphiti). While mem0 won on cost with equivalent accuracy, Graphiti's graph structure may be theoretically superior for tasks requiring complex relational reasoning.
- Failure signatures:
    - High rate of "I don't know" (IDK) responses from the Responder
    - Statistical insignificance of results, failing to reject the null hypothesis
    - Non-robustness of simple efficiency metrics like "cost-per-successful-query" when accuracy differences are not significant
- First 3 experiments:
    1. Run the full evaluation with the current configuration to establish baseline performance, cost, and accuracy metrics for both mem0 and Graphiti under unconstrained network conditions
    2. Replicate the study by introducing network constraints (200ms latency, 8 Mbit bandwidth) using Toxiproxy to test the system's robustness in a hybrid cloud-edge environment
    3. Analyze the specific question types where Graphiti's raw accuracy was higher to identify if there's a pattern (e.g., multi-hop reasoning questions) that could form a new, targeted experiment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific failure modes causing high "I don't know" (IDK) rates in current memory retrieval architectures?
- Basis in paper: The authors state in Section 4.4 that "the DMAS responded to a significant number of questions with IDK" and suggest investigating root causes, such as mem0’s compression algorithm versus Graphiti’s engine.
- Why unresolved: The current analysis focuses on aggregate accuracy statistics rather than qualitative retrieval failures or the loss of information fidelity during memory ingestion.
- What evidence would resolve it: Ablation studies running experiments with a standard RAG system (e.g., vanilla Qdrant) as a control group to isolate whether compression or graph structuring is responsible for the information loss.

### Open Question 2
- Question: How does DMAS performance vary in dynamic peer-to-peer network scenarios with real-time instability?
- Basis in paper: Section 4.4 notes that the "chosen network constraints had limited impact" and proposes future experiments in "peer-to-peer DMAS scenarios" involving dynamic monitoring of peer disconnections and jitter.
- Why unresolved: The current testbed uses static, controlled network profiles (unconstrained vs. fixed latency/bandwidth) which fail to simulate the volatility of real-world edge environments.
- What evidence would resolve it: Results from experiments where agents must dynamically switch between individual and collaborative task execution based on fluctuating network health metrics.

### Open Question 3
- Question: Does the cost-accuracy balance hold when agents possess full autonomy over API discovery and memory management?
- Basis in paper: Section 4.4 highlights that "the given DMAS has little autonomy" because phases were "completely prescribed," suggesting an improved implementation where agents "explore... on their own accord."
- Why unresolved: The current "coordinator" agent follows a rigid tool-calling sequence; it is unknown if the efficiency of mem0 or Graphiti degrades when agents must dynamically plan their own memory ingestion strategies.
- What evidence would resolve it: Comparative trials where the coordinator agent must autonomously discover available services (like the LoCoMo API) and formulate its own memory storage protocols without hardcoded workflows.

## Limitations
- The performance comparison is based on a single benchmark (LoCoMo) and specific network conditions (200ms latency, 8Mbps bandwidth)
- Architectural differences between mem0 and Graphiti implementations beyond their core data structures could contribute to performance variations not fully isolated in the analysis
- The statistical Pareto efficiency framework relies on α=0.05 as the threshold for significance, which may not be optimal for all applications

## Confidence

- **High confidence:** mem0's superior computational efficiency (86.5% faster loading, 14.7% faster query response) is well-supported by direct measurements
- **Medium confidence:** The claim that mem0 maintains equivalent accuracy to Graphiti is supported by statistical analysis showing non-significant differences (p > 0.05)
- **Low confidence:** The generalizability of these findings to other benchmarks, data types with complex relational structures, or different network conditions beyond the tested constraints

## Next Checks
1. Replicate with alternative benchmarks: Test both memory systems on knowledge graphs or relational reasoning tasks where Graphiti's architecture might theoretically excel to validate if accuracy differences remain non-significant across diverse workloads.

2. Isolate implementation factors: Create controlled experiments that vary only the underlying data structure (vector vs. graph) while keeping all other implementation details constant to confirm the architectural contribution to performance differences.

3. Vary network conditions systematically: Conduct experiments across a broader range of network constraints (latency: 50-500ms, bandwidth: 2-50Mbps) to establish the boundaries where mem0's efficiency advantage holds and identify potential thresholds where Graphiti's structure provides benefits.