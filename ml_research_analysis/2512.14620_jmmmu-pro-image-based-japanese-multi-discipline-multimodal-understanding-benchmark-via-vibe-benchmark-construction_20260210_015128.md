---
ver: rpa2
title: 'JMMMU-Pro: Image-based Japanese Multi-discipline Multimodal Understanding
  Benchmark via Vibe Benchmark Construction'
arxiv_id: '2512.14620'
source_url: https://arxiv.org/abs/2512.14620
tags:
- image
- lmms
- jmmmu-pro
- question
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces JMMMU-Pro, an image-based Japanese Multi-discipline
  Multimodal Understanding Benchmark that embeds both question images and text into
  a single image, requiring integrated visual-textual understanding. To construct
  it efficiently, the authors propose Vibe Benchmark Construction, where a generative
  model (Nano Banana Pro) creates visual questions and humans verify and refine outputs
  through prompt adjustments.
---

# JMMMU-Pro: Image-based Japanese Multi-discipline Multimodal Understanding Benchmark via Vibe Benchmark Construction

## Quick Facts
- arXiv ID: 2512.14620
- Source URL: https://arxiv.org/abs/2512.14620
- Reference count: 40
- Key outcome: Open-source LMMs achieve only 45.83% accuracy on JMMMU-Pro, far below the 87.04% of closed-source models, revealing major capability gaps in integrated visual-textual reasoning.

## Executive Summary
This paper introduces JMMMU-Pro, an image-based Japanese Multi-discipline Multimodal Understanding Benchmark that requires integrated visual-textual understanding by embedding both question images and text into single composite images. The authors propose Vibe Benchmark Construction, where a generative model (Nano Banana Pro) creates visual questions and humans verify and refine outputs through prompt adjustments. Experiments show that all open-source LMMs struggle significantly on JMMMU-Pro, with the best scoring only 45.83% accuracy, compared to much higher performance on the original JMMMU. This performance drop highlights the benchmark's value in evaluating true visual-textual integration abilities. Closed-source models perform substantially better, revealing a major capability gap. The findings demonstrate that solving JMMMU-Pro requires not only strong OCR but also deeper multimodal reasoning, making it a crucial tool for guiding future LMM development.

## Method Summary
JMMMU-Pro is constructed by embedding question images and texts into single composite images, requiring integrated visual-textual understanding. The Vibe Benchmark Construction method uses Nano Banana Pro to generate candidate visual questions with varying parameters (background, color, font, margin, state, aspect ratio), followed by human verification and refinement. Approximately 95% of questions are automatically generated, with 5% manually constructed for challenging cases. The benchmark is evaluated on 14 LMMs using both direct and chain-of-thought prompting, measuring multiple-choice accuracy and OCR performance via Levenshtein distance.

## Key Results
- Open-source LMMs achieve maximum 45.83% accuracy on JMMMU-Pro, significantly lower than their performance on JMMMU (separate modalities).
- Closed-source models achieve 87.04% accuracy on JMMMU-Pro, revealing substantial capability gaps between model types.
- OCR accuracy correlates with JMMMU-Pro performance (r=0.593), but models with similar OCR scores show divergent task performance, indicating reasoning beyond text extraction is required.
- Best-performing model (GPT-5.2) achieves 83.33% accuracy, while the best open-source model (Qwen3-VL-8B) achieves only 47.27%.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Embedding question text and question images into single composite images forces integrated visual-textual processing that cannot be shortcut through modality separation.
- Mechanism: The benchmark construction eliminates the architectural advantage of receiving text as clean token input—models must extract text via OCR, parse layout, identify question regions, AND perform disciplinary reasoning simultaneously.
- Core assumption: Real-world Japanese LMM usage frequently involves screenshots with embedded text, making this evaluation ecologically valid.
- Evidence anchors:
  - [abstract] "creating a benchmark that requires integrated visual-textual understanding through visual perception"
  - [section 1] "users commonly provide LMMs with screenshots that include both Japanese text and images"
  - [corpus] Related papers (MangaVQA, JDocQA) involve Japanese text-in-images but target lower reasoning complexity; no corpus evidence directly validates this specific integrated mechanism
- Break condition: If models solve tasks by first running isolated OCR then treating extracted text as standard input without visual context integration, the "integrated understanding" claim weakens.

### Mechanism 2
- Claim: Vibe Benchmark Construction achieves scalability by shifting human role from content creation to verification-and-prompt-adjustment.
- Mechanism: Nano Banana Pro generates photorealistic composite images with clean embedded Japanese text; humans perform binary accept/reject decisions and minor prompt refinements rather than manual image composition, achieving 95% automated generation.
- Core assumption: The image generation model's text embedding accuracy is sufficient that human verification reliably catches systematic errors.
- Evidence anchors:
  - [abstract] "an image generative model (e.g., Nano Banana Pro) produces candidate visual questions, and humans verify the outputs"
  - [section 3.3] "approximately 95% of all the questions in JMMMU-Pro were generated by Nano Banana Pro"
  - [corpus] LogicOCR (mentioned in section 2) attempted similar approach with GPT-Image-1 but faced "reduced photorealism" and "dataset shrinkage caused by sample filtering"
- Break condition: If the 29% first-round failure rate contains systematic patterns that bias certain question types or domains, benchmark validity may be compromised despite human review.

### Mechanism 3
- Claim: Japanese OCR capability is necessary but not sufficient for JMMMU-Pro; models require additional integrated vision-language reasoning abilities.
- Mechanism: OCR accuracy correlates with JMMMU-Pro performance (r=0.593), but models with comparable OCR scores show divergent task performance—indicating a second capability dimension beyond text extraction.
- Core assumption: The Levenshtein-distance OCR metric captures the perception quality relevant to downstream reasoning.
- Evidence anchors:
  - [section 5.2] "correlation coefficient between OCR accuracy and JMMMU-Pro accuracy is 0.593"
  - [section 5.2] "Heron-NVILA and Sarashina2.2-V are comparable for OCR performance, the performance for JMMMU-Pro differs a lot"
  - [corpus] No corpus papers establish Japanese OCR-reasoning decoupling; appears to be a novel empirical finding in this work
- Break condition: If OCR measurement fails to capture layout understanding or region-identification errors that are actually part of OCR-adjacent perception, the "reasoning beyond OCR" attribution may be overstated.

## Foundational Learning

- Concept: Multimodal Benchmark Separation vs. Integration
  - Why needed here: Understanding why JMMMU (separate modalities) shows higher scores than JMMMU-Pro (integrated) reveals what the new benchmark actually tests.
  - Quick check question: If a model scores 50% on JMMMU but 30% on JMMMU-Pro with identical questions, what specific capability gap does this 20-point drop indicate?

- Concept: Chain-of-Thought Prompting Variability
  - Why needed here: CoT effectiveness differs across models and across JMMMU vs. JMMMU-Pro for the same model, suggesting prompt strategy must be task-aware.
  - Quick check question: Why would CoT help Qwen3-VL-8B on JMMMU-Pro but hurt it on JMMMU?

- Concept: Japanese Text Rendering in Image Generation
  - Why needed here: The pipeline relies on clean Japanese text embedding; understanding failure modes (long text, small fonts, formulas) prevents misinterpreting benchmark difficulty as model failure.
  - Quick check question: What characteristics made 5% of samples require manual construction despite using a state-of-the-art generation model?

## Architecture Onboarding

- Component map:
  - Base dataset: JMMMU-verified-2025-12 (1,320 questions, 28 subjects, culture-agnostic 720 + culture-specific 600 split)
  - Generation engine: Nano Banana Pro API (gemini-3-pro-image-preview), 1K resolution output
  - Prompt parameters: Background (9 types: workbook, whiteboard, projector, etc.), Color (6), Font (5), Margin (2), State (3: phone photo, PC screenshot, phone screenshot), Aspect ratio (4)
  - Review pipeline: Custom annotation tool → 71% first-pass acceptance → regeneration with prompt adjustment → final cross-check
  - Fallback path: Manual construction (67 samples) for long text, small fonts, extreme ratios, chemical formulas, policy-rejected content

- Critical path:
  1. Input: Question text + question image from JMMMU
  2. Generate: Nano Banana Pro with parameterized prompt (avoid image-tag instructions—they degrade quality)
  3. Review: Check text accuracy, image correctness, layout naturalness
  4. Decision: Accept (71%), regenerate same prompt, regenerate adjusted prompt, or escalate to manual
  5. Validation: Cross-check across authors for consistency standards

- Design tradeoffs:
  - Tag handling: Explicit image-tag instructions degraded generation; accepted uncontrolled tag variations to preserve image quality
  - Automation coverage: 95% automated vs. 5% manual represents practical ceiling for current generation models
  - Diversity vs. control: 6 parameter dimensions create layout variety but complicate failure pattern analysis

- Failure signatures:
  - Vision-side weakness: Models performing similarly on CA and CS subsets of JMMMU-Pro despite different JMMMU gaps indicate fundamental OCR/layout issues rather than domain-specific reasoning failures
  - Open-source ceiling: Best open-source model at 47.27% vs. closed-source at 87.04% reveals architectural or training-data gaps
  - CoT inconsistency: Models showing opposite CoT effects between JMMMU and JMMMU-Pro suggest reasoning scaffolding interacts with perception quality

- First 3 experiments:
  1. Baseline comparison: Evaluate target model on both JMMMU and JMMMU-Pro with Direct and CoT prompts; calculate per-subset gaps (CS vs. CA) to diagnose vision-side vs. domain-knowledge weaknesses.
  2. OCR isolation test: Run provided OCR extraction prompt; compute Levenshtein accuracy; identify whether failures correlate with specific text characteristics (length, font style, background complexity).
  3. Error taxonomy: Sample 50 failures; classify as (a) OCR/extraction errors, (b) reasoning errors on correctly extracted text, (c) layout/region identification errors; prioritize improvement direction based on distribution.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural or training improvements are needed for open-source LMMs to achieve both strong Japanese OCR capability and integrated visual-textual reasoning?
- Basis in paper: [explicit] The authors state "solving JMMMU-pro requires improving both OCR capability and the ability to interpret language and vision in an integrated manner through visual perception," and show that "high OCR ability does not necessarily translate directly into high JMMMU-Pro accuracy."
- Why unresolved: The correlation analysis (r=0.593) reveals OCR and reasoning are partially decoupled, but the paper does not identify which model components or training regimes address each.
- What evidence would resolve it: Ablation studies varying OCR pretraining, vision-language alignment layers, and reasoning-focused fine-tuning on JMMMU-Pro performance.

### Open Question 2
- Question: How does Vibe Benchmark Construction generalize to other languages and to alternative image generation models beyond Nano Banana Pro?
- Basis in paper: [inferred] The methodology is demonstrated only with Japanese and Nano Banana Pro, but the authors claim it "offers an efficient guideline for future development of image-based VQA benchmarks."
- Why unresolved: Nano Banana Pro's Japanese text embedding capability may not be matched by other models, and no cross-lingual validation is provided.
- What evidence would resolve it: Replicating the pipeline with other generative models (e.g., DALL-E, Stable Diffusion variants) and for non-Japanese benchmarks, measuring pass rates and manual correction needs.

### Open Question 3
- Question: Can the performance gap between closed-source and open-source LMMs on JMMMU-Pro be attributed primarily to data, architecture, or training compute?
- Basis in paper: [explicit] The authors highlight a "substantial and concerning gap between closed-source and open-source LMMs" (e.g., GPT-5.2 at 83.33% vs. best open-source at 47.27%).
- Why unresolved: The paper documents the gap but cannot isolate causes due to the opaque nature of closed-source models.
- What evidence would resolve it: Controlled experiments scaling open-source models' training data and compute, combined with architectural analyses matching closed-source model reported specifications where available.

## Limitations

- The benchmark relies on Nano Banana Pro, a generative model whose access details remain unspecified, potentially limiting reproducibility.
- The claim that JMMMU-Pro tests "integrated visual-textual understanding" depends on the assumption that models cannot solve tasks by performing isolated OCR followed by standard text-only reasoning, which is not definitively proven.
- The 29% first-round failure rate in the Vibe Benchmark Construction process suggests potential systematic biases that could affect benchmark validity, though human verification is intended to catch these issues.

## Confidence

- **High confidence**: The empirical finding that open-source LMMs perform substantially worse (45.83% max) than closed-source models (87.04% max) on JMMMU-Pro, and that this gap exceeds the performance difference on the original JMMMU benchmark. This demonstrates a real capability difference between model classes.
- **Medium confidence**: The correlation (r=0.593) between OCR accuracy and JMMMU-Pro performance suggests that while OCR capability is necessary, it is not sufficient for high performance. The paper's attribution of remaining performance differences to "integrated reasoning" is plausible but not definitively proven.
- **Low confidence**: The claim that Vibe Benchmark Construction achieves scalability through human verification-and-prompt-adjustment is weakened by the 29% first-round failure rate and lack of systematic error analysis to rule out potential biases introduced during the verification process.

## Next Checks

1. **OCR-then-reasoning validation**: Run target models on JMMMU-Pro by first extracting text via a high-quality OCR pipeline, then feeding extracted text to the same models without images. Compare performance to direct JMMMU-Pro results to quantify the "integrated understanding" advantage.

2. **Error pattern analysis**: Sample 100 model failures and categorize them as (a) OCR/extraction errors, (b) reasoning errors on correctly extracted text, or (c) layout/region identification errors. This will reveal whether the performance gap stems from perception quality or reasoning depth.

3. **Benchmark generalization test**: Evaluate the same models on JMMMU-Pro questions converted back to the original JMMMU format (separate image and text). If performance on converted questions matches JMMMU-Pro, this suggests the benchmark primarily tests text extraction difficulty rather than integrated reasoning.