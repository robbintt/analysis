---
ver: rpa2
title: GPT-5 Model Corrected GPT-4V's Chart Reading Errors, Not Prompting
arxiv_id: '2510.06782'
source_url: https://arxiv.org/abs/2510.06782
tags:
- gpt-5
- average
- what
- holf
- holf-multi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study evaluated how model architecture versus prompt engineering
  affects chart reading accuracy. Using 107 difficult questions from the CHART-6 benchmark
  where GPT-4V failed, it compared three LLMs (GPT-4V, GPT-4o, GPT-5) under three
  prompting conditions: full instructions, question-only, and GPT-5-generated chart
  descriptions.'
---

# GPT-5 Model Corrected GPT-4V's Chart Reading Errors, Not Prompting

## Quick Facts
- arXiv ID: 2510.06782
- Source URL: https://arxiv.org/abs/2510.06782
- Authors: Kaichun Yang; Jian Chen
- Reference count: 40
- Primary result: Model architecture, not prompt engineering, drives accuracy gains in chart reading tasks.

## Executive Summary
This study evaluated whether advances in large language model architecture or prompt engineering drive improvements in chart reading accuracy. Using 107 difficult questions from the CHART-6 benchmark where GPT-4V failed, the researchers compared three models (GPT-4V, GPT-4o, GPT-5) under three prompting conditions: full instructions, question-only, and GPT-5-generated chart descriptions. GPT-5 significantly outperformed both GPT-4o and GPT-4V, with accuracy improvements of 20-40 percentage points. Surprisingly, prompt variations had minimal effects, with chart descriptions sometimes reducing performance. Statistical analysis confirmed model type as the dominant factor in correct responses, while prompt conditions showed no significant impact. The findings indicate that advances in model architecture, particularly agentic reasoning capabilities, drive performance improvements in visualization understanding more than prompt engineering.

## Method Summary
The study evaluated zero-shot Visual Question Answering on 107 difficult chart-image questions from the CHART-6 benchmark where GPT-4V previously failed. Three models (gpt-5-2025-08-07, gpt-4o-2024-05-13, and gpt-4-turbo-preview) were tested under three prompting conditions: full CHART-6 instructions, question-only, and GPT-5-generated chart descriptions. Each question was queried five times per condition with a maximum output length of 3000 tokens. Accuracy was measured as binary correctness for GGR, VLAT, and CALVI datasets, while HOLF/HOLF-Multi used a relaxed ±5% tolerance. The study used logistic regression and Tukey's HSD test to analyze the effects of model type and prompt conditions on accuracy.

## Key Results
- GPT-5 substantially outperformed both GPT-4o and GPT-4V on all chart reading tasks, with 20-40 percentage point accuracy improvements.
- Prompt variations (full instructions, question-only, chart descriptions) had minimal effects on performance, with chart descriptions sometimes reducing accuracy.
- Statistical analysis confirmed model type as the dominant factor in correct responses, while prompt conditions showed no significant impact.
- Both models still faced difficulties on complex datasets requiring deception detection (CALVI) or multi-hop reasoning (HOLF), with accuracy often below 50%.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** GPT-5's superior performance stems from its agentic reasoning architecture, which enables more robust integration of visual perception and multi-step inference.
- **Mechanism:** The model uses a unified system with dedicated reasoning component that can perform extended internal chain-of-thought processing before generating an answer.
- **Core Assumption:** Performance gain is driven by architectural improvements in reasoning capabilities rather than simply larger training data.
- **Evidence Anchors:** [abstract] "model architecture dominates the inference accuracy"; [section 2.2] Describes GPT-5 as "the new agentic reasoning model"; [corpus] Weak direct evidence from neighbor papers.

### Mechanism 2
- **Claim:** Detailed chart descriptions do not improve—and sometimes harm—performance because reasoning models are optimized for direct task execution.
- **Mechanism:** GPT-5's reasoning process is designed to extract necessary information directly from visual input during its internal reasoning chain.
- **Core Assumption:** The model's training has optimized it for direct perception-to-reasoning pathways, making intermediate textual descriptions redundant or interfering.
- **Evidence Anchors:** [results] "prompts with chart descriptions did not provide benefits"; [results] "significant positive interaction between GPT-5 with GPT-5 Description condition (β=0.219, p<0.001)"; [corpus] Relevant neighbor paper on LLM sensitivity to input perturbations.

### Mechanism 3
- **Claim:** Persistent low accuracy on complex datasets (CALVI, HOLF) indicates current limitations in visual deception detection and multi-hop quantitative reasoning.
- **Mechanism:** These datasets require identifying misleading design choices or synthesizing information across multiple chart elements and data points.
- **Core Assumption:** Errors are due to fundamental reasoning/evaluation limitations, not just perceptual inaccuracies.
- **Evidence Anchors:** [results] "models generally performed poorly on CALVI and HOLF"; [discussion] "both models still faced difficulties on more complex datasets"; [corpus] Neighbor paper evaluating LVLM hallucinations in chart understanding.

## Foundational Learning

**Concept: Zero-shot Visual Question Answering (VQA)**
- **Why needed here:** The entire evaluation framework depends on understanding what it means for a model to answer chart questions without task-specific training.
- **Quick check question:** How would you explain the difference between a model answering a chart question "zero-shot" versus one that was fine-tuned on similar chart-question pairs?

**Concept: Multimodal Reasoning Integration**
- **Why needed here:** The paper compares a "multimodal" GPT-4V with an "agentic" GPT-5; grasping how visual and language streams are combined is critical.
- **Quick check question:** Can a model be multimodal without having integrated reasoning? What would be the behavioral signature of each?

**Concept: Chain-of-Thought (CoT) & Reasoning Models**
- **Why needed here:** GPT-5 is described as a reasoning model using internal CoT; this distinguishes it from models that might just pattern-match.
- **Quick check question:** If you could only see a model's final answer and not its reasoning process, what experiment could you design to infer whether it used a CoT-like mechanism?

## Architecture Onboarding

**Component Map:**
Image + Question -> Input Processor -> Real-time Router -> Deep Reasoning Model (with internal CoT) -> Output Generator -> Final Answer

**Critical Path for a Chart Question:**
1. Chart image and question are received and encoded.
2. Router analyzes complexity; difficult chart questions trigger the deep reasoning path.
3. Reasoning model performs steps like: identify chart type, locate relevant axes/legend, extract values, perform calculations, verify consistency.
4. Final answer is formulated and returned.

**Design Tradeoffs:**
- **Latency vs. Depth:** Engaging the deep reasoning model improves accuracy but increases response time.
- **Prompt Concision vs. Detail:** Concise, direct prompts are more effective for reasoning models, trading off potential benefit of detailed context.
- **Generality vs. Specialization:** A general reasoning model may outperform specialized chart-reading models on diverse tasks but could be less efficient.

**Failure Signatures:**
- **Precision over Accuracy:** Giving "24" instead of "25" when visual estimation is ambiguous.
- **Description Interference:** Performance dropping when verbose, generated descriptions are added to the prompt.
- **Complex Reasoning Collapse:** Low accuracy on CALVI/HOLF datasets requiring deception detection or multi-hop reasoning.

**First 3 Experiments:**
1. **Prompt Ablation Test:** Systematically remove/replace components of the chart description to identify which, if any, elements provide non-noisy signal to GPT-5.
2. **Routing Trigger Analysis:** Feed a spectrum of chart-question pairs and measure correlation between router decision and final accuracy/timing.
3. **Reasoning Trace Inspection:** For correct and incorrect answers on complex datasets, analyze internal reasoning chains to identify common failure modes.

## Open Questions the Paper Calls Out

**Open Question 1**
- **Question:** Which specific structural components of chart descriptions (e.g., title-only, axes-only, legend-only) are necessary to shift or improve model answers?
- **Basis in paper:** [explicit] The Discussion section explicitly proposes "Ablation of description components: title-only / axes-only / legend-only / style-only, to map which elements (if any) shift answers."
- **Why unresolved:** The current study only tested full GPT-5 generated descriptions against no descriptions, finding no consistent benefit, but did not isolate specific informative elements.
- **What evidence would resolve it:** Experimental results comparing model performance using partial descriptions containing only specific visual elements.

**Open Question 2**
- **Question:** Does GPT-5's superior performance on difficult chart reasoning tasks generalize to simpler visualization tasks, such as basic value retrievals?
- **Basis in paper:** [explicit] The authors state that "Further testing on other datasets and reasoning tasks is necessary to generalize its performance, for example, for the simple value retrievals in Bendeck and Stasko."
- **Why unresolved:** This study focused exclusively on a subset of 107 "difficult" questions from the CHART-6 benchmark where GPT-4V had previously failed.
- **What evidence would resolve it:** Evaluation of GPT-5 on standard visualization literacy benchmarks containing simpler, direct data extraction tasks.

**Open Question 3**
- **Question:** How does model answer consistency and accuracy vary with incremental changes in description length or content perturbations?
- **Basis in paper:** [explicit] The Discussion calls for "Answer consistency and perturbation curves: for each chart, estimate each model's consistency... and tolerance to incremental description length."
- **Why unresolved:** The current analysis compared three distinct prompt types but did not measure the robustness of responses to minor variations in the input description.
- **What evidence would resolve it:** Sensitivity analysis measuring the variance in model correctness as description verbosity or detail is gradually altered.

## Limitations
- The study's conclusions are based on a single dataset (CHART-6) and may not generalize to other chart types or question formats.
- The analysis assumes observed accuracy improvements are solely due to architectural differences, but potential confounding factors cannot be ruled out.
- The study does not explore the impact of different sampling parameters or temperature settings on performance.

## Confidence
- **High Confidence:** GPT-5 outperforms GPT-4V and GPT-4o on the tested dataset, and model architecture is a more significant factor than prompting variations.
- **Medium Confidence:** The specific claim that GPT-5's agentic reasoning architecture is the primary driver of performance gains.
- **Medium Confidence:** The observation that chart descriptions generally do not improve performance.

## Next Checks
1. **Dataset Generalization Test:** Evaluate the same models and prompting conditions on a different, held-out chart understanding benchmark to verify if GPT-5's advantages persist across datasets.
2. **Ablation Study on Reasoning Traces:** If model access permits, disable the internal chain-of-thought generation in GPT-5 and measure the impact on chart reading accuracy to directly test the importance of the reasoning architecture.
3. **Controlled Prompt Engineering Experiment:** Systematically test GPT-5 with minimal, structured chart descriptions (e.g., only axis ranges, only legend) to determine if specific, non-noisy information can be beneficial, challenging the "noise interference" hypothesis.