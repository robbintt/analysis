---
ver: rpa2
title: 'FoodRL: A Reinforcement Learning Ensembling Framework For In-Kind Food Donation
  Forecasting'
arxiv_id: '2511.04865'
source_url: https://arxiv.org/abs/2511.04865
tags:
- food
- time
- forecasting
- data
- ensemble
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FoodRL introduces a reinforcement learning-based metalearning framework
  to dynamically ensemble forecasts for in-kind food donations, addressing the high
  volatility and concept drift common in food bank data. By clustering similar forecasting
  models, FoodRL reduces the action space and enables the RL agent to adaptively assign
  weights based on recent performance and contextual information.
---

# FoodRL: A Reinforcement Learning Ensembling Framework For In-Kind Food Donation Forecasting

## Quick Facts
- **arXiv ID**: 2511.04865
- **Source URL**: https://arxiv.org/abs/2511.04865
- **Reference count**: 27
- **Primary result**: RL-based ensembling consistently outperforms baseline methods for food donation forecasting, especially during concept drift periods

## Executive Summary
FoodRL introduces a reinforcement learning-based metalearning framework that clusters and dynamically weights diverse forecasting models for in-kind food donation forecasting. The framework addresses the high volatility and concept drift common in food bank data by reducing action space complexity through model clustering and enabling adaptive weight assignment based on recent performance and contextual information. Evaluated on multi-year donation data from two structurally distinct U.S. food banks—one on the East Coast affected by hurricanes and one on the West Coast impacted by wildfires—FoodRL consistently outperforms baseline methods, particularly during periods of disruption.

## Method Summary
FoodRL is a reinforcement learning-based metalearning framework for time-series ensemble forecasting. It clusters base forecasting models (252-294 models including ARIMA, MA, LSTM, BSTS, ETS variants) into 40-60 groups using K-means based on prediction similarity, then trains a PPO agent to dynamically weight these clusters. The RL agent observes temporal features extracted via TSFEL from rolling historical windows, cluster predictions, and previous weights to output normalized weight distributions. The reward signal is negative MAPE. The framework is evaluated on two food bank datasets (EFB: 14 years monthly, WFB: 5 years monthly) using the final two years as test data.

## Key Results
- FoodRL achieves the lowest MAPE across both East Coast Food Bank (EFB) and West Coast Food Bank (WFB) datasets
- The framework outperforms all constituent models as well as baselines including Simple Averaging, Genetic Algorithm, and standard RL
- FoodRL shows especially strong performance for moderate and extreme donation declines, as well as slight trends, though all models struggle with extreme increases driven by natural disasters

## Why This Works (Mechanism)

### Mechanism 1: Clustering Reduces Action Space Complexity
By clustering similar forecasting models, FoodRL reduces the action space and enables the RL agent to adaptively assign weights based on recent performance and contextual information. The framework clusters 252-294 base models into 40-60 groups, with the RL agent assigning weights to clusters rather than individual models, and final predictions computed as weighted sums of cluster-averaged outputs. Core assumption: models within the same cluster produce sufficiently correlated predictions that averaging them preserves predictive signal while enabling meaningful weight differentiation across clusters.

### Mechanism 2: Sequential Decision-Making Formulation
Formulating ensemble weighting as a sequential decision-making problem enables the system to adapt weights based on evolving temporal patterns and past performance. The PPO-based RL agent observes a state vector (TSFEL-extracted temporal features, cluster predictions, previous weights) and outputs normalized weight distributions, with the reward signal being negative MAPE. Core assumption: the mapping from state features to optimal weight assignments is learnable from limited historical samples and generalizes to future time steps, including during concept drift.

### Mechanism 3: Adaptive Response to Concept Drift
Different concept drift conditions favor different model behaviors, and a dynamic ensemble can adaptively weight clusters based on detected drift characteristics. The framework identifies five drift categories via K-means clustering of donation patterns (extreme/moderate increases, extreme/moderate declines, slight trends), with the RL agent's state features implicitly encoding drift-relevant signals. Core assumption: temporal and statistical features extracted from historical windows contain sufficient signal to distinguish drift types before or during their occurrence, enabling responsive weight reallocation.

## Foundational Learning

**Concept: Concept Drift in Time Series**
- Why needed here: Food donation data is non-stationary—distribution shifts due to seasonality, disasters, and economic changes. Static models trained on past data degrade as these shifts occur.
- Quick check question: A model trained on 2015-2019 donation data performs well initially but degrades in 2020. Name two factors that could cause this degradation even if the model architecture is unchanged.

**Concept: Ensemble Weighting vs. Model Selection**
- Why needed here: The paper explicitly contrasts weighting (FoodRL's approach) with selection (choosing one "best" model). Weighting hedges across model uncertainties; selection commits to one model's biases.
- Quick check question: With 200 candidate models, selecting the top performer at each time step might yield worse long-term performance than averaging cluster predictions. Why?

**Concept: Action Space Complexity in RL**
- Why needed here: The RL agent's action space directly affects learning difficulty. With 252+ individual model weights, the space is high-dimensional and sparse. Clustering reduces this to 40-60 discrete actions, improving sample efficiency.
- Quick check question: If you cluster 200 models into 10 groups, what type of information is potentially lost, and what is gained in terms of RL learning dynamics?

## Architecture Onboarding

**Component Map:**
Historical data → Base models generate predictions → Predictions clustered → RL agent observes state (features + cluster predictions + previous weights) → Agent outputs weight vector → Weighted ensemble prediction → Reward computed (negative MAPE) → Policy updated via PPO

**Critical Path:**
Historical data → Base models generate predictions → Predictions clustered → RL agent observes state (features + cluster predictions + previous weights) → Agent outputs weight vector → Weighted ensemble prediction → Reward computed (negative MAPE) → Policy updated via PPO

**Design Tradeoffs:**
- **Cluster count (k)**: Fewer clusters simplify learning but risk losing model diversity. Paper uses 40-60 based on hyperparameter tuning. Assumption: diminishing returns beyond this range.
- **RL algorithm selection**: PPO chosen over TD3, SAC, DDPG due to stability in small-sample regimes. Clipped surrogate objective constrains large policy updates, reducing variance.
- **Reward formulation**: MAPE tested alongside MAE, SMAPE with minimal difference. MAPE is scale-invariant but sensitive to small actual values—a potential issue for low-volume periods.
- **Feature window sizes**: Multiple rolling windows capture both short-term fluctuations and long-range patterns. Paper skipped manual feature selection as more features improved performance.

**Failure Signatures:**
- **Extreme increase MAPE > 40%**: All models (including FoodRL) struggle with disaster-driven donation spikes. This indicates the feature set lacks anticipatory signals for sudden positive shocks.
- **Weight instability**: If consecutive time steps show wildly different weight assignments, the policy may be undertrained or the state representation insufficient.
- **No improvement over Simple Averaging**: Suggests clustering over-smoothed useful model differences, or the RL agent failed to learn meaningful policies given limited data.

**First 3 Experiments:**
1. **Baseline comparison**: Run Simple Averaging, GA, unclustered RL, and FoodRL on identical train/test splits. Verify FoodRL achieves statistically significant MAPE reductions (Wilcoxon signed-rank test, p < 0.05) and identify which drift categories show largest gains.
2. **Cluster count ablation**: Test k ∈ {20, 40, 60, 80} and plot MAPE vs. k. Hypothesis: performance degrades at extremes—too few clusters lose diversity; too many increase action space without data support.
3. **Per-drift performance profiling**: Segment test data by the five drift categories. Compute MAPE per category for all methods. Confirm FoodRL excels on declines/trends while identifying whether extreme increases remain an unsolved challenge.

## Open Questions the Paper Calls Out

**Open Question 1**: Can the integration of external event data or specialized reward functions significantly reduce the high error rates observed during "extreme increase" scenarios?
- Basis in paper: The authors state, "For future work, we aim to enhance performance during extreme increase scenarios," while noting that current results show high errors (MAPE > 40%) during these periods.
- Why unresolved: The current framework, while robust for declines and slight trends, struggles to predict the sharp spikes associated with events like hurricanes, as shown in the drift cluster analysis.

**Open Question 2**: Which specific categories of granular data (e.g., real-time weather, specific donor information) most effectively refine the RL agent's ensemble weighting policy?
- Basis in paper: The authors write, "We also plan to incorporate more granular data to further refine the model."
- Why unresolved: The current study relies primarily on TSFEL statistical features and lagged observations; the marginal utility of domain-specific external variables remains untested.

**Open Question 3**: What architectural adaptations are required to improve FoodRL's performance consistency at the weekly forecasting level?
- Basis in paper: The authors explicitly list a goal to "...improve forecasting accuracy at the weekly level" in the future work section.
- Why unresolved: The paper highlights that the "extreme increase" cluster is harder to predict at the weekly level, and the current feature engineering may not be fully optimized for weekly volatility.

## Limitations
- **Data accessibility**: The constituent predictions dataset from Sharma et al. (2021) is not directly available, requiring reconstruction or special request, which limits reproducibility and validation of FoodRL's improvements.
- **Concept drift detection methodology**: While the paper identifies five drift categories, the criteria for categorizing drifts and the temporal stability of these categories across test years are not fully specified.
- **Extreme event handling**: FoodRL does not consistently outperform baselines during extreme donation increases (e.g., disaster-driven spikes). The feature set may lack anticipatory signals for such shocks.

## Confidence
- **High Confidence**: FoodRL outperforms all baselines (SA, GA, standard RL) on overall MAPE for both EFB and WFB, with statistically significant differences.
- **Medium Confidence**: FoodRL excels during moderate/extreme declines and slight trends; struggles with extreme increases.
- **Medium Confidence**: Clustering models reduces action space complexity and improves RL learning efficiency.

## Next Checks
1. **Drift category robustness**: Segment test data by drift type and compute per-category MAPE for all methods. Confirm whether FoodRL's gains persist across different concept drift regimes.
2. **Cluster count sensitivity**: Perform ablation study varying k ∈ {20, 40, 60, 80} and plot MAPE vs. k to identify optimal cluster granularity and validate the claim about action space reduction.
3. **Feature set necessity**: Run ablation tests removing TSFEL features or using only raw predictions to determine whether the feature extraction step provides measurable performance gains.