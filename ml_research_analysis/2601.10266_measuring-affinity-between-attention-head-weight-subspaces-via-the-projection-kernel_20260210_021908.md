---
ver: rpa2
title: Measuring Affinity between Attention-Head Weight Subspaces via the Projection
  Kernel
arxiv_id: '2601.10266'
source_url: https://arxiv.org/abs/2601.10266
tags:
- heads
- head
- layer
- scores
- weight
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of accurately measuring relationships
  between attention heads in Transformers, where existing metrics like the Composition
  Score (CS) are limited by normalization effects and scaling biases. The authors
  propose the Projection Kernel (PK), a principal-angle-based measure of subspace
  similarity, to quantify the overlap between attention-head weight subspaces.
---

# Measuring Affinity between Attention-Head Weight Subspaces via the Projection Kernel

## Quick Facts
- **arXiv ID:** 2601.10266
- **Source URL:** https://arxiv.org/abs/2601.10266
- **Authors:** Hiroaki Yamagiwa; Yusuke Takase; Hidetoshi Shimodaira
- **Reference count:** 40
- **Primary result:** Introduces Projection Kernel (PK), a principal-angle-based metric, to quantify attention-head weight subspace overlap and demonstrates its superiority over Composition Score (CS) for detecting head relationships in GPT2-small.

## Executive Summary
This paper addresses the challenge of accurately measuring relationships between attention heads in Transformer models. The authors propose the Projection Kernel (PK) as an alternative to the Composition Score (CS) metric, arguing that PK better captures the geometric overlap between attention-head weight subspaces. PK is derived from orthonormal bases of weight matrices and measures principal angles between subspaces, providing a more robust and interpretable measure of head affinity that is less affected by normalization effects and scaling biases that plague CS.

## Method Summary
The Projection Kernel quantifies the similarity between attention-head weight subspaces by measuring the squared Frobenius norm of the projection between orthonormal basis matrices. For weight matrices U and U', PK is computed as ||U^T U'||_F² = tr(PP') where P is the projection matrix. The authors apply PK to all pairwise combinations of Query (OQ), Key (OK), and Value (OV) weight matrices across all heads in GPT2-small, generating affinity scores that are visualized in wiring diagrams and used for head classification tasks. The method is compared against CS using IOI task head class annotations, and PK's informativeness is evaluated against random orthogonal matrix baselines.

## Key Results
- PK more clearly captures known head-to-head interactions in wiring diagrams compared to CS, with PK detecting 57/60 edges versus CS's 18/60 when compared to original weights
- PK identifies L4H7 as an Identity Head hub with high inlet/outlet scores, consistent with known functional roles
- PK outperforms CS in head class detection and classification tasks, showing improved PR-AUC and ROC-AUC metrics
- PK demonstrates robustness to weight preprocessing (LN folding, centering), while CS values change substantially under the same operations

## Why This Works (Mechanism)
PK works by directly measuring the geometric overlap between weight subspaces through principal angles, which captures the actual dimensional alignment of the spaces rather than their projections onto a shared basis. Unlike CS, which can be distorted by normalization and scaling effects, PK measures the intrinsic geometric relationship between subspaces using orthonormal bases derived from SVD or QR decomposition. This approach provides a more stable and interpretable measure of subspace similarity that better reflects functional relationships between attention heads.

## Foundational Learning

**Principal Angles**: Measure the minimal angles between two subspaces in high-dimensional space. Needed because simple vector similarity doesn't capture the geometric relationship between entire subspaces. Quick check: For two 1D subspaces, principal angles reduce to the angle between the two vectors.

**Orthonormal Basis**: A set of mutually orthogonal unit vectors that span a subspace. Required for PK computation to ensure the basis matrices represent the subspace structure without redundancy. Quick check: Verify U^T U = I for each basis matrix.

**Frobenius Norm**: The matrix analog of the Euclidean norm, computed as the square root of the sum of squared elements. Used in PK to measure the magnitude of projection between basis matrices. Quick check: ||A||_F = sqrt(sum of all a_ij^2).

**Subspace Projection**: The operation of mapping vectors from one subspace onto another. Forms the core of PK by measuring how much one subspace overlaps with another. Quick check: P = UU^T is the projection matrix for basis U.

## Architecture Onboarding

**Component Map**: GPT2-small (12 layers × 12 heads) → Weight matrices (W_Q, W_K, W_V, W_O) → Orthonormal bases (U_Q, U_K, U_V, U_O) → PK scores → Wiring diagrams / Classification metrics

**Critical Path**: Load pretrained weights → Apply weight preprocessing (LN folding, centering) → Compute orthonormal bases via SVD/QR → Calculate PK for all head pairs → Generate wiring diagrams → Evaluate against IOI annotations

**Design Tradeoffs**: PK provides more robust geometric measurement but requires orthonormalization computation; CS is simpler but sensitive to scaling and normalization. The choice of d_head=64 balances computational tractability with sufficient representational capacity.

**Failure Signatures**: High PK scores between unrelated head classes may indicate spurious correlations; low PK scores between functionally related heads suggest the metric misses important relationships. Inconsistent results across preprocessing steps indicate metric instability.

**Three First Experiments**:
1. Verify orthonormalization by checking U^T U = I for all basis matrices and ensuring PK values lie in [0, 64]
2. Compare PK wiring diagrams before and after weight preprocessing to confirm robustness (expect ~95% edge overlap)
3. Test PK on synthetic orthogonal matrices to validate against the random baseline distribution

## Open Questions the Paper Calls Out

**Open Question 1**: Does the Projection Kernel robustly capture head interactions in larger models that utilize relative positional encodings (e.g., RoPE)? The authors note that GPT2-small uses absolute positional embeddings, whereas many recent models use RoPE which applies position-dependent rotations, and they "do not evaluate whether PK works well for models that use RoPE."

**Open Question 2**: Is the hub behavior observed in L4H7 a general property of Identity Heads or an idiosyncratic feature of GPT2-small? The discussion notes that while Identity Heads are known, they are under-studied, and explicitly states: "A more detailed analysis of Identity Heads, including whether L4H7 is special, is left for future work."

**Open Question 3**: How can weight-based subspace overlap be correlated with specific token-level functional behaviors? The authors note that PK "cannot capture token-level relationships" and misses specific behaviors known in the IOI task (e.g., specific token query effects).

## Limitations

- PK measures static weight geometry and may miss dynamic token-level interactions that occur during inference
- The metric's effectiveness for models using relative positional encodings like RoPE has not been validated
- While PK shows improvements over CS, the absolute performance gains in head classification tasks are modest (e.g., 60% vs 55% detection rates)

## Confidence

**High confidence**: PK's mathematical derivation as a principal-angle-based similarity measure and its implementation using orthonormal bases are sound and correctly computed

**Medium confidence**: PK's superior informativeness for head detection and classification tasks compared to CS, as demonstrated through PR-AUC and ROC-AUC metrics

**Medium confidence**: PK's ability to identify known functional relationships (L4H7 as Identity Head hub) based on wiring diagrams and token projection experiments

## Next Checks

1. Conduct ablation studies on the orthonormalization method by comparing PK results using QR vs SVD decomposition to quantify numerical stability effects
2. Test PK's robustness beyond GPT2-small by applying it to other Transformer architectures (e.g., BERT, T5) and different model scales
3. Develop a controlled synthetic benchmark where ground-truth head relationships are known to validate whether PK scores correlate with actual functional dependencies rather than just statistical correlations