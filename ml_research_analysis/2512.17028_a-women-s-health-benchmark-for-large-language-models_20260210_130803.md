---
ver: rpa2
title: A Women's Health Benchmark for Large Language Models
arxiv_id: '2512.17028'
source_url: https://arxiv.org/abs/2512.17028
tags:
- women
- health
- error
- query
- failure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Women's Health Benchmark (WHB), the first
  evaluation framework specifically designed to assess large language models (LLMs)
  on women's health topics. The benchmark comprises 96 rigorously validated model
  stumps covering five medical specialties, three query types, and eight error categories.
---

# A Women's Health Benchmark for Large Language Models

## Quick Facts
- arXiv ID: 2512.17028
- Source URL: https://arxiv.org/abs/2512.17028
- Authors: Victoria-Elisabeth Gruber; Razvan Marinescu; Diego Fajardo; Amin H. Nassar; Christopher Arkfeld; Alexandria Ludlow; Shama Patel; Mehrnoosh Samaei; Valerie Klug; Anna Huber; Marcel Gühner; Albert Botta i Orfila; Irene Lagoja; Kimya Tarr; Haleigh Larson; Mary Beth Howard
- Reference count: 26
- All 13 evaluated LLMs show ~60% failure rates on women's health queries, with all models struggling to identify urgent medical situations

## Executive Summary
This paper introduces the Women's Health Benchmark (WHB), the first evaluation framework specifically designed to assess large language models (LLMs) on women's health topics. The benchmark comprises 96 rigorously validated model stumps covering five medical specialties, three query types, and eight error categories. Across 13 state-of-the-art LLMs, the study reveals an overall failure rate of approximately 60%, with performance varying significantly by specialty and error type. Notably, all models struggle with identifying urgent medical situations, while newer models like GPT-5 show improved performance in avoiding inappropriate recommendations. The findings underscore that current AI chatbots are not yet reliable for women's health advice, highlighting the critical need for specialty-specific validation and further development in this domain.

## Method Summary
The WHB uses 96 model stumps—prompts that generate incorrect model answers—created by 17 medical experts who generated 345 prompts over six weeks. Each prompt was randomly assigned to one of 13 LLMs, and 96 incorrect responses were selected as benchmark cases. A single PhD evaluator assessed whether model responses contained the specific errors identified by experts. The benchmark covers five specialties (OB/GYN, emergency medicine, primary care, oncology, neurology), three query types (symptom recognition, treatment advice, health information), and eight error categories. All 13 LLMs were evaluated on all 96 stumps, with failure rates calculated by specialty, query type, and error type.

## Key Results
- Overall failure rate of approximately 60% across all 13 evaluated LLMs
- All models universally struggle with identifying urgent medical situations (missed urgency)
- Performance varies by specialty: OB/GYN shows 56.7% failure rate while emergency medicine shows 66.7%
- Newer models like GPT-5 show improved performance in avoiding inappropriate recommendations compared to older models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Underrepresentation of women in historical clinical research propagates through LLM training data, causing elevated failure rates on women's health queries.
- Mechanism: Training corpora derived from medical literature reflect decades of exclusion of pregnant women and women of child-bearing age from clinical studies, plus sex-biased diagnostic protocols. LLMs trained on this data lack sufficient signal for women-specific presentations.
- Core assumption: Training data composition is the primary driver of observed performance gaps rather than model architecture limitations.
- Evidence anchors: [abstract] "current models show approximately 60% failure rates on the women's health benchmark"; [section] "women have been underrepresented in research and clinical trials... excluding pregnant women and women of child-bearing age from participating in clinical studies for decades"; [corpus] Related work on "Reducing Large Language Model Safety Risks in Women's Health" similarly identifies hallucination risks in obstetrics contexts (arXiv:2503.00269)
- Break condition: If training corpora were enriched with sex-stratified clinical evidence and women-specific guidelines, failure rates should decrease differentially for obstetrics/gynecology vs. general queries.

### Mechanism 2
- Claim: Models rely on shortcut learning and spurious correlations rather than clinical reasoning, producing confident but incorrect responses.
- Mechanism: Imbalanced or noisy medical training datasets contain surface patterns that correlate with answers without capturing true causal relationships. Models exploit these shortcuts, especially for underrepresented populations.
- Core assumption: The observed errors stem from learned heuristics rather than random generation failures.
- Evidence anchors: [abstract] "all models struggle with identifying urgent medical situations"; [section] "models may rely on simplified patterns or stereotypes instead of true medical reasoning, leading to confident but incorrect responses"; [corpus] Weak direct corpus evidence for this specific mechanism; related work on bias detection (B-score, arXiv:2505.18545) addresses bias patterns but not shortcut learning directly
- Break condition: If models were tested on counterfactual cases that break spurious correlations, performance should degrade selectively on shortcut-dependent queries.

### Mechanism 3
- Claim: Newer, larger models show selective improvement on "inappropriate recommendations" while "missed urgency" remains a universal weakness.
- Mechanism: Scale and alignment training improve avoidance of clearly harmful outputs, but recognizing time-critical escalation requires integration of clinical triage reasoning that current training does not emphasize.
- Core assumption: Urgency recognition requires different capabilities than recommendation appropriateness.
- Evidence anchors: [abstract] "all models struggle with identifying urgent medical situations, while newer models like GPT-5 show improved performance in avoiding inappropriate recommendations"; [section] "missed urgency is a universal weakness in all models, indicating the importance of human oversight for time-sensitive cases"; [corpus] Related work on reliable abstention in RAG systems for women's health (arXiv:2509.04482) addresses when models should decline to answer, relevant to urgency triage
- Break condition: If urgency-specific training were added (e.g., triage classification objectives), failure rates on "missed urgency" should decrease independently of overall performance.

## Foundational Learning

- Concept: **Model stumps as benchmark units**
  - Why needed here: The WHB uses "model stumps" (prompts that produce incorrect model answers) rather than correct-answer benchmarks, requiring understanding that evaluation targets failure modes specifically.
  - Quick check question: Can you explain why a benchmark built from model failures tests different capabilities than one built from correct responses?

- Concept: **Sex vs. gender in clinical data**
  - Why needed here: The paper distinguishes biological sex differences (chromosomal, hormonal) from gender effects (social roles, behaviors), both affecting how conditions present and are documented in training data.
  - Quick check question: For a symptom presentation dataset, what would be two distinct sources of sex-based vs. gender-based bias?

- Concept: **Specialty-specific validation**
  - Why needed here: Failure rates vary dramatically by specialty (neurology 76.9% vs. OB/GYN 56.7%), meaning aggregate benchmarks mask critical gaps.
  - Quick check question: Why would evaluating on overall medical QA performance be insufficient for deploying in women's health contexts?

## Architecture Onboarding

- Component map:
  - **Dataset**: 96 model stumps organized by 5 specialties × 3 query types × 8 error categories
  - **Expert cohort**: 17 validated clinicians, pharmacists, researchers (vetted via publications/experience)
  - **Evaluation loop**: Expert prompts → Random model assignment → Expert rejection with justification → Quality review → Benchmark inclusion
  - **Scoring**: Single PhD evaluator assesses whether model output contains the expert-identified error

- Critical path: Expert prompt creation → Random model response → Expert rejection with justification → Internal quality review → Benchmark stump → All-model evaluation → Failure rate calculation by specialty/error/query type

- Design tradeoffs:
  - **Stump-based vs. gold-standard**: Benchmark captures failure modes but may miss errors not caught by initial random model assignment
  - **Single evaluator**: Efficiency gained at cost of potential subjectivity; authors note this limitation
  - **Small sample sizes**: Neurology (n=3 stumps) and oncology (n=11) have limited statistical power

- Failure signatures:
  - **Missed urgency (9.4% of stumps but universal failure)**: All models fail to escalate time-sensitive cases
  - **Incorrect treatment advice (76.3% failure rate)**: Most challenging error category
  - **Inappropriate recommendations (variable)**: Large gap between best (GPT-5) and worst (GPT-4o-mini, Mistral-large) models
  - **Specialty skew**: Neurology and oncology show highest failure rates but smallest sample sizes

- First 3 experiments:
  1. **Baseline replication**: Run WHB stumps against target model, calculate failure rates by specialty, error type, and query type; compare against published GPT-5 baseline (53.1% approval)
  2. **Urgency-specific probing**: Isolate the 9 "missed urgency" stumps and test with explicit triage-framing prompts to measure sensitivity to escalation cues
  3. **Error stratification analysis**: For each error category, measure whether failures cluster in specific specialties (e.g., are dosage errors concentrated in oncology vs. primary care?)

## Open Questions the Paper Calls Out

- **Question**: Why do all LLMs universally struggle with "missed urgency" indicators, and what architectural or training interventions could address this critical safety failure?
  - Basis in paper: [explicit] The authors state: "all models struggle with identifying urgent medical situations" and "missed urgency is a universal weakness in all models, indicating the importance of human oversight for time-sensitive cases."
  - Why unresolved: The paper identifies the problem but does not investigate its causes or propose solutions; it only notes that even the best-performing models fail here.
  - What evidence would resolve it: Ablation studies comparing urgency detection across different training regimes, or fine-tuning experiments targeting emergency triage tasks with before/after benchmark comparisons.

- **Question**: How does LLM performance on women's health queries change in multi-turn conversational contexts compared to the single-turn model stumps evaluated in this study?
  - Basis in paper: [explicit] The Outlook section states: "Another emphasis will be the development of a multi-turn benchmark to evaluate the performance of the models on longer conversations."
  - Why unresolved: The current benchmark only evaluates single-turn interactions, but real-world health consultations typically involve follow-up questions and clarification.
  - What evidence would resolve it: A longitudinal evaluation using the same medical scenarios extended to 3-5 turn conversations, with performance comparison to single-turn results.

- **Question**: Would expanding specialty coverage to surgery, cardiology, and dermatology reveal similar or different failure patterns compared to the five specialties currently tested?
  - Basis in paper: [explicit] The Outlook section states: "Future work will extend our benchmark to include more medical specialties such as surgery, cardiology, and dermatology."
  - Why unresolved: Current results show dramatic specialty variation (neurology at 76.9% failure vs. obstetrics at 56.7%), but the pattern may not generalize.
  - What evidence would resolve it: Additional model stumps in the proposed specialties evaluated on the same 13 LLMs, with error type distribution analysis.

## Limitations

- **Single-evaluator scoring introduces unknown subjectivity**: All model responses were assessed by one PhD-level evaluator, but inter-rater reliability was not measured, creating uncertainty about consistency across error categories and specialties.

- **Small sample sizes for key specialties**: Neurology (n=3 model stumps, 39 total cases) and oncology (n=11 stumps) have insufficient statistical power for subgroup analysis, making performance differences within these categories potentially spurious.

- **Unknown prompting protocol details**: While 13 models were evaluated, exact system prompts, temperature settings, and API versions are not specified, creating potential reproducibility gaps between original results and new evaluations.

## Confidence

- **High confidence**: Overall 60% failure rate finding, based on consistent results across all 13 models and the large sample size (96 model stumps × 13 models = 1,248 total evaluations).
- **Medium confidence**: Specialty-specific performance differences (OB/GYN 56.7% vs. emergency medicine 66.7% vs. primary care 56.3%), though these estimates have overlapping confidence intervals and the neurology sample is too small for reliable inference.
- **Low confidence**: Exact error category performance without CIs, as the paper doesn't provide statistical measures for individual error types (though "missed urgency" being universal across all models is a robust pattern).

## Next Checks

1. **Inter-rater reliability assessment**: Have 2-3 additional qualified evaluators independently score a random subset (e.g., 20-30 model responses) to quantify agreement rates and identify error categories with highest disagreement.

2. **Urgency-specific prompting experiment**: Test whether explicit triage-framing prompts ("Is this a medical emergency?") improve performance on the 9 "missed urgency" stumps, isolating whether the failure stems from prompt design vs. model capability.

3. **Counterfactual correlation testing**: Design a small set of counterfactual cases that break known spurious correlations (e.g., symptoms that typically correlate with certain diagnoses but shouldn't in specific contexts) to empirically test the shortcut learning hypothesis.