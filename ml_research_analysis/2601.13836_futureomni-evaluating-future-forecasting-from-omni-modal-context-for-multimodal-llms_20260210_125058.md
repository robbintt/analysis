---
ver: rpa2
title: 'FutureOmni: Evaluating Future Forecasting from Omni-Modal Context for Multimodal
  LLMs'
arxiv_id: '2601.13836'
source_url: https://arxiv.org/abs/2601.13836
tags:
- video
- audio
- event
- modality
- future
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FutureOmni, the first benchmark designed
  to evaluate multimodal large language models' ability to forecast future events
  from joint audio-visual inputs. Unlike existing benchmarks that focus on retrospective
  understanding, FutureOmni contains 919 videos and 1,034 multiple-choice QA pairs
  across 8 domains, requiring models to perform cross-modal causal and temporal reasoning.
---

# FutureOmni: Evaluating Future Forecasting from Omni-Modal Context for Multimodal LLMs

## Quick Facts
- arXiv ID: 2601.13836
- Source URL: https://arxiv.org/abs/2601.13836
- Reference count: 40
- Current models struggle with audio-visual future prediction, best accuracy reaching only 64.8% (Gemini 3 Flash)

## Executive Summary
This paper introduces FutureOmni, the first benchmark designed to evaluate multimodal large language models' ability to forecast future events from joint audio-visual inputs. Unlike existing benchmarks that focus on retrospective understanding, FutureOmni contains 919 videos and 1,034 multiple-choice QA pairs across 8 domains, requiring models to perform cross-modal causal and temporal reasoning. Evaluation of 20 models, including both omni-modal and video-only systems, reveals that current models struggle with audio-visual future prediction, with the best accuracy reaching only 64.8% (Gemini 3 Flash). To address this limitation, the authors curate a 7K-sample instruction-tuning dataset and propose an Omni-Modal Future Forecasting (OFF) training strategy. Experiments demonstrate that OFF substantially improves future forecasting performance and generalization across audio-visual and video-only benchmarks.

## Method Summary
FutureOmni is a benchmark for evaluating multimodal LLMs' ability to forecast future events from joint audio-visual inputs. It contains 919 videos and 1,034 multiple-choice QA pairs across 8 domains, requiring models to perform cross-modal causal and temporal reasoning. The authors curate a 7K-sample instruction-tuning dataset with rationales and propose an Omni-Modal Future Forecasting (OFF) training strategy using LoRA fine-tuning (rank 64, lr=1e-5, 1 epoch) with frozen visual/audio encoders. The method trains on the FutureOmni-7K dataset with Premise -> Rationale -> Target format, then evaluates on the FutureOmni benchmark and generalization benchmarks (WorldSense, DailyOmni, Video-MME).

## Key Results
- Current models struggle with audio-visual future prediction, with the best accuracy reaching only 64.8% (Gemini 3 Flash)
- OFF training strategy substantially improves future forecasting performance across both audio-visual and video-only benchmarks
- Visual perception errors (51.6%) are identified as the primary bottleneck, significantly outweighing reasoning failures (30.8%)

## Why This Works (Mechanism)

### Mechanism 1: Adversarial Distractor Design
The benchmark constructs multiple-choice options where visually plausible answers are explicitly contradicted by audio (and vice versa), or where temporal order is inverted (Reverse-Causal). To select the correct future event, the model must verify the premise against both the visual scene and the audio track simultaneously, forcing genuine cross-modal integration rather than relying on statistical shortcuts from a single dominant modality.

### Mechanism 2: Rationale-Enhanced Training
The OFF strategy fine-tunes models on a 7K dataset where samples include not just the Q&A, but the explicit rationale linking the past context to the future event. This explicitly teaches the model the "why" behind the prediction, distilling the causal reasoning process (Premise → Rationale → Future) rather than just memorizing event correlations.

### Mechanism 3: Attention Mechanism Optimization
The OFF training objective implicitly optimizes the model to identify high-information-density moments (visual cues or audio triggers) that are causal precursors to the future event. This shifts internal attention mechanisms to prioritize informative "keyframes" in both video and audio streams, creating a sparse, discriminative attention pattern.

## Foundational Learning

- **Audio-Visual Symbolic Alignment**: Understanding future events often requires decoding symbolic audio (language/warnings) and aligning it with visual context. Quick check: Can the model distinguish between a character saying "Be careful" vs. "I'm fine" while viewing a risky visual action, and weight the audio warning heavily in its prediction?

- **Temporal Causality vs. Correlation**: FutureOmni distinguishes between mere succession (Event A then Event B) and logical causality (Event A causes Event B). Quick check: If a video shows a door opening and then a bell ringing, does the model predict the bell ringing because of the door, or just as a subsequent event?

- **The "Cold Start" Context Problem**: The paper finds models fail on short videos because they lack sufficient historical context to establish a trajectory. Quick check: How does the model handle prediction when the input video is <30 seconds long? Does it default to generic priors?

## Architecture Onboarding

- **Component map**: Pre-trained Vision Encoder (e.g., SigLIP/CLIP) + Audio Encoder (e.g., BEATs/Whisper) → Projector → LLM Backbone (e.g., Qwen2.5, Llama-3) → OFF Adapter (LoRA modules)

- **Critical path**: 1. Data Filtering: Use "Audio Coordinated" filter to ensure training videos actually require audio. 2. Training: Apply LoRA (rank 64) to LLM backbone while keeping encoders frozen. 3. Train on FutureOmni-7K with Premise -> Rationale -> Target format.

- **Design tradeoffs**: LoRA vs. Full Fine-Tuning (LoRA for efficiency but may not fully adapt projection layers), Omni vs. Video-Only (omni-models necessary for causal forecasting but higher computational cost).

- **Failure signatures**: Visual Perception Bottleneck (51.6% of errors from hallucinated objects/missed actions), Speech-Heavy Collapse (performance drops ~10% on Speech vs. Music), Short-Video Floundering (low accuracy on videos <2 minutes).

- **First 3 experiments**: 1. Modality Ablation: Run baseline model on FutureOmni using Audio+Video, Video-only, and Video+Subtitle. 2. OFF Effectiveness: Train Qwen2.5-Omni-7B using OFF vs. baseline instruction tune, compare accuracy on Speech subset. 3. Generalization Test: Evaluate OFF-tuned model on WorldSense or Video-MME to verify positive transfer.

## Open Questions the Paper Calls Out

- How can the "Contextual Cold Start" phenomenon in short-duration forecasting be mitigated without relying on longer input context? The paper notes all models struggle most with the shortest duration interval ([0,2) min), lacking "necessary narrative buildup."

- What distinct architectural improvements are required to bridge the persistent performance gap between speech-based and music/sound-based forecasting? Speech is consistently the most challenging modality, with a ~10% gap compared to music for Qwen3-Omni.

- To what extent does visual perception accuracy limit future forecasting performance relative to causal reasoning capabilities? Error analysis identifies visual perception errors (51.6%) as the primary bottleneck, significantly outweighing reasoning failures (30.8%).

## Limitations

- Synthetic future event bias: The adversarial distractor design may create artificial scenarios that don't reflect natural audio-visual forecasting, potentially limiting real-world applicability
- Small training set: 7K instruction-tuning samples may not provide sufficient diversity for robust generalization across all audio-visual scenarios
- Teacher model quality dependency: OFF performance is contingent on the DeepSeek-V3.2 rationales being logically sound and free from hallucinations

## Confidence

- High confidence: The benchmark design methodology (adversarial distractors, cross-modal validation) is well-justified and the evaluation results showing current model limitations are reliable
- Medium confidence: The OFF training strategy's effectiveness is supported by experiments, but generalization to real-world scenarios needs further validation
- Low confidence: The claim that models are "genuinely" performing cross-modal integration rather than exploiting statistical patterns in the dataset requires more rigorous testing

## Next Checks

1. **Real-world generalization test**: Evaluate the OFF-trained model on unedited, naturally occurring videos (e.g., surveillance footage, YouTube vlogs) to verify that improvements transfer beyond the curated benchmark

2. **Temporal robustness validation**: Test model performance across a spectrum of video lengths (0.5s to 5min) to confirm that short-video performance improvements aren't artifacts of dataset bias

3. **Cross-dataset reasoning transfer**: Fine-tune on FutureOmni-7K then evaluate on WorldSense without further adaptation to verify whether future forecasting training enhances general audio-visual reasoning abilities