---
ver: rpa2
title: Coarse-to-Fine Open-Set Graph Node Classification with Large Language Models
arxiv_id: '2512.16244'
source_url: https://arxiv.org/abs/2512.16244
tags:
- classification
- samples
- detection
- space
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the challenge of open-set graph node classification,
  aiming to classify in-distribution (ID) samples while detecting and classifying
  out-of-distribution (OOD) samples without true OOD labels. The proposed Coarse-to-Fine
  Classification (CFC) framework leverages large language models (LLMs) in two stages:
  a coarse classifier uses LLM prompts for OOD detection and label generation, and
  a GNN-based fine classifier refines detection and classification using semantic
  OOD samples.'
---

# Coarse-to-Fine Open-Set Graph Node Classification with Large Language Models
## Quick Facts
- arXiv ID: 2512.16244
- Source URL: https://arxiv.org/abs/2512.16244
- Reference count: 25
- Up to 70% accuracy in OOD classification; 10% improvement in OOD detection over state-of-the-art

## Executive Summary
This paper introduces a Coarse-to-Fine Classification (CFC) framework for open-set graph node classification, leveraging large language models (LLMs) to detect and classify out-of-distribution (OOD) samples without requiring true OOD labels. Unlike prior methods that rely on synthetic OOD data, CFC uses semantically meaningful OOD samples generated via LLM prompts, enhancing interpretability and accuracy. The framework operates in two stages: a coarse classifier for initial OOD detection and label generation, followed by a GNN-based fine classifier that refines detection and classification. Experiments demonstrate up to 70% accuracy in OOD classification and a 10% improvement in OOD detection over state-of-the-art methods on graph and text datasets, with validation on non-textual graphs and text classification tasks.

## Method Summary
The CFC framework addresses open-set graph node classification by employing LLMs in a two-stage process. In the coarse stage, LLM prompts are used to generate semantically meaningful OOD samples and perform initial OOD detection and label generation. The fine stage involves a GNN-based classifier that refines the detection and classification using these OOD samples. This approach contrasts with prior methods that depend on synthetic OOD data, offering improved interpretability and accuracy. The framework is evaluated on graph and text datasets, showing significant performance gains in both OOD detection and classification.

## Key Results
- Achieves up to 70% accuracy in OOD classification
- Improves OOD detection by 10% over state-of-the-art methods
- Validated on graph and text datasets, including non-textual graphs and text classification tasks

## Why This Works (Mechanism)
The CFC framework leverages the semantic understanding capabilities of LLMs to generate meaningful OOD samples, which are then used to train a GNN-based fine classifier. This approach addresses the limitations of synthetic OOD data by providing more interpretable and contextually relevant samples. The coarse-to-fine structure allows for efficient initial filtering and refinement, improving overall classification accuracy and robustness in open-set scenarios.

## Foundational Learning
- Large Language Models (LLMs): Advanced models for generating semantically meaningful OOD samples.
  - Why needed: To create interpretable and contextually relevant OOD samples for training.
  - Quick check: Verify LLM-generated samples align with true OOD characteristics.
- Graph Neural Networks (GNNs): Deep learning models for graph-structured data.
  - Why needed: To capture complex relationships in graph node classification tasks.
  - Quick check: Ensure GNN architecture is suitable for the dataset's graph structure.
- Open-Set Classification: Classification task where test data may include unseen classes.
  - Why needed: To handle real-world scenarios where OOD samples are common.
  - Quick check: Validate framework's ability to detect and classify OOD samples accurately.

## Architecture Onboarding
- Component Map: LLM prompts -> Coarse classifier (OOD detection/label generation) -> GNN-based fine classifier -> Refined classification
- Critical Path: LLM-generated OOD samples -> Coarse classification -> Fine classification refinement
- Design Tradeoffs: Semantic meaningfulness vs. computational overhead of LLM usage
- Failure Signatures: Poor LLM-generated OOD samples may lead to inaccurate coarse classification; GNN overfitting on limited OOD samples
- First Experiments: 1) Test LLM prompt effectiveness on sample generation; 2) Evaluate coarse classifier accuracy on synthetic vs. real OOD samples; 3) Assess GNN fine classifier performance with varying OOD sample sizes

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on semantically meaningful OOD samples may limit practical applicability if such samples are unavailable or unrepresentative.
- Performance evaluated primarily on graph and text datasets, raising generalizability concerns for other data types.
- Potential biases in LLM-generated OOD samples and computational overhead are not thoroughly addressed.

## Confidence
- OOD classification accuracy (70%): Medium
- OOD detection improvement (10%): Medium
- Generalizability across datasets: Low

## Next Checks
1. Evaluate CFC's performance on diverse, real-world datasets across multiple domains to assess generalizability and robustness.
2. Conduct ablation studies to quantify the impact of LLM-generated OOD samples on overall framework performance and investigate potential biases.
3. Benchmark CFC against a wider range of state-of-the-art open-set classification methods, including those not specifically designed for graph data, to establish its relative performance.