---
ver: rpa2
title: Using Large Language Models to Assess Teachers' Pedagogical Content Knowledge
arxiv_id: '2505.19266'
source_url: https://arxiv.org/abs/2505.19266
tags:
- scoring
- task
- rater
- teachers
- raters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates construct-irrelevant variance (CIV) in
  large language model (LLM) scoring of teachers' pedagogical content knowledge (PCK)
  compared to human raters and traditional machine learning (ML) models. Using generalized
  linear mixed models, the authors analyzed variance components across two PCK tasks
  (analyzing student thinking and evaluating teacher responsiveness) with 187 teachers
  and three scoring sources.
---

# Using Large Language Models to Assess Teachers' Pedagogical Content Knowledge

## Quick Facts
- arXiv ID: 2505.19266
- Source URL: https://arxiv.org/abs/2505.19266
- Reference count: 21
- LLMs show construct-irrelevant variance patterns similar to human raters when scoring teachers' PCK, with varying severity and sensitivity profiles

## Executive Summary
This study examines construct-irrelevant variance (CIV) when large language models (LLMs) score teachers' pedagogical content knowledge (PCK) compared to human raters and traditional machine learning models. Using two PCK assessment tasks with 187 teachers, the researchers analyzed variance components across different scoring sources. The findings reveal that scenario-level variance contributes minimally to overall variance, while rater-related factors substantially influence scoring outcomes. The LLM was found to be the most lenient rater and showed slightly increased context sensitivity compared to human raters, though it remained more stable overall.

## Method Summary
The study employed generalized linear mixed models to analyze variance components in PCK assessment scoring across three rater types: human raters, a traditional machine learning model, and a large language model. Two PCK tasks were used: Task I focused on analyzing student thinking, while Task II addressed evaluating teacher responsiveness. The analysis examined how variance was distributed across scenarios, teachers, and rater characteristics to understand construct-irrelevant variance patterns. Raters included pre-service teachers, and the ML model was trained on this same population.

## Key Results
- Scenario-level variance contributed minimally to overall variance across both tasks
- Rater-related factors, particularly in Task II, contributed substantially to construct-irrelevant variance
- The ML model was the most severe and least sensitive rater, while the LLM was the most lenient
- The LLM showed slightly increased context sensitivity in Task II but remained more stable than human raters

## Why This Works (Mechanism)
The study demonstrates that LLMs can effectively score PCK assessments with patterns of construct-irrelevant variance similar to human raters. The mechanism involves the LLM's ability to interpret open-ended responses and apply consistent scoring criteria across different scenarios, though with distinct severity and sensitivity profiles compared to human and ML scoring approaches.

## Foundational Learning
**Pedagogical Content Knowledge (PCK)**: The specialized knowledge teachers need to effectively teach specific content areas. Why needed: Core construct being assessed; quick check: Understanding PCK dimensions helps interpret task design and scoring criteria.

**Construct-Irrelevant Variance (CIV)**: Variance in assessment scores attributable to factors unrelated to the construct being measured. Why needed: Primary analytical focus; quick check: Understanding CIV helps evaluate scoring validity and fairness.

**Generalized Linear Mixed Models (GLMM)**: Statistical models that account for both fixed and random effects in hierarchical data structures. Why needed: Analytical method for partitioning variance components; quick check: Understanding GLMM helps interpret variance decomposition results.

**Rater Severity and Sensitivity**: Metrics describing how strictly raters score and their ability to discriminate between response quality levels. Why needed: Key comparative metrics between scoring approaches; quick check: Understanding these concepts helps interpret rater profile comparisons.

## Architecture Onboarding
**Component Map**: PCK Tasks (Task I -> Task II) -> Response Collection -> Three Rater Types (Human -> ML -> LLM) -> GLMM Analysis -> Variance Component Output

**Critical Path**: Task administration → Response scoring by three rater types → GLMM variance decomposition → CIV pattern analysis

**Design Tradeoffs**: Human raters offer nuanced understanding but introduce rater variability; ML models provide consistency but may lack interpretive depth; LLMs balance interpretive capability with consistency but introduce their own scoring patterns

**Failure Signatures**: High scenario-level variance would indicate poor task design; extreme rater severity differences would suggest calibration issues; low sensitivity across raters would indicate inability to discriminate response quality

**First Experiments**: 1) Test single scenario across all rater types to establish baseline scoring patterns, 2) Compare LLM scoring across different temperature settings to assess stability, 3) Conduct inter-rater reliability analysis focusing on cases of maximum disagreement

## Open Questions the Paper Calls Out
None

## Limitations
- Results may not generalize beyond the specific PCK tasks and assessment context used in the study
- Limited transparency regarding LLM configuration (model version, temperature settings, prompting strategies)
- Human raters were pre-service teachers rather than practicing educators or content experts, potentially affecting CIV patterns

## Confidence
**High confidence**: Minimal scenario-level variance contribution is well-supported; comparative severity and sensitivity patterns are robust
**Medium confidence**: Interpretation of LLM's "slightly increased context sensitivity" is supported but could benefit from additional qualitative analysis
**Low confidence**: Specific quantitative comparisons of CIV magnitude between LLM and human raters in Task II are limited by small scenario numbers

## Next Checks
1. Test the same scoring approaches across different PCK assessment contexts and subject domains to evaluate generalizability
2. Systematically vary LLM parameters (temperature, prompting strategies, model versions) to identify optimal configurations that minimize CIV
3. Conduct think-aloud protocols and error analysis with all three rater types to understand the cognitive and algorithmic bases of scoring decisions, particularly for cases of substantial divergence