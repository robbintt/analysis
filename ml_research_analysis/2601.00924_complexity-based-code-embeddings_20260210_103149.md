---
ver: rpa2
title: Complexity-based code embeddings
arxiv_id: '2601.00924'
source_url: https://arxiv.org/abs/2601.00924
tags:
- code
- embeddings
- dataset
- complexity
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel approach for generating numerical code
  embeddings by dynamically profiling algorithms' runtime behavior using various complexity
  metrics (e.g., cycles, instructions, cache misses) and fitting them to r-Complexity
  functions. The embeddings are used to classify C++ solutions from competitive programming
  into 11 algorithmic categories using tree-based classifiers.
---

# Complexity-based code embeddings

## Quick Facts
- **arXiv ID:** 2601.00924
- **Source URL:** https://arxiv.org/abs/2601.00924
- **Reference count:** 19
- **Primary result:** Novel complexity-based embeddings achieve 90% average F1-score on 11-class algorithmic classification of C++ competitive programming solutions

## Executive Summary
This paper introduces a novel approach for generating numerical code embeddings by dynamically profiling algorithms' runtime behavior using hardware performance counters and fitting them to r-Complexity functions. The embeddings are used to classify C++ solutions from competitive programming into 11 algorithmic categories using tree-based classifiers. The XGBoost model achieved an average F1-score of 90% on a multi-label dataset of 5,949 solutions, with binary classification (math/non-math) reaching 96-97% accuracy. The results demonstrate that dynamic code embeddings based on runtime complexity can effectively capture algorithmic patterns and enable high-accuracy classification.

## Method Summary
The method involves compiling C++ solutions to binaries, generating synthetic inputs of varying sizes, and executing them while collecting 9 hardware/software metrics via Linux perf (cycles, instructions, cache misses, etc.). Each metric's growth curve is fitted to one of 6 discrete function families (log-log, log-polynomial, fractional power, polynomial, power, factorial) by searching over parameterized configurations. The fitting process produces 4-tuples (FEATURE_TYPE, FEATURE_CONFIG, INTERCEPT, R-VAL) per metric, which are aggregated into 36-dimensional embeddings. Tree-based classifiers (XGBoost, Random Forest, Decision Tree) are then trained on these embeddings to predict algorithmic categories.

## Key Results
- XGBoost achieved 94% precision and 91% F1 (weighted average) on 11-class multi-label classification
- Binary classification (math/non-math) reached 96-97% accuracy
- Decision Tree and Random Forest also performed well, while neural networks were "rather unsatisfactory"
- 66/34 train/test split used across 5,949 C++ solutions from Codeforces

## Why This Works (Mechanism)

### Mechanism 1
Runtime profiling metrics, when fitted to complexity function templates, encode algorithmic behavior patterns sufficient for classification. The system executes compiled binaries against synthetic inputs of varying sizes, collects 9 hardware/software metrics via Linux perf, then fits each metric's growth curve to one of 6 discrete function families. Each fit produces a 4-tuple: (FEATURE_TYPE, FEATURE_CONFIG, INTERCEPT, R-VAL). The 36-dimensional embedding aggregates these tuples across metrics.

### Mechanism 2
Discretizing the continuous function search space into a fixed set of templates preserves discriminative power while making fitting tractable. Rather than searching all possible r-Complexity functions, the authors sample specific configurations (logarithmic exponents ∈ {0,1,2,...,10}, polynomial exponents ∈ {1,1.3,1.5,...,10}, etc.). This reduces the regression problem to selecting the best-fitting template per metric.

### Mechanism 3
Tree-based classifiers (especially XGBoost) can map complexity-based embeddings to algorithmic labels more effectively than neural networks for this task. The 36-dim embedding is fed to Decision Tree, Random Forest, and XGBoost classifiers. XGBoost achieved 94% precision / 91% F1 (weighted avg) on 11-class multi-label classification. Neural networks were tested but "performance was rather unsatisfactory" relative to decision trees.

## Foundational Learning

- **Big-O and asymptotic complexity**: The r-Complexity model extends Big-O notation; understanding Θ(f(n)) is prerequisite to grasping Θ_r(g(n)) and the fitting process.
  - *Quick check:* Given f(n) = 3n² + 5n log n, what is its Big-Θ class?

- **Hardware performance counters**: The embedding relies on `perf` metrics (cycles, branch-misses, cache-references, stalled-cycles-frontend). Understanding what these measure clarifies why they capture algorithmic behavior.
  - *Quick check:* What does "stalled-cycles-frontend" indicate about a program's execution?

- **Curve fitting / regression**: The core mechanism fits observed (input_size, metric_value) pairs to function templates. Understanding least-squares fitting and goodness-of-fit is essential.
  - *Quick check:* If data points suggest f(n) ≈ 2n² + 100, how would you fit this to a polynomial template?

## Architecture Onboarding

- **Component map**: Data Acquisition (Codeforces solutions → compile → generate inputs → execute → collect perf profiles → store) → Embeddings (read profiling data → fit r-Complexity functions → output 36-dim vectors) → Classification (XGBoost/RandomForest/DecisionTree → predict algorithmic labels)

- **Critical path**: 1) Synthetic input generation (manual, requires domain judgment to avoid outliers) → 2) Profiling execution (must run on consistent hardware/architecture for reproducibility) → 3) Complexity fitting (selects best template per metric; determines embedding quality) → 4) Classifier training (66/34 train/test split per paper)

- **Design tradeoffs**: Input count vs. pipeline cost (50 inputs per problem; more inputs improve fit quality but slow profiling); Architecture specificity (embeddings depend on profiling hardware, recommend re-profiling on target hardware); Discretization granularity (broader parameter search improves fit but increases computation)

- **Failure signatures**: Low recall on rare classes (<35 samples show recall drops to 0.68–0.74); Outlier inputs triggering trivial solution paths produce misleading complexity estimates; Architecture drift causing absolute metric value changes

- **First 3 experiments**: 1) Reproduce binary classification (math/non-math) to verify ~96% accuracy; 2) Ablate metrics (remove one category and measure F1 degradation); 3) Cross-architecture test (profile same binaries on different CPU and compare embedding similarity)

## Open Questions the Paper Calls Out

- **Open Question 1**: Can complexity-based embeddings effectively distinguish malicious software from benign code, or detect code plagiarism? The paper states these embeddings "can be used to further analysis, such as: plagiarism detection... and malware detection," but the current study was limited to algorithmic classification on the Codeforces platform.

- **Open Question 2**: Can a mapping function be derived to normalize performance counters across different hardware architectures? Section 3 notes the embedding is "dependent on the architecture," and Section 4 suggests a mapping function might exist but relies on re-processing or similar performance families. Experiments were strictly confined to a single CPU type.

- **Open Question 3**: Does increasing the number and diversity of synthetic input generators improve the robustness of the complexity estimation? Section 7 identifies "further work on the development of the TheInputsCodeforces database, in order to support more generators," suggesting it may enhance model generality. The current dataset relied on 50 manually generated inputs per problem, which authors noted is "toilsome" and potentially limited.

## Limitations

- Architecture-dependent embeddings that may not transfer across different hardware platforms without re-profiling
- Manual synthetic input generation methodology that is inadequately specified and potentially inconsistent
- Discretized function space that may miss algorithmic patterns outside the specified parameter ranges
- Class imbalance affecting performance on rare algorithmic categories with fewer training samples

## Confidence

- **High Confidence**: Binary classification results (96-97% accuracy for math/non-math) are robust and demonstrate clear separation between these two algorithmic domains
- **Medium Confidence**: 11-class multi-label classification (average 90% F1) is supported by experimental results but may be inflated by class imbalance and very few samples for some categories
- **Low Confidence**: Claim that tree-based classifiers outperform neural networks lacks rigorous ablation studies and hyperparameter optimization for both approaches

## Next Checks

1. **Input Generation Protocol**: Develop and document a systematic synthetic input generation methodology that captures the full complexity space of each algorithmic category, then compare classification performance against the paper's "manual" approach.

2. **Architecture Portability Test**: Profile the same code corpus on at least two different hardware architectures (e.g., Intel vs. AMD) and quantify the variance in FEATURE_TYPE assignments and overall classification performance to assess cross-platform generalizability.

3. **Function Space Coverage Analysis**: Systematically identify competitive programming algorithms whose complexity functions fall outside the discretized search space (e.g., O(n^1.23), O(n log log n)), then measure how including these affects classification accuracy and whether the function templates need expansion.