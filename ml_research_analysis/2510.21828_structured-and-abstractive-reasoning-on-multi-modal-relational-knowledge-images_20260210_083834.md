---
ver: rpa2
title: Structured and Abstractive Reasoning on Multi-modal Relational Knowledge Images
arxiv_id: '2510.21828'
source_url: https://arxiv.org/abs/2510.21828
tags:
- task
- data
- star
- answer
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of structured and abstractive
  reasoning on multi-modal relational knowledge (MMRK) images, which contain complex
  semantic relations between entities. The authors introduce an automatic STAR data
  engine that synthesizes images with MMRK and generates instruction data with reliable
  chain-of-thought reasoning.
---

# Structured and Abstractive Reasoning on Multi-modal Relational Knowledge Images

## Quick Facts
- arXiv ID: 2510.21828
- Source URL: https://arxiv.org/abs/2510.21828
- Reference count: 26
- Primary result: Smaller MLLMs (3B-34B) trained on synthetic STAR data outperform GPT-4o on multi-modal reasoning tasks

## Executive Summary
This paper addresses the challenge of structured and abstractive reasoning on multi-modal relational knowledge (MMRK) images containing complex semantic relations. The authors introduce an automatic STAR data engine that synthesizes images with MMRK and generates instruction data with reliable chain-of-thought reasoning. They propose a two-stage training framework combining supervised fine-tuning and preference alignment to enhance multi-modal large language models' STAR capabilities. Experiments on 5 open-source MLLMs show that smaller models significantly outperform GPT-4o on STAR tasks after training, demonstrating effective transfer learning and scalability.

## Method Summary
The authors develop a two-stage training framework for MLLMs on STAR tasks. Stage 1 uses supervised fine-tuning with LoRA on the STAR-64K dataset synthesized from MMKGs. Stage 2 applies preference alignment (DPO/ORPO/SimPO) on failure cases from Stage 1 to refine outputs. The STAR-64K dataset contains 64K samples generated via automatic synthesis from three MMKG sources, with subgraphs limited to max 9 entities. Each sample includes chain-of-thought reasoning before the final answer. The framework is evaluated on 5 open-source MLLMs ranging from 3B to 34B parameters.

## Key Results
- Two-stage training (SFT + PA) outperforms single-stage approaches on all STAR tasks
- Smaller models (3B-34B) significantly outperform GPT-4o after training on STAR-64K
- Chain-of-thought prompts and multi-modal entity information are critical for performance
- Single-task training can outperform full-data training on simpler tasks, while full multi-task training excels at complex reasoning

## Why This Works (Mechanism)

### Mechanism 1
Two-stage training progressively improves STAR capabilities beyond what either stage achieves alone. Stage 1 SFT establishes baseline competency and output format; Stage 2 PA specifically targets failure cases from Stage 1, using correct answers as preferred samples and model errors as unpreferred samples to refine the output distribution.

### Mechanism 2
Chain-of-thought prompts guide MLLMs to systematically identify and reason over elements in MMRK images. CoT templates decompose tasks into explicit recognition steps, reducing hallucination by forcing intermediate grounding.

### Mechanism 3
Multi-modal entity information contributes to STAR performance, with text playing a dominant role. Entity images provide visual grounding; entity texts provide semantic disambiguation. Text is more critical because it carries precise relational semantics that images alone cannot convey.

## Foundational Learning

- **Multi-modal Knowledge Graphs (MMKGs)**: Understanding triple structures (entity-relation-entity) and multi-modal attachments is prerequisite, as the data engine samples subgraphs from MMKGs as source for synthesizing MMRK images.
  - Quick check: Given a subgraph with entities {A, B, C} and relations {R1: A→B, R2: B→C}, what would the MMRK image visualization contain?

- **Preference Alignment (DPO/ORPO/SimPO)**: Understanding how these methods differ in constructing the preference objective (reference model vs. reference-free) is essential for Stage 2 training.
  - Quick check: In DPO, what is the role of the reference model M_ref, and how does it differ from the policy model M?

- **Chain-of-Thought Reasoning**: All instruction data includes CoT reasoning before the final answer. Understanding how CoT decomposes tasks is essential for data synthesis and evaluation.
  - Quick check: For Task #6 (Error Detection), what intermediate reasoning steps should the CoT include before identifying the wrong entity?

## Architecture Onboarding

- **Component map**: MMKG Source -> Subgraph Sampler -> Task Processor -> GraphViz Visualizer -> Instruction Synthesizer -> STAR-64K Dataset -> Stage 1 (SFT on LoRA-tuned MLLM) -> Stage 2 (PA using DPO/ORPO/SimPO on failure cases) -> Evaluation (Task-specific accuracy + LLM-as-Judge scoring)

- **Critical path**: Subgraph sampling limits complexity (max 9 entities) to keep tasks tractable; Task-specific processing creates 8 task variants; Stage 1 SFT establishes baseline; Stage 2 PA corrects Stage 1 failures; Evaluation uses Qwen2.5-VL-72B as judge for CoT quality scoring

- **Design tradeoffs**: Single-task vs. full-data training (single-task can outperform on simpler tasks but full multi-task improves complex reasoning via transfer learning); LoRA rank {8, 16} (higher rank increases capacity but risks overfitting); PA method selection (DPO requires reference model; SimPO is reference-free but may be less stable)

- **Failure signatures**: Hallucinated entities/relations in CoT that don't exist in the MMRK image; Zero-shot models count incorrectly or fabricate entities; Stage 1-only models may correct format but retain reasoning errors

- **First 3 experiments**: Replicate Stage 1 SFT on a single task (e.g., Task #1) with and without CoT prompts to verify the ablation finding; Run inference on Stage 1 checkpoint to collect failure cases, then train Stage 2 PA (start with DPO) and compare accuracy delta; Ablate modality contribution by re-synthesizing MMRK images without entity images or without entity texts, train Stage 1, and compare performance drops on Task #1 and Task #7

## Open Questions the Paper Calls Out

- **Open Question 1**: How can MLLM architectures be modified to overcome the diminishing returns observed when scaling training data for specific STAR tasks? The authors identify the data scaling ceiling but conduct experiments only on fixed, existing backbone architectures without proposing or testing architectural modifications.

- **Open Question 2**: To what extent does the structured reasoning capability acquired from synthetic MMRK images transfer to human-generated or naturally occurring relational diagrams? While the authors demonstrate strong results on the synthetic STAR-64K dataset, they do not evaluate whether this "structured reasoning" capability generalizes to noisy, hand-drawn, or stylistically diverse real-world schemas.

- **Open Question 3**: Why does the omission of entity text cause a greater performance decline than the omission of entity images in MMRK reasoning? The paper identifies this dependency but does not investigate the underlying cause, leaving it unclear if this is due to limitations in current visual encoders or the semantic ambiguity of the visual entities in the dataset.

## Limitations

- Dataset generation fidelity is uncertain due to unspecified random walk hyperparameters and missing CoT generation prompts
- Generalization beyond synthetic data is unproven as all experiments use the same STAR-64K dataset for training and evaluation
- Reference-free vs. reference-based preference alignment methods are not compared, with DPO choice appearing arbitrary

## Confidence

**High Confidence**: The two-stage training framework consistently improves performance across all 5 MLLMs tested. The ablation studies on CoT prompts and modality contributions are methodologically sound and reproducible.

**Medium Confidence**: Claims about smaller models outperforming GPT-4o require careful interpretation due to different evaluation protocols and training conditions.

**Low Confidence**: The scalability claim lacks robustness testing and doesn't examine model behavior on out-of-distribution MMRK images or naturally occurring relational knowledge visualizations.

## Next Checks

1. **Dataset Generation Replication**: Implement the STAR-64K data engine with specified random walk constraints and generate a validation subset. Compare the distribution of task types and complexity metrics against the paper's reported statistics to verify the synthesis process fidelity.

2. **Cross-Domain Generalization Test**: Apply the best-performing Stage 2 model to naturally occurring relational knowledge visualizations (e.g., academic knowledge graphs, organizational charts) that weren't part of the synthetic training data. Measure performance drop and characterize failure modes to assess real-world applicability.

3. **Preference Alignment Method Comparison**: Train Stage 2 models using DPO, ORPO, and SimPO on the same failure case set. Compare not just final accuracy but also training stability (loss curves), sensitivity to hyperparameters, and qualitative differences in CoT outputs to determine if DPO is truly optimal or merely one viable option.