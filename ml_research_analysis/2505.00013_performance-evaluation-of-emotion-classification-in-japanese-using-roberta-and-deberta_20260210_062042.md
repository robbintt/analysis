---
ver: rpa2
title: Performance Evaluation of Emotion Classification in Japanese Using RoBERTa
  and DeBERTa
arxiv_id: '2505.00013'
source_url: https://arxiv.org/abs/2505.00013
tags:
- emotion
- emotions
- japanese
- wrime
- eight
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates fine-tuned Japanese PLMs for emotion classification.
  It compares BERT, RoBERTa, and DeBERTa-v3 variants against LLMs on WRIME, a corpus
  with reader-centric emotion annotations.
---

# Performance Evaluation of Emotion Classification in Japanese Using RoBERTa and DeBERTa

## Quick Facts
- arXiv ID: 2505.00013
- Source URL: https://arxiv.org/abs/2505.00013
- Reference count: 31
- Primary result: DeBERTa-v3-large achieves highest mean accuracy (0.860) and F1-score (0.662) on Japanese emotion classification

## Executive Summary
This study evaluates fine-tuned Japanese PLMs for emotion classification, comparing BERT, RoBERTa, and DeBERTa-v3 variants against LLMs on WRIME, a corpus with reader-centric emotion annotations. DeBERTa-v3-large emerges as the top performer with a mean accuracy of 0.860 and F1-score of 0.662, outperforming both other PLMs and LLMs. The DeBERTa model demonstrates robust performance across both high- and low-frequency emotions. A pip-installable package is released for practical use, though the evaluation is limited to a single corpus and does not include statistical significance testing.

## Method Summary
The study fine-tunes various Japanese PLMs including BERT, RoBERTa, and DeBERTa-v3 variants on the WRIME corpus, which contains reader-centric emotion annotations. The models are evaluated against LLMs like ChatGPT-4o and TinySwallow using standard classification metrics. The evaluation focuses on mean accuracy and F1-scores across emotion categories, with particular attention to performance on both frequent and rare emotions. The study releases a pip-installable package for practical deployment.

## Key Results
- DeBERTa-v3-large achieves highest mean accuracy (0.860) and F1-score (0.662)
- LLMs significantly underperform, with ChatGPT-4o and TinySwallow scoring 0.527 and 0.292 in mean F1
- DeBERTa maintains robust performance across both high- and low-frequency emotions
- pip-installable package released for practical use

## Why This Works (Mechanism)
The superior performance of DeBERTa-v3-large stems from its advanced architecture incorporating disentangled attention mechanisms and enhanced pre-training strategies that better capture semantic relationships in Japanese text. The model's ability to maintain high performance across both frequent and rare emotions suggests effective handling of class imbalance through its fine-tuning approach on the WRIME corpus.

## Foundational Learning
1. **Japanese PLMs** - Why needed: Japanese language has unique morphological and syntactic features requiring specialized tokenization; Quick check: Verify model uses appropriate Japanese tokenizer
2. **Disentangled attention** - Why needed: Separates content and position information for better semantic understanding; Quick check: Confirm model architecture documentation
3. **Fine-tuning methodology** - Why needed: Adapts pre-trained models to specific emotion classification task; Quick check: Review learning rate and epoch selection
4. **Class imbalance handling** - Why needed: Ensures rare emotions are properly classified; Quick check: Examine class weighting or sampling strategies
5. **Emotion classification metrics** - Why needed: F1-score balances precision and recall for imbalanced classes; Quick check: Verify metric calculations
6. **LLM limitations** - Why needed: Understanding why generative models underperform on classification; Quick check: Review prompt engineering attempts

## Architecture Onboarding

**Component Map:**
Japanese Text -> Tokenizer -> PLM (BERT/RoBERTa/DeBERTa) -> Classification Head -> Emotion Label

**Critical Path:**
Preprocessing → Tokenization → Fine-tuning → Evaluation → Deployment

**Design Tradeoffs:**
- Model size vs. performance: DeBERTa-v3-large performs best but may be resource-intensive
- Fine-tuning time vs. accuracy: More epochs improve performance but increase computational cost
- Generalization vs. specialization: Single corpus evaluation limits broader applicability

**Failure Signatures:**
- Poor performance on rare emotions indicates insufficient training data or class imbalance
- Low F1-scores suggest precision-recall tradeoff issues
- LLMs underperforming points to prompt engineering or fine-tuning gaps

**First Experiments:**
1. Test model performance on additional Japanese emotion corpora
2. Conduct ablation studies on different fine-tuning hyperparameters
3. Compare statistical significance of performance differences

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation confined to single WRIME corpus, limiting generalizability
- No statistical significance testing on performance differences
- Limited LLM exploration without broader model versions or prompt engineering
- Lack of detailed per-emotion category analysis beyond frequency-based performance

## Confidence
High confidence in DeBERTa-v3-large achieving highest reported metrics within specific corpus and setup
Medium confidence in relative LLM underperformance due to limited model scope
Low confidence in cross-corpus or cross-language generalizability

## Next Checks
1. Replicate experiments on additional Japanese emotion corpora or multilingual emotion datasets to test generalizability
2. Conduct ablation studies on prompt engineering and hyperparameter tuning for LLMs to determine if performance gaps can be reduced
3. Perform statistical significance testing on model performance differences and detailed per-emotion category analysis to validate robustness claims