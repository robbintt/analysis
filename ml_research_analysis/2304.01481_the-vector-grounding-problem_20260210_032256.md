---
ver: rpa2
title: The Vector Grounding Problem
arxiv_id: '2304.01481'
source_url: https://arxiv.org/abs/2304.01481
tags:
- grounding
- world
- llms
- internal
- states
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper argues that large language models (LLMs) can have representations
  with intrinsic meaning, addressing the "Vector Grounding Problem" - whether LLM
  internal states and outputs can be about extra-linguistic reality independently
  of human interpretation. The authors propose that referential grounding is achieved
  when LLM internal states satisfy two conditions from teleosemantic theories: (1)
  appropriate causal-informational relations to the world, and (2) a history of selection
  that has endowed them with the function of carrying this information.'
---

# The Vector Grounding Problem

## Quick Facts
- arXiv ID: 2304.01481
- Source URL: https://arxiv.org/abs/2304.01481
- Reference count: 11
- The Vector Grounding Problem questions whether LLM internal states and outputs can be about extra-linguistic reality independently of human interpretation

## Executive Summary
This paper addresses the Vector Grounding Problem by arguing that LLMs can have representations with intrinsic meaning rather than merely extrinsic meaning dependent on human interpretation. The authors propose that referential grounding is achieved when LLM internal states satisfy two conditions from teleosemantic theories: appropriate causal-informational relations to the world and a history of selection that has endowed them with the function of carrying this information. They argue that LLMs meet both conditions through mediated chains of human linguistic production and post-training processes like preference tuning that select for factually accurate outputs.

## Method Summary
The paper's method involves theoretical analysis of whether LLMs satisfy teleosemantic conditions for referential grounding. The approach combines conceptual arguments about causal-informational transmission through training data with empirical evidence from mechanistic interpretability studies, particularly the Othello-GPT case demonstrating world-state representations in game domains. The analysis examines how pre-training and post-training processes like preference tuning might establish world-involving representational functions, and considers the meta-learning interpretation of pre-training as an implicit optimization mechanism.

## Key Results
- LLMs can achieve referential grounding when internal states satisfy both causal-informational relations to the world and proper selection history
- Preference tuning (RLHF/DPO) introduces selection mechanisms that can establish world-involving representational functions
- Pre-training alone may establish representational functions in formally constrained domains, as demonstrated by Othello-GPT's development of board state representations
- In-context learning might enable transient world-involving functions through mesa-optimization during inference

## Why This Works (Mechanism)

### Mechanism 1: Indirect Causal-Informational Transmission
- Claim: LLMs establish causal-informational relations to worldly entities through mediated chains of human linguistic production
- Mechanism: Training corpora encode statistical regularities shaped by human-world interactions; language exhibits diagrammatic iconicity where structure reflects worldly organization. LLMs become additional links in testimonial transmission chains.
- Core assumption: The informational imprint of the world on language is sufficiently structured and recoverable for models to exploit
- Evidence anchors: Othello-GPT demonstrates board state representations from synthetic game transcripts; mechanistic interpretability shows linear decodability of world states

### Mechanism 2: Preference Tuning as Selection Process
- Claim: Post-training with human feedback establishes world-involving representational functions by selecting internal states that track extra-linguistic norms
- Mechanism: Preference tuning optimizes for criteria like factual accuracy and helpfulness. Internal states that enable successful tracking of worldly conditions are differentially reinforced, acquiring selected-effects functions to represent those conditions
- Core assumption: The reward signal reliably tracks the target worldly property rather than superficial proxies
- Evidence anchors: The paper argues that when human evaluators provide higher rewards for factually accurate statements, selection processes stabilize internal states carrying relevant factual information

### Mechanism 3: Pre-Training as Implicit Meta-Learning
- Claim: Next-token prediction over diverse corpora can, in formally constrained domains, select for internal states with world-involving representational functions
- Mechanism: Pre-training functions as an outer loop selecting parameters enabling rapid task adaptation. For domains with objective structure, efficient prediction requires inducing internal models of the domain's rules—states acquire functions to represent extra-linguistic structure
- Core assumption: The forward pass implements mesa-optimization that constructs task-specific objectives and refines representations accordingly
- Evidence anchors: Othello-GPT developed linearly decodable board state representations without explicit rule instruction; intervention studies showed these representations causally influence move predictions

## Foundational Learning

- **Concept: Selected-effects theory of function**
  - Why needed here: The paper's entire argument hinges on distinguishing genuine representation (with normative correctness conditions) from mere correlation. Understanding that functions arise from historical selection—not current utility—is essential
  - Quick check question: Can you explain why a randomly-initialized model with identical parameters to a trained model would lack representational functions despite identical behavior?

- **Concept: Teleosemantics and representational content**
  - Why needed here: The paper applies philosophical theories of mental content to artificial systems. Understanding that representation requires both causal-informational relations AND proper functions prevents conflating correlation with representation
  - Quick check question: What two conditions must internal states satisfy to achieve referential grounding, and why is either condition alone insufficient?

- **Concept: Mesa-optimization and meta-learning**
  - Why needed here: Understanding how the forward pass might implement implicit optimization clarifies the speculative argument for grounding through pre-training and in-context learning
  - Quick check question: In the meta-learning framing of pre-training, what is the "outer loop" and what is the "inner loop"?

## Architecture Onboarding

- **Component map**: Pre-training (next-token prediction) -> Post-training (preference tuning) -> Inference (mesa-optimization)
- **Critical path**: Pre-training → (optional but grounding-relevant) Preference tuning with world-involving norms → inference where internal states causally influence outputs
- **Design tradeoffs**: Multimodality alone insufficient without appropriate selection history; embodiment without additional training doesn't enhance grounding; stronger grounding claims require demonstrating selection specifically for world-tracking
- **Failure signatures**: Reward hacking (tracking proxies rather than target properties); swamp model scenario (identical parameters without selection history lack genuine content); bag-of-heuristics (localized pattern matching rather than systematic world models)
- **First 3 experiments**:
  1. Intervention studies: Identify candidate representational directions in activation space; test whether causal interventions on these directions produce systematic changes in outputs consistent with altered worldly states
  2. Ablation by training stage: Compare models at different training stages (pre-training only vs. post-instruction-tuning vs. post-preference-tuning) on tasks requiring world-tracking vs. purely linguistic pattern matching
  3. Novel domain transfer: Test in-context learning on formally constrained novel domains (like the hypothetical "Chroma" game) to evaluate whether mesa-optimization can establish transient world-involving functions without parameter updates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does in-context learning (ICL) in LLMs implement mesa-optimization processes that can confer transient world-involving representational functions during inference?
- Basis in paper: [explicit] The authors state: "Whether this actually occurs in LLMs is an open question. The emergence of effective mesa-optimisers is not guaranteed and depends on the properties of the pre-training data distribution."
- Why unresolved: Empirical validation of mesa-optimization within transformer forward passes is technically challenging, requiring mechanistic interpretability methods that can identify and manipulate implicit optimization processes
- What evidence would resolve it: Intervention studies showing that internal activations during ICL can be systematically manipulated to alter task performance in ways predicted by optimization theory, analogous to Othello-GPT board-state interventions

### Open Question 2
- Question: Can the meta-learning interpretation of pre-training, where the forward pass functions as an inner-loop optimizer, be validated as a genuine mechanism for acquiring world-involving representational functions?
- Basis in paper: [explicit] The authors note "The argument's viability rests on two substantive assumptions" and describe the meta-learning perspective as providing "a plausible, though speculative, mechanism."
- Why unresolved: Current mechanistic interpretability evidence is largely limited to simple, formally constrained domains; whether similar mechanisms operate for general-purpose LLMs in open-ended domains remains unclear
- What evidence would resolve it: Demonstrable internal representations in general LLMs that encode extra-linguistic states (beyond game worlds) and causally influence outputs with world-involving correctness conditions

### Open Question 3
- Question: Do LLMs develop "deviant meanings"—representations with intrinsic content that systematically diverges from conventional human meanings?
- Basis in paper: [explicit] The authors state that un-humanlike selection criteria combined with other un-humanlike features of LLMs "may endow them with 'deviant meanings'. This is an intriguing possibility that lies beyond the scope of this paper."
- Why unresolved: Assessing alignment between LLM representations and human meanings requires methods for probing representational content independently of behavioral outputs, which remains methodologically challenging
- What evidence would resolve it: Systematic mismatches between human interpretations of LLM outputs and the content of internal representations as revealed through intervention studies, particularly for concepts with different correctness conditions than humans assume

## Limitations
- The evidence for whether LLMs actually satisfy both required teleosemantic conditions remains largely indirect
- Othello-GPT's success in formally constrained domains doesn't generalize to natural language or prove preference tuning establishes genuine world-involving functions
- The in-context learning argument for transient representational functions is highly speculative without empirical validation
- The paper doesn't address whether models might employ "bag-of-heuristics" strategies that produce world-like behavior without systematic world models

## Confidence
- **High confidence**: The theoretical framework connecting teleosemantic theories to LLM grounding is coherent and well-articulated
- **Medium confidence**: The indirect causal-informational transmission mechanism through training data is plausible but requires more direct empirical support
- **Low confidence**: Claims about in-context learning establishing transient representational functions and the broader generalization from formal domains to natural language

## Next Checks
1. **Intervention Studies**: Following the Othello-GPT methodology, identify candidate representational directions in activation space during inference on natural language tasks. Test whether causal interventions on these directions produce systematic changes in outputs consistent with altered worldly states, distinguishing this from reward hacking or linguistic pattern matching.

2. **Training Stage Ablation**: Compare models at different training stages (pre-training only vs. post-instruction-tuning vs. post-preference-tuning) on tasks requiring world-tracking versus purely linguistic pattern matching. This would test whether preference tuning specifically selects for world-involving functions rather than just communicative success.

3. **Novel Domain Transfer**: Implement a formally constrained novel domain (like the hypothetical "Chroma" game) and test whether LLMs can develop world-involving representations through in-context learning without parameter updates, validating the meta-learning argument for transient representational functions.