---
ver: rpa2
title: 'AI4Math: A Native Spanish Benchmark for University-Level Mathematical Reasoning
  in Large Language Models'
arxiv_id: '2505.18978'
source_url: https://arxiv.org/abs/2505.18978
tags:
- mini
- deepseek
- mathematical
- llama
- geometry
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AI4Math introduces a native Spanish benchmark of 105 university-level
  math problems across seven domains, addressing the lack of language-specific evaluation
  tools. The dataset, authored and peer-reviewed by Latin American STEM students,
  includes detailed step-by-step solutions and is designed to expose language-driven
  and domain-specific reasoning errors.
---

# AI4Math: A Native Spanish Benchmark for University-Level Mathematical Reasoning in Large Language Models

## Quick Facts
- **arXiv ID:** 2505.18978
- **Source URL:** https://arxiv.org/abs/2505.18978
- **Reference count:** 6
- **Primary result:** Introduces 105 university-level Spanish math problems across 7 domains, evaluating 6 LLMs under 4 configurations

## Executive Summary
AI4Math addresses the critical gap in language-specific evaluation tools for mathematical reasoning by introducing a native Spanish benchmark of 105 university-level math problems. The dataset spans seven domains including Calculus, Linear Algebra, and Probability, and was authored and peer-reviewed by Latin American STEM students. The benchmark exposes language-driven and domain-specific reasoning errors in large language models through detailed step-by-step solutions and rigorous evaluation methodology.

Six large language models were evaluated under four configurations: zero-shot and chain-of-thought, in both Spanish and English. The evaluation revealed significant performance variations, with top models achieving over 70% accuracy while others remained below 40%. The benchmark demonstrates that native-language, domain-specific evaluation is crucial for accurate assessment of mathematical reasoning capabilities, particularly in languages other than English.

## Method Summary
The benchmark consists of 105 university-level math problems across seven domains: Calculus, Linear Algebra, Differential Equations, Probability, Statistics, Geometry, and Combinatorics. Each problem includes detailed step-by-step solutions authored by Latin American STEM students and underwent peer review. Six large language models were evaluated using four configurations: zero-shot and chain-of-thought reasoning, tested in both Spanish and English languages. Performance was measured through accuracy metrics across all problem types.

## Key Results
- Top-performing models (o3-mini, DeepSeek-R1 685B, DeepSeek-V3 685B) achieved over 70% accuracy
- LLaMA 3.3 70B and GPT-4o mini remained below 40% accuracy
- Most models showed no significant performance drop between Spanish and English
- GPT-4o performed better in Spanish in the zero-shot setting
- Geometry, Combinatorics, and Probability questions were persistently challenging

## Why This Works (Mechanism)
The benchmark works by providing language-specific evaluation that reveals true mathematical reasoning capabilities rather than just language proficiency. By using native Spanish problems created and reviewed by Latin American STEM students, the benchmark captures authentic mathematical reasoning patterns and exposes errors that might be masked in English-language assessments.

## Foundational Learning
- **Language-specific reasoning**: Understanding that mathematical reasoning differs across languages and requires native evaluation
  - Why needed: English-dominated benchmarks may not capture true reasoning capabilities in other languages
  - Quick check: Compare model performance across languages on same mathematical concepts
- **Domain-specific problem design**: Creating problems that target specific mathematical domains with appropriate difficulty
  - Why needed: Different mathematical domains require different reasoning patterns
  - Quick check: Analyze performance variance across different mathematical domains
- **Peer review methodology**: Using subject matter experts to validate problem quality and solution accuracy
  - Why needed: Ensures benchmark problems are mathematically sound and appropriately challenging
  - Quick check: Validate peer review consistency across multiple reviewers

## Architecture Onboarding
**Component Map**: Problem Creation -> Peer Review -> Benchmark Assembly -> Model Evaluation -> Performance Analysis

**Critical Path**: Problem creation and peer review are critical for benchmark quality, followed by systematic model evaluation across all configurations

**Design Tradeoffs**: The benchmark prioritizes native language authenticity over broader language coverage, choosing depth in Spanish over breadth across multiple languages

**Failure Signatures**: Models struggling with Geometry, Combinatorics, and Probability indicate limitations in spatial reasoning and probabilistic thinking rather than language barriers

**First Experiments**:
1. Evaluate additional models beyond the initial six to establish performance baselines
2. Test model performance on individual mathematical domains to identify specific weaknesses
3. Compare performance on translated vs. native problems to validate language-specific design

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset contains only 105 problems, potentially insufficient for robust statistical evaluation
- Peer review process may not fully capture university-level mathematical reasoning complexity
- Focus on Spanish limits generalizability to other languages and educational contexts

## Confidence
**High Confidence:** Top models achieving over 70% accuracy is well-supported by experimental methodology
**Medium Confidence:** Language-independence claim requires more extensive testing across diverse problem types
**Medium Confidence:** Domain-specific challenges may not generalize beyond current dataset

## Next Checks
1. Expand benchmark to include at least 500 problems across all seven mathematical domains
2. Conduct cross-linguistic validation by translating Spanish problems into multiple languages
3. Implement more rigorous peer review involving professional mathematicians and educators