---
ver: rpa2
title: 'Automated Landfill Detection Using Deep Learning: A Comparative Study of Lightweight
  and Custom Architectures with the AerialWaste Dataset'
arxiv_id: '2508.18315'
source_url: https://arxiv.org/abs/2508.18315
tags:
- learning
- images
- accuracy
- waste
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates the effectiveness of deep learning models
  for automated landfill detection using the AerialWaste dataset. The research focuses
  on lightweight architectures, finding that complex models are prone to overfitting,
  while simpler models like MobileNetV2, GoogLeNet, and DenseNet achieve better generalization.
---

# Automated Landfill Detection Using Deep Learning: A Comparative Study of Lightweight and Custom Architectures with the AerialWaste Dataset

## Quick Facts
- arXiv ID: 2508.18315
- Source URL: https://arxiv.org/abs/2508.18315
- Reference count: 40
- Lightweight models (MobileNetV2, GoogLeNet, DenseNet) outperform complex architectures, with ensemble fusion achieving 92.33% accuracy

## Executive Summary
This study evaluates deep learning models for automated landfill detection using the AerialWaste dataset of 10,434 aerial/satellite images. The research demonstrates that lightweight architectures generalize better than complex models when training data is limited, with overfitting observed in deeper models like DenseNet121 and GoogLeNet. An ensemble approach combining MobileViT-XS (CNN + local transformer) and ViT-Tiny (pure transformer) models significantly improves performance, achieving 92.33% accuracy, 92.67% precision, and 92.41% F1-score. The study emphasizes the importance of transfer learning, model optimization, and feature fusion techniques for large-scale environmental monitoring applications.

## Method Summary
The study uses the AerialWaste dataset (7,278 training, 1,818 validation, 2,607 test images) from Lombardy, Italy, balanced via augmentation to 4,852 positive/negative training images each. Individual models (MobileNetV2, SqueezeNet, DenseNet121, GoogLeNet, MobileViT-XS, ViT-Tiny) were trained with ImageNet pretraining, AdamW optimizer, batch size 64, learning rate 1e-4, max 100 epochs. The ensemble model concatenates MobileViT-XS and ViT-Tiny feature vectors through a fully connected layer, with three-model fusion achieved by averaging prediction probabilities. Image preprocessing includes 224×224 resize, normalization (mean=[0.3201, 0.3334, 0.2832], std=[0.2004, 0.1818, 0.1764]), and augmentation (rotation, flipping, color jitter, random cropping).

## Key Results
- Lightweight models (MobileNetV2, GoogLeNet, DenseNet) generalize better than complex architectures, avoiding overfitting observed in DenseNet121 and GoogLeNet
- Ensemble fusion of MobileViT-XS and ViT-Tiny achieves 92.33% accuracy, 92.67% precision, 92.33% sensitivity, 92.41% F1-score, and 92.71% specificity
- Concatenated feature fusion outperforms simple probability averaging by ~0.5-1% in accuracy
- Transfer learning from ImageNet provides effective initialization despite domain shift between natural and aerial imagery

## Why This Works (Mechanism)

### Mechanism 1: Lightweight architectures resist overfitting
- Lightweight models with fewer parameters resist overfitting by having reduced capacity to memorize training patterns, forcing them to learn generalizable visual features instead
- Core assumption: AerialWaste dataset (10,434 images) is insufficient for deep architectures to learn without memorization
- Evidence: Complex models prone to overfitting; MobileNetV2, SqueezeNet achieve better generalization
- Break condition: If dataset size increases substantially (>50K diverse images) or data augmentation becomes more aggressive

### Mechanism 2: Ensemble fusion captures complementary features
- MobileViT-XS extracts local convolutional features while ViT-Tiny captures long-range dependencies via self-attention; concatenation allows learned fusion to weight each representation optimally
- Core assumption: Landfill detection requires both fine-grained texture recognition and global scene understanding
- Evidence: Ensemble model achieves 91.56% accuracy vs. individual models at 88-89%
- Break condition: If one architecture already captures sufficient feature diversity, fusion provides diminishing returns

### Mechanism 3: Transfer learning provides effective initialization
- Pretrained weights encode universal visual features (edges, textures, shapes) that transfer across domains; freezing early layers preserves these while fine-tuning adapts to waste-specific patterns
- Core assumption: Low-level visual features in natural images are relevant to aerial waste site detection
- Evidence: Lightweight models benefit significantly from transfer learning
- Break condition: If aerial imagery features are fundamentally dissimilar from natural images

## Foundational Learning

- **Concept: Overfitting vs. Generalization Trade-off**
  - Why needed: Paper's central finding is model complexity must match dataset scale
  - Quick check: If validation accuracy plateaus while training accuracy continues rising, what does this indicate about model capacity relative to data?

- **Concept: Feature Fusion Strategies**
  - Why needed: Ensemble's success depends on how outputs are combined
  - Quick check: Why might concatenating feature vectors before final classifier outperform averaging class probabilities from independent models?

- **Concept: Transfer Learning Domain Shift**
  - Why needed: ImageNet-to-aerial transfer works but requires understanding why
  - Quick check: What types of visual features learned from natural images would transfer well to satellite/aerial imagery?

## Architecture Onboarding

- **Component map**: Input preprocessing (resize 224×224, normalize) -> Backbone A (MobileViT-XS) + Backbone B (ViT-Tiny-R-S16-P8-224) -> Concatenated features -> Fully connected layer (fc1) -> Softmax classifier -> NLLLoss

- **Critical path**: Load pretrained weights -> Optionally freeze early layers -> Process each image through both backbones independently -> Concatenate feature vectors -> Pass through learned fusion classifier -> Fine-tune with AdamW optimizer

- **Design tradeoffs**: Concatenation vs. averaging (preserves information but requires additional parameters); frozen layers vs. full fine-tuning (reduces overfitting risk but may limit adaptation); model count in ensemble (improves results but scales computational cost)

- **Failure signatures**: High training accuracy with low validation accuracy (overfitting; reduce model capacity or increase augmentation); individual models outperforming ensemble (check fusion layer learning rate); poor performance on negative class (dataset imbalance)

- **First 3 experiments**: 1) Train MobileNetV2_050 and ViT-Tiny separately; expect ~89-91% accuracy individually. 2) Compare concatenation fusion vs. simple probability averaging; expect concatenation to outperform by ~0.5-1%. 3) Test AdamW vs. Ranger vs. SGD with warm restarts on ensemble; expect AdamW to achieve highest accuracy (~91.14%).

## Open Questions the Paper Calls Out

- Can adaptive weighting mechanisms for feature fusion outperform the current concatenation approach in the ensemble model?
- How does the model's performance generalize to diverse geographical regions and weather conditions outside the Lombardy dataset?
- Does self-supervised training effectively mitigate the overfitting observed in deeper architectures for this specific application?

## Limitations

- Study relies on single geographic region (Lombardy, Italy) limiting generalizability to diverse environmental conditions and waste types
- Manual ground truth correction process lacks transparency about which specific images were reclassified
- Computational efficiency claims remain untested on actual deployment hardware
- Dataset size (10,434 images) may limit conclusions about model complexity trade-offs

## Confidence

**High Confidence**: Ensemble fusion methodology and reported performance metrics (>92% accuracy, precision, F1-score, specificity)
**Medium Confidence**: Generalization claims about lightweight vs. deep models across different datasets
**Low Confidence**: Transfer learning domain transfer effectiveness and computational efficiency assertions

## Next Checks

1. **Dataset Expansion Test**: Replicate study using geographically diverse aerial waste detection dataset to validate lightweight architecture generalization claims
2. **Computational Efficiency Benchmark**: Measure inference latency and memory usage on edge devices (Raspberry Pi, Jetson Nano) to verify deployment claims
3. **Transfer Learning Ablation**: Compare ImageNet pretraining against domain-specific pretraining using large aerial/satellite imagery datasets (SpaceNet, xView) to quantify transfer learning benefits