---
ver: rpa2
title: 'Bidirectional Mamba for Single-Cell Data: Efficient Context Learning with
  Biological Fidelity'
arxiv_id: '2504.16956'
source_url: https://arxiv.org/abs/2504.16956
tags:
- gene
- genemamba
- cell
- data
- single-cell
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GeneMamba, a state space model for single-cell
  RNA sequencing analysis that addresses computational inefficiency and scalability
  limitations of transformer-based models. GeneMamba leverages bidirectional Mamba
  architecture with linear-time complexity and biologically informed training objectives
  including pathway-aware contrastive loss.
---

# Bidirectional Mamba for Single-Cell Data: Efficient Context Learning with Biological Fidelity

## Quick Facts
- arXiv ID: 2504.16956
- Source URL: https://arxiv.org/abs/2504.16956
- Reference count: 40
- Key outcome: GeneMamba achieves 42% faster training than transformer baselines while demonstrating superior performance in multi-batch integration (Avg_batch: 0.9536, Avg_bio: 0.8131) and cell type annotation (accuracy: 0.9713, Macro-F1: 0.7710).

## Executive Summary
GeneMamba introduces a bidirectional Mamba architecture for single-cell RNA sequencing analysis that addresses computational inefficiency and scalability limitations of transformer-based models. The model leverages state space models with linear-time complexity and biologically informed training objectives including pathway-aware contrastive loss. Pretrained on nearly 30 million cells, GeneMamba demonstrates superior performance across multiple benchmarks while achieving significant training speed improvements over transformer baselines.

## Method Summary
GeneMamba processes single-cell RNA-seq data by ranking genes within each cell by normalized expression and selecting top-k genes as input sequences. The bidirectional Mamba architecture uses shared-weight state space model layers in both forward and reverse directions, combined via a learned gating mechanism. The model is pretrained with a combination of language modeling loss and pathway-aware contrastive loss using InfoNCE objectives. The architecture achieves linear computational complexity compared to quadratic transformers, enabling efficient training on large-scale single-cell datasets.

## Key Results
- Multi-batch integration: Avg_batch 0.9536, Avg_bio 0.8131 on immune dataset
- Cell type annotation: Accuracy 0.9713, Macro-F1 0.7710 on hPancreas
- Training efficiency: Up to 42% faster than transformer baselines
- Gene correlation analysis: Superior separation of positive/negative gene pairs compared to baselines

## Why This Works (Mechanism)

### Mechanism 1: Bidirectional Context Aggregation via State Space Dynamics
The Bi-Mamba architecture processes gene sequences in both forward and reverse directions using shared-weight SSM layers, then combines outputs via a learned gating mechanism. This allows each gene token to receive contextual information from both upstream and downstream genes without quadratic attention costs. The gating function `o_t = z_t · h_t + (1 - z_t) · h̃_t` learns to balance bidirectional contributions.

### Mechanism 2: Rank-Based Gene Tokenization Preserves Relative Expression Signals
Genes are ranked by normalized expression within each cell, then top-k genes are selected as token sequences. This normalizes across cells implicitly, as high expression in one cell becomes a rank position comparable to another cell regardless of sequencing depth. The ranking approach is claimed to be more robust to batch effects than absolute expression values.

### Mechanism 3: Pathway-Aware Contrastive Loss Structures Embedding Space
The model uses InfoNCE loss to pull same-pathway gene embeddings closer while pushing different-pathway pairs apart. This contrastive objective enforces biologically meaningful embedding geometry where genes in the same pathway have similar representations. The loss function `L_pathway = -1/|P| Σ log[exp(s̃_ij) / Σ_k exp(s̃_ik)]` with temperature scaling controls separation sharpness.

## Foundational Learning

- **Concept: State Space Models (SSMs) and Selective State Propagation**
  - Why needed: GeneMamba replaces transformer attention with SSMs; understanding how `h_t = f_A(x_t)h_{t-1} + f_B(x_t)x_t` enables input-dependent information filtering is essential.
  - Quick check: Can you explain why making parameters A, B, C input-dependent (rather than static) allows the model to selectively remember or forget information across the sequence?

- **Concept: Single-Cell RNA-seq Data Sparsity and Batch Effects**
  - Why needed: The paper's design choices (ranking, contrastive loss) directly address scRNA-seq challenges; without understanding these, the architectural rationale is opaque.
  - Quick check: Why might two identical cell types from different experimental batches show different gene expression distributions, and how does GeneMamba's ranking approach address this?

- **Concept: Contrastive Learning and InfoNCE Loss**
  - Why needed: The pathway loss is a contrastive objective; understanding positive/negative pair construction and temperature scaling is necessary to interpret training dynamics.
  - Quick check: What happens to embedding separation if the temperature parameter τ is too large vs. too small in the normalized similarity `s̃ = sim/τ`?

## Architecture Onboarding

- **Component map:** Input gene expression matrix → Gene Rank Module (normalize, rank, select top-k) → Embedding Layer (gene token embeddings) → Bi-Mamba Blocks (24 layers) → Task Heads (classification MLP or direct embedding extraction)

- **Critical path:**
  1. **Preprocessing pipeline:** Normalization uses gene-specific median via t-digest (Equation 1)—incorrect normalization breaks downstream ranking.
  2. **Bi-Mamba block:** Shared weights between forward/reverse passes; gating weights must be trained to balance directions.
  3. **Pretraining loss combination:** `L = L_lang + γL_pathway` with γ=0.1; incorrect weighting may cause one loss to dominate.

- **Design tradeoffs:**
  - **Sequence length (2048 vs. 4096 genes):** Longer sequences capture more genes but double training time (Table 5: 5.5h → 11h).
  - **Rank-based vs. continuous embeddings:** Ranking discards absolute values but improves batch robustness; if absolute expression matters for your task, consider value projection instead.
  - **Unidirectional vs. bidirectional:** Table 7 shows Bi-Mamba (BLEU 0.987) outperforms unidirectional Mamba (BLEU 0.532) on reconstruction, but if causal modeling is required, unidirectional may be more appropriate.

- **Failure signatures:**
  - **Poor batch integration (low Avg_batch):** Check if normalization factors are computed consistently between pretrain and finetune data.
  - **Cell type annotation fails on rare cell types:** Examine Macro-F1 vs. Accuracy gap; if Accuracy high but Macro-F1 low, model is biased toward majority classes.
  - **Gene correlation analysis shows no separation:** Verify pathway annotations cover your genes of interest; sparse pathway coverage reduces contrastive loss effectiveness.

- **First 3 experiments:**
  1. **Reproduce gene rank reconstruction** on PBMC12k (Table 3): Input ranked genes → model → output ranked genes → compute BLEU/L-Dist. This validates the Bi-Mamba block is functioning correctly before complex downstream tasks.
  2. **Ablate bidirectionality:** Compare full GeneMamba vs. GeneMamba_U (unidirectional) on cell type annotation. Expect performance drop per Table 7 results.
  3. **Probe pathway loss contribution:** Train with `γ=0` (language loss only) vs. `γ=0.1` on gene correlation separation task. This isolates the contribution of biological supervision to embedding quality.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can GeneMamba be optimized to capture subtle signals from low-expression genes and rare cell populations?
  - Basis: The "Broader Impacts and Limitations" section explicitly states, "the model may struggle with rare cell types or subtle signals from low-expression genes."
  - Evidence needed: A benchmark on a dataset with known rare cell populations, showing improved recall scores after architectural or loss-function modifications.

- **Open Question 2:** Can the GeneMamba architecture be successfully expanded to handle multi-modal single-cell data or spatial transcriptomics?
  - Basis: The conclusion suggests "future work could focus on... expanding to more flexible architectures" to support broader applications.
  - Evidence needed: Successful implementation of a multi-modal GeneMamba variant that outperforms transformer baselines on spatial integration tasks.

- **Open Question 3:** What specific data characteristics in the Multiple Sclerosis (MS) dataset cause GeneMamba to underperform compared to transformer baselines?
  - Basis: In Table 2, GeneMamba shows a significant performance drop on the MS dataset (Acc: 0.6825) compared to scGPT (Acc: 0.8471), a trend not seen in other datasets.
  - Evidence needed: An ablation study identifying whether the drop is caused by batch effects, class imbalance, or specific gene expression patterns in the MS data.

## Limitations

- The pathway-aware contrastive loss mechanism lacks direct validation through ablation studies.
- The biological mechanism for why bidirectional context improves gene relationship modeling remains speculative.
- The model may struggle with rare cell types or subtle signals from low-expression genes due to rank-based discretization.

## Confidence

- **High confidence:** Computational efficiency claims (linear vs quadratic complexity), training time comparisons (42% faster), and basic pretraining/finetuning procedures are well-specified and reproducible.
- **Medium confidence:** Multi-batch integration and cell type annotation results, as these depend on correct implementation of normalization and ranking but follow standard protocols.
- **Low confidence:** Biological mechanism claims (bidirectional context benefits, pathway loss effectiveness) due to limited external validation and speculative biological justification.

## Next Checks

1. **Ablate the pathway loss component:** Train GeneMamba with γ=0 (language loss only) and compare gene correlation analysis results to determine if the pathway loss provides meaningful biological structure beyond what language modeling achieves alone.

2. **Validate tokenization strategy choice:** Systematically compare rank-based, bin-based, and value projection tokenization approaches on the same model architecture to empirically determine which best handles batch effects and preserves biological signal.

3. **Test bidirectional context necessity:** Compare unidirectional vs. bidirectional GeneMamba on tasks where causal gene relationships are well-established (e.g., transcription factor → target gene prediction) to determine if bidirectional processing introduces noise in directed regulatory contexts.