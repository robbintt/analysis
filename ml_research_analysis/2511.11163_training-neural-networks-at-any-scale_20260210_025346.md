---
ver: rpa2
title: Training Neural Networks at Any Scale
arxiv_id: '2511.11163'
source_url: https://arxiv.org/abs/2511.11163
tags:
- learning
- norm
- which
- neural
- scaling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reviews modern optimization methods for training neural
  networks, emphasizing efficiency and scalability. The core contribution is a unified
  algorithmic framework that integrates both geometry-aware methods (like preconditioned
  stochastic gradient descent) and conditional gradient methods (like stochastic Frank-Wolfe).
---

# Training Neural Networks at Any Scale
## Quick Facts
- arXiv ID: 2511.11163
- Source URL: https://arxiv.org/abs/2511.11163
- Reference count: 40
- Primary result: Introduces a unified optimization framework combining geometry-aware and conditional gradient methods, with Maximal Update Parameterization (µP) enabling principled scaling and hyperparameter transfer across model sizes.

## Executive Summary
This paper presents a comprehensive framework for training neural networks at any scale, integrating modern optimization methods with principled scaling techniques. The core contribution is the Maximal Update Parameterization (µP) framework, which ensures feature learning remains consistent across different model widths by carefully aligning initialization variances and learning rates. The approach bridges optimization theory with practical deep learning, offering actionable insights for scaling models effectively while maintaining performance.

The work unifies geometry-aware optimization methods (like preconditioned SGD) with conditional gradient methods (like stochastic Frank-Wolfe) into a single algorithmic framework. This theoretical foundation is complemented by practical implementations that have been used to train models with trillions of parameters. The paper demonstrates how µP enables width-independent feature evolution and allows optimal hyperparameters to transfer from small to large models, addressing one of the most challenging aspects of scaling deep learning systems.

## Method Summary
The paper introduces a unified algorithmic framework that combines geometry-aware optimization methods with conditional gradient approaches. At its core is the Maximal Update Parameterization (µP), which operates by carefully scaling layerwise initialization variances and learning rates to ensure feature learning remains consistent as model width changes. The framework uses Tensor Programs theory to analyze the limiting behavior of neural networks as width approaches infinity, providing theoretical guarantees for width-independent feature evolution. The approach integrates preconditioned stochastic gradient descent with appropriate norm choices and regularizers, while also incorporating stochastic Frank-Wolfe methods for specific optimization scenarios.

## Key Results
- Demonstrates width-independent feature evolution across model sizes using µP, enabling reliable hyperparameter transfer
- Provides theoretical analysis through Tensor Programs showing consistent training dynamics at scale
- Achieves practical success with optimizers like Scion and Muon, which have been used to train trillion-parameter models
- Shows that carefully aligned initialization variances and learning rates are critical for maintaining feature learning across scales

## Why This Works (Mechanism)
The mechanism relies on µP's ability to maintain consistent feature learning dynamics by scaling initialization variances and learning rates proportionally to model width. This ensures that as models grow larger, their training behavior remains predictable and transferable. The Tensor Programs framework provides the theoretical foundation by analyzing the infinite-width limit, showing that certain parameterizations preserve feature evolution patterns. This mathematical analysis translates into practical benefits: hyperparameters optimized on small models can be directly applied to large models without re-tuning, dramatically reducing the computational cost of scaling experiments.

## Foundational Learning
- **Tensor Programs Theory**: Understanding the limiting behavior of neural networks as width approaches infinity is essential for analyzing scale-dependent phenomena. Quick check: Verify that the infinite-width limit preserves the key training dynamics you're studying.
- **Preconditioned Stochastic Gradient Descent**: Geometry-aware optimization methods that adapt to the loss landscape's curvature. Quick check: Measure the effective conditioning of your optimization problem before and after preconditioning.
- **Maximal Update Parameterization (µP)**: A principled approach to scaling that ensures feature learning remains consistent across model sizes. Quick check: Compare feature evolution trajectories across different widths to verify width-independence.
- **Frank-Wolfe Methods**: Conditional gradient approaches that can be more efficient than gradient descent in certain constrained optimization settings. Quick check: Evaluate convergence speed in problems with complex feasible regions.
- **Initialization Variance Scaling**: The practice of scaling initialization variances proportionally to model width to maintain consistent signal propagation. Quick check: Monitor activation statistics across layers and widths to ensure stable signal flow.
- **Hyperparameter Transfer**: The ability to use optimal hyperparameters from small models directly on large models. Quick check: Validate that learning rate schedules optimized on small models perform well on large-scale training.

## Architecture Onboarding
Critical path: Data → Model Architecture → µP Scaling → Training → Evaluation
Component map: Data Preprocessing -> Model Definition -> µP Parameterization -> Optimizer Selection -> Training Loop -> Evaluation
Design tradeoffs: µP provides better scaling properties but requires careful initialization and learning rate tuning compared to standard parameterization.
Failure signatures: Training instability when initialization variances aren't properly scaled, degraded performance when learning rates don't match width scaling.
First experiments: 1) Train identical architectures with standard vs. µP parameterization across multiple widths, 2) Transfer hyperparameters from small to large models and measure performance degradation, 3) Compare feature evolution trajectories across widths using activation visualization.

## Open Questions the Paper Calls Out
None

## Limitations
- The framework's effectiveness across all neural network architectures hasn't been fully validated, with potential limitations for non-standard or emerging architectures.
- Computational overhead of geometry-aware methods and preconditioned SGD in extremely large-scale settings isn't thoroughly analyzed.
- Empirical validation is primarily focused on specific benchmarks, with unclear performance across diverse domains like reinforcement learning or graph neural networks.

## Confidence
- High: Theoretical framework of µP and connection to Tensor Programs is well-established with compelling width-independent feature evolution analysis
- Medium: Practical implementations (Scion, Muon) show promise but lack detailed robustness analysis across different training scenarios
- Low: Universal hyperparameter transfer claims need more empirical validation, especially for non-standard architectures

## Next Checks
1. Cross-architecture testing: Validate µP effectiveness across diverse architectures (transformers, CNNs, RNNs) to ensure generalizability
2. Computational efficiency analysis: Quantify overhead of geometry-aware methods in large-scale settings to assess practical feasibility
3. Long-tail distribution performance: Test framework on imbalanced datasets to evaluate real-world robustness