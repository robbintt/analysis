---
ver: rpa2
title: 'TransportAgents: a multi-agents LLM framework for traffic accident severity
  prediction'
arxiv_id: '2601.15519'
source_url: https://arxiv.org/abs/2601.15519
tags:
- severity
- reasoning
- across
- neiss
- transportagent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TransportAgent, a hybrid multi-agent LLM framework, addresses the
  challenge of accurate traffic crash severity prediction by integrating specialized
  LLM agents with an MLP fusion module. Each agent focuses on distinct categories
  such as demographics, environment, or incident details, generating intermediate
  severity assessments that are combined into a final prediction.
---

# TransportAgents: a multi-agents LLM framework for traffic accident severity prediction

## Quick Facts
- arXiv ID: 2601.15519
- Source URL: https://arxiv.org/abs/2601.15519
- Reference count: 40
- Primary result: Hybrid multi-agent LLM framework achieving 73.31% accuracy on CPSRMS and 76.9% on NEISS for traffic crash severity prediction

## Executive Summary
TransportAgent addresses the challenge of accurate traffic crash severity prediction by integrating specialized LLM agents with an MLP fusion module. Each agent focuses on distinct categories such as demographics, environment, or incident details, generating intermediate severity assessments that are combined into a final prediction. Tested on CPSRMS and NEISS datasets, TransportAgent achieves 73.31% accuracy on CPSRMS and 76.9% on NEISS with the LLaMA-3.3 backbone, outperforming traditional ML, prompting-based LLMs, and existing multi-agent systems.

## Method Summary
TransportAgent implements a three-stage hybrid pipeline: (1) Data Preprocessing Team - Feature Selection Agent removes irrelevant/target-leaking fields, Conceptual Category Organizer partitions features into groups (e.g., demographics, environment, incident context); (2) Severity Assessment Team - category-specific LLM agents output intermediate severity scores; (3) Integration Manager - MLP fuses score vectors into final severity prediction using cross-entropy loss. The framework uses 10-shot examples for prompting baselines and employs 3:1 train/test splits on CPSRMS (n=1555) and NEISS (n=1059 whole, n=397 balanced subset) datasets.

## Key Results
- TransportAgent achieves 73.31% accuracy on CPSRMS and 76.9% on NEISS with LLaMA-3.3-70B-Instruct backbone
- Outperforms traditional ML, prompting-based LLMs, and existing multi-agent systems across both datasets
- Demonstrates robustness across different LLM backbones (GPT-3.5-turbo, GPT-4o-mini, LLaMA-3.3) with consistent accuracy improvements
- Produces balanced, well-calibrated severity predictions, especially for high-severity cases, with reduced over-prediction of Level 3 severity compared to vanilla CoT LLMs

## Why This Works (Mechanism)

### Mechanism 1: Category-Decomposed Reasoning Reduces Context Saturation
Partitioning heterogeneous crash data into conceptual categories allows specialized agents to reason more effectively than a single monolithic LLM. The Conceptual Category Organizer assigns features to distinct groups; each Evaluation Agent receives only category-relevant inputs, producing an intermediate severity score $s_k = \text{Agent}_k(V_k, T)$. This prevents LLM overwhelm by diverse signals and reduces hallucination risk.

### Mechanism 2: Supervised MLP Fusion Corrects LLM Calibration Bias
An MLP integration module learns to weight intermediate agent scores more reliably than prompting-based aggregation, particularly for high-severity classes. The MLP takes the vector $s = [s_1, s_2, \ldots, s_d]^\top$ and learns parameters $\theta$ via cross-entropy loss minimization on labeled training data, producing calibrated logits $z = \text{MLP}(s; \theta)$.

### Mechanism 3: Structured Preprocessing Filters Noise Before Agent Reasoning
Feature Selection Agent removes target-leaking and irrelevant attributes before category assignment, improving downstream agent effectiveness. The agent evaluates each annotated attribute $\hat{V}$ against task $T$, outputting $V_{\text{selected}} = \text{Agent}(\hat{V}, T)$. This reduces context length and removes hallucination triggers.

## Foundational Learning

- **Multi-Agent LLM Decomposition**: Why needed: Understanding how to partition complex tasks across specialized agents is essential for designing the Data Preprocessing and Severity Assessment teams. Quick check: Given a heterogeneous dataset with structured fields and free-text narratives, what criteria would you use to define agent-specialized categories?

- **Cross-Entropy Loss and Softmax Calibration**: Why needed: The MLP integration module is trained to minimize cross-entropy; understanding this objective clarifies how intermediate scores are combined into final predictions. Quick check: If your MLP consistently over-predicts one severity class, what adjustment to the loss function or training data might help?

- **Feature Relevance and Multicollinearity Diagnostics**: Why needed: The paper validates low within-category interdependence using Cramér's V and Spearman's ρ; interpreting these metrics is necessary to assess whether category decomposition is well-founded. Quick check: For two categorical features with Cramér's V = 0.45, would you assign them to the same agent or different agents? Why?

## Architecture Onboarding

- **Component map**: Feature Selection Agent -> Conceptual Category Organizer -> Group Orchestrator -> Evaluation Agent Set -> MLP Fusion Module

- **Critical path**: Raw record enters → Feature Selection Agent removes irrelevant/target-leaking fields → Selected features → Conceptual Category Organizer partitions into ~5-7 groups → Each group → dedicated Evaluation Agent → intermediate severity score $s_k$ → Score vector $s$ → MLP → final severity prediction $\hat{y} = \arg\max_k z_k$

- **Design tradeoffs**: More categories vs. agent load (finer granularity improves specialization but increases LLM calls); MLP depth vs. overfitting (deeper MLP may capture non-linear fusion but risks overfitting); Open vs. closed backbones (LLaMA-3.3 offers reproducibility; GPT-3.5/4o-mini provide stronger reasoning)

- **Failure signatures**: Collapsed predictions (MLP always predicts majority class - check class imbalance and cross-entropy); High variance across splits (>5% fluctuation - inspect MLP regularization); Agent redundancy (minimal degradation when removing agent - re-evaluate category definitions)

- **First 3 experiments**: 1) Baseline replication: Run TransportAgent on CPSRMS with GPT-3.5-turbo, reproducing 74.6% accuracy result; 2) Ablation sweep: Remove Feature Selection Agent and measure accuracy/macro-F1 degradation; 3) Backbone substitution: Swap GPT-3.5-turbo for LLaMA-3.1-8B and assess performance degradation

## Open Questions the Paper Calls Out

### Open Question 1
Can TransportAgent's hybrid multi-agent architecture generalize to safety-critical prediction tasks beyond traffic crash severity, such as healthcare triage or financial risk assessment? The conclusion states the framework "offers a generalizable blueprint for combining reasoning-oriented LLMs with structured numerical learning" but only evaluates on two traffic-related U.S. datasets.

### Open Question 2
What is the optimal number and granularity of conceptual categories for the Severity Assessment Team, and how does performance scale with agent count? The ablation study shows removing individual category-specific agents degrades performance, but doesn't systematically vary the number of agents or test alternative category partitioning schemes.

### Open Question 3
How computationally efficient and cost-effective is TransportAgent compared to single-agent LLM approaches, particularly for real-time or high-throughput deployment scenarios? The multi-agent framework requires multiple LLM inference calls per incident but provides no analysis of latency, token consumption, or API costs relative to baselines.

## Limitations
- MLP architecture and training hyperparameters are not specified, making exact replication difficult
- Prompt templates for LLM agents are not disclosed, leaving room for behavioral variance
- Exact method for extracting severity scores from LLM outputs is not described
- Feature-to-category mappings and category definitions are inferred rather than explicitly provided

## Confidence
- **High**: TransportAgent's overall design (multi-agent decomposition + MLP fusion) is clearly specified and internally consistent
- **Medium**: Performance metrics (73.31% accuracy on CPSRMS, 76.9% on NEISS) are reproducible with correct data and prompt templates, but exact prompt tuning may affect results
- **Low**: Cross-model robustness claims (LLaMA-3.3 vs GPT-3.5-turbo vs GPT-4o-mini) are based on reported numbers, but no statistical significance testing or variance analysis is provided

## Next Checks
1. **Prompt Template Fidelity**: Implement and test the exact prompt structure for each agent type (Feature Selection, Category Organizer, Severity Evaluator) to ensure behavioral consistency
2. **MLP Hyperparameter Sensitivity**: Run ablation studies on MLP depth, width, and regularization to identify overfitting risks and robustness bounds
3. **Cross-Dataset Generalization**: Evaluate TransportAgent on an external traffic crash dataset (e.g., FARS) to test generalizability beyond CPSRMS/NEISS