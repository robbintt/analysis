---
ver: rpa2
title: Interpretable Text Classification Applied to the Detection of LLM-generated
  Creative Writing
arxiv_id: '2601.07368'
source_url: https://arxiv.org/abs/2601.07368
tags:
- uni00000048
- uni00000057
- text
- uni00000044
- uni00000056
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of distinguishing human-written
  creative fiction from LLM-generated text, a task where humans perform poorly but
  machine learning models achieve high accuracy. Using a dataset of 16,136 text samples
  from detective novels, various ML classifiers were tested with unigram features.
---

# Interpretable Text Classification Applied to the Detection of LLM-generated Creative Writing

## Quick Facts
- arXiv ID: 2601.07368
- Source URL: https://arxiv.org/abs/2601.07368
- Reference count: 7
- Primary result: Linear classifier achieves 98% accuracy distinguishing LLM-generated creative writing from human-written detective fiction using unigram features

## Executive Summary
This study addresses the challenge of distinguishing human-written creative fiction from LLM-generated text, a task where humans perform poorly but machine learning models achieve high accuracy. Using a dataset of 16,136 text samples from detective novels, various ML classifiers were tested with unigram features. A linear classifier achieved 98% accuracy, outperforming humans who were near chance level. Analysis revealed that LLMs use a broader variety of synonyms and introduce stylistic changes undetectable by humans but identifiable by ML models. Key factors include temporal drift, Americanisms, foreign language usage, and colloquialisms. The findings suggest robust classification resistant to manipulation, aiding publishers in identifying AI-generated content.

## Method Summary
The researchers created a dataset of 16,136 text samples from Agatha Christie and Dorothy Sayers detective novels, chunking each into 100-word excerpts. Each excerpt was rewritten using GPT-4.1 to generate a paired LLM-produced sample. A custom inclusive tokenizer preserved case and punctuation while normalizing quotes. Various classifiers were trained using unigram frequency features, with a linear classifier achieving 98% accuracy. The interpretable model allowed direct inspection of feature weights to understand classification decisions.

## Key Results
- Linear classifier achieved 98% accuracy distinguishing LLM-generated from human-written detective fiction
- Humans were near chance level (50-60%) at the same classification task
- LLM text showed 22% higher unigram entropy, indicating broader synonym usage
- Key distinguishing features included temporal drift (modern words in vintage text), Americanisms, foreign language usage, and colloquialisms

## Why This Works (Mechanism)

### Mechanism 1: Lexical Diversity & Synonym Skewing
- Machine learning classifiers detect LLM-generated text because LLMs tend to sample from a broader probability distribution of synonyms than human authors in constrained creative domains. When rewriting text, the LLM selects varied dialogue verbs and descriptors (e.g., replacing "said" with "remarked," "responded," or "exclaimed"). This increases the lexical entropy of the generated text compared to the human baseline, creating a distinct statistical fingerprint in unigram frequencies.

### Mechanism 2: Temporal & Dialectal Drift
- The classifiers function effectively because the LLM fails to mimic the specific temporal and regional linguistic constraints of the source material (1920s–30s British English). The LLM generates text reflecting modern usage patterns (temporal drift) and American spelling/phrasing (dialectal drift), introducing tokens (e.g., "exited," "realize") that statistically distinguish it from the vintage human texts.

### Mechanism 3: Stylistic Flattening
- LLM-generated text is identifiable via the removal of colloquialisms and foreign language nuances present in the original human works. The LLM acts as a standardizer, often rewriting colloquial speech (e.g., "'em" to "them") or removing foreign phrases (e.g., French), thereby stripping away features that act as strong indicators of the "Human" class.

## Foundational Learning

- **Bag-of-Words (Unigram) Modeling**
  - Why needed here: The study demonstrates that high accuracy (0.98) is achievable using simple unigram features alone, bypassing the need for complex transformer-based detectors.
  - Quick check question: Can you explain why the frequency of the word "exited" might be more informative for classification than the frequency of "the" in this specific dataset?

- **Linear Classifiers & Interpretability**
  - Why needed here: The authors prioritize a linear model over deep learning to provide *exact* explanations (weights) for classifications, contrasting with "black-box" methods like SHAP or LIME.
  - Quick check question: In a linear classifier distinguishing Class 0 (Human) from Class 1 (LLM), would a positive weight for the token "today" vs "to-day" indicate Class 0 or Class 1?

- **Shannon Entropy**
  - Why needed here: This metric is used to quantify the "randomness" or variety in the text, providing a theoretical basis for why LLM text is statistically distinct (higher entropy/variance).
  - Quick check question: If Class 1 (LLM) has higher entropy than Class 0 (Human), does it mean the LLM uses *more* distinct words or *fewer* distinct words per unit of text?

## Architecture Onboarding

- **Component map:**
  Raw text (Project Gutenberg) → Chunking (100-word excerpts) → OpenAI API (GPT-4.1) → Rewritten Excerpt → Custom Inclusive Tokenizer → Bag-of-Words (Unigrams) → Frequency Counts → Linear Classifier (Training with Holdout Validation) → Weight Inspection & Entropy Calculation

- **Critical path:** The **Custom Tokenizer** is the most critical preprocessing step. The paper emphasizes that standard tokenizers often introduce "artifacts" (like distinct dash types) that allow classifiers to "cheat" based on formatting rather than linguistic content.

- **Design tradeoffs:** The study trades the potentially higher raw accuracy of complex neural networks for the *interpretability* of a linear model. They argue that for high-stakes detection (e.g., plagiarism), knowing *why* a text was flagged is worth a potential (though unobserved here) drop in accuracy.

- **Failure signatures:**
  - **Artifact Leakage:** If the model achieves near-perfect accuracy solely on punctuation quirks (e.g., em-dash vs hyphen), it will fail on real-world data where punctuation is normalized.
  - **Overfitting to Author:** If the model learns specific names (e.g., "Poirot") rather than stylistic features, it will fail on novels by other authors.

- **First 3 experiments:**
  1. **Artifact Ablation:** Run classification using only punctuation tokens. If accuracy is high, the data cleaning failed; if low, proceed with word-based analysis.
  2. **Feature Weight Inspection:** Train the linear model and extract the top 10 positive (LLM) and negative (Human) weighted features. Manually verify they correspond to linguistic traits (e.g., "exited", "remarked") rather than formatting.
  3. **Entropy Verification:** Calculate the Shannon entropy of a random sample of generated vs. human texts to confirm the hypothesis that LLMs introduce higher lexical variety (higher entropy).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the high detection accuracy persist when LLMs generate text from scratch (outlines or continuations) rather than rewriting existing excerpts?
- Basis in paper: [explicit] The authors note the "rewriting" methodology may artificially increase variation and state "further work is required to... establish the effects of different generation methods."
- Why unresolved: The study exclusively used GPT-4.1 to rewrite excerpts, creating a dependency on source text that may not exist in autonomous generation.
- What evidence would resolve it: Evaluating the classifier on creative writing generated entirely from prompt outlines without source text.

### Open Question 2
- Question: Do the identified linguistic markers (e.g., synonym variety, Americanisms) generalize to other LLMs and genres outside of detective fiction?
- Basis in paper: [explicit] The conclusion lists "analyzing text produced by different LLMs, in various genres" as a future investigation line.
- Why unresolved: The dataset is limited to British detective novels from the 1920s and a single OpenAI model.
- What evidence would resolve it: Replicating the analysis on diverse genres (e.g., sci-fi) and models (e.g., Claude, Gemini).

### Open Question 3
- Question: Can the explicit knowledge of these unigram features be utilized by malicious actors to evade detection?
- Basis in paper: [explicit] The introduction asks if explaining the classification "assist[s] bad actors."
- Why unresolved: The paper posits robustness theoretically but does not conduct adversarial testing to see if masking identified features lowers accuracy.
- What evidence would resolve it: An experiment where actors specifically adjust identified features (e.g., removing "exited," adding "said") to bypass the classifier.

## Limitations

- **Temporal and Genre Specificity:** The high accuracy (98%) is based on a very specific dataset: early 20th century British detective fiction. Performance on contemporary fiction or other genres remains untested and likely lower.
- **Mechanistic Attribution:** The relative contribution of each identified mechanism (lexical diversity, temporal drift, stylistic flattening) to overall accuracy is not quantified.
- **Model Generalization:** The study uses a linear classifier, which may achieve lower accuracy than more complex models, especially on out-of-domain data.

## Confidence

- **High Confidence:** The core finding that a simple linear classifier can achieve high accuracy (98%) in distinguishing LLM-generated text from human-written text in this specific dataset.
- **Medium Confidence:** The identification of the three mechanisms as the basis for classification, though their relative importance is not established.
- **Low Confidence:** The claim of "robust classification resistant to manipulation" due to its extrapolation beyond tested conditions.

## Next Checks

1. **Out-of-Domain Validation:** Test the trained model on contemporary fiction from diverse authors and genres to measure accuracy drop and validate generalizability.

2. **Mechanism Ablation Study:** Retrain the classifier after removing features associated with each proposed mechanism to quantify their relative contribution to accuracy.

3. **Adversarial Robustness Test:** Evaluate performance against human-edited LLM text designed to evade detection by mimicking the style of the human corpus.