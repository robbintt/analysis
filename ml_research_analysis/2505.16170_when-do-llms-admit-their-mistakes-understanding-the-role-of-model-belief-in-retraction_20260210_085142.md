---
ver: rpa2
title: When Do LLMs Admit Their Mistakes? Understanding The Role Of Model Belief In
  Retraction
arxiv_id: '2505.16170'
source_url: https://arxiv.org/abs/2505.16170
tags:
- retraction
- belief
- steering
- answer
- llama3
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study examines when and why large language models (LLMs) retract
  their incorrect answers, focusing on spontaneous acknowledgment of mistakes. Using
  model-specific testbeds built from knowledge-based question answering tasks (WIKIDATA
  and CELEBRITY), researchers found that while LLMs can retract, they rarely do so
  even when verification questions show they know the answer is wrong.
---

# When Do LLMs Admit Their Mistakes? Understanding The Role Of Model Belief In Retraction

## Quick Facts
- arXiv ID: 2505.16170
- Source URL: https://arxiv.org/abs/2505.16170
- Reference count: 40
- Large language models retract incorrect answers rarely, but spontaneous retraction is driven by internal belief states during generation

## Executive Summary
This study examines when and why large language models (LLMs) spontaneously retract incorrect answers. Using knowledge-based question answering tasks (WIKIDATA and CELEBRITY), researchers found that retraction behavior is driven by the model's momentary internal belief—its real-time judgment about correctness during generation. Linear probes trained on external true-false datasets revealed that these belief signals better predict retraction than factual correctness. Crucially, steering experiments confirmed causality: negative belief steering strongly promotes retraction by encouraging verification attempts and altering attention dynamics, while positive belief steering suppresses retraction.

## Method Summary
The researchers constructed model-specific testbeds from knowledge-based question answering tasks, generating answers and using verification questions to determine when models knew their answers were wrong. They trained linear probes on hidden states from external true/false datasets (UTQA) to capture belief signals at the last token of generated answers. Activation steering was then applied using difference-in-means vectors from probe training data to intervene on belief states at the answer's final token. Supervised fine-tuning experiments with LoRA appended retraction phrases to incorrect answers, demonstrating improved retraction performance when internal belief aligns with factual correctness.

## Key Results
- Retraction behavior is driven by momentary internal belief during generation, not parametric knowledge
- Belief probes predict retraction with ~0.85 AUROC in middle layers, outperforming factual correctness probes (~0.55-0.70)
- Negative belief steering increases retraction rates to >70%, while positive steering suppresses retraction to near zero
- Attention value vectors, not weights, are the primary conduit through which belief influences retraction decisions

## Why This Works (Mechanism)

### Mechanism 1: Internal Belief as Retraction Predictor
- Claim: A model's momentary internal belief—its real-time assessment of answer correctness during generation—reliably predicts whether it will retract, independent of its parametric knowledge.
- Mechanism: Linear probes trained on external true/false datasets capture a "belief direction" in hidden states at the last token of generated answers. Low belief scores correlate with subsequent retraction behavior, while high scores correlate with commitment to answers—even when factually wrong.
- Core assumption: The probe direction captures the model's internal judgment state rather than objective truth; belief can diverge from parametric knowledge stored elsewhere.
- Evidence anchors: [abstract] "A model retracts only when it 'believes' its answers to be incorrect during generation"; [section 4.1] Figure 2 shows AUROC for predicting retraction reaches ~0.85 vs ~0.55-0.70 for factual correctness.

### Mechanism 2: Causal Control Through Activation Steering
- Claim: Intervening on the belief direction causally controls retraction behavior in both directions.
- Mechanism: The difference-in-means vector from UTQA training data defines a steering direction. Adding this vector to activations at the answer's last token shifts the model's belief state and consequently its retraction behavior.
- Core assumption: The belief direction is sufficiently disentangled from other functional circuits that steering minimally disrupts natural generation quality.
- Evidence anchors: [abstract] "Steering experiments demonstrate that this belief causally drives retraction"; [section 4.2] Figure 3 shows retraction rate shifts from ~10-25% to >70% (negative) or <5% (positive).

### Mechanism 3: Attention Value Vectors as Primary Conduit
- Claim: Belief influences retraction primarily by modifying attention value vectors (what is attended to and how it's represented), not just attention weights (where attention flows).
- Mechanism: Negative belief steering increases attention to answer tokens and alters the representations extracted from those tokens. Patching attention value vectors from steered models into unsteered models reproduces most of the steering effect.
- Core assumption: The retraction decision circuit reads from attention outputs rather than directly from hidden states; belief modifies what information reaches downstream decision layers.
- Evidence anchors: [section 5.2] "Patching attention value vectors... restores more the retraction behavior observed with full steering"; Table 5: Patch V achieves recall 0.97 vs Full Steer 0.97 for negative steering.

## Foundational Learning

- Concept: **Activation Steering / Representation Engineering**
  - Why needed here: Core intervention method. Involves computing direction vectors from contrastive examples and adding scaled vectors to intermediate activations without retraining.
  - Quick check question: Given hidden states h⁺ for correct answers and h⁻ for incorrect answers, what vector would you add to increase a model's belief in its answer's correctness?

- Concept: **Linear Probing of Hidden States**
  - Why needed here: Diagnostic tool for detecting internal model states. A linear classifier trained on hidden states can reveal what information is linearly accessible at each layer.
  - Quick check question: If a probe trained on dataset A achieves high accuracy on dataset B but low accuracy on dataset C, what does this suggest about the probed dimension?

- Concept: **Attention Weights vs. Attention Value Vectors**
  - Why needed here: Critical distinction for mechanistic analysis. Weights = "where to look" (softmax over positions); Value vectors = "what is extracted" (weighted sum of value projections).
  - Quick check question: If you want to change *how* information from earlier tokens is used (not just *which* tokens), which component should you modify?

## Architecture Onboarding

- Component map: Input Query → Model Generation → [Answer Token: Last Position] → Hidden State (layer l) → Belief Probe (linear) → Low belief? → Continue Gen → Attention Heads → Attention Values → Retraction Decision
- Critical path:
  1. Extract hidden state at answer's last token position
  2. Compute belief score via trained linear probe (middle layers most predictive: 6-14 for Llama3.1-8B)
  3. Low belief → increased continuation probability → attention to answer tokens → retraction generation
- Design tradeoffs:
  - Steering strength (α): Higher values increase effect but risk unnatural generation
  - Layer selection: Middle layers (6-20) balance effectiveness vs. disruption; exact range model-dependent
  - In-distribution vs. out-of-distribution steering vectors: UTQA-derived vectors generalize better than dataset-specific retraction directions
- Failure signatures:
  - Oversteering produces unnatural phrasings (e.g., appending "nor" or "Nope" mechanically)
  - Belief probe AUROC near 0.5 suggests wrong layer or probe training issue
  - Steering doesn't generalize across datasets suggests overfitting to training distribution
- First 3 experiments:
  1. Probe validation: Train linear probe on UTQA, evaluate on held-out set, then apply to model's own generations
  2. Steering sanity check: Apply negative belief steering to 20 incorrect examples, inspect naturalness
  3. Patching ablation: Implement attention weight vs. value vector patching for 50 examples

## Open Questions the Paper Calls Out

- **Cross-task generalization**: Does the causal relationship between internal belief and retraction generalize to long-form generation and complex reasoning tasks?
- **Training recipe effects**: How do specific training recipes determine the architectural location (layers) of the belief-retraction mechanism?
- **Automated error detection**: Can belief steering be applied effectively in reasoning tasks without external or manual identification of the specific error location?

## Limitations
- Reliance on external probe datasets (UTQA) that may not fully capture nuanced belief representations within specific LLM architectures
- Relatively narrow task scope—knowledge-based QA with verifiable answers may not extend to open-ended generation or ambiguous domains
- Focus on spontaneous retraction without external pressure leaves unclear how models behave under adversarial prompting or reputational costs

## Confidence
- High confidence: The mechanistic finding that belief signals predict retraction better than factual correctness
- Medium confidence: The specific attention mechanism (value vectors vs weights) through which belief influences retraction
- Medium confidence: The generalization of belief direction across datasets from UTQA to WIKIDATA/CELEBRITY

## Next Checks
1. **Probe Ablation Study**: Train belief probes on multiple probe datasets and compare their predictive power for retraction across diverse tasks
2. **Attention Mechanism Dissection**: Systematically vary attention patching—test weights, values, key/value projections separately—to isolate which components carry the primary belief-to-retraction signal
3. **Cross-Task Generalization**: Apply the belief steering framework to non-QA tasks: code generation, mathematical reasoning, and open-ended generation to test mechanism extension beyond knowledge verification