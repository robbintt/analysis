---
ver: rpa2
title: 'Conservative Bias in Multi-Teacher Learning: Why Agents Prefer Low-Reward
  Advisors'
arxiv_id: '2512.17180'
source_url: https://arxiv.org/abs/2512.17180
tags:
- learning
- teacher
- reward
- teachers
- drift
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper reveals an unexpected phenomenon in interactive reinforcement
  learning: when given a choice between teachers with different reward structures,
  learning agents overwhelmingly prefer conservative, low-reward teachers (93.16%
  selection rate) over those offering 20x higher rewards. Through 1,250 experimental
  runs in navigation tasks with multiple expert teachers, we discovered that agents
  systematically choose the lowest-reward teacher, prioritizing consistency over optimality.'
---

# Conservative Bias in Multi-Teacher Learning: Why Agents Prefer Low-Reward Advisors

## Quick Facts
- arXiv ID: 2512.17180
- Source URL: https://arxiv.org/abs/2512.17180
- Authors: Maher Mesto; Francisco Cruz
- Reference count: 9
- Primary result: Agents select conservative, low-reward teachers 93.16% of the time despite 20x higher rewards available

## Executive Summary
This paper reveals an unexpected phenomenon in interactive reinforcement learning: when given a choice between teachers with different reward structures, learning agents overwhelmingly prefer conservative, low-reward teachers (93.16% selection rate) over those offering 20x higher rewards. Through 1,250 experimental runs in navigation tasks with multiple expert teachers, we discovered that agents systematically choose the lowest-reward teacher, prioritizing consistency over optimality. Critical performance thresholds exist at teacher availability ρ≥ 0.6 and accuracy ω≥ 0.6, below which the framework fails catastrophically. The framework achieves 159% improvement over baseline Q-learning under concept drift.

## Method Summary
The framework uses tabular Q-learning agents (α=0.1, γ=0.9, ε decaying 0.2→0.01) in a 10×10 GridWorld environment. Five pre-trained teacher agents each specialize in different goal positions, with configurable availability (ρ) and accuracy (ω) parameters. Students select teachers via either goal similarity (for drift adaptation) or performance-based cumulative reward tracking (for bias experiments). Teachers provide action advice probabilistically based on their availability, returning either the optimal action (with probability ω) or the worst action (with probability 1-ω). The system tests concept drift by changing goals every τ=10 episodes.

## Key Results
- Agents select conservative, low-reward teachers 93.16% of the time despite 20x higher rewards available
- Critical performance thresholds exist at teacher availability ρ≥ 0.6 and accuracy ω≥ 0.6
- The framework achieves 159% improvement over baseline Q-learning under concept drift
- Moderate goal perception uncertainty (σ≈1.0) improves performance by increasing exploration diversity

## Why This Works (Mechanism)

### Mechanism 1: Cumulative Reward Tracking Creates Risk Aversion
Agents prefer teachers who minimize early penalties over those offering higher long-term rewards. During early learning, failures dominate. Teachers with smaller step penalties accumulate higher cumulative scores, triggering selection via Equation 4. Once selected, positive feedback reinforces the preference—selected teachers accumulate more positive experiences, increasing future selection probability.

### Mechanism 2: Phase Transitions at Critical Thresholds
Multi-teacher frameworks require minimum availability (ρ≥0.6) and accuracy (ω≥0.6) to provide net-positive learning signal. Below ω≥0.6, incorrect advice (returning worst action) occurs >40% of the time, systematically corrupting Q-value updates. Below ρ≥0.6, advice is too sparse to shape policy before the agent's own exploration dominates.

### Mechanism 3: Moderate Uncertainty as Implicit Regularization
Moderate goal perception uncertainty (σ≈1.0) improves performance by increasing exploration diversity without exceeding the accuracy threshold. Teachers with noisy goal perception provide varied advice, forcing broader state-action exploration. This prevents premature convergence to locally optimal policies.

## Foundational Learning

- **Q-Learning with ϵ-greedy exploration**: All agents use tabular Q-learning with identical hyperparameters (α=0.1, γ=0.9, ϵ decaying 0.2→0.01). Understanding the update rule is prerequisite to interpreting how advice shapes value functions.
  - Quick check: Given α=0.1, how many episodes of incorrect advice (reward=-10 for timeout) are needed to significantly shift a Q-value from an initial correct estimate of +8?

- **Concept drift in reinforcement learning**: The framework specifically addresses non-stationary environments where goals change every τ=10 episodes. Students must recognize drift and adapt with teacher assistance.
  - Quick check: In a cyclic drift pattern with 5 goal positions, what happens to a standard Q-learning agent's policy after 1,000 episodes?

- **Interactive reinforcement learning (IRL) with action advising**: Teachers provide action-level advice (not reward shaping). The student must integrate this with its own exploration.
  - Quick check: If ρ=0.8 and ω=0.7, what is the probability the student receives correct advice on any given timestep?

## Architecture Onboarding

- **Component map**: Student Agent <- Selection Mechanism <- Teacher Agents (5x) <- Environment
- **Critical path**: 1) Train 5 specialist teachers (1,000 episodes each) on respective goal positions; 2) Initialize student with empty Q-table; 3) For each episode: check for drift → select teacher → execute action selection (Equation 2) → update Q-values; 4) Track cumulative rewards per teacher for selection decisions
- **Design tradeoffs**: Goal similarity vs. performance-based selection; high ρ/ω vs. resource cost; conservative vs. aggressive teachers
- **Failure signatures**: Below-threshold performance if ρ<0.6 or ω<0.6; no drift recovery if selection strategy wrong; teacher collapse if one dominates >95% selection in bias experiments
- **First 3 experiments**: 1) Baseline validation: Run Q-learning without teachers under concept drift (τ=10, 5 goal cycle); 2) Threshold mapping: Systematically vary ρ and ω (25 configurations, 50 runs each); 3) Bias detection: Static environment with 5 teachers with different reward structures

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does conservative bias persist in continuous control domains with policy gradient methods (e.g., PPO, TRPO)?
- Basis in paper: [explicit] The authors state: "A critical next step is to investigate whether conservative bias persists in continuous control domains using policy gradient architectures (e.g., PPO, TRPO). This would establish the generality of our findings beyond value-based methods in discrete spaces."
- Why unresolved: All experiments use tabular Q-learning in discrete GridWorld; policy gradient methods have different optimization dynamics that may alter selection behavior.
- What evidence would resolve it: Replicate the multi-teacher framework in continuous control benchmarks (e.g., MuJoCo) using PPO/TRPO agents and measure teacher selection distributions.

### Open Question 2
- Question: What theoretical mechanisms explain the convergence of both critical thresholds at ρ, ω ≥ 0.6?
- Basis in paper: [explicit] Section 6.2 notes: "The convergence of both thresholds at 0.6 hints at a deeper symmetry in how frequency and quality of information affect learning, a pattern that invites further theoretical investigation."
- Why unresolved: The paper provides plausible mechanistic explanations but no formal derivation; the 0.6 threshold is empirically observed without theoretical grounding.
- What evidence would resolve it: Formal analysis linking learning rate α, discount factor γ, and advice error rates to derive stability boundaries; or systematic ablation across different α, γ values.

### Open Question 3
- Question: How does conservative bias manifest when teachers co-evolve or adapt alongside the student agent?
- Basis in paper: [explicit] Future directions state: "Extending to teachers that co-evolve with the agent or to novel goals not seen during training would test robustness and scalability."
- Why unresolved: All teachers are pre-trained with fixed Q-tables; their advice never changes during student learning.
- What evidence would resolve it: Experiments with teachers that continue learning during student training, measuring whether selection preferences stabilize or shift dynamically.

## Limitations
- Conservative bias mechanism depends critically on incorrect advice returning worst actions rather than random actions
- Exact mapping between teacher reward profiles and assigned starting positions/exploration rates remains unspecified
- Framework's behavior in the narrow band between 0.5-0.6 for both thresholds is not characterized

## Confidence
- **High confidence**: The existence of phase transitions at ρ≥0.6 and ω≥0.6 (supported by systematic threshold testing across 25 configurations)
- **Medium confidence**: The 93.16% conservative teacher selection rate (experimentally validated but sensitive to implementation details)
- **Low confidence**: The proposed mechanism that cumulative reward tracking creates risk aversion (largely theoretical inference from behavioral data)

## Next Checks
1. **Parameter sensitivity analysis**: Systematically test the effect of varying learning rate α and discount factor γ on the conservative bias magnitude to determine if the phenomenon persists across hyperparameter settings.

2. **Mechanism isolation**: Implement a variant where incorrect advice returns random actions instead of worst actions to verify that the asymmetric damage model is necessary for the conservative bias.

3. **Generalization testing**: Validate whether the conservative bias emerges in continuous action spaces (e.g., cart-pole or mountain car) rather than just grid-world navigation, to assess the mechanism's broader applicability.