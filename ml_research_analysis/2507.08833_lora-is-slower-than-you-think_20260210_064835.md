---
ver: rpa2
title: LoRA Is Slower Than You Think
arxiv_id: '2507.08833'
source_url: https://arxiv.org/abs/2507.08833
tags:
- lora
- fine-tuning
- paca
- training
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the assumption that LoRA always speeds up
  LLM fine-tuning. While LoRA reduces memory usage by updating low-rank matrices instead
  of full weight matrices, experiments show it can be slower than full fine-tuning
  due to GPU processing overhead from adapter modules.
---

# LoRA Is Slower Than You Think

## Quick Facts
- arXiv ID: 2507.08833
- Source URL: https://arxiv.org/abs/2507.08833
- Authors: Seokmin Ko
- Reference count: 17
- This paper challenges the assumption that LoRA always speeds up LLM fine-tuning

## Executive Summary
This paper demonstrates that LoRA (Low-Rank Adaptation), commonly assumed to accelerate LLM fine-tuning, can actually be slower than full fine-tuning due to GPU processing overhead from adapter modules. Through experiments on LLaMA2-7B with the MMLU benchmark, the author shows that LoRA's theoretical computational savings are negated by sequential GPU kernel processing. To address this, the paper proposes selective non-adaptive fine-tuning using Partial Connection Adaptation (PaCA) on only the most task-relevant upper layers, achieving comparable accuracy to LoRA while reducing training time by approximately 2 hours and 49 minutes.

## Method Summary
The study compares three fine-tuning approaches: standard LoRA (rank 8, all layers), PaCA (all layers, rank 16), and Three Quarters PaCA (top 24 of 32 layers, rank 24). All methods use 16-bit mixed precision, AdamW optimizer, cosine learning rate scheduler, batch size 8 with gradient accumulation 4, and sequence length 512. The Three Quarters PaCA variant freezes the lower 8 layers and applies PaCA only to the upper 24 layers, with column density doubled to maintain parameter count parity with LoRA. Training runs for 1 epoch on LLaMA2-7B with MMLU evaluation.

## Key Results
- LoRA can be slower than full fine-tuning due to GPU kernel overhead from sequential adapter processing
- Three Quarters PaCA achieves 52.15% MMLU accuracy vs LoRA's 52.02%, reducing training time by 2 hours 2 minutes compared to LoRA
- Upper Half PaCA (16 of 32 layers) drops to 47.10% accuracy, demonstrating importance of sufficient layer selection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LoRA introduces GPU kernel overhead that may negate its theoretical computational savings.
- Mechanism: While LoRA theoretically reduces FLOPs by factorizing weight updates into low-rank matrices, the separate adapter operations create sequential processing bottlenecks on GPUs, preventing optimal kernel fusion.
- Core assumption: The latency cost of launching and coordinating separate kernels for adapters exceeds the arithmetic speedup gained from low-rank factorization on modern GPU hardware.
- Evidence anchors: Abstract states LoRA "can be slower than full fine-tuning due to GPU processing overhead"; Introduction notes GPUs are specialized in processing kernels at once, while LoRA layers are processed sequentially.

### Mechanism 2
- Claim: Selective layer freezing reduces backpropagation overhead without degrading task performance.
- Mechanism: By freezing the lower L-K layers and applying adaptation only to the upper K layers, the system eliminates gradient computation and storage for the majority of the network.
- Core assumption: Task-specific knowledge adaptation is primarily localized in the upper layers of the transformer architecture, requiring minimal updates to lower-level features.
- Evidence anchors: Section 3 describes freezing lower layers and applying PaCA to upper layers; Table 2 shows Three Quarters PaCA achieving 52.02% accuracy vs LoRA's 52.15%.

### Mechanism 3
- Claim: In-place column selection (PaCA) is structurally faster than adding adapter branches.
- Mechanism: PaCA modifies specific columns of the existing weight matrix using a binary mask, rather than computing a separate side path, maintaining the structure of a single weight matrix operation during the forward pass.
- Core assumption: The hardware handles masked updates or sparse gradients more efficiently than it handles the additional arithmetic and memory access patterns of separate low-rank adapter modules.
- Evidence anchors: Section 2.2 explains PaCA divides the weight matrix into blocks with learnable binary masks; Table 2 shows PaCA is faster than LoRA.

## Foundational Learning

- **GPU Kernel Fusion**
  - Why needed here: To understand why fewer parameters (LoRA) can be slower than more parameters (Full-FT) due to kernel launch overhead and memory access patterns.
  - Quick check question: Why might executing two small matrix multiplications sequentially be slower than one large matrix multiplication on a GPU?

- **Parameter-Efficient Fine-Tuning (PEFT)**
  - Why needed here: Contextualizes LoRA and PaCA as methods to adapt models while updating only a tiny fraction of weights.
  - Quick check question: If a model has 7B parameters and rank r=8, how does the memory requirement differ between storing gradients for Full Fine-Tuning vs. LoRA?

- **Layer-wise Relevance**
  - Why needed here: The proposed method relies on the heuristic that upper layers are more critical for task adaptation than lower layers.
  - Quick check question: In a Transformer, which layers typically capture high-level semantic concepts vs. low-level syntactic patterns?

## Architecture Onboarding

- Component map: LLaMA2-7B (32 Layers) -> Selector (determines K) -> PaCA modules (Binary Mask M + Weight W updates) -> AdamW Optimizer

- Critical path:
  1. Configure Selection: Set K to define active layer range (e.g., "Three Quarters" implies last 24 layers)
  2. Initialize Mask: Generate binary mask M for target modules with appropriate column density
  3. Forward Pass: Standard matrix multiplication with PaCA logic involving standard weights
  4. Backward Pass: Mask gradients so only specific columns in specific layers receive updates

- Design tradeoffs:
  - Speed vs. Modularity: PaCA modifies main weights requiring distinct checkpointing, while LoRA keeps original weights frozen and swaps small adapters
  - Rank vs. Width: The paper doubles PaCA columns to match LoRA parameter count, assuming column updates are equivalent in expressivity to low-rank updates

- Failure signatures:
  - Accuracy Collapse: "Upper Half PaCA" dropped to 47.10% accuracy; increase layer selection scope if accuracy degrades
  - Speed Stagnation: Check if gradients are accidentally computed for frozen layers or mask density is too high

- First 3 experiments:
  1. Baseline Timing: Run Table 1 experiment locally (LoRA vs Full-FT) to quantify GPU overhead on your hardware
  2. Layer Ablation: Compare "Upper Half" vs "Three Quarters" PaCA to verify trade-off between layer count and MMLU accuracy
  3. Mask Density Calibration: Run PaCA with varying columns (16 vs 24 vs 32) to find minimum width for LoRA-level performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can automated layer-selection mechanisms outperform the manual heuristic of selecting the top K layers?
- Basis in paper: The conclusion explicitly states "Future work will explore automated layer-selection mechanisms."
- Why unresolved: The current study manually tested configurations like "Upper Half" and "Three Quarters," finding the former caused significant accuracy drops while the latter succeeded.
- What evidence would resolve it: A dynamic selection algorithm that identifies task-critical layers more precisely than a fixed upper-layer cutoff, maintaining accuracy while maximizing speed.

### Open Question 2
- Question: Can the selective non-adaptive fine-tuning approach be effectively combined with other PEFT techniques?
- Basis in paper: The conclusion proposes "the extension of our method to other PEFT techniques."
- Why unresolved: The paper focuses exclusively on PaCA and LoRA; it's unclear if removing adapter modules and freezing lower layers works for methods like prefix-tuning or prompt-tuning.
- What evidence would resolve it: Experiments applying selective layer freezing to other adapters, demonstrating consistent speed improvements and memory reductions.

### Open Question 3
- Question: Do the training speed benefits of adapter-free fine-tuning persist across model architectures larger than 7B parameters?
- Basis in paper: The empirical validation is limited to LLaMA2-7B and GPT-2 variants, while the introduction notes LLMs scale to 175B+.
- Why unresolved: The overhead of adapter modules may behave differently relative to standard computation as weight matrix dimensions increase significantly.
- What evidence would resolve it: Benchmarks on larger models (30B–70B parameters) showing selective PaCA retains its wall-clock time advantage over LoRA.

## Limitations

- The claimed GPU kernel overhead disadvantage of LoRA may be hardware-specific, as timing comparisons were performed only on A6000 GPUs
- The mask learning mechanism for PaCA is referenced from external work without implementation details, creating potential reproduction barriers
- The optimal layer selection boundary (24/32 layers) is benchmark-specific to MMLU and may not generalize to other tasks

## Confidence

- **High Confidence**: The mathematical framework for LoRA's computational cost (O(dr²) vs O(d²)) and the claim that lower layers capture more general features are well-established
- **Medium Confidence**: The specific timing measurements showing LoRA being slower than full fine-tuning are hardware-dependent and may not generalize
- **Low Confidence**: The mask density calibration (24 columns per layer) and its relationship to LoRA's rank-8 parameterization requires careful verification

## Next Checks

1. **Hardware Replication**: Run the timing comparison (LoRA vs Full-FT) on multiple GPU architectures (A100, H100, consumer GPUs) to verify the kernel overhead claim is not hardware-specific
2. **Layer Transferability**: Test the selective freezing approach on a different benchmark (SuperGLUE or domain-specific tasks) to assess whether upper-layer relevance generalizes beyond MMLU
3. **Mask Density Sweep**: Systematically vary the number of PaCA columns (16, 24, 32) while keeping layer selection fixed to find the minimum parameter count achieving LoRA-level accuracy