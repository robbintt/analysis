---
ver: rpa2
title: ABHINAYA -- A System for Speech Emotion Recognition In Naturalistic Conditions
  Challenge
arxiv_id: '2505.18217'
source_url: https://arxiv.org/abs/2505.18217
tags:
- speech
- emotion
- challenge
- loss
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The Abhinaya system addresses the challenge of speech emotion
  recognition (SER) in naturalistic conditions by leveraging large language models
  (LLMs) and speech large language models (SLLMs). The approach combines five models:
  two speech-based (using WavLM-Large and SALMONN SLLMs), two text-based (using LLaMA
  models), and one multimodal model (SALMONN with speech and text).'
---

# ABHINAYA -- A System for Speech Emotion Recognition In Naturalistic Conditions Challenge

## Quick Facts
- arXiv ID: 2505.18217
- Source URL: https://arxiv.org/abs/2505.18217
- Reference count: 0
- Primary result: 4th place out of 166 submissions in Interspeech 2025 Naturalistic SER Challenge with 44.02% macro-F1

## Executive Summary
The Abhinaya system addresses the challenge of speech emotion recognition (SER) in naturalistic conditions by leveraging large language models (LLMs) and speech large language models (SLLMs). The approach combines five models: two speech-based (using WavLM-Large and SALMONN SLLMs), two text-based (using LLaMA models), and one multimodal model (SALMONN with speech and text). To handle class imbalance, the system explores various loss functions, including weighted cross-entropy, weighted focal loss, and vector scaling loss. The models are fine-tuned using self-supervised learning representations and large language models, with attentive pooling for utterance-level aggregation. The system employs majority voting for ensemble predictions, achieving state-of-the-art performance. On the Interspeech 2025 Naturalistic SER Challenge test set, the Abhinaya system ranked 4th out of 166 submissions, achieving a macro-F1 score of 44.02%, surpassing the baseline by 33.68% relative improvement. The approach demonstrates the effectiveness of combining speech-only, text-only, and speech-text models with specialized loss functions for handling imbalanced naturalistic SER datasets.

## Method Summary
The Abhinaya system employs an ensemble of five models: two speech-based (WavLM-Large and SALMONN-13B), two text-based (LLaMA-3.3-70B-Instruct and LLaMA-3.1-8B), and one multimodal (SALMONN-7B with speech-text input). All models use attentive statistics pooling for utterance-level aggregation and are fine-tuned with parameter-efficient methods (LoRA). The system uses different loss functions optimized per modality: weighted focal loss for speech models and vector scaling loss for text and speech-text models. Majority voting is used for ensemble predictions, with the strongest individual model (SALMONN-13B) serving as tiebreaker. The system processes MSP-PODCAST dataset audio (10-second max) with Whisper-large-v3 ASR transcripts for text-based and multimodal models.

## Key Results
- Ranked 4th out of 166 submissions in Interspeech 2025 Naturalistic SER Challenge
- Achieved 44.02% macro-F1 score on test set
- Surpassed baseline by 33.68% relative improvement
- Majority vote ensemble achieved 24.56% relative gain over best individual model (S2)
- Performance on rare classes (fear, contempt, disgust) remains challenging (F1 < 30% for speech models)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multi-modal ensembling (speech-only + text-only + speech-text) yields performance gains exceeding any single modality.
- **Mechanism:** Speech encoders capture prosodic and acoustic cues (e.g., pitch, rhythm), text encoders capture lexical and semantic content, and speech-text joint models learn cross-modal correlations. Majority voting aggregates complementary predictions, with the best single model (S2) used as a tiebreaker.
- **Core assumption:** Emotion manifestations in speech and text provide non-redundant information that can be integrated at the decision level.
- **Evidence anchors:**
  - [abstract] "integrating speech-based, text-based, and speech-text models"
  - [section 4.3] "A majority vote ensemble of the five models achieves a 24.56% relative gain over the best individual model (S2) on the test set."
  - [corpus] Weak/missing — neighboring papers describe similar multi-modal fusion but do not provide causal analysis.
- **Break condition:** If modalities are highly correlated (e.g., ASR errors remove lexical signal), ensembling yields diminishing returns; computational cost increases without accuracy gain.

### Mechanism 2
- **Claim:** Loss function choice interacts with model type and initial representation quality.
- **Mechanism:** Weighted focal loss (WFL) dynamically emphasizes hard, low-confidence samples—benefiting speech models whose zero-shot representations are less discriminative. Vector scaling (VS) loss applies class-wise temperature/bias adjustments, which the paper hypothesizes works better when initial class separation is already reasonable (as in text models).
- **Core assumption:** The effectiveness of class-conditional vs. sample-conditional reweighting depends on the initial feature space geometry.
- **Evidence anchors:**
  - [section 3.4] Defines WFL with γ≥0 and VS loss with class-dependent scaling.
  - [section 4.5] "Speech-only models (S1, S2) benefit most from weighted focal loss... VS loss is effective for the text-based (T2) and speech-text (ST1) models."
  - [corpus] Neighbor paper "Multi-Loss Learning for SER with Energy-Adaptive Mixup" explores multi-loss frameworks but does not isolate this interaction.
- **Break condition:** If the initial representations are near-random, loss reweighting may amplify noise rather than signal; hyperparameter sensitivity increases.

### Mechanism 3
- **Claim:** Fine-tuning SLLMs via LoRA + attentive pooling improves over zero-shot by adapting to emotion-specific distributions while preserving pre-trained knowledge.
- **Mechanism:** The Q-Former and LoRA adapters are trained to align speech representations with emotion categories; the frozen backbone prevents catastrophic forgetting. Attentive statistics pooling (weighted mean + std) aggregates frame-level features into utterance-level embeddings.
- **Core assumption:** Emotion-relevant information is distributed across transformer layers and can be pooled without sequential modeling loss.
- **Evidence anchors:**
  - [section 3.1.2] "speech encoder and LLaMA weights remain frozen, while only the Q-former and the low-rank adapters (LoRA) in the LLaMA module are trained"
  - [section 4.3] "its zero-shot validation performance is only 18.63%, hence fine-tuning significantly improves the performance" to 37.68% for S2.
  - [corpus] No direct causal validation in neighbors; SALMONN and Q-Former usage is reported descriptively.
- **Break condition:** If emotion cues require fine-grained temporal dynamics not captured by utterance-level pooling, performance plateaus; longer sequences increase memory without proportional gain.

## Foundational Learning

- **Concept:** Self-supervised speech representations (WavLM, HuBERT, wav2vec 2.0)
  - **Why needed here:** The S1 model builds on WavLM-Large; understanding contrastive/masked prediction pre-training helps explain why frozen convolutional features + fine-tuned transformers work.
  - **Quick check question:** Can you explain why masked prediction objectives yield representations transferable to paralinguistic tasks like SER?

- **Concept:** Parameter-efficient fine-tuning (LoRA, adapters)
  - **Why needed here:** S2, T2, and ST1 rely on LoRA (r=8, α=32) to train with limited GPU memory; misconfiguring rank or alpha can degrade or fail to adapt.
  - **Quick check question:** What happens to gradient flow if LoRA rank is set too low relative to the intrinsic dimensionality of the adaptation task?

- **Concept:** Class imbalance mitigation (weighted loss, focal loss, logit adjustment)
  - **Why needed here:** The training set has ~26:1 ratio between neutral and fear classes; applying standard cross-entropy leads to majority-class bias.
  - **Quick check question:** For a 10-class problem with class frequencies [5000, 1000, 500, 200, 100, 50, 30, 20, 10, 5], what weighting scheme would you start with, and what are its failure modes?

## Architecture Onboarding

- **Component map:**
  - S1 (WavLM-Large-317M): frozen conv encoder → fine-tuned transformer → attentive statistics pooling → softmax head
  - S2 (SALMONN-13B): Whisper+BEATS encoder (frozen) → Q-Former (trainable) → LLaMA-13B with LoRA (trainable adapters) → attentive pooling → softmax
  - T1 (LLaMA-3.3-70B-Instruct): zero-shot prompted inference; no gradient updates
  - T2 (LLaMA-3.1-8B): LoRA fine-tuning → last-layer extraction → attentive pooling → softmax
  - ST1 (SALMONN-7B): speech + ASR transcript concatenated → joint SLLM fine-tuning (LoRA) → pooling → softmax
  - Decision layer: majority voting over 5 predictions; S2 tiebreaker

- **Critical path:**
  1. ASR transcription (Whisper-large-v3) → required for T1, T2, ST1
  2. Individual model training with loss selected per modality (WFL for S1/S2; VS for T2/ST1)
  3. Checkpoint selection on balanced validation set (326 samples per class)
  4. Inference on all 5 models → majority vote → submit

- **Design tradeoffs:**
  - Model size vs. ensemble diversity: Larger models (S2-13B, T1-70B) are stronger individually but memory-heavy; smaller models (S1-317M, T2-8B) contribute via ensemble diversity.
  - Speech-text fusion vs. cost: ST1 improves over speech-only SALMONN-7B (35.43% vs 33.87% val F1) but requires longer sequences and more memory.
  - Zero-shot vs. fine-tuned: T1 (zero-shot 70B) is outperformed by T2 (fine-tuned 8B), suggesting adaptation matters more than scale for this task.

- **Failure signatures:**
  - Rare classes (fear, contempt, disgust) remain poorly classified by speech-only models; text/speech-text models mitigate but do not fully solve (see Table 3: fear F1 12.22% for S1 vs 27.74% for T2).
  - If ST1 is excluded, test F1 drops ~2% absolute (Comb. IV vs ABHINAYA in Table 4), indicating speech-text fusion is structurally important.
  - Training only 3 epochs on ST1 (as at submission deadline) leaves performance on the table; full training yields SoTA.

- **First 3 experiments:**
  1. **Baseline replication:** Train S1 with WCE loss on the provided training split; validate on the balanced validation set. Target: ~32-33% val F1. Confirms data pipeline and pooling implementation.
  2. **Loss function ablation:** For S2 (SALMONN-13B), compare WCE vs. WFL vs. VS loss on the same data. Expect WFL to outperform based on paper findings (Table 2: 37.68% vs 36.34% vs 33.17%).
  3. **Minimal ensemble test:** Combine S2 + T2 + ST1 with majority voting (Comb. I from Table 4). Measure improvement over S2 alone. Target: ~1-2% absolute gain on validation, confirming ensembling mechanics before scaling to 5 models.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent do ASR errors in the Whisper-generated transcripts degrade the performance of the text-based and speech-text models?
- Basis: [inferred] The system relies on ASR transcripts for models T1, T2, and ST1 during testing because ground truth text was unavailable for the evaluation set.
- Why unresolved: The paper assumes ASR transcripts are sufficient but does not quantify the specific error propagation from the ASR system to the final SER accuracy.
- Evidence: An ablation study on the validation set comparing model performance using ASR-generated text versus human-annotated ground truth text.

### Open Question 2
- Question: Why does Vector Scaling (VS) loss significantly improve text-based models while degrading speech-only models compared to Weighted Focal Loss?
- Basis: [inferred] Table 2 shows divergent optimal loss functions (WFL for speech, VS for text); the paper hypothesizes regarding "initial separation" but provides no validation.
- Why unresolved: The underlying representation dynamics causing this discrepancy are not analyzed, leaving the interaction between modality and loss function unclear.
- Evidence: Analysis of logit distributions and inter-class separation in the embedding space during the early training phases of speech versus text models.

### Open Question 3
- Question: Can the performance of the 5-model ensemble be maintained or exceeded by a single, efficient unified model?
- Basis: [inferred] Abhinaya relies on a computationally expensive ensemble of five diverse and large models (WavLM, SALMONN, LLaMA).
- Why unresolved: While effective, the complexity and inference cost of the ensemble are high; the paper does not explore model distillation or unification.
- Evidence: Experiments distilling the ensemble's knowledge into a single Speech LLM and comparing the Macro-F1 score against the ensemble baseline.

### Open Question 4
- Question: How can speech-based models be specifically improved to capture minority emotional classes (e.g., fear, contempt) where text models currently outperform them?
- Basis: [inferred] Table 3 reveals that speech models (S1, S2) significantly underperform compared to text models on rare classes like fear and contempt.
- Why unresolved: The paper identifies this modality gap but offers no specific architectural or data solutions to enhance acoustic sensitivity to these specific emotions.
- Evidence: Implementing class-specific data augmentation or specialized attention mechanisms for S1/S2 and measuring the resulting F1 gain on minority classes.

## Limitations
- **ASR quality dependence**: Performance hinges on Whisper-large-v3 transcription accuracy, which degrades in naturalistic conditions; no error analysis or WER metrics provided.
- **Loss function hyperparameters**: Effectiveness of WFL and VS loss is sensitive to hyperparameters (γ, τ) with no systematic ablation or sensitivity analysis.
- **Class imbalance persistence**: Despite weighted losses, rare emotions (fear, contempt, disgust) remain poorly classified (F1 < 30% for speech models); no targeted solutions proposed.

## Confidence
- **High confidence**: Ensemble architecture and majority voting methodology are clearly described and validated; ranking (4th out of 166) and relative improvement (33.68%) are verifiable.
- **Medium confidence**: Interaction between loss function choice and model type is reported but lacks rigorous justification or ablation studies isolating effects.
- **Low confidence**: ASR transcription quality metrics and impact analysis are absent, despite the entire text-based and speech-text pipeline depending on transcription accuracy.

## Next Checks
1. **ASR error analysis**: Compute and report WER for the MSP-PODCAST dataset, and evaluate emotion classification performance when using ground-truth transcripts versus ASR-generated ones. This will quantify the impact of ASR errors on the text-based and speech-text models.

2. **Loss function ablation study**: For each model type (speech-only, text-only, speech-text), systematically vary γ (for WFL) and γ, τ (for VS loss) over a grid, and measure the impact on macro-F1 for each emotion class. This will reveal whether the reported hyperparameters are optimal or robust.

3. **Minority class performance audit**: For fear, contempt, and disgust, compute class-specific F1 scores for each model and ensemble configuration. Investigate whether targeted data augmentation, resampling, or curriculum learning can improve these underrepresented emotions without degrading overall performance.