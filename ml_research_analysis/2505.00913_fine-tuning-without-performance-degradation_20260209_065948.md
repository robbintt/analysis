---
ver: rpa2
title: Fine-Tuning without Performance Degradation
arxiv_id: '2505.00913'
source_url: https://arxiv.org/abs/2505.00913
tags:
- policy
- performance
- offline
- fine-tuning
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Fine-tuning policies learned offline is challenging due to performance
  degradation during early fine-tuning. Many existing algorithms suffer from either
  severe performance drops or slow learning.
---

# Fine-Tuning without Performance Degradation

## Quick Facts
- arXiv ID: 2505.00913
- Source URL: https://arxiv.org/abs/2505.00913
- Reference count: 40
- Primary result: AJS achieves faster learning and less performance degradation than SAC/InAC variants during offline-to-online fine-tuning

## Executive Summary
Fine-tuning policies learned offline is challenging due to performance degradation during early fine-tuning. Many existing algorithms suffer from either severe performance drops or slow learning. The authors propose Automatic Jump Start (AJS), which gradually increases exploration by using online performance estimates to decide when to switch from a conservative guide policy to a more exploratory policy. AJS uses Fitted Q Evaluation (FQE) to estimate performance without requiring hyperparameter tuning. Experiments on D4RL tasks show that AJS achieves faster learning and significantly less performance degradation compared to SAC, InAC, and variants of Jump Start RL, while maintaining competitive final performance.

## Method Summary
AJS maintains two policies: a guide policy (updated conservatively via InAC) and an exploration policy (updated aggressively via SAC). During execution, the agent follows the guide policy for the first h steps of each episode, then switches to the exploration policy. FQE estimates the performance of the combined policy, and if it exceeds the initial offline value, h is reduced to allow more exploration. This creates a feedback loop where exploration is permitted only when value estimates verify it is safe.

## Key Results
- AJS achieves faster learning than SAC and InAC variants on D4RL tasks
- Significantly less performance degradation during early fine-tuning compared to SAC
- Maintains competitive final performance while reducing reliance on manual hyperparameter tuning
- Performance improvement is most pronounced on Medium datasets where exploration is needed

## Why This Works (Mechanism)

### Mechanism 1: Unseen State Overestimation
Performance degradation during early fine-tuning appears to be driven by the agent encountering out-of-distribution states where the offline value function has arbitrary or overestimated values. In offline learning, Q-values for unseen states are never corrected. When fine-tuning begins, the agent explores these states. If an unseen state-action pair has an erroneously high Q-value, the policy shifts to exploit it. This bootstrap error propagates, corrupting the policy before the value function can be corrected.

### Mechanism 2: Conservative vs. Exploratory Trade-off
Algorithms that constrain updates to the offline data distribution (conservative) prevent degradation but learn slowly, while standard online methods (exploratory) learn fast but risk collapse. Algorithms like InAC implicitly constrain policy updates to the support of the offline data (via the behavior policy term). This prevents the agent from taking risky actions that lead to unseen states (stability), but limits the ability to discover better policies outside the data distribution (slow improvement). SAC maximizes entropy and reward freely, allowing rapid improvement but exposing the agent to the overestimation issues described in Mechanism 1.

### Mechanism 3: Automated Guide Step Adjustment via FQE
Replacing manual hyperparameter schedules with an online performance estimator allows the agent to safely regulate the balance between the conservative guide and the exploratory policy. AJS maintains a guide policy (updated conservatively) and an exploration policy. Instead of manually tuning when to switch between them, it uses Fitted Q Evaluation (FQE) to estimate the performance of the current switching policy. If the estimated performance exceeds the initial offline value, the agent reduces the time it follows the guide policy, allowing more exploration.

## Foundational Learning

- **Concept: Offline RL Distribution Shift**
  - Why needed here: The paper's core problem—performance degradation—stems from the mismatch between the static offline dataset distribution and the states visited during online interaction.
  - Quick check question: Why is "out-of-distribution" error more dangerous in offline RL than in supervised learning? (Hint: bootstrapping).

- **Concept: Conservative Q-Learning / InAC**
  - Why needed here: AJS relies on "InAC" (In-sample Actor-Critic) as its stable guide policy component. You must understand why InAC constrains updates to avoid overestimation.
  - Quick check question: How does the InAC target policy formula differ from standard SAC, and what specific problem does that term solve?

- **Concept: Off-Policy Evaluation (OPE) & FQE**
  - Why needed here: The "Automatic" in AJS comes from using Fitted Q Evaluation to judge the policy. You need to distinguish between learning a policy and evaluating a fixed policy using existing data.
  - Quick check question: Does FQE require interaction with the environment to produce its estimate, or can it run purely on logged data?

## Architecture Onboarding

- **Component map:** Guide Policy (InAC) -> Exploration Policy (SAC) -> Switching Logic (h) -> FQE Estimator
- **Critical path:**
  1. Initialize all networks from offline training (vital: do not reset weights)
  2. Set initial guide step h to max episode length (pure conservative rollout)
  3. Rollout: For each step t < h, sample from πg; else sample from πe
  4. Update: Train πg with InAC loss, πe with SAC loss
  5. OPE Update: Periodically update the FQE estimator on the buffer
  6. Adjust h: If FQE(current) ≥ FQE(offline), reduce h (allow more exploration)

- **Design tradeoffs:**
  - FQE Frequency: Updating FQE every step is expensive; updating too rarely makes h stale
  - Entropy Source: The paper notes πg can be updated with a smaller entropy to maintain stability, while πe handles the heavy exploration

- **Failure signatures:**
  - Stagnation: h never decreases. Likely cause: FQE underestimates the new policy's value, or the tolerance is too tight
  - Collapse: h decreases too fast. Likely cause: FQE overestimates value (overoptimistic OPE), allowing aggressive exploration before πe is stable

- **First 3 experiments:**
  1. Baseline Reproduction: Run SAC fine-tuning and InAC fine-tuning on a D4RL task (e.g., Hopper-Expert) to confirm the degradation vs. stagnation trade-off
  2. Fixed Schedule Ablation: Implement a "Fixed Schedule" version where h decreases linearly. Compare against AJS to see if the automatic adjustment actually outperforms a lucky guess
  3. FQE Sensitivity: Monitor the correlation between the FQE estimate and the actual Monte Carlo return during training. If they diverge, the mechanism is breaking

## Open Questions the Paper Calls Out

- Question: Does the strict avoidance of performance degradation in AJS prevent agents from traversing regions of the state space necessary to escape local optima and reach higher global returns?
- Question: How does the reliability of Fitted Q Evaluation (FQE) scale to high-dimensional, vision-based control tasks?
- Question: What is the sensitivity of AJS to the frequency and accuracy of the FQE updates during the fine-tuning process?

## Limitations

- FQE accuracy is highly sensitive to behavior policy quality and state coverage, potentially failing in sparse-reward or high-dimensional environments
- The paper doesn't thoroughly explore FQE failure modes or provide robustness guarantees against estimation error
- No analysis of how well AJS handles environments with deceptive local optima where initial degradation might be necessary

## Confidence

- **High Confidence:** The empirical observation that standard SAC fine-tuning causes performance degradation while InAC prevents it (Figure 5)
- **Medium Confidence:** The claim that unseen state overestimation is the primary driver of degradation
- **Medium Confidence:** The effectiveness of AJS in balancing stability and exploration

## Next Checks

1. **FQE Estimate Validation:** During AJS execution, track both FQE estimates and actual Monte Carlo returns. Compute the correlation and mean absolute error between them throughout training to verify FQE reliability.
2. **Component Ablation:** Implement a version of AJS where the guide step h is manually scheduled (fixed decrement schedule). Compare learning curves to assess whether automatic adjustment provides measurable benefit beyond a well-tuned manual schedule.
3. **Edge Case Testing:** Run AJS on D4RL datasets with extreme sparsity (e.g., Adroit tasks) or limited state coverage. Monitor for cases where FQE underestimates performance, causing h to remain too high and learning to stagnate.