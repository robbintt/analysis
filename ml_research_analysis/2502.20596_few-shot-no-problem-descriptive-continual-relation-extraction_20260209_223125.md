---
ver: rpa2
title: 'Few-Shot, No Problem: Descriptive Continual Relation Extraction'
arxiv_id: '2502.20596'
source_url: https://arxiv.org/abs/2502.20596
tags:
- relation
- learning
- descriptions
- continual
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses few-shot continual relation extraction (FCRE),
  where models must learn new relations from limited data while retaining knowledge
  of previously learned ones, combating catastrophic forgetting. The authors propose
  a retrieval-based solution that uses large language models (LLMs) to generate detailed
  descriptions for each relation, serving as stable class representations.
---

# Few-Shot, No Problem: Descriptive Continual Relation Extraction

## Quick Facts
- arXiv ID: 2502.20596
- Source URL: https://arxiv.org/abs/2502.20596
- Reference count: 13
- Few-shot continual relation extraction with 16.58% and 9.96% accuracy drops on FewRel and TACRED

## Executive Summary
This paper tackles few-shot continual relation extraction (FCRE), where models must learn new relations from limited data while retaining knowledge of previously learned ones. The authors propose a retrieval-based solution using large language models to generate detailed descriptions for each relation, serving as stable class representations. A bi-encoder retrieval training paradigm is introduced, incorporating sample-based learning and description-pivot learning with mutual information and hard margin losses. Experiments on FewRel and TACRED datasets show state-of-the-art performance, with accuracy drops of only 16.58% on FewRel and 9.96% on TACRED using LLM2Vec, outperforming baselines by up to 5.82%.

## Method Summary
The method uses LLM-generated relation descriptions as stable semantic anchors to combat catastrophic forgetting in FCRE. A bi-encoder architecture encodes both input samples and relation descriptions, trained with a joint loss combining sample-based contrastive learning (SCL, HSMT) and description-pivot losses (LHM, LMI). During inference, a descriptive retrieval strategy fuses prototype proximity and description similarity via rank fusion. The approach maintains a memory buffer of exemplars and generates K=7 diverse descriptions per relation using Gemini 1.5, creating robust class representations that persist across sequential tasks.

## Key Results
- Achieves 16.58% and 9.96% accuracy drops on FewRel and TACRED respectively, outperforming baselines by up to 5.82%
- Rank fusion inference (DRI) improves over NCM by 1.31% (FewRel/BERT) and 6.66% (TACRED/BERT)
- LMI loss contributes 3.14% accuracy gain on FewRel (BERT), confirming description-based semantic alignment importance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-generated relation descriptions function as stable knowledge anchors that resist degradation across sequential tasks
- Mechanism: The authors generate K diverse, detailed descriptions per relation using Gemini 1.5. Unlike memory buffer samples—which are finite and prone to causing overfitting—descriptions encode the invariant semantic definition of a relation. These are encoded once and reused, providing consistent reference points during both training and inference
- Core assumption: The LLM's generated descriptions are semantically accurate and sufficiently diverse to capture each relation's meaning better than K-shot samples from the buffer
- Evidence anchors: [abstract] "starting with a large language model to generate descriptions for each relation... serving as stable class representations"; [section 3.1] "These descriptions inherently represent the class more accurately than the underlying information from a set of samples, serving as stable pivots"
- Break condition: If generated descriptions contain systematic errors or hallucinations for specific relation types, they could encode misleading anchors that persist across tasks

### Mechanism 2
- Claim: The mutual information loss (LMI) binds input representations to their semantically correct descriptions, reducing spurious associations
- Mechanism: LMI maximizes mutual information between a sample's hidden representation and its K corresponding descriptions while minimizing association with descriptions from other classes. The InfoNCE-style formulation creates a contrastive pressure: samples must become more "retrievable" by their own descriptions
- Core assumption: The InfoNCE lower bound is sufficiently tight to provide meaningful gradient signal; K descriptions per class capture enough semantic variance
- Evidence anchors: [section 3.2] "ensures that the representation of the input sample is strongly associated with its corresponding label, while reducing its association with incorrect labels"; [table 4] Ablation shows removing LMI drops accuracy from 68.24% to 65.10% on FewRel (BERT)
- Break condition: If descriptions are semantically similar across relations, LMI may struggle to create discriminative boundaries

### Mechanism 3
- Claim: Reciprocal rank fusion of prototype distance and description similarity improves inference robustness over either signal alone
- Mechanism: DRI combines two ranking signals: negative Euclidean distance to class prototypes and cosine similarity to description embeddings. The α hyperparameter balances these; ϵ controls sensitivity to lower-ranked relations. This fusion mitigates cases where prototypes drift or where descriptions fail to distinguish fine-grained relations
- Core assumption: Prototype and description signals provide complementary information; neither is universally superior
- Evidence anchors: [table 3] DRI improves over NCM by 1.31% (FewRel/BERT) and 6.66% (TACRED/BERT); [section 3.3] "dual focus on both spatial and semantic alignment ensures that the final prediction is informed by a richer, more robust understanding"
- Break condition: If α is poorly tuned for a dataset's characteristics, fusion may amplify noise from the weaker signal

## Foundational Learning

- Concept: Contrastive Learning (Supervised & Triplet variants)
  - Why needed here: The paper builds on SCL and HSMT as the sample-based learning backbone. Understanding how positive/negative pairs shape the latent space is essential before adding description-pivot losses
  - Quick check question: Given a batch with 3 samples from class A and 2 from class B, can you identify all positive/negative pairs for a class A sample?

- Concept: Catastrophic Forgetting in Continual Learning
  - Why needed here: FCRE's core challenge is maintaining performance on R₁...Rₜ₋₁ after learning Rₜ. The paper explicitly targets the accuracy drop (Δ) across 8 tasks
  - Quick check question: Why does rehearsal from a small memory buffer often fail to prevent forgetting in few-shot scenarios?

- Concept: Prototypical Networks / Nearest Class Mean (NCM)
  - Why needed here: DRI is positioned against NCM as a baseline. Understanding how prototypes aggregate class information clarifies what DRI adds via description fusion
  - Quick check question: How is a class prototype computed in NCM, and what failure modes emerge when classes have high intra-class variance?

## Architecture Onboarding

- Component map:
  - Description Generator -> Encoder -> Training Loop -> Memory Buffer -> Inference (DRI)
  - Gemini 1.5 (offline) -> BERT/LLM2Vec with cloze-style prompts -> Combined loss (LSC, LST, LHM, LMI) -> Stores descriptions and exemplars -> Fuses prototype distance + description similarity

- Critical path:
  1. Preprocessing: Generate K=7 descriptions per relation using Gemini 1.5 with Figure 2 prompt
  2. Task Training: Train on current task Dₜ with combined loss (Eq. 15)
  3. Memory Update: Select L samples per relation (closest to centroid); update prototype set P
  4. Memory Rehearsal: Fine-tune on augmented memory M* to reinforce old relations
  5. Inference: For test sample, compute DRI score against all seen relations; return argmax

- Design tradeoffs:
  - K (descriptions per relation): Higher K improves semantic coverage but increases LMI computation. Paper finds K=7 optimal on TACRED; K=1 degrades significantly
  - Memory size L per relation: Larger L improves prototype quality but increases storage and rehearsal cost
  - α in DRI: Controls prototype vs. description contribution. Default appears balanced; may need tuning per dataset
  - LLM usage: Descriptions generated once offline—minimal runtime overhead, but quality depends on LLM capability and prompt design

- Failure signatures:
  - High Δ (forgetting): Suggests description quality issues, insufficient memory rehearsal, or α misalignment
  - Low T₁ accuracy: Check encoder initialization or soft prompt configuration
  - Small gap between DRI and NCM: Description embeddings may not be discriminative; verify LMI is training
  - Ablation shows LHM/LMI negligible: Check description loading pipeline; descriptions may not be reaching the loss functions

- First 3 experiments:
  1. Baseline sanity check: Run with K=1 and NCM inference; should approximate ablated performance in Table 4
  2. K sensitivity: Sweep K ∈ {1, 3, 5, 7, 10} on validation split; plot final accuracy as in Figure 6
  3. Component ablation: Remove LMI only, then LHM only; compare to full model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the method robust to the complete absence of "Raw" relation descriptions during the initial prompt phase?
- Basis in paper: [inferred] The authors note that "datasets used for benchmarking already provide each relation with a concise description," but the method relies on this seed text to generate detailed LLM descriptions
- Why unresolved: Real-world schemas may only provide relation names without semantic descriptions, potentially breaking the description generation pipeline
- What evidence would resolve it: Experiments where the LLM is prompted with only the relation name and a few labeled samples, rather than a pre-existing definition

### Open Question 2
- Question: To what extent does the choice of the description-generating LLM impact final performance?
- Basis in paper: [inferred] The authors explicitly state, "We employ Gemini 1.5... to generate K diverse, detailed, and illustrative descriptions," but do not compare this against other open or closed-source models
- Why unresolved: The quality of the "stable class representations" is entirely dependent on the generating model's capabilities and potential hallucinations
- What evidence would resolve it: Ablation studies comparing performance when using descriptions generated by different models (e.g., Llama 3, GPT-4) versus Gemini 1.5

### Open Question 3
- Question: Is the optimal number of generated descriptions ($K$) sensitive to the semantic complexity of the specific relation?
- Basis in paper: [inferred] Figure 6 shows accuracy peaking at $K=7$ on TACRED, but the paper treats $K$ as a global hyperparameter rather than a dynamic variable
- Why unresolved: Complex or ambiguous relations might require more descriptions ($K>7$) to capture the full semantic distribution, while simple relations might need fewer
- What evidence would resolve it: Analysis of performance when $K$ is dynamically adjusted per relation based on validation loss or semantic entropy

## Limitations
- Hyperparameter sensitivity: Critical parameters (β weights, α, L, K) are not fully specified, requiring assumption-based tuning that may affect reproducibility
- LLM dependency: Performance hinges on Gemini 1.5's description quality; no ablation of LLM choice or quality verification
- Dataset scope: Evaluation limited to FewRel and TACRED; unclear generalizability to other relation extraction domains

## Confidence
- High confidence: Core mechanism of using LLM descriptions as semantic anchors; joint training with description-pivot losses; rank fusion inference strategy
- Medium confidence: Relative performance gains vs. baselines; optimal K=7 finding (may be dataset-specific)
- Low confidence: Long-term stability beyond 8 tasks; performance on datasets with more nuanced relation distinctions

## Next Checks
1. Hyperparameter ablation study: Systematically sweep βSC, βST, βHM, βMI, α, and K on FewRel to quantify sensitivity and identify optimal configurations
2. LLM robustness test: Generate descriptions using alternative LLMs (GPT-4, LLaMA) and measure performance variance to assess dependency
3. Cross-dataset generalization: Apply the method to a third relation extraction dataset (e.g., SemEval-2010 Task 8) to validate broader applicability