---
ver: rpa2
title: 'AutoScale: Linear Scalarization Guided by Multi-Task Optimization Metrics'
arxiv_id: '2508.13979'
source_url: https://arxiv.org/abs/2508.13979
tags:
- scalarization
- learning
- gradient
- task
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates linear scalarization for multi-task learning
  and introduces AutoScale, a two-phase framework that uses multi-task optimization
  (MTO) metrics to automatically determine optimal task weights without costly hyperparameter
  search. The key insight is that well-performing linear scalarization weights exhibit
  specific trends in certain MTO metrics such as high gradient magnitude similarity
  and low condition number.
---

# AutoScale: Linear Scalarization Guided by Multi-Task Optimization Metrics

## Quick Facts
- arXiv ID: 2508.13979
- Source URL: https://arxiv.org/abs/2508.13979
- Reference count: 40
- Primary result: AutoScale achieves 0.703 NDS and 0.703 mAP on NuScenes dataset, outperforming grid-searched weights while being 2-3× faster than gradient-manipulating MTOs

## Executive Summary
AutoScale introduces a novel approach to multi-task learning that automatically determines optimal task weights for linear scalarization without expensive hyperparameter search. The framework leverages the observation that well-performing task weights exhibit specific patterns in multi-task optimization metrics such as gradient magnitude similarity and condition number. By optimizing these metrics during training, AutoScale can efficiently identify and apply effective task weights. The method is evaluated across three diverse datasets, including a large-scale autonomous driving benchmark, demonstrating consistent performance improvements over existing approaches.

## Method Summary
AutoScale is a two-phase framework that automatically determines optimal task weights for linear scalarization in multi-task learning. In phase one, the method optimizes chosen MTO metrics (gradient magnitude similarity or condition number) over local windows during training to estimate suitable task weights. In phase two, it applies the fixed weight for the remaining training period. The key insight is that good task weights exhibit specific trends in MTO metrics, which AutoScale exploits to guide weight selection without costly grid searches or gradient manipulation.

## Key Results
- AutoScale achieves 0.703 NDS and 0.703 mAP on NuScenes dataset, outperforming grid-searched weights
- The method is 2-3× faster than gradient-manipulating MTO approaches while maintaining or improving performance
- Consistent superior performance across three diverse datasets including autonomous driving benchmarks

## Why This Works (Mechanism)
AutoScale works by exploiting the relationship between task weight quality and specific multi-task optimization metrics. The framework recognizes that when linear scalarization weights are well-chosen, certain MTO metrics like gradient magnitude similarity and condition number exhibit characteristic patterns. By monitoring and optimizing these metrics during training, AutoScale can identify effective weight configurations without exhaustive search. The two-phase approach first estimates optimal weights by metric optimization, then applies these fixed weights for efficient final training.

## Foundational Learning
- **Multi-task optimization metrics**: These quantify the relationship between task gradients and overall optimization dynamics, necessary for understanding how different tasks interact during training
- **Linear scalarization**: A method for combining multiple task losses using weighted sums, fundamental for controlling task importance in multi-task learning
- **Gradient magnitude similarity**: Measures how similar gradient directions are across tasks, important for ensuring balanced learning
- **Condition number**: A metric indicating the conditioning of the optimization landscape, relevant for training stability
- **Hyperparameter search efficiency**: Understanding the computational cost of different search strategies is crucial for evaluating AutoScale's advantages
- **Autonomous driving datasets**: Domain-specific knowledge about multi-task challenges in perception tasks helps contextualize the results

## Architecture Onboarding

**Component map:** Input data -> Task-specific networks -> Task losses -> Linear scalarization weights -> Combined loss -> Optimizer -> Parameter updates

**Critical path:** Data input → Network forward pass → Individual task loss computation → Weighted loss combination → Backpropagation → Parameter update

**Design tradeoffs:** 
- AutoScale trades off some training flexibility for efficiency by fixing weights after phase one
- The choice between gradient magnitude similarity and condition number metrics involves balancing sensitivity to gradient alignment versus optimization stability
- Two-phase approach adds complexity but eliminates need for expensive grid search

**Failure signatures:** 
- Poor initial weight estimates leading to suboptimal phase two performance
- Metrics not correlating with actual task performance in certain domains
- Overfitting to specific metric optimization at expense of task objectives

**3 first experiments:**
1. Test AutoScale's ability to recover from poor initial weight initialization
2. Compare performance using gradient magnitude similarity vs condition number metrics
3. Measure training time and performance across datasets with varying task difficulty ratios

## Open Questions the Paper Calls Out
None

## Limitations
- Performance claims are primarily validated on three datasets, with limited exploration of metric sensitivity
- The generalizability of specific metrics to domains with different loss landscapes remains uncertain
- No extensive ablation studies examining the contribution of each phase to final performance

## Confidence
High: NuScenes dataset results (0.703 NDS and 0.703 mAP)
Medium: Efficiency claims (2-3× faster than gradient-manipulating MTOs)
Medium: Generalizability across diverse datasets

## Next Checks
1. Test AutoScale's performance when initialized with poor task weights to verify the metric optimization can recover regardless of starting point
2. Evaluate on additional datasets with varying task difficulty ratios and gradient magnitudes to test metric robustness
3. Conduct ablation studies removing phase one to quantify the contribution of each phase to final performance