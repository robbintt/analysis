---
ver: rpa2
title: 'M2-Reasoning: Empowering MLLMs with Unified General and Spatial Reasoning'
arxiv_id: '2507.08306'
source_url: https://arxiv.org/abs/2507.08306
tags:
- reasoning
- data
- arxiv
- answer
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces M2-Reasoning-7B, a multimodal large language
  model designed to excel in both general and spatial reasoning. The authors address
  the challenge of dynamic spatial interactions, which existing MLLMs struggle with,
  by developing a novel data pipeline and training strategy.
---

# M2-Reasoning: Empowering MLLMs with Unified General and Spatial Reasoning

## Quick Facts
- **arXiv ID**: 2507.08306
- **Source URL**: https://arxiv.org/abs/2507.08306
- **Reference count**: 40
- **Primary result**: Achieves SOTA on 8 benchmarks with 45.0 average score on general reasoning tasks and 82.3 on spatial reasoning tasks.

## Executive Summary
M2-Reasoning-7B is a multimodal large language model designed to excel in both general and spatial reasoning tasks. The model addresses the challenge of dynamic spatial interactions through a novel two-stage training pipeline: cold-start fine-tuning with quality-filtered data followed by reinforcement learning with verifiable rewards (RLVR). The approach uses step-wise dynamic optimization to mitigate task conflicts and task-specific rewards to provide tailored incentive signals. Experimental results demonstrate state-of-the-art performance across eight benchmarks, with particular strength in spatial reasoning tasks requiring numerical estimates like distance and size.

## Method Summary
The method employs a two-stage training pipeline. Stage 1 (Cold-Start SFT) trains on 168K curated chain-of-thought samples filtered for quality and 6.2M foundational data. Stage 2 (Dynamic RLVR) uses 126.2K verifiable samples with curriculum sampling by difficulty and dynamic advantage weighting. The model architecture adapts M2-Omni with Qwen2.5-7B-Instruct as the LLM backbone. Task-specific rewards are employed: rule-based for general reasoning and Exponential Decay Numeric Matching (EDNM) for spatial tasks requiring numerical outputs. Training uses modified GRPO with cosine-annealed KL penalty and task-specific reward formulations.

## Key Results
- Achieves SOTA performance with 45.0 average score on eight general reasoning benchmarks
- Excels in spatial reasoning with 82.3 average score on CV-Bench and 42.3 on VSI-Bench
- Strong performance on distance and size estimation tasks using EDNM rewards
- Demonstrates effectiveness of quality-filtered CoT synthesis and curriculum-based RLVR

## Why This Works (Mechanism)

### Mechanism 1: Quality-Filtered CoT Synthesis Pipeline
The pipeline generates multiple reasoning chains per question using high-temperature sampling, then filters them via answer accuracy and quality assessment. Only high-quality chains with logical coherence and verification richness proceed to training. This ensures the model learns from well-structured reasoning trajectories rather than noisy or incorrect patterns.

### Mechanism 2: Curriculum Sampling + Dynamic Advantage Weighting
Samples are pre-scored for difficulty and training proceeds from easy to hard. During RLVR, dynamic advantage weighting assigns higher importance to moderate-difficulty samples (accuracy ~0.5) which provide the most informative gradients. This approach balances exploration and exploitation while focusing learning on samples that maximize information gain.

### Mechanism 3: Task-Specific Reward Formulation (EDNM for Spatial)
For spatial tasks requiring numerical answers, EDNM provides smooth continuous rewards: R = γ · exp(-λ · |x - x_gt| / |x_gt|). This contrasts with binary rewards for general reasoning. The smooth reward helps bootstrap perceptual abilities that initially produce rough estimates, providing stable gradient signals for continuous output spaces.

## Foundational Learning

- **Concept: Chain-of-Thought (CoT) Reasoning**
  - Why needed here: The entire training pipeline assumes the model can generate and learn from structured reasoning traces
  - Quick check question: Given a math problem, can you distinguish between a direct answer and a step-by-step reasoning trace?

- **Concept: Policy Gradient Methods (specifically GRPO)**
  - Why needed here: RLVR uses Group Relative Policy Optimization for reward-based training
  - Quick check question: In GRPO, why is the advantage computed relative to a group of completions rather than a baseline value function?

- **Concept: Spatial Perception in MLLMs**
  - Why needed here: The paper targets spatial reasoning tasks (depth, distance, size, room layout)
  - Quick check question: Why might an MLLM struggle to estimate absolute distances (in meters) from a single image, versus relative ordering?

## Architecture Onboarding

- **Component map**: Input (Image/Video + Text) → Vision Encoder → Tokenizer → LLM Backbone → MLP Projector → Output (Structured reasoning + final answer)

- **Critical path**:
  1. Data curation: Quality filtering → difficulty scoring → curriculum ordering
  2. Cold-start: Activate reasoning patterns, standardize output format
  3. RLVR: Dynamic sampling → generation → reward computation → advantage → policy update
  4. Evaluation: Benchmarks with task-specific accuracy or EDNM

- **Design tradeoffs**:
  - Data volume vs. quality: 294.2K curated samples vs. larger noisy datasets
  - Unified training vs. task-specific: Single model handles both general and spatial tasks
  - Binary vs. continuous rewards: General tasks use exact match; spatial uses EDNM

- **Failure signatures**:
  - Pathological repetition: Model loops on phrases, indicating reward hacking
  - Short reasoning chains: May limit complex problem decomposition
  - Visual hallucinations: Fine-grained perception errors
  - Training instability: Exploding KL divergence in early RLVR steps

- **First 3 experiments**:
  1. Ablate quality filtering: Compare MathVista/LogicVista scores with unfiltered vs. filtered data
  2. Ablate EDNM: Replace with binary reward for spatial tasks and compare CV-Bench/VSI-Bench scores
  3. Ablate curriculum sampling: Compare training loss curves and final scores with random vs. difficulty-ordered data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can multimodal models be encouraged to generate longer, deeper reasoning chains comparable to text-only models like DeepSeek-R1?
- Basis in paper: The Conclusion states that M2-Reasoning-7B exhibits "Constrained Reasoning Depth," producing shorter chains than text-only counterparts, which limits multi-step problem solving.

### Open Question 2
- Question: What specific mechanisms can effectively mitigate pathological repetition (reasoning loops) during generation?
- Basis in paper: The Conclusion identifies "Pathological Repetition" as a key limitation where the model gets trapped in redundant loops, derailing coherent thought.

### Open Question 3
- Question: Does training on "controlled spatial simulations" fully transfer to robust reasoning in unconstrained, real-world visual environments?
- Basis in paper: Section 2.2.1 notes the use of "controlled spatial simulations" for "accurate ground truth," while the Introduction claims the goal is "real-world applications."

## Limitations
- Relies on proprietary quality assessment model (Qwen2.5-7B-Instruct) limiting reproducibility
- Dynamic advantage weighting assumes optimal difficulty at ~50% accuracy, which may shift during training
- Acknowledged model pathology (repetitive phrase generation) not fully addressed

## Confidence
- **High confidence**: SOTA benchmark performance (45.0 average on general reasoning, 82.3 on spatial reasoning)
- **Medium confidence**: Effectiveness of quality-filtered CoT pipeline
- **Medium confidence**: EDNM reward formulation's superiority for spatial tasks

## Next Checks
1. **Ablation study on quality filtering**: Train with unfiltered CoT data vs. filtered. Compare MathVista/LogicVista scores to isolate the impact of the quality assessment pipeline.
2. **Reward function sensitivity**: Test EDNM with different decay rates (λ=1, λ=3) on spatial tasks to determine if the chosen parameters are optimal or task-dependent.
3. **Generalization to unseen domains**: Evaluate M2-Reasoning-7B on out-of-distribution reasoning tasks (e.g., medical or legal reasoning) to assess whether the unified training approach transfers beyond the curated benchmarks.