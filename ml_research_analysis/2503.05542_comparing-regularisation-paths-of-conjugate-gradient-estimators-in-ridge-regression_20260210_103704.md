---
ver: rpa2
title: Comparing regularisation paths of (conjugate) gradient estimators in ridge
  regression
arxiv_id: '2503.05542'
source_url: https://arxiv.org/abs/2503.05542
tags:
- gradient
- error
- prediction
- ridge
- conjugate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper compares the statistical performance of conjugate gradients
  (CG), gradient descent (GD), and gradient flow (GF) for ridge regression, focusing
  on prediction error along their regularization paths. The authors derive a key comparison
  theorem showing that the risk of CG iterates can be bounded by that of GF iterates
  at transformed time indices, up to a constant factor depending on the spectrum of
  the empirical covariance matrix.
---

# Comparing regularisation paths of (conjugate) gradient estimators in ridge regression

## Quick Facts
- **arXiv ID**: 2503.05542
- **Source URL**: https://arxiv.org/abs/2503.05542
- **Reference count**: 29
- **Primary result**: The paper derives bounds comparing prediction risk of CG iterates to GF iterates at transformed time indices, showing CG achieves comparable statistical performance to GF and ridge regression while requiring fewer iterations.

## Executive Summary
This paper establishes a theoretical foundation for comparing the statistical performance of conjugate gradient (CG), gradient descent (GD), and gradient flow (GF) estimators in ridge regression. The authors prove that CG iterates achieve prediction risk comparable to GF and ridge regression by mapping iteration time to a regularization path through a time reparameterization. A key contribution is an explicit non-standard error decomposition that isolates the stochastic error in CG, which is otherwise intractable due to data dependency. The paper demonstrates that the constant factor comparing CG to GF risk is uniformly bounded for common spectral distributions of the feature covariance matrix, including polynomial eigenvalue decay, Marchenko-Pastur type, and spiked covariance models. Numerical simulations and real data examples confirm the theoretical predictions, showing CG achieves similar prediction performance to GF and ridge regression while requiring fewer iterations.

## Method Summary
The paper analyzes ridge regression through three estimators: ridge regression (analytical baseline), gradient flow (continuous-time linear idealization), and conjugate gradient (discrete-time, non-linear, computationally efficient). The method involves implementing penalized CG via Algorithm 1, tracking residual polynomials to determine effective time indices, and monitoring empirical prediction risk along the iteration path. The analysis uses a novel error decomposition separating approximation and stochastic terms, with risk bounds depending on the spectrum of the empirical covariance matrix. The comparison between CG and GF is achieved through a time reparameterization that accounts for the difference in their residual polynomials.

## Key Results
- CG iterates achieve prediction risk comparable to GF and ridge regression by mapping iteration time to a regularization path
- A non-standard error decomposition isolates and controls the stochastic error in CG, which is otherwise intractable due to data dependency
- The constant factor comparing CG to GF risk is uniformly bounded for common spectral distributions including polynomial eigenvalue decay, Marchenko-Pastur type, and spiked covariance models
- CG achieves comparable prediction performance to GF and ridge regression while requiring significantly fewer iterations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Conjugate Gradient (CG) iterates achieve prediction risk comparable to Gradient Flow (GF) and Ridge Regression (RR) by mapping iteration time to a regularisation path.
- **Mechanism:** The paper establishes that the risk of the $t$-th CG iterate can be bounded by the risk of the GF iterate at a transformed time index $\tau_t$, up to a constant factor. This works because CG's data-dependent residual polynomials $R^{CG}_t$ can be upper bounded by GF's deterministic exponential filters $R^{GF}_t$ via a time reparameterisation $\rho_t = |(R^{CG}_t)'(0)|$.
- **Core assumption:** The target vector $\gamma$ (e.g., $\beta_0$ or $\beta_\lambda$) satisfies a geometric condition (Condition 3.10) relative to the spectrum of the empirical covariance matrix, which holds for standard ridge-type targets.
- **Break condition:** The bound fails if the geometric condition (3.10) on the target vector is violated, or if the iteration time $t$ is too small ($t < 1/2\|\hat{\Sigma}_\lambda\|$) to provide significant regularization.

### Mechanism 2
- **Claim:** A non-standard error decomposition allows the isolation and control of the stochastic error in CG, which is otherwise intractable due to data dependency.
- **Mechanism:** Unlike standard linear methods where the residual filter $R$ is deterministic, CG's filter depends on $y$. The authors decompose the CG error into approximation and stochastic terms involving the "small" part of the residual polynomial ($R^{CG}_{t,<}$). This isolates the contribution of the first zero $x_{1,t}$ of the residual polynomial, allowing comparison to the linear GF case without the cross-term vanishing issues typical of non-linear estimators.
- **Break condition:** The analysis relies on bounding the "cross term" by doubling the other error terms; if the cross term dominates (unlikely under the assumed conditions but theoretically possible in extreme noise), the bound loosens significantly.

### Mechanism 3
- **Claim:** The "constant factor" comparing CG to GF risk is uniformly bounded for common spectral distributions of the feature covariance matrix.
- **Mechanism:** The comparison factor $C_{t,\lambda}$ depends on the ratio of sums of eigenvalues. If eigenvalues decay polynomially, geometrically, or follow a Marchenko-Pastur distribution, the ratio of the "tail" mass to the "head" mass remains controlled. This prevents the CG stochastic error from exploding relative to GF.
- **Break condition:** If eigenvalues decay too slowly (e.g., $s_i \sim i^{-\alpha}$ with $\alpha \in (0, 1/2)$) or the dimension $p \to \infty$ rapidly without a corresponding increase in spike strength, the constant $C_{t,\lambda}$ may become unbounded, invalidating the uniform comparison.

## Foundational Learning

- **Concept:** Ridge Regression (Tikhonov Regularization)
  - **Why needed here:** The paper analyzes a *penalized* criterion. You must understand how the penalty $\lambda$ biases the estimator but reduces variance (stabilizes the inverse) to grasp the trade-offs in the error decomposition.
  - **Quick check question:** In the ridge solution $\hat{\beta}^{RR}_\lambda = (X^\top X + \lambda I)^{-1} X^\top y$, what happens to the coefficients as $\lambda \to \infty$?

- **Concept:** Conjugate Gradient (CG) Method
  - **Why needed here:** CG is the primary algorithm under study. Unlike Gradient Descent, its iterates depend on the specific data vector $y$ (via Krylov subspaces), making its statistical analysis difficult. You need to distinguish its "greedy" nature from linear methods.
  - **Quick check question:** Why is CG generally considered "non-linear" in the context of statistical analysis, even though it solves a linear system?

- **Concept:** Spectral Decomposition & Eigenvalue Decay
  - **Why needed here:** The paper's main theoretical contribution bounds errors based on the *spectrum* of $\hat{\Sigma}$. Understanding how eigenvalue decay (polynomial vs. exponential) affects the "effective rank" and condition number is crucial for interpreting the constant factor $C_{t,\lambda}$.
  - **Quick check question:** If a covariance matrix has "polynomial eigenvalue decay" with exponent $\alpha$, do the small eigenvalues contribute significantly to the trace?

## Architecture Onboarding

- **Component map:** Input data $(X, y)$ and penalty $\lambda$ -> Ridge estimator $\hat{\beta}^{RR}_\lambda$ (analytical baseline) -> Gradient Flow $\hat{\beta}^{GF}_{\lambda,t}$ (continuous-time linear idealization) -> Conjugate Gradients $\hat{\beta}^{CG}_{\lambda,t}$ (discrete-time, non-linear, computationally efficient) -> Analysis Engine (residual polynomials, error decomposition, spectral constant)

- **Critical path:**
  1. Implement Algorithm 1 (Penalised CG) to generate iterates $\hat{\beta}^{CG}_{\lambda,k}$
  2. Calculate the residual polynomials $R^{CG}_k$ to track the "effective time" $\tau_t$
  3. Monitor the empirical prediction risk along the iteration path
  4. **Key Insight:** Stop early. The risk $R^{in}$ typically follows a U-shape; the goal is to stop at the "oracle" iterate before the noise dominates (overfitting)

- **Design tradeoffs:**
  - **Computational vs. Statistical:** CG converges numerically much faster than GD (fewer matrix-vector products). The paper proves this speed does *not* come at the cost of statistical efficiency; the "regularisation path" is statistically similar
  - **Implicit vs. Explicit Regularization:** You can achieve low risk either by stopping CG early (implicit) or running it to convergence with a large $\lambda$ (explicit). This paper bridges the two, showing they are comparable

- **Failure signatures:**
  - **Non-monotonicity:** Unlike GF (which can be monotonic for large $\lambda$), CG risk might oscillate slightly due to its data-dependent nature, though the *bound* is monotonic
  - **Spectrum Sensitivity:** If $\hat{\Sigma}$ has very slow eigenvalue decay (ill-conditioned), the constant factor $C_{t,\lambda}$ explodes, and CG might require many more iterations to match GF's risk profile
  - **High-Dimensional Break:** If $p \gg n$ and the signal is not concentrated on the top eigenvectors, the "effective rank" $N(\lambda)$ is large, and out-of-sample transfer (Prop 3.13) requires careful handling of concentration bounds

- **First 3 experiments:**
  1. **Replicate Figure 2 (Simulation):** Generate data with a spiked covariance model. Plot the in-sample risk of CG, GD, and RR vs. iteration number $k$. Verify that CG reaches the minimum risk significantly faster than GD
  2. **Spectral Stress Test:** Generate data with $s_i \sim i^{-\alpha}$ for $\alpha \in \{0.5, 1.0, 2.0\}$. Calculate the empirical constant $C_{t,\lambda}$ during CG iterations and check if it remains bounded as predicted by Example 3.9
  3. **Out-of-Sample Transfer:** Split a real dataset (e.g., Riboflavin) into train/test. Run CG on the training set. Evaluate if the optimal stopping time on the training risk correlates with the minimum on the test risk, testing the transfer bounds of Proposition 3.13

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the risks of gradient flow or ridge regression be bounded by the risk of conjugate gradients (CG) in the converse direction of Corollary 3.10?
- **Basis in paper:** [explicit] The conclusion asks, "A natural question is whether we can bound the risks of the regularisation paths in the converse direction of Corollary 3.10."
- **Why unresolved:** The paper argues this is unlikely universally because CG uses data-dependent Krylov subspaces while gradient flow relies only on the empirical covariance. Additionally, gradient flow bias decays exponentially while ridge regression bias decays polynomially.
- **What evidence would resolve it:** A proof showing non-existence of universal bounds, or derivation of specific conditions on the target $\beta_0$ and noise under which a converse bound holds.

### Open Question 2
- **Question:** How do the eigenvalue conditions for the CG/GF comparison in Theorem 3.7 relate to the conditions for "benign overfitting" in high-dimensional regression?
- **Basis in paper:** [explicit] The conclusion states that "a closer investigation of the benign overfitting phenomenon for gradient methods would be fascinating, in particular concerning the eigenvalue conditions in Tsigler and Bartlett [23] and ours in Theorem 3.7."
- **Why unresolved:** The paper establishes comparison bounds involving a constant $C_{\lambda}$ dependent on the spectrum, but does not verify if these spectral conditions align with those required for benign overfitting in minimum-norm interpolators.
- **What evidence would resolve it:** A theoretical analysis mapping the constant $C_{\lambda}$ to the eigenvalue decay rates required for benign overfitting, or an empirical study correlating the two phenomena.

### Open Question 3
- **Question:** How can explicit model selection and early stopping rules be designed for CG iterates based on the established risk bounds?
- **Basis in paper:** [explicit] The conclusion notes that "model selection and early stopping rules for CG iterates seem very attractive (compare Hucker and Reiß [12] for the case $\lambda=0$)..."
- **Why unresolved:** While the paper derives oracle inequalities and minimal risk bounds, it does not propose a practical algorithm to select the iteration number $k$ in real-time without oracle knowledge.
- **What evidence would resolve it:** Development of a data-driven stopping rule (e.g., via residual-based criteria) and proof of its minimax optimality or oracle inequality.

## Limitations

- The geometric condition on the target vector may not hold for all signal structures, particularly when signals are not well-aligned with leading eigenvectors
- The uniform boundedness of the comparison constant depends critically on specific spectral properties that may not cover all practical cases
- The paper focuses primarily on in-sample prediction risk, with out-of-sample bounds relying on concentration inequalities that may be conservative

## Confidence

- **High confidence:** The monotonicity properties of GF prediction errors and the explicit risk bounds for CG vs GF are mathematically rigorous
- **Medium confidence:** The uniform boundedness claims for the comparison constant across different spectral distributions are plausible but rely on specific eigenvalue decay rates
- **Medium confidence:** The out-of-sample risk transfer bounds are theoretically sound but depend on concentration inequalities

## Next Checks

1. **Spectral Stress Test:** Generate synthetic data with varying eigenvalue decay rates (e.g., s_i ~ i^(-α) for α ∈ [0.5, 2.0]) and empirically measure the comparison constant C_t,λ during CG iterations to verify the uniform boundedness claims across the full spectrum of spectral conditions.

2. **Signal Structure Robustness:** Test the theoretical bounds when the signal β₀ is not aligned with the leading eigenvectors (e.g., random sparse signals vs. signals concentrated on top eigenvectors) to determine how the geometric condition affects the practical performance gap between CG and GF.

3. **High-Dimensional Scaling:** Extend the analysis to p >> n settings with varying effective ranks N(λ) to test the limits of the out-of-sample risk transfer bounds and investigate whether the required concentration inequalities hold in severely underdetermined regimes.