---
ver: rpa2
title: Nearly Tight Bounds for Exploration in Streaming Multi-armed Bandits with Known
  Optimality Gap
arxiv_id: '2502.01067'
source_url: https://arxiv.org/abs/2502.01067
tags:
- algorithm
- arms
- sample
- bound
- complexity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the sample-memory-pass trade-offs for\
  \ pure exploration in multi-pass streaming multi-armed bandits (MABs) when the optimality\
  \ gap \u0394[2] is known a priori. Prior work showed that without knowing \u0394\
  [2], \u0398(log(1/\u0394[2])) passes are necessary and sufficient to achieve the\
  \ worst-case optimal sample complexity of O(n/\u0394\xB2[2]) with single-arm memory."
---

# Nearly Tight Bounds for Exploration in Streaming Multi-armed Bandits with Known Optimality Gap

## Quick Facts
- arXiv ID: 2502.01067
- Source URL: https://arxiv.org/abs/2502.01067
- Reference count: 40
- This paper investigates sample-memory-pass trade-offs for pure exploration in multi-pass streaming multi-armed bandits with known optimality gap Δ[2]

## Executive Summary
This paper addresses the fundamental question of how many passes are required to achieve instance-optimal sample complexity in streaming multi-armed bandits when the optimality gap Δ[2] is known. The authors establish a nearly tight characterization showing that Ω(log(n)/log log(n)) passes are necessary for sublinear memory algorithms, while providing a matching upper bound algorithm using O(log(n)) passes with single-arm memory. The key technical innovation is a novel lower bound framework for batched instance distributions, which enables the construction of hard instances with two special arms per batch.

## Method Summary
The paper develops both lower and upper bound results using complementary approaches. For the lower bound, the authors introduce a novel batched instance distribution framework where each batch contains two special arms, enabling a reduction to multi-armed bandits with batch updates. This construction allows them to prove that Ω(log(n)/log log(n)) passes are necessary for any algorithm achieving near-optimal sample complexity with sublinear memory. For the upper bound, they present an elimination-based algorithm that generalizes classical multi-pass MAB algorithms by using geometrically increasing elimination gaps (ϵp = Δ[2] · n^(1-p/P) for the p-th pass). This algorithm achieves O(∑ᵢ₌₂ⁿ 1/Δᵢ² · log(n)) sample complexity with O(log(n)) passes and single-arm memory, establishing a nearly tight trade-off.

## Key Results
- Lower bound: Any algorithm achieving O(∑ᵢ₌₂ⁿ 1/Δᵢ² · log(n)) sample complexity with o(n/polylog(n)) memory requires Ω(log(n)/log log(n)) passes
- Upper bound: An elimination-based algorithm achieves O(∑ᵢ₌₂ⁿ 1/Δᵢ² · log(n)) sample complexity, O(log(n)) passes, and single-arm memory
- The algorithm provides optimal sample-pass trade-off: P passes achieve O(∑ᵢ₌₂ⁿ 1/Δᵢ² · n^(2/P) · log(nP)) sample complexity

## Why This Works (Mechanism)
The lower bound construction exploits the fundamental limitation that with few passes and sublinear memory, algorithms cannot distinguish between instances with carefully designed special arms in each batch. The batched instance distribution framework creates a hard instance class where information accumulates slowly across passes, forcing the algorithm to make many passes to achieve near-optimal sample complexity. The upper bound algorithm leverages the known optimality gap to design geometrically increasing elimination gaps that balance exploration and exploitation across passes, while maintaining memory efficiency through single-arm processing.

## Foundational Learning

1. **Streaming Multi-armed Bandits** - Sequential decision-making under partial information with constraints on memory and multiple passes through data
   *Why needed:* This work specifically addresses the streaming setting where arms arrive sequentially and only limited memory is available
   *Quick check:* Can the algorithm handle arm arrivals in arbitrary order without storing all past observations?

2. **Instance-optimal Sample Complexity** - Achieving sample complexity that scales with the specific instance parameters rather than worst-case bounds
   *Why needed:* The goal is to match the information-theoretic optimum O(∑ᵢ₌₂ⁿ 1/Δᵢ²) rather than the worst-case O(n/Δ²₂)
   *Quick check:* Does the algorithm adapt to different gap structures without prior knowledge of all gaps?

3. **Batched Instance Distribution Framework** - A novel technique for constructing hard instance distributions with special structure across batches
   *Why needed:* Enables proving lower bounds for algorithms that process data in passes
   *Quick check:* Can this framework be extended to prove lower bounds for other multi-pass algorithms?

4. **Elimination-based Multi-pass Algorithms** - Algorithms that progressively eliminate suboptimal arms across multiple passes
   *Why needed:* Provides a constructive upper bound matching the lower bound's order of magnitude
   *Quick check:* How does the elimination threshold scale with the pass number?

5. **Sample-memory-pass Trade-offs** - The fundamental relationships between computational resources in streaming algorithms
   *Why needed:* Characterizes the fundamental limits of what's achievable under different resource constraints
   *Quick check:* What happens to sample complexity as the number of passes approaches log(n)?

## Architecture Onboarding

Component Map: Lower Bound Framework -> Hard Instance Construction -> Information-Theoretic Analysis -> Ω(log(n)/log log(n)) Passes Bound
                    ↘
                     Upper Bound Algorithm -> Geometric Elimination Gaps -> O(log(n)) Passes Bound

Critical Path: The paper establishes the lower bound through a reduction argument using batched instances, then designs an upper bound algorithm that nearly matches this bound. The geometric elimination strategy in the upper bound is the key innovation that enables the O(log(n)) pass complexity.

Design Tradeoffs: The algorithm trades off between exploration (learning about arms) and exploitation (eliminating clearly suboptimal arms) across passes. Using geometrically increasing elimination gaps allows the algorithm to be aggressive in early passes while being conservative in later passes when fewer arms remain.

Failure Signatures: If the algorithm uses too few passes, it cannot achieve the instance-optimal sample complexity even with sublinear memory. If it uses too many passes, it becomes inefficient. The critical threshold appears to be around log(n)/log log(n) passes for sublinear memory algorithms.

First Experiments:
1. Implement the algorithm and verify it finds the best arm with O(log(n)) passes on synthetic instances
2. Test the algorithm's sample complexity on instances with different gap structures to verify the O(∑ᵢ₌₂ⁿ 1/Δᵢ² · log(n)) bound
3. Empirically determine the minimum number of passes needed for various problem sizes and memory constraints

## Open Questions the Paper Calls Out
None explicitly mentioned in the provided content.

## Limitations
- The lower bound construction is specific to the batched instance distribution model and may not generalize to other problem variants
- The assumption of known optimality gap Δ₂ may not hold in practical applications
- The upper bound algorithm has an extra log(n) factor compared to the information-theoretic optimum, suggesting potential room for improvement

## Confidence

*Lower Bound (High)*: The Ω(log(n)/log log(n)) pass complexity lower bound is supported by a rigorous construction and information-theoretic arguments. The batched instance distribution framework appears novel and well-justified.

*Upper Bound (Medium)*: The O(∑ᵢ₌₂ⁿ 1/Δᵢ² · log(n)) sample complexity upper bound is achieved by a concrete algorithm with clear analysis. However, the extra log(n) factor compared to the optimal sample complexity introduces uncertainty about whether this is fundamentally necessary or an artifact of the analysis.

*Trade-off Characterization (Medium)*: The characterization of the sample-pass-memory trade-off appears comprehensive for the considered setting, but may not extend to other bandit variants or constraints.

## Next Checks

1. Implement the proposed algorithm and empirically verify the O(log(n)) pass complexity and sample complexity bounds on synthetic instances with varying gap structures.

2. Rigorously test whether the log(n) factor in the upper bound is necessary by attempting to construct matching lower bound instances or improve the algorithm's analysis.

3. Extend the batched instance distribution framework to derive lower bounds for related problems (e.g., unknown Δ₂, different memory constraints) to assess the framework's generality.