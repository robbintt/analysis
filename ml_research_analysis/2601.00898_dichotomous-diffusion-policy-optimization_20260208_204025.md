---
ver: rpa2
title: Dichotomous Diffusion Policy Optimization
arxiv_id: '2601.00898'
source_url: https://arxiv.org/abs/2601.00898
tags:
- policy
- learning
- diffusion
- conference
- offline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DIPOLE, a reinforcement learning algorithm
  for stable and controllable diffusion policy optimization. The key idea is to decompose
  the optimal policy into a pair of stably learned dichotomous policies - one maximizing
  rewards and the other minimizing them - which can be linearly combined during inference
  for flexible control over greediness.
---

# Dichotomous Diffusion Policy Optimization

## Quick Facts
- arXiv ID: 2601.00898
- Source URL: https://arxiv.org/abs/2601.00898
- Reference count: 38
- Key outcome: DIPOLE achieves stable training of large diffusion policies by decomposing optimal policy into bounded positive/negative policies, enabling controllable greediness via linear combination

## Executive Summary
This paper addresses the challenge of training large diffusion policies in reinforcement learning by introducing DIPOLE, a method that decomposes the optimal policy into a pair of stably learned dichotomous policies—one maximizing rewards and the other minimizing them. By using sigmoid-weighted bounded weights instead of unstable exponential weights, DIPOLE prevents loss explosion while maintaining the ability to achieve greedy reward maximization. The approach enables flexible control over greediness through linear combination of policy scores at inference time, similar to classifier-free guidance. Evaluations demonstrate superior performance on both standard offline RL benchmarks (ExORL, OGBench) and large-scale vision-language-action models for autonomous driving (NAVSIM).

## Method Summary
DIPOLE reformulates KL-regularized RL by decomposing the optimal policy π* ∝ μ·exp(βG) into two bounded policies: π+ (positive, high-reward) and π- (negative, low-reward). Both are trained simultaneously using sigmoid weights σ(βG) and (1-σ(βG)) respectively, avoiding exponential weight instability. During inference, a combined score ε̃ = (1+ω)ε+ - ωε- is computed, where ω controls greediness. The method builds on pretrained diffusion policies with LoRA adapters for the dichotomous components, and uses expectile regression for value learning. Rejection sampling can further improve performance by selecting the best action from multiple candidates.

## Key Results
- DIPOLE outperforms state-of-the-art methods on ExORL and OGBench benchmarks in both offline and offline-to-online settings
- Training a 1-billion parameter vision-language-action model on NAVSIM shows significant improvements in safety (collision rate) and progress metrics
- The method achieves stable training without loss explosion, even with large β values
- Flexible controllability through ω enables tuning between exploration and exploitation without retraining

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing the optimal policy into bounded positive and negative policies stabilizes training while preserving greedy optimization capability.
- Mechanism: The standard KL-regularized optimal policy π* ∝ μ·exp(βG) suffers from loss explosion when β is large (exp grows unbounded). DIPOLE reformulates this using sigmoid weighting: π* ∝ μ·σ(βG)·exp(ωβG), which can be algebraically decomposed into π* ∝ [π+]^(1+ω)/[π-]^ω where π+ uses σ(βG) ∈ [0,1] and π- uses (1-σ(βG)) ∈ [0,1]. Both weights are strictly bounded, preventing numerical instability.
- Core assumption: The advantage function G(s,a) can be accurately estimated; the reference policy μ has sufficient coverage of high-quality actions.
- Evidence anchors:
  - [abstract]: "decompose the optimal policy into a pair of stably learned dichotomous policies - one maximizing rewards and the other minimizing them"
  - [Section 3.2, Eq. 7-8]: Shows the mathematical decomposition from sigmoid-weighted form to ratio of dichotomous policies
  - [corpus]: Related work on stable diffusion policy training (e.g., "Expressive Value Learning for Scalable Offline RL") addresses similar stability challenges but via different mechanisms
- Break condition: If σ(βG) becomes uniformly ~0.5 (poor advantage signal), both policies collapse to near-reference behavior, providing no optimization benefit.

### Mechanism 2
- Claim: Sigmoid-based reweighting enables arbitrary greediness without loss explosion.
- Mechanism: Unlike exp(βG) which grows unbounded and causes loss domination by few high-return samples, σ(βG) saturates smoothly at 0 and 1. The greediness factor ω separately controls how aggressively to combine policies during inference (not training), decoupling training stability from inference-time greediness.
- Core assumption: There exists a meaningful separation between high-G and low-G actions in the dataset; sigmoid's saturation points align with this separation.
- Evidence anchors:
  - [Section 3.1]: "A fundamental issue is that weighted regression can only achieve greedy reward maximization when the temperature parameter is set to a large value, which easily leads to exploding loss"
  - [Section 3.2]: "Both π+ and π- are weighted by strictly bounded sigmoid weight functions, instead of the unstable and unbounded exponential weight term"
  - [corpus]: Limited direct corpus comparison; sigmoid weighting for RL is relatively novel in diffusion policy context
- Break condition: If β is set too low, σ(βG) ≈ 0.5 everywhere, providing weak differentiation; if too high, may approach step function, losing smooth gradient signal.

### Mechanism 3
- Claim: Linear combination of positive and negative policy scores during inference enables flexible, controllable greediness.
- Mechanism: During sampling, the combined noise predictor is: ε̃ = (1+ω)ε+ - ωε-. This mirrors classifier-free guidance (CFG) but applies it to RL: ε+ pushes toward high-reward regions, ε- pushes away from low-reward regions. By adjusting ω at inference time, users control exploration-exploitation without retraining.
- Core assumption: The score functions from both policies are well-calibrated; ε- meaningfully captures low-reward directions to repel from.
- Evidence anchors:
  - [Section 3.2, Eq. 10]: "∇_a log π*(a|s) = (1+ω)∇_a log π+(a|s) - ω∇_a log π-(a|s)"
  - [Section 3.2]: "Remarkably similar to classifier-free guidance...enabling flexible control of the optimality level of generated actions with the greediness factor ω"
  - [corpus]: CFGRL (mentioned in paper) uses similar guidance mechanism but lacks theoretical grounding and uses identical weights for positive/negative samples
- Break condition: If negative policy is poorly trained (captures noise rather than low-reward structure), subtracting its score adds noise rather than meaningful repulsion; if ω is set too high, actions may overshoot into out-of-distribution regions.

## Foundational Learning

- **KL-regularized Reinforcement Learning**
  - Why needed here: DIPOLE builds on the KL-constrained objective framework; understanding that the optimal policy has closed-form π* ∝ μ·exp(βG) is prerequisite to grasping why decomposition helps.
  - Quick check question: Can you explain why constraining policy updates to stay close to a reference policy prevents catastrophic forgetting in offline RL?

- **Diffusion/Flow Matching for Policy Representation**
  - Why needed here: The method trains diffusion models as policies; understanding the denoising process, noise schedules (α_t, σ_t), and score functions is essential for implementing the weighted regression losses.
  - Quick check question: How does the noise prediction network ε_θ relate to the score function ∇ log p(a_t|s) in diffusion models?

- **Classifier-Free Guidance (CFG)**
  - Why needed here: DIPOLE's inference-time combination (1+ω)ε+ - ωε- is structurally identical to CFG; understanding how CFG amplifies conditional signal by subtracting unconditional drift is crucial for the controllability mechanism.
  - Quick check question: In CFG, why does increasing the guidance scale improve conditional fidelity but may harm sample diversity?

## Architecture Onboarding

- **Component map:**
  - Base diffusion policy network (pretrained via imitation): ε_θ(s, a_t, t)
  - Positive policy head (LoRA adapter): ε+_θ1 trained with σ(βG) weights
  - Negative policy head (LoRA adapter): ε-_θ2 trained with (1-σ(βG)) weights
  - Advantage estimator: Q(s,a) and V(s) networks (IQL-style expectile regression)
  - Inference combiner: ε̃ = (1+ω)ε+ - ωε-

- **Critical path:**
  1. Pretrain base diffusion policy on offline dataset (behavior cloning)
  2. Train advantage estimator V(s) via expectile regression, Q(s,a) via TD
  3. Compute G(s,a) = A(s,a) = Q(s,a) - V(s) for each sample
  4. Train positive/negative policies using bounded sigmoid weights (Eq. 9)
  5. At inference, combine with chosen ω and optionally apply rejection sampling

- **Design tradeoffs:**
  - **Shared vs. separate backbones:** Paper uses shared backbone with LoRA adapters (~1.4% params each) for efficiency; full separation may improve expressiveness but increases memory
  - **Rejection sampling:** Adds inference cost (N samples per decision) but improves performance (Table 1 shows DIPOLE w/o rs underperforms full DIPOLE)
  - **β vs. ω:** β controls training-time weight steepness; ω controls inference-time greediness—these can be tuned somewhat independently

- **Failure signatures:**
  - **Positive policy collapse:** If all σ(βG) → 1, positive policy overfits to few top samples; symptoms: high training variance, poor generalization
  - **Negative policy noise:** If (1-σ(βG)) weights too uniform, negative policy learns noise; symptoms: subtracting ε- adds jitter to actions
  - **Advantage misestimation:** If Q/V poorly calibrated, sigmoid weights misrank samples; symptoms: policy underperforms simple behavior cloning baseline
  - **ω overshoot:** At very high ω (>10), combined actions may leave data support; symptoms: unrealistic/collision-causing trajectories in driving domain

- **First 3 experiments:**
  1. **Ablation on weight boundedness:** Compare exp(βG) vs. σ(βG) weighting on Walker-walk task; plot training loss variance and final return. Expect: exp weighting diverges at high β, sigmoid remains stable.
  2. **ω sensitivity sweep:** For a fixed trained model, sweep ω ∈ {0.5, 1, 2, 5, 10} and plot performance vs. trajectory diversity. Expect: sweet spot around ω=2-5, degradation at extremes.
  3. **Scaling sanity check:** Train small MLP policy on ExORL Jaco vs. full VLA on NAVSIM; compare per-parameter gradient norms and training stability. Expect: similar loss curves (normalized), confirming scalability.

## Open Questions the Paper Calls Out
- **Performance dependence on value function quality:** The paper explicitly acknowledges in Appendix F that performance is highly dependent on the quality of the value function. If the advantage estimation is inaccurate, the sigmoid weights will misrank samples, potentially leading to suboptimal policy decomposition.
- **Behavior-regularized constraint:** The method remains within the behavior-regularized optimization framework, which inherently constrains the policy relative to a reference policy. The paper does not validate whether DIPOLE overcomes the fundamental support constraint of offline RL or can effectively extrapolate to high-reward regions absent from the offline dataset.
- **Computational overhead:** While the paper demonstrates the method on large-scale models (1B parameters), it does not address the computational overhead of maintaining and evaluating two distinct neural networks (or LoRA adapters) during deployment, which could be critical for real-time applications like autonomous driving.

## Limitations
- Performance is highly dependent on the quality of the learned advantage function—poorly estimated Q/V networks will propagate errors through the sigmoid weights
- The method remains within behavior-regularized frameworks, inheriting limitations around distributional constraints
- Large-scale evaluation shows improvements but lacks direct ablation of the DIPOLE mechanism versus scale alone

## Confidence

**High confidence:** Stability improvements from bounded sigmoid weights vs. exponential; controllability through ω at inference.

**Medium confidence:** Scalability to billion-parameter models; generalization across diverse benchmarks.

**Low confidence:** Long-horizon performance in complex environments; sensitivity to exact hyperparameter choices (β, k shifts).

## Next Checks
1. Conduct ablation studies on advantage estimation quality—compare DIPOLE performance using learned vs. ground-truth advantages on a controlled toy task.
2. Evaluate policy performance on out-of-distribution states to quantify the behavior-regularization constraint's impact on exploration.
3. Perform a systematic sweep of β (training sigmoid steepness) and ω (inference greediness) to identify optimal operating regions and failure modes.