---
ver: rpa2
title: 'GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via General
  Samples Replay'
arxiv_id: '2508.04676'
source_url: https://arxiv.org/abs/2508.04676
tags:
- tasks
- replay
- samples
- task
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces General Sample Replay (GeRe), a simple yet
  effective method to address catastrophic forgetting in continual learning of large
  language models (LLMs). The core idea is to use a small, fixed set of general pretraining
  texts for replay learning, combined with an activation state constrained optimization
  via threshold-based margin (TM) loss.
---

# GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via General Samples Replay

## Quick Facts
- **arXiv ID**: 2508.04676
- **Source URL**: https://arxiv.org/abs/2508.04676
- **Reference count**: 40
- **Primary result**: General pretraining text replay alone outperforms task-specific replay in continual LLM learning

## Executive Summary
This paper introduces General Sample Replay (GeRe), a method addressing catastrophic forgetting in continual learning of large language models (LLMs). The core innovation is using a small, fixed set of general pretraining texts for replay learning, combined with an activation state constrained optimization via threshold-based margin (TM) loss. Unlike traditional replay methods requiring laborious task-specific replay samples, GeRe demonstrates that replaying general samples alone can simultaneously preserve general capabilities and enhance performance on sequential downstream tasks. Through extensive experiments on 15 diverse tasks using Llama-3.1-8B, GeRe consistently outperforms standard replay strategies (including logit and feature imitation) in both full-parameter and LoRA settings.

## Method Summary
GeRe uses a fixed 1K general replay samples from SlimPajama-627B corpus. The method involves offline distillation of these samples to extract last-layer hidden states and compute per-dimension activation thresholds. During training, replay samples are mixed with downstream task data using batch insertion (4 replay + 60 task samples per batch). The model is jointly optimized with Cross-Entropy (CE) loss for the task and Threshold-based Margin (TM) loss for replay, with dynamic weight balancing. Full-parameter: LR=3e-6, 15 epochs/task. LoRA: LR=1e-4, 8 epochs/task, r=8, α=32, q_proj/k_proj only.

## Key Results
- GeRe achieves 60.78% MMLU score under full-parameter tuning, outperforming standard replay methods
- Average task performance (AP) reaches 74.48% (F1 Avg 66.94) across 15 tasks
- TM loss consistently outperforms L1/L2 feature imitation and KL-based logit imitation
- Batch Insertion strategy improves stability compared to variable mixing approaches

## Why This Works (Mechanism)

### Mechanism 1
A fixed set of general pretraining texts can serve as a permanent replay buffer to mitigate catastrophic forgetting. The general samples implicitly capture the base model's world knowledge and reasoning patterns. By mixing these samples into every task's training, the model is continuously constrained to maintain activation patterns associated with general capabilities, thereby anchoring core knowledge while adapting to new tasks.

### Mechanism 2
Constraining the activation states of neurons during replay via Threshold-based Margin (TM) loss improves robustness over standard feature imitation. TM loss discretizes hidden-state activations into three states (positive, non-activated, negative) using statistically derived thresholds. Instead of forcing exact value matching, TM applies a margin loss that penalizes predictions only when they exit the acceptable threshold region, providing a softer constraint that preserves qualitative activation patterns.

### Mechanism 3
Batch Insertion (BI)—ensuring each training batch contains a fixed proportion of replay samples—stabilizes continual learning by guaranteeing consistent gradient influence from replay data. By modifying the data sampler to inject ρBI × nbatch replay samples into every batch, BI avoids scenarios where long sequences of downstream-only batches cause drift.

## Foundational Learning
- **Continual Learning & Catastrophic Forgetting**: Why needed here because GeRe is specifically a continual-learning method addressing forgetting in LLM sequential fine-tuning. Quick check: Can you explain why updating a model on a new task typically degrades its performance on earlier tasks?
- **Knowledge Distillation (Logit & Feature-based)**: Why needed here because the paper compares TM loss against KL-based logit imitation and L1/L2 feature imitation as distillation strategies. Quick check: What is the difference between logit distillation (e.g., KD loss) and feature distillation (e.g., matching intermediate representations)?
- **Neural Activation States in Deep Learning**: Why needed here because TM loss operates on hidden-state activations, interpreting them as positive/negative/non-activated states based on thresholds. Quick check: In a feed-forward network, what are "activations," and why might their statistical distribution matter for knowledge preservation?

## Architecture Onboarding
- **Component map**: General Replay Sample Set D(g) -> Offline Distillation Step -> TM Loss Module -> Batch Insertion Sampler -> Dynamic Weight Balancer
- **Critical path**: 1. Prepare and distill D(g) once (offline). 2. For each downstream task: Load task data with BI sampler -> Forward pass -> Compute L_CE and L_TM -> Compute dynamic weight ω_TM -> Total loss L = L_CE + ω_TM · L_TM -> Backprop and update -> Evaluate after each task
- **Design tradeoffs**: Replay Set Size & Quality (larger may be better but increases storage), Loss Weight Strategy (fixed vs dynamic), Layer for TM (only last layer vs multiple layers)
- **Failure signatures**: Over-constraint (TM dominates, slow learning), Under-constraint (L_TM decays to near-zero), Instability with BI (on tiny datasets, reduces replay variety)
- **First 3 experiments**: 1. Validate core claim: Baseline vs BaselineR on 3-5 tasks, verify MMLU and AP improve with replay. 2. Compare distillation strategies: BaselineR+KL, BaselineR+L1, BaselineR+L2, BaselineR+TM, compare final F1 Avg. 3. Test BI sensitivity: BaselineR+TM with/without BI across two task scales, observe whether BI helps more with large-scale tasks.

## Open Questions the Paper Calls Out
- **Unified Loss Weighting**: Can a unified loss weighting strategy be developed that remains effective across both full-parameter and LoRA fine-tuning settings? The authors observe that dynamic weighting performs best for full-parameter tuning, whereas a fixed weight is superior for LoRA, concluding that "Unified settings remain for future research."
- **Optimal Batch Insertion Ratio**: What is the optimal Batch Insertion ratio for general replay samples when scaling to significantly larger downstream datasets? The authors note effectiveness varies and state that "an optimal replay insertion ratio, which warrants further investigation."
- **Multi-layer TM Loss**: Does incorporating hidden states from intermediate layers into the TM loss improve retention of low-level linguistic knowledge compared to using only the last layer? The method explicitly restricts TM loss calculation to the last layer, but catastrophic forgetting often degrades lower-level syntactic capabilities.

## Limitations
- **Sample Set Representativeness**: The claim that 1K general replay samples are universally sufficient rests on an untested assumption about sample quality and domain coverage across all potential downstream domains.
- **Activation State Validity**: The TM loss mechanism assumes discretized activation states meaningfully encode semantic knowledge, but this is not empirically validated beyond observed performance gains.
- **Instruction Template Sensitivity**: The method shows sensitivity to instruction template design, with COPA causing "spurious forgetting" due to format mismatches, suggesting potential fragility in task formatting interactions.

## Confidence
- **High Confidence**: The core empirical finding that GeRe outperforms standard replay methods (KL, L1, L2) across multiple metrics (MMLU, AP, F1 Avg) is well-supported by experimental results.
- **Medium Confidence**: The claim that general samples alone can preserve general capabilities while enhancing sequential task performance is supported but not conclusively proven.
- **Low Confidence**: The mechanism explanation for why activation state constraints work better than exact value matching is speculative, lacking interpretability analysis or ablation studies.

## Next Checks
1. **Domain Transfer Robustness Test**: Apply GeRe to tasks from specialized domains (biomedical, legal, scientific) not well-represented in general pretraining data to measure effectiveness of general replay samples.
2. **Threshold Sensitivity Analysis**: Systematically vary the threshold computation method in TM loss (e.g., 2σ instead of 1σ, quantile-based thresholds) to determine whether the specific TM formulation is critical.
3. **Instruction Template Ablation**: Create multiple instruction template variants for the same downstream tasks and measure both task performance and forgetting on previously learned tasks to quantify sensitivity to instruction design.