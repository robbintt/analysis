---
ver: rpa2
title: Efficient Strategy Synthesis for MDPs via Hierarchical Block Decomposition
arxiv_id: '2506.17792'
source_url: https://arxiv.org/abs/2506.17792
tags:
- sharp
- prism
- state
- refinement
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents SHARP, a hierarchical adaptive refinement approach
  for accelerating policy synthesis in large Markov decision processes (MDPs). The
  method addresses the state-space explosion problem by dynamically partitioning the
  state space and iteratively refining only the most critical regions based on value
  spread criteria.
---

# Efficient Strategy Synthesis for MDPs via Hierarchical Block Decomposition

## Quick Facts
- **arXiv ID:** 2506.17792
- **Source URL:** https://arxiv.org/abs/2506.17792
- **Reference count:** 40
- **Primary result:** Hierarchical adaptive refinement approach achieves up to 2× speedup on large MDPs while maintaining solution accuracy

## Executive Summary
This paper presents SHARP, a hierarchical adaptive refinement approach for accelerating policy synthesis in large Markov decision processes. The method addresses the state-space explosion problem by dynamically partitioning the state space and iteratively refining only the most critical regions based on value spread criteria. SHARP constructs a hierarchy tree where each node represents a partition, solves local sub-MDPs with boundary conditions, and propagates refined values up the hierarchy. The approach was implemented as an extension to PRISM and evaluated on warehouse robotics scenarios with up to 1 million states.

## Method Summary
SHARP accelerates policy synthesis for large MDPs by combining hierarchical state space decomposition with adaptive refinement. The method partitions the state space into blocks, solves local sub-MDPs treating boundary states as absorbing with fixed values from parent nodes, and iteratively refines partitions based on value spread thresholds. The algorithm uses a bottom-up propagation mechanism to maintain global consistency without re-solving the monolithic model. Implementation extends PRISM's explicit engine with geometric partitioning and local VI solvers for sub-MDPs.

## Key Results
- Achieves up to 2× speedup over PRISM's standard solvers on 1024×1024 warehouse MDPs
- Maintains solution accuracy matching PRISM results exactly
- Shows particularly significant improvements (2.5× speedup) for reward-based properties
- Demonstrates near-linear scaling behavior as problem size increases from 512² to 1024² states
- Requires higher memory overhead (18-24GB vs 5GB) due to hierarchy tree maintenance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Partitioning the MDP and solving sub-MDPs with fixed boundary conditions reduces the complexity of global convergence.
- **Mechanism:** SHARP partitions the state space into blocks. It treats boundary states (neighbors outside the current block) as absorbing states with fixed values inherited from the parent node. This transforms the global Bellman update into a set of smaller, independent local fixed-point iterations.
- **Core assumption:** The local sub-MDP solutions provide a sufficient approximation of the global dynamics, assuming boundary values are relatively stable or updated iteratively.
- **Evidence anchors:**
  - [Abstract] Mentions "dynamically partitioning the state space" and solving "local sub-MDPs with boundary conditions."
  - [Section IV-C] Defines the induced sub-MDP $M_B$ where boundary states are made absorbing with fixed values $V_{parent}(t)$.
  - [Corpus] Related work on MDP decomposition (e.g., arXiv:2512.00838) supports the general viability of decomposition for scalability, though SHARP's specific boundary-condition approach is distinct.
- **Break condition:** If boundary values fluctuate significantly between iterations (indicating strong coupling between partitions), convergence may slow or require many global iterations.

### Mechanism 2
- **Claim:** Adaptive refinement based on "value spread" accelerates synthesis by focusing computational effort on heterogeneous regions.
- **Mechanism:** The algorithm calculates the value spread $\Delta(v) = \max(V) - \min(V)$ for a block. If $\Delta(v)$ exceeds a threshold $\theta$, the block is subdivided. This avoids refining "safe" or homogeneous regions (like open space in a warehouse) while refining "fragile" regions (near obstacles or goals).
- **Core assumption:** Regions with low variance in value estimates require less granular modeling to achieve an optimal policy.
- **Evidence anchors:**
  - [Section IV-B] Explicitly defines the value spread criterion and "refine-where-needed" principle.
  - [Figure 3] Visualizes refinement focusing on the goal area (Block 9) rather than the start area.
  - [Corpus] Corpus evidence on specific value-spread heuristics is weak; this appears to be a heuristic contribution of this specific paper.
- **Break condition:** In problems where optimal paths traverse large areas of seemingly "uninteresting" states that actually contain subtle bottlenecks, this heuristic might delay necessary refinement.

### Mechanism 3
- **Claim:** Bottom-up value propagation ensures global consistency without re-solving the monolithic model.
- **Mechanism:** After local sub-MDPs converge, values are propagated up the hierarchy tree. Parents adopt the values of their children. This updates the estimates for boundary conditions in the next iteration.
- **Core assumption:** Information propagates efficiently through the tree depth to update boundary conditions for upstream/downstream blocks.
- **Evidence anchors:**
  - [Section IV-D] Describes `PROPAGATE ALLUP` where parents inherit child values.
  - [Algorithm 1] Lines 8-9 show the iterative solve-propagate loop.
- **Break condition:** If the hierarchy depth $D$ is too shallow, information from one side of the MDP may take many iterations to propagate to the other side via the root, slowing convergence.

## Foundational Learning

- **Concept: Markov Decision Process (MDP) & Policy Synthesis**
  - **Why needed here:** You must understand that SHARP is trying to find a policy $\sigma: S \to A$ (mapping states to actions) that maximizes probability or minimizes cost, rather than just verifying a property.
  - **Quick check question:** Can you distinguish between the probability of reaching a goal ($P_{max}$) and the expected cost to do so ($R_{min}$)?

- **Concept: Value Iteration (VI) & Bellman Operator**
  - **Why needed here:** SHARP is not a new solver logic; it is an acceleration wrapper *around* Value Iteration. You need to know that VI iteratively updates state values until they stop changing (convergence).
  - **Quick check question:** If a state's value is updated based on its neighbors, why does fixing the neighbors' values (boundary conditions) allow us to solve a smaller system?

- **Concept: The State-Space Explosion Problem**
  - **Why needed here:** This is the motivation. As grid size increases ($512^2$ to $1024^2$), the number of states grows quadratically. Standard VI requires sweeping over all states in every iteration, becoming prohibitively slow.
  - **Quick check question:** Why does treating a group of states as a single block (hierarchical abstraction) reduce the computational complexity?

## Architecture Onboarding

- **Component map:**
  - Input: MDP model ($S, A, \delta, r$), Property ($P_{max}/R_{min}$), Hyperparameters ($N_x, N_y, \theta, D$)
  - Partitioner: Divides the root node into an initial grid (e.g., $8 \times 8$)
  - Hierarchy Tree: Data structure holding nodes (blocks) and their depth. Stores local values/policies
  - Sub-MDP Solver: Standard VI engine applied to a single block + boundary states
  - Refinement Engine: Checks $\Delta(v) > \theta$ and splits nodes
  - Output: Global value vector and memoryless policy $\sigma$

- **Critical path:** The `while` loop in Algorithm 1 (Lines 7-15). The system must: (1) Solve Leaves → (2) Propagate Up → (3) Check Spread → (4) Refine. The speedup depends on the Refinement Engine keeping the number of "Solve" operations low compared to full sweeps.

- **Design tradeoffs:**
  - **Memory vs. Speed:** The paper notes SHARP uses significantly more memory (up to 18-24GB vs 5GB for PRISM on large models) to maintain the hierarchy tree and intermediate data structures, trading RAM for CPU time
  - **Overhead vs. Scale:** For small models ($< 2 \cdot 10^5$ states), SHARP is *slower* than standard PRISM (e.g., 1.09s vs 0.14s) due to the overhead of managing partitions. Use SHARP only for large-scale problems

- **Failure signatures:**
  - **Stagnation:** If $\theta$ is set too high, SHARP never refines, resulting in a coarse, potentially suboptimal policy
  - **Memory Exhaustion:** Setting initial partitions too small or depth $D$ too high on dense graphs may cause OutOfMemory errors due to excessive leaf nodes (See Table III)
  - **Non-convergence:** If boundary values oscillate, the global loop (`maxIters`) may hit the limit without convergence

- **First 3 experiments:**
  1.  **Baseline Sanity Check:** Run SHARP on a small 128x128 grid against standard PRISM. *Expectation:* SHARP may be slower. Confirm results match PRISM exactly.
  2.  **Scaling Stress Test:** Run on 1024x1024 "Single Wall" (W1024SW-b). Vary initial partitions ($8 \times 8$ vs $16 \times 16$). *Expectation:* Identify the sweet spot where speedup > 1.5x (likely at $8 \times 8$ or $12 \times 12$)
  3.  **Refinement Sensitivity:** Test depth $D \in \{1, 3, 5\}$ on a model with a single narrow bottleneck (high uncertainty). *Expectation:* Higher depth should trigger refinement only at the bottleneck, maintaining speed while solving the critical region

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can SHARP be effectively adapted to support non-grid domains and highly irregular MDP structures, such as those found in self-adaptive systems or software product lines?
- **Basis in paper:** [explicit] The authors explicitly list "extending SHARP to support non-grid domains" as a primary objective for future work (Section VIII). They also note in Section VI-D that "more experiments on non-grid and highly irregular MDPs... are needed to establish SHARP's applicability."
- **Why unresolved:** The current evaluation is restricted to robotic warehouse scenarios modeled as 2D grids. The authors rely on "geometric partitioning" (Section IV-A), and it is unclear if the value-spread refinement criteria apply effectively to irregular state spaces without spatial coordinates.
- **What evidence would resolve it:** A demonstration of SHARP applied to non-spatial benchmarks (e.g., software product lines) with a modified partitioning strategy, showing comparable speedups and accuracy to the grid-based results.

### Open Question 2
- **Question:** How can SHARP be extended to synthesize policies for multi-objective properties?
- **Basis in paper:** [explicit] The authors list extending SHARP to "support... multi-objective properties" as a key component of their future work (Section VIII).
- **Why unresolved:** The current implementation only handles single reachability objectives ($P_{opt}$ or $R_{opt}$). Multi-objective synthesis requires reasoning about trade-offs (Pareto fronts), and it is unclear how hierarchical refinement based on a single value spread would handle conflicting objective gradients.
- **What evidence would resolve it:** An algorithmic extension defining multi-dimensional boundary conditions and a refinement criterion that accounts for value spreads across multiple objectives simultaneously.

### Open Question 3
- **Question:** Can the efficiency of SHARP's refinement strategy be improved by incorporating domain-specific knowledge?
- **Basis in paper:** [explicit] The authors state a goal to "enhance SHARP's refinement strategy by incorporating domain knowledge" (Section VIII).
- **Why unresolved:** Currently, refinement is triggered by a generic "value spread" heuristic (threshold $\theta$). The authors suggest domain knowledge could optimize this, but they do not specify what form this would take or how it would integrate with the hierarchical tree construction.
- **What evidence would resolve it:** A study comparing the current agnostic refinement strategy against a domain-informed variant (e.g., one that prioritizes partitions near known obstacles) in terms of iterations to convergence and total runtime.

### Open Question 4
- **Question:** Does SHARP retain its performance advantages when implemented within a highly optimized C++ toolchain like Storm, compared to its current Java/PRISM implementation?
- **Basis in paper:** [inferred] While the authors compare against PRISM, they acknowledge in Section VI-D that comparing against Storm entails "implementing the core of our approach... within these tools." They currently isolate algorithmic gains within PRISM to avoid confounding variables, leaving the performance in a native C++ environment an open question.
- **Why unresolved:** PRISM (Java) and Storm (C++) have different memory management and numerical library performance profiles. It is possible that Storm's optimized native engine reduces the marginal gain of SHARP's hierarchical approach, or conversely, that SHARP accelerates Storm further.
- **What evidence would resolve it:** A direct benchmark comparison of a native SHARP implementation inside the Storm model checker against Storm's standard solvers on the same warehouse benchmarks.

## Limitations

- The adaptive refinement threshold θ appears to be tuned for specific warehouse scenarios and may not generalize well to other MDP structures where value spreads have different statistical properties
- The paper provides limited analysis of convergence guarantees for the hierarchical approach, particularly when strong coupling exists between distant partitions
- Memory overhead (18-24GB vs 5GB) represents a significant trade-off that may limit deployment on resource-constrained systems

## Confidence

- **High confidence:** SHARP achieves speedups on large warehouse MDPs (1024×1024) as claimed, supported by Table III showing consistent improvements
- **Medium confidence:** The mechanism of hierarchical decomposition with boundary conditions works as described, though theoretical convergence analysis is limited
- **Low confidence:** The generalizability of value spread-based refinement to non-grid MDPs and other application domains

## Next Checks

1. **Convergence stress test:** Evaluate SHARP on MDPs with known strong coupling (e.g., narrow corridors connecting distant regions) to measure how many iterations are required for boundary value propagation across the hierarchy
2. **Parameter sensitivity analysis:** Systematically vary θ and initial partition sizes across different MDP topologies to establish robust parameter ranges rather than tuned values
3. **Cross-domain validation:** Apply SHARP to non-grid MDPs (e.g., random graphs, biological networks) to assess whether the value spread heuristic remains effective outside the warehouse robotics domain