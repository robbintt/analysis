---
ver: rpa2
title: Scalable Data Synthesis for Computer Use Agents with Step-Level Filtering
arxiv_id: '2512.10962'
source_url: https://arxiv.org/abs/2512.10962
tags:
- action
- steps
- filtering
- step-level
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of training computer use agents
  (CUAs) by developing a scalable data synthesis pipeline that converts noisy teacher
  rollouts into high-quality training data without human annotation. The core idea
  is step-level filtering, which evaluates each action individually using a grading
  model and retains only correct steps, combined with thought augmentation to strengthen
  reasoning.
---

# Scalable Data Synthesis for Computer Use Agents with Step-Level Filtering

## Quick Facts
- **arXiv ID**: 2512.10962
- **Source URL**: https://arxiv.org/abs/2512.10962
- **Reference count**: 38
- **Primary result**: Step-level filtering improves 7B CUA performance on WebVoyager by 15% over SoTA, with distilled 7B StepRM matching o4-mini grading quality.

## Executive Summary
This paper tackles the challenge of training computer use agents (CUAs) by developing a scalable data synthesis pipeline that converts noisy teacher rollouts into high-quality training data without human annotation. The core innovation is step-level filtering, which evaluates each action individually using a grading model and retains only correct steps, combined with thought augmentation to strengthen reasoning. Using this pipeline, the authors synthesize WebSTAR, a dataset of 13.3K trajectories and 267K graded steps from OpenAI's computer-use-preview model, and train Qwen-2.5-VL-Instruct models (7B and 32B). On WebVoyager, the 7B model trained with step-level filtering surpasses the SoTA open-source CUA UI-TARS-1.5-7B by over 15%. Additionally, they construct WebSCORE, a dataset for step-level grading, and train StepRM, a 7B reward model distilled from o4-mini that matches its grading quality at much lower cost. The results establish step-level filtering as essential for scalable CUA training, with WebSTAR and WebSCORE providing new high-quality resources for the field.

## Method Summary
The paper presents a three-stage pipeline: (1) collect noisy trajectories from a teacher CUA (OpenAI computer-use-preview) on 1,326 OpenWebVoyager queries, (2) generate structured thoughts for each step using o4-mini and score steps individually with o4-mini (0-10 scale), and (3) filter steps with score > 5 for SFT training. The authors train Qwen-2.5-VL-7B/32B-Instruct models with frozen vision towers, using LlamaFactory on A100 80GB GPUs with sequence length 8192, learning rate 1e-5, and 2 epochs. They also distill StepRM from o4-mini using WebSCORE (200K steps) to enable cost-effective step-level filtering at scale.

## Key Results
- Step-level filtering yields 47.0% average success rate vs. 31.9% for trajectory-level filtering on 7B models
- 7B model trained with step-level filtering outperforms UI-TARS-1.5-7B by over 15% on WebVoyager
- StepRM (7B) matches o4-mini grading quality while being far more efficient to deploy
- Thought augmentation consistently improves Pass@4 metrics across benchmarks and model sizes
- 32B model trained with step-level filtering achieves 59.1% average success rate on WebVoyager

## Why This Works (Mechanism)

### Mechanism 1: Step-Level Filtering Aligns Supervision with Training Objective
- Claim: Selectively training on high-quality individual steps from otherwise noisy trajectories improves agent performance more than using entire (successful) trajectories.
- Mechanism: The standard Supervised Fine-Tuning (SFT) objective for CUAs trains a model to predict the correct thought and action at each step, conditioned on history. This is a step-level prediction problem. Trajectory-level filtering, which keeps all steps from a successful trajectory, includes many suboptimal or incorrect intermediate actions. This misaligns the optimization signal, pushing the model to replicate flawed decisions. Step-level filtering evaluates each action individually, masking the loss for incorrect steps while retaining them in the context. This ensures the optimization signal is derived only from correct, goal-directed actions.
- Core assumption: A capable grading model (here, o4-mini or a distilled reward model like StepRM) can reliably distinguish between a correct step and an incorrect/suboptimal one.
- Evidence anchors:
  - [abstract] The core idea is step-level filtering, which evaluates actions individually to retain only correct steps.
  - [Section 3.2] "The key limitation of trajectory-level filtering is that even successful trajectories often contain suboptimal or incorrect intermediate steps... The SFT objective only applies to the correct steps."
  - [Section 4.2] Training on correct steps yields a 47.0% average success rate vs. 31.9% for trajectory-level filtering on a 7B model.
  - [corpus] Research on GUI Critic Models (OS-Oracle) and predictive guardrails (SafePred) similarly emphasize step-level or predictive evaluation to improve agent reliability and safety.

### Mechanism 2: Thought Augmentation Enhances Robustness and Stability
- Claim: Adding structured reasoning traces ("thoughts") to each step before the action improves an agent's consistency and reliability.
- Mechanism: Generating explicit thoughts forces the model to articulate a situation description, reasoning, and actionable instruction. This creates a more robust bridge between visual perception and action. The process reduces variance in decision-making by grounding the action in an interpretable, step-by-step rationale.
- Core assumption: The generated thoughts are of high quality and correctly guide the action.
- Evidence anchors:
  - [abstract] The pipeline is "complemented by reasoning augmentation for improved planning."
  - [Section 3.3] Thoughts are structured with situation description, reasoning, and actionable instruction. "Thought augmentation enhances robustness" with results showing consistent benefits in Pass@4 metrics.
  - [Section 3.3] Pass@4 provides a more faithful measure of performance... "Across benchmarks and model sizes, Pass@4 consistently improves with thought augmentation."
  - [corpus] Related work (e.g., LPS-Bench) underscores long-horizon planning as a key challenge for CUA safety and performance, supporting the need for explicit reasoning.

### Mechanism 3: Efficient Step-Level Grading via Distillation
- Claim: A lightweight 7B multimodal reward model (StepRM), distilled from a powerful but expensive proprietary grader (o4-mini), can perform step-level filtering at scale cost-effectively.
- Mechanism: The expensive o4-mini model is used to grade a large dataset (WebSCORE, 200K steps), creating a labeled dataset for distillation. A smaller, open model (StepRM based on Qwen2.5-VL-7B) is trained on this data to predict the scalar quality score. This model can then replace o4-mini in the pipeline, drastically reducing the cost of grading thousands of steps per trajectory.
- Core assumption: The student model (StepRM) can generalize the grading capability of the teacher model (o4-mini) to new, unseen steps and trajectories.
- Evidence anchors:
  - [abstract] "We train StepRM, a 7B multimodal reward model distilled from o4-mini, which matches its grading quality while being far more efficient to deploy at scale."
  - [Section 4.4] "StepRM achieves performance on par with o4-mini across multiple benchmarks, while being far more efficient..."
  - [corpus] Other work on scalable oversight (VerificAgent) and safety (SafePred) suggests efficient critic or guardrail models are a key area of interest for practical CUA deployment.

## Foundational Learning

- Concept: **Supervised Fine-Tuning (SFT) Objective**
  - Why needed here: This is the core mathematical framework for training the CUA. Understanding the loss function and how step-level filtering modifies it with a binary mask is essential to grasp the mechanism.
  - Quick check question: How does the loss function differ when using step-level filtering versus trajectory-level filtering?

- Concept: **ReAct Framework (Reasoning + Acting)**
  - Why needed here: The paper uses "thought augmentation" based on this paradigm. Knowing that an agent generates a thought before an action is key to understanding the training data structure.
  - Quick check question: What are the three key components of a generated thought as described in the paper?

- Concept: **Data Synthesis Pipeline**
  - Why needed here: The entire contribution is built around this pipeline (Rollout -> Grading -> Filtering). One must understand the flow from raw, noisy teacher data to a clean training set.
  - Quick check question: Why can't we just use the raw trajectories from a strong teacher CUA (like OpenAI's) for SFT without filtering?

## Architecture Onboarding

- Component map: Teacher CUA -> Thought Augmentation -> Step Grading -> Filtering -> Student CUA
- Critical path: `Rollout -> Thought Augmentation -> Step Grading (o4-mini) -> Step Filtering -> SFT (Student CUA)`. The optional distillation path runs in parallel: `Graded Steps -> Train StepRM -> Deploy StepRM as Grader`.
- Design tradeoffs:
  - **Grading Cost vs. Scale**: Using o4-mini for grading is accurate but expensive at scale. Training and using StepRM is cheaper upfront but requires the distillation step and may have slightly lower fidelity.
  - **Strictness vs. Data Retention**: A higher grading threshold yields cleaner data but discards more steps, requiring more initial rollouts to build a dataset of a given size. The paper chooses 5 as a practical balance.
  - **SFT-only vs. RL**: The paper focuses on SFT for its stability and low cost but acknowledges RL could further improve performance, especially for long-horizon credit assignment.
- Failure signatures:
  - **Performance Plateau**: If the student CUA fails to improve, check the grading model's accuracy. A bad grader will produce a noisy training set.
  - **Overly Cautious Agent**: If the agent struggles with tasks requiring exploration, the grading model might be too strict (penalizing exploratory steps).
  - **Context Detachment**: If the agent ignores past actions/screenshots, ensure the input to the policy includes the full sequence of past thoughts/actions, even for masked steps.
- First 3 experiments:
  1. **Trajectory vs. Step-Level Filtering Ablation**: Train identical student models on the *same* raw rollouts. Train one using only trajectory-level filtering (all steps in successful trajectories) and one using step-level filtering. Compare task success rates on a benchmark like WebVoyager.
  2. **Grading Threshold Sweep**: Using a fixed dataset size, train student models with different grading score cutoffs (e.g., 2, 4, 5, 6, 8) to find the optimal balance between data quality and quantity.
  3. **StepRM vs. o4-mini Grading**: Compare the performance of student models trained on data filtered by the distilled StepRM versus data filtered by the teacher o4-mini to validate the cost-effective alternative for scalable synthesis.

## Open Questions the Paper Calls Out

- **Can StepRM be effectively integrated into reinforcement learning frameworks (e.g., PPO) to enable sample-efficient training with dense step-level rewards?**
  - Basis in paper: [explicit] Section 4.4 states: "We leave the integration of StepRM into RL pipelines as an exciting direction for future work."
  - Why unresolved: StepRM was only evaluated as a grading filter for SFT; its potential as a critic for online RL remains unexplored despite the authors identifying sparse rewards and credit assignment as key CUA training challenges.
  - What evidence would resolve it: Training a CUA with PPO using StepRM as the reward model, comparing sample efficiency and final performance against SFT-only baselines.

- **How does the data synthesis pipeline perform when the teacher CUA has low task success rates in the target domain?**
  - Basis in paper: [explicit] Limitations section states: "the scalability of our pipeline is partly constrained by the quality of the teacher CUA used for data collection... As CUAs themselves improve, we expect this bottleneck to diminish."
  - Why unresolved: The pipeline assumes access to a capable teacher (OpenAI CUA), but domains where even strong CUAs rarely succeed would yield insufficient correct steps for training.
  - What evidence would resolve it: Measuring downstream student performance when synthesizing data from progressively weaker teacher models or harder task domains.

- **Does step-level filtering generalize to desktop OS environments beyond web browsers?**
  - Basis in paper: [inferred] All experiments are conducted on web-based benchmarks (WebVoyager, Mind2Web), though the paper cites desktop environments (OSWorld, Windows Agent Arena) as relevant CUA domains. The action space in Table 1 includes desktop operations, but no desktop evaluation is performed.
  - Why unresolved: Desktop environments introduce different challenges (window management, multi-application workflows) that may affect grading consistency and filtering effectiveness.
  - What evidence would resolve it: Evaluating models trained with step-level filtering on OSWorld or Windows Agent Arena benchmarks.

## Limitations
- The pipeline's scalability is constrained by the quality of the teacher CUA used for data collection
- All experiments are conducted on web-based benchmarks, with no evaluation on desktop OS environments
- The paper focuses on SFT and leaves RL integration as future work, potentially missing performance gains from online learning

## Confidence
- **High**: Step-level filtering consistently outperforms trajectory-level filtering across model sizes and benchmarks
- **High**: Thought augmentation provides consistent improvements in Pass@4 metrics
- **Medium**: StepRM achieves performance on par with o4-mini but requires distillation overhead
- **Low**: The pipeline's effectiveness in desktop environments remains unexplored

## Next Checks
- Verify the 15% performance improvement over UI-TARS-1.5-7B on WebVoyager by reproducing the ablation study comparing step-level vs. trajectory-level filtering
- Check the grading consistency by measuring score variance across repeated o4-mini calls (reported median std 0.49)
- Validate the cost-effectiveness of StepRM by comparing training time and inference latency against o4-mini grading