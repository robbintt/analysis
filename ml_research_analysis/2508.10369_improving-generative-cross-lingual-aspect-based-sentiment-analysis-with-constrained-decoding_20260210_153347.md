---
ver: rpa2
title: Improving Generative Cross-lingual Aspect-Based Sentiment Analysis with Constrained
  Decoding
arxiv_id: '2508.10369'
source_url: https://arxiv.org/abs/2508.10369
tags:
- cross-lingual
- language
- absa
- sentiment
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of aspect-based sentiment analysis
  (ABSA) in low-resource languages, where current approaches are limited to simpler
  tasks and often rely on unreliable translation tools. The authors propose a novel
  method using constrained decoding with sequence-to-sequence models to eliminate
  translation dependencies and improve cross-lingual performance.
---

# Improving Generative Cross-lingual Aspect-Based Sentiment Analysis with Constrained Decoding

## Quick Facts
- **arXiv ID:** 2508.10369
- **Source URL:** https://arxiv.org/abs/2508.10369
- **Reference count:** 40
- **Primary result:** Novel constrained decoding method improves cross-lingual ABSA performance by up to 5% F1, outperforming translation-based approaches

## Executive Summary
This paper addresses the challenge of aspect-based sentiment analysis (ABSA) in low-resource languages by proposing a generative approach using sequence-to-sequence models with constrained decoding. The method eliminates the need for translation tools by training on source-language data and applying the model directly to target languages through multilingual pre-trained representations. By constraining the token vocabulary during generation based on the input sentence and task-specific markers, the approach prevents the model from generating aspect terms in the wrong language. Experiments across seven languages and six ABSA tasks demonstrate significant performance improvements over state-of-the-art methods, with the most complex task (TASD) showing an average 5% F1 improvement.

## Method Summary
The authors frame ABSA as a sequence-to-sequence generation problem, using multilingual pre-trained models (mT5 or mBART) fine-tuned on source-language data with special element markers ([A], [C], [P]) to indicate aspect terms, categories, and sentiment polarity. The key innovation is scheme-guided constrained decoding, which dynamically restricts the token vocabulary during generation based on the current state. When generating aspect terms, only tokens present in the input sentence are allowed; for categories, only predefined category tokens are permitted; and for sentiment polarity, only "great," "ok," or "bad" are valid. This approach enables zero-shot cross-lingual transfer without requiring target-language annotations or translation tools, and supports multi-tasking where a single model handles all ABSA tasks.

## Key Results
- Constrained decoding improves cross-lingual ABSA performance by an average of 5% F1 on the most complex TASD task
- The method eliminates translation dependencies while outperforming state-of-the-art approaches that rely on translation tools
- Multi-tasking results show over 10% improvement when using constrained decoding across all tasks
- mT5-large consistently outperforms mBART and achieves competitive results with fine-tuned LLMs, though with longer training and inference times

## Why This Works (Mechanism)

### Mechanism 1: Scheme-Guided Constrained Decoding
- Claim: Constraining the token vocabulary during generation improves cross-lingual ABSA performance by preventing the model from generating aspect terms in the wrong language.
- Mechanism: At each decoding step, the algorithm dynamically restricts candidate tokens based on the current generation state. When generating aspect terms, only tokens present in the input sentence are allowed. For aspect categories, only predefined category tokens are permitted. For sentiment polarity, only "great," "ok," or "bad" are valid.
- Core assumption: The model has learned correct semantic mappings but may generate tokens from the wrong vocabulary due to source-language dominance during fine-tuning.
- Evidence anchors:
  - [abstract] "constrained decoding with sequence-to-sequence models to eliminate translation dependencies and improve cross-lingual performance"
  - [Section 3.4] Algorithm 1 shows the state-based token restriction logic
  - [Section 5.1] "constrained decoding mitigates this problem effectively by restricting the available tokens for generation to only those from the input target language sentence"

### Mechanism 2: Element Marker Prompting for Structured Generation
- Claim: Special token markers ([A], [C], [P]) appended to input and prepended to outputs guide the model to generate structured sentiment tuples.
- Mechanism: The input prompt "sentence | [A] [C] [P]" signals the expected output format. During training, the model learns to generate "[A] term [C] category [P] polarity" sequences. This transforms ABSA from classification into conditional text generation.
- Core assumption: The pre-trained multilingual model has sufficient transferable representations to map sentiment concepts across languages when given consistent format cues.
- Evidence anchors:
  - [Section 3.2] "These markers precede each element, collectively forming the target sequence"
  - [Section 3.3] Equation 2 shows the conditional probability training objective
  - [corpus] Related work by Zhang et al. (2021) uses similar paraphrase generation for ABSA, supporting the generative framing

### Mechanism 3: Cross-Lingual Zero-Shot Transfer via mPLM Representations
- Claim: Fine-tuning on source-language data enables zero-shot inference on target languages through shared multilingual representations.
- Mechanism: The multilingual pre-trained encoder (mT5 or mBART) creates language-agnostic contextual embeddings. Fine-tuning on source-language ABSA data teaches the decoder to extract sentiment tuples from these representations, which transfer to target languages without additional fine-tuning.
- Core assumption: Pre-training has aligned semantic representations across languages sufficiently for the sentiment extraction task.
- Evidence anchors:
  - [Section 1] "these multilingual models are fine-tuned on labelled data in the source language and applied directly to target language data"
  - [Table 4-7] Cross-lingual results are typically 4-10% worse than monolingual, demonstrating partial transfer
  - [corpus] XLM-R and mT5 are established cross-lingual transfer baselines (Conneau et al., 2020; Xue et al., 2021)

## Foundational Learning

- **Sequence-to-Sequence Models (Encoder-Decoder)**
  - Why needed here: The entire methodology frames ABSA as text generation rather than classification, requiring understanding of how encoders create contextual representations and decoders generate conditioned outputs.
  - Quick check question: Can you explain how the encoder's output representation conditions the decoder's generation at each timestep?

- **Beam Search vs. Greedy Decoding**
  - Why needed here: The paper uses greedy search but constrained decoding could be combined with beam search; understanding search strategies is essential for implementation.
  - Quick check question: How does constrained decoding modify the candidate selection at each beam search step?

- **Zero-Shot Cross-Lingual Transfer**
  - Why needed here: The core experimental setup involves training on one language and testing on another without target-language labels.
  - Quick check question: What properties of multilingual pre-training enable zero-shot transfer, and when would you expect it to fail?

## Architecture Onboarding

- **Component map:**
  Input Text → Element Marker Appender → mT5/mBART Encoder → Contextual Embeddings → Decoder with Constrained Decoding → Generated Output: [A] term [C] category [P] polarity

- **Critical path:**
  1. Construct input: `sentence + " | [A] [C] [P]"`
  2. Fine-tune mT5/mBART on source-language data for 20 epochs
  3. At inference, apply Algorithm 1's constrained decoding logic after each token generation

- **Design tradeoffs:**
  - **mT5 vs. mBART**: mT5 outperforms mBART in most settings (Table 4-7). mT5 uses Adafactor optimizer with lr=1e-4; mBART uses AdamW with lr=1e-5.
  - **Single-task vs. Multi-task**: Multi-task models require 6× training data and 7.45× training time (Table 10) but enable one model for all tasks with comparable performance when using constrained decoding.
  - **LLM vs. mT5**: LLMs (Orca 2 13B) achieve similar performance but require 15.32× inference time and QLoRA for fine-tuning on consumer GPUs.

- **Failure signatures:**
  - **Source-language generation**: Model generates aspect terms in source language instead of target → constrained decoding fixes this
  - **Format violations**: Missing markers or malformed output → constrained decoding enforces structure
  - **Turkish target**: Consistently lowest performance across all models (non-Indo-European language, limited pre-training data)
  - **LLM zero-shot**: F1 scores below 30% for compound tasks (Table 6-8) → requires fine-tuning

- **First 3 experiments:**
  1. **Baseline validation**: Fine-tune mT5 on English training data, evaluate on English test set without constrained decoding. Expected F1: ~67% for TASD (Table 6, monolingual). This validates the training pipeline.
  2. **Cross-lingual transfer test**: Same model from Experiment 1, evaluate on Spanish test set without constrained decoding. Expect F1 drop (~48%) and observe source-language aspect term generation errors. This demonstrates the problem constrained decoding solves.
  3. **Constrained decoding validation**: Apply Algorithm 1 to the same cross-lingual setup. Expect ~57% F1 (Table 6, en→es with CD), confirming the 5-10% improvement claimed. Verify no source-language tokens appear in output.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the constrained decoding approach maintain its performance advantages when applied to domains other than restaurant reviews?
- Basis in paper: [explicit] The Conclusion section explicitly proposes verifying "the effectiveness of our cross-lingual ABSA methods across different domains and languages, expanding beyond the restaurant domain."
- Why unresolved: The study relies exclusively on datasets derived from SemEval-2016 Task 5 and CsRest-M, which cover only the restaurant domain.
- What evidence would resolve it: Experiments applying the method to diverse domains (e.g., laptop or hotel reviews) in cross-lingual settings to compare performance against the established restaurant benchmarks.

### Open Question 2
- Question: Is it possible to successfully transfer sentiment-related knowledge simultaneously across different domains and languages?
- Basis in paper: [explicit] The Conclusion lists "exploring cross-domain, cross-lingual ABSA" as a potential direction to assess the potential of transferring knowledge across different domains and languages at the same time.
- Why unresolved: Existing research typically treats cross-lingual and cross-domain transfer as separate problems; combining them introduces significant complexity that remains uninvestigated.
- What evidence would resolve it: A study evaluating models trained on a source domain/language (e.g., English restaurants) and tested on a target domain/language (e.g., French laptops) to measure transfer degradation.

### Open Question 3
- Question: Can future iterations of multilingual Large Language Models (LLMs) outperform smaller models using constrained decoding without requiring expensive fine-tuning?
- Basis in paper: [inferred] The authors note that current LLMs struggle in zero-shot and few-shot cross-lingual settings and recommend smaller mT5 models due to LLMs' English-centric pre-training and higher computational costs.
- Why unresolved: The current generation of open-source LLMs lags behind specialized sequence-to-sequence models in cross-lingual transfer, creating uncertainty about the future architecture choice for this task.
- What evidence would resolve it: Evaluating next-generation multilingual LLMs with stronger inherent cross-lingual alignment to see if they can match fine-tuned mT5 performance in zero-shot scenarios.

## Limitations

- Restricted evaluation to restaurant reviews limits generalizability to other domains like electronics, hotels, or social media text
- Turkish consistently shows 10-15% performance degradation, highlighting poor cross-lingual transfer for non-Indo-European languages
- Constrained decoding relies on exact string matching for aspect terms, failing when target-language aspects have no direct lexical overlap with source-language training data

## Confidence

- **High Confidence:** The claim that constrained decoding improves cross-lingual ABSA performance by preventing source-language aspect generation has strong empirical support. Table 6-7 clearly show consistent improvements across multiple language pairs and tasks when constrained decoding is applied.
- **Medium Confidence:** The assertion that mT5-large outperforms mBART and that LLMs achieve competitive results with fine-tuning has experimental backing, but the comparison has limitations due to different optimizers and QLoRA usage.
- **Low Confidence:** The claim that the proposed method "eliminates the need for translation tools" overstates the practical impact, as it still requires translation for annotation in the traditional approach and relies heavily on multilingual pre-training.

## Next Checks

1. **Robustness to formatting variations**: Evaluate the constrained decoding approach when output formatting is relaxed (e.g., allowing flexible ordering of tuple elements or partial matches) to assess whether improvements hold under more realistic evaluation metrics.

2. **Cross-domain generalization**: Test the mT5-large model trained on restaurant reviews with constrained decoding on a different domain (e.g., laptop reviews from SemEval-2016) to verify whether improvements transfer beyond the training domain.

3. **Constraint relaxation analysis**: Systematically evaluate performance when relaxing the aspect term constraint (e.g., allowing generation of synonyms or related terms) to determine if strict lexical matching is necessary or if performance gains can be maintained with more flexible constraints.