---
ver: rpa2
title: 'Beg to Differ: Understanding Reasoning-Answer Misalignment Across Languages'
arxiv_id: '2512.22712'
source_url: https://arxiv.org/abs/2512.22712
tags:
- reasoning
- trace
- b-instruct
- answer
- qwen2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study introduces a human-validated framework to evaluate\
  \ reasoning-answer alignment across languages, focusing on whether model-generated\
  \ reasoning traces logically support their conclusions. Analyzing 65k reasoning\
  \ traces from GlobalMMLU questions across 6 languages and 6 frontier models, the\
  \ researchers uncover that while models achieve high task accuracy, their reasoning\
  \ often fails to support their conclusions\u2014especially in non-Latin scripts,\
  \ which show at least twice as much misalignment as Latin scripts."
---

# Beg to Differ: Understanding Reasoning-Answer Misalignment Across Languages

## Quick Facts
- arXiv ID: 2512.22712
- Source URL: https://arxiv.org/abs/2512.22712
- Reference count: 21
- Primary result: Models achieve high task accuracy but reasoning often fails to support conclusions, especially in non-Latin scripts

## Executive Summary
This study introduces a human-validated framework to evaluate reasoning-answer alignment across languages, focusing on whether model-generated reasoning traces logically support their conclusions. Analyzing 65k reasoning traces from GlobalMMLU questions across 6 languages and 6 frontier models, the researchers uncover that while models achieve high task accuracy, their reasoning often fails to support their conclusions—especially in non-Latin scripts, which show at least twice as much misalignment as Latin scripts. Error analysis via human annotation reveals that failures primarily stem from evidential errors (unsupported claims, ambiguous facts) followed by illogical reasoning steps. Automated evaluation correlates strongly with human judgment, enabling large-scale analysis. The findings demonstrate that answer accuracy provides an incomplete picture of multilingual reasoning capabilities and highlight the need for reasoning-aware evaluation frameworks.

## Method Summary
The researchers developed a comprehensive evaluation framework that combines human annotation with automated assessment to analyze reasoning-answer alignment across 6 languages and 6 frontier models using GlobalMMLU questions. They collected 65,000 reasoning traces and employed human annotators to validate alignment quality, identifying error types such as evidential failures and illogical reasoning. The automated evaluation system was trained to detect misalignment by correlating with human judgment, allowing for scalable analysis. The study systematically compared Latin and non-Latin script languages, revealing significant disparities in reasoning quality, and provided detailed error categorization to understand the nature of reasoning failures.

## Key Results
- Models achieve high task accuracy but reasoning often fails to support conclusions, especially in non-Latin scripts
- Non-Latin scripts show at least twice the misalignment rate compared to Latin scripts
- Automated evaluation correlates strongly with human judgment, enabling large-scale analysis

## Why This Works (Mechanism)
The framework works by explicitly evaluating the logical connection between reasoning steps and final answers, rather than just measuring answer correctness. By collecting extensive human-annotated data, the researchers established ground truth for what constitutes proper reasoning alignment. The automated evaluation leverages this human judgment to identify patterns of misalignment at scale. The systematic comparison across language scripts reveals that writing systems may influence reasoning quality, possibly due to differences in linguistic structure, cultural context, or training data representation. The error categorization helps identify specific failure modes, enabling targeted improvements in model reasoning capabilities.

## Foundational Learning
- **Reasoning trace analysis**: Understanding how models generate step-by-step reasoning to reach conclusions; needed to identify where logical breakdowns occur
- **Cross-linguistic evaluation**: Assessing model performance across different languages and scripts; needed to uncover systematic disparities in reasoning quality
- **Human-annotation correlation**: Establishing reliable ground truth through human judgment to train automated evaluation systems; needed to ensure automated assessments reflect true reasoning quality
- **Error type classification**: Categorizing reasoning failures into evidential and illogical types; needed to provide actionable insights for model improvement
- **Script-based comparison**: Analyzing differences between Latin and non-Latin scripts; needed to identify systematic biases in multilingual reasoning

## Architecture Onboarding
**Component Map**: Question -> Model Generation -> Reasoning Trace -> Alignment Evaluation (Human + Automated) -> Error Classification -> Analysis
**Critical Path**: Model output → Human annotation → Automated evaluation training → Large-scale analysis
**Design Tradeoffs**: Human annotation provides high-quality ground truth but limits scale; automated evaluation enables large-scale analysis but may miss nuanced reasoning failures
**Failure Signatures**: Unsupported claims, ambiguous facts, illogical reasoning steps, script-based disparities
**First Experiments**: 1) Test automated evaluation on diverse multilingual reasoning tasks beyond GlobalMMLU, 2) Conduct cross-cultural validation studies across different linguistic communities, 3) Perform ablation studies varying prompt engineering techniques across different model families

## Open Questions the Paper Calls Out
None

## Limitations
- Automated evaluation relies on prompt engineering and model-based scoring, which may introduce biases across different model families
- Focus on GlobalMMLU questions may not capture the full diversity of real-world multilingual reasoning tasks
- The finding that non-Latin scripts show twice the misalignment rate may be influenced by specific question distribution and cultural contexts in the dataset

## Confidence
- High confidence: Automated evaluation correlation with human judgment, error type distribution, overall misalignment rates
- Medium confidence: Non-Latin vs Latin script misalignment ratio, generalizability to other multilingual tasks
- Medium confidence: Cultural and linguistic factors affecting reasoning quality

## Next Checks
1. Test the automated evaluation framework on a diverse set of multilingual reasoning tasks beyond GlobalMMLU to assess generalizability
2. Conduct cross-cultural validation studies to determine if misalignment patterns hold across different linguistic communities and educational contexts
3. Perform ablation studies varying prompt engineering techniques to quantify their impact on automated evaluation reliability across different model families