---
ver: rpa2
title: 'Activation Steering Meets Preference Optimization: Defense Against Jailbreaks
  in Vision Language Models'
arxiv_id: '2509.00373'
source_url: https://arxiv.org/abs/2509.00373
tags:
- steering
- spo-vlm
- visual
- preference
- activation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SPO-VLM, a two-stage defense framework that
  enhances the safety of Vision Language Models (VLMs) against adversarial attacks.
  The framework combines activation-level intervention with sequence-level preference
  optimization via reinforcement learning.
---

# Activation Steering Meets Preference Optimization: Defense Against Jailbreaks in Vision Language Models

## Quick Facts
- arXiv ID: 2509.00373
- Source URL: https://arxiv.org/abs/2509.00373
- Authors: Sihao Wu; Gaojie Jin; Wei Huang; Jianhong Wang; Xiaowei Huang
- Reference count: 11
- Key outcome: SPO-VLM reduces toxicity scores by over 27.79% and significantly lowers jailbreak success rates while maintaining or improving visual understanding performance

## Executive Summary
This paper proposes SPO-VLM, a two-stage defense framework that enhances the safety of Vision Language Models (VLMs) against adversarial attacks. The framework combines activation-level intervention with sequence-level preference optimization via reinforcement learning. In Stage I, adaptive layer-specific steering vectors are computed from diverse datasets to suppress harmful behaviors. In Stage II, these vectors are refined through preference optimization guided by toxicity suppression and visual-consistency rewards. Experiments show that SPO-VLM reduces toxicity scores by over 27.79% and significantly lowers jailbreak success rates while maintaining or improving visual understanding performance. It also generalizes well to unseen attack types and outperforms prior defenses like ASTRA.

## Method Summary
SPO-VLM is a two-stage defense framework that enhances the safety of Vision Language Models (VLMs) against adversarial attacks. In Stage I, adaptive layer-specific steering vectors are computed from diverse datasets to suppress harmful behaviors at the activation level. In Stage II, these vectors are refined through preference optimization guided by toxicity suppression and visual-consistency rewards using reinforcement learning. This approach balances safety and task performance, demonstrating substantial reductions in toxicity and jailbreak success rates while preserving or enhancing visual understanding capabilities.

## Key Results
- Toxicity scores reduced by over 27.79%
- Significant reduction in jailbreak success rates
- Outperforms prior defenses such as ASTRA while maintaining or improving visual understanding performance

## Why This Works (Mechanism)
The mechanism works by combining activation-level steering with sequence-level preference optimization. Stage I computes layer-specific steering vectors that directly intervene in the model's activation space to suppress harmful behaviors. Stage II refines these vectors through reinforcement learning, optimizing for both safety (via toxicity suppression) and task performance (via visual-consistency rewards). This dual approach ensures that the model's behavior is shaped both at the representation level and at the output sequence level, providing robust defense against jailbreaks while maintaining visual understanding capabilities.

## Foundational Learning
- **Activation Steering**: Direct manipulation of neural activations to influence model behavior. Needed to intervene early in the model's processing pipeline. Quick check: Verify that steering vectors are correctly applied to target layers.
- **Preference Optimization**: Reinforcement learning to align model outputs with desired behaviors. Needed to refine steering vectors for optimal safety-performance trade-off. Quick check: Monitor reward convergence during optimization.
- **Layer-Specific Intervention**: Tailoring defenses to specific layers of the model. Needed to target the most influential layers for safety-critical behaviors. Quick check: Assess impact of steering on different layers.
- **Visual-Consistency Rewards**: Reward function to maintain task performance. Needed to ensure safety interventions don't degrade visual understanding. Quick check: Evaluate model performance on visual reasoning tasks post-defense.

## Architecture Onboarding

**Component Map**: Dataset -> Stage I (Steering Vector Computation) -> Stage II (Preference Optimization) -> Enhanced VLM

**Critical Path**: Steering vectors are computed from diverse datasets, then refined through preference optimization, resulting in a safer VLM.

**Design Tradeoffs**: Balancing safety and task performance; computational overhead of two-stage training; generalization to unseen attacks.

**Failure Signatures**: Ineffective steering vectors, reward hacking in preference optimization, or degradation in visual understanding.

**First Experiments**:
1. Validate steering vector computation on a small, controlled dataset.
2. Test preference optimization reward functions on a simplified task.
3. Evaluate the combined effect of both stages on a subset of jailbreak attacks.

## Open Questions the Paper Calls Out
None

## Limitations
- Computational overhead of the two-stage training process, especially the reinforcement learning component, may limit practical deployment in resource-constrained settings.
- Robustness against adaptive adversaries who may specifically target the steering mechanism is not explicitly tested.
- Datasets used for computing steering vectors are not fully characterized in terms of diversity and potential biases, which could affect generalizability.

## Confidence
- High: The framework design and integration of activation steering with preference optimization are technically sound and clearly explained.
- High: The reported experimental results (toxicity reduction >27.79%, improved visual understanding, and better generalization) are well-supported by the provided data and comparisons.
- Medium: The claim of outperforming prior defenses such as ASTRA is based on reported metrics, but the robustness against adaptive adversaries is not thoroughly validated.

## Next Checks
1. Conduct a comprehensive ablation study to quantify the individual and combined contributions of activation steering and preference optimization to the observed performance gains.
2. Evaluate the framework's robustness against adaptive adversaries who are aware of the defense mechanism and attempt to craft attacks specifically targeting the steering vectors.
3. Assess the computational overhead and scalability of the two-stage training process, especially the reinforcement learning component, across different model sizes and deployment scenarios.