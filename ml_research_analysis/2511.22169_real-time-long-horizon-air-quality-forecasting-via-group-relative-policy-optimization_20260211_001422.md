---
ver: rpa2
title: Real-Time Long Horizon Air Quality Forecasting via Group-Relative Policy Optimization
arxiv_id: '2511.22169'
source_url: https://arxiv.org/abs/2511.22169
tags:
- forecasting
- grpo
- policy
- long
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of reliable long-horizon air quality
  forecasting in East Asia, where existing global models struggle due to regional
  complexity and lack of real-time observational data. The authors construct and release
  the CMAQ-OBS dataset, combining ground observations and high-resolution CMAQ reanalysis
  to significantly reduce forecasting error (59.5% improvement over CAMS) and enable
  real-time initialization from over 1,800 monitoring stations.
---

# Real-Time Long Horizon Air Quality Forecasting via Group-Relative Policy Optimization

## Quick Facts
- arXiv ID: 2511.22169
- Source URL: https://arxiv.org/abs/2511.22169
- Reference count: 40
- Reduces False Alarm Rate by 47.3% compared to SFT baseline

## Executive Summary
This paper addresses the challenge of reliable long-horizon air quality forecasting in East Asia, where existing global models struggle due to regional complexity and lack of real-time observational data. The authors construct and release the CMAQ-OBS dataset, combining ground observations and high-resolution CMAQ reanalysis to significantly reduce forecasting error (59.5% improvement over CAMS) and enable real-time initialization from over 1,800 monitoring stations. They propose a two-stage framework—FAKER-Air—which first employs supervised fine-tuning with temporal accumulation loss to address exposure bias in multi-step predictions, then refines predictions via Group-Relative Policy Optimization (GRPO) with class-wise AQI rewards and curriculum rollout scheduling to align forecasts with asymmetric operational costs. This approach reduces the False Alarm Rate by 47.3% compared to the SFT-only baseline while maintaining competitive F1-scores, improving operational reliability for air quality warning systems on extended lead times.

## Method Summary
The authors propose FAKER-Air, a two-stage framework for long-horizon air quality forecasting. Stage 1 employs supervised fine-tuning (SFT) with an Aurora-based 3D encoder-decoder architecture trained on the CMAQ-OBS dataset using temporal accumulation loss to mitigate exposure bias. Stage 2 applies Group-Relative Policy Optimization (GRPO) with class-wise AQI rewards and curriculum rollout scheduling to align predictions with asymmetric operational costs. The framework reduces False Alarm Rate by 47.3% while maintaining competitive F1-scores for PM2.5/PM10 forecasting up to 120 hours ahead.

## Key Results
- Reduces False Alarm Rate by 47.3% compared to SFT baseline
- Maintains competitive F1-scores while significantly lowering FAR
- Achieves near-ideal bias performance with GRPO refinement
- Improves F1-score for PM2.5 from 54.40 to 59.90 using temporal accumulation loss

## Why This Works (Mechanism)

### Mechanism 1
Replacing global reanalysis data with a fused regional dataset (CMAQ-OBS) allows the model to capture local aerosol dynamics that global models miss. Global models like CAMS suffer from regional bias and update latency (5-day delay). By combining real-time ground observations (OBS) with high-resolution CMAQ reanalysis, the model receives physics-consistent spatial fields validated against local ground truth, effectively aligning the training distribution with the target domain. Core assumption: The 59.5% error reduction in CMAQ over CAMS translates directly to improved neural network generalization for East Asia.

### Mechanism 2
Temporal Accumulation (TA) loss mitigates exposure bias in long-horizon rollouts by penalizing multi-step error accumulation. Standard teacher forcing trains the model on ground truth only, creating a distribution shift when it must consume its own predictions during inference. TA loss forces the model to generate N-step trajectories during training and applies weighted MSE to the sequence of predictions, exposing the model to its own error propagation early. Core assumption: The linearly increasing step weights ($w_i$) correctly prioritize long-horizon stability over single-step accuracy.

### Mechanism 3
Group-Relative Policy Optimization (GRPO) aligns predictions with asymmetric operational costs (e.g., false alarms vs. missed events) better than point-wise MSE. Standard MSE treats all errors equally, causing over-prediction. GRPO samples multiple trajectory groups, evaluates them using a class-wise AQI reward (binary correctness), and converts rewards into relative advantages via softmax normalization. It updates the policy to maximize the likelihood of high-advantage trajectories, effectively optimizing for discrete operational metrics. Core assumption: A simple binary reward (1 for correct class, 0 otherwise) is sufficient to guide the policy through the complex continuous space of PM concentration predictions.

## Foundational Learning

- **Exposure Bias**
  - Why needed here: The paper identifies this as the core failure mode in long-horizon forecasting. Without understanding that training on ground truth (teacher forcing) creates a mismatch with inference on model predictions, the TA loss mechanism makes little sense.
  - Quick check question: Does the model generate predictions based on its *own* previous outputs during training, or does it always look at the ground truth?

- **Asymmetric Cost / Loss Alignment**
  - Why needed here: The motivation for the complex GRPO stage rests on the premise that "False Alarms" and "Missed Severe Events" have different real-world costs (public trust vs. public safety). MSE does not capture this.
  - Quick check question: Why would a model minimizing Mean Squared Error tend to over-predict pollution levels in uncertain scenarios?

- **Curriculum Learning**
  - Why needed here: The authors employ "Curriculum Rollout scheduling" to stabilize GRPO. Understanding that long-horizon policy optimization is unstable without starting with shorter horizons is critical for reproducing the results.
  - Quick check question: What happens to the variance of policy gradients if you try to train on 120-hour rollouts immediately?

## Architecture Onboarding

- **Component map:** Real-time OBS + CMAQ inputs → SFT Pre-training (TA Loss) → GRPO Policy Init → Curriculum Rollout (Sampling + Ranking) → Final Operational Model
- **Critical path:** Real-time OBS + CMAQ inputs → SFT Pre-training (TA Loss) → GRPO Policy Init → Curriculum Rollout (Sampling + Ranking) → Final Operational Model
- **Design tradeoffs:**
  - TA Loss Horizon (T): Longer T (e.g., 4 steps) improves temporal consistency but increases training time and gradient complexity
  - Reward Type: MSE reward is stable but fails to lower FAR; AQI reward aligns with goals but requires curriculum scheduling to stabilize
  - Group Size (G): Increasing trajectory samples per batch improves ranking quality but multiplies compute cost
- **Failure signatures:**
  - High FAR with SFT: Indicates MSE is dominating; the model is uncertain and biasing high
  - Collapse in GRPO: If curriculum rollout is skipped, the model may degrade into predicting only the majority class ("Good") to maximize naive rewards
- **First 3 experiments:**
  1. SFT Baseline Validation: Train Aurora with and without TA Loss ($T=4$) on the CMAQ-OBS dataset to verify the F1-score lift
  2. Reward Sensitivity Check: Run GRPO with MSE reward vs. AQI reward to confirm that MSE reward fails to reduce False Alarm Rate
  3. Curriculum Ablation: Attempt GRPO training with a fixed long horizon immediately vs. the curriculum schedule to observe training stability

## Open Questions the Paper Calls Out
None explicitly identified in the provided text.

## Limitations
- The CMAQ-OBS dataset is not publicly available, making independent verification difficult
- The effectiveness of binary AQI rewards for guiding continuous concentration predictions remains unproven in other domains
- GRPO's novel application to air quality forecasting lacks peer validation and may be sensitive to hyperparameters

## Confidence
- **High Confidence:** The SFT baseline with Temporal Accumulation Loss and the overall framework architecture are well-specified and reproducible
- **Medium Confidence:** The 59.5% error reduction claim is based on a cited external source rather than direct validation within this paper
- **Low Confidence:** The full GRPO training dynamics, particularly the interaction between curriculum rollout scheduling and the softmax advantage weighting, are complex and sensitive to unspecified hyperparameters

## Next Checks
1. **Dataset Fidelity Check:** Reconstruct a proxy dataset by fusing publicly available CMAQ reanalysis with ground observations from Korean and Chinese air quality databases. Train the SFT baseline on this proxy and compare F1-score improvements to those reported in Table 1.
2. **GRPO Reward Function Ablation:** Implement both the AQI reward and an alternative continuous reward (e.g., negative MSE or a scaled distance to the correct AQI bin). Train GRPO models with both rewards under identical conditions. Compare FAR and F1-score to isolate the specific impact of the discrete AQI reward.
3. **Curriculum Rollout Stability Test:** Train two GRPO models: one with the full curriculum rollout schedule and one that starts directly at H=4. Monitor training loss curves, reward variance, and the final FAR/F1 metrics.