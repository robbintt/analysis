---
ver: rpa2
title: 'LM-mixup: Text Data Augmentation via Language Model based Mixup'
arxiv_id: '2510.20449'
source_url: https://arxiv.org/abs/2510.20449
tags:
- data
- arxiv
- preprint
- quality
- low-quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LM-mixup, a method that distills multiple
  low-quality, redundant instruction-following samples into high-quality, information-dense
  outputs. The approach first constructs a 144K-sample dataset (MIXTURE) with hierarchical
  mappings from noisy or semantically redundant inputs to clean targets, then applies
  supervised fine-tuning followed by reinforcement learning with Group Relative Policy
  Optimization.
---

# LM-mixup: Text Data Augmentation via Language Model based Mixup

## Quick Facts
- **arXiv ID:** 2510.20449
- **Source URL:** https://arxiv.org/abs/2510.20449
- **Reference count:** 40
- **Primary result:** Distilling low-quality instruction-following data into high-quality outputs through hierarchical mapping and reinforcement learning matches or exceeds full-dataset training with only 3% of the data

## Executive Summary
This paper introduces LM-mixup, a method for distilling multiple low-quality, redundant instruction-following samples into high-quality, information-dense outputs. The approach first constructs a 144K-sample dataset (MIXTURE) with hierarchical mappings from noisy or semantically redundant inputs to clean targets, then applies supervised fine-tuning followed by reinforcement learning with Group Relative Policy Optimization. The RL stage uses multi-dimensional rewards for quality, semantic alignment, and format compliance. Experiments show that fine-tuning on just ~3% of the data (mixup-enhanced low-quality + original high-quality) matches or exceeds full-dataset training and state-of-the-art selection methods across multiple benchmarks, demonstrating that properly distilled low-quality data is a valuable resource for instruction tuning.

## Method Summary
LM-mixup works by first constructing a hierarchical mapping from noisy or redundant instruction samples to clean target outputs. A 144K-sample dataset (MIXTURE) is created through this process. The model undergoes supervised fine-tuning on this distilled dataset, followed by reinforcement learning using Group Relative Policy Optimization (GRPO). The RL stage employs multi-dimensional rewards that evaluate output quality, semantic alignment with instructions, and format compliance. The approach demonstrates that training on a small fraction of carefully processed data can match or exceed performance of training on full datasets, effectively turning low-quality data into a valuable resource for instruction tuning.

## Key Results
- Fine-tuning on just ~3% of the data (mixup-enhanced low-quality + original high-quality) matches or exceeds full-dataset training
- The approach outperforms state-of-the-art data selection methods across multiple benchmarks
- Properly processed low-quality data proves valuable for instruction tuning, challenging the assumption that only high-quality data should be used

## Why This Works (Mechanism)
The method leverages hierarchical mapping to extract clean targets from noisy or redundant inputs, effectively concentrating information. The multi-dimensional reward function in RL ensures outputs maintain quality while preserving semantic meaning and following format requirements. By combining supervised fine-tuning with GRPO, the approach balances learning from distilled examples with reinforcement-based optimization. The efficiency gain comes from extracting maximum value from low-quality data rather than discarding it, reducing the need for extensive high-quality annotations.

## Foundational Learning
- **Hierarchical mapping** - why needed: Transforms noisy/redundant inputs into clean targets; quick check: Verify mapping accuracy through human evaluation
- **Group Relative Policy Optimization (GRPO)** - why needed: Enables efficient RL without value function estimation; quick check: Compare reward convergence with traditional PPO
- **Multi-dimensional reward functions** - why needed: Balances quality, semantic alignment, and format compliance; quick check: Ablate individual reward components to measure impact
- **Data distillation** - why needed: Extracts high-value information from low-quality samples; quick check: Measure information density before/after distillation
- **Instruction tuning** - why needed: Adapts models to follow instructions effectively; quick check: Benchmark on standard instruction-following tasks
- **Mixup augmentation** - why needed: Creates synthetic examples that blend multiple instruction-response pairs; quick check: Evaluate generalization on unseen instructions

## Architecture Onboarding

**Component Map:** Data preprocessing -> Hierarchical mapping -> Supervised fine-tuning -> RL with GRPO -> Evaluation

**Critical Path:** Low-quality data → Hierarchical mapping → MIXTURE dataset → Supervised fine-tuning → Reinforcement Learning → Final model

**Design Tradeoffs:** The approach trades computational cost of hierarchical mapping and RL for reduced need for high-quality annotated data. While the distillation process adds complexity, it enables effective use of abundant low-quality data, potentially reducing annotation costs significantly.

**Failure Signatures:** Poor hierarchical mapping quality leads to noisy targets; inadequate reward design causes mode collapse or loss of semantic alignment; insufficient fine-tuning on distilled data results in poor adaptation to the mixup-enhanced format.

**First Experiments:** 1) Evaluate mapping accuracy on held-out pairs, 2) Test supervised fine-tuning performance on MIXTURE vs original data, 3) Compare GRPO with baseline RL methods on a small subset.

## Open Questions the Paper Calls Out
None

## Limitations
- The hierarchical mapping strategy's generalizability to other domains with different noise patterns remains untested
- The effectiveness of GRPO depends heavily on specific reward function design, which may not transfer to other instruction tuning scenarios
- Claims about optimal balance between low-quality and high-quality data proportions lack systematic validation

## Confidence

**High Confidence Claims:**
- Empirical results showing comparable or superior performance using 3% of the data are well-supported
- Demonstration that properly processed low-quality data can be valuable for instruction tuning is clearly validated

**Medium Confidence Claims:**
- General applicability of the hierarchical mapping approach to other instruction datasets
- Scalability of the distillation process to much larger datasets or different domains
- Long-term stability of the mixup-enhanced training methodology

**Low Confidence Claims:**
- Claims about the optimal balance between low-quality and high-quality data proportions
- Universal effectiveness of the specific reward function components across different tasks
- Assertion that this approach is broadly superior to all other data selection/augmentation methods

## Next Checks
1. **Cross-Dataset Generalization Test:** Apply LM-mixup to a completely different instruction-following dataset (e.g., from a different domain or with different noise characteristics) and evaluate whether similar performance gains are achieved without retraining the reward models.

2. **Ablation Study on Reward Components:** Systematically remove or modify individual reward function components (quality, semantic alignment, format compliance) to quantify their relative contributions and determine if the current weighting is optimal.

3. **Scaling Analysis:** Evaluate the method's performance as the ratio of low-quality to high-quality data varies from 0% to 100%, identifying the sweet spot and testing whether the approach maintains effectiveness at scale (10x-100x larger datasets).