---
ver: rpa2
title: Text Chunking for Document Classification for Urban System Management using
  Large Language Models
arxiv_id: '2504.00274'
source_url: https://arxiv.org/abs/2504.00274
tags:
- text
- coding
- chunking
- whole
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study explored using large language models (LLMs) to code\
  \ textual documents on urban systems, aiming to reduce human resource constraints\
  \ and improve consistency. It compared two prompting methods\u2014whole-text analysis\
  \ and text chunking\u2014across three OpenAI models (GPT-4o, GPT-4o-mini, o1-mini)\
  \ using 10 case documents and a codebook of 17 digital twin characteristics."
---

# Text Chunking for Document Classification for Urban System Management using Large Language Models

## Quick Facts
- **arXiv ID**: 2504.00274
- **Source URL**: https://arxiv.org/abs/2504.00274
- **Reference count**: 40
- **Key outcome**: Text chunking significantly improves LLM recall and internal agreement for deductive coding tasks without sacrificing precision

## Executive Summary
This study explores using large language models for coding textual documents on urban systems to reduce human resource constraints and improve consistency. The research compares whole-text analysis versus text chunking approaches across three OpenAI models (GPT-4o, GPT-4o-mini, o1-mini) using 10 case documents and a codebook of 17 digital twin characteristics. Results demonstrate that chunking significantly improves LLM performance metrics, with GPT-4o recall increasing from 76.58% to 95.09% and o1-mini from 72.18% to 94.91%. The chunking approach also enhances inter-rater reliability when LLMs serve as additional coders alongside human raters.

## Method Summary
The methodology employs deductive binary classification of 10 urban system documents across 17 "digital twin" characteristics using OpenAI models (gpt-4o, gpt-4o-mini, o1-mini). Documents are preprocessed via PyPDF2/PyTesseract and cleaned for formatting issues. Fixed-size 500-word chunks are created, and each chunk is processed through LLM API calls with a specific zero-shot prompt asking for evidence and explanation. Classification uses keyword search on LLM outputs, and consensus is determined through 15 iterations per condition with mode aggregation. Fleiss' Kappa measures inter-rater reliability against human consensus.

## Key Results
- GPT-4o recall improved from 76.58% to 95.09% when switching from whole-text to chunked analysis
- o1-mini internal agreement increased from 64.47% to 89.89% with chunking, the largest improvement among models
- Adding GPT-4o (chunked) as 4th rater improved Fleiss K in 7 of 10 papers, with K values ranging from 0.407-0.437

## Why This Works (Mechanism)

### Mechanism 1
Fixed-size text chunking improves LLM classification recall without sacrificing precision for certain model architectures by constraining context per evaluation, reducing "context aggregation bias" where whole-document processing dilutes localized thematic signals across competing interpretations. Document characteristics are locally clustered rather than uniformly distributed, allowing relevant signals to be captured within 500-word windows.

### Mechanism 2
Reasoning-focused models (o1-mini with chain-of-thought) show stronger volume-sensitivity than direct-shot models, performing disproportionately better with chunked inputs. Recursive reasoning processes benefit from constrained scope where each reasoning chain operates on tractable context; large documents overwhelm planning/self-refinement loops with competing signals.

### Mechanism 3
LLM consensus aggregation (mode across 15 iterations) as an additional rater improves inter-rater reliability in human-AI teams. LLMs introduce independent variance patterns distinct from human coders; combining multiple independent perspectives increases agreement probability through variance reduction.

## Foundational Learning

- **Deductive Coding with Codebooks**: Entire methodology depends on translating 17 binary characteristics into consistent classification decisions across raters. Quick check: Can you explain why a codebook definition might produce different classifications from two raters reading the same text?

- **Inter-Rater Reliability (Fleiss' Kappa)**: Distinguishes "agreement" from "agreement beyond chance"; critical for interpreting whether LLM adds real value. Quick check: Why might percent agreement of 80% still indicate poor reliability for binary classifications?

- **LLM Context Window and Attention Distribution**: Explains why chunking helps—longer contexts don't guarantee better retrieval of relevant signals due to attention dilution. Quick check: If a model has 128K token context window, why might it still miss information in a 10K token document?

## Architecture Onboarding

- **Component map**: PDF Input → Text Extraction (PyPDF2 + OCR) → Preprocessing (hyphenation, newline fixes) → Fixed-Size Chunking (500 words) → Per-Chunk LLM API Calls (reset context between calls) → Keyword-Based Classification (yes/clearly stated/mentions/etc.) → Per-Document Aggregation (OR logic) → Iteration Loop (15x) → Consensus (mode) → Fleiss Kappa calculation vs. human raters

- **Critical path**: Chunking configuration (size, boundary handling) → Prompt construction → Consensus aggregation logic. These directly determine whether localized signals are captured and correctly aggregated.

- **Design tradeoffs**:
  - Fixed-size vs. semantic chunking: Fixed is simpler but risks cutting mid-sentence; semantic preserves meaning but requires additional processing
  - OR vs. voting aggregation: Paper uses OR (one TRUE = document TRUE), optimizing recall; would need threshold voting for precision-focused tasks
  - Iteration count: 15 iterations provide stable consensus but multiply API costs; paper shows diminishing returns after ~9-12 iterations

- **Failure signatures**:
  - High false negative rate on specific dimensions (Data Ownership: 40% human disagreement; V2P: 60%) indicates codebook definition problems, not chunking failures
  - Low internal agreement (<70%) suggests model-volume mismatch—switch to chunking or smaller model
  - Fleiss K < 0.40 with LLM as rater indicates chance-level agreement—re-evaluate prompt-codebook alignment

- **First 3 experiments**:
  1. Baseline chunk size sweep: Test 250, 500, 750, 1000 word chunks on 3 documents to find optimal size for your domain before full deployment
  2. Dimension-specific error analysis: Manually review failures on your worst-performing dimension to distinguish codebook ambiguity from model limitation
  3. Human-AI teaming validation: Run 2 humans + LLM consensus (n=3) on held-out documents; verify Fleiss K > 0.40 before operational use

## Open Questions the Paper Calls Out

- How does semantic or thematic chunking compare to fixed-size chunking for deductive coding? The authors identify the risk of fixed-size chunking cutting off context at boundaries and explicitly call for research on semantic or thematic chunking.

- Can prompt engineering mitigate classification errors in codebook dimensions with low agreement? The authors note that prompt engineering was out of scope and suggest future research should focus on performance with different codebooks to address specific dimension failures.

- Do the benefits of text chunking persist across non-OpenAI or future LLM architectures? The authors list whether these observed effects continue with later versions of the OpenAI or other developed LLMs as an area of potential research.

## Limitations

- Generalizability to other domains remains untested; study focuses specifically on urban systems and digital twin characteristics
- Codebook definition quality issues affect multiple dimensions (Data Ownership: 40% human disagreement; V2P Connection: 60%), suggesting fundamental ambiguity
- Keyword-based classification brittleness creates fragile pipeline where models may provide correct positive classifications using different phrasing

## Confidence

- **High confidence**: Chunking improves LLM internal agreement and recall for GPT-4o and o1-mini (p < 0.05)
- **Medium confidence**: LLM consensus can serve as reliable additional rater in human-AI teams (Fleiss K improvements in 6-7 of 10 papers)
- **Low confidence**: The specific 500-word chunk size is optimal; domain transfer without codebook refinement

## Next Checks

1. **Cross-domain replication**: Test chunking approach on 2-3 different document types (legal contracts, scientific papers, policy documents) with domain-specific codebooks to assess generalizability

2. **Chunk size optimization**: Conduct systematic comparison of 250, 500, 750, 1000 word chunks on held-out documents to identify optimal size for different characteristic types

3. **Classification pipeline robustness**: Implement LLM-as-a-judge approach for final classification instead of keyword matching, comparing agreement rates and error patterns