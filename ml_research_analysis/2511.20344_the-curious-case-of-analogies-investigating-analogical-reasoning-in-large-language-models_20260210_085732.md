---
ver: rpa2
title: 'The Curious Case of Analogies: Investigating Analogical Reasoning in Large
  Language Models'
arxiv_id: '2511.20344'
source_url: https://arxiv.org/abs/2511.20344
tags:
- information
- analogical
- reasoning
- layers
- relational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work investigates the internal mechanisms of large language
  models (LLMs) in analogical reasoning, focusing on how models encode and apply relational
  information across different entities. Using attention knockout, patchscopes, and
  structural alignment metrics, the study identifies three key findings: (1) mid-upper
  layers in LLMs encode both attributive and relational information critical for analogical
  reasoning, with relational information showing a sharp decline in incorrect cases;
  (2) models struggle not only with extracting relational information but also applying
  it to new entities, with performance improvements of up to 38.1% observed when patching
  representations from the second entity into the linking position; (3) successful
  analogical reasoning is associated with strong structural alignment between source
  and target stories, as measured by mutual alignment scores, while failures reflect
  weaker or distractor-biased alignment.'
---

# The Curious Case of Analogies: Investigating Analogical Reasoning in Large Language Models

## Quick Facts
- arXiv ID: 2511.20344
- Source URL: https://arxiv.org/abs/2511.20344
- Reference count: 23
- Models encode relational information in mid-upper layers, but struggle to apply it to new entities, with patching improvements up to 38.1%

## Executive Summary
This work investigates how large language models perform analogical reasoning by examining internal mechanisms of information encoding and transfer. Using attention knockout, patchscopes, and structural alignment metrics, the study reveals that mid-upper transformer layers encode both attributive and relational information critical for analogies, but models struggle not only with extracting this relational information but also applying it to new entities. The linking token ("as") serves as a bottleneck for information transfer, and successful reasoning correlates with strong structural alignment between source and target stories. These findings highlight both the emerging capabilities and limitations of LLMs in relational reasoning compared to human cognition.

## Method Summary
The study examines analogical reasoning in LLMs using two task formats: proportional analogies ("e1 is to e2 as e3 is to e4") and story analogies (selecting structurally analogous stories from distractors). Models tested include Llama-2-13B, Gemma-7B, and Qwen2.5-14B. Key techniques include attention knockout to identify causally important layers, linear probing to test information decodability, Patchscopes for interpreting hidden states, and activation patching to test causal relationships between positions. Datasets were filtered to exclude reasoning shortcuts and ensure models must reason rather than recall. Structural alignment was measured using Mutual Alignment Score (MAS) between token representations.

## Key Results
- Mid-upper layers encode both attributive and relational information, with relational information declining sharply in incorrect cases
- Models struggle to apply relational information to new entities, with patching improvements up to 38.1% when e2 representations are applied to link position
- Successful analogical reasoning correlates with strong structural alignment (MAS) between source and target stories, while failures show weaker or distractor-biased alignment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Relational information encoded in mid-upper layers enables analogical reasoning
- Mechanism: Models process entity pairs through mid-to-upper transformer layers, where both attributive and relational information propagate. The resolution token attends strongly to e2 to retrieve this encoded relation for answer generation.
- Core assumption: Correct analogical outputs require relational representations that are explicitly decodable from hidden states.
- Evidence anchors:
  - [abstract] "both attributive and relational information propagate through mid-upper layers in correct cases, whereas reasoning failures reflect missing relational information within these layers"
  - [section 4.2] "blocking attention edges to either e2 or e3 results in noticeable performance drops... mainly around the mid-upper layers"
  - [corpus] Related work on function vectors (Todd et al. 2024) supports that models encode task-level abstractions, though limited to simpler tasks.
- Break condition: If relational information is absent or degraded in mid-upper layers (Figure 3b), answer resolution fails regardless of attributive knowledge.

### Mechanism 2
- Claim: The linking token ("as") serves as a transfer bottleneck for relational application
- Mechanism: The link position receives relational information from e2 and propagates it downstream. When this transfer fails, patching e2 representations directly into early link layers can recover performance.
- Core assumption: Assumption: Information flow through the link is causal for applying relations to target entities, not merely correlational.
- Evidence anchors:
  - [section 5.2] "patching the representations of e2 to the link leads to noticeable performance gains up to 38.1%"
  - [section 4.2] "information propagating from the link heavily affects model generations in incorrect cases, particularly in the early to middle layers"
  - [corpus] Limited corpus evidence on link-specific transfer mechanisms; this appears novel to this work.
- Break condition: If e2 lacks proper relational encoding, patching to link cannot recover performance—both encoding and transfer must function.

### Mechanism 3
- Claim: Structural alignment between analogous situations correlates with reasoning success
- Mechanism: Models develop abstract relational representations in middle layers where analogical structure becomes linearly separable from lexical similarity. Successful reasoning shows higher Mutual Alignment Score (MAS) between source-target pairs versus source-distractor pairs.
- Core assumption: Assumption: Token-level cosine similarity in hidden states reflects meaningful structural correspondence, not surface patterns.
- Evidence anchors:
  - [section 6.2] "layers 20 through 30 showing an average accuracy of 82.9%" for linear probes distinguishing analogical from lexically similar stories
  - [section 6.2] "for correct cases, the MAS between source and target stories consistently exceeds that between source and distractor stories"
  - [corpus] Related work (Jiayang et al. 2023; Webb et al. 2023) evaluates analogical capabilities but doesn't analyze internal alignment mechanisms.
- Break condition: When distractor stories have higher MAS than target stories, models fail to identify correct analogical structure despite knowledge availability.

## Foundational Learning

- **Attention Knockout / Causal Tracing**:
  - Why needed here: Understanding which token positions and layers are causally responsible for analogical reasoning requires systematically disabling attention pathways.
  - Quick check question: Can you explain why blocking attention from resolution token to e2 causes performance drops but blocking to e1 doesn't?

- **Linear Probing**:
  - Why needed here: Determining whether relational information is linearly decodable from hidden states requires training classifiers on intermediate activations.
  - Quick check question: Why does high probe accuracy in middle layers suggest explicit encoding rather than distributed representations?

- **Activation Patching**:
  - Why needed here: Establishing causal roles of specific representations requires intervention—replacing activations at one position with those from another.
  - Quick check question: Why must patched representations pass through early layers to be "contextualized" rather than patching directly to final layers?

## Architecture Onboarding

- **Component map**:
  - Entity positions (e1, e2, e3, e4): Input tokens representing analogy terms
  - Link position ("as"): Transfer node between entity pairs
  - Resolution token: Final position where answer is generated
  - Mid-upper layers (roughly layers 20-30 in 40+ layer models): Relational encoding zone
  - Attention heads: Information routing pathways between positions

- **Critical path**:
  1. Entity tokens → embedding → early contextualization
  2. e2 encodes relational information by mid-upper layers
  3. Link receives and transfers relational information from e2
  4. Resolution token attends to e2/e3 in upper layers
  5. Answer decoded from resolution token's final hidden state

- **Design tradeoffs**:
  - Mid-upper layer patching (e2) vs. early layer patching (link): Earlier patching allows more contextualization but risks information dilution
  - Knowledge filtering vs. coverage: Strict filtering ensures reasoning analysis but reduces dataset size
  - Two-option vs. multi-option story analogy format: Reduces positional bias but may not reflect real-world complexity

- **Failure signatures**:
  - High attributive information but low relational information in e2/e3 → extraction failure
  - Strong link influence in incorrect cases → transfer failure
  - Distractor MAS > target MAS → alignment failure
  - Answer resolution without e2 attention → shortcut reasoning (filtered out)

- **First 3 experiments**:
  1. Run attention knockout on e2 and e3 across all layers to identify critical layer ranges for your model architecture
  2. Apply Patchscopes with custom relational prompts to verify what information is actually encoded at each position
  3. Compute Mutual Alignment Score for source-target vs. source-distractor pairs to quantify structural alignment quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific representational or mechanistic failures cause activation patching to be ineffective in the majority (approx. 62%) of incorrect cases where the link fails to transfer relational information?
- Basis in paper: [explicit] In Section 5.2, the authors note that while patching representations from $e_2$ to the link rectifies errors in up to 38.1% of cases, the intervention fails for the remaining cases. The paper concludes that failures stem "not only from representational gaps but also from limitations in relational application," but does not fully diagnose why the patching mechanism itself fails to facilitate transfer in the majority of instances.
- Why unresolved: The paper demonstrates that patching works partially but does not isolate the variables differentiating successful interventions from unsuccessful ones.
- What evidence would resolve it: An analysis of the attention patterns and residual stream norms in the cases where patching fails, compared to those where it succeeds, to determine if the failure is due to a lack of relational encoding at the source or an inability of the target position to integrate the patched vector.

### Open Question 2
- Question: Can the identified "structural alignment" (MAS) be explicitly reinforced or trained to overcome the model's vulnerability to surface-level lexical distractors?
- Basis in paper: [explicit] Section 6.2 states that "incorrect cases exhibit a much smaller alignment gap, with distractors often receiving greater alignment, suggesting that the model fails to reliably identify the intended analogical structure." The authors highlight this vulnerability to surface-level interference but do not test methods to mitigate it.
- Why unresolved: The study establishes a correlation between high Mutual Alignment Score (MAS) and correct reasoning, and low MAS and failure, but leaves open whether inducing higher MAS would force correct reasoning.
- What evidence would resolve it: Intervention experiments that artificially boost the alignment between source and target token representations (or suppress alignment with distractors) to see if it causally improves the model's selection of the analogical story over the lexically similar one.

### Open Question 3
- Question: Why is "application" of relations a bottleneck for LLMs when it is relatively easy for humans, and is this bottleneck a product of architecture or training data?
- Basis in paper: [explicit] The Abstract and Section 5 contrast human and model behavior: "Unlike humans, LLMs often struggle not only when relational information is missing, but also when attempting to apply it to new entities." Section 5.2 confirms that even with correct extraction, the "link" often fails to convey information.
- Why unresolved: The paper identifies *where* the failure happens (the link/application stage) and *that* it differs from human cognition, but does not explain *why* the Transformer architecture struggles with this specific step relative to the extraction step.
- What evidence would resolve it: Comparing the "application" performance of models trained on standard data vs. those trained on datasets augmented with explicit relation-mapping tasks to determine if the bottleneck is a lack of exposure to mapping operations or an architectural constraint of next-token prediction.

## Limitations

- Knowledge filtering reliability: Automated filtering to exclude reasoning shortcuts may not fully eliminate all shortcut strategies, potentially affecting the validity of downstream analyses
- Information decoding fidelity: The distinction between attributive and relational information using Patchscopes assumes carefully crafted prompts can reliably extract intended information types
- Structural alignment metric validity: Token-level cosine similarity (MAS) may not fully capture meaningful structural alignment versus surface lexical patterns

## Confidence

- **High Confidence**: The core finding that mid-upper layers encode both attributive and relational information for analogical reasoning, with relational information specifically declining in incorrect cases
- **Medium Confidence**: The conclusion that models struggle with applying relational information to new entities, demonstrated through activation patching experiments with 38.1% performance improvement
- **Low Confidence**: The interpretation of structural alignment as the primary mechanism for successful analogical reasoning, as the metric itself and its relationship to human-like analogical reasoning is less established

## Next Checks

- Check 1: Conduct ablation studies with multiple filtering thresholds to assess sensitivity of results to filtering criteria and determine how robust the core findings are to different filtering approaches
- Check 2: Perform controlled experiments varying the complexity and type of analogical relationships (e.g., taxonomic vs. functional relations) to test whether identified mechanisms generalize across different analogical reasoning domains
- Check 3: Implement alternative structural alignment metrics (e.g., probing for abstract relational representations) and compare their predictive power for reasoning success against MAS to validate whether token-level alignment captures intended structural relationships