---
ver: rpa2
title: 'GAPS: A Clinically Grounded, Automated Benchmark for Evaluating AI Clinicians'
arxiv_id: '2510.13734'
source_url: https://arxiv.org/abs/2510.13734
tags:
- clinical
- reasoning
- safety
- evidence
- rubric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces GAPS, a clinically grounded framework for
  evaluating AI clinicians across four dimensions: reasoning depth (Grounding), answer
  completeness (Adequacy), robustness to input perturbations (Perturbation), and safety.
  The authors developed a fully automated pipeline that generates guideline-anchored
  questions and rubrics from authoritative clinical sources, overcoming scalability
  and subjectivity limitations of prior benchmarks.'
---

# GAPS: A Clinically Grounded, Automated Benchmark for Evaluating AI Clinicians

## Quick Facts
- arXiv ID: 2510.13734
- Source URL: https://arxiv.org/abs/2510.13734
- Reference count: 26
- Benchmark construction from clinical guidelines using automated pipeline with 90% expert agreement

## Executive Summary
GAPS (Guideline-based Assessment for Perturbation and Safety) is an automated benchmark framework designed to evaluate AI clinicians across four clinically-grounded dimensions: reasoning depth (Grounding), answer completeness (Adequacy), robustness to input perturbations (Perturbation), and safety. The framework addresses critical limitations of existing medical AI benchmarks by automating the generation of guideline-anchored questions and evaluation rubrics from authoritative clinical sources, enabling scalable and objective assessment. The GAPS-NSCLC-preview benchmark was constructed from the NCCN Non-Small Cell Lung Cancer guideline, containing 92 questions and 1,691 rubrics, and demonstrated strong alignment with expert clinician evaluations while revealing significant performance gaps in current AI systems, particularly at higher reasoning levels and under adversarial conditions.

## Method Summary
The GAPS framework employs a fully automated pipeline that extracts clinical knowledge from authoritative guidelines (initially NCCN Non-Small Cell Lung Cancer guidelines) and transforms it into structured questions and rubrics. The pipeline consists of two main phases: first, generating guideline-anchored questions by mapping clinical scenarios to specific guideline recommendations; second, creating comprehensive rubrics that encode the expected answers based on clinical evidence. The framework evaluates AI clinicians across four dimensions using automated scoring: Grounding assesses reasoning depth against clinical evidence, Adequacy measures answer completeness and precision, Perturbation tests robustness to adversarial inputs, and Safety evaluates responses to potentially harmful scenarios. The automated evaluation was validated against expert clinicians, showing 90% agreement and Cohen's Kappa of 0.77, demonstrating the pipeline's reliability for objective assessment.

## Key Results
- GAPS-NSCLC-preview benchmark contains 92 questions and 1,691 rubrics derived from NCCN guidelines
- Expert clinician validation showed 90% agreement with automated evaluations (Cohen's Kappa 0.77)
- AI models showed sharp performance degradation at higher reasoning levels (G3-G4), struggling to apply evidence in complex scenarios
- Models demonstrated high vulnerability to adversarial inputs and safety failures in certain clinical scenarios

## Why This Works (Mechanism)
The GAPS framework succeeds by directly anchoring evaluation to authoritative clinical guidelines, eliminating subjectivity inherent in manual benchmark creation. By automating question and rubric generation from structured medical knowledge, the framework scales effectively while maintaining clinical validity. The four-dimensional evaluation approach comprehensively captures the multifaceted nature of clinical reasoning, from basic knowledge recall to complex evidence application and safety awareness. The automated pipeline ensures consistency in evaluation criteria and enables objective comparison across different AI systems, addressing the reproducibility and scalability challenges that plague traditional manual benchmarking approaches.

## Foundational Learning
- Clinical guideline extraction: Understanding how to parse structured medical knowledge from authoritative sources like NCCN guidelines
- Why needed: Essential for creating evidence-based evaluation criteria that reflect current clinical standards
- Quick check: Verify that extracted recommendations align with official guideline text

- Multi-dimensional evaluation design: Implementing assessment across reasoning depth, completeness, robustness, and safety
- Why needed: Captures the full spectrum of clinical decision-making capabilities beyond simple accuracy metrics
- Quick check: Confirm that each dimension independently measures distinct aspects of clinical competence

- Automated rubric generation: Creating structured evaluation criteria from guideline content
- Why needed: Enables scalable and objective assessment without manual intervention
- Quick check: Validate that generated rubrics produce consistent scoring across similar clinical scenarios

- Adversarial perturbation testing: Designing challenging inputs to test model robustness
- Why needed: Identifies vulnerabilities in clinical reasoning under realistic uncertainty conditions
- Quick check: Ensure perturbations maintain clinical relevance while testing model limitations

## Architecture Onboarding

**Component Map:** Clinical Guidelines -> Knowledge Extraction -> Question Generation -> Rubric Creation -> Multi-dimensional Evaluation -> Scoring Engine

**Critical Path:** The pipeline follows a linear progression from guideline ingestion through question generation, rubric creation, evaluation across four dimensions, and final scoring. Each component depends on successful completion of the previous step.

**Design Tradeoffs:** Automated generation prioritizes scalability and consistency over nuanced expert judgment, potentially missing subtle clinical context. The framework trades some depth of evaluation for broad coverage and reproducibility. Focusing on structured guidelines may miss important aspects of clinical practice not explicitly documented.

**Failure Signatures:** Performance degradation at higher reasoning levels (G3-G4) indicates limitations in complex evidence application. High vulnerability to adversarial inputs suggests insufficient robustness to clinical uncertainty. Safety failures reveal gaps in harm prevention capabilities. Poor Adequacy scores indicate incomplete or imprecise clinical reasoning.

**First 3 Experiments:**
1. Validate automated rubric generation against expert clinician assessments on a subset of questions
2. Test model performance across all four evaluation dimensions using a diverse set of clinical scenarios
3. Conduct perturbation analysis with progressively challenging adversarial inputs to identify robustness thresholds

## Open Questions the Paper Calls Out
None

## Limitations
- Current benchmark covers only NSCLC guidelines, limiting generalizability across medical specialties
- Validation performed on single guideline source, requiring testing across diverse clinical domains
- Perturbation analysis focused on synthetic adversarial examples rather than comprehensive real-world clinical variations

## Confidence
- **High Confidence**: Automated pipeline implementation and reproducibility are well-documented with clear methodology
- **Medium Confidence**: Claims about overcoming scalability limitations require validation across multiple medical domains
- **Medium Confidence**: Safety evaluation results need broader testing beyond limited adversarial examples

## Next Checks
1. Extend GAPS to additional medical specialties (e.g., cardiology, infectious diseases) and evaluate whether the automated pipeline maintains high agreement with domain experts across diverse clinical domains

2. Conduct a comprehensive perturbation analysis using real-world clinical variations (incomplete patient histories, ambiguous symptoms, conflicting evidence) rather than synthetic adversarial examples to assess model robustness in practical settings

3. Perform longitudinal validation by updating the benchmark with newer guideline versions to test whether the framework can track AI system improvements over time and adapt to evolving clinical standards