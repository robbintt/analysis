---
ver: rpa2
title: Multimodal Functional Maximum Correlation for Emotion Recognition
arxiv_id: '2512.23076'
source_url: https://arxiv.org/abs/2512.23076
tags:
- emotion
- recognition
- multimodal
- mfmc
- modalities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work tackles multimodal emotion recognition from physiological\
  \ signals, where emotions manifest as coordinated yet heterogeneous responses across\
  \ central and autonomic systems. Existing self-supervised learning (SSL) approaches\
  \ rely on pairwise contrastive objectives that fail to capture higher-order interactions\
  \ across multiple modalities, such as coordinated brain\u2013body responses."
---

# Multimodal Functional Maximum Correlation for Emotion Recognition

## Quick Facts
- arXiv ID: 2512.23076
- Source URL: https://arxiv.org/abs/2512.23076
- Authors: Deyang Zheng; Tianyi Zhang; Wenming Zheng; Shujian Yu
- Reference count: 40
- Key outcome: MFMC achieves SOTA or competitive emotion recognition performance by capturing higher-order multimodal dependencies through functional maximum correlation

## Executive Summary
This paper addresses multimodal emotion recognition from physiological signals by proposing a novel self-supervised learning framework called Multimodal Functional Maximum Correlation (MFMC). The key insight is that emotions manifest as coordinated responses across central and autonomic nervous systems, requiring a method that captures higher-order interactions beyond pairwise relationships. MFMC maximizes multimodal dependence using a Dual Total Correlation objective with a tight sandwich bound, optimized through functional maximum correlation analysis.

The framework demonstrates significant performance improvements across three public affective computing benchmarks, particularly excelling in subject-independent protocols where inter-subject variability poses challenges. MFMC shows improvements from 78.9% to 86.8% accuracy in subject-dependent settings and from 27.5% to 33.1% in subject-independent settings on CEAP-360VR using EDA signals alone. The method also remains competitive within 0.8 percentage points of the best-performing method on challenging EEG subject-independent splits.

## Method Summary
MFMC introduces a principled self-supervised learning framework for multimodal emotion recognition that captures higher-order dependencies through functional maximum correlation. The core innovation lies in maximizing the Dual Total Correlation (DTC) across multiple physiological modalities using a trace surrogate optimization approach. By avoiding pairwise contrastive objectives, MFMC directly models joint multimodal interactions, making it particularly effective for capturing the complex, coordinated responses characteristic of emotional states across brain and body signals.

## Key Results
- Subject-dependent accuracy on CEAP-360VR improved from 78.9% to 86.8%
- Subject-independent accuracy on CEAP-360VR improved from 27.5% to 33.1% using EDA signal alone
- Competitive performance within 0.8 percentage points of best method on MAHNOB-HCI EEG subject-independent split
- Consistent SOTA or competitive performance across three public affective computing benchmarks

## Why This Works (Mechanism)
MFMC works by directly optimizing higher-order multimodal dependencies through functional maximum correlation, avoiding the limitations of pairwise contrastive approaches. The Dual Total Correlation objective captures joint interactions across multiple modalities simultaneously, which is crucial for modeling the coordinated brain-body responses that characterize emotional states. By deriving a tight sandwich bound and optimizing it with FMCA-based trace surrogate, the method achieves both theoretical rigor and practical effectiveness in capturing complex physiological patterns.

## Foundational Learning

**Dual Total Correlation (DTC)**: Measures total dependence among multiple random variables by quantifying shared information beyond pairwise relationships. Needed because emotions involve coordinated responses across multiple physiological systems that cannot be captured by simple pairwise correlations. Quick check: Verify DTC captures more shared information than sum of pairwise mutual informations.

**Functional Maximum Correlation Analysis (FMCA)**: Extends correlation analysis to functional spaces, enabling optimization over infinite-dimensional representations. Required to handle the complex, non-linear relationships in physiological signals. Quick check: Confirm FMCA converges and finds meaningful representations for physiological data.

**Sandwich Bound Optimization**: Provides a tractable approximation for intractable correlation maximization objectives. Essential for making DTC maximization computationally feasible. Quick check: Validate that the bound is tight and optimization remains stable across different initialization conditions.

**Multimodal Dependence Maximization**: Framework for jointly modeling relationships across multiple signal types. Critical for capturing the coordinated central and autonomic nervous system responses in emotion. Quick check: Test whether joint optimization improves over separate modality-specific training.

## Architecture Onboarding

Component Map: Physiological Signal Inputs -> FMCA Feature Extractors -> DTC Objective Function -> Trace Surrogate Optimizer -> Emotion Classification Output

Critical Path: The core computational path flows from raw physiological signals through FMCA-based feature extraction, where functional representations are learned to maximize the DTC objective via trace surrogate optimization, ultimately feeding into emotion classification layers.

Design Tradeoffs: The method trades computational complexity for richer dependency modeling - while pairwise contrastive methods are simpler, MFMC captures more comprehensive multimodal interactions at the cost of increased optimization complexity.

Failure Signatures: Performance degradation may occur when multimodal signals are poorly synchronized, when one modality dominates the DTC objective, or when the sandwich bound approximation becomes loose due to highly complex signal relationships.

First Experiments:
1. Baseline comparison using standard pairwise contrastive learning on the same datasets
2. Ablation study removing the DTC objective to isolate its contribution
3. Performance analysis under varying levels of multimodal signal quality and synchronization

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Theoretical framework relies on Dual Total Correlation approximation which may introduce errors
- Computational complexity may limit scalability to larger datasets or additional modalities
- Cross-dataset generalization performance is not thoroughly evaluated
- Practical deployment feasibility not assessed through runtime comparisons

## Confidence
- High confidence in theoretical framework and mathematical derivations
- Medium confidence in empirical performance improvements due to limited dataset diversity
- Medium confidence in claims about capturing "higher-order interactions" - needs more practical validation

## Next Checks
1. Evaluate MFMC's cross-dataset generalization by training on one dataset and testing on another
2. Conduct ablation studies removing the functional maximum correlation component to quantify its specific contribution
3. Measure and compare computational efficiency (training/inference time) against baseline methods to assess practical scalability