---
ver: rpa2
title: Training-Free Voice Conversion with Factorized Optimal Transport
arxiv_id: '2506.09709'
source_url: https://arxiv.org/abs/2506.09709
tags:
- conversion
- voice
- transport
- knn-vc
- mkl-vc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Factorized MKL-VC, a training-free modification
  for kNN-VC pipeline. The method replaces kNN regression with a factorized optimal
  transport map in WavLM embedding subspaces, derived from Monge-Kantorovich Linear
  solution.
---

# Training-Free Voice Conversion with Factorized Optimal Transport

## Quick Facts
- arXiv ID: 2506.09709
- Source URL: https://arxiv.org/abs/2506.09709
- Reference count: 0
- Primary result: Factorized MKL-VC significantly improves content preservation and robustness with short reference audio compared to kNN-VC, achieving performance comparable to FACodec

## Executive Summary
This paper introduces Factorized MKL-VC, a training-free modification for kNN-VC pipelines that replaces kNN regression with a factorized optimal transport map in WavLM embedding subspaces. The method addresses the non-uniform variance across WavLM dimensions by solving independent Monge-Kantorovich Linear maps per factorized block, ensuring effective feature transformation. Experiments on LibriSpeech and FLEURS datasets demonstrate that MKL-VC significantly improves content preservation and robustness with short reference audio compared to kNN-VC, while achieving performance comparable to FACodec.

## Method Summary
The method replaces kNN regression with a factorized optimal transport map derived from the Monge-Kantorovich Linear solution. WavLM embeddings are sorted by standard deviation and partitioned into K-dimensional blocks. For each block, the method computes the optimal transport map between source and target distributions using the formula T(i)(x) = μ₂ + Σ₁⁻¹ᐟ²(Σ₁ᐟ²Σ₂Σ₁ᐟ²)¹ᐟ²Σ₁⁻¹ᐟ²(x - μ₁), where μ and Σ represent mean and covariance matrices. This factorization addresses the non-uniform variance across dimensions, ensuring all components contribute to the transformation.

## Key Results
- MKL-VC significantly outperforms kNN-VC in content preservation, achieving WER=8.131 with K=2 versus kNN-VC's WER=13.748
- MKL-VC shows superior robustness with short reference audio, maintaining content integrity where kNN-VC fails
- Cross-lingual performance improves substantially, with MKL-VC achieving WER=39% in German-French conversion versus kNN-VC's WER=97%
- MKL-VC achieves performance comparable to FACodec, particularly in cross-lingual voice conversion domains

## Why This Works (Mechanism)

### Mechanism 1
kNN-VC fails with short references because it must find exact nearest neighbors in the target corpus. MKL-VC instead matches statistical distributions per dimension group using the Monge-Kantorovich Linear map, which can generate embeddings absent from the reference audio. The map T(x) = μ₂ + Σ₁⁻¹ᐟ²(Σ₁ᐟ²Σ₂Σ₁ᐟ²)¹ᐟ²Σ₁⁻¹ᐟ²(x - μ₁) transforms source distribution to target distribution analytically. Core assumption: WavLM embeddings within each factorized subspace follow approximately multivariate Gaussian distributions.

### Mechanism 2
WavLM embeddings exhibit highly non-uniform variance, with ~100/1024 dimensions dominating L2/cosine distance. Joint optimal transport would minimize cost primarily on high-variance dimensions, ignoring the rest. Factorization splits embeddings into K-dimensional blocks, solving independent MKL maps per block, ensuring all dimensions contribute to transformation. Core assumption: Covariance matrices are approximately block-diagonal.

### Mechanism 3
Lower MKL dimension K yields better content preservation; higher K yields better speaker similarity. Smaller K means finer factorization, which better satisfies the Gaussian assumption. Better distributional fit yields more accurate transport and content preservation. Larger K preserves more cross-dimensional correlations, capturing speaker identity better but risking mis-specification. Core assumption: The Gaussian assumption is more valid for smaller dimensional subspaces.

## Foundational Learning

- **Optimal Transport (Monge-Kantorovich formulation)**: Essential for understanding why MKL replaces nearest-neighbor matching with an analytical optimal transport map. Quick check: Explain why the Monge formulation (finding map T) differs from the Kantorovich formulation (finding coupling π), and why MKL assumes Gaussian distributions.

- **WavLM embedding structure and self-supervised speech representations**: Critical for understanding the method's exploitation of WavLM properties (meaningful cosine distance, non-uniform variance). Quick check: Why would a contrastive-learning-trained encoder like WavLM have "perceptually close sounds encoded close together" in latent space, and how does this relate to the paper's Figure 2 variance structure?

- **Covariance matrix square root and positive definite matrix operations**: Required for understanding the MKL formula computation. Quick check: Given covariance matrices Σ₁ and Σ₂, what numerical methods would you use to compute (Σ₁ᐟ²Σ₂Σ₁ᐟ²)¹ᐟ², and what happens if Σ₁ is singular?

## Architecture Onboarding

- **Component map**: Source Audio → WavLM Encoder → Source Embeddings → Sort dims by std deviation → Partition into N/K blocks of K dims → Compute μ₂, Σ₂ per block from Reference Audio → Apply MKL map T(i) per block → Concatenate transported blocks → HiFi-GAN Vocoder → Converted Audio

- **Critical path**: The MKL map computation per block. Errors in covariance estimation (especially Σ₂ from short references) or numerical instability in matrix square roots directly corrupt conversion quality.

- **Design tradeoffs**:
  - K=2: Best content preservation (WER≈8%), good for intelligibility-critical applications
  - K=8-16: Balanced performance, reasonable tradeoff
  - K≥64: High speaker similarity but content degradation

- **Failure signatures**: Robotic voice artifacts (absent in MKL-VC but present in other baselines), cross-lingual pronunciation leakage (mitigated by MKL-VC), content collapse at high K (K=256 shows WER=65%).

- **First 3 experiments**:
  1. **Gaussian validation on your data**: Before applying MKL to a new encoder or dataset, replicate Figure 3's Wasserstein distance analysis to verify the Gaussian assumption holds for your embedding space.
  2. **K parameter sweep for your use case**: Run Table 2-style ablation on your target domain to find optimal K, as the paper's K=2 optimum may not generalize.
  3. **Reference duration robustness test**: Test progressively shorter references (5s, 3s, 1s) on your target speakers to characterize where covariance estimation degrades unacceptably.

## Open Questions the Paper Calls Out
- Can the factorized MKL approach be effectively generalized to other speech encoders where Gaussianity assumptions may not hold?
- Does the block-diagonal covariance assumption discard significant inter-dimensional correlations necessary for modeling complex prosodic features?
- Can the trade-off between content preservation and speaker similarity governed by K be automatically optimized rather than manually tuned?

## Limitations
- Gaussian distribution assumptions in factorized subspaces are not rigorously validated across diverse datasets
- Covariance matrix estimation from short reference audio could be unstable for less common speakers or languages
- Block-diagonal covariance assumption for factorization lacks empirical validation of correlation structure in WavLM embeddings

## Confidence
**High confidence**: Claims about MKL-VC's improvement over kNN-VC in content preservation and robustness with short references (supported by Tables 2-4 and direct ablation studies).

**Medium confidence**: Claims about Gaussian distribution validity in factorized subspaces (supported by Figure 3 but limited to LibriSpeech) and the block-diagonal covariance assumption for factorization.

**Low confidence**: Claims about MKL-VC's behavior with references shorter than 5 seconds, performance with highly dissimilar speakers, and generalization to non-WavLM encoders without re-validation.

## Next Checks
1. **Gaussian assumption validation**: Replicate Figure 3's Wasserstein distance analysis on your data before applying MKL-VC to verify the Gaussian assumption holds.

2. **K parameter optimization for your domain**: Run Table 2-style ablation on your target speakers, reference durations, and language pairs to find optimal K.

3. **Reference duration robustness testing**: Systematically test MKL-VC with progressively shorter references (5s, 3s, 1s) on your target speakers, monitoring WER/CER and SIM degradation.