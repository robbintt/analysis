---
ver: rpa2
title: 'A Llama walks into the ''Bar'': Efficient Supervised Fine-Tuning for Legal
  Reasoning in the Multi-state Bar Exam'
arxiv_id: '2504.04945'
source_url: https://arxiv.org/abs/2504.04945
tags:
- tenant
- llama
- performance
- explanation
- first
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates whether smaller language models can achieve
  effective legal reasoning on the Multi-state Bar Exam with limited fine-tuning data.
  The authors fine-tune Llama 2 7B and Llama 3 8B using 1,514 MBE questions and test
  them on the 2022 MBE questions licensed from JD Advising.
---

# A Llama walks into the 'Bar': Efficient Supervised Fine-Tuning for Legal Reasoning in the Multi-state Bar Exam

## Quick Facts
- arXiv ID: 2504.04945
- Source URL: https://arxiv.org/abs/2504.04945
- Authors: Rean Fernandes; André Biedenkapp; Frank Hutter; Noor Awad
- Reference count: 40
- Key outcome: Domain-specific fine-tuning improves Llama 2 7B from 18.5% to 36.8% and Llama 3 8B from 35.8% to 52.5% accuracy on MBE questions

## Executive Summary
This paper evaluates whether smaller language models can achieve effective legal reasoning on the Multi-state Bar Exam with limited fine-tuning data. The authors fine-tune Llama 2 7B and Llama 3 8B using 1,514 MBE questions and test them on the 2022 MBE questions licensed from JD Advising. They compare models trained with and without structured IRAC explanations, testing different response formats (JSON, Markdown, Numbered list), prompt types (zero-shot vs one-shot), and answer orderings (answer-first vs explanation-first). Their results show that domain-specific fine-tuning improves accuracy from 18.5% to 36.8% for Llama 2 and from 35.8% to 52.5% for Llama 3, though both models plateau below the human baseline of 67.5%. Fine-tuning also dramatically reduces parsing failures, with both models achieving parse failure rates below 5% after 20 training samples.

## Method Summary
The study fine-tunes Llama 2 7B and Llama 3 8B using 1,514 MBE training questions with QLoRA (NF4, r=64, α=32) on a single 32GB GPU. Training uses cosine scheduler with warm restarts (warmup_ratio=0.1, cycles=2) for 10 epochs. The models are evaluated on 200 licensed 2022 MBE questions, testing structured vs unstructured explanations, three response formats (JSON, Markdown, NumberedList), and answer-first vs fact-first orderings. Performance is measured by accuracy and parsing failure rate, with a 67.5% human baseline.

## Key Results
- Domain-specific fine-tuning improves accuracy from 18.5% to 36.8% for Llama 2 and from 35.8% to 52.5% for Llama 3
- Fine-tuning dramatically reduces parsing failures, with both models achieving parse failure rates below 5% after 20 training samples
- Llama 3 benefits significantly from structured explanations, while Llama 2 shows minimal difference between structured and unstructured formats

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific supervised fine-tuning with limited samples improves legal reasoning accuracy in smaller LLMs.
- Mechanism: SFT exposes the model to task-specific patterns (question structures, legal domain vocabulary, answer justification formats) that are underrepresented in general pretraining. The model learns to map legal fact patterns to correct answer choices through gradient updates on domain-relevant tokens.
- Core assumption: The training questions are representative of the test distribution and do not overlap with the model's pretraining data.
- Evidence anchors:
  - [abstract]: "domain-specific fine-tuning improves accuracy from 18.5% to 36.8% for Llama 2 and from 35.8% to 52.5% for Llama 3"
  - [section 4.1]: "Llama 3 demonstrates higher initial accuracy (0.48)... while Llama 2 starts near random chance but exhibits steady, predictable gains (R² = 0.991)"
  - [corpus]: JBE-QA paper confirms domain-specific legal QA evaluation is tractable for LLMs; Analyzing Effects of SFT paper examines how SFT shapes model knowledge.
- Break condition: Accuracy gains plateau despite additional samples (observed for Llama 3 at ~20 samples), suggesting model capacity limits rather than data insufficiency.

### Mechanism 2
- Claim: Structured IRAC explanations improve learning consistency for more capable models (Llama 3) but provide minimal benefit for weaker models (Llama 2).
- Mechanism: IRAC formatting (Issue, Rule, Application, Conclusion) creates standardized training signals that reduce variance in how explanations guide the model toward correct answers. More capable models with larger context windows can leverage this structure; smaller models lack the capacity to exploit the organizational pattern.
- Core assumption: The distillation model (Llama 3 70B) produces IRAC explanations that are factually consistent with the original explanations.
- Evidence anchors:
  - [abstract]: "We distill the dataset using Llama 3 (70B) to transform explanations into a structured IRAC format"
  - [section 4.3]: "Llama 3 reveals markedly different learning patterns... structured format demonstrates strong correlation with training sample size (R² = 0.839), whereas unstructured explanations show poor correlation (R² = 0.248)"
  - [corpus]: Long-Short CoT Mixture SFT paper explores how reasoning structure in training data affects model behavior.
- Break condition: Llama 2 shows R² = 0.989 (structured) vs R² = 0.940 (unstructured)—essentially identical—indicating structure provides no advantage when base model capacity is insufficient.

### Mechanism 3
- Claim: SFT with minimal samples rapidly teaches output format adherence, dramatically reducing parsing failures.
- Mechanism: Next-token prediction training on formatted examples teaches the model both the stop conditions and structural constraints of the target output format. This learning occurs faster than domain knowledge acquisition because format patterns are syntactically simpler and more consistent across examples.
- Core assumption: Parsing failures are primarily due to format non-adherence rather than ambiguous or malformed training examples.
- Evidence anchors:
  - [abstract]: "fine-tuning also dramatically reduces parsing failures, with both models achieving parse failure rates below 5% after 20 training samples"
  - [section 4.2]: "fine-tuning with just a single sample reduces Llama 2's error rate to 30.98%... and Llama 3's rate to 12.50%"
  - [corpus]: Weak direct evidence; RoSTE paper addresses quantization-aware SFT but not format adherence specifically.
- Break condition: Parsing failures cannot be eliminated entirely (observed floor of ~1-2%) due to edge cases in model generation that escape structured training.

## Foundational Learning

- Concept: **QLoRA (Quantized Low-Rank Adaptation)**
  - Why needed here: The paper uses QLoRA to fine-tune 7-8B parameter models on a single 32GB GPU. Without quantization, full fine-tuning would require substantially more memory.
  - Quick check question: How does 4-bit quantization (NF4) enable larger batch sizes compared to standard LoRA?

- Concept: **IRAC Framework (Issue, Rule, Application, Conclusion)**
  - Why needed here: The paper uses IRAC as a structured reasoning format for distillation. Understanding this legal analysis framework is necessary to interpret why structured explanations might improve legal reasoning.
  - Quick check question: What is the intended function of each IRAC component in guiding legal analysis?

- Concept: **Option Selection Bias in LLMs**
  - Why needed here: The paper identifies inherent biases (Llama 2 prefers option C, Llama 3 prefers D) that affect multiple-choice performance. Recognizing this phenomenon is important for interpreting baseline accuracy.
  - Quick check question: How can you detect whether a model has an option selection bias on a balanced test set?

## Architecture Onboarding

- Component map:
  Data Pipeline: Question/solution extraction -> JSON consolidation -> IRAC distillation (Llama 3 70B) -> format-specific prompt templating
  Training Pipeline: QLoRA adapter configuration (r=64, α=32) -> cosine scheduler with warm restarts -> 10 epochs
  Inference Pipeline: Prompt formatting -> model generation -> regex-based response handler -> accuracy/bias evaluation
  Evaluation: 2022 MBE test set (200 questions, licensed from JD Advising)

- Critical path:
  1. Verify no train/test overlap (explicitly checked by authors)
  2. Run baseline inference on untrained models to establish parsing failure rates and accuracy floors
  3. Select generation parameters (format, prompt type, response type) before training
  4. Fine-tune with increasing sample sizes (1, 10, 20, 75, 125, 225) to identify plateau point
  5. Evaluate on held-out test set with same parsing logic

- Design tradeoffs:
  - **LoRA vs QLoRA**: QLoRA chosen for memory efficiency; LoRA did not show better performance under hardware constraints
  - **Structured vs unstructured explanations**: Only beneficial for Llama 3; adds distillation cost without guaranteed improvement
  - **JSON vs Markdown vs Numbered List**: JSON reduces parsing complexity but shows only marginal accuracy differences
  - **Answer-first vs fact-first**: Fact-first (explanation before answer) shows higher performance for Llama 3, suggesting reasoning-before-conclusion is advantageous

- Failure signatures:
  - **Sentence repetition loop**: Model repeats same tokens until max length; caused by failure to learn stop token (resolved by SFT)
  - **Hallucinated questions**: Model generates new fictitious questions after answering (Llama 2 baseline only)
  - **Prompt repetition**: Model echoes prompt text before generating response (Llama 3 baseline, causes parsing failures)
  - **Option bias**: Systematic over-selection of specific options (C for Llama 2, D for Llama 3)

- First 3 experiments:
  1. **Baseline characterization**: Run inference on untrained Llama 3 8B with zero-shot and few-shot prompts across all three response formats; record accuracy and parsing failure rates to establish comparison benchmarks.
  2. **Minimum viable SFT**: Fine-tune with 1 sample per domain (7 total) using IRAC-structured explanations; verify that parsing failures decrease by >50% compared to baseline.
  3. **Sample efficiency curve**: Train adapters with 10, 20, and 75 samples per domain; plot accuracy to identify where Llama 3 plateaus (expected: ~20 samples based on paper findings).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can advanced test-time inference strategies (e.g., chain-of-thought decoding) yield comparable legal reasoning performance to supervised fine-tuning without requiring dataset curation?
- Basis in paper: [explicit] The authors state, "we did not investigate whether computational resources might be better invested in advanced test-time inference strategies... rather than in dataset creation and model fine-tuning."
- Why unresolved: The study deliberately confined itself to the SFT setting to establish a fundamental baseline, excluding zero-shot reasoning techniques like chain-of-thought decoding.
- What evidence would resolve it: A comparative study measuring accuracy versus compute cost between SFT adapters and models utilizing chain-of-thought decoding on the MBE.

### Open Question 2
- Question: Is there a correlation between the factuality of the generated explanation and the correctness of the final answer choice?
- Basis in paper: [explicit] Section 5 states, "We do not perform an analysis of the correlation between the correctness or factuality of the explanation and the prediction of the model."
- Why unresolved: Evaluating this requires either expensive human expert verification or "LLM-as-a-Judge" methods which introduce inherent biases the authors wished to avoid.
- What evidence would resolve it: An analysis using human experts or a validated automated pipeline to grade the consistency between the model's reasoning steps and its selected option.

### Open Question 3
- Question: Do the MBE-optimized adapters generalize effectively to broader legal reasoning tasks found in benchmarks like LegalBench?
- Basis in paper: [explicit] The authors note in Section 2.2 that they found LegalBench tasks "too specific" for their MBE goal, but mention in Section 5 that "Future work will include evaluating our fine-tuned model adapters on the LegalBench."
- Why unresolved: The adapters were trained specifically on the multiple-choice MBE format and have not been tested on the diverse, open-ended tasks within LegalBench.
- What evidence would resolve it: Performance results of the fine-tuned adapters on the LegalBench suite of tasks.

### Open Question 4
- Question: Would alignment methods like Direct Preference Optimization (DPO) significantly improve performance over the SFT baseline?
- Basis in paper: [explicit] Section 1 states the authors "deliberately exclud[ed] reinforcement learning-based approaches... to quantify performance improvements from a more fundamental baseline perspective."
- Why unresolved: The study focused on supervised learning; the impact of preference optimization for legal reasoning in this data-limited regime remains untested.
- What evidence would resolve it: A comparison of SFT-only models against models further aligned using DPO with a curated preference dataset containing labeled samples of explanation-entailment.

## Limitations

- Evaluation is constrained by a limited test set (200 questions) that requires a paid license from JD Advising, preventing full reproducibility and independent verification.
- The study compares only two model sizes (7B and 8B) against a single human baseline (67.5%), without testing intermediate or larger models to establish scaling relationships.
- The distillation process using Llama 3 70B to create IRAC explanations introduces potential distributional shifts that are not characterized or validated against human-annotated explanations.

## Confidence

- **High Confidence**: Domain-specific fine-tuning improves accuracy (36.8% → 52.5%) and reduces parsing failures (<5% after 20 samples). SFT reduces option selection bias.
- **Medium Confidence**: Structured IRAC explanations provide differential benefit for Llama 3 but not Llama 2. 20 training samples suffice for Llama 3 to plateau.
- **Low Confidence**: The 67.5% human baseline is representative of expert performance on this specific test set. Format differences (JSON vs Markdown vs NumberedList) have minimal impact on accuracy.

## Next Checks

1. **Replicate parsing failure reduction**: Run zero-shot inference on Llama 3 8B with the same test set and parsing logic to verify the 30.98% baseline parsing failure rate, then fine-tune with 10 samples per domain and confirm parsing failures drop below 10%.

2. **Validate structured explanation benefit**: Train two identical Llama 3 8B models on the same 20 samples per domain, one with IRAC-structured explanations and one with unstructured explanations, then measure accuracy differences and correlation with sample size (R²).

3. **Test option bias elimination**: Analyze option selection frequencies on the test set before and after fine-tuning for both Llama 2 and Llama 3 to confirm that systematic biases (C for Llama 2, D for Llama 3) are reduced by at least 50% after training.