---
ver: rpa2
title: 'HSSBench: Benchmarking Humanities and Social Sciences Ability for Multimodal
  Large Language Models'
arxiv_id: '2506.03922'
source_url: https://arxiv.org/abs/2506.03922
tags:
- data
- answer
- qwen2
- question
- experts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HSSBench is a large-scale multimodal benchmark for evaluating Large
  Language Models on Humanities and Social Sciences (HSS) tasks across six languages.
  It contains 13,152 carefully curated samples spanning six categories and 45 types,
  with data generated through a novel multi-agent pipeline involving domain experts
  and automated agents.
---

# HSSBench: Benchmarking Humanities and Social Sciences Ability for Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2506.03922
- Source URL: https://arxiv.org/abs/2506.03922
- Reference count: 40
- Primary result: HSSBench exposes significant performance gaps in MLLMs on HSS tasks, with accuracy often below 60% compared to STEM benchmarks.

## Executive Summary
HSSBench is a large-scale multimodal benchmark designed to evaluate Large Language Models on Humanities and Social Sciences (HSS) tasks across six languages. It addresses the critical gap in current MLLM evaluations, which focus primarily on STEM disciplines while neglecting the interdisciplinary and abstract reasoning demands of HSS domains. The benchmark contains 13,152 carefully curated samples spanning six categories and 45 types, with data generated through a novel multi-agent pipeline involving domain experts and automated agents. When evaluated across 20+ mainstream MLLMs, models showed significantly lower performance on HSSBench compared to STEM-focused benchmarks, demonstrating the unique challenges HSS tasks pose for current MLLM architectures.

## Method Summary
HSSBench is an inference-only evaluation benchmark containing 13,152 multimodal samples across six HSS categories in six UN languages. The benchmark employs four evaluation settings: Multiple Choice vs. Open-Ended, crossed with Chain-of-Thought (CoT) vs. Direct prompts. Models must output answers in a specific `[[X]]` format for multiple choice. Open-ended answers are evaluated by a secondary LLM judge using detailed grading prompts. The dataset was generated through a multi-agent pipeline with domain experts, ensuring visual dependency through validation agents that filter questions answerable without images.

## Key Results
- MLLMs achieved significantly lower accuracy on HSSBench (often below 60%) compared to STEM benchmarks
- Chain-of-Thought prompting sometimes degraded performance, exacerbating hallucinations in visual tasks
- Models performed better with expert textual annotations than with raw images, indicating difficulty extracting fine-grained visual cues
- Performance varied significantly across the six UN languages, revealing multilingual alignment gaps

## Why This Works (Mechanism)

### Mechanism 1: Horizontal Reasoning Stress-Testing
HSSBench induces failure in MLLMs by forcing "horizontal reasoning" (interdisciplinary context linking) rather than the "vertical reasoning" (sequential logic) common in STEM benchmarks. The dataset structure requires models to map visual inputs to abstract cultural or historical concepts, demanding interpretation through regional and historical contexts rather than fixed symbolic systems.

### Mechanism 2: Disassociation of Visual Priors from Textual Priors
The benchmark exposes "cross-modal knowledge transfer" gaps where models possess textual knowledge but fail to trigger it when queries are mediated through images. The validation pipeline ensures visual dependency, revealing when models identify concepts textually but cannot connect them to corresponding visual features.

### Mechanism 3: Negative Constraint Induction via Confounding Options
Introducing "None of the above" or confounding options degrades performance by disrupting models' ability to weigh relative credibility. This forces binary verification rather than relative ranking, exposing models' lack of internal confidence calibration to reject plausible but incorrect distractors.

## Foundational Learning

- **Cross-Modal Knowledge Transfer**: The core challenge is retrieving domain-specific knowledge triggered by visual features. Quick check: Can you distinguish between identifying a "graph" (visual recognition) versus identifying a "marginal revenue curve indicating a monopoly" (cross-modal knowledge)?

- **Horizontal vs. Vertical Reasoning**: Understanding why STEM benchmarks are insufficient requires grasping this distinction. Vertical is step-by-step logic; Horizontal is context-linking and synthesis. Quick check: Does solving the problem require a fixed sequence of logical steps (Math) or synthesis of cultural context and visual cues (History)?

- **Negative Sampling in Evaluation**: The paper uses confounding options to prove models are guessing rather than knowing. Understanding this helps interpret result "truthfulness." Quick check: Why does adding "None of the above" to multiple-choice rigorously test model calibration?

## Architecture Onboarding

- **Component map**: Visual Encoder -> Projector -> LLM Backbone -> HSSBench Evaluator
- **Critical path**: Visual Feature Extraction → Concept Alignment (failure point) → Knowledge Retrieval (LLM) → Answer Generation
- **Design tradeoffs**: CoT vs. Direct Prompting (CoT can exacerbate hallucinations in HSS tasks); MC vs. Open-Ended (MC allows relative ranking while OE requires robust generative capabilities)
- **Failure signatures**: "Hallucination Drift" (generating incorrect historical background), "Surface Match" (selecting based on detected objects without understanding context), "Language Asymmetry" (performance variation across languages)
- **First 3 experiments**: 1) Visual Ablation Test (replace images with text descriptions), 2) Confounding Option Stress Test (add "None of the above" to 150 samples), 3) Cross-Lingual Consistency Check (evaluate same samples across all six languages)

## Open Questions the Paper Calls Out

- **Question 1**: What mechanisms cause Chain-of-Thought prompting to degrade performance in HSS tasks compared to direct answering? The paper identifies the phenomenon but doesn't propose mitigation strategies for the distraction or reasoning flaws caused by extended reasoning steps in horizontal thinking tasks.

- **Question 2**: How can MLLMs be trained to autonomously extract fine-grained visual cues necessary for HSS tasks without relying on external expert textual descriptions? Current models lack the capability to "focus" on deep symbolic information without explicit textual guidance.

- **Question 3**: How can MLLMs better internalize the mapping between abstract HSS concepts and their corresponding visual representations? The paper establishes the cross-modal knowledge transfer gap but leaves the solution for connecting this knowledge across modalities as an open challenge.

## Limitations

- The benchmark construction pipeline's multi-agent systems and exact validation rules are not fully specified, creating uncertainty about reproducibility
- Evaluation depends on LLM judges for open-ended questions, introducing potential instability in scoring
- Inference hyperparameters (temperature, max_tokens) for evaluated models are not explicitly listed, complicating exact reproduction

## Confidence

- **High Confidence**: Core claim that MLLMs perform significantly worse on HSSBench vs STEM benchmarks is well-supported by 20+ model evaluations
- **Medium Confidence**: Mechanism explaining horizontal vs vertical reasoning is logically sound but may not capture full challenge
- **Low Confidence**: Specific contribution of confounding options to performance degradation is supported but interpretation about verification vs ranking is speculative

## Next Checks

1. **Visual Ablation Study**: Replace images with detailed text descriptions for 200 samples and compare performance to assess specific "visual information loss" gap
2. **Confounding Option Stress Test**: Add "None of the above" to 150 randomly selected samples and measure accuracy drop to quantify impact on model calibration
3. **Cross-Lingual Consistency Audit**: Evaluate 100 samples across all six UN languages and analyze score variance to map knowledge alignment gaps