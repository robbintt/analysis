---
ver: rpa2
title: A Proof-of-Concept for Explainable Disease Diagnosis Using Large Language Models
  and Answer Set Programming
arxiv_id: '2512.23932'
source_url: https://arxiv.org/abs/2512.23932
tags:
- symptom
- language
- diagnosis
- medical
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces McCoy, a framework combining Large Language
  Models (LLMs) with Answer Set Programming (ASP) to create an interpretable disease
  diagnosis system. McCoy translates medical literature into ASP rules using LLMs,
  integrates patient data, and applies an ASP solver to generate diagnoses with transparent
  explanations.
---

# A Proof-of-Concept for Explainable Disease Diagnosis Using Large Language Models and Answer Set Programming

## Quick Facts
- arXiv ID: 2512.23932
- Source URL: https://arxiv.org/abs/2512.23932
- Reference count: 40
- 95–100% accuracy on selected diseases with interpretable explanations

## Executive Summary
McCoy is a neuro-symbolic framework that combines Large Language Models with Answer Set Programming to create an explainable disease diagnosis system. The approach automates the translation of medical literature into ASP rules using carefully structured prompts, then integrates patient symptoms and applies an ASP solver to generate diagnoses with transparent justifications. This addresses the labor-intensive nature of knowledge base construction in symbolic AI while maintaining interpretability through logical explanations for each diagnosis.

## Method Summary
The system translates medical literature into ASP code using structured prompts that guide LLMs to extract diagnostic rules with proper format and relationships. Patient symptoms are encoded as facts and processed by an ASP solver (Clingo) along with the generated rules. The solver finds stable models that satisfy diagnostic criteria while minimizing unsupported assumptions. xclingo generates explanation trees showing which rules and symptom relationships led to each diagnosis. The approach was evaluated on a public dataset with three diseases, achieving 95–100% accuracy while providing interpretable justifications for diagnoses.

## Key Results
- Achieved 95–100% accuracy on diagnosis of chickenpox, pneumonia, and common cold
- Successfully generated interpretable explanation trees for all diagnoses
- Demonstrated that structured prompts significantly improve LLM-generated ASP rule quality
- Showed potential for scaling beyond manual knowledge engineering through automated rule extraction

## Why This Works (Mechanism)

### Mechanism 1
Carefully designed prompts enable LLMs to extract structured diagnostic rules from unstructured medical text with sufficient fidelity for symbolic reasoning. Prompts specify output format constraints, require decomposed rules, and instruct the LLM to encode symptom relationships and alternative diagnoses. Core assumption: LLM's parametric medical knowledge is accurate enough for target diseases; prompt structure compensates for reasoning limitations. Evidence: Structured prompts produce decomposed rules with linked_symptom relations while poor prompts yield single diagnosis rules. Break condition: LLM hallucinations produce incorrect symptom-disease associations.

### Mechanism 2
ASP's stable model semantics guarantees diagnoses satisfy all encoded constraints while minimizing unsupported assumptions. Patient symptoms populate facts, diagnosis rules trigger when conditions are met, choice rules allow hypothetical symptoms, and minimization penalizes assumptions. Core assumption: Medical diagnostic criteria can be expressed as Horn-clause-style rules with closed-world assumption. Evidence: Formal specification of choice rules, constraints, and minimization in equations 10-12. Break condition: Under-specified or conflicting constraints cause no stable models or multiple equally-ranked diagnoses.

### Mechanism 3
The declarative rule structure enables automatic generation of human-readable diagnostic justifications. Each rule has explicit antecedents, and xclingo traces which rules fired and which symptoms triggered them. This produces explanation trees showing diagnosis chains. Core assumption: Clinicians can validate and trust logical traces even without understanding ASP syntax. Evidence: Explanation tree examples show symptom hierarchy and linked_symptom relations. Break condition: Deep, redundant paths or obscure symptom links without clinical grounding reduce trust.

## Foundational Learning

- Concept: Answer Set Programming (stable models, grounding, solving)
  - Why needed here: Understanding how declarative rules become executable programs and why minimization matters for diagnostic precision
  - Quick check question: Given rules `diagnosis(a) :- has(x), has(y).` and `diagnosis(b) :- has(x).`, if patient has `x` only, what diagnoses are derived?

- Concept: Prompt engineering for structured code generation
  - Why needed here: McCoy's accuracy depends critically on prompt design; poor prompts produce unusable output
  - Quick check question: What prompt elements differentiate Figure 3 (fails) from Figure 4 (succeeds)?

- Concept: Neuro-symbolic integration patterns
  - Why needed here: McCoy exemplifies LLM-as-translator + symbolic-solver pattern; understanding trade-offs vs end-to-end neural approaches
  - Quick check question: Why use ASP solver instead of having the LLM directly output diagnoses?

## Architecture Onboarding

- Component map: Literature Input -> Prompt Constructor -> LLM Module -> ASP Knowledge Base -> Patient Data Encoder -> Clingo Solver -> xclingo Explainer
- Critical path: Literature → Prompt → LLM → ASP rules → Merge with patient facts → Clingo solve → Parse answer set → Generate explanation tree
- Design tradeoffs:
  - Automation vs. verification: Fully automated rule extraction trades expert validation for scalability
  - Completeness vs. ambiguity: Richer rules improve accuracy but risk conflicting diagnoses
  - Assumption minimization vs. coverage: Strict minimization may miss valid diagnoses requiring unobserved symptoms
- Failure signatures:
  - Solver returns UNSATISFIABLE: Rules over-constrained or patient data contradicts all diagnosis paths
  - Multiple stable models with no ranking: Under-specified disease differentiation
  - Explanation tree references symptoms not in patient data: LLM hallucinated symptom links
  - Syntax errors in generated ASP: Prompt failed to enforce valid Clingo syntax
- First 3 experiments:
  1. **Reproduction test**: Implement McCoy on the Kaggle dataset (same 3 diseases); verify 95-100% accuracy claim; log all cases where solver returns multiple diagnoses
  2. **Prompt ablation**: Compare Figure 3 vs Figure 4 prompts on rule quality metrics (syntax validity, rule decomposability, symptom coverage); measure LLM-to-ASP translation error rate
  3. **Disease scaling**: Add 2-3 new diseases (e.g., influenza, bronchitis) with overlapping symptoms; measure if accuracy degrades and if explanation trees remain interpretable; test if linked_symptom rules cause cross-disease confusion

## Open Questions the Paper Calls Out
1. Can McCoy maintain diagnostic accuracy when scaling from 3 diseases to comprehensive clinical coverage?
2. How can LLM-generated diagnostic rules be validated for medical correctness?
3. Which guardrails effectively reduce hallucinations in ASP rule generation?
4. How does McCoy compare to existing medical diagnosis systems on standardized benchmarks?

## Limitations
- Evaluation limited to three diseases on one dataset without comparison to existing systems
- No systematic evaluation of LLM prompt effectiveness or rule validation
- Safety concerns from fully automated rule generation without expert validation
- Accuracy claims not benchmarked against black-box models or standard diagnostic tools

## Confidence
- **High**: ASP solver correctly implements diagnostic logic when provided valid rules; explanation generation via xclingo is technically sound
- **Medium**: LLM can translate medical text to syntactically valid ASP rules; structured prompts improve rule quality
- **Low**: System generalizes beyond 3 diseases; explanations meaningfully improve clinician trust without validation studies

## Next Checks
1. Evaluate McCoy on diseases with overlapping symptoms (e.g., flu vs. COVID) to test diagnostic differentiation and explanation clarity
2. Conduct user study with clinicians comparing McCoy explanations to standard diagnostic reports for interpretability and trust
3. Benchmark against black-box LLM diagnosis systems on same datasets, measuring both accuracy and explanation quality